{
  "title": "A novel time–frequency Transformer based on self–attention mechanism and its application in fault diagnosis of rolling bearings",
  "url": "https://openalex.org/W4200049650",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1901944692",
      "name": "Ding Yifei",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A169794247",
      "name": "Jia Min-ping",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2648532441",
      "name": "Miao Qiuhua",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2362477951",
      "name": "Cao Yudong",
      "affiliations": [
        "Southeast University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6786690893",
    "https://openalex.org/W2809350318",
    "https://openalex.org/W2258884143",
    "https://openalex.org/W2119566395",
    "https://openalex.org/W1597576211",
    "https://openalex.org/W2493554679",
    "https://openalex.org/W3208230679",
    "https://openalex.org/W6659116940",
    "https://openalex.org/W1975514583",
    "https://openalex.org/W6682387991",
    "https://openalex.org/W6758511600",
    "https://openalex.org/W2548257861",
    "https://openalex.org/W3041218800",
    "https://openalex.org/W2794869810",
    "https://openalex.org/W2801396593",
    "https://openalex.org/W6744300682",
    "https://openalex.org/W2794081072",
    "https://openalex.org/W2601590138",
    "https://openalex.org/W3093676973",
    "https://openalex.org/W6639569935",
    "https://openalex.org/W2925209208",
    "https://openalex.org/W3106650808",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6732520560",
    "https://openalex.org/W2090218979",
    "https://openalex.org/W6690072570",
    "https://openalex.org/W2115954437",
    "https://openalex.org/W2781541244",
    "https://openalex.org/W3000241683",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6638824847",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6737137598",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W6752378368",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6772129282",
    "https://openalex.org/W6686164453",
    "https://openalex.org/W6682610290",
    "https://openalex.org/W6794390325",
    "https://openalex.org/W2157494358",
    "https://openalex.org/W6725100599",
    "https://openalex.org/W1967352108",
    "https://openalex.org/W2907007702",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4229854985",
    "https://openalex.org/W3160064936",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2151060079",
    "https://openalex.org/W4236557640",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W4213226746",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W4251558360",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W3110992226",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W1581242383"
  ],
  "abstract": null,
  "full_text": "A novel time–frequency Transformer based on self–attention mechanism and its\napplication in fault diagnosis of rolling bearings\nYifei Ding, Minping Jia∗, Qiuhua Miao, Yudong Cao\naSchool of Mechanical Engineering, Southeast University, Nanjing 211189, PR China\nAbstract\nThe scope of data-driven fault diagnosis models is greatly extended through deep learning (DL). However, the classical\nconvolution and recurrent structure have their defects in computational e ﬃciency and feature representation, while\nthe latest Transformer architecture based on attention mechanism has not yet been applied in this ﬁeld. To solve\nthese problems, we propose a novel time–frequency Transformer (TFT) model inspired by the massive success of\nvanilla Transformer in sequence processing. Specially, we design a fresh tokenizer and encoder module to extract\neﬀective abstractions from the time–frequency representation (TFR) of vibration signals. On this basis, a new end-\nto-end fault diagnosis framework based on time–frequency Transformer is presented in this paper. Through the case\nstudies on bearing experimental datasets, we construct the optimal Transformer structure and verify its fault diagnosis\nperformance. The superiority of the proposed method is demonstrated in comparison with the benchmark models and\nother state-of-the-art methods.\nKeywords: Fault diagnosis, Deep learning, Transformer, Self–attention mechanism, Rolling bearings\n1. Introduction\nCurrently, rotating machinery is widely used in aviation, aerospace, shipbuilding, automobile and other industrial\nﬁelds, acting as the power source and support of many industrial systems [1]. The rolling bearing is a key vulnerable\npart of rotating machinery, which is prone to failure under a harsh working environment and alternating load [2].\nTherefore, the research of rolling bearing fault diagnosis is of great importance to ensure the safety and reliability of\nfacilities [3].\nThe ultimate goal of fault diagnosis is to recognize the status of the target component in the machine and thus\ndetermine whether the machine needs maintenance. Generally, existing fault diagnosis methods consist of two cate-\ngories: model-based [4] approaches and data-driven [5] methods. Model-based methods often require a lot of prior\nknowledge, making di ﬃcult to accurately establish the diagnosis model of composite components under complex\nconditions. Data-driven methods [5] aim to convert the data provided by sensors into a parametric or non-parametric\ncorrelation model. Data-driven methods can eﬀectively and rapidly process machinery signals, providing accurate di-\nagnosis results with low requirement for prior expertise. Therefore, they are becoming more and more attractive with\nthe development of various intelligent approach. The most common intelligent fault diagnosis method is currently de-\nveloped on machine learning (ML) approaches such ask-nearest neighbor (k-NN) [6], support vector machine (SVM)\n[2], self-organized map (SOM) network [7], etc. Most intelligent fault diagnosis methods for rolling bearings are\nbuilt on the processing and analysis of vibration signals [1], which employ a discrimination model with the input of\nman-made features extracted from acquired raw signals. Note that these man-made features include: 1) time domain\nstatistic moment [8] such as root-mean-square (RMS), kurtosis, skewness, etc., 2) frequency spectrum [9] processed\nby fast Fourier transform (FFT), power spectrum estimation, etc., 3) time–frequency domain energy [10] obtained with\nempirical mode decomposition (EMD), variational mode decomposition (VMD), etc., and even 4) fusion features [11]\nextracted with principal component analysis (PCA), etc.\n∗Corresponding author\nEmail address: mpjia@seu.edu.cn (Minping Jia)\nPreprint submitted to Elsevier December 7, 2021\narXiv:2104.09079v3  [cs.AI]  4 Dec 2021\nRecently, the development of deep learning (DL) allowed us to automatically learn representations from a large\namount of data, thus avoiding the need for manual design features [12]. Shao et al. [3] proposed a rolling bearing\nfault diagnosis method based on a deep belief network (DBN). Mao et al. [13] combined auto-encoder (AE) and ex-\ntreme learning machine (ELM) to diagnose fault mode of rolling bearings, which utilizes FFT spectrum of vibration\nsignals as input. Based on DL architecture, the convolutional neural network (CNN) which is speciﬁcally designed for\nvariable and complex signals, has shown great merits in feature extraction. Xu et al. [14] introduced VMD and deep\nconvolutional neural networks (DCNN) to perform fault classiﬁcation of the rolling bearing in wind turbines. Jia et\nal. [15] employed a deep normalized convolutional neural network (NCNN) for imbalanced fault classiﬁcation of ma-\nchinery and analyzed its mechanism via visualization. Besides, recurrent neural network (RNN), another signiﬁcant\nDL model, shows an advantage in learning internal features from the input of sequences, so it is also widely used in the\nﬁeld of diagnosis in dependence of time-series vibration signals. Liu et al. [16] oﬀered a fault diagnosis framework of\nrolling bearings with recurrent neural network (RNN) and auto-encoder (AE). To solve the problem of convergence\ndiﬃculty and gradient extinction of common RNNs, Chenet al. [17] introduced the long short-term memory network\n(LSTM), a variant of RNN, to the prediction of mechanical state. Zhao et al. [18] came up with an end-to-end fault\ndiagnosis method based on LSTM neural network, which can directly classify the raw process data without speciﬁc\nfeature extraction and classiﬁer design. In addition to the above listed, a large number of fault diagnosis methods\nbased on deep learning are constantly proposed [19, 20]. CNN and RNN are the two most common architectures in\nthe ﬁeld of DL and DL-based fault diagnosis. However, these two types of neural networks have their own defects.\nFor example, RNNs and their variants are not suitable for parallel computation, which is very ine ﬃcient for training\non large-scale datasets. In addition, RNN variants still cannot completely avoid long-range dependence problem, that\nis, the diﬃculty in establishing an eﬀective connection between distant sequences [21]. As for CNN, another pillar of\nDL, it also suﬀers from some shortcomings such as the lack of capturing the relationships between targets, the equal\ntreatment of all pixel points and lack of pertinence [22]. Besides, the local receptive ﬁeld of convolutional kernel\nresults in the need for numerous convolutional layers to be stacked in order to obtain global information [23].\nNowadays, attention mechanism, which can associate di ﬀerent positions of a sequence to compute its unique\nrepresentation, has now been successfully applied in a variety of tasks including natural language processing (NLP),\ncomputer vision (CV), and even fault diagnose [24]. Long et al. [25] introduced a motor fault diagnosis method\nusing attention mechanism and improved AdaBoost. Li et al. [24] leveraged the attention mechanism to improve the\ndata-driven diagnosis approach and visualized its e ﬀect on learned knowledge. The attention mechanism assists the\ndeep network to focus more on informative data segments and ignore those that contribute less to the ﬁnal output.\nHowever, all these attempts only embed attention mechanism as an auxiliary module into backbone models such as\nCNN or RNN, which cannot completely avoid the defects of these classical models.\nTo take full advantage of the attention mechanism, Vaswani et al. [26] proposed a new architecture based only\non attention mechanism – Transformer, which abandons all the recurrent and convolutional structures. We call this\nversion vanilla Transformer. The proposal of Transformer has set o ﬀ a revolution in the ﬁeld of NLP. So far, there\nhave been numerous varieties of Transformer, and a large number of successful practices have been put into machine\ntranslation, sentence generation, etc [27]. Computer vision and other ﬁelds are also attempting to introduce and\nimprove Transformer to meet the new challenges [28]. Considering that fault diagnosis often needs to process the\nsignal sequence and extract its internal correlation, Transformer-like models should have yielded unusually brilliant\nresults in this ﬁeld. Besides, even when dealing with two-dimensional input such as time–frequency representation,\nTransformer is good at grasping its inherent temporal correlation. Unfortunately, having said that, Transformer has\nnot been used in fault diagnosis and related ﬁelds.\nTo better model the temporal information in bearing signals, construct long-distance dependence and extract more\neﬀective hidden representation from its time–frequency representation (TFR), a new model named time–frequency\nTransformer (TFT) is proposed in this paper. Constrained by the di ﬃculty in extracting useful features directly from\nthe raw vibration signal [24], we apply synchrosqueezed wavelet transform (SWT) [29] to obtain time–frequency\nrepresentations (TFRs). SWT has been applied to many prognostic and health management (PHM) studies of bearings\ndue to their good performance in non-stationary vibration signal processing [30, 31, 32, 33]. Then, TFT is proposed\nto provide a discriminate model between TFRs and bearing fault modes. We designed a novel tokenizer focused on\nTFRs, and an encoder composed of Transformer blocks to establish hidden representations. Thus, we proposed an\nend-to-end approach for fault diagnosis. Through the case studies on bearing experimental datasets, we constructed the\noptimal Transformer structure and veriﬁed the performance of the proposed diagnosis method. Through comparison\n2\nwith the benchmark models and other state-of-the-art methods, superiority of the proposed method is proved. The\nmain contributions of this paper can be listed as follows.\n1) We proposed a novel time–frequency Transformer, which can avoid some drawbacks of classical models such as\nRNN and CNN, to extract e ﬀective information from time–frequency representation with only attention mecha-\nnism.\n2) We proposed an end-to-end fault diagnosis framework based on time–frequency Transformer and synchrosqueezed\nwavelet transform, which proves to be eﬀective and superior on bearing experimental datasets.\n2. Preliminaries\nThis section will brieﬂy introduce the vanilla Transformer proposed by Vaswani et al. [26] in 2017. Variants\nof Transformer applicated in the ﬁelds of natural language processing (NLP) and computer vision (CV) will also be\nreviewed.\n2.1. Transformer\nRecurrent models have shown a good capability to process sequence input in the form of [x1,x2,..., xt]. Along\nthe direction of the input sequence, they generate a sequence of hidden states ht, as a function of the previous hidden\nstate ht−1 and the input token xt for position t. At each step the model is auto-regressive, consuming the previously\ngenerated symbols as additional input when generating the next. This inherently sequential nature precludes paral-\nlelization within training examples, which becomes critical at longer sequence lengths. Vaswani et al. [26] proposed\nTransformer, a new architecture relying entirely on attention mechanism to draw global dependencies between input\nand output. Transformer completely abandons the traditional recurrent structure to realize the parallel calculation of\nsequence input. In addition, convolution operation which is diﬃcult to globally model the relationship between local\nfeatures is eliminated from in Transformer [21].\nEncoder\nDecoder\nFig. 1: Architecture of the vanilla Transformer [26].\n3\nTransformer is a multilayer structure by stacking Transformer blocks, whose vanilla form is shown in Fig. 1. The\nTransformer block is characterized by a multi-head self–attention mechanism, a position-wise feed-forward network,\nlayer normalization [34] module and residual connector [35]. The input to the Transformer is often a tensor of\nshape Rb ×Rn, where b is the batch size, n is the sequence length (note that the di ﬀerence from the dimension of\nsequence). The input ﬁrst passes through an embedding layer, which converts each one-hot token into an embedding\nof d dimensions to obtain a new tensor, i.e., Rb ×Rn ×Rd. Then, the new tensor is added to a sinusoidal position\nencoding, and passes through a multi-head self–attention module. The input and output of the multi-head self–\nattention are connected by a layer normalization layer and a residual connector. The combined output is then sent\nto a two-layer position-wise feed-forward network, which similarly connects the input and output through a residual\nconnector and a layer normalization layer. Such sublayer residual connectors with layer normalization have the\nfollowing form\nXout = LayerNorm(FA/FF (Xin) + Xin) (1)\nwhere FA/FF denotes multi-head self–attention or position-wise feed-forward layers.\n2.1.1. Multi-head self–attention\nThe multi-head self–attention mechanism is a key deﬁning characteristic of Transformer models, whose mecha-\nnism behind can be viewed as learning an alignment, that is, each token in the sequence attempts to gather information\nfrom others [36]. Generally, through linear transformations on a group of input embeddings X with dimension dmodel,\nwe get queries Qs = XWq\ns and keys Ks = XWk\ns with dimension dk, and values Vs = XWv\ns with dimension dv. A\nsingle-head scaled dot-product attention calculates the dot products of all queries and keys, divides each by scaling\nfactor √dk, and apply a softmax function to obtain the weights on the values.\nAs (Qs,Ks,Vs) = softmax\n(QsK⊤\ns√dk\n)\nVs (2)\nHowever, instead of mapping with only one version of linear transformations, it is more beneﬁcial to project the\ninput tokens to diﬀerent queries, keys and values h times through diﬀerent learned linear transformations. Thus, the\nso-called multi-head self–attention is introduced. Through the parallel self–attention calculation on queries, keys, and\nvalues of each projected version, h diﬀerent outputs headi are obtained. These headi are concatenated and once again\nprojected, resulting in the multi-head self–attention\nAh (X) = concat (head1,..., headh) WO\nwhere headi = As\n(\nXWq\ni ,XWk\ni ,XWv\ni\n) (3)\nwhere Wq\ni ∈Rdmodel×dk , Wk\ni ∈Rdmodel×dk and Wv\ni ∈Rdmodel×dv denote i-th version of linear projection on embeddings X to\nobtain diﬀerent queries, keys and values, respectively. WO ∈Rh·dv×dmodel denotes the linear projection on concatenated\nmulti-head. Note that dk = dv = dmodel/h in Transformer.\n2.1.2. Position-wise feed-forward layers\nThe output of multi-head self–attention module is then passed through a two-layer feed-forward network, whose\nhidden layer is activated by ReLU. This feed-forward layer operates on each position independently hence the term\nposition-wise.\nFF (XA) = ReLU (0,XAW1 + b1) W2 + b2 (4)\nwhere W1 ∈Rdmodel×df f , W2 ∈Rdf f ×dmodel , b1 ∈Rdf f , b2 ∈Rdmodel denotes the weights and bias of two layers, respectively.\n2.1.3. Transformer block\nTransformer composes of several stacked Transformer blocks. A Transformer block usually contains a multi-head\nself–attention module and a position-wise feed-forward module, both of which use a residual connector and layer\nnormalization to get the combined output of the module.\nXA = LayerNorm (Ah (X) + X)\nXFF = LayerNorm (FF (XA) + XA) (5)\n4\nwhere XA and XFF are the output of the multi-head self–attention module and the position-wise feed-forward module,\nrespectively. Note that the stacked multiple Transformer blocks take the same structure, while do not share the same\nparameters.\n2.1.4. Transformer Mode\nIt is important to note the diﬀerences in the mode of usage of the Transformer block. Transformers generally can\nbe divided into three categories, named: 1) encoder-only (e.g., for classiﬁcation), 2) decoder-only (e.g., for language\nmodeling), and 3) encoder-decoder (e.g., for machine translation). The vanilla Transformer proposed by Vaswani uses\nan encoder-decoder structure (as shown in Fig. 1) for machine translation, which is a Seq2Seq problem. The decoder\nof vanilla Transformer uses Transformer blocks that are diﬀerent from the aforementioned those for the encoder. The\nproposed method in this paper, however, adopts an encoder-only structure, so details of decoders are not introduced.\n2.2. Transformer variants\nRecurrent Neural Networks, especially LSTM [37] and GRU [38], have been widely used for sequence modeling\nand inference problems such as machine translation and language modeling before Transformer was proposed [27].\nThe proposal of Transformer brings brand new solutions to the NLP community. Devlin et al. [39] proposed the\nbidirectional Transformers (BERT), which is one of the most powerful models in the NLP ﬁeld. Besides, models\nsuch as XLNet [40] and GPT [41] further expand the application of Transformer. Tay et al. [42] surveyed a dizzying\nnumber of eﬃcient Transformer variants, providing an organized and comprehensive overview of existing work and\nmodels across multiple application ﬁelds.\nTransformer is used for sequence modeling to solve several problems of traditional recurrent structures: 1) Di ﬃ-\nculty in training parallelization. 2) Di ﬃculty in modeling long-range dependencies. In many sequence transduction\ntasks, learning long-range dependence is a key challenge. The recurrent models cannot establish a direct connection\nbetween non-adjacent tokens, which greatly limits the overall understanding of the input sequence. 3) The problem\nof gradient explosion and gradient vanishes. The proposal of LSTM and GRU alleviated this problem but did not\nfundamentally eliminate the weight accumulation caused by multiple recurrences.\nThe great success of Transformer in NLP has also attracted the attention of researchers in the ﬁeld of CV . For many\nyears, convolutional neural network (CNN) is the fundamental pillar in CV . However, this convolution operating on\nthe pixel matrix also has some defects, e.g., it is di ﬃcult to capture the relationship between targets, and treats all\npixels equally without pertinence [22]. Many attempts [43, 44, 45] introduce attention mechanisms into visual tasks.\nThe trend of using Transformer as neural network module is more and more obvious. Dosovitskiyet al. [28] proposed\na vision Transformer (ViT) for supervised image classiﬁcation. It divides the image into several patches, and patches\nare treated the same way as tokens (words) in an NLP application. Wu et al. [22] input convolution image features\ninto Transformer as tokens, and used them for image classiﬁcation and object detection. Guo et al. [46] put forward\na point cloud Transformer (PCT) for point cloud learning to solve 3D computer vision problems.\nRNN and CNN, as the two most important network structures in DL, have been widely used in the ﬁeld of bearing\nPHM. The vibration signals of bearings contain abundant temporal correlation, which can be well modeled by RNN\nto give eﬀective fault diagnosis or residual useful life (RUL) prediction. CNN can extract useful fault or degradation\ninformation from the feature matrix composed of time, frequency and time–frequency domain features. A large\nnumber of model implementations based on CNN are also derived as reviewed in Section 1. Sometimes, scholars also\nbuild a hybrid model combining these two structures. A large number of attempts broaden the application of DL in\nthe ﬁeld of bearing PHM. However, as mentioned above, these two models are now dwarfed for their inherent defects\nby the emergence of Transformer. To the best of our knowledge, no Transformer variant has been proposed to model\nthe vibration signal and its TFRs, which can be useful for bearing PHM.\n3. Time–frequency Transformer\nThis section will introduce the proposed time–frequency Transformer (TFT) in detail, whose overall diagram is\nshown in Fig. 2. The network architecture is mainly composed of a tokenizer, an encoder and a classiﬁer.\n5\nTime-frequency Transformer (TFT)\nTransformer Encoder\nFlatten & Linear Projection\nClassifier\nPosition + Patch    \nEmbeddings\n* Class embedding\nInput Embeddings\nMulti-Head\nSelf-Attention\nK Q V\nAdd & Norm\nFeed Forward\nAdd & Norm\n*0 1 2 3 4 5 6 7 8 …\nTFR Patches\n...Time-frequency\nrepresentation\nN ×\nFig. 2: The proposed time–frequency Transformer.\n3.1. Tokenizer\nThe vanilla Transformer accepts a 1D token sequence and obtains token embeddings with a tokenizer by dictionary\nquery. However, the input token here to be processed is time–frequency representation (TFR), rather than the words\nthat can be queried in a dictionary. Thus, a speciﬁc tokenizer needs to be designed. To process 2D TFR data, we will\ndesign a new tokenizer module, which mainly includes ﬂattening, segmentation, linear mapping and adding position\nencodings.\n3.1.1. Token embedding\nTo obtain the tokens sequence, TFR is segmented into several patches along the time direction. Speciﬁcally, given\na TFR x ∈RNt×Nf ×C, where Nt and Nf are the length in time and frequency direction, respectively. C is the number\nof channels, which generally refers to the stacked layers of multi-sensor signals. We ﬁrst reshape x into sequence\nx′∈RNt×(Nf ·C) to ﬂatten multiple channels, and cut it along the time direction to get a patch sequence\n[\nx1\np,x2\np,..., xNt\np\n]\nwith length Nt, where xi\np ∈RNf ·C, i = 1,..., Nt . Then, to obtain the sequence of token embeddings with dimension\ndmodel, a learnable linear transformation is used to obtain the projected patches sequence xt. This process is expressed\nas:\nx\nreshape\n−→xp\nxt = xpWt\n(6)\nwhere Wt ∈RNf ×dmodel denotes the learnable linear mapping. Such processing is based on the view that TFR is formed\nby splicing the instantaneous spectrum of the signal over a period of time. This is diﬀerent from gridding segmentation\non images by ViT [28], because we believe that grid cutting TFR will not retain instantaneous spectrum estimation at\na certain time in each patch. The segmented TFR patches can be regarded as a sequence of instantaneous spectrum in\na period of time, and processing such temporal sequence is the strength of Transformer-based structure.\n3.1.2. Class token\nSimilar to the class token in BERT [39], a randomly initialized trainable embedding x 0\nt = xclass ∈Rdmodel is added\nto the beginning of the embedded token sequence. Thus, an embedding sequence x t =\n[\nxclass; x1\npWt,x2\npWt,..., xNt\np Wt\n]\nwith length Nt + 1 is obtained. Note that output z 0\nN of the class token after processed by the subsequent Transformer\nencoder will serve as the hidden representation of TFR.\n6\n3.1.3. Position encoding\nSince the Transformer contains no recurrence or convolutional operations, to make full use of the sequence order,\nwe should inject some information about the relative or absolute position of the tokens into the embedding sequence.\nVanilla Transformer employs a kind of sinusoid position encoding based on corpus dictionary and token location\n[26], which is not suitable for the problem we are trying to solve. We propose to use a learnable position encoding\nEpos ∈R(Nt+1)×dmodel to extract position information more ﬂexibly through the learning process. The position encodings\nand token embeddings are added to get the input embeddings\nz0 =\n[\nxclass; x1\npWt,x2\npWt,..., xNt\np Wt\n]\n+ Epos (7)\nTwo types of learnable position encodings, 1D and 2D, are considered. 1) The 1D position encoding E1d pos ∈\nR(Nt+1)×dmodel is broadcasted from a vector e1d ∈RNt+1, i.e., E1d pos = broadcast(e1d). 1D position encoding can only\nencode the relative and absolute position information among Nt + 1 tokens since the vector elements in the same\ntoken share the same encoding. 2) 2D position encoding E2d pos ∈R(Nt+1)×dmodel is a learning matrix with dimensions\n(Nt + 1) ×dmodel, so it can simultaneously encode the location information between and within the tokens. Subsequent\ncase studies will analyze the performance of these two encodings.\n3.2. Encoder\nIn TFT, the encoder can be regarded as a feature extraction structure, which undertakes the task of mining cat-\negory related information from the input embeddings sequence. The Transformer encoder, which is composed of\nN Transformer blocks, takes the embedded sequence z 0 as the input. Our TFT model basically employs the vanilla\nTransformer block structure, that is, multi-head self–attention module and position-wise feed-forward layers with\nresidual connector and layer normalization, which has been described in Section 2. Transformer block and multi-head\nself–attention mechanism inside are the key deﬁning characteristics of Transformer-like models. Besides, we also\nmade the following improvements.\n3.2.1. GeLU activation\nTo improve the convergence of the network, we adopt Gaussian error linear units (GeLU) activation [47] in feed-\nforward layers instead of ReLU activation used in vanilla Transformer. GeLU is deﬁned as the product of input x and\nmask m ∼Bernouli(Φ(x)), where Φ(x) = P(X ≤x), X ∼N(0,1) is the cumulative distribution function of the standard\nnormal distribution. This distribution is chosen since neuron inputs tend to follow a normal distribution, especially\nwith layer normalization. In this setting, inputs have a higher probability of being “dropped” as x decreases, so the\ntransformation applied to x is stochastic yet depends upon the input.\nGeLU(x) = xP (X ≤x) = xΦ (x) = x ·1\n2\n[\n1 + er f\n(\nx/\n√\n2\n)]\n(8)\nWe can estimate GeLU as\n0.5x\n(\n1 + tanh\n[√\n2/π\n(\nx + 0.044715x3)])\n(9)\nif greater feedforward speed is worth the cost of exactness. GeLU and ReLU are plotted in Fig. 3. It can be seen that\nGeLU activation is continuously di ﬀerentiable and has more obvious nonlinearity than the non-di ﬀerentiable ReLU\nactivation at x = 0.\n3.2.2. Transformer blocks in TFT\nThe embedding sequence z0 =\n[\nz0\n0; z1\n0; ... ; zNt\n0\n]\nobtained by tokenizer goes through multiple Transformer blocks to\nextract the connection among the tokens using self–attention mechanism. An encoder contains N Transformer blocks\ncan be represented as\nz′\nl = LayerNorm (Ah (zl−1) + zl−1) ,l = 1,..., N\nzl = LayerNorm\n(\nFF\n(\nz′\nl−1\n)\n+ z′\nl−1\n)\n,l = 1,..., N (10)\n7\n/uni00000017\n/uni00000015\n/uni00000013/uni00000015\n/uni00000013\n/uni00000014\n/uni00000015\n/uni00000016/uni00000035/uni00000048/uni0000002f/uni00000038\n/uni0000002a/uni00000048/uni0000002f/uni00000038\nFig. 3: The ReLU and GeLU (µ= 0,σ = 1).\nwhere Ah(·) denotes the multi-head self–attention function with h heads in Eq. (2). FF (·) denotes the position-wise\nfeed-forward module with dmodel dimensions input and output, and df f dimensions hidden layer. That is, the multi-\nhead self–attention modules and the feed-forward modules are alternately connected. The extensive use of residual\nconnectors and layer normalization reduces the training di ﬃculty of deep network, which is conducive to faster and\nmore stable convergence.\nIn TFT, the class token used to output classiﬁcation features is regarded as the next token to be predicted, that\nis, the output of token characterizing category is regarded as an auto-regressive prediction problem. This kind of\nsetting is born out of the idea that obtaining the information needed by the words to be predicted from the front words\nwhen Transformer is ﬁrst used in NLP tasks. When applied to classiﬁcation problems, this solution also enables\nTransformer to better establish the relationship between the input sequence and the class token to be predicted. Thus,\nthe hidden features z0\nN of the class token output through each layer of the encoder can contain suﬃcient information in\nthe input sequence. In the last layer of the encoder, the hidden features z0\nN will be served as the output of the encoder\nfor subsequent classiﬁcation.\n3.3. Classiﬁer\nThe function of classiﬁer is to map the hidden features with category information to one-hot encoding of class\nlabels. The classiﬁer consists of two layers of feed-forward multi-layer perceptron (MLP) and a softmax activation,\nCLA\n(\nz0\nN\n)\n= softmax\n(\nGeLU\n(\nz0\nN Wc1 + bc1\n)\nWc2 + bc2\n)\n(11)\nwhere Wc1 ∈ Rdmodel×df f , Wc2 ∈ Rdmodel×Ncla , bc1 ∈ Rdf f , bc2 ∈ RNcla are the weights and biases of the two layers,\nrespectively. Ncla is the number of categories. To reduce the number of hyperparameters, the hidden layer uses the\nsame hidden dimension df f as in encoder.\nGenerally speaking, we can easily know the probability of each category according to the softmax output of the\nnetwork. Thus, the category of input samples can be predicted.\n3.4. Training of TFT\nTraining of TFT follows the general deep learning scheme that using stochastic gradient descent (SGD) and error\nback-propagation (BP) algorithm to minimize the empirical risk. Given a training set D = {xi,yi\n}n\ni=1 contains n\nsamples, the network adopts the cross-entropy (CE) loss function which is suitable for the classiﬁcation problem\nJ(θ) = 1\nn\nn∑\ni=1\nLCE\n(yi,ˆyi\n) (12)\nwhere yi and ˆyi are the expected and estimated output of sample xi. θ denotes the trainable parameters of TFT and\nLCE (·) denotes the cross-entropy loss function.\n1) Optimizer: We use Adam optimizer [48] for TFT training. Adam wields an adaptive gradient strategy to accelerate\ntraining error convergence and allow the training trajectory to cross non-smooth regions of the loss landscape [49].\n8\n2) Regularization: We introduce dropout [50] into each sub-module of the network, which only plays a role in\nthe training stage. By cutting o ﬀ the connections of some neurons, dropout forces the network to learn more\nrobust parameters to reduce overﬁtting. In addition, we use label smoothing [51], mainly through soft one-hot to\nadd noise, which reduces the weight of the real sample label category in the calculation of the loss function, to\nsuppress the over-ﬁtting eﬀect.\nThe detailed TFT training steps are shown in Algorithm 1.\nAlgorithm 1:Training of time–frequency Transformer.\nInput: Training set X= {xi,yi\n}nS\ni=1, where xi ∈RNt×N f ×C , batch size nb\n1 Initialize\n{\nW(l),b(l)}\nof TFT;\n2 for epoch=1,2,. . . ,maxepoch do\n3 for step=1,2,. . . ,maxstep do\n4 //Tokenizer\n5 for each xi in {xi}nb\ni=1 do\n6 Flatten and slice xi into patches sequence, obtain xi,p =\n[\nx1\ni,pWt; ... ; xNt\ni,pWt\n]\n;\n7 Linear projection, obtain\n[\nx1\ni,t; ... ; xNt\ni,t\n]\n;\n8 Add class token, obtain xi,t =\n[\nxclass; x1\ni,pWt; ... ; xNt\ni,pWt\n]\n;\n9 Add position encoding, obtain zi,0 = xi,t + Epos ;\n10 Stack batches, obtain input sequences Zo;\n11 end\n12 // Encoder\n13 for block l=1,. . . ,N do\n14 Z′\nl = LayerNorm (Ah (Zl−1) + Zl−1);\n15 Zl = LayerNorm\n(\nFF\n(\nZ′\nl\n)\n+ Z′\nl\n)\n;\n16 end\n17 // Classiﬁer\n18 Obtain hidden representation of class token Z0\nN ;\n19 ˆY = softmax\n(\nGeLU\n(\nZ0\nN Wc1 + bc1\n)\nWc2 + bc2\n)\n;\n20 // Calculate loss and gradient descent\n21 Cross entropy LCE\n(\nY, ˆY\n)\n;\n22 Batch loss J\n(\nY, ˆY; W(l),b(l))\n;\n23 Calculate gradients ∂J\n∂W(l) , ∂J\n∂b(l) ;\n24 Apply gradients W(l) ←W(l) −η ∂J\n∂W(l) ,b(l) ←b(l) −η ∂J\n∂b(l) ;\n25 end\n26 end\nOutput: Weights and biases\n{\nW(l),b(l)}\n4. Fault diagnosis framework based on TFT\nTo improve the generalization performance of intelligent fault diagnosis and speed up learning and inference, a\nnew fault diagnosis method of rolling bearings based on TFT is proposed. Speciﬁcally, the process framework of the\nfault diagnosis method based on TFT is shown in Fig. 4, its speciﬁc diagnostic steps can be described as follows:\n1) Collecting vibration signals from the rolling bearings.\n2) Converting the collected vibration signals into TFRs through synchrosqueezed wavelet transforms (SWT), and\nlabel the samples for training.\n3) The relevant hyperparameters and model structure of the TFT are determined.\n4) The established model is fully trained and then applied to identify test samples.\n5) Outputting the fault diagnosis results and evaluating the performance of the proposed method.\n9\nTransformer Encoder\nFlatten & Linear Projection\nClassifier\nInput Embeddings\nMulti-Head\nSelf-Attention\nK Q V\nAdd & Norm\nFeed Forward\nAdd & Norm\n*0 1 2 3 4 5 6 7 8 …\n...\nFault diagnosis via time-frequency TransformerModel selection\n mod eld  ffd  h  N  dpr  Pos Average \nacc % Params \nbase 64 256 8 6 0.1 1D 99.97 335016 \nA \n16 32     99.24 18456 \n32 64     99.29 62280 \n64 128     99.62 226728 \n128 512     99.73 1292392 \n256 1024     99.96 5074920 \nB \n  1    94.46  \n  2    97.05  \n  4    99.22  \n  8    99.59  \nC \n   2   98.57 135080 \n   4   99.72 235048 \n   8   99.91 434984 \nD     0.01  98.24  \n    0.5  74.95  \nE      None 97.77 334791 \n     2D 99.78 349191 \n \nData acquisition & process\nSWT\nAcquisition\nResults & analysis\nSNR\nFig. 4: The framework of the proposed method.\n5. Case studies and analysis\nTo verify and analyze the e ﬀectiveness of the proposed TFT and its application in bearing fault diagnosis, we\nimplement two case studies in this section. The two bearing datasets used are collected from accelerated bearing life\ntester (ABLT-1A). The testing system is mainly composed of a test head, sensors, test bearings, electronic control\nsystem, computer monitoring system and data acquisition system. The speciﬁc real scene is shown in Fig. 5. In our\nimplementation, MATLAB is used for signal processing, while TensorFlow 2.1 framework is used for deep learning.\nAll programs run on a computer with the following conﬁguration: AMD Ryzen 2600, NVIDIA RTX 1060, 16GB\nRAM.\nIn the following case studies, we use three most popular deep learning models as benchmark models, as follows:\n1) Multilayer perceptron (MLP) [52], as known as deep neural network (DNN). 2) Convolutional neural networks\n(CNN). To improve the convergence performance of deep convolutional networks, we employ ResNet18 [35] with\nresidual connection. 3) Recurrent neural network (RNN). To alleviate the long-range dependence problem of classical\nRNN, we use the improved GRU [38].\nTest head Sensor Computer \nmonitoring\nTest \nbearings\nElectronic\ncontrol\nData \nacquisition\nFig. 5: ABLT-1A.\n10\n5.1. Case 1: ABLT-1A Bearing Dataset 6308\n5.1.1. Dataset description\nThe rolling bearing HRB6308 is selected as the experimental bearing in this experiment. The faulty bearing is in-\nstalled in the ﬁrst channel of the sensor, and the other three normal bearings are installed in the remaining channels of\nthe sensor. The vibration signal of the fault or normal rolling bearing is collected by the monoaxial vibration accelera-\ntion sensor with a sampling frequency of 12,800Hz. The experimental dataset is detailed as follows. Under zero-load\nconditions, seven types of fault conditions such as normal (6308N), inner ring fault (6308IRF), inner ring weak fault\n(6308IRWF), outer ring fault (6308ORF), outer ring weak fault (6308ORWF), inner and outer ring compound fault\n(6308IORF), and inner and outer ring compound weak fault (6308IORWF) are simulated. Accordingly, vibration\ndata were collected for each failure type of the bearings, which were running at 1,050 rpm. For each failure mode\ninterception, 2000 samples of the length 1024 were obtained, totaling 2000×7 = 14000 samples. In addition, 60% of\nall datasets are used as training dataset, 20% as validation dataset for model selection and cross validation, and 20%\nas test dataset for ﬁnal test. In each training and test, the datasets are randomly divided to ensure the comprehensive\nevaluation of the model performance.\n5.1.2. Data preprocessing\nWhile the raw vibration signal contains su ﬃcient health condition information of bearings, it is not clear to be\nused for fault diagnosis directly [24]. Considering the non-stationarity of vibration signals, synchrosqueezed wavelet\ntransforms (SWT) is used to process the raw data to obtain TFRs. The resolution of TFRs obtained by SWT is 1280×\n2560, which is too large to input a network. This will signiﬁcantly increase scale of the network and computational\nexpense. Similar to Ref. [53], the bicubic interpolation algorithm [54] is used to resize the resolution to 224 ×224,\nwhich is the common input size. Finally, input images with a shape of 224 ×224 ×1 are obtained. Vibration signals\nand corresponding TFRs of bearings with diﬀerent fault modes are drawn in Fig. 6. These TFRs will be the input of\nthe proposed fault diagnosis model.\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c\n(a)\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c (b)\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c (c)\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c (d)\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c\n(e)\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c (f)\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013/uni00000018/uni00000013/uni00000011/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001b\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000024/uni00000050/uni00000053/uni0000004f/uni0000004c/uni00000057/uni00000058/uni00000047/uni00000048\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c (g)\nFig. 6: Signals and SWT results of bearings (a) 6308N (b) 6308IRF (c) 6308IRWF (d) 6308ORF (e) 6308ORWF (f) 6308IORF (g) 6308IORWF.\n11\n5.1.3. Model selection\nIn this section, we will determine the structure and hyperparameter selection of the model based on the cross-\nvalidation on ABLT-1A Bearing Dataset 6308, and evaluate the inﬂuence of these hyperparameter selections on the\nmodel size and generalization performance. To increase the feasibility of the research work, all parameters and\ndiagnostic results are cross-validated for multiple average veriﬁcation and analysis. The hyperparameters we need\nto determine include: (A) embedding dimension dmodel and hidden dimension df f , (B) number of attention heads h,\n(C) number of Transformer blocks N, (D) dropout rate rdp , and (E) selection of position encoding. Each model was\ntrained 10 times for cross-validation. Speciﬁcally, the average of the results of cross-validation is employed. The\ntest results of each group are shown in Table 1, where base denotes the ﬁnal selected model for subsequent research,\naverage acc denotes mean testing accuracy, and params denotes the total number of trainable parameters in the model.\nThe empty item in the params column indicates that this hyperparameter does not aﬀect the total number of trainable\nparameters. It can be seen from the table that the selection of these parameters has a certain impact on the model scale\nand performance. In particular, diﬀerent dimensions and encoder layers will greatly aﬀect the scale of the model.\nTable 1: Model selection and inﬂuence of some hyperparameters.\ndmodel df f h N r dp Position coding Average acc % Params num\nbase 64 256 8 6 0.1 1D 99.94 335,016\nA\n16 32 99.14 18,456\n32 64 99.21 62,280\n64 128 99.52 226,728\n128 512 99.73 1,292,392\n256 1024 99.93 5,074,920\nB\n1 94.46\n2 97.05\n4 99.22\n16 99.59\nC\n2 98.57 135,080\n4 99.72 235,048\n8 99.91 434,984\nD 0.01 98.24\n0.5 74.95\nE None 97.77 334,791\n2D 99.78 349,191\nIn addition, for the embedding dimension dmodel, hidden layer dimension df f and the number of attention heads h\nthat have a great impact on performance, the statistical box chart of test results is shown in Fig. 7. Too small embed-\nding dimension and hidden layer dimension cannot provide the network with enough parameterization ability, so the\nnetwork performance is poor. In contrast, too large dimension will lead to over-parameterization of the network. At\nthis time, our samples will not be enough to train the over-parameterized network, which will lead to the degradation\nof the generalization performance. This is also called over-ﬁtting. Besides, we notice that unreasonable selection of\nhyperparameters will not only reduce the average accuracy, but also make the network performance more unstable,\nthat is, the error distribution is more dispersed. The number of attention heads also shows a similar pattern. Finally,\nthe optimal network structure and hyperparameter selection of TFT are shown in Table 2.\nFor a fair comparison, the parameter settings of the three benchmark models are also standardized. The detailed\nmodel structure and hyperparameter settings are shown in Table 3.\n5.1.4. Diagnosis results\nBased on the optimal network structure and hyperparameter setting, the TFT model is trained on ABLT-1A Bearing\nDataset 6308. As can be seen from Fig. 8, after about 40 epochs, accuracy and loss values of the training and validation\ndatasets have been very stable, and the model has started to converge, indicating the strong convergence ability of the\nTFT. Note that in the early stage of training, the training loss is larger than the validation loss because the use of\ndropout limits the model capacity in training. With the process of training, dropout will drive the network to learn\nmore robust features. Finally, the training loss and validation loss of the network are basically stable at the same value,\nindicating the good generalization ability of the network. The regularization technique we used fully guarantees the\nrobust generalization of the network.\n12\n(a)\n (b)\nFig. 7: The inﬂuence (a) embedding dimension and hidden dimension (b) the number of attention heads on the accuracy of time–frequency\nTransformer.\nTable 2: Optimal structure and hyperparameters of the proposed time–frequency Transformer.\nValue\nInput size [224, 224, 3]\nBatch size 32\nMax epochs 100\nLearning rate lr 5e-5\nOptimizer Adam\nLabel smoothing rate ϵls 0.1\nNum of encoder layers N 6\nEmbedding dimension dmodel 64\nHidden dimension df f 256\nNum of attention heads h 8\nDropout raterdp 0.1\nPosition encoding 1D\nThen, the trained model is used to classify the testing dataset to evaluate performance. Training and test have been\nrepeated 10 times on TFT and three benchmark models. We draw the results in Table 4. Besides, the total number of\ntrainable parameters and the average training time are also calculated to comprehensively compare the performance of\nthe models. Comparing the test performance of the 4 models, the proposed TFT achieves the best prediction accuracy.\nIts maximum prediction accuracy can reach 100%, and the average accuracy is also the highest. The accuracy variance\nof TFT is smaller, indicating that the prediction result is more stable. The performance of RNN is second only to TFT,\nwhose maximum accuracy is 100% and average accuracy is lower than that of TFT. Meanwhile, the variance of RNN\nis larger, so the result is not as stable and reliable as TFT. The prediction accuracy of DNN and CNN is signiﬁcantly\nlower than that of TFT and RNN, which is obviously related to the characterization ability of the model itself. DNN\nand CNN are just simple multiplication or convolution operation on inputs, lacking the grasp of temporal information.\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b\n/uni00000013/uni00000011/uni00000018\n/uni00000014/uni00000011/uni00000013\n/uni00000014/uni00000011/uni00000018\n/uni00000015/uni00000011/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056\n/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000042/uni0000004f/uni00000052/uni00000056/uni00000056\n/uni00000059/uni00000044/uni0000004f/uni00000042/uni0000004f/uni00000052/uni00000056/uni00000056\n(a)\n/uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013\n/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni00000008/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000042/uni00000044/uni00000046/uni00000046\n/uni00000059/uni00000044/uni0000004f/uni00000042/uni00000044/uni00000046/uni00000046 (b)\nFig. 8: The (a) loss and (b) accuracy curves of the proposed TFT.\n13\nTable 3: Structure and hyperparameter setting of three benchmark models.\nModel Structure (units and activation) Hyperparameter\nDNN\nDense (512, activation=’ReLU’)\nDropout ( )\nDense (128, activation=’ReLU’)\nDropout ( )\nDense (num class, activation=’ReLU’)\nDropout rate rdp\nMax epochs = 100\nBatch size = 32\nOptimizer = Adam(lr =1e-5)\nCNN\nResNet18 ( )\nGlobalAveragePooling2D ( )\nDense(num class)\nDropout rate rdp\nMax epochs = 100\nBatch size = 16\nOptimizer = Adam (lr =5e-5)\nGRU\nGRU (224, dropout) × 6\nDense (128, activation=’ReLU’)\nDense (num class, activation=’ReLU’)\nDropout rate rdp\nMax epochs = 100\nBatch size = 32\nOptimizer = Adam (lr =5e-5)\nTable 4: Test performance, size and training time usage of the models on ABLT-1A Bearing Dataset6308.\nModel Mean accuracy Best accuracy Std Params num Training time (s)\nTFT 99.94% 100.00% 0.05 335,016 690\nDNN 80.15% 85.71% 3.81 25,757,191 740\nCNN 92.56% 97.83% 0.55 11,176,839 1030\nRNN 97.03% 100.00% 1.56 1,844,103 1800\nFurthermore, we compare the scale and training time of these models. DNN contains the most trainable parameters\nbecause of its internal fully connected structure. This will lead to the model over parameterized, thus make it easier to\noverﬁt when the number of samples is limited. Interestingly, the parameters of CNN are slightly less than that of DNN,\nbut the training time is much longer than that of DNN. This is obviously due to the fact that although convolutional\nstructure reduces the number of trainable parameters through weight sharing, calculation amount corresponding to\neach parameter increases signiﬁcantly. The number of trainable parameters in RNN is much less than that in CNN\nand DNN, but the training time of RNN is the longest for its non-parallel computation.\nGenerally speaking, the accuracy of TFT and RNN which can grasp the temporal information is higher. But for\nRNN, this comes at the cost of a very long training time. The results show that the proposed TFT method based on the\nTransformer structure can process the time series information well with higher prediction accuracy. Moreover, TFT\ncan realize parallel computing, so its training is much faster than models with recurrent structure.\nFig. 9 shows the confusion matrix of the best and the worst test results of TFT, with the accuracy of 100% and\n99.88%, respectively. The columns represent the predict labels while the rows represent the true labels for di ﬀerent\nhealth states. In the best case, the accuracy is 100% for each health condition. In the worst one, the accuracy is\n100% for most conditions except for IORWF and IRWF. A small number of samples with IORWF and IRWF were\nmisjudged as IRWF and ORWF, respectively, indicating greater challenge of weak fault classiﬁcation. It is also worth\nnoting that, even in the worst case, TFT does not fail to identify the three non-weak fault states and the normal state.\nTo visualize the extracted features of these models, t-distributed stochastic neighbor embedding (t-SNE) [55] is\nused to simplify the extracted high-dimensional features of the last hidden layer extracted by the above four methods\ninto two-dimensional vector distribution, the visualization results of the test samples are shown in Fig. 10. As\nindicated that only the hidden features extracted by the TFT algorithm can accurately separate all faults.\n5.1.5. Visualization of attention weights\nSince the proposed TFT extracts features from TFRs completely based on attention mechanism, it is necessary to\nexplore its mechanism by attention visualization. The attention mechanism adopted by Transformer is mainly used to\nexert diﬀerent degrees of attention on the tokens and form the relationship between di ﬀerent tokens. Here, we try to\nshow the attention degree of attention mechanism on di ﬀerent tokens, namely attention weight. Firstly, the attention\nweight tensors of the ﬁrst and last self–attention layers are derived. Note that the attention weight is not the output\nof the attention layer, but the weight softmax\n(\nQsK⊤\ns /√dk\n)\nof the input in Eq. 2. Since the calculation results of\nmulti-head attention are concatenated in the network, we sum the weight matrix of h attention heads. Furthermore,\n14\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni0000002c/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni0000002c/uni00000035/uni00000029/uni0000002c/uni00000035/uni0000003a/uni00000029\n/uni00000031\n/uni00000032/uni00000035/uni00000029/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni0000002c/uni00000032/uni00000035/uni00000029\n/uni0000002c/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni0000002c/uni00000035/uni00000029\n/uni0000002c/uni00000035/uni0000003a/uni00000029\n/uni00000031\n/uni00000032/uni00000035/uni00000029\n/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni00000037/uni00000055/uni00000058/uni00000048/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n(a)\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni0000002c/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni0000002c/uni00000035/uni00000029/uni0000002c/uni00000035/uni0000003a/uni00000029\n/uni00000031\n/uni00000032/uni00000035/uni00000029/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni0000002c/uni00000032/uni00000035/uni00000029\n/uni0000002c/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni0000002c/uni00000035/uni00000029\n/uni0000002c/uni00000035/uni0000003a/uni00000029\n/uni00000031\n/uni00000032/uni00000035/uni00000029\n/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni00000037/uni00000055/uni00000058/uni00000048/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001c/uni0000001c/uni00000017/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000019/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001c/uni0000001c/uni0000001a/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000016\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013 (b)\nFig. 9: Confusion matrix of (a) the best and (b) the worst results of time–frequency Transformer on ABLT-1A Bearing Dataset 6308.\n/uni00000014/uni00000013\n/uni00000018\n/uni00000013/uni00000018/uni00000014/uni00000013\n/uni00000014/uni00000013\n/uni00000018\n/uni00000013\n/uni00000018\n/uni00000014/uni00000013\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni0000002c/uni00000032/uni00000035/uni0000003a/uni00000029/uni0000002c/uni00000035/uni00000029/uni0000002c/uni00000035/uni0000003a/uni00000029/uni00000031/uni00000032/uni00000035/uni00000029/uni00000032/uni00000035/uni0000003a/uni00000029\n/uni00000014/uni00000013/uni00000013\n/uni00000018/uni00000013\n/uni00000013/uni00000018/uni00000013\n/uni00000018/uni00000013\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013\n(a)\n/uni00000015/uni00000013\n/uni00000014/uni00000013\n/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013\n/uni00000015/uni00000013\n/uni00000014/uni00000013\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013 (b)\n/uni00000015/uni00000013\n/uni00000014/uni00000013\n/uni00000013/uni00000014/uni00000013\n/uni00000014/uni00000013\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013 (c)\n/uni00000018/uni00000013\n/uni00000015/uni00000018\n/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013\n/uni00000018/uni00000013\n/uni00000013\n/uni00000018/uni00000013 (d)\nFig. 10: The 2D visualization result of learned features via (a) TFT (b) DNN (c) CNN (d) RNN.\nthe target of attention heads is tokens, and our TFT uses time–frequency cutting patches as tokens. Therefore, the\nattention mechanism actually works along the time direction. The attention mechanism and the TFRs of bearing\nsignals are drawn in Fig. 11. These ﬁgures show the normalized attention weight of the ﬁrst and last attention layers\non diﬀerent tokens. The larger the value is, the greater the attention weight is.\nIt can be seen from Fig. 11 that the attention weight distribution of di ﬀerent fault samples in the ﬁrst transformer\nblock is almost the same. This obviously adheres to our intuition since the network cannot distinguish di ﬀerent fault\ntypes in the input layer, but “observe” di ﬀerent samples with the same strategy. With the layer-by-layer attention\nprocessing, the network will be able to attach di ﬀerent attention weights to di ﬀerent fault types. Combined with the\nTFRs, it can be found that the last Transformer block focuses on the tokens with larger values, that is, it pays more\nattention to the time when the amplitude is more obvious. Through such concentration of attention, the TFT model can\neﬀectively grasp the characteristic information from the TFRs, and observe each token with di ﬀerent weights. Thus,\nTFT can accurately extract the key features of di ﬀerent fault types and avoid the interference of fault independent\nfactors.\n5.1.6. Anti-noise robustness test\nTo evaluate the anti-noise performance and robustness of the designed algorithm and fault diagnosis method, we\noperate an anti-noise robustness test. Specially, di ﬀerent degrees of signal-to-noise (SNR) is added into the original\nbearing signals. The same experimental setup was used to evaluate the variation in performance of each fault diag-\nnosis methods above with di ﬀerent SNR Settings. By adding di ﬀerent degrees of noise to the vibration signal, this\nexperiment evaluates the ability of these fault diagnosis methods to adapt to a noise-changing environment. As shown\nin Fig. 12, it is worth noting that the proposed method achieves the best performance. When SNR is bigger than 5,\n15\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n(a)\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051 (b)\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051 (c)\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051 (d)\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n(e)\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051 (f)\n/uni00000029/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051\n/uni00000037/uni0000004c/uni00000050/uni00000048\n/uni0000002f/uni00000044/uni00000056/uni00000057/uni00000003/uni00000024/uni00000057/uni00000057/uni00000051 (g)\nFig. 11: Attention weights visualization of bearings (a) 6308N (b) 6308IRF (c) 6308IRWF (d) 6308ORF (e) 6308ORWF (f) 6308IORF (g)\n6308IORWF.\nthat is, under the condition of relatively small noise, the proposed TFT can provide an acceptable prediction accuracy.\nWith the SNR decreasing, the accuracy of all these methods decreases. When the SNR is smaller than -5, the accuracy\nof DNN and CNN almost decreased to a meaningless value. Note that the meaningless value means the accuracy\nalmost equal to 14.29% of guess blindly. To sum up, the proposed TFT method has relatively high diagnosis accuracy\nunder noise interference environment, and its accuracy is higher than those of other benchmark methods. Also, the\nproposed TFT method is less disturbed by noise, and the accuracy decreases more slowly with the increase of noise.\n5.2. ABLT-1A Bearing Dataset 6205\n5.2.1. Dataset and benchmark test description\nThe rolling bearing HRB6205 is selected as the experimental bearing in this experiment. Di ﬀerent the previous\ncase, a triaxial acceleration sensor with the sampling frequency of 12,000Hz is used to collect vibration signals of fault\nor normal rolling bearing. Thus, the original vibration signal is converted into three-channel digital signal by a data\nacquisition card. In addition, we have taken into account the diﬀerent rotational speeds of 1200 rpm, 1500 rpm, 1750\nrpm, 2000 rpm. The speciﬁc experimental dataset is described as follows. Under zero-load conditions, six types of\nfault conditions such as normal (6205N), inner ring fault (6205IRF), outer ring fault (6205ORF), ball fault (6205BF),\ninner and outer ring compound fault (6205IORF), and outer ring and ball compound fault (6205ORBF) are simulated\nrespectively. We obtained 1000 samples with a length of 1024 for each health state and each speed. That is, a total\nof 1000 ×6 ×4 = 24000 samples. To distinguish the samples under di ﬀerent speeds, the dataset is divided into four\nsubsets according to the speed, and each subset contains 6000 samples. The benchmark will be conducted on each\nsub-dataset. 60% of each subset set is used as training dataset, 20% as validation dataset, and 20% as test dataset for\nﬁnal test. In each training and testing, the subsets are randomly divided to ensure the comprehensive evaluation of the\nmodel performance.\n16\nSNR\nFig. 12: Average diagnostic accuracy for diﬀerent fault diagnosis methods under diﬀerent SNRs.\nThe proposed TFT model and three benchmark methods in Case 1 are used in the benchmark test. The model\nstructure and hyperparameter setting are basically the same as the optimal setting in Case 1. Similarly, SWT is used\nto process the vibration signals of each sample to obtain the TFRs. Di ﬀerently, it should be noted that the signals\nfrom the three channels are processed separately and then stacked into multi-channel TFRs to make full use of the\nmulti-channel information in the dataset. After downsampling by bicubic interpolation algorithm, the input data with\na shape of 224 ×224 ×3 is obtained. The proposed TFR and CNN can directly process multi-channel input, while\nDNN and RNN need ﬂattening operation in the input layer.\n5.2.2. Diagnosis with multi-channel TFRs\nBased on the optimal network structure and hyperparameter setting, the TFT model is trained respectively on four\nsubsets of Bearing Dataset 6205. In addition, the three benchmark models are fully trained for comparison. The fully\ntrained models are applied to the diagnosis of test set samples in each subset, and the average results are shown in\nTable 5. Note that the test for each subset is repeated ﬁve times, and the table shows the average accuracy of all\nsubsets.\nIt can be seen that the accuracy of TFT is higher when multi-channel data is used. Both TFT and CNN can directly\ninput multi-channel data and construct feature graphs fusing multi-channel information. When DNN and RNN are\ndealing with larger dimension input data, the large network size obviously limits the generalization ability. Besides,\nthe large amount of multi-channel input data does not signiﬁcantly increase the training time of TFT and CNN due\nto the parallel computing capability. Particularly, hardly has the training time of TFT increased. Since RNN has no\nparallel processing ability and no learnable embedding module, its training time is greatly increased. It can be seen\nthat compared with other benchmark models, the proposed TFT can better deal with multi-channel TFRs and obtain\nbetter diagnosis performance with lower computational cost and network size.\nTable 5: Test performance, size and training time usage of the models on ABLT-1A Bearing Dataset 6205.\nAverage Params num Training time (s)\nTFT 99.97% 363,431 750\nDNN 67.17% 77,137,286 1,510\nCNN 92.65% 11,182,598 1,320\nRNN 96.38% 16,368,134 3,700\n5.2.3. Diagnosis across di ﬀerent conditions\nBearing Dataset 6205 contains the vibration signals of bearings collected at di ﬀerent speeds, which enables us\nto test the performance of TFT diagnosis under multiple speed conditions. Di ﬀerent from the previous benchmark\ntest, which divided the dataset into four subsets, we mixed and scrambled all samples with diﬀerent speeds in this test.\n17\nTherefore, the mixed dataset contains 24000 samples in total. Based on the optimal TFT structure and hyperparameter\nsetting, the model training is repeated 10 times and used for inference of test samples. The ﬁnal result shows that the\naverage classiﬁcation accuracy is 99.87%, and the highest accuracy is 99.92%. The confusion matrix of the best\nresults is shown in Fig. 13. Only a few samples with the real label of IORF were misclassiﬁed into BF.\nTo further investigate the fault feature extraction of TFT under multiple speed conditions, t-SNE is used to visu-\nalize the hidden features as shown in Fig. 14. Di ﬀerent colors in the ﬁgure represent di ﬀerent health states, while\ndiﬀerent markers represent diﬀerent speeds. It can be seen that the hidden features after highly abstracted with TFT\nhave good distinguishability. The samples of diﬀerent health states can be well distinguished. Furthermore, the sam-\nples of the same state, even if with diﬀerent speeds, can concentrate well. Only in the samples with IORF and ORBF\nfaults, the 1200r samples are slightly separated from the samples with other speeds, but this does not prevent the\nclassiﬁer from classifying them into one category. In addition, several IORF 1500r samples that were misclassiﬁed as\nBF failures can also be clearly observed.\n/uni00000025/uni00000029/uni0000002c/uni00000032/uni00000035/uni00000029/uni0000002c/uni00000035/uni00000029/uni00000031\n/uni00000032/uni00000035/uni00000025/uni00000029/uni00000032/uni00000035/uni00000029\n/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000025/uni00000029\n/uni0000002c/uni00000032/uni00000035/uni00000029\n/uni0000002c/uni00000035/uni00000029\n/uni00000031\n/uni00000032/uni00000035/uni00000025/uni00000029\n/uni00000032/uni00000035/uni00000029\n/uni00000037/uni00000055/uni00000058/uni00000048/uni00000003/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000011/uni0000001c/uni0000001c/uni00000018/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\nFig. 13: Confusion matrix of the best results of time–frequency Transformer on multiple speed ABLT-1A Bearing Dataset 6308.\n/uni00000014/uni00000013/uni00000013\n/uni00000018/uni00000013\n/uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013\n/uni0000001a/uni00000018\n/uni00000018/uni00000013\n/uni00000015/uni00000018\n/uni00000013\n/uni00000015/uni00000018\n/uni00000018/uni00000013\n/uni0000001a/uni00000018\n/uni00000014/uni00000013/uni00000013\n/uni00000025/uni00000029/uni00000003/uni00000014/uni00000015/uni00000013/uni00000013/uni00000055\n/uni00000025/uni00000029/uni00000003/uni00000014/uni00000018/uni00000013/uni00000013/uni00000055\n/uni00000025/uni00000029/uni00000003/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000055\n/uni00000025/uni00000029/uni00000003/uni00000015/uni00000013/uni00000013/uni00000013/uni00000055\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni00000003/uni00000014/uni00000015/uni00000013/uni00000013/uni00000055\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni00000003/uni00000014/uni00000018/uni00000013/uni00000013/uni00000055\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni00000003/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000055\n/uni0000002c/uni00000032/uni00000035/uni00000029/uni00000003/uni00000015/uni00000013/uni00000013/uni00000013/uni00000055\n/uni0000002c/uni00000035/uni00000029/uni00000003/uni00000014/uni00000015/uni00000013/uni00000013/uni00000055\n/uni0000002c/uni00000035/uni00000029/uni00000003/uni00000014/uni00000018/uni00000013/uni00000013/uni00000055\n/uni0000002c/uni00000035/uni00000029/uni00000003/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000055\n/uni0000002c/uni00000035/uni00000029/uni00000003/uni00000015/uni00000013/uni00000013/uni00000013/uni00000055\n/uni00000031/uni00000003/uni00000014/uni00000015/uni00000013/uni00000013/uni00000055\n/uni00000031/uni00000003/uni00000014/uni00000018/uni00000013/uni00000013/uni00000055\n/uni00000031/uni00000003/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000055\n/uni00000031/uni00000003/uni00000015/uni00000013/uni00000013/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000025/uni00000029/uni00000003/uni00000014/uni00000015/uni00000013/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000025/uni00000029/uni00000003/uni00000014/uni00000018/uni00000013/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000025/uni00000029/uni00000003/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000025/uni00000029/uni00000003/uni00000015/uni00000013/uni00000013/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000029/uni00000003/uni00000014/uni00000015/uni00000013/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000029/uni00000003/uni00000014/uni00000018/uni00000013/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000029/uni00000003/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000055\n/uni00000032/uni00000035/uni00000029/uni00000003/uni00000015/uni00000013/uni00000013/uni00000013/uni00000055\nFig. 14: The 2D visualization result of learned features from diﬀerent rotate speed.\n5.3. Comparison with state-of-the-art\nThe above comparisons with three representative benchmark models signiﬁcantly demonstrates the advantages of\nTFT in terms of feature extraction capability, network size, training speed, classiﬁcation accuracy, and robustness.\n18\nTable 6: Performance of the comparison methods on the two datasets.\nDiagnose methods Signal processing Dataset6308 Dataset6205 Average\nELM [56] MRSVD (multi-resolution SVD) 79.58% 71.95% 75.77%\nSVM [57] EEMD (ensemble EMD) 88.90% 87.94% 88.42%\nDCNN [14] VMD 93.67% 95.82% 94.75%\nCNN [58] DWT (discrete wavelet transform) 97.54% 98.16% 97.85%\nDBN [3] Time-domain statistic features 98.20% 99.75% 98.98%\nLSTM [17] EMD 99.12% 97.54% 98.33%\nAE-ELM [13] FFT 91.03% 87.58% 89.31%\nAE [19] Vibration signals 93.59% 92.81% 93.20%\nGRU-NP-DAE [16] Vibration signals 96.17% 95.38% 95.78%\nTFT SWT 99.94% 99.97% 99.96%\nThese comparisons are mainly aimed at the performance play of the network itself, while may be not su ﬃcient for\nspeciﬁc target tasks. Therefore, comparison with other fault diagnosis methods in practical application is necessary to\nverify the eﬀectiveness of TFT-based diagnosis.\nIn this section, several state-of-the-art techniques for fault diagnosis are employed to comparison on Dataset6308\nand Dataset 6205. These comparison methods include Li’s extreme learning machine (ELM) [56], Zhang’s support\nvector machine (SVM) [57], Xu’s deep convolutional neural networks (DCNN) [14], Chen’s convolutional neural\nnetworks (CNN) [58], Shao’s deep belief network (DBN) [3], Chen’s long short-time memory (LSTM) network [17],\nLiu’s GRU-based non-linear predictive denoising autoencoder (GRU-NP-DAE) [16], Shao’s auto-encoder (AE) [19]\nand Mao’s auto-encoder extreme learning machine (AE-ELM) [13]. Most of these methods are based on the latest\nCNN and RNN type models and utilize di ﬀerent inputs, e.g. vibration signals, time domain statistics, frequency\ndomain, and time–frequency domain features, respectively.\nThese state-of-the-art methods are used for Dataset 6308 and Dataset 6205, respectively, whose results are shown\nin Table 6. The performance of these methods varies using diﬀerent data processing methods and diagnostic models.\nAmong them, models with the ability to grasp temporal information, such as RNNs and their variants, clearly bring\nmore signiﬁcant performance gains compared to traditional ELMs and SVMs. Besides, TFT outperforms other DL-\nbased approaches even all receive time–frequency inputs, which is consistent with the benchmark in case studies\nbefore. Among all these state-of-the-art solutions, the proposed TFT achieves the highest classiﬁcation accuracy on\nboth datasets, which further proves the superiority of the proposed method on rolling bearing fault diagnosis.\n6. Conclusion\nIn this paper, we proposed a new time–frequency Transformer (TFT) inspired by vanilla Transformer architecture\nto process time–frequency representations (TFRs). Based on TFT, this paper presented an end-to-end fault diagnosis\nframework for rolling bearings. In this method, the vibration signals of rolling bearing are processed by SWT to obtain\nmulti-channel TFRs, and then the TFRs are input into TFT to extract discriminative hidden features and accurately\nclassify fault modes.\nThe proposed TFT has the following characteristics: 1) TFT completely abandons the tedious recurrence structure\nand convolution operation commonly used in traditional DL-based frameworks, completely relying on multi-head\nself–attention mechanism and feed-forward neural network layers. Compared with classical CNN and RNN, this\ngreatly improves the parallel computing ability of the network and reduces the network scale. 2) The feature extraction\nstructure based on self–attention mechanism with residual connector can accurately focus on the eﬀective feature areas\nin the input graphs. Compared with RNNs that with long-range dependent defects, this model can better establish the\nrelationship in input sequences.\nThe eﬀectiveness of the proposed fault diagnosis method of rolling bearing based on TFT is veriﬁed by case studies\non several experimental data in this paper. Compared with other benchmark models and state-of-the-art methods, the\nsuperiority of this method is veriﬁed. The diagnosis framework has the following advantages: 1) Compared with other\nmethods based on classical deep learning model, this method has higher diagnosis accuracy and faster training speed.\n2) This method can make better use of the collected multi-channel signals, and contribute this advantage to higher\ndiagnostic accuracy and e ﬃciency. 3) This method can adapt to a certain degree of noise environment and provide\n19\neﬀective fault diagnosis under strong noise conditions. 4) This method can be used for fault diagnosis under multiple\nworking conditions (speeds).\nConsidering that there is still much room for improvement, our future work will focus on the following. 1) We\nwill try to apply Transformer architecture to the prognostic ﬁeld. 2) We will try to further improve the model, such as\nusing convolution operation in the tokenizer module to improve the local receptive ﬁeld.\nAcknowledge\nThe authors gratefully acknowledge the ﬁnancial support of the National Natural Science Foundation of China\n(No. 52075095).\nReferences\n[1] X. Zhao, M. Jia, J. Bin, T. Wang, Z. Liu, Multiple-Order Graphical Deep Extreme Learning Machine for Unsupervised Fault Diagnosis of\nRolling Bearing, IEEE Trans. Instrum. Meas. 70 (2021). doi:10.1109/TIM.2020.3041087.\n[2] X. Yan, M. Jia, A novel optimized SVM classiﬁcation algorithm with multi-domain feature and its application to fault diagnosis of rolling\nbearing, Neurocomputing 313 (2018) 47–64. doi:10.1016/j.neucom.2018.05.002.\n[3] H. Shao, H. Jiang, X. Zhang, M. Niu, Rolling bearing fault diagnosis using an optimization deep belief network, Meas. Sci. Technol. 26 (11)\n(2015). doi:10.1088/0957-0233/26/11/115002.\n[4] P. M. Frank, S. X. Ding, T. Marcu, Model-based fault diagnosis in technical processes, Transactions of the Institute of Measurement and\nControl 22 (1) (2000) 57–101. doi:10.1177/014233120002200104.\n[5] Z. Gao, C. Cecati, S. X. Ding, A Survey of Fault Diagnosis and Fault-Tolerant Techniques—Part I: Fault Diagnosis With Model-Based and\nSignal-Based Approaches, IEEE Trans. Ind. Electron. 62 (6) (2015) 3757–3767. doi:10.1109/TIE.2015.2417501.\n[6] S. Dong, X. Xu, R. Chen, Application of fuzzy C-means method and classiﬁcation model of optimized K-nearest neighbor for fault diagnosis\nof bearing, J Braz. Soc. Mech. Sci. Eng. 38 (8) (2016) 2255–2263. doi:10.1007/s40430-015-0455-9 .\n[7] D. Xiao, J. Ding, X. Li, L. Huang, Gear Fault Diagnosis Based on Kurtosis Criterion VMD and SOM Neural Network, Appl. Sci. 9 (24)\n(2019) 5424. doi:10.3390/app9245424.\n[8] P. Durkhure, A. Lodwal, Fault Diagnosis of Ball Bearing Using Time Domain Analysis and Fast Fourier Transformation.\n[9] O. R. Seryasat, M. A. shoorehdeli, F. Honarvar, A. Rahmani, Multi-fault diagnosis of ball bearing using FFT, wavelet energy entropy\nmean and root mean square (RMS), in: 2010 IEEE International Conference on Systems, Man and Cybernetics, 2010, pp. 4295–4299.\ndoi:10.1109/ICSMC.2010.5642389.\n[10] Y . Yu, YuDejie, C. Junsheng, A roller bearing fault diagnosis method based on EMD energy entropy and ANN, Journal of Sound and Vibration\n294 (1) (2006) 269–277. doi:10.1016/j.jsv.2005.11.002.\n[11] Q. Wang, Y . B. Liu, X. He, S. Y . Liu, J. H. Liu, Fault Diagnosis of Bearing Based on KPCA and KNN Method (2014).doi:10.4028/www.\nscientific.net/AMR.986-987.1491.\n[12] R. Salakhutdinov, Deep learning, in: S. A. Macskassy, C. Perlich, J. Leskovec, W. Wang, R. Ghani (Eds.), The 20th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY , USA - August 24 - 27, 2014, ACM, 2014, p. 1973.\ndoi:10.1145/2623330.2630809.\nURL https://doi.org/10.1145/2623330.2630809\n[13] W. Mao, J. He, Y . Li, Y . Yan, Bearing fault diagnosis with auto-encoder extreme learning machine: A comparative study, Proceedings of\nthe Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science 231 (8) (2017) 1560–1578. doi:10.1177/\n0954406216675896.\n[14] Z. Xu, C. Li, Y . Yang, Fault diagnosis of rolling bearing of wind turbines based on the Variational Mode Decomposition and Deep Convolu-\ntional Neural Networks, Appl. Soft Comput. J. 95 (2020). doi:10.1016/j.asoc.2020.106515.\n[15] F. Jia, Y . Lei, N. Lu, S. Xing, Deep normalized convolutional neural network for imbalanced fault classiﬁcation of machinery and its under-\nstanding via visualization, Mech. Syst. Signal Process. 110 (2018) 349–367. doi:10.1016/j.ymssp.2018.03.025.\n[16] H. Liu, J. Zhou, Y . Zheng, W. Jiang, Y . Zhang, Fault diagnosis of rolling bearings with recurrent neural network-based autoencoders, ISA\nTransactions 77 (2018) 167–178. doi:10.1016/j.isatra.2018.04.005.\n[17] Z. Chen, Y . Liu, S. Liu, Mechanical state prediction based on LSTM neural netwok, in: 2017 36th Chinese Control Conference (CCC), 2017,\npp. 3876–3881. doi:10.23919/ChiCC.2017.8027963.\n[18] H. Zhao, S. Sun, B. Jin, Sequential Fault Diagnosis Based on LSTM Neural Network, IEEE Access 6 (2018) 12929–12939. doi:10.1109/\nACCESS.2018.2794765.\n[19] H. Shao, H. Jiang, H. Zhao, F. Wang, A novel deep autoencoder feature learning method for rotating machinery fault diagnosis, Mechanical\nSystems and Signal Processing 95 (2017) 187–204. doi:10.1016/j.ymssp.2017.03.034.\n[20] H. Zhang, R. Wang, R. Pan, H. Pan, Imbalanced Fault Diagnosis of Rolling Bearing Using Enhanced Generative Adversarial Networks, IEEE\nAccess 8 (2020) 185950–185963. doi:10.1109/access.2020.3030058.\n[21] J. F. Kolen, S. C. Kremer, Gradient Flow in Recurrent Nets: The Di ﬃculty of Learning LongTerm Dependencies, in: A Field Guide to\nDynamical Recurrent Networks, 2010. doi:10.1109/9780470544037.ch14.\n[22] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez, K. Keutzer, P. Vajda, Visual Transformers: Token-based Image\nRepresentation and Processing for Computer Vision, ArXiv200603677 Cs Eess (2020). arXiv:2006.03677.\n[23] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y . Wu, R. Pang, Conformer: Convolution-augmented\nTransformer for Speech Recognition, ArXiv200508100 Cs Eess (2020). arXiv:2005.08100.\n20\n[24] X. Li, W. Zhang, Q. Ding, Understanding and improving deep learning-based rolling bearing fault diagnosis with attention mechanism, Signal\nProcess. 161 (2019) 136–154. doi:10.1016/j.sigpro.2019.03.019.\n[25] Z. Long, X. Zhang, L. Zhang, G. Qin, S. Huang, D. Song, H. Shao, G. Wu, Motor fault diagnosis using attention mechanism and improved\nadaboost driven by multi-sensor information, Measurement 170 (2021) 108718. doi:10.1016/j.measurement.2020.108718.\n[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in: I. Guyon,\nU. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp.\n5998–6008.\nURL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[27] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V . Le, G. E. Hinton, J. Dean, Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer, in: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings, OpenReview.net, 2017.\nURL https://openreview.net/forum?id=B1ckMDqlg\n[28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, N. Houlsby, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ArXiv201011929 Cs (2020).\narXiv:2010.11929.\n[29] I. Daubechies, J. Lu, H. T. Wu, Synchrosqueezed wavelet transforms: An empirical mode decomposition-like tool, Appl. Comput. Harmon.\nAnal. 30 (2) (2011) 243–261. doi:10.1016/j.acha.2010.08.002.\n[30] J. Wen, H. Gao, S. Li, L. Zhang, X. He, W. Liu, Fault diagnosis of ball bearings using Synchrosqueezed wavelet transforms and SVM, in:\nProceedings of 2015 Prognostics and System Health Management Conference, PHM 2015, 2016. doi:10.1109/PHM.2015.7380084.\n[31] C. Li, V . Sanchez, G. Zurita, M. Cerrada Lozada, D. Cabrera, Rolling element bearing defect detection using the generalized synchrosqueezing\ntransform guided by time-frequency ridge enhancement, ISA Trans. 60 (2016) 274–284. doi:10.1016/j.isatra.2015.10.014.\n[32] C. Yi, Y . Lv, H. Xiao, T. Huang, G. You, Multisensor signal denoising based on matching synchrosqueezing wavelet transform for mechanical\nfault condition assessment, Meas. Sci. Technol. 29 (4) (2018). doi:10.1088/1361-6501/aaa50a.\n[33] W. Liu, W. Chen, Z. Zhang, A Novel Fault Diagnosis Approach for Rolling Bearing Based on High-Order Synchrosqueezing Transform and\nDetrended Fluctuation Analysis, IEEE Access 8 (2020) 12533–12541. doi:10.1109/ACCESS.2020.2965744.\n[34] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer Normalization, ArXiv160706450 Cs Stat (2016). arXiv:1607.06450.\n[35] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, IEEE Computer Society, 2016, pp. 770–778. doi:10.1109/CVPR.\n2016.90.\n[36] D. Bahdanau, K. Cho, Y . Bengio, Neural machine translation by jointly learning to align and translate, in: Y . Bengio, Y . LeCun (Eds.), 3rd\nInternational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,\n2015.\nURL http://arxiv.org/abs/1409.0473\n[37] S. Hochreiter, J. Schmidhuber, Long Short-Term Memory, Neural Comput. 9 (8) (1997) 1735–1780.doi:10.1162/neco.1997.9.8.1735.\n[38] J. Chung, C ¸ . G¨ulc ¸ehre, K. Cho, Y . Bengio, Gated feedback recurrent neural networks, in: F. R. Bach, D. M. Blei (Eds.), Proceedings of the\n32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, V ol. 37 of JMLR Workshop and Conference\nProceedings, JMLR.org, 2015, pp. 2067–2075.\nURL http://proceedings.mlr.press/v37/chung15.html\n[39] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in:\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota, 2019, pp. 4171–4186.\ndoi:10.18653/v1/N19-1423.\nURL https://www.aclweb.org/anthology/N19-1423\n[40] Z. Yang, Z. Dai, Y . Yang, J. G. Carbonell, R. Salakhutdinov, Q. V . Le, Xlnet: Generalized autoregressive pretraining for language under-\nstanding, in: H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-Buc, E. B. Fox, R. Garnett (Eds.), Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, 2019, pp. 5754–5764.\nURL https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html\n[41] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, Language models are few-shot learners, in: H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nURL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n[42] Y . Tay, M. Dehghani, D. Bahri, D. Metzler, Eﬃcient Transformers: A Survey, ArXiv200906732 Cs (2020). arXiv:2009.06732.\n[43] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, X. Tang, Residual attention network for image classiﬁcation, in: 2017 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, IEEE Computer Society, 2017,\npp. 6450–6458. doi:10.1109/CVPR.2017.683.\nURL https://doi.org/10.1109/CVPR.2017.683\n[44] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018, IEEE Computer Society, 2018, pp. 7132–7141. doi:10.1109/CVPR.2018.00745.\nURL http://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_\npaper.html\n[45] H. Zhang, I. J. Goodfellow, D. N. Metaxas, A. Odena, Self-attention generative adversarial networks, in: K. Chaudhuri, R. Salakhutdinov\n21\n(Eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,\nV ol. 97 of Proceedings of Machine Learning Research, PMLR, 2019, pp. 7354–7363.\nURL http://proceedings.mlr.press/v97/zhang19d.html\n[46] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, S.-M. Hu, PCT: Point Cloud Transformer, ArXiv201209688 Cs (2020). arXiv:\n2012.09688.\n[47] D. Hendrycks, K. Gimpel, Gaussian Error Linear Units (GELUs) (2016). arXiv:1606.08415.\n[48] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: Y . Bengio, Y . LeCun (Eds.), 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\nURL http://arxiv.org/abs/1412.6980\n[49] J. Zhang, T. He, S. Sra, A. Jadbabaie, Why gradient clipping accelerates training: A theoretical justiﬁcation for adaptivity, in: 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net, 2020.\nURL https://openreview.net/forum?id=BJgnXpVYwS\n[50] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A simple way to prevent neural networks from overﬁtting,\nTech. rep. (2014). doi:10.5555/2627435.2670313.\n[51] C. Szegedy, V . Vanhoucke, S. Ioﬀe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision, in: 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, IEEE Computer Society, 2016, pp. 2818–\n2826. doi:10.1109/CVPR.2016.308.\nURL https://doi.org/10.1109/CVPR.2016.308\n[52] D. Rumelhart, G. Hinton, R. Williams, Learning internal representations by error propagation, in: Readings in Cognitive Science, Elsevier,\n1988, pp. 399–421. doi:10.1016/B978-1-4832-1446-7.50035-2 .\n[53] Y . Ding, M. Jia, Y . Cao, Remaining Useful Life Estimation Under Multiple Operating Conditions via Deep Subdomain Adaptation, IEEE\nTrans. Instrum. Meas. 70 (2021) 1–11. doi:10.1109/TIM.2021.3076567.\n[54] R. G. Keys, Cubic Convolution Interpolation for Digital Image Processing, IEEE Trans. Acoust. Speech Signal Process. 29 (6) (1981) 1153–\n1160. doi:10.1109/TASSP.1981.1163711.\n[55] L. Van der Maaten, G. Hinton, Visualizing data using t-SNE., J. Mach. Learn. Res. 9 (11) (2008).\n[56] Y . Li, X. Wang, J. Wu, Fault diagnosis of rolling bearing based on permutation entropy and Extreme Learning Machine, in: 2016 Chinese\nControl and Decision Conference (CCDC), 2016, pp. 2966–2971. doi:10.1109/CCDC.2016.7531490.\n[57] X. Zhang, Y . Liang, J. Zhou, Y . zang, A novel bearing fault diagnosis model integrated permutation entropy, ensemble empirical mode\ndecomposition and optimized SVM, Measurement 69 (2015) 164–179. doi:10.1016/j.measurement.2015.03.017.\n[58] R. Chen, X. Huang, L. Yang, X. Xu, X. Zhang, Y . Zhang, Intelligent fault diagnosis method of planetary gearboxes based on convolution\nneural network and discrete wavelet transform, Computers in Industry 106 (2019) 48–59. doi:10.1016/j.compind.2018.11.003.\n22",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7344400882720947
    },
    {
      "name": "Computer science",
      "score": 0.6212615966796875
    },
    {
      "name": "Vibration",
      "score": 0.4612530469894409
    },
    {
      "name": "Feature learning",
      "score": 0.44547325372695923
    },
    {
      "name": "Encoder",
      "score": 0.4168383777141571
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40244120359420776
    },
    {
      "name": "Electronic engineering",
      "score": 0.35209864377975464
    },
    {
      "name": "Engineering",
      "score": 0.2858662009239197
    },
    {
      "name": "Electrical engineering",
      "score": 0.10733398795127869
    },
    {
      "name": "Voltage",
      "score": 0.1020805835723877
    },
    {
      "name": "Acoustics",
      "score": 0.07246056199073792
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76569877",
      "name": "Southeast University",
      "country": "CN"
    }
  ],
  "cited_by": 405
}