{
  "title": "Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers",
  "url": "https://openalex.org/W4385571345",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2890434579",
      "name": "Wanjun Zhong",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2047317366",
      "name": "Tingting Ma",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2125988827",
      "name": "Jiahai Wang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2111795920",
      "name": "Jian Yin",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2106204710",
      "name": "Tiejun Zhao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3143182302",
      "name": "Chin-Yew Lin",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1966337374",
      "name": "Nan Duan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3034446185",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W3207095490",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W3155023596",
    "https://openalex.org/W4320858367",
    "https://openalex.org/W3203211567",
    "https://openalex.org/W4287888135",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W3153739498",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4293998609",
    "https://openalex.org/W3208314443",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4280525617",
    "https://openalex.org/W2971253865",
    "https://openalex.org/W4280638382",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4226320735",
    "https://openalex.org/W4288614645",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4221148939",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2996164352",
    "https://openalex.org/W4280637601",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4288262459"
  ],
  "abstract": "This paper presents ReasonFormer, a unified reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making. Inspired by dual-process theory in cognitive science, the representation module (automatic thinking) and reasoning modules (controlled thinking) are decoupled to capture different levels of cognition. Upon the top of the representation module, the pre-trained reasoning modules are modular and professional in specific and fundamental reasoning skills (e.g., logic, simple QA, etc). To mimic the controlled compositional thinking process, different reasoning modules are dynamically activated and composed in both parallel and cascaded manners to control what reasoning skills are activated and how deep the reasoning process will be reached to solve the current problems. The unified reasoning framework solves multiple tasks with a single model, and is trained and inferred in an end-to-end manner. Evaluated on 11 datasets requiring different reasoning skills and complexity, ReasonFormer demonstrates substantial performance boosts, revealing the compositional reasoning ability. Few-shot experiments exhibit better generalization ability by learning to compose pre-trained skills for new tasks with limited data, and decoupling the representation module and the reasoning modules. Further analysis shows the modularity of reasoning modules as different tasks activate distinct reasoning skills at different reasoning depths.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7587â€“7600\nJuly 9-14, 2023 Â©2023 Association for Computational Linguistics\nDisentangling Reasoning Capabilities from Language Models with\nCompositional Reasoning Transformers\nWanjun Zhong1âˆ—, Tingting Ma2âˆ—, Jiahai Wang1, Jian Yin1,\nTiejun Zhao2, Chin-Yew Lin3 and Nan Duan3\n1 The School of Computer Science and Engineering, Sun Yat-sen University\nGuangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, P.R.China\n2 Harbin Institute of Technology 3 Microsoft\nzhongwj25@mail2.sysu.edu.cn, hittingtingma@gmail.com\n{wangjiah,issjyin}@mail.sysu.edu.cn, tjzhao@hit.edu.cn\n{cyl, nanduan}@microsoft.com;\nAbstract\nThis paper presents ReasonFormer, a unified\nreasoning framework for mirroring the mod-\nular and compositional reasoning process of\nhumans in complex decision-making. Inspired\nby dual-process theory in cognitive science, the\nrepresentation module (automatic thinking) and\nreasoning modules (controlled thinking) are de-\ncoupled to capture different levels of cognition.\nUpon the top of the representation module, the\npre-trained reasoning modules are modular and\nprofessional in specific and fundamental rea-\nsoning skills (e.g., logic, simple QA, etc). To\nmimic the controlled compositional thinking\nprocess, different reasoning modules are dy-\nnamically activated and composed in both par-\nallel and cascaded manners to control what rea-\nsoning skills are activated and how deep the\nreasoning process will be reached to solve the\ncurrent problems. The unified reasoning frame-\nwork solves multiple tasks with a single model,\nand is trained and inferred in an end-to-end\nmanner. Evaluated on 11 datasets requiring dif-\nferent reasoning skills and complexity,Reason-\nFormer demonstrates substantial performance\nboosts, revealing the compositional reasoning\nability. Few-shot experiments exhibit better\ngeneralization ability by learning to compose\npre-trained skills for new tasks with limited\ndata, and decoupling the representation module\nand the reasoning modules. Further analysis\nshows the modularity of reasoning modules as\ndifferent tasks activate distinct reasoning skills\nat different reasoning depths. 1\n1 Introduction\nPrevailing language models (LMs) (Devlin et al.,\n2018; Brown et al., 2020) demonstrate impressive\nperformance in natural language processing tasks,\nâˆ— Equal contributions during internship at Microsoft.\n1Code and model are available at https:\n//github.com/microsoft/KC/tree/main/\npapers/ReasonFormer.\nQuestion: What cause car accident?\nSemantic Understanding\n(Intuitive) System 1\n(Controlled) System 2\nStep 1: Memorizing Fact Knowledge\nDriving relates to {speed, attention, rule following}\nAlcohol hurts attention â€¦\nStep 2: Logical Deduction\nalcohol â†’affect attention â†’driving accident  â€¦\nStep 3: Answering Question\nalcohol, over-speeding, distraction â€¦ \nFigure 1: Compositional reasoning process of humans in\ncomplex decision-making. Humans solve the problems\nby cascaded executions of fundamental skills.\nand have ushered in a new trend in AI research. De-\nspite the emerging fervor, the homogeneous LMs\nrelying on a single call of the model are less mod-\nular and are hard to explicitly model the complex\nreasoning process (Helwe et al., 2021) like humans.\nIn the dual-process theory (Daniel, 2017) in cog-\nnitive psychology, there are two cognitive systems\ninteracted to form a whole reasoning process. Sys-\ntem 1 (automatic thinking) generates intuitive pat-\nterns of ideas, and System 2 (controlled thinking)\nconstructs reasoning in an orderly logical series\nof compositional reasoning processes. Besides, in\nthe process of System 2, different functional brain\nareas could be modular and interact with each other.\nSystem 2 can decide how to compose different rea-\nsoning skills and when to stop thinking. As the\nexample shown in Fig. 1, when finding the cause\nof a car accident, humans intuitively comprehend\nthe question (System 1), and then conduct com-\npositional reasoning (System 2: recalling fact â†’\nlogical deduction â†’answering question).\nWe would like to incorporate this mechanism\n7587\ninto AI models in decision-making, and make the\nfollowing assumptions: (1) the representation mod-\nule (System 1) and reasoning module (System 2)\ncan be decoupled and (2) the â€œcomplicated\" rea-\nsoning process can be disentangled into multi-step\nexecutions of compositional â€œfundamental\" reason-\ning modules, whose compositionality can be learnt\nwith limited data. Also, the â€œfundamental\" nature\nof basic reasoning skills allows them to have rich\ntraining instances for reliable skill pre-training.\nUnder these motivations, this paper proposes the\nmodular and compositional reasoning framework -\nReasonFormer, to mirror humanâ€™s compositional\nreasoning process, with the following characteris-\ntics: (1) the representation module and reasoning\nmodules are decoupled; (2) reasoning modules are\nmodular and professional in fundamental reason-\ning skills; (3) reasoning modules are compositional\nin parallel and cascaded manner, to dynamically\ndecide the activated reasoning skills and the reason-\ning complexity; (4) the general-purpose reasoning\nframework is end-to-end and unified in solving\nmultiple tasks with one model.\nSpecifically, the representation module learns\ncontextual representations of problems. Upon the\ntop of the it, there are cascaded reasoning mod-\nules to perform compositional multi-step reasoning.\nThe reasoning modules are pre-trained to expert\nin specific reasoning skills (e.g., logic, QA, fact,\netc.). These pre-trained reasoning skills are con-\nsidered relatively fundamental and have rich re-\nsources. Two additional blocks complete the whole\nframework: the reasoning router and the reason-\ning adapter. The reasoning router decides which\nreasoning skills are activated in each reasoning\nstep, and when to stop the reasoning process. The\nadapter adapts the reused reasoning modules to\ndifferent steps of the reasoning process.\nWe comprehensively evaluate the framework on\n11 datasets emphasizing different reasoning skills\nand complexity, and highlight the following find-\nings: (1) Substantial performance boosts demon-\nstrate modelsâ€™ harvest of compositional reasoning\nability, and both the reasoning-centric pre-training\nand reasoning adapter bring compounding perfor-\nmance gains. (2) Results of few-shot experiments\nshow that specialized modules enables better gener-\nalization by learning to compose pre-trained skills\nfor low-resource tasks, and decoupling of repre-\nsentation module and reasoning modules. (3) Fur-\nther analysis reveals the distinct reasoning skills\nrequired for different tasks at different reasoning\ndepths, shoring up the modularity of reasoning\nmodules.\n2 Reasoning Skills Formulation\nThe compositional reasoning process of LMsâ€™ re-\nlies on the pre-training of several fundamental rea-\nsoning skills and their compositionality. Hence, the\nselection of skills is critical.\nSelection Principles. There are two major prin-\nciples in selecting skills: (1) Fundamental: Com-\nplex problems can be decomposed and solved by\nsimpler basic skills. So the basic skills should be\nmore fundamental, well-defined, and can be cov-\nered in the required skill set of as many tasks as pos-\nsible; (2) Resourceful: Reliable skill pre-training\nrequires large-scale pre-training data. However, in\nthe real-world scenario, the annotated data is ex-\npensive to obtain for most reasoning tasks. So it is\nexpected that there are already rich resource or data\ncan be collected via self(semi)-supervised manner.\nBasic Skills Selection. Humans always solve\ncomplex problem with fundamental skills, like un-\nderstanding key information (e.g., entity and its\ntype) of events, recalling related facts, understand-\ning causal relations between events, and extracting\nanswers for the question. This motivates us to se-\nlect the following basic skills: the logic ability\nto logically deduce the cause or consequence of\nevents; simple question answering (QA)to un-\nderstand the context and answer simple questions;\nnamed entity recognition (NER)to identify im-\nportant entities in the context; natural language\ninference (NLI)to identify semantic relevance of\ntwo sentences and factual knowledgeto memo-\nrize commonsense knowledge and understand daily\nevents. There is an additional general skill to learn\nthe commonly shared knowledge across selected\nskills. We keep this setting in our paper as they are\nrelatively well defined and resourceful 2.\nWe adopt self-supervised methods to construct\npre-training corpus for {logic ability, factual knowl-\nedge, NER}, semi-supervised method to construct\npre-training corpus for simple QA, and large-scale\nsupervised data for NLI. Further details are given\nin Â§ 4.2 and examples are given in Appendix A.\n2It is worth noting that this selection is tentative. There\nare plausible ways for selecting basic skills or knowledge\ndomains, which also inspire future directions.\n7588\nğ‘†ğ‘˜ğ‘–ğ‘™ğ‘™ğ‘…ğ‘œğ‘¢ğ‘¡ğ‘’ğ‘Ÿ!+ğ‘†ğ‘¡ğ‘œğ‘ğºğ‘ğ‘¡ğ‘’!\nCompositional Reasoning Modules (System 2)\nIterative cascaded reasoning at the ğ’Šğ’•ğ’‰ğ’”ğ’•ğ’†ğ’‘Question: What cause car accident? Please give the answer:EncoderInput DecoderOutput\nRepresentation Module(System 1)ReasoningModules(System 2)Transformer Layerğ‘›Ã— DistractionAlcohol â€¦\nğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡\nğ‘…$%&'ğ‘…() ğ‘…*+,!&â€¦ ğ‘…-./.0%* LN \nMHAAdapter\nAdapterFFN+\nLN +\nRM Layer w/Adapter\nFigure 2: ReasonFormer framework. The representation module (Â§ 3.1) and reasoning modules (RMs) (Â§ 3.2) are\ndecoupled to form the compositional reasoning process. The RMs are pre-trained with different reasoning skills\nRskill (Â§ 2). The reasoning adapter (Â§ 3.2.1) adapts the shared RMs to different reasoning steps. Router decides\nactivated skills. Stop gate decides when to stop reasoning (Â§ 3.2.2). Red lines indicate cascaded reasoning process.\n3 ReasonFormer Framework\nAs shown in Fig. 2, the general-purpose reason-\ning framework is built based on encoder-decoder\narchitecture to process multiple tasks (i.e., all pre-\ntraining tasks and downstream tasks) with a unified\nmodel, where all tasks are tackled as unified text-to-\ntext generation tasks. We first reformat all the tasks\ninto the same format using hard prompts (Sanh\net al., 2021). For example, the question-answering\ntask input can be prompted with the template: The\nquestion is {Question}. Please give the answer:\",\nand the expected output is the answer text.\nGiven the prompted task inputs, the modular and\ncompositional framework consists of two compo-\nnents in its encoder: the representation module\n(System 1) and the reasoning modules (System 2).\nThe representation module(Â§ 3.1) captures the\nintuitive understanding of problems by calculating\ninitial contextual representations. Upon the top of\nthe representation module, there are several pre-\ntrained reasoning modules(Â§ 3.2) with different\nreasoning skills, waiting for interaction to form a\ncompositional reasoning process. For reasoning\nprocess organization, there are reasoning routers\n(Â§ 3.2.2) to decide the (parallel) activated skills and\nwhen to stop the (cascaded) reasoning process.\n3.1 Representation Module\nSimilar to the perceptive function of System 1,\nthe representation module targets basic contex-\ntual understanding, and builds the foundation of\nthe following-up reasoning process. As LMs ex-\nhibit impressive ability on contextual understand-\ning, we build the representation module with cas-\ncaded Transformer layers. Given the tokenized\ninput X with length m, the initial representations\nlearnt from representation module are denoted as:\nH0 = {h0\n[CLS], h0\n1, h0\n2..., h0\nm} (1)\nwhere [CLS] is a special token.\n3.2 Reasoning Modules\nTo simulate the cognitive process (System 2)\nformed by controlled interaction between various\nfunctional areas in human brains, the reasoning\nmodules are modular and compositional. Reason-\ning modules (RMs) learn different reasoning skills\nspecified during pre-training, and are automatically\ncomposed during downstream adaptation (Â§ 3.3)\nwith reasoning router (Â§ 3.2.2). Compositionality\nis not only at the parallel level (different skills), but\nalso at the cascaded level (multi-step reasoning)\nSince different reasoning steps intuitively model\ndifferent levels of information, there are additional\n7589\nreasoning adaptersto adapt the reused modules\nto different reasoning steps.\n3.2.1 Reasoning Modules Architecture\nEach reasoning module is implemented by sev-\neral Transformer layers. As shown in Fig.2(b),\nthe shared reasoning modules with the same skill at\ndifferent reasoning depths have shared parameters\n(excluding the reasoning adapter). For example,\nFact modules at steps {0, 1, ..., n}share major pa-\nrameters. The output from the last reasoning step\nwill be recursively taken as the inputs of the reused\nreasoning modules with step-specific adapters.\nReasoning Adapter. To adapt the reused reason-\ning module to different depths of the reasoning\nprocess, we add step-specific reasoning adapters\nto the reasoning modules. Inspired by Houlsby\net al. (2019) on domain adaptation, as shown in\nFig. 2, we add two reasoning adapters following\nthe multi-head attention layer and FFN layer in\nthe Transformer layer of reasoning modules. Be-\nsides, the reasoning adapters for different skills and\ndifferent reasoning depths are non-shared.\n3.2.2 Reasoning Router\nTo compose the reasoning process, the reasoning\nrouter is critical in deciding which skills are acti-\nvated per step, and how many reasoning steps are\nrequired for problem-solving. As the example in\nFig. 1, problem-solving needs to recall facts, and\nmake logical deductions, then answer questions.\nTherefore, the activated skills and reasoning depths\nmay varied for every instance.\nAt the parallel level of each step, theskill router\ncalculates activating scores for reasoning modules.\nAfter each reasoning step, the stop gatedecides\nwhether executed reasoning steps are sufficient in\nproblem-solving through a stop gating mechanism.\nUnlike Mixture-of-Experts (MoE) (Shazeer\net al., 2017) that uses token-wise routing, we adopt\nan instance-level routing strategy, which can cap-\nture more comprehensive semantics of problems.\nSkill Router. Since the ith reasoning step has\nn reasoning modules: {R1, Â·Â·Â· , Rn}and a skill\nrouter Si, the output Hi of the ith reasoning step\ncan be calculated by router-weighted averaged out-\nputs from the k activated reasoning modules:\nHi =\nkâˆ‘\nj=1\nSi( ËœHiâˆ’1)jRj( ËœHiâˆ’1) (2)\nwhere Si( ËœHiâˆ’1)j (scalar weight) and Rj( ËœHiâˆ’1)\n(updated hidden vectors) are the outputs from the\nrouter and the jth reasoning module, respectively.\nSince deciding the skills is a non-trivial task,\nwe adopt a relatively complex router for deeper\nunderstanding. We use one Transformer layer T\nto project the original output for routing weight\ncalculation. Then, we use an FFN layer followed by\na Softmax function for weighted score calculation:\nSi( ËœHiâˆ’1) =Softmax(FFN(T( ËœHiâˆ’1))) (3)\nAfterwards, we sparsely activate(Shazeer et al.,\n2017) k reasoning modules with top-k skill routing\nscores at each reasoning step. The router training\nobjectives are detailed in Â§ 3.3.\nStop Gate. After each reasoning step, the stop\ngate decides whether the current reasoning depth\nis sufficient to solve the problem. Taking Hi as\nthe input, the stop gate uses a residual gating mech-\nanism Gi\nstop to control the information flow from\nexecuted reasoning steps and calculate the final\noutput ËœHi for the ith reasoning step by:\nËœHi = Hiâˆ’1 + Gi\nstop(Hi) (4)\nAn FFN layer is used as the stop gate Gi\nstop. When\nthe reasoning process is sufficient, the following-up\nprocess will be softly stopped by Gi\nstop.\n3.3 Pre-training and Adaptation\nThe unified model enables multi-task learning for\nboth pre-training and downstream tasks. The ma-\njor difference between pre-training and adaptation\nis that only in the pre-training stage we have the\nsupervision for the activated skills.\nPre-training. Before reasoning pre-training, the\nmodel weights of ReasonFormer are initialized\nwith pre-trained weights from T5 (Raffel et al.,\n2020). The details of model initialization and pre-\ntraining corpus collection are introduced in Â§ 4.3.1\nand Â§ 4.2, respectively. Since model acknowledges\nwhich skill it is learning, we add skill routing loss\nLr in addition to the teacher-forcing loss, to guide\nthe routers in activating skills. For example, if the\ncurrent instance focuses on logic ability, it should\nactivate {logic ability, general} skills. Lr can be\nset as the cross-entropy loss for the multi-skill clas-\nsification, where the activated skill has label 1 and\n0 otherwise. During pre-training, all the reasoning\nsteps activate the same skill for one instance.\n7590\nAdaptation. During downstream adaptation, we\nhave no prior knowledge about the required skills\nfor different tasks, so we expect the model can\nautomatically learn which skills are essential for\neach specific task. Therefore, we adopt standard\nteacher-forcing loss for generative training.\n4 Experiment Setup\n4.1 Datasets\nTo verify the effectiveness ofReasonFormer, we\nextensively conduct experiments on11 datasets em-\nphasizing different reasoning types and complexity.\nSpecifically, ReClor (Yu et al., 2020) emphasizes\non logical reasoning. Commonsense QA (CSQA)\n(Talmor et al., 2018), ARC (Clark et al., 2018),\nPIQA (Bisk et al., 2020) and HellaSwag (Zellers\net al., 2019) stress commonsense knowledge. Ab-\nductive NLI (aNLI) (Bhagavatula et al., 2019) is\na natural language inference dataset. HotpotQA\n(Yang et al., 2018a) and WikiHop (Welbl et al.,\n2018) focus on multi-hop question answering. Mu-\nTual (Cui et al., 2020), DREAM (Sun et al., 2019)\nfocus on reasoning over dialogue. RACE (Lai et al.,\n2017) is a general QA dataset. These datasets are\nrelated to the fundamental reasoning skills (Â§ 2)\nand fit nicely for analyzing the compositional rea-\nsoning process modeled by ReasonFormer.\nDuring Evaluation, the Hotpot QA adopts Ex-\nact Match (EM) as the metric, while the rest tasks\nuse accuracy as the metric. The answer for multi-\nchoice QA and classification tasks are selected by\nthe highest log-likelihood scores of options.\n4.2 Pre-training Corpus\nTo reduce the manual efforts in data collection,\nwe mainly select self(semi)-supervised pre-training\ncorpus construction methods.\nTo improve LMsâ€™ logic ability, we adopt the\nself-supervised logical pre-training corpus built by\nLogiGAN (Pi et al., 2022), which uses logical in-\ndicators to identify logical phenomena in a gen-\neral text corpus. For QA-centric pre-training, we\nadopt the semi-supervised pre-training corpus con-\nstruction method from ProQA (Lewis et al., 2021;\nZhong et al., 2022a), which adopts a generation-\nfiltering pipeline to build QA-centric corpus. To\nhelp the model in identifying entitiesfrom text,\nwe use the self-supervised NER corpus (Chen et al.,\n2022) built from Wikidata and Wikipedia anchor\nlink. To learn factual knowledge, we use Wikidata\nas a commonsense knowledge base to construct\nself-supervised pre-training corpus. Specifically,\nwe sample 1 million fact triples from Wikidata and\nconstruct the KG completion task (Moiseev et al.,\n2022) by recovering the masked tailed entities with\nthe head entities and relations given as inputs.\nFurthermore, since natural language inference\ntask already have rich supervised data, we directly\nuse MNLI (Williams et al., 2018) and SNLI (Bow-\nman et al., 2015) datasets as the pre-training corpus.\nFinally, 1 million instances are collected for each\nreasoning skill, and there are 5 millions pre-training\ninstances in total for 5 reasoning skills. The exam-\nples and prompts for constructing inputs/outputs of\nthe pre-training corpus are given in Appendix A.\n4.3 Models\n4.3.1 Model Initialization\nWe adopt encoder-decoder framework. In the\nencoder, the representation module has 9 Trans-\nformer layers, each shared reasoning module has\n3 Transformer layers and the maximum reasoning\ndepths is 3. We initialize the major model param-\neters from pre-trained T5base (Raffel et al., 2020).\nThus, the representation module is initialized by\nthe 1th â†’9th layers of T5 encoder, and the reason-\ning module is initialized by 9th â†’12th layers of\nT5 encoder. The decoder is the same with T5.\n4.3.2 Compared Methods\nThe major focuses of the experiment are to ex-\nplore the effectiveness of ReasonFormer, and ver-\nify our hypotheses that complex problems can be\ndisentangled and solved by compositional reason-\ning modules, and the decoupling of representation\nmodule and reasoning modules. We compare Rea-\nsonFormer with two series of methods.\nT5 series. (1) Vanilla T5 is the released T5\nmodel (Raffel et al., 2020) (google/t5-v1_1-base)\npre-trained with C4 corpus excluding other super-\nvised data; (2) Reasoning Pre-Trained T5 (RPT-\nT5) is the T5 model continually pre-trained with\nour reasoning-centric pre-training corpus (Â§ 4.2).\nMoRM series. Inspired by Mixture-of-Experts\n(MoE) methods (Shazeer et al., 2017; Lepikhin\net al., 2020a), we develop Mixture-of-Reasoning\nModules (MoRM) methods for comparison. Un-\nlike MoE that builds parallel experts in the FFN\nlayer of Transformer Layers, MoRM builds par-\nallel reasoning modules (RMs) on the top of the\nrepresentation module, and sparsely activate these\nRMs. Specifically, after initialized with T5, the last\n7591\nDatasets Reasoning\nT5 Series MoRM ReasonFormer\nT5 RPT-T5 w/o RPT (S) RPT (S) RPT (F) S F\nActivated Paramters (M) 248 248 272 272 357 294 407\nReClor Logic 35.2 36.8 35.4 36.8 35.4 39 39.4\nARC Commonsense 31.4 32.7 25.4 34.1 31.1 35.1 34.1\nCSQA Commonsense 56.5 65.1 57.2 63 64.7 66.9 68.2\nRACE General 63.8 67.4 66.4 68.8 70.9 72.5 73.5\nDREAM General 59.3 64.5 56.6 61.8 67.7 70.5 70.5\naNLI NLI 66.9 66.3 68.2 68.8 69.8 69.6 69.5\nMuTual Dialog 67.3 70.2 66.8 69.5 70.5 72.2 72.5\nWikiHop MultiHop 63.6 66.1 63.5 66.1 66.9 67.1 67.4\nHotpotQA MultiHop 61.1 63.3 63.1 63.3 63.8 65.2 65.5\nHellaswag Commonsense 31.5 33.7 34.2 37.9 43 53.9 54.9\nPIQA Commonsense 61.4 63.3 64.6 65.4 67.6 67.5 67.9\nAvg. 54.3 57.2 54.6 57.7 59.2 61.8 62.1\nTable 1: Main results on 11 reasoning tasks. RPT indicates reasoning-centric pre-trainingintroduced in Â§ 2 &\nÂ§ 4.2. S indicates sparse activation (top 2) of RMs, while F denotes full activation of all RMs.\n3 Transformer layers in the encoder are duplicated\nparallelly for Ns (numbers of skills) times, and\nthe outputs of them are weighted average by the\nrouting scores of the activated RMs. It increases\nthe model size in the similar way with Reason-\nFormer, so it can verify whether the improvements\nare brought by the increased parameters. Besides,\nthe major differences betweenReasonFormer and\nMoRM are (1) MoRM involves no cascaded reason-\ning steps (depth=1); (2) Like MoE, RMs in MoRM\nare jointly trained for all instances without skill\nrouting loss (Â§ 3.3), emphasizing no expertise of\nRMs. We also report the results of MoRM after\nreasoning-centric pre-training (RPT-MoRM).\n5 Experiment Analysis\n5.1 Main Results\nAs presented in Table 1, ReasonFormer outper-\nform T5 series and MoRM series across all tasks\nemphasizing the wide scope of different reasoning\nskills. Thus, we have the following findings:\nReasonFormer > MoRM & T5: Reason-\nFormer surpasses other methods (even with more\nactivated parameters) by a large margin, giving evi-\ndence to our primary hypothesis that the expertise\nof reasoning modules and the cascaded composi-\ntional reasoning process essentially help the model\nin solving complex reasoning problems.\nRPT-T5 > T5: The substantial performance\nboosts brought by RPT demonstrate that reasoning-\ntargeted pre-training is essential in injecting various\nreasoning abilities into LMs.\nSparse v.s. Full: Sparse activation of RMs leads\nto slightly reduced but comparable performance\ncompared with full activation. It suggests that al-\nthough activating more skills is beneficial, the most\nessential RM still plays the key role in problem-\nsolving. The modularity of RMs can reduce the\ncomputation burden while keeping performance.\nThese positive findings manifest that Reason-\nFormer can model compositional reasoning and\nverify our primary hypothesis that the complex\nproblem can be decomposed and well solved with\npre-trained basic skills, and the representation mod-\nule can be decoupled with the reasoning modules.\n5.2 Ablation Study\nWe explore the truly functional components ofRea-\nsonFormer through ablation studies on 7 datasets.\nWe evaluate the effectiveness of the following com-\nponents: (1) reasoning pre-training; (2) cascaded\nreasoning mechanism; (3) expertise of reasoning\nskills (skill gating loss) and (4) reasoning adapter.\nReasoning Pre-training. We assume that the\nfirst factor contributing to the improvements is the\nmulti-task reasoning-centric pre-training. Since\nvanilla LMs mainly focus on learning contextual\nsemantics, and donâ€™t emphasize higher-level rea-\nsoning ability (Pi et al., 2022; Helwe et al., 2021),\nit is intuitive that reasoning-driven pre-training can\nenhance the model in solving complex problems.\nResults in Table 2 suggest that the ablation of\npre-training from all models leads to a substan-\ntial performance drop, showing the importance of\nreasoning-centric pre-training in helping the rea-\nsoning modules to learn fundamental skills.\nCascaded Reasoning Mechanism. The second\nhypothesis is that the cascaded reasoning mech-\nanism facilitates problem-solving with different\n7592\nModules Models/Dataset ARC CSQA DREAM WikiHop HotpotQA Hellaswag PIQA Avg.\nCascaded (S)\nReasonFormer 35.1 66.9 70.5 67.1 65.2 53.9 67.5 60.9\nw/o RPT 24.1 56.9 59.5 64.2 64.4 34.7 65.6 52.8\nw/o adapter 33.1 64.2 68.4 66.8 64.8 47.1 67.4 58.8\nSingle (S)\nReasonFormer 34.8 63.7 65.9 66.6 63.9 39.7 66.3 57.3\nw/o RPT 25.4 57.3 56.7 63.6 63.1 34.3 64.6 52.1\nw/o modularity 34.1 63.1 61.8 66.1 63.4 37.9 65.4 55.9\nTable 2: Ablation study on 7 datasets. RPT denotes reasoning pre-training (Â§ 2). Modularity denotes skill gating\nloss (Â§ 3.2.2), and adapter is reasoning adapter (Â§ 3.2.1). Cascaded and Single are different in the cascaded steps.\nModels Freezed #Tuned ReClor CSQA RACE ARC MuTual WikiHop Avg.\nModules Para. (M) Acc Acc Acc Acc Acc Acc\nT5 no 248 28.2 23.2 26.2 25.1 32.6 18.3 25.6\nReasonFormer\nno 294 29.0 39.2 29.2 30.1 40.3 26.4 32.4\nRM 251 29.4 39.1 29.2 31.1 38.7 26.9 32.4\nrep. 230 29.2 38.9 29.2 30.1 38.0 26.5 32.0\nRM+rep. 188 29.2 37.8 28.4 28.8 31.6 25.4 30.2\nTable 3: Few-shot experiments after freezing different modules. The representation module is abbreviated as rep.\ncomplexity and composition orders. Single is an\nablated version of ReasonFormer in which the\nreasoning modules are not cascaded horizontally\n(depth=1) and the adapter is also eliminated. Com-\nparison between performances of Cascaded and\nSingle version of ReasonFormer (Line 1 v.s. Line\n4) demonstrates that the cascaded reasoning mech-\nanism brings notable improvements and reveals the\neffectiveness of multi-step reasoning process.\nExpertise of RMs. We assume that modularity\nand expertise of reasoning modules enables them\nto be flexibly composed. We ablate it from the\nSingle version of ReasonFormer (Line 6) by pre-\ntraining all the RMs jointly without skill routing\nloss (Â§ 3.3) using the whole pre-training corpus.\nThe apparent performance drop suggests that the\nexpertise of RMs enables the model to discriminate\nthe functionality of various skills and selectively\ncompose them to form a whole reasoning process.\nReasoning Adapter. The reasoning adapters\nadapt the shared RMs to different reasoning steps.\nIt is intuitively important as different levels of cog-\nnition focus on the information at different gran-\nularity. From Table 2, eliminating the reasoning\nadapter (Line 3) from ReasonFormer (Cascaded)\nharms the overall performance, testifying the dis-\ntinct mechanisms at different levels of reasoning\nand the importance of reasoning-centric adaptation.\n5.3 Low-resource Experiments\nIt is interesting to know whether the fundamental\nskills of RMs can be easily composed to solve new\ntasks with limited training data, and whether the\nrepresentation module and RMs can be decoupled\nduring adaptation. If the answer is true, then the\nmodelâ€™s generalization ability will be greatly en-\nhanced with easy composition of pre-trained skills.\nUnder these motivations, we conduct few-shot\nexperiments. We first examine the generalization\nof ReasonFormerand examine the decoupling of\nmodules in ReasonFormer by freezing different\nmodules during learning. Wefreeze the RMs(Line\n3) to testify whether the skills can be directly reused\nwithout further fine-tuning. Then we freeze the\nrepresentation module(Line 4) to verify the de-\ncoupling of representation and RMs. From Table 3,\nwe highlight the following findings.\n(1) ReasonFormer outperforms T5, showing\nthat the generalization ability of ReasonFormer is\nenhanced by reasoning pre-training and explicit\nmodeling of the compositional reasoning process.\n(2) Freezing RMs (Line 3) achieves comparable\nand even slightly better performance than its fully\ntuned version, demonstrating that the learned skills\ncan be easily composed with limited training data\nwithout further tuning RMs.\n(3) Freezing the representation module (Line 4)\nalso leads to comparable performance, proving that\nthe representation module and RMs can be decou-\npled in adaptation. It suggests that it is feasible to\nreduce computation burden during few-shot adap-\ntation by freezing the well-trained representation\nmodule and only tuning the RMs for tasks, which\nis more efficient when the representation module\n(e.g., gigantic LM) is extremely large for tuning.\n(4) Freezing both modules (Line 5) hurts per-\nformance, showing that model adaptation to data\n7593\nCommonsense QAaNLI Hotpot QAQuestion: Where would you find magazines along side other works?Options: A.  Doctor    B.BookstoreC.  Market    D. Train stationE. MortuaryActivated Skills:Activated Skills:Activated Skills:\nPremise: Julie had a coworker named Barry who loved to make trouble for others.Julie was embarrassed.Hypothesis Choices:A. Barry didnâ€™t tell anyone that Julie fartedB. Barry laughed at Julieâ€™s unzipped pants\nQuestion: What is the name of the fight song of the university whose main campus is in LawrencePassages: Kansas Song is a fight song of the University of Kansa, which is a public research university in the U.S.. The main campus in Lawrence â€¦ğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.99)ğ‘µğ‘¬ğ‘¹(0.3)ğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.7)ğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.7)ğ‘µğ‘³ğ‘°(0.3)\nğ‘¹ğ’†ğ’‚ğ’”ğ’ğ’ğ’Šğ’ğ’ˆğ‘ºğ’•ğ’†ğ’‘ğ’”\nğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.99)ğ‘­ğ’‚ğ’„ğ’•(0.97) ğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.65)ğ‘¸ğ‘¨(0.35)\nğ‘¹ğ’†ğ’‚ğ’”ğ’ğ’ğ’Šğ’ğ’ˆğ‘ºğ’•ğ’†ğ’‘ğ’”\nğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.97)ğ‘¸ğ‘¨(0.7)ğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.3)ğ‘®ğ’†ğ’ğ’†ğ’“ğ’‚ğ’(0.6)ğ‘¸ğ‘¨(0.4)\nğ‘¹ğ’†ğ’‚ğ’”ğ’ğ’ğ’Šğ’ğ’ˆğ‘ºğ’•ğ’†ğ’‘ğ’”\nFigure 3: Case study for activated skills analysis on 3 datasets. The answer is marked with orange.\ndistribution of specific tasks is still essential.\n5.4 Reasoning Skills Analysis\nQualitative analyses are conducted to explore how\nthe pre-trained skills are composed to solve differ-\nent reasoning tasks, and how the skills changed at\ndifferent reasoning depths. Therefore, we calculate\nthe skill routing weights at every reasoning step (up\nto 3) for three tasks (i.e., {Commonsense QA, aNLI,\nHotpot QA}. The case study provides examples and\ncorresponding (top 2) activated skills at each step.\nAs shown in Fig. 3, the activated skills are varied\nfor different tasks, and are dynamically composed\nto form a series of reasoning steps. For common-\nsense reasoning, it emphasizes {fact, QA}. For NLI\ntask, it emphasize {NER, NLI}. For multi-hop QA\ntask, it executes the QA module for multiple steps.\nThe statistical analysis of averaged routing scores\non the whole evaluation set also demonstrate the\nsame trend. These observations show improved in-\nterpretability of decision-making and give evidence\nto the hypothesis that the compositional cognitive\nprocess of humans can be transferred to AI model.\n6 Related Works\nMulti-step Reasoning. Multi-step reasoning is\na characteristic of human thinking. Multi-hop rea-\nsoning (Yang et al., 2018b; Yu et al., 2021) asks\nthe system to logically switch attention to different\ncontexts (Zhong et al., 2022b) or make a multi-\nstep deduction for a new conclusion (Dalvi et al.,\n2019; Zhong et al., 2021). Recently, chain-of-\nthought prompting (Wei et al., 2022) provides the\nmodel with manual prompts about the intermediate\nreasoning steps. Creswell and Shanahan (2022)\nuse LMs to iteratively select evidence and gener-\nate inferences. However, they always require dis-\ncrete manual-written reasoning traces. Dohan et al.\n(2022) is a position paper raising interest in mod-\neling these cascaded inference processes of LMs\nwith a probabilistic program.\nLM Modularity. Since human brains have vari-\nous functional areas, it is inspiring to explore the\nmodularity of LMs. Mixture-of-Experts (MoE)\n(Shazeer et al., 2017; Lepikhin et al., 2020b) use\nexperts in FFN layers for sparse learning. However,\ntheir major motivation is to increase the model ca-\npacity while keeping efficiency, without emphasis\non the speciality of expert. Recent works begin\nto explore domain-specific experts (Gururangan\net al., 2021) and modality-specific experts (Wang\net al., 2021). SkillNet proposes skill-specific ex-\nperts (Zhang et al., 2022). However, the activated\nskills need to be manually specified, and do not\nexplicitly model the cascaded reasoning process\nand disentangling of perception and cognition.\nConsidering these directions in the whole pic-\nture, this paper targets to explore the modeling of\nmodular and the compositional multi-step reason-\ning process of AI models in an end-to-end manner.\n7 Conclusion\nThis paper stimulates the compositional reason-\ning process of humans in decision-making, and\nmakes the following hypotheses: (1) the intuitive\nperception system (System 1) and cognitive reason-\ning system (System 2) can be decoupled and (2)\nthe complex decision-making can be disentangled\ninto multi-step execution of fundamental reason-\ning skills. Correspondingly, we propose Reason-\nFormer, a compositional general-purpose reason-\ning framework. ReasonFormer decouples the rep-\nresentation module and reasoning modules, which\nare pre-trained to expert in fundamental reason-\ning skills. The reasoning modules are dynamically\ncomposed in parallel and cascaded manner to form\na whole reasoning process. ReasonFormer is end-\n7594\nto-end and unified in solving multiple tasks with\none model. Extensive experiments on 11 tasks re-\nveal the compositional reasoning ability ofReason-\nFormer and disentangling of representation and\nreasoning modules.\nLimitations\nAs mentioned in Sec. 2, the current selection of\nfundamental reasoning skills for language models\nis limited by the availability of well-defined tasks\nand clear definitions of those tasks, as well as the\navailability of sufficient training data. As a result,\nsome skills may overlap or may not be fundamental\nenough. For example, simple QA skill may over-\nlap with NER skill to some extent. In the future,\nit would be worthwhile to explore self-supervised\ntraining tasks that can inject more fundamental\nabilities into language models. Additionally, the\nselection and combination of fundamental reason-\ning skills can be further explored. For example, the\ninclusion of numerical reasoning ability to solve\nmathematical problems. Additionally, methods for\nskill-centric pre-training corpus construction can\nalso be explored to improve the effectiveness of\nthese skills.\nEthics Statement\nThe present study was conducted in accordance\nwith ethical principles. This study involved the\nanalysis using publicly available data and knowl-\nedge sources (e.g., Wikipedia). Thus, this work did\nnot involve any human participants and potential\nrisks regarding credentials or privacy. Therefore,\nno ethical clearance was required and there were\nno potential risks associated with the conduct of\nthis research.\nAcknowledgments\nJian Yin is the corresponding author. Wan-\njun Zhong and Jian Yin are supported by the\nNational Natural Science Foundation of China\n(U1911203, U2001211, U22B2060),Guangdong\nBasic and Applied Basic Research Foundation\n(2019B1515130001), Key-Area Research and\nDevelopment Program of Guangdong Province\n(2020B0101100001)\nReferences\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Scott Wen-tau Yih, and\nYejin Choi. 2019. Abductive commonsense reason-\ning. arXiv preprint arXiv:1908.05739.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432â€“7439.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877â€“1901.\nJiawei Chen, Qing Liu, Hongyu Lin, Xianpei Han, and\nLe Sun. 2022. Few-shot named entity recognition\nwith self-describing networks. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5711â€“5722, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nAntonia Creswell and Murray Shanahan. 2022. Faith-\nful reasoning using large language models. arXiv\npreprint arXiv:2208.14271.\nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming\nZhou. 2020. Mutual: A dataset for multi-turn dia-\nlogue reasoning. CoRR, abs/2004.04494.\nBhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen\ntau Yih, and Peter Clark. 2019. Everything happens\nfor a reason: Discovering the purpose of actions in\nprocedural text. ArXiv, abs/1909.04745.\nKahneman Daniel. 2017. Thinking, fast and slow.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-dickstein, et al. 2022. Language model\ncascades. arXiv preprint arXiv:2207.10342.\n7595\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A\nSmith, and Luke Zettlemoyer. 2021. Demix layers:\nDisentangling domains for modular language model-\ning. arXiv preprint arXiv:2108.05036.\nChadi Helwe, ChloÃ© Clavel, and Fabian M Suchanek.\n2021. Reasoning with transformer-based models:\nDeep learning, but shallow reasoning. In 3rd Confer-\nence on Automated Knowledge Base Construction.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790â€“2799. PMLR.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020a.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. Learning.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020b.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. Learning.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Min-\nervini, Heinrich KÃ¼ttler, Aleksandra Piktus, Pontus\nStenetorp, and Sebastian Riedel. 2021. PAQ: 65 mil-\nlion probably-asked questions and what you can do\nwith them. Transactions of the Association for Com-\nputational Linguistics, 9:1098â€“1115.\nFedor Moiseev, Zhe Dong, Enrique Alfonseca, and Mar-\ntin Jaggi. 2022. SKILL: Structured knowledge infu-\nsion for large language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1581â€“1588,\nSeattle, United States. Association for Computational\nLinguistics.\nXinyu Pi, Wanjun Zhong, Yan Gao, Nan Duan, and\nJian-Guang Lou. 2022. Logigan: Learning logical\nreasoning via adversarial pre-training. arXiv preprint\narXiv:2205.08794.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1â€“67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\nand Claire Cardie. 2019. DREAM: A challenge\ndataset and models for dialogue-based reading com-\nprehension. Transactions of the Association for Com-\nputational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937.\nWenhui Wang, Hangbo Bao, Li Dong, and Furu Wei.\n2021. Vlmo: Unified vision-language pre-training\nwith mixture-of-modality-experts. arXiv preprint\narXiv:2111.02358.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.\n2018. Constructing datasets for multi-hop reading\ncomprehension across documents. Transactions of\nthe Association for Computational Linguistics, 6:287â€“\n302.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112â€“1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38â€“45, Online. Association\nfor Computational Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018a. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. CoRR, abs/1809.09600.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018b. HotpotQA: A\n7596\ndataset for diverse, explainable multi-hop question\nanswering. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nJianxing Yu, Qinliang Su, Xiaojun Quan, and Jian Yin.\n2021. Multi-hop reasoning question generation and\nits application. IEEE Transactions on Knowledge\nand Data Engineering.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng.\n2020. Reclor: A reading comprehension dataset re-\nquiring logical reasoning. In International Confer-\nence on Learning Representations (ICLR).\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint\narXiv:1905.07830.\nFan Zhang, Duyu Tang, Yong Dai, Cong Zhou,\nShuangzhi Wu, and Shuming Shi. 2022. Skillnet-nlu:\nA sparsely activated model for general-purpose nat-\nural language understanding. arXiv e-prints, pages\narXivâ€“2203.\nWanjun Zhong, Yifan Gao, Ning Ding, Yujia Qin,\nZhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and\nNan Duan. 2022a. Proqa: Structural prompt-based\npre-training for unified question answering. arXiv\npreprint arXiv:2205.04040.\nWanjun Zhong, Junjie Huang, Qian Liu, Ming Zhou, Ji-\nahai Wang, Jian Yin, and Nan Duan. 2022b. Reason-\ning over hybrid chain for table-and-text open domain\nqa. arXiv preprint arXiv:2201.05880.\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu,\nDaya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and\nNan Duan. 2021. Ar-lsat: Investigating analytical\nreasoning of text. arXiv preprint arXiv:2104.06598.\nA Example of Pre-training Tasks\nFor the basic question answering skill, QA-centric\npre-training uses a generation-filtering pipeline to\nbuild semi-supervised large-scale corpus (Lewis\net al., 2021; Zhong et al., 2022a): (1) use anno-\ntated QA data to train a passage-to-question-answer\ngenerator (2) taking the wikipedia passages as in-\nputs, and generates corresponding pseudo ques-\ntions and answers (3) filtering passage, question,\nanswer pairs with a QA model.\nFor logic skill, we use the automatically con-\nstructed data from LogicGAN (Pi et al., 2022).It\nuses logical indicators (e.g., Therefore, as a re-\nsult) to automatically identify logical inference phe-\nnomenon presented via natural language, and mask\ncorresponding causes/results of events, and ask the\npre-trained model to recover them to learn logical\nreasoning ability.\nFor the natural language inference, we the pub-\nlic annotated corpus SNLI (Bowman et al., 2015)\nand MNLI (Williams et al., 2018). Given a sen-\ntence as premise, the model is expected to predict\nwhether the premise sentence entails the hypothesis\nsentence.\nFor the named entity recognition skill, we use\nweakly-annotated data (Chen et al., 2022) obtained\nfrom Wikipedia and Wikidata. The mentions are\nthe text with anchorlink and the types are obtained\nfrom Wikidata â€œinstance ofâ€ or â€œsubclass ofâ€ prop-\nerties. We design three pretrain tasks similar to\nChen et al. (2022): 1) given the sentence, identify\nall mentions in the sentence 2) given the sentence\nand interested types, output all mentions with these\ntypes in the sentence 3) given the sentence and\nmentions, predict all types of the mentions.\nFor the fact skill, we use fact triples from Wiki-\ndata, and design a task that predict the tail entity\ngiven the head entity and relation as Moiseev et al.\n(2022).\nA summary of the examples for each tasks is\npresented in Fig 4.\nB Implementation Details\nPretraining Details We use â€œgoogle/t5-v1_1-\nbaseâ€ from HuggingFace (Wolf et al., 2020) im-\nplementation as base model for all our experiments.\nWe use a learning rate of 5e-5 and train all models\nwith 5 epochs. The warmup ratio is set to 0.1. The\ntotal batch size is set to 72 for shared model and\n64 for private model. The down projection hidden\nsize of adapter is set to 256. We use 8 V100 GPUs\nfor model training.\nDownstream Adaptation Details For all the full\ndata experiments, we use a learning rate of 1e-4\nand training epoch of 10 with a batch size of 48 for\nour models. The model is validated at the end of\neach epoch. For all the few-shot experiments, we\nuse a learning rate of 1e-5 and training epochs of\n200 with a batch size of 8. The model is validated\nper 200 steps.\n7597\nSkill Corpus Example Prompt\nQA Wikipedia Context: â€¦appeared to Saint Bernadette Soubirous in 1858. At the endâ€¦\nQuestion:To whom did the Virgin Mary allegedly appear in 1858 in Lourdes \nFrance?\nAnswer:  Saint Bernadette Soubirous\nInput: {context} [SEP] give me the answer of \n{question}\nOutput: {answer}\nLogic BookCorpus Context: All men are mortal, and Socrates is a man. Therefore, [MASK], â€¦.\nMASK: Socrates is mortal\nInput: {context} [SEP] give me the missing \nstatement\nOutput: {mask}\nNLI SNLI, MNLI Premise:Conceptually cream skimming has two basic dimensions - product \nand geography.\nHypothesis:Product and geography are what make cream skimming work.  \nLabel:Neutral\nInput: {premise} [SEP] {hypothesis} give me \nthe relation between the first and second \nsentences\nOutput: {label}\nNER\nWikipedia,\nWikidata\nContext: â€¦whose family had originally migrated from the state of Mysore.\nType_str:city\nMention_str: Mysore is city.\nInput: {context} [SEP] give me the mentions \nwith types {type_str}\nOutput: {mention_str}\nContext:â€¦ nationalist, communist and anarchist who was among the \nfounding members of the Communist Party of India (Tashkent group).\nMention_str:India, nationalist, Tashkent\nInput: {context} [SEP] give me all mentions\nOutput: {mention_str}\nContext: â€¦whose family had originally migrated from the state of Mysore.\nMention_str: Mysore\nType_str: Mysore is city.\nInput: {context} [SEP] give me the types of \nmentions {mention_str}\nOutput: {type_str}\nFact Wikidata Head:      Knut Wijkmark\nRelation: child\nTail: Nils Wijkmark\nInput:    {head} {relation} [SEP] give me the \nmissing entity:\nOutput: {tail}\nFigure 4: Examples of pre-training tasks.\n7598\nACL 2023 Responsible NLP Checklist\nA For every submission:\nâ–¡\u0013 A1. Did you describe the limitations of your work?\nLast section after the conclusion section.\nâ–¡\u0013 A2. Did you discuss any potential risks of your work?\nI donâ€™t think of any risk. It is shown in the ethical statement section.\nâ–¡\u0013 A3. Do the abstract and introduction summarize the paperâ€™s main claims?\nLeft blank.\nâ–¡\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB â–¡\u0013 Did you use or create scientiï¬c artifacts?\nThe applied datasets are mentioned in Section 4.\nâ–¡\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.\nâ–¡ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. These datasets are suitable for academic research purpose.\nâ–¡ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciï¬ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. These datasets are suitable for academic research purpose.\nâ–¡ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiï¬es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\nâ–¡\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4.1\nâ–¡\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiï¬cant, while on small test sets they may not be.\nWe cover a wide scope of public evaluation datasets. The complete details canâ€™t be introduced in the\nmain pages due to the page limits.\nC â–¡\u0013 Did you run computational experiments?\nSection 4\nâ–¡\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7599\nâ–¡\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix B and Section 4\nâ–¡\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\nâ–¡\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nThe evaluation metrics are introduced in Section 4.1\nD â–¡\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nâ–¡ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\nâ–¡ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participantsâ€™ demographic\n(e.g., country of residence)?\nNo response.\nâ–¡ D3. Did you discuss whether and how consent was obtained from people whose data youâ€™re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\nâ–¡ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\nâ–¡ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7600",
  "topic": "Deductive reasoning",
  "concepts": [
    {
      "name": "Deductive reasoning",
      "score": 0.7111733555793762
    },
    {
      "name": "Computer science",
      "score": 0.6745743155479431
    },
    {
      "name": "Reasoning system",
      "score": 0.6575038433074951
    },
    {
      "name": "Verbal reasoning",
      "score": 0.6352211236953735
    },
    {
      "name": "Visual reasoning",
      "score": 0.6113753914833069
    },
    {
      "name": "Qualitative reasoning",
      "score": 0.5955488085746765
    },
    {
      "name": "Non-monotonic logic",
      "score": 0.5809462666511536
    },
    {
      "name": "Analytic reasoning",
      "score": 0.5616978406906128
    },
    {
      "name": "Adaptive reasoning",
      "score": 0.5415984988212585
    },
    {
      "name": "Automated reasoning",
      "score": 0.5412139892578125
    },
    {
      "name": "Modular design",
      "score": 0.523813784122467
    },
    {
      "name": "Model-based reasoning",
      "score": 0.5208002328872681
    },
    {
      "name": "Opportunistic reasoning",
      "score": 0.4892094135284424
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.4810795485973358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47686895728111267
    },
    {
      "name": "Psychology of reasoning",
      "score": 0.46463990211486816
    },
    {
      "name": "Cognition",
      "score": 0.4232610762119293
    },
    {
      "name": "Programming language",
      "score": 0.2894613444805145
    },
    {
      "name": "Psychology",
      "score": 0.09005016088485718
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}