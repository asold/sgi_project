{
    "title": "Double-branch feature fusion transformer for hyperspectral image classification",
    "url": "https://openalex.org/W4313646123",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2113705901",
            "name": "Lanxue Dang",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2160296457",
            "name": "Libo Weng",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2166075115",
            "name": "Yane Hou",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2121855495",
            "name": "Xianyu Zuo",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A1983143503",
            "name": "Yang Liu",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2113705901",
            "name": "Lanxue Dang",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2160296457",
            "name": "Libo Weng",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2166075115",
            "name": "Yane Hou",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A2121855495",
            "name": "Xianyu Zuo",
            "affiliations": [
                "Henan University"
            ]
        },
        {
            "id": "https://openalex.org/A1983143503",
            "name": "Yang Liu",
            "affiliations": [
                "Henan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2069231830",
        "https://openalex.org/W2001298023",
        "https://openalex.org/W1998030734",
        "https://openalex.org/W2767892373",
        "https://openalex.org/W2015415108",
        "https://openalex.org/W2161815745",
        "https://openalex.org/W2499938349",
        "https://openalex.org/W2614166664",
        "https://openalex.org/W2991616716",
        "https://openalex.org/W4320339642",
        "https://openalex.org/W2129777864",
        "https://openalex.org/W1978381081",
        "https://openalex.org/W2114819256",
        "https://openalex.org/W2151665594",
        "https://openalex.org/W2765452533",
        "https://openalex.org/W2888715336",
        "https://openalex.org/W2609880332",
        "https://openalex.org/W1964155876",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W2764276316",
        "https://openalex.org/W2888119354",
        "https://openalex.org/W3125860323",
        "https://openalex.org/W2966187434",
        "https://openalex.org/W3197240722",
        "https://openalex.org/W4220805125",
        "https://openalex.org/W4293661538",
        "https://openalex.org/W3128776197",
        "https://openalex.org/W2971432438",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W3031696400",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2312906878",
        "https://openalex.org/W2793941577"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports\nDouble‑branch feature fusion \ntransformer for hyperspectral \nimage classification\nLanxue Dang 1,2,3, Libo Weng 1, Yane Hou 1*, Xianyu Zuo 1 & Yang Liu 1,2\nDeep learning methods, particularly Convolutional Neural Network (CNN), have been widely used \nin hyperspectral image (HSI) classification. CNN can achieve outstanding performance in the field of \nHSI classification due to its advantages of fully extracting local contextual features of HSI. However, \nCNN is not good at learning the long‑distance dependency relation and dealing with the sequence \nproperties of HSI. Thus, it is difficult to continuously improve the performance of CNN‑based models \nbecause they cannot take full advantage of the rich and continuous spectral information of HSI. This \npaper proposes a new Double‑Branch Feature Fusion Transformer model for HSI classification. We \nintroduce Transformer into the process of HSI on account of HSI with sequence characteristics. The \ntwo branches of the model extract the global spectral features and global spatial features of HSI \nrespectively, and fuse both spectral and spatial features through a feature fusion layer. Furthermore, \nwe design two attention modules to adaptively adjust the importance of spectral bands and pixels \nfor classification in HSI. Experiments and comparisons are carried out on four public datasets, and \nthe results demonstrate that our model outperforms any compared CNN‑Based models in terms of \naccuracy.\nDue to the advancement of current imaging spectrometry techniques, hyperspectral image (HSI) contains \nrich spectral and spatial information with high spectral and spatial  resolution1, so pixel-level classification \ncan be  achieved2,3. HSI are widely used in many fields, such as atmospheric environment  research4, precision \n agriculture5–7, and ocean  research8. However, there is a lot of redundant information in the spectral bands of \nHSI and the difficulty in obtaining samples of  HSI9 brings difficulties to the classification of HSI. In early studies \nof HSI classification, some machine learning-based approaches, such as  SVM10, k-NN11, and multilayer percep-\ntron (MLP)12, were used for HSI classification. However, most of them focus on the spectral information of HSI \nwithout taking full advantage of the spatial information of HSI. Although some methods based on morphological \n profiles13 and Gabor  feature14 are presented to extract spatial features, the classification accuracy is still unsatisfac-\ntory. This is because these methods can only extract low-level features and the limited training samples of HSI.\nThe rapid development of deep learning techniques has brought the more diversified effective approaches for \nHSI classification. Deep learning follows an “end-to-end” design philosophy and can automatically extract linear \nand nonlinear features. Compared with traditional methods, which require a large amount of domain expert \nknowledge, deep learning methods can avoid designing manual features and improve the generalization ability of \nthe model. Some deep learning-based models, such as Stacked Autoencoder (SAE)15, Recurrent Neural Network \n(RNN)16,17, and deep belief network (DBN) 18, have been merged and successfully applied to HSI classification. \nHang et al.17 proposed a model consisting of two RNN layers that can extract complementary information from \nnon-adjacent spectral bands of HSI. RNN-based models can extract spectral features by considering the spectral \ndimension of HSI as a sequence, but they are prone to gradient vanishing, and difficult to learn long-distance \ndependency  relations19.\nConvolutional Neural Network (CNN) can effectively extract the spatial features of HSI, due to its powerful \nability to extract local contextual information. A lot of CNN-based models have appeared in recent years. Hu \net al.20 firstly used CNN for HSI classification and proposed a 1DCNN-based model, which includes multiple \n1DCNNs and only considers the spectral features of HSI. Although the performance of 1DCNN-based model \nis poor, it has promoted the development of CNN-based models in HSI classification. Subsequently, a series of \nCNN-based models taking account of spectral and spatial features of HSI has been developed. Zhong et al. 21 \nOPEN\n1Henan Key Laboratory of Big Data Analysis and Processing, Henan University, Kaifeng 475001, China. 2Henan \nProvince Engineering Research Center of Spatial Information Processing, Henan University, Kaifeng 475001, \nChina. 3School of Computer and Information Engineering, Henan University, Kaifeng 475001, China.  *email: \nhouyane@henu.edu.cn\n2\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\npresented a 3DCNN-based model through a 3D convolution kernel to extract spectral-spatial features of HSI. \nPaoletti et al.22 designed a 2DCNN-based model based on deep pyramid  network23, which can improve the clas-\nsification performance by stacking a large number of convolution kernels. Li et al.24 proposed a 3DCNN-based \nDouble-Branch model, where the two branches extract spectral and spatial features of HSI respectively. Gao \net al.25 proposed a small convolution and feature reuse (SC-FR) module by combining cascaded 1 × 1 convolu-\ntional layers and cross-layer connections. There is only one 3 × 3 convolution in the model to extract spatial fea-\ntures of hyperspectral images. Dang et al.26 proposed a dual-path and small-convolution-based module (DPSC) \nfor the extraction of spatial and spectral features from hyperspectral images. Both of these models are based \non small convolutions to build lightweight models. Chang et al. 27 proposed a method based on a consolidated \nconvolutional neural network (C-CNN) composed of 2DCNN and 3DCNN to learn the spatial-spectral features \nand abstract spatial features of hyperspectral images. Shi et al.28 proposed a model based on multi-scale feature \nfusion and double attention mechanism to extract features from hyperspectral images. Although the CNN-based \nmodels have made some progress in HSI classification, the performance of them is still insufficient. First, HSI \nusually contains hundreds of bands and the spectral characteristics of some ground objects are extremely similar. \nCNN is not good at learning long-distance dependency relations of spectral  bands29, and cannot accurately clas-\nsify such objects. Secondly, the size of the convolution kernel in the CNN-Based model is usually small, and it \nis easy to extract the local features rather than the global features of the entire neighborhood pixel blocks. These \nproblems cause the bottleneck of the CNN-based model in the classification of HSI. Improving the performance \nof CNN-based model in HSI classification becomes very important and meaningful.\nThe development of  Transformer30 techniques brings a new idea to HSI classification, which was originally \nused in the field of Nature Language Processing (NLP). Transformer is very effective at processing sequence \n data30, which can extract global features of input data through a self-attention mechanism, and can better learn \nlong-distance dependency relations of input  data31,32. Dosovitskiy et al.32 proposed the first Transformer-based \nmodel for computer vision, Vision Transformer(ViT), and achieved good results. This model extracts global \nfeatures by segmenting the image into patches. We can apply Transformer to extract features of HSI by regarding \nHSI as a sequence. HSI can be regarded as sequences in two ways. One is that the spectral bands of HSI are rich \nand continuous, so the entire spectral bands can be treated as a sequence. The other is that the spectral vector \nof each pixel can be considered as a word vector in the NLP  field31, because of each pixel representing a ground \nobject. However, simply applying the Transformer model, for example, vision transformer (ViT) 32, into HSI \nclassification still has many problems. First of all, segmenting the neighborhood pixel blocks with a fixed size \nlike ViT makes it difficult to extract the low-level features of the input  data33. Next, segmenting neighborhood \npatches only in the spatial dimension still fails to learn long-range dependency relations for the spectral features \nof HSI.In view of this, this paper proposes a Double-Branch Feature Fusion Transformer (denotedas DBFFT) \nmodel for HSI classification. The proposed model adopts two branches to extract spectral and spatial features \nof HSI respectively. The spectral branch consists of a spectral attention module and Transformer encoder block. \nThe spatial branch is made up of a spatial attention module and Transformer encoder block. In addition, a feature \nfusion layer is designed between these two branches to fuse spectral and spatial features. The outputs obtained \nby the two branches are fused by addition operation, and finally used for classification. The main contributions \nof this paper can be described as follows:\n• The proposed model extracts the spectral features and spatial features of HSI respectively through a Double-\nBranch structure. In the two branches, according to the sequence characteristics of hyperspectral images, \nPixel-wise embedding and Band-wise embedding are adopted to effectively extract the long-distance depend-\nency relations of spectral dimension of HSI and the global spatial feature of HSI.\n• We design a CNN-based spectral attention module and a spatial attention module, which can adaptively \nadjust the importance of spectral and spatial features of the input data, and extract rich spectral and spatial \nfeatures.\n• Our proposed model adopts label smooth techniques to alleviate the overfitting phenomenon of the model \nwhen the number of samples is small. In addition, we design a feature fusion layer to fuse the features \nextracted by the two branches to improve the performance of the model.\nThe remainder of this paper is organized as follows. In Sect. “ Methodology” , we describe the details of our \nproposed model. In Sect. “Experiments results and analysis” , we present and analyze the experimental results, in \naddition to analyzing the factors that affect the performance of the model. In Sect. “Conclusion” , we give conclu-\nsions and present directions for future work.\nMethodology\nOverview of the proposed model. We set the HSI to be a data cube with length S, width M, and number \nof bands C. We take each labeled pixel as the center and segment a 3D cube of size H × H × C called the neigh-\nborhood pixel block, where H is the length and width of the neighborhood pixel block, C represents the number \nof spectral bands of the HSI. We take neighborhood pixel blocks as input to the model to fully utilize the spectral \nand spatial information of HSI.\nFigure 1 shows the overall structure of our proposed model. The model contains two branches to extract \nspectral features and spatial features of HSI respectively. We take the upper branch as the spectral branch and \nthe lower branch as the spatial branch. The spectral branch consists of the spectral attention module and the \nTransformer encoder block. The spatial branch is made up of a spatial attention module and a Transformer \nencoder block. Inspired by  CrossViT34, we add a feature fusion layer between the two branches to fuse the spatial \nfeatures and the spectral features.\n3\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nThe spectral branch first uses the spectral attention module to extract the rich spectral features of the neigh-\nborhood pixel blocks of size H × H × C . Then the dimension of the spectral dimension is reduced from C to k \nto remove redundant information, and a new feature map of size H × H × k will be gotten. We set k = 32. After \nthat, the feature map is segmented according to the spectral dimension to obtain k patches of size H × H , which \nare flattened and processed by linear projection to generate a sequence of shape (batch size, k + 1 , M ), where \nM represents the length of the vector in the sequence. This sequence will be used as input to the Transformer \nencoder block of the spectral branch. The spectral branch of our proposed model can utilize self-attention to \nextract global features, capturing the long-distance dependency relations of the spectral dimension.\nThe spatial branch first uses the spatial attention module to extract the rich spatial features of the neigh -\nborhood pixel blocks of size H × H × C to obtain a new feature map of size H × H × C . The feature map is \nsegmented by pixel, and H × H vectors of length C are obtained and processed by linear projection to generate \na sequence of shape (batch size, H × H , M). Use this sequence as the input to the Transformer encoder block. \nThe spatial branch can extract the global spatial features of HSI.\nFinally, the outputs of the two branches are fused to fuse spectral features and spatial features. We will describe \nthe abovementioned parts in detail in the following sections.\nDepth‑wise separable convolution. As shown in Fig.  2, the depth-wise separable convolution consists \nof a depth-wise convolution layer and a 1 × 1 convolution layer. Depth-wise separable convolution can extract \nrich low-level features from HSI at the beginning of the entire attention module. Each convolution kernel in the \ndepth-wise convolution only extracts spatial features in one spectral dimension. The 1 × 1 convolution fuses the \nfeatures of different spectral bands to obtain a feature map. Since the spectral information of HSI is rich and \nredundant, the use of depth-wise separable convolution can reduce the redundant information of the extracted \nspectral dimension and the interference of redundant bands on feature extraction.\nFigure 1.  The structure of the DBFFT. This model consists of two branches. The upper branch consists of a \nspectral attention module and Transformer encoder block to extract spectral features of HSI. The lower branch \nconsists of spatial attention module and Transformer encoder block to extract spatial features of HSI.\nFigure 2.  The depth-wise separable convolution consists of two parts: (a) depth-wise convolution. (b) 1 × 1 \nconvolution.\n4\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nSpectral attention module. The redundant spectral information of raw HSI data will interfere with the \nrecognition of the model. Therefore, by processing the HSI with the spectral attention module, the influence of \nnoise information on the model is reduced, and the redundant information of HSI is reduced. The framework of \nthe module is shown in Fig. 3. We extract the pixel-centered neighborhood pixel block of shape H × H × C as \ninput, where H represents the size of the neighborhood pixel block and C represents the spectral dimension of \nthe HSI. First, the low-level features of the neighborhood pixel blocks are extracted through two layers of depth-\nwise separable convolution layers. Second, the spectral attention se∈ R 1×1×C  is generated by spectral attention \nto adjust the importance of each spectral band, and then the obtained feature map is fused with the original data \nto retain the original spectral and spatial features. Finally, the spectral features of the spectral dimension are \nfused through two 1 × 1 convolution layers with GeLU. The above process does not change the size of the neigh-\nborhood pixel blocks, but it can reduce the spectral dimension and redundant spectral features.\nThe spectral attention mechanism can automatically adjust the importance of different spectral bands for \nclassification and reduce the interference of useless bands to the model. Figure  4 shows the whole process of \ngenerating spectral attention. Inspired by SE-block 35, our computational process for generating spectral atten -\ntion se is defined as follows:\nwhere E represents the obtained feature map after the neighborhood pixel block is processed by two depth-wise \nseparable convolution layers, E\n(\nk , i, j\n)\n represents the value of the position (i, j) of the k-th channel of the feature \nmap E, havg represents the result of global average pooling, havg\n(k)  represents the value of the kth channel of havg , \nand σ1 and σ2 represent ReLU and sigmoid activation functions, respectively. FC1 and FC2 are two fully connected \nlayers. The first layer reduces the dimension from M to M/r, and the second layer increases the dimension from \nM/r to M. We set r to be 16.\nAfter spectral attention se and feature map F1 are multiplied by band, the importance of different bands can \nbe automatically adjusted.\n(1)havg\n(k) = 1\nH × H\nH∑\ni=1\nH∑\nj=1\nE\n(\nk,i,j\n)\n(2)se = σ2\n(\nFC 2\n(\nσ1\n(\nFC 1\n(\nhavg))))\nFigure 3.  The structure of the attention module. The input of this model is the neighborhood pixel patch of the \noriginal hyperspectral image, and the output is the feature map.\nFigure 4.  Generate spectral attention. This module contains a global average pooling and a multilayer \nperceptron (MLP) consisting of two fully connected layers.\n5\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nSpatial attention module. Since we use the neighborhood pixel block as the input of the model, we usu-\nally regard the labels of all pixels of the neighborhood pixel block as the label of the center pixel. It will lead to \nthe interference of the information of the pixels with different labels of the original center pixel to the  model36. \nTherefore, we use a spatial attention module to enhance the information of pixels that are helpful for classifica-\ntion and weaken the information of pixels that interfere with classification. The framework of the spatial atten-\ntion module is the same as Fig. 3, the difference lies in the part that generates the attention, which will generate \na spatial attention. And this module does not change the spectral dimension of the input data.\nFigure 5 shows the whole process of generating spatial attention. Inspired by  CBAM37, we first perform global \naverage pooling and global max pooling in the spectral dimension to generate savg  and smax  of shape H × H × 1 . \nThe calculation process of this part is described in Eqs. ( 3) and (4).\nwhere F represents the feature map obtained after the neighborhood pixel block is processed by two depth-wise \nseparable convolution layers in the spatial branch, F(κ,i,j) represents the value of the position (i,j) of the feature \nmap F on the kth channel, savg  represents the result of global average pooling, Savg\n(i,j) represents the value of the \nposition (i, j) of savg  , Max(F) represents the maximum value of all channels of each pixel in the feature map F ..\nThen, we concatenate savg  and smax  . After processing through a convolutional layer and a sigmoid activation \nfunction, the spatial attention sa∈ RH ×H ×1 is obtained.\nAfter the spatial attention sa and the feature map F2 are multiplied by pixels, the importance of different pixels \nfor classification can be automatically adjusted.\nPixel‑wise embedding and Band‑wise embedding. The classic ViT structure segments the image \ninto patches according to a fixed size. When ViT has simply been applied to segment the image, it is not suit-\nable for the characteristics of HSI because each pixel on the HSI represents a ground object. Meanwhile, such \na segmentation method cannot learn the long-distance dependency relations of the spectral bands of HSI. To \nbetter combine the characteristics of HSI, we adopt Pixel-wise embedding and Band-wise embedding in the two \nbranches to better learn the global features of HSI. In the spatial branch, we perform Pixel-wise embedding on \nthe feature maps of the spatial attention module. We segment the feature map of shape H × H × C by pixel to \ngenerate H × H vectors of length C . Finally, the length of the vector is adjusted to M by the full connection layer \nprocessing, and M is set to 64. We did not add position embedding to the vectors because the CNN can encode \nthe absolute position of the  image38.\nConsidering that the spectral dimension information of the feature map is rich and continuous, we use Band-\nwise embedding to segment the HSI according to the spectral dimension, and then flatten the two-dimensional \npatch of each band. After that, the vector of output length M is processed through the fully connected layer as \nthe input of the Transformer. This can learn long-distance dependency relations in the spectral dimension of \nHSI. Lastly, the generated sequence is used as the input of the transformer, after adding the positional embed -\nding and the learnable embedding. Figure  6 illustrates how Pixel-wise embedding and Band-wise embedding \nprocess feature maps into sequences. Although the linear projection methods of the two branches are different \nfor the characteristics of HSI, the length of the vector after linear projection is the same, which is to facilitate the \nfusion of features at the feature fusion layer.\n(3)Savg\n(i,j) = 1\nc\nc∑\nk=1\nF\n(\nκ , i,j\n)\n(4)smax = Max(F)\n(5)sa\n′\n= conv\n([\nsavg,smax ])\n(6)sa = sigmoid\n(\nsa\n′ )\nFigure 5.  Generate spatial attention. This module concatenates the outputs of global average pooling and global \nmax pooling through a convolutional layer.\n6\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nTransformer encoder block. Each branch of our proposed model contains two Transformer encoder \nblocks respectively to extract global features of HSI. As shown in the Fig.  7a, each transformer encoder block \nconsists of a multi-head self-attention mechanism sublayer and a Feedforward network sublayer, and each sub-\nlayer has LayerNormalization and residual connections. Figure  7b shows the processing of the self-attention \nmechanism in Transformer. The self-attention mechanism can extract the global features of the input sequence, \nand its calculation process is described in Eq. (7).\nwhere K , Q , V are obtained by multiplying the input sequence with wQ , wK and wV respectively. dk represents \nthe dimension of the vector in K, whose role is to obtain a stable gradient by  scaling19. Multi-head self-attention \nmechanism is to concatenate the outputs obtained by multiple self-attentions. Multiple heads are computed \nindependently and each head has a different focus on the sequence. The formula is defined as follows:\nwhere W o is a matrix and h represents the number of heads.\n(7)z = Attention(Q ,K ,V )softmax\n(QkT\n√dk\n)\nV\n(8)Mulit − Head attention(K,Q,V) = concat(z1,z2,... ,zh)W o\nFigure 6.  Two ways of linear projection methods. (a) Band-wise embedding (b) Pixel-wise embedding.\nFigure 7.  Structure of the Transformer encoder block and the illustration of the self-attention mechanism. (a) \nTransformer encoder block. (b) self-attention mechanism.\n7\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nThe Feedforward network consists of two fully connected layers and a GeLU activation function, which can \nfurther transform the features learned in self-attention mechanism. Equation (9) gives its calculation process.\nwhere σ denotes GeLU activation function.\nFeature fusion layer. Our proposed model extracts spatial and spectral features of HSI on two branches \nseparately. Inspired by  CrossViT34, we add a feature fusion layer between the two branches to fuse the features \nextracted by the two branches. Specifically, we consider exchanging the class tokens (i.e. the Learnable Embed-\nding illustrated in Fig.  6) of the output sequence of the Transformer encoder block of the spectral branch and \nthe first vector of the output sequence of the Transformer encoder block of the spatial branch. It is because the \nTransformer-based model uses the first vector of the output sequence to classify. Thus, we can think of this vector \nas a summary of the entire  sequence34. Therefore, the class token of the output sequence of the spectral branch \ncontains rich spectral features, and the first vector of the output sequence of the spatial branch contains rich \nspatial features. By exchanging these two vectors, the fusion of spectral and spatial features can be facilitated.\nLabel smooth. When the training samples that are used to train the model are insufficient, the generaliza-\ntion ability of the model will be reduced, which will lead to overfitting of the model. In practical applications, this \nproblem of insufficient HSI samples is also very common. In order to decrease the influence of the overfitting \nphenomenon on the model, we introduce a regularization technique label smooth to alleviate it.\nFirst, we change each label to use a one-shot representation. The vector yn represents the one-shot representa-\ntion of each label y, its dimension is S dimension, where S represents the number of classes, and the value on the \nvector is 1 when n = y, otherwise it is 0. Then, we add noise ε to the label as follows:\nwhere y\n′\nn is the new label obtained after label smooth, ε is the noise.\nThe model tends to become more \"confident\" during the training process, but the lack of training set samples \nand mislabeling of the dataset will cause the model to generate more misclassifications in the test set. By adding \nnoise to each label, the model becomes \"unconfident\", the generalization ability of the model is improved, and \nthe overfitting of the model is alleviated.\nExperiments results and analysis\nData sets description. We adopt four public datasets: Kennedy Space Center (KSC), Salinas (SA), Univer-\nsity of Pavia (PU), and Houston 2013(HU) to evaluate the performance of the proposed model.\nKennedy Space Center (KSC): This dataset was collected by AVIRIS sensors over the Kennedy Space Center \n(KSC) in Florida, USA. This dataset contains 512 × 614 pixels, and after removing the noise-affected bands, a total \nof 176 bands are available for experiments. It has a spatial resolution of 18 m and a wavelength range of 400 to \n2500 nm. It contains a total of 13 land cover categories with a total of 5211 labeled pixels. The training samples, \nvalidation samples and test samples for each category are shown in the Table 1.\nSalinas (SA): This dataset was collected by AVIRIS sensors over the Salinas Valley in California. This dataset \ncontains 512  × 217 pixels, and after removing the noise-affected bands, a total of 204 bands are available for \nexperiments. It has a spatial resolution of 3.7 m and a wavelength range of 400 to 2500 nm. It contains a total of \n16 land cover categories with a total of 54,129 labeled pixels. The training samples, validation samples and test \nsamples for each category are shown in the Table 2.\n(9)Feedforward network\n(\ninput\n)\n= FC\n(\nσ\n(\nFC\n(\ninput\n)))\n(10)y\n′\nn = (1 − ε)yn + ε\nS\nTable 1.  Number of training, validation, and test samples for KSC dataset.\nNO Class Train Val Test\n1 Scrub 39 38 684\n2 Willow swamp 12 13 218\n3 CP hammock 13 13 230\n4 Slash pine 13 13 226\n5 Oak/broadleaf 8 9 144\n6 Hardwood 12 11 206\n7 Swamp 6 5 94\n8 Graminoid marsh 22 22 387\n9 Spartina marsh 26 26 468\n10 Cattail marsh 20 21 363\n11 Salt marsh 21 21 377\n12 Mud flats 26 25 452\n13 Water 46 47 834\nTotal 264 264 4683\n8\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nUniversity of Pavia (PU): This dataset was collected by ROSIS sensors over the University of Pavia in northern \nItaly. This dataset contains 610 × 340 pixels, and after removing noise-affected bands, a total of 103 bands are \navailable for experiments. It has a spatial resolution of 1.3 m and a wavelength range of 430 to 860 nm. It contains \na total of 9 land cover categories with a total of 42,776 labeled pixels. The training samples, validation samples \nand test samples for each category are shown in the Table 3.\nHouston 2013 (HU): This dataset was collected by the ITRES CASI-1500 sensor over the University of Houston \ncampus, which is provided by the 2013 IEEE GRSS Data Fusion Competition 39. This dataset contains 349 × 1905 \npixels. This dataset has 144 spectral bands for experiments. It contains a total of 15 land cover categories with \na total of 15,029 labeled pixels. The training samples, validation samples, and test samples for each category are \nshown in Table 4.\nFor deep learning methods, the more samples are used for training, the better the performance of the model \nwill be gotten. It means that the training of the model will be more time-consuming as well as requiring more \nlabeled pixels. Our proposed model can still maintain the optimal performance in the case of small samples. \nTherefore, for KSC, we consider 5% of the samples for training, 5% for validation, and the rest for testing. For \nPU, SA, and HU, we consider 1% of samples for training, 1% for validation, and the rest for testing.\nExperimental setup. The software environment for our experiments is Python version 3.7.0 and the deep \nlearning framework in PyTorch version 1.2.0. The hardware environment for our experiments is RTX2060 GPU \nwith 6 GB RAM and AMD CPU R7-4800 at 2.9 GHz with 16 GB RAM. We choose SGD  optimizer40 to optimize \nthe training parameters of the model, and the loss function chooses the cross-entropy loss function. The learn-\ning rate is set to 0.001, 0.001, 0.01, and 0.001 on KSC, SA, PU, and HU respectively. The epoch on four datasets \nis set to 200.\nTable 2.  Number of training, validation, and test samples for SA dataset.\nNO Class Train Val Test\n1 Brocoli_green_weeds_1 21 20 1968\n2 Brocoli_green_weeds_2 37 38 3651\n3 Fallow 20 20 1936\n4 Fallow_rough_plow 14 14 1366\n5 Fallow_smooth 27 27 2624\n6 Stubble 40 40 3879\n7 Celery 36 36 3507\n8 Grapes_untrained 113 113 11,045\n9 Soil_vinyard_develop 62 63 6078\n10 Corn_senesced_green_weeds 33 33 3212\n11 Lettuce_romaine_4wk 11 11 1046\n12 Lettuce_romaine_5wk 20 19 1888\n13 Lettuce_romaine_6wk 10 9 897\n14 Lettuce_romaine_7wk 11 11 1048\n15 Vinyard_untrained 73 73 7122\n16 Vinyard_vertical_trellis 18 19 1770\nTotal 546 546 53,037\nTable 3.  Number of training, validation, and test samples for PU dataset.\nNO Class Train Val Test\n1 Asphalt 67 66 6498\n2 Meadows 186 187 18,276\n3 Gravel 21 21 2057\n4 Trees 31 31 3002\n5 Sheets 13 14 1318\n6 Bare soils 51 50 4928\n7 Bitumen 14 13 1303\n8 Bricks 37 37 3608\n9 Shadows 9 10 928\nTotal 429 429 41,918\n9\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nIn order to quantitatively evaluate the classification performance of the model, we choose OA (overall accu-\nracy), AA (average accuracy), and kappa coefficient (κ) as the evaluation indicators of the model.\nParameters setting. We analyze some factors that affect the training and performance of the model, which \nare batch size, learning rate, number of head and input size. To be fair, each of our subsequent experiments was \nrepeated ten times, and the metrics used were the average of 10 experiments. We chose 10 different random \nseeds for 10 experiments to exclude variability due to random factors in the experiments.\n(1) Batch size: Batch size is important for model training, which affects the convergence of the model. We \nconsider the sets of {16, 32, 64} for experiments. The results are shown in the Fig. 8, we can see that choos-\ning the appropriate batch size for training is very important for the final performance of the model, so we \nchose to use 16 on KSC, 64 on SA, 64 on PU, and 32 on HU.\n(2) Learning rate: The learning rate affects the convergence speed of the model during training, and it plays an \nimportant role in the performance of the model. We choose a learning rate sets of {0.01, 0.001, 0.0001} for \nexperiments. As shown in the Fig. 9, choosing different learning rates to train the model has a great impact \non the final performance of the model. Based on the above results, we choose to use 0.001 on KSC, 0.001 \non SA, 0.01 on PU, and 0.001 on HU, respectively.\nTable 4.  Number of training, validation, and test samples for the HU dataset.\nNO Class Train Val Test\n1 Healthy grass 13 13 1225\n2 Stressed grass 13 13 1228\n3 Synthetic grass 7 7 683\n4 Trees 13 12 1219\n5 Soil 12 13 1217\n6 Water 3 4 318\n7 Residential 13 13 1242\n8 Commercial 13 12 1219\n9 Road 13 13 1226\n10 Highway 13 12 1202\n11 Railway 12 13 1210\n12 Parking lot1 13 12 1208\n13 Parking lot2 5 5 459\n14 Tennis court 4 5 419\n15 Running Track 7 7 646\nTotal 154 154 14,721\nFigure 8.  OA (%) of DBFFT with different batch size in the four datasets.\n10\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\n(3) Number of heads: Transformer’s multi-head self-attention can extract the global relationship between vec-\ntors in the sequence. Different heads can extract different relationships between vectors and other vectors. \nWe select a set of head numbers {4, 6, 8} to evaluate the effect of head count on the model. As shown in \nthe Fig. 10, different head counts affect the performance of the model. We use 4 on KSC, 4 on SA, 6 on PU, \nand 4 on HU respectively, according to the experimental results.\n(4) Input size: The input size determines the spatial information that the model can use for classification. To \nbetter evaluate the effect of size on the model, we choose a set of sizes {3, 5, 7, 9, 11}. As shown in the Fig. 11, \nas the size increases, the OA of the model continues to increase.  In the HU dataset, the OA of size 11×11 is \nlower than the OA of size 9×9, but its value is still higher than that of sizes 3×3, 5×5, and 7×7. This indicates \nthat the increase of spatial information can improve the information that can be mined by the model. We \nchoose the size of 11 × 11 as the input size of the model on the PU, KSC, SA datasets, and 9 × 9 as the input \nsize of the model on the HU dataset.\nFigure 9.  OA (%) of DBFFT with different learning rate in the four datasets.\nFigure 10.  OA (%) of DBFFT with different number of heads in the four datasets.\n11\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nComparison results of different methods. In this section, our proposed model is compared with the \ntraditional method MLP as well as five deep learning models, such as 1D-CNN 20, M3D-DCNN41,  pResNet22, \n SSRN21,  DBDA24,  SCFR25 and  DPSCN26. Among these methods, except for MLP and 1D-CNN, the neighbor -\nhood pixel patch is used as the input of the model. The hyperparameters (such as input size, learning rate) and \ntraining skills (such as early stopping, learning rate dynamic adjustment) of all the models are set according to \ntheir original paper to ensure fairness. We repeat each group of experiments in the four datasets 10 times with \nrandomly selected training samples to ensure the fairness of the experiments. Meanwhile, we will also report \nthe mean and standard deviation for all the metrics. Now, we briefly introduce the methods mentioned above \nin the following.\n(1) MLP: It is a multilayer perceptron that consists of two fully connected layers and a ReLU.\n(2) 1D-CNN: It consists of 1D convolutional layers and fully connected layers.\n(3) M3D-DCNN: This model extracts multi-scale information by combining multiple 3D convolution kernels \nof different sizes, and the size of the neighborhood pixel block is 7 × 7.\n(4) pResNet: This model is based on 2DCNN. By introducing a deep pyramid  network23, the depth of the model \nis improved to extract rich spectral and spatial information. The size of the neighborhood pixel block is \n11 × 11.\n(5) SSRN: This model consists of multiple spectral residual blocks and spatial residual blocks. The two residual \nblocks are based on ResNet and 3DCNN. The size of the neighborhood pixel block is 7 × 7.\n(6) DBDA: A 3DCNN-based Double-Branch model, each branch consists of DenseNet and attention mecha-\nnism. The size of the neighborhood pixel block is 9 × 9.\n(7) SCFR: This model is completely composed of 1 × 1 convolutions except that the first layer is composed of \n3 × 3 convolution. The size of the neighborhood pixel block is 7 × 7.\n(8) DPSCN: This model is constructed by the dual-path small convolution (DPSC) module. DPSC module \nconsists of 1 × 1 convolution and with a residual path and a density path. The size of the neighborhood pixel \nblock is 9 × 9.\nThe classification results of different models on the four datasets are shown in Tables  5, 6, 7 and 8, and the \nbest classification results are shown in bold. It can be seen that the performance of our proposed model is the \nbest on all four datasets. MLP and 1D-CNN, which only utilize the spectral information of HSI, have the lowest \nperformance on all four datasets. The accuracy of the model using spatial information is higher than the MLP and \n1D-CNN, which proves the importance of spatial information for HSI classification. It is worth noting that the \nperformance of M3D-DCNN is much lower than pResNet, SSRN, DBDA, and DBFFT on the Four datasets. The \nreason is that the depth of M3D-DCNN is shallow and it is difficult to extract deep features of HSI. Furthermore, \nin the case of small samples, M3D-DCNN overfits the training data. The pResNet model performs poorly on \nPU, KSC, and HU, and its OA on PU, KSC and HU is 4.23%, 2.76%,8.81% lower than DBFFT, respectively. The \nreason is that pResNet stacks a large number of convolution kernels, which leads to too many training parameters \nof the model, resulting in overfitting of the model in the case of a small sample. In addition, the over-reliance of \nthe 2DCNN-based model on the spatial features of HSI also leads to the poor performance of the model. SCFR \nand DPSCN are mainly composed of 1 × 1 convolutions, and these two models utilize a small amount of 3 × 3 \nconvolutions to extract spatial information. SCFR performed poorly on all four datasets, suggesting that SCFR did \nnot extract enough spatial features. The performance of DPSCN on PU is close to DBFFT, and OA is only 0.08% \nlower than DBFFT, but on KSC, SA, and HU, OA is 2.28%, 4.9%, and 2.2% lower than DBFFT, respectively. This \nindicates the poor generalization ability of DPSCN. Both SSRN and DBDA are 3D-CNN based models, but their \nperformance on all four datasets is much lower than that of our proposed model. DBDA, which is the same as our \nFigure 11.  OA (%) of DBFFT with different input size in the four datasets.\n12\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nproposed model, is also a Double-Branch structure, but the OA on KSC, SA, PU, and HU is 1.35%, 1.01%, 0.16%, \n0.9% lower than DBFFT, respectively. This illustrates the importance of global features for HSI classification. \nOur model is not only optimal on OA, but also on AA and κ, which proves that our model has better stability.\nFigures 12, 13, 14 and 15 show the original false-color image of the HSI, the ground truth map, the classifica-\ntion maps of DBFFT, and all the compared methods. We can find that there is a lot of salt and pepper noise on \nthe classification maps of MLP and 1DCNN that only use spectral information for classification. The classifica-\ntion map of the CNN-Based model based on spectral and spatial information and the classification map of our \nproposed model are more smooth. However, M3D-DCNN has worse classification results than pResNet, SSRN, \nDBDA, SCFR, DPSCN, and DBFFT due to its severe overfitting. Our proposed model extracts global spectral \nfeatures and global spatial features by introducing a self-attention mechanism, and fuses spectral and spatial \nfeatures through a feature fusion layer to obtain a very smooth and ideal classification map. Compared with all \nTable 5.  Classification results of 5% samples of KSC dataset. Significant values are in bold.\nClass MLP 1D-CNN\nM3D-\nDCNN SSRN pResNet DBDA SCFR DPSCN Proposed\n1 91.39 91.14 96.74 99.33 99.63 99.97 98.42 99.66 99.97\n2 82.75 85.23 82.57 98.53 95.64 98.21 90.09 96.24 98.72\n3 84.48 88.48 77.17 97.65 88.13 87.04 89.65 96.00 95.52\n4 50.18 58.58 46.42 90.22 72.04 86.46 58.67 86.15 95.62\n5 45.14 54.03 45.90 86.18 78.06 78.33 64.17 81.18 85.42\n6 44.17 46.65 64.27 96.50 86.80 95.68 79.56 87.48 97.62\n7 78.30 71.17 76.49 91.49 92.87 89.47 85.43 93.72 91.49\n8 83.15 85.48 81.09 99.30 97.47 99.30 95.06 98.84 99.90\n9 91.24 93.72 93.08 99.51 99.70 100.0 98.42 90.00 100.0\n10 87.16 88.65 85.76 100.0 98.04 99.97 97.85 99.45 100.0\n11 94.96 93.87 99.05 99.39 98.89 98.17 99.02 99.23 98.81\n12 86.02 88.81 91.88 99.47 99.49 99.38 92.72 98.98 99.65\n13 99.96 99.80 100.0 100.0 100.0 100.0 99.72 100.0 100.0\nOA(%) 85.17 ± 0.92 86.80 ± 0.87 87.08 ± 1.46 98.29 ± 0.59 95.88 ± 0.60 97.29 ± 1.44 92.95 ± 0.80 96.36 ± 3.16 98.64 ± 0.40\nAA(%) 78.38 ± 1.31 80.43 ± 1.18 80.03 ± 2.29 96.74 ± 1.31 92.83 ± 1.24 94.77 ± 2.60 88.37 ± 1.41 94.38 ± 2.88 97.13 ± 0.87\nκ × 100 83.48 ± 1.03 85.29 ± 0.97 85.59 ± 1.63 98.10 ± 0.66 95.42 ± 0.67 96.98 ± 1.60 92.15 ± 0.89 95.94 ± 3.52 98.49 ± 0.44\nTable 6.  Classification results of 1% samples of SA dataset. Significant values are in bold.\nClass MLP 1D-CNN\nM3D-\nDCNN SSRN pResNet DBDA SCFR DPSCN Proposed\n1 96.45 94.14 98.26 99.97 98.10 100.0 94.01 98.92 99.96\n2 98.73 98.88 99.74 99.91 99.73 100.0 98.29 98.95 100.0\n3 94.98 95.90 99.47 98.97 99.27 98.92 93.99 100.0 99.44\n4 99.53 99.03 99.17 99.88 99.48 99.36 78.97 99.84 99.73\n5 96.27 96.79 94.76 97.98 98.28 96.00 98.62 97.76 99.08\n6 99.78 99.64 99.54 100.0 99.99 100.0 100.0 100.0 100.0\n7 99.42 99.46 99.16 99.99 99.54 99.93 99.22 99.92 99.98\n8 80.08 84.45 83.31 94.02 92.75 95.73 84.77 92.70 96.48\n9 99.37 99.11 98.86 99.89 99.54 100.0 99.83 80.00 100.0\n10 86.26 88.04 90.38 96.77 96.07 96.75 86.86 96.41 97.36\n11 90.61 92.93 95.69 99.21 96.91 99.53 81.00 99.00 99.80\n12 98.83 99.25 99.73 99.98 99.47 99.94 99.22 80.00 99.94\n13 97.06 96.35 98.29 99.43 99.71 99.05 99.45 99.65 99.89\n14 91.80 92.82 95.59 97.73 99.49 99.67 98.35 89.48 99.60\n15 57.42 58.74 67.79 90.51 93.16 91.80 80.63 92.64 96.35\n16 92.15 91.73 88.19 97.81 96.48 99.14 93.25 98.13 99.31\nOA(%) 87.88 ± 0.56 89.06 ± 0.57 90.41 ± 1.35 96.98 ± 0.42 96.82 ± 0.51 97.49 ± 0.78 91.52 ± 1.82 93.74 ± 5.24 98.50 ± 0.41\nAA(%) 92.42 ± 0.41 92.95 ± 0.58 94.25 ± 0.83 98.25 ± 0.33 98.00 ± 0.42 98.49 ± 0.48 92.90 ± 3.13 95.21 ± 4.85 99.18 ± 0.21\nκ × 100 86.48 ± 0.63 87.80 ± 0.64 89.31 ± 1.51 96.64 ± 0.47 96.46 ± 0.57 97.21 ± 0.87 90.56 ± 2.04 93.06 ± 5.78 98.33 ± 0.45\n13\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nTable 7.  Classification results of 1% samples of PU dataset. Significant values are in bold.\nClass MLP 1D-CNN\nM3D-\nDCNN SSRN pResNet DBDA SCFR DPSCN Proposed\n1 85.56 92.00 94.43 98.68 93.38 98.81 95.72 98.97 99.17\n2 95.31 96.23 97.33 99.66 99.32 99.84 98.57 99.85 99.82\n3 60.24 75.85 70.20 87.37 65.62 89.81 84.64 96.07 93.91\n4 81.44 89.49 93.53 97.27 95.59 96.09 93.75 97.46 95.53\n5 99.59 99.83 98.10 99.99 98.86 99.80 89.80 99.98 99.79\n6 75.57 86.87 83.31 99.01 95.79 99.76 94.26 98.70 99.74\n7 73.45 77.67 78.91 97.78 80.04 99.03 88.68 96.26 97.71\n8 77.46 79.55 87.73 95.50 88.76 97.12 94.95 97.37 97.41\n9 99.57 99.79 96.65 99.89 98.73 97.74 99.47 89.77 96.31\nOA(%) 86.78 ± 1.07 91.17 ± 0.47 92.24 ± 0.98 98.26 ± 0.23 94.53 ± 0.74 98.60 ± 0.33 95.72 ± 0.89 98.68 ± 0.63 98.76 ± 0.29\nAA(%) 83.13 ± 1.59 88.59 ± 0.71 88.91 ± 1.37 97.24 ± 0.32 90.68 ± 1.26 97.56 ± 0.67 93.32 ± 2.87 97.97 ± 3.19 97.71 ± 0.62\nκ × 100 82.33 ± 1.45 88.27 ± 0.61 89.66 ± 1.33 97.69 ± 0.30 92.73 ± 0.98 98.15 ± 0.44 94.31 ± 1.19 98.240.84 ± 98.36 ± 0.39\nother models, our classification map generates the least noise on the four datasets, and the classification map is \nthe most accurate and smooth.\nFigure 16 shows a part of the SA classification map, and we can see that in the case of small training set sam-\nples, class 8 and class 15 are extremely prone to misclassification on both our proposed model and the comparison \nmodel. MLP , 1D-CNN and M3D-DCNN misclassify a lot of these two classes. Our proposed model has the least \nnumber of misclassifications on class 8 and class 15 compared to other models, which is the performance of our \nproposed model in the face of overfitting.\nTable 9 reports the training time and test time of the proposed model and 5 models with similar performance. \nIt can be seen that our model outperforms DBDA and SSRN in both training time. On the SA dataset, the train-\ning time of SSRN is 3 times that of ours, and the training time of DBDA is 2 times that of us. Compared with \nDPSCN and SCFR, our model requires more training time and testing time, but DPSCN and SCFR can only \nachieve similar performance to our proposed model on some datasets, and perform poorly on other datasets. For \nexample on the SA dataset, the OA of DPSCN and SC-FR is 4.76% and 6.98% lower than our proposed model, \nrespectively. We thought it was worth the extra time to get better performance.\nInvestigation of training sample. The excellent performance of deep learning methods relies on a large \nnumber of labeled datasets, but it is usually difficult to obtain enough labeled data for HSI. Therefore, we test the \nperformance of our proposed model and all compared models under different numbers of training set samples. \nTable 8.  Classification results of 1% samples of HU dataset. Significant values are in bold.\nClass MLP 1D-CNN\nM3D-\nDCNN SSRN pResNet DBDA SC-FR DPSCN Proposed\n1 92.74 93.74 90.40 95.98 91.20 93.19 90.35 94.52 93.19\n2 87.24 88.15 80.04 95.33 92.39 92.98 89.24 90.91 95.98\n3 97.28 98.13 85.07 99.78 91.67 99.30 83.57 99.68 98.77\n4 91.79 89.65 85.91 95.18 92.36 95.41 91.55 95.65 96.20\n5 96.42 94.85 91.50 99.35 94.76 99.65 98.81 99.51 98.72\n6 84.65 83.52 41.35 77.61 51.01 84.40 80.57 84.40 82.04\n7 73.41 75.91 64.54 88.05 75.97 89.04 76.90 84.75 88.91\n8 61.81 52.99 57.38 68.70 65.31 71.20 63.15 68.56 72.03\n9 62.75 66.38 61.66 81.94 74.79 86.43 74.43 81.64 84.28\n10 56.02 54.58 54.46 87.05 77.59 90.82 75.57 90.26 92.06\n11 61.79 65.74 59.06 82.22 72.33 82.92 68.61 81.35 83.47\n12 56.38 53.42 54.72 78.49 76.99 79.41 72.01 80.50 85.69\n13 12.16 13.16 35.38 79.54 71.70 80.54 78.76 84.47 88.50\n14 87.37 80.31 52.89 98.97 86.80 100.0 98.47 89.86 98.88\n15 97.99 97.65 90.70 99.91 86.25 99.97 99.74 99.30 99.47\nOA(%) 74.89 ± 1.96 74.32 ± 1.40 69.43 ± 5.50 88.27 ± 1.61 81.27 ± 2.28 89.18 ± 2.55 81.60 ± 2.56 87.88 ± 1.57 90.08 ± 1.43\nAA(%) 74.65 ± 1.76 73.88 ± 1.37 67.00 ± 6.38 88.54 ± 2.03 80.08 ± 2.70 89.68 ± 2.01 82.78 ± 2.65 88.36 ± 2.06 90.55 ± 1.14\nκ × 100 72.83 ± 2.12 72.21 ± 1.52 66.89 ± 5.97 87.32 ± 1.75 79.74 ± 2.47 88.30 ± 2.75 80.10 ± 2.77 86.90 ± 1.70 89.27 ± 1.54\n14\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nFor KSC, we take 1%, 3%, 5%, 10%, and 20% of labeled pixels as training samples. For PU, we choose 0.8%, \n1%, 5%, 10%, and 20% of labeled pixels as training samples. For SA, we consider 0.5%, 1%, 3%, 5%, and 10% of \nlabeled pixels as training samples. For HU, we consider 0.5%, 1%, 5%, 15%, and 20% of labeled pixels as train-\ning samples. As shown in Fig.  17, as the training samples increase, the OA of all models also increases. In the \ncase of large training samples, all performances of SSRN, DBDA, pResNet and our proposed model are close to \nperfect. But when the training samples are reduced, our proposed model consistently outperforms other models. \nIt should be mentioned that our proposed model has the highest accuracy on all sample proportions of SA, and \nit only performs suboptimally at 20% sample proportion on PU and KSC datasets. Considering the difficulty of \nsample acquisition of HSI, our proposed model is more suitable for the actual situation.\nEffect of label smooth. To verify the effect of label smooth on model training, we retrain the models with \nlabel smooth removed and compare their performance. The results are shown in Fig.  18. On the four datasets, \nthe performance of the model will be improved by adding label smooth during training. It proves that the model \ncombined with label smooth has stronger generalization ability.\nEffect of feature fusion layer. In this section, we will compare the performance of the proposed model \nwith that model not having feature fusion layer. The results are shown in Fig. 19. We can see that feature fusion \nFigure 12.  Classification maps of different models on the KSC dataset. (a) False-color image (b) Ground-truth \nmap. (c) MLP . (d) 1D-CNN. (e) M3D-DCNN. (f) SSRN. (g) pResNet. (h) DBDA. (i) SCFR. (j) DPSCN. (k) \nDBFFT.\n15\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nsignificantly improves the performance of the model on all four datasets, which proves that feature fusion layer \nimproves the performance of the model by fusing the spectral and spatial features of HSI.\nEffect of attention mechanism. We verify the effectiveness of the attention mechanism by removing \nthe spectral attention module, spatial attention module, and removing both attention modules from the model \nrespectively. The experimental results are shown in Fig.  20. We can see that the performance of the model on \nall four datasets decreases significantly when both modules are removed, and the performance of the model is \nreduced by 0.91%, 1.04%, 1.26%, and 3.61% on KSC, PU, SA, and HU, respectively. After only removing the \nspatial attention module, the performance of the model is reduced by 0.88%, 0.95%, 1.2%, and 3.44% on KSC, \nPU, SA, and HU, respectively. It is revealing that the spatial attention module plays a major role in improving the \nperformance of the model. When we remove the spectral attention module, the results show that it has a certain \nbut non-significant impact on the performance of the model. Therefore, we can conclude that the model can \nimprove the performance of the model after adding the attention mechanism.\nConclusion\nIn this paper, we propose a Double-Branch feature fusion Transformer (DBFFT) model for HSI classification. \nThe proposed model can overcome the shortcomings of CNN-based models, which are not good at learning the \nlong-distance dependency relations of spectral bands and extracting global spatial features of HSI. We firstly \npresent two attention mechanism modules to extract spectral and spatial features separately. According to the \ncharacteristics of HSI, we adopt Pixel-wise embedding and Band-wise embedding on the spectral branch and \nspatial branch to process the feature maps to better utilize the self-attention mechanism to extract the global \nFigure 13.  Classification maps of different models on the SA dataset. (a) False-color image. (b) Ground-truth \nmap. (c) MLP . (d) 1D-CNN. (e) M3D-DCNN. (f) SSRN. (g) pResNet. (h) DBDA. (i) SCFR. (j) DPSCN. (k) \nDBFFT.\n16\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nFigure 14.  Classification maps of different models on the PU dataset. (a) False-color image. (b) Ground-truth \nmap. (c) MLP . (d) 1D-CNN. (e) M3D-DCNN. (f) SSRN. (g) pResNet. (h) DBDA. (i) SCFR. (j) DPSCN. (k) \nDBFFT.\n17\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nFigure 15.  Classification maps of different models on the HU dataset. (a) False-color image. (b) Ground-truth \nmap. (c) MLP . (d) 1D-CNN. (e) M3D-DCNN. (f) SSRN. (g) pResNet. (h) DBDA. (i) SCFR. (j) DPSCN. (k) \nDBFFT.\n18\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nFigure 16.  Part of the classification map for different models on the SA dataset. (a) Ground-truth map. (b) \nMLP . (c)1D-CNN. (d) M3D-DCNN. (e) SSRN. (f) pResNet. (g) DBDA. (h) SCFR. (i) DPSCN. (j) DBFFT.\nTable 9.  Training time, and test time for different models on the four data sets.\nSSRN pResNet DBDA SC-FR DPSCN Proposed\nPU\nTraining time (s) 194.83 61.52 170.67 8.60 37.74 117.35\nTest time (s) 10.41 7.84 21.78 3.14 4.65 13.09\nKSC\nTraining time (s) 218.54 38.85 199.82 5.81 26.11 187.97\nTest time (s) 1.59 0.93 3.46 0.41 0.52 3.23\nSA\nTraining time (s) 566.60 82.41 342.82 10.92 50.51 165.67\nTest time (s) 19.73 12.68 44.84 5.10 7.18 21.61\nHU\nTraining time (s) 98.39 24.60 103.03 3.79 15.53 62.09\nTest time (s) 4.37 2.70 9.16 0.99 1.57 5.71\nFigure 17.  OA (%) of DBFFT with different number of training samples in the four datasets. (a) KSC dataset. \n(b) SA dataset. (c) PU dataset. (d) HU dataset.\n19\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nspatial and global spectral features of HSI. Then, we design a feature fusion layer to fuse the spectral and spatial \nfeatures of the two branches. In view of the limited number of training samples of HSI, our model can outperform \nthe CNN-based model in the case of small samples. In addition, we also employ the label smooth technique to \nimprove the generalization ability of the model in small sample scenarios.\nIn the future, we will do more works to improve the proposed model to achieve more effectiveness and \nperformance. The first work is to improve the structure of the proposed model to enhance its ability to extract \nglobal features and generalization. Another is to improve the fusion ability of the spectral and spatial features \nFigure 18.  The effect of label smooth on the performance of the model.\nFigure 19.  The effect of feature fusion on the performance of the model.\n20\nVol:.(1234567890)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\nwith a more effective feature fusion layer. Finally, more hyperspectral image datasets could be considered, not \njust these few public datasets.\nData availability\nThe data that support the findings of this study are available from the Grupo de Inteligencia Computacional \n(GIC) website (http:// www. ehu. eus/ ccwin tco/ index. php/ Hyper spect ral_ Remote_ Sensi ng_ Scenes).\nReceived: 13 July 2022; Accepted: 3 January 2023\nReferences\n 1. Landgrebe, D. Hyperspectral image data analysis. IEEE Signal Process. Mag. 19(1), 17–28. https:// doi. org/ 10. 1109/ 79. 974718 (2002).\n 2. Fauvel, M., Tarabalka, Y ., Benediktsson, J. A., Chanussot, J. & Tilton, J. C. Advances in spectral-spatial classification of hyperspectral \nimages. Proc. IEEE 101(3), 652–675. https:// doi. org/ 10. 1109/ JPROC. 2012. 21975 89 (2013).\n 3. Li, J., Marpu, P . R., Plaza, A., Bioucas-Dias, J. M. & Benediktsson, J. A. Generalized composite kernel framework for hyperspectral \nimage classification. IEEE Trans. Geosci. Remote Sens. 51(9), 4816–4829. https:// doi. org/ 10. 1109/ TGRS. 2012. 22302 68 (2013).\n 4. Ibrahim, A. et al. Atmospheric correction for hyperspectral ocean color retrieval with application to the Hyperspectral Imager for \nthe Coastal Ocean (HICO). Remote Sens. Environ. 204, 60–75 (2018).\n 5. Mahesh, S., Jayas, D., Paliwal, J. & White, N. Hyperspectral imaging to classify and monitor quality of agricultural materials. J. \nStored Prod. Res. 61, 17–26 (2015).\n 6. Haboudane, D., Miller, J. R., Pattey, E., Zarco-Tejada, P . J. & Strachan, I. B. Hyperspectral vegetation indices and novel algorithms \nfor predicting green LAI of crop canopies: Modeling and validation in the context of precision agriculture. Remote Sens. Environ. \n90(3), 337–352 (2004).\n 7. Manjunath, K., Ray, S. & Vyas, D. Identification of indices for accurate estimation of anthocyanin and carotenoids in different \nspecies of flowers using hyperspectral data. Remote Sens. Lett. 7(10), 1004–1013 (2016).\n 8. Han, Y ., Li, J., Zhang, Y ., Hong, Z. & Wang, J. Sea ice detection based on an improved similarity measurement method using \nhyperspectral data. Sensors 17(5), 1124 (2017).\n 9. Paoletti, M. E., Haut, J. M., Plaza, J. & Plaza, A. Deep learning classifiers for hyperspectral imaging: A review. ISPRS J. Photogramm. \nRemote Sens. 158, 279–317 (2019).\n 10. Fauvel, M., Benediktsson, J. A., Chanussot, J. & Sveinsson, J. R. Spectral and spatial classification of hyperspectral data using SVMs \nand morphological profiles. IEEE Trans. Geosci. Remote Sens. 46(11), 3804–3814. https:// doi. org/ 10. 1109/ TGRS. 2008. 922034 \n(2008).\n 11. Hongwei, Z. & Basir, O. An adaptive fuzzy evidential nearest neighbor formulation for classifying remote sensing images. IEEE \nTrans. Geosci. Remote Sens. 43(8), 1874–1889. https:// doi. org/ 10. 1109/ TGRS. 2005. 848706 (2005).\n 12. Collobert, R. & Bengio, S. Links between perceptrons, MLPs and SVMs. Proc. ICML https:// doi. org/ 10. 1145/ 10153 30. 10154 15 \n(2004).\n 13. Benediktsson, J. A., Palmason, J. A. & Sveinsson, J. R. Classification of hyperspectral data from urban areas based on extended \nmorphological profiles,\". IEEE Trans. Geosci. Remote Sens. 43(3), 480–491. https:// doi. org/ 10. 1109/ TGRS. 2004. 842478 (2005).\n 14. Li, W . & Du, Q. Gabor-filtering-based nearest regularized subspace for hyperspectral image classification. IEEE J. Select Topics \nAppl. Earth Observ. Remote Sens. 7(4), 1012–1022 (2014).\n 15. Okan, A., Özdemir, B., Gedik, B.E., Y asemin, C. & Çetin, Y . Hyperspectral classification using stacked autoencoders with deep \nlearning. In Proc.WHISPERS. 1–4 (2014).\n 16. Zhou, F ., Hang, R., Liu, Q. & Yuan, X. HSI classification using spectral-spatial LSTMs. Neurocomputing 328, 39–47. https:// doi. \norg/ 10. 1016/j. neucom. 2018. 02. 105 (2019).\n 17. Hang, R., Liu, Q., Hong, D. & Ghamisi, P . Cascaded recurrent neural networks for hyperspectral image classification. IEEE Trans. \nGeosci. Remote Sens. 57(8), 5384–5394. https:// doi. org/ 10. 1109/ TGRS. 2019. 28991 29 (2019).\n 18. Larochelle, H. & Bengio, Y . Classification using discriminative restricted boltzmann machines. In Proc. ICML. 536–543 (2008).\n 19. Hong, D. et al. SpectralFormer: Rethinking hyperspectral image classification with transformers. IEEE Trans. Geosci. Remote Sens. \n60, 1–15. https:// doi. org/ 10. 1109/ TGRS. 2021. 31307 16 (2022).\nFigure 20.  The effect of attention mechanism on the performance of the model.\n21\nVol.:(0123456789)Scientific Reports |          (2023) 13:272  | https://doi.org/10.1038/s41598-023-27472-z\nwww.nature.com/scientificreports/\n 20. Wei, Hu., Huang, Y ., Wei, Li., Zhang, F . & Li, H. Deep convolutional neural networks for hyperspectral image classification. J. Sens. \nhttps:// doi. org/ 10. 1155/ 2015/ 25861 9(2015) (2015).\n 21. Zhong, Z., Li, J., Luo, Z. & Chapman, M. Spectral-spatial residual network for hyperspectral image classification: A 3-D deep \nlearning framework. IEEE Trans. Geosci. Remote Sens. 56(2), 847–858. https:// doi. org/ 10. 1109/ TGRS. 2017. 27555 42 (2018).\n 22. Paoletti, M. E. et al. Deep pyramidal residual networks for spectral-spatial hyperspectral image classification. IEEE Trans. Geosci. \nRemote Sens. 57(2), 740–754. https:// doi. org/ 10. 1109/ TGRS. 2018. 28601 25 (2019).\n 23. Dongyoon, H., Kim, J., & Kim, J. Deep pyramidal residual networks. In Proc. CVPR. 5927–5935 (2017).\n 24. Rui, L., Zheng, S., Duan, C., Y ang, Y . & Wang, X. Classification of hyperspectral image based on double-branch dual-attention \nmechanism network. Remote Sens. 12(3), 582. https:// doi. org/ 10. 3390/ rs120 30582 (2020).\n 25. Gao, H. et al. Convolutional neural network for spectral-spatial classification of hyperspectral images. Neural Comput. 31(8997), \n9012. https:// doi. org/ 10. 1007/ s00521- 019- 04371-x (2019).\n 26. Dang, L., Pang, P ., Zuo, X., Liu, Y . & Lee, J. A dual-path small convolution network for hyperspectral image classification. Remote \nSens. 13(17), 3411. https:// doi. org/ 10. 3390/ rs131 73411 (2021).\n 27. Chang, Y .-L. et al. Consolidated convolutional neural network for hyperspectral image classification. Remote Sens.  14(7), 1571. \nhttps:// doi. org/ 10. 3390/ rs140 71571 (2022).\n 28. Shi, H. et al. H2A2Net: A hybrid convolution and hybrid resolution network with double attention for hyperspectral image clas -\nsification. Remote Sensing. 14(17), 4235. https:// doi. org/ 10. 3390/ rs141 74235 (2022).\n 29. He, X., Chen, Y . & Lin, Z. Spatial-spectral transformer for hyperspectral image classification. Remote Sens. 13(3), 498 (2021).\n 30. Vaswani, A. et al. Attention is all you need. arXiv preprint arXiv: 1706. 03762 (2017).\n 31. He, J., Zhao, L., Y ang, H., Zhang, M. & Li, W . HSI-BERT: Hyperspectral image classification using the bidirectional encoder \nrepresentation from transformers. IEEE Trans. Geosci. Remote Sens. 58(1), 165–178. https://  doi. org/ 10. 1109/ TGRS. 2019. 29347 \n60 (2020).\n 32. Dosovitskiy, A. et al. An image is worth 16×16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.  \n11929 (2020).\n 33. Yuan, K., Guo, S., Liu, Z., Zhou, A., Yu F ., & Wu, W . Incorporating convolution designs into visual transformers. In Proc. ICCV. \n579–588 (2021).\n 34. Chen C. F . R., Fan, Q. & Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proc. ICCV . \n357–366 (2021).\n 35. Hu J., Shen, L. & Sun, G. Squeeze-and-excitation networks. In Proc. CVPR. 7132–7141. (2018).\n 36. Zhu, M., Jiao, L., Liu, F ., Y ang, S. & Wang, J. Residual spectral-spatial attention network for hyperspectral image classification. \nIEEE Trans. Geosci. Remote Sens. 59(1), 449–462. https:// doi. org/ 10. 1109/ TGRS. 2020. 29940 57 (2021).\n 37. Sanghyun, W ., Park, J., & Lee, J.-Y . CBAM: Convolutional block attention module. In Proc. ECCV . 3–19 (2018).\n 38. Kayhan O. S. & Gemert, J. C. V . On translation invariance in CNNs: Convolutional layers can exploit absolute spatial location. In \nProc. CVPR. 14274–14285 (2020).\n 39. Acito, N., Matteoli, S., Rossi, A., Diani, M. & Corsini, G. Hyperspectral airborne “Viareggio 2013 Trial” data collection for detec-\ntion algorithm assessment. IEEE J. Select. Topics Appl. Earth Observ. Remote Sens. 9(6), 2365–2376 (2016).\n 40. Donoho, D. L. High-dimensional data analysis: The curses and blessings of dimensionality. AMS Math Chall. Lect. 1, 32 (2000).\n 41. He, M., Li, B. & Chen, H. Multi-scale 3D deep convolutional neural network for hyperspectral image classification. Proc. ICIP  \nhttps:// doi. org/ 10. 1109/ ICIP . 2017. 82970 14 (2017).\nAuthor contributions\nConceptualization, L.D. and L.W .; methodology, L.D., L.W . and Y .H.; software, L.W .; validation, L.W .; formal \nanalysis, Y .L.; investigation, L.W .; writing—original draft preparation, L.W .; writing—review and editing, X. Z., \nY .L. and L.D; visualization, L.W .; funding acquisition, X. Z. and Y .L. All authors have read and agreed to the \npublished version of the manuscript.\nFunding\nThis research was funded by National Natural Science Foundation of China, grant number 62176087; Shenzhen \nScience and Technology Innovation Commission (SZSTI)-Shenzhen Virtual University Park (SZVUP) Spe -\ncial Fund Project (2021Szvup032); National Key Research and Development Program of China, grant number \n2019YFE0126600; Major Project of Science and Technology of Henan Province, grant number 201400210300.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to Y .H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}