{
  "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
  "url": "https://openalex.org/W4389523900",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5017609232",
      "name": "Ruixuan Xiao",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2128972220",
      "name": "Yiwen Dong",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099242985",
      "name": "Junbo Zhao",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2145193382",
      "name": "Runze Wu",
      "affiliations": [
        "NetEase (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2243664479",
      "name": "Minmin Lin",
      "affiliations": [
        "NetEase (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100962162",
      "name": "Gang Chen",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2192582118",
      "name": "Wang Haobo",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3039366696",
    "https://openalex.org/W3101345273",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2978426779",
    "https://openalex.org/W2774918944",
    "https://openalex.org/W4287854750",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W3001197829",
    "https://openalex.org/W3101889167",
    "https://openalex.org/W4385567008",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3156031277",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4228998172",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W4205737716",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4312729595",
    "https://openalex.org/W4206648492",
    "https://openalex.org/W2171671120",
    "https://openalex.org/W4281790610",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2970043232",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W2098742124",
    "https://openalex.org/W2996108195",
    "https://openalex.org/W4385567101",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W4281492306",
    "https://openalex.org/W2948367246",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W2963735582",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W4310513546",
    "https://openalex.org/W2981873476",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14520–14535\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nFreeAL: Towards Human-Free Active Learning in the Era of Large\nLanguage Models\nRuixuan Xiao1, Yiwen Dong1, Junbo Zhao1, Runze Wu2\nMinmin Lin2, Gang Chen1, Haobo Wang1∗\n1Zhejiang University, Hangzhou, China\n2NetEase Fuxi AI Lab, Hangzhou, China\n{xiaoruixuan,dyw424,j.zhao,cg,wanghaobo}@zju.edu.cn\n{wurunze1,linminmin01}@corp.netease.com\nAbstract\nCollecting high-quality labeled data for model\ntraining is notoriously time-consuming and\nlabor-intensive for various NLP tasks. While\ncopious solutions, such as active learning for\nsmall language models (SLMs) and prevalent\nin-context learning in the era of large language\nmodels (LLMs), have been proposed and allevi-\nate the labeling burden to some extent, their\nperformances are still subject to human in-\ntervention. It is still underexplored how to\nreduce the annotation cost in the LLMs era.\nTo bridge this, we revolutionize traditional ac-\ntive learning and propose an innovative col-\nlaborative learning framework FreeAL to in-\nteractively distill and filter the task-specific\nknowledge from LLMs. During collaborative\ntraining, an LLM serves as an active annota-\ntor inculcating its coarse-grained knowledge,\nwhile a downstream SLM is incurred as a stu-\ndent to filter out high-quality in-context sam-\nples to feedback LLM for the subsequent la-\nbel refinery. Extensive experiments on eight\nbenchmark datasets demonstrate that FreeAL\nlargely enhances the zero-shot performances\nfor both SLM and LLM without any human\nsupervision. The code is available at https:\n//github.com/Justherozen/FreeAL.\n1 Introduction\nModern machine learning models typically require\na huge collection of precisely labeled data, which\ncan be a labor-intensive and time-consuming pro-\ncess. Even worse, it can be unrealistic in some\npractical scenarios that demand much expertise,\nsuch as medical diagnosis and industrial applica-\ntions. To this end, a plethora of approaches have\nbeen investigated to reduce the burden of anno-\ntation, including semi-supervised learning (Sohn\net al., 2020; Berthelot et al., 2019), learning with\nlabel noise (Han et al., 2018; Li et al., 2020), and so\non. Amongst them, active learning (Ein-Dor et al.,\n∗Corresponding author.\nFigure 1: Comparisons of FreeAL with traditional active\nlearning (AL) algorithms and supervised fine-tuning on\nthe SST-2 dataset. FreeAL surpasses all the active learn-\ning rivals and achieves near-supervised performance\nwithout human annotation.\n2020; Yuan et al., 2020; Margatina et al., 2021)\nis a prominent solution that interactively queries\nan external expert or oracle to mark the new data\npoints that the model wants to learn from. These\nmethods alleviate the labeling burden to some ex-\ntent but still require human efforts in the annotation\nor construction of the oracle to start with.\nThe recent prevalent large language models\n(LLMs) (Ouyang et al., 2022; Thoppilan et al.,\n2022; OpenAI, 2023), such as ChatGPT and PaLM\n(Chowdhery et al., 2022), have exhibited strong\nzero-shot learning ability by proper prompt design,\nyet becoming a new remedy for data efficiency.\nEven more inspiringly, LLMs emerge with the\nso-called in-context learning (ICL) (Brown et al.,\n2020) ability to learn from a few task-related la-\nbeled samples for boosted performance. Despite\nthe promise, some studies (Bang et al., 2023) find\nthat LLMs tend to underperform compared to fine-\ntuned small language models (SLMs) on challeng-\ning tasks, which is also verified in our empirical\n14520\nstudies (Table 3). One possible reason is that ICL\ncan not fully exploit supervised training samples\ndue to limited context length. Moreover, their ex-\ntremely large size and limited accessibility also\nhinder their training and generalization on specific\ntasks. To date, it is still questionable how can we\ngeneralize to downstream tasks with the least hu-\nman annotation in the era of LLMs.\nIn this work, we present a novel collaborative\nlearning paradigm FreeAL that revolutionizes tra-\nditional active learning by interactively distilling\nand filtering the task-related knowledge from the\nLLMs. Our intuition is that, while LLMs are hard\nto fine-tune, they are competent zero-shot learn-\ners (Wei et al., 2022; Kojima et al., 2022) and can\nprovide coarse-grained knowledge for downstream\ntasks. On the other hand, SLMs are effective weak\nlearners (Li et al., 2020) that can distill valuable\nclean samples from noisy supervision. To integrate\nLLMs and SLMs synergistically as a whole, we\ndesign a collaborative training framework where\nLLM operates as an active annotator infusing its\nknowledge and the SLM acts as a student to filter\nout the high-quality input-label pairs to feed back\nthe LLM for subsequent label refinery. Empirically,\nFreeAL iteratively boosts the unsupervised perfor-\nmance of both SLMs and LLMs during collabo-\nrative training for transductive and inductive set-\ntings. As depicted in Figure 1, FreeAL allows us to\nachieve an extraordinary annotation-performance\ntrade-off by obtaining competitive results on par\nwith the supervised counterparts while fully elimi-\nnating human annotation costs.\nOverall, our main contributions can be summa-\nrized as follows,\n• To the best of our knowledge, we are among\nthe first to overhaul traditional active learning\nin the era of LLMs for boosted generalization\nperformance without any human supervision.\n• We propose a novel collaborative learning\nframework called FreeAL to employ the\nLLMs as active annotators and the SLMs as\nweak filters to interactively distill the task-\nrelated knowledge from the LLMs.\n• Our proposed FreeAL largely improves the\nunsupervised learning performance for both\nthe LLMs and the SLMs, even approaching\nthe supervised counterparts in some scenarios.\nOur results prove the feasibility of human-free\nactive labeling in the era of LLMs.\n2 Related Work\n2.1 Prompt-based Zero/Few-shot Learning\nThe emergent ability of LLMs has sparked height-\nened interest in prompt-based zero-shot and few-\nshot learning (Ye et al., 2021; Schick and Schütze,\n2021). Instead of fine-tuning on massive down-\nstream data, in-context learning (ICL) (Brown et al.,\n2020), which suits LLMs to new tasks with few-\nshot input-label exemplars as demonstrations with-\nout training, has shown promising few-shot perfor-\nmance. It has been further improved by later works\n(Liu et al., 2022; Lu et al., 2022; SU et al., 2023).\nOn the other hand, zero-shot learning is much\nmore challenging without task-specific data. Direct\nsteering LLMs for predictions without in-context\ndemonstrations can lead to significantly degraded\nperformance (Gao et al., 2021). To bridge this,\nsome methods (Wei et al., 2022; Sanh et al., 2022;\nXu et al., 2022) adopt instruction tuning with a\nmulti-task paradigm to further pre-train the LLMs\nwith a collection of different tasks in shared prompt-\ning templates. However, these methods require\ncumbersome training for LLMs and the overwhelm-\ning bulk of cross-task human annotations. Another\nnew line of research (Ye et al., 2022a; Meng et al.,\n2022; Ye et al., 2022b) endeavors to ameliorate\nzero-shot learning merely via dataset generation,\nwhile the synthesized data commonly involves a no-\ntable portion of low-quality samples and misses the\nnuanced semantics present in the original data. In\nour work, we take inspiration from active learning\nwith an innovative viewpoint to distill and filter the\nrich knowledge from LLMs for boosted zero-shot\ngeneralization performance.\n2.2 Active Learning\nActive learning (AL) is a prevailing paradigm in\nvarious NLP tasks (Yuan et al., 2020; Zhao et al.,\n2020; Shelmanov et al., 2021; Wang et al., 2022)\nthat aims to reduce labeling effort by selecting only\nthe most useful examples to annotate. In each it-\neration of active learning, a model is trained on\nthe currently labeled data and then tasked with se-\nlecting the most informative yet-to-be-labeled data\npoint to be labeled for boosted performance. Based\non different querying strategies (Settles and Craven,\n2008), existing traditional active learning methods\ncan be categorized into uncertainty-based meth-\nods (Prabhu et al., 2019; Margatina et al., 2021)\nand diversity-based methods (Sener and Savarese,\n2018; Ru et al., 2020; Ash et al., 2020). While these\n14521\nFigure 2: Overview of FreeAL. In each collaborative training loop, the LLM serves as an active annotator imbuing\nits knowledge. Then the SLM is employed as a filter to distill the task-related knowledge with robust self-training\nfrom LLM and filter out a high-quality demonstration pool Ddemo to feedback the subsequent label refinery of LLM.\nmethods relieve the annotation burden to some ex-\ntent, they still count on human experts as expensive\nsupervision sources to start with. To overcome\nthis high cost, we investigate the opportunities of\nleveraging the rich knowledge of LLMs as a low-\ncost supervision source for boosting generalization\nperformance without human effort.\n3 Background\nWe consider unsupervised classification tasks with-\nout human annotations. Given an unlabeled train-\ning dataset Dtrain = {xi}n\ni=1 with nsamples, where\nx ∈ Xis the input text and the corresponding\nground-truth label y ∈Y = {1,...,C }is inac-\ncessible. Our task is to predict the true label for\nboth the training dataset Dtrain and test dataset Dtest.\nOur framework employs a pre-trained large lan-\nguage model (LLM) Pand a downstream small\nlanguage model (SLM) S. For the LLM, we define\na natural language template T(·) which contains\nadditional task-related information and a verbalizer\nV(·) which maps each class label in {1,...,C }\nto a pre-defined token in the prompt. For the fine-\ntuning of SLM Swith parameters θ, we adopt the\ncross entropy loss li = −∑\nj∈Y ˜yij log Sj(xi,θ)\nfor training, where Sj(xi,θ) is the j-th entry of\nSLM’s output softmax probability for the input xi\nwith the pseudo label ˜yij.\nFew-shot In-context Learning. When super-\nvised data are available, we can directly em-\nploy the few-shot ICL for inference. In con-\ncrete, given a demonstration supporting pool\nDdemo = {xdemo\ni ,˜ydemo\ni }m\ni=1 for prompt retrieval\nduring ICL, we construct a prompt including a\ntest input xtest and m-shot in-context examples\n{(xdemo\nj ,˜ydemo\nj )}m\nj=1 retrieved from Ddemo as the\ndemonstration. The final prompt steers the LLM\nand the prediction is obtained via,\narg maxPy∈Y (V(y) |T(xdemo\n1 ,˜ydemo\n1 ),\n...,T(xdemo\nm ,˜ydemo\nm ),T(xtest))\n(1)\nDespite the simplicity, the success of ICL largely\nhinges on the demonstration pool Ddemo, which\nrequires human efforts of careful annotation for\nevery individual scenario and can be particularly\nannoying for challenging tasks. To bridge this gap,\nwe resort to our proposed plug-and-play method\nFreeAL without involving any human supervision.\n4 FreeAL\nIn this section, we introduce our proposed frame-\nwork FreeAL which investigates the opportunity\nfor human-free active learning in the LLMs era. In\ncontrast to traditional active learning that requests\nhuman annotation in each training loop, FreeAL\nemploys LLMs as weak annotators. In each train-\ning loop, we alternate the following steps:\n1. Active labeling of the to-be-labeled samples\nvia LLMs based on the feedback from SLMs.\n2. Training weakly supervised SLMs to distill\nthe task-related knowledge from noisy anno-\ntations of LLMs and in turn feedback to them.\nThe overview of the FreeAL framework is dis-\nplayed in Figure 2 and its overall pipeline is also\nshown in Algorithm 1. In what follows, we will\nelaborate on our FreeAL framework minutely.\n14522\n4.1 Active Labeling by LLMs\nIn this step, we leverage the strong in-context\nlearning ability of LLMs to assign weak labels to\nunsupervised training corpora. In particular, the\ncore challenge lies in the construction of a proper\nprompt containing demonstration samples. To this\nend, we introduce two practical strategies for the\ndifferent life cycles of FreeAL.\nInitial Annotation by Self-generated Demonstra-\ntion. At the initial round of FreeAL, we are given\na purely unsupervised training set Dtrain. To en-\nable pseudo-labeling via LLMs, we may directly\nperform zero-shot ICL without access to a demon-\nstration pool Ddemo. However, such a strategy can\nlargely impede the knowledge-distilling process of\nSLMs due to shoddy initial annotations. To rem-\nedy this, we design a novel self-generated demon-\nstration technique by virtual data generation. No-\ntably, when given some unlabeled samples and task-\ndescriptive instructions, humans can imitate the ex-\npression styles of these texts and leverage their own\nknowledge to generate similar label-aware samples.\nMotivated by this, we steer LLMs to first mimic the\nformat of unlabeled samples from Dtrain, which is\nimportant to ICL according to recent research (Min\net al., 2022), and then generate new label-aware\nexamples to construct the initial Ddemo.\nSpecifically, the data-generation prompt con-\ntains a hand-crafted task-descriptive instruction\nρgen that explains the task background and Q\nrandomly-selected unlabeled samples cgen from\nDtrain as prototypes to imitate. An example of the\nprompt is shown in Appendix B.3. The generation\nprocess can be formulated as,\n{(xgen,˜ygen)}← P(ρgen,T(cgen)) (2)\nThe generated samples constitute the generated\ndataset Dgen = {(xgen,˜ygen)}, which is then used\nas demonstration pool (i.e.,Ddemo = Dgen) for the\nsubsequent labeling. Next, we follow the standard\nICL pipelines with demonstration selection (Liu\net al., 2022). Each prompt contains m-nearest-\nneighbors from Ddemo with the highest embedding\nsimilarity to xi. The ICL process follows Eq.(1).\nWith the demonstrations seen in the prompt, the\nLLM is able to provide passable initial annotations\n˜y of the training dataset Dtrain = {xi,˜yi}n\ni=1, the\nannotations ˜y are employed as pseudo-labels for\nthe subsequent training of SLM.\nAlgorithm 1 Pipeline of FreeAL\nInput: Unlabeled dataset Dtrain = {xi}n\ni=1; pre-\ntrained LLM Pand a downstream SLM S;\n1: round←1\n2: while not convergent do\n3: # For LLM: active annotation\n4: if round= 1then\n5: # Initial self-generated demonstration\n6: Generate {(xgen,˜ygen)}as Eq.(2);\n7: Ddemo = Dgen = {(xgen,˜ygen)};\n8: else\n9: Receive Ddemo from SLM;\n10: end if\n11: In-context learning as Eq.(1) for labeling;\n12: round←round+ 1;\n13: # For SLM: knowledge distillation\n14: Robust self-training as Eq.(3)\n15: # Construction of Ddemo\n16: Filter out class-wise clean subset Dj\nclean\n17: Adopt k-medoids on Dj\nclean for Dj\ndemo\n18: Ddemo = ∪j∈Y Dj\ndemo\n19: Dnoisy = Dtrain \\(∪j∈Y Dj\nclean)\n20: Feed Ddemo and Dnoisy back to LLM\n21: round←round+ 1;\n22: end while\nRefined Annotation in Subsequent Rounds. In\nthe later rounds, the SLM Sis trained using the\nweak annotation given by the LLM P. Meanwhile,\nthe SLM filters out a high-quality demonstration\npool Ddemo as feedback; details are shown in Sec-\ntion 4.2. Then with a high-quality Ddemo, the LLM\nPre-annotates the remaining noisy-prone samples\nvia few-shot ICL according to Eq. (1).\n4.2 Knowledge Distillation by SLMs\nGiven the acquired weak annotations from LLM, it\nis difficult for the LLM to distinguish its own errors\ndue to the confirmation bias. Fortunately, previous\nstudies (Han et al., 2018; Li et al., 2020) in weakly-\nsupervised learning have shown that deep models\nhave the potential of detecting noisy samples dur-\ning the training procedure. Therefore, after receiv-\ning weak labels, our intention is two-fold: (i)-train\na strong and robust downstream SLM that maxi-\nmally distills task-specific knowledge; (ii)-employ\nthe derived SLM to filter out a high-quality demon-\nstration pool to feedback LLM.\n14523\n4.2.1 Robust Self-training\nMotivated by the memorization effect of DNNs\n(Zhang et al., 2017), the SLM tends to first fit easy\npatterns in the early stage of training. Thus, noisy\nsamples mostly pose larger loss values. To this\nend, we adopt the selection-based technique (Li\net al., 2020) from noisy label learning to train a\nrobust SLM for knowledge distillation.\nFormally, after a few warm-up epochs with stan-\ndard training on noisy labels, given the standard\ncross-entropy loss li that reflects how well the\nmodel fits the sample xi, we fit a two-component\nGMM to the loss li to find out those clean samples.\nLet wi = p(g |li) represent the probability of xi\nbelonging to the Gaussian component with smaller\nmean g, which can also be deemed as its clean\nprobability. Then we divide the training dataset\ninto a clean subset and a noisy subset by setting a\nthreshold τ on wi , which is considered as a labeled\nset Dl and a noisy set Du respectively,\nDl = {(xi,˜yi) |xi ∈Dtrain,wi ≥τ},\nDu = {(xi) |xi ∈Dtrain,wi <τ } (3)\nTo improve the robustness of training, we uti-\nlize consistency regularization for boosted perfor-\nmance, which assumes that a classifier should pro-\nduce a similar prediction on a local neighbor of\neach data point. Given an input xi, we adopt back-\ntranslation (Sennrich et al., 2016) to paraphrase it\nand obtain the augmented version xaug\ni . For the\nlabeled and unlabeled data, the consistency regular-\nizations are formulated,\nLl\ncr = 1\n|Dl|\n∑\nxi∈Dl\nlce(xaug\ni ,˜yi),\nLu\ncr = 1\n|Du|\n∑\nxi∈Du\nlkl(S(xaug\ni ),S(xi))\n(4)\nwhere lce and lkl are standard cross entropy and KL\ndivergence respectively. Finally, the total loss for\nself-training of SLM is aggregated,\nLtotal = Lclean + α(Ll\ncr + Lu\ncr) (5)\nwhere Lclean is the cross entropy loss on Dl, α\nis the loss weight parameter. We refer readers to\nAppendix B.1 for more implementation details.\n4.2.2 Demonstration Pool Filtering\nWhile the SLM S can filter out a clean subset to\nenhance its performance during self-training, other\nstubborn noisy labels are hard to correct by SLM\nitself due to the confirmation bias. Thanks to our\nrobust SLM, we can filter out those clean and rep-\nresentative samples and construct a high-quality\ndemonstration pool Ddemo for the LLM to refur-\nbish its potentially wrong predictions in previous\nrounds. One may directly reuse the GMM-based\nselection criterion again and take Dl as demonstra-\ntions. However, such a selection procedure is too\naggressive since excessively over-selecting some\nnoisy samples may still improve the self-training\nprocedure. To this end, we would like to filter out a\nmore curated Ddemo that prioritizes representative\nexamples with accurate labels to be included.\nThe construction process mainly contains two\nsteps in a class-wise manner to cover every class\nand ensure diversity. For the training subset Dj\ntrain\nof class j, following the memory effect of DNNs\n(Zhang et al., 2017), we utilize the small loss cri-\nterion and select samples with the smallest cross-\nentropy loss li in the first Rpercent to construct\nDj\nclean = {(xi,˜yi) | rank(li) ≤ R%,˜yi = j}.\nIn practice, we set a small R to ensure the high\nprecision of Dj\nclean. Secondly, we further adopt\na simple clustering algorithm k-medoids on the\nembeddings of SLM to filter out the most repre-\nsentative medoids samples from Dj\nclean to construct\nDj\ndemo. When the k-medoids algorithm gets con-\nverged, the medoids of kclusters are collected as\nDj\ndemo. Finally the integral demonstration set is\nmerged from each class as Ddemo = ∪j∈Y Dj\ndemo.\nWith a high quality Ddemo, the great potential\nof LLM P can be unleashed to refine those noisy-\nprone samples Dnoisy = Dtrain \\(∪j∈Y Dj\nclean) via\nfew-shot ICL as described in section 4.1.\n5 Experiment\nIn this section, we provide the experimental results\nto verify the effectiveness of the FreeAL frame-\nwork. More results, including visualizations and\nmodel selection, can be found in Appendix.\n5.1 Setup\nDatasets. We evaluate the performance of\nFreeAL on both sequence-level and token-level\ntasks. For sequence-level tasks, we choose SST-2\n(Socher et al., 2013), MR (Pang and Lee, 2005)\ndataset for sentiment classification, SUBJ (Pang\nand Lee, 2004) dataset for subjectivity classifica-\ntion and TREC (V oorhees and Tice, 2000) for topic\nclassification. For token-level tasks, CoNLL03\n(Tjong Kim Sang and De Meulder, 2003) dataset\n14524\nModel Round Demons/Annos SST-2 MR SUBJ TREC CoNLL03 MA BC5-C BC5-D\nGPT-3.5-Turbo\n0 Zero-shot 88.93 89.99 57.11 43.36 64.19 59.51 69.28 27.74\n1 Self-generated 92.16 91.74 86.54 70.74 70.89 59.78 81.05 47.12\n3 Selected by Round 2 94.93 92.89 90.33 77.70 74.71 61.38 82.40 52.59\nRoBERTa 2 Annotated by Round 1 94.70 92.43 92.24 76.75 74.49 61.41 81.61 52.89\n4 Annotated by Round 3 95.49 92.64 92.85 81.59 78.79 62.15 82.81 59.25\nTable 1: Comparisons of transductive performance on training datasets of different tasks. BC5-C/D refers to\nBC5CDR-Chemical/Disease dataset. For the token-level NER tasks (including CoNLL03, BC5-C, BC5-D) the\nF1-score is given and For the other sequence-level tasks the test accuracy is provided.\nModel Round Demons/Annos SST-2 MR SUBJ TREC CoNLL03 MA BC5-C BC5-D\nGPT-3.5-Turbo\n0 Zero-shot 92.47 90.05 55.65 77.20 66.47 59.71 67.85 29.60\n1 Self-generated 93.73 90.85 83.85 80.00 70.22 59.97 76.90 50.68\n3 Selected by Round 2 95.91 93.10 90.27 79.80 70.80 60.93 80.77 52.70\nRoBERTa 2 Annotated by Round 1 94.29 89.35 92.95 86.80 71.82 61.91 80.55 53.38\n4 Annotated by Round 3 94.66 90.20 94.45 91.40 76.12 62.64 81.13 58.90\nTable 2: Comparisons of inductive performance on test datasets of different tasks. BC5-C/D refers to BC5CDR-\nChemical/Disease dataset. For the token-level NER tasks (including CoNLL03, BC5-C, and BC5-D) the F1-score is\ngiven and For the other sequence-level tasks the test accuracy is provided.\nis adopted for named entity recognition (NER). To\nvalidate the feasibility of FreeAL in practical sce-\nnarios such as medical diagnosis and biochemi-\ncal applications that demand highly specialized\ndomain-specific expertise, we also conduct exper-\niments on BC5CDR (Li et al., 2016) dataset with\nchemical and disease interactions as token-level\nNER tasks and Medical Abstract (MA) (Schopf\net al., 2022) dataset describing 5 different classes\nof patient conditions as sequence-level classifica-\ntion task. More details are listed in Table 4.\nPerformance Evaluation. In this work, we eval-\nuate FreeAL from two aspects: (i)- Transductive\nPerformance: Given unsupervised training data,\nwe evaluate the training accuracy of FreeAL which\nreflects how well task-specific knowledge is dis-\ntilled; (ii)-Inductive Generalization: utilize the\nderived models, including the SLM and Ddemo for\nLLM, to further assess the generalization efficiency\non the unseen test set with the inductive learning\nparadigm. We report the classification accuracy or\nthe F1 score on both training and testing sets. We\ntest the performance at different rounds. Round 0\ndenotes vanilla zero-shot learning of LLM. Round\n1 and round 2 denote the performance of LLM and\nSLM in the first training loop, while round 3 and\n4 are those of the second refinery training loop, as\nshown in Figure 2. For all experiments, we run\nthree times and report the averaged results.\nBaselines. We compare FreeAL with multiple\nzero-shot and supervised baselines for LLMs and\nSLMs respectively. For LLMs, they are vanilla\nzero-shot ICL without demonstrations (Brown\net al., 2020), supervised ICL with standard demon-\nstration retrieval (Liu et al., 2022) from human-\nlabeled training data, and supervised ICL with k-\nmedoids to first filter a representative subset for\ndemonstration retrieval. For SLMs, they are zero-\nshot distillation (Hinton et al., 2015; Smith et al.,\n2022) that finetunes the SLMs by using the annota-\ntions from zero-shot ICL of LLM as ground-truths,\nand standard supervised fine-tuning that finetunes\nthe SLM with human-labeled data. We also com-\npare FreeAL with some traditional active learning\nbaselines in section 5.3.1, including (1) Random: It\nacquires annotation of to-be-labeled data randomly.\n(2) Entropy (Holub et al., 2008): It is the most\ncommonly used uncertainty-based baseline that ac-\nquires samples with the highest predictive entropy.\n(3) CAL (Margatina et al., 2021): It is a recent\nactive learning method that acquires contrastive\nexamples for pre-trained language models.\nImplementation Details. We adopt OpenAI’s\nGPT-3.5-Turbo language model, also known as\nChatGPT, as our LLM and we use RoBERTa-Base\nfrom Huggingface Transformers (Wolf et al., 2020)\nas the downstream SLM. For the biomedical tasks\nincluding MA and BC5DER dataset, we utilize a\n14525\nModel Ablation Human SST-2 MR SUBJ TREC CoNLL MA BC5-C BC5-D\nGPT-3.5-Turbo\nZero-shot ICL ✗ 92.47 90.05 55.65 77.20 66.47 59.71 67.85 29.60\nFreeAL (ours) ✗ 95.91 93.10 90.27 79.80 70.80 60.93 80.77 52.70\n∆ Absolute gain - +3.44 +3.05 +34.6 +2.60 +4.33 +1.22 +12.9 +23.1\nSupervised ICL (Standard) ✓ 96.06 92.85 89.30 81.50 85.46 61.22 82.24 68.63\nSupervised ICL (k-medoids) ✓ 96.10 93.19 90.35 82.60 84.97 61.13 82.06 67.93\nRoBERTa\nZero-shot distillation ✗ 92.81 88.60 59.25 82.80 69.71 61.22 77.05 31.98\nFreeAL (ours) ✗ 94.66 90.20 94.45 91.40 76.12 62.64 81.13 58.90\n∆ Absolute gain - +1.85 +1.60 +35.2 +8.60 +6.41 +1.42 +4.08 +26.9\nSupervised FT ✓ 94.89 91.05 95.95 96.70 88.11 63.96 87.26 75.38\nTable 3: Performance comparison of FreeAL with the zero-shot and supervised counterparts on the test dataset.\nBC5-C/D refers to BC5CDR-Chemical/Disease dataset. The results of FreeAL are in bold. Supervised FT refers to\nsupervised fine-tuning. The absolute gain indicates the improvement of FreeAL compared to the zero-shot baseline.\nDataset Domain #Token #Train #Test\nSST-2 Sentiment cls 19.3 6,920 1,821\nMR Sentiment cls 21.6 8,662 2,000\nSUBJ Subjectivity cls 24.5 8,000 2,000\nTREC Topic cls 10.2 5,452 500\nCoNLL03 NER 14.59 14,041 3,453\nMA Medical cls 205.3 11,550 2,888\nBC5CDR NER 25.92 4,560 4,797\nTable 4: A list of benchmarks used in the experiments.\n#Train and #Test indicate the size of the training and\ntest dataset. #Token is the number of tokens on average\nfor the corresponding training dataset.\nBioMed-RoBERTa-base (Gururangan et al., 2020)\nthat is pre-trained on the Semantic Scholar corpus\nas SLM for boosted performance. For fair compar-\nisons, all the ICL processes of the LLM comprise\nm = 10 context examples as demonstrations ex-\ncept on MA dataset 5 is adopted due to the maxi-\nmum context length 4,096 for GPT-3.5-Turbo. The\ncollaborative training process is performed on the\ntraining dataset first and then the fine-tuned SLM\nand the Ddemo for LLM are utilized to be evalu-\nated on the test dataset. More details of the robust\nself-training are put in Appendix B.2.\n5.2 Main Results\nTable 1 and Table 2 display the results of FreeAL\nat different rounds in the collaborative training\nprogress on the training and test dataset for trans-\nductive and inductive performance respectively. Ta-\nble 3 reports the comparisons of FreeAL with other\nzero-shot and supervised counterparts.\nBased on these results, it can be observed that\nFreeAL significantly enhances the unsupervised\nperformance of both LLM and SLM. Free exceeds\nthe zero-shot ICL for the LLM by 3.44%, 3.05%,\nand 2.60% on the SST-2, MR, and TREC dataset re-\nspectively. It reaches a staggering lead of 34.6% on\nthe SUBJ dataset where the LLM fails to adapt to\non its own. In the medical diagnosis and biochemi-\ncal fields, FreeAL also exhibits a notable advantage\nof 12.9% and 23.1% on the chemical and disease\ntasks of the BC5CDR dataset. FreeAL showcases a\nsimilar trend of leading performance for the SLMs.\nInterestingly, In comparison to the supervised coun-\nterparts, FreeAL achieves competitive performance\non par with these supervised rivals on some sim-\nple tasks such as SST-2 and SUBJ datasets and\ngreatly narrows the gap between the zero-shot and\nfully-supervised performances on other challeng-\ning tasks. Notably, the performance can be further\nimproved with more interaction rounds (also larger\ncost), but 4 rounds of interaction can achieve satis-\nfactory results empirically. These results suggest\nthat FreeAL is able to fully distill the task-related\nknowledge from LLMs’ weak supervision. More\nanalyses can be found in Section 5.3.2.\n5.3 Analysis\n5.3.1 Comparisons with Active Learning\nWe also compare our FreeAL framework with some\ntraditional active learning methods on the SST-2\nand MR dataset. As shown in Table 5 and Fig-\nure 1, It can be observed that FreeAL outstrips\nthe traditional active learning baselines with 20%\nand 50% acquired human annotations, which fur-\nther indicates that FreeAL can serve as a superior\nalternative to traditional active learning by lever-\naging the rich knowledge of LLMs as a low-cost\nhuman-free supervision source.\n14526\nMethod Human Anno SST-2 MR\nRandom 20% samples 92.42 88.10\n50% samples 92.92 89.10\nEntropy 20% samples 92.37 88.65\n50% samples 94.29 90.00\nCAL 20% samples 93.36 88.45\n50% samples 94.56 89.75\nFreeAL (ours) ✗ 94.66 90.20\nTable 5: Comparisons of FreeAL with traditional active\nlearning algorithms on the SST-2 and MR dataset.\n5.3.2 Effect of Collaborative Training\nFrom a more nuanced perspective of the perfor-\nmance improvements at different rounds on the\ntraining set in Table 1, it can be noticed that FreeAL\niteratively refines the noisy annotations during the\ncollaborative training. The improvement from\nround 0 to 1 indicates the effectiveness of self-\ngenerated demonstrations for better initial annota-\ntions. The performance advancements from rounds\n1 to 2 and rounds 3 to 4 demonstrate the ability\nof robust self-training for SLM to distill valuable\nknowledge from noisy annotations. Further, the per-\nformance boost from round 2 to round 3 verifies the\nefficacy of the active label refinery process. For the\ntest dataset in Table 2, the performance changes fol-\nlow a similar trend with some fluctuations, which\nhave been further discussed in Appendix A.1.\nTo further validate the efficacy of collaborative\ntraining, we also conduct additional ablation exper-\niments for the components of FreeAL as shown in\nTable 6. For the generalization performance on the\nSLM, we compare FreeAL with its variant that dis-\ncards robust self-training and adopts the standard\ncross entropy loss for training (including round 2\nand 4). It can be observed that robust self-training\nlargely improves the performance of FreeAL. For\nthe performance of LLM, we ablate FreeAL with\nother selection strategies from traditional active\nlearning rather than small loss selection, including\nrandom selection and entropy selection that selects\nsamples with the lowest entropy values with the\nsame budget as small loss selection. We can see\nthat entropy selection slightly makes up for the\npoor performance of random selection, but still\nlags behind FreeAL by a notable margin.\n5.3.3 Impact of In-Context Examples m\nThen, we show the effect of different numbers m\nof in-context examples during the process of ICL\nModel Ablation SST-2 MR\nRoBERTa FreeAL 94.66 90.20\nw/o robust self-training 89.18 88.95\nGPT-3.5-Turbo\nFreeAL 95.91 93.10\nwith random selection 95.12 92.15\nwith entropy selection 95.65 92.67\nTable 6: Ablation study of FreeAL for the SLM and\nLLM on the SST-2 dataset and MR dataset.\nFigure 3: Ablation study of different numbers of in-\ncontext examples mon the SST-2 and MR dataset.\non the SST-2 and MR datasets. As shown in Figure\n3, FreeAL is able to produce a competitive perfor-\nmance to the supervised rivals over a wide range of\nmfrom 1 to 20, this further verifies the robustness\nof FreeAL and we can simply adopt m = 10for\nfair comparisons in our experiments.\n6 Conclusion\nIn this work, we overhaul the traditional active\nlearning in the era of LLMs and propose a novel\nframework called FreeAL that merely relies on the\nknowledge of LLMs to enhance human-free gen-\neralization performance. The key idea of FreeAL\nis to distill and filter the task-related knowledge\nfrom LLMs with a collaborative framework, where\nthe LLM is employed as an active annotator and\nthe SLM is engaged as a weak learner to filter out\nvaluable samples for label refinery. The empirical\nresults indicate that FreeAL can largely improve\nunsupervised performance and reaches compara-\nble performance with supervised rivals in some\ntasks. While our FreeAL framework operates au-\ntonomously without human supervision, it is flexi-\nble and can be easily boosted with additional lim-\nited human supervision, which we leave for our\nfuture work. We hope that our work can spark\nheightened interest in developing new active anno-\ntation algorithms in the era of LLMs.\n14527\nLimitations\nOur proposed FreeAL is a collaborative framework\nthat aims to enhance unsupervised performance\nwithout human effort. Despite its effectiveness,\nthere is still much potential for improvement. First,\nthe effectiveness of FreeAL largely hinges on the\nstrong ability of LLMs. For some domains that\nare extremely challenging or eccentric, the com-\nmonly adopted GPT-3.5-Turbo nowadays may fail\nto provide a qualified initial annotation, even with\nself-generated demonstrations. Our model is antic-\nipated to be suitable for these circumstances with\nthe advancement of more powerful LLMs across\ndiverse domains. Besides, we thoroughly forgo\nhuman efforts in our FreeAL framework while in\npractical scenarios there may exist more or less\navailable human support. It remains underexplored\nhow to effectively combine the supervision from\nhuman experts and LLMs to synergize their individ-\nual strengths, and we leave it for our future work.\nEthics Statement\nWhile our proposed FreeAL serves as an innova-\ntive way to enhance generalization performance\nwithout human intervention, the predictions and\nself-generated demonstrations of the adopted LLM\nAPI may include bias and unfairness. Indeed, if\none utilizes FreeAL with such biased annotations,\nit may unpleasantly yield unfair and biased pre-\ndictions based on characteristics like race, gender,\ndisabilities, LGBTQ, or political orientation. To\nalleviate this issue, we recommend that potential\nusers first use bias reduction and correction tech-\nniques to remove biased text and predictions so as\nto improve overall fairness and ethical standard.\nAcknowledgements\nThis work is majorly supported by the NSFC un-\nder Grants (No. 62206247), and in part by the\nNational Key Research and Development Program\nof China (No. 2022YFB3304101). Junbo Zhao\nalso thanks the sponsorship by the Fundamental\nResearch Funds for the Central Universities (No.\n226-2022-00028). This paper is also supported\nby Netease Youling Crowdsourcing Platform1. As\nthe importance of data continues rising, Netease\nYouling Crowdsourcing Platform is dedicated to\nutilizing various advanced algorithms to provide\nhigh-quality, low-noise labeled samples.\n1https://fuxi.163.com\nReferences\nJordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy,\nJohn Langford, and Alekh Agarwal. 2020. Deep\nbatch active learning by diverse, uncertain gradient\nlower bounds. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. CoRR, abs/2302.04023.\nDavid Berthelot, Nicholas Carlini, Ian J. Goodfellow,\nNicolas Papernot, Avital Oliver, and Colin Raffel.\n2019. Mixmatch: A holistic approach to semi-\nsupervised learning. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5050–5060.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\n14528\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim. 2020.\nActive Learning for BERT: An Empirical Study. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7949–7962, Online. Association for Computa-\ntional Linguistics.\nJiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng\nYe, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang,\nZhenguo Li, and Lingpeng Kong. 2023. Self-guided\nnoise-free data generation for efficient zero-shot\nlearning. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu,\nMiao Xu, Weihua Hu, Ivor W. Tsang, and Masashi\nSugiyama. 2018. Co-teaching: Robust training of\ndeep neural networks with extremely noisy labels. In\nAdvances in Neural Information Processing Systems\n31: Annual Conference on Neural Information Pro-\ncessing Systems 2018, NeurIPS 2018, December 3-8,\n2018, Montréal, Canada, pages 8536–8546.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nAlex Holub, Pietro Perona, and Michael C. Burl. 2008.\nEntropy-based active learning for object recognition.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR Workshops 2008, Anchorage, AK,\nUSA, 23-28 June, 2008, pages 1–8. IEEE Computer\nSociety.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In NeurIPS.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers, and\nZhiyong Lu. 2016. Biocreative V CDR task corpus:\na resource for chemical disease relation extraction.\nDatabase J. Biol. Databases Curation, 2016.\nJunnan Li, Richard Socher, and Steven C. H. Hoi.\n2020. Dividemix: Learning with noisy labels as\nsemi-supervised learning. In 8th International Con-\nference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nSheng Liu, Jonathan Niles-Weed, Narges Razavian, and\nCarlos Fernandez-Granda. 2020. Early-learning reg-\nularization prevents memorization of noisy labels.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021. Active learning by ac-\nquiring contrastive examples. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 650–663, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language mod-\nels: Towards zero-shot language understanding. In\nNeurIPS.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\n14529\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings\nof the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL-04), pages 271–278,\nBarcelona, Spain.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In Proceedings of the\n43rd Annual Meeting of the Association for Compu-\ntational Linguistics (ACL’05), pages 115–124, Ann\nArbor, Michigan. Association for Computational Lin-\nguistics.\nAmeya Prabhu, Charles Dognin, and Maneesh Singh.\n2019. Sampling bias in deep active classification: An\nempirical study. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 4058–4068, Hong Kong, China. Association\nfor Computational Linguistics.\nDongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingx-\nuan Wang, Weinan Zhang, Yong Yu, and Lei Li. 2020.\nActive sentence learning by adversarial uncertainty\nsampling in discrete space. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 4908–4917, Online. Association for Computa-\ntional Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTim Schopf, Daniel Braun, and Florian Matthes.\n2022. Evaluating unsupervised text classification:\nZero-shot and similarity-based approaches. CoRR,\nabs/2211.16285.\nOzan Sener and Silvio Savarese. 2018. Active learning\nfor convolutional neural networks: A core-set ap-\nproach. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceed-\nings. OpenReview.net.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 86–96,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nBurr Settles and Mark Craven. 2008. An analysis of\nactive learning strategies for sequence labeling tasks.\nIn Proceedings of the 2008 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1070–1079, Honolulu, Hawaii. Association for Com-\nputational Linguistics.\nArtem Shelmanov, Dmitri Puzyrev, Lyubov\nKupriyanova, Denis Belyakov, Daniil Larionov,\nNikita Khromov, Olga Kozlova, Ekaterina Artemova,\nDmitry V . Dylov, and Alexander Panchenko. 2021.\nActive learning for sequence tagging with deep\npre-trained models and Bayesian uncertainty\nestimates. In Proceedings of the 16th Conference\nof the European Chapter of the Association for\nComputational Linguistics: Main Volume , pages\n1698–1712, Online. Association for Computational\nLinguistics.\nRyan Smith, Jason A. Fries, Braden Hancock, and\nStephen H. Bach. 2022. Language models in the\nloop: Incorporating prompting into weak supervision.\nCoRR, abs/2205.02318.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\nZhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk,\nAlexey Kurakin, and Chun-Liang Li. 2020. Fix-\nmatch: Simplifying semi-supervised learning with\nconsistency and confidence. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nHongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.\nSelective annotation makes language models better\nfew-shot learners. In The Eleventh International Con-\nference on Learning Representations.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\n14530\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Agüera\ny Arcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nEllen M. V oorhees and Dawn M. Tice. 2000. Building\na question answering test collection. In SIGIR 2000:\nProceedings of the 23rd Annual International ACM\nSIGIR Conference on Research and Development\nin Information Retrieval, July 24-28, 2000, Athens,\nGreece, pages 200–207. ACM.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195–4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nXudong Wang, Long Lian, and Stella X. Yu. 2022. Un-\nsupervised selective labeling for more effective semi-\nsupervised learning. In Computer Vision - ECCV\n2022 - 17th European Conference, Tel Aviv, Israel,\nOctober 23-27, 2022, Proceedings, Part XXX , vol-\nume 13690 of Lecture Notes in Computer Science ,\npages 427–445. Springer.\nYisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jin-\nfeng Yi, and James Bailey. 2019. Symmetric cross\nentropy for robust learning with noisy labels. In 2019\nIEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October 27\n- November 2, 2019, pages 322–330. IEEE.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Wang\nYanggang, Haiyu Li, and Zhilin Yang. 2022. Zero-\nPrompt: Scaling prompt-based pretraining to 1,000\ntasks improves zero-shot generalization. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 4235–4252, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiang-\ntao Feng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022a. ZeroGen: Efficient zero-shot learning via\ndataset generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11653–11669, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng,\nTao Yu, and Lingpeng Kong. 2022b. ProGen: Pro-\ngressive zero-shot dataset generation via in-context\nfeedback. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022 , pages 3671–\n3683, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7163–7189, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nOfer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna\nWeinshall. 2022. Active learning through a covering\nlens. In NeurIPS.\nYue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang,\nand Chao Zhang. 2022. AcTune: Uncertainty-based\nactive self-training for active fine-tuning of pretrained\nlanguage models. In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 1422–1436, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nMichelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-\nGraber. 2020. Cold-start active learning through self-\nsupervised language modeling. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7935–7948,\nOnline. Association for Computational Linguistics.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin\nRecht, and Oriol Vinyals. 2017. Understanding deep\n14531\nlearning requires rethinking generalization. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nZhilu Zhang and Mert R. Sabuncu. 2018. Generalized\ncross entropy loss for training deep neural networks\nwith noisy labels. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montréal, Canada, pages\n8792–8802.\nYuekai Zhao, Haoran Zhang, Shuchang Zhou, and Zhi-\nhua Zhang. 2020. Active learning approaches to\nenhancing neural machine translation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 1796–1806, Online. Association\nfor Computational Linguistics.\nA Additional Experimental Results\nA.1 Discussion on Model Selection\nWith our collaborative training paradigm, we are\nable to interactively distill and filter task-related\nknowledge from LLMs. Empirically, our FreeAL\nmethod significantly enhances the zero-shot (dis-\ntillation) performance of both SLMs and LLMs as\ndiscussed in Section 5. One intriguing finding is\nthat, in the majority of evaluation cases, the final\nSLMs outperform the LLMs. This observation can\nbe attributed to the superior distillation ability of\nSLMs during the weakly-supervised fine-tuning\nprocess. Consequently, we believe that SLMs re-\nmain a viable choice for practical deployment due\nto their impressive fine-tuned performance and low\ncomputational requirements. Furthermore, in more\ngeneral scenarios, we recommend the utilization\nof a validation set to determine the most suitable\nmodel for deployment.\nA.2 FreeAL Can Reduce the Annotation Cost\nAs FreeAL solely depends on the knowledge of\nLLM and not on human efforts, it can naturally be\nleveraged as a low-cost data labeler in real-world\nscenarios. In Table 7, we evaluate the cost disparity\nbetween FreeAL and human annotations. Follow-\ning previous work (Wang et al., 2021), for human\nlabeling, it costs $0.11 per 50 input tokens with a\nminimum of $0.11. For FreeAL, the cost per exam-\nple for m-shot inference is estimated approximately\nas (#Token ×(m+ 1) + 100)×2 ×(2 ×10−6),\nwhere #Token is the average token numbers in\nTable 4, (2 ×10−6) is the cost for GPT-3.5-Turbo\nper token, 100 is roughly the tokens for the task-\nspecific descriptions and the model reply. For each\nSource SST-2 MR SUBJ TREC MA\nHuman 0.11 0.11 0.11 0.11 0.55\nFreeAL 1.2e−3 1.3e−3 1.5e−3 8.5e−4 4.5e−3\nTable 7: Comparisons of annotation cost($) per example\nbetween human labeling and FreeAL.\nModel Round Annotations SST-2 MR\nRoBERTa\n- Vanilla FreeAL 94.66 90.20\n1 Initial 10% from LLM 87.97 81.20\n2 Another 10% from LLM 93.69 87.75\n3 Another 10% from LLM 93.76 88.95\nTable 8: Results with multi-round annotation strategies.\nsample, the ICL is performed at most twice as ini-\ntial annotations and refined annotations. It can be\nobserved that FreeAL can serve as a much cheaper\ndata labeler while achieving passable performance.\nWhen entrusted with a training set that is too\nlarge to label the entire dataset, the annotation\ncost can be further reduced by a simple multi-\nround solution. The core idea is to rely more on\nthe weakly-supervised-learning capability of SLM\nto distill from a small number of annotated labels.\nSpecifically, for the initial annotation round of\nLLM, we randomly sample a subset of P% sam-\nples (empirically we set P = 10) to be annotated\nby LLM. After that, for robust self-training, we per-\nform the original training process for the labeled\ndata Dlabeled and simply extend the consistency\nregularization Lu\ncr for the noisy set Du to the origi-\nnally unlabeled data (i.e., Du = Du ∪Dunlabeled\n). For the demonstration pool filtering, the con-\nstruction process of Ddemo is the same, while for\nDnoisy we randomly sample another subset of P%\nsamples from the unlabeled samples to be anno-\ntated by LLM for the next iterations. The amount\nof iteration rounds can be larger than the original\nFreeAL if available to gradually distill the task-\nrelated knowledge with limited annotation cost.\nAs shown in the Table 8, such a simple remedy\nis able to achieve competitive results close to the\noriginal FreeAL with merely 10% of the previous\ncost each round, which proves the feasibility of\nFreeAL when we cannot afford to label the entire\ndataset. Notably, the process of randomly sam-\npling the to-be-annotated subset on SLMs can be\nfurther improved with other advanced query strate-\ngies (e.g., uncertainty-based), which is a classic\ntopic in traditional active learning.\n14532\nModel Ablation Round SST-2 MR\nGPT-3.5-Turbo FreeAL Round 1 ⇒2 ⇒3 93.73 ⇒95.91 90.85 ⇒93.10\nFreeAL w/o interaction Round 1 ⇒3 93.73 ⇒95.37 90.85 ⇒92.15\nRoBERTa FreeAL Round 2 ⇒3 ⇒4 94.29 ⇒94.66 89.35 ⇒90.20\nFreeAL w/o interaction Round 2 ⇒4 94.29 ⇒94.27 89.35 ⇒89.55\nTable 9: Ablation results of the interaction between the LLM and SLM for FreeAL on the SST-2 and MR dataset.\nModel SST-2 MR SUBJ\nRoBERTa-Base 94.66 90.20 94.45\nRoBERTa-Large 95.83 91.15 95.80\nTable 10: Comparisons of FreeAL with different SLMs.\nA.3 Impact of SLM’s Size\nWe also conduct experiments to reveal the impact\nof the size of SLM. As depicted in Table 10, when\nthe size of SLM grows larger from RoBERTa-\nBase to RoBERTa-Large, FreeAL displays superior\nperformance. This observation indicates that our\nFreeAL is compatible with different sizes of down-\nstream SLM and the performance can be further\nimproved with a larger SLM.\nA.4 Comparisons with Other AL Methods\nHere we provide comparisons with some other ac-\ntive learning selection strategies, including Prob-\nCover (Yehuda et al., 2022), BADGE (Ash et al.,\n2020), Region Entropy and Region CAL (Yu et al.,\n2022) in the Table 11. It can be observed that\nFreeAL exceeds all its rivals, which consistently\ndemonstrates the superior performance of FreeAL.\nA.5 Comparisons with\nDataset-generation-based Methods\nWe further supplement comparisons with some\ndataset-generation-based methods, including Ze-\nroGen (Ye et al., 2022a), ProGen (Ye et al., 2022b)\nand SunGen (Gao et al., 2023). Our FreeAL is\nfundamentally different from them in several per-\nspectives. First, these dataset-generation-based\nmethods are tailored for an extreme scenario where\ntraining data is completely missing, which is un-\npractical in reality. Second, these methods typi-\ncally generate low-quality samples, because they\noverlook the nuances and semantics present in the\noriginal authentic data. As a result, they mostly\nrequire generating a huge amount of synthetic data\nfor decent performance. For example, on the SST-2\ndataset, these methods generate 200k synthesized\nsamples while authentic training samples are only\nMethod Human Anno SST-2 MR\nProbCover 20% samples 92.92 87.95\n50% samples 93.49 89.75\nBADGE 20% samples 93.14 88.15\n50% samples 93.97 89.90\nRegion Entropy 20% samples 92.53 87.55\n50% samples 94.03 88.75\nRegion CAL 20% samples 92.37 88.20\n50% samples 92.70 89.00\nFreeAL (ours) ✗ 94.66 90.20\nTable 11: Comparisons of FreeAL with some other\nactive learning algorithms on the SST-2 and MR dataset.\n6.9k. Empirically, our FreeAL still outperforms\nthese dataset-generation-based methods by a no-\ntable margin, as shown in Table 12.\nA.6 Results with More Distillation Methods\nWe also provide the comparisons with some other\nrobust distillation methods, including GCE (Zhang\nand Sabuncu, 2018), SL (Wang et al., 2019) and\nELR (Liu et al., 2020) in Table 13. We can see\nthat FreeAL largely advances the performances of\nall these distillation baselines. Overall, FreeAL is\ndesigned as a flexible framework and we choose an\nempirically strong self-training algorithm for distil-\nlation to prove the feasibility of human-free active\nlearning. One may design more power distillation\nalgorithms for improved results, which we leave\nfor future work.\nA.7 Effect of Interaction for LLM and SLM\nTo further demonstrate the importance of interac-\ntion between the LLM and the SLM. We provide\nthe inductive performance for FreeAL without in-\nteraction. For the LLM, it directly adopts its own\npredictions on the training dataset at round 1 as the\ndemonstration pool directly for testing. While the\nSLM employs its own predicted labels as supervi-\nsion at round 2 directly for testing.\nAs displayed in Table 9, we observe that the\nSLM itself is hard to distill from its own predic-\n14533\nMethod SST-2 SUBJ\nZeroGen 87.27 80.45\nProGen 88.42 -\nSunGen 89.45 83.25\nFreeAL (DitilBERT) 91.82 92.15\nFreeAL (RoBERTa) 94.66 94.45\nTable 12: Comparisons of FreeAL with dataset-\ngeneration-based methods. We adopt DistilBERT as\nthe SLM of FreeAL for fair comparisons.\ntions due to the inevitable confirmation bias, e.g.,\nimproves 0.2% compared with FreeAL’s improve-\nment of 0.85% on the MR dataset and even de-\ngrades on the SST-2 dataset. For the LLM, it can\nself-improve itself, but still underperforms our col-\nlaborative mechanism. Notably, LLM has an in-\nescapable upper bound on the performance, accord-\ning to our empirical findings where SLM outper-\nforms LLM on 6 out of a total of 8 datasets. Such\nresults indicate that the interaction between LLM\nand SLM can bring new opportunities to converge\nto a consensus result between them.\nA.8 Additional Visualization Results\nWe further provide some additional visualization\nresults, including the transductive performance on\nthe training dataset (i.e., the accuracy of pseudo\nlabels) at different rounds in Figure 4 and the vi-\nsualization of comparisons with traditional active\nlearning methods on the MR dataset in Figure 5.\nB Additional Implementation Details\nB.1 More Details of Robust Self-training\nDuring robust self-training, we also involve a\nmixup training strategy that interpolates the em-\nbeddings and the corresponding pseudo labels on\nthe clean subset Dl to encourage linear behavior be-\ntween samples. A virtual mixed training example\nis generated by linearly interpolating the randomly\nsampled pair of examples (xi,˜yi) and (xj, ˜yj) in\nDl and taking a convex combination of labels as\nthe regression target,\nEmb(xm) =σEmb(xi) + (1−σ)Emb(xj)\nym = σ˜yi + (1−σ) ˜yj\n(6)\nwhere Emb(xi) is the embedding of xi and σ ∼\nBeta(ς,ς ) and we simply set ς = 4. The mixup\nloss is denoted as Lmix. the total loss for self-\ntraining of SLM is aggregated,\nLtotal = Lclean + α(Lcr + Lmix) (7)\nMethod SST-2 MR\nZero-shot distillation 92.81 88.60\nFreeAL with GCE 93.68 88.90\nFreeAL with SL 93.91 89.50\nFreeAL with ELR 94.01 89.70\nVanilla FreeAL 94.66 90.20\nTable 13: Comparisons with more distillation methods.\nFigure 4: Performance of FreeAL on the training set at\ndifferent rounds during collaborative training.\nB.2 More Implementation Details\nIn our experiments, for the LLM API, we follow\nthe default official settings of the GPT-3.5-Turbo-\n0301 version. In the demonstration retrieval of ICL,\nwe adopt the unsupervised embeddings with bert-\nbase-uncased at the initial annotation round and the\nembeddings of SLM for later rounds. The construc-\ntion and retrieval of Ddemo are both performed in a\nclass-wise manner to compose the final demonstra-\ntions. For the robust self-training of SLM, we adopt\nthe hyperparameters either from previous works or\nfixed at a moderate value empirically without care-\nful tuning. We finetune the SLM on the basis of the\ntrainer of Huggingface for 50 epochs. The batch\nsize is fixed at 32 with a maximum sequence length\nof 128. We adopt the AdamW optimizer with a\nlearning rate selected from {3e−4,3e−5,3e−6}\nand a weight decay of 0.01. For robust self-training,\nthe threshold τof GMM selection is fixed at 0.7 and\nthe ratio Rof demonstration selection is fixed at\n20. The loss weight parameter αis linearly ramped\nup from 0 to 1 to avoid overfitting false labels at\nthe start. For evaluation of performance for LLM,\nas LLMs sometimes output ambiguous predictions\noutside the label space, these values are treated\nas random labels in the label space and repeated\nmultiple times to evaluate the average performance\n14534\nStep Prompt Details\nDemonstration Generation\nYou are required to produce 100 English examples with labels for the task\nof text classification on the MR (Movie Review) dataset. These samples\nwill be used as prompt examples for the GPT model. MR dataset is\nused in sentiment-analysis experiments and this dataset contains movie-\nreview documents labeled with respect to their overall sentiment polarity\n(positive or negative). The task is to classify a movie review as positive\nor negative according to their overall sentiment polarity. For example,\n100 of the unlabeled samples in MR dataset are as follows: [\"review\":\n\"enigma is well-made , but it’s just too dry and too placid .\"] [\"review\":\n\"the weakest of the four harry potter books has been transformed into the\nstronger of the two films by the thinnest of margins .\"] ......\nActive Annotation\nYou are a helpful assistant for the task of text classification on the MR\n(Movie Review) dataset. You reply with brief, to-the-point answers with\nno elaboration as truthfully as possible. MR (Movie Review) dataset is\nused in sentiment-analysis experiments and this dataset contains movie-\nreview documents labeled with respect to their overall sentiment polarity\n(positive or negative). Your task is to a binary classification to classify a\nmovie review as positive or negative according to their overall sentiment\npolarity. The category is divided into two types: ’positive’ and ’negative’.\nGiven a movie review: <QUERY>. How do you feel about the sentiment\npolarity of the given movie review, is this positive or negative? please\nanswer in a single line with ’positive’ or ’negative’.\nTable 14: An example of prompt design on the MR dataset for the step of demonstration generation and active\nannotations. The in-context examples are omitted for the active annotation process here.\nFigure 5: Comparisons of FreeAL with traditional active\nlearning (AL) algorithms and supervised fine-tuning on\nthe MR dataset. FreeAL surpasses all the active learning\nrivals and achieves near-supervised performance with-\nout human annotation.\nduring evaluation. Then in subsequent rounds, the\nSLMs adopt their own previous predictions to re-\nplace these ambiguous annotations of LLMs for\nrobust self-training. For token-level tasks, as the\nselection process is performed on the token level\nin a different manner, we select those tokens with\nhigh confidence and matched predictions to pseudo-\nlabels as clean and then filter out those samples\nwhose tokens are all clean to constitute the clean\nsubset. The consistency regularization and mixup\nloss are only suitable for the sequence-level tasks\nand are disabled in the token-level NER tasks.\nB.3 Prompt Design\nWe provide our prompt design on the MR dataset\nfor the initial demonstration generation step and ac-\ntive annotation step in Table 14. Notably, we adopt\nthe GPT-3.5-Turbo as our LLM so the prompts are\nalso in the chat style with instructions.\n14535",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7329657077789307
    },
    {
      "name": "Context (archaeology)",
      "score": 0.7102994918823242
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6168947815895081
    },
    {
      "name": "Active learning (machine learning)",
      "score": 0.5544033050537109
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5276861190795898
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48559921979904175
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.4481028914451599
    },
    {
      "name": "Language model",
      "score": 0.4172949194908142
    },
    {
      "name": "Natural language processing",
      "score": 0.33086147904396057
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}