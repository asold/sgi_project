{
    "title": "Document Context Language Models",
    "url": "https://openalex.org/W2197913429",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4222075082",
            "name": "Ji, Yangfeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223012566",
            "name": "Cohn, Trevor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2361730617",
            "name": "Kong, Lingpeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221974744",
            "name": "Dyer, Chris",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223518938",
            "name": "Eisenstein, Jacob",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2207587218",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2950752421",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2110951295",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W2140676672",
        "https://openalex.org/W1916559533",
        "https://openalex.org/W2251356693",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2251849926",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2164492273",
        "https://openalex.org/W1518951372",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2154137718",
        "https://openalex.org/W1532325895",
        "https://openalex.org/W182831726",
        "https://openalex.org/W2949190276",
        "https://openalex.org/W2130942839"
    ],
    "abstract": "Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence.",
    "full_text": "Workshop track - ICLR 2016\nDOCUMENT CONTEXT LANGUAGE MODELS\nYangfeng Ji1, Trevor Cohn2, Lingpeng Kong3, Chris Dyer3 & Jacob Eisenstein1\n1 School of Interactive Computing, Georgia Institute of Technology\n2 Department of Computing and Information Systems, University of Melbourne\n3 School of Computer Science, Carnegie Mellon University\nABSTRACT\nText documents are structured on multiple levels of detail: individual words are re-\nlated by syntax, and larger units of text are related by discourse structure. Existing\nlanguage models generally fail to account for discourse structure, but it is crucial\nif we are to have language models that reward coherence and generate coherent\ntexts. We present and empirically evaluate a set of multi-level recurrent neural net-\nwork language models, called Document-Context Language Models (DCLMs),\nwhich incorporate contextual information both within and beyond the sentence.\nIn comparison with sentence-level recurrent neural network language models, the\nDCLMs obtain slightly better predictive likelihoods, and considerably better as-\nsessments of document coherence.\n1 I NTRODUCTION\nStatistical language models are essential components of natural language processing systems, such\nas machine translation (Koehn, 2009), automatic speech recognition (Jurafsky & Martin, 2000),\ntext generation (Sordoni et al., 2015) and information retrieval (Manning et al., 2008). Language\nmodels estimate the probability of a word for a given context. In conventional language models,\ncontext is represented by n-grams, so these models condition on a ﬁxed number of preceding words.\nRecurrent Neural Network Language Models (RNNLMs; Mikolov et al., 2010) use a dense vector\nrepresentation to summarize context across all preceding words within the same sentence. But\ncontext operates on multiple levels of detail: on the syntactic level, a word’s immediate neighbors\nare most predictive; but on the level of discourse and topic, all words in the document lend contextual\ninformation.\nRecent research has developed a variety of ways to incorporate document-level contextual informa-\ntion. For example, both Mikolov & Zweig (2012) and Le & Mikolov (2014) use topic information\nextracted from the entire document to help predict words in each sentence; Lin et al. (2015) propose\nto construct contextual information by predicting the bag-of-words representation of the previous\nsentence with a separate model; Wang & Cho (2015) build a bag-of-words context from the pre-\nvious sentence and integrate it into the Long Short-Term Memory (LSTM) generating the current\nsentence. These models are all hybrid architectures in that they are recurrent at the sentence level,\nbut use a different architecture to summarize the context outside the sentence.\nIn this paper, we explore multi-level recurrent architectures for combining local and global infor-\nmation in language modeling. The simplest such model would be to train a single RNN, ignoring\nsentence boundaries: as shown in Figure 1, the last hidden state from the previous sentence t−1 is\nused to initialize the ﬁrst hidden state in sentence t. In such an architecture, the length of the RNN\nis equal to the number of tokens in the document; in typical genres such as news texts, this means\ntraining RNNs from sequences of several hundred tokens, which introduces two problems:\nInformation decay In a sentence with thirty tokens (not unusual in news text), the contextual in-\nformation from the previous sentence must be propagated through the recurrent dynamics\nthirty times before it can reach the last token of the current sentence. Meaningful document-\nlevel information is unlikely to survive such a long pipeline.\nLearning It is notoriously difﬁcult to train recurrent architectures that involve many time\nsteps (Bengio et al., 1994). In the case of an RNN trained on an entire document, back-\npropagation would have to run over hundreds of steps, posing severe numerical challenges.\n1\narXiv:1511.03962v4  [cs.CL]  21 Feb 2016\nWorkshop track - ICLR 2016\nyt −1,1\nxt −1,1\nyt −1,2\nxt −1,2\nyt −1, M−1\nxt −1, M−1\nyt −1, M\nxt −1, M xt , 1\nyt ,1\nxt , 2 xt , N −1 xt , N\nyt ,2 yt , N−1 yt , N\nFigure 1: A fragment of document-level recurrent neural network language model (DRNNLM ). It is\nalso an extension of sentence-level RNNLM to the document level by ignoring sentence boundaries.\nIn this paper, we use multi-level recurrent structures to solve both of these problems, thereby suc-\ncessfully efﬁciently leveraging document-level context in language modeling. We present several\nvariant Document-Context Language Models (DCLMs), and evaluate them on predictive likelihood\nand their ability to capture document coherence.\n2 M ODELING FRAMEWORK\nThe core modeling idea of this work is to integrate contextual information from the RNN language\nmodel of the previous sentence into the language model of the current sentence. We present three\nalternative models, each with various practical or theoretical merits, and then evaluate them in sec-\ntion 4.\n2.1 R ECURRENT NEURAL NETWORK LANGUAGE MODELS\nWe start from a recurrent neural network language model (RNNLM ) to explain some necessary terms.\nGiven a sentence {xn}N\nn=1, a recurrent neural network language model is deﬁned as\nhn =g(hn−1,xn) (1)\nyn =softmax (Wohn + b) , (2)\nwhere xn ∈RK is the distributed representation of the n-th word, hn ∈RH is the corresponding\nhidden state computed from the word representation and the previous hidden state hn−1, and b\nis the bias term. K and H are the input and hidden dimension respectively. As in the original\nRNNLM (Mikolov et al., 2010), yn is a prediction of the (n+ 1)-th word in the sequence.\nThe transition function g(·) could be any nonlinear function used in neural networks, such as the\nelementwise sigmoid function, or more complex recurrent functions such as the LSTM (Hochreiter\n& Schmidhuber, 1997) or GRU (Chung et al., 2014). In this work, we use LSTM, as it consistently\ngives the best performance in our experiments. By stacking two LSTM together, we are able obtain\na even more powerful transition function, called multi-layer LSTM (Sutskever et al., 2014). In a\nmulti-layer LSTM, the hidden state from a lower-layer LSTM cell is used as the input to the upper-\nlayer, and the hidden state from the ﬁnal-layer is used for prediction. In our following models, we\nﬁx the number of layers as two.\nIn the rest of this section, we will consider different ways to employ the contextual information for\ndocument-level language modeling. All models obtain the contextual representation from the hidden\nstates of the previous sentence, but they use this information in different ways.\n2.2 M ODEL I: C ONTEXT -TO-CONTEXT DCLM\nThe underlying assumption of this work is that contextual information from previous sentences\nneeds to be able to “short-circuit” the standard RNN, so as to more directly impact the generation\nof words across longer spans of text. We ﬁrst consider the relevant contextual information to be the\nﬁnal hidden representation from the previous sentence t−1, so that,\nct−1 = ht−1,M (3)\nwhere M is the length of sentence t−1. We then create additional paths for this information to\nimpact each hidden representation in the current sentencet. Writing xt,nfor the word representation\nof the n-th word in the t-th sentence, we have,\nht,n =gθ (ht,n−1,s(xt,n,ct−1)) (4)\n2\nWorkshop track - ICLR 2016\nyt −1,1\nxt −1,1\nyt −1,2\nxt −1,2\nyt −1, M−1\nxt −1, M−1\nyt −1, M\nxt −1, M\nxt , 1\nyt ,1\nxt , 2 xt , N −1 xt , N\nyt ,2 yt , N−1 yt , N\n(a) CCDCLM\nyt −1,1\nxt −1,1\nyt −1,2\nxt −1,2\nyt −1, M−1\nxt −1, M−1\nyt −1, M\nxt −1, M xt , 1\nyt ,1\nxt , 2 xt , N −1 xt , N\nyt ,2 yt , N−1 yt , N (b) CODCLM\nFigure 2: Context-to-context and context-to-output DCLMs\nwhere gθ (·) is the activation function parameterized by θand s(·) is a function that combines the\ncontext vector with the input xt,n for the hidden state. In future work we may consider a variety of\nforms for this function, but here we simply concatenate the representations,\ns(xt,n,ct−1) = [xt,n,ct−1]. (5)\nThe emission probability for yt,n is then computed from ht,n as in the standard RNNLM (Equa-\ntion 2). The underlying assumption of this model is that contextual information should impact the\ngeneration of each word in the current sentence. The model therefore introduces computational\n“short-circuits” for cross-sentence information, as illustrated in Figure 2(a). Because information\nﬂows from one hidden vector to another, we call this the context-to-context Document Context\nLanguage Model, abbreviated CCDCLM.\nWith this speciﬁc architecture, the number of parameters is H(16H+ 3K+ 6) +V(H+ K+ 1),\nwhere H is the size of the hidden representation, K is the size of the word representation, and V is\nthe vocabulary size. The constant factors come with the weight matrices within a two-layer LSTM\nunit. This is in the same complexity class as the standard RNNLM. Special handling is necessary\nfor the ﬁrst sentence of the document. Inspired by the idea of sentence-level language modeling, we\nintroduce a dummy contextual representation c0 as a START symbol for a document. This is another\nparameter to be learned jointly with the other parameters in this model.\nThe training procedure of CCDCLM is similar to a conventional RNNLM: we move from left to\nright through the document and compute a softmax loss on each outputyt,n. We then backpropagate\nthis loss through the entire sequences.\n2.2.1 M ODEL II: C ONTEXT -TO-OUTPUT DCLM\nRather than incorporating the document context into the recurrent deﬁnition of the hidden state, we\ncan push it directly to the output, as illustrated in Figure 2(b). Let ht,n be the hidden state from a\nconventional RNNLM of sentence t,\nht,n = gθ(ht,n−1,xt,n) . (6)\nThen, the context vector ct−1 is directly used in the output layer as\nyt,n ∼softmax (Whht,n + Wcct−1 + b) (7)\nwhere ct−1 is deﬁned in Equation 3. Because the document context impacts the output directly, we\ncall this model the context-to-output DCLM(CODCLM). The modiﬁcation on the model archi-\ntecture from CCDCLM to CODCLM leads to a notable change on the number of parameters. The\ntotal number of parameters of CODCLM is H(13H+ 3K+ 6) +V(2H+ K+ 1). The difference\nof the parameter numbers between these CODCLM and CCDCLM is VH −3H2. Recall that V is\nthe vocabulary size and H is the size of latent representation, in most cases we have V ≥104 and\nH ≈102. Therefore V ≫Hin all reasonable cases, and CODCLM includes more parameters than\nCCDCLM in general.\nWhile the CODCLM has more parameters that must be learned, it has a potentially important com-\nputational advantage. By shifting ct−1 from hidden layer to output layer, the relationship of any two\nhidden vectors ht and ht′ from different sentences is decoupled, so that each can be computed in\nisolation. In a guided language generation scenario such as machine translation or speech recogni-\ntion — the most common use case of neural language models — this means that decoding decisions\nare only pairwise dependent across sentences. This is in contrast with the CCDCLM, where the\n3\nWorkshop track - ICLR 2016\ntying between each ht and ht+1 means that decoding decisions are jointly dependent across the\nentire document. This joint dependence may have important advantages, as it propagates contextual\ninformation further across the document; the CCDCLM and CODCLM thereby offer two points on\na tradeoff between accuracy and decoding complexity.\n2.3 A TTENTIONAL DCLM\nOne potential shortcoming of CCDCLM and CODCLM is the limited capacity of the context vec-\ntor, ct−1, which is a ﬁxed dimensional representation of the context. While this might sufﬁce for\nshort sentences, as sentences grow longer, the amount of information needing to be carried forward\nwill also grow, and therefore a ﬁxed size embedding may be insufﬁcient. For this reason, we now\nconsider an attentional mechanism, based on conditional language models for translation (Sutskever\net al., 2014; Bahdanau et al., 2015) which allows for a dynamic capacity representation of the con-\ntext.\nCentral to the attentional mechanism is the context representation, which is deﬁned separately for\neach word position in the output sentence,\nct−1,n =\nM∑\nm=1\nαn,mht−1,m (8)\nαn = softmax (an) (9)\nan,m = w⊤\na tanh (Wa1ht,n + Wa2ht−1,m) (10)\nwhere ct−1,n is formulated as a weighted linear combination of all the hidden states in the previous\nsentence, with weights αconstrained to lie on the simplex using the softmax transformation. Each\nweight αn,m encodes the importance of the context at position mfor generating the current word\nat n, deﬁned as a neural network with a hidden layer and a single scalar output. Consequently each\nposition in the generated output can ‘attend’ to different elements of the context sentence, which\nwould arguably be useful to shift the focus to make best use of the context vector during generation.\nThe revised deﬁnition of the context in Equation 8 requires some minor changes in the generat-\ning components. We include this as an additional input to both the recurrent function (similar to\nCCDCLM), and output generating function (akin to CODCLM), as follows\nht,n = gθ\n(\nht,n−1,\n[\nc⊤\nt−1,n,x⊤\nt,n\n]⊤)\n(11)\nyt,n ∼ softmax (Wotanh (Whht,n + Wcct−1,n + b)) (12)\nwhere the output uses a single hidden layer network to merge the local state and context, before\nexpanding the dimensionality to the size of the output vocabulary, using Wo. The extended model\nis named as attentional DCLM(ADCLM).\n3 D ATA AND IMPLEMENTATION\nWe evaluate our models with perplexity and document-level coherence assessment. The ﬁrst data\nset used for evaluation is the Penn Treebank (PTB) corpus (Marcus et al., 1993), which is a standard\ndata set used for evaluating language models (e.g., Mikolov et al., 2010). We use the standard split:\nsections 0-20 for training, 21-22 for development, and 23-24 for test (Mikolov et al., 2010). We\nkeep the top 10,000 words to construct the vocabulary, and replace lower frequency words with\nthe special token U NKNOWN . The vocabulary also includes two special tokens S TART and END to\nindicate the beginning and end of a sentence. In total, the vocabulary size is 10,003.\nTo investigate the capacity of modeling documents with larger context, we use a subset of the North\nAmerican News Text (NANT) corpus (McClosky et al., 2008) to construct another evaluation data\nset. As shown in Table 1, the average length of the training documents is more than 30 sentences.\nWe follow the same procedure to preprocess the dataset as for the PTB corpus, and keep the top\n15,000 words from the training set in the vocabulary. Some basic statistics of both data sets are\nlisted in Table 1.\n4\nWorkshop track - ICLR 2016\nAverage Document Length\n# Documents # Tokens # Sentences\nPTB Training 2,000 502 21\nDevelopment 155 516 22\nTest 155 577 24\nNANT Training 26,462 783 32\nDevelopment 148 799 33\nTest 2,753 778 32\nTable 1: Basic statistics of the Penn Treebank (PTB) and North American News Text (NANT) data\nsets\n3.1 I MPLEMENTATION\nWe use a two-layer LSTM to build the recurrent architecture of our document language models,\nwhich we implement in the CNN package (https://github.com/clab/cnn). The rest of\nthis section includes some additional details of our implementation, which is available online at\nhttps://github.com/jiyfeng/dclm.\nInitialization All parameters are initialized with random values drawn from the range\n[−\n√\n6/(d1 + d2),\n√\n6/(d1 + d2)], where d1 and d2 are the input and output dimensions of the pa-\nrameter matrix respectively, as suggested by Glorot & Bengio (2010).\nLearning Online learning was performed using AdaGrad (Duchi et al., 2011) with the initial learn-\ning λ= 0.1. To avoid the exploding gradient problem, we used the norm clipping trick proposed by\nPascanu et al. (2012) and ﬁxed the norm threshold as τ = 5.0.\nHyper-parameters Our models include two tunable hyper-parameters: the dimension of word\nrepresentation K and the hidden dimension of LSTM unit H. We consider the values\n{32,48,64,96,128,256}for both K and H. The best combination of K and H for each model\nis selected by the development sets via grid search. In all experiments, we ﬁx the hidden dimension\nof the attentional component in ADCLM as 48.\nDocument lengthAs shown in Table 1, the average length of documents is more than 500 tokens,\nwith extreme cases having over 1,000 tokens. In practice, we noticed that training on long documents\nleads to a very slow convergence. We therefore segment documents into several non-overlapping\nshorter documents, each with at mostLsentences, while preserving the original sentence order. The\nvalue of Lused in most experiments is 5, although we compare with L= 10in subsection 4.1.\n4 E XPERIMENTS\nWe compare the three DCLM-style models ( CCDCLM, CODCLM, ADCLM) with the following\ncompetitive alternatives:\nRecurrent neural network language model (RNNLM ) The model is trained on individual sen-\ntences without any contextual information (Mikolov et al., 2010). The comparison between\nour models and this baseline system highlights the contribution of contextual information.\nRNNLM w/o sentence boundary (DRNNLM ) This is a straightforward extension of sentence-level\nRNNLM to document-level, as illustrated in Figure 1. It can also be viewed a conventional\nRNNLM without considering sentence boundaries. The difference between R NNLM and\nDRNNLM is that DRNNLM is able to consider (a limited amount of) extra-sentential context.\nHierarchical RNNLM (HRNNLM ) We also adopt the model architecture of HRNNLM (Lin et al.,\n2015) as another baseline system, and reimplemented it with several modiﬁcations for a\nfair comparison. Comparing to the original implementation (Lin et al., 2015), we ﬁrst re-\nplace the sigmoid recurrence function with a long short-term memory (LSTM) as used in\nDCLMs. Furthermore, instead of using pretrained word embedding, we update word rep-\nresentation during training. Finally, we jointly train the language models on both sentence-\nlevel and document-level. These changes resulted in substantial improvements over the\n5\nWorkshop track - ICLR 2016\nPTB NANT\nModel Dev Test Dev Test\nBaselines\n1. RNNLM (Mikolov et al., 2010) 69.24 71.88 109.48 194.43\n2. RNNLM w/o sentence boundary (DRNNLM ) 65.27 69.37 101.42 181.62\n3. Hierarchical RNNLM (HRNNLM ) (Lin et al., 2015) 66.32 70.62 103.90 175.92\nOur models\n4. Attentional DCLM (ADCLM) 64.31 68.32 96.47 170.99\n5. Context-to-output DCLM (CODCLM) 64.37 68.49 95.10 173.52\n6. Context-to-context DCLM (CCDCLM) 62.34 66.42 96.77 172.88\nTable 2: Perplexities of the Penn Treebank (PTB) and North American News Text (NANT) data\nsets.\n0 20 40 60 80 100 120 140\n# Updates ( × 50)\n6.5\n6.0\n5.5\n5.0\n4.5\n4.0\nNormalized Log-likelihood Length Threshold = 5\nLength Threshold = 10\nFigure 3: Effect of length thresholds on predictive log-likelihood on the PDTB development set.\noriginal version of theHRNNLM ; they allow us to isolate what we view as the most substan-\ntive difference between the DCLM and this modeling approach, which is how contextual\ninformation is identiﬁed and exploited.\n4.1 P ERPLEXITY\nTo make a fair comparison across different models, we follow the conventional way to compute\nperplexity. Particularly, the S TART and END tokens are only used for notational convenience. The\nEND token from the previous sentence was never used to predict the S TART token in the current\nsentence. Therefore, we have the same computation procedure on perplexity for the models with\nand without contextual information.\nTable 2 present the results on language modeling perplexity. The best perplexities are given by the\ncontext-to-context DCLM on the PTB data set (line 6 in Table 2), and attentional DCLM on the\nNANT data set (line 4 in Table 2). All DCLM-based models achieve better perplexity than the prior\nwork. While the improvements on the PTB dataset are small in an absolute sense, they consistently\npoint to the value of including multi-level context information in language modeling. The value of\ncontext information is further veriﬁed by the model performance on the NANT dataset. Of interest\nis the behavior of the attentional DCLM on two data sets. This model combines both the context-to-\ncontext and context-to-output mechanisms. Theoretically, ADCLMis considerably more expressive\nthen the CODCLM and CCDCLM. On the other hand, it is also complex to learn and innately favors\nlarge data sets.\nIn all the results reported in Table 2, the document length threshold was ﬁxed as L = 5, meaning\nthat documents were partitioned into subsequences of ﬁve sentences. We were interested to know\nwhether our results depended on this parameter. Taking L = 1would be identical to the standard\nRNNLM, run separately on each sentence. To test the effect of increasingL, we also did an empirical\n6\nWorkshop track - ICLR 2016\ncomparison between L= 5and L= 10with CCDCLM. Figure 3 shows the two curves on the PTB\ndevelopment set. The x-axis is the number of updates on CCDCLM with the PTB training set.\nThe y-axis is the mean per-token log-likelihood given by Equation 2 on the development set. As\nshown in this ﬁgure, L = 10seems to learn more quickly per iteration in the beginning, although\neach iteration is more time-consuming, due to the need to backpropagate over longer documents.\nHowever, after a sufﬁcient number of updates, the ﬁnal performance results are nearly identical,\nwith a slight advantage to the L = 5 setting. This suggests a tradeoff between the amount of\ncontextual information and the ease of learning.\n4.2 L OCAL COHERENCE EVALUATION\nThe long-term goal of coherence evaluation is to predict which texts are more coherent, and then\nto optimize for this criterion in multi-sentence generation tasks such as summarization and machine\ntranslation. A well-known proxy to this task is to try to automatically distinguish an original docu-\nment from an alternative form in which the sentences are scrambled (Barzilay & Lapata, 2008; Li &\nHovy, 2014). Multi-sentence language models can be applied to this task directly, by determining\nwhether the original document has a higher likelihood; no supervised training is necessary.\nWe adopt the speciﬁc experimental setup proposed by Barzilay & Lapata (2008). To give a robust\nmodel comparison with the limited number of documents available in the PTB test set, we employ\nbootstrapping (Davison & Hinkley, 1997). First, a new test set D(ℓ) is generated by sampling the\ndocuments from the original test set with replacement. Then, we shufﬂed the sentences in each\ndocument d ∈ D(ℓ) to get a pseudo-document d′. The combination of d and d′ is a single test\nexample. We repeated the same procedure to produce 1,000 test sets, where each test set includes\n155 pairs — one for each document in the PTB test set. Since each test instance is a pairwise choice,\na random baseline will have expected accuracy of 50%.\nTo evaluate the models proposed in this paper, we use the conﬁguration with the best development set\nperplexity, as shown in Table 2. The results of accuracy and standard deviation are calculated over\n1,000 resampled test sets. As shown in Table 3, the best accuracy is 83.26% given by CCDCLM,\nwhich also gives the smallest standard deviation 3.77%. Furthermore, all DCLM-based models\nsigniﬁcantly outperform the RNNLM with p <0.01 given by a two-sample one-side z-test on the\nbootstrap samples. In addition, the CCDCLM and CODCLM are outperform the HRNNLM with\np< 0.01 with statistic z= 36.55 and 31.26 respectively.\nIn addition, we also evaluated the models trained on the NANT dataset on this coherence evaluation\ntask. With the same 1,000 test sets, the best accuracy number across different models is 72.85%\nobtained from CODCLM. Compare to the results in Table 3, we believe the performance drop is\ndue to the domain mismatch. Even though PTB and NANT are both corpora collecting from news\narticles, they have totally different distributions on words, sentence lengths and even document\nlengths as shown in Table 1.\nUnlike some prior work on coherence evaluation (Li & Hovy, 2014; Li et al., 2015; Lin et al.,\n2015), our approach is not trained on supervised data. Supervised training might therefore improve\nperformance further. However, we emphasize that the real goal is to make automatically-generated\ntranslations and summaries more coherent, and we should therefore avoid overﬁtting on this artiﬁcial\nproxy task.\n5 R ELATED WORK\nNeural language models (NLMs) learn the distributed representations of words together with the\nprobability function of word sequences. In the NLM proposed by Bengio et al. (2003), a feed-\nforward neural network with a single hidden layer was used to calculate the language model prob-\nabilities. One limitation of this model is only ﬁxed-length context can be used. Recurrent neu-\nral network language models (RNNLMs) avoid this problem by recurrently updating a hidden\nstate (Mikolov et al., 2010), thus enabling them to condition on arbitrarily long histories. In this\nwork, we make a further extension to include more context with a recurrent architecture, by allow-\ning multiple pathways for historical information to affect the current word. A comprehensive review\nof recurrent neural networks language models is offered by De Mulder et al. (2015).\n7\nWorkshop track - ICLR 2016\nAccuracy\nModel Mean (%) Standard deviation (%)\nBaselines\n1. RNNLM w/o sentence boundary (DRNNLM ) 72.54 8.46\n2. Hierarchical RNNLM (HRNNLM ) (Lin et al., 2015) 75.32 4.42\nOur models\n3. Attentional DCLM (ADCLM) † 75.51 4.12\n4. Context-to-output DCLM (CODCLM) †∗ 81.72 3.81\n5. Context-to-context DCLM (CCDCLM) †∗ 83.26 3.77\n†signiﬁcantly better than DRNNLM with p-value < 0.01\n∗signiﬁcantly better than HRNNLM with p-value < 0.01\nTable 3: Coherence evaluation on the PTB test set. The reported accuracies are calculated from\n1,000 bootstrapping test sets (as explained in text).\nConventional language models, including the models with recurrent structures (Mikolov et al.,\n2010), limit the context scope within a sentence. This ignores potentially important information from\npreceding text, for example, the previous sentence. Targeting speech recognition, where contextual\ninformation may be especially important, Mikolov & Zweig (2012) introduce the topic-conditioned\nRNNLM , which incorporates a separately-trained latent Dirichlet allocation topic model to capture\nthe broad themes of the preceding text. Our focus here is on discriminatively-trained end-to-end\nmodels.\nLin et al. (2015) recently introduced a document-level language model, called hierarchical recurrent\nneural network language model (HRNNLM ). As in our approach, there are two channels of informa-\ntion: a RNN for modeling words in a sentence, and another recurrent model for modeling sentences,\nbased on a bag-of-words representation of each sentence. (Contemporaneously to our paper, Wang\n& Cho (2015) also construct a bag-of-words representation of previous sentences, which they then\ninsert into a sentence-level LSTM.) Our modeling approach is more uniﬁed and compact, employ-\ning a single recurrent neural network architecture, but with multiple channels for information to feed\nforward into the prediction of each word. We also go further than this prior work by exploring an\nattentional architecture (Bahdanau et al., 2015).\nMoving away from the speciﬁc problem of language modeling, we brieﬂy consider other approaches\nfor modeling document content. Li & Hovy (2014) propose to use a convolution kernel to summarize\nsentence-level representations for modeling a document. The model is for coherence evaluation, in\nwhich the parameters are learned via supervised training. Related convolutional architectures for\ndocument modeling are considered by Denil et al. (2014) and Tang et al. (2015). Encoder-decoder\narchitectures provide an alternative perspective, compressing all the information in a sequence into\na single vector, and then attempting to decode the target information from this vector; while this\nidea has notably applied in machine translation (Cho et al., 2014), it can also be employed for\ncoherence modeling (Li et al., 2015). The hierarchical sequence-to-sequence model of Li et al.\n(2015) conditions the start word of each sentence on contextual information provided by the encoder,\nbut does not apply this idea to language modeling. Different from the models with hierarchical\nstructures, paragraph vector (Le & Mikolov, 2014) encodes a document to a numeric vector by\ndiscarding document structure and only retaining topic information.\n6 C ONCLUSION\nContextual information beyond the sentence boundary is essential to document-level text generation\nand coherence evaluation. We propose a set of document-context language models (DCLMs), which\nprovide various approaches to incorporate contextual information from preceding texts. Empirical\nevaluation with perplexity shows that the DCLMs give better word prediction as language models, in\ncomparison with conventional RNNLMs; performance is also good on unsupervised coherence as-\nsessment. Future work includes testing the applicability of these models to downstream applications\nsuch as summarization and translation.\n8\nWorkshop track - ICLR 2016\nAcknowledgments This work was initiated during the 2015 Jelinek Memorial Summer Work-\nshop on Speech and Language Technologies at the University of Washington, Seattle, and was sup-\nported by Johns Hopkins University via NSF Grant No IIS 1005411, DARPA LORELEI Contract\nNo HR0011-15-2-0027, and gifts from Google, Microsoft Research, Amazon and Mitsubishi Elec-\ntric Research Laboratory. It was also supported by a Google Faculty Research award to JE.\nREFERENCES\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In ICLR, 2015.\nRegina Barzilay and Mirella Lapata. Modeling local coherence: An entity-based approach. Com-\nputational Linguistics, 34(1):1–34, 2008.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient\ndescent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic\nlanguage model. The Journal of Machine Learning Research, 3:1137–1155, 2003.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. In EMNLP, 2014.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nAnthony Christopher Davison and David Victor Hinkley. Bootstrap methods and their application,\nvolume 1. Cambridge university press, 1997.\nWim De Mulder, Steven Bethard, and Marie-Francine Moens. A survey on the application of re-\ncurrent neural networks to statistical language modeling. Computer Speech & Language, 30(1):\n61–98, 2015.\nMisha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, and Nando de Freitas. Modelling,\nVisualising and Summarising Documents with a Single Convolutional Neural Network. arXiv\npreprint arXiv:1406.3830, 2014.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.\nXavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In International conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nDan Jurafsky and James H Martin.Speech and language processing. Pearson Education India, 2000.\nPhilipp Koehn. Statistical Machine Translation. Cambridge University Press, 2009.\nQuoc Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. In ICML,\n2014.\nJiwei Li and Eduard Hovy. A model of coherence based on distributed sentence representation. In\nEMNLP, 2014.\nJiwei Li, Thang Luong, and Dan Jurafsky. A Hierarchical Neural Autoencoder for Paragraphs\nand Documents. In ACL-IJCNLP, pp. 1106–1115, Beijing, China, July 2015. Association for\nComputational Linguistics.\nRui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li. Hierarchical Recurrent Neural\nNetwork for Document Modeling. In EMNLP, pp. 899–907, Lisbon, Portugal, September 2015.\nAssociation for Computational Linguistics.\n9\nWorkshop track - ICLR 2016\nChristopher D Manning, Prabhakar Raghavan, and Hinrich Sch ¨utze. Introduction to Information\nRetrieval. Cambridge university press Cambridge, 2008.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\nDavid McClosky, Eugene Charniak, and Mark Johnson. BLLIP North American News Text, Com-\nplete. Linguistic Data Consortium, 2008.\nTomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.\nIn SLT, pp. 234–239, 2012.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In INTERSPEECH, 2010.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\nnetworks. arXiv preprint arXiv:1211.5063, 2012.\nAlessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell,\nJian-Yun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive\ngeneration of conversational responses. In NAACL, 2015.\nIlya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural net-\nworks. In NIPS, 2014.\nDuyu Tang, Bing Qin, and Ting Liu. Document Modeling with Gated Recurrent Neural Network\nfor Sentiment Classiﬁcation. In EMNLP, pp. 1422–1432, Lisbon, Portugal, September 2015.\nAssociation for Computational Linguistics.\nTian Wang and Kyunghyun Cho. Larger-Context Language Modelling. arXiv preprint\narXiv:1511.03729, 2015.\n10"
}