{
  "title": "Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images",
  "url": "https://openalex.org/W4200547174",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2621088709",
      "name": "Teerapong Panboonyuen",
      "affiliations": [
        "Chulalongkorn University"
      ]
    },
    {
      "id": "https://openalex.org/A2650032745",
      "name": "Kulsawasd Jitkajornwanich",
      "affiliations": [
        "King Mongkut's Institute of Technology Ladkrabang"
      ]
    },
    {
      "id": "https://openalex.org/A2288792227",
      "name": "Siam Lawawirojwong",
      "affiliations": [
        "National Science and Technology Development Agency"
      ]
    },
    {
      "id": "https://openalex.org/A72637010",
      "name": "Panu Srestasathiern",
      "affiliations": [
        "National Science and Technology Development Agency"
      ]
    },
    {
      "id": "https://openalex.org/A228581428",
      "name": "Peerapon Vateekul",
      "affiliations": [
        "Chulalongkorn University"
      ]
    },
    {
      "id": "https://openalex.org/A2621088709",
      "name": "Teerapong Panboonyuen",
      "affiliations": [
        "Chulalongkorn University"
      ]
    },
    {
      "id": "https://openalex.org/A2650032745",
      "name": "Kulsawasd Jitkajornwanich",
      "affiliations": [
        "King Mongkut's Institute of Technology Ladkrabang"
      ]
    },
    {
      "id": "https://openalex.org/A2288792227",
      "name": "Siam Lawawirojwong",
      "affiliations": [
        "National Science and Technology Development Agency"
      ]
    },
    {
      "id": "https://openalex.org/A72637010",
      "name": "Panu Srestasathiern",
      "affiliations": [
        "National Science and Technology Development Agency"
      ]
    },
    {
      "id": "https://openalex.org/A228581428",
      "name": "Peerapon Vateekul",
      "affiliations": [
        "Chulalongkorn University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3109998321",
    "https://openalex.org/W3179856054",
    "https://openalex.org/W2995766874",
    "https://openalex.org/W3130455691",
    "https://openalex.org/W3174867596",
    "https://openalex.org/W3025070692",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3128776197",
    "https://openalex.org/W3171853541",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W3203700770",
    "https://openalex.org/W3190334976",
    "https://openalex.org/W3157525179",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3174350209",
    "https://openalex.org/W3137684688",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2894878591",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W3015724987",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2963973518",
    "https://openalex.org/W2962731685",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3040725977",
    "https://openalex.org/W3183174367"
  ],
  "abstract": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8145860433578491
    },
    {
      "name": "Transformer",
      "score": 0.7462650537490845
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6614442467689514
    },
    {
      "name": "Segmentation",
      "score": 0.661439061164856
    },
    {
      "name": "Encoder",
      "score": 0.5719428658485413
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.49938201904296875
    },
    {
      "name": "Deep learning",
      "score": 0.4373878836631775
    },
    {
      "name": "Parsing",
      "score": 0.4338943362236023
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40228939056396484
    },
    {
      "name": "Computer vision",
      "score": 0.37596553564071655
    },
    {
      "name": "Voltage",
      "score": 0.09571480751037598
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}