{
  "title": "Refining Language Models with Compositional Explanations",
  "url": "https://openalex.org/W3169203618",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Yao, Huihan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099397746",
      "name": "Chen Ying",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281841293",
      "name": "Ye, Qinyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286422205",
      "name": "Jin, Xisen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127770351",
      "name": "Ren Xiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2996507500",
    "https://openalex.org/W3034218934",
    "https://openalex.org/W2951576127",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2953180101",
    "https://openalex.org/W2031342017",
    "https://openalex.org/W2053075547",
    "https://openalex.org/W2592141621",
    "https://openalex.org/W1999043044",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W22861983",
    "https://openalex.org/W2953462175",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2604821579",
    "https://openalex.org/W2948060161",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2946189081",
    "https://openalex.org/W2963798744",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2120708938",
    "https://openalex.org/W3034526587",
    "https://openalex.org/W2566376500",
    "https://openalex.org/W1588847699",
    "https://openalex.org/W2473555522",
    "https://openalex.org/W2022204871",
    "https://openalex.org/W2958514452",
    "https://openalex.org/W3023264755",
    "https://openalex.org/W3161695571",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2983693488",
    "https://openalex.org/W2949197630",
    "https://openalex.org/W2118045473",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2890727387",
    "https://openalex.org/W2155541015",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W3035215724",
    "https://openalex.org/W3022133124",
    "https://openalex.org/W2964186069",
    "https://openalex.org/W2612017161",
    "https://openalex.org/W3099540969",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2963275094",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102684260",
    "https://openalex.org/W3000642987",
    "https://openalex.org/W2336525064",
    "https://openalex.org/W2626667877",
    "https://openalex.org/W2963522845",
    "https://openalex.org/W2963028402",
    "https://openalex.org/W1496189301",
    "https://openalex.org/W2962680264",
    "https://openalex.org/W2962772482"
  ],
  "abstract": "Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.",
  "full_text": "Reﬁning Language Models with Compositional\nExplanations\nHuihan Yao1 Ying Chen2 Qinyuan Ye3 Xisen Jin3 Xiang Ren3\n1Peking University 2Tsinghua University 3University of Southern California\nyaohuihan@pku.edu.cn chenying17@mails.tsinghua.edu.cn\n{qinyuany,xisenjin,xiangren}@usc.edu\nAbstract\nPre-trained language models have been successful on text classiﬁcation tasks, but\nare prone to learning spurious correlations from biased datasets, and are thus vul-\nnerable when making inferences in a new domain. Prior work reveals such spurious\npatterns via post-hoc explanation algorithms which compute the importance of\ninput features. Further, the model is regularized to align the importance scores\nwith human knowledge, so that the unintended model behaviors are eliminated.\nHowever, such a regularization technique lacks ﬂexibility and coverage, since only\nimportance scores towards a pre-deﬁned list of features are adjusted, while more\ncomplex human knowledge such as feature interaction and pattern generalization\ncan hardly be incorporated. In this work, we propose to reﬁne a learned language\nmodel for a target domain by collecting human-provided compositional explana-\ntions regarding observed biases. By parsing these explanations into executable\nlogic rules, the human-speciﬁed reﬁnement advice from a small set of explanations\ncan be generalized to more training examples. We additionally introduce a regular-\nization term allowing adjustments for both importance and interaction of features\nto better rectify model behavior. We demonstrate the effectiveness of the proposed\napproach on two text classiﬁcation tasks by showing improved performance in\ntarget domain as well as improved model fairness after reﬁnement1.\n1 Introduction\nWith recent advances in model architectures and pre-training techniques, neural language models [4,\n31, 25] have achieved impressive results on a broad set of natural language processing (NLP) tasks,\nsuch as sentiment analysis and hate speech detection [3, 45]. However, when a source model (ﬁne-\ntuned on some upstream dataset) is applied to a target domain with a different data distribution, the\nmodel may suffer from poor performance due to some spurious feature patterns learned from the\nupstream dataset [33, 44, 6]. Moreover, some spurious patterns may cause unintended biases in the\ndownstream tasks, resulting in fairness and trust concerns about the model [21].\nPrior work suggests that humans can identify such spurious patterns through examining the visualized\n“heat-map” (Fig. 1) produced by a post-hoc model explanation algorithm [ 13]. As a prominent\nexample, feature attribution methods [ 42, 14, 19] interpret model prediction on an instance by\nassigning an importance (attribution) score to each input feature (or token, in the context of NLP\ntasks), which helps uncover an overemphasis or understatement of a speciﬁc feature (e.g., “Sweden” is\noveremphasized as indication of hate speech, as in Fig. 1). To alleviate these spurious patterns, recent\nattempts study model regularization methods that update models in a differentiable and incremental\nfashion, which looks to align feature attribution scores with the “intended” scores manually speciﬁed\nby human annotators [38, 35, 30]. For example, attribution scores on overemphasised, unintended\ntokens are decreased (to close to zero) through updating the model weights [21].\n1Code and data are available at https://github.com/INK-USC/expl-reﬁnement.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2103.10415v3  [cs.CL]  31 Dec 2021\nFigure 1: An illustration of post-hoc model ex-\nplanation heat-map for hate speech detection.\nA trained hate speech classiﬁer mis-classiﬁes the\nsentence as non-hateful. After observing the heat-\nmap, human annotators may suggest that “Swe-\nden” be adjusted to neutral, and “failure” should\ncontribute more to predicting hate speech.\nDespite these initial successes, existing model regu-\nlarization methods have limited capacity in conveying\ncomplex human feedback regarding spurious patterns,\nand limited regularization strength as the regulariza-\ntion term is enforced on only instances associated with\nhuman feedback (while the vast amount of unlabeled\ndata is not leveraged) [38, 36, 30, 47]. To pinpoint a\nspurious pattern precisely, an annotator needs to de-\nscribe it by composing multiple assertions regarding\nfeature attribution and feature interaction (e.g., “to be\na failure” modiﬁes “Sweden” in Fig. 1). However,\nprevious work consider only the former [36, 30] and\nomit the latter when characterizing spurious patterns.\nMoreover, reﬁning these data-hungry language models\noften requires large amount of labeled data. To extend the coverage of regularization, one must match\n(generalize) one human feedback to multiple (unlabeled) instances in the target domain – i.e., identify\ninstances that potentially suffer from the same spurious patterns, and then regularize the model with a\nlarger set of instances.\nTo this end, we introduce Reﬁning Language Model with Compositional Explanation (REMOTE ),\na framework that alleviates spurious patterns of a trained model and addresses the aforementioned\nlimitations, by soliciting complex and compositional explanations from human and reﬁning the\nmodel with broadened coverage during regularization (see Fig. 2 for an overview). Firstly, human\nannotators are shown the post-hoc explanations of the source model’s predictions on target domain\ndata (Fig. 1). They are asked to describe the spurious patterns they ﬁnd and their suggestions to adjust\nthe importance scores and interactions of features. Secondly, we extract executable ﬁrst-order logic\nrules from these human-provided compositional explanations. The execution of the logic rules are\ndecomposed to several atomic operations. We devise softened versions of these operations so the\nlogic rules provide noisy labels and reﬁnement advice to a larger number of instances in the target\ndomain. Lastly, we update model weights according to the suggestions in the explanations, using the\nenlarged set of instances obtained in the previous step.\nWe highlight two major contributions of the REMOTE framework. First, to the best of our knowledge,\nREMOTE is the ﬁrst work that studies gathering feature-level supervision from complex human\nexplanations. Prior work [15, 47] has explored producing pseudo labels from explanations (i.e., what\nis the correct label?), while we focus on more concrete feature-level supervision ( i.e., why is this\nlabel correct?) with the goal of reducing spurious patterns. Second, we quantify and regularize\nthe interaction between features, in addition to feature attributions used in prior work. This greatly\nimproves the expressiveness of human explanations by supporting more complex rationale that\ninvolves more than one feature.\nWe validate our approach on three pairs of datasets in hate speech classiﬁcation and sentiment analysis.\nCompared with direct transfer (evaluate the source model on target domain data) and other baselines\n(distillation and weight regularization), we observe notable performance improvements after reﬁning\nthe model with our proposed framework. In addition, we demonstrate that REMOTE can reduce\nunintended biases on group identiﬁers in hate speech detection.\n2 Related Work\nHuman-in-the-loop Learning. The idea of bringing human into the learning process to enhance\nthe model has been explored through multiple paths. One direction of prior work is to ask human\nannotators to label important instances ( i.e. active learning [ 39]) or to determine which model\nto use [9]. A more interpretable direction is to associate features with speciﬁc labels as a source\nof supervision [ 32], or let users suggest adjustments of low-level features by examining model\nexplanations [24, 43, 26]. These explanation-related methods are limited to relatively simple models\nincluding linear classiﬁers [24] and CNN [26] because the model explanations directly correspond to\nthe low-level features. With model-agnostic post-hoc explanation algorithms that explain predictions\nof a trained model without interfering with its learned weights [ 42, 19], our interactive machine\nlearning method enables inspection and feedback to models as complex as BERT. On the other hand,\nseveral works [15, 47, 51] argue that natural language explanations rather than numeric labels as\nhuman feedback provide a richer and more efﬁcient source of supervision for training. Our method\n2\nFigure 2: Overview for model reﬁnement with explanation regularization. Post-hoc explanation heat-maps\nare presented to annotators, and compositional explanations describing the model reﬁnement suggestions are\ncollected (Sec. 3.2). We parse them to ﬁrst-order rules and match unlabeled data in the target domain with a\nneural model (Sec. 3.3), and use them to reﬁne the model (Sec. 3.4).\nlearns from natural language in different setting, where we aim to adapt a trained model to another\ndomain. Another line of work has studied explanation regularization as an interpretable approach\nto impose prior knowledge into neural networks [ 38, 35, 36, 30, 7]. Compared with them, our\nwork incorporate complicated human knowledge conveyed by natural language and further proposes\nregularization on feature interaction to better take context into consideration.\nModel Transfer and Domain Adaptation. Sequential transfer learning, also known as model\ntransfer [46], considers a model being trained sequentially over different labeled datasets that are not\navailable at the same time. Existing model transfer methods update the model (using labeled data\nfrom target domain) with various ﬁne-tuning techniques – e.g., updating layer weights [8], creating\nlearning rate schedule [18], adding regularization [48, 28, 27], and model distillation [17, 37]. More\nrecent work also studies transferring models without labeled target data, known as unsupervised\nmodel adaptation [27], by generating target-like data [29], or learning target-speciﬁc features [27].\nOur work goes beyond supervision in the form of “instance-label” data and considers more complex\nhuman feedback on feature attribution and interaction. In another relevant thread, unsupervised\ndomain adaptation (UDA) looks to adapt a trained model to a new domain using unlabeled data from\nthe target domain, by updating feature representation to minimize the distribution divergence between\ndomains [12, 11], or updating models to match the distribution statistical moments at different\norders [41, 52, 34]. However, UDA methods typically require access to labeled instances from the\nsource domain, which are not available in our problem setting.\n3 Model Reﬁnement with Compositional Explanations\nWe study the problem of reﬁning a source model for better adapting to a new domain (Sec. 3.1). By\npresenting the post-hoc explanation heat-map computed on the target data for the source model, we\nsolicit from human annotators compositional explanations that describe what spurious patterns are\nobserved in the instances and how to adjust the feature attribution and interaction scores to alleviate\nthese spurious behaviors (Sec. 3.2). We aim to generalize the collected explanations to instances\nin the target domain (Sec. 3.3) and update model weights based on these explanation-generalized\ninstances (Sec. 3.4).\n3.1 Problem Formulation\nWe consider adapting a text classiﬁcation model fS trained on some source data Dtrain\nS (e.g., a\nsentiment classiﬁer trained on news articles) to a new target domain T (e.g., tweets) during a model\nreﬁnement stage. We focus on a challenging setting [28, 29] where the upstream labeled data Dtrain\nS\nis not available (e.g., due to privacy constraints [27], or access restriction [29]) but unlabeled data in\nthe target domain (DT) are readily accessible, during model reﬁnement. This setting also reﬂects the\ncommon practice nowadays, as users may download trained models from public model repositories\nand look to deploy the model to their own data. Our problem setting is different from (unsupervised)\ndomain adaptation [11, 34], where labeled source data (Dtrain\nS ) is available for making model update.\nOur work also distinguishes from traditional transfer learning [18, 28] which focuses on leveraging\nlabeled data in the form of “instance-label” pairs in the target domain [18], while we solicit human\nexplanations on model’s spurious patterns for adjusting feature attribution and interaction scores.\nWe evaluate model’s performance on target data (Dtest\nT ) and source data (Dtest\nS ) as the measurement\nof success. We expect the target performance to be improved, while the source performance to be\nmaximally preserved, after model reﬁnement. In addition, a reﬁned model should no longer rely on\n3\nspurious patterns (e.g., being over-sensitive to group identiﬁers for hate speech detection). Therefore,\nwe report False Positive Rate Difference (FPRD) on a synthetic dataset [5] as a fairness metric.\n3.2 Compositional Explanation for Model Reﬁnement\nIn the following, we deﬁne the format of compositional explanations used in our study, and the\nprocedure to solicit these explanations from annotators. Table 1: Three types of expressions for\ndescribing spurious patterns.\n1. Existence of a featureExample:X is “jews”. Y is “parasite”.\n2. Characteristic of a single feature\nDescription:Entity type, part-of-speech tag, sen-timent label of a word/phrase.Example:X is aPersonentity. X isverb. Y is apositiveword.\n3. Relation between featuresDescription:Semantic roles, co-reference, dis-tance, etc.Example:X is thesubjectof Y . X is two tokensaway from Y . X modiﬁes Y .\nCompositional Explanations. Our compositional expla-\nnations consist of two parts: spurious patterns and reﬁne-\nment advice. We deﬁne a spurious pattern as words or\nphrases in a sentence, whose attribution/interaction scores\ndo not align with human judgment. Annotators are re-\nquired to describe a spurious pattern precisely and sufﬁ-\nciently using three types of expressions: (1) existence of a\nfeature; (2) characteristic of a feature; (3) relation between\nfeatures. We list examples for these expressions in Ta-\nble 1. Annotators then provide reﬁnement advice, i.e., their\nadvice to increase or decrease the attribution/interaction\nscores of features. The annotator also provide a label for\nthe current instance. Compositionality in human explanations can not only improve the precision\nof matching instances, but enable annotators to describe their observations more ﬂexibly as well.\nWe provide concrete examples of the compositional explanations in Table 2 and “Compositional\nExplanation” block in Fig. 2.\nExplanation Solicitation. We ﬁrst use the source model fS to make predictions on a small set\nof unlabeled instances randomly sampled from DT and present these post-hoc explanation heat-\nmaps [19] to human annotators (see Fig. 1). When shown with an instance with its heat-map, human\nannotators will: 1) read the sentence and provide a label, 2) inspect the heat-map, and 3) write an\nexplanation if a spurious pattern is found. We estimate the time cost of each step and the ratio of\nﬁnding spurious patterns, and will elaborate it in Sec. 4.2. If the annotator identiﬁes a spurious pattern\nin the heat-map for instance xref and provides an explanation, we refer to xref as the reference\ninstance for the explanation. In this step, we have obtained a set of instances xref along with their\nraw human-provided explanations e, which we denote as E0 = {(xref,e)}.\n3.3 Generalizing Explanation in Target Domain\nWith the collected human explanations, now we detail how to parse the collected natural-language\nexplanations into executable logic rules B →H, and how to execute those logic rules by softening\ntheir constraints so that they can be generalized to multiple unlabeled instances in DT.\nExplanation Parsing. Each raw explanation eis parsed into a ﬁrst-order logic rule in the form of\nB →H. Here the description of the spurious pattern is parsed into rule body Band the reﬁnement\nadvice is parsed into rule head H (see “First-Order Logic Rule” in Fig. 2). We parse the raw\nexplanations using a semantic parser based on Combinatory Categorial Grammar (CCG) [53], which\ncan deal with natural language with linguistic variations and thus is friendly to annotators. To tailor\nthe parser to our needs, we deﬁne a lexicon str2predicateto map 301 common expressions (e.g.,\n“directly after”, “negative word”) to 83 predicates and implement the corresponding operations with\natomic modules (described in next paragraph). Annotators can iteratively modify their explanations\nor update the lexicon until they make sure their explanations are accepted by the parser. We denote\nthe collection of parsed explanations as E= {(xref,B,H )}. More details are in Appendix B.\nExplanation Generalization with Matching Model G. With Eobtained in previous steps, we now\naim at generalizing from one explanation (xref,B,H ) to multiple unlabeled instances in DT. That\nis, for each unlabeled instance xin DT, we attempt to match it with each spurious pattern B and\nthe reference instance xref we have obtained. If the matching is successful, we consider that xmay\nsuffer from a spurious pattern similar to B, and will use this instance in the later regularization phase.\nFor this purpose, we construct an executable matching model G from each rule body B. G is\ndynamically constructed from three atomic execution units (Individuality Module, Interaction Module\nand Compositionality Module), following the predicates in B. We introduce these modules as follows.\n4\nIndividuality module is used to output the feature ( i.e., word) in the unlabeled instance x that\ncorresponds to a feature qref in the reference instance xref. The module will ﬁrst search if an exact\nmatch of qref exists in xk. If an exact match does not exist, it will search for words of same type\nwith qref in xk, including named entity type, constituency parse structure, etc. We leave more details\nin Appendix E. If no feature is found after this step, Individuality module will return None.\nTable 2: An example of compositional explana-\ntion. The explanation characterizes where a spuri-\nous pattern is (in the instance), and how the attribu-\ntion/interaction scores should be regularized.\nInformation shown to Annotators:Reference Instance:They prove moredistressing than attractive.Information from Heat-map:Predicted label is positive. Attribu-tion score ofdistressingis low.Compositional ExplanationSpurious Pattern: X is “distressing”. Y is “than”. Z is “attrac-tive”. X is negative. Z is positive. X is immediately before Y . Y isimmediately before Z.Noisy Label:Negative.Reﬁnement Advice: Attribution score of X should be increased.Matched InstanceInstance:Self-ﬂagellation is moredepressing than entertaining.Noisy Label:Negative.Reﬁnement Advice:X is “depressing”. Attribution score of Xshould be increased.Soft-Matching Details:“distressing” and “de-pressing” are synonyms. “attractive” and “entertaining” are syn-onyms. The semantic similarity is captured by softenedIndividualitymodule.\nInteraction module examines whether the relation\nbetween features described in B holds true in the\nunlabeled instance xref. Given xref and two words\nqref,1,qref,2 in it, we ﬁrst call Individuality module\nto ﬁnd matched qk,1,qk,2 in xk, and then use Inter-\naction module to examine their relation. We use\neither natural distance (number of words between\nthe two words qref,1,qref,2) or dependency distance\n(distance between qref,1,qref,2 on the dependency\ntree), depending on the descriptions given by an-\nnotators. The former is applicable to explanations\nsuch as “X is within 3 words before Y”, and the lat-\nter is applicable to explanations such as “Feature\nX modiﬁes feature Y”. The Interaction module will\noutput True of False, indicating whether the relation\ndescribed in Bis satisﬁed in xref.\nCompositionality module is used to execute logic\noperations (i.e., “AND”, “OR”) speciﬁed in the ex-\nplanation. Based on intermediate output produced by other modules, the module will output True or\nFalse, indicating whether Bmatches with an unlabeled instance x.\nBroadening Coverage with Softened Matching. The matching process described above enforces\nconstraints in Bstrictly, e.g., the word “nice” will be rejected in an Individuality module looking for\nthe word “good”, despite their semantic similarity. To broaden the coverage ofstrict matching, we\npropose soft matching for each module, which relaxes the rule body and generates a larger number of\nmatched instances to enhance regularization strength.\nTable 2 provides a concrete example. Based on the reference sentence “They prove moredistressing\nthan attractive”, together with its explanation, a sentence will be matched if and only if it contains\ndistressing and attractive in strict matching. In contrast, in soft version, distressing can be generalized\nto depressing and attractive to entertaining.\nFor Individuality module, we allow synonyms, coreference and morphological/spelling variation of\nqref in xk using a neural model. Given a sentencexk = [w1,w2,...,w m], we ﬁrst encode the sentence\nwith a BERT-Base model [4] and obtain each token’s contextualized representations[v1,v2,...,v m].\nGiven a phrase qk = [wl1,...,w l2] in xk, we apply mean pooling over the token representations to\nget its phrase representation, i.e., ∑l2\nj=l1 vj/(l2 −l1). We compute the representation for qref in\nan analogous way. The softened individuality module will output a set of candidate spans whose\nrepresentations have the highest cosine similarity to qref’s.\nFor Interaction module, we denote the distance between qk,1 and qk,2 as d, and human-speciﬁed\ndistance constraint between qref,1 and qref,2 as dref. Instead of strictly following the distance\nconstraint, we compute a score indicating how close d ≤dref is to being correct: z = max(1−\n1\n4 ( d−dref\n|dref|+1 )2,0) if d>d ref and 1 otherwise.\nFor Compositionality module, soft logic / Lukasiewiczt logic [ 22] operations are used to aggre-\ngate two intermediate scores produced by other modules, i.e., AND(z1,z2) = max(z1 + z2 −\n1,0); OR(z1,z2) = min(z1 + z2,1).\nUsing the three atomic modules, we are able to generalize from human-provided explanations to more\nunlabeled instances in DT. After the matching process, a matched instance xk will be associated\nwith a noisy label yk, some reﬁnement advice rk, and a conﬁdence score zk. If strict matching is\nemployed, zk is set to one. If soft matching is employed, zk is computed from scores produced by\neach module. We use Dmatch\nT = G(DT) ={(xk,yk,rk,zk)}to denote this set of instances.\n5\n3.4 Learning with Explanation-Generalized Data\nObjective for Model Update. The learning objective for reﬁning fS is deﬁned as follows.\nL= L′+ α(Lattr + Linter), (1)\nwhere L′is the classiﬁcation loss using the noisy labels {yk}, Lattr and Linter are regularization\nterms computed using reﬁnement advice {rk}, and αis the hyperparameter to control the strength of\nthe regularization terms. We discuss the selection of αin Sec. 4.4.\nWith strict matching, the noisy labels Cstrict = {yk}and reﬁnement advice Rstrict = {rk}are\nless noisy; with soft matching, the noisy labels Csoft and reﬁnement advice Rsoft can cover more\ntraining data. As noisy labels and reﬁnement advice incorporate different information, it is preferable\nto decouple the matching process for the two components, such as using Rsoft with Cstrict.\nRegularization with Reﬁnement Advice. We denote the attribution and interaction scores produced\nby the source model before adjustment as φand ϕ. The reﬁnement advice rk contains a set of target\nscores tc\np or τc\np,q that suggest how the attribution or interaction scores of speciﬁc phrases should be\nadjusted. Target scores tc\np and τc\np,q of phrases p,q regarding class care set to 0 if it was suggested\nto decrease the score, and 1 if suggested to increase it. The regularization terms are the squared L2\ndistance between current and target importance and interaction scores, summed over all Cclasses\nand phrases p∈xk.\nLattr =\nC∑\nc\n∑\np∈xk\n(φc(p; xk) −tc\np)2; Linter =\nC∑\nc\n∑\n{p,q}∈xk\n(ϕc(p,q; xk) −τc\np,q)2. (2)\nWe consider two feature attribution methods for regularization - Integrated Gradient [ 42] and\nSampling and Occlusion [19]. Next, we brieﬂy introduce their formulations and our approach to\nquantify feature interactions.\nImportance Attribution Score Computation. Integrated Gradients (IG) computes an importance\nscore of a feature (word) wi as the integrated gradient along the straight line path from an input\nsentence xand a neutral baseline x′ = [w′\n1,w′\n2,...,w ′\nm] (e.g., a sequence of all padding tokens).\nFormally, the importance score of a word wi for a label cis written as, φc(wi; x) = (wi −w′\ni) ·∫1\nα=0\n∂fc(x′+α·(x−x′))\n∂wi\n,where fc(·) is the model prediction score for the class c.\nSampling and occlusion (SOC) assigns importance score of a phrase p = [wi,...,w j] (one or a\nsequence of words) as the expectation of the prediction change under context replacement within\na ﬁx-sized neighboring region δ. We use fc(x−δ; ˆxδ) to denote model prediction when the words\nin xwithin δ region are replaced with sampled ˆxδ; and use fc(x−{δ,p}; ˆxδ; 0p) to further denote\nthe model prediction when the phrase pis replaced with padding tokens 0p. The importance of the\nphrase pin the input xis computed as, φc(p; x) = 1\n|S|\n∑\nˆxδ∈S[fc(x−δ; ˆxδ) −fc(x−{δ,p}; ˆxδ; 0p)],\nwhere ˆxδ are replaced neighboring words sampled from a replacement set S(e.g., sampled from a\nlanguage model pre-trained on the training set). Speciﬁcally, with the neighboring range δset as 0,\nφc(p; x) =fc(x) −fc(x−p; 0p),the explanation is the same as input occlusion (or leave-one-out)\nalgorithm [54], which is the prediction difference between erasing and keeping pin the sentence.\nQuantifying Feature Interactions. We borrow the deﬁnition of interaction from the cooperative\ngame theory [10]. Under this deﬁnition, interaction describes how importance of a phrase changes\nwhen other words or phrases are absent or present. Based on the deﬁnition, we deﬁne the interaction\nscore between two phrases pand qfor predicting a class cas\nϕc(p,q; x) =φc(p; x) −φc\n−q(p; x), (3)\nwhere φc\n−q(p,x) denotes the importance score of pafter masking the phrase qfrom the sentence.\n4 Experiments\n4.1 Experiment Setup\nDatasets. For hate speech detection, we use Stormfront [2] and HatEval [1] as upstream datasets, and\nthe Gab Hate Corpus (GHC) [20] as the downstream dataset. Stormfront is a corpus collected from\nthe white supremacist website and HatEval is the ofﬁcial Hate Speech dataset from SemEval-2019.\nOur two upstream datasets contain shorter and simpler sentences than those of GHC, which was\n6\nTable 3: Statistics for explanation solicitation and generalization. For each dataset pair, we report\nthe annotation information (total time, numbers of labels and explanations written in corresponding\ntime, and explanation yield rate). We also provide the number and precision of matched instances in\nstrict and soft version, and the size of negative sampling instances in hate speech detection. Finally,\nwe include size of Csample, the sets of instances with ground truth labels, which we sample from\ntarget domain based on same time cost and use in baseline methods.\nDataset Pair Total Time |labeled| |E|Exp. Yield Rate|strict| Prec. ofstrict|soft| Prec. ofsoft |balanced| |sample|\nHatEval→GHC 80 mins 212 34 16% 329 0.751 370 0.692 1400 394Stormfront→GHC 94 mins 285 40 14% 237 0.717 278 0.658 1600 464\nAmazonMusic→SST-2 15 mins 47 29 62% 1308 0.942 1737 0.917 0 204\ncollected from a social network with a high rate of hate speech. For sentiment analysis, we ﬁrst\ntrain on AmazonMusic [ 16], and apply the model to the Stanford Sentiment Treebank-2 (SST-2)\ndataset [40]. Details of the datasets are described in Appendix A.\nCompared Methods. We consider two variants ofREMOTE in the experiments: (i) using only Rsoft,\nand (ii) both Rsoft and Cstrict. We include more experiments REMOTE using both Rsoft and Csoft\nin Appendix D. Detailed experiment settings about hyper-parameters are included in Appendix A.\nWe compare our method with the following two lines of works. (1) Model transfermethods: L2\nregularization [28] places a penalty on the difference between the source and the new model\nparameters. Distillation method [37] adds a loss between prediction of source model and new\nmodel to encourage behavior reservation. (2) Explanation-based learning methods, including\nBabbleLabble [15] and NExT [47], use human explanations to generate pseudo-labeled data for\nmodel updating. Since our explanation design is different from [15, 47], their matching process is\nnot directly applicable. We adopt our strict and soft matching method to generate pseudo-labeled\ndata and conduct an ablation study (Sec. 4.4) to compare with: ﬁne-tune with strict-matched data\nCstrict (as BabbleLabble), and ﬁne-tune with soft-matched data Csoft (as NExT). More experiments\ncompared with other unsupervised model transfer methods [29] are included in Appendix D.\nAll methods other than REMOTE are trained with only labeled instances from DT. To form fair\ncomparison with the baselines (when annotation time is controlled), we collect a set of labeled\ninstances (denoted as Csample) in the following way: we ﬁrst estimate the amount of instances that\ncost the same annotation time as the time used in collecting the explanations (detailed time cost is\ndiscussed in Sec. 4.2); then we randomly sample from DT this amount of examples along with their\nground-truth labels. In addition, we report performance of the source model ﬁne-tuned over the fully\nlabeled DT set (denoted as Call), referred as ﬁne-tune (Call). As an “oracle method”, it provides a\nreference on how much space is left for improvement for source model.\nEvaluation Metrics. In addition to F1-scores on source domain and target domain test sets, we also\nevaluate the hate-speech classiﬁers on the Identity Phrase Templates Test Set [5] (77,000 samples,\nwith 50% of them as “toxic”). We evaluate the unintended biases for identity phrasesT by computing\nthe False Positive Rate Difference (FPRD) [5] as ∑\nt∈T |FPR −FPRt|, where FPRt is the false\npositive rate computed upon the subset of samples that contain the term t. A smaller value in FPRD\nsuggests that the model is less biased.\n4.2 Explanation Collection and Generalization\nExplanation Solicitation and Time Cost. In Sec. 3.2 we introduce three steps in explanation\nsolicitation - providing a label, inspecting the heat-map, writing the explanation. We use SOC [19],\na post-hoc explanation algorithm to generate heat-maps for annotators, and estimate the time cost\nfor each step during annotation via our self-designed explanation solicitation interface (Sec. F). It\ntakes on average 12/5/25 seconds for the three steps respectively to collect explanations for hate\nspeech detection, and 4/2/14 seconds for sentiment analysis. We present annotation information in\ndetails on three data pairs in Table 3. In hate speech detection, due to difference in toxicity ratios\nbetween upstream and downstream datasets, we only collect explanations with noisy label as hateful.\nWe defer the discussion on this choice to the case study on hate/nonhate-labeled rules in Sec. 4.4.\nTo balance the label distribution, we randomly sample some instances in target domain as non-hate\nexamples Cbalanced, the sizes of which are also included in Table 3. Experiment shows that model\nperformance is not sensitive to the different numbers and sampling strategies for negative samples\n(see Appendix D). Our explanations are annotated by three CS graduate students with hourly wage as\n$20. To obtain high-quality explanations, two annotators need to verify the explanations written by\nthe other annotator. Full agreement among annotators happened 90+% of time.\n7\nTable 4: Main Results on three pairs of datasets with different language models. We report F1 scores on\nsource domain and target domain with standard deviation, and FPRD on IPTTS as fairness metric for hate speech\ntask. For each setting, we reﬁne the source model with best F1, and run experiment in three random seeds\ncontrolling the order of the data during ﬁne-tuning. The annotation time cost of each dataset pair is provided.\nBest results are bold.\nDataset HatEval→GHC(80 mins) Stormfront→GHC(94 mins) AmazonMusic→SST-2(15 mins)\nMetrics Source F1 (↑) Target F1 (↑) FPRD (↓) Source F1(↑) Target F1 (↑) FPRD (↓) Source F1 (↑) Target F1 (↑)\nBERT-Large\nSource model 63.7±0.5 31.4±1.4 124.3 59.5±1.1 41.9±1.4 17.1 92.9±0.2 87.7±1.0Fine-tune (Csample) 60.8±2.5 40.7 ±0.6 175.1 49.6±1.8 45.0 ±3.1 24.3 91.4±1.6 88.5 ±1.1L2-reg. (Csample) 58.1±2.9 41.8 ±1.8 102.2 49.9±1.8 45.3 ±1.9 12.2 90.5±1.1 88.9 ±1.6Distillation (Csample) 63.1±2.5 43.3 ±2.0 132.7 46.5±2.3 48.8 ±1.1 23.4 91.9±0.3 89.4 ±0.4REMOTE(Rsoft) 61.5±0.2 37.5 ±0.7 55.5 56.4±2.0 45.7 ±0.9 0.5 92.7±0.1 89.4 ±0.2REMOTE(Rsoft+Cstrict) 62.0±0.4 46.1±1.0 15.3 49.0±3.4 52.2±0.4 10.0 92.7±0.2 90.3±0.2\nFine-tune (Call) 51.3±5.6 52.5 ±0.4 98.0 46.0±3.8 53.8 ±1.6 142.3 92.5±0.2 94.4 ±0.4\nRoBERTa-Base\nSource model 62.7±0.9 30.9±1.9 61.6 57.4±1.2 39.6 ±1.2 43.8 92.4±0.4 87.5±0.9Fine-tune (Csample) 61.3±3.0 40.8 ±1.2 185.2 56.7±2.6 40.3 ±1.9 20.7 91.0±0.9 89.0 ±0.6L2-reg. (Csample) 62.4±2.0 42.2 ±0.8 292.7 48.8±1.5 41.7 ±0.5 46.2 90.9±1.0 89.0 ±0.6DistillationCsample) 61.8±1.4 42.1 ±1.2 152.5 48.4±1.6 40.9 ±0.3 28.1 91.3±0.5 89.1 ±0.5REMOTE(Rsoft) 61.1±0.6 40.5 ±1.1 15.9 57.0±1.0 40.9 ±0.6 36.7 92.0±0.2 88.5 ±0.7REMOTE(Rsoft+Cstrict) 57.5±0.9 44.7±1.0 97.8 57.6±1.9 50.1 ±1.7 77.5 91.4±0.2 89.5±0.5\nFine-tune (Call) 51.4±3.2 50.6 ±0.4 263.2 52.2±4.9 50.5 ±1.5 294.0 91.2±0.0 95.1 ±0.4\nQuality Check of Explanation Generalization. We generalize the collected explanations on unla-\nbeled instances in target domain in strict and soft version. To best utilize the collected explanations,\nwe tune the threshold of conﬁdence score in soft matching process from 0.5 to 0.9 based on the\nperformance on dev sets, and set them as 0.7, 0.55 and 0.6 for Stormfront →GHC, HatEval →GHC\nand Amazon →SST-2 respectively. To prove the effectiveness of our matching neural model, we\nevaluate the matching quality by the precision of noisy labels. The statistics are shown in Table 3.\n4.3 Performance Comparison\nIn comparisons below, we keep the total human annotation time per model ﬁxed, by controlling the\nnumber of explanations in REMOTE , and the number of labeled instances in model transfer methods.\nComparison with Model Transfer Baselines.Table 4 shows results on three source→target dataset\npairs with source models trained using BERT-Large [4] and RoBERTa-Base [31]. REMOTE with\nRsoft + Cstrict outperforms all other methods that have access to Csample with the same annotation\ntime on target performance. This demonstrates that REMOTE is highly label-efﬁcient compared to the\nmodel transfer methods.\nComparison between R EMOTE variants. REMOTE with just Rsoft shows notable target F1 im-\nprovement over the source models among all dataset pairs and language models. On top of that,\nREMOTE with Rsoft + Cstrict performs consistently better than with just Rsoft. We can conclude\nthat our generated noisy labels are sufﬁciently accurate and important in a low-resource situation.\nTable 5: Signiﬁcant Test on Main Results. We report p-value\nbetween target F1 of REMOTE and each baseline method on every\ndataset pair in Table 4. The difference is regarded as statically\nsigniﬁcant when p≤0.05.\nDataset HatEval→GHC Stormfront→GHC Amazon→SST-2\nMetrics P-Value Sig. or notP-Value Sig. or notP-Value Sig. or not\nBERT-Large\nFine-tune (Csample) 0.0013 yes 0.0163 yes 0.0494 yesL2-reg (Csample) 0.0224 yes 0.0003 yes 0.2071 noDistillation (Csample) 0.0959 no 0.0073 yes 0.0252 yes\nRoBERTa-Base\nFine-tune (Csample) 0.0124 yes 0.0026 yes 0.3297 noL2-reg (Csample) 0.0278 yes 0.0012 yes 0.3297 noDistillation (Csample) 0.0449 yes 0.0008 yes 0.3827 no\nFairness and Source Domain Per-\nformance. REMOTE preserves source\nF1 better than ﬁne-tune (Call) in most\ncases. We also observe that REMOTE\nmitigates unintended bias (as FPRD\nvalues are reduced) and simultane-\nously achieves target F1 close to the\nperformance of ﬁne-tune ( Call). We\nattribute this to our design of RE-\nMOTE which allows annotators to pin-\npoint spurious patterns precisely with\ncompositional explanations, and our\nfeature-level regularization method\nwhich reﬁnes the model by teaching\nwhy an label is correct and reduces unintended biases.\nSigniﬁcance Test. To show the statistical signiﬁcance between our method and baseline methods, we\ndo an unpaired t-test between REMOTE and each baseline methods. We report the P-values in Table 5.\nWe found that the improvements brought by REMOTE is statistically signiﬁcant in most cases.\n8\nTable 6: Effectiveness of model regularization technique on BERT-Base. We compare REMOTE (Rsoft +\nCstrict) (based on IG and SOC) with other methods using Cstrict to show the effect of Rsoft .\nDataset HatEval→GHC Stormfront→GHC AmazonMusic→SST-2\nMetrics Source F1 (↑) Target F1 (↑) FPRD (↓) Source F1(↑) Target F1 (↑) FPRD (↓) Source F1 (↑) Target F1 (↑)\nSource model 64.2±0.3 29.5 ±2.5 115.6 57.2±0.7 42.1±1.5 16.0 91.4±0.4 83.5 ±2.5Fine-tune (Cstrict) [15] 60.3±1.4 45.1 ±2.2 80.2 42.0±1.6 49.0 ±0.5 59.2 90.7±0.1 86.0 ±0.6L2-reg (Cstrict) 62.7±1.1 46.3 ±0.2 77.1 46.5±0.7 49.9 ±0.8 86.4 90.7±0.3 86.8 ±0.6Distillation (Cstrict) 63.2±1.0 46.3±0.9 65.4 46.4±1.3 49.4 ±1.1 49.7 90.6±0.5 86.7 ±1.0Fine-tune (Csoft) [47] 62.4±1.8 45.4 ±2.0 57.9 49.6±4.1 47.9 ±0.6 164.0 90.4±0.9 86.3 ±0.4REMOTE(Rsoft+Cstrict) w. IG 64.2±0.4 47.2 ±1.3 129.5 51.4±4.6 49.5 ±1.1 12.8 91.2±0.1 87.0±0.7REMOTE(Rsoft+Cstrict) w. SOC63.2±0.6 46.6 ±1.1 49.0 49.4±1.9 51.1±1.6 34.6 91.1±0.2 87.3±0.1\nFine-tune (Call) 60.0±2.3 51.5 ±0.9 333.3 46.9±2.4 52.9 ±1.0 115.0 90.4±0.1 92.9 ±0.4\n20 30 40 50 60 70 80 90\nTime (min)\n30\n35\n40\n45\n50\n55\n60F1-score (%)\nSource model:\n42.1\nFine-tune  ( all):\n52.9\nREMOTE ( soft + strict)\nFine-tune ( sample)\nDistillation ( sample)\n(a) StormFront →GHC\n20 30 40 50 60 70 80\nTime (min)\n25\n30\n35\n40\n45\n50\n55F1-score (%)\n Source model:\n29.5\nFine-tune  ( all):\n51.5\nREMOTE ( soft + strict)\nFine-tune ( sample)\nDistillation ( sample)\n (b) HatEval →GHC\nFigure 3: Label Efﬁciency Study on BERT-Base.\n10 20 30 all(34)\nNumber of Rules\n30\n35\n40\n45\n50\n55F1 score (%)\n36.0\n37.7 38.0\n39.9\n45.6 46.1 46.5\n48.2\nSource\nmodel:\n29.5\nREMOTE ( soft)\nREMOTE ( soft + strict)\nFigure 4: Performance on HatE-\nval →GHC on BERT-Base using\ndifferent number of explanations.\nAcross Different Language Models. We conduct experiments across different language models to\nstudy whether improvement brought by REMOTE is consistent. In Table 4, performance for both\nBERT-Large and RoBERTa-Base shows that incorporating noisy labels and regularization advice can\ngenerally help, and it can be widely applied to different pre-trained language models. We also include\nadditional experiments using BERT-Base [4] and BiLSTM+Attention in Appendix C.\nStudy on Label Efﬁciency. We further analyze the performance trends of different methods\nwhen varying the annotation time. Figure 3 shows that with the same amount of annotation time,\nREMOTE consistently outperforms the ﬁne-tuning and distillation baseline, demonstrating a better\nlabel efﬁciency. Within 30 minutes of annotation, REMOTE improves the F1 score on the downstream\ndataset signiﬁcantly for HateEval →GHC. This indicates that even a small number of compositional\nexplanations can provide strong supervision towards adapting the model to the target domain.\n4.4 Performance Analysis\nEffectiveness of Explanation Regularization. In Table 6, REMOTE (Rsoft + Cstrict) has lower\nFPRD and higher target F1 scores compared with ﬁne-tuning or performing model transfer using\nnoisy labels (Cstrict or Csoft). We observe that Rsoft plays an essential role in reducing unintended\nbiases. As Rsoft contains speciﬁed information regarding identity groups, it enables the model to\nbetter capture the context involving identity phrases, and thus improves the fairness of the model.\nWe include comparison between solely using Rstrict and Rsoft in Appendix D. Rsoft always leads\nto better performance than Rstrict. With soft-matching, regularization advice is applied to more\ninstances and thus take more effect.\nTable 7: Comparison on ﬁne-tuning with different\ndatasets. We compare performance of simply ﬁne-\ntuning the source model on BERT-Base with Csample,\nCstrict and Csoft on hate speech detection.\nDataset Stormfront→GHC HatEval→GHC\nMetrics F1 (↑) FPRD (↓) F1 ( ↑) FPRD (↓)\nSource model 42.1±1.5 16.0 29.5 ±2.5 115.6Fine-tune (Csample) 41.0±0.1 302.6 45.6±0.1 20.3Fine-tune (Cstrict) 45.1±2.2 80.2 49.0±0.5 59.2Fine-tune (Csoft) 45.4±2.0 57.9 47.9±0.6 164.0\nEffectiveness of Noisy Labels. To show the\neffectiveness of the noisy labels generated by\nREMOTE , we further compare the performance\namong ﬁne-tuning with Csample (labeled in-\nstances randomly selected from DT), Cstrict\nand Csoft (noisy labels generated in different\nmatching versions) on hate speech detection. It\nshows that models ﬁne-tuned with noisy labels\ncan yield better performance than ground-truth\nlabels. It is partly because that Csample is ran-\ndomly sampled and does not target cases with spurious patterns. In contrast, Cstrict and Csoft\ngeneralize collected explanations to unlabeled instances to create hateful instances, and then ran-\ndomly samples some unlabeled instances from DT as non-hateful instances. Regarding quantity, the\nnumbers of matched instances are smaller than the size of Csample, but the sizes of Cstrict or Csoft\n(generalized hateful instances and sampled non-hateful instances) are larger than Csample. Overall,\nquality and quantity of data both contribute to the ﬁnal performance.\n9\nPerformance Change from Increasing the Number of Explanations. We investigate the perfor-\nmance in HatEval →GHC setting using different number of explanations. We present the results in\nFig. 4. Note that we randomly sample the subset of explanations for three times to reduce variance.\nWe observe that that the performance continuously grows when more explanations are introduced.\nThis indicates that model can learn more diverse knowledge from more explanations. We also conduct\nthe same set of experiments for Stormfront →GHC setting. Results are included in Appendix D.\nAblation Study on Attribution and Interaction. To demonstrate the necessity of regularizing both\nattribution and interaction, we conduct ablation experiments that regularize them separately. Results\nin Table 8(b) show that both attribution and interaction can contribute to performance improvements\nand bias mitigation. Regularization with both attribution and interaction achieves higher F1 score in\ntarget domain than regularizing with only one of them.\nTable 8: Ablation study on hate speech detection on BERT-Base with REMOTE (Rsoft).\n(a) Ablation study on hate/nonhate-labeled rules.\nDataset Stormfront→GHC Hateval →GHC\nMetrics F1 (↑) FPRD (↓) F1 ( ↑) FPRD (↓)\nSource Model 42.1±1.5 16.0 29.5 ±2.5 115.6Nonhate-labeled rules 43.2±0.4 9.0 32.1 ±0.5 175.3All rules 43.2 ±0.4 8.2 35.7 ±2.0 173.0Hate-labeled rules45.7±1.4 12.6 39.9±4.4 56.2\n(b) Case study on attribution (a.) and interaction (i.).\nDataset Stormfront→GHC HatEval→GHC\nMetrics F1 (↑) FPRD (↓) F1 ( ↑) FPRD (↓)\nSource model 42.1±1.5 16.0 29.5 ±2.5 115.6Reg. with a. 44.6±1.6 7.2 36.8 ±4.7 39.8Reg. with i. 43.2±0.1 8.5 31.8 ±0.5 170.7Reg. with a. & i.45.7±1.4 12.6 39.9±4.4 56.2\nCase Study on Hate-labeled Rules and Nonhate-labeled Rules for GHC. To reveal the reason\nof only collecting hate-labeled explanations for GHC, we show the ablation results in Table 8(a).\n0 0.003 0.005 0.007 0.01 0.02 0.03\nRegularization Strength\n40\n42\n44\n46\n48\n50\n52F1 score (%) Source model: 42.1\n50.0 50.1 50.9 51.1 50.4 49.5 49.6\nFigure 5: Sensitivity of reg.\nstrength on Stormfront to GHC.\nNote that, here “label” is the noisy label given by human annota-\ntors. Besides the hate-labeled rules, we also collect nonhate-labeled\nrules, and conduct experiments with them separately and together.\nWe can see that regularization with only hateful rules has the best\nperformance on the target domain and has low unintended biases.\nSensitivity Study on Regularization Strength.We tuneαbetween\n0.003 and 0.03 in the experiments of REMOTE (Rsoft + Cstrict) and\nreport the results in Fig. 5. REMOTE is not very sensitive to α, and\nimproves the source model performance with a wide range of values.\nResults on another dataset pair are included in Appendix D.\nDiscussion on Optimisation Time Cost. The time cost for computing the loss in Eq. (2) depends\non the proportion of phrases that are given target scores (by generalizing the human-provided\nexplanations). In our implementation, only the phrases that are given target scores will be traversed\nwhen computing the loss. Typically, very few phrases in a matched sentence would have target scores\nassigned. For example in the HatEval →GHC setting, 291 out of the 370 soft-matched instances have\nonly one phrase/phrase pair with target score assigned. The main time cost comes from computing the\nimportance/interaction scores in line 264 and Eq 3. With the hardware and training details speciﬁed\nin Appendix A, it usually takes 50s for one iteration with Rsoft loss and 30s without it.\n5 Conclusion\nIn this work, we introduce a novel framework to reﬁne a trained language model with human-provided\ncompositional explanations. To break the bottlenecks of previous explanation regularization methods,\nour method (1) supports more diverse ways of regularization ( i.e., both feature attributions and\ninteractions) and (2) broadens regularization coverage by generalizing explanations to unlabeled\ninstances. We believe our work opens up a new way to communicate human knowledge to model\nlearning and reﬁnement, and explores the possibility of supervising model features, as opposed to the\ntraditional paradigm of supervising a model with large-scale labeled examples. Extensive experiments\ndemonstrate that our framework can improve source model’s performance on different tasks and\nreduce unintended bias. Future work may expand on our method by adapting it to more challenging\ntasks such as reading comprehension and language generation, or by incorporating human feedback\nin multiple turns in an active learning setting.\n10\nAcknowledgments and Disclosure of Funding\nThis research is supported in part by the Ofﬁce of the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007,\nthe DARPA MCS program under Contract No. N660011924033 with the United States Ofﬁce Of\nNaval Research, the Defense Advanced Research Projects Agency with award W911NF-19-20271,\nand NSF SMA 18-29268. The views and conclusions contained herein are those of the authors and\nshould not be interpreted as necessarily representing the ofﬁcial policies, either expressed or implied,\nof ODNI, IARPA, or the U.S. Government. We would like to thank all the collaborators in USC INK\nresearch lab for their constructive feedback on the work.\nSocietal Impact\nOur approach can be widely applied to adapt trained text classiﬁers without accessing upstream\ndata. Therefore, for social media websites aiming to detect hate speech, or customer services doing\nsentiment analysis based on customer feedback, different service providers can share trained neural\nmodels without leaking users’ private information. This also reduces the need of collecting a large\namount of users’ data to ﬁne-tune trained text classiﬁers.\nIn addition, as the natural language explanations provided are precise enough, the unintended biases\nof an existing model can be reduced after reﬁnement. However, if the explanations are not inspected\nby other annotators or do not pass quality check, but are still applied to reﬁne the model, the model\nmay be at risk of biased explanations that are maliciously injected. To avoid this situation, human\nannotators need to be required to reach agreement on explanations.\nReferences\n[1] Valerio Basile, C. Bosco, E. Fersini, Debora Nozza, V . Patti, F. Pardo, P. Rosso, and M. Sanguinetti.\nSemeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter. In\nSemEval@NAACL-HLT, 2019.\n[2] Ona de Gibert, Naiara Perez, Aitor García-Pablos, and Montse Cuadros. Hate speech dataset from a white\nsupremacy forum. arXiv preprint arXiv:1809.04444, 2018.\n[3] R. Delgado. The harm in hate speech. Law & Society Review, 47:232–233, 2013.\n[4] J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL-HLT, 2019.\n[5] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating\nunintended bias in text classiﬁcation. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,\nand Society, pages 67–73, 2018.\n[6] J. Donahue, Y . Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A\ndeep convolutional activation feature for generic visual recognition. In ICML, 2014.\n[7] G. Erion, J. Janizek, Pascal Sturmfels, Scott M. Lundberg, and Su-In Lee. Learning explainable models\nusing attribution priors. ArXiv, abs/1906.10670, 2019.\n[8] Bjarke Felbo, A. Mislove, Anders Søgaard, I. Rahwan, and S. Lehmann. Using millions of emoji\noccurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. In EMNLP,\n2017.\n[9] R. Fiebrink, P. Cook, and D. Trueman. Human model evaluation in interactive supervised learning. In CHI,\n2011.\n[10] K. Fujimoto, I. Kojadinovic, and J. Marichal. Axiomatic characterizations of probabilistic and cardinal-\nprobabilistic interaction indices. Games Econ. Behav., 55:72–99, 2006.\n[11] Yaroslav Ganin and V . Lempitsky. Unsupervised domain adaptation by backpropagation. ArXiv,\nabs/1409.7495, 2015.\n[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classiﬁ-\ncation: A deep learning approach. In ICML, 2011.\n11\n[13] Riccardo Guidotti, A. Monreale, F. Turini, D. Pedreschi, and F. Giannotti. A survey of methods for\nexplaining black box models. ACM Computing Surveys (CSUR), 51:1 – 42, 2019.\n[14] Xiaojie Guo, Y . Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation.\nIEEE Transactions on Image Processing, 26:982–993, 2017.\n[15] Braden Hancock, P. Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and C. Ré. Training classi-\nﬁers with natural language explanations. Proceedings of the conference. Association for Computational\nLinguistics. Meeting, 2018:1884–1895, 2018.\n[16] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. Proceedings of the 25th International Conference on World Wide Web,\n2016.\n[17] Geoffrey E. Hinton, Oriol Vinyals, and J. Dean. Distilling the knowledge in a neural network. ArXiv,\nabs/1503.02531, 2015.\n[18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In ACL,\n2018.\n[19] Xisen Jin, Junyi Du, Zhongyu Wei, X. Xue, and Xiang Ren. Towards hierarchical importance attribution:\nExplaining compositional semantics for neural sequence models. ArXiv, abs/1911.06194, 2020.\n[20] Brendan Kennedy, Mohammad Atari, Aida Mostafazadeh Davani, Leigh Yeh, Ali Omrani, Yehsong Kim,\nKris Coombs, Shreya Havaldar, Gwenyth Portillo-Wightman, Elaine Gonzalez, et al. The gab hate corpus:\nA collection of 27k posts annotated for hate speech. PsyArXiv, 2018.\n[21] Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani, and Xiang Ren. Contextual-\nizing hate speech classiﬁers with post-hoc explanation. ACL, 2020.\n[22] Angelika Kimmig, Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. A short introduc-\ntion to probabilistic soft logic. In NIPS 2012, 2012.\n[23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,\n2015.\n[24] Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. Principles of explanatory\ndebugging to personalize interactive machine learning. In Proceedings of the 20th international conference\non intelligent user interfaces, pages 126–137, 2015.\n[25] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\nAlbert: A lite bert for self-supervised learning of language representations. ArXiv, abs/1909.11942, 2020.\n[26] Piyawat Lertvittayakumjorn, Lucia Specia, and Francesca Toni. Find: human-in-the-loop debugging deep\ntext classiﬁers. arXiv preprint arXiv:2010.04987, 2020.\n[27] Rui Li, Qianfen Jiao, Wenming Cao, H. Wong, and Si Wu. Model adaptation: Unsupervised domain\nadaptation without source data. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 9638–9647, 2020.\n[28] X. Li, Yves Grandvalet, and F. Davoine. Explicit inductive bias for transfer learning with convolutional\nnetworks. In ICML, 2018.\n[29] Jian Liang, D. Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer\nfor unsupervised domain adaptation. In ICML, 2020.\n[30] Frederick Liu and B. Avci. Incorporating priors with feature attribution on text classiﬁcation. In ACL,\n2019.\n[31] Y . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv,\nabs/1907.11692, 2019.\n[32] Gideon S. Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learning\nwith weakly labeled data. J. Mach. Learn. Res., 11:955–984, 2010.\n[33] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and\nData Engineering, 22:1345–1359, 2010.\n12\n[34] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for\nmulti-source domain adaptation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV),\npages 1406–1415, 2019.\n[35] Gregory Plumb, Maruan Al-Shedivat, Eric Xing, and Ameet Talwalkar. Regularizing black-box models for\nimproved interpretability (hill 2019 version). NeurIPS, 2020.\n[36] Laura Rieger, Chandan Singh, William Murdoch, and Bin Yu. Interpretations are useful: penalizing\nexplanations to align neural networks with prior knowledge. In International Conference on Machine\nLearning, pages 8116–8126. PMLR, 2020.\n[37] M. Riemer, Elham Khabiri, and Richard Goodwin. Representation stability as a regularizer for improved\ntext analytics transfer learning. ArXiv, abs/1704.03617, 2017.\n[38] A. Ross, M. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training differentiable models\nby constraining their explanations. In IJCAI, 2017.\n[39] Burr Settles. Active learning literature survey. University of Wisconsin-Madison Department of Computer\nSciences, 2009.\n[40] R. Socher, Alex Perelygin, J. Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts.\nRecursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.\n[41] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In AAAI,\n2016.\n[42] M. Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. ArXiv,\nabs/1703.01365, 2017.\n[43] Stefano Teso and Kristian Kersting. Explanatory interactive machine learning. In Proceedings of the 2019\nAAAI/ACM Conference on AI, Ethics, and Society, pages 239–245, 2019.\n[44] A. Torralba and Alexei A. Efros. Unbiased look at dataset bias. CVPR 2011, pages 1521–1528, 2011.\n[45] F. D. Vigna, A. Cimino, Felice Dell’Orletta, M. Petrocchi, and M. Tesconi. Hate me, hate me not: Hate\nspeech detection on facebook. In ITASEC, 2017.\n[46] D. Wang and T. Zheng. Transfer learning for speech and language processing. 2015 Asia-Paciﬁc Signal\nand Information Processing Association Annual Summit and Conference (APSIPA), pages 1225–1237,\n2015.\n[47] Ziqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Z. Liu, and X. Ren. Learning\nfrom explanations with neural execution tree. In ICLR, 2020.\n[48] Georg Wiese, Dirk Weissenborn, and Mariana Neves. Neural domain adaptation for biomedical question\nanswering. ArXiv, abs/1706.03610, 2017.\n[49] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase-level sentiment analysis.\nIn HLT/EMNLP, 2005.\n[50] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npages 38–45, Online, October 2020. Association for Computational Linguistics.\n[51] Qinyuan Ye, Xiao Huang, Elizabeth Boschee, and Xiang Ren. Teaching machine comprehension with\ncompositional explanations. In Findings of the Association for Computational Linguistics: EMNLP 2020,\npages 1599–1615, 2020.\n[52] W. Zellinger, Thomas Grubinger, E. Lughofer, T. Natschläger, and Susanne Saminger-Platz. Central\nmoment discrepancy (cmd) for domain-invariant representation learning. ArXiv, abs/1702.08811, 2017.\n[53] Luke Zettlemoyer and M. Collins. Learning to map sentences to logical form: Structured classiﬁcation\nwith probabilistic categorial grammars. ArXiv, abs/1207.1420, 2005.\n[54] Luisa M. Zintgraf, T. Cohen, Tameem Adel, and M. Welling. Visualizing deep neural network decisions:\nPrediction difference analysis. ICLR, 2017.\n13\nA Experiment Details for Reproducibility\nDataset. For experiments on hate speech detection, we download the Gab Hate Corpus (GHC) from\n[20]2. Stormfront and HatEval datasets are downloaded from [2]3 and [1]4. For sentiment analysis,\nAmazonMusic is released by [16]5 and SST-2 dataset can be downloaded from [40]6.\nFor Gab Hate Corpus (GHC) dataset, we randomly re-split the dataset into train/dev/test sets by the\nratio 8:1:1. For all the other datasets, we follow their original train/dev/test splits. Target training set\nis used as the “target domain data” after removing the ground-truth labels. Target dev set is used for\nearly stopping when updating the target models and also for tuning model hyper-parameters. Target\ntest set is used for evaluating “target F1” metric. We ﬁne-tune a pre-trained language model (e.g.,\nBERT-Base) over the source training set to generate the source model. Source dev set is used for\nearly stopping when training the source model. Source test set is used for evaluating the “source F1”\nmetric for the updated models. Statistics of each dataset pair are included in Table 9.\nTable 9: Statistics for the dataset pairs.\nDataset Pair Source train/dev/test Target train/dev/test\nHatEval→GHC 9000 / 1000 / 3000 22132 / 2766 / 2767Stormfront→GHC 7896 / 978 / 1998 22132 / 2766 / 2767AmazonMusic→SST-2 3000 / 300 / 8302 67349 / 872 / 1821\nImplementation and Infrastructure. All our experiments are implemented with Transformers\nlibrary [50]7. All experiments are done with one single GPU. We use NVIDIA Quadro RTX 8000\nfor large-sized language models (i.e., BERT-Large) and NVIDIA GeForce RTX 2080 Ti for other\nmodels (e.g., BERT-Base, RoBERTa-Base, Bi-LSTM+Attention).\nHyperparameters. We use Adam [23] optimizer throughout all the experiments. Batch size is\nset to be 32 in all experiments for all the methods. We conduct grid search on learning rate and\nregularization strength for each experiment using the target dev set. For REMOTE (R) and all the\nbaselines, learning rate is selected from the range {5e-6, 8e-6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5, 6e-5,\n7e-5}. Regularization strength (when applicable) is selected from {0.01, 0.02, 0.03, 0.04, 0.05}. For\nREMOTE (R+ C), learning rate is selected from the range {1e-5, 2e-5, 3e-5, 4e-5}. Regularization\nstrength is selected from {0.003, 0.005, 0.007, 0.01, 0.02, 0.03}.\nEarly Stopping. We evaluate the model performance over target dev set every 400 steps for\nREMOTE (R+ C) and every 100 steps for REMOTE (R) and all the baselines. The training will be\nearly-stopped when the target dev F1 stops improving for 10 iterations, and the learning rate is halved\nwhen the dev F1 stops improving for 5 iterations.\nMultiple Runs. For every experiment setting, we select the best conﬁguration of hyper-parameters\nbased on the target dev set F1 using one random seed. Then we train the model using this hyper-\nparameter conﬁguration with two additional random seeds and report the mean and standard deviation.\nLabel Balancing for Hate Speech Tasks. For hate speech tasks, due to the unbalanced ratio of\nnegative and positive examples (approximately 10:1), we re-weight the training loss so that positive\nexamples are weighted 10 times as negative examples for all the models.\nB Details on Explanation Parsing\nLexicon Details. To help the parser understand the collected natural language explanations, we\ndeﬁne a lexicon str2predicate that maps 301 raw words/phrases into 83 predicates. In addition,\n2https://osf.io/nqt6h/\n3https://github.com/Vicomtech/hate-speech-dataset\n4https://competitions.codalab.org/competitions/19935\n5https://sites.google.com/a/eng.ucsd.edu/ruining-he/\n6https://dl.fbaipublicﬁles.com/glue/data/SST-2.zip\n7https://github.com/huggingface/transformers\n14\nwe deﬁne a lexicon predicate2func that maps the predicates to 25 functions. For example, when\nhuman annotator write the word political, politician, religious or nationality in their explanation,\nthe word will ﬁrst be mapped to the predicate $NorpNER. NORP is the shorthand for “nationalities\nor religious or political groups”. Then $NorpNER will be processed and combined with other\npredicates in the same sentence (using CCG Parser), and the ﬁnal parsing result may map it to the\nfunction @NER(NORP), which will identify whether a given word is indeed a NORP named entity.\nWe have included these lexicons in our code.\nParser Evaluation and Modiﬁcation. As a pre-validation of the parsed rule, we ﬁrst execute it on\nits reference instance xref and discard the rule if the execution outcome of the rule head Bover xref\nis False (i.e., the parsed rule cannot match the reference instance xref where the explanation was\nsolicited from). This step ensures the quality of explanations and parsed rules. After a ﬁrst run of\nparsing of the explanations, we further evaluate the quality of the parsed rules to ensure the parser is\nworking properly. If the parsed rules are not equivalent to their original explanations, we modify the\nparser’s lexicon to adjust the parsing results. For example, if the annotator wrote “X is religion”,\nwhile “religion” is not in the pre-deﬁned str2predicate lexicon, the sentence will be ignored by\nCCG parser at ﬁrst. To correct the mistake, we can add “ religion” to the lexicon. About 85% of\nexplanations can be parsed correctly in the ﬁrst time (as we manually inspected). After updating the\nlexicon, the parsing will reach 100% accuracy on all the collected explanations.\nParser Evaluation on Out-Of-Domain Data. To understand how well our parser can generalize\nto out-of-domain data, we further collected 116 explanations following our annotation guideline,\non held-out data from Stormfront →GHC and HatEval →GHC. Note that this set of instances are\ndisjoint from the annotation sets used in our reported experiments. We present the parsing results\nto human annotators for veriﬁcation. This evaluation shows that the accuracy of semantic parser\ncan reach 92.2% on the held-out data (without any update to the parser). Therefore, we believe\nour semantic parser is reliable when applied to out-of-domain data. We also admit that the parse\nmay encounter unseen vocabulary and typos when human annotators are not strictly following the\nannotation guideline.\nC Results on Reﬁning BERT-Base and Bi-LSTM+Attention\nTable 10: Results on three pairs of datasets with BERT-Base and BiLSTM+Attention.We report F1 scores\non source domain and target domain, and FPRD on IPTTS as fairness metric for hate speech task. Best results\nare bold. The annotation time cost of each dataset pair is provided. We use SOC for feature attribution.\nDataset HatEval→GHC(80 mins) Stormfront→GHC(94 mins) AmazonMusic→SST-2(15 mins)\nMetrics Source F1 (↑) Target F1 (↑) FPRD (↓) Source F1(↑) Target F1 (↑) FPRD (↓) Source F1 (↑) Target F1 (↑)\nBERT-Base\nSource model 64.2±0.3 29.5 ±2.5 115.6 57.2±0.7 42.1 ±1.5 16.0 91.4±0.4 83.5 ±2.5Fine-tune (Csample) 58.0±5.1 41.0 ±0.1 302.6 56.4±0.4 45.6±0.1 20.3 88.9±0.6 86.7 ±1.0L2-reg (Csample) 59.8±4.2 41.3 ±0.7 278.7 55.8±0.9 46.8 ±1.2 26.8 89.0±0.6 86.8 ±0.6Distillation (Csample) 60.8±5.1 42.4 ±1.6 315.4 54.4±2.0 46.6 ±1.4 112.6 87.4±0.9 86.7 ±1.0REMOTE (Rs) 63.5±0.7 39.9 ±4.4 56.2 49.9±3.5 45.7 ±1.4 12.6 91.1±0.0 85.3 ±0.1REMOTE (Rs+Ch) 63.2±0.6 46.6 ±1.1 49.0 47.4±1.9 51.1±1.6 34.6 91.1±0.2 87.3 ±0.1\nFine-tune (Call) 60.0±2.3 51.5 ±0.9 333.3 46.9±2.4 52.9 ±1.0 115.0 90.4±0.1 92.9 ±0.4\nBiLSTM + Attention\nSource model 60.5±0.7 20.2 ±1.2 115.2 44.7±2.2 26.3 ±4.2 157.2 81.1±1.5 54.0 ±5.5Fine-tune (Csample) 60.8±0.2 23.7 ±0.8 71.9 42.9±2.7 29.1 ±2.3 201.8 78.9±0.4 61.0±1.3L2-reg (Csample) 60.4±0.3 24.2 ±0.1 21.3 44.3±0.3 30.1 ±0.9 157.3 78.7±0.4 62.6 ±1.1Distillation (Csample) 60.1±0.5 24.3 ±0.8 16.4 45.2±0.9 29.5±0.9 151.3 78.6±0.6 61.6 ±1.3REMOTE (Rsoft) 61.1±0.1 27.0±0.4 69.3 36.8±5.1 31.5 ±0.9 18.6 78.5±0.7 64.9 ±1.5REMOTE (Rsoft+Cstrict) 58.7±1.5 31.3±2.5 7.4 42.2±2.2 33.5±0.4 65.2 76.9±0.5 66.1±0.6\nFine-tune (Call) 58.5±1.7 38.5 ±1.7 124.2 42.5±2.0 44.5 ±0.2 323.3 79.2±0.2 81.7 ±1.1\nAs additional results for performance comparison, we conduct experiments on BERT-Base and\nBi-LSTM+Attention under the same setting as in Table 4 and summarize the results in Table 10.\nWe observe similar patterns as in Table 4. The results show that REMOTE can cope with different\nlanguage models and obtain consistent improvement over the baselines – it constantly yields the best\ntarget performance and fairness among all compared methods, while preserves source performance\nbetter than ﬁne-tune (Call) in most cases.\n15\nTable 11: Results with R EMOTE (Rsoft + Ssoft ). By using Csoft as noisy labels rather than Cstrict, the\ntarget domain F1 scores are lower in most cases.\nDataset HatEval→GHC(80 mins) Stormfront→GHC(94 mins) AmazonMusic→SST-2(15 mins)\nMetrics Source F1 (↑) Target F1 (↑) FPRD (↓) Source F1(↑) Target F1 (↑) FPRD (↓) Source F1 (↑) Target F1 (↑)\nBERT-Large\nSource model 63.7±0.5 31.4±1.4 124.3 59.5±1.1 41.9±1.4 17.1 92.9±0.2 87.7±1.0REMOTE(Rsoft+Cstrict) 62.0±0.4 46.1 ±1.0 15.3 49.0±3.4 52.2±0.4 10.0 92.7±0.2 90.3±0.2REMOTE(Rsoft+Csoft) 61.8±0.2 47.4±0.9 11.2 50.7±1.9 51.6 ±2.7 17.8 92.7±0.0 89.5 ±0.1\nFine-tune (Call) 51.3±5.6 52.5 ±0.4 98.0 46.0±3.8 53.8 ±1.6 142.3 92.5±0.2 94.4 ±0.4\nRoBERTa-Base\nSource model 62.7±0.9 30.9±1.9 61.6 57.4±1.2 39.6 ±1.2 43.8 92.4±0.4 87.5±0.9REMOTE(Rsoft+Cstrict) 57.5±0.9 44.7±1.0 97.8 57.6±1.9 50.1 ±1.7 77.5 91.4±0.2 89.5±0.5REMOTE(Rsoft+Csoft) 59.2±1.4 44.0 ±1.0 8.6 46.0±2.6 49.6 ±0.8 19.2 91.2±0.3 89.1 ±0.9\nFine-tune (Call) 51.4±3.2 50.6 ±0.4 263.2 52.2±4.9 50.5 ±1.5 294.0 91.2±0.0 95.1 ±0.4\n80\n85\n90\n95\n85.2 85.3\nREMOTE ( strict)\nREMOTE ( strict)\nHatEval->GHCStormfront->GHCAmazonMusic->SST-2\n35\n40\n45\n50F1 score (%)\n38.8\n43.8\n39.9\n45.7\nFigure 6: REMOTE with\nRstrict and Rsoft on\nBERT-Base.\n10 20 30 all(40)\nNumber of Rules\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5F1 score (%)\n42.6\n44.3 43.8 44.644.5\n47.0 48.0\n51.1\nSource\nmodel:\n42.1\nREMOTE ( soft)\nREMOTE ( soft + strict)\nFigure 7: Stormfront →GHC per-\nformance with different numbers of\nexplanations.\nStormfront -> GHC\n20\n30\n40\n50\n60\n70F1 score (%)\n49.1 50.0 48.8\n500 neg samples\n1600 neg samples\n3000 neg samples\nFigure 8: Sensitivity study on dif-\nferent negative samplings on Storm-\nfront →GHC.\nD Additional Experiments for Performance Analysis\nAblation Study on Reﬁnement with Strict/Soft-Matched Labels. Table 11 compares the model\nperformance of REMOTE using Rsoft + Cstrict and Rsoft + Csoft. It shows that reﬁnement with\nstrict-matched labels (Cstrict) often outperform the model reﬁned with soft-matched labels (Csoft).\nIt shows that model reﬁnement is sensitive to the precision of noisy labels so we decide to use\nstrict-matched labels in our main experiments.\nAblation Study on Regularization with Strict/Soft-Matched Instances. Figure 6 compares the\ntarget F1 performance of REMOTE using Rstrict and Rsoft as regularization advice without noisy\nlabels. For the three dataset pairs, Rsoft always yields better performance than Rstrict. With\nsoft-matching, regularization advice are generalized to more instances and thus take more effect.\nTherefore, we present results on REMOTE the regularization term fromRsoft in the main experiments.\nPerformance changes by Varying the Number of Explanations.In addition to the setting reported\nin Fig. 4, we also conduct experiment on Stormfront →GHC by varying the number of explanations.\nIn Fig. 7, result shows that model performance continuously grows when more explanations are\nintroduced, which is the same pattern as in Fig. 4.\n0 0.003 0.005 0.007 0.01 0.02 0.03\nRegularization Strength\n80\n82\n84\n86\n88\n90F1 score (%)\nSource model:83.5\n86.0\n87.3 87.2 87.1 86.5 85.9 85.6\nFigure 9: Sensitivity of reg. strength on\nAmazon to SST-2.\nSensitivity Study on Negative Samples. For hate speech\ndetection experiments, we randomly sample (unlabeled)\nexamples from the training set and treat them as “negative”\n(or “non-hate”) examples to balance the label ratio. We\ncontrol the number of sampled instances in the Storm-\nfront →GHC setting (as 500, 1600, and 3000 instances),\nand show the results in Fig. 8. For each number, we ran-\ndomly sample the instances for 3 times. We observed\nthat performance is not very sensitive to the number of\nnegative samples included. Our main results are based\non sampling 1,600 negative instances, which has slightly\nbetter performance among all.\nSensitivity Study on Reg. Strength. We report results on sensitivity of model reﬁnement to\nregularization strength on dataset pair Amazon →SST-2 in Figure 9. We tune reg. strength from\n0.0003 to 0.03 in the experiments of REMOTE (Rsoft + Cstrict). We conclude that REMOTE is not\n16\nsensitive to the selection of regularization strength, and the reﬁned model constantly perform better\nthan source model.\nTable 12: Comparison with unsupervised model adaptation\nmethod. We compare REMOTE (Rsoft + Cstrict) (based on IG\nand SOC) with a representative UMA method SHOT[29].\nDataset HatEval→GHCStormfront→GHCAmazon→SST-2\nMetrics Target F1 (↑) Target F1 (↑) Target F1 (↑)\nSource model 29.5±2.5 42.1±1.5 83.5±2.5SHOT [29] 25.4±1.6 28.8±1.9 61.0±16.1REMOTEw. IG 47.2±1.3 49.5±1.1 87.0±0.7REMOTEw. SOC 46.6±1.1 51.1±1.6 87.3±0.1\nﬁne-tune (Call) 51.5±0.9 52.9±1.0 92.9±0.4\nComparison with Unsupervised\nModel Adaptation Method. Recent\nworks have studied how to adapt a\nmodel trained with source dataset to\na target domain only with unlabeled\ntarget instances and no source data.\nThis setting is known as unsupervised\nmodel adaptation (UMA). We apply\na popular UMA method SHOT [ 29]\non our task and report the results in\nTable 12. We found that the SHOT harms rather than improves model performance in the target\ndomain. SHOT was originally proposed for computer vision tasks such as object detection and digit\nrecognition. We conjecture that SHOT, an approach proposed for computer vision tasks, may not\nbe directly applicable to a different modality (i.e., natural language). We defer thorough study on\nextending UMA method to our problem as future work.\nTable 13: Ablation Study on soft version of INTER\nmodule. For StormFront →GHC, we report results on\nREMOTE n soft version and replace the softened INTER\nmodule with strict version on BERT-Base.\nDataset Stormfront→GHC\nMetrics Source F1 (↑) Target F1 (↑) FPRD (↓)\nSource model 57.2±0.7 42.1±1.5 16.0REMOTEall soft but INTER) 51.0±0.9 44.8 ±0.4 5.2REMOTEall soft) 49.9 ±3.5 45.7±1.4 12.6\nAblation Study on Soft Version of Interac-\ntion Module. To understand the effect of\n“softening” change to the Interaction mod-\nule , we conduct an ablation study on Storm-\nfront →GHC using BERT-base, as shown in\ntable 13. Speciﬁcally, we set all other mod-\nules in REMOTE as their soft versions but only\nthe Interaction module as its strict version, and\ncompare it with “ REMOTE (all soft)” to show\nthe effectiveness of softening the Interaction. Results show that softening the interaction module\nis an important operation in generalizing explanations to a broader set of unlabeled instances (as\ndiscussed in Sec 3.3). When we replace the softened version of Interaction with its strict counterpart,\nthe performance signiﬁcantly drops.\nE Details about Individuality modules\nIn this section we introduce details about how to conduct strict matching via Individuality module.\nGiven the reference sentence xref and a word qref in it, the module ﬁnds a word qk in the unlabeled\ninstance xk, where qk has the same semantic type or plays the same grammatical role with qref.\nThe model determines whether qref and qk have the same semantic type according to their named\nentity types, their sentiment types, and whether they are both identity phrases or hateful words. For\nsentiment labels, we use the subjectivity lexicon [ 49] to decide if a word is positive, negative or\nneutral. For identity phrases, we take the list in [19] which contains group identiﬁers such as “women”\nor “black”. In addition, because we aim at hate speech task, we use a list of hateful words obtained\nfrom HateBase8. We’ve uploaded the aformetioned lists together with our code, except for hateful\nword list due to license issue. As for the grammatical roles of qref and qk, the model compares their\nrelations to the dependency trees and constituency trees of the sentences they are from (dependency\nparser and constituency parser implemented in spaCy9).\nF Explanation Solicitation Interface\nThe screenshots of the interface for explanation solicitation of the hate speech detection task are\nincluded in Fig. 10. The annotators are given the following instructions:\n1) Read the sentence and provide a label. You will see a sentence on the top of each page. Please\ndecide the label of the sentence, and ﬁll in a number (0 for non-hateful, and 1 for hateful) in the blank.\n8https://hatebase.org/search_results\n9https://spacy.io/\n17\n(a) Initial interface shown for each instance\n(b) Heat-map and explanation slots shown after annotators select buttons\nFigure 10: Interface for Explanation Solicitation\n2) Inspect the heat-map. Please select the “Show Prediction and Heatmap” button. You will see a\npredicted label, and a heat-map. In the heat-map, if a word span is considered to be related to hate\nspeech, it is marked as red.\n3) Write an explanation if a spurious pattern is found. If you think the predicted label and the\nheat-map do not align with your interpretation of the sentence, please select the “Show Explanation\nSlots” button. Please specify at least one phrase that you believe the color in the heat-map should be\nchanged, and ﬁll in the phrases in the left-hand side slots, and select the actions that you suggest in\nthe rightmost drop-down lists. You can explain the characteristic of this phrase in its neighboring\nslot, such as the sentiment or the part-of-speech tag. If you choose to describe the characteristic,\nplease select the “soft” option in the neighboring drop-down list. You can also describe the relations\nbetween the speciﬁed phrases in the “Observed relations” slot.\nAdditional Instructions. The duration of annotation process is measured. Please press the “Pause”\nbutton or close the window when you decide to leave. The program will save your progress. Once you\nﬁnish ﬁlling the slots, you can select the “Next” button to proceed to the next sentence. You can also\nselect “Skip” to skip the current sentence.Though NLP knowledge about post-hoc explanation scores\nand text classiﬁcation tasks is required to use our system, we believe the heatmap-based annotation\ninterface is accessible to lay users.\nG Case Study\nFig. 11 demonstrates an example to show the effect of model reﬁnement. The corresponding human\nexplanation is on the top. The ﬁrst heat-map is produced by SOC algorithm based on source model\nfS, and the second heat-map is based on fT, the reﬁned model. We observe that the reﬁned model\nmakes correct prediction. The differences between the two maps demonstrate that attribution scores\nare adjusted according to human explanations, as expected.\n18\nFigure 11: Post-hoc explanation heat-maps before and after model reﬁnement. Top: Before\nregularization; Bottom: After REMOTE regularization. Word spans contributing to hate are red, and\nnon-hate ones are blue.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7856262922286987
    },
    {
      "name": "Spurious relationship",
      "score": 0.7838382720947266
    },
    {
      "name": "Parsing",
      "score": 0.620774507522583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5855613350868225
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.5776112079620361
    },
    {
      "name": "Executable",
      "score": 0.5750584006309509
    },
    {
      "name": "Machine learning",
      "score": 0.5698876976966858
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.465646356344223
    },
    {
      "name": "Generalization",
      "score": 0.43091684579849243
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4284393787384033
    },
    {
      "name": "Language model",
      "score": 0.41277724504470825
    },
    {
      "name": "Natural language processing",
      "score": 0.3901263475418091
    },
    {
      "name": "Mathematics",
      "score": 0.09104189276695251
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}