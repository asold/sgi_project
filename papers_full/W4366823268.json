{
  "title": "What’s the next word in large language models?",
  "url": "https://openalex.org/W4366823268",
  "year": 2023,
  "authors": [],
  "references": [
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4220949944",
    "https://openalex.org/W4324032752",
    "https://openalex.org/W4313485929",
    "https://openalex.org/W4297243351",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4360836968"
  ],
  "abstract": null,
  "full_text": "nature machine intelligence\n Volume 5 | April 2023 | 331–332 | 331\nhttps://doi.org/10.1038/s42256-023-00655-z\nEditorial\nWhat’s the next word in large language models?\nWe are trying to keep up with \nthe torrent of developments and \ndiscussions in AI and language \nmodels since ChatGPT was \nunleashed on the world.\nT\nhe past two decades have seen \na steady rise in the adoption of \nmachine learning tools in every\n-\nday applications, such as in search \nengines, recommender systems, \nlanguage translation tools, image editing \napps, health applications and many more. \n \nA new phase may be starting with the advent of \nAI generative tools that are powered by large \nlanguage models (LLMs), such as ChatGPT for \ntext and DALL-E or Stable Diffusion for images, \nwhich give millions of people direct access to \npowerful creative applications. Many news \narticles and commentaries have been written \nto debate the opportunities, disruptive soci-\netal impact and ethical concerns of LLMs and \ntheir downstream applications. A Correspond-\nence in this issue, for instance, discusses the \ndilemma that is faced by higher education in \nallowing or banning the use of ChatGPT and \nrelated tools by students.\nKeeping up with developments in this area \nis challenging, as tech companies race to \ncompete in developing new, more powerful \nand versatile versions of LLMs. Just in recent \nweeks, Meta reported their LLaMA model on \n24 February; Google announced PaLM-E on  \n10 March, a multimodal version of the \nPaLM language model, which incorporates \nrobot sensor data; Baidu introduced their \nLLM-based Chatbot ERNIE on 15 March; Ope-\nnAI revealed their next GPT version — GPT-4 — \non 14 March; and GitHub announced Copilot X \non 22 March, which adopts GPT-4 and Chatbot \nfeatures to support code developers.\nThen there are the policy and ethics \nresponses. Getty Images are suing Stability \nAI — the creators of Stable Diffusion — for \ncopyright infringement; Italy is banning  \nChatGPT; Canada’s federal privacy watchdog \nhas launched a probe into privacy concerns \nover ChatGPT; and, as widely reported, an \nOpen Letter from the Future of Life Insti\n-\ntute calling for a pause on ‘giant AI’ for at \nleast 6 months has been signed by thou\n-\nsands, including well-known AI researchers \nand commentators. Within a few days, a \nresponse from AI ethics experts appeared \nto criticize the Open Letter for fuelling hype \nand ignoring ongoing societal harms from \nAI systems, which will not be solved by a \n \n6-month pause.\nThe scale of developments and the unprec-\nedented level of continuing wide public inter-\nest have made it difficult for both experts and \ninterested parties to make sense of the latest AI \nbreakthroughs. It may be surprising to many, \nperhaps, that the connection between LLMs \nand human language understanding is heavily \ndebated by researchers1. A conservative view is \nthat LLMs are just very good at next-word pre-\ndiction, unrelated to any real understanding \nof language. A chatbot like ChatGPT may seem \nto have a confident answer to everything, but \nit also makes simple factual and conceptual \nmistakes. This is arguably because LLMs have \nno real experiences and no understanding of \nthe real world, in a non-linguistic way. They \nlearn ‘form’ of language but no meaning, as \nargued in an influential paper from 2020 by \nEmily Bender and Alexander Koller 2. On the \nother hand, the way language is handled in \nhuman brains will incorporate at least some \nsort of next-word prediction and there may \nbe shared computational principles between \nLLMs and human language\n3.\nIt is often pointed out in this debate on \n‘understanding’ and LLMs that the models lack \ngrounding in the physical world. But is sen -\nsory grounding really needed for meaning and \nunderstanding? This fundamental question \nwas debated by six experts in machine learn-\ning, cognitive science, neuroscience, philoso-\nphy and linguistics at a recent conference on \nthe philosophy of deep learning. The answer \nwas, of course, far from straightforward. One \nof the panelists, Ellie Pavlick from Brown Uni-\nversity and Google AI, pointed out that much \nof human understanding and knowledge is \ntransferred by language alone and it may be \npossible to have a good understanding of the \nworld without sensory grounding. Her group \npublished a study in 2021 reporting that \nGPT-3 can learn concepts such as ‘north’ and \n‘left’ in a grid world4. They reasoned that it is \npossible for a model to devise a conceptual \nstructure from text alone that looks like what \na model would learn when it could interact in \na grounded world.\nA next step in the development of LLMs is \nto combine them with multimodal capabili -\nties, including sensory input. OpenAI’s GPT-4 \nhas been trained as a multimodal model, but \nat the time of writing, the ability to analyse \nor even generate images has not been shown \noutside of the launch demo and is not avail -\nable for the general public to use. Training \non images in addition to text could either \nbe seen as the solution to ground text more \nfirmly in human experience, or it could just be \nseen as adding more ungrounded data. Add-\ning sensory data such as in Google’s PaLM-E \nmodel could bring a new level of grounding  \nfor LLMs.\nThese are clearly exciting times for large \nlanguage models. The underlying approach —  \nthe combination of pre-training with trans -\nformer architecture — is a game changer for \napplications in many scientific research areas \nsuch as materials discovery5, molecular prop-\nerty predictions6 and protein design7. Other \ninteresting developments are in improving \nthe efficiency of LLMs by careful parameter \ntuning8 or, rather than scaling the models up \nfurther, making them smaller while preserv -\ning similar capabilities; researchers from  \nStanford University developed the Alpaca  \nmodel, a fine-tuned version of LLaMA that is \ntrained with text that is generated by GPT-3, \nand that, the authors say, costs only US$600 \nto reproduce. A potential advantage of smaller \nmodels with explicit internal dialogues is that \nthe reasoning to reach the output can be more \neasily explained.\nWith the whirlwind of developments that \nhave both scientific and societal impact, it \nis challenging to see through the hype. In \na recent preprint, Microsoft researchers \nreported on a range of experiments to dem -\nonstrate the powerful performance of GPT-4 \nand were sufficiently impressed to conclude \nthat there are ‘sparks of artificial general intel-\nligence’9. The paper quickly came under fire \nby experts. LLMs are clearly capable of tack -\nling a range of complex tasks, and the widely \ndemonstrated possibility of harnessing the \npower of language provides exciting, surpris-\ning scientific opportunities — without reach-\ning for the elusive idea of artificial general \nintelligence.\nPublished online: 24 April 2023\n Check for updates\nnature machine intelligence Volume 5 | April 2023 | 331–332 | 332\nEditorial\nReferences\n1. Mitchell, M. & Krakauer, D. C. Proc. Natl Acad. Sci. USA 120, \ne2215907120 (2023).\n2. Bender, E. M. & Koller, A. in Proceedings of the 58th Annual \nMeeting of the Association for Computational Linguistics \n5185–5198 (2020).\n3. Goldstein, A. et al. Nat. Neurosci. 25, 369–380 (2022).\n4. Patel, R. & Pavlick, E. in International Conference on \nLearning Representations (2022).\n5. Kang, Y., Park, H. & Smit, B. et al. Nat. Mach. Intell. 5, \n309–318 (2023).\n6. Ross, J., Belgodere, B. & Chenthamarakshan, V. et al. Nat. \nMach. Intell. 4, 1256–1264 (2022).\n7. Castro, E., Godavarthi, A. & Rubinfien, J. et al. Nat. Mach. \nIntell. 4, 840–851 (2022).\n8. Ding, N., Qin, Y. & Yang, G. et al. Nat. Mach. Intell. 5, \n220–235 (2023).\n9. Bubeck, S. et al. Preprint at arXiv https://doi.org/10.48550/\narXiv.2303.12712 (2023).",
  "topic": "Word (group theory)",
  "concepts": [
    {
      "name": "Word (group theory)",
      "score": 0.694850504398346
    },
    {
      "name": "Computer science",
      "score": 0.5712899565696716
    },
    {
      "name": "Linguistics",
      "score": 0.34816062450408936
    },
    {
      "name": "Philosophy",
      "score": 0.06415849924087524
    }
  ],
  "institutions": [],
  "cited_by": 23
}