{
  "title": "CyberQ: Generating Questions and Answers for Cybersecurity Education Using Knowledge Graph-Augmented LLMs",
  "url": "https://openalex.org/W4393161287",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5001138430",
      "name": "Garima Agrawal",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A5048580366",
      "name": "Kuntal Kumar Pal",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A5034806935",
      "name": "Yuli Deng",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A5100338946",
      "name": "Huan Liu",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A5074702216",
      "name": "Ying‐Chih Chen",
      "affiliations": [
        "Arizona State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6784560321",
    "https://openalex.org/W3103891289",
    "https://openalex.org/W4380997469",
    "https://openalex.org/W4308335753",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W1608774410",
    "https://openalex.org/W2029127831",
    "https://openalex.org/W6799654988",
    "https://openalex.org/W2804978390",
    "https://openalex.org/W3015927585",
    "https://openalex.org/W3172815687",
    "https://openalex.org/W2108617939",
    "https://openalex.org/W4296701967",
    "https://openalex.org/W2745184686",
    "https://openalex.org/W2942735942",
    "https://openalex.org/W3176422463",
    "https://openalex.org/W6721598494",
    "https://openalex.org/W4309637085",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W4289533956",
    "https://openalex.org/W2141216835",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3202546167",
    "https://openalex.org/W2148922214",
    "https://openalex.org/W4381948587",
    "https://openalex.org/W3202285719",
    "https://openalex.org/W3175195562",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6744345652",
    "https://openalex.org/W2986836624",
    "https://openalex.org/W4288059420",
    "https://openalex.org/W3016364508",
    "https://openalex.org/W2742797553",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4385681149",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2480399835",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3174328218",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W4382652828",
    "https://openalex.org/W3189121698",
    "https://openalex.org/W4386566718",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3156082048",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4361217417",
    "https://openalex.org/W2810423422",
    "https://openalex.org/W4401043168",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3203314757",
    "https://openalex.org/W2764339638",
    "https://openalex.org/W4366733551",
    "https://openalex.org/W4287815536",
    "https://openalex.org/W2777514883",
    "https://openalex.org/W4307653761",
    "https://openalex.org/W3155807546"
  ],
  "abstract": "Building a skilled cybersecurity workforce is paramount to building a safer digital world. However, the diverse skill set, constantly emerging vulnerabilities, and deployment of new cyber threats make learning cybersecurity challenging. Traditional education methods struggle to cope with cybersecurity's rapidly evolving landscape and keep students engaged and motivated. Different studies on students' behaviors show that an interactive mode of education by engaging through a question-answering system or dialoguing is one of the most effective learning methodologies. There is a strong need to create advanced AI-enabled education tools to promote interactive learning in cybersecurity. Unfortunately, there are no publicly available standard question-answer datasets to build such systems for students and novice learners to learn cybersecurity concepts, tools, and techniques. The education course material and online question banks are unstructured and need to be validated and updated by domain experts, which is tedious when done manually. In this paper, we propose CyberGen, a novel unification of large language models (LLMs) and knowledge graphs (KG) to generate the questions and answers for cybersecurity automatically. Augmenting the structured knowledge from knowledge graphs in prompts improves factual reasoning and reduces hallucinations in LLMs. We used the knowledge triples from cybersecurity knowledge graphs (AISecKG) to design prompts for ChatGPT and generate questions and answers using different prompting techniques. Our question-answer dataset, CyberQ, contains around 4k pairs of questions and answers. The domain expert manually evaluated the random samples for consistency and correctness. We train the generative model using the CyberQ dataset for question answering task.",
  "full_text": "CyberQ: Generating Questions and Answers for Cybersecurity Education Using\nKnowledge Graph-Augmented LLMs\nGarima Agrawal1, Kuntal Pal1, Yuli Deng1, Huan Liu1, Ying-Chih Chen2\n1School of Computing and Augmented Intelligence, Arizona State University, USA\n2Mary Lou Fulton Teachers College, Arizona State University, USA\n(garima.agrawal; kkpal; ydeng19; huanliu; ychen495)@asu.edu\nAbstract\nBuilding a skilled cybersecurity workforce is paramount to\nbuilding a safer digital world. However, the diverse skill set,\nconstantly emerging vulnerabilities, and deployment of new\ncyber threats make learning cybersecurity challenging. Tra-\nditional education methods struggle to cope with cybersecu-\nrity’s rapidly evolving landscape and keep students engaged\nand motivated. Different studies on students’ behaviors show\nthat an interactive mode of education by engaging through a\nquestion-answering system or dialoguing is one of the most\neffective learning methodologies. There is a strong need to\ncreate advanced AI-enabled education tools to promote in-\nteractive learning in cybersecurity. Unfortunately, there are\nno publicly available standard question-answer datasets to\nbuild such systems for students and novice learners to learn\ncybersecurity concepts, tools, and techniques. The educa-\ntion course material and online question banks are unstruc-\ntured and need to be validated and updated by domain ex-\nperts, which is tedious when done manually. In this paper,\nwe propose CyberGen, a novel unification of large language\nmodels (LLMs) and knowledge graphs (KG) to generate the\nquestions and answers for cybersecurity automatically. Aug-\nmenting the structured knowledge from knowledge graphs in\nprompts improves factual reasoning and reduces hallucina-\ntions in LLMs. We used the knowledge triples from cyber-\nsecurity knowledge graphs (AISecKG) to design prompts for\nChatGPT and generate questions and answers using different\nprompting techniques. Our question-answer dataset, CyberQ,\ncontains around 4k pairs of questions and answers. The do-\nmain expert manually evaluated the random samples for con-\nsistency and correctness. We train the generative model using\nthe CyberQ dataset for question answering task.\nIntroduction\nCybersecurity education uses problem-based learning\n(PBL) (Dolmans and Schmidt 2010) to engage students in\nlearning complex tools and solving real-time multi-faceted\nthreat intelligence scenarios. It demands a progressive and\nadaptive learning strategy carefully designed to meet the\nlearning needs of students at different levels like K-12, un-\ndergraduate, graduate, and professional students. Traditional\ncybersecurity education systems struggle to keep pace with\nthe evolving threat landscape and understand the learning\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ngoals of users with diverse backgrounds. These systems\nneed help keeping the course material up to date and engag-\ning and motivating the students. Also, most of the current\ninstruction methods in teaching cybersecurity are teacher-\ncentered, favoring passive learning through listening and ob-\nserving lectures presented by the teacher. It creates a need\nto design an ”Active Learning” methodology (Bonwell and\nSutherland 1996) that requires students to not only cogni-\ntively engage with the course material (Bonwell and Eison\n1991) but also get involved and thinking about it critically\nrather than just passively receiving it (King 2002).\nDifferent frameworks studied the student behaviors to\nquantify the impact of instruction mode. One of the popular\nmethods is ICAP framework (Chi and Wylie 2014), which\ndifferentiates the students’ overt behaviors (Menekse et al.\n2013) into four modes as Interactive, Constructive, Active\nand Passive. It suggests that as the students become more\nengaged with the learning materials, from passive to active\nto constructive to interactive, their learning will increase.\nThe passive mode is defined as students receiving informa-\ntion or lectures, whereas active mode is searching for in-\nformation online by following the procedures provided by\ninstructors. In the constructive mode, students tend to un-\nderstand the concepts by self-constructing the outcomes for\na new situation using AI-based visualization tools like Con-\nceptMaps and Knowledge graphs (Agrawal et al. 2022). The\ninteractive mode loosely refers to human-computer systems\nin a joint-dialogue. Interactive learning requires substantive\ndialogue rather than parallel monologues (Chen and Terada\n2021). The learners engage in the highest level of learning\nwhen they interact with a device or a computer through a di-\nalogue. The question-answering systems through dialoguing\nare an effective way to promote cognitive engagement and\ninteractive learning. The ICAP framework (Chi et al. 2018)\nsupports using AI-based intelligent tutoring systems among\nstudents, especially for engineering courses.\nCybersecurity education requires cutting-edge AI tools\nto engage students and keep them updated on industry\ntrends. Unfortunately, there are no standardized datasets\nfor building AI-driven chatbot-style teaching tools in this\nfield. Instead, educational materials consist of unstructured\ntext from sources like lecture notes, books, websites, and\nvideos. Creating a question-answer database involves labor-\nintensive data collection efforts, as each source varies in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23164\nwriting style. Additionally, ensuring the accuracy and con-\nsistency of answers demands validation from cybersecu-\nrity experts, a time-consuming and costly process. Further-\nmore, the rapid evolution of cybersecurity technology ren-\nders course materials obsolete quickly.\nWe use a novel unification of large language models\n(LLMs) and knowledge graphs (KGs) to construct the\nquestion-answer dataset for cybersecurity education.\nLanguage models (LMs) such as BERT (Devlin et al.\n2018), RoBERTa (Liu et al. 2019), and T5 (Raffel et al.\n2020) are pre-trained models beneficial for various natu-\nral language processing (NLP) tasks, including question-\nanswering (Su et al. 2019) and text generation (Li et al.\n2022). Advanced LLMs like GPT-3, GPT-4, and Chat-\nGPT (Yang et al. 2023), with billions of parameters, have\ndemonstrated their potential in fields like education (Ma-\nlinka et al. 2023) and recommendation systems (Liu et al.\n2023). Conversely, KGs store vast amounts of informa-\ntion in triples (head entity, relation, tail entity) for struc-\ntured knowledge representation (Ji et al. 2021). Although\ntrained on extensive text data, LLMs sometimes produce\ninaccurate statements and mix facts (Shuster et al. 2021).\nKnowledge graphs provide precise but incomplete and non-\ngeneralizable knowledge (Abu-Salih 2021). There is a po-\ntential synergy between LLMs and KGs, with structured\nknowledge from KGs reducing LLM inaccuracies (Pan\net al. 2023; Agrawal et al. 2023a). Dialog models like\nLaMDA (Thoppilan et al. 2022) have used task-specific\nqueries to access structured knowledge through fine-tuning.\nIn our work, we leverage knowledge graph-enhanced\nLLMs generation, combining these two complemen-\ntary technologies. We used the knowledge triples from\nAISecKG (Agrawal et al. 2023b), the cybersecurity knowl-\nedge graph (KG), to prompt the large language model, Chat-\nGPT, and generate the cybersecurity-related questions and\nanswers. This approach improves the factual grounding and\nunlocks the reasoning capability of LLMs by providing the\ngrounding contexts using the chain of prompts (Wei et al.\n2022). We use three prompting techniques consecutively to\nautomate the generation of question-answer (QA), to be dis-\ncussed later in the third section. Cybersecurity domain hu-\nman experts evaluated the generated questions and answers\nto validate their correctness and consistency. Lastly, we train\na generative model on the cybersecurity QA dataset, for\nopen-ended question-answering.\nThe paper’s three key contributions are as follows::\n1. We present an innovative knowledge graph-enhanced\nLLMs generation method, CyberGen, that combines\nnatural generation capability of LLMs and reliable do-\nmain knowledge from KG to create QA pairs.\n2. We introduce CyberQ, a comprehensive question-\nanswer dataset meticulously curated and validated by do-\nmain experts (to the best of our knowledge, first in cyber-\nsecurity education).\n3. We show the application of CyberQ by training a gen-\nerative model for QA tasks. It can be used to build an\ninteractive system to educate students and beginners in\ncybersecurity concepts and tools.\nThe following sections cover related work, question-\nanswer generation CyberGen method, CyberQ dataset eval-\nuation, and model implementation for question-answering.\nRelated Work\nAI in Education:AI chatbots and Question-answering sys-\ntems are becoming popular in education to answer how-\nto questions, conduct quizzes and assessments, assist fac-\nulty, and provide administrative services (Chen, Cheng, and\nHeh 2021; Mzwri and Turcs´anyi-Szabo 2023). Several stud-\nies have shown that these chatbots have been perceived to\nbenefit the educational system mainly in the integration of\ncontents, quick access to educational information, allowing\nmultiple users (Wu et al. 2020), motivation and engagement\nof students (Adamopoulou and Moussiades 2020), and im-\nmediate assistance (Okonkwo and Ade-Ibijola 2020). How-\never, implementing chatbots in education faces challenges\nlike evaluating the effectiveness and students’ perception\nof using these tools, ensuring the accuracy of content, and\nmaintaining and updating the AI model. The other signifi-\ncant bottlenecks are the availability of knowledge banks and\nquestion-answer datasets to build these models (Okonkwo\nand Ade-Ibijola 2021).\nKnowledge Graphs in Education:State-of-the-art meth-\nods like knowledge graphs have been used in education\nto represent unstructured knowledge (Chen et al. 2018;\nMao 2021; Fariani, Junus, and Santoso 2023; Xia and Qi\n2023) and construct knowledge graph question-answering\nsystems (KGQA) (Agrawal, Bertsekas, and Liu 2023; Chen,\nWu, and Zaki 2023; Perez-Beltrachini et al. 2023). New\nprogramming questions based on knowledge graphs were\ngenerated by Chung et al. (Chung, Hsiao, and Lin 2023).\nWang et al. (Wang et al. 2022) used large language mod-\nels to generate educational question-answers automatically.\nEduChat (Dan et al. 2023) is an LLM-based chatbot to cre-\nate smart education for Chinese middle and high school\ncurricula. However, there are still challenges in generating\ndomain-specific questions and answer data using LLMs as\nthey tend to suffer from hallucinations. Domain experts must\nevaluate the generated texts for factual correctness.\nCybersecurity Education:It is a domain that can primar-\nily benefit from AI-based tools to develop an interactive\nquestion-answering system. Active learning approaches like\na search engine for scientific publications in cybersecu-\nrity (Oliveira, Sousa, and Prac ¸a 2021) and curriculum mod-\nules to teach cybersecurity (Chung 2017) have been pro-\nposed. Sayan et al. (Sayan, Hariri, and Ball 2017) built a cy-\nber security assistant to assist security analysts in gathering\nresources and information about cyber attacks and defenses.\nA syntactic matching approach to automatically generate\nshort factoid questions was tested on cybersecurity books\nand reports from 2008-2014 (Danon and Last 2017). How-\never, these methods are not scalable. Ji et al. (Ji, Choi, and\nGao 2022) developed a question-answering system for cyber\nthreat knowledge from open-source cyber threat intelligence\n(OSCTI) reports.\nThere is limited research in developing interactive AI-\nbased education solutions in cybersecurity for students\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23165\nand novice learners. Structured flow graphs were con-\nstructed from Capture-The-Flag (CTF) procedural cyberse-\ncurity texts (Pal et al. 2021) to teach students vulnerabil-\nity analysis. Knowledge graphs were proposed to guide stu-\ndents to work on cybersecurity projects (Deng et al. 2019;\nDeng, Zeng, and Huang 2021). A semi-automated approach\nwas used for constructing knowledge graphs from unstruc-\ntured cybersecurity course material to enhance the learning\nexperience of students (Agrawal et al. 2022). In this work,\na chatbot was also developed using an intent-classification\nSVM model. The students could query the bot to ask ques-\ntions about their cybersecurity project topics. The results of\nthe surveys and interviews to assess the students’ perception\nof using these tools show that the students found these tools\ninformative. However, the questions and answers dataset\nwas manually curated and limited to course project ques-\ntions. A comprehensive ontology, AISecKG (Agrawal et al.\n2023b), was proposed for learning cybersecurity. The triple\ndataset was annotated to generate cybersecurity knowledge\ngraphs and train a language model to identify cybersecurity-\nrelated named entities.\nDownstream applications like self-learning QA systems\nfor students need a scalable method to generate an open-\nended question-answer dataset. To address this gap, in this\npaper, we augment the knowledge triples from cybersecu-\nrity knowledge graphs, AISecKG, to automatically generate\nquestion-answers from LLMs, which are evaluated by the\ndomain expert. We also present a question-answering gener-\native model to answer the open-ended questions.\nQuestion-Answer Generation\nProblem Formulation:\nWe aim to generate open-ended questions and answers for\ntopics related to attacks and defense mechanisms. It is an ex-\nceptionally time-consuming and knowledge-intensive task\nfor a domain expert to create questions and answers spe-\ncific to exposed vulnerabilities, attacks, and the security de-\nfense tools and techniques. Even to automatically generate\nthe questions from LLMs, these topics need specific domain\nexpertise to design the prompts and validate the responses\ngenerated by the LLMs.\nWe use structured knowledge from cybersecurity KG to\nminimize domain expert involvement in prompt design. Our\nthree-step prompting method, CyberGen, mitigates halluci-\nnations in LLM responses. ChatGPT was selected over other\nLLMs due to its training with Reinforcement Learning from\nHuman Feedback (RLHF), which aligns its responses more\nclosely with human expectations and reduces hallucinations\ncompared to open-source counterparts.\nCybersecurity Knowledge-Graph\nAISecKG (Agrawal et al. 2023b) is a cybersecurity educa-\ntion ontology that describes the interactions between dif-\nferent concepts, applications, and roles in the cybersecurity\ndomain. These three categories have a further 12 types of\nentities. The concepts are classified as features, functions,\ndata, attacks, vulnerabilities, and techniques. The applica-\ntions denote the tools, systems, and apps. The roles are user,\nFigure 1: CyberGen: KG-augmented LLMs pipeline to gen-\nerate question-answers by using Zero-Shot (ZS), Few-Shot\n(FS) and Ontology-Driven (OD) prompt-chaining.\nattacker, and security teams. There are nine relations to rep-\nresent the fundamental interactions between these cyberse-\ncurity entities. The schema presented in the ontology con-\ntains 68 unique edges.\nIn this work, we selected the ten most prevalent entities\nfor each type from AISecKG, totaling 120 entities for ques-\ntion and answer generation. Since our focus is on vulner-\nabilities and attacks, we chose the related schema tuples.\nFor instance, the entity “session ID”, categorized as a fea-\nture, the tuples considered from ontology were (‘attacker,\n‘can\nexploit, ‘feature’) and (‘securityTeam’, ‘can analyze’,\n‘feature’). The prompts were designed using these tuples.\nCyberGen\nWe refer to our prompting method as CyberGen. In our\napproach, we employ three different prompt techniques to\ngenerate questions from ChatGPT: Zero-Shot (ZS), Few-\nShot (FS) within context, and Ontology-Driven (OD) us-\ning domain-specific schemas. These prompting techniques\nare used sequentially to provide context and enhance the\nprompts with knowledge from AISecKG triples. Our goal is\nnot to compare these techniques or determine superiority but\nto create a sequence of prompts, forming a coherent thought\nprocess for ChatGPT. This step-by-step prompt chaining as-\nsists the language model in generating a continuous stream\nof thoughts and constructing a mental mind map.In Table 1,\nwe provide examples of prompts and the resulting questions\nusing these techniques. These samples illustrate how incor-\nporating structured knowledge enhances the complexity and\ndepth of the generated questions. Domain experts validate\nthe responses generated by LLMs, as depicted in the pipeline\noutlined in Figure 1, which illustrates the methodology we\nemploy to generate question-answers. We term our final val-\nidated dataset as “CyberQ.”\nStep-wise Prompting Techniques: In the initial step, we\nemploy a Zero-Shot (ZS) approach by presenting ChatGPT\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23166\nwith an entity and asking it to provide a paragraph ex-\nplaining that entity. This helps establish context and assess\nChatGPT’s familiarity with the concept. In the second\nstep, we use the generated write-up to formulate questions\nand answers. Following that, we employ Few-Shot (FS)\nprompting by offering two sample in-context questions\nfrom the ontology tuple, prompting ChatGPT to create\nquestions and answers accordingly. Lastly, we utilize a\nscenario-based prompt from the knowledge graph ontology\nschema to instruct ChatGPT in generating specific, intricate\nquestions with comprehensive answers, rather than generic\nones.\nZero-Shot (ZS) Prompting Considering the entity,\n“session ID, ” from AISecKG, we began with the initial\nprompt, “Tell me about session ID. ”Following ChatGPT’s\ngeneration of a brief paragraph on the entity, we presented\na subsequent prompt: “Generate questions with answers\non session ID based on the above write-up. ” As shown\nin Table 1, the questions generated in this case tend to\nbe generic. Additionally, we asked ChatGPT to rephrase\nthese questions, which were later used to test the model.\nWe skipped the entity if the initial paragraph provided\nafter the first response was unsatisfactory. This step serves\nto maintain factual accuracy in responses and reduce the\noccurrence of erroneous information.\nIn-Context Few-Shot (FS) Prompting During Few-\nShot prompting, two sample questions were provided for\ngenerating similar questions. These sample questions were\nmanually curated and carefully selected, aligning with the\nschema and paths from the cybersecurity knowledge graph,\nAISecKG. In this scenario, we noticed that the generated\nquestions often mirrored the pattern in the provided exam-\nples. As demonstrated in Table 1, some of the questions\nwere essentially paraphrases of the example questions.\nIn-Domain Ontology-Driven (OD) Prompting The\nthird method is using the schema triples from the AISecKG\nontology. For example, the corresponding triple for“session\nID” in the AISecKG dataset is (‘attacker, ’ ‘can\nexploit,\n‘session ID’). The prompt designed for this method was\nby augmenting the domain-specific knowledge triple in\nthe prompt. We gave the context as a use case and asked\nChatGPT to generate questions and answers for that situa-\ntion. For example, “Generate ten questions with answers\non situations in which attacker can exploit session ID”. We\nused two or three tuples from the schema related to attacks\nand vulnerabilities for each entity to generate questions and\nanswers for different scenarios. As shown in the Table 1,\nthe generated responses are more complex and specific to a\nsituation, and domain coverage is much higher.\nMotivation behind CyberGen\nThe motivation for using the step-wise prompt chaining, Cy-\nberGen using these three techniques can be summarized as\nfollows:\nContextual Depth: The step-by-step chaining approach\nallows the language model to delve deeper into the topic\nby gradually building upon the information provided in\nprevious prompts. This incremental approach helps create\na richer context and ensures that subsequent questions and\nanswers are more informative and contextually relevant.\nKnowledge Augmentation: Each prompting method\ncontributes knowledge and context to the questions and\nanswers. ZS prompting lays the foundation, while FS\nand OD add more context and complexity. This approach\nallows the LLMs to generate a wide spectrum of questions,\ncovering both basic and advanced levels of knowledge.\nQuestion Types and Complexity level:In NLP, Question-\nAnswering tasks fall into two broad categories: Open-\nDomain Question Answering (ODQA) and Closed-Domain\nQuestion Answering (CDQA). However, our QA dataset\nfocuses exclusively on cybersecurity, leading us to adopt a\nmore specific terminology: Open-Book and Closed-Book\nQA solutions. For the Close-Book approach, we leverage\nzero-shot (ZS) prompting to generate questions that require\ncomprehensive knowledge. These questions are similar to\nreading comprehension tasks, like the SQuAD (Rajpurkar\net al. 2016) dataset. These closed-book questions, generated\nusing ZS prompting, are relatively straightforward and more\naccessible to answer. The Few-Shot (FS) and Ontology-\nDriven (OD) questions fall under the Open-Book category,\ndemanding a deeper understanding of the subject matter.\nBy employing these three distinct techniques, our dataset,\nCyberQ, offers a well-balanced mix of straightforward and\nchallenging questions.\nSeamless QA Generation:In the rapidly evolving field of\ncybersecurity, our pipeline ensures effortless updates to the\nQA dataset. Leveraging two key inputs:x(LLM) and y(KG),\nit uses, automated process(our method) to access reliable\n(KG) and generate multi-format questionsefficiently. This\napproach is generalizable and can be readily applied to\ncreate QA datasets for different domains.\nCyberQ Dataset\nAround 4k questions were generated by chaining the three\nprompting techniques as proposed in CyberGen. Each tech-\nnique’s distributions of questions are as shown in Table 2.\nMost of the questions generated by ChatGPT are Wh-type\nseeking content information. The focus is on explanation of\nconcepts, use of tools, and ‘how-to’ setup instructions. Some\nstrategic questions are related to attack analysis and defense\ntechniques. All the questions are open-ended with varying\nlength answers. First the domain expert validated the ques-\ntions and answers to ensure they were consistent and factu-\nally correct as per domain, and students can use those. Then\nwe did a statistical analysis to gain more insights of the val-\nidated data.\nDomain Expert Evaluation\nIn education, ensuring the accuracy of learning materials is\ncrucial. To validate the generated cybersecurity questions\nand answers, we enlisted domain experts who possess sub-\nstantial knowledge and experience. In our case, two of the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23167\nMethod 1—Zero-Shot (ZS) Prompting\nEntity: [“session ID”] , Ontology: - None\nPrompt 1: “Tell me about session ID.”\nResponse —\nPara: “A session ID (session identifier) is a unique identifier that is assigned to user or a client during a session...”\nPrompt 2: “Generate questions with answers based on the above write-up.”\nResponse —\nQ1: What is a session ID, and why is it important?\nAnswer: “A session ID is a unique identifier assigned to a user during a session with a web server. It is important...”\nQ2: How is a session ID generated?\nAnswer: “A session ID is generated on the server-side and is usually a random string of characters...”\nMethod 2 — In-Context Few-Shot (FS) Prompting\nEntity: [“session ID”, “vulnerability”], Ontology: (“feature”, can expose, “vulnerability”)\nPrompt 3:“Generate questions with answers similar to questions, Q1: Will the app become vulnerable if the session\nID is exposed?, Q2: Which tools are used to analyze vulnerability exposed by the session ID?”\nResponse —\nQ1: “What are some commonly used tools to analyze vulnerabilities related to the session ID?”\nAnswer: “There are several tools that can be used to analyze vulnerabilities related to session ID, such as Burp Suite..”\nQ2: “How does exposing a session ID make an application vulnerable?”\nAnswer: “If a session ID is exposed, an attacker could potentially use it to hijack the user’s session and gain unautho-\nrized...”\nMethod 3 — In-Domain Ontology-Driven (OD) Prompting\nEntity: [“session ID”, “vulnerability”], Ontology: (“feature”, can expose, “vulnerability”)\nPrompt 4:“Generate questions with answers on situations in which Session ID can expose vulnerabilities.”\nResponse —\nQ1:What is session replay, and how does it exploit session IDs?\nAnswer: “Session replay is an attack where an attacker records a user’s session, including their session ID, and re-\nplays...”\nQ2: What is session hijacking, and how does it expose a user’s session ID to attackers?\nAnswer: “Session hijacking is an attack where an attacker steals a user’s session ID and uses it to impersonate the\nuser...”\nEntity: [“session ID”, “attacker”], Ontology: [(“attacker”, can exploit, “feature”)]\nPrompt 5:“Generate questions with answers on situations in which an attacker can exploit session ID.”\nResponse —\nQ1: What is session fixation, and how can an attacker exploit it?\nAnswer:“Session fixation is an attack where an attacker sets a user’s session ID before the user logs in...”\nQ2: In what ways can a user’s session ID be compromised by attackers?\nAnswer: “An attacker can steal a user’s session ID through a variety of means, such as intercepting network traffic...”\nEntity: [“session ID”,“securityTeam”], Ontology: [(“securityTeam”, can analyze, “feature”)]\nPrompt 6:“Generate questions with answers on situations in which an ethical hacker can analyze session ID.”\nResponse —\nQ1: How can an ethical hacker analyze the effectiveness of a website’s session ID management?\nAnswer: “An ethical hacker can analyze the effectiveness of a website’s session ID management by testing for com-\nmon...”\nQ2: What tools can an ethical hacker use to analyze session ID vulnerabilities?\nAnswer: “An ethical hacker can use a variety of tools to analyze session ID vulnerabilities, such as OW ASP ZAP...”\nTable 1: CyberQ samples with varying complexity by chaining three prompting techniques of CyberGen.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23168\nFigure 2: Word cloud generated for CyberQ.\nauthors are graduate researchers in cybersecurity, and the\nthird author is an experienced cybersecurity instructor teach-\ning at the university level. These experts manually reviewed\nand validated the responses. The first and second authors de-\nsigned and generated the prompts, while the third author per-\nformed the manual validation. The question-answers were\nevaluated based on feedback and observations from domain\nexperts, yielding the following findings:\nKnowledgeable: The primary evaluation criterion in the\ncontext of education is to determine the accuracy and knowl-\nedge level of ChatGPT’s responses in cybersecurity.\n• Over 95% of the question-answers generated by Chat-\nGPT in the cybersecurity domain were factually correct.\n• Some questions related to the analysis and mitigation of\nattacks were answered incorrectly, leading to the rejec-\ntion of such question-answer pairs.\n• Examples include hallucinations when answering ques-\ntions about using “network ingress filtering” techniques\nto mitigate “Smurf attacks” and how “network adminis-\ntrators” can configure devices to prevent these attacks.\nConsistency: The evaluation included an assessment of the\nconsistency of the generated responses, focusing on rele-\nvance to the context.\n• ChatGPT tends to stay on-topic but can sometimes pro-\nduce responses that lack context, leading to unrelated\nquestions. For example, in a prompt about “Trojan Horse\nmalware,” many generated questions were unrelated.\nReliability: The reliability of ChatGPT’s responses was\nevaluated, revealing instances of biased responses.\n• Responses can be a mix of ideas, especially when there\nis insufficient supporting information.\n• In some cases, ChatGPT provided different answers to\nsimilar questions.\n• For example, ChatGPT suggested “Windows” is secure\neven after examples of vulnerabilities were provided.\nHowever, its stance changes when prompted differently.\nGeneral observationsFew other general observations were\nas follows:\n• ChatGPT displayed repetition in its responses, often\nparaphrasing questions and answers, leading to generic\nand limited technical details.\n• Initially, ChatGPT struggled to recognize certain entities,\nlike ”brute-force scripts” and ”client-network,” when us-\ning ZS prompting, but improved with additional context.\n• Zero-Shot prompts generated generic WH-questions,\nwhile Ontology-Driven situation-based questions be-\ncame more complex and specific with added context.\n• However, in few instances with Few-Shot and Ontology-\nDriven prompts, there were higher occurrences of hallu-\ncination and repetition when additional context was in-\ntroduced. ChatGPT occasionally appeared confused and\nrambled or repeated itself in such cases.\nStatistical Analysis of Dataset\nThe question-answers was statistically analyzed based on\nthe readability, answer length, and vocabulary diversity\nwith respect to attacks. Table 2 shows the computed metric\nvalues for all three prompting techniques.\nReadability: Readability assesses how easily the aver-\nage reader comprehends a text, considering factors like\nlexical, syntactic, semantic, and stylistic complexity. To\ngauge language readability complexity, we employed\nthe Flesch-Kincaid Grade Level and Gunning Fog index\nmetrics. These metrics express readability in U.S. grade\nlevels, spanning from fifth grade to college graduates and\nprofessionals. They also indicate the years of education\nneeded to grasp the text; lower grades signify higher\nscores. For our QA dataset, scores fall within the 4-16\nrange, indicating grammatical correctness and consistency,\ntargeting college graduates and professionals. These scores\nwere calculated using the textstat library in Python.\nAnswer Length: We analyzed the answer length: and\ntermed the questions into “very short” for less than ten\ntokens, “short” for 10 to 20 tokens, and “long” for over 20\ntokens. In Table 2, it is evident that due to the domain’s\ncomplexity, we have fewer “very short” (30), mostly “short”\n(1061), and “long” (2439) questions.\nVocabulary Diversity: V ocabulary diversity, concern-\ning attacks, is measured using the token-type ratio (TTR).\nTTR calculates the ratio of unique words to the total number\nof words in the text, indicating lexical variation. A high\nTTR suggests a broader, less focused vocabulary. In our\ncontext, a high TTR suggests that questions and answers are\nmore generic, not solely centered on attacks. In Table 2, it’s\nevident that TTR ratios are higher for Zero-Shot and Few-\nShot questions compared to Ontology-Driven questions and\nanswers. This discrepancy arises because Ontology-Driven\nquestions explicitly concentrate on scenarios related to\nattacks, vulnerabilities, and attackers. The word clouds\ngenerated from CyberQ dataset Figure 2 show the most\nfrequent words in the dataset.\nApplication of CyberQ\nOur approach to data generation using LLMs and the gen-\nerated question-answer dataset have various applications in\ncybersecurity education. Large language models (LLMs) are\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23169\nQuestions Answers\nZS FS\nOD ZS FS OD\nReadability\nFlesch-Kincaid 6.3\n9.9 8.3 14.6 16.7 15.6\nGunning Fog 4.78 6.46 4.83 9.68 11.11 9.88\nAnswer Length\nV\nery Short – – – 22 0 8\nShort – – – 515 16 530\nLong – – – 490 316 1633\nVocab\nulary Diversity Token-type-ratio 0.16 0.17 0.08 0.17 0.20 0.09\nTotal\nQuestions 1027 332 2171\nTable 2: Statistical analysis of dataset to gain insights from three prompting techniques.\nDataset Model\nBLEU ROUGE METEORRouge1 Rouge2\nRougeL RougeLS\nOD T5-S 0.10\n0.39 0.22 0.35 0.35 0.28\nT5-B 0.12 0.41 0.24 0.37 0.37 0.30\nFS T5-S 0.04\n0.32 0.20 0.29 0.29 0.20\nT5-B 0.04 0.33 0.20 0.30 0.30 0.21\nZS T5-S 0.17\n0.43 0.27 0.40 0.40 0.33\nT5-B 0.18 0.46 0.29 0.43 0.43 0.36\nTable 3: Question-Answering Performance (BLEU,\nROUGE and METEOR) of each model on the Question-\nAnswering Task for three subsets of CyberQ.\noften not available to the students. Also, LLMs tend to hal-\nlucinate, so their responses can only be trusted for educa-\ntion purposes with expert validation. It is hard for students\nto develop a QA agent to help them learn. Here, we show\nan application of using such a QA dataset to create a small\nlanguage model with knowledge of the security domain.\nQuestion-Answering Model\nWe develop an open-ended question-answering model for\ncybersecurity education. The answers to the questions are\nelaborate, detailing a process, and often contain abstract in-\nformation. Therefore, we consider developing a generative\nQA model that has been fine-tuned on CyberQ dataset. The\nbase of our QA models is the T5 model (Roberts et al. 2019):\nt5-small and t5-base. We separately trained our models on\nthree versions of data in CyberQ, Zero-Shot, in-context Few-\nShot, and in-domain Ontology-Driven methods. For each\ndataset, we split the dataset into train-dev-test in the ratio\n70:20:10. We trained all the models for 20 epochs with a\nlearning rate of 5e-4 and batch size of 20 with a maximum\nsequence length of 128. The question-answer dataset Cy-\nberQ and implementation code for the QA model are avail-\nable in our github repository 1.\nResults and Analysis\nThe results of our QA models can be seen in Table 3. Since\nour task is generative QA, the answers are elaborate and of-\nten might not precisely match the generated gold answers.\nHence, we measure the performance using three popular\n1https://github.com/garima0106/AISECKG-QA-Dataset.git\ngenerative metrics: BLEU (Papineni et al. 2002), ROUGE\n(Lin 2004), and METEOR (Banerjee and Lavie 2005). We\ncompare our prediction results with the ChatGPT-generated\nexpert-validated answers. We can see that for each of the\nthree datasets, t5-base outperforms t5-small in almost all the\nmetrics, which shows that with the increase in model pa-\nrameters, the performance also increases. We also see that\nour model performs best for the Zero-Shot dataset, which\nwe believe is because the questions and answers in ZS are\nsimple and straight-forward, and the model finds it easy to\nanswer those questions. The model needs to be trained on\nmore training samples to improve the performance, and con-\ntext information can be added to the training data. We do not\nuse large models above the T5-base because large computa-\ntions to train these models may not be readily available to all\nstudents or novice professionals.\nConclusion\nThis work presents an open-ended question-answer (QA)\ndataset, CyberQ, in cybersecurity education. We also trained\nsmall Question-Answering models (having fewer parame-\nters) based on CyberQ dataset to build AI-enabled self-paced\ninteractive education systems. Such tools are effective learn-\ning modes, especially for complex subjects like cyberse-\ncurity. However, creating a question-answering knowledge\nbase for cybersecurity is cognitively demanding for a subject\nmatter expert. Even to use the LLMs to generate questions\nand answers on cybersecurity automatically, domain exper-\ntise is needed to design the specific kinds of prompts. This\nwork shows a novel method, CyberGen, to augment knowl-\nedge triples from cybersecurity knowledge graphs AISecKG\nto create prompts and generate questions and answers from\nLLMs which were validated by the domain expert. In this\nstudy, we demonstrate the capability of ChatGPT in gen-\nerating QA dataset. However, our methods are applicable\nacross various language models, such as Flan-T5 (Chung\net al. 2022), or Llama (Touvron et al. 2023), to generate ad-\nditional QA pairs. Besides its primary use in QA tasks, this\ndataset can be a valuable resource for students in other cy-\nbersecurity tasks, such as vulnerability or binary analysis.\nWe aim to showcase the versatility of our approach in sup-\nporting a broader range of applications in different domains.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23170\nAcknowledgments\nWe are thankful to National Science Foundation under Grant\nNo. 2114789 for supporting this research work. Any opin-\nions, findings, and conclusions or recommendations ex-\npressed in this material are those of the author(s) and do not\nnecessarily reflect the views of the National Science Foun-\ndation.\nReferences\nAbu-Salih, B. 2021. Domain-specific knowledge graphs:\nA survey. Journal of Network and Computer Applications,\n185: 103076.\nAdamopoulou, E.; and Moussiades, L. 2020. Chatbots: His-\ntory, technology, and applications. Machine Learning with\nApplications, 2: 100006.\nAgrawal, G.; Bertsekas, D.; and Liu, H. 2023. Auction-\nBased Learning for Question Answering over Knowledge\nGraphs. Information, 14(6): 336.\nAgrawal, G.; Deng, Y .; Park, J.; Liu, H.; and Chen, Y .-\nC. 2022. Building Knowledge Graphs from Unstructured\nTexts: Applications and Impact Analyses in Cybersecurity\nEducation. Information, 13(11): 526.\nAgrawal, G.; Kumarage, T.; Alghami, Z.; and Liu, H. 2023a.\nCan Knowledge Graphs Reduce Hallucinations in LLMs?:\nA Survey. arXiv preprint arXiv:2311.07914.\nAgrawal, G.; Pal, K.; Deng, Y .; Liu, H.; and Baral, C. 2023b.\nAISecKG: Knowledge Graph Dataset for Cybersecurity Ed-\nucation. AAAI-MAKE 2023: Challenges Requiring the Com-\nbination of Machine Learning 2023.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An Automatic\nMetric for MT Evaluation with Improved Correlation with\nHuman Judgments. In Proceedings of the ACL Workshop\non Intrinsic and Extrinsic Evaluation Measures for Ma-\nchine Translation and/or Summarization, 65–72. Ann Arbor,\nMichigan: Association for Computational Linguistics.\nBonwell, C. C.; and Eison, J. A. 1991. Active learning: Cre-\nating excitement in the classroom. 1991 ASHE-ERIC higher\neducation reports. ERIC.\nBonwell, C. C.; and Sutherland, T. E. 1996. The active\nlearning continuum: Choosing activities to engage students\nin the classroom. New directions for teaching and learning,\n1996(67): 3–16.\nChen, L. E.; Cheng, S. Y .; and Heh, J.-S. 2021. Chatbot:\na question answering system for student. In 2021 Inter-\nnational Conference on Advanced Learning Technologies\n(ICALT), 345–346. IEEE.\nChen, P.; Lu, Y .; Zheng, V . W.; Chen, X.; and Yang, B. 2018.\nKnowedu: A system to construct knowledge graph for edu-\ncation. Ieee Access, 6: 31553–31563.\nChen, Y .; Wu, L.; and Zaki, M. J. 2023. Toward Subgraph-\nGuided Knowledge Graph Question Generation With Graph\nNeural Networks. IEEE Transactions on Neural Networks\nand Learning Systems.\nChen, Y .-C.; and Terada, T. 2021. Development and valida-\ntion of an observation-based protocol to measure the eight\nscientific practices of the next generation science standards\nin K-12 science classrooms. Journal of Research in Science\nTeaching, 58(10): 1489–1526.\nChi, M. T.; Adams, J.; Bogusch, E. B.; Bruchok, C.; Kang,\nS.; Lancaster, M.; Levy, R.; Li, N.; McEldoon, K. L.; Stump,\nG. S.; et al. 2018. Translating the ICAP theory of cognitive\nengagement into practice. Cognitive science, 42(6): 1777–\n1832.\nChi, M. T.; and Wylie, R. 2014. The ICAP framework: Link-\ning cognitive engagement to active learning outcomes. Edu-\ncational psychologist, 49(4): 219–243.\nChung, C.-Y .; Hsiao, I.-H.; and Lin, Y .-L. 2023. AI-assisted\nprogramming question generation: Constructing semantic\nnetworks of programming knowledge by local knowledge\ngraph and abstract syntax tree. Journal of Research on Tech-\nnology in Education, 55(1): 94–110.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nChung, W. 2017. Developing curricular modules for cyber-\nsecurity informatics: An active learning approach. In 2017\nIEEE International Conference on Intelligence and Security\nInformatics (ISI), 164–166. IEEE.\nDan, Y .; Lei, Z.; Gu, Y .; Li, Y .; Yin, J.; Lin, J.; Ye, L.; Tie,\nZ.; Zhou, Y .; Wang, Y .; et al. 2023. EduChat: A Large-Scale\nLanguage Model-based Chatbot System for Intelligent Edu-\ncation. arXiv preprint arXiv:2308.02773.\nDanon, G.; and Last, M. 2017. A syntactic approach\nto domain-specific automatic question generation. arXiv\npreprint arXiv:1712.09827.\nDeng, Y .; Lu, D.; Huang, D.; Chung, C.-J.; and Lin, F. 2019.\nKnowledge graph based learning guidance for cybersecurity\nhands-on labs. In Proceedings of the ACM conference on\nglobal computing education, 194–200.\nDeng, Y .; Zeng, Z.; and Huang, D. 2021. Neocyberkg: En-\nhancing cybersecurity laboratories with a machine learning-\nenabled knowledge graph. In Proceedings of the 26th ACM\nConference on Innovation and Technology in Computer Sci-\nence Education V . 1, 310–316.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDolmans, D.; and Schmidt, H. 2010. The problem-based\nlearning process. Lessons from problem-based learning, 13–\n20.\nFariani, R. I.; Junus, K.; and Santoso, H. B. 2023. A Sys-\ntematic Literature Review on Personalised Learning in the\nHigher Education Context. Technology, Knowledge and\nLearning, 28(2): 449–476.\nJi, S.; Pan, S.; Cambria, E.; Marttinen, P.; and Philip, S. Y .\n2021. A survey on knowledge graphs: Representation, ac-\nquisition, and applications. IEEE transactions on neural\nnetworks and learning systems, 33(2): 494–514.\nJi, Z.; Choi, E.; and Gao, P. 2022. A Knowledge Base Ques-\ntion Answering System for Cyber Threat Knowledge Acqui-\nsition. In 2022 IEEE 38th International Conference on Data\nEngineering (ICDE), 3158–3161. IEEE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23171\nKing, A. 2002. Structuring peer interaction to promote high-\nlevel cognitive processing. Theory into practice, 41(1): 33–\n39.\nLi, J.; Tang, T.; Zhao, W. X.; Nie, J.-Y .; and Wen, J.-R. 2022.\nPretrained language models for text generation: A survey.\narXiv preprint arXiv:2201.05273.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain: Association for Computational\nLinguistics.\nLiu, J.; Liu, C.; Lv, R.; Zhou, K.; and Zhang, Y . 2023. Is\nchatgpt a good recommender? a preliminary study. arXiv\npreprint arXiv:2304.10149.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMalinka, K.; Peres´ıni, M.; Firc, A.; Hujn ´ak, O.; and Janus,\nF. 2023. On the educational impact of ChatGPT: Is Artificial\nIntelligence ready to obtain a university degree? InProceed-\nings of the 2023 Conference on Innovation and Technology\nin Computer Science Education V . 1, 47–53.\nMao, Y . 2021. Summary and evaluation of the application\nof knowledge graphs in education 2007–2020. Discrete Dy-\nnamics in Nature and Society, 2021: 1–10.\nMenekse, M.; Stump, G. S.; Krause, S.; and Chi, M. T. 2013.\nDifferentiated overt learning activities for effective instruc-\ntion in engineering classrooms. Journal of Engineering Ed-\nucation, 102(3): 346–374.\nMzwri, K.; and Turcs´anyi-Szabo, M. 2023. Internet Wizard\nfor Enhancing Open Domain Question Answering Chatbot\nKnowledge-base in Education.\nOkonkwo, C. W.; and Ade-Ibijola, A. 2020. Python-Bot:\nA chatbot for teaching python programming. Engineering\nLetters, 29(1).\nOkonkwo, C. W.; and Ade-Ibijola, A. 2021. Chatbots appli-\ncations in education: A systematic review. Computers and\nEducation: Artificial Intelligence, 2: 100033.\nOliveira, N.; Sousa, N.; and Prac ¸a, I. 2021. A Search Engine\nfor Scientific Publications: A Cybersecurity Case Study. In\nInternational Symposium on Distributed Computing and Ar-\ntificial Intelligence, 108–118. Springer.\nPal, K. K.; Kashihara, K.; Banerjee, P.; Mishra, S.; Wang, R.;\nand Baral, C. 2021. Constructing flow graphs from proce-\ndural cybersecurity texts. arXiv preprint arXiv:2105.14357.\nPan, S.; Luo, L.; Wang, Y .; Chen, C.; Wang, J.; and Wu,\nX. 2023. Unifying Large Language Models and Knowledge\nGraphs: A Roadmap. arXiv preprint arXiv:2306.08302.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, 311–318. Philadel-\nphia, Pennsylvania, USA: Association for Computational\nLinguistics.\nPerez-Beltrachini, L.; Jain, P.; Monti, E.; and Lapata,\nM. 2023. Semantic Parsing for Conversational Ques-\ntion Answering over Knowledge Graphs. arXiv preprint\narXiv:2301.12217.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSquad: 100,000+ questions for machine comprehension of\ntext. arXiv preprint arXiv:1606.05250.\nRoberts, A.; Raffel, C.; Lee, K.; Matena, M.; Shazeer, N.;\nLiu, P. J.; Narang, S.; Li, W.; and Zhou, Y . 2019. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer.\nSayan, C.; Hariri, S.; and Ball, G. 2017. Cyber security as-\nsistant: Design overview. In 2017 IEEE 2nd International\nWorkshops on Foundations and Applications of Self* Sys-\ntems (FAS* W), 313–317. IEEE.\nShuster, K.; Poff, S.; Chen, M.; Kiela, D.; and Weston, J.\n2021. Retrieval augmentation reduces hallucination in con-\nversation. arXiv preprint arXiv:2104.07567.\nSu, D.; Xu, Y .; Winata, G. I.; Xu, P.; Kim, H.; Liu, Z.; and\nFung, P. 2019. Generalizing question answering system with\npre-trained language model fine-tuning. In Proceedings of\nthe 2nd Workshop on Machine Reading for Question An-\nswering, 203–211.\nThoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kul-\nshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du,\nY .; et al. 2022. Lamda: Language models for dialog appli-\ncations. arXiv preprint arXiv:2201.08239.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWang, Z.; Valdez, J.; Basu Mallick, D.; and Baraniuk, R. G.\n2022. Towards human-like educational question generation\nwith large language models. In International conference on\nartificial intelligence in education, 153–166. Springer.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWu, E. H.-K.; Lin, C.-H.; Ou, Y .-Y .; Liu, C.-Z.; Wang, W.-\nK.; and Chao, C.-Y . 2020. Advantages and constraints of a\nhybrid model K-12 E-Learning assistant chatbot. Ieee Ac-\ncess, 8: 77788–77801.\nXia, X.; and Qi, W. 2023. learning behavior interest propa-\ngation strategy of MOOCs based on multi entity knowledge\ngraph. Education and Information Technologies, 1–29.\nYang, J.; Jin, H.; Tang, R.; Han, X.; Feng, Q.; Jiang, H.;\nYin, B.; and Hu, X. 2023. Harnessing the power of llms in\npractice: A survey on chatgpt and beyond. arXiv preprint\narXiv:2304.13712.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23172",
  "topic": "Computer security",
  "concepts": [
    {
      "name": "Computer security",
      "score": 0.5348758697509766
    },
    {
      "name": "Graph",
      "score": 0.5198822021484375
    },
    {
      "name": "Computer science",
      "score": 0.47052252292633057
    },
    {
      "name": "Mathematics education",
      "score": 0.3493092358112335
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2253952920436859
    },
    {
      "name": "Psychology",
      "score": 0.2213197946548462
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    }
  ]
}