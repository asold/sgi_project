{
  "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
  "url": "https://openalex.org/W3194309076",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2767776798",
      "name": "Ruiqi Zhong",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2098551379",
      "name": "Kristy Lee",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A1992233393",
      "name": "Zheng Zhang",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2056960045",
      "name": "Dan Klein",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6824034269",
    "https://openalex.org/W3114796327",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2964756555",
    "https://openalex.org/W3126259453",
    "https://openalex.org/W3100452485",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3021336872",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3094300879",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3114114934",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2992306696",
    "https://openalex.org/W2753160622",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3154200459",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2963777589",
    "https://openalex.org/W3157005959",
    "https://openalex.org/W3167525829",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W2969513720",
    "https://openalex.org/W3178814223",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W3190860428",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3102743123",
    "https://openalex.org/W2893160345",
    "https://openalex.org/W4293350112",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2970200208",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W3085177480",
    "https://openalex.org/W3158635868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962910668",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W3118905363",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W4287075708",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3043372854",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W2777746208",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W1527313647",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2963709474",
    "https://openalex.org/W2949433733"
  ],
  "abstract": "Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can “prompt” the LM with the review and the label description “Does the user like this movie?”, and ask whether the next word is “yes” or “no”. However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2856–2878\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2856\nAdapting Language Models for Zero-shot Learning by Meta-tuning on\nDataset and Prompt Collections\nRuiqi Zhong Kristy Lee ∗ Zheng Zhang∗ Dan Klein\nComputer Science Division, University of California, Berkeley\n{ruiqi-zhong, kristylee, zhengzhang1216, klein}@berkeley.edu\nAbstract\nLarge pre-trained language models (LMs)\nsuch as GPT-3 have acquired a surprising abil-\nity to perform zero-shot learning. For exam-\nple, to classify sentiment without any train-\ning examples, we can “prompt\" the LM with\nthe review and the label description “ Does\nthe user like this movie? \", and ask whether\nthe next word is “ Yes\" or “ No\". However,\nthe next word prediction training objective\nis still misaligned with the target zero-shot\nlearning objective. To address this weakness,\nwe propose meta-tuning, which directly opti-\nmizes the zero-shot learning objective by ﬁne-\ntuning pre-trained language models on a col-\nlection of datasets. We focus on classiﬁcation\ntasks, and construct the meta-dataset by ag-\ngregating 43 existing datasets and annotating\n441 label descriptions in a question-answering\n(QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-\nsized QA model and the previous SOTA zero-\nshot learning system based on natural lan-\nguage inference. Additionally, increasing pa-\nrameter count from 220M to 770M improves\nAUC-ROC scores by 6.3%, and we forecast\nthat even larger models would perform bet-\nter. Therefore, measuring zero-shot learning\nperformance on language models out-of-the-\nbox might underestimate their true potential,\nand community-wide efforts on aggregating\ndatasets and unifying their formats can help\nbuild models that answer prompts better.\n1 Introduction\nThe goal of zero-shot classiﬁcation (ZSC) is to\nclassify textual inputs using label descriptions\nwithout any examples (Yin et al., 2019). Large\nlanguage models - whose only training objective\nis to predict the next word given the context - have\nacquired a surprising ability to perform ZSC (Rad-\nford et al., 2019; Brown et al., 2020; Le Scao and\nRush, 2021). For example, to classify whether the\nsentence “This movie is amazing!\" is positive, we\ncan prompt the language model with the context\n“Review: This movie is amazing! Positive Re-\nview? ___ \", and check whether the next word\nis more likely to be “ Yes\" or “ No\" (Zhao et al.,\n2021). To convert ZSC into a language modeling\n(LM) task that an LM model is likely to perform\nwell, many recent works focus on ﬁnding better\nprompts (Shin et al., 2020; Schick and Schütze,\n2020a,b; Gao et al., 2021).\nHowever, the LM training objective is corre-\nlated but still misaligned with the target objective\nto answer prompts. Our work addresses this weak-\nness by directly optimizing the zero-shot classi-\nﬁcation objective through ﬁne-tuning (Section 4).\nThis requires us to 1) unify different classiﬁcation\ntasks into the same format, and 2) gather a col-\nlection of classiﬁcation datasets and label descrip-\ntions (prompts) for training (Section 2). Since we\nﬁne-tune our model on a meta-dataset, we name\nour approach meta-tuning.\nWe focus on binary classiﬁcation tasks and\nunify them into a “ Yes\"/“No\" QA format (Clark\net al., 2019; McCann et al., 2018), where the input\nis provided as the context and the label informa-\ntion is provided in the question (Figure 1 (a)). Us-\ning this format, we gathered a diverse set of clas-\nsiﬁcation datasets from 43 different sources listed\non Kaggle, SemEval, HuggingFace, and other pa-\npers. These tasks range from hate speech detec-\ntion, question categorization, sentiment classiﬁ-\ncation to stance classiﬁcation, etc, and the genre\nranges from textbooks, social media, to academic\npapers, etc. In total, these datasets contain 204\nunique labels, and we manually annotated 441 la-\nbel descriptions (Figure 2).\nTo evaluate ZSC, we need to deﬁne what counts\nas a task that the model has not seen during train-\ning time. While prior work considers different\nnotions of “unseen\" by disallowing the same la-\nbel or the same dataset to appear during training,\nour work deﬁnes “unseen\" more harshly by dis-\n2857\nGreat movie, must see!\nA total waste of time. \nClassiﬁcation Format\nclassify 0\n1\n[Question] Is the review positive? \nQuestion Answering Format\n[Context] Great movie, must see!\n[Context] A total waste of time. answer “No”\n“Yes”\nConvert\nMeta-tune\nEvaluate on Unseen Tasks\n(a) Task Conversion (b) Meta-tuning and Evaluation (c) Results\nSentiment Classification\nTopic Classification\nQuestion Categorization\nHate Speech Detection\nStance Classification\nFigure 1: (a) We convert the format to question answering. We manually annotate label descriptions (questions)\nourselves (Section 2). (b) We ﬁnetune the UniﬁedQA (Khashabi et al., 2020) model (with 770 M parameters) on a\ndiverse set of tasks (Section 4), and evaluate its 0-shot classiﬁcation (ZSC) performance on an unseen task.(c) For\neach label description (question) we evaluate the AUC-ROC score for the “Yes\" answer, and each dot represents a\nlabel description (Section 3). The x-value is the ZSC performance of UniﬁedQA; the y-value is the performance\nafter meta-tuning. In most cases, the y-value improves over the x-value (above the red line) and is better than\nrandom guesses (above the black line) by a robust margin (Section 5).\nallowing similar datasets. For example, we con-\nsider AG News topic classiﬁcation dataset (Zhang\net al., 2015) and the topic classiﬁcation dataset\nfrom Yin et al. (2019) to be similar, even though\ntheir sources and label spaces are different.\nMeta-tuning improves ZSC over UniﬁedQA for\nmost labels (Figure 1 (c)). Moreover, larger mod-\nels are better, and hence we forecast that meta-\ntuning would work for even larger models. We\nalso ﬁnd that the performance can be slightly im-\nproved by training on datasets similar to the test\ndataset, ensembling different label descriptions, or\ninitializing with a QA model (Section 5.1). All of\nour ﬁndings reliably hold under different robust-\nness checks (Section 5.2), and our approach out-\nperforms the previous SOTA Yin et al. (2019) us-\ning the same pre-training method (Section 5.3).\nOur results suggest two promising future di-\nrections (Section 6). First, large language mod-\nels’ (e.g. GPT-3) potential for zero-shot learn-\ning, as currently measured by context-prompting,\nmight have been broadly underestimated; meta-\ntuning might signiﬁcantly improve their perfor-\nmance. Second, community-wide efforts on ag-\ngregating and unifying datasets can scale up train-\ning and evaluation for zero-shot learning models.\nOn the ﬂip side, however, the meta-tuning ap-\nproach might incentivize providers of LM infer-\nence APIs to collect prompts from users, hence\npotentially leading to security, privacy, and fair-\nness concerns at a greater scale (Section A).\nContributions To summarize, we 1) curate a\ndataset of classiﬁcation datasets with expert an-\nnotated label descriptions. 2) demonstrate a sim-\nple approach to train models to perform zero-shot\nlearning, and 3) identify several factors that im-\nprove performance; in particular, larger pretrained\nmodels are better. 1\n2 Data\nWe gather a wide range of classiﬁcation datasets\nand unify them into the “ Yes\"/“No\" question an-\nswering format for binary classiﬁcation. Then we\ngroup similar datasets together to determine what\ncounts as unseen tasks during evaluation.\nGathering classiﬁcation datasets We collect\nclassiﬁcation datasets from Kaggle2, Huggingface\n(Wolf et al., 2020), SemEval 3, and other papers.\nWe looked through these sources and only con-\nsidered English classiﬁcation datasets. We also\nskipped the tasks that we felt were already bet-\nter represented by other datasets in our collection.\nThen we manually examined a few examples in\neach remaining dataset to make sure it seemed\nplausibly clean.\nThe goals of these classiﬁcation datasets in-\nclude, but are not limited to sentiment classiﬁca-\ntion (IMDB Reviews, Maas et al. (2011a)), topic\nclassiﬁcation (AG News, Zhang et al. (2015)),\ngrammaticality judgement (CoLA, Warstadt et al.\n(2018)), paraphrase detection (QQP 4), deﬁnition\n1Code and data available here: https://github.\ncom/ruiqi-zhong/Meta-tuning.\n2https://www.kaggle.com\n3https://semeval.github.io\n4https://www.kaggle.com/c/\n2858\nIs the review positive?\nDoes the user like this movie?\nIs the review negative?\nDoes the user ﬁnd this movie bad?\nMovie Review Classification\nPositive\nNegative\nTagsDataset Name\nLabels Descriptions\nReview\nManually \nAnnotated\nGood vs. Bad\nFigure 2: For each dataset, we annotate 1-3 descrip-\ntions for each label in the form of questions, and asso-\nciate it with a set of property tags. The question an-\nswering format can be seen in Figure 1 (a).\ndetection (SemEval 2020 Task 6, Spala et al.\n(2019)), stance classiﬁcation (SemEval 2016 Task\n6, Mohammad et al. (2016)), etc. The genre in-\ncludes academic papers, reviews, tweets, posts,\nmessages, articles, and textbooks. The compre-\nhensive list of datasets is in Appendix B. Overall,\nwe aim for a high diversity of tasks and genres by\nbuilding upon what the broader research commu-\nnity has studied. Our approach is complementary\nto that of Weller et al. (2020), which asks turkers\nto generate tasks, and that of Mishra et al. (2021),\nwhich generates tasks by decomposing existing\ntemplates used to construct reading comprehen-\nsion datasets. The concurrent work of Bragg et al.\n(2021) uniﬁes the evaluation for few-shot learn-\ning; their zero-shot evaluation setup is the closest\nto ours, and they used templates and verbalizers\n(Schick and Schütze, 2020a) to specify the seman-\ntics of a task.\nSome of our datasets are noisy and not peer re-\nviewed, or contain tasks that are too complicated\n(e.g. Multi-NLI, Williams et al. (2018)) for ZSC.\nTo make our evaluation more informative, we only\ninclude them for training but not testing. We make\nthese decisions before running our experiments in\nSection 5 to prevent selection bias.\nUnifying the dataset format We convert each\nclassiﬁcation dataset into a “Yes\"/“No\" question\nanswering format and provide label information\nin the question. For each label, we annotate 1-\n3 questions. If the label is null (for example, a\ntext that does not express a particular emotion in\nan emotion classiﬁcation dataset), we skip this la-\nbel. Three of the authors5 manually annotated 441\nquestions for 204 unique labels, and each question\nquora-question-pairs\n5One of them is a graduate student and the other two are\nundergrads; all of them study Computer Science and have\ntaken an NLP class.\nAre these two questions asking for the same thing?\nDoes the tweet contain irony?\nIs this news about world events?\nDoes the text contain a definition?\nIs the tweet an offensive tweet?\nIs the text objective?\nDoes the question ask for a numerical answer?\nIs the tweet against environmentalist initiatives?\nIs this abstract about Physics?\nDoes the tweet express anger?\nDoes the user dislike this movie?\nIs the sentence ungrammatical?\nIs this text expressing a need for evacuation?\nIs this text about Society and Culture?\nIs this a spam?\nFigure 3: Some example manually annotated label de-\nscriptions (questions). Three of the authors manually\nwrote 441 questions in total, and each of them is proof-\nread by at least another author.\nis proofread by at least another author. See Figure\n2 for a concrete example, and Figure 3 for some\nrepresentative label descriptions.\nAdditionally, some datasets contain thousands\nof labels (Chalkidis et al., 2019; Allaway and\nMcKeown, 2020). In this case, we use templates\nto automatically synthesize label descriptions and\nexclude them from evaluation.\nGrouping similar datasets Our goal is to test\nthe models’ ability to generalize to tasks that are\ndifferent enough from the training tasks. There-\nfore, at test time, we need to exclude not only\nthe same dataset that appeared in the meta-tuning\nphase, but also ones that are similar.\nThis poses a challenge: whether two datasets\nperform the same task involves subjective opinion,\nand there is no universally agreed deﬁnition. On\none extreme, most datasets can be counted as dis-\nsimilar tasks, since they have different label spaces\nand input distributions. On the other extreme, all\ndatasets can be considered the same task, since\nthey can all be uniﬁed into the question answer-\ning format.\nTo tackle this challenge, we create a set of tags,\neach describing a dataset property. The set of\ntags includes domain classiﬁcation, article, emo-\ntion, social-media, etc, and the full set of them\ncan be seen in Appendix C. Then we deﬁne the\n2859\nMovie Review Classification \nHotel Review Classification \nAirline Review Classification\nQuestion Paraphrase Detection \nAnswer Type Classification\nStance Classification \nLiberal/Conservative Classification\nHate Speech Detection \nOffensive Speech Detection\nReview\nGood vs. Bad\nSocial Media\nSocietal\nQuestion Categorization\nSocial Media\nSocietal\nEmotion\nFigure 4: Example dataset groups based on tags. We\nnever train and test on datasets from the same group,\ne.g. train on hotel review and test on movie review.\ntwo datasets to be similar if they are associated\nwith the same set of tags, and prohibit the model to\nlearn from one and test on the other. For example,\nour work considers the topic classiﬁcation datasets\nfrom Zhang et al. (2015) (AG News) and Yin et al.\n(2019) to be similar since they both classify top-\nics for articles, even though their sources and label\nspaces are different. Some example dataset groups\ncan be seen in Figure 4.\nNevertheless, our procedure is not bullet-proof\nand one can argue that our notion of unseen\ntasks, though harsher than prior works (Yin et al.,\n2019; Pushp and Srivastava, 2017), is still lenient.\nTherefore, as additional robustness checks, for\neach dataset we evaluate, we manually identify\nand list the most relevant dataset that is allowed\nduring training in Appendix F . For example, the\nmost relevant dataset to the IMDB review senti-\nment classiﬁcation dataset is the emotion classiﬁ-\ncation dataset from Yin et al. (2019), which clas-\nsiﬁes the input text into 9 emotions, such as “joy\",\n“surprise\", “guilt\", etc. We consider the emotion\nclassiﬁcation dataset to be relevant, since senti-\nment classiﬁcation often involves identifying emo-\ntions. However, one can also argue that they are\ndifferent tasks: their input and label spaces are\ndifferent, and sadness can be caused by a great\ntragedy, or a bad movie that wastes the users’\ntime. The comprehensive list of label descriptions\ngrouped by dataset similarity is in Appendix D.\nIn total, we spend around 200 hours to collect\nthis dataset. This time estimate includes skim-\nming through the dataset repos and recent NLP\npapers, writing programs to download the datasets\nand unify their format, annotating label descrip-\ntions, performing quality controls, and document-\ning the collection process.\n3 Metrics\nTo reliably aggregate performance across differ-\nent datasets and present as much information as\npossible, we report a set of descriptive statistics\nand provide visualizations whenever we compare\ntwo models. We generally do not reduce a model’s\nperformances on different datasets into one scalar\nquantity and compare this number only.\nDescriptive statistics For each label description\n(question), we calculate the AUC-ROC score 6 by\ntreating the “Yes\" answer as the positive class. Af-\nter calculating the AUC-ROC score for each label,\nwe calculate the following set of descriptive statis-\ntics to compare two models. Suppose that model\nY is hypothetically better than X. Denoting ∆\nas the change of AUC-ROC of a label description\nfrom X to Y , we can summarize how ∆ is dis-\ntributed across the set of label descriptions with\nthe following statistics:\n• E[∆]: the average change in AUC-ROC.\n• P[∆ > t]: the fraction of label descriptions\nwhere the change is over the threshold t.\n• P[∆ < −t]: the fraction of label descriptions\nwhere the change is less than −t.\n• Std[∆]: the standard deviation of the change.\nIn the main paper, we weight each label descrip-\ntion equally in this distribution to calculate the\nabove statistics. We may also weight each label or\ndataset equally, and the corresponding results are\nin Appendix E. To make sure our conclusions are\nrobust, we consider one model to be better only\nwhen E[∆] > 0 and P[∆ > t] > P[∆ < −t]\nfor all t ∈{1%, 5%, 10%}, under all three types\nof weighting. In other words, we claim that one\nmodel is better than the other only when 12 condi-\ntions simultaneously hold.\nVisualizations We use scatter plots to visual-\nize and compare the performance of two models,\nwhere each dot represents a label description, its x-\nvalue represents the AUC-ROC score of the model\nX, and its y-value represents that of Y . If most\ndots are above the identity line y = x, the model\nY is better than X.\nThe descriptive statistics and the visualizations\nare explained in Figure 5.\n6We do not evaluate F-score or accuracy, since they are\nvery sensitive to the decision cutoff, and usually additional\ncalibration is needed (Zhao et al., 2021).\n2860\nFigure 5: Each dot represents a label description, and\nits x/y-value each represents the performance of model\nX/Y (measured by AUC-ROC score). For example, on\nlabel description D1, model X/Y has AUC-ROC score\n0.5/0.65. If the dot is above the black line ( y = 0.5),\nmodel Y is performing better than random guesses. If\nthe dot is above the red line (y = x), model Y is better\nthan model X. Since one out of two dots are above\ny = x + 0.05, we have P[∆ > 5%] = 0.5.\n4 Model\nArchitecture We format the inputs to the model\nin the same way as UniﬁedQA (Khashabi et al.,\n2020), which concatenates the context to the ques-\ntion and adds a “ [SEP]\" token in between. Then\nwe feed the concatenated input into the T5 en-\ncoder and produce the answer score by normal-\nizing the “ Yes\"/“No\" probability of the ﬁrst de-\ncoded token. Unless otherwise noted, we initial-\nize our model with T5-Large (770 Million pa-\nrameters). We sometimes compare to or initial-\nize with the UniﬁedQA model (Khashabi et al.,\n2020), which is trained on a wide range of ques-\ntion answering datasets. For a fair comparison,\nwe use the UniﬁedQA model initialized with T5-\nLarge as well. To meta-tune non-Seq2Seq pre-\ntrained models, such as BERT (Devlin et al., 2019)\nor RoBERTa (Liu et al., 2019), we add an MLP\nlayer on top of the pooled output/“ [CLS]\" token\nto classify between “Yes\"/“No\". We leave the im-\nprovement on model architectures (Ye and Ren,\n2021; Li and Liang, 2021; Lester et al., 2021) and\ntraining objectives (Murty et al., 2021; Yin et al.,\n2020) for future work.\nMeta-tuning We create a training distribution\nthat balances between datasets, label descriptions,\nand “ Yes\"/“No\" answers. To create the next\ntraining datapoint for meta-tuning, we select a\ndataset from the training split uniformly at random\n(u.a.r.); then we select a label description (ques-\ntion) u.a.r. and with 50% probability select a tex-\ntual input with the answer “Yes\"/“No\". To prevent\nover-ﬁtting, we do not train on any combination of\nlabel description and textual input twice. Unless\notherwise noted, we meta-tune the model for 5000\nsteps and use batch size 32. We did not tune any\nhyper-parameters or training conﬁgurations since\nthey work well during our ﬁrst attempt. To evalu-\nate ZSC performance on each dataset, we leave out\none group of similar datasets as the evaluation set\nand train on the rest. Altogether, the experiments\ntake around 250 GPU hours on Quadro 8000.\n5 Results\n5.1 Hypotheses and Conclusions\nWe investigate and validate the following hypothe-\nses, sorted by importance in descending order.\n• Meta-tuned models outperform general ques-\ntion answering models in zero-shot classiﬁ-\ncation.\n• Larger pre-trained models are better.\n• Pre-training does the heavy lifting.\n• Performance can be improved by training\non similar datasets, initializing with a QA\nmodel, or ensembling label descriptions.\n• Early stopping is crucial to performance.\nMeta-tuned models are better. We compare a\nmeta-tuned T5-Large model (770 M parameters) 7\nwith the same-sized UniﬁedQA model (Khashabi\net al., 2020) out of the box. Relevant descriptive\nstatistics can be seen in the ﬁrst row of Table 1\nand Figure 6 (a). Adapting the model for ZSC im-\nproves the average AUC-ROC by 3.3%.\nLarger pre-trained models are better. We\ncompare T5-Base (220 Million parameters)\nagainst T5-Large (770 M). The statistics can be\nseen in the second row of Table 1 and Figure 6\n(b). Increasing the model size from 220 M to\n770M improves the average AUC-ROC by 6.3%.\n7This model is initialized with T5, not UniﬁedQA.\n2861\nE[∆] P[∆ > 1%] P[∆ < −1%] Std(∆)\nMeta-tuned vs. UniﬁedQA 3.3% 59.5% 28.1% 9.5%\nLarger 6.3% 75.1% 15.1% 8.1%\nPre-trained vs. Random 23.8% 95.7% 3.2% 14.0%\nTrain on Similar 0.7% 43.8% 20.5% 3.2%\nEnsemble Descriptions 0.7% 28.9% 16.8% 3.1%\nInitialize with UniﬁedQA 1.1% 54.1% 24.3% 6.9%\nTable 1: The statistics used to compare two models, introduced in Section 3. The larger E[∆] and the difference\nbetween P[∆ > 1%] and P[∆ < −1%], the better. Row 1 ﬁnds that a meta-tuned model is better than UniﬁedQA;\nrow 2 ﬁnds that the larger model is better; row 3 ﬁnds that pre-training does the heavy lifting; row 4, 5, and 6\nﬁnds that the performance can be improved by training on similar datasets, ensembling label descriptions, and\ninitializing with a UniﬁedQA model. Note that Std(∆) is the standard deviation of individual descriptions, not the\nstandard deviation of the estimated mean. Due to space constraint we only show t = 1% in this table.\nPre-training does the heavy lifting. In Figure\n(c) and the third row of Table 1, we compare pre-\ntrained and random initializations, where the latter\ncannot beat the random baseline (average AUC-\nROC 0.503). Hence, meta-tuning alone is far from\nenabling the model to perform ZSC. An intuitive\ninterpretation is that the model already “knows\"\nhow to perform ZSC after pre-training under the\nLM objective, and learns how to use this knowl-\nedge during meta-tuning.\nTraining on similar datasets improves perfor-\nmance. Unlike before, we no longer avoid train-\ning on similar datasets from the same group. In-\nstead, we perform straightforward leave-one-out\ncross-validation. The statistics can be seen in the\nfourth row of Table 1 and Figure 6 (d), and it im-\nproves the average AUC-ROC by 0.7%. The per-\nformance gain is not as signiﬁcant as increasing\nthe model size or adapting for ZSC. We conjecture\nthat it is because we have not collected enough\ndatasets; otherwise, there might be more similar\ndatasets, hence improving ZSC performance.\nEnsembling label descriptions improves perfor-\nmance. Instead of asking the model a single\nquestion for each label and obtain the probabil-\nity of the answer being “Yes\", we can average the\nprobability obtained by asking multiple questions\nwith the same meaning. This approach is differ-\nent from traditional ensembling, which typically\nneeds to store/train multiple models to average\nacross them. The ﬁfth row of Table 1 and Figure 6\n(e) veriﬁes that ensembling descriptions improves\nperformance slightly (0.7% AUC-ROC score).\nInitializing with UniﬁedQA improves perfor-\nmance. Figure 6 (f) and the sixth row of Table 1\ncompare the UniﬁedQA against against the T5 ini-\ntialization. Initializing with UniﬁedQA improves\naverage AUC-ROC by 1.1%.\nEarly stopping is crucial to performance. If\nwe train the model for too long, the model might\nsimply “memorize\" that certain label descriptions\ncorrespond to certain training tasks, and the per-\nformance on unseen tasks may drop. To explore\nthis possibility, we meta-tune our models for 100K\nsteps, which is 20 times as long as our default set-\nting and encourages the model to memorize the\ntraining tasks. We then evaluate them on the three\nbenchmark zero-shot classiﬁcation datasets by Yin\net al. (2019) (which we describe in more details in\nthe next section). We calculate the average AUC-\nROC across all label descriptions for each of the 3\ndatasets, and plot them in Figure 7.\nThe performance decreases 8 as training con-\ntinues. On the other hand, however, the perfor-\nmance drop of 3% in AUC-ROC is not fatal and\nthe model’s performance is still much better than\nrandom guesses.\n5.2 Robustness Checks\nWe examine a series of additional results to make\nsure our conclusions are robust. The observed\nimprovements in Table 1 and Figure 6 might be\ncaused by the improvement of a small number of\nlabels that are annotated with more descriptions,\nor by the improvement on a dataset with more\ndistinct labels. Appendix E.1 compares the per-\nformance by assigning equal weights to each la-\nbel/datasets.\nTo provide additional supporting evidence for\n8Kendall rank correlation coefﬁcients are negative with\np <0.005 for topic and situation classiﬁcation\n2862\nMeta-tuned \nUniﬁedQA \nT5 - Large\nT5 - Base \nAvoid Similar Datasets\nTrain on Similar\nNo Ensemble\nEnsemble\nQA Initialized\nT5 Initialized\n(a) (b) \n(d) (e) (f) \nRandomly Initialized\nPretrained\n(c) \nFigure 6: The interpretation of these ﬁgures can be seen in Figure 5. (a) compares a meta-tuned model ( y) against\nUniﬁedQA ( x); (b) compares T5-Large (770 M parameters) against T5-base (220M); (c) compares the T5 pre-\ntrained initialization against the random initialization; (d), (e), and (f) investigate whether performance can be\nimproved by training on similar datasets, ensembling different label descriptions (questions), and initializing with\nUniﬁedQA. Conclusion: Since most dots are above the red liney = x for all 6 ﬁgures and above the random guess\nbaseline (y = 0.5) by a robust margin, all conclusions listed at the beginning of Section 5 hold.\nFigure 7: Each curve corresponds to the models’ per-\nformance on a dataset from Yin et al. (2019). x-value\nis the number of training steps; y-value is the average\nAUC-ROC score across all label descriptions, relative\nto the value at step 5000. Training for too long de-\ncreases performance on unseen tasks.\nour forecast that larger models are better, Ap-\npendix E.2 compares a 60M-parameter model\nagainst a 220M-parameter model, and ﬁnds that\nthe latter is much better. One concern, however,\nis that our models are initialized with T5 (Raffel\net al., 2019), which is trained on the open web and\nmight have seen the datasets we gathered. There-\nModel emotion situation topic\nYin et al. (2019) 25.2 38.0 52.1\nMeta-tuned 28.2 48.4 54.3\nTable 2: “Prior\" means the best performing system\nfrom Yin et al. (2019) for each dataset; “Meta-tuned\"\nmeans meta-tuning on RoBERTa. Our approach is bet-\nter on all three datasets.\nfore, larger models might be better simply because\nthey are better at memorization (Sagawa et al.,\n2020). Appendix E.3 addresses this by showing\nthat larger models are also better with BERT ini-\ntialization (Devlin et al., 2019), which is trained\non Wikipedia and Book Corpus (Zhu et al., 2015).\nWe also report the models’ performance on each\ndataset for readers’ reference in Appendix G.\n5.3 Comparison with Yin et al. (2019)\nThis section shows that our approach has higher\nperformance than the zero-shot classiﬁcation sys-\ntem built by Yin et al. (2019). Their system en-\nsembles several natural language inference models\nbased on RoBERTA-Large (355M parameters, Liu\net al. (2020)), and another model trained to catego-\nrize Wikipedia articles. It was evaluated on three\nclassiﬁcation datasets:\n2863\n• topic (10-way): classiﬁes article domains,\nsuch as family & relationship, education,\nsports, etc. The metric is accuracy.\n• emotion (10-way): classiﬁes emotion types,\nsuch as joy, anger, guilt, shame, etc. The met-\nric is label-weighted F1.\n• situation (12-way): classiﬁes disaster situa-\ntions, e.g. regime change, crime & violence,\nand the resource they need, e.g.search & res-\ncue. The metric is label-weighted F1.\nWe use the exact same evaluation metrics as in\nYin et al. (2019), and the same label resolution\nstrategy when the model answers “Yes\"9 for multi-\nlabel classiﬁcation. Concretely, when the model\npredicts “Yes\" on multiple labels, the one with the\nhighest probability is selected. For a fair compari-\nson, we meta-tune RoBERTa of the same size and\ncompare it with the highest performing model in\nYin et al. (2019) for each of the three datasets.\nThe results are in Table 2, and our model has\nhigher performance across all 3 datasets using the\nsame pre-training method.\n6 Discussion and Future Directions\nMain takeaways We construct a dataset of clas-\nsiﬁcation datasets to adapt the language model\nfor zero-shot classiﬁcation via meta-tuning. The\nadapted model outperforms a general-purpose\nquestion answering model and the prior state of\nthe art based on natural language inference. We\nforecast that meta-tuning would be more effective\non larger models, and the current engineering ceil-\ning for zero-shot learning might have been broadly\nunder-estimated.\nAggregating and unifying datasets The main\nbottleneck of our research is to manually gather a\nwide range of datasets and unify their format. The\ndifﬁculties are: 1) we need to brainstorm and re-\nview the NLP literature extensively to decide what\nnew tasks to look for; 2) different datasets en-\ncode their data in different formats, and we need to\nwrite programs manually for each of them to con-\nvert to the desired format; 3) it is hard to tell the\nquality of a dataset purely by its provenance, and\nsometimes we need to examine the dataset manu-\nally. If we as a community can aggregate and unify\ndatasets better, we could potentially train and eval-\nuate zero-shot learning models at a larger scale.\n9or “Entailment\" for natural language inference models.\nMeta-tuning as a probe There is a growing in-\nterest in measuring the intelligence (Hendrycks\net al., 2021a,b) or the few-shot learning ability\n(Brown et al., 2020) of large language models\nlike GPT-3. However, since these models are not\nadapted to answer those prompts (Holtzman et al.,\n2021), we suspect that its knowledge and true\npotential to perform few-shot learning is much\nhigher than reported. Since pre-training does the\nheavy lifting and meta-tuning is unlikely to pro-\nvide additional ZSC ability to the model, we can\npotentially ﬁrst use meta-tuning as a probe to make\nthem adapted to answering prompts before mea-\nsuring their performance.\nStill, to make this methodology rigorous, inter-\npreting and controlling the strength of the probes\nwill be an important future direction (Hewitt and\nLiang, 2019). For example, if the training set con-\ntains a prompt that is too similar to the prompt to\nbe tested, the probe will be meaningless.\nBeyond Shallow Correlations One possibility\nis that the model only learns shallow statistical\ncorrelations from meta-tuning rather than “more\nsophisticated reasoning skills\". For example, the\nword “exciting\" might occur in positive reviews\nmore. This is unlikely, given that larger models\nare consistently better than smaller or randomly\ninitialized ones. To explain this performance gap,\nlarger models must have learned to use more com-\nplicated features during meta-tuning.\nRelation to Meta/Multitask-Learning Our\nmethod is closely related to, but different from\nmeta-learning (Yin, 2020; Murty et al., 2021)\nand multi-task learning (Ye et al., 2021; Agha-\njanyan et al., 2021). Both meta-learning and\nmultitask-learning typically involve at least a\nfew examples from the target task; in our setup,\nhowever, the model does not learn from any target\ntask examples. The “meta” in our name does not\nmean “meta-learning”, but reﬂects the fact that\nour model learns from a meta-dataset of tasks.\nNevertheless, our framework can be easily\nadapted to a few-shot learning setup, which en-\nables the language model to learn to learn from in-\ncontext examples (see below). Since this approach\nmodels the learning process as a sequence classi-\nﬁcation problem, it can be seen as a form of meta-\nlearning similar to (Ravi and Larochelle, 2016).\nAnnotating Prompts Three of our authors an-\nnotated the label descriptions. Since they are all\n2864\nComputer Science major students who understand\nmachine learning and natural language processing,\nthey might not be representative of the ﬁnal user\npopulation of this ZSC application. Annotating\nprompts that match the target user distribution will\nbe an important research direction.\nAdditionally, shorter and more natural descrip-\ntions sometimes fail to capture the exact seman-\ntics of the label. For example, in Yin et al. (2019),\nthe description of the label “medical\" is “ people\nneed medical assistance \"; or alternatively, it can\nbe longer but more accurate: “ people need an al-\nlied health professional who supports the work of\nphysicians and other health professionals \". How\nto scalably generate more accurate and detailed la-\nbel descriptions without expert efforts will be an-\nother future direction.\nOptimizing Prompts Our work is complemen-\ntary to recent works that optimize the prompts\nto achieve better accuracy. Even if our meta-\ntuned model is specialized in answering prompts,\nit might still react very differently towards differ-\nent prompts. For example, in the stance classiﬁ-\ncation dataset (Barbieri et al., 2020), we annotated\ntwo label descriptions (prompts) for the same la-\nbel: “Does this post support atheism?\" and “Is the\npost against having religious beliefs?\". They have\nsimilar meanings, but the former has much lower\naccuracy than the later. We conjecture that this\nis because the model cannot ground abstract con-\ncepts like “atheism\".\nOther extensions We conjecture that meta-\ntuning can be extended to more diverse tasks be-\nyond zero-shot binary classiﬁcation. To extend\nto multi-label classiﬁcation, we need to develop\na procedure to resolve the labels when the model\npredicts positive for more than one labels. To ex-\ntend to few-shot learning, we need to increase the\ncontext length to ﬁt several training examples into\nthe input, which requires a larger context window\nand hence more computational resources. To ex-\ntend to other sequence generation tasks, we need\nto collect a wide range of diverse sequence genera-\ntion tasks to meta-tune the model, such as machine\ntranslation, summarization, free-form question an-\nswering, grammar correction, etc.\nAcknowledgements\nWe thank Eric Wallace for his feedbacks through-\nout the project. We thank Steven Cao, David\nGaddy, Haizhi Lai, Jacob Steinhardt, Kevin Yang\nand anonymous reviewers for their comments on\nthe paper.\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivas-\ntava, Xilun Chen, Luke Zettlemoyer, and Sonal\nGupta. 2021. Muppet: Massive multi-task rep-\nresentations with pre-ﬁnetuning. arXiv preprint\narXiv:2101.11038.\nEmily Allaway and Kathleen McKeown. 2020. Zero-\nShot Stance Detection: A Dataset and Model us-\ning Generalized Topic Representations. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8913–8931, Online. Association for Computational\nLinguistics.\nTiago Almeida, José María Gómez Hidalgo, and\nTiago Pasqualini Silva. 2013. Towards sms spam\nﬁltering: Results under a new dataset. International\nJournal of Information Security Science, 2(1):1–18.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetE-\nval: Uniﬁed benchmark and comparative evaluation\nfor tweet classiﬁcation. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 1644–1650, Online. Association for Compu-\ntational Linguistics.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019. SemEval-2019 task 5: Multilin-\ngual detection of hate speech against immigrants and\nwomen in Twitter. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n54–63, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 29. Curran Associates, Inc.\nJonathan Bragg, Arman Cohan, Kyle Lo, and Iz Belt-\nagy. 2021. Flex: Unifying evaluation for few-shot\nnlp. arXiv preprint arXiv:2107.07170.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Úl-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2020.\n2865\nExtracting training data from large language models.\narXiv preprint arXiv:2012.07805.\nIlias Chalkidis, Emmanouil Fergadiotis, Prodromos\nMalakasiotis, and Ion Androutsopoulos. 2019.\nLarge-scale multi-label text classiﬁcation on EU leg-\nislation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 6314–6322, Florence, Italy. Association\nfor Computational Linguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Compu-\ntational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2020. Aligning AI With Shared Human Values.\narXiv e-prints, page arXiv:2008.02275.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021a. Aligning {ai} with shared human values. In\nInternational Conference on Learning Representa-\ntions.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021b. Measuring massive multitask lan-\nguage understanding. In International Conference\non Learning Representations.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form compe-\ntition: Why the highest probability answer isn’t al-\nways right. CoRR, abs/2104.08315.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1896–1907, Online. As-\nsociation for Computational Linguistics.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 2627–2636, On-\nline. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-\ntuning: Optimizing continuous prompts for gener-\nation. arXiv preprint arXiv:2101.00190.\nXin Li and Dan Roth. 2002. Learning question clas-\nsiﬁers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nHairong Liu, Mingbo Ma, Liang Huang, Hao Xiong,\nand Zhongjun He. 2019. Robust neural machine\ntranslation with joint textual and phonetic embed-\nding. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 3044–3049, Florence, Italy. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011a. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011b. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\n2866\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nTsvetomila Mihaylova, Georgi Karadzhov, Pepa\nAtanasova, Ramy Baly, Mitra Mohtarami, and\nPreslav Nakov. 2019. SemEval-2019 task 8: Fact\nchecking in community question answering forums.\nIn Proceedings of the 13th International Workshop\non Semantic Evaluation, pages 860–869, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral,\nand Hannaneh Hajishirzi. 2021. Natural instruc-\ntions: Benchmarking generalization to new tasks\nfrom natural language instructions. arXiv preprint\narXiv:2104.08773.\nRishabh Misra. 2019. Imdb spoiler dataset.\nRishabh Misra, Mengting Wan, and Julian McAuley.\n2018. Decomposing ﬁt semantics for product size\nrecommendation in metric spaces. In Proceedings\nof the 12th ACM Conference on Recommender Sys-\ntems, pages 422–426. ACM.\nSaif Mohammad, Felipe Bravo-Marquez, Moham-\nmad Salameh, and Svetlana Kiritchenko. 2018.\nSemEval-2018 task 1: Affect in tweets. In Proceed-\nings of The 12th International Workshop on Seman-\ntic Evaluation, pages 1–17, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 task 6: Detecting stance in tweets.\nIn Proceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016), pages 31–\n41, San Diego, California. Association for Compu-\ntational Linguistics.\nShikhar Murty, Tatsunori B Hashimoto, and Christo-\npher D Manning. 2021. Dreca: A general task aug-\nmentation strategy for few-shot natural language in-\nference.\nBo Pang and Lillian Lee. 2004. A sentimental edu-\ncation: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In Proceed-\nings of the 42nd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL-04), pages\n271–278, Barcelona, Spain.\nPushpankar Kumar Pushp and Muktabh Mayank Sri-\nvastava. 2017. Train once, test anywhere: Zero-\nshot learning for text classiﬁcation. arXiv preprint\narXiv:1712.05972.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nSachin Ravi and Hugo Larochelle. 2016. Optimization\nas a model for few-shot learning.\nSara Rosenthal, Noura Farra, and Preslav Nakov.\n2017. SemEval-2017 task 4: Sentiment analysis in\nTwitter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 502–518, Vancouver, Canada. Association for\nComputational Linguistics.\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and\nPercy Liang. 2020. An investigation of why over-\nparameterization exacerbates spurious correlations.\nIn International Conference on Machine Learning ,\npages 8346–8356. PMLR.\nTimo Schick and Hinrich Schütze. 2020a. Exploit-\ning cloze questions for few-shot text classiﬁcation\nand natural language inference. arXiv preprint\narXiv:2001.07676.\nTimo Schick and Hinrich Schütze. 2020b. It’s\nnot just size that matters: Small language mod-\nels are also few-shot learners. arXiv preprint\narXiv:2009.07118.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Auto-\nPrompt: Eliciting Knowledge from Language Mod-\nels with Automatically Generated Prompts. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 4222–4235, Online. Association for Compu-\ntational Linguistics.\nSasha Spala, Nicholas Miller, Franck Dernoncourt, and\nCarl Dockhorn. 2020. SemEval-2020 task 6: Deﬁni-\ntion extraction from free text with the DEFT corpus.\nIn Proceedings of the Fourteenth Workshop on Se-\nmantic Evaluation, pages 336–345, Barcelona (on-\nline). International Committee for Computational\nLinguistics.\nSasha Spala, Nicholas A. Miller, Yiming Yang, Franck\nDernoncourt, and Carl Dockhorn. 2019. DEFT: A\ncorpus for deﬁnition extraction in free- and semi-\nstructured text. In Proceedings of the 13th Linguis-\ntic Annotation Workshop, pages 124–131, Florence,\nItaly. Association for Computational Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. SemEval-2018 task 3: Irony detection in En-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\n2867\nEric Wallace, Tony Z Zhao, Shi Feng, and Sameer\nSingh. 2020. Customizing triggers with concealed\ndata poisoning. arXiv preprint arXiv:2010.12563.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471.\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew Peters. 2020. Learning from task descrip-\ntions. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP), pages 1361–1375, Online. Associa-\ntion for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. As-\nsociation for Computational Linguistics.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossﬁt: A few-shot learning challenge for cross-\ntask generalization in NLP. CoRR, abs/2104.08835.\nQinyuan Ye and Xiang Ren. 2021. Zero-shot learning\nby generating task-speciﬁc adapters. arXiv preprint\narXiv:2101.00420.\nMingzhang Yin, George Tucker, Mingyuan Zhou,\nSergey Levine, and Chelsea Finn. 2020. Meta-\nlearning without memorization. In International\nConference on Learning Representations.\nWenpeng Yin. 2020. Meta-learning for few-shot nat-\nural language processing: A survey. arXiv preprint\narXiv:2007.09604.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019.\nBenchmarking zero-shot text classiﬁcation:\nDatasets, evaluation and entailment approach.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages\n3914–3923, Hong Kong, China. Association for\nComputational Linguistics.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. SemEval-2019 task 6: Identifying and catego-\nrizing offensive language in social media (OffensE-\nval). In Proceedings of the 13th International Work-\nshop on Semantic Evaluation , pages 75–86, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Proceedings of the 28th International\nConference on Neural Information Processing Sys-\ntems - Volume 1 , NIPS’15, page 649–657, Cam-\nbridge, MA, USA. MIT Press.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,\nand Sameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\narXiv preprint arXiv:2102.09690.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n19–27.\n2868\nA Ethics\nData and incentives In the existing prompting\nframework, end users send the natural language\ndescriptions and a few training examples to the\nlarge language model inference API to perform\nfew-shot learning (Brown et al., 2020). This be-\ncomes a natural source of training data for meta-\ntuning. Hence, the success of meta-tuning pre-\nsented in this paper might incentivize for-proﬁt\norganizations who provide language model infer-\nence APIs to collect prompts from the users, and\ntrain on these data.\nPrivacy, security, and fairness If a model is\nmeta-tuned on user-provided data, certain secu-\nrity, privacy and fairness concerns can potentially\nemerge. For example, Carlini et al. (2020) shows\nthat it is possible to extract the training data from\nlarge language models, and hence meta-tuned sys-\ntems might expose some users’ prompts to other\nusers. Wallace et al. (2020) shows that it is possi-\nble to poison the model through training data and\ntrigger unwanted behaviors; the meta-tuning pro-\ncedure might be susceptible to these data poison-\ning attacks as well. Finally, meta-tuning might\nperpetuate existing societal biases hidden in the\nusers’ prompts (Bolukbasi et al., 2016).\nIf not addressed properly, these concerns might\nhave a broader negative societal impact through\nmeta-tuning. Compared to other domain-speciﬁc\nand task-speciﬁc machine learning applications,\nmeta-tuned models might be applied to a much\nwider range of tasks, deployed at a larger scale,\nand serving a more diverse set of user population.\nTherefore, biased or poisoned training data for one\ntask from one user population might compromise\nfairness and performance of another task and harm\nanother user population; additionally, malicious or\nbiased data might even tamper with the few-shot\nlearning capability (“meta-poisoning\").\nPotential abuse As shown in Figure 6, the\nAUC-ROC score for a lot of tasks are still well\nbelow 0.9, and hence our system is far from solv-\ning a signiﬁcant fraction of tasks. Therefore, even\nthough our system is ﬂexible and has the poten-\ntial to perform a wide range of tasks, it does not\npresent an elixir to all classiﬁcation tasks. Par-\nticularly, it should not be applied to higher stake\nscenarios (e.g. hate speech detection, fake news\ndetection, etc), since its efﬁcacy, robustness, and\nfairness properties remain unknown.\nB Datasets\nIMDB movie review sentiment classiﬁcation\n(Maas et al., 2011b). Classiﬁes whether the user\nlikes the movie.\nPOSITIVE : “’My favourite police series of all\ntime turns to a TV-ﬁlm. Does it work? Yes. ...\"\nNEGATIVE : “ \"Stupid! Stupid! Stupid! I can\nnot stand Ben stiller anymore.\"\nZero Shot Emotion Classiﬁcation (Yin et al.,\n2019). This task classiﬁes a textual input\ninto 9 emotion types {“sadness\", “joy\", “anger\",\n“disgust\", “fear\", “surprise\", “shame\", “guilt\",\n“love\"}, and none-type if not any of the above. For\nexample,\nJOY: “Making new friends is always fun , spe-\ncially when playing dress up\"\nANGER : “People that smoke cigarettes irritate\nmy soul.\"\nZero Shot topic Classiﬁcation (Yin et al.,\n2019). This task classiﬁes an articles into 10 topic\nclasses, such as “Health\", ‘Sports\", “Family & Re-\nlationships\", “Science & Mathematics\", etc.\nSCIENCE : “ How the human species evolved ?\nHow the human species evolved ? A tough ques-\ntion as it overlaps science and theology ...\"\nSPORTS : “ Number one formula one driver ?\nDepends on your question.Kimi Raikkonen won\nmore races ...\"\nZero Shot Situation Classiﬁcation (Yin et al.,\n2019). This task classiﬁes what type of situation\nthe user is having and what aids it needs. The\nclasses include “food\", “terrorism\", etc.\nFOOD : \"livestock , crops , rural and urban hous-\ning , roads , river and canal embankments , and\nother infrastructure suffered extensive damage\"\nTERRORISM : \"Daish attacked Tahurmahu vil-\nlage with chemical gas.\"\nSemEval 2017 Task 4 (Barbieri et al., 2020)\n(Rosenthal et al., 2017). This task classiﬁes the\nsentiment of a tweet as {“negative\", “neutral\",\n“positive\"}. For example,\nNEGATIVE : “That sucks if you have to take the\nSATs tomorrow\"\nNEUTRAL : “ @user Well said on HMW. Can\nyou now address why Texans fans ﬁle out of\nthe stadium midway through the 4th qtr of every\ngame?\"\n2869\nPOSITIVE : “ \"This is the last Sunday w/o foot-\nball .....,NFL is back baby\"\"\nETHICS Justice (Hendrycks et al., 2020). This\ntask identiﬁes whether the justiﬁcation provided\nfor a given scenario is deemed reasonable. For ex-\nample,\nREASONABLE : “I deserve to be paid by my boss\nbecause I keep her house clean daily.\"\nUNREASONABLE : “ I deserve to be given a\nbrand new car by my boss because I keep her\nhouse clean daily.\"\nETHICS Deontology (Hendrycks et al., 2020).\nThis task identiﬁes whether the excuse provided\nfor a given scenario is a reasonable excuse. For\nexample,\nSCENARIO : “Aren’t you supposed to be prepar-\ning for the meeting?\"\nEXCUSE (REASONABLE ): “No because I have\neverything already set up in the conference room.\"\nEXCUSE (UNREASONABLE ): “No because you\ndon’t have a meeting\"\nETHICS Common Sense (Hendrycks et al.,\n2020). This task identiﬁes identiﬁes whether a\nscenario demonstrates common sense. For exam-\nple,\nCOMMON SENSE : “I went to the principal’s of-\nﬁce to change my records before going to a differ-\nent school.\"\nNOT COMMON SENSE : “I secured the loan be-\ncause I would make the payments.\"\nEURLEX57K (Chalkidis et al., 2019). This\ntask classiﬁes European legislation.\nNATIONAL CURRENCY : “ Council Regulation\n(EC) No 2595/2000 of 27 November 2000 amend-\ning Regulation (EC) No 1103/97 on certain provi-\nsions relating to the introduction of the euro\"\nSOUTHERN AFRICA : “ 95/458/EC: Commis-\nsion Regulation (EC) No 302/2006 of 20 February\n2006 on import licences in respect of beef and veal\nproducts originating in Botswana, Kenya, Mada-\ngascar, Swaziland, Zimbabwe and Namibia\"\nSemEval 2019 Task 6 (Barbieri et al., 2020)\n(Zampieri et al., 2019). This task classiﬁes the\ntweet as either offensive or not offensive. For ex-\nample,\nOFFENSIVE : “@user She has become a parody\nunto herself? She has certainly taken some heat\nfor being such an....well idiot. Could be optic too\nWho know with Liberals They’re all optics. No\nsubstance\"\nNOT OFFENSIVE : “@user @user She is great.\nHi Fiona!\"\nClick Bait Detection 10 This task detects\nwhether a news title is a click bait.\nCLICK BAIT : “ Can You Pass This Basic\nTrigonometry Quiz\"\nNON CLICK BAIT : “ NASCAR driver Kyle\nBusch wins 2011 Jeff Byrd 500\".\nAbstract Domain Classiﬁcation 11 This clas-\nsiﬁes the abstract into 4 domains: “Physcis\",\n“Maths\", “Computer Science\", “Statistics\". For\nexample,\nPHYSICS : “ a ever-growing datasets inside ob-\nservational astronomy have challenged scientists\ninside many aspects, including an efﬁcient and in-\nteractive data exploration and visualization. many\ntools have been developed to confront this chal-\nlenge ...\"\nMATHS: “a main result of this note was a exis-\ntence of martingale solutions to a stochastic heat\nequation (she) inside the riemannian manifold ...\"\nSemEval 2019 Task 5 (Barbieri et al., 2020)\n(Basile et al., 2019). This task identiﬁes whether\nthe tweet contains hate speech towards women\nand/or immigrants or not. For example,\nHATE SPEECH : “This account was temporarily\ninactive due to an irrational woman reporting us\nto Twitter. What a lack of judgement, shocking.\n#YesAllMen\"\nNO HATE SPEECH : “@user nice new signage.\nAre you not concerned by Beatlemania -style hys-\nterical crowds crongregating on you. . .\"\nSemEval 2019 Task 8 (Mihaylova et al., 2019).\nThis task identiﬁes whether the text is an exam-\nple of a question asking for factual information,\nan example of a question asking for an opinion, or\nan example of socializing. For example,\nFACTUAL : “is there any place i can ﬁnd scented\nmassage oils in qatar?\"\nOPINION : “ hi there; i can see a lot of mas-\nsage center here; but i dont which one is better.\n10https://www.kaggle.com/c/\nclickbait-news-detection\n11https://www.kaggle.\ncom/abisheksudarshan/\ntopic-modeling-for-research-articles?\nselect=Train.csv\n2870\ncan someone help me which massage center is\ngood...and how much will it cost me? thanks\"\nSOCIALIZING : “ Hello people...let’s play this\ngame...you have to write something good about the\nperson whose ’post’ is above you on QL.You can\nwrite anything and you can write&#160; multiple\ntimes.\"\nSemEval 2018 Task 3 (Barbieri et al., 2020)\n(Van Hee et al., 2018). This task identiﬁes whether\nthe tweet contains irony or not. For example,\nIRONY : “seeing ppl walking w/ crutches makes\nme really excited for the next 3 weeks of my life\"\nNO IRONY : “@user on stage at #ﬂzjingleball at\nthe @user in #Tampa #iheartradio\"\nSemEval 2018 Task 1 (Barbieri et al., 2020;\nMohammad et al., 2018) This task classiﬁes a\ntweet as one of 4 emotion types {“sadness\", “joy\",\n“anger\", “optimism\"}. For example,\nSADNESS : “ @user I so wish you could some-\nday come to Spain with the play, I can’t believe\nI’m not going to see it #sad\"\nJOY: “ #ThisIsUs has messed with my mind\n&amp; now I’m anticipating the next episode\nwith #apprehension &amp; #delight! #istherea-\nhelplineforthis\"\nANGER : “@user Haters!!! You are low in self\nworth. Self righteous in your delusions. You cower\nat the thought of change. Change is inevitable.\"\nOPTIMISM : “ Don’t be #afraid of the space\nbetween your #dreams and #reality. If you can\n#dream it, you can #make it so\"\nSemEval 2016 Task 6 (Mohammad et al., 2016;\nBarbieri et al., 2020) This task classiﬁes a tweet’s\nstance as {“neutral\", “against\", “favor\"}. Each\ntweet contains a stance on one of the ﬁve differ-\nent target topics {“abortion\", “atheism\", “climate\nchange\", “feminism\", “hillary\"}. For example,\nNEUTRAL : “@user maybe that’s what he wants\n#SemST\"\nAGAINST : “ Life is #precious & so are babies,\nmothers, & fathers. Please support the sanctity of\nHuman Life. Think #SemST\"\nFAVOUR: “ @user @user Nothing to do with\nme. It’s not my choice, nor is it yours, to dic-\ntate what another woman chooses. #feminism\n#SemST\"\nSemEval 2020 Task 6 (Spala et al., 2020). This\ntask classiﬁes whether textbook sentence contains\na deﬁnition. For example,\nCONTAINS DEFINITION : “ Since 2005, auto-\nmated sequencing techniques used by laborato-\nries are under the umbrella of next-generation se-\nquencing, which is a group of automated tech-\nniques used for rapid DNA sequencing\"\nDOESN ’T CONTAIN DEFINITION : “ These au-\ntomated low-cost sequencers can generate se-\nquences of hundreds of thousands or millions of\nshort fragments (25 to 500 base pairs ) in the span\nof one day.\"\nTREC (Li and Roth, 2002). This task classiﬁes\na question into one of six question types: DESC\n(description), ABBR (abbreviation), ENTY (en-\ntity), HUM (people/individual), LOC (location),\nNUM (numeric information), each of which have\nspeciﬁc ﬁne-grained sub-categories. For example,\nDESC: “ How did serfdom develop in and then\nleave Russia?\"\nABBR: “What is the full form of .com?\"\nENTY: “What ﬁlms featured the character Pop-\neye Doyle?\"\nHUM: “What contemptible scoundrel stole the\ncork from my lunch?\"\nLOC: “ What sprawling U.S. state boasts the\nmost airports?\"\nNUM: “How many Jews were executed in con-\ncentration camps during WWII?\"\nSUBJ (Pang and Lee, 2004). This task classiﬁes\na sentence as being subjective or objective. For\nexample,\nSUBJECTIVE : “ smart and alert, thirteen con-\nversations about one thing is a small gem.\"\nOBJECTIVE : “ the movie begins in the past\nwhere a young boy named sam attempts to save\ncelebi from a hunter.\"\nThe Corpus of Linguistic Acceptability\n(Warstadt et al., 2018).This task detects if sen-\ntences are grammatically acceptable by their\noriginal authors. For example,\nGRAMMATICALLY ACCEPTABLE : “ Her little\nsister will disagree with her.\"\nGRAMMATICALLY NOT ACCEPTABLE : “ Has\nnot Henri studied for his exam?\"\nThe Multi-Genre NLI Corpus (Williams et al.,\n2018). This task detects if a premise is a contra-\ndiction or entailment of a hypothesis, or if a hy-\npothesis holds neutral view on the premise.. For\nexample,\n2871\nNEUTRAL : “Premise: Exoatmospheric Kill Ve-\nhicles orbiting Earth would be programmed to col-\nlide with warheads. Hypothesis: Exoatmospheric\nKill Vehicles would be very expensive and hard to\nmake.\"\nENTAILMENT : “Premise: so we have to run our\nclocks up forward an hour and i sure do hate to\nloose that hour of sleep in the morning. Hypoth-\nesis: I don’t like the time change that results in\nlosing an hour of sleeping time.\"\nCONTRADICTION : “Premise: The mayor orig-\ninally hoped groundbreaking would take place six\nmonths ago, but it hasn’t happened yet. Hypoth-\nesis: The mayor doesn’t want groundbreaking to\nhappen at all.\"\nMetaphor as a Medium for Emotion: An Em-\npirical Study (?). This task detects if the appli-\ncation of a word is Literal or Metaphorical. For\nexample,\nWORD : ABUSE\nLITERAL : “This boss abuses his workers.\"\nMETAPHORICAL : “ Her husband often abuses\nalcohol.\"\nPolitical Preference Classiﬁcation (Allaway\nand McKeown, 2020). This task predicts a com-\nment’s stand point on a political topic. For exam-\nple,\nTOPIC : C OMPANIES REGULATION\nCON: “ Regulation of corporations has been\nsubverted by corporations. States that incorporate\ncorporations are not equipped to regulate corpo-\nrations that are rich enough to inﬂuence elections,\nare rich enough to muster a legal team that can\nbankrupt the state. Money from corporations and\ntheir principals cannot be permitted in the politi-\ncal process if democracy is to survive.\"\nPRO: “ Regulation is to a corporation what a\nconscience is to a living person. Without a con-\nscience, we would all be sociopaths. Corporations\ndo not have a conscience, thus they need regula-\ntion to make sure they are focused on beneﬁting\nsociety instead on merely beneﬁting themselves.\"\nNEUTRAL : “ Without government to ensure\ntheir behavior, companies will attempt to make a\nproﬁt even to the DETRIMENT of the society that\nsupports the business. We have seen this in the en-\nvironment, in ﬁnances, in their treatment of work-\ners and customers. Enough.\"\nAirline Service Review 12 This task classiﬁes if\nan airline review has a positive or negative senti-\nment. For example,\nPOSITIVE : “This is such a great deal! Already\nthinking about my 2nd trip to Australia; I haven’t\neven gone on my 1st trip yet!\"\nNEGATIVE : “ amazing to me that we can’t get\nany cold air from the vents.\"\nCovid-19 Tweets Sentiment Analysis 13 This\ntask classiﬁes if a tweet has a positive or negative\nsentiment. For example,\nPOSITIVE : “ Taken by Henk Zwoferink on Sat-\nurday in Wargl, our black beauty hauled a train\nbringing the last tourists home. Our colleagues\nare #workinghard to keep supply chains running\nwhile respecting the measures to ensure every-\none’s #safety. A pleasure to work with such #Ded-\nicatedPeople!\"\nNEGATIVE : “So far, the Minister does not seem\nto have made statement on the catastrophe that\ncan develop if the issue of markets operation is not\naddressed. Food insecurity has potential to make\ncurrent Covid-19 panic look like a kindergarten\nand could lead to riots. I submit.\"\nHotel Review 14 This task predicts if a hotel re-\nview is a positive or negative review. For example,\nNEGATIVE : “ The single rooms like hospital\nrooms single rooms hotel sparse intentional know\nugly like trapped hospital white walls sink basin\nroom small rectangle shape.the beds hard rocks\nblankets rough really noisy.this overrated hotel\nstayed fans type hotels\"\nPOSITIVE : “loved stay, stayed univ, inn 10 days\napril 2005 thoroughly enjoyed, free parking clean\nspacious room friendly staff great breakfast snack,\nloved location, deﬁnitely stay, \"\nStock Market Sentiment 15 This task predicts\nif a comment holds a positive or negative view on\nthe performance of the stock market. For example,\nNEGATIVE : “ GPS wow that wa s a fast fast\nfade...\"\nPOSITIVE : “user Maykiljil posted that: I agree\nthat MSFT is going higher & possibly north of 30\"\n12https://www.kaggle.com/welkin10/\nairline-sentiment\n13https://www.kaggle.com/datatattle/\ncovid-19-nlp-text-classification?select=\nCorona_NLP_test.csv\n14https://www.kaggle.com/andrewmvd/\ntrip-advisor-hotel-reviews\n15https://www.kaggle.com/yash612/\nstockmarket-sentiment-dataset\n2872\nAG-News (Zhang et al., 2015). This task classi-\nﬁes the topic of news based on their contents. For\nexample,\nWORLD NEWS : “ Greek duo could miss drugs\nhearing\"\nSPORTS NEWS : “ AL Wrap: Olerud Cheers\nYankees by Sinking Ex-Team\"\nBUSINESS NEWS : “ Lowe’s Second-Quarter\nProﬁt Rises\"\nTECH NEWS : “ Satellite boosts Olympic secu-\nrity\"\nReal and Fake News 16 This task classiﬁes if a\nnews is fake or real. For example,\nREAL : “ WASHINGTON (Reuters) - Alabama\nSecretary of State John Merrill said he will certify\nDemocratic Senator-elect Doug Jones as winner\non Thursday despite opponent Roy Mooreâ\nx80\nx99s challenge, in a phone call on CNN. Moore, a\nconservative who had faced allegations of groping\nteenage girls when he was in his 30s, ﬁled a court\nchallenge late on Wednesday to the outcome of a\nU.S. Senate election he unexpectedly lost.\"\nFAKE : “Ronald Reagan shut down the Berkeley\nprotests many years ago THIS is how you do it!\"\nDisaster Tweets 17 This task detects if a tweet\nannounces an emergency or a disaster. For exam-\nple,\nCONTAINS DISASTER : “ Our Deeds are the\nReason of this #earthquake May ALLAH Forgive\nus all.\"\nDOES NOT CONTAIN DISASTER : “My dog at-\ntacked me for my food #pugprobs.\"\nObama vs Trump Tweets 18 This task detects if\na tweet was send by Obama or Trump. For exam-\nple,\nOBAMA : “Michelle and I are delighted to con-\ngratulate Prince Harry and Meghan Markle on\ntheir engagement. We wish you a lifetime of joy\nand happiness together.\"\nTRUMP : “Together, we dream of a Korea that is\nfree, a peninsula that is safe, and families that are\nreunited once again!\"\n16https://www.kaggle.com/amananandrai/\nag-news-classification-dataset?select=\ntrain.csv\n17https://www.kaggle.com/c/\nnlp-getting-started/data?select=train.\ncsv\n18https://www.kaggle.com/shaharz/\nclassifying-tweets-of-trump-and-obama\nKaggle Sexually Explicit Tweets 19 This\ndataset provides positive examples of profane\ncomments. For example,\nEXPLICIT “What do guys say when you get\nnaked in front of them for the ﬁrst time?\"\nDemocratic vs Republican Tweets 20 This task\ndetects if a tweet was send by the Democratic or\nRepublican Party. For example,\nDEMOCRATIC : “ #YuccaMountain would re-\nquire moving tens of thousands of metric tons of\nradioactive waste across the country and through\nSouthern Nevada.\"\nREPUBLICAN : “ Stopped by One Hour Heat-\ning&amp; Air Conditioning to discuss the beneﬁts\ntax reform will bring to their business.\"\nWomen E-commerce Clothing Reviews 21\nThis task predicts if the buyer likes or recommends\na product base on its review. For example,\nLIKE : “After reading the previous reviews, i or-\ndered a size larger. i am so glad i did it! it ﬁts\nperfectly! i am 5’4\"/115/32dd and went with the s\nregular. so beautiful! i can’t wait to wear it!\"\nDISLIKE : “ The zipper broke on this piece the\nﬁrst time i wore it. very disappointing since i love\nthe design. I’m actually going to try to replace the\nzipper myself with something stronger, but annoy-\ning that it’s come to that.\"\nQuora Question Pairs 22 This task predicts if\na pair of Quora question is asking for the same\nthing. For example,\nSAME : “ Question 1: How many months does\nit take to gain knowledge in developing Android\napps from scratch?; Question 2: How much time\ndoes it take to learn Android app development\nfrom scratch?\"\nDIFFERENT : “ Question 1: How would you re-\nview the site Waveclues? ; Question 2: Is there a\ngood pay for reviews site out there?\"\nHeadline Sarcasm Detection This task detects\nif is a news headline contains scarcasm. For ex-\nample,\n19https://www.kaggle.com/harsh03/\nsexually-explicit-comments\n20https://www.kaggle.com/kapastor/\ndemocratvsrepublicantweets?select=\nExtractedTweets.csv\n21https://www.kaggle.com/nicapotato/\nwomens-ecommerce-clothing-reviews\n22https://www.kaggle.com/c/\nquora-question-pairs/data\n2873\nSARCASM : “ guy who just wiped out immedi-\nately claims he’s ﬁne\"\nNO SARCASM : “ Donald trump efﬁgies burn\nacross Mexico in Easter ritual\"\nCompany Account Tweets 23 This task detects\nwhether the tweet is targeted towards a company\naccount. For example,\nYES: “@VirginTrains Oh, that’s nice. What are\nyou doing about it? What are you targets next\nyear?\"\nNO: “@115738 That’s the best kind of trick-or-\ntreating. All treats, my friend. -Becky\"\nSMS Spam Detection (Almeida et al., 2013)\nThis task detects whether the SMS is a spam mes-\nsage. For example,\nSPAM: “ Thank you, winner notiﬁed by sms.\nGood Luck! No future marketing reply STOP to\n84122 customer services 08450542832\"\nHAM: “Lol great now I am getting hungry.\"\nClothing Fitness (Misra et al., 2018) Checking\nwhether the customer complains that the cloth is\ntoo small or too large.\nSMALL : “runs a bit small. wish it ﬁt\".\nLARGE : “too big\".\nWater Problem Topic Classiﬁcation 24 Classi-\nfying the topic of a report on water problems. The\nlabels include “biological\", “climatic indicator\",\n“environmental technology\", etc. For example,\nBIOLOGICAL : “ Mineralization of organic\nphosphorus in bottom sediments reaches 40–80%\nand as we found out during the project implemen-\ntation it intensiﬁed in autumn-winter period.\"\nCLIMATIC INDICATOR : “ The average amount\nof precipitation in the lower part of the basin\nmakes 470 mm to 540 mm. The relative average\nannual air humidity makes 60-65%\".\nENVIRONMENTAL TECHNOLOGY : “ Most of\nwastewater treatment facilities require urgent\nmodernization and reconstruction\".\nSexist Statement Detection 25 This task classi-\nﬁes whether the statement is sexist. For example,\nSEXIST : “ It’s impossible for a girl to be faith-\nful.\"\n23https://www.kaggle.com/thoughtvector/\ncustomer-support-on-twitter\n24https://www.kaggle.com/vbmokin/\nnlp-reports-news-classification?select=\nwater_problem_nlp_en_for_Kaggle_100.csv\n25https://www.kaggle.com/dgrosz/\nsexist-workplace-statements\nNON SEXIST : “ Without strength, can we work\nto create wealth?\"\nMovie Spoiler Detection (Misra, 2019) 26 This\ntask classiﬁes whether the movie review is a\nspoiler. For example,\nSPOILER : “I must say that this movie was good\nbut several things were left unsaid. For those who\nhave seen the movie know what I am talking about\nbut for those who haven’t, I don’t want to give\nspoilers. I was also impressed by Vin Diesel’s act-\ning skills. Overall I have to say it was a good\nmovie ﬁlled with several twists and turns.\"\nNON SPOILER : “ The Great Wall amazes with\nits spectacular effects, both on screen and sound.\nUsually I do not appreciate 3D movies, but in this\ncase I felt like it worth it.However, being hon-\nest, the storytelling and the story itself had its\nweaknesses. There were many logical lapses, and\nfor me, many details are still waiting to be an-\nswered.On the other hand, expect decent acting\nespecially from the main characters.All in all, The\nGreat Wall is a solid popcorn-movie, but I ex-\npected a more elaborated unfolding of the legend\nit tells about.\"\nNews Summary/headline Topic Classiﬁcation\n27 This task classiﬁes the topic of the summary of\na news. For example,\nPOLITICS : “City and state ofﬁcials said they re-\nceived little advance warning of the decision.\"\nBUSINESS : “ The streaming giant’s third-\nquarter earnings were nothing like the Upside\nDown.\"\nC Dataset Property Tags\nHere we list all the dataset property tags (Section\n2). We deﬁne two datasets to be “similar\" if they\nhave the set of tags, and disallow meta-tuning on\ndatasets that are similar to evaluation dataset.\nsocial media: whether the source is from social\nmedia (e.g. tweets).\nsocial/political: whether the task is highly re-\nlated to political/social topics. Some examples in-\nclude stance classiﬁcation and hate speech detec-\ntion.\ntopic classiﬁcation: whether the task classiﬁes\nthe topics of the input.\n26https://www.kaggle.com/rmisra/\nimdb-spoiler-dataset?select=IMDB_\nreviews.json\n27https://www.kaggle.com/rmisra/\nnews-category-dataset\n2874\ngood vs. bad : whether the task classiﬁes\nwhether the text is judging something to be good\nor bad.\npaper: whether input text comes from a paper.\nreview: whether the input text is a review of a\nproduct (e.g. movie, hotel).\nquestions: whether the input texts are questions.\nSome examples include classifying whether the\nquestion asks for factual information or subjective\nopinion and detecting whether two questions have\nthe same meaning.\nemotion: whether the task classiﬁes certain\nemotion in the text, for example “hate\", “surprise\",\n“joy\", etc.\nBesides, we do not assign tags to datasets that\nwe are conﬁdent to be different enough from other\ntasks (e.g. extracting whether a text contains def-\ninition), and allow the model to be meta-tuned on\nall other datasets.\nD List of Label Descriptions\nPlease refer to the appendix in our arXiv\nversion: https://arxiv.org/abs/2104.\n04670. Somehow the acl_pubcheck software\npackage always gives us errors.\nE Robustness Checks\nWe report all the descriptive statistics mentioned\nin Section 3 under 3 different types of descrip-\ntion weighting. We additionally compare T5-small\nvs. T5-base, BERT-medium vs. BERT-Base and\nBERT-Base vs. BERT Large. All the results can\nbe seen in Table 3, 4, and 5 Due to space con-\nstraint, we abbreviate P[∆ > t] as > tif t is pos-\nitive, and < t if t is negative. Notice that, since\nwe only have around 20 datasets to evaluate the\nmodel, most of the results presented here are not\nstatistically signiﬁcant at the dataset level; never-\ntheless,\nE.1 Different Description Weighting\nWe weight each label and dataset equally in Table\n4 and 5. We ﬁnd that, under almost all compar-\nisons across different weighting, the mean change\n¯∆ is positive, and the change above a certain\nthreshold t is more frequent than the change below\na certain threshold −t. The only single exception\nthe “Ensemble\" row in Table 5, where there are\nslightly more datasets where the change is lower\nthan -1%. Nevertheless, given that the trend is still\npositive under t = 5% and 10%, and two other\ndescription weightings, we may still conclude that\nensembling label descriptions is more likely to im-\nprove model performance.\nE.2 Larger T5 Models are Better\nIn addition to comparing T5-Base (220 Million\nparameters) vs. T5-Large (770M), we also com-\npare T5-small (60M) vs. T5-base (220M). Across\nall metrics, larger models are signiﬁcantly better.\nMost notably, there is a sudden jump in perfor-\nmance when increasing model size from T5-small\nto T5-base (sometimes 15% increase in ¯∆).\nE.3 Larger BERT Models are Better\nWe also compare different sizes of BERT (Turc\net al., 2019) (41, 110, and 330M) parameters.\nAcross all metrics, larger models are signiﬁcantly\nbetter.\nF Most Relevant Datasets\nTo ensure that we are testing the models’ ability\nto generalize to an unseen tasks, we disallow both\ntraining and testing on datasets that are too sim-\nilar, which is deﬁned as “having the same set of\ndataset property tags\" (Section 2). To help inter-\npret how we deﬁne unseen tasks, for each dataset\nthat we evaluate on, we try to ﬁnd the “most rel-\nevant\" dataset that the model has seen during the\nmeta-tuning phase, and list it in Table 6.\nG Performance Break Down\nFor each model, we average the AUC-ROC scores\nfor each label description for each dataset, and re-\nport the results in Table 7.\nH Accuracy\n2875\n¯∆ > 1% < -1% > 5% < -5% > 10% <-10% std(∆)\nMeta-tuned vs QA 3.3% 59.5% 28.1% 31.4% 10.3% 15.7% 5.9% 9.5%\n220 vs 770M (T5) 6.3% 75.1% 15.1% 47.6% 2.7% 27.0% 0.5% 8.1%\nPre-trained vs. Random 23.8% 95.7% 3.2% 91.4% 1.6% 83.2% 1.1% 14.0%\nEnsemble 0.7% 28.9% 16.8% 8.7% 1.7% 1.7% 0.6% 3.1%\nInitialized with QA 1.1% 54.1% 24.3% 24.3% 11.9% 6.5% 4.9% 6.9%\nTrain on similar 0.7% 43.8% 20.5% 6.5% 4.3% 1.6% 1.1% 3.2%\n60 vs 220M (T5) 14.4% 86.5% 10.3% 79.5% 4.3% 61.1% 2.2% 12.6%\n41 vs. 110M (BERT) 4.3% 65.9% 22.7% 40.0% 10.8% 20.5% 5.9% 9.1%\n110 vs. 340M (BERT) 1.4% 46.5% 35.7% 23.8% 17.3% 11.4% 6.5% 8.5%\nTable 3: All results, with metrics explained in Section 3 and Appendix E. Each label description is weighted\nequally.\n¯∆ > 1% < -1% > 5% < -5% > 10% <-10% std(∆)\nMeta-tuned vs QA 3.0% 57.5% 30.7% 31.3% 11.5% 16.2% 7.3% 10.2%\n220M vs 770M (T5) 5.8% 75.8% 15.5% 46.9% 3.5% 25.6% 1.4% 7.8%\nPre-trained vs. Random 23.7% 93.5% 5.5% 89.4% 3.4% 82.5% 2.1% 15.1%\nEnsemble 0.5% 25.0% 18.8% 6.9% 1.6% 1.7% 0.7% 3.1%\nInitialized with QA 1.2% 54.0% 24.0% 26.0% 11.8% 8.1% 5.3% 7.3%\nTrain on similar 0.7% 44.5% 20.1% 6.0% 4.3% 1.7% 0.8% 3.1%\n60 vs 220M (T5) 15.2% 85.7% 11.4% 79.1% 3.9% 62.5% 1.9% 13.3%\n41 vs. 110M (BERT) 4.8% 67.0% 21.5% 41.9% 9.2% 22.5% 4.9% 9.0%\n110 vs. 340M (BERT) 1.1% 44.3% 36.3% 21.9% 18.2% 11.0% 7.3% 8.5%\nTable 4: All results, with metrics explained in Section 3 and Appendix E. Each label is weighted equally.\n¯∆ > 1% < -1% > 5% < -5% > 10% <-10% std(∆)\nMeta-tuned vs QA 1.2% 55.4% 35.7% 31.2% 17.7% 15.6% 13.6% 11.2%\n220 vs 770M (T5) 6.3% 77.4% 16.5% 51.7% 7.0% 31.6% 4.5% 9.0%\nPre-trained vs. Random 20.2% 89.8% 8.5% 84.8% 6.1% 76.6% 1.5% 15.1%\nEnsemble 0.1% 18.6% 20.2% 4.3% 1.9% 1.5% 1.2% 2.8%\nInitialized with QA 2.3% 59.2% 22.5% 34.3% 9.9% 13.9% 5.7% 7.2%\nTrain on similar 0.6% 48.8% 25.4% 7.3% 5.7% 1.3% 0.9% 3.3%\n60 vs 220M (T5) 12.1% 84.6% 12.9% 73.6% 3.5% 52.9% 2.2% 11.6%\n41 vs. 110M (BERT) 7.0% 74.6% 13.8% 58.5% 6.8% 31.5% 2.9% 8.9%\n110 vs. 340M (BERT) 1.1% 45.6% 36.1% 25.5% 18.6% 10.8% 9.3% 8.8%\nTable 5: All results, with metrics explained in Section 3 and Appendix E. Each dataset is weighted equally.\n2876\nEvaluation Dataset Most Relevant Training Dataset\nSemEval 2016 Task 6, stance classiﬁcations on\nissues like feminism, atheism, etc\nSemEval 2019 Task 5, detecting hate speech\nagainst women and immigrants\nSemEval 2019 Task 6, classifying whether the\ntext is offensive\nA dataset from Kaggle that classiﬁes sexually ex-\nplicit comments\nSemEval 2019 Task 5, detecting hate speech\nagainst women and immigrants\nSemEval 2016 Task 6, stance classiﬁcations on\nissues like feminism, atheism, etc\nTREC, classifying the type the question is\nasking about (e.g. numbers, acronyms, hu-\nman/occupations, etc)\nAG News, which classiﬁes news into different\ncategories (e.g. sports, world events).\nSemEval 2019 Task 8, classifying whether the\nquestion is asking for subjective opinion, factual\ninformation, or simply having a conversation\nN/A\nSUBJ, classifying whether the text contains sub-\njective or objective information\nN/A\nQQP, classifying whether two questions have the\nsame meaning\nN/A\nYin et al. (2019) emotion classiﬁcation, classi-\nfying text into 9 emotion types, such as “joy\",\n“anger\", “guilt\", “shame\", etc.\nClassifying whether an IMDB movie review is\npositive.\nYin et al. (2019) situation classiﬁcation, classify-\ning which disaster situation people are experienc-\ning, e.g. “regime change\", “crime and violence\",\nand what resource they need, e.g. “food and wa-\nter\", “search and rescue\".\nClassifying (binary) whether a tweet is related to\na natural disaster.\nYin et al. (2019) topic classiﬁcation, classify-\ning the domain of an article into domains such\nas “family and relationship\", “education\", “busi-\nness\", “sports\"\nclassifying the domain of a paper abstract into\nphysics, maths, computer sciences, and statistics.\nAG News, which classiﬁes news into different\ncategories (e.g. sports, world events).\nAbstract Domain classiﬁcation, classifying the\ndomain of a paper abstract into physics, maths,\ncomputer sciences, and statistics.\nAbstract Domain classiﬁcation, classifying the\ndomain of a paper abstract into physics, maths,\ncomputer sciences, and statistics.\nAG News, which classiﬁes news into different\ncategories (e.g. sports, world events).\nIMDB movie reviews, classifying whether the\nuser feels positive about the movie\nStock market sentiment, classifying whether a\ncomment is optimistic about the market.\nCoLA, classifying whether a sentence is gram-\nmatical\nN/A\nSemEval 2020 Task 6, classifying whether a sen-\ntence contains a deﬁnition\nN/A\nSpam classiﬁcation, classifying whether a text\nmessage is a spam\nclick-bait classiﬁcation, classifying whether the\ntitle of an article is a clickbait.\nSemEval 2018 Task 1, classifying a tweet as one\nof 4 emotion types {“sadness\", “joy\", “anger\",\n“optimism\"}\nClassifying whether an IMDB movie review is\npositive.\nSemEval 2018 Task 3, classifying whether a\ntweet is ironic\nclassifying whether a news title is sarcastic.\nTable 6: For each dataset that we evaluate on, we list the task in the training split that we consider to be the most\nrelevant. We list “N/A\" if we think that none of the training dataset is particularly relevant.\n2877\nQA QA + Meta Meta T5 220M BERT 340M\nAbstract Classiﬁcation 76.9% 84.3% 81.2% 68.0% 85.3%\nAG News 76.5% 82.0% 77.8% 69.9% 69.5%\nStance (Hillary) 74.8% 79.8% 73.8% 69.0% 63.2%\nHate Speech 59.4% 66.0% 64.1% 59.6% 69.2%\nStance (Feminism) 67.8% 71.6% 69.1% 61.0% 64.8%\nStance (Climate) 75.8% 81.7% 79.6% 72.0% 76.2%\nEmotion Classiﬁcation∗ 67.6% 70.5% 68.0% 65.0% 64.0%\nEmotion Classiﬁcation (SemEval) 81.6% 85.2% 81.7% 76.1% 74.2%\nIrony Detection 67.9% 83.4% 80.2% 61.0% 64.9%\nStance (Atheism) 60.2% 62.4% 65.6% 55.1% 60.9%\nQQP 54.1% 61.1% 68.6% 56.7% 66.9%\nTREC 59.3% 63.9% 76.4% 73.4% 66.9%\nStance (Abortion) 58.2% 61.3% 62.8% 60.5% 59.5%\nOffensive Speech 76.6% 80.4% 79.5% 74.5% 80.6%\nCoLA 52.3% 49.4% 49.8% 49.6% 50.0%\nSUBJ 62.8% 66.8% 58.7% 54.5% 50.2%\nSituation Classiﬁcation∗ 73.9% 80.4% 79.3% 75.5% 79.5%\nSPAM Detection 57.2% 45.4% 35.0% 49.3% 47.8%\nIMDB Movie Review 92.9% 94.0% 90.5% 67.7% 84.4%\nTopic Classiﬁcation∗ 77.6% 82.7% 84.0% 77.5% 80.7%\nDeﬁnition Detection 72.8% 73.5% 63.9% 63.6% 60.2%\nQuestion Type Classiﬁcation 75.1% 73.8% 59.3% 51.8% 64.5%\nTable 7: Zero shot performance of each model on each dataset. “QA\" means the UniﬁedQA model; “QA + Meta\"\nmeans meta-tuning with UniﬁedQA initialization; “Meta\" means meta-tuning on T5 (770M) parameters. To save\nspace, we use “*\" to denote datasets from Yin et al. (2019).\n2878\nDataset name #classes Accuracy\n2016SemEval6TweetEvalStanceAtheism 3 66\nKaggleNewsTopicClassiﬁcation 4 64\n2019SemEval6TweetEvalOffensive 2 28\n2019SemEval8Qtype 2 73\n2018SemEval3TweetEvalIrony 2 39\n2016SemEval6TweetEvalStanceHillary 3 55\nsubj 2 61\ntrec 6 38\nKaggleQuoraQPairs 2 50\ndeﬁnition 2 32\nBenchmarkingZeroshotTopic 10 59\n2019SemEval5TweetEvalHate 2 42\ncola 2 55\n2018SemEval1TweetEvalEmotion 4 72\n2016SemEval6TweetEvalStanceAbortion 3 64\nKaggleIMDBMovieReview 2 85\n2016SemEval6TweetEvalStanceClimate 3 61\nKaggleSMSSPAM 2 14\n2016SemEval6TweetEvalStanceFeminist 3 53\nTable 8: We report the accuracy of the meta-tuned model for completeness according to the request of the reviewers.\nHowever, given that accuracy is very sensitive to thresholding (Zhao et al., 2021) and is generally unreliable when\nthe labels are imbalanced, these numbers are not likely to be informative. Additionally, to speed up evaluation, we\nuse a subsample of the original test split for some datasets, so these numbers are not directly comparable to those\nin the other papers either.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8495044708251953
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7405303716659546
    },
    {
      "name": "Machine learning",
      "score": 0.6672128438949585
    },
    {
      "name": "Meta learning (computer science)",
      "score": 0.6491428017616272
    },
    {
      "name": "Construct (python library)",
      "score": 0.6165456175804138
    },
    {
      "name": "Inference",
      "score": 0.5703610181808472
    },
    {
      "name": "Language model",
      "score": 0.526098370552063
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5156306028366089
    },
    {
      "name": "Natural language processing",
      "score": 0.4963546395301819
    },
    {
      "name": "Question answering",
      "score": 0.48133453726768494
    },
    {
      "name": "Word (group theory)",
      "score": 0.4741089940071106
    },
    {
      "name": "Shot (pellet)",
      "score": 0.45344430208206177
    },
    {
      "name": "Focus (optics)",
      "score": 0.4408383071422577
    },
    {
      "name": "Task (project management)",
      "score": 0.20097512006759644
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ]
}