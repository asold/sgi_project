{
  "title": "Performance of Generative Pretrained Transformer on the National Medical Licensing Examination in Japan",
  "url": "https://openalex.org/W4366815612",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5090396619",
      "name": "Yudai Tanaka",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5046454748",
      "name": "Takuto Nakata",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5023358509",
      "name": "Ko Aiga",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5079089371",
      "name": "Takahide Etani",
      "affiliations": [
        "Kanazawa University",
        "Keio University Shonan Fujisawa",
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A5032378133",
      "name": "Ryota Muramatsu",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5015094224",
      "name": "Shun Katagiri",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5103019292",
      "name": "Hiroyuki Kawai",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5063031690",
      "name": "Fumiya Higashino",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5077482664",
      "name": "Masahiro Enomoto",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5007303109",
      "name": "Masao Noda",
      "affiliations": [
        "Jichi Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5079635419",
      "name": "Mitsuhiro Kometani",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5050705557",
      "name": "Masayuki Takamura",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5045375203",
      "name": "Takashi Yoneda",
      "affiliations": [
        "Kanazawa University"
      ]
    },
    {
      "id": "https://openalex.org/A5060499521",
      "name": "Hiroaki Kakizaki",
      "affiliations": [
        "Stelic (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5035537632",
      "name": "Akihiro Nomura",
      "affiliations": [
        null,
        "Kanazawa University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4361298490",
    "https://openalex.org/W4200088206",
    "https://openalex.org/W4281884846",
    "https://openalex.org/W2747680751",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4316671929",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4323981476",
    "https://openalex.org/W4365143687"
  ],
  "abstract": "Abstract The remarkable performance of ChatGPT, launched in November 2022, has significantly impacted the field of natural language processing, inspiring the application of large language models as supportive tools in clinical practice and research worldwide. Although ChatGPT recently scored high on the United States Medical Licensing Examination, its performance on medical licensing examinations of other nations, especially non-English speaking nations, has not been sufficiently evaluated. This study assessed ChatGPT’s performance on the National Medical Licensing Examination (NMLE) in Japan and compared it with the actual minimal passing rate for this exam. In particular, the performances of both the GPT-3.5 and GPT-4 models were considered for the comparative analysis. We initially used a model and prompt tuning set of 290 questions without image data from the previous 116 th NMLE (held in February 2022) to maximize the performance for delivering correct answers and explanations of the questions. Thereafter, we tested the performance of the best ChatGPT model (GPT-4) with tuned prompts on a dataset of 262 questions without images from the latest 117 th NMLE (held in February 2023). The best model with the tuned prompts scored 82.7% for the essential questions and 77.2% for the basic and clinical questions, both of which sufficed the minimum passing rates of 80.0% and 74.6%, respectively. Simultaneously, we identified the three major factors contributing to the generation of the incorrect answers—insufficient medical knowledge, information on Japan-specific medical system and guidelines, and mathematical errors. In conclusion, GPT-4 powered ChatGPT with our optimally tuned prompts achieved a minimum passing rate in the latest 117 th NMLE in Japan. Although we express strong concerns regarding the use of the current ChatGPT for medical purposes so far, these artificial intelligence models may soon have the potential to serve as one of the best “sidekicks” for solving medical and healthcare problems. Author summary ChatGPT’s remarkable performance has inspired the use of large language models as supportive tools in clinical practice and research. Although it scored well in the US Medical Licensing Examination, its effectiveness in relevant examinations of non-English speaking countries remain unexplored. This study assessed the performance of ChatGPT with GPT-3.5 and GPT-4 models in Japan’s National Medical Licensing Examination (NMLE). Initially, we used a tuning set of 290 questions from the 116th NMLE, and then the GPT-4 model with tuned prompts was tested on 262 questions from the 117th NMLE. The model scored 82.7% for essential and 77.2% for basic and clinical questions, surpassing the minimum passing rates. Incorrect answers were attributed to insufficient medical knowledge, Japan-specific medical system information, and mathematical errors. In conclusion, GPT-4 powered ChatGPT achieved a minimum passing rate and might have the potential for a valuable tool for fulfilling the needs of medical and healthcare fields.",
  "full_text": " 1 \nPerformance of Generative Pretrained Transformer on the National Medical 1 \nLicensing Examination in Japan 2 \n 3 \n 4 \nYudai Tanaka1,2,3*, Takuto Nakata1,2,3*, Ko Aiga2*, Takahide Etani1,4,5, Ryota Muramatsu1,3, Shun 5 \nKatagiri1, Hiroyuki Kawai1, Fumiya Higashino1, Masahiro Enomoto1, Masao Noda6, Mitsuhiro 6 \nKometani2, Masayuki Takamura7, Takashi Yoneda2,8, Hiroaki Kakizaki9, Akihiro 7 \nNomura2,7,8,10,11* 8 \n 9 \n 10 \n1School of Medicine, Kanazawa University, Kanazawa, Japan 11 \n2Department of Health Promotion and Medicine of the Future, Kanazawa University Graduate 12 \nSchool of Medicine, Kanazawa, Japan 13 \n3Department of Molecular and Cellular Pathology, Kanazawa University Graduate School of 14 \nMedicine, Kanazawa, Japan 15 \n4Graduate School of Media and Governance, Keio University, Fujisawa, Japan 16 \n5Advanced Research Center for Human Sciences, Waseda University, Saitama, Japan 17 \n6Department of Otolaryngology and Head and Neck Surgery, Jichi Medical University, Tochigi, 18 \nJapan 19 \n7Department of Cardiovascular Medicine, Kanazawa University Graduate School of Medical 20 \nSciences, Kanazawa, Japan 21 \n8College of Transdisciplinary Sciences for Innovation, Kanazawa University, Kanazawa Japan 22 \n9MICIN, Inc., Tokyo, Japan 23 \n10Frontier Institute for Tourism Science, Kanazawa University, Kanazawa, Japan 24 \n11CureApp Institute, Karuizawa, Japan 25 \n*Contributed equally 26 \n 27 \n 28 \n 29 \nCorresponding author: 30 \nAkihiro Nomura, MD, PhD 31 \nAssociate Professor 32 \nCollege of Transdisciplinary Sciences for Innovation, Kanazawa University  33 \nKakuma-machi, Kanazawa, Ishikawa, 9201192, Japan 34 \nE-mail: anomura@med.kanazawa-u.ac.jp 35 \nORCID ID: 0000-0001-6647-8240  36 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n 2 \nAbstract 37 \nThe remarkable performance of ChatGPT, launched in November 2022, has significantly impacted 38 \nthe field of natural language processing, inspiring the application of large language models as 39 \nsupportive tools in clinical practice and research worldwide. Although ChatGPT recently scored 40 \nhigh on the United States Medical Licensing Examination, its performance on medical licensing 41 \nexaminations of other nations, especially non-English speaking nations, has not been sufficiently 42 \nevaluated. This study  assessed ChatGPT ’s performance on the National Med ical Licensing 43 \nExamination (NMLE) in Japan and compared it with the actual minimal passing rate for this exam. 44 \nIn particular, the performances of both the GPT-3.5 and GPT-4 models were considered for the 45 \ncomparative analysis. We initially used a model and prompt tuning set of 290 questions without 46 \nimage data from the previous 116th NMLE (held in February 2022) to maximize the performance 47 \nfor delivering correct answers and explanations of the questions. Thereafter, we tested the 48 \nperformance of the best ChatGPT model (GPT-4) with tuned prompts on a dataset of 262 questions 49 \nwithout images from the latest 117th NMLE (held in February 2023). The best model with the 50 \ntuned prompt s scored 82.7% for the essential questions and 77.2% for the basic and clinical 51 \nquestions, both of which sufficed the minimum passing rates of 80.0% and 74.6% , respectively. 52 \nSimultaneously, we identified the three major factors contributing to the generation of the incorrect 53 \nanswers—insufficient medical knowledge, information on Japan-specific medical system  and 54 \nguidelines, and mathematical errors. In conclusion, GPT-4 powered ChatGPT with our optimally 55 \ntuned prompts achieved a minimum passing rate in the latest 117th NMLE in Japan. Although we 56 \nexpress strong concerns regarding the use of the current ChatGPT for medical purposes  so far, 57 \nthese artificial intelligence models may soon have the potential to serve as one of the best 58 \n“sidekicks” for solving medical and healthcare problems. 59 \n 60 \n 61 \nKey words: medical licensing examination, large language model, generative pretrained 62 \ntransformer, artificial intelligence  63 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 3 \nAuthor summary (150 words) 64 \nChatGPT's remarkable performance has inspired the use of large language models as supportive 65 \ntools in clinical practice and research. Although it  scored well in the US Medical Licensing 66 \nExamination, its effectiveness in relevant examinations of non-English speaking countries remain 67 \nunexplored. This study assessed the performance of ChatGPT with GPT-3.5 and GPT-4 models in 68 \nJapan's National Medical Licensing Examination (NMLE). Initially, we used a tuning set of 290 69 \nquestions from the 116th NMLE, and then the GPT-4 model with tuned prompts was tested on 262 70 \nquestions from the 117th NMLE. The model scored 82.7% for essential and 77.2% for basic and 71 \nclinical questions, surpassing the minimum passing rates. Incorrect answers were attributed to 72 \ninsufficient medica l knowledge, Japan -specific medical system information, and mathematical 73 \nerrors. In conclusion, GPT-4 powered ChatGPT achieved a minimum passing rate and might have 74 \nthe potential for a valuable tool for fulfilling the needs of medical and healthcare fields.  75 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 4 \nIntroduction 76 \nIn recent decades, artificial intelligence (AI) algorithms have been widely applied in medical and 77 \nhealthcare fields [1]. Currently, the AI algorithms available for clinical application s have been 78 \ndeveloped using previous rule-based methods as well as recent machine learning (ML) methods 79 \nincluding its subfield of deep learning, promoted by the continually increasing availability of  80 \ncomputer resources and vast amount of medical data [2]. Consequently, these medical AI products 81 \nhave been implemented to obtain target ed outputs such as the prediction of future disease risk, 82 \nclassification as diagnostic support, or generation of various texts or images using natural 83 \nlanguage processing (NLP) in medicine [1-3]. 84 \nNLP is an area of AI that addresses the interaction between human languages and machines 85 \n[4]. The major roles of NLP in medicine and healthcare include serving as supportive tools in 86 \nclinical practice and research [3]. Beyond the prediction of certain risk factors or clinical decision-87 \nmaking, NLP assists physicians and researchers to efficiently extract, translate, classify and 88 \nanalyze patients’ information and clinical-free text in electronic medical and health records , in 89 \naddition to dialogue generation and answering medical information [3, 4]. The performance of 90 \nNLP has dramatically improved following the emergence of transformer-based large language 91 \nmodels (LLMs). A transformer is a type of neural network model that employs self-attention 92 \nmechanism, relating multiple positions of a single sequence to compute a representation of the 93 \nsequence [5]. LLMs are created using advanced ML techniques, especially deep neural networks, 94 \ntrained on enormous amounts of text data from the Internet and other sources [4]. A few notable 95 \nLLMs include pretrained Bidirectional Encoder Representations from Transformers ( BERT) [6], 96 \nLanguage Models for Dialog Applications (LaMDA) [7], Pathway Language Model (PaLM) [8], 97 \nLarge Language Model Meta (LLaMA) [9], and Generative Pretrained Transformer (GPT)-3 and 98 \nlater models [10-12]. 99 \nRecently, InstructGPT (GPT-3.5)—a GPT model employing 175 billion parameters with 100 \nsupervised fine-tuning and reinforcement learning from human feedback [11]—and its dialogue-101 \noptimized chatbot (ChatGPT) launched in November 2022 have significantly impacted NLP fields 102 \n[13]. By predicting the subsequent element of the texts, ChatGPT can comprehend user prompts 103 \nand generate human-like responses, expressed in ethical, sentimental, logical, and creative manner, 104 \nwithout any additional training (e.g., foundation model) [14]. Although GPT is a non -domain-105 \nspecific LLM , not exclusively intended to be used for medica l or healthcare fields , recent 106 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 5 \npublications have demonstrated that ChatGPT (GPT-3.5) possesses sufficient ability to pass the 107 \nUnited States Medical Licensing Examination [15, 16] . In contrast, a nother study reported 108 \nChatGPT's inadequate performance on non-English-based Korean medical questions [17]. 109 \nAlthough the performance variation can be attributed to differences in  languages, domestic 110 \nhealthcare systems, diagnostic criteria , and treatment strategies , the relationship between these 111 \ndifferences and ChatGPT’s performance in answering medical question s remains unclear. 112 \nFurthermore, the performance of ChatGPT with the current GPT-4 model employing an estimated 113 \n10 trillion parameters [12] has not yet been evaluated on the latest Medical Licensing Examination, 114 \nwhich was originally written in non-English texts and held after the completion of GPT-4 model 115 \ntraining (August 2022) [18]. 116 \nTherefore, this study tested the performance of GPT (both GPT-3.5 and GPT-4 models) on 117 \nthe 117th National Medical Licensing Examination (NMLE) (held in February 2023 in Japan), 118 \nwhich was originally conducted in the Japanese language . In particular, questions from the 119 \nprevious year (116th NMLE exam held in Feb ruary 2022) were used as a model and prompt 120 \nperformance tuning set before using the latest questions (117th exam held in February 2023) as a 121 \nperformance testing set to verify whether GPT can qualify for the actual minimal passing rate of 122 \nthis examination. 123 \n 124 \n 125 \nResults 126 \nImproving performance through English translation and tuned prompts in 116th NMLE (2022) 127 \nInitially, we used the non-image-based questions from 116th NMLE in Japan to develop the optimal 128 \ninput prompts for ChatGPT to maximize the correct answer rate. We extracted the question data 129 \nfrom the 116th NMLE containing 394 questions (originally 400 questions, but six were officially 130 \nremoved from scoring evaluation) . Thereafter, we removed questions with image data (n = 104) 131 \nand analyzed the remaining 290 questions without image data (Figure 1). 132 \nUsing the ChatGPT API powered by GPT3.5 , we initially tested its performance for the 133 \noriginal questions in Japanese language. Initially, we obtained a correct answer rate  of 52. 8% 134 \n(153/290) with an output error rate of 5.5% (16/290). Accordingly, we used updated prompts to 135 \ntranslate the original Japanese NMLE questions into English using ChatGPT before inputting them 136 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 6 \nas questions. Although this marginally increased the correct answer rate to 56.2% (163/290), the 137 \noutput errors increased to 14.8% (43/290; Figure 2). 138 \nTo further improve the correct answer rate and reduce the errors, we tuned our prompts 139 \nfor each question type (Basics of Medicine, Clinical Medicine, and Comprehension). In particular, 140 \nwe provided sample outputs and directed the model to translate the questions into plain English 141 \nand create summaries before answering the questions (Figure 3). This tuned prompt improved the 142 \ncorrect answer rate  to 63.1% (18 3/290) with a reduced output error rate of 7.6% (2 2/290). 143 \nFurthermore, we applied the above-tuned prompt s to the GPT-4-based ChatGPT, which  144 \ndemonstrated a correct answer rate of 82.8% (240/290) and a minimal error rate of 1.0% (3/29 0) 145 \n(Figure 2).  146 \n 147 \nGPT-4-based ChatGPT performance on 117th (2023) NMLE with tuned prompt 148 \nThereafter, we evaluated that the performance of the best model (GPT-4) with a tuned prompt for 149 \nthe test set of 262 questions without image data from the 117th NMLE in Japan, held in February 150 \n4th and 5th, 2023, after the completion of GPT-4 model training in August 2022 (Figure 1). With a 151 \ntuned prompt, the best model achieved a correct answer rate  of 78.6% (206/262) and an output 152 \nerror rate of 0.8% (2/262) (Table 1). 153 \nThe present results were compared with the actual minimal passing rate on the examination. 154 \nThe current model with a tuned prompt scored 82.7% (129/156) for essential questions and 77.2% 155 \n(139/180) for basic and clinical questions, both of which qualified the minimum passing rates of 156 \n80.0% and 74.6%, respectively (Figure 2) [19]. Notably, we applied the GPT-4 model with tuned 157 \nprompts to the entire set of 395 questions (text-only) in the 117th NMLE, regardless of containing 158 \nimage data (originally 400 questions, but five were officially removed from scor ing evaluation). 159 \nThis optimal model attained near-passing levels of 78.5% (157/200) for essential questions and 160 \n73.2% (216/295) for basic and clinical questions. 161 \n 162 \nExploratory analysis of incorrect ChatGPT responses and their associated explanations 163 \nTo further enhance the performance of the model , we performed an exploratory analysis of 56 164 \nincorrect answers provided by the optimal GPT-4 model with tuned prompts for the 117th NMLE 165 \nquestions. As listed in Table 2, the three primary factors contributing to the generation of incorrect 166 \nanswers by the model included insufficient medical knowledge (33/56, 58.9%), Japan -specific 167 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 7 \nmedical system information (17/56, 30.4%), and mathematical errors (4/56, 7.1%). Concerning the 168 \ninsufficient medical knowledge, the areas of incorrect answers were not specific and spanned 169 \nacross various medical fields. Notably, certain answers were o utdated or critically incorrect in 170 \ncurrent medical contexts (Figure 4). In terms of Japan-specific medical system, ChatGPT failed 171 \nto adequately answer questions related to Japanese medicolegal laws applicable in the medical and 172 \nhealthcare field, guidance from the Ministry of Health, Labour, and Welfare (MHLW) in Japan, 173 \nand guidelines, especially those related to public health . Additionally, we noted several 174 \nmathematical errors such as in  addition calculations (e.g., the explanation and addition formula 175 \nwere correct, but the answer was wrong) and handling decimal points (because of translation errors 176 \nfrom the phrase “rounding to first decimal point” from Japanese). 177 \n 178 \n 179 \nDiscussion 180 \nThis study evaluated the performance of GPT on the Japanese Medical Licensing Examination. 181 \nThe results indicate that 1) GPT-4 with a tuned prompt cleared the minimal passing rate on the 182 \n116th (2022) NMLE in Japan; 2) GPT-4 with tuned prompt qualified the minimum passing rate on 183 \nthe latest 117 th NMLE (2023); and 3) Inadequate medical knowledge, Japan -specific medical 184 \nsystem information, and mathematical errors were the primary factors associated with the incorrect 185 \nanswers generated by the optimal model. Despite the absence of image data in the questions, this 186 \nstudy demonstrated the first attempt to use the best available ChatGPT model with tuned prompts 187 \nto achieve a minimum passing rate for the latest 117th NMLE in Japan. 188 \nThis study provides several conclusions. First, GPT-4 with a tuned prompt cleared the 189 \nminimal passing rate on the 116th NMLE in Japan held in February 2022. Although GPT-3.5-based 190 \nChatGPT achieved a correct answer rate of 52.8% for Japanese questions, it increased to 56.2% 191 \nafter translating the questions into English. As GPT-3, the original GPT-3.5, was primarily trained 192 \nin English, it delivers a higher performance when responding to prompts in English compared to 193 \nother languages  [10]. Similarly, a recent multilingual performance evaluation of GPT -4, an 194 \nimproved version of GPT-3, confirmed that the best performance is more generally obtained with 195 \nEnglish prompts [12]. After tuning our prompts to include a translation procedure into plain 196 \nEnglish and modifying the output format based on the question type, the correct response rate 197 \nincreased to 6 3.1%. This finding is consistent with previous studies claiming that prompt 198 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 8 \nengineering can improve model task performance [12, 20]. These improved correct response rates 199 \ncan be attributed to English being the majority of the language in the training data, i.e., the Internet, 200 \nused by non-experts [21]. Although the error rate increased to 14.8% upon translating the Japanese 201 \nquestions into English, it notably decreased to 7.6% after tuning the prompts by including the 202 \nformat of the output. This result suggests that providing samples and standardizing the output 203 \nformat can produce the desired output format and reduce the number  of errors. Finally, upon 204 \napplying these optimized prompts to GPT-4-based ChatGPT, the correct response rate increased to 205 \n82.8% and the error rate plummeted to 1.0%. This significant improvement in performance can be 206 \nascribed to the advanced architecture and training of GPT-4 [12]. 207 \nSecond, even in case of the latest 117th NMLE (2023), GPT-4 with tuned prompt qualified 208 \nthe actual minimum passing rate. GPT-4 has passed various professional examinations in English, 209 \nincluding the practice bar exam with a score in the top 10% of examinees  [12]. A previous study 210 \nreported that ChatGPT (GPT-3.5) failed to achieve the minimum passing rates [22]. However, this 211 \nstudy demonstrated that ChatGPT (GPT-4) can pass the 117th NMLE with the optimized prompt 212 \ntuning method proposed herein. The current results can be derived from the exquisite combination 213 \nof essential factors such as English translation and optimally tuned prompts for obtaining correct 214 \nanswers through the best performance of the latest ChatGPT model. 215 \n Third, inadequate medical knowledge, information related to the medical and healthcare 216 \nsystem guidelines of Japan, and mathematical errors formed the three major factors of the incorrect 217 \nanswers generated by the best available ChatGPT model with tuned prompts. Among the incorrect 218 \nanswers associated under inadequate medical knowledge, no significant bias was observed for the 219 \nmedical fields relevant to each question. Furthermore, even after providing incorrect answers, the 220 \nmodel output plausible but wrong medical explanations (so-called hallucinations in LLM outputs 221 \n[23]). Therefore, even if the model exhibits a performance level that surpasses the minimum score 222 \nfor the NMLE, a broader range of specialized and up -to-date medical knowledge regarding 223 \nstandard treatments  should be inputted . In addition, output receivers should be equipped with 224 \nprofessional medical knowledge to assess the correctness of the output. For the Japan -specific 225 \nsystem, several incorrect answers were observed, especially in public health -related questions, 226 \nwhich are based on Japanese laws, guidelines, and unique systems. Although the GPT-4 powered 227 \nChatGPT delivered improved performance in terms of output differences between the languages, 228 \nevery country should perform their individual localization in terms of the applicable laws and 229 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 9 \nsystems considering the language differences. Furthermore, in certain cases related to 230 \nmathematical errors, the calculation formula in the explanation was correct, but the result and the 231 \nfinal answer output were incorrect. Moreover, an instruction of \"approximating the decimal place\" 232 \nwas not properly comprehended by ChatGPT during the Japanese-to-English translation. As such, 233 \ncalculation problems are reported as one of the areas where LLMs still exhibit relatively low 234 \naccuracy [24], indicating that calculation problems may be a relatively unsuitable field for current 235 \nChatGPT. 236 \nAs discussed, we express strong concerns regarding the use of the current ChatGPT for  237 \nmedical purposes, as OpenAI has already indicated that the models should not be used for  238 \nproviding triage, diagnosis, or treatment options for life -threating issues or severe medical  239 \nconditions [25]. Indeed, for use in medical settings, an approval must be obtained from regulatory 240 \nagencies, e.g., software as a medical device . Moreover, utilizing such technology is already 241 \ndifficult with its several black-box aspects  [12]. Various countries have released statements 242 \nregarding the applications of LLMs  in medical fields [26, 27]. Although the versatility of these 243 \nmodels hinders the verification of their validity and they require enormous computational 244 \nresources and costs, we believe that the advanced medical foundation AI model [28] can replace 245 \ntask-specific approach AI models and will appear not far off , with scientifically proven clinical 246 \nefficacy and safety in medical and healthcare fields. 247 \nThe novelty of this study is that it is the first research to achieve a minimum passing rate 248 \nusing 262 non-image questions in the latest 117th NMLE in Japan with the ChatGPT GPT-4 version 249 \nwith the optimally tuned prompts. The limitations of this study were as follows. First, we only 250 \nused questions without image data to evaluate the performance of the best available model with 251 \ntuned prompts, although it might be fair to assess the ability of the model to pass the examination 252 \nusing all questions, regardless of image data. However, as revealed from the Results, we observed 253 \na favorable model performance even upon using the entire question set in the 117th NMLE in Japan. 254 \nSecond, t he NMLE in Japan uniquely included strongly not -recommended \"contraindication\" 255 \nanswer choices within the questions. The MHLW in Japan has set the minimum passing criteria 256 \nregarding selecting contraindication answer choices to be equal or less than three for the 116 th 257 \nNMLE or two for the 117 th NMLE. As the real number of contraindication answer choices were 258 \nnot officially announced by the MHLW, we could not use them in the current performance 259 \nevaluation. 260 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 10 \nIn conclusion,  GPT-4 powered ChatGPT with optimal ly tuned prompts achieved a 261 \nminimum passing rate in the latest 117 th NMLE in Japan. In addition, the model scored near -262 \npassing levels for the entire test dataset of 395 questions, regardless of medical image data. The 263 \nupcoming GPT-4 version , which features enhanced image recognition capabilities , will easily 264 \nqualify the minimum passing rate and achieve top-tier scores, as reported in other English-based 265 \nexaminations [12]. We again express strong concerns in terms of using of the current ChatGPT for 266 \nmedical purposes so far. However, beyond its original design of answering examination questions 267 \nfor humans, these AI models might have the potential be regarded as one of the best “sidekicks” 268 \nfor solving problems and fulfilling the current needs in the medical and healthcare fields  in the 269 \nnear future.  270 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 11 \nMaterials and methods 271 \nStudy overview 272 \nThis study evaluated the performance of GPT models on the NMLE in Japan. We utilized both the 273 \nGPT-3.5 and GPT-4 models of ChatGPT (Open AI, Inc., San Francisco, CA, USA) . Initially, the 274 \nquestions from the 116th NMLE in Japan (February 2022) were used as a model and prompt tuning 275 \nset to optimize the performance of obtaining the correct answers and explanations. Subsequently, 276 \nwe assessed the performance of the best ChatGPT model (GPT -4) with the tuned prompts for 277 \nanswering the questions from the 117th NMLE in Japan (February 2023). 278 \n 279 \nInput source 280 \nThe questions and answers for the 116th NMLE in Japan were obtained from the official website 281 \nof the MHLW, Japan [29]. For the latest 117th NMLE, we manually performed optical character 282 \nrecognition on the original question papers to create input data and extracted the official answers 283 \nfrom the MHLW website [19]. The examination comprised six blocks (A–F), with 75 questions in 284 \nblocks A, C, D, and E, and 50 questions in blocks B and F. Note that s ix questions in the 116th 285 \nNMLE and five in the 117th NMLE were excluded. In addition, all image-containing questions 286 \nwere removed from both the prompt-tuning and the performance-testing datasets, because up till 287 \nearly April 2023, only text -based questions could be used as input to the ChatGPT interface , 288 \nincluding the API. The number of image-containing questions was 104 in the 116th NMLE and 133 289 \nin the 117th NMLE. Thereafter, according to the Japanese NMLE scoring method, the remaining 290 \nquestions without image data were classified into the categories of \"Essential” and “Basic and 291 \nClinical.” The 116th NMLE in Japan included 47 questions related to basics of medicine (essential), 292 \n24 questions of clinical medicine (essential), 14 questions on comprehension (essential), 65 293 \nquestions regarding basics of medicine (general), 30 questions in basics of medicine (specifics), 294 \n31 questions of clinical medicine (general), 60 questions of clinical medicine (specifics), and 19 295 \nquestions on comprehension. The 117th NMLE in Japan comprised 45 questions related to basics 296 \nof medicine (essential), 22 questions of clinical medicine (essential), 15 questions on 297 \ncomprehension (essential), 61 questions from the basics of medicine (general), 27 questions on the 298 \nbasics of medicine (specifics), 3 6 of clinical medicine (general), 46 questions related to clinical 299 \nmedicine (specifics), and 10 questions regarding comprehension. Finally, we used 290 questions 300 \n(without image data) from the 116th NMLE and 262 questions (without image data) from the 117th 301 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 12 \nNMLE in Japan for analyses. The entire set of 395 text-based questions, irrespective of image data, 302 \nfrom the 117th NMLE in Japan was considered for the exploratory analysis. 303 \n 304 \nGenerative Pretrained Transformer 305 \nThe GPT, developed by OpenAI [14], is a type of AI model used for NLP tasks. Following the 306 \nresearch path from the original GPT, GPT -2, and GPT -3, OpenAI’s DL approach leverages 307 \nextensive amounts of data and intensive computation to create increasingly sophisticated and 308 \ncapable language models [18]. ChatGPT has been fine-tuned from the initial GPT-3.5, and later, 309 \nGPT-4—a LLM trained in early 2022 to produce text [13, 30]. GPT-4 is OpenAI’s latest and most 310 \nadvanced AI model that can solve difficult problems with greater accuracy [18]. In this study, we 311 \nused ChatGPT powered by both the GPT-3.5 and GPT-4 versions.  312 \n 313 \nPrompt engineering to maximize the correct answer rate 314 \nWe used the 116th NMLE in Japan to generate the most suitable prompts for ChatGPT to answer 315 \nthe 117th NMLE questions. Using the ChatGPT API, we first instructed ChatGPT to respond to the 316 \noriginal questions in Japanese language. We manually coded the Hyper Text Markup Language 317 \n(HTML) to represent the bold, italic, superscript, and subscript characters in the original text 318 \n(Figure 3A). Second, we instructed ChatGPT to translate the original Japanese NMLE questions 319 \ninto English using its own capabilities before inputting them as questions (Figure 3B). In addition, 320 \nwe compiled and analyzed the output errors. Thereafter, we provided prompts with restr iction 321 \nsentences designed to prevent the reoccurrence of these errors, along with sample outputs 322 \nillustrating the desired output format. Finally, we inquired ChatGPT to improve the prompt itself. 323 \nWe further refined the prompts using the 116th NMLE questions to achieve higher rates of correct 324 \nanswers and output in the desired format, because prompt tuning can improve the task accuracy 325 \ncompared to training the entire model [12, 20] . The final optimized two-step prompts for the 326 \nEnglish translation process and the process of answering the medical questions are illustrated in 327 \nFigure 3 C, wherein  each process comprised \" system,\" \"sample output ,\" and \"question input \" 328 \nsections. We organized the output examples according to each medical question category (basics 329 \nof medicine, clinical medicine, and comprehension). In brief, ChatGPT was initially instructed to 330 \ntranslate the HTML-based Japanese questions into plain, direct , and improved English , while  331 \nmaintaining the original HTML codes without deleting or adding new text. In both processes, the 332 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 13 \nsystem of requirement and an exemplary output scenario were provided within the prompts. In the 333 \nquestion input section, the HTML-based Japanese questions were inputted for the English 334 \ntranslation process, and the English-translated questions were consequently inputted to the process 335 \nof answering the medical questions (Figure 3). To minimize output variability, all input prompts 336 \nwere executed with the temperature parameter set to 0. 337 \n GPT-3.5-based analyses were performed using the ChatGPT API with custom Python code 338 \non the Googl e Colaboratory interface. GPT-4-based analysis was conducted using ChatGPT 339 \nwebsite console, with eight investigators (Y . T., T. N., K. A., T. E., R. M., S. K., H. K., and F. H.) 340 \nmanually inputting prompts one by one and changing a thread each time. Specifically, they inputted 341 \nthe questions, choices, and appropriate prompts into ChatGPT and summarized the output answers. 342 \nWe used the GPT-3.5 version GPT3.5-turbo-0301 for the \"Japanese,” “English,” and “English with 343 \ntuned prompt” analyses, and the GPT-4 model version released on March 14 th 2023 for the 344 \n“English with tuned prompt” analysis. 345 \n 346 \nOutcomes 347 \nThe target outcome of this study is the correct answer rate. We manually compared ChatGPT’s 348 \noutput a nswers with the official answers to determine the correctness of the output answers. 349 \nAccordingly, the correct answer rate was calculated as the number of correct answers divided by 350 \nthe number of questions. We defined the output errors as incorrect answers. To evaluate the 351 \npotential performance for passing the 117th NMLE in Japan, we applied the minimum passing rates, 352 \nnot the minimum passing scores, to evaluate the model performance because the image-containing 353 \nquestions were excluded from the analyses. 354 \n 355 \nPerformance evaluation 356 \nIn the primary performance evaluation, we assessed the correct answer rate for questions without 357 \nimages in the 117th NMLE in Japan using the best ChatGPT model (GPT -4) with tuned prompt, 358 \nwhich was comp ared to the actual minimally passing rate on the examination. In the secondary 359 \nperformance evaluation, we examined the correct answer rate for all questions in the 117th exam 360 \nusing the optimal  model and prompt s. In addition, the medical reasonableness of the generated 361 \nexplanations for each answer was assessed by two independent clinical physicians (M.N. and 362 \nM.K.) and was double-checked by another independent clinical physician (A.N.). Furthermore, we 363 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 14 \nanalyzed the content of the incorrect answers along with their explanations to identify the areas in 364 \nwhich the application of the current ChatGPT for medicine may be relatively weak. 365 \n 366 \n 367 \nAcknowledgments 368 \nWe express our gratitude to Yasuhiro Onogi and Yuichi Miyamae at MICIN, Inc. for their insightful 369 \nonline discussions regarding this project. We thank Dr. Hozumi for dedicating his time to discuss 370 \nthis topic with us. We also thank ChatGPT (GPT-4) and Enago English proofreading service for 371 \nEnglish proofreading. 372 \n 373 \n 374 \nData availability 375 \nThe ChatGPT APIs used in this study are accessible via GitHub 376 \n(https://github.com/yudaitanaka1026/ChatGPT_NMLE_Japan).  377 \n 378 \n 379 \nConflict of Interest 380 \nThe authors declare no conflicts of interest relevant to this article. 381 \n 382 \n 383 \nFinancial disclosure 384 \nNone. 385 \n 386 \n 387 \nAuthor contributions 388 \nConceptualization: Yudai Tanaka, Takuto Nakata, Ko Aiga, Hiroaki Kakizaki, and Akihiro 389 \nNomura. 390 \nData curation: Yudai Tanaka, Takuto Nakata, Ko Aiga, Takahide Etani, Ryota Muramatsu, Shun 391 \nKatagiri, Hiroyuki Kawai, Fumiya Higashino, and Masahiro Enomoto. 392 \nFormal analysis: Yudai Tanaka. 393 \nMethodology: Yudai Tanaka, Takuto Nakata, Hiroaki Kakizaki, and Akihiro Nomura. 394 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 15 \nProject administration: Akihiro Nomura. 395 \nSupervision: Masayuki Takamura, Takashi Yoneda, and Hiroaki Kakizaki. 396 \nValidation: Masao Noda, Mitsuhiro Kometani, and Akihiro Nomura. 397 \nVisualization: Yudai Tanaka, Takuto Nakata, Ko Aiga, and Akihiro Nomura. 398 \nWriting – original draft: Yudai Tanaka, Takuto Nakata, Ko Aiga, and Akihiro Nomura. 399 \nWriting – review and editing: Yudai Tanaka, Takuto Nakata, Ko Aiga, Takahide Etani, Ryota 400 \nMuramatsu, Shun Katagiri, Hiroyuki Kawai, Fumiya Higashino, Masahiro Enomoto, Masao 401 \nNoda, Masayuki Takamura, Mitsuhiro Kometani, Takashi Yoneda, Hiroaki Kakizaki, and Akihiro 402 \nNomura.  403 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 16 \nReferences 404 \n1. Haug CJ, Drazen JM. Artificial Intelligence and Machine Learning in Clinical Medicine, 405 \n2023. N Engl J Med. 2023;388(13):1201-8. doi: 10.1056/NEJMra2302038. PubMed PMID: 406 \n36988595. 407 \n2. Nomura A, Noguchi M, Kometani M, Furukawa K, Yoneda T. Artificial Intelligence in 408 \nCurrent Diabetes Management and Prediction. Curr Diab Rep. 2021;21(12):61. Epub 20211213. 409 \ndoi: 10.1007/s11892-021-01423-2. PubMed PMID: 34902070; PubMed Central PMCID: 410 \nPMCPMC8668843. 411 \n3. Aramaki E, Wakamiya S, Yada S, Nakamura Y . Natural Language Processing: from 412 \nBedside to Everywhere. Yearb Med Inform. 2022;31(1):243-53. Epub 20220602. doi: 10.1055/s-413 \n0042-1742510. PubMed PMID: 35654422; PubMed Central PMCID: PMCPMC9719781. 414 \n4. Khurana D, Koli A, Khatter K, Singh S. Natural language processing: state of the art, 415 \ncurrent trends and challenges. Multimed Tools Appl. 2023;82(3):3713-44. Epub 20220714. doi: 416 \n10.1007/s11042-022-13428-4. PubMed PMID: 35855771; PubMed Central PMCID: 417 \nPMCPMC9281254. 418 \n5. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all 419 \nyou need. Advances in neural information processing systems. 2017;30. 420 \n6. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional 421 \nTransformers for Language Understanding. Proceedings of the 2019 Conference of the North 422 \nAmerican Chapter of the Association for Computational Linguistics: Human Language 423 \nTechnologies. 2019;1:4171-86. doi: https://doi.org/10.18653/v1/N19-1423. 424 \n7. Thoppilan R, De Freitas D, Hall J, Shazeer N, Kulshreshtha A, Cheng H-T, et al. 425 \nLaMDA: Language Models for Dialog Applications2022 January 01, 2022:[arXiv:2201.08239 426 \np.]. Available from: https://ui.adsabs.harvard.edu/abs/2022arXiv220108239T. 427 \n8. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. PaLM: Scaling 428 \nLanguage Modeling with Pathways2022 April 01, 2022:[arXiv:2204.02311 p.]. Available from: 429 \nhttps://ui.adsabs.harvard.edu/abs/2022arXiv220402311C. 430 \n9. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, et al. LLaMA: 431 \nOpen and Efficient Foundation Language Models2023 February 01, 2023:[arXiv:2302.13971 p.]. 432 \nAvailable from: https://ui.adsabs.harvard.edu/abs/2023arXiv230213971T. 433 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 17 \n10. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language models 434 \nare few-shot learners. Advances in neural information processing systems. 2020;33:1877-901. 435 \n11. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al. Training language 436 \nmodels to follow instructions with human feedback. Advances in Neural Information Processing 437 \nSystems. 2022;35:27730-44. 438 \n12. OpenAI. GPT-4 Technical Report2023 March 01, 2023:[arXiv:2303.08774 p.]. Available 439 \nfrom: https://ui.adsabs.harvard.edu/abs/2023arXiv230308774O. 440 \n13. OpenAI. Introducing ChatGPT 2022 [cited 2023 Apr 8]. Available from: 441 \nhttps://openai.com/blog/chatgpt. 442 \n14. Lee P, Bubeck S, Petro J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for 443 \nMedicine. N Engl J Med. 2023;388(13):1233-9. doi: 10.1056/NEJMsr2214184. PubMed PMID: 444 \n36988602. 445 \n15. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepano C, et al. Performance 446 \nof ChatGPT on USMLE: Potential for AI-assisted medical education using large language 447 \nmodels. PLOS Digit Health. 2023;2(2):e0000198. Epub 20230209. doi: 448 \n10.1371/journal.pdig.0000198. PubMed PMID: 36812645; PubMed Central PMCID: 449 \nPMCPMC9931230. 450 \n16. Gilson A, Safranek CW, Huang T, Socrates V , Chi L, Taylor RA, et al. How Does 451 \nChatGPT Perform on the United States Medical Licensing Examination? The Implications of 452 \nLarge Language Models for Medical Education and Knowledge Assessment. JMIR Med Educ. 453 \n2023;9:e45312. Epub 20230208. doi: 10.2196/45312. PubMed PMID: 36753318; PubMed 454 \nCentral PMCID: PMCPMC9947764. 455 \n17. Huh S. Are ChatGPT's knowledge and interpretation ability comparable to those of 456 \nmedical students in Korea for taking a parasitology examination?: a descriptive study. J Educ 457 \nEval Health Prof. 2023;20:1. Epub 20230111. doi: 10.3352/jeehp.2023.20.1. PubMed PMID: 458 \n36627845; PubMed Central PMCID: PMCPMC9905868. 459 \n18. OpenAI. GPT-4 is OpenAI’s most advanced system, producing safer and more useful 460 \nreponses. 2023 [cited 2023 Apr 8]. Available from: https://openai.com/product/gpt-4. 461 \n19. Ministry of Health Labour and Welfare in Japan. Announcement of the results of the 462 \n117th National Medical Licensing Examination in Japan. 2023 [cited 2023 Apr 10]. Available 463 \nfrom: https://www.mhlw.go.jp/general/sikaku/successlist/2023/siken01/about.html. 464 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 18 \n20. Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt 465 \ntuning. arXiv preprint arXiv:210408691. 2021. 466 \n21. Web Technology Surveys. Usage statistics of content languages for websites. Available 467 \nfrom: https://w3techs.com/technologies/overview/content_language. 468 \n22. Kaneda Y , Tanimoto T, Ozaki A, Sato T, Takahashi K. Can ChatGPT Pass the 2023 469 \nJapanese National Medical Licensing Examination? Preprintsorg. 2023;2023030191. doi: 470 \nhttps://doi.org/10.20944/preprints202303.0191.v1. 471 \n23. Lee K, Firat O, Agarwal A, Fannjiang C, Sussillo D. Hallucinations in neural machine 472 \ntranslation. 2018. 473 \n24. Shakarian P, Koyyalamudi A, Ngu N, Mareedu L. An Independent Evaluation of 474 \nChatGPT on Mathematical Word Problems (MWP)2023 February 01, 2023:[arXiv:2302.13814 475 \np.]. Available from: https://ui.adsabs.harvard.edu/abs/2023arXiv230213814S. 476 \n25. OpenAI. Usage policies. 2023. Available from: https://openai.com/policies/usage-477 \npolicies. 478 \n26. Ordish J. Large Language Models and software as a medical device.: Medicines and 479 \nHealthcare products Regulatory Agency (MHRA); 2023. Available from: 480 \nhttps://medregs.blog.gov.uk/2023/03/03/large-language-models-and-software-as-a-medical-481 \ndevice/. 482 \n27. Ministry of Health Labour and Welfare in Japan. Software as a Medical Device 483 \n(Japanese). 2023. Available from: 484 \nhttps://www.mhlw.go.jp/stf/seisakunitsuite/bunya/0000179749_00004.html. 485 \n28. Moor M, Banerjee O, Abad ZSH, Krumholz HM, Leskovec J, Topol EJ, et al. Foundation 486 \nmodels for generalist medical artificial intelligence. Nature. 2023;616(7956):259-65. Epub 487 \n20230412. doi: 10.1038/s41586-023-05881-4. PubMed PMID: 37045921. 488 \n29. Ministry of Health Labour and Welfare in Japan. Questions and answers of the 116th 489 \nNational Medical Licensing Examination in Japan. 2022. Available from: 490 \nhttps://www.mhlw.go.jp/seisakunitsuite/bunya/kenkou_iryou/iryou/topics/tp220421-01.html. 491 \n30. OpenAI. What is ChatGPT? 2023. Available from: 492 \nhttps://help.openai.com/en/articles/6783457-what-is-chatgpt. 493 \n  494 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 19 \nFigure legends 495 \nFigure 1. Study overview. 496 \nQuestions from the 116th NMLE in Japan were used as the prompt-tuning dataset and those from 497 \n117th NMLE were utilized as the performance-testing dataset after removing the image-based 498 \nquestions. During the prompt tuning process, questions from the prompt-tuning dataset were 499 \ninput into GPT-3.5-turbo and GPT-4, using simple prompts in both Japanese and English along 500 \nwith tuned prompts in English. Subsequently, we evaluated the outputs from GPT-3.5-turbo and 501 \nGPT-4 with tuned prompts. After tuning the prompts, the ChatGPT (GPT-4) model optimized 502 \nwith the tuned prompts was tested on the performance-testing dataset (117th NMLE). 503 \n 504 \nFigure 2. Variations in the rate of correct answers across languages, prompt tuning levels, 505 \nand GPT models. 506 \nTranslating the Japanese questions into English text improved the correct answer rate; however, 507 \nit increased the output error rate. Upon further tuning the prompts, the correct answer rate 508 \nimproved and the output error decreased. Moreover, switching from the GPT-3.5 model to the 509 \nGPT-4 model enhanced the correct answer rate and almost eliminated errors. 510 \n 511 \nFigure 3. Examples of prompts for English translation and answering medical questions. 512 \nA: A simple “Japanese prompt” used for answering Japanese questions. 513 \nB: Simple “English prompts” used for Japanese-to-English translation and answering translated 514 \nquestions. 515 \nC: Our optimized “English with tuned prompts”. 516 \nThe final optimized two-step prompts comprised a \"system,\" \"sample output,\" and \"question 517 \ninput\" sections. ChatGPT was initially instructed to translate HTML-based Japanese questions 518 \ninto simple, direct, and improved English. In both processes, the system of requirement and an 519 \nexemplary output scenario were provided within the prompts. In the question input section, the 520 \nJapanese questions were inputted to the English translation process, and sequentially, the 521 \nEnglish-translated questions were used to obtain the answers of the 117th NMLE questions. 522 \n 523 \nFigure 4. Examples of potentially outdated or critically incorrect outputs from the model in 524 \ncurrent medical contexts. 525 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 20 \nA: A question on the primary treatment for hyperventilation syndrome in the emergency 526 \ndepartment. The suggestion of paper bag method for raising the carbon-dioxide concentration in 527 \nthe blood has been commonly used in the past, but it is not always the first choice, as it can 528 \nworsen symptoms in certain patients with secondary hyperventilation, e.g., those with lung 529 \ndiseases causing low blood oxygen levels. In such answers, it seems that outdated, traditional 530 \ninformation can prevail over the latest information, especially if it has been a standard practice 531 \nover a period and related information is widely available on the Internet. 532 \nB: A question on the initial outpatient treatment for a type-2 diabetes patient with poor control 533 \nand combined diabetic retinopathy and neuropathy. The long-term treatment goal for diabetes is 534 \nstrict blood sugar control, but in this case, strict blood sugar control with sulfonylurea drugs 535 \nduring the initial treatment may aggravate the risk of diabetic retinopathy, raising strong 536 \nconcerns on ChatGPT's answer. 537 \n538 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 21 \nTables \nTable 1. Performance of optimal GPT-4 model with tuned prompt for the 117th NMLE in Japan. \n \n  Essential Basics and Clinical \n  \nBasics of \nmedicine \n(essential) \nClinical \nmedicine \n(essential) \nComprehension \n(essential) \nBasics of \nmedicine \n(general) \nBasics of \nmedicine \n(specifics) \nClinical \nmedicine \n(general) \nClinical \nmedicine \n(specifics) \nComprehension \nNo. of questions without image data 45 22 15 61 27 36 46 10 \nNo. of correct answers 36 19 12 47 25 22 37 8 \nNo. of output errors 1 0 0 0 1 0 0 0 \nNo. of incorrect answers 8 3 3 14 1 14 9 2 \nCorrect answer rate 80.0% 86.4% 80.0% 77.0% 92.6% 61.1% 80.4% 80.0% \nOutput error rate 2.2% 0.0% 0.0% 0.0% 3.7% 0.0% 0.0% 0.0% \nScore weight x1 x3 x1 \nTotal score (correct answer rate) 129/156 (82.7%) 139/180 (77.2%) \nMinimum passing rate 80.0% 74.6% \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 22 \nTable 2. Summary of incorrect answers from the optimal model. \n \nTotal incorrect answer N = 56 \nInsufficient medical knowledge 33 (58.9%) \n   Breast surgery 1 \n   Dermatology 2 \n   Emergency medicine 2 \n   Endocrinology 6 \n   Gastroenterology 2 \n   Immunology 1 \n   Medical interview 1 \n   Medical procedure 1 \n   Nephrology 2 \n   Neurology 1 \n   Obstetrics and gynecology 2 \n   Ophthalmology 1 \n   Pediatrics 2 \n   Physical examination 1 \n   Psychiatry 1 \n   Public health 1 \n   Rehabilitation 1 \n   Respiratory medicine 3 \n   Rheumatology 1 \n   Urology 1 \nJapan-specific medical system 17 (30.4%) \n   Clinical research 1 \n   Emergency  1 \n   Psychiatry 1 \n   Public health 14 \nMathematical errors 4 (7.1%) \n   Respiratory 1 \n   Pediatrics 1 \n   Cardiology 1 \n   Medical interview 1 \nOthers 2 (3.6%) \n   Issue in English translation 1 \n   Not providing an answer 1 \n \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 23 \nFigures \nFigure 1. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 24 \nFigure 2. \n \n \n \n \n \n \n \n \n \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 25 \n“Input a HTML-coded original question in Japanese here”\nSimple prompts for answering medical questions\nQuestion input\nSystem\nSystemAnswer the following questions with reasons.                          \nWhich of the following physical examinations is difficult to perform \non a patient with impaired consciousness?                                                      \nTendon reflex | Nuchal rigidity | Pupillary light reflex | Finger-to-\nnose test | Knee extension test (lower limb dropping test) ）\nSimple prompts for answering medical questions\nQuestion input\nSimple prompts for English translation\nSystemTranslate the following Japanese into English.\n“Input a HTML-coded original question in Japanese here”\nQuestion input\nA\nB\nC\nSystem\nSample output\n\"# system: Describe AI assistant's settings and so on                 \nYou need an English translator, spell checker, and medical language \nexpert who can translate your Japanese text into English. You want \nthe translation to be improved and simplified to make it easier to \nunderstand for non-specialists at a high school level. Your request is \nto keep the meaning intact, but with a more literal translation. Your \ntask is to provide multiple options for the answer, and use only \nHTML codes to provide the response.\nRestrictions:\n- The response is only one translation, containing only corrections \nand improvements to the Japanese text, not notes or anything else.\n- Do not remove any HTML code or add any new code to the \nresponse.\n- Your responses should only include translated English sentences \nwith HTML codes.\n- You don't have to add the HTML code with options.\n- Set temperature = 0\n=Sample=\nJapanese text:\n“Input a HTML-coded sample question in Japanese here”\nSample Output:\n<p>What is commonly observed in patients with scleroderma \nkidney?</p> \nAortic aneurysm|sacral arthritis|salmon pink rash|nephrotic\nsyndrome|thrombotic microangiopathy [TMA].                               \n# user: User's utterance                                                   \nJapanese sentences:\n“Input a HTML-coded original question in Japanese here” Question Input\nTuned prompts for English translation\nSystem\nSample output\n\"# system: Describe AI assistant's settings and so on.                  \nAs a doctor, provide a diagnosis, treatment, and prevention for any \nillness or disease based on a thorough examination of the patient's \nage, symptoms, and clinical course. Use your expertise to answer \nclinical medicine or public health questions related to Japan, and \noutput your choice.\nRestrictions:\n- Summarize the contents of the question without repeating it.\n- Fill in the blanks in the output and provide an explanation for your \nanswer.\n- For multiple-choice questions, choose the most probable option(s).\n- For questions without a specific number of selections, choose only \none.\nOutput:\nIn summary, (summarise the question).\nThe answer requires (number) of choices.\nThe answer is (word) because (explain the rationale for your choice).\narrow examination, and head MRI are not indicated for this patient's \npresentation.\n=SAMPLE=\nIn summary, which two diseases exhibit low complementemia out of \nthe options provided, which are Cellulitis, Bacterial pneumonia, \nIschemic colitis, Acute glomerulonephritis, and Mixed \ncryoglobulinemia?\nThe answer requires selecting (two) choices.\nThe answer is (Acute glomerulonephritis) and (Mixed \ncryoglobulinemia) because these two diseases are known to be \nassociated with low complementemia. Acute glomerulonephritis is \nan immune-mediated kidney disease that can lead to low levels of \ncomplement proteins. Mixed cryoglobulinemia is a type of vasculitis \nthat is characterized by the presence of abnormal proteins in the \nblood, which can lead to low complement levels. Cellulitis, bacterial \npneumonia, and ischemic colitis are not typically associated with low \ncomplementemia.                                                                        \n# user: User's utterance                                                  \nQuestion:                                                                       \n<p>Which physical examination is difficult to perform on a patient \nwith impaired consciousness?</p>                                       \nTendon reflex|neck stiffness|light reflex|finger-to-nose test|knee\nraise test (lower limb dropping test)\"                                                                           \nQuestion input\nTuned prompts for answering medical questions\nFigure 3. \n \n \n \n \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint \n 26 \nFigure 4. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.17.23288603doi: medRxiv preprint ",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.5044416189193726
    },
    {
      "name": "Medical education",
      "score": 0.4762837290763855
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4663723111152649
    },
    {
      "name": "Transformer",
      "score": 0.46573296189308167
    },
    {
      "name": "Computer science",
      "score": 0.40197229385375977
    },
    {
      "name": "Operations research",
      "score": 0.3401336669921875
    },
    {
      "name": "Medicine",
      "score": 0.3026071786880493
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2996196746826172
    },
    {
      "name": "Engineering",
      "score": 0.20042160153388977
    },
    {
      "name": "Electrical engineering",
      "score": 0.0906125009059906
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}