{
  "title": "Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling",
  "url": "https://openalex.org/W4287888343",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2560654518",
      "name": "Jakob Prange",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096245607",
      "name": "Nathan Schneider",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154593323",
      "name": "Lingpeng Kong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1632114991",
    "https://openalex.org/W4287124808",
    "https://openalex.org/W3106290101",
    "https://openalex.org/W3042429451",
    "https://openalex.org/W4288364692",
    "https://openalex.org/W4230813817",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2103763702",
    "https://openalex.org/W3097562574",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W3117283657",
    "https://openalex.org/W3138301265",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W123237996",
    "https://openalex.org/W2986387660",
    "https://openalex.org/W4310854123",
    "https://openalex.org/W2986009340",
    "https://openalex.org/W4288410857",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2095589793",
    "https://openalex.org/W2888882903",
    "https://openalex.org/W4287280906",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W3127943074",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2760664922",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W2982442115",
    "https://openalex.org/W2250864392",
    "https://openalex.org/W2741481273",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W3034329603",
    "https://openalex.org/W2949674892",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2171421863",
    "https://openalex.org/W3174281149",
    "https://openalex.org/W2096871981",
    "https://openalex.org/W2564486991",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2970378492",
    "https://openalex.org/W2889647547",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W2397198482",
    "https://openalex.org/W3103028291",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2251433671",
    "https://openalex.org/W2966753611",
    "https://openalex.org/W3205374364",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2972425807",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W2151222319",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3035261420"
  ],
  "abstract": "We examine the extent to which, in principle, different syntactic and semantic graph representations can complement and improve neural language modeling. Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone? With an ensemble setup consisting of GPT-2 and ground-truth graphs from one of 7 different formalisms, we find that the graph information indeed improves perplexity and other metrics. Moreover, this architecture provides a new way to compare different frameworks of linguistic representation. In our oracle graph setup, training and evaluating on English WSJ, semantic constituency structures prove most useful to language modeling performance—outpacing syntactic constituency structures as well as syntactic and semantic dependency structures.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4375 - 4391\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLinguistic Frameworks Go Toe-to-Toe\nat Neuro-Symbolic Language Modeling\nJakob Prange Nathan Schneider\nGeorgetown University\n{jp1724, nathan.schneider}@georgetown.edu\nLingpeng Kong\nThe University of Hong Kong\nlpk@cs.hku.hk\nAbstract\nWe examine the extent to which, in principle,\ndifferent syntactic and semantic graph repre-\nsentations can complement and improve neural\nlanguage modeling. Speciﬁcally, by condition-\ning on a subgraph encapsulating the locally rel-\nevant sentence history, can a model make better\nnext-word predictions than a pretrained sequen-\ntial language model alone? With an ensem-\nble setup consisting of GPT-2 and ground-truth\ngraphs from one of 7 different formalisms, we\nﬁnd that the graph information indeed improves\nperplexity and other metrics. Moreover, this ar-\nchitecture provides a new way to compare dif-\nferent frameworks of linguistic representation.\nIn our oracle graph setup, training and evalu-\nating on English WSJ,semantic constituency\nstructures prove most useful to language mod-\neling performance—outpacing syntactic con-\nstituency structures as well as syntactic and\nsemantic dependency structures.\n1 Introduction\nLinguistic theories posit that humans can take ad-\nvantage of hierarchical structure related to some\nnotion of compositionality to produce and com-\nprehend utterances with complex meanings. Yet\nexplicit representations of this kind of structure are\nharder to come by than raw text, and large-scale\npretrained neural language models (e.g.,Devlin\net al., 2019; Radford et al., 2019) have managed\nto perform strikingly well at contextually encoding\nand predicting words from distributional evidence\nalone. At the same time, there are good reasons to\ndoubt that these models can be said tounderstand\nlanguage in any meaningful way (Trott et al., 2020;\nBender and Koller, 2020; Merrill et al., 2021). To\naddress this conundrum, people have started to ex-\nplore probing pretrained models (Liu et al., 2019;\nTenney et al., 2019a, inter alia) and supplement-\ning training data with linguistic structure guidance\n(Strubell et al., 2018; Swayamdipta et al., 2018;\nPeng et al., 2019; Wu et al., 2021, inter alia).\nA question that has received less attention is\nwhich kind of symbolic linguistic representation\n(SLR) is most conducive to guiding neural lan-\nguage models (LMs). Numerous domain-general\ncandidates exist (Abend and Rappoport, 2017;\nOepen et al., 2019, 2020; Žabokrtský et al., 2020;\nMüller , 2020): some are focused on syntactic struc-\nture, others on semantics (§2; big grey example\ngraphs in the left panels of ﬁgure1). Frameworks\nvary along several dimensions, with different label\ninventories and treatments of speciﬁc constructions.\nFormal differences include the type of structure (de-\npendency or constituency, one or multiple parents,\nprojectivity) and its relation to the input string. In\ngeneral, different design choices may aim to cap-\nture different kinds of generalizations or facilitate\ndifferent kinds of processing, and may make pars-\ning raw text easier or harder. It is often not obvious\nwhich framework should be chosen for best results\non an external task—or indeed, how to even per-\nform a controlled comparison across frameworks.\nIn this paper we investigate whether structurally\nguided language modeling can serve as a bench-\nmark task for directly comparing linguistic repre-\nsentations. Speciﬁcally, we evaluate on next-word\nprediction—a relatively neutral task in that it does\nnot rely on any artiﬁcial test suite, nor does it tar-\nget a speciﬁc downstream application where one\nlinguistic framework may have an advantage.1\nWe devise a method for selecting and encoding\npartial views of linguistic graphs over the preced-\ning context relevant to predicting the next token\n(§ 3 and § 4).2 We call these viewsslices (small\nper-token graphs and dashed lines in ﬁgure1).\nOur neuro-symbolic encoder statically allocates\ndistinct vector dimensions for different structural\n1Our ﬁndings are limited to a particularlanguage (English)\nand domain (ﬁnancial news) in which gold graphs from multi-\nple frameworks are available for the same sentences, but such\nannotations could be obtained for other samples in the future.2Our code is available to the research community athttps:\n//github.com/jakpra/LinguisticStructureLM.\n4375\nUD:Syntactic dependency tree with functional labelsPTG:Deep syntactic / quasi-semantic argument structure\n0\n1\n2\n3\namod\nnsubj:pass\naux:pass\n0\n1\n2\n3PRED\nEXT PAT\nGPT-2:Neural language model with multihead attention\nN umerousinjuries were ???? N umerousinjuries were ???? N umerousinjuries were ????0 0\n0\n1 2\n1 2\n3= 1 1\n1\n2\n02\n3\n02\n3= =\nPTB-fxn:Syntactic phrase/function-labeled constituency treeEDS:Semantic predication/modiﬁcation and variable binding\n\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\nlogits\u0000\u0000\u0000\nlogits\u0000\u0000\n+\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\nreported\n0\n1 2\n3\nNP-SUBJ VP\nVP\n0\n1\n2\n3BVARG1 ARG2\nN umerousinjuries were ???? N umerousinjuries were ????\n0\n1\n0\n1\n0\n1\n01\n2\n01\n2\n3= = 1 1\n0 1\n2\n0 1\n2\n0 1\n2\n3= =\nFigure 1:Contrasting GPT-2’s incremental attention mechanism (top right) with incremental contextslices obtained\nfrom linguistic graphs (left four panels) of four different formalisms (§5.2). As shared tokenization we use GPT-2’s\nbyte-pair encoding. Slice nodes are color-coded by local relation type (black: target, cyan: parent, blue: child,\ngreen: coparent, yellow: sibling, purple: grandparent, brown: aunt). Dashed lines indicate the token anchoring of\noriginal (big grey) graph nodes and, correspondingly, which previous tokens (empty circles) are accessible for each\nnext-token prediction. In the bottom right we visualize how different models arrive at their prediction (§3 and § 4.3).\nrelations within each slice, which in the incremen-\ntal setting is much faster and more ﬂexible than\ncomputation-intensive deep graph encoders. Using\nthis encoding, we compare 7 SLR formalisms by\nvirtue of their incremental language modeling ca-\npability in an controlled experimental setup (§5) on\njointly annotated ground-truth data (Oepen et al.,\n2019, 2020). The results (§6) suggest that linguistic\ngraphs are indeed informative for next-word predic-\ntion, complementing what is learned in pretraining.\nThis invites future research quantifying different\nformalisms’ design choices (§7).\n2 Background: Symbolic Linguistic\nRepresentation\nFollowing a long tradition in formal linguistics,\ngraph-structured representations of language quali-\ntatively describe grammatical and logical relations\namong words. The SLR paradigm has recently seen\na revival in the form of larger-scaletreebanking and\nsembanking for training neural parsers.\nFormally, an SLR instance is a directed acyclic\ngraph (DAG)\u0000 = \u0000\u0000, \u0000, \u0000\u0000, with vertices\u0000, la-\nbeled edges\u0000, and an anchoring function\u0000 \u0000 \u0000 \u0000\n\u0000 that maps each vertex to a (potentially empty)\nsubset of tokens in the sentence. We broadly distin-\nguish SLR frameworks along two dimensions:3\n3See Abend and Rappoport(2017); Koller et al.(2019);\nPrange et al.(2019b) for more detailed taxonomies.\nScope. A main goal ofsyntactic representations\nis to explain distributional patterns in word or-\nder; they tend to be rooted trees with often pro-\njective anchoring functions.Semantic formalisms\nare meaning-oriented, aiming to capture the higher-\nlevel logic expressed in a sentence; thus, they may\nhave more complex structures, including reentrant\nedges and discontiguous anchors.\nStructure. SLRs can further be subdivided into\ndependency and constituency structures. The for-\nmer are relatively shallow, while the latter contain\nabstract nodes with no or multiple word anchors.\n3 Overview: Language Modeling with\nLinguistic Graphs\nOur main goal is to quantify the predictive power\nof different SLRs by combining them with a pre-\ntrained language model and measuring how this\naffects next-token generation performance. A lan-\nguage model (LM) assigns probabilities to sen-\ntences and can be used to both process existing\nsentences and generate new ones. As is standard\npractice, we treat sentences as length-\u0000 sequences\nof word tokens,\u0000 = \u0000\u00000,\u0000 1, … ,\u0000 \u0000\u00001\u0000. Anincre-\nmental LM factorizes the joint probability of the\nsentence in terms of the probability of each word\n\u0000\u0000 conditioned on previous tokens\u0000<\u0000; eq. (1).\nHere we describe at a high level how we process\n(oracle) SLR graphs for use in this language mod-\neling scenario, i.e., to obtain context-conditional\n4376\nvocabulary distributions from them. In contrast to\nsequential LMs, contexts are now graph-structured,\nand which context tokens to select as well as in\nwhat way they are related to the target token is de-\ntermined by the underlying SLR graph\u0000; eq. (2).\n\u0000\u0000\u0000(\u0000)=\n\u0000\u00001\u0000\n\u0000=0\n\u0000\u0000\u0000(\u0000\u0000\u0000\u0000<\u0000) (1)\n\u0000\u0000\u0000\u0000(\u0000) \u0000=\u0000(\u0000\u0000\u0000) (2)\nThis general idea is closely related to syntactic\nlanguage modeling (Pauls and Klein, 2012; Gub-\nbins and Vlachos, 2013, inter alia). We extend this\nline of work to arbitrarily complex syntactic and\nsemantic DAG structures and, in doing so, take par-\nticular care to restrict conditioning contexts from\naccessing not only futurewords but also futuresub-\ngraphs, so effectively top-down and left-to-right.\nOur procedure is as follows:\nFirst, we select for each token position\u0000 to be\npredicted a subgraph\u0000\u0000, called the token’sslice.\nSlices are bothadmissible in the language model-\ning setting, i.e., they do not violate the left-to-right\nconditioning order, andrelevant to the token predic-\ntion according to some criteria—here we consider\ncriteria based on structural relationships generally,\nwithout relying on formalism-speciﬁc labels (§4.1).\nConsider the small colored subgraphs for each to-\nken in ﬁgure1: the EDS-slice for the target ‘re-\nported’, for example, starts at node 3, and extends\nto the ARG2-child 2, ARG1-coparent 1, and BV-\ncoparent 0, which are anchored, respectively, in the\nspans ‘injuries’, ‘Numerous’, and ‘Numerous in-\njuries’). Recall from §2 that context words\u0000<\u0000 are\ncontained in\u0000\u0000, to the extent that they are anchored\nin a node reachable from\u0000\u0000. Inspired by Markov\nassumptions of independence in generative model-\ning and Markov blankets in causal networks, SLR\ngraph slicing thus allows us to factorize\u0000(\u0000\u0000\u0000) as\n\u0000(\u0000\u0000\u0000) \u0000=\n\u0000\u00001\u0000\n\u0000=0\n\u0000(\u0000\u0000\u0000\u0000\u0000). (3)\nNext, we encode each graph slice as a ﬁxed-\nsized vector. Prior approaches to encoding linguis-\ntic graphs for neural modeling have involved seri-\nalization, e.g., as parser transition sequences (Qian\net al., 2021, inter alia), recursive auto-encoders\n(Tai et al., 2015; Roth and Lapata, 2016), and graph-\nconvolutional networks (GCNs;Yang and Deng,\n2020; Wu et al., 2021). However, transition se-\nquences for non-tree graphs are subject to spurious\nambiguity; and we ﬁnd that graph-structured neural\nnetworks are impractical in the incremental setting\n(§ 6.5). Instead, we propose a computationally inex-\npensive method for statically and deterministically\nprojecting slices into a high-dimensional space by\nvector concatenation (§4.2).\nFinally, we compute output distributions\n\u0000(\u0000\u0000\u0000\u0000\u0000) from the vector representations (§4.3).\n4 Modeling Details\n4.1 Slicing Graphs\nA slice\u0000\u0000 is a connected subgraph of\u0000 that cap-\ntures \u0000\u0000’s linguistically structured context, masking\n\u0000\u0000 itself (or else estimating\u0000(\u0000\u0000\u0000\u0000\u0000) would be triv-\nial). \u0000\u0000 always minimally consists of\u0000\u0000’sdirect\nanchor node \u0000\u0000 = Select({\u0000 \u0000 \u0000\u0000 \u0000 \u0000(\u0000)}). Start-\ning from\u0000\u0000, we traverse the graph and add vertices\nand edges that are connected to\u0000\u0000 via paths of a\nfew speciﬁc relative types, REL. Here we settle on\n6 types: parents, siblings, grandparents, parents’\nsiblings, children, and coparents. The vertices\u0000\u0000\nand edges\u0000\u0000 for slice\u0000\u0000 = \u0000\u0000\u0000,\u0000 \u0000,\u0000 \u0000 consist then\nof the union of these sets.4\nTo prevent information leakage from future to-\nkens, we discard from\u0000\u0000 all nodes{\u0000 \u0000 \u0000(\u0000)=\n\u0000\u0000,\u0000 > \u0000} which areonly anchored in tokensfol-\nlowing \u0000\u0000. E.g., in ﬁgure1, the UD-slice for the\ntoken ‘were’ does not contain the parent node 3\nbecause that is anchored only in the following to-\nken ‘reported’ (and thus the sibling 1 cannot be\naccessed either). If a node’s anchorscontain or\noverlap with \u0000\u0000 (i.e., the node is a non-terminal\nabove \u0000\u0000), we retain the node and its edges but\nremove its token anchors.\n4.2 Vectorizing Graph Slices\nBecause slices can be large, we partition each\nslice’s nodes by structuralrelative type, in order to\naggregate them into a ﬁxed-length summary vec-\ntor. Speciﬁcally, we allocate capacities for each\nrelative type:\u0000rel =2 for parents, siblings, aunts,\nand children, and 1 for grandparents and coparents.\nUp to\u0000 \u00001 , relative nodes\u0000rel are added ‘with\nhigh resolution’, maintaining their identity and or-\nder; beyond the capacity, relatives are aggregated\n‘with low resolution’; eq. (4). Within each rela-\ntive type, precedence\u0000 is given to relatives whose\ntoken anchors are sequentially closer to\u0000\u0000.\n4See appendicesA.1 and A.2 for details.\n4377\nHiRes\u0000,rel = \u0000\u0000rel,\u0000 \u0000 \u0000<\u0000 rel\n\u0000\nLoRes\u0000,rel ={ \u0000rel,\u0000 \u0000 \u0000 \u0000 \u0000rel} (4)\nNext we look up the relatives’ edge label and\nword vector encodings5 \u0000\u0000\u0000 and \u0000\u0000\u0000 and collate them\ninto a single vector\u0000\u0000\u0000,rel per relative type. High-\nresolution vectors are concatenated\u0000 and low-\nresolution vectors are averaged; eq. (5). Finally,\nwe concatenate all of these (zero-padded) relative-\nvectors to obtain the ﬁnal vector representation of\nthe whole slice,\u0000\u0000\u0000; eq. (6). At a high level, this vec-\ntor essentially speciﬁes a deterministic, structured,\ntyped, discrete self-attention over the token history.\n\u0000\u0000HiRes\n\u0000,rel =\n\u0000\n\u0000\u0000HiRes\u0000,rel\n\u0000\u0000\u0000\u0000; \u0000\u0000\u0000\n\u0000\n\u0000\u0000LoRes\n\u0000,rel =\n\u0000\n\u0000\u0000LoRes\u0000,rel\n\u0000\u0000\u0000\u0000; \u0000\u0000\u0000\n\u0000\n\u0000LoRes\u0000,rel\u0000+\n(5)\n\u0000\u0000\u0000 =\n\u0000\nrel\u0000REL\n\u0000\n\u0000\u0000HiRes\n\u0000,rel ; \u0000\u0000LoRes\n\u0000,rel\n\u0000\n(6)\n4.3 Predicting Emission Distributions\nWe compute model posteriors for next-token pre-\ndictions as\n\u0000\u0000(\u0000\u0000 = \u0000\u0000\u0000 context\u0000,\u0000) = Sof tMax(logits\u0000,\u0000)[\u0000],\nwhere \u0000 is either a pure SLR model or LM, or an\nensemble of the two (bottom right of ﬁgure1).\nSLR only. As described above, we deﬁne\ncontext\u0000,\u0000\u0000\u0000 as \u0000\u0000, which is encoded as\u0000\u0000\u0000.W e\nobtain \u0000\u0000\u0000\u0000 by letting the slice-vectors serve as\ninputs to a\u0000-multilayer perceptron (MLP) with\na ﬁnal softmax layer over the vocabulary, which\nyields the estimated token emission distributions.\nlogits\u0000,\u0000\u0000\u0000 = MLP\u0000( \u0000\u0000\u0000)\nMLP\u0000(\u0000)= \u0000(\u0000) \u0000… \u0000(1)(\u0000)\u0000 Emb\u0000,\nwhere Emb is an embedding matrix.\nLM + SLR. Since we want to measure whether\nand how much the information contained in the\nSLR can contribute to state-of-the-art language\nmodels, our primary experimental condition is a\ncombined setup \u0000Ensemble, where logits obtained\n5See appendixA.3 for details.\nSentences Tokens Vocabulary\nTrain 26,325 658,475 27,344\nTrain (EarlyStop) 23,692 591,829 26,422\nDev (EarlyStop) 2,633 66,646 10,073\nEval 921 22,596 5,364\nTable 1:Data statistics.\nfrom slice-encodings are added to a base neural\nLM’s logits before taking the softmax:\nlogits\u0000,Ensemble = logits\u0000,\u0000\u0000\u0000 + logits\u0000,\u0000\u0000 ,\nwith logits\u0000,\u0000\u0000 = LM(\u0000<\u0000).\nLM only. \u0000\u0000\u0000, i.e., the bare LM without any\nexposure to SLR graphs, serves as a baseline.\n5 Experimental Setup\nAll models are implemented in PyTorch and exper-\niments are run on 1 NVIDIA Tesla T4 GPU. Model\nhyperparameters are reported in appendixA.5.\n5.1 Data\nOur dataset consists of the intersection of Wall\nStreet Journal (WSJ; English ﬁnancial news) sen-\ntences that have been annotated with syntactic trees\nin the Penn Treebank (PTB;Marcus et al., 1993;\nHovy et al., 2006)6 as well as a range of seman-\ntic representation formalisms for the MRP 2019\n& 2020 shared tasks (Oepen et al., 2019, 2020).\nSummary statistics are shown in table1. Our pre-\nprocessing steps are described in appendixB.\n5.2 SLR Formalisms\nThe 7 (versions of) linguistic representation frame-\nworks examined in this study are listed in table2,\nalong with their classiﬁcations along the scope and\nstructure dimensions. We draw the structural de-\npendencies vs. constituencies distinction (described\nat a high level in §2) based on speciﬁc properties\nof the MRP shared task data: a framework is con-\nsidered a dependency framework if all edges are\nonly between pairs of individual word anchors at a\ntime; if there are any unanchored7 nodes or nodes\nanchored in more than one linguistic word token,\nit is considered a constituency framework.8 Below\nwe give a brief description of each framework.\nPTB trees specify hierarchically nested syntactic\nconstituents. We consider two labeling variants: ba-\nsic phrase structure (-phr) and phrase types reﬁned\nwith functional speciﬁcations (-fxn).\n6https://catalog.ldc.upenn.edu/LDC2013T19\n7Not including “ROOT” nodes in UD.8See appendixA.4 for details.\n4378\nUniversal Dependencies (UD; Nivre et al., 2016,\n2020; de Marneffe et al. , 2021) is a syntac-\ntic dependency representation with coarse, cross-\nlinguistically applicable edge labels.\nDELPH-IN MRS Bi-Lexical Dependencies\n(DM; Ivanova et al., 2012) and Elementary De-\npendency Structures (EDS; Oepen and Lønning,\n2006) are derived from underspeciﬁed logical\nforms computed by the English Resource Grammar\n(Flickinger, 2000; Copestake et al., 2005).\nPrague Semantic Dependencies (PSD; Hajiˇc\net al., 2012) and Prague Tectogrammatical Graphs\n(PTG) are syntactico-semantic predicate–argument\nstructures converted from the Prague Functional\nGenerative Description (Sgall et al., 1986; Böh-\nmová et al., 2003; Hajiˇc et al., 2012).\n5.3 Language Model\nThe base language model we use in all our experi-\nments is GPT-2 (Radford et al., 2019, as distributed\nin the huggingface-transformers PyTorch library).\nGPT-2 is a Transformer model (Vaswani et al.,\n2017) pretrained on a diverse collection of web\ntexts. In contrast to other widely-used Transform-\ners like BERT (Devlin et al., 2019), which optimize\nbidirectional masked language modeling, GPT-2\nis incremental, i.e., next-word decisions only take\ninto account thepreceding context.\n5.4 Training\nWe train all models for 10 epochs with the AdamW\noptimizer (Loshchilov and Hutter, 2019), minimiz-\ning cross-entropy between the model posterior and\nthe ground truth at each token position.\nWe perform early stopping with the last 10% of\nthe original training corpus set aside for develop-\nment scoring after each epoch.9 We keep the model\nstate that achieves the best perplexity on the dev set.\nPeak development performance is reached after\u00003\nepochs for SLR models, whereas ﬁnetuning GPT-2\nby itself takes between 7 and 9 epochs.\n5.5 Evaluation\nWe compute model perplexity (PPL) as the most\nstandard language modeling evaluation measure,\nas well as accuracy (Acc) and conﬁdence (Conf) of\na model’s top-ranked guess, mean reciprocal rank\nof the correct answer (MRR), and entropy of the\n9The R-GCN baseline (table6) is always trained for the\nfull 10 epochs, but to ensure fairness, it is also only compared\nto concatenation-based encoders that have been trained for the\nfull 10 epochs, too.\nmodel’s token prediction posterior (H). All metrics\nare reported as microaverages over the evaluation\ndata at the BPE token level.10\n6 Findings\n6.1 Main Results\nThe most striking observation in terms of overall\nmodel performance (table2) is that ground-truth\nlinguistic graphs of all investigated linguistic for-\nmalisms improve vanilla GPT-2 by a large margin,\nin all metrics. This improvement holds up when\ncompared to a version of GPT-2 that is exposed\nto the raw WSJ text without the graphs; with this\ncondition we control for mere domain differences\nbetween our evaluation data and the data GPT-2\nwas trained on originally (‘+Domain’ in table2).\nThe large performance gap suggests that at least\na subset of the oracle knowledge about linguistic\nstructure isnot yet encodedin the base language\nmodel, which learns from only raw text.\nWe observed that if we keep training for the\nentirety of 10 epochs, rather than early stopping\nbased on development performance, we somewhat\noverﬁt to the training set. While accuracy itself\nis not affected very much by this, the models be-\ncome increasingly overconﬁdent (overall conﬁdence\noverall accuracy\n,\nwhich gets up to 8–12%, compared to\u00004% with\nthe vanilla GPT-2 model and in most cases even\nslightly less than that with the early-stopped SLR\nmodels). This leads to overall worse perplexity.\n6.2 Differences between Formalisms\nComparing across rows in table2, we ﬁnd a con-\nsiderable performance spread. The general trend,\nwhich is relatively consistent in all metrics,11 is\nindicated by the order of rows, with UD having\nthe smallest (though still respectable) improvement\nover the baseline, and PTG and EDS the largest.\nInterestingly, there are two marked separations:\na primary one between dependency and con-\nstituency formalisms, and a secondary one between\nsyntactic (i.e., more surface-oriented) and semantic\n(more abstract) formalisms. This is summarized\n10We compute average PPL over\nall sentences \u0000 by exponentiating last:\nexp \u0000 1\n\u0000\n\u0000 \u0000w\u0000\u0000\n\u0000\n\u0000\n\u0000\u0000w\u0000\u0000\u00001\n\u0000=0 \u0000 log\u0000\u0000\n\u0000\n\u0000\u0000\u0000 = \u0000\u0000\n\u0000\u0000 \u0000context\u0000\u0000,\u0000\n\u0000\n\u0000\n11The multitude of metrics might thus seem redundant. But\nsince each measurement emphasizes different properties of\nmodel performance, we consider it a a very interesting result\n(and, potentially, a success of our modeling technique and\nexperimental setup) to achieve this broad consistency.\n4379\nTraining Efﬁciency Language Model Quality\nModel Scope/Struct #Labels Speed ↑ Size ↓ PPL ↓ H [nats] ↓ Acc [%] ↑ Conf [%] ↑ MRR ↑\nGPT-2 – 124.4M 59.3 4.09 30.0 31.2 .403\n+ Domain 15 45.9 ±.09 3.64 ±.008 33.3 ±.02 34.9 ±.07 .435 ± .3e-3\n*** ***\n+ UD syn dep 39 14 +54.1M 32.7 ±.18 3.30 ±.013 39.1 ±.15 40.1 ±.14 .486 ±1.2e-3\n*** \u0000\n+ DM sem dep 59 15 +54.4M 31.4 ±.08 3.24 ±.026 38.9 ±.10 40.2 ±.37 .491 ± .6e-3\n*** \u0000\n+ PSD sem dep 90 16 +54.9M 30.7 ±.09 3.21 ±.014 39.1 ±.11 40.9 ±.11 .491 ± .5e-3\n** ***\n+ PTB-phr syn const 38 14 +54.1M 29.8 ±.18 3.14 ±.029 41.2 ±.19 42.8 ±.42 .507 ±1.3e-3\n*** *\n+ PTB-fxn syn const 537 14 +62.7M 29.0 ±.28 3.07 ±.049 42.0 ±.30 43.8 ±.60 .514 ±1.8e-3\n*** *\n+ PTG sem const 72 15 +54.6M 26.8 ±.26 3.03 ±.041 43.1 ±.12 44.6 ±.51 .522 ± .9e-3\n*** \u0000\n+ EDS sem const 10 15 +53.6M 24.7 ±.28 2.92 ±.048 43.1 ±.17 45.0 ±.55 .527 ±1.3e-3\nTable 2:Main results: performance of language models combined with 7 SLR formalisms of different scope,\nstructure, and label set (each corresponding to a\u0000Ensemble in § 4.3), compared to vanilla GPT-2 and a version\nof GPT-2 that has been domain-ﬁnetuned on the raw text of the SLR training corpus (\u0000\u0000\u0000). We report each\nquality metric as mean± stdev over 3 random seeds. We also report model size in #parameters (all non-baseline\nmodels as absolute difference to baseline) and training speed in sentences per second as measures of efﬁciency.\nStatistical signiﬁcance of the PPL and Acc differences to the next-best model (always adjacent rows) is reported\nas ***\u0000<. 0001 / **\u0000<. 001 /* \u0000<. 005 / \u0000not signiﬁcant (approximate randomization test as described in\nRiezler and Maxwell(2005), with\u0000 =10,000 shufﬂes). We only consider a difference signiﬁcant if\u0000<\u0000 for all\nthree random model initialization seeds. Best results in each column arebolded. For conﬁdence, ‘best’ means\nbest-calibrated, i.e., the smallest relative difference to accuracy.\nDep Const Avg\nSyn 32.7 (1) *** 29.4 ±0.6 (2) 30.5 ±2.0 (3)\n*** *** \u0000\nSem 31.0 ±0.5 (2) *** 25.7 ±1.5 (2) 28.4 ±3.2 (4)\nAvg 31.6 ±1.0 (3) ** 27.6 ±2.3 (4) 29.3 ±2.8 (7)\nTable 3:Model perplexity (lower is better) summarized\nin terms of two SLR dimensions: Scope (syntax vs. se-\nmantics) and structure (dependency vs. constituency).\u0000\n± \u0000 (\u0000) over frameworks per condition. Statistical signif-\nicance of the difference between the two closest SLRs\nof each pair of conditions is reported as ***\u0000<. 0001 /\n**\u0000<. 001 /* \u0000<. 005 / \u0000not signiﬁcant (approximate\nrandomization test with\u0000 =10,000 shufﬂes).\nin table3. A limiting factor for dependency rep-\nresentations in the incremental LM setting is that\nrelations between the target token and subsequent\ntokens are entirely ignored, whereas constituency\ngraphs can back off to higher-level structures. Fur-\nther, the syntactic graphs we use are always trees,\nso they never populate the coparent capacity in the\nslices. Semantic constituencyrepresentations, with\ntheir abstract and meaning-oriented labeling and\nstructure schemes, jump out as being especially\npredictive of the underlying text, as compared to\nboth syntax and shallow semantics.\nWe note that the function-enhanced PTB label\nset has a slight advantage over the basic phrase-\nstructure labels; and that, among the two closely\nrelated pairs of formalisms (DM/EDS and PSD/\nPTG, which each are dependency and constituency\nversions converted from the same underlying gram-\nmars), the constituency versions always work better\nthan the dependency versions in our setting. There\nis, however, no consistent ranking between DM/\nEDS on one hand and PSD/PTG on the other. In\nterms of perplexity, EDS works better than PTG,\nand PSD better than DM, but these differences are\nnot signiﬁcant for accuracy.\n6.3 Differences between Word Classes\nTo better understand where particular strengths\nand weaknesses of the baseline LM and linguis-\ntically enhanced models lie, we analyze subsets of\ntokens by part-of-speech (POS) tag (table4, see\nappendix C for more details). Across all models\nthere is a clear and expected separation between\nrather predictable function words, more perplex-\ning content words, and numbers, punctuation, and\nmiscellaneous tokens somewhere in the middle.\nAverage perplexity of the tested SLR models is\nbetter than baseline GPT-2 in all POS classes but\none. The one exception is the noun class, where\n4380\nboth the SLR macro-average and UD in particular\ndo not raise performance. Only EDS and DM show\nperplexity improvements on nouns; PTB even has a\nnoticeable negative impact. We conjecture that this\nmay have to do with relatively deep NP nesting in\nPTB (compared to the other formalisms), such that\nthe current slicing hyperparameters (relative types\nand capacities) are too strict and hide informative\nsignals like modiﬁers and verb attachment.\nSome formalisms seem to be particularly well-\nsuited for the prediction of certain POS: UD for\nverbs; PTB and PTG for adpositions and subordi-\nnating conjunctions; EDS for pronouns, determin-\ners, and numbers; PTG, PSD, and EDS for coor-\ndinating conjunctions. The advantage of EDS and\nDM on nouns, pronouns, determiners, and numbers\ncan likely be attributed to their explicit representa-\ntion of variable binding/quantiﬁcation. Similarly,\nPTG and PSD have detailed categories for coordi-\nnation, distinguishing, e.g., con- and disjunction.\nFor nouns and modiﬁers, the spread across for-\nmalisms is particularly wide, which suggests that\nSLRs diverge quite a bit on these types of words\n(e.g., whether adjectives and certain nouns can\ncount as predicates) and that this diversity has a\nstrong effect on utility for language modeling.\n6.4 Model Ablations\nThe linguistically enriched models consist of a\nsubstantial number of newly learned parameters—\naround 50–60M each, an additional\u000050% the size\nof vanilla GPT-2. Although model size does not\nseem to be correlated with performance among the\nSLR-enriched models, it could still be that the ad-\nditional capacity allows the models to store more\ninformation about the words’ distributions than the\nbaseline GPT-2 model, without ever truly using the\nconcrete linguistic structures.\nWe check this by randomly shufﬂing (\u0000) two\ncore graph properties: (i) the assignment of edgela-\nbels, and (ii) theanchoring mapping between graph\nnodes and word tokens in each graph. If the mod-\nels are largely independent of the correct label and\nstructure assignments, these changes should have\na very small effect on performance (Dubossarsky\net al., 2018; Hewitt and Liang, 2019).\nBut on the contrary, we ﬁnd that performance\nworsens considerably in the ablated settings com-\npared to the full combined models of each formal-\nism (table5, see appendixC for more details). This\n12https://universaldependencies.org/u/pos/\nEval\nToks\nTrain\nVocab\nPerplexity↓\nPOS GPT-2 UD EDS SLR Avg\nAll 22,596 27,344 45.9 32.7 24.7 29.3 ±2 .8\ncontent\nnoun 7,731 18,435 142.5 122.0 98.0 122.6 ±13.9\nverb 2,639 7,100 128.8 80.4 85.9 84.9 ±4 .5\nmod 2,235 6,292 228.7 158.8 98.6 124.4 ±22.6\nfunction\naux 582 95 17.6 11.1 5.9 9.1 ±2 .1\nadp 1,957 232 10.1 7.3 5.5 5.3 ±1 .6\npart 645 27 3.7 2.0 1.6 1.9 ±0 .3\nsconj 268 96 15.4 12.3 6.8 6.8 ±3 .9\ncconj 548 35 13.0 7.4 1.9 4.1 ±2 .1\ndet 1,726 91 9.4 7.8 4.4 6.0 ±1 .3\npron 868 149 22.9 17.5 5.4 11.0 ±4 .0\nnum 719 1,059 72.6 57.1 47.5 54.1 ±4 .6\npunct 2,527 68 4.9 2.3 2.7 2.6 ±0 .3\nmisc 151 183 7.0 4.6 4.0 4.5 ±0 .8\nTable 4:Breakdown by Universal POS,12 in terms of\nPPL of domain-trained GPT-2, two exemplary SLR-\ncombined models, and the macro-average± stdev over\nall SLR-combined models. Best results (within the vari-\nance) in each row arebolded. We show token counts\nand observed vocabulary size for reference.\nAblation Applied in DM PTB SLR Avg\nFull 31.4 29.0 29.3 ±2 .8\n\u0000 Labels testing +4.7 +73.9 +28.3 ±28.3\n\u0000 Anchors testing +34.8 +223.1 +106.0 ±73.1\n\u0000 Both testing +33.4 +207.4 +95.9 ±68.9\n\u0000 Labels training +1.4 +9.0 +4.2 ±3 .3\n\u0000 Anchors training +8.5 +17.5 +13.3 ±4 .6\n\u0000 Both training +7.8 +18.3 +13.4 ±5 .0\n\u0000 Labels both +1.3 +9.3 +4.3 ±3 .4\n\u0000 Anchors both +7.9 +17.5 +13.5 ±5 .0\n\u0000 Both both +7.3 +18.1 +13.6 ±5 .2\n\u0000 SLR both +14.5 +16.9 +16.6 ±2 .8\nTable 5:Ablations measured in\u0000PPL for two exem-\nplary SLR-combined models and the macro-average±\nstdev over all SLR-combined models.Full and \u0000SLR\ncorrespond, respectively, to table2’s rows 4 (DM) / 7\n(PTB-fxn) and row 2 (GPT-2+Domain).\nconﬁrms that the models really do acquire—and\nare quite sensitive to—the graph-encoded linguistic\nsignals, relying to a large part on this new informa-\ntion in making their predictions.\nShufﬂing only edge labels while leaving the\nrest of the graphs unchanged has a smaller ef-\nfect than changing how tokens are anchored in\nthe graph structure. This suggests that the linguis-\ntic graphs’ entire structural arrangement of labels\nand attention-like selection of context words play\na crucial role—more so than knowing the type of\neach individual (correctly attached) grammatical\nrelations. Note that the\u0000 Anchors setting, too,\nchanges which edge labels are used in the predic-\n4381\nTraining Efﬁciency LM Quality\nModel \u0000Speed ↑ \u0000Size ↓ \u0000PPL ↓ \u0000Acc ↑\nUD \u000050% \u00001.9M +2.9 ±.08 \u00000.4 ±.02\nDM \u000047% +2.5M +1.6 ±.35 +0.1 ±.14\nPSD \u000056% +9.1M +3.6 ±.23 \u00000.9 ±.15\nPTB-phr \u000043% \u00001.9M +6.8 ±.39 \u00001.7 ±.11\nPTB-fxn \u000086% +107.7M +10.5 ±.21 \u00002.7 ±.15\nPTG \u000053% +5.9M +6.2 ±.22 \u00003.5 ±.03\nEDS \u000047% \u00008.5M \u00000.2 ±.07 +0.1 ±.03\nTable 6:Performance differences between R-GCN slice\nencoder baseline and our concatenation-based encoder\n(table 2). Relative differences (\u0000) for speed in sentences\nper second; absolute differences (\u0000) otherwise. Means\n± stdev over 2 runs without early stopping.\ntion of a given token, resulting in a smaller differ-\nence between\u0000 Anchors and\u0000 Both.\nIf a model has learned to rely on correct labels\nand structure during training, then perturbing these\nproperties at test time has a highly adverse effect,\nconfusing the model and leading to a drastic de-\ncrease in performance—even worse than not con-\nsulting SLR graphs at all! Given previous ﬁndings\nthat syntactic structure is to some extent already\nlearned in pretraining (Linzen et al., 2016; Tenney\net al., 2019b), we conjecture that this representa-\ntional capacity gets ofﬂoaded to the graphs at train-\ning time, and thus test-time permutations fool the\nPTB model to a much greater extent than DM.\nAs expected, exposing models to shufﬂed graphs\nat training time renders the additional model pa-\nrameters practically neutral, resulting in similar\nperplexity as the base LM. In this case, it also does\nnot matter whether test-time graphs are correct or\nrandom (training vs. both in column 2)—either way,\nthe model learns to mostly disregard the random\nstructure as noise.\n6.5 Comparison with R-GCN Encoding\nAs an additional strong baseline, we compare our\nconcatenation-based slice vector encoding to a\ngraph neural network from the literature. We\nchoose relational graph-convolutional networks\n(R-GCN; Schlichtkrull et al., 2018; Kipf and\nWelling, 2017) as a suitable representative of this\ntype of model, which has been used successfully\nby Wu et al.(2021) to encode DM graphs.\nResults are shown in table6. Contrasting with\ntable 2, there is a big difference in training speed:\nour simple encoder is on average roughly twice as\nfast as the computation-heavy alternative, whose\ntime and space complexity is dominated by the\nnumber of labels.13\nWe observe at best similar LM quality as with\nour concatenation method (EDS and DM), but for\nmost formalisms performance degrades. We follow\nSchlichtkrull et al.and Wu et al.in using 2 R-GCN\nlayers with basis matrix regularization. Possible\ndisadvantages of this for encoding linguistic graphs\nare the ﬁxed path length (2 layers exclude parent’s\nsiblings; but 3 layers would include a lot of irrel-\nevant information) and that many of the trained\nparameters are shared between different relations.\nIn contrast, our concatenation encoding forces the\nMLP input layer to learn distinct parameters for\neach structural relative type and edge label.\n7 Discussion\n7.1 Related Work\nResearchers have long been interested in scaffold-\ning sequential language models with linguistic-\nstructure-based inductive biases. Syntactic lan-\nguage modelingdates back to the pre-neural era,\nwhen Pauls and Klein(2012) andGubbins and Vla-\nchos (2013) generalized Markov assumptions from\nword n-grams to syntactic subtrees. These ideas\nhave since been adapted to recurrent neural network\n(RNN) LMs (Mirowski and Vlachos, 2015) and ex-\npanded on (Dyer et al., 2016; Choe and Charniak,\n2016; Shen et al., 2018, 2019). Ek et al.(2019)\ncondition RNN-LMs on predicted syntactic and\nsemantic (unstructured) tags, interestingly ﬁnding\nless or sometimes no beneﬁt, especially on the se-\nmantic side. They hypothesize this might be due to\ntagging errors—an issue our oracle setup avoids.\nIn the era of attention-based neural modeling of\nlanguage dominated by pretrained Transformers,\nmodels are often ﬁnetuned for and evaluated on\nspeciﬁc NLP tasks—like semantic role labeling,\nmachine translation, natural language inference,\ngraph-to-text generation, or the GLUE benchmark\n(Wang et al., 2019)—rather than language mod-\neling in its own right, which makes it difﬁcult to\ncompare them directly to our ﬁndings. There have\nbeen two main directions: One group of approaches\ncontinues the old syntactic language modeling tra-\ndition by incrementally generating words and SLRs\nwith either joint (Peng et al., 2019; Qian et al.,\n2021; Sartran et al., 2022) or iteratively-coupled\nLM and parser models (Choshen and Abend, 2021).\nThe second group assumes parsed input sentences,\n13And this is a very optimistic estimate of R-GCN training\nspeed in practice; see appendixA.6.\n4382\nwhich are then used to guide the model, e.g. by di-\nrectly optimizing Transformers’ attention weights\nto reﬂect linguistic graph structures (Strubell et al.,\n2018; Bai et al., 2021; Slobodkin et al., 2021).\nRather than controlling the existing sequential at-\ntention, Hajdik et al. (2019) process serialized\ngraphs directly with a sequence-to-sequence model,\nand Wu et al.(2021) extend a pretrained Trans-\nformer with an additional graph encoder. Notably,\nWu et al.(2021) andSlobodkin et al.(2021) exper-\niment with a few different semantic and syntactic\nSLRs, while all other studies we have looked at are\nlimited to either syntax or very shallow semantics.\nAnother relevant line of work employsprobing\ntasks in investigating to what extent grammar and\nmeaning are already encoded in neural language\nmodels trained predominantly on raw text with lit-\ntle to no linguistic supervision (Linzen et al., 2016;\nTenney et al., 2019a,b; Hewitt and Manning, 2019;\nLiu et al., 2019; Kim et al., 2019; Wu et al., 2020;\nGeiger et al., 2021, inter alia). Among the probing\nliterature, the works ofKuznetsov and Gurevych\n(2020) andKulmizev et al.(2020) are noteworthy in\nthat they investigate subtle differences between dif-\nferent (versions of) frameworks roughly covering\nthe same representational scope, namely, semantic\nroles and syntactic dependencies, respectively.\nOrthogonal approaches tocomparing SLR de-\nsigns have involved measuring how well different\nframeworks complement each other for joint pars-\ning or can be merged or converted into one another\n(Prange et al., 2019a; Hershcovich et al., 2020).\n7.2 Limitations and Future Work\nWhile the use of oracle graphs has both theoreti-\ncal advantages (measuring an upper bound without\nneeding to account for potential errors or uncer-\ntainties) and practical ones (saving the computa-\ntional overhead from training and running a parser),\nground-truth SLR graphs are a very limited re-\nsource and generally assumed to only be available\nat training time. There is no guarantee our results\ntranslate to the non-oracle setting. For instance, it\ncould be that the most helpful abstract semantic\ninformation is also the hardest to predict. And de-\nspite segmenting the existing sentence-level graph\ninto token-level slices, the human annotator who\ncreated the graph in the ﬁrst place has seen and an-\nalyzed the whole sentence, thus already resolving\ncrucial ambiguities and simplifying the task based\non knowledge ‘from the future’. In subsequent\nwork, we plan toparse graph slices incrementally,\nwhich will both relax theconditional modeling as-\nsumption into a more broadly interpretablejoint\nmodel and enable test-time use of the full system\non datasets without linguistic annotations.\nWe also only test formalisms that are explicitly\nanchored in linguistic units, roughly corresponding\nto LM (sub-)word tokens. This prevents us from\napplying the same paradigm to some other widely-\nused unanchored formalisms like AMR (Banarescu\net al., 2013) without some changes to the setup.\n7.3 Broader Impact\nOur experiments yield evidence which—at least\nin the case of encoding contexts for next-word\nprediction—supports the thesis of Bender and\nKoller (2020), Trott et al.(2020), and others that\nlinguistic meaning goes beyondform. Computa-\ntional models of language that exclusively learn\nfrom even very large amounts of raw text are thus\ngenerally expected to hit a ceiling14 which can only\nbe overcome with access to higher-level structures\nand mechanisms of understanding.\nIt further seems to matter in which manner and\nshape linguistic graph structure is drawn. Assum-\ning a perfect incremental parser, deeper structure\nand semantic categorization seems to be particu-\nlarly beneﬁcial for integration with a standard lan-\nguage model. This is in line with previous ﬁndings\nby, e.g.,Tenney et al.(2019b) that while pretrained\nLMs tend to encode shallow syntactic structure,\nabstract relations are more difﬁcult to probe for.\nWe thus see a promising research direction in\nmoving towards linguistic scaffolding of language\nmodels with representations that aremore complex\nthan tags or dependencies and that capturemean-\ningful relationsbeyond surface structure.\n8 Conclusion\nWe have presented evidence that symbolic linguis-\ntic representations of various frameworks have the\npotential to aid a pretrained incremental Trans-\nformer in task-neutral next-word prediction. To\nthis end, we have proposed a framework-agnostic\nneural encoding scheme for linguistic graphs and\napplied it to an English dataset jointly annotated\nwith 7 different formalisms. The results highlight\nthe importance of appreciating complex linguistic\nstructure and handling its computational represen-\ntation with nuance.\n14See alsoMerrill et al.(2021) for formal proofs.\n4383\nAcknowledgements\nWe would like to thank Katrin Erk and Chris Dyer;\nmembers of the Georgetown NERT/GUCL and\nHKU NLP labs; the organizers, reviewers, and au-\ndience of MASC-SLL 2022; as well as the anony-\nmous ARR reviewers for their extremely insightful\nfeedback and suggestions.\nReferences\nOmri Abend and Ari Rappoport. 2017.The state of\nthe art in semantic representation. InProc. of ACL,\npages 77–89, Vancouver, Canada.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang,\nJing Bai, Jing Yu, and Yunhai Tong. 2021.Syntax-\nBERT: Improving pre-trained transformers with syn-\ntax trees. In Proc. of EACL, pages 3011–3020, On-\nline. Association for Computational Linguistics.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013.Abstract Meaning Representation\nfor sembanking. InProc. of LAW-ID, pages 178–186,\nSoﬁa, Bulgaria.\nEmily M. Bender and Alexander Koller. 2020.Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. InProceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nAlena Böhmová, Jan Haji ˇc, Eva Hajiˇcová, and Barbora\nHladká. 2003. The Prague Dependency Treebank:\nA three-level annotation scenario. In Anne Abeillé,\neditor, Treebanks: Building and Using Parsed Cor-\npora, Text, Speech and Language Technology, pages\n103–127. Springer Netherlands, Dordrecht.\nDo Kook Choe and Eugene Charniak. 2016.Parsing\nas language modeling. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2331–2336, Austin, Texas.\nAssociation for Computational Linguistics.\nLeshem Choshen and Omri Abend. 2021.Transition\nbased graph decoder for neural machine translation.\nArXiv:2101.12640.\nAnn Copestake, Dan Flickinger, Carl Pollard, and\nIvan A Sag. 2005. Minimal recursion semantics:\nAn introduction.Research on language and compu-\ntation, 3(2):281–332.\nMarie-Catherine de Marneffe, Christopher D. Man-\nning, Joakim Nivre, and Daniel Zeman. 2021.Uni-\nversal Dependencies. Computational Linguistics,\n47(2):255–308.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. InProc. of NAACL-HLT, pages 4171–4186.\nHaim Dubossarsky, Eitan Grossman, and Daphna Wein-\nshall. 2018. Coming to your senses: on controls\nand evaluation sets in polysemy research. In Proc.\nof EMNLP, pages 1732–1740, Brussels, Belgium.\nAssociation for Computational Linguistics.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and\nNoah A. Smith. 2016.Recurrent Neural Network\nGrammars. InProc. of NAACL-HLT, pages 199–209,\nSan Diego, CA, USA.\nAdam Ek, Jean-Philippe Bernardy, and Shalom Lappin.\n2019. Language modeling with syntactic and seman-\ntic representation for sentence acceptability predic-\ntions. InProceedings of the 22nd Nordic Conference\non Computational Linguistics, pages 76–85, Turku,\nFinland. Linköping University Electronic Press.\nDan Flickinger. 2000. On building a more effcient gram-\nmar by exploiting types.Natural Language Engineer-\ning, 6(1):15–28.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christo-\npher Potts. 2021.Causal abstractions of neural net-\nworks. InProc. of NeurIPS.\nJoseph Gubbins and Andreas Vlachos. 2013.Depen-\ndency language models for sentence completion. In\nProc. of EMNLP, pages 1405–1410, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nValerie Hajdik, Jan Buys, Michael Wayne Goodman,\nand Emily M. Bender. 2019.Neural text genera-\ntion from rich semantic representations. InProc. of\nNAACL-HLT, pages 2259–2266, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJan Hajiˇc, Eva Hajiˇcová, Jarmila Panevová, Petr Sgall,\nOndˇrej Bojar, Silvie Cinková, Eva Fuˇcíková, Marie\nMikulová, Petr Pajas, Jan Popelka, Jiˇrí Semecký,\nJana Šindlerová, Jan Štˇepánek, Josef Toman, Zdeˇnka\nUrešová, and Zdenˇek Žabokrtský. 2012.Announcing\nPrague Czech-English dependency treebank 2.0. In\nProc. of LREC, pages 3153–3160, Istanbul, Turkey.\nEuropean Language Resources Association (ELRA).\nDaniel Hershcovich, Nathan Schneider, Dotan Dvir,\nJakob Prange, Miryam de Lhoneux, and Omri\nAbend. 2020.Comparison by conversion: Reverse-\nengineering UCCA from syntax and lexical se-\nmantics. In Proc. of COLING, pages 2947–2966,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019.Designing and\ninterpreting probes with control tasks. In Proc. of\nEMNLP-IJCNLP, pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\n4384\nJohn Hewitt and Christopher D. Manning. 2019.A\nstructural probe for ﬁnding syntax in word represen-\ntations. InProc. of NAACL-HLT, pages 4129–4138,\nMinneapolis, MN, USA.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006.OntoNotes:\nthe 90% solution. In Proc. of HLT-NAACL, pages\n57–60, New York City, USA.\nAngelina Ivanova, Stephan Oepen, Lilja Øvrelid, and\nDan Flickinger. 2012.Who did what to whom? a\ncontrastive study of syntacto-semantic dependencies.\nIn Proc. of LAW, pages 2–11, Jeju, Republic of Korea.\nAssociation for Computational Linguistics.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019.Probing what different\nNLP tasks teach machines about function word com-\nprehension. InProc. of *SEM, pages 235–249, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nThomas N. Kipf and Max Welling. 2017. Semi-\nSupervised Classiﬁcation with Graph Convolutional\nNetworks. InProc. of ICLR.\nAlexander Koller, Stephan Oepen, and Weiwei Sun.\n2019. Graph-based meaning representations: Design\nand processing. InProc. of ACL: Tutorial Abstracts,\npages 6–11, Florence, Italy. Association for Compu-\ntational Linguistics.\nArtur Kulmizev, Vinit Ravishankar, Mostafa Abdou,\nand Joakim Nivre. 2020.Do neural language models\nshow preferences for syntactic formalisms?In Proc.\nof ACL, pages 4077–4091, Online. Association for\nComputational Linguistics.\nIlia Kuznetsov and Iryna Gurevych. 2020.A matter of\nframing: The impact of linguistic formalism on prob-\ning results. InProceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 171–182, Online. Association\nfor Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the Ability of LSTMs to Learn\nSyntax-Sensitive Dependencies. TACL, 4:521–535.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019.Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proc. of NAACL-HLT, pages\n1073–1094, Minneapolis, Minnesota.\nIlya Loshchilov and Frank Hutter. 2019.Decoupled\nweight decay regularization. InProc. of ICLR, New\nOrleans, LA, USA.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993.Building a large annotated cor-\npus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313–330.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A. Smith. 2021.Provable Limitations of Ac-\nquiring Meaning from Ungrounded Form: What\nWill Future Language Models Understand?TACL,\n9:1047–1060.\nPiotr Mirowski and Andreas Vlachos. 2015.Depen-\ndency recurrent neural language models for sentence\ncompletion. In Proc. of ACL-IJCNLP, pages 511–\n517, Beijing, China. Association for Computational\nLinguistics.\nStefan Müller. 2020. Grammatical theory: From\ntransformational grammar to constraint-based ap-\nproaches. Language Science Press.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajiˇc, Christopher D. Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman.\n2016. Universal Dependencies v1: a multilingual\ntreebank collection. InProc. of LREC, pages 1659–\n1666, Portorož, Slovenia.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Hajiˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProc. of LREC, pages 4034–4043, Marseille, France.\nEuropean Language Resources Association.\nStephan Oepen, Omri Abend, Lasha Abzianidze, Jo-\nhan Bos, Jan Hajic, Daniel Hershcovich, Bin Li,\nTim O’Gorman, Nianwen Xue, and Daniel Zeman.\n2020. MRP 2020: The second shared task on cross-\nframework and cross-lingual meaning representation\nparsing. In Proc. of MRP at CoNLL, pages 1–22,\nOnline. Association for Computational Linguistics.\nStephan Oepen, Omri Abend, Jan Hajic, Daniel Her-\nshcovich, Marco Kuhlmann, Tim O’Gorman, Nian-\nwen Xue, Jayeol Chun, Milan Straka, and Zdenka\nUresova. 2019.MRP 2019: Cross-framework mean-\ning representation parsing. In Proc. of MRP at\nCoNLL, pages 1–27, Hong Kong. Association for\nComputational Linguistics.\nStephan Oepen and Jan Tore Lønning. 2006.\nDiscriminant-based MRS banking . In Proc.\nof LREC , Genoa, Italy. European Language\nResources Association (ELRA).\nAdam Pauls and Dan Klein. 2012.Large-scale syntactic\nlanguage modeling with treelets. In Proceedings\nof the 50th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 959–968, Jeju Island, Korea. Association for\nComputational Linguistics.\nHao Peng, Roy Schwartz, and Noah A. Smith. 2019.\nPaLM: A hybrid parser and language model. InProc.\nof EMNLP-IJCNLP, pages 3644–3651, Hong Kong,\nChina. Association for Computational Linguistics.\n4385\nJakob Prange, Nathan Schneider, and Omri Abend.\n2019a. Made for each other: Broad-coverage se-\nmantic structures meet preposition supersenses. In\nProc. of CoNLL, pages 174–185, Hong Kong, China.\nAssociation for Computational Linguistics.\nJakob Prange, Nathan Schneider, and Omri Abend.\n2019b. Semantically constrained multilayer anno-\ntation: the case of coreference. In Proc. of DMR,\npages 164–176, Florence, Italy.\nPeng Qian, Tahira Naseem, Roger Levy, and Ramón Fer-\nnandez Astudillo. 2021.Structural guidance for trans-\nformer language models. InProc. of ACL-IJCNLP,\npages 3735–3745, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019.Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nStefan Riezler and John T. Maxwell. 2005.On some\npitfalls in automatic evaluation and signiﬁcance test-\ning for MT. In Proceedings of the ACL Workshop\non Intrinsic and Extrinsic Evaluation Measures for\nMachine Translation and/or Summarization, pages\n57–64, Ann Arbor, Michigan. Association for Com-\nputational Linguistics.\nMichael Roth and Mirella Lapata. 2016.Neural seman-\ntic role labeling with dependency path embeddings.\nIn Proc. of ACL, pages 1192–1202, Berlin, Germany.\nAssociation for Computational Linguistics.\nLaurent Sartran, Samuel Barrett, Adhiguna Kuncoro,\nMiloš Stanojevi´c, Phil Blunsom, and Chris Dyer.\n2022. Transformer Grammars: Augmenting Trans-\nformer language models with syntactic inductive bi-\nases at scale. ArXiv: 2203.00633.\nMichael Schlichtkrull, Thomas N. Kipf, Peter Bloem,\nRianne van den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In The Semantic Web, pages 593–\n607, Cham. Springer International Publishing.\nPetr Sgall, Eva Hajiˇcová, and Jarmila Panevová. 1986.\nThe meaning of the sentence and its semantic and\npragmatic aspects. academia.\nYikang Shen, Zhouhan Lin, Chin-wei Huang, and\nAaron Courville. 2018.Neural language modeling\nby jointly learning syntax and lexicon. In Proc. of\nICLR.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2019.Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\nProc. of ICLR.\nAviv Slobodkin, Leshem Choshen, and Omri Abend.\n2021. Semantics-aware attention improves neural\nmachine translation. ArXiv:2110.06920.\nEmma Strubell, Patrick Verga, Daniel Andor, David\nWeiss, and Andrew McCallum. 2018.Linguistically-\ninformed self-attention for semantic role labeling. In\nProc. of EMNLP, pages 5027–5038, Brussels, Bel-\ngium. Association for Computational Linguistics.\nSwabha Swayamdipta, Sam Thomson, Kenton Lee,\nLuke Zettlemoyer, Chris Dyer, and Noah A. Smith.\n2018. Syntactic scaffolds for semantic structures. In\nProc. of EMNLP, pages 3772–3782, Brussels, Bel-\ngium.\nKai Sheng Tai, Richard Socher, and Christopher D. Man-\nning. 2015.Improved semantic representations from\ntree-structured Long Short-Term Memory networks.\nIn Proc. of ACL-IJCNLP, pages 1556–1566, Beijing,\nChina.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProc. of ACL, pages 4593–4601, Florence, Italy. As-\nsociation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Sam Bowman, Dipanjan Das, and\nEllie Pavlick. 2019b.What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. InProc. of ICLR.\nSean Trott, Tiago Timponi Torrent, Nancy Chang, and\nNathan Schneider. 2020.(Re)construing Meaning\nin NLP. InProc. of ACL, pages 5170–5184, Online.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017.Attention is all\nyou need. In Proc. of NeurIPS, pages 5998–6008,\nLong Beach, CA, USA.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. InProc. of\nICLR.\nZhaofeng Wu, Hao Peng, and Noah A. Smith. 2021.\nInfusing Finetuning with Semantic Dependencies.\nTACL, 9:226–242.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. InProc. of ACL, pages\n4166–4176, Online. Association for Computational\nLinguistics.\nKaiyu Yang and Jia Deng. 2020.Strongly incremental\nconstituency parsing with graph neural networks. In\nProc. of NeurIPS, volume 33, pages 21687–21698.\nCurran Associates, Inc.\nZdenˇek Žabokrtský, Daniel Zeman, and Magda\nŠevˇcíková. 2020. Sentence meaning representations\nacross languages: What can we learn from existing\nframeworks? Computational Linguistics, 46(3):605–\n665.\n4386\nA Additional Modeling Details\nA.1 Selecting Anchor Nodes\nIn case there are multiple anchoring options (see,\ne.g., EDS nodes 0 vs. 1 for ﬁrst token in ﬁgure1),\nwe use the following tie-breaker heuristics: Select\nthe anchor node with the most parents and children;\nif still a tie, select the anchor with the highest node\nID (tends to be hierarchically lower, i.e., vertically\ncloser to the token anchor).\nA.2 Relative Types\n\u0000\u0000\u0000 = \u0000\u0000, \u0000, \u0000, \u0000, \u0000, \u0000\u0000, namely, parents\u0000\u0000, sib-\nlings \u0000\u0000,\u0000, grandparents\u0000\u0000,\u0000, aunts\u0000\u0000,\u0000 (all indexed\nby parent\u0000), children \u0000\u0000, and coparents\u0000\u0000,\u0000 (in-\ndexed by child\u0000). This is the anchor node’s Markov\nblanket, plus siblings, grandparents, and aunts. We\nchose this set of relations based on general no-\ntions of linguistic hierarchy (predicate-argument,\nhead-dependent) and preliminary experiments, but\nwithout tuning for speciﬁc formalisms. Precise def-\ninitions are given in table7. Relative nodes are\npermanently associated with the label of the edge\nthat got them selected.\nA.3 Representing Tokens and Labels\nWe use GPT-2’s pretrained global embeddings\n(from the lowest layer, before any local contex-\ntualization) to obtain embeddings for relative token\nanchors in the slice-vector. When a token anchor\nin a linguistic graph consists of multiple BBPE to-\nkens, we average their embeddings. We reuse the\ntranspose of the same embedding matrix again to\nproject the last hidden state of the token-emission\nMLP into the vocabulary.\nSLR edge labels are encoded as one-hot vectors\nin the slice vectors, which lowers the potential for\nunnecessary random initialization variance of from-\nscratch embeddings.\nA.4 Distinguishing Dependencies from\nConstituencies\nWhile this distinction—as deﬁned in §5.2 in terms\nof the anchoring mapping between graph nodes\nand word tokens—can be subtle for individual\nsentences, it nonetheless affects slice encoding.\nIn PSD, for example, auxiliaries are unanchored,\nwhereas in PTG they are grouped with their main\npredicate (ﬁgure2).\nrel Name Deﬁnition \u0000\n\u0000\u0000 parent {\u0000 \u0000( \u0000, \u0000\u0000)\u0000 \u0000} 2\n\u0000\u0000,\u0000 sibling {\u0000 \u0000( \u0000, \u0000)\u0000 \u0000}\u0000 \u0000 \u0000 \u0000\u0000 2\n\u0000\u0000,\u0000 grandparent {\u0000 \u0000( \u0000, \u0000)\u0000 \u0000}\u0000 \u0000 \u0000 \u0000\u0000 1\n\u0000\u0000,\u0000 aunt {\u0000 \u0000( \u0000, \u0000)\u0000 \u0000 \u0000 \u0000 \u0000 \u0000\u0000,\u0000} 2\n\u0000\u0000 \u0000 \u0000\u0000\n\u0000\u0000 child {\u0000 \u0000( \u0000\u0000,\u0000 )\u0000 \u0000} 2\n\u0000\u0000,\u0000 coparent {\u0000 \u0000( \u0000, \u0000)\u0000 \u0000}\u0000 \u0000 \u0000 \u0000\u0000 1\nTable 7:Relative types and capacities.\nA.5 Model Hyperparameters\nWe report our model and training hyperparameters\nin table8. We didnot perform explicit hyperparam-\neter tuning, besides some manual testing early in\ndevelopment on a subset of the MRP shared task\ndata. Those data are annotated with SLR frame-\nworks other than the ones we compare here, and we\nended up excluding them from our experiments for\nlack of overlap with most of the other frameworks’\nannotations.\nA.6 Efﬁcient Batching for R-GCN\nIn our incremental setting we need to apply the\nR-GCN to each token-level slice, which would lead\nto multiple days15 of training for each model if\ndone naively. We achieve a considerable speedup\nby exploiting the oracle graphs at training and eval-\nuation time to pre-compute slices and running the\nR-GCN only once per sentence batch.\nB Data Preprocessing\nB.1 Sentence Filtering\nTo establish a common ground for comparison, we\ntake the intersection of sentences occurring in the\nannotated datasets ofall linguistic formalisms.\nIn a ﬁrst step, we discard two sentences whose\nlinguistic graph in at least one formalism is\nempty.18 We then select only those 35,513 train-\ndev / 1,401 eval sentences that appear in both the\nMRP 2019 and 2020 datasets (the 2019 corpus con-\ntains 143/1,958 more in train-dev/eval).19 Next,\n15Projected timeline based on a few iterations, which is\nconﬁrmed byYang and Deng(2020).16For label set\u0000. The factor 16 arises from the capacities\nchosen (table7), and the extra embedding allocation is for\naveraged preceding unanalyzable/within-anchor tokens.17For bidirectional label set\u0000\u0000, which is twice as big as\u0000.18The sentence “It is.” in DM and a ‘sentence’ consisting\nof the @-symbol in PTG.19‘train-dev’ refers to the data split that was used as training\ndata in both the MRP and 2019 tasks, and which we split 90%/\n10% into our training and development data. ‘eval’ refers to\nthe data that was used as evaluation data in MRP 2019 and as\ndevelopment data in MRP 2020, and which we evaluate our\n4387\nPTG(constituencies)0\n1\n2\n3PRED\nEXT PAT\nN umerousinjuries were reported\n1 1\n1\n2\n02\n3\n02\n3= =\nPSD(dependencies)\n1\n2\n3\nEXT PAT-arg\nN umerousinjuries were reported\n1 1\n1\n2\n1\n2\n2\n3= =\nFigure 2:Example of subtle differences in constituency\n(PTG) and dependency (PSD) versions of the same un-\nderlying formalism, the Prague Functional Description.\nPTG has an abstract PRED node as well as a multiword\nanchor where PSD does not, which results in diverging\nslice representations for the last two tokens.\nwe take the intersection of these sentences and\nOntoNotes 5.0, which contains the gold PTB syn-\ntax annotations. 26,719/929 sentences remain in\nthe train-dev/eval set. The MRP graph format oper-\nates on raw-text character offsets, while PTB and\nUD trees operate on word tokens. We are able re-\nconstruct offset-based text anchors for PTB and\nUD from the raw text strings used in the MRP data\nfor all but 394 train-dev / 8 eval sentences, which\nleaves us with the ﬁnal 26,325 train-dev and 921\neval sentences.\nIn a few cases, where the linguistic graph has\nno edges, we add an artiﬁcial edge with a dummy\nlabel.\nB.2 Tokenization\nWe follow the sentence segmentation of the Penn\nTreebank corpus. Within sentences, we obtain\ntoken boundaries from GPT-2’s pretrained byte-\nlevel byte-pair encoding (BBPE) tokenizer. The\nBBPE tokens are then aligned with the formalism-\ndependent SLR node anchors via raw-text charac-\nter offsets. Tokens that are continuations of mul-\ntiword anchors in the graph (‘reported’ in PTG,\nﬁgure 1); subword tokens of a single graph an-\nchor (‘N-umerous’); or are unanchored in the graph\n(‘ were’ in EDS), are treated asunanalyzable, i.e.,\ntheir slice consists of a copy of the preceding to-\nmodels on.\nGPT-2\nEmbedding dim 768\nV ocabulary 50,257\nActivation GELU\nDropout 0.1\nLearning rate 1e-6\nMLP\nInput dim 16 \u0000\u0000\u0000\u0000 + 17 \u0000 76816\nLayers 2\nHidden dims 1,024; 768\nActivation ReLU\nDropout 0.2\nLearning rate 1e-4\nR-GCN\nInput dim 768\nLayers 2\nHidden dims 768; 768\nActivation ReLU\nBasis matrices \u00000.1\u0000 \u0000\u0000\u0000\u0000\u000017\nLearning rate 1e-4\nOther training settings\nEpochs 10\nBatch size 8\nTable 8:Model and training hyperparameters\nken’s slice, plus the preceding within-anchor to-\nkens.\nB.3 UD Conversion\nQuasi-gold UD 2.0 trees are obtained from\nthe UD converter released with the Java Stan-\nford Parser v4.2.0 (https://nlp.stanford.edu/\nsoftware/lex-parser.html) on the PTB trees.\nB.4 PTB Labels\nBy convention, phrasal and functional labels in\nPTB are node labels. To match the labeled-edges-\nunlabeled-nodes format of the other formalisms,\nwe losslessly convert them to edge labels (namely,\non each node’s single incoming edge), discarding\nthe preterminal nodes’ POS labels. In preliminary\nexperiments we saw that including the POS tags is\nmuch more beneﬁcial than phrase structure only;\nbut since we do not include word-level tags in any\nof the other conditions, this would be an unfair\ncomparison. We focus here on sentence-level struc-\nture and leave studies of word-level tags to future\nwork.\nB.5 Data Splits\nWe split the corpus into training/development and\nevaluation data following the MRP task setup.\nSpeciﬁcally, we evaluate on the data split that was\n4388\nused as evaluation data in MRP 2019 and as de-\nvelopment data in 2020, as only for this data gold\nannotations in all formalisms have been released.\nWe do not perform empirical hyperparameter tun-\ning. In early development, a small subset of the\ndata was used.\nC Detailed Results\nWe report detailed results without early stopping\n(table 9), breakdowns by POS-class (table 10\nand appendix C.1), as well as ablation experi-\nments (table 11) for all SLR formalisms. In ta-\nbles 4 and 10 and ﬁgure3 we merge the POS tags\n{NOUN, PROPN} into ‘noun’, {ADJ, ADV} into\n‘mod’, and {INTJ, SYM, X} into ‘misc’.\nC.1 Lexico-Semantic or Syntactic\nKnowledge?\nIn § 6.3 we have found part-of-speech-speciﬁc pat-\nterns of model performance. But whenever, for a\ncertain syntactic word class\u0000, a formalism\u0000 is\nmore conducive to next-word prediction than a for-\nmalism \u0000, it is not clear whether this is the case\nbecause the choices get narrowed down to\u0000 itself\nor whether it is caused by either complementary\nor completely independent signals, perhaps at the\nlexical or semantic-structure levels.\nWe investigate this by rerunning the experiment\nwith each token’s UPOS tag as an additional input.\nIf this is more or less the same information as is\ngained—to different extents—from the SLRs, then\nthe results should be similar to before, and SLR-\nconditional differences should disappear.\nA few particularly interesting POS subsets are\nshown in ﬁgure3. We discuss them in order.\nAmong content words, nouns and verbs are sim-\nilar both in terms of baseline performance and in\nhow much easier it becomes to select the correct\nlexical item if the part-of-speech is known. At the\nsame time, the individual SLR formalisms differ\nquite a lot in how much information they contribute\nabout the POS class itself and about lexical choice\nwithin the part-of-speech. The respective best for-\nmalisms (EDS for nouns, PTB and UD for verbs)\napproximate oracle POS knowledge by themselves\nand still contribute substantial complementary in-\nformation when the actual POS tag is revealed. In\ncontrast, PTB does not seem to provide any useful\nsignal about nouns to the incremental LM—neither\nindependently nor in conjunction with the POS.\nModiﬁers (adjectives and adverbs) display a\nrather interesting behavior: the factthat a word\nof this type is coming next is very hard to predict\nfrom just the preceding raw context, which makes\nsense since they tend to addoptional meaning on\ntop of the (obligatory) logical and grammatical\ncontent. However, once the decision to modify has\nbeen made, the contextual choice becomes much\neasier than that for nouns or verbs. In both cases,\nall SLRs are quite helpful, with UD on the lower\nend and EDS leading the ﬁeld.\nWe ﬁnd similar tendencies among auxiliaries\n(\u0000function verbs) and pronouns (\u0000function nouns)\nas with (content) verbs and (content) nouns, but\nnaturally at a much smaller scale. Despite their\nfunctional-grammatical distribution and behavior,\nthe semantic frameworks EDS and PTG consis-\ntently outperform the syntactic ones UD and PTB\neven on these ‘small’ words. A possible explana-\ntion for this interaction with auxiliaries in particular\ncould be that EDS and PTG do not analyze them\nseparately at all, but rather group them, respec-\ntively, with the preceding context20 or their main\npredicate. The models might be able to leverage\nthis to focus on things like subject-verb agreement,\nlocal cohesion, or anticipating the main predicate.\nMore explicit syntactic analyses of auxiliaries (in-\ncrementally inaccessible forward-pointing depen-\ndencies in UD; VP-nesting in PTB), in contrast,\nmay restrict the model from directly making these\nconnections. Adding POS information in the input\ndecreases SLR-dependent differences.\nFor ‘subordinators’ in the broad sense, i.e., sub-\nordinating conjunctions at the clausal level and\nadpositions for nominal complements, PTB and\nPTG are particularly well-suited. By themselves\nthey are alreadyat leastas informative as POS, and\nthey still add a small but noticeable complementary\nsignal when the POS is revealed.\nDeterminers and coordinating conjunctions,\nwhich both already show extremely low perplexity\nwith some SLR models (namely, EDS, PSD, and\nPTG), entirely lose any reliance on particular SLRs\nwhen their POS is known.\n20EDS, like PSD, actually has no anchors for auxiliaries;\nwe attach them to the preceding semantic unit by default.\n4389\nTraining Efﬁciency Language Model Quality\nModel Scope/Struct #Labels Speed ↑ Size ↓ PPL ↓ H [nats] ↓ Acc [%] ↑ Conf [%] ↑ MRR ↑\nGPT-2 – 124.4M 59.3 4.09 30.0 31.2 .403\n+ Domain 15 45.8 ±.03 3.61 ±.002 33.4 ±.05 35.3 ±.02 .436 ± .3e-3\n+ UD syn dep 39 14 +54.1M 35.2 ±.24 3.09 ±.014 39.2 ±.11 42.3 ±.18 .488 ± .8e-3\n+ DM sem dep 59 15 +54.4M 34.2 ±.32 3.05 ±.019 38.8 ±.15 42.5 ±.26 .490 ±1.0e-3\n+ PSD sem dep 90 16 +54.9M 34.1 ±.43 2.96 ±.014 39.2 ±.17 44.0 ±.17 .491 ±1.4e-3\n+ PTB-phr syn const 38 14 +54.1M 33.5 ±.30 2.97 ±.026 40.3 ±.09 43.9 ±.34 .500 ± .6e-3\n+ PTB-fxn syn const 537 14 +62.7M 32.4 ±.37 2.92 ±.030 41.1 ±.18 44.8 ±.36 .507 ±1.3e-3\n+ PTG sem const 72 15 +54.6M 29.6 ±.20 2.68 ±.028 43.4 ±.08 48.8 ±.32 .524 ± .5e-3\n+ EDS sem const 10 15 +53.6M 26.6 ±.09 2.78 ±.024 43.1 ±.10 46.6 ±.24 .527 ± .8e-3\nTable 9:Main resultswithout early stopping: performance of language models combined with 7 SLR formalisms\nof different scope, structure, and label set (each corresponding to a\u0000Ensemble in § 4.3), compared to vanilla GPT-2\nand a version of GPT-2 that has been domain-ﬁnetuned on the raw text of the SLR training corpus (\u0000\u0000\u0000). We report\neach quality metric as mean± stdev over 5 random seeds. We also report model size in #parameters and training\nspeed in sentences per second as measures of efﬁciency. Best results in each column arebolded. For conﬁdence,\n‘best’ means best-calibrated, i.e., the smallest relative difference to accuracy.\nQRXQ\nYHUE\nPRG\n3HUSOH[LW\\\u0003\u000bORJ\u0003VFDOH\f\n\u0017\u0013\u0011\u0013\n\u0018\u0013\u0011\u0013\n\u0019\u0013\u0011\u0013\n\u001a\u0013\u0011\u0013\n\u001b\u0013\u0011\u0013\n\u001c\u0013\u0011\u0013\u0014\u0013\u0013\u0011\u0013\n\u0015\u0013\u0013\u0011\u0013\nQRXQ YHUE PRG\nDX[\nSURQ\n3HUSOH[LW\\\n\u0013\u0011\u0013\n\u0014\u0013\u0011\u0013\n\u0015\u0013\u0011\u0013\nDX[ SURQ\n%/\u0003Z\u0012R\u0003326\n%/\u0003Z\u0012\u0003326\n8'\n'0\n36'\n37%\u0010SKU\n37%\u0010I[Q\n37*\n('6\nDGS\nVFRQM\n3HUSOH[LW\\\n\u0013\u0011\u0013\n\u0014\u0013\u0011\u0013\nDGS VFRQM\nGHW\nFFRQM\n3HUSOH[LW\\\n\u0013\u0011\u0013\n\u0014\u0013\u0011\u0013\nGHW FFRQM\nFigure 3:Model perplexity (lower is better) with UPOS as additional input. Top left: nouns, verbs, and modiﬁers; top\nright: auxiliaries and pronouns; bottom left: adpositions and subordinating conjunctions; bottom right: determiners\nand coordinating conjunctions. Big gray squares mark baseline (ﬁnetuned GPT-2) performance without (dark) and\nwith (light) POS inputs and SLR-speciﬁc data points without/with POS inputs follow below the squares in each\nrespective column. Mind the different y-axis scales, and in particular the log scale in the top-left plot, which makes\nit easier to read very big and slightly smaller (but still big) differences at the same time.\n4390\nEval Train Perplexity\nPOS Toks Vocab GPT-2 UD DM PSD PTB-phr PTB-fxn PTG EDS\nAll 22,596 27,344 45.9 ± .1 32.7 ±0 .2 31.4 ±0 .1 30.7 ±0 .1 29.8 ±0 .2 29.0 ±0 .3 26.8 ±0 .3 24.7 ±0 .3\nnoun 7,731 18,435 142.5 ± .7 122.0 ±0 .1 119.0 ±1 .0 120.9 ±0 .7 138.0 ±2 .5 139.6 ±4 .5 120.6 ±2 .5 98.0 ±3 .6\nverb 2,639 7,100 128.8 ± .8 80.4 ±1 .2 89.4 ±1 .1 90.7 ±0 .7 81.7 ±0 .9 79.3 ±0 .7 86.9 ±2 .5 85.9 ±1 .4\nmod 2,235 6,292 228.7 ± .5 158.8 ±3 .8 145.9 ±3 .3 130.5 ±2 .0 123.4 ±2 .1 113.5 ±3 .3 100.2 ±4 .0 98.6 ±4 .0\naux 582 95 17.6 ± <. 1 11.1 ±0 .1 6.5 ±0 .1 9.3 ±0 .2 11.0 ±0 .2 10.9 ±0 .1 9.4 ±0 .1 5.9 ±0 .1\nadp 1,957 232 10.1 ± <. 1 7.3 ±0 .1 5.7 ±0 .1 7.2 ±0 .1 4.3 ± < 0.1 3.6 ± < 0.1 3.5 ±0 .1 5.5 ±0 .1\npart 645 27 3.7 ± <. 1 2.0 ± < 0.1 2.1 ±0 .1 2.3 ±0 .1 1.7 ±0 .1 1.7 ± < 0.1 1.7 ± < 0.1 1.6 ± < 0.1\nsconj 268 96 15.4 ± .1 12.3 ±0 .1 7.7 ±0 .2 11.3 ±0 .6 3.5 ± < 0.1 2.6 ± < 0.1 3.5 ±0 .1 6.8 ±0 .1\ncconj 548 35 13.0 ± .1 7.4 ±0 .3 6.3 ±0 .5 2.5 ±0 .1 4.3 ±0 .1 4.3 ±0 .1 2.1 ± < 0.1 1.9 ± < 0.1\ndet 1,726 91 9.4 ± .1 7.8 ±0 .1 6.7 ±0 .4 4.2 ±0 .1 6.5 ±0 .4 6.6 ±0 .4 5.5 ±0 .6 4.4 ±0 .2\npron 868 149 22.9 ± <. 1 17.5 ±0 .2 14.4 ±0 .3 12.1 ± < 0.1 9.9 ±0 .5 9.5 ±0 .1 8.5 ±0 .1 5.4 ±0 .1\nnum 719 1,059 72.6 ± .3 57.1 ±0 .7 55.2 ±0 .6 53.3 ±0 .5 58.4 ±0 .6 58.6 ±2 .5 48.3 ±0 .4 47.5 ±1 .0\npunct 2,527 68 4.9 ± <. 1 2.3 ± < 0.1 2.8 ± < 0.1 3.2 ± < 0.1 2.3 ± < 0.1 2.4 ±0 .1 2.6 ±0 .1 2.7 ± < 0.1\nmisc 151 183 7.0 ± <. 1 4.6 ±0 .1 5.1 ±0 .5 5.2 ±0 .3 3.7 ±0 .1 3.5 ±0 .2 5.6 ±0 .2 4.0 ±0 .4\nTable 10:PPL breakdown by UPOS classes of individual models (3-seed-averages), and the macro-average± stdev over all SLR-combined models. We show token counts\nand observed vocabulary sizes for reference. mod—adjectives and adverbs; aux—auxiliary verbs; adp—adpositions; part—particles; sconj—subordinating conjunctions;\ncconj—coordinating conjunctions; det—determiners; pron—pronouns; num—numbers; punct—punctuation. Best results (within the variance) in each row arebolded.\nAblation Applied in UD DM PSD PTB-phr PTB-fxn PTG EDS\nFull 32.7 31.4 30.7 29.8 29.0 26.8 24.7\n\u0000 Labels testing +10.2 +4.7 +5.8 +63.0 +73.9 +18.7 +21.8\n\u0000 Anchors testing +110.8 +34.8 +22.4 +177.0 +223.1 +103.4 +70.4\n\u0000 Both testing +92.4 +33.4 +19.2 +168.9 +207.4 +80.1 +70.1\n\u0000 Labels training +2.0 +1.4 +1.6 +8.4 +9.0 +5.3 +1.9\n\u0000 Anchors training +13.0 +8.5 +6.3 +16.9 +17.5 +18.1 +12.8\n\u0000 Both training +13.0 +7.8 +6.1 +17.3 +18.3 +18.5 +12.8\n\u0000 Labels both +2.0 +1.3 +1.7 +8.4 +9.3 +5.4 +2.0\n\u0000 Anchors both +13.1 +7.9 +6.0 +16.7 +17.5 +19.2 +14.0\n\u0000 Both both +13.3 +7.3 +6.0 +16.8 +18.1 +19.4 +14.1\n\u0000 SLR both +13.2 +14.5 +15.2 +16.1 +16.9 +19.1 +21.2\nTable 11:Ablations measured in absolute perplexity difference (\u0000PPL). Full and \u0000SLR correspond, respectively, to table2’s rows 4 (DM) / 6 (PTB-phr) and row 2 (GPT-2\n+Domain).\n4391",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6659598350524902
    },
    {
      "name": "Linguistics",
      "score": 0.5424045920372009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44796234369277954
    },
    {
      "name": "Natural language processing",
      "score": 0.43873587250709534
    },
    {
      "name": "Philosophy",
      "score": 0.10202574729919434
    }
  ]
}