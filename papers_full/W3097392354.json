{
  "title": "Language Model is all You Need: Natural Language Understanding as Question Answering",
  "url": "https://openalex.org/W3097392354",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5026771837",
      "name": "Mahdi Namazifar",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5014990977",
      "name": "Alexandros Papangelis",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5087941479",
      "name": "Gökhan Tür",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5068709817",
      "name": "Dilek Hakkani‐Tür",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3045834900",
    "https://openalex.org/W3034861927",
    "https://openalex.org/W2979400990",
    "https://openalex.org/W3045689439",
    "https://openalex.org/W6770169672",
    "https://openalex.org/W2963578915",
    "https://openalex.org/W3046213497",
    "https://openalex.org/W6782625805",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6780633456",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6752542827",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6759393636",
    "https://openalex.org/W2887280559",
    "https://openalex.org/W2097550833",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W2963033987",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3015637204",
    "https://openalex.org/W2965145066",
    "https://openalex.org/W3041843309",
    "https://openalex.org/W2949176913",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2735585661",
    "https://openalex.org/W2917128112",
    "https://openalex.org/W2987705220",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W3049346316",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2988252747",
    "https://openalex.org/W3025915678"
  ],
  "abstract": "Different flavors of transfer learning have shown tremendous impact in advancing research and applications of machine learning. In this work we study the use of a specific family of transfer learning, where the target domain is mapped to the source domain. Specifically we map Natural Language Understanding (NLU) problems to QuestionAnswering (QA) problems and we show that in low data regimes this approach offers significant improvements compared to other approaches to NLU. Moreover we show that these gains could be increased through sequential transfer learning across NLU problems from different domains. We show that our approach could reduce the amount of required data for the same performance by up to a factor of 10.",
  "full_text": "LANGUAGE MODEL IS ALL YOU NEED: NATURAL LANGUAGE UNDERSTANDING AS\nQUESTION ANSWERING\nMahdi Namazifar, Alexandros Papangelis, Gokhan Tur, Dilek Hakkani-T¨ur\nAmazon Alexa AI\nABSTRACT\nDifferent ﬂavors of transfer learning have shown tremendous impact\nin advancing research and applications of machine learning. In this\nwork we study the use of a speciﬁc family of transfer learning, where\nthe target domain is mapped to the source domain. Speciﬁcally we\nmap Natural Language Understanding (NLU) problems to Question\nAnswering (QA) problems and we show that in low data regimes\nthis approach offers signiﬁcant improvements compared to other\napproaches to NLU. Moreover we show that these gains could be\nincreased through sequential transfer learning across NLU problems\nfrom different domains. We show that our approach could reduce\nthe amount of required data for the same performance by up to a\nfactor of 10.\nIndex Terms— Transfer Learning, Question Answering, Natural\nLanguage Understanding\n1. INTRODUCTION\nTransferring the knowledge that machine learning models learn from\na source domain to a target domain, which is known as transfer\nlearning (Figure 1a) [1, 2], has shown tremendous success in Natural\nLanguage Processing (NLP) [3, 4, 5, 6, 7].\nOne of the most prominent advantages of transfer learning is\nmanifested in low data regimes. As the models become increasingly\ncomplex, in most cases this complexity comes with requirements\nfor larger training data which makes transferring the learning from a\nhigh data domain to a low data domain very impactful. In this work\nwe focus on the type of transfer learning in which the target domain\nis ﬁrst mapped to the source domain. Next a model is trained on\nthe source domain. Then the transfer of knowledge is done through\nﬁne-tuning of this model on the mapping of the target domain (to\nthe source domain), as shown in Figure 1b. As an example of this\ntransfer learning paradigm in NLP, decaNLP [8] could be mentioned\nwhere 10 NLP tasks are mapped to the Question Answering (QA)\nproblem, in which given a context the model should ﬁnd the answer\nto a question.\nIn this work, we map Natural Language Understanding (NLU)\nproblems to the QA problem. Here NLU refers to determining the\nintent and value of slots in an utterance [9]. For instance in “show\ncheap Italian restaurants” intent could be inform and the value for\nslot cuisine is “Italian” and for slot price range is “cheap”. More\nspeciﬁcally in our approach to which we refer as QANLU, we\nbuild slot and intent detection questions and answers based the the\nNLU annotated data. QA models are ﬁrst trained on QA corpora\nand then ﬁne-tuned on questions and answers created from NLU\nannotated data. In this approach transfer learning happens through\ntransferring knowledge of ﬁnding the answer to a question given\na context, that is acquired by the model during the training of the\nQA model, to ﬁnding the value of an intent or a slot in text input.\nThrough our computational results we show that QANLU in low\ndata regimes and few-shot settings signiﬁcantly outperforms the\nsentence classiﬁcation and token tagging approaches for intent and\nslot detection tasks, as well as the newly introduced “IC/SF few-\nshot” approach [10] for NLU. We also show that QANLU sets a new\nstate of the art performance on slot detection on the Restaurants-8k\ndataset [11]. Furthermore, we show that augmenting the QA corpora\nwith questions and answers created based on NLU annotated data\nimproves the performance of QA models. Throughout this work we\nuse span selection based QA models built on top of transformer-\nbased language models [6]. That being said, our approach is quite\ngeneric and could be extended to any type of QA system.\n2. RELATED WORKS\nFraming NLP tasks as QA has been studied in the past. For instance\n[8] maps 10 NLP tasks (excluding intent and slot detection) into QA\nand trains a single model for all of them. However, this work does\nnot explore the task of intent and slot classiﬁcation. In a similar\nline of reasoning, [12] poses the Dialogue State Tracking (DST)\ntask as machine reading comprehension (MRC), formulated as QA.\n[13] builds on that work achieving competitive DST results with full\ndata and in few-shot settings. [14] also explores DST as QA, using\ncandidate values for each slot in the question (similar to the Multiple-\nChoice setting of [13]) achieving slightly better results than [13]. We\npropose a method that is conceptually similar but focuses on low-\nresource applications and does not require designing and training of\na new model architecture or extensive data pre-processing, achieving\nstrong results in slot and intent detection with an order of magnitude\nless data. Here we do not discuss all intent or slot detection methods.\nHowever, some notable few-shot NLU works include [15, 16, 17, 11,\n15], and we compare against their results when appropriate. Other\ninteresting approaches that do not require training include priming\npre-trained language models, e.g. [18].\n3. QUESTION ANSWERING FOR NATURAL LANGUAGE\nUNDERSTANDING (QANLU)\n3.1. Slot Detection\nConsider a set of text records T = {t1, t2, ..., tn} in which each\nrecord is annotated for the set of slots S = s1, s2, ..., sm. Also for\neach slot sj consider a set of questionsQsj = {qsj 1, qsj 2, ..., qsj kj }\nthat could be asked about sj given any text record ti. The following\nis an example of such a setting:\nS : {cuisine, price range, area}, ti : “Show cheap Italian restaurants”\ncuisine: “Italian”, price range: “cheap”, area: “”\nQ : {Qcuisine, Qprice range, Qarea}\narXiv:2011.03023v1  [cs.CL]  5 Nov 2020\n(a) Transfer Learning from source domain to\ntarget domain\n (b) Transfer learning through mapping a target\ndomain to source domain. In this work we\nmap NLU to QA tasks.\n (c) Sequential transfer learning for QANLU.\nFig. 1:\nwhere\nQcuisine : {“what cuisine was mentioned?”,\n“what type of food was speciﬁed?”}\nQprice range : {“what price range?”}\nQarea : {“what part of town was mentioned?”, “what area?”}\nGiven T, S, and Q it is straightforward to create the set of all\nthe possible questions and their corresponding answers for each ti\nas the context for the questions:\nContext: “Show cheap Italian restaurants”\nwhat cuisine was mentioned? “Italian”\nwhat type of food was speciﬁed? “Italian”\nwhat price range? “cheap”\nwhat part of town was mentioned? “”\nwhat area? “”\nWe experiment with different ways of creating the set Q. This set\ncould be handcrafted, i.e. for each slot we create a set of questions\nseparately, or created using templates such as “what was\nmentioned?” where we the blank is ﬁlled with either the slot name\nor a short description of the slot, if available.\n3.2. Intent Detection\nFor intent detection we add “yes. no.” at the beginning of the context\nand for each intent we create a question like “is the intent asking\nabout ?” where the blank is ﬁlled with the intent. The answer\nto these questions are “yes” or “no” from the segment that was added\nto the beginning of the context depending on whether the intent is in\nthe context or not.\n3.3. Question Answering Model\nIn this work we use span detection based QA models that are\nbuilt on top of transformers [19] as are described in [6]. We also\nuse the SQuAD2.0 [20] data format for creating questions and\nanswers, as well as the corpus for the source domain (QA). Note\nthat in converting annotated NLU data to questions and answers\nin QANLU, since for each text record we ask all the questions for\nall the slots (whether they appear in the text or not), many of the\nquestions are not answerable. As was discussed earlier, we use\npre-trained QA models that are trained on SQuAD2.0 (the green box\nin Figure 1b) and ﬁne-tune them with the questions and answers that\nare created from the NLU tasks. We also study how in a sequential\ntransfer learning style we can improve the performance of NLU\nthrough QANLU (Figure 1c).\n4. COMPUTATIONAL RESULTS\nIn this section we present our computational results for QANLU.\nOur experiments are done on the ATIS [21, 22] and Restaurants-\n8k [11] datasets. All of the experiments are implemented using\nHuggingface [23], and we also use pre-trained language models and\nQA models provided by Huggingface and ﬁne-tune them for our QA\ndata. We base our experiments mainly on pre-trained DistilBERT\n[24] and ALBERT [25] models.\n4.1. ATIS\nThe ATIS dataset is an NLU benchmark that provides manual\nannotations for utterances inquiring a ﬂight booking system. Since\nthe original ATIS dataset does not have a validation set, we use the\nsplit of the original training set into training and validation that is\nproposed in [26]. For each slot in ATIS we create a question set and\nfor each record in ATIS we create the set of questions and answers\nbased on all the question sets and the slot and intent annotation of\nthe record, according to the approach described in Section 3. In\nthe ﬁrst set of experiments we study how our QANLU approach\ncompares to the widely used joint token and sentence classiﬁcation\n[9] in few-shot settings using different stratiﬁcation in sampling of\nthe training records for the few-shot setting. Table 2 summarizes\nthe results. In this table we report F1 scores for both slots and\nintent detection tasks. The reason why we use F1 scores for intent\ndetection is that in the ATIS dataset each record could have more\nthan one intent. Each value in Table 2 is an average over 5 runs\nwith different random seeds. Each row in this table represents one\nsample of the ATIS training data. The set of rows titled “N uniform\nsamples” are sampled uniformly with samples of sizes of 10, 20, 50,\n100, 200, and 500 ATIS records. The set of rows titled “N samples\nper slot” are sampled such that each sample includes at least N\ninstances for any of the slots, where N is 1, 2, 5, or 10. The set\nof rows titled “ N samples per intent” are sampled such that each\nintent appear in at least N instances, where N is 1, 2, 5, or 10. The\nnumbers in parenthesis in front of N represent the number of ATIS\nrecords in the sample. For each ATIS record we have 179 questions\nand answers for intents and slots.\nIn Table 2 we report performance of models based on both\nDistilBERT and ALBERT. For QANLU we ﬁne-tune a QA model\ntrained on SQuAD2.0 data (“distilbert–base–uncased–distilled–\nsquad”2 for DistilBERT and “twmkn9/albert–base–v2–squad2”2 for\nALBERT) on our questions and answers for ATIS samples. We\nAlso train joint intent and token classiﬁcation models for the ATIS\n2Model acquired from www.huggingface.co/models\ntraining samples based on pre-trained DistilBERT and ALBERT\nmodels (“distilbert–base–uncased”2 and “albert–base–v2”2)3. We\ncompare the results of QANLU models with the classiﬁcation based\nmodels (noted as QANLU and Cls in the table, respectively). It is\nclear that QANLU models outperform classiﬁcation based models,\noften by a wide margin. For instance for the ALBERT based model,\nfor the case where there is at least 1 sample per slot the QA based\nmodel outperforms the classiﬁcation based model by 26% (86.37\nvs 68.26). It is notable that the gap between the two approaches\nnarrows as the number of samples increases, with the exception of\nintent detection for the uniform sample with only 10 samples. In a\ncloser look at this sample, the intent for all the records is the same\n(“atis ﬂight” which is the intent for 74% of the ATIS training set)\nand that could explain why the models almost always predict the\nsame value for the intent.\nThe fact that for both DistilBERT and ALBERT based models\nwe see that the QANLU signiﬁcantly outperforms the intent and\nslot classiﬁcation models in few-shot settings indicates that the\nperformance improvements are likely stemmed from transfer learning\nfrom reading comprehension that is learned in the QA task.\nIn this set of experiments we used handcrafted questions for each\nslot. One could argue that creating questions for slots is as difﬁcult\nor perhaps more difﬁcult as getting data annotated speciﬁcally for\nintents and slots. To see if we can detour the manual question\ncreation process we also experimented with questions that were\ncreated using frames based on a brief description of each slot as well\nas using the tokenized slots names. These frame based questions\ncould be easily created for free by running some simple scripts.\nThe experimental results show no signiﬁcant degradation in the\nperformance of QANLU models trained on frame based questions.\nIn another set of experiments we compare QANLU with another\nfew-shot approach (few-shot IC/SF) proposed in [10]. We use the\nexact same split of the ATIS dataset that is created in that paper.\nResults are in Table 1.\nFew-shot IC/SF QANLU\nF1 score 43.10 68.69\nTable 1: QANLU vs Few-shot IC/SF [10] Slot detection F1. 43.10 is\nreported in Table 5 of [10]\nThe few-shot IC/SF results (43.10) are average of multiple runs\nof a BERT model ﬁrst pre-trained on the training set, and then\nﬁne-tuned on a “support” set sampled from the test set, and then\nevaluated on a “query” set also sampled from the test set. We used\nthe exact same training set that used in that work and trained a\nBERT (base size) based QANLU model on the training set. We\nthen directly evaluated that model on the exact same test set created\nin [10], without any ﬁne-tuning on a support set. The resulting F1\nscore (68.98) is 60% higher than what is reported [10].\n4.2. Restaurants-8k\n4.2.1. QANLU for Restaurants-8k\nThe Restaurants-8k dataset [11] is a set of annotated utterances\ncoming from actual conversations in the restaurant booking domain.\nThe dataset only contains the user side utterances and slot (5 in total)\nannotations. The system side of the conversations are missing, but\ngiven the set of slots that are annotated at every user turn, using\n3We also tried these models ﬁne-tuned on SQuAD2.0, but they didn’t\nperform as well on the intent and token classiﬁcation tasks\nsimple frames we can build a full context for token classiﬁcation\nand QANLU approaches.\nThe rest of data preparation process is identical to what we\ndescribed in Section 3.1. We take both uniform and stratiﬁed\nsamples of the training data to create few-shot settings for training\nQANLU models, and compare the results with token classiﬁcation\nmodels. The QANLU model is again a QA model trained on\nSQuAD2.0 (“distilbert–base–uncased–distilled–squad” 2) that we\nﬁne-tune on the sampled training sets. The token classiﬁcation\nmodel is built on top of “distilbert–base–uncased” 2. The results\nare captured in the curves “QANLU (SQ →R8k)” (SQ stands for\nSQuAD2.0 and R8k stands for Restaurants-8k) and “Cls” (stands\nfor token classiﬁcation and similar to the ATIS case is based on [9]\nwithout the sentence classiﬁcation head) in Figure 2. We discuss the\nresults in the next subsection.\n4.2.2. Sequential Transfer Learning from ATIS to Restaurants-8k\nIn another set of experiments we study whether QANLU would\nenable transfer learning from one NLU domain to another. This\nis referred to as sequential transfer learning in the literature. For\nthis purpose we ﬁne-tune a QANLU model that was trained on\nthe entire ATIS training set, on samples of Restaurants-8k dataset.\nWe compare the performance of the resulting model with QANLU\nﬁrst trained on SQuAD2.0 and then ﬁne-tuned on Restaurants-8k\nsamples, as well as the token classiﬁcation model.\n4.2.3. Restaurants-8k Results\nIn Figure 2 the curve QANLU (SQ → ATIS → r8k) is the squential\ntransfer learning model based on “distilbert–base–uncased–distilled–\nsquad”2 model (DistilBERT base model trained on SQuAD2.0).\nFrom the ﬁgure we can see that except for 10 and 20 uniform\nsamples, for all the samples ﬁne-tuning of SQuAD2.0 QA models\non Restaurants-8k results in signiﬁcantly higher F1 scores compared\nto the token classiﬁcation approach. For uniform samples of size\n10 and 20 the QANLU model (trained on SQuAD2.0 and ﬁne-\ntuned on Restaurants-8k samples) performs poorly. Our intuition\non the reason behind this poor performance is the small number\nof questions and answers for these samples (15 per record), and\nmost likely it is not sufﬁcient for the model to learn how to handle\nNLU style questions. On the other hand for the sequential transfer\nlearning QANLU model (SQ →ATIS→R8k column of Figure 2)\nwe see that the model outperforms both the token classiﬁcation\nmodel and the QANLU model trained on SQuAD2.0 and ﬁne-\ntuned on Restaurants-8k samples by a wide margin (in some cases\nby over 50%). These numbers are also shown in Figure 2. This\nsuggests that perhaps using QA as the canonical problem where\nNLU problems from different domains could be mapped to, could\nfacilitate transfer learning across these NLU problems specially in\nfew-shot settings. Also note that when the entire data is used for\ntraining the performance difference vanishes (96.98 for SQ → R8k,\n96.43 for SQ → ATIS →R8k, and 95.94 for Cls), which suggests\nthat the QANLU approach is as strong as the state of the art outside\nof few-shot settings.\nAlso Figure 3 shows a comparison between QANLU and Span-\nConveRT [11] in few-shot settings. The few-shot F1 scores of\nSpan-ConveRT on Restaurants-8k are borrowed from Table 3 of\n[11]. In these experiment in order to match the settings of Span-\nConverRT we do not create the previous turn for the context, hence\nthe difference between QANLU numbers in Figure 3 compared\nto Figure 2. From this ﬁgure it is notable that with 20 data\nIntent Slot\nDistilBERT ALBERT DistilBERT ALBERT\nN QANLU Cls QANLU Cls QANLU Cls QANLU Cls\nN uniform\n10 71.80 71.78 72.18 71.78 67.23 61.60 64.24 54.78\nsamples\n20 83.95 77.80 83.28 75.36 78.53 56.70 74.53 51.67\n50 86.07 78.93 86.32 73.90 83.84 76.61 80.26 74.04\n100 93.08 87.91 92.14 80.20 85.69 80.34 83.13 77.50\n200 94.30 90.97 96.78 85.02 91.24 85.32 89.57 83.63\n500 96.40 95.45 96.77 90.62 92.31 91.15 91.18 86.69\nN samples per\n1 (75) 88.72 86.47 90.91 84.93 88.47 76.24 86.37 68.26\nslot (Total)\n2 (136) 91.68 84.91 92.11 82.42 90.77 84.42 90.17 79.49\n5 (302) 94.34 93.74 95.52 87.47 93.11 91.38 87.82 86.50\n10 (549) 97.10 96.19 94.21 92.73 94.11 93.93 92.27 91.68\nN samples per\n1 (17) 40.32 27.91 54.49 25.73 62.57 55.38 62.22 51.05\nintent (Total)\n2 (33) 78.24 47.20 62.22 23.52 75.39 65.09 74.99 61.01\n5 (81) 86.49 74.08 89.36 41.28 84.40 80.25 82.70 71.83\n10 (152) 91.23 91.16 90.13 68.93 88.37 83.40 86.32 78.25\nAll N/A (4478) 98.23 98.37 97.59 97.90 95.70 95.80 94.48 95.37\nTable 2: QANLU vs. intent and token classiﬁcation (Cls) [9] for ATIS in few-shot settings. Each row is associated with a different sampling size and strategy\nof ATIS data. Values in bold represent statistically signiﬁcant difference at p-value 0.05. Note that QANLU performs signiﬁcantly better (in some cases by\nmore the 20%) compared to joint intent and slot classiﬁcation.\nFig. 2: Slot detection with QANLU vs token classiﬁcation. SQ →\nR8k indicates QANLU ﬁrst trained on SQuAD2.0 and the ﬁne-tuned on\nsamples of Restaurants-8k. SQ →ATIS→ R8k is QANLU ﬁrst trained\non SQuAD2.0, then ﬁne-tuned on entire ATIS, and then ﬁne-tuned on\nsamples of Restaurants-8k (sequential transfer learning). Cls is for the token\nclassiﬁcation approach. Numbers associated with each point are F1 scores.\nFig. 3: QANLU compared to Span-ConveRT [11] in few-shot settings. The\nnumbers associated with each point are the sample size and F1, respectively.\npoints QANLU reaches the higher performance than Span-ConveRT\nachieves with 256 data points, which translates to a 10x reduction in\nthe amount of data needed. Also with the entire training set QANLU\nperforms within less than 1% of the state-of-the-art.\n5. DISCUSSION\nThe customary feeding token embeddings of a sentence into a\nnetwork and mapping the output of the network for each token onto\na certain number of classes for NLU seems somewhat far from\nour intuition on how humans understand natural language. The\nmain research question that we try to answer is whether all NLP\nproblems can be efﬁciently and effectively mapped to one canonical\nproblem. If the answer is yes, could that canonical problem be\nQA? In this work we scratch the surface on these questions, in\nthat we showcase the strength of transfer learning that happens in\nthis paradigm in learning from few examples for intent and slot\ndetection. But our experiments were limited to span detection QA\nproblem and SQuAD2.0 QA data. Future works will include going\nbeyond this conﬁguration and also expanding across different NLP\nproblems. Measuring how much transfer of knowledge could be\nachieved across different NLP tasks would be interesting to know.\nAnother future direction could be studying how the questions for\nQANLU could be generated automatically based on the context.\nOne interesting side product of QANLU is that the questions\nand answers created for NLU tasks could augment the questions and\nanswers of the QA task (SQuAD2.0 in this work) in order to improve\nthe QA model performance. To study this idea we used the exact\ntraining script that Huggingface provides for training QA models on\nthe SQuAD2.0 and also the SQuAD2.0 augmented with questions\nand answers that we created for ATIS QANLU. The training scripts\nspecify 2 training epochs. It could be argued that this comparison\nwould not be fair since 2 passes over the augmented data means\na lot more optimization steps since there are many more questions\nand answers in the augmented data. To account for this we also run\nthe training on the original SQuAD2.0 data for the same number\nof optimization steps as it takes to run 2 epochs on the augmented\ndata (9000 steps). The results (QA F1 on the validation set) are\nshown in Table 3. As the numbers show training the same models\non the augmented data signiﬁcantly improves the performance of\nthe ﬁnal QA model on the Development set of SQuAD2.0. We\nbelieve this result could be an indication that we can not only transfer\nfrom QA to other NLU tasks, we can also improve QA through data\naugmentation by mapping NLU problems to QA.\nSQuAD2.0 SQuAD2.0 + ATIS SQuAD2.0\n(2 epochs) (2 epochs = 9k steps (9k steps)\n“bert-base-cased” 70.07 74.29 65.42\n“distilbert-base-uncased” 55.58 60.26 57.03\n“albert-base-v2” 78.05 79.26 76.44\nTable 3: F1 scores of QA models on original SQuAD2.0 and the augmented\nSQuAD2.0 with ATIS QANLU Data. Data augmentation improves the\nperformance of QA models.\n6. REFERENCES\n[1] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao\nYang, and Chunfang Liu, “A survey on deep transfer learning,”\nin Artiﬁcial Neural Networks and Machine Learning – ICANN\n2018, Vˇera K˚urkov´a, Yannis Manolopoulos, Barbara Hammer,\nLazaros Iliadis, and Ilias Maglogiannis, Eds., 2018, pp. 270–\n279.\n[2] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,\nYongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He, “A\ncomprehensive survey on transfer learning,” 2020.\n[3] Zaid Alyafeai, Maged S. Al-shaibani, and I. Ahmad, “A survey\non transfer learning in natural language processing,” ArXiv,\nvol. abs/2007.04239, 2020.\n[4] Jeffrey Pennington, Richard Socher, and Christopher D.\nManning, “Glove: Global vectors for word representation,”\nin Empirical Methods in Natural Language Processing\n(EMNLP), 2014, pp. 1532–1543.\n[5] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,\nand Jeff Dean, “Distributed representations of words and\nphrases and their compositionality,” in Advances in Neural\nInformation Processing Systems 26, C. J. C. Burges, L. Bottou,\nM. Welling, Z. Ghahramani, and K. Q. Weinberger, Eds., pp.\n3111–3119. 2013.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, “BERT: Pre-training of deep bidirectional\ntransformers for language understanding,” in Proceedings of\nthe 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, Volume 1, June 2019, pp. 4171–4186.\n[7] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, W. Li, and\nP. Liu, “Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer,” ArXiv, vol. abs/1910.10683, 2019.\n[8] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and\nRichard Socher, “The natural language decathlon: Multitask\nlearning as question answering,” CoRR, vol. 1806.08730,\n2018.\n[9] Qian Chen, Zhu Zhuo, and Wen Wang, “BERT for joint intent\nclassiﬁcation and slot ﬁlling,” CoRR, vol. 1902.10909, 2019.\n[10] Jason Krone, Yi Zhang, and Mona Diab, “Learning to\nclassify intents and slot labels given a handful of examples,”\nin Proceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, 2020, pp. 96–108.\n[11] Sam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli ´c, and\nMatthew Henderson, “Span-convert: Few-shot span extraction\nfor dialog with pretrained conversational representations,”\n2020.\n[12] Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung\nChung, and Dilek Hakkani-Tur, “Dialog state tracking: A\nneural reading comprehension approach,” arXiv preprint\narXiv:1908.01946, 2019.\n[13] Shuyang Gao, Sanchit Agarwal, Tagyoung Chung, Di Jin, and\nDilek Hakkani-Tur, “From machine reading comprehension\nto dialogue state tracking: Bridging the gap,” arXiv preprint\narXiv:2004.05827, 2020.\n[14] Li Zhou and Kevin Small, “Multi-domain dialogue state\ntracking as dynamic knowledge graph enhanced question\nanswering,” arXiv preprint arXiv:1911.06192, 2019.\n[15] Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and Larry\nHeck, “Towards zero-shot frame semantic parsing for domain\nscaling,” arXiv preprint arXiv:1707.02363, 2017.\n[16] Hemanthage S Bhathiya and Uthayasanker Thayasivam, “Meta\nlearning for few-shot joint intent detection and slot-ﬁlling,”\nin Proceedings of the 2020 5th International Conference on\nMachine Learning Technologies, 2020, pp. 86–92.\n[17] Darsh J Shah, Raghav Gupta, Amir A Fayazi, and Dilek\nHakkani-Tur, “Robust zero-shot cross-domain slot ﬁlling with\nexample values,” 2019.\n[18] Andrea Madotto, “Language models as few-shot learner\nfor task-oriented dialogue systems,” arXiv preprint\narXiv:2008.06239, 2020.\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia\nPolosukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems 30 , I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, Eds., pp. 5998–6008. 2017.\n[20] Pranav Rajpurkar, Robin Jia, and Percy Liang, “Know what\nyou don’t know: Unanswerable questions for SQuAD,” in\nProceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics, pp. 784–789.\n[21] Charles T. Hemphill, John J. Godfrey, and George R.\nDoddington, “The ATIS spoken language systems pilot\ncorpus,” in Speech and Natural Language: Proceedings of\na Workshop Held at Hidden Valley, Pennsylvania, June 24-\n27,1990, 1990.\n[22] G. Tur, D. Hakkani-T ¨ur, and L. Heck, “What is left to\nbe understood in atis?,” in 2010 IEEE Spoken Language\nTechnology Workshop, 2010, pp. 19–24.\n[23] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault,\nR´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush, “Huggingface’s\ntransformers: State-of-the-art natural language processing,”\nArXiv, vol. abs/1910.03771, 2019.\n[24] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas\nWolf, “Distilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter,” 2020.\n[25] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut, “Albert: A lite\nbert for self-supervised learning of language representations,”\nArXiv, vol. abs/1909.11942, 2020.\n[26] Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, and Philip S\nYu, “Joint slot ﬁlling and intent detection via capsule neural\nnetworks,” in Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), 2019.",
  "topic": "Natural language understanding",
  "concepts": [
    {
      "name": "Natural language understanding",
      "score": 0.7968053817749023
    },
    {
      "name": "Computer science",
      "score": 0.7206265926361084
    },
    {
      "name": "Transfer of learning",
      "score": 0.7018471956253052
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6262435913085938
    },
    {
      "name": "Question answering",
      "score": 0.5876718163490295
    },
    {
      "name": "Natural language processing",
      "score": 0.5444676876068115
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5172553658485413
    },
    {
      "name": "Natural language",
      "score": 0.5108980536460876
    },
    {
      "name": "Labeled data",
      "score": 0.4168325662612915
    },
    {
      "name": "Machine learning",
      "score": 0.37038350105285645
    },
    {
      "name": "Mathematics",
      "score": 0.06355315446853638
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ]
}