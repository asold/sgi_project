{
  "title": "A tutorial on open-source large language models for behavioral science",
  "url": "https://openalex.org/W4389395857",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2116202430",
      "name": "Rui Mata",
      "affiliations": [
        "Max Planck Institute for Human Development",
        "University of Basel"
      ]
    },
    {
      "id": "https://openalex.org/A2077204653",
      "name": "Dirk U. Wulff",
      "affiliations": [
        "Max Planck Institute for Biological Cybernetics"
      ]
    },
    {
      "id": "https://openalex.org/A2550663075",
      "name": "Marcel Binz",
      "affiliations": [
        "University of Basel"
      ]
    },
    {
      "id": "https://openalex.org/A5111260829",
      "name": "Zak Hussain",
      "affiliations": [
        "University of Basel",
        "Max Planck Institute for Human Development"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386497319",
    "https://openalex.org/W3163824338",
    "https://openalex.org/W2929581986",
    "https://openalex.org/W4226325671",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W4387780733",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4387575740",
    "https://openalex.org/W2626804490",
    "https://openalex.org/W4214895350",
    "https://openalex.org/W6857563048",
    "https://openalex.org/W3128410644",
    "https://openalex.org/W2161080627",
    "https://openalex.org/W2778689844",
    "https://openalex.org/W4361193900",
    "https://openalex.org/W2136219431",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W6851923665",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4385347627",
    "https://openalex.org/W2917705988",
    "https://openalex.org/W2948834178",
    "https://openalex.org/W4320481230",
    "https://openalex.org/W4283205649",
    "https://openalex.org/W1502958265",
    "https://openalex.org/W3082619305",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W3034344071",
    "https://openalex.org/W2949527586",
    "https://openalex.org/W7030677884",
    "https://openalex.org/W2141007997",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3111307227",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W2054698589",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4309267672",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4376583107",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4377864916",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4387607602",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2972680241",
    "https://openalex.org/W4221068952",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4382986603",
    "https://openalex.org/W3171953676",
    "https://openalex.org/W4387106968",
    "https://openalex.org/W4388598844",
    "https://openalex.org/W4379933116",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W4385570594",
    "https://openalex.org/W4387891143",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4226435710",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4387839193",
    "https://openalex.org/W4392271821",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W4383313862",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2901707424",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W4393074580",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4384154861",
    "https://openalex.org/W2527306555",
    "https://openalex.org/W1967390364",
    "https://openalex.org/W2970552039",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2526501380",
    "https://openalex.org/W2788767722",
    "https://openalex.org/W4283688409",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4387687061"
  ],
  "abstract": "Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.",
  "full_text": "A tutorial on open-source large language models for behavioral science\nZak Hussain1,2, Marcel Binz3,4, Rui Mata1, and Dirk U. Wulff2,1\n1University of Basel\n2Max Planck Institute for Human Development\n3Max Planck Institute for Biological Cybernetics\n4Helmholtz Center for Computational Health\nLarge language models (LLMs) have the potential to revolutionize behavioral science by ac-\ncelerating and improving the research cycle, from conceptualization to data analysis. Unlike\nclosed-source solutions, open-source frameworks for LLMs can enable transparency, repro-\nducibility, and adherence to data protection standards, which gives them a crucial advantage\nfor use in behavioral science. To help researchers harness the promise of LLMs, this tutorial\noffers a primer on the open-source Hugging Face ecosystem and demonstrates several appli-\ncations that advance conceptual and empirical work in behavioral science, including feature\nextraction, fine-tuning of models for prediction, and generation of behavioral responses. Exe-\ncutable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial\ndiscusses challenges faced by research with (open-source) LLMs related to interpretability and\nsafety and offers a perspective on future research at the intersection of language modeling and\nbehavioral science.\nIntroduction\nLarge language models (LLMs)—machine learning sys-\ntems trained on vast amounts of text and other inputs—are\nincreasingly being used in science (Van Noorden & Perkel,\n2023), and significantly advancing the capacity to analyze\nand generate meaningful linguistic information. These mod-\nels are poised to change the scientific workflow in numerous\nways and are already used across all aspects of the research\ncycle, from conceptualization to data analysis. For example,\nin psychology (Demszky et al., 2023) and related disciplines\n(Korinek, 2023), LLMs are being used to automate research\nprocesses, predict human judgments, and run in-silico behav-\nioral experiments.\nScientific applications of LLMs require high levels of\ntransparency and reproducibility (Bockting et al., 2023). In\naddition, many applications in behavioral science involve\nsensitive information (e.g., personal or health data) or tar-\nget vulnerable populations (e.g., children) and thus require\nspecific data protection protocols. Open-source frameworks\nthat provide full transparency and respect privacy require-\nments are therefore indispensable for applications of LLMs\nin behavioral science.\nWe aim to help advance the responsible use of LLMs in\nbehavioral science by providing a comprehensive tutorial on\napplications using an open-source framework that maximizes\ntransparency, reproducibility, and data privacy. Specifically,\nwe provide a primer on the Hugging Face ecosystem, cover-\ning several applications of LLMs, including conceptual clar-\nification, prediction of behavioral outcomes, and generation\nof human-like responses. Our target audience consists of be-\nhavioral researchers with a basic knowledge of programming\nprinciples who are interested in adding LLMs to their work-\nflows. We hope that this resource will help researchers in\npsychology and related disciplines to adopt LLMs for a wide\nrange of tasks, whilst also maintaining an appreciation of the\nsubtle complexities of drawing scientific conclusions from\nsuch flexible and opaque models.\nIn what follows, we first provide a short primer on\ntransformer-based language models. Second, we consider\napplications of LLMs in behavioral science and introduce\nthe Hugging Face ecosystem and associated Python libraries.\nThird, we present three areas of application—feature ex-\ntraction, fine-tuning, and text generation—and present sev-\neral use cases in behavioral research. Finally, we address\nsome advantages and limitations of current open-source ap-\nproaches and consider future directions at the intersection of\nLLMs and behavioral research.\nA Primer on Transformer-Based Language Models\nToday’s LLMs are based on the transformer architecture\n(Vaswani et al., 2017), which is a family of neural network\nmodels that draw on a common set of building blocks. In this\nsection, we first introduce these building blocks in sequence\n(see Figure 1) before discussing important architectural vari-\nants and ways of applying LLMs in behavioral science.\nTokenizers\nWhen text input is fed into an LLM, it is first broken up\ninto digestible pieces known as tokens. This decomposition\n2 HUSSAIN, BINZ, MATA, & WULFF\nPrompt Tokenizer Output probabilities\nPositional \nencoding\n+ Attention  \nblock\nNx\nModel  \nhead\nInput \nembedding\nFigure 1 Overview of the LLM processing pipeline. The diagram illustrates the sequence of operations performed on an\ninput prompt, including how it is tokenized and then processed by the transformer architecture to produce a set of output\nprobabilities.\nis carried out by the LLM’s tokenizer, which is a model in its\nown right. For instance, the WordPiece tokenizer (Wu et al.,\n2016) underlying the popular class of BERT models (Bidi-\nrectional Encoder Representations from Transformers; De-\nvlin et al., 2018) breaks up the sentence \"Open-source LLMs\nrock.\" into \"[CLS]\", \"open\", \"-\", \"source\", \"ll\", \"##ms\",\n\"rock\", \".\", and \"[SEP]\". This example illustrates that to-\nkens are often words, but not always. They can also be punc-\ntuation (\"-\" and \".\"), subwords (\"ll\" and \"##ms\"), and spe-\ncial tokens (\"[CLS]\"—shorthand for \"classification\"—and\n\"[SEP]\"—for \"separator\"). Tokenizers can di ffer between\nmodels and include lower- and upper-case tokens.\nThere are several principles behind tokenization. First,\nincluding punctuation helps the LLM represent the logical\nstructure of sentences and produce text that contains sentence\nclauses or multiple sentences. Second, the use of subwords\nsignificantly reduces the number of tokens the LLM must\nlearn by assigning stand-alone tokens only to frequent words\nand constructing all other words using subword tokens. Note\nthat subword tokens that do not start a word begin with \"##\"\nto signify that they follow the previous token without a space.\nWhether a word is assigned a stand-alone token or decom-\nposed into subwords depends on the specific algorithm and\nthe text corpus used by the algorithm to identify the most\neffective set of tokens. Which words the tokenizer assigns\nstand-alone tokens to has been found to have important im-\nplications for what an LLM learns (Ali et al., 2023). Third,\nplacing special tokens at the beginning (\"[CLS]\") and end\n(\"[SEP]\") of texts enables the LLM to predict the first word\nand the end of a text and to learn numerical representations\nthat can stand in as a representation of the entire text.\nBefore we continue, we should note that consistent with\nmuch of the literature on large language models (Devlin et\nal., 2018, e.g., ), our use of the term \"token\" does not fol-\nlow the distinction between types and tokens in philosophy\nand linguistics, where tokens refer to specific instances or\noccurrences of types (Wetzel, 2018). Thus, here and in the\nliterature on LLMs, the term \"token\" refers to both types and\ntokens.\nInput Embeddings\nAfter tokenization, each token is assigned a numeric vec-\ntor. Prior to training, this simply consists of random num-\nbers. In the case of BERT or GPT-2 models, these vectors\nconsist of 768 numbers (dimensions). The embedding vec-\ntors mark the starting point of the neural network and are\nlearned during training, where their values are adjusted iter-\natively to assist the model in achieving its objective. After\ntraining, the input embeddings reflect, in an abstract fashion,\na context-general meaning of each token relative to the other\ntokens.\nIn contrast to other approaches to semantic representation\n(Li, 2022; Siew et al., 2019), such as those treating words\nor tokens as nodes in a network, vector-based embeddings\nhave two key advantages. First, they permit LLMs to be\nmore efficient: The number of embedding dimensions is typ-\nically at least an order of magnitude smaller than the number\nof tokens, resulting in substantially fewer parameters being\nneeded to represent the relationships between tokens. For in-\nstance, the WordPiece tokenizer used by BERT encompasses\nabout 30,000 tokens, which is roughly 40 times more than\nthe number of embedding dimensions (i.e., 768). Second,\nthey assist the model in performing generalization: The em-\nbedding vectors encode the relationships between tokens to\none another, such that the model shows similar behavior for\nsimilar tokens.\nInput embeddings do not reflect the location of tokens\nwithin a given input or context. To capture order, most\nLLMs, such as GPT-2 and successors, include another com-\nponent called positional encoding. This is a second token\nembedding that represents the relative position of tokens us-\ning a combination of sine and cosine functions. The posi-\ntional encoding is typically added to the input embedding\nto form embeddings that reflects both the context-general\nmeaning of a token and its position in the input to the model.\nAttention Blocks\nThe attention block is the central building block of trans-\nformer models and is what distinguishes them from other\nneural network-based language model architectures, such as\nWord2Vec(e.g., Mikolov et al., 2013) and recurrent neural\nnetwork-based language models (Graves, 2012). The pur-\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 3\n[CLS]\nopen\n-\nsource\nll\n##ms\nrock\n.\n[SEP]\n[CLS]\nopen\n-\nsource\nll\n##ms\nrock\n.\n[SEP]\n[CLS]\nopen\n-\nsource\nll\n##ms\nrock\n.\n[SEP]\n[CLS]\nopen\n-\nsource\nll\n##ms\nrock\n.\n[SEP]\nQueries\nKeys\nValues\nEmbedding  \n+ Pos weighting[CLS]\nopen\n-\nsource\nll\n##ms\nrock\n.\n[SEP] Attention matrix\ndot \nproduct \nsoft \nmax \nContextualized \nembeddingAttention heads\n+ +\nNon-linear \nfeed-forward\nlinear\nlinear\ndot \nproduct\nnon- \nlinear linear\nLinear \nfeed-forward\nLinear \nfeed-forward\nN embedding dimensions\nconcat\nconcat\nconcat\nHead 1\nHead 2 Head 3 Head 4\nFigure 2 Transformer attention block. The diagram illustrates how the input embeddings are passed to multiple attention\nheads, each performing a series of operations to generate queries, keys, and values, and leading ultimately to contextualized\nembeddings.\npose of the attention block is to produce embeddings that\nrepresent the in-context meaning of tokens. For example,\nconsider again the sentence \"Open-source LLMs rock.\" After\ninput embedding and positional encoding, each token in the\nsentence is represented using a context-general embedding\nvector. These context-general embeddings reflect the mean-\ning of tokens broadly, not considering the specific context in\nwhich they occur. However, the meaning of tokens can vary\nconsiderably across contexts: consider, for example, the pol-\nysemous \"rock.\" The transformer architecture uses the atten-\ntion mechanism to capture these context-specific meanings.\nThe components of the attention block are illustrated in\nFigure 2. It begins with the token embeddings, which, in\nthe case of the first attention block, are the sum of the in-\nput embeddings and the positional encoding, normalized to\nhave a zero mean and unit variance. Entering the attention\nmechanism, these embeddings are transformed by a linear\nlayer into three new, lower-dimensional embeddings called\nqueries, keys, and values. This transformation can be likened\nto a method—principal component analysis— known to be-\nhavioral researchers in that it produces output variables that\nare lower dimensional linear combinations of the input vari-\nables. The names of these three smaller embeddings sug-\ngest that they can be likened to a retrieval, where a query is\ncompared to keys to identify a value matching the query. Al-\nthough this analogy should not be taken too literally, it does\nreflect how the queries, keys, and values collaborate to pro-\nduce contextualized embeddings for each token. Specifically,\nthe queries and keys combine to determine how to recombine\nthe values to build contextualized embeddings.\nComputationally, the attention mechanism works as fol-\nlows. First, the dot product of each pair ofqueries and keys is\ncomputed, forming a square matrix of attention scores. The\nattention scores are then normalized by √dk, to account for\nthe dimensionality d of the keys k. These normalized atten-\ntion scores are next fed row-wise into the softmax function\nexi / P\nj exj , where each xi is a scalar attention score on key\ni, to produce attention weights w i j. Finally, the attention\nweights of each row, which now add to 1, are used to pro-\nduce a weighted sum P\nj wi j ∗vj of the values v of all tokens.\nThese weighted sums are the new contextualized embeddings\nfor each token. This attention mechanism can also be repre-\nsented using the following matrix notation\nAttention(Q, K, V) = softmax\n QKT\n√dk\n!\nV\nwhere Q, K, and V are matrices respectively containing the\nqueries, keys, and values, and T denotes the matrix trans-\npose. This process of generating contextualized embeddings\nby mixing values as prescribed by the queries and keys does\nnot per se produce useful contextualized embeddings. This\nis only achieved in training, which allows the model to figure\nout how to construct queries, keys, and values such that the\ncontextualized embedding helps it achieve the model objec-\ntive, such as predicting the next unit in a sequence (see sec-\ntion Model Output and Training). In other words, the model\nuses the attention mechanism to learn how context can help\nit solve a prediction problem.\nThe next step in the attention block is to gather the contex-\ntualized embeddings from the multiple attention mechanisms\nthat are executed in parallel across several attention heads.\nRunning multiple attention mechanisms in parallel permits\n4 HUSSAIN, BINZ, MATA, & WULFF\nthe model to produce different contextualized embeddings—\nbased on different queries, keys, and values—that may focus\non different kinds of relations between tokens. However, it\nshould be noted that these relations are typically not human-\ninterpretable (e.g., Vig, 2019; Vig & Belinkov, 2019). Figure\n2 shows , for illustrative purposes, four attention heads; how-\never, current models usually have significantly more. The\ncontextualized embeddings from the four attention heads are\nconcatenated such that the final embedding for each token is\na combination of the embeddings produced by the di fferent\nheads. This final contextual embedding has the same dimen-\nsionality as the input embeddings (i.e. 768 in BERT or GPT-\n2 models).\nThe attention block further processes the contextualized\nembeddings in several steps. First, they are passed through\na linear layer. They are then added to the initial embeddings\nthat entered the attention heads. This addition is called a\nskip connection and is thought to help stabilize the model\nduring training. After the skip connection, the embeddings\nare passed through a larger layer with a nonlinear activation\nfunction that plays two important roles. First, through its\nnonlinearity, it provides considerable flexibility in process-\ning and recombining the contextualized token embeddings.\nSecond, through its larger number of nodes, it provides a\nnumber of weights (parameters) that enhance the network’s\nmemory capacity. In the case of BERT models, this nonlinear\nlayer is four times larger than the token embeddings, imply-\ning an upscaling to 3,072 dimensions and over two million\nweights connecting the two layers. After the large, nonlin-\near layer, the token embeddings are scaled back down to the\nstandard embedding size using a linear layer, so as to match\nthe required input size for the next model block, and passed\nthrough another skip connection prior to the nonlinear layer\nto provide stability.\nUltimately, the attention block produces embeddings for\neach token that are the same size as the initial embeddings\nbut are now contextualized such that each token’s embedding\nis a complex recombination with the other tokens’ embed-\ndings. Following the first attention block, most transformer\nmodels add additional attention blocks that take the contex-\ntualized embeddings from the previous block as input. As a\nresult, transformer models produce several layers of abstrac-\ntion where the final contextualized embeddings are combina-\ntions of recombinations.\nModel Heads and Training\nThe final component of the model is the model head. This\nproduces the model output, which can be adjusted to numer-\nous tasks. During pre-training—that is, the initial phase of\ntraining a language model on a large corpus by learning lin-\nguistic patterns and relationships within the data—the model\nhead usually performs token classification. This means pre-\ndicting one or more of the tokens in the model’s vocabu-\nlary based on the model input. There are two dominant ap-\nproaches to pre-training LLMs through token classification:\nmasked language modeling and causal language modeling .\nOf course, given the availability of high-quality, pre-trained\nLLMs, we believe most behavioral scientists will have little\nreason to train their own LLM from scratch. Furthermore,\nthe computational resources and technical expertise required\nto do so can be prohibitive. However, discussing these two\nmodes of training will both help in illustrating the role of a\nmodel head (see Figure 3) as well as provide some necessary\nbackground for making informed decisions about which kind\nof pre-trained model to use for the task at hand (see section\nChoosing Between Models).\nIn masked language modeling , a special \"[MASK]\" to-\nken is inserted into the token sequence in place of one or\nmore randomly selected tokens. For instance, in our ex-\nample sentence, replacing the word \"rock\" would result in\nthe token sequence \"[CLS]\", \"open\", \"-\", \"source\", \"ll\",\n\"##ms\", \"[MASK]\", \".\", \"[SEP]\" as model input. As with\nany other token, the model will produce an embedding for\nthe \"[MASK]\" token that reflects its context. This contex-\ntual information can thus be leveraged by the model head to\npredict the masked token. To achieve this, the model head\ntakes the \"[MASK]\" token’s final contextualized embedding,\npasses it first through a final hidden layer, which can be lin-\near or nonlinear, and then into the final linear layer with a\nsoftmax output that has as many output nodes as there are\ntokens in the model’s vocabulary. As in the attention mech-\nanism, the softmax function produces values that sum to 1,\nmeaning they can be interpreted as probabilities. The model\nhead uses these probabilities to predict which token is behind\nthe \"[MASK]\" token. Before pre-training, these predictions\nwill be no better than chance. However, during training, the\nmodel will incrementally adjust its parameters (\"weights\")\nin a direction that produces higher probabilities for tokens\nthat are behind the \"[MASK]\" token and lower probabilities\nfor those that are not. In our example sentence, this would\nmean learning to assign a higher probability for the target,\n\"rock\", and other tokens plausibly fitting the context, such\nas \"impress\" or \"excel\", and a lower probability for unfitting\ntokens, such as \"cat\", \"taste\", or \",\".\nThe second and now more popular way to pre-train LLMs\nis known as causal language modeling (or autoregressive\nmodeling). In this mode of training, the model head also\nperforms token classification. However, instead of predict-\ning a masked token from its complete surrounding context,\nthe model is trained to predict the next token in a sequence\nbased only on the tokens that preceded it (i.e., it does not\nget access to future tokens). To perform this kind of train-\ning, causal language models use di fferent tokenizers with-\nout a \"[MASK]\" token. Models trained using causal lan-\nguage modeling also implement a di fferent type of attention\nmechanism that manually sets all attention to future tokens\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 5\n##ms\nMasked language modeling in Encoders\nll\n[CLS]\nopen\n-\nsource\n[MASK]\n##ms\n.\n[SEP]\n[CLS]\nopen\n-\nsource\nll\n##ms\n[MASK]\n.\n[SEP]\nAttention block\n[MASK]\nModel head\nContextualized \nembedding\nBi-directional \nattention matrix\nOutput \nlayer\nLinear \nlayer\nMasked \nembedding\nselect\n[CLS]\nthink\n##tablished\nexcel\nll\noffer\nrock\ntaste\nleaders\n,\ncat\nenable\nsucceed\nimpress\nmother\n[SEP]\nlinear softmax\nCausal language modeling in Decoders\nopen\n-\nsource\nll\n##ms\nrock\n.\nAttention block Model head\nContextualized \nembedding\nCausal \nattention matrix\nOutput \nlayer\nLinear \nlayer\nMasked \nembedding\nselect\n[CLS]\nthink\n##tablished\nsucceed\nll\noffer\nrock\ntaste\nleaders\n,\ncat\nenable\nexcel\nimpress\nmother\n[SEP]\nlinear softmax\nmasked\n##ms\nopen\n-\nSource\nll\nFigure 3 Overview of pre-training mode and transformer family. The figure illustrates two pre-training modes (masked lan-\nguage modeling, causal language modeling) and associated architecture family (encoder, decoder).\nto zero. This means that each token’s contextualized embed-\nding is composed only of its ownvalue vector and the values\nof the tokens preceding it. To predict the next token, the\nmodel head selects the contextualized embedding of the last\navailable token of the input (which is used analogously to the\n\"[MASK]\" token in masked language modeling). For exam-\nple, to predict the token \"rock\" based on preceding tokens,\nthe model head would select the contextualized embedding\nof the \"##ms\" token, including information about the pre-\nceding tokens and itself, and predict which of all possible\ntokens likely follow. After training, the model will assign\nhigh probabilities to suitable tokens, such as \"enable\", \"of-\nfer\", or the target token \"rock\", but not to unsuitable tokens.\nImportantly, these will not be the same tokens predicted in\nmasked language learning, due to the difference in the infor-\nmation accessible to the model. Specifically, not being able\nto look into the future, a model trained through the causal\nlanguage mode would likely assign some probability to the\ntoken \",\" to mark the end of a sentence clause. This is an\nunlikely prediction for a model trained in masked language\nmode, which would have access to the period token \".\" later\nin that sequence.\nFinally, it is important to note that token classification, as\nemployed in masked and causal language modeling, is not\nthe only pre-training objective, nor are masked and causal\nlanguage modeling the only modes of pre-training. One other\npre-training mode for transformer models is next-sentence\nprediction. In this mode, the model head is set up to predict a\nsingle numerical value between 0 and 1, reflecting the prob-\nability with which two input sentences occurred adjacently\nin the training data. Next-sentence prediction is used in the\npertaining of some BERT-style models, typically in addition\nto masked language learning. Next-sentence prediction il-\nlustrates that the transformer model head can be adjusted to\npredict different data types. This flexibility is exploited fre-\nquently, for instance, in the fine-tuning of LLMs to perform\nspecific tasks based on smaller, task-related datasets (see sec-\ntion Fine-tuning).\nAn Overview of Model Types\nSince the inception of the transformer architecture\n(Vaswani et al., 2017), many model variants have emerged\nthat differ in important ways, including the architecture fam-\nily (i.e., encoder, decoder, or encoder–decoder), model size,\nstage of training reached by the model, and openness.\nConcerning the architecture family, it is helpful to distin-\nguish the encoder, decoder, and encoder–decoder architec-\ntures (see Figure 3). The encoder architecture is character-\nized by bidirectional attention, pre-training through masked\nlanguage modeling (and, sometimes, next-sentence predic-\ntion), and the use of special tokens such as \"[CLS]\", \"[SEP]\",\nand \"[MASK]\". The goal of the encoder architecture is to\nproduce accurate contextualized embeddings, including for\nthe special tokens. Prominent examples following the en-\ncoder architecture are the models of the BERT family (e.g.,\nDistilBERT (Sanh et al., 2019) or RoBERTa (Liu et al.,\n2019)), the instructor models (Su et al., 2022), and Open AI’s\nAda model (Green et al., 2022, December 15). The decoder\narchitecture, on the other hand, is characterized by causal\nattention and pre-training through causal language model-\ning. The goal of the decoder architecture is to generate text\nvia next-token prediction. Prominent examples of the de-\ncoder architecture are the GPT (OpenAI, 2023) and LLaMA\nmodel families (Touvron et al., 2023). Finally, the encoder–\ndecoder architecture is characterized by a combination of the\ntwo, and is the original transformer architecture as proposed\nin Vaswani et al. (2017). It is trained with next-token pre-\ndiction, where the input text is first fed to the encoder, the\nencoder’s last hidden state is then passed as input to the de-\ncoder, which then predicts the next token. Prominent ex-\namples of the encoder–decoder architecture are the BART\n(Lewis et al., 2019) and T5 (Raffel et al., 2020) models.\n6 HUSSAIN, BINZ, MATA, & WULFF\nA second key differentiating factor between LLMs is size.\nSize is often measured in terms of the number of weights\nin a model, which can vary between a few hundred mil-\nlion (e.g., most BERT models) and several hundred billion\n(Smith et al., 2022, e.g., Megatron Turing NLG)—or even\nthe trillion weights supposedly reached by OpenAI’s GPT-4\nmodel. Although the number of weights plays a large role\nin determining a model’s capacity to learn from the training\ndata, how the weights are distributed throughout the various\nmodel components also matters (Kaplan et al., 2020). The\nsize of LLMs can also di ffer in a more functional way, by\nallowing for different context sizes. The context size is the\nmaximum number of tokens in a sequence that the attention\nmechanism can evaluate at any given time, and it determines\nthe complexity of connections between tokens that the model\ncan consider. This is important for applications such as few-\nshot learning (see section Applications of LLMs in Behav-\nioral Science). From a practical perspective, context size also\ndetermines the amount of random-access memory (RAM)\nneeded to run a model. For large decoder models such as\nLLaMA-2 (70 billion weights), RAM requirements can be\nas high as 300 gigabytes, resulting in a need for expensive,\nhigh-performance graphical processing units (GPUs).\nA third di fferentiating factor is the stage of training\nreached by the model. First, there are pre-trained models,\nwhich have been trained on a large corpus of text using\nmasked or causal language modeling. The text corpus typ-\nically includes websites, which in turn include blogs, news\narticles, Wikipedia, social media platforms (e.g., Reddit),\nand other sources of text (e.g., books, academic articles).\nLarger pre-trained models are sometimes also called foun-\ndation models (Bommasani et al., 2023), emphasizing their\npurpose as a basis for task-specific training. Second, there\nare fine-tuned models, which are pre-trained models that have\nbeen further trained on task-specific data to selectively in-\ncrease their performance on certain tasks. These can be ba-\nsic tasks, such as token classification or prediction of numer-\nical variables, or more complex tasks, such as named-entity\nrecognition or question answering.\nA fourth di fferentiating factor that concerns the set of\nfine-tuned models and has played an especially important\nrole in the growth in popularity of LLMs in recent times is\nwhether the model is a \"chat\" model. Specific fine-tuning\nregimes exist to make pre-trained models better suited to\nhigh-quality, assistant-style interactions through a chat inter-\nface. These include training steps with explicit human in-\nput such as supervised fine-tuning and reinforcement learn-\ning from human feedback (Christiano et al., 2017; Touvron\net al., 2023). In supervised fine-tuning, human \"annotators\"\ngenerate prompts and appropriate assistant-style responses to\nthose prompts, such that the model may learn via \"imitation\"\nto become a good assistant. Reinforcement learning from\nhuman feedback is a more complex, multistage procedure in\nwhich human annotators indicate their preferences between\nmodel outputs according to specific criteria (e.g., safety or\nhelpfulness) to build a \"preference dataset\" (stage 1) (Tou-\nvron et al., 2023). This dataset is then used to train a reward\nmodel that learns the annotators’ preferences (stage 2), which\nin turn provides feedback on the outputs of the LLM in much\nvaster quantities than would be possible with human annota-\ntions alone. This enables the LLM to learn via reinforcement\nto become a better assistant. Such fine-tuning can be seen\nas an example of how LLMs can be tailored to specific be-\nhavioral applications. A prominent example of a chat model\nis ChatGPT, but other open-source models exist, including\nLLaMA-2 Chat (Touvron et al., 2023) or Falcon Chat (TII,\n2023).\nA fifth and final di fferentiating factor is openness. LLMs\ndiffer in terms of how much information is available about\nthe training data, training method, or architecture (see Bom-\nmasani et al., 2023) and how openly available the models\nthemselves are. Some models, such as GPT-4, are only\navailable through remote user interfaces, whereas others are\nmostly (e.g., LLaMA) or fully (e.g., BERT) open-source.\nMost open-source models can be accessed and employed\nvia the Hugging Face ecosystem introduced in this tutorial,\nwhich has significant advantages over closed-source models\nin terms of transparency and reproducibility.\nApplications of LLMs in Behavioral Science\nLLMs can be employed in several ways in behavioral sci-\nence. The most basic but often e ffective mode is feature ex-\ntraction (e.g., Aka & Bhatia, 2022; Binz & Schulz, 2023a;\nHussain et al., 2023; Wul ff & Mata, 2023). Feature extrac-\ntion sends text input through the model and records contextu-\nalized embeddings, typically at the final layer. The resulting\nembedding vectors can then be utilized in many ways. For\nexample, the use cases presented in the next sections demon-\nstrate how feature vectors can be used to predict the simi-\nlarity between personality constructs, choices in reinforce-\nment learning tasks, or numerical judgments such as risk or\nhealth perception ratings. Feature extraction is commonly\nperformed using encoder models with bidirectional attention\n(Muennighoff et al., 2022), which allows them to better uti-\nlize all available information during pre-training. However,\nwhen the goal is to predict the next element in a sequence, de-\ncoder models can be equally or more e ffective, also because\ncurrent decoder models are significantly larger and trained\non more data than encoder models.\nAnother class of applications utilizes the model’s ability to\npredict outcomes with no or minimal supervision. The use of\ntransformer models without any kind of task-specific training\nto predict verbal or numerical outcomes is called zero-shot\nlearning. This approach can be used to generate categorical\nand numerical predictions—for example, to predict the cat-\negory of a news article or the sentiment of a social media\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 7\npost (e.g., Gilardi et al., 2023; Pelicon et al., 2020; Widmann\n& Wich, 2023). Zero-shot learning can be performed using\nall three types of transformer architectures, with some differ-\nences in implementation. Few-shot learning is an extension\nof zero-shot learning where minimal supervision is provided\nin the model input—for instance, in the form of a handful\nof input–output pairs. In classifying a social media post’s\nsentiment, this would imply including pairs of the post’s text\nand known sentiment in the model input. Few-shot learn-\ning can, in principle, be used with any model type; however,\nit is most commonly employed using modern decoder mod-\nels, which tend to contain more parameters and show better\nperformance on few-shot tasks (Brown et al., 2020).. O ff-\nthe-shelf, large-scale decoder models, such as GPT-4, pro-\nvide good zero-shot and few-shot performance (Rathje et al.,\n2023; Törnberg, 2023).\nHowever, fine-tuning smaller language models on a spe-\ncific task can result in equally good or even better perfor-\nmance relative to the zero- or few-shot performance of larger\nmodels (Chae & Davidson, 2023; Rosenbusch et al., 2023;\nT. Wang et al., 2022). Unlike zero- or few-shot learning—\nboth of which leave model parameters unchanged during the\n\"learning\" phase—fine-tuning involves explicit updating of\nmodel weights with the goal of improving task-specific per-\nformance. As a result, fine-tuning is an important strategy\nfor applications in behavioral science (e.g., Demszky et al.,\n2023).\nThe final class of application is text generation . This\napplication is specific to encoder-decoder and decoder-only\nmodels. Text generation can be used to perform various tasks\nincluding summarization, question answering, or simply free\ntext generation. Some examples of the use of free text gen-\neration include comparing reasoning abilities in LLMs and\nhumans (Webb et al., 2023; Yax et al., 2023) and simulat-\ning the responses of study participants (Argyle et al., 2023).\nThese simulations need not be constrained only topredicting\nhuman behavior, but could also be used to suggest explana-\ntions for why people behaved a certain way (Bhatia, 2023).\nChoosing Between Models\nGiven the various model types available and their differing\napplications in behavioral science, one natural question that\nfollows is which model to use for the application at hand.\nIn view of the constantly developing architectural landscape\nand the important role played by task-specific properties, it is\ndifficult to predict model success. Nevertheless, some heuris-\ntics tend to hold.\nFirst, larger models tend to perform better than smaller\nones (Kaplan et al., 2020). On the other hand, they also de-\nmand more computational resources. Modern large language\nmodels typically exceed the capacity of personal computers,\nrequiring access to either high-performance clusters or cloud\ncomputing services. These requirements can be alleviated\nwith quantization (e.g., Frantar et al., 2022; Ma et al., 2024),\nreducing a model’s numerical precision, or knowledge dis-\ntillation (Hinton et al., 2015), transferring a model’s repre-\nsentations into a smaller model. However, these approaches\nusually come with some loss in performance. Ultimately,\nchoices concerning model size must factor in computational\nresource availability.\nSecond, decoder models are more suited than en-\ncoder models to tasks where tokens must be predicted in\nsequence—that is, in the order that the tokens would be writ-\nten (see, e.g., section Extracting Token Probability and Per-\nplexity). This is due to their causal language modeling pre-\ntraining objective, which prevents them from \"cheating\" by\nhaving access to future tokens when predicting the present\ntoken. On the flip side, when representations of the con-\ntext (before and) after a given token of interest are desired\n(see, e.g., sections Relating Personality Measures and Pre-\ndicting Health Perception), encoder models typically outper-\nform similarly sized decoder models. This is often the case\nfor feature extraction tasks unless the goal is to generate pre-\ndictions that are more sequential in nature (see, e.g., section\nPredicting Repeated Choice), in which case a decoder model\nmay be better suited.\nThird, chat models that have been fine-tuned with\nassistant-oriented regimes, such as reinforcement learning\nfrom human feedback, are often better at producing coher-\nent human-like output. This makes them more useful for a\nvariety of tasks, such as text labeling, information retrieval,\nand text generation. The latter includes applications such as\ngenerating study materials or simulating human participants\nin behavioral experiments (see section Text Generation).\nIn addition to these heuristics, several empirical bench-\nmarks exist that can help with selecting the right model\nfor a task. There are separate benchmarks for various\ntasks, covering both feature extraction (e.g., hugging-\nface.co/spaces/mteb/leaderboard) and text output (e.g.,\nhuggingface.co/spaces/lmsys/chatbot-arena-leaderboard).\nAn overview of benchmark results for open models ac-\ncessible through Hugging Face can be accessed at hug-\ngingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.\nGenerally, it is useful to select the model whose benchmark\nperformance is high for tasks similar to the task at hand. Of\ncourse, benchmark performance may fail to generalize.\nThe Hugging Face Ecosystem\nThe Hugging Face ecosystem has two main components:\nan online hub (\"the Hub\", huggingface.co /docs/hub) and a\nset of Python libraries (as illustrated in Figure 4, hugging-\nface.co/docs). Hosting over 300,000 models and 60,000\ndatasets, the Hub constitutes an impressive community-\ndriven effort to democratize language modeling, as well as\nother types of modeling, such as computer vision. Further-\nmore, thanks to Hugging Face’s Python libraries, many of the\n8 HUSSAIN, BINZ, MATA, & WULFF\nFigure 4 The main components of the Hugging Face ecosystem. The figure is adapted from Tunstall et al. (2022).\nsteps traditionally required to implement LLMs have become\nconsiderably more accessible, often requiring just a few lines\nof code each. These steps typically include processing the\ndata, initializing the model, and applying the model to a spe-\ncific task. This section introduces some of the more crucial\ncomponents of libraries, such as datasets, tokenizers,\ntransformers, and accelerate, and outlines how these\ncomponents can help with these tasks.\nThe first step in almost any language modeling pipeline is\ndata processing. This usually involves data loading, cleaning,\nand reshaping. Because natural language processing (NLP)\ndatasets can sometimes be in the order of tens or even hun-\ndreds of gigabytes, they cannot always be loaded into RAM\nor stored on the hard drive.datasets addresses this issue by\nenabling users to convert their data to Apache Arrow format,\nallowing it to be flexibly and e fficiently read from the hard\ndrive or streamed online, thus preventing RAM or hard drive\nstorage overload.\nOnce the dataset has been loaded in datasets.Dataset\nformat, it must be tokenized before it can be fed into a model.\nIt is essential that the method of tokenization is matched to\nthe pre-trained model. Hugging Face’s tokenizers and\nhigher-level API alternatives from the transformers li-\nbrary make it easy to initialize the appropriate tokenizer us-\ning the tokenizer’s.from_pretrained() method. This can\nbe done by passing the model checkpoint—a unique charac-\nter string that identifies the model on the Hugging Face Hub\n(see huggingface.co/models).\nHugging Face models can be loaded in a single line us-\ning the transformers.AutoModel.from_pretrained()\nmethod, and placed on the graphics processing unit (GPU) if\na compatible GPU is available. This can speed up model in-\nference and fine-tuning to such an extent that it may make an\notherwise infeasible task feasible. Training and inference can\nbe further optimized by distributing across multiple GPUs\nwith accelerate. In the examples that follow,accelerate\nworks in the background of the transformers library via\narguments such as device_map=\"auto\" to automatically\noptimize the distribution of resources across processing units\nto allow easy upscaling to larger models.\nIt is important to note that the Hugging Face ecosystem\nis dependent on deep learning libraries such as PyTorch (py-\ntorch.org) or TensorFlow (tensorflow.org), and interacts with\npopular Python libraries such as NumPy (numpy.org) and\nPandas (pandas.pydata.org). Furthermore, it interfaces with\nseveral other Python libraries, including SentenceTransform-\ners (sbert.net/docs/quickstart.html), offering a high-level API\nto obtain embeddings from over 500 models hosted on the\nHugging Face Hub in addition to providing its own pre-\ntrained models.\nIn what follows, we demonstrate how the Hugging Face\necosystem can be used for three types of applications,\nnamely, feature extraction, fine-tuning, and text genera-\ntion, by presenting several use cases in behavioral re-\nsearch with (accompanying code). Comprehensive and\nrichly commented code is available in notebook format at\ngithub.com/Zak-Hussain/LLM4BeSci.git, a GitHub reposi-\ntory with instructions for running the code online in a Google\nColaboratory environment. The repository also provides\na means of keeping the code base for this tutorial up-to-\ndate. Keep in mind that the Hugging Face ecosystem is\nin active development, making it likely that specific as-\npects of the code presented in this paper will be depre-\ncated by the time of reading. We plan to regularly up-\ndate the GitHub repository and respond to update requests,\nwhich can be submitted as GitHub issues at github.com/Zak-\nHussain/LLM4BeSci/issues/new. For further information on\nHugging Face, we suggest the Hugging Face textbook by\nTunstall et al. (2022).\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 9\nFeature Extraction\nRelating Personality Measures\nFeature extraction from LLMs is already being lever-\naged in diverse ways to assist research in personality psy-\nchology (e.g., Abdurahman et al., 2023; Cutler & Con-\ndon, 2023; Wul ff & Mata, 2023). In this example, we\nshow how LLMs can be used to predict the relationship be-\ntween existing personality measures (i.e., personality items\nand constructs). Specifically, we walk the reader through\nan analysis pipeline emulating the work of Wul ff and Mata\n(2023) that used feature extraction to generate item and con-\nstruct embeddings—representations of psychometric items\nand constructs in a vector space obtained from LLMs—and\nthen applied these vectors in downstream tasks, such as sim-\nilarity comparison and clustering, to both validate di fferent\nmodels and tackle the lack of conceptual clarity in personal-\nity psychology (Wulff & Mata, 2023). See github.com /Zak-\nHussain/LLM4BeSci.git to run this example in a Google Co-\nlaboratory environment.\nThe example begins by loading the relevant data, in\nthis case, data concerning the IPIP-NEO-300 five-factor\npersonality inventory (Goldberg et al., 2006), into a\npandas.DataFrame, neo_items (see Table 1). The goal\nin this example will be to obtain item embeddings—\nrepresentations of personality items and constructs—using\nfeature extraction, so that these embeddings can be used to\nestimate the similarity between items and constructs. The\nsimilarity between items and constructs can ultimately be\nused to uncover the structure of psychological constructs and\nto inform the labeling of these measures (Wul ff & Mata,\n2023). The data set used in the example has three columns:\n'item' (the personality item description), 'construct'\n(the personality construct to which the item belongs), and\n'factor' (the Big Five personality factor to which the con-\nstruct belongs). Once the data has been loaded (and any nec-\nessary cleaning and reshaping performed), they are converted\ninto a datasets.Dataset object for efficient storage using\nthe from_pandas() method.\nThe text input is now ready for tokenization. As men-\ntioned earlier (see section Tokenizers), the tokenizer must\nbe consistent with the model to be used downstream. As\nsuch, a model checkpoint ( model_ckpt) must first be de-\nfined. In our example, we use a lightweight version of BERT\n('distilbert-base-uncased') to ensure ease of storing\nand running the model on most hardware setups. However,\nit could easily be replaced by a larger model from the Hug-\nging Face Hub, hardware limitations permitting. With the\nmodel_ckpt specified, the model tokenizer can be loaded\nwith AutoTokenizer.from_pretrained(model_ckpt).\nTokenization is performed efficiently across groups of in-\nputs, called batches, by mapping dat with a user-defined\nbatch_tokenizer function. This takes two important argu-\nments: padding and truncation. padding is used to fill\nup sequences with zeros to match the length of the longest\nsequence in the batch, thus ensuring that all sequences in\nthe batch have the same length. This is essential for train-\ning and inference with deep learning models, which oper-\nate on fixed-size input tensors. Tensors are a generalization\nof vectors, including vectors, matrices, and higher order ar-\nrays. truncation is the process of cutting o ff the end of a\nsequence to ensure that it fits within the model’s maximum\ncontext size. In the case of BERT models, this is 512 tokens.\nIt is worth noting that alternative strategies exist if the to-be-\ndropped part of the sequence is thought to contain useful in-\nformation, including splitting up the sequence into digestible\nbatches and averaging the embeddings across batches.\nAs output, the batch_tokenizer returns a\nPython dictionary with two keys: 'input_ids' and\n'attention_mask'. 'input_ids' maps to a list of\nintegers uniquely representing each token in the sequence.\n'attention_mask, which is not to be confused with\nthe learned attention weights, maps to a list of ones and\nzeros, where the ones stand in place of each token and\nthe zeros pad the sequence to match the longest in the\nbatch due to padding. For instance, the personality item\n'Go straight for the goal.' tokenizes to:\n{\n# ['[CLS]', 'go', 'straight', 'for', 'the', 'goal', '.', '[SEP]'\n'input_ids': [101, 2175, 3442, 2005, 1996, 3125, 1012, 102],\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, ..., 0]\n}\nwith the 'input_ids' referring to the tokens \"[CLS]\", \"go\",\n\"straight\", \"for\", \"the\", \"goal\", \".\", and \"[SEP]\". The final\npre-processing step involves converting the data to the Py-\nTorch (torch) format such that they can be passed to the\nmodel.\nThe data are now ready to be fed into the model. Model ar-\nchitecture and pre-trained weights are loaded in a single line\nwith AutoModel.from_pretrained(model_ckpt). The\ncode next detects whether a Compute Unified Device Ar-\nchitecture (CUDA)-compatible or Apple Metal Performance\nShader (MPS)-compatible GPU is available using PyTorch\nand, if so, sets the device to the GPU. Otherwise, the CPU is\nused. The model is then moved to the device with the to()\nmethod.\nData can be more efficiently fed into the model in batches\nand this is done automatically using the Dataset.map()\nmethod. To extract the features (i.e., the numerical vector\nrepresentations of the inputs), the researcher must define a\nfunction extract_features(). It takes batches of dat as\ninput and selects the columns containing the model inputs\nby checking whether the column name is referenced in a\nlist accessed via tokenizer.model_input_names. In this\ncase, the list contains two model inputs: 'input_ids' and\n'attention_mask'. These are then input to the model with\ngradient calculations disabled by torch.no_grad(). This\n10 HUSSAIN, BINZ, MATA, & WULFF\ntext construct factor\nGo straight for the goal. Achievement-Striving Conscientiousness\nPlunge into tasks with all my heart. Achievement-Striving Conscientiousness\nRemain calm under pressure. Vulnerability Neuroticism\nTable 1 Three example items from IPIP-NEO-300 five-factor personality inventory (Kajonius& Johnson, 2019).\nFigure 5 Personality psychology application. ( A) Correlations between predicted versus observed item similarities and ( B)\npredicted versus observed construct similarities based on embeddings from DistilBERT, Instructor (instructor-xl), Cohere\n(Cohere-embed-english-v3.0), and ada (text-embedding-ada-002) models. C. Pairwise controlled manifold approximation\nprojection (Y. Wang et al., 2021) of Instructor-XL construct-level features. Colors reflect the Big Five personality factor to\nwhich the construct belongs. Error bars represent 95% confidence intervals.\nis done for e fficiency reasons: By default, torch models\nbuild a computational graph in the background in order to\nperform gradient descent. Because feature extraction only\nperforms a forward pass through the model (i.e., there is no\nweight updating), no computational graph is required.\nFinally, the extract_features() function extracts\nthe activations of the last layer of the model via\nmodel(**inputs).last_hidden_state. This returns a\ntensor of shape batch_size, n_tokens, hidden_dim—\nin this case 8, 16, 768, respectively—due to being passed\nthrough dat.map() with batch_size=8, padding=True ,\nand the number of embedding dimensions in the model be-\ning 768. It is worth mentioning that because padding=True\npads to the length of the longest sequence in the batch,\nn_tokens will not always equal 16. From this tensor, the\nfirst token features at position 0 are selected, moved back\nonto the CPU, and converted to a NumPy array per the re-\nquirement of Dataset.map(). The first token is the special\n\"[CLS]\" token, inserted at the beginning of each input. It is\nknown to produce a holistic representation of the content of\nthe entire input and is therefore a common target in feature\nextraction (Tunstall et al., 2022). However, it should be noted\nthat other feature extraction strategies exist than those focus-\ning on the \"[CLS]\" token, such as taking the average of all\noutput representations (known as \"mean-pooling\", see, e.g.,\nReimers & Gurevych, 2019).\nThe function is then applied using the dat.map()\nmethod, which runs the items in batches of eight through the\nfunction. Depending on the researcher’s RAM constraints,\nthe batch_size could be increased. Finally, the features are\nconverted into a pandas.DataFrame for later downstream\nuse.\nIn our example, the application of item embeddings in-\nvolves similarity comparison between personality items.\nThe similarity between the items is evaluated by passing\ndat['hidden_state'], containing the features extracted\nfor each item, to sklearn’s cosine_similarity() func-\ntion. This function computes the cosine similarity, a mea-\nsure commonly used to evaluate the similarity between em-\nbedding vectors, for each pair of items and returns a square\nNumPy array of pairwise item similarities.\n############ Data processing ###########\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n# Loading data and converting to HF Dataset\nneo_items = pd.read_csv(\n'NEO_items.csv', usecols=['construct', 'item']\n)\ndat = Dataset.from_pandas(neo_items)\n# Tokenizing\nmodel_ckpt = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 11\nbatch_tokenizer = lambda batch: tokenizer(\nbatch['item'], padding=True, truncation=True\n)\ndat = dat.map(batch_tokenizer, batched=True)\n# Setting to torch format\ndat.set_format(\n'torch', columns=['input_ids', 'attention_mask']\n)\n######### Feature extraction ###########\nfrom transformers import AutoModel\nimport torch\n# Initialising model and moving to the GPU\nmodel = AutoModel.from_pretrained(model_ckpt)\nif torch.cuda.is_available():\ndevice = torch.device('cuda')\nelif torch.backends.mps.is_available():\ndevice = torch.device('mps')\nelse:\ndevice = torch.device('cpu')\nmodel = model.to(device)\n# Extracting features\ndef extract_features(batch):\ninputs = {\nk:v.to(device) for k, v in batch.items()\nif k in tokenizer.model_input_names\n}\nwith torch.no_grad():\nlast_hidden_state = model(**inputs).last_hidden_state\nreturn {\n\"hidden_state\": last_hidden_state[:,0].cpu().numpy()\n}\n# Apply feature extraction function to data\ndat = dat.map(extract_features, batched=True, batch_size=8)\n# Calculating the cosine similarity between items\nsims = cosine_similarity(dat['hidden_state'])\nBefore we discuss the actual use of the cosine similari-\nties obtained, it should be noted that there are higher-level\nAPI alternatives to feature extraction, such as the Sentence-\nTransformers library (sbert.net/docs/quickstart.html) and the\nHugging Face transfomers.pipeline abstraction. There\nare, however, two reasons for introducing feature extraction\nas we do above. First, even if the researcher opts for the\nhigher-level API in their own work, the lower-level code\ncan give them a better understanding of what is happen-\ning in the background. For instance, it highlights the im-\nportance of tokenization, allows researchers to easily in-\nspect which inputs get fed into the model ( 'input_ids'\nand 'attention_mask'), and indicates which features get\nextracted (the last hidden state). This background under-\nstanding can be crucial for debugging code and knowing\nhow to appropriately adjust it to the research context. Sec-\nond, and almost by definition, the lower-level API has the\nadvantage of greater customizability. That being said, due\nto their simplicity, higher-level APIs will often be the best\noption for researchers wishing to implement their own fea-\nture extractor. For the sake of brevity and consistency,\nwe demonstrate how this can be done with the higher-level\ntransformers.pipeline API, and refer readers to the\nSentenceTransformers library for an alternative approach.\nThe pipeline object, pipeline, achieves the same re-\nsults as the example above but with considerably less code.\npipeline takes 'feature-extraction' as a positional\nargument upon initialisation. In the same line, the desired\nmodel and corresponding tokenizer can be loaded by speci-\nfying model=model_ckpt and tokenizer=model_ckpt as\narguments. Rather than moving the data and model onto the\nGPU with the to() method, this is all done in the back-\nground by setting device=device. PyTorch is specified as\nthe chosen deep-learning framework for running the model\nwith framework='pt'. Finally, feature extraction is run by\npassing the personality items as a list of strings to the now\ninitialized feature_extractor, with tokenization options\nsuch as padding and truncation provided as a dictionary ar-\ngument via tokenize_kwargs. The feature extraction re-\nturns a list of tensors with each tensor corresponding to a\nsample (i.e., personality item), and with the \"[CLS]\" features\naccessible at index [0, 0].\nimport pandas as pd\nfrom transformers import pipeline\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\n# Loading data\nneo_items = pd.read_csv(\n'NEO_items.csv', usecols=['construct', 'item']\n)\n# Switching to GPU if available\nif torch.cuda.is_available():\ndevice = torch.device('cuda')\nelif torch.backends.mps.is_available():\ndevice = torch.device('mps')\nelse:\ndevice = torch.device('cpu')\n# Loading the feature extraction pipeline\nmodel_ckpt = 'distilbert-base-uncased'\nfeature_extractor = pipeline(\n'feature-extraction', model=model_ckpt, tokenizer=model_ckpt,\ndevice=device, framework='pt', batch_size=8\n)\n# Extracting the features for all items\nfeatures = feature_extractor(\nneo_items['item'].to_list(), return_tensors='pt',\ntokenize_kwargs= {'padding': True, 'truncation': True}\n)\n# Extracting [CLS] features and converting to dataframe\nfeatures = pd.DataFrame([sample[0, 0].numpy() for sample in features])\n# Calculating the cosine similarity between items\nsims = cosine_similarity(features)\nFollowing Wulff and Mata (2023), the cosine similarities\nbetween features can be compared with observed correla-\ntions between participant responses at both the item and the\nconstruct level. With 'distilbert-base-uncased', a rel-\natively simple baseline model, the semantic content of the\npersonality items—as captured by the extracted features—\nis correlated with the absolute observed similarities with\na Pearson r = .14. By aggregating item features and\nitem correlations to the construct level, this correlation in-\ncreases to r = .32 (see Figure 5, panels A and B). As\nfurther shown in Wul ff and Mata (2023), these correlations\n12 HUSSAIN, BINZ, MATA, & WULFF\nare higher for more recent, larger models. For instance,\nhkunlp/instructor-xl is a considerably larger alterna-\ntive to DistilBERT that contains 1.2 billion as opposed to\n67 million parameters. At the time of writing, it is at the\ntop end of the Massive Text Embedding Benchmark (MTEB)\nLeaderboard (huggingface.co/spaces/mteb/leaderboard), and\nis also openly available through the Hugging Face Hub. As\nsuch, it presents a promising alternative to DistilBERT. In-\ndeed, it achieves considerably higher item-wise ( r = .39)\nand construct-wise ( r = .56) correlations, which are com-\nparable to those produced by top-of-the-range paid-API-\naccess embedding models from Cohere ( Cohere, Cohere-\nembed-english-v3.0) and OpenAI (ada, text-embedding-ada-\n002). Finally, as illustrated in Figure 5C, plotting a two-\ndimensional projection of the similarities between constructs\nreveals that the placement of constructs largely recovers the\nBig Five personality factors to which the constructs are as-\nsigned. Overall, this example highlights that item embed-\ndings generated through feature extraction can accurately\ncapture the empirical correlations between personality items\nand constructs and the overall structure of human personality,\nalthough performance differs across models.\nMore generally, we should point out that LLMs are not\nonly capable of reproducing known facts about personality\npsychology. Their ability to capture the conceptual relation-\nships between items, constructs, and their labels has been\nexploited by Wulff and Mata (2023) to produce a mapping\nof personality constructs and associated labels that increases\nparsimony and reduces jingle–jangle fallacies ; that is, the\nproliferation of measures that have been given similar labels,\nyet capture di fferent constructs (jingle fallacies), as well as\nmeasures that have received different labels, yet capture the\nsame construct (jangle fallacies). Consequently, feature ex-\ntraction can be a powerful tool in contributing to conceptual\nclarity in this field.\nPredicting Health Perception\nIn this section, we move into the domain of prediction,\nwith the goal of showing how behavioral researchers can\nuse LLMs to predict human judgments and decisions us-\ning the now-familiar feature extraction approach coupled\nwith regression modeling (or other predictive modeling ap-\nproaches). We use the example of predicting health per-\nceptions following the approach of Aka and Bhatia (2022).\nWe believe that predictive applications of LLMs such as this\npresent a promising means of both tracking real-world per-\nceptions and behaviors (e.g. Hussain et al., 2023), as well\nas enabling (in-silico) testing of potential interventions for\nimproving communication between, for instance, health ex-\nperts or policymakers and the general public (Aka & Bhatia,\n2022). See github.com /Zak-Hussain/LLM4BeSci.git to run\nthis example in a Google Colaboratory environment.\nThe dataset used here has two columns: 'text' and\ntext label\nBroken leg. A broken leg (leg fracture)... 49.33\nBulimia. Bulimia is an eating disorder... 34.18\nHyperacusis. Hyperacusis is when... 53.82\nTable 2Three examples from Aka and Bhatia (2022)’s health\ndataset containing short health descriptions from the U.K.\nNational Health Service’s web page and average participant\nratings of the severity of these health states.\n'labels' (Table 2; Aka & Bhatia, 2022). 'text' is\ncomposed of short descriptions of various health states ex-\ntracted from the U.K. National Health Service’s web page.\n'labels' contains average numerical ratings of the sever-\nity of these health states from 782 participants, with higher\nratings indicating less severe states. The goal is to build a\nmodel that predicts people’s perception of the severity of the\nhealth states presented.\nAs demonstrated in the last section, the hid-\nden state representation of each health descrip-\ntion in the 'text' column can be extracted using\n'distilbert-base-uncased'. These features can\nthen be converted to a pandas.DataFrame, and used as\npredictors in a regression model to predict the health ratings.\nSo as not to repeat code, this section begins with these\nfeatures already extracted.\nModel performance is evaluated out-of-sample. In the\nsimplest case, this involves splitting the data into a training\nand a test set using sklearn’s train_test_split, with\n80% used for training and 20% for testing (as determined\nby test_size=.2). randome_state=42 is used for repro-\nducibility.\nIt is important to remember that the extracted features\nare high-dimensional. In this case, there are 768 predic-\ntors, as determined by the number of hidden dimensions in\n'distilbert-base-uncased'. With only 621 samples in\nthe training set, there are more predictors than samples. In\nsuch a case, an ordinary least squares regression solution\ncannot be identified. To address this issue, and more general\nissues associated with high-dimensional data such as over-\nfitting and multicollinearity, researchers commonly employ\nregularization. In this case, sklearn’s RidgeCV is used,\nwhere the regularization penalty ( alpha) is automatically\ntuned using cross-validation within the training set.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\n# Extract features and labels (see previous section)\nfeatures, labels = dat['hidden_state'], dat['labels']\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\nfeatures, labels, test_size=.2, random_state=42\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 13\nFigure 6 Predicting health perception. A. Predicted versus observed health ratings using DistilBERT. B. Comparing the\nperformance of DistilBERT, Instructor (instructor-xl), Cohere (Cohere-embed-english-v3.0), and ada (text-embedding-ada-\n002), with 10-fold cross-validation using ridge regression. Error bars reflect±1 SD\n)\n# Scaling the data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n# Initializing ridge regression, fit, and evaluate\nregr = RidgeCV(alphas=[10**i for i in range(-5, 7)])\nregr.fit(X_train, y_train)\nprint(regr.score(X_test, y_test))\nOnce the regression model has been initialized using\nRidgeCV() and assigned as regr, it is then fitted to the stan-\ndardized training data with regr.fit(). Standardization\nis commonly performed in regularized regression, such as\nridge regression, to ensure that all predictors are given equal\nweight in the regularization penalty. Finally, model perfor-\nmance is evaluated with regr.score().\nFigure 6A shows a high alignment of predicted and ob-\nserved health state ratings, implying that much of the vari-\nance in health state ratings can be explained by the Distil-\nBERT features.\nFor the purposes of keeping the tutorial code simple, out-\nof-sample performance was measured using a single train–\ntest split with a single model; ideally, performance would be\nevaluated across multiple splits to ensure the robustness of\nthe results. Using ridge regression with automatic (nested)\ntuning of the regularization penalty hyper-parameter (Varma\n& Simon, 2006), feature extraction with DistilBERT on av-\nerage explains over half of the variance in health perceptions\n(R2 = .58). This performance, although considerable, is infe-\nrior to that of larger models. As Figure 6B shows, the open-\nsource Instructor model (instructor-xl), as well as the pro-\nprietary Cohere (Cohere-embed-english-v3.0) and ada (text-\nembedding-ada-002) models benefit from their larger size,\nall showing a relatively similar increase in performance rel-\native to DistilBERT, with the best-performing model achiev-\ning R2 = .70. These results could potentially be improved\nfurther by using di fferent prediction algorithms, including\nnonlinear algorithms, or more fine-grained hyper-parameter\ntuning.\nOverall, the analysis shows that LLMs implementing a\nsimple feature extraction coupled with a regression-based\nprocedure can achieve impressive performance when tasked\nwith predicting human judgments.\nPredicting Repeated Choice\nThe previous examples have demonstrated that LLM fea-\nture extraction can be used to predict aspects of human psy-\nchology and judgments. But what about more complex\nhuman behavior? This section demonstrates that the fea-\nture extraction pipeline can also be applied to decisions in\na repeated choice paradigm that involves sequential cogni-\ntive reasoning. Moreover, it shows that only minor code\nchanges are needed to employ some of the largest and most\nadvanced LLMs currently available for these purposes. See\ngithub.com/Zak-Hussain/LLM4BeSci.git to run this example\nin a Google Colaboratory environment.\n14 HUSSAIN, BINZ, MATA, & WULFF\nThe experimental data in this section come from a\nparadigm known as the horizon task (Wilson et al., 2014).\nIn this task, participants repeatedly choose between two op-\ntions. Upon selecting an option they receive a probabilistic\nreward. Each game starts with four observational trials, in\nwhich the examples are predetermined by a computer pro-\ngram, followed by either one or six choices. Participants are\ninstructed to accumulate as much reward as possible over the\nexperiment. The data considered here are combined from\ntwo previous studies (Feng et al., 2021; Wilson et al., 2014)\nin which 60 participants each played 320 games, making a\ntotal of 67,200 choices.\nIn line with earlier work (Binz & Schulz, 2023a; Coda-\nForno et al., 2023), the model inputs, also known as prompts,\nare designed as follows (see also Figure 7A). Each prompt\ncontains information about a single game, starting with a list\nof previous choices and outcomes in the game. Following a\nbrief instruction, the text continues “Q: Which machine do\nyou choose? A: Machine”, missing only the number of the\nselected machine (1 or 2). Choices and outcomes are sequen-\ntially added to the list as the LLM interacts with the task.\nOnce the prompts have been loaded in a\npandas.DataFrame, the same out-of-sample testing\npipeline used in the previous section on health judgments\ncan be applied. The only change necessary is to replace\nsklearn’s RidgeCV with LogisticRegressionCV to\naccount for the binary criterion (i.e., machine 1 or 2).\nAnalogous to RidgeCV, LogisticRegressionCV also\nperforms L2-regularization with automatic regularization\nparameter tuning. For the sake of brevity, we omit this code\nfrom the code block.\nWith the model ready, it can be fitted using the features\nextracted from the 'distilbert-base-uncased' model\nusing a training portion of the data. The resulting model\nachieved a considerable accuracy of 78 .9% on the test set.\nHowever, this performance can be improved using larger\nmodels. This time, we consider decoder models, which,\nbased on currently available models, are better suited for the\ntask at hand in two ways. First, current decoder models are\nlarger in size than encoder models and can thus potentially\ncapture more sophisticated reasoning (Brown et al., 2020).\nSecond, their design may better match the sequential nature\nof the task, which mirrors the next-token prediction setting.\nFortunately, the Hugging Face interface makes it possible\nto use some of the most advanced models with only minor\ncode modifications. In line with the goal of sharing repro-\nducible code that can run in a Google Colaboratory environ-\nment, we use a smaller Llama 2 model in the analysis ex-\nample (Touvron et al., 2023). Llama 2 is a family of large-\nscale decoder models developed by Meta that are available\nin di fferent sizes, training stages, and formats through the\nHugging Face Hub. The specific model used in the example\nis 'TheBloke/Llama-2-13B-GPTQ', which has been cre-\nated by quantizing the 13 billion parameter model to 4 bits to\nreduce its RAM footprint without a significant loss in accu-\nracy (see, e.g., Frantar et al., 2022). With 13 billion param-\neters, the model is two orders of magnitude larger than the\n'distilbert-base-uncased' model used thus far.\nA few additional minor modifications are\nneeded to use the Llama 2 model. First, because\n'TheBloke/Llama-2-13B-GPTQ' is a di fferent class\nof model, that is, a decoder causal language model,\nthere is no \"[CLS]\" token that could be used to stand in\nfor the entire input. Instead, in causal language mod-\nels, the most important token to generate predictions\nis the final token in the input, that is, the token \"ma-\nchine\", which is the only token whose contextualized\nembeddings are determined by those of all other tokens.\nThe hidden state for the final token in the sequence\ncan be extracted using last_hidden_state[:,-1].\nSecond, loading this model requires the more specific\nAutoModelForCausalLM.from_pretrained() method as\nopposed to the generic AutoModel.from_pretrained()\nmethod. Within the function, device_map=\"auto\",\nwhich is available only for certain larger models, is\nused to allocate the model automatically to GPU or\nCPU resources, making the explicit casting of tensors\nvia .to(device) obsolete, and revision='main'\nis used to indicate the specific model branch. (See\nhttps://huggingface.co/TheBloke/Llama-2-13B-GPTQ\nfor the list of options on the corresponding model card.)\n# extract features\ndef extract_features(batch):\ninputs = {\nk:v for k, v in batch.items()\nif k in tokenizer.model_input_names\n}\nwith torch.no_grad():\nlast_hidden_state = model(\n**inputs,\noutput_hidden_states=True\n).hidden_states[-1]\nreturn {\n\"hidden_state\": last_hidden_state[:,-1].cpu().numpy()\n}\n# load model\nmodel_ckpt = 'TheBloke/Llama-2-13B-GPTQ'\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_ckpt,\ndevice_map=\"auto\",\nrevision=\"main\"\n)\nUsing the 13B Llama 2 model with these settings required\nonly around 16 GB GPU memory, implying that it can be\nreplicated on many standard personal computers. Predicting\nrepeated choice using the Llama 2 features resulted in a sub-\nstantial increase in accuracy over the DistilBERT model to\n82.9% (see Figure 7B). This performance is on a par with that\nof a psychological model using handcrafted features such as\nthe difference in estimated rewards or the difference in uncer-\ntainty, as described by Binz and Schulz (2022) and Gershman\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 15\nYou made the following observations in the past:\n- Machine 1 delivered 34 dollars.\n- Machine 1 delivered 41 dollars.\n- Machine 2 delivered 57 dollars.\n- Machine 1 delivered 37 dollars.\nYour goal is to maximize the sum of received dollars\nwithin six additional choices.\nQ: Which machine do you choose?\nA: Machine\nA\nFigure 7 Predicting repeated choices. A. Example prompt for the text-based version of the horizon task. B. Accuracy of\npredicting human choices on the test data for different models.\n(2018), which achieved a test accuracy of 82.6%.\nOverall, the results presented in this section demonstrate\nthat LLMs can produce feature vectors that make it possible\nto predict complex decision-making behavior at a level com-\nparable with that of current psychological models. Given that\nconsiderably larger models are available and that the model\nin the example may have su ffered from quantization, it is\nplausible that the ceiling for state-of-the-art LLMs is consid-\nerably higher. With such strong performances, these results\nhint at behavioral applications of LLMs that go beyond pre-\ndicting observed data, such as investigating how changes to\nthe experimental stimuli or instructions may impact partici-\npant behavior. Such changes may be used, for instance, as\nin-silico pilot studies to inform future experimental designs,\nor to test specific hypotheses that were not testable with the\noriginal experimental data.\nFine-Tuning\nFeature extraction can be a powerful approach for predict-\ning human judgments and behavior, especially when labeled\ntask-specific data are scarce or if the researcher does not have\naccess to strong GPUs. However, as the extracted features\nare domain-general, they may not be optimal for all tasks.\nAn alternative approach is fine-tuning, sometimes referred to\nas transfer learning. During fine-tuning, all model weights\nare typically optimized for the given task (for an alternative\nstrategy, see, e.g., Hu et al., 2022).\nThis section again uses data from Aka and Bhatia\n(2022), as illustrated in Table 2. See github.com /Zak-\nHussain/LLM4BeSci.git to run this example in a Google\nColaboratory environment. So as not to repeat code,\nwe begin with dat already tokenized, and with the de-\nvice already set to the GPU. The code starts by setting\ntorch.manual_seed(42). This helps to ensure the repro-\nducibility of stochastic processes such as stochastic gradient\ndescent and its variants, including the 'adamw_torch' opti-\nmizer used for fine-tuning the model.\nThe data are again split into a training and a\ntest set. Because the plan is to use the Hugging\nFace ecosystem downstream as well, the data are kept\nin the native Hugging Face format using the inbuilt\ndatasets.Dataset.train_test_split() method. Mir-\nroring sklearn’s train_test_split(), a test set size and\n\"random state\" can be set, this time with test_size and\nseed as arguments. The method returns the split data sets\nin the form of a datasets.DatasetDict object, which be-\nhaves much like a regular Python dictionary. Technically, it\ninherits from dict, and has the keys 'train' and 'test'\nthat map to the respective data sets.\nNext, 'distilbert-base-uncased' is ini-\ntialized as the model_ckpt, but this time with\ntransformers.AutoModelForSequenceClassification,\nas opposed to the basic tranformers.AutoModel from\nearlier. This loads the model with a \"classification head\"\nattached. Despite its name, this head can also be used\nfor regression tasks such as the one at hand by specifying\nnum_labels=1 as an argument in the from_pretrained()\nmethod.\nHaving initialized the model, it is now time to set up the\ntraining loop with a transformers’ TrainingArguments\nobject. This involves specifying the name of the\noutput directory where model predictions and check-\npoints will be saved ( output_dir), the batch size for\ntraining and evaluation (per_device_train_batch_size,\nper_device_eval_batch_size), and the frequency with\nwhich training and evaluation metrics are recorded\n(logging_strategy, evaluation_strategy). Because\nneural networks usually need to be trained for multiple it-\n16 HUSSAIN, BINZ, MATA, & WULFF\nerations over the training set, the number of iterations, also\ncalled epochs, can be specified with num_train_epochs. It\nis important to note that more advanced strategies exist to\nautomatically halt training when certain test-performance-\noptimizing heuristics are triggered (e.g., early stopping ).\nHowever, for the purposes of this tutorial, we stick to manu-\nally specify num_train_epochs.\nAn evaluation function, compute_metrics(), is defined\nto evaluate model performance with metrics other than the\nmodel’s loss, which is automatically logged. This takes a\ntransformers.EvalPrediction as input, which can be\nunpacked into the model’s predictions of the health per-\nception ratings ( preds) and the actual ratings ( labels).\nAn object to compute the explained variance ( R2) is then\nloaded using Hugging Face’s evaluate library, using\nevaluate.load(\"r_squared\"), and the R2 is computed\nwith the object’s compute() method using the preds and\nlabels.\nTraining is carried out by the transformers.Trainer.\nIn this case, the trainer is initialized with the model\n(model=model), the data set to be used for training the model\n(train_dataset=dat['train']), the data set to be used\nfor evaluating the model ( eval_dataset=dat['test']),\nand the function to evaluate the model’s performance on the\ntest set ( compute_metrics=compute_metrics). Model\ntraining is then initiated by running trainer.train().\nfrom transformers import (\nAutoModelForSequenceClassification, TrainingArguments, Trainer\n)\nimport evaluate\n# For reproducibility\ntorch.manual_seed(42)\n# Splitting the data into train and test sets\ndat = dat.train_test_split(test_size=.2, seed=42)\n# Initialising model and moving to the GPU\nmodel = AutoModelForSequenceClassification.from_pretrained(\nmodel_ckpt, num_labels=1\n)\nmodel = model.to(device)\n# Setting up training arguments for the trainer\nmodel_name = f\"{model_ckpt}-finetuned-health\"\nbatch_size = 8\ntraining_args = TrainingArguments(\noutput_dir=model_name,\nper_device_train_batch_size=batch_size,\nper_device_eval_batch_size=batch_size,\nlogging_strategy=\"epoch\",\nevaluation_strategy=\"epoch\",\nnum_train_epochs=10,\noptim='adamw_torch',\n)\ndef compute_metrics(eval_preds):\n\"\"\"\nComputes the coefficient of determination (R2) on the test set\n\"\"\"\npreds, labels = eval_preds\nmetric = evaluate.load(\"r_squared\")\nr2 = metric.compute(predictions=preds, references=labels)\nreturn {\"r_squared\": r2}\n# Fine-tuning and evaluating the model\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=dat['train'],\neval_dataset=dat['test'],\ncompute_metrics=compute_metrics,\n)\ntrainer.train()\nIn our example, after ten epochs, test performance\nplateaued at around R2 = .50. This is slightly below the\nR2 = .54 achieved using feature extraction. As before, re-\npeated train–test splits would need to be run using, for in-\nstance, (repeated) k-fold cross-validation, to achieve a more\nreliable test performance estimate. For the sake of brevity,\nthe code for k-fold cross-validation is not included in this\ntutorial; however, the pattern of results remains the same.\nThus, using our approach, feature extraction outperforms\nfine-tuning when it comes to predicting health perceptions.\nThis result highlights that fine-tuning may not always be\nthe dominant choice when it comes to using LLMs for pre-\ndicting human judgments and behavior. Factors such as data\nquantity and quality as well as the availability of computa-\ntional resources will play a large role in determining which\napproach makes the most sense. It is also worth noting that a\nhost of fine-tuned models are available for download from\nthe Hugging Face Hub. As such, depending on the task\nat hand, the researcher may find that fine-tuned models for\nspecific tasks or domains are already available at hugging-\nface.co/models. Of course, the data used for fine-tuning\nmay still differ considerably from the data that the researcher\nwishes to apply the fine-tuned model to. As such, it is always\nimportant to compare fine-tuned models to their pre-trained\nalternatives.\nText Generation\nPerhaps one of the major features contributing to the pro-\nliferation of LLMs is their ability to generate convincing,\nhuman-like text in response to prompts. In behavioral sci-\nence, this capacity has made it possible to perform a vast bat-\ntery of in-silico behavioral experiments (e.g., Binz & Schulz,\n2023b; Yax et al., 2023): As long as the experiment can\nbe converted into a text-based format—setting multimodal\nmodels aside for present purposes—the model can \"partici-\npate\" in it.\nFollowing Yax et al. (2023), this section draws on the ex-\nample of the cognitive reflection test (CRT) to demonstrate\nhow this can be done (Frederick, 2005). The three-question\nCRT is designed to measure a person’s ability to inhibit intu-\nitive but incorrect responses in favour of deliberation. Here,\nwe focus on how researchers can run experiments on LLMs\nin a replicable manner on their own hardware or using freely\navailable cloud computing resources. See github.com /Zak-\nHussain/LLM4BeSci.git to run this example in a Google Co-\nlaboratory environment.\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 17\nWe again use a quantized 13 billion parameter version\nof Llama 2 (Touvron et al., 2023). As for predicting re-\npeated choice, despite the vast increase in model size, the\ncode for running the model uses some familiar Hugging\nFace components: a transformers.pipeline is used, the\nmodel_ckpt is defined and passed as an argument to model\nand tokenizer when initializing the pipeline. Likewise,\ndevice_map='auto' is used for efficient upscaling to larger\nmodels.\nHowever, as the goal is to have the model re-\nspond to the CRT questions with text, there are two\nmain di fferences from predicting repeated choice. First,\na transformers.pipeline for 'text_generation' is\nused, which gets the model to generate responses with\nminimal code. Second, a chat-based version of Llama\n2, 'TheBloke/Llama-2-13B-chat-GPTQ', is employed\n(TheBloke, 2022). This model version has been further\ntrained with techniques such as supervised fine-tuning and\nreinforcement learning from human feedback to align it more\nclosely with the preferences of humans who wish to use it as\nan AI assistant (Touvron et al., 2023).\nThe final arguments in the pipeline are specifically to do\nwith text generation. max_new_tokens=512 means that the\nmodel can produce a maximum of 512 tokens in response\nto the prompt. do_sample=False prevents the model from\nperforming random sampling from the softmax output dis-\ntribution over its vocabulary. This forces the model to em-\nploy a strategy known as greedy search decoding, whereby\nthe model output at each timestep is simply the token with\nthe highest probability in the output distribution. This can\nbe contrasted with text-generation techniques involving sam-\npling, which seek to improve the diversity of model outputs\nby adding some randomness to the generation process. How-\never, this added diversity can come at the expense of coher-\nence and reproducibility (Tunstall et al., 2022).\nThe CRT comprises three questions, which are assigned\nto prompt. In order to create a more realistic experi-\nmental context for the model, the code uses the Llama\n2 chat-specific prompting template recommended on the\n'TheBloke/Llama-2-13B-chat-GPTQ' model card page\n(TheBloke, 2022):\n'''\n[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n{prompt}[/INST]\n'''\nIn this case, the {system prompt}, which is the broader\ncontext given to the model to help guide it, is a general de-\nscription of the psychological study along with the instruc-\ntions for participants, whereas the {prompt} contains the\nCRT questions. Both together form the prompt_template.\nGenerating the output is then as straightforward as pass-\ning the prompt_template to generator, which returns the\noutput of the model as a list. The generated tokens can be ac-\ncessed with the zero index, which returns a dictionary with a\nsingle key: 'generated_text'. Accessing the key’s value\nreturns a string composed of the text generated by the model\nwith the input ( prompt_template) prepended. In order to\nreturn only the output generated, the string is sliced such\nthat the printed output begins after the last character of the\nprompt_template.\nfrom transformers import pipeline\n# Load the model and tokenizer\nmodel_ckpt = 'TheBloke/Llama-2-13B-chat-GPTQ'\ngenerator = pipeline(\n\"text-generation\", model=model_ckpt, device_map='auto',\nrevision='main', tokenizer=model_ckpt, max_new_tokens=512,\ndo_sample=False\n)\n# Format the prompt\nprompt = (\n\"1. A bat and a ball cost $1.10 in total. \"\n\"The bat costs $1.00 more than the ball. \"\n\"How much does the ball cost?\\n\"\n\"2. If it takes 5 machines 5 minutes to make 5 widgets, \"\n\"how long would it take 100 machines to make 100 widgets?\\n\"\n\"3. In a lake, there is a patch of lily pads. \"\n\"Every day, the patch doubles in size. \"\n\"If it takes 48 days for the patch to cover the entire lake, \"\n\"how long would it take for the patch to cover half of the \"\n\"lake?\\n\"\n)\nprompt_template = (\nf\"[INST] <<SYS>>\\n\"\n\"You are about to participate in a psychology experiment \"\n\"with three questions. \"\n\"Please take your time to consider your answer to each \"\n\"question, and provide a short answer.\\n\"\nf\"<</SYS>>\\n{prompt}[/INST]\\n\"\n)\n# Generate and print the output\noutput = generator(prompt_template)\nprint(output[0]['generated_text'][len(prompt_template):])\nThe model output is the following. It answers two out of\nthe three questions correctly.\n'''\nSure, I'd be happy to participate in the experiment!\nHere are my answers to each question:\n1. The ball costs $0.10.\n2. It would take 5 minutes for 100 machines to make 100 widgets.\n3. It would take 47 days for the patch to cover half of the lake.\n'''\nOn average, human participants answer between 1 and 2\nquestions correctly, suggesting that LLMs can answer prob-\nlems from the CRT at or above human level. However, one\nshould be careful not to draw strong conclusions about the\nmodels’ reasoning capabilities from these results. It is pos-\nsible that examples of the CRT are in Llama 2’s pre-training\nset, with (in-)correct answers included. Similar to findings\non CRT pre-exposure among humans (Haigh, 2016), this\nmay inflate the model’s performance and conclusions about\nits ability to reason (Mitchell, 2023).\nThe main purpose of this example is to show that open-\naccess LLMs such as Llama 2 can be run on freely available\n18 HUSSAIN, BINZ, MATA, & WULFF\nExcerpt BT_easiness\nAn honest and poor old woman was... -0.05\nOur plate illustrates the residence of... -2.98\nJust as wildebeest are the main grazers... -2.46\nTable 3Three examples from Crossley et al. (2023)’s CLEAR\ncorpus containing text excerpts leveled for 3rd to 12th grade\nreaders and teachers’ readability scores.\nhardware and used as stand-in \"participants\" in behavioral\nexperiments, with the potential to generate insights about the\nexperiment, the model, and perhaps human psychology as\nwell.\nToken Probability and Perplexity Extraction\nThis tutorial has introduced three common LLM use\ncases: feature extraction, fine-tuning, and text generation.\nAlthough these are perhaps more well-known use cases, they\nare not exhaustive. For instance, an additional use of LLMs\nis to deploy them on the very tasks for which they were pre-\ntrained; namely, to assign probabilities to tokens. During\ntraining, the model is incentivized to assign high probabili-\nties to tokens and token sequences that are common in the\ntraining data and vice versa for those that are uncommon. As\na result, the probabilities produced by a trained model can be\nused to detect text sequences that (the model has learned) are\nuncommon. Measures based on token and token sequence\nprobabilities have thus been used to, for instance, investigate\nhow language models capture grammatical gender (An et al.,\n2019) and to predict human reading times (e.g., Merkx &\nFrank, 2020).\nThe present example demonstrates how the log probabil-\nities extracted from GPT-2 can be used to predict teachers’\ntext readability ratings. Specifically, these are teachers’ rat-\nings of how di fficult student readers would find certain text\nexcerpts, obtained from the CommonLit Ease of Readability\n(CLEAR) corpus (Table 3; Crossley et al., 2023).\nA common metric to evaluate the probability of a se-\nquence is called perplexity (Jelinek et al., 1977). Given a\nsequence of tokens X = (x0, x1, ...,xt), perplexity can be de-\nfined as\nexp\n−1\nt\ntX\ni=1\nlog p(xi|x<i)\n\nwith p(xi|x<i) being the probability assigned to the i-th token\nin the sequence given the preceding tokens. Perplexity is thus\nthe inverse geometric mean of sequentially produced token\nprobabilities. As perplexity assumes sequentially produced\ntokens, it is not well-defined for masked language modeling.\nIn our example, we therefore rely on GPT-2, which is a de-\ncoder model trained using causal language modeling and a\ndirect precursor to today’s GPT-4.\nThe code begins by reading the 'Exerpt' and\n'BT_easiness' columns of the CLEAR corpus into a\npandas.DataFrame. It then loads a perplexity metric ob-\nject from the evaluate library’s load method. This ob-\nject has a convenient compute method, which allows the re-\nsearcher to specify from which model they wish to compute\nthe perplexity ( model_id='openai-community/gpt2' in\nthis case), the text sequences to compute perplexity on\n(clear['Excerpt'])), the batch size ( 8). The device de-\nfaults to the GPU if a CUDA-compatible GPU is available.\nThe method returns a dictionary containing a list of perplex-\nity scores for each sample ('perplexities') and the aver-\nage of those scores ('mean\\_perplexity').\nimport pandas as pd\nfrom evaluate import load\n# Load the data\nclear = pd.read_excel(\n'clear.xlsx',\nusecols=['Excerpt', 'BT_easiness'],\n)\n# Load the perplexity metric\nperplexity = load('perplexity', module_type='metric')\n# Compute the perplexity\nclear['perplexity'] = perplexity.compute(\nmodel_id='openai-community/gpt2',\npredictions=clear['Excerpt'],\nbatch_size=8\n)['perplexities']\nTo evaluate whether perplexity can predict readability, we\nfeed perplexity as a feature into an ordinary least squares\nregression. Using 1,000 randomly sampled excerpts from\nCLEAR, the perplexity model achieves a 10-fold cross-\nvalidation R2 = .20 (SD = .05), implying that perplexity\ncan account for a significant portion of text readability vari-\nance. The predictive accuracy of perplexity can be compared\nto an established predictor of readability, the Flesch Read-\ning Ease (Flesch, 1948), calculated based on average sen-\ntence and word lengths. Flesch Reading Ease achieves an\nR2 = .28 (SD = .04), which is only slightly higher than\nthe performance of perplexity. Moreover, combining the two\nfeatures leads to a considerable increase in predictive accu-\nracy (R2 = .44, SD = .05), indicating the added value of\nperplexity for the prediction of readability.\nCompared to the other use cases presented above, to-\nken probability and perplexity measures have been less fre-\nquently employed in behavioral science research. Neverthe-\nless, these measures show promise in behavioral research in\nat least two respects. First, they can be used to predict human\nperceptions and evaluations of text, such as the readability\nof study vignettes or the surprisingness of statements. Sec-\nondly, they can be employed to evaluate the likelihood of hu-\nman responses to text, such as response to open-text format\nitems.\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 19\nOpen Questions and Future Directions\nThis tutorial has introduced the Hugging Face ecosystem\nas an open-source approach to using LLMs in behavioral\nscience. This section discusses the advantages and limita-\ntions of (open-source) language modeling in behavioral sci-\nence, the broader societal-level risks posed by (open-source)\nLLMs, and future directions for behavioral science research\nwith LLMs.\nGood Science Practices with LLMs\nThe goal of this tutorial has been to make the responsible\nuse of LLMs for behavioral science more accessible. How-\never, using LLMs responsibly includes following good sci-\nence practices. This requires more than the ability to imple-\nment LLMs in cod; it also necessitates a substantive under-\nstanding of what that code is doing and an appreciation of\nthe complex and nuanced theoretical questions concerning\nLLM capabilities. We hope that through sectionA Primer on\nTransformer-Based Language Models, and the explanations\naccompanying the code in this tutorial, readers will come\naway with a thorough understanding of what this code is do-\ning. Nevertheless, a few cautionary points are in order.\nThe first concerns hyper-parameters. Like most popular\nstatistical libraries in behavioral science, Hugging Face li-\nbraries enable control over a vast array of hyper-parameters,\nand many of these parameters have default settings. Al-\nthough these defaults often make code more readable, they\ncan also lead to complacency. Against this tendency, we\nstress the importance of making active decisions between the\nuniverses of possible parameter settings. In cases where sub-\nstantive justifications are lacking, we encourage the use of\nmultiverse analyses ((i.e., with different plausible parameter\nsetups; Steegen et al., 2016), computational resources per-\nmitting.\nThe second concerns performance evaluation. In addition\nto repeated out-of-sample testing, we emphasize the impor-\ntance of other evaluation and sanity-checking strategies, such\nas testing meaningful synthetic baseline models. This could\ninclude randomly shu ffling the pairings between extracted\nfeatures and labels to identify possible data leakages—a per-\nnicious problem in machine learning more broadly (see, e.g.,\nKaufman et al., 2012). Alternatively, it could mean artifi-\ncially constructing perfectly predictive fine-tuning features\nto ensure model fitting is working properly.\nThe third and final point is more theoretical. The re-\ncent proliferation of LLMs has revived long-running debates\nover whether LLMs possess capacities such as true \"under-\nstanding\" (\"thinking\", \"reasoning\", etc.; see, e.g., Mitchell\n& Krakauer, 2023). Although we believe that the validity\nof the applications described in this tutorial does not depend\non whether LLMs truly possess such capacities, we antici-\npate that the broader conclusions drawn from scientific stud-\nies involving LLMs will often be mediated by researchers’\nbackground beliefs concerning these questions. As such,\nwe would like to highlight the existence of a vast scientific\nand philosophical literature pertaining to the (evaluation of)\nlanguage model capabilities (e.g. Bender et al., 2021; Gün-\nther et al., 2019; Mitchell & Krakauer, 2023; Searle, 1980;\nTuring, 1950)—a literature that has by no means reached a\nconsensus—to guard against snap judgments or uncritical de-\nfault positions.\nIn their broader form, these words of caution do not\nconcern LLMs alone but are part of good science practice\nmore generally. However, the complexity, opacity, and\nquirkiness of neural network models can exacerbate these\nissues in ways that require special attention.\nOpen-Source LLMs and Open Behavioral Science\nBehavioral science is going through an open science revo-\nlution guided by principles such as transparency, accessibil-\nity, and sharing (Vicente-Saez & Martinez-Fuentes, 2018).\nOpen-source and open-access language modeling frame-\nworks such as Hugging Face are closely aligned with these\nprinciples. For instance, with Hugging Face, all analysis\nsteps—from data preprocessing to model validation—are in\nprinciple accessible to fellow scientists wishing to better un-\nderstand and perhaps reproduce what others have done. Like-\nwise, models fine-tuned using thetransformers library can\neasily be shared on the Hugging Face Hub, making it eas-\nier for researchers to build on and benefit from the work of\ntheir peers. With over 300,000 models and 60,000 datasets,\nHugging Face stands as an exemplary case of the power of\nsharing in research and beyond.\nHugging Face also supports reproducibility. Features such\nas the ability to set seeds help improve the reproducibility of\nnondeterministic processes such as gradient descent. Like-\nwise, because models are saved to the hard drive (instruc-\ntions for locating models saved to hard drive are regularly\nupdated at stackoverflow.com /questions/61798573/where-\ndoes-hugging-faces-transformers-save-models), the precise\nversion of the model used for the analysis can be permanently\nsaved for future reproductions. This stands in contrast to less\nopen alternatives such as the OpenAI API, which, at the time\nof writing, does not provide the ability to access the same\nversion of the model indefinitely into the future after model\nupdates.\nOpen-source and open-access language modeling frame-\nworks also have considerable advantages when it comes to\ndata privacy: Because models can be saved and run locally,\nsensitive data can remain on the appropriate hardware, and\nresearchers can be sure that the creators of the model will\nnot have access to it. This is crucial in behavioral science,\nwhere ensuring the privacy of participant data is paramount\nfrom an ethical and legal perspective.\n20 HUSSAIN, BINZ, MATA, & WULFF\nOpen-source language modeling frameworks also help\nmitigate an important disadvantage of LLMs: their poor in-\nterpretability. Interpretability is a general problem for neu-\nral network models, whose complexity and abstract repre-\nsentational nature have earned them the label \"black-box\"\nmodels (Alishahi et al., 2019). In behavioral science, the\nlimited interpretability of LLMs can hinder a researcher’s\nability to draw strong theoretical conclusions. For instance,\nbeing able to interpret the internal states of the model pre-\nsented in the section on Text Generation would help clarify\nwhether it had simply \"memorized\" answers to the CRT or\nactually \"reasoned\" them through. However, a commitment\nto openness—in this case, transparency about the data used\nto train the model—could also help resolve this uncertainty\nby revealing whether the CRT even featured in the training\ndata at all. In general, interpretability is worsened when re-\nsearchers are not given information about important details\nconcerning the model’s pre-training data, tokenizer, architec-\nture, weights, and fine-tuning regimes. Of course, even when\nthese details are known, it can remain a mystery why a model\nperforms well on some tasks but not on others. This point is\nexemplified by the existence of emergent abilities in LLMs:\nabilities that arise from model upscaling, but whose arrival\nis incredibly difficult to predict, even for the developers who\ntrained the model (Wei et al., 2022).\n(Open-Source) LLMs and Society\nAlthough we believe that open-source and open-access\nlanguage modeling has its advantages for research, mak-\ning LLMs publicly accessible also comes with consider-\nable risks. LLMs are, after all, powerful tools, and in the\nhands of bad actors, they could be used to do serious harm\n(e.g., spreading mis- and disinformation; Weidinger et al.,\n2022). Increasing access to LLMs will also have environ-\nmental impacts (Strubell et al., 2019), especially when more\nresearchers have the ability not only to use these models for\ninference but also to train them via ecosystems such as Hug-\nging Face.\nFurthermore, it is important to be aware of the broader\nrisks that present and future LLMs may pose to society, es-\npecially if they are poorly aligned with people’s preferences\nand values (Bockting et al., 2023; Russell, 2019). Con-\ncerns such as these have motivated research programs into\nAI alignment from leading AI companies (Leike et al., 2018;\nLeike & Sutskever, 2023, July 5). They have also led to the\nuse of more explicit human behavioral data for fine-tuning\nLLMs (e.g., via explicit human feedback on model outputs)\nto achieve closer alignment with human preferences. As has\nbeen pointed out (Irving & Askell, 2019; Russell, 2019), this\nendeavor presents a unique opportunity for behavioral scien-\ntists, who of course have expertise in collecting high-quality\nhuman data.\nFuture Direction for LLMs and Behavioral Science\nThis insight points to an interesting future direction at the\nintersection of behavioral science and language modeling.\nAlthough this tutorial has focused on how LLMs can be used\nas tools for behavioral science research (LLMs→behavioral\nscience), an inverse relationship also holds promise: the use\nof behavioral science methods to build more interpretable,\nhuman-aligned LLMs (behavioral science →LLMs). For\ninstance, as foreshadowed in the section on Text Genera-\ntion, behavioral experiments can be run on LLMs to pro-\nvide greater insight into the models’ capabilities (e.g., Binz\n& Schulz, 2023b; Yax et al., 2023). Likewise, interpretabil-\nity techniques such as probing can be combined with psy-\ncholinguistic methods to asses how aligned the internal rep-\nresentations of LLMs are to people’s semantic representa-\ntions (Aeschbach et al., 2024; Cassani et al., 2023; Sucholut-\nsky et al., 2023). This relationship is mutually reinforcing\n(behavioral science →LLMs →behavioral science): LLMs\nthat are more aligned with people may also serve as more\nplausible and predictive psychological models (e.g., Binz &\nSchulz, 2023a), and LLMs that are more interpretable may\nallow for deeper theoretical insights—into their psychologi-\ncal plausibility, but also into human cognition and behavior\nitself.\nConclusion\nLLMs hold immense promise for behavioral science and\nopen-source frameworks play a crucial role in promoting\ntransparency, reproducibility, and data protection. This tu-\ntorial has provided a practical overview of using the open-\nsource Hugging Face ecosystem, covering feature extrac-\ntion, fine-tuning, and text generation, in order to empower\nresearchers to harness the potential of LLMs for behavioral\napplications. While acknowledging the current limitations\nof this approach, we hope that these efforts can help catalyze\nLLM utilization and o ffer novel insights and opportunities\nfor future research in behavioral science.\nDeclarations\nFunding\nSwiss National Science Foundation grant (197315), to Dirk\nU. Wulff\nSwiss National Science Foundation grant (204700), to Rui\nMata\nConflicts of interest/Competing interests\nThe authors declare that they have no competing interests.\nEthics approval\nNot applicable.\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 21\nConsent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nAvailability of data and materials\nThe datasets analyzed for the current tutorial are\navailable in the LLM4BeSci repository, github.com /Zak-\nHussain/LLM4BeSci.git.\nCode availability\nThe code used for the current tutorial is avail-\nable in the LLM4BeSci repository, github.com /Zak-\nHussain/LLM4BeSci.git.\nAuthors’ contributions\nConceptualization: ZH, DUW\nMethodology: ZH, MB, DUW\nFormal Analysis: ZH, MB, DUW\nWriting – Original Draft Preparation: ZH (sections Re-\nlating Personality Measures, Predicting Health Perception,\nThe Hugging Face Ecosystem , Open Questions and Future\nDirections), MB ( Predicting Repeated Choice), RM ( Intro-\nduction), DUW (A Primer on Transformer-Based Language\nModels)\nWriting – Review & Editing: ZH, MB, RM, DUW\nAcknowledgements\nWe thank Susannah Goss and Laura Wiles for editing\nthe manuscript. We also thank Ada Aka and Sudeep Bha-\ntia for making their health perception data available for our\nuse. This work was supported by grants from the Swiss Sci-\nence Foundation to Rui Mata (204700) and Dirk U. Wul ff\n(197315).\nOpen Practices Statement\nThe data used in all sections are available at\ngithub.com/Zak-Hussain/LLM4BeSci.git. Aside from\nthe specific results for Cohere (Cohere-embed-english-\nv3.0) and ada (text-embedding-ada-002)—these models are\nbehind a paywall—all results can be reproduced using freely\navailable software. None of the analyses were preregistered,\nas they are presented for demonstration purposes only.\nReferences\nAbdurahman, S., Vu, H., Zou, W., Ungar, L., & Bhatia, S.\n(2023). A deep learning approach to personality as-\nsessment: Generalizing across items and expand-\ning the reach of survey-based research. Journal of\nPersonality and Social Psychology. Advance online\npublication. https://doi.org/10.1037/pspp0000480\nAeschbach, S., Mata, R., & Wulff, D. U. (2024). Mapping the\nmind with free associations: A tutorial using the r\npackage associator. PsyArXiv. https://doi.org/https:\n//doi.org/10.31234/osf.io/ra87s\nAka, A., & Bhatia, S. (2022). Machine learning models for\npredicting, understanding, and influencing health\nperception. Journal of the Association for Con-\nsumer Research, 7(2), 142–153. https : / /doi . org/\n10.1086/718456\nAli, M., Fromm, M., Thellmann, K., Rutmann, R., Lüb-\nbering, M., Leveling, J., Klug, K., Ebert, J., Doll,\nN., Schulze Buschho ff, J., Jain, C., Weber, A. A.,\nJurkschat, L., Abdelwahab, H., John, C., Ortiz\nSuarez, P., Ostendorff, M., Weinbach, S., Sifa, R.,\n. . . Flores-Herr, N. (2023). Tokenizer choice for\nLLM training: Negligible or crucial? arXiv. https:\n//arxiv.org/abs/2310.08754\nAlishahi, A., Chrupała, G., & Linzen, T. (2019). Analyzing\nand interpreting neural networks for NLP: A report\non the first BlackboxNLP workshop. Natural Lan-\nguage Engineering, 25(4), 543–557. https://doi.org/\n10.1017/S135132491900024X\nAn, A., Qian, P., Wilcox, E., & Levy, R. (2019). Represen-\ntation of constituents in neural language models:\nCoordination phrase as a case study. arXiv preprint\narXiv:1909.04625.\nArgyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting,\nC., & Wingate, D. (2023). Out of one, many: Using\nlanguage models to simulate human samples. Polit-\nical Analysis, 31(3), 337–351. https: // doi.org /10.\n1017/pan.2023.2\nBender, E. M., Gebru, T., McMillan-Major, A., &\nShmitchell, S. (2021). On the dangers of stochas-\ntic parrots: Can language models be too big? Pro-\nceedings of the 2021 ACM conference on fairness,\naccountability, and transparency, 610–623.\nBhatia, S. (2023). Exploring the sources of variance in risky\ndecision making with large language models. https:\n//doi.org/10.31234/osf.io/3hrnc\nBinz, M., & Schulz, E. (2022). Modeling human exploration\nthrough resource-rational reinforcement learning.\nAdvances in Neural Information Processing Sys-\ntems, 35, 31755–31768.\nBinz, M., & Schulz, E. (2023a). Turning large language mod-\nels into cognitive models. arXiv. https: // arxiv.org/\nabs/2306.03917\n22 HUSSAIN, BINZ, MATA, & WULFF\nBinz, M., & Schulz, E. (2023b). Using cognitive psychology\nto understand GPT-3. Proceedings of the National\nAcademy of Sciences, 120(6), e2218523120. https:\n//doi.org/10.1073/pnas.2218523120\nBockting, C. L., Van Dis, E. A. M., Van Rooij, R., Zuidema,\nW., & Bollen, J. (2023). Living guidelines for gen-\nerative AI: Why scientists must oversee its use.Na-\nture, 622(7984), 693–696. https://doi.org/10.1038/\nd41586-023-03266-1\nBommasani, R., Klyman, K., Longpre, S., Kapoor, S.,\nMaslej, N., Xiong, B., Zhang, D., & Liang, P.\n(2023). The Foundation Model Transparency Index.\narXiv. https://arxiv.org/abs/2310.12941\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., et al. (2020). Language models are\nfew-shot learners. Advances in neural information\nprocessing systems, 33, 1877–1901.\nCassani, G., Guenther, F., Attanasio, G., Bianchi, F., &\nMarelli, M. (2023). Meaning modulations and sta-\nbility in large language models: An analysis of\nBERT embeddings for psycholinguistic research.\nPsyArXiv. https://doi.org/10.31234/osf.io/b45ys\nChae, Y ., & Davidson, T. (2023). Large language models for\ntext classification: From zero-shot learning to fine-\ntuning. OSF. https://osf.io/5t6xz/\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S.,\n& Amodei, D. (2017). Deep reinforcement learning\nfrom human preferences. Advances in Neural Ifor-\nmation Pocessing Systems, 30, 4299–4307.\nCoda-Forno, J., Binz, M., Akata, Z., Botvinick, M., Wang,\nJ. X., & Schulz, E. (2023). Meta-in-context learn-\ning in large language models. arXiv. https: // arxiv.\norg/abs/2305.12907\nCrossley, S., Heintz, A., Choi, J. S., Batchelor, J., Karimi,\nM., & Malatinszky, A. (2023). A large-scaled cor-\npus for assessing text readability. Behavior Re-\nsearch Methods, 55(2), 491–507.\nCutler, A., & Condon, D. M. (2023). Deep lexical hypothe-\nsis: Identifying personality structure in natural lan-\nguage. Journal of Personality and Social Psychol-\nogy, 125(1), 173–197. https : / /doi . org/ 10 . 1037/\npspp0000443\nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clap-\nper, M., Chandhok, S., Eichstaedt, J. C., Hecht, C.,\nJamieson, J., Johnson, M., Jones, M., Krettek-Cobb,\nD., Lai, L., JonesMitchell, N., Ong, D. C., Dweck,\nC. S., Gross, J. J., & Pennebaker, J. W. (2023). Us-\ning large language models in psychology. Nature\nReviews Psychology, 2, 688–701. https: // doi.org /\n10.1038/s44159-023-00241-5\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018).\nBERT: Pre-training of deep bidirectional transform-\ners for language understanding. arXiv. https://arxiv.\norg/abs/1810.04805\nFeng, S. F., Wang, S., Zarnescu, S., & Wilson, R. C. (2021).\nThe dynamics of explore–exploit decisions reveal a\nsignal-to-noise mechanism for random exploration.\nScientific Reports, 11(1), 3077. https: //doi.org /10.\n1038/s41598-021-82530-8\nFlesch, R. (1948). A new readability yardstick. Journal of\napplied psychology, 32(3), 221.\nFrantar, E., Ashkboos, S., Hoefler, T., & Alistarh, D. (2022).\nGPTQ: Accurate post-training quantization for gen-\nerative pre-trained transformers. arXiv. https : / /\narxiv.org/abs/2210.17323\nFrederick, S. (2005). Cognitive reflection and decision mak-\ning. Journal of Economic Perspectives , 19(4), 25–\n42. https://doi.org/10.1257/089533005775196732\nGershman, S. J. (2018). Deconstructing the human al-\ngorithms for exploration. Cognition, 173, 34–42.\nhttps://doi.org/10.1016/j.cognition.2017.12.014\nGilardi, F., Alizadeh, M., & Kubli, M. (2023). ChatGPT out-\nperforms crowd workers for text-annotation tasks.\nProceedings of the National Academy of Sciences ,\n120(30), e2305016120. https: // doi. org/10. 1073/\npnas.2305016120\nGoldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R.,\nAshton, M. C., Cloninger, C. R., & Gough, H. G.\n(2006). The International Personality Item Pool and\nthe future of public-domain personality measures.\nJournal of Research in Personality , 40(1), 84–96.\nhttps://doi.org/doi.org/10.1016/j.jrp.2005.08.007\nGraves, A. (2012).Supervised sequence labelling with recur-\nrent neural networks. Springer.\nGreen, Sanders, Weng, & Neelakantan. (2022, December\n15). New and improved embedding model.OpenAI.\nhttps :/ /openai . com/ blog / new - and - improved -\nembedding-model\nGünther, F., Rinaldi, L., & Marelli, M. (2019). Vector-space\nmodels of semantic representation from a cognitive\nperspective: A discussion of common misconcep-\ntions. Perspectives on Psychological Science, 14(6),\n1006–1033.\nHaigh, M. (2016). Has the standard Cognitive Reflection Test\nbecome a victim of its own success? Advances in\nCognitive Psychology, 12(3), 145–149. https: //doi.\norg/doi.org/10.5709/acp-0193-5\nHinton, G., Vinyals, O., & Dean, J. (2015). Distilling the\nknowledge in a neural network.\nHu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang,\nS., Wang, L., & Chen, W. (2022). LoRA: Low-rank\nadaptation of large language models. International\nConference on Learning Representations . https : / /\nopenreview.net/forum?id=nZeVKeeFYf9\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 23\nHussain, Z., Mata, R., & Wul ff, D. U. (2023). Novel em-\nbeddings improve the prediction of risk perception.\nPsyArXiv. https://doi.org/10.31234/osf.io/yrjfb\nIrving, G., & Askell, A. (2019). AI safety needs social scien-\ntists. Distill. https://doi.org/10.23915/distill.00014\nJelinek, F., Mercer, R. L., Bahl, L. R., & Baker, J. K. (1977).\nPerplexity—a measure of the di fficulty of speech\nrecognition tasks. The Journal of the Acoustical So-\nciety of America, 62(S1), S63–S63.\nKajonius, P. J., & Johnson, J. A. (2019). Assessing the struc-\nture of the Five Factor Model of Personality (IPIP-\nNEO-120) in the public domain. Europe’s Journal\nof Psychology, 15(2), 260–275. https: //doi.org /10.\n5964/ejop.v15i2.1671\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J.,\n& Amodei, D. (2020). Scaling laws for neural lan-\nguage models. arXiv. https: // arxiv.org /abs/2001.\n08361\nKaufman, S., Rosset, S., Perlich, C., & Stitelman, O. (2012).\nLeakage in data mining: Formulation, detection,\nand avoidance. ACM Transactions on Knowledge\nDiscovery from Data (TKDD), 6(4), 1–21.\nKorinek, A. (2023). Language models and cognitive automa-\ntion for economic research. NBER Working Paper\nSeries, (30957). https://doi.org/10.3386/w30957\nLeike, J., Krueger, D., Everitt, T., Martic, M., Maini, V ., &\nLegg, S. (2018). Scalable agent alignment via re-\nward modeling: A research direction. arXiv. https:\n//arxiv.org/abs/1811.07871\nLeike, J., & Sutskever, I. (2023, July 5). Introducing su-\nperalignment. OpenAI. https : // openai . com/ blog /\nintroducing-superalignment\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V ., & Zettlemoyer,\nL. (2019). BART: Denoising sequence-to-sequence\npre-training for natural language generation, trans-\nlation, and comprehension. arXiv. https://arxiv.org/\nabs/1910.13461\nLi, H. (2022). Language models: Past, present, and future.\nCommunications of the ACM , 65(7), 56–63. https:\n//doi.org/10.1145/3490443\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., & Stoyanov,\nV . (2019). ROBERTa: A robustly optimized BERT\npretraining approach. arXiv. https: // arxiv.org/abs/\n1907.11692\nMa, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S.,\nDong, L., Wang, R., Xue, J., & Wei, F. (2024). The\nera of 1-bit llms: All large language models are in\n1.58 bits. arXiv preprint arXiv:2402.17764.\nMerkx, D., & Frank, S. L. (2020). Human sentence pro-\ncessing: Recurrence or attention? arXiv preprint\narXiv:2005.09471.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Ef-\nficient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781.\nMitchell, M. (2023). How do we know how smart AI systems\nare? Science, 381(6654), eadj5957.\nMitchell, M., & Krakauer, D. C. (2023). The debate over un-\nderstanding in ai’s large language models.Proceed-\nings of the National Academy of Sciences, 120(13),\ne2215907120.\nMuennighoff, N., Tazi, N., Magne, L., & Reimers, N.\n(2022). MTEB: Massive Text Embedding Bench-\nmark. arXiv: https://arxiv.org/abs/2210.07316\nOpenAI. (2023). GPT-4 technical report. https://openai.com/\nresearch/gpt-4\nPelicon, A., Pranji ´c, M., Miljkovi ´c, D., Škrlj, B., & Pollak,\nS. (2020). Zero-shot learning for cross-lingual news\nsentiment classification. Applied Sciences, 10(17),\nArticle 5993. https://doi.org/10.3390/app10175993\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., & Liu, P. J. (2020).\nExploring the limits of transfer learning with a uni-\nfied text-to-text transformer. The Journal of Ma-\nchine Learning Research, 21(1), 5485–5551. https:\n//doi.org/10.5555/3455716.3455856\nRathje, S., Mirea, D.-M., Sucholutsky, I., Marjieh, R.,\nRobertson, C., & Van Bavel, J. J. (2023). GPT\nis an e ffective tool for multilingual psychological\ntext analysis. PsyArXiv. https : / /osf . io/ preprints /\npsyarxiv/sekf5/\nReimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence\nembeddings using siamese bert-networks. Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing. https: //arxiv.org/\nabs/1908.10084\nRosenbusch, H., Stevenson, C. E., & Van Der Maas,\nH. L. J. (2023). How accurate are GPT-3’s hypothe-\nses about social science phenomena? Digital Soci-\nety, 2, Article 26. https: //doi.org/10.1007/s44206-\n023-00054-2\nRussell, S. (2019). Human compatible: Artificial intelligence\nand the problem of control. Penguin.\nSanh, V ., Debut, L., Chaumond, J., & Wolf, T. (2019). Distil-\nBERT, a distilled version of BERT: Smaller, faster,\ncheaper and lighter. arXiv. https: // arxiv.org /abs/\n1910.01108\nSearle, J. R. (1980). Minds, brains, and programs.Behavioral\nand brain sciences, 3(3), 417–424.\nSiew, C. S., Wulff, D. U., Beckage, N. M., & Kenett, Y . N.\n(2019). Cognitive network science: A review of re-\nsearch on cognition through the lens of network rep-\n24 HUSSAIN, BINZ, MATA, & WULFF\nresentations, processes, and dynamics. Complexity,\n2019, Article 2108423. https: // doi.org /10.1155 /\n2019/2108423\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajb-\nhandari, S., Casper, J., Liu, Z., Prabhumoye, S.,\nZerveas, G., Korthikanti, V ., Zhang, E., Child, R.,\nAminabadi, R. Y ., Bernauer, J., Song, X., Shoeybi,\nM., He, Y ., Houston, M., Tiwary, S., & Catanzaro,\nB. (2022). Using DeepSpeed and Megatron to train\nMegatron-Turing NLG 530B, a large-scale genera-\ntive language model. arXiv. https: // arxiv.org /abs/\n2201.11990\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W.\n(2016). Increasing transparency through a multi-\nverse analysis. Perspectives on Psychological Sci-\nence, 11(5), 702–712.\nStrubell, E., Ganesh, A., & McCallum, A. (2019). Energy\nand policy considerations for deep learning in NLP.\narXiv. https://arxiv.org/abs/1906.02243\nSu, H., Shi, W., Kasai, J., Wang, Y ., Hu, Y ., Ostendorf,\nM., Yih, W.-t., Smith, N. A., Zettlemoyer, L., &\nYu, T. (2022). One embedder, any task: Instruction-\nfinetuned text embeddings. arXiv. https://arxiv.org/\nabs/2212.09741\nSucholutsky, I., Muttenthaler, L., Weller, A., Peng, A., Bobu,\nA., Kim, B., Love, B. C., Grant, E., Achterberg, J.,\nTenenbaum, J. B., Collins, K. M., Hermann, K. L.,\nOktar, K., Gre ff, K., Hebart, M. N., Jacoby, N.,\nZhang, Q., Marjieh, R., Geirhos, R., . . . Gri ffiths,\nT. L. (2023). Getting aligned on representational\nalignment. arXiv. https://arxiv.org/abs/2310.13018\nTheBloke. (2022). Llama-2-7b-chat-gptq. https : / /\nhuggingface . co/ TheBloke / Llama - 2 - 7b - Chat -\nGPTQ\nTII. (2023). Falcon-40b-instruct: A 40b parameters causal\ndecoder-only model [Accessed: 2023-11-16]. https:\n//huggingface.co/tiiuae/falcon-40b-instruct\nTörnberg, P. (2023). ChatGPT-4 outperforms experts and\ncrowd workers in annotating political Twitter mes-\nsages with zero-shot learning. arXiv. https: // arxiv.\norg/abs/2304.06588\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava,\nP., Bhosale, S., et al. (2023). Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nTunstall, L., V on Werra, L., & Wolf, T. (2022).Natural lan-\nguage processing with transformers. O’Reilly.\nTuring, A. M. (1950). I.—COMPUTING MA-\nCHINERY AND IN LIGENCE [_eprint:\nhttps://academic.oup.com/mind/article-\npdf/LIX/236/433/30123314/lix-236-433.pdf].\nMind, 59(236), 433–460. https: //doi.org /10.1093 /\nmind/LIX.236.433\nVan Noorden, R., & Perkel, J. M. (2023). AI and science:\nWhat 1,600 researchers think. Nature, 621(7980),\n672–675. https: // doi.org /10.1038 /d41586- 023-\n02980-0\nVarma, S., & Simon, R. (2006). Bias in error estimation when\nusing cross-validation for model selection. BMC\nBioinformatics, 7(1), 1–8. https: //doi.org/10.1186/\n1471-2105-7-91\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017).\nAttention is all you need.Advances in Neural Infor-\nmation Processing Systems, 30, 5998–6008.\nVicente-Saez, R., & Martinez-Fuentes, C. (2018). Open sci-\nence now: A systematic literature review for an inte-\ngrated definition. Journal of Business Research, 88,\n428–436. https://doi.org/10.1016/j.jbusres.2017.12.\n043\nVig, J. (2019). A multiscale visualization of attention in the\ntransformer model. arXiv. https : / /arxiv. org/ abs /\n1906.05714\nVig, J., & Belinkov, Y . (2019). Analyzing the structure of\nattention in a transformer language model. arXiv.\nhttps://arxiv.org/abs/1906.04284\nWang, T., Roberts, A., Hesslow, D., Le Scao, T., Chung,\nH. W., Beltagy, I., Launay, J., & Ra ffel, C. (2022).\nWhat language model architecture and pretraining\nobjective works best for zero-shot generalization?\nIn K. Chaudhuri, S. Jegelka, L. Song, C. Szepes-\nvari, G. Niu, & S. Sabato (Eds.), Proceedings of the\n39th international conference on machine learning\n(pp. 22964–22984). https: //proceedings.mlr.press /\nv162/wang22u.html\nWang, Y ., Huang, H., Rudin, C., & Shaposhnik, Y .\n(2021). Understanding how dimension reduction\ntools work: An empirical approach to deciphering\nt-SNE, UMAP, TriMAP, and PaCMAP for data vi-\nsualization. The Journal of Machine Learning Re-\nsearch, 22(1), 9129–9201. https: // dl.acm.org /doi/\nabs/10.5555/3546258.3546459\nWebb, T., Holyoak, K. J., & Lu, H. (2023). Emergent ana-\nlogical reasoning in large language models. Nature\nHuman Behaviour, 7(9), 1526–1541. https : / /doi .\norg/https://doi.org/10.1038/s41562-023-01659-w\nWei, J., Tay, Y ., Bommasani, R., Ra ffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., Chi, E. H., Hashimoto, T., Vinyals, O.,\nLiang, P., Dean, J., & Fedus, W. (2022). Emergent\nabilities of large language models. arXiv. https ://\narxiv.org/abs/2206.07682\nWeidinger, L., Uesato, J., Rauh, M., Gri ffin, C., Huang,\nP.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B.,\nTUTORIAL ON OPEN-SOURCE LARGE LANGUAGE MODELS 25\nKasirzadeh, A., Biles, C., Brown, S., Kenton, Z.,\nHawkins, W., Stepleton, T., Birhane, A., Hendricks,\nL. A., Rimell, L., Isaac, W., . . . Gabriel, I. (2022).\nTaxonomy of risks posed by language models. Pro-\nceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, 214–229. https:\n//doi.org/10.1145/3531146.3533088\nWetzel, L. (2018). Types and Tokens. In E. N. Zalta\n(Ed.), The Stanford encyclopedia of philosophy\n(Fall 2018). Metaphysics Research Lab, Stanford\nUniversity.\nWidmann, T., & Wich, M. (2023). Creating and comparing\ndictionary, word embedding, and transformer-based\nmodels to measure discrete emotions in German po-\nlitical text.Political Analysis, 31(4), 626–641. https:\n//doi.org/10.1017/pan.2022.15\nWilson, R. C., Geana, A., White, J. M., Ludvig, E. A.,\n& Cohen, J. D. (2014). Humans use directed and\nrandom exploration to solve the explore–exploit\ndilemma. Journal of Experimental Psychology:\nGeneral, 143(6), 2074–2081. https : // doi . org/10 .\n1037/a0038199\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,\nMacherey, W., Krikun, M., Cao, Y ., Gao, Q.,\nMacherey, K., Klingner, J., Shah, A., Johnson, M.,\nLiu, X., Kaiser, Ł., Gouws, S., Kato, Y ., Kudo, T.,\nKazawa, H., . . . Dean, J. (2016). Google’s neural\nmachine translation system: Bridging the gap be-\ntween human and machine translation. arXiv. https:\n//arxiv.org/abs/1609.08144\nWulff, D. U., & Mata, R. (2023). Automated jingle–jangle\ndetection: Using embeddings to tackle taxonomic\nincommensurability. https://doi.org /10.31234 /osf.\nio/9h7aw\nYax, N., Anlló, H., & Palminteri, S. (2023). Studying and im-\nproving reasoning in humans and machines. arXiv.\nhttps://arxiv.org/abs/2309.12485",
  "topic": "Open science",
  "concepts": [
    {
      "name": "Open science",
      "score": 0.6310235857963562
    },
    {
      "name": "Computer science",
      "score": 0.6018666625022888
    },
    {
      "name": "Executable",
      "score": 0.5979708433151245
    },
    {
      "name": "Behavioral modeling",
      "score": 0.558729350566864
    },
    {
      "name": "Interpretability",
      "score": 0.5486434698104858
    },
    {
      "name": "Data science",
      "score": 0.5092414617538452
    },
    {
      "name": "Conceptualization",
      "score": 0.473821759223938
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.45997121930122375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.20230528712272644
    },
    {
      "name": "Programming language",
      "score": 0.16827073693275452
    },
    {
      "name": "Computer security",
      "score": 0.16086864471435547
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1850255",
      "name": "University of Basel",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210120221",
      "name": "Max Planck Institute for Human Development",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210112925",
      "name": "Max Planck Institute for Biological Cybernetics",
      "country": "DE"
    }
  ],
  "cited_by": 14
}