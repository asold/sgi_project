{
    "title": "On the effect of dropping layers of pre-trained transformer models",
    "url": "https://openalex.org/W3136363192",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2256769809",
            "name": "Hassan Sajjad",
            "affiliations": [
                "Dalhousie University"
            ]
        },
        {
            "id": "https://openalex.org/A2576593349",
            "name": "Fahim Dalvi",
            "affiliations": [
                "Hamad bin Khalifa University"
            ]
        },
        {
            "id": "https://openalex.org/A2123720402",
            "name": "Nadir Durrani",
            "affiliations": [
                "Hamad bin Khalifa University"
            ]
        },
        {
            "id": "https://openalex.org/A1989325080",
            "name": "Preslav Nakov",
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2256769809",
            "name": "Hassan Sajjad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2576593349",
            "name": "Fahim Dalvi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123720402",
            "name": "Nadir Durrani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1989325080",
            "name": "Preslav Nakov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385567355",
        "https://openalex.org/W6736082010",
        "https://openalex.org/W6780438243",
        "https://openalex.org/W6786240170",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W6757034678",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6605323724",
        "https://openalex.org/W6796578917",
        "https://openalex.org/W7066667914",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W6760434389",
        "https://openalex.org/W6762945437",
        "https://openalex.org/W6718053083",
        "https://openalex.org/W2995816250",
        "https://openalex.org/W6800911330",
        "https://openalex.org/W6838720080",
        "https://openalex.org/W6691459498",
        "https://openalex.org/W6767023738",
        "https://openalex.org/W6769995297",
        "https://openalex.org/W6762537594",
        "https://openalex.org/W2970820321",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W6750615492",
        "https://openalex.org/W6737236263",
        "https://openalex.org/W2924902521",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2946817437",
        "https://openalex.org/W2773956126",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2963400886",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W3101248447",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3034709122",
        "https://openalex.org/W3019828385",
        "https://openalex.org/W4280538731",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3103368673",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3196986263",
        "https://openalex.org/W3101163004",
        "https://openalex.org/W3183859557",
        "https://openalex.org/W3038012435",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W4224051624",
        "https://openalex.org/W2599674900",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W3174088532",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3103838460",
        "https://openalex.org/W2998216295",
        "https://openalex.org/W4241900798",
        "https://openalex.org/W3104136798",
        "https://openalex.org/W2771472444",
        "https://openalex.org/W2970557265",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2597601064",
        "https://openalex.org/W3034296505",
        "https://openalex.org/W3104350794",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2980965328",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W4238634189",
        "https://openalex.org/W4288347855",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2773621464",
        "https://openalex.org/W4237040408",
        "https://openalex.org/W2989458363",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2971031791",
        "https://openalex.org/W3035038672",
        "https://openalex.org/W3170035135",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W4283691240",
        "https://openalex.org/W3037116584",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2997244573",
        "https://openalex.org/W2973061659",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2974875810",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3034292689",
        "https://openalex.org/W2986591322"
    ],
    "abstract": null,
    "full_text": "On the Eﬀect of Dropping Layers of Pre-trained\nTransformer Models\nHassan Sajjad♣1 Fahim Dalvi♦ Nadir Durrani♦ Preslav Nakov♠1\n♣Faculty of Computer Science, Dalhousie University, Canada\n♦Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar\n♠Mohamed bin Zayed University of Artiﬁcial Intelligence, Abu Dhabi, UAE\nhsajjad@dal.ca,{faimaduddin, ndurrani}@hbku.edu.qa, preslav.nakov@mbzuai.ac.ae\nAbstract\nTransformer-based NLP models are trained using billions of parameters, lim-\niting their applicability in computationally constrained environments. While\nthe number of parameters generally correlates with performance, it is not clear\nwhether the entire network is required for a downstream task. Motivated by\nthe recent work on pruning and distilling pre-trained models, we explore strate-\ngies to drop layers in pre-trained models, and observe the eﬀect of pruning on\ndownstream GLUE tasks. We were able to prune BERT, RoBERTa and XLNet\nmodels up to 40%, while maintaining up to 98% of their original performance.\nAdditionally we show that our pruned models are at par with those built using\nknowledge distillation, both in terms of size and performance. Our experiments\nyield interesting observations such as: (i) the lower layers are most critical to\nmaintain downstream task performance, (ii) some tasks such as paraphrase de-\ntection and sentence similarity are more robust to the dropping of layers, and\n(iii) models trained using diﬀerent objective function exhibit diﬀerent learning\npatterns and w.r.t the layer dropping. 1\nKeywords: pre-trained transformer models, eﬃcient transfer learning,\ninterpretation and analysis\n1The work was done while the author was at QCRI\n1The code is available at https://github.com/hsajjad/transformers/.\nPreprint submitted to Journal of Computer Speech and Language August 16, 2022\narXiv:2004.03844v3  [cs.CL]  13 Aug 2022\n1. Introduction\nPre-trained Transformer models have achieved state-of-the-art performance\non natural language processing tasks and have been adopted as feature extrac-\ntors for solving downstream tasks such as question answering, natural language\ninference, and sentiment analysis. The current state-of-the-art Transformer-\nbased pre-trained models consist of dozens of layers and millions of parameters.\nWhile deeper and wider models yield better performance, they also need large\nGPU/TPU memory. For example, BERT-large [1] is trained with 335 million\nparameters, and requires at least 24 GB of GPU memory to load. The larger\nsize of these models limits their applicability in time- and memory-constrained\nenvironments.\nSeveral methods have been proposed to reduce the size of pre-trained mod-\nels. Notable approaches include pruning parts of the network after training [2,\n3, 4], reduction through weight factorization and sharing [5], compression via\nknowledge-distillation [6] and quantization [7, 8]. Our work falls under the class\nof pruning methods.\nThe central argument governing pruning methods is that deep neural models\nare over-parameterized and that not all parameters are strictly needed, espe-\ncially at the inference time. For example, previous research has shown that most\nof the attention heads can be removed [9, 3] or reallocated [10] without signif-\nicantly impacting performance. Gordon et al. [11] pruned the least important\nweights in the network. We build our work based on similar observations, but\nwe are interested in (i) whether it is necessary to use all layers of a pre-trained\nmodel for downstream tasks, and if not, ( ii) which layers are necessary to keep\nin order to maintain good task-speciﬁc performance while achieving eﬃciency\nin transfer learning.\nMotivated by recent ﬁndings in representation learning, we propose novel\nstrategies to drop layers in pre-trained models. Voita et al. [12] showed that\nthe top layers are biased towards the pre-training objective, leading us to ques-\ntion whether they are necessary for downstream tasks. Michel et al. [9], Dalvi\n2\net al. [13] discussed over-parameterization and the redundancy in pre-trained\nmodels, leading us to question whether adjacent layers contain redundant infor-\nmation. More concretely, we drop top, bottom, middle, or alternate layers in the\nnetwork. We additionally present methods to ﬁnd layers that contribute least\nin the network by using their activation patterns and weights. We apply our\nstrategies on four state-of-the-art pre-trained models, BERT [1], RoBERTa [14],\nALBERT [5] and XLNet [15]. The ﬁrst three are auto-encoders, while XLNet is\nan auto-regressive model. ALBERT presents an interesting case in the mix as\nits layers share parameters. We additionally experiment using DistilBERT to\nanalyze whether a distilled model can be pruned further. We evaluate against\nGLUE benchmark [16] a suite of language understanding tasks. Our ﬁndings\nare summarized below:\n• We propose practical strategies to drop layers in pre-trained models for\neﬃcient transfer learning.\n• We show that dropping top layers works consistently well across diﬀerent\ntasks and pre-trained models, e.g., yielding 40% reduction in size while\npreserving up to 98.2% of the performance.\n• Our reduced models perform on par with models built using knowledge\ndistillation in terms of accuracy, model size and inference speed, without\nrequiring costly training of a new model.\n• One-third of a distilled models can also be pruned successfully with an\naverage loss of 0.75 points\n• Despite having cross-layer parameter sharing, ALBERT can still be pruned\nfor eﬃcient inference with a small drop in performance.\n• Certain downstream tasks require as few 3 layers to maintain performance\nwithin 1% threshold.\n• Comparing architectures, models show diﬀerent learning dynamics. For\nexample, compared to BERT, RoBERTa and XLNet learn task-speciﬁc\n3\nknowledge earlier in the network and are thus more robust to layer-dropping.\nContribution. While a number of studies partially overlap with the strategies\nand the ﬁndings presented in this work, this is the ﬁrst work that thoroughly\ninvestigates the eﬀect of various layer-dropping methods using a variety of pre-\ntrained models and on a large number of tasks. We showed that i) models\nhave diﬀerent learning dynamics, ii) a smaller close to optimal network can be\nachieved by optimizing the number of layers to drop with respect to the task\nat hand, iii) a distilled model can also beneﬁt from layer-dropping. Our work\nrecommends to use top layer-dropping as an essential baseline when building\ndistilled models. Moreover, it provides a cheap way to get smaller models of\nany architecture rapidly, that are both memory and speed eﬃcient.\n2. Related Work\nEﬃcient Pre-trained Models: Work done on exploring methods to down-\nscale pre-trained models can be categorized into architecture-invariant compres-\nsion [5, 17, 8], knowledge distillation [18, 6], and pruning [11, 19].\nQuantization [7, 8], an architecture-invariant method, reduces the numerical\nprecision of the weights of the model to fewer bits. Knowledge distillation (KD)\nalso known as student-teacher model [20] trains a smaller model that mimics\nthe behavior of the larger model. Researchers have experimented with learning\nfrom the outputs of the encoder layers [21, 22], from the output logits [6, 23],\nand from the attention maps [22, 24]. Another distinction is between general-\npurpose distilled models [6, 24] and task-speciﬁc ones [22, 25, 23, 21, 26].\nPruning methods involve removing some parts of the networks that are either\nredundant or less relevant to the task at hand. [11, 19, 27] pruned the least\nimportant weights in the network. Michel et al. [9], Voita et al. [3] demonstrated\nthat most of the attention heads can be pruned at test time, which reduces the\ncomputation, and speeds up inference. Fan et al. [28] introduced LayerDrop\nduring training that resulted in pre-trained models that are robust towards\ndropping of layers at inference time. Our work is similar to them as we also\n4\nremove layers from the network. But we show that layers can be dropped\nsafely from the pre-trained models without the need for additional training\nusing LayerDrop. Nevertheless our strategies can also be applied to a model\ntrained using LayerDrop.\nRecently, Peer et al. [29] proposed a greedy layer pruning method that drops\nlayers based on their independent performance on the end task. Their assump-\ntion is that a local decision about a layer aligns with a globally correct selection\nof layers. We demonstrate that our results are comparable to theirs, but we\nneed no additional training to ﬁnd an optimal set of layers.\nSun et al. [21], Xu et al. [30] used the bottom six layers of the BERT-base\nmodel to initialize the student model. This is similar to one of our strategies.\nHowever, their performance is much lower compared to our method. Moreover,\nwe provide a comprehensive evaluation of our strategies on four pre-trained\nmodels to prove their eﬃcacy in reducing the size of the network.\nLiu et al. [31], Schwartz et al. [32], Xin et al. [33], Zhou et al. [34] speed up\nthe inference time by introducing dynamic exiting strategies. The limitation of\ntheir work are the memory footprints of the model that remain identical to the\noriginal model.\nRepresentation analysis: A number of studies have analyzed representations\nof pre-trained models at layer-level and showed that they learn linguistic in-\nformation [35, 36, 37, 38, 39, 40, 41, 42, 43, 44]. Belinkov et al. [45], Sajjad\net al. [46] provided a comprehensive literature review of such work. While the\nrepresentation analysis uncovers, what linguistic properties diﬀerent layers cap-\nture, they do not reﬂect which layers are important for transfer learning to a\ndownstream task. Recently, Tamkin et al. [47], Merchant et al. [48], Durrani\net al. [49] attempted to address this by analyzing layer-wise transferability of\nfeatures during ﬁne-tuning. Tamkin et al. [47] reinitialized individual layers of\npre-trained model and observed the eﬀect on the ﬁne-tuning performance. Mer-\nchant et al. [48] used probing classiﬁer, layer-wise similarity and layer-ablation\nfor their analysis. Our work is similar to their layer-ablation study which they\ncarried out to understand the diﬃculty of a downstream task, but the premise of\n5\nFigure 1: Illustration of layer-dropping strategies. K represents the number of layers that are\ndropped. For example, K = 4 in the top-layer strategy means top four layers of the model are\ndropped. In the contribution-based dropping, we select layers based on a similarity threshold.\nThe number mentioned in the ﬁgure e.g. [2,3] shows the layers which are dropped based on\nthe similarity threshold.\nour work is very diﬀerent. We gauge the importance of various subsets of layers\nwith respect to the performance on downstream tasks, to achieve eﬃcient mod-\nels. Durrani et al. [49] used layer-wise and neuron probing classiﬁers [50, 51]\nand showed that core-linguistic knowledge is preserved in the lower layers of\nﬁne-tuned models. This resonates with our empirical ﬁnding that shows that\nhigher layers can be safely pruned for eﬃcient transfer learning.\n3. Methodology\nConsider a pre-trained language model M with an embedding layer E0 and\nL encoder layers: {l1, l2, . . . , lL}. We probe whether it is necessary to keep all\nlayers of the network for downstream tasks. We explore six strategies, that we\ndescribe below (also shown in Figure 1), to drop encoder layers from the model.\nEach pruning regime is followed by task-speciﬁc ﬁne-tuning to analyze the eﬀect\nof layer-dropping on the performance of the task.\n3.1. Top-Layer Dropping\nThe top layers in pre-trained models are specialized towards the underlying\nobjective function [12]. Zhang et al. [52] reinitialized the upper layers when\nﬁne-tuning towards GLUE task. We hypothesize that the top layers may not be\n6\nimportant when ﬁne-tuning towards the a downstream task. In this strategy,\nwe drop top K layers from the model. The output of layer lL−K serves as the\nlast layer of the reduced network. Then, a task-speciﬁc layer is added on top of\nthis layer to perform task-speciﬁc ﬁne-tuning. Figure 1 shows an example with\ndropping top 4 and 6 layers.\n3.2. Alternate Dropping\nDeep neural networks are innately redundant. Sun et al. [21] and Jiao\net al. [22] amalgamated information from adjacent layers of the teacher model\ninto a single layer of the student model. We hypothesize that neighbouring\nlayers preserve similar information and may be dropped safely without any\nsubstantial loss of information. We drop N alternating odd or even layers from\ntop to bottom of the network. For example in a 12-layer model with K = 4,\nwe consider two sets of alternate layers: Odd-alternate Dropping – {5,7,9,11}\nand Even-alternate Dropping – {6,8,10,12}, see Figure 1 for illustration. When\ndropping an in-between layer li, the output of the previous layer li−1 becomes\nthe input of the next layer li+1, causing a mismatch in the expected input to\nli+1. However, we hope that during task-speciﬁc ﬁne-tuning, the model will\nrecover from this discrepancy.\n3.3. Parameter-Based Dropping\nIn this approach, we estimate the importance of a given layer based on the\nmodel parameters. More speciﬁcally, we rank the layers based on their weights.\nWe tested two hypotheses: ( i) higher magnitude of the weights signals higher\nlayer importance, (ii) higher variance of the weights corresponds to higher layer\nimportance. We refer to the former as Aggregation Method, where we aggre-\ngate the weights of a layer, and we call the latter a Variance Method, where we\ncalculate the variance of each layer. We drop the layers with the lowest aggre-\ngation or variance scores. Note that a transformer block has various sub-layers,\nbut in our experiments we only used the ﬁnal weights. We leave experiments\nwith other layers within a transformer block as a possible direction for future\nwork.\n7\n3.4. Contribution-Based Dropping\nOur next strategy is based on the idea that a layer contributing below a\ncertain threshold might be a good candidate for dropping. We deﬁne the con-\ntribution of a layer li in terms of the cosine similarity between its input and\nits output representations. A layer li with a high similarity (above a certain\nthreshold) indicates that its output has not changed much from its input, and\ntherefore it can be dropped from the network. More concretely, in the forward\npass, we calculate the cosine similarity between the representation of the sen-\ntence token (CLS) before and after each layer. We average the similarity scores\nof each layer over the development set, and select layers that have an aver-\nage similarity above a certain threshold for dropping. This contribution-based\nstrategy can be seen as a principled variation of alternate dropping.\n3.5. Symmetric Dropping\nThe bottom layers are closer to the input while the top layers are closer\nto the output. It is possible that both the top layers and the bottom layers\nare more important than the middle layers. The Symmetric dropping strategy\nretains the top and the bottom X layers, and drop K middle layers, where\n2X + K = L. For example, in a 12-layer model, if K = 6, we retain three top\nand three bottom layers, dropping layers 4–9. The output of layer 3 would then\nserve as an input to layer 10.\n3.6. Bottom-Layer Dropping\nPrevious work on analyzing layers in Neural Networks [35, 41, 39, 53, 54] has\nshown that the lower layers model local interactions between words (which is\nimportant for morphology and lexical semantics), thus providing essential input\nto the higher layers. Removing lower layers could be therefore catastrophic. We\nstill perform these experiments for the sake of completeness. We remove the\nbottom K layers of the model. The output of the embedding layer l0 serves as\nan input to layer lK+1 of the original model.\n8\nTask Description Train Dev\nSST-2 Sentiment analysis 67349 872\nMRPC Microsoft Research paraphrase corpus 3668 408\nMNLI Natural language inference 392702 9815\nQNLI Question natural language inference 104743 5463\nQQP Quora question pairs 363846 40430\nRTE Recognizing textual entailment 2490 277\nSTS-B Semantic textual similarity 5749 1500\nTable 1: Data statistics of the GLUE tasks. All tasks are binary classiﬁcation tasks, except\nfor STS-B which is a regression task. Recall that the test sets are not publicly available, and\nhence we use development set to report results.\n4. Experimental Setup\nDatasets. We evaluated our strategies on General Language Understanding\nEvaluation (GLUE) tasks [16] tasks, which serves as a defacto standard to eval-\nuate pre-trained language models. Table 1 provides statistics of each dataset.\nMore speciﬁcally, we evaluated on the following tasks: SST-2 for sentiment anal-\nysis with the Stanford sentiment treebank [55], MNLI for natural language in-\nference [56], QNLI for Question NLI [57], QQP for Quora Question Pairs,2 RTE\nfor recognizing textual entailment [58], MRPC for Microsoft Research para-\nphrase corpus [59], and STS-B for the semantic textual similarity benchmark\n[60]. We left out WNLI, due to the irregularities in its dataset, as also reported\nby others,3 as well as CoLA due to large variance and unstable results across\nﬁne-tuning runs.\nModels. We experimented with three state-of-the-art 12-layered pre-trained mod-\nels 4 BERT [1], RoBERTa [14] and XLNet [15]. We additionally experimented\nusing a 12-layered ALBERT [5] model and a distilled model, DistilBERT [6].\n2http://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n3http://gluebenchmark.com/faq\n4For the sake of clarity when the trends are similar across models, we present the results\nof selected models only.\n9\nOur selection of models encourage interesting comparison between diﬀerent\ntypes of models such as auto-regressive vs. auto-encoder and a large model\nvs. its distilled version. All experiments are conducted using the transformers\nlibrary [61]. We used the default settings and did not optimize the parameters.\nWe limit our experiments to the base versions of the transformers as we could\nnot experiment with BERT-large or XLNet-large due to memory limitations. 5\nHowever, our strategies are straightforward to apply to models of any depth.\nEnd-to-End Procedure. Given a pre-trained model, we drop layers using one\nof the strategies described in Section 3. We then performed task-speciﬁc ﬁne-\ntuning using GLUE training sets for three epochs as prescribed by [1] 6 and\nevaluated on the oﬃcial devsets.\n5. Evaluation Results\nWe experimented with dropping K number of layers where K = 2, 4, 6 in\nBERT, RoBERTa and XLNet, andK = 1, 2, 3 in DistilBERT (a 6-layer model).\nAs an example, for K = 2 on a 12-layer model, we drop the following layers:\ntop strategy – {11, 12}; bottom strategy – {1, 2}; even-alternate – {10, 12};\nodd-alternate – {9, 11}; symmetric – {6, 7}. For the parameter-based strategy,\nwe calculate the score of every layer based on the aggregated weights and the\nvariance in the weights, and we drop the layers with the lowest score. In the\ncontribution-based strategy, the dropping of layers is dependent on a similarity\nthreshold. We calculate the similarity between input and output of each layer\nand remove layers with similarity above the threshold values of 0 .95, 0.925 and\n0.9. These values were chosen empirically. A threshold value below 0.9 or above\n5In order to ﬁt large models in our TitanX 12GB GPU cards, we tried to reduce the\nbatch size, but this yielded poor performance, see https://github.com/google-research/\nbert#out-of-memory-issues .\n6We experimented with using more epochs, especially for dropping strategies that exclude\nin-between layers, in order to let the weight matrix adapt to the changes. However, we did\nnot see any beneﬁt in going beyond three epochs.\n10\n0.95 resulted in either more than half of the network being considered as similar,\nor none of the layers to be similar.\n5.1. Comparing Strategies\nFigure 2 presents average classiﬁcation performance of BERT and XLNet\nusing various layer-dropping strategies. We observe similar trends for RoBERTa\nand DistilBERT and limit the presentation of results to two models here.\nTop-layer dropping consistently outperforms other strategies when\ndropping 6 layers. We dropped half of the top layers (yellow bars in the top\nstrategy) with an average loss of only 2.91 and 1.81 points for BERT and XLNet\nrespectively. The Bottom-layer dropping strategy performed the worst across all\nmodels, as expected, showing that it is more damaging to remove information\nfrom the lower layers of the network. The behavior of top and bottom dropping\nis consistent across all models. It nicely connects with ﬁndings in representation\nlearning, i.e. lower layers learn core-linguistic phenomena and our results show\nthat they are important to maintain task-speciﬁc performance.\nParameter-based strategy using variance is the second best strat-\negy at K = 6 . Compared to most of the other strategies presented in this\nwork, the parameter-based strategy makes a more informed decision based on\nthe parameters of the model, i.e., the weights. We found the variance-based\nstrategy to outperform the aggregation-based one, and thus we limit our discus-\nsion to the former only. The variance-based method selected diﬀerent layers to\ndrop for each model. The order of the six layers to drop is {1, 12, 8, 9, 11, 2}for\nBERT, {11, 12, 6, 7, 5, 10}for RoBERTa and {11, 12, 7, 8, 9, 10}for XLNet. One\ncommon observation here is that the last 2–3 layers and the middle layers of the\nmodels can be removed safely with a small drop in performance (see the results\nof the variance-based method in Figure 2). Moreover, BERT is an exception\nwhere the ﬁrst two contextualized layers {1, 2}are also selected to be removed.\nThis resulted in a huge loss in performance (see the results for BERT when\ndropping 6 layers based on the variance-based method). Interestingly, dropping\n6-layers of XLNet resulted in a model that was identical to that of the top-layer\n11\nstrategy, i.e., removing the top-6 layers. RoBERTa presents an interesting case\nwhere the parameter-based strategy resulted in a drop of the middle layers and\nof the top layers, while keeping the lower and the higher middle layers. The\naverage results for RoBERTa when using the variance-based method are lower\nby 0.73 point only compared to the top-layer method. The promising results of\nthe parameter-based method on two out of three models show its eﬃcacy. Note\nthat our current exploration is limited to the parameters of the base models.\nFine-tuning substantially changes the parameters [49], which may result in a\ntask-wise informed dropping of layers. We did not try task-speciﬁc pruning as\nthe focus of our work is on task-agnostic eﬃcient models.\nDropping top alternate layers is better than dropping top consec-\nutive layers. The Odd-alternate dropping strategy gave better results than\nthe top at K = 2 (blue bars in the Odd-alternate strategy), across all the\ntasks. Looking at the layers that were dropped: top – {11, 12}; even-alternate\n– {10, 12}; odd-alternate – {9, 11}, we can say that ( i) dropping last two con-\nsecutive layers {11, 12}is more harmful than removing alternate layers, and\n(ii) keeping the last layer {9, 11}is more important than keeping the second\nlast layer with its alternate pair. At K = 6, the Alternate dropping strategies\nshow a large drop in the performance, perhaps due to removal of lower lay-\ners. Recall that our results from the bottom strategy showed lower layers to be\ncritical for transfer learning.\nThe Symmetric strategy gives importance to both top and bottom layers and\ndrops the middle layers. Dropping two middle layers from BERT degrades the\nperformance by 0 .97 points and makes it the second best strategy at K = 2.\nHowever, on XLNet the performance degrades drastically when dropping the\nsame set of layers. Comparing these two models, XLNet is sensitive to the\ndropping of middle layers while BERT shows competitive results to the Top-\nlayer dropping strategy even after removing 4 middle layers. We analyze the\ndiﬀerence in the behavior of models in Section 6.\nFor Contribution-based strategy, we chose layers {3, 5}at threshold 0.95 and\n{3, 5, 8, 9}at threshold 0.925 for BERT, and layers{9, 10, 11}at threshold 0.925\n12\n(a) BERT\n(b) XLNet\nFigure 2: Average classiﬁcation performance on GLUE tasks when using diﬀerent layer-\ndropping strategies and when removing diﬀerent numbers of layers for BERT and XLNet.\nNote that the contribution-based strategy selects layers based on the similarity threshold. In\nsome cases it does not select (2,4 or 6) number of layers, which results in some missing bars\nin the ﬁgure. The horizontal red line represents the results using the full model.\nand {8, 9, 10, 11}at threshold 0.9 for XLNet. Using a lower or a higher similarity\nthreshold resulted in dropping none or more than half of the layers in the network\nrespectively. For BERT, the contribution-based dropping did not work well since\nthe method chose a few lower layers for dropping. On the contrary, it worked\nquite well on XLNet where higher layers were selected. This is in-line with the\nﬁndings of top and bottom strategy that all models are robust to dropping of\nhigher layers compared to dropping of lower layers.\nThe contribution-based strategy is based on the activations of each layer,\nwhich is an input-dependent process. Depending on the nature of the input\nor the task, the activation patterns will change. We suspect that this is one\nof the reasons for the failure of the strategy. A strategy based on task-speciﬁc\n13\nDrop.SST-2 MNLI QNLI QQP STS-B RTE MRPC\nBERT\n0/12 92.43 84.04 91.12 91.07 88.79 67.87 87.99\n2/12 92.20 (0.23↓) 83.26 (0.78↓) 89.84 (1.28↓) 90.92 (0.15↓) 88.70 (0.09↓) 62.82 (5.05↓) 86.27 (1.72↓)\n4/12 90.60 (1.83↓) 82.51 (1.53↓) 89.68 (1.44↓) 90.63 (0.44↓) 88.64 (0.15↓) 67.87 (0.00) 79.41 (8.58↓)\n6/12 90.25 (2.18↓) 81.13 (2.91↓) 87.63 (3.49↓) 90.35 (0.72↓) 88.45 (0.34↓) 64.98 (2.89↓) 80.15 (7.84↓)\nRoBERTa\n0/12 92.20 86.44 91.73 90.48 89.87 68.95 88.48\n2/12 93.46 (1.26↑) 86.53 (0.09↑) 91.23 (0.50↓) 91.02 (0.54↑) 90.21 (0.34↑) 71.84 (2.89↑) 89.71 (1.23↑)\n4/12 93.00 (0.80↑) 86.20 (0.24↓) 90.57 (1.16↓) 91.12 (0.64↑) 89.77 (0.10↓) 70.40 (1.45↑) 87.50 (0.98↓)\n6/12 91.97 (0.23↓) 84.44 (2.00↓) 90.00 (1.73↓) 90.91 (0.43↑) 88.92 (0.95↓) 64.62 (4.33↓) 85.78 (2.70↓)\nXLNET\n0/12 93.92 85.97 90.35 90.55 88.01 65.70 88.48\n2/12 93.35 (0.57↓) 85.67 (0.30↓) 89.35 (1.00↓) 90.69 (0.14↑) 87.59 (0.42↓) 66.06 (0.36↑) 86.52 (1.96↓)\n4/12 92.78 (1.14↓) 85.46 (0.51↓) 89.51 (0.84↓) 90.75 (0.20↑) 87.74 (0.27↓) 67.87 (2.17↑) 87.25 (1.23↓)\n6/12 92.20 (1.72↓) 83.48 (2.49↓) 88.03 (2.32↓) 90.62 (0.07↑) 87.45 (0.56↓) 65.70 (0.00) 82.84 (5.64↓)\nDistilBERT\n0/6 90.37 81.78 88.98 90.40 87.14 60.29 85.05\n1/6 90.37 (0.00) 80.41 (1.37↓) 88.50 (0.48↓) 90.33 (0.07↓) 86.21 (0.93↓) 59.93 (0.36↓) 84.80 (0.25↓)\n2/6 90.25 (0.12↓) 79.41 (2.37↓) 86.60 (2.38↓) 90.19 (0.21↓) 86.91 (0.23↓) 62.82 (2.53↑) 82.60 (2.45↓)\n3/6 87.50 (2.87↓) 77.07 (4.71↓) 85.78 (3.20↓) 89.59 (0.81↓) 85.19 (1.95↓) 58.48 (1.81↓) 77.45 (7.60↓)\nTable 2: Task-wise performance for the top-layer dropping strategy using the oﬃcial GLUE\ndevelopment sets. Drop. represents the number of layers that are dropped in comparison to\nthe total number of layers in the model. The red numbers with downward arrow shows the\ndrop in performance in comparison to using the full model i.e. 0/12 and the blue numbers\nwith upward arrow shows the gain in performance.\ncontribution might yield a better performance. However, in this work we focused\non task-independent eﬃcient models, leaving task-dependent models for future\nwork.\n5.2. Task-wise Results\nTop-layer strategy works consistently well for all models at K = 6. In the\nrest of the paper, we discuss the results for the Top-layer strategy only, unless\nspeciﬁed otherwise. Table 27 presents the results for the individual GLUE tasks\n7We use default settings provided in the Transformer library. This causes a slight mismatch\nbetween some numbers mentioned in the original papers of each models and our paper.\n14\nusing the Top-layer strategy on three pre-trained models and a distilled model.\nWe observe the same trend as for the averaged results: for most of the tasks,\nwe can safely drop half of the top layers in BERT, RoBERTa and XLNet losing\nonly 1-3 points.\nThe paraphrase task (QQP) and sentence similarity task (STS-B)\nare least aﬀected by the dropping of layers. When dropping half of the\nlayers, there was no loss in performance for QQP on XLNet and RoBERTa, and\na loss of 0 .72 only for BERT. Similarly, for STS-B we observed a decrease of\nonly 0.56, 0.95 and 0 .34 points for XLNet, RoBERTa and BERT respectively.\nIn contrast, RTE and MRPC tasks show substantial change (gain/drop) in the\nperformance with layer-dropping when compared with using the full model (see\nBERT and RoBERTa 0/12,2/12,4/12 results). This is due to the small size of\nthe dev sets, 408 and 277 instances for MRPC and RTE respectively. A few\nright and wrong predictions cause a large variation in the overall score. We use\nMcNemar’s test at p=value=0.05, and we found these diﬀerences, such as 5 .05\npoints drop in the performance of BERT for RTE, statistically insigniﬁcant.\nDropping top two layers for RoBERTa resulted in better perfor-\nmance and stability. Interestingly, in several cases for RoBERTa, dropping\ntwo layers resulted in better performance than using the full model. Moreover,\nwe observed that layer-dropping resulted in stable runs and was less prone to\ninitialization seed and batch size. We used default settings for all the model and\ndid not investigate the eﬀect of parameter optimization on the performance of\nthe pre-trained and reduced models to have comparable results.\nA distilled model can also be pruned successfully. We observed a\nsimilar trend, dropping layers in DistilBERT compared to BERT model. It is\ninteresting to see that an already distilled version of the model can be further\npruned by a third, with an average loss of 0 .75 points only. However, dropping\nhalf of its layers drastically degrades the performance on several tasks. Schwartz\net al. [32] also showed that pruning is orthogonal to model distillation.\n15\n5.3. Memory and Speed Comparison\nDropping layers reduces the number of parameters in the network, signiﬁ-\ncantly speeding up the task-speciﬁc ﬁne-tuning and the inference time. Table 3\ncompares the number of parameters, and the speed up in the ﬁne-tuning and\ndecoding time, versus the loss in performance. We see that dropping top half\nof the layers of the network, reduced the number of parameters by 40%, speed-\ning up ﬁne-tuning and inference by 50% with average performance loss between\n0.89–2.91 points. The results for RoBERTa are even remarkable; as with all the\nmemory and speed improvements, the average performance dropped by only 0.89\npoints. Dropping 4 layers (which gives a speed-up of 33%), RoBERTa achieved\na performance close to dropping no layers. XLNet also showed robustness to\nthe drop of top 4 layers and the performance dropped by only 0 .23 points. It is\nworth noting that a better trade-oﬀ between computational eﬃciency and loss\nin performance can be achieved by optimizing for a speciﬁc task. For example\nQQP maintained performance within 1% on XLNet when 9 layers were dropped\n(See Table 4). This corresponds to 60% reduction in the number of parameters\nand 80% reduction in terms of inference time.\n6. Discussion\nNow we perform further analysis and discuss variations of our methodology.\nWe limit the results to 5 most stable tasks (SST-2, MNLI, QNLI, QQP, STS-B).\n6.1. Task-speciﬁc optimal number of layers to drop.\nThe variation in the amount of loss for each task with the dropping of lay-\ners in Table 2 suggests that the task-speciﬁc optimal number of layers would\nresult in a better balance between the size of the pruned model and the loss in\nperformance. In this section, we present the results of the optimal number of\nlayers for each task. For these experiments, we split the standard development\nset into equal-sized hold-out set and dev set. We ﬁnd the minimum number of\nlayers required to maintain 1%, 2%, and 3% performance on the dev set using\n16\nDrop. Loss Param. Fine-tuning Inference\nspeedup seconds\nBERT ∥ RoBERTa\n0/12 0.00 ∥ 0.00 110M 1.00 -\n2/12 1.33 ∥ -0.42 94M 1.24 17% ↓\n4/12 2.00 ∥ 0.01 80M 1.48 33% ↓\n6/12 2.91 ∥ 0.89 66M 1.94 50% ↓\nXLNET\n0/12 0.00 116M 1.00 -\n2/12 0.54 101M 1.20 16% ↓\n4/12 0.23 86M 1.49 32% ↓\n6/12 1.81 71M 1.96 49% ↓\nTable 3: Comparing the number of parameters (Param.), the speed up in the ﬁne-tuning step,\nand the inference time for diﬀerent models. Fine-tuning speedupshows how many times the\nmodel speeds up compared to the original network. We report inference time on the QQP\ndevset consisting of 40.4k instances with a batch size of 32.\nour top-layer strategy and we verify that the ﬁndings generalize to the hold-out\ntest. Table 4 shows the optimal number of layers on dev and the corresponding\npercentage of performance drop on the hold-out set (in parentheses). For most\nof the cases, the optimal number of layers found using the dev set aligns well\nwith the hold-out set. For example, BERT QNLI with 1% loss in performance\nshowed that one layer can be dropped safely and this results in a loss of 0.84\npoints absolute compared to using the full model.\nOverall, RoBERTa and XLNet showed most robustness towards the dropping\nof layers while maintaining performance threshold of 1%. For example, QQP\nmaintained performance within 1 point even when the top 9 and 8 layers of\nXLNet and RoBERTa respectively were dropped. Essentially, the model consists\nof only three layers – {1, 2, 3}. On the contrary, dropping 9 layers in BERT\nresulted in a loss of 3% points for the QQP task.\n17\nSST-2 MNLI QNLI QQP STS-B\n1% Loss Threshold\nBERT 7(1.6) 3(1.04) 1(0.84) 6(0.75) 7(1.16)\nRoBERTa 4(0.00) 4(0.20) 5(0.87) 8(0.77) 5(1.22)\nXLNet 8(1.38) 5(1.22) 4(0.51) 9(0.60) 7(0.05)\n2% Loss Threshold\nBERT 7(1.60) 5(1.26) 3(1.68) 8(1.60) 7(1.16)\nRoBERTa 4(0.00) 5(1.26) 6(1.42) 9(1.51) 6(2.31)\nXLNet 8(1.38) 5(1.22) 6(1.46) 9(0.60) 8(1.22)\n3% Loss Threshold\nBERT 8(2.06) 6(2.42) 5(2.60) 9(2.27) 8(2.61)\nRoBERTa 5(0.69) 6(2.73) 7(2.37) 10(3.21) 7(3.00)\nXLNet 8(1.38) 6(1.55) 7(1.61) 9(0.60) 9(2.46)\nTable 4: Number of layers dropped from the network while maintaining performance within a\npre-deﬁned threshold. The numbers outside brackets are the optimal number of layers found\nusing the dev set and the numbers within brackets report the performance loss on the hold-out\nset. For example in 7(1.6), 7 are the optimal number of layers that can be dropped based on\nthe dev set and 1.6 is the performance loss when 7 layers are dropped on the hold-out set.\n6.2. Comparing Pre-trained Models\nOur pruning strategies illuminate model-speciﬁc peculiarities that help us in\ncomparing and understanding the learning dynamics of these models.RoBERTa\nand XLNet learn task-speciﬁc knowledge earlier in the network com-\npared to BERT. Figure 3 shows the average layer-wise performance of each\nmodel. RoBERTa learns task-level information much earlier in the model (see\nthe steep slope of the yellow line for lower layers). Although XLNet starts similar\nto BERT but in the lower-middle layers, it learns the task information relatively\nfaster than BERT. For both RoBERTa and XLNet, the performance matures\nclose to the 7th layer of the model while BERT improves with every higher layer\nuntil the 11 th layer. Since XLNet and RoBERTa mature much earlier in the\nnetwork, this suggests that top layers in these networks might be redundant for\ndownstream tasks and are a good candidate for dropping in exchange for a small\n18\nFigure 3: Average layer-wise classiﬁcation results.\nloss in performance. This observation is in line with the results presented in\nTable 2. For example, we showed that the drop of top two layers of RoBERTa\nresulted in either marginal drop in performance or improvement in performance.\nThe diﬀerence between the learning dynamics of BERT and RoBERTa en-\ncourages further investigation into what caused RoBERTa to learn task-speciﬁc\nknowledge earlier in the network. Is it because of the large amount of training\ndata used for RoBERTa or because of better pre-training procedures such as\ndynamic masking, and exclusion of next sentence prediction loss? Does early\nlearning of task-speciﬁc knowledge as in XLNet and RoBERTa reﬂect towards\na better and robust pre-trained model? Answering these questions is important\nfor improving the design of pre-trained models and require future exploration.\n6.3. Pruning the ALBERT Model\nALBERT is based on the cross-layer parameter sharing. Because of this, our\nlayer dropping strategies do not save any memory as opposed to using BERT and\nother transformers. However, it still makes the inference faster by speeding up\nthe forward pass. Table 5 presents the results on ﬁve GLUE tasks. Interestingly,\ndropping the top-6 layers did not result in drastic degradation of the model\nperformance and, in some cases, the results even improved compared to using\nthe baseline model. For example, in the case of SST-2, the performance of a\n19\nDrop SST-2 MNLI QNLI QQP STS-B\n0/12 89.79 83.39 90.24 90.29 89.61\n2/12 91.40 83.82 89.55 89.64 89.54\n4/12 91.63 82.73 90.24 88.51 87.00\n6/12 90.14 81.64 89.11 90.08 88.21\nTable 5: ALBERT: task-wise performance for the top-layer dropping strategy using the oﬃcial\nGLUE dev-sets. Drop shows the number of layer dropped/the total layers in the model.\n6-layered model is 90.14, which is 0.35 points absolute better than the baseline.\nCompared to the 6-layered BERT model (Table 2), the drop in the performance\nof ALBERT-6 is relatively small. We hypothesize that the parameter sharing\nin the case of ALBERT make the model learn much richer representation in the\nshared contextualized layers of the model, which yields a model that is robust\ntowards layer-dropping. These results are encouraging and show that the model\nthat was designed to be space-eﬃcient can be further improved towards run-time\neﬃciency by simply pruning some of its layers.\n6.4. Comparing against Distilled Models\nWe now compare the performance of our pruned models when applying the\ntop-layer dropping strategy to distilled and pruned models built using various so-\nphisticated architectures and training procedures. In particular, we compare to\nprevious work [6, 25, 23] that used KD to build 6-layered distilled models. More\nspeciﬁcally, we present the result of the following distilled models; Vanilla-KD\n– a distilled model built using the original KD method [18], BERT-PKD [21] –\npatient knowledge distillation method that encourages a student model to learn\nfrom various layers of the teacher model, and BERT-TH – a theseus compression\nmethod that gradually distill layers of a large model. Additionally, we compare\nwith the pruned RoBERTa model of [28] that used layer-level dropout during\ntraining of a pre-trained model and showed that it enables robust dropping of\nlayers at test time. We also compare to the greedy layer pruning method [29],\n20\nwhich creates task-speciﬁc smaller-size models by dropping layers in a greedy\nfashion. All these models are identical in size to our smaller models obtained by\ndropping the top-6 layers in BERT and RoBERTa. We refer to them asBERT-6\nand RoBERTa-6. Table 6 compares the results. 8\nOur pruned models (BERT-6 and RoBERTa-6) showed competi-\ntive performance compared to their distilled versions built using KD.\nThis result is quite surprising, given that our pruned models do not require any\nadditional training, while building a distilled model using KD requires training\nfrom scratch, which is a time consuming and computation expensive process.\nThe top layer-dropping works consistently for all model types including distilled\nmodels and a large set of language understanding tasks. Moreover, our setup\noﬀers the ﬂexibility to choose diﬀerent sizes of the model based on the com-\nputational requirements and the speciﬁcs of a downstream task. The result of\npreserving bottom layers of the model suggests selective compression applied\nto pre-trained models. For example, in KD while combining information from\nvarious layers of the large model, it is advisable to preserve the bottom layers\nand distilled the top layers. Similarly, pruning methods such as weight and\nattention-head pruning, and quantization can be aggressively applied to top\nlayers of the models while preserving the bottom layers.\nOur RoBERTa-6 has comparable results to the 6-layer pruned\nmodel trained using LayerDrop and Greedy layer pruning. Fan et al.\n[28] used layer-level dropout during training of a pre-trained model and showed\nthat it enables robust dropping of layers at test time. Similar to us, they directly\npruned top 6-layers of their large model and ﬁne-tuned it for speciﬁc tasks. Ta-\nble 6 (row 7 and 10) compares top-layer dropping using their model and the\noriginal RoBERTa model. On two out of three tasks, dropping top-layers from\nthe original RoBERTa model outperformed training a new model using Layer-\nDrop. This shows that the current models are already robust and the top-layer\n8There is an exhaustive list of task-speciﬁc distilled models but we show the results for a\nfew for comparison.\n21\nNo. Model SST-2 MNLI QNLI QQP STS-B\n1. Vanilla-KD 90.50 80.10 88.00 88.10 84.90\n2. BERT-PKD 91.30 81.30 88.40 88.40 86.20\n3. BERT-TH 91.80 82.10 88.80 88.80 87.80\n4. GLP6 91.20 81.30 87.60 86.80 87.60\n5. DistilBERT 90.37 81.78 88.98 90.40 87.14\n6. BERT-6 90.25 81.13 87.63 90.35 88.45\n7. Fan et al. RoBERTa-6 92.50 82.90 89.40 - -\n8. GLP6 92.00 85.60 90.80 87.80 86.60\n9. DistilRoBERTa 92.50 84.00 90.80 89.40 88.30\n10. RoBERTa-6 91.97 84.44 90.00 90.91 88.92\nTable 6: Comparing 6-layered BERT and RoBERTa models. Results of Vanilla-KD, BERT-\nPKD and BERT-TH are taken from Xu et al. [30]. Fan et al. results and GLP6 are taken from\n[28, 29]. BERT-6 and RoBERTa-6 represent our models achieved by pruning top 6 layers.\ndropping strategy can be directly applied to the available pre-trained models.\nSimilarly, we found that despite optimizing the model towards a downstream\nGLUE task, the greedy layer pruning (GLP 6) did not show a clear advantage\nover our 6-layered model. For example, compared to BERT (rows 4 and 6), our\nBERT-6 model yields better or comparable performance to GLP 6 on the QQP,\nSTS-B, MNLI and QNLI tasks, and performs worse only on the SST-2 task.\n6.5. Layer-Dropping using Fine-tuned Models\nHere, we question whether dropping layers from a ﬁne-tuned model is more\neﬀective than dropping them from a base model? To answer this, we ﬁrst ﬁne-\ntune the model, drop the layers, and then ﬁne-tune the reduced model again.\nTable 7 presents the results on BERT and XLNet. We found this setup to\nbe comparable to dropping layers directly from the pre-trained model in most\nof the cases. This shows that dropping top layers directly from a pre-trained\nmodel does not lose any critical information which was essential for a speciﬁc\ntask. However, we think that pruning a ﬁne-tuned model may lose task-speciﬁc\ninformation because the model is optimized for the task. Dropping layers may\n22\nModel SST-2 MNLI QNLI QQP STS-B\nBERT-6 92.25 81.13 87.63 90.35 88.45\nBERT-FT-6 90.02 80.85 87.24 90.34 88.16\nXLNet-6 92.20 83.48 88.03 90.62 87.45\nXLNet-FT-6 92.43 83.75 86.80 90.77 87.60\nTable 7: Layer-dropping using task-speciﬁc models. XLNet-FT-6 ﬁrst ﬁne-tunes the pre-\ntrained model, removes the layers and performs ﬁne-tuning again.\nhave severe eﬀect. This is reﬂected in some of the results of BERT-6.\nGradual Dropping:. In another attempt to preserve the model’s performance\nduring the dropping process, we iteratively drop one layer after every two epochs\nof the ﬁne-tuning process. This did not yield any improvement over dropping\nlayers directly from the model.\n7. Conclusion\nWe proposed strategies to drop layers in pre-trained models and analyzed the\nmodel behavior on downstream tasks. We conducted experiments using a variety\nof pre-trained models and using a diverse set of natural language understanding\ntasks and showed that one can reduce the model size by up to 40%, while\nmaintaining up to 98% of their original performance on downstream tasks. Our\npruned models performed on par with distilled models building using knowledge\ndistillation. However, unlike distilled models, our approach does not require re-\ntraining, is applicable to a large set of pre-trained models including distilled\nmodels, and provides the ﬂexibility to balance the trade-oﬀ between accuracy\nand model size. Moreover, we made several interesting observations such as,\ni) the lower layers are most critical to maintain downstream task performance,\nii) certain downstream tasks require as few as only 3 layers out of 12 layers to\nmaintain within 1% performance threshold, iii) networks trained using diﬀerent\nobjective functions have diﬀerent learning patterns e.g. XLNet and RoBERTa\nlearns task-speciﬁc information much earlier in the network compared to BERT.\n23\nReferences\n[1] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep\nbidirectional transformers for language understanding, in: Proceedings of\nthe 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), Association for Computational Linguistics,\nMinneapolis, Minnesota, 2019.\n[2] P. Michel, O. Levy, G. Neubig, Are sixteen heads really better than\none?, in: Advances in Neural Information Processing Systems 32, Cur-\nran Associates, Inc., 2019, pp. 14014–14024. URL: http://papers.nips.\ncc/paper/9551-are-sixteen-heads-really-better-than-one.pdf .\n[3] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, I. Titov, Analyzing multi-\nhead self-attention: Specialized heads do the heavy lifting, the rest can\nbe pruned, in: Proceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, Florence, Italy, 2019. doi: 10.18653/\nv1/P19-1580.\n[4] J. S. McCarley, Pruning a bert-based question answering model, 2019.\narXiv:1910.06360.\n[5] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, Albert:\nA lite bert for self-supervised learning of language representations, 2019.\narXiv:1909.11942.\n[6] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter, 2019. arXiv:1910.01108.\n[7] O. Zafrir, G. Boudoukh, P. Izsak, M. Wasserblat, Q8bert: Quantized 8bit\nbert, 2019. arXiv:1910.06188.\n[8] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney,\nK. Keutzer, Q-bert: Hessian based ultra low precision quantization of bert,\n2019. arXiv:1909.05840.\n24\n[9] P. Michel, O. Levy, G. Neubig, Are sixteen heads really better than one?,\nCoRR abs/1905.10650 (2019). URL: http://arxiv.org/abs/1905.10650.\n[10] H. Peng, R. Schwartz, D. Li, N. A. Smith, A mixture of h −1 heads is\nbetter than h heads, 2020. arXiv:2005.06537.\n[11] M. A. Gordon, K. Duh, N. Andrews, Compressing BERT: Studying the ef-\nfects of weight pruning on transfer learning, ArXiv abs/2002.08307 (2019).\n[12] E. Voita, R. Sennrich, I. Titov, The bottom-up evolution of representations\nin the transformer: A study with machine translation and language model-\ning objectives, in: Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Con-\nference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong,\nChina, 2019.\n[13] F. Dalvi, H. Sajjad, N. Durrani, Y. Belinkov, Analyzing redundancy\nin pretrained transformer models, in: Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP),\nAssociation for Computational Linguistics, Online, 2020, pp. 4908–\n4926. URL: https://aclanthology.org/2020.emnlp-main.398. doi:10.\n18653/v1/2020.emnlp-main.398.\n[14] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized BERT pre-\ntraining approach, CoRR abs/1907.11692 (2019). URL: http://arxiv.\norg/abs/1907.11692. arXiv:1907.11692.\n[15] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, Q. V. Le, Xlnet:\nGeneralized autoregressive pretraining for language understanding, 2019.\narXiv:1906.08237.\n[16] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. Bowman, GLUE: A\nmulti-task benchmark and analysis platform for natural language under-\nstanding, in: Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\n25\nAnalyzing and Interpreting Neural Networks for NLP, Association for Com-\nputational Linguistics, Brussels, Belgium, 2018, pp. 353–355. URL: https:\n//www.aclweb.org/anthology/W18-5446. doi:10.18653/v1/W18-5446.\n[17] Q. Cao, H. Trivedi, A. Balasubramanian, et al., Faster and just as accurate:\nA simple decomposition for transformer models, ICLR Openreview (2020).\n[18] G. E. Hinton, S. Osindero, Y. W. Teh, A fast learning algorithm for deep\nbelief nets, Neural Computation 18 (2006) 1527–1554.\n[19] I. Guyon, A. Elisseeﬀ, An introduction to variable and feature selection,\nMachine Learning Research 3 (2003).\n[20] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a\nneural network, 2015. URL: http://arxiv.org/abs/1503.02531, cite\narxiv:1503.02531Comment: NIPS 2014 Deep Learning Workshop.\n[21] S. Sun, Y. Cheng, Z. Gan, J. Liu, Patient knowledge distillation for BERT\nmodel compression, in: Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP),\nAssociation for Computational Linguistics, Hong Kong, China, 2019,\npp. 4322–4331. URL: https://www.aclweb.org/anthology/D19-1441.\ndoi:10.18653/v1/D19-1441.\n[22] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang,\nQ. Liu, Tinybert: Distilling bert for natural language understanding, 2019.\narXiv:1909.10351.\n[23] R. Tang, Y. Lu, L. Liu, L. Mou, O. Vechtomova, J. Lin, Distilling\ntask-speciﬁc knowledge from BERT into simple neural networks, CoRR\nabs/1903.12136 (2019). URL: http://arxiv.org/abs/1903.12136.\n[24] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, D. Zhou, ”mobilebert: Task-\nagnostic compression of bert by progressive knowledge transfer”, in: In-\n26\nternational Conference on Learning Representations, 2020. URL: https:\n//openreview.net/forum?id=SJxjVaNKwB.\n[25] I. Turc, M.-W. Chang, K. Lee, K. Toutanova, Well-read students\nlearn better: On the importance of pre-training compact models, 2019.\narXiv:1908.08962.\n[26] H. Tsai, J. Riesa, M. Johnson, N. Arivazhagan, X. Li, A. Archer, Small\nand practical bert models for sequence labeling, 2019. arXiv:1909.00100.\n[27] A. Renda, J. Frankle, M. Carbin, Comparing rewinding and ﬁne-tuning in\nneural network pruning, in: ICLR, 2020.\n[28] A. Fan, E. Grave, A. Joulin, Reducing transformer depth on demand with\nstructured dropout, 2019. arXiv:1909.11556.\n[29] D. Peer, S. Stabinger, S. Engl, A. J. Rodr´ ıguez-S´ anchez, Greedy\nlayer pruning: Decreasing inference time of transformer models,\nCoRR abs/2105.14839 (2021). URL: https://arxiv.org/abs/2105.\n14839. arXiv:2105.14839.\n[30] C. Xu, W. Zhou, T. Ge, F. Wei, M. Zhou, Bert-of-theseus: Compressing\nbert by progressive module replacing, ArXiv abs/2002.02925 (2020).\n[31] W. Liu, P. Zhou, Z. Zhao, Z. Wang, H. Deng, Q. Ju, Fastbert: a self-\ndistilling bert with adaptive inference time, 2020. arXiv:2004.02178.\n[32] R. Schwartz, G. Stanovsky, S. Swayamdipta, J. Dodge, N. A. Smith, The\nright tool for the job: Matching model and instance complexities, 2020.\narXiv:2004.07453.\n[33] J. Xin, R. Tang, J. Lee, Y. Yu, J. Lin, Deebert: Dynamic early exiting for\naccelerating bert inference, 2020. arXiv:2004.12993.\n[34] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, F. Wei, BERT loses patience:\nFast and robust inference with early exit, 2020. arXiv:2006.04152.\n27\n[35] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, J. Glass, What do Neu-\nral Machine Translation Models Learn about Morphology?, in: Proceed-\nings of the 55th Annual Meeting of the Association for Computational\nLinguistics (ACL), Association for Computational Linguistics, Vancou-\nver, 2017. URL: https://aclanthology.coli.uni-saarland.de/pdf/P/\nP17/P17-1080.pdf.\n[36] F. Dalvi, N. Durrani, H. Sajjad, Y. Belinkov, S. Vogel, Understanding\nand improving morphological learning in the neural machine translation\ndecoder, in: Proceedings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), Asian Federation\nof Natural Language Processing, Taipei, Taiwan, 2017, pp. 142–151. URL:\nhttps://aclanthology.org/I17-1015.\n[37] Y. Belinkov, L. M` arquez, H. Sajjad, N. Durrani, F. Dalvi, J. Glass, Evaluat-\ning layers of representation in neural machine translation on part-of-speech\nand semantic tagging tasks, in: Proceedings of the Eighth International\nJoint Conference on Natural Language Processing (Volume 1: Long Pa-\npers), Asian Federation of Natural Language Processing, Taipei, Taiwan,\n2017, pp. 1–10. URL: https://aclanthology.org/I17-1001.\n[38] A. Conneau, G. Kruszewski, G. Lample, L. Barrault, M. Baroni, What you\ncan cram into a single vector: Probing sentence embeddings for linguistic\nproperties, in: Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2018.\n[39] N. F. Liu, M. Gardner, Y. Belinkov, M. E. Peters, N. A. Smith, Lin-\nguistic knowledge and transferability of contextual representations, in:\nProceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Association for Computa-\ntional Linguistics, Minneapolis, Minnesota, 2019, pp. 1073–1094. URL:\nhttps://www.aclweb.org/anthology/N19-1112.\n28\n[40] I. Tenney, P. Xia, B. Chen, A. Wang, A. Poliak, R. T. McCoy, N. Kim,\nB. V. Durme, S. R. Bowman, D. Das, E. Pavlick, What do you learn from\ncontext? probing for sentence structure in contextualized word representa-\ntions, 2019. arXiv:1905.06316.\n[41] I. Tenney, D. Das, E. Pavlick, BERT rediscovers the classical NLP pipeline,\nin: Proceedings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics, Association for Computational Linguistics, Florence,\nItaly, 2019, pp. 4593–4601. URL: https://www.aclweb.org/anthology/\nP19-1452. doi:10.18653/v1/P19-1452.\n[42] N. Durrani, F. Dalvi, H. Sajjad, Y. Belinkov, P. Nakov, One size does\nnot ﬁt all: Comparing NMT representations of diﬀerent granularities, in:\nProceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Association for Computa-\ntional Linguistics, Minneapolis, Minnesota, 2019, pp. 1504–1516. URL:\nhttps://aclanthology.org/N19-1154. doi:10.18653/v1/N19-1154.\n[43] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, J. Glass, On the linguis-\ntic representational power of neural machine translation models, Compu-\ntational Linguistics 46 (2020) 1–52. URL: https://aclanthology.org/\n2020.cl-1.1. doi:10.1162/coli_a_00367.\n[44] D. Arps, Y. Samih, L. Kallmeyer, H. Sajjad, Probing for constituency struc-\nture in neural language models, 2022. doi: 10.48550/ARXIV.2204.06201.\n[45] Y. Belinkov, S. Gehrmann, E. Pavlick, Interpretability and analysis in neu-\nral NLP, in: Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics: Tutorial Abstracts, Association for Computa-\ntional Linguistics, Online, 2020, pp. 1–5. URL: https://aclanthology.\norg/2020.acl-tutorials.1. doi:10.18653/v1/2020.acl-tutorials.1.\n[46] H. Sajjad, N. Durrani, F. Dalvi, Neuron-level Interpretation of Deep NLP\n29\nModels: A Survey, CoRR abs/2108.13138 (2021). URL: https://arxiv.\norg/abs/2108.13138. arXiv:2108.13138.\n[47] A. Tamkin, T. Singh, D. Giovanardi, N. D. Goodman, Investigating trans-\nferability in pretrained language models, ArXiv abs/2004.14975 (2020).\n[48] A. Merchant, E. Rahimtoroghi, E. Pavlick, I. Tenney, What happens to\nbert embeddings during ﬁne-tuning?, ArXiv abs/2004.14448 (2020).\n[49] N. Durrani, H. Sajjad, F. Dalvi, How transfer learning impacts lin-\nguistic knowledge in deep NLP models?, in: Findings of the Associa-\ntion for Computational Linguistics: ACL-IJCNLP 2021, Association for\nComputational Linguistics, Online, 2021, pp. 4947–4957. URL: https:\n//aclanthology.org/2021.findings-acl.438. doi: 10.18653/v1/2021.\nfindings-acl.438.\n[50] F. Dalvi, N. Durrani, H. Sajjad, Y. Belinkov, D. A. Bau, J. Glass, What is\none grain of sand in the desert? analyzing individual neurons in deep nlp\nmodels, in: Proceedings of the Thirty-Third AAAI Conference on Artiﬁcial\nIntelligence (AAAI, Oral presentation), 2019.\n[51] N. Durrani, H. Sajjad, F. Dalvi, Y. Belinkov, Analyzing individual neu-\nrons in pre-trained language models, in: Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP),\nAssociation for Computational Linguistics, Online, 2020, pp. 4865–\n4880. URL: https://aclanthology.org/2020.emnlp-main.395. doi:10.\n18653/v1/2020.emnlp-main.395.\n[52] T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, Y. Artzi, Revisiting few-\nsample bert ﬁne-tuning, 2020. arXiv:2006.05987.\n[53] F. Dalvi, A. R. Khan, F. Alam, N. Durrani, J. Xu, H. Sajjad, Discov-\nering latent concepts learned in BERT, in: International Conference on\nLearning Representations, 2022. URL: https://openreview.net/forum?\nid=POTMtpYI1xH.\n30\n[54] H. Sajjad, N. Durrani, F. Dalvi, F. Alam, A. R. Khan, J. Xu, Analyzing\nencoded concepts in transformer language models, in: Proceedings of the\n2022 Conference of the North American Chapter of the Association for\nComputational Linguistics, NAACL ’22, Association for Computational\nLinguistics, Seattle, Washington, USA, 2022.\n[55] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,\nC. Potts, Recursive deep models for semantic compositionality over a\nsentiment treebank, in: Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, Association for Computa-\ntional Linguistics, Seattle, Washington, USA, 2013, pp. 1631–1642. URL:\nhttps://www.aclweb.org/anthology/D13-1170.\n[56] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\npus for sentence understanding through inference, in: Proceedings of\nthe 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume\n1 (Long Papers), Association for Computational Linguistics, New Or-\nleans, Louisiana, 2018, pp. 1112–1122. URL: https://www.aclweb.org/\nanthology/N18-1101. doi:10.18653/v1/N18-1101.\n[57] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, SQuAD: 100,000+\nquestions for machine comprehension of text, in: Proceedings of the\n2016 Conference on Empirical Methods in Natural Language Process-\ning, Association for Computational Linguistics, Austin, Texas, 2016,\npp. 2383–2392. URL: https://www.aclweb.org/anthology/D16-1264.\ndoi:10.18653/v1/D16-1264.\n[58] L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, B. Magnini, The ﬁfth\npascal recognizing textual entailment challenge, in: In Proc Text Analysis\nConference (TAC’09, 2009.\n[59] W. B. Dolan, C. Brockett, Automatically constructing a corpus of sen-\ntential paraphrases, in: Proceedings of the Third International Work-\n31\nshop on Paraphrasing (IWP2005), 2005. URL: https://www.aclweb.org/\nanthology/I05-5002.\n[60] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, L. Specia, SemEval-2017\ntask 1: Semantic textual similarity multilingual and crosslingual focused\nevaluation, in: Proceedings of the 11th International Workshop on Se-\nmantic Evaluation (SemEval-2017), Association for Computational Lin-\nguistics, Vancouver, Canada, 2017, pp. 1–14. URL: https://www.aclweb.\norg/anthology/S17-2001. doi:10.18653/v1/S17-2001.\n[61] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Brew, Huggingface’s transformers:\nState-of-the-art natural language processing, ArXiv abs/1910.03771 (2019).\n32"
}