{
  "title": "Power transformer fault diagnosis under measurement originated uncertainties",
  "url": "https://openalex.org/W2051408490",
  "year": 2012,
  "authors": [
    {
      "id": "https://openalex.org/A2098128952",
      "name": "Hui Ma",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2075377855",
      "name": "Chandima Ekanayake",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2484610329",
      "name": "Tapan K. Saha",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2098128952",
      "name": "Hui Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2075377855",
      "name": "Chandima Ekanayake",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2484610329",
      "name": "Tapan K. Saha",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2126857563",
    "https://openalex.org/W2117078830",
    "https://openalex.org/W1968530169",
    "https://openalex.org/W2164877242",
    "https://openalex.org/W2164911188",
    "https://openalex.org/W6677954473",
    "https://openalex.org/W2156909104",
    "https://openalex.org/W1994807229",
    "https://openalex.org/W2133772980",
    "https://openalex.org/W2012608140",
    "https://openalex.org/W2084277209",
    "https://openalex.org/W6663379545",
    "https://openalex.org/W2138825343",
    "https://openalex.org/W2050873240",
    "https://openalex.org/W4240924624",
    "https://openalex.org/W2084880510",
    "https://openalex.org/W2153375913",
    "https://openalex.org/W6629680036",
    "https://openalex.org/W1985702987",
    "https://openalex.org/W2155074104",
    "https://openalex.org/W2147813562",
    "https://openalex.org/W2156145910",
    "https://openalex.org/W2153635508",
    "https://openalex.org/W4299857007",
    "https://openalex.org/W2900519066",
    "https://openalex.org/W2118978333",
    "https://openalex.org/W2165496236",
    "https://openalex.org/W1528620860",
    "https://openalex.org/W2101566415"
  ],
  "abstract": "This paper addresses the problem of diagnosing the fault symptoms of power transformers with measurement originated uncertainties, which arise from the imprecision of samples (i.e. due to noises and outliers) and the effect of class imbalance (i.e. samples are unequally distributed between different fault types) in a training dataset used to identify different fault types. Two fuzzy support vector machine (FSVM) algorithms namely fuzzy c-means clustering-based FSVM (FCM-FSVM) and kernel fuzzy c-means clustering-based FSVM (KFCM-FSVM) have been applied in this paper to deal with any noises and outliers in training dataset. In order to reduce the effect of class imbalance in training dataset, two approaches including between-class weighting and random oversampling have been adopted and integrated with FCM-FSVM and KFCM-FSVM. The case studies show that KFCM-FSVM algorithm and its variants have consistent tendency to attain satisfied classification accuracy in transformer fault diagnosis using dissolved gas analysis (DGA) measurements.",
  "full_text": "Power transformer fault diagnosis under measurement originated\nuncertainties\nAuthor\nMa, Hui, Ekanayake, Chandima, Saha, Tapan\nPublished\n2012\nJournal Title\nIEEE Transactions on Dielectrics and Electrical Insulation\nVersion\nAccepted Manuscript (AM)\nDOI\n10.1109/TDEI.2012.6396956\nRights statement\n© 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be\nobtained for all other uses, in any current or future media, including reprinting/republishing this\nmaterial for advertising or promotional purposes, creating new collective works, for resale or\nredistribution to servers or lists, or reuse of any copyrighted component of this work in other\nworks.\nDownloaded from\nhttp://hdl.handle.net/10072/173337\nGriffith Research Online\nhttps://research-repository.griffith.edu.au\n \nPower Transformer Fault Diagnosis under Measurement \nOriginated Uncertainties  \n \nHui Ma, Chandima Ekanayake  and  Tapan K. Saha  \nThe University of Queensland \nBrisbane, Australia \n \nABSTRACT \nThis paper addresses the problem of diagnosing the fault symptoms of power \ntransformers with measurement originated uncertainties, which arise from the \nimprecision of samples (i.e. due to noises and outliers) and the effect of class imbalance \n(i.e. samples are unequally distributed between different fault types) in a training \ndataset used to identify different fault types. Two fuzzy support vector machine \n(FSVM) algorithms namely fuzzy c-means clustering–based FSVM (FCM-FSVM) and \nkernel fuzzy c-means clustering–based FSVM (KFCM-FSVM) have been applied in \nthis paper to deal with any noises and outliers in training dataset. In order to reduce \nthe effect of class imbalance in training dataset, two approaches including between-\nclass weighting and random oversampling have been adopted and integrated with \nFCM-FSVM and KFCM-FSVM. The case studies show that KFCM-FSVM algorithm \nand its variants have consistent tendency to attain satisfied classification accuracy in \ntransformer fault diagnosis using dissolved gas analysis (DGA) measurements. \n   Index Terms — Condition monitoring, dissolved gas analysis, measurement \noriginated uncertainties, and power transformer. \n \n1   INTRODUCTION  \n   DISSOVLED  gas analysis (DGA) has been widely \nadopted by the electricity utilities for power transformer fault \ndiagnosis [1-3]. The conventional DGA interpretation methods \nmake use of concentrations or relative proportions of by-\nproduct gases such as hydrogen, methane, acetylene, ethylene, \nethane, carbon monoxide and carbon dioxide to detect thermal \nand discharge faults occurring in transformers. In the key gas \nmethod, several gas contents are evaluated to detect four types \nof faults including oil overheating, paper overheating, partial \ndischarge, and arcing. In the ratio based methods such as \nDörnenburg Ratios, Rogers Ratios, Duval Triangle and \nIEC/IEEE standards, the diagnosis is made by computing \nseveral key gas ratios and mapping these ratios to the \npredefined fault patterns.  \nAlthough widely adopted in utilities, the above \nconventional DGA interpretation methods still suffer some \nlimitations. The diagnosis criteria may vary amongst utilities \nand there might be discrepancies among diagnoses by using \ndifferent ratios. Sometimes the conventional methods may not \nbe able to provide an interpretation for every possible \ncombination of ratio values and give an explicit diagnosis. To \naddress these limitations artificial intelligence (AI) techniques \nhave been developed to identify different fault conditions of \ntransformers [4-14].  One of the key advantages of AI \ntechniques is the utilization of historic DGA dataset, i.e. the \nDGA measurement records of which the transformers fault \nconditions are already known. By exploring the underlying \nmathematical relationship between these DGA records and the \ncorresponding fault types, the AI algorithms can predict the \nfault type of any new DGA measurement for a transformer of \ninterest. In AI algorithms the process of exploiting the \nmathematical relationship is called the training process and the \nhistoric DGA dataset is referred to as the training dataset.   \nIn the training process, it is normally assumed that the fault \ntype is known with certainty for each DGA record. However, \ndue to the complexity of the transformer insulation system, \nimprecision of the measurement system, influence from any \nenvironmental noise and interference, limitation of human \ninterpretation on the measurement data, and possible presence \nof multiple faults, there is a certain degree of inaccuracy and \nuncertainty in determining the fault type for each DGA record \nof training dataset. As a result, some DGA records can be fully \nassigned to one of the fault types while others may not be \nassigned exactly to one of the fault types. Moreover, there \nexist some data points which are distant from the rest of the \ndata points. Although these data points (outliers) are not error \ndata, they may also cause difficulties in the training process. \nOn the other hand, the occurrence rate of some types of faults \nin power transformer is rather low, which implies that the \nDGA records of these type of faults are very limited and the \ntraining dataset may exhibit an unequal distribution between \nManuscript received on 12 May 2012, in final form 14 August 2012. \n \ndifferent fault types (i.e. class imbalance problem [15]). Figure \n1 illustrates the concept of noise/outliers and class imbalance \nin the training dataset.  \n \nFigure 1. Illustration of measurement originated uncertainties.  \n(This figure shows a two dimensional dataset with two classes of samples. ζ1 \nand ζ2 represent the two axes. The majority class has a large number of \nsamples and the minority class has a relatively small number of samples.)  \n \nIn this paper the above phenomena of noises and outliers \nand the class imbalance in training dataset are referred to as \nmeasurement originated uncertainties . Obviously these \nuncertainties will introduce considerable challenges in \nreaching a correct decision of fault type for any new DGA \nmeasurement of the transformer under investigation. \nTherefore, appropriate techniques have to be employed to \ncarry out the necessary processing of training dataset with \nmeasurement originated uncertainties.  \nThis paper applies clustering based fuzzy support vector \nmachine (FSVM) algorithms to deal with the noises and \noutliers of the training dataset in recognizing different fault \ntypes in transformers. These are: (a) Fuzzy c-means \nclustering–FSVM (FCM-FSVM); (b) Kernel fuzzy c-means \nclustering–FSVM (KFCM-FSVM). FSVM is a variant of \nsupport vector machine (SVM) and it is effective in dealing \nwith noises and outliers present in the training dataset [16-19]. \nIn FCM-FSVM and KFCM-FSVM, the samples (e.g. DGA \nrecords) in training dataset are assigned with different weights. \nThe samples that are outliers or noises corrupted will be \nassigned a smaller weight by the algorithms and consequently \ntreated less importantly in the training process. In order to \nreduce the effect of class imbalance of training dataset, two \napproaches namely between-class weighting and random \noversampling are adopted and integrated with FCM-FSVM \nand KFCM-FSVM.  \nThe paper is organized as follows. Section 2 provides a \nbrief literature review on AI based DGA interpretation \nmethods. Section 3 presents the mathematic formulation of \nFSVM.  Section 4 details the FCM-FSVM and KFCM-FSVM, \nincluding data clustering, weights calculation and \nimplementation. Section 5 presents the approaches for dealing \nwith the class imbalance problem in training dataset. Section 6 \npresents the case studies and Section 7 concludes the paper. \n \n \n2  AI TECHNIQUES FOR TRANSFORMER \nFAULT DIAGNOSIS BASED ON DGA  \nOver the past ten years AI based techniques have been \nextensively explored for transformer fault diagnosis, including \nexpert system, fuzzy inference system, artificial neural \nnetworks, wavelet transformation and hybrid system …etc [4 \n–14]. This section provides a brief review on AI techniques for \ntransformer fault diagnosis based on DGA measurements. \nThe expert system constructs diagnosis rules upon a \nknowledge base, which may include transformers’ information \nand experts’ experience regarding fault symptoms [4]. Fuzzy \ninference system makes use of fuzzy if-then rules to \nincorporate the experts’ knowledge [5, 6]. The major \ndrawbacks of the expert and fuzzy inference system for DGA \ninterpretation are that their performances depend on the \ncompleteness of the knowledge base and they cannot make \nautomatic adjustments (i.e. re-computing the weights) when \nnew knowledge becomes available.    \nArtificial neural network (ANN) has also been applied to \ntransformer diagnosis [7-9].  ANN can learn hidden relations \nbetween DGA data and the faults occurring in transformers. \nThe major advantage of ANN over the expert and fuzzy \ninference system is that ANN can incrementally extend its \nknowledge base by incorporating newly available data. \nHowever, ANN still suffers some drawbacks such as having \nno explanation ability, slow convergence, and requiring a \nrelatively large number of training data.      \nAttempts have been made to integrate ANN with expert or \nfuzzy inference system to improve overall diagnosis accuracy. \nWang et al combined ANN and expert system to take \nadvantages of the self-learning and non-linear mapping \ncapability of ANN and the explicit knowledge representation \ncapability of expert system [10].  Naresh et al  proposed a \nneural fuzzy approach which makes use of fuzzy logic’s \ncapability in uncertainty representation and ANN’s capability \nin learning [11].  \nHuang et al integrated fuzzy logic and neural net with an \nevolutionary algorithm because of its capability in solving \noptimization problems [12, 13]. In [12], the hybrid \nevolutionary-fuzzy logic algorithm was developed to solve a \nmixed-integer combinatorial optimization problem. This \nhybrid algorithm automatically modified 24 if-then rules and \nadjusted the weights of the fuzzy logic system. In [13], a \nhybrid neural net and evolutionary algorithm was developed. \nThis hybrid algorithm makes use of the nonlinear mapping \ncapability of neural net and the global search capability of \nevolutionary algorithm.  As a result, it is able to overcome \nsome problems of conventional neural network, especially the \nslow convergence and the needs of manually determining the \nnetwork structure and parameters.  \nDifferent AI techniques have their own merits and \ndrawbacks for transformer fault diagnosis. There are still \navenues for further research, especially the applicability and \nadaptability of AI techniques when they are applied at \ndifferent utilities. This paper adopts support vector machine \n(SVM) technique for transformer fault diagnosis. SVM \ntechnique has been proved as a powerful tool for solving \nclassification problems [16-19]. Especially, this paper \ninvestigates clustering based fuzzy support vector machine to \nξ 1\nξ 2\nsamples in majority class \nsamples in minority class \nnoises and outliers \n \ndeal with the noises and outliers as well as the class imbalance \nproblem in training dataset for transformer fault diagnosis \nusing DGA measurements.   \n \n3  FUZZY SUPPORT VECTOR MACHINE \nThe SVM algorithm adopts kernel-based technique to \ntransform the training samples from the original input space \ninto a higher dimensional feature space, in which a hyperplane \nthat maximizes the margin between different groups of \nsamples is sought (Figure 2). This margin maximization is \nformulated as a quadratic programming (QP) problem and \nsolved by introducing the Lagrange multipliers [16, 17].  \n \nFigure 2. Illustration of data transformation in SVM.  \n \nHowever one of the main drawbacks of the normal SVM is \nthat all samples in the training dataset are treated uniformly. \nThus, the normal SVM lacks the ability to deal with the \ntraining dataset with noises and outliers [18, 19]. To solve this \nproblem this paper resorts to fuzzy support vector machine \n(FSVM). Differing from general fuzzy classification which \ninvolves fuzzification, defuzzification and fuzzy reasoning, \nFSVM assigns different weights for different samples in the \ntraining dataset by taking into account their relevance and then \nincorporates the weights into the normal SVM training process \nto reduce the effect of noise and outliers. The mathematic \nformulation of FSVM is briefly reviewed as follows. \nIt is assumed that there are N  samples [ ]Nx ..., , x1=X  in the \ntraining dataset and each sample belongs to one of the T \nindependent classes { }Tyy ..., ,1 . In FSVM each sample within \nthe training dataset is assigned a weight and these weights are \nadded to the original training dataset, which then becomes \n{ }\nN\nkkkk y 1,, x =ρ , where 10 << kρ  are the weights \ndescribing the degree of kx belonging to ky . This paper \nadopts fuzzy c-means clustering (FCM) and kernel fuzzy c-\nmeans clustering (KFCM) algorithm to facilitate the \ncomputation of samples weights. The details of weights \ncomputing will be discussed in the next section.  \nFSVM algorithm starts with transforming the samples X  \nfrom the original input space mℜ  to a higher dimensional \nfeature space Ω  through a nonlinear function ( )Xφ . Then it \nsearches for an optimal separation hyperplane in that feature \nspace by solving the following quadratic programming (QP) \nproblem [18]: \n \nMinimize     ∑ =+ N\nk k kCT\n12\n1 ξρww                           (1)                               \nSubject to    ( ) NkbTy kkk ..., , 11x =−≥\n\n\n\n +⋅\nξφw   (2) \n             and    Nkk ..., , 10 =≥ξ                                     (3) \n \nIn the above equations, the pair (w, b) defines the separation \nhyperplane, in which w is a normal vector of the hyperplane \nand b is a bias. C  is the regularization parameter to balance the \nmargin maximization and misclassification, and \n0≥kξ  are \nthe error terms due to the misclassifications. The above QP \nproblem can be transformed into its dual form [16], [18]: \n \nMaximize  ( ) jkj k j\nNk Nj k\nNk k Ky y x , x\n2\n1\n1 11 ααα ∑ = ∑ =∑ = −       \n(4)         \nSubject to  ∑ = =N\nk k ky1 0α                                                (5) \n            and  Nk Ckk ..., , 1,0 =≤≤ ρα                                 (6) \n \nwhere kα is the Lagrange multiplier, and ( )jkK x , xis the \nkernel function in the form of ( ) ( ) ( )j\nT\nkjkK xxx , x φφ= .  \nAfter obtaining the maximized hyperplane, FSVM can \npredict the class label for a new sample x as  \n \n( )\n\n\n\n\n\n\n\n\n+= ∑\n=\nN\nk\nkk k bKyy\n1\nx , xsgn α                   (7)  \n \n \n4 FUZZY CLUSTERING BASED  \nFUZZY SUPPORT VECTOR MACHINE  \n \nTo decide the weights for samples in the training dataset for \nthe above FSVM, two algorithms namely fuzzy \nc-means \nclustering (FCM) and kernel fuzzy c-means clustering \n(KFCM) are adopted. Both algorithms group samples based on \ntheir similarity, in which similar samples (i.e. having the same \ntype of fault condition in transformer) form a cluster (group). \nThe samples weights calculation is performed in two steps: (1) \nClusters formation. In this step, the centers of each cluster and \nsamples within each cluster are determined. Firstly, the FCM \n(KFCM) is performed to decide a number of cluster centers as \nwell as the samples membership values to each center. Then, \neach sample is assigned to one cluster, to which the sample \nhas the largest membership value. (2) Samples weights  \ncomputation. This step computes a sample’s weight based on \nits Euclidean distance to the center of its belonged cluster. An \nexponentially decaying function (an inverse of Euclidean \n( )XX φ→\nOriginal Input Space Feature Space \nξ 2\nξ 1\nψ2\nψ1\nψ3\n \ndistance between a sample and its cluster center, refer to \nEquation 18) is used to compute the final weights for samples \nin each cluster. \nThis section provides a brief review of FCM and KFCM \nand the implementation of the resultant FCM-FSVM and \nKFCM-FSVM algorithm. \n \n4.1 FCM ALGORITHM \nIn FCM algorithm, the forming of clusters is achieved by \nminimizing a quadratic objective function. The formed \nclusters are represented in the format of a fuzzy partition \nmatrix and a collection of several cluster centers.  \nIt is assumed that there are N  samples \n[ ]Nx ..., , x1=X , which \nbelong to c ( Nc <≤2 ) clusters. Now the task for FCM is to \ndecide the cluster centers, which is accomplished through the \nminimization of a quadratic objective function as follows [20]:  \n \nMinimize    ( ) ( )\n2\n1 1\nvxVU, X; ij\nmc\ni\nN\nj j iuJ −= ∑\n=\n∑\n=\n     (8) \nSubject to     Nju\nc\ni j i ,... 2 , 1, 1\n1\n=∀=∑\n=\n                                (9) \n \nwhere \n[ ] n\nic Rv ,v,..., v , vV 21 ∈=  is the cluster centers to \nbe decided, [ ]j iuU = is the partition matrix, in which j iu  is \nthe membership of the j-th sample with the i-th group, \nj iij ,, 0vx\n2\n∀>− is a squared distance norm, and m  is \nthe fuzziness coefficient.  \nThe pair of (U,V) that minimizes the above objective \nfunction can be obtained through the following iterations [20]: \n \n  ( ) Njci\nc\nk\nm\nkj\nij\nj i ≤≤≤≤\n\n\n\n\n\n\n\n\n−\n−\n=\n∑\n=\n− 1 ,1 ,\nvx\nvx\n1\n1\n12µ    (10) \n \nciN\nj\nm\nj i\nN\nj j\nm\nj i\ni ≤≤=\n∑\n=\n∑\n=\n1 ,\nx\nv\n1\n1\nµ\nµ\n                         (11) \n \nIn the implementation, FCM algorithm starts with a random \npartition matrix. The membership value \nj iu is updated to new \nj iu  \nafter each iteration and the iterations will be terminated if \nε<− new \nj ij i j i\nuu\n,\nmax , where ε is a small positive value.  \n4.2 KFCM ALGORITHM \nIn the above FCM algorithm, the squared-norm is adopted \nas the similarity measure for samples clustering. However this \nmay only be effective in clustering “spherical” datasets. To \ncater for clustering more general datasets, a number of \nvariants of FCM have been proposed [21-23]. Amongst these \nvariants, the kernelized version of FCM, named as kernel \nFCM (KFCM) performs better than the “standard” FCM [22, \n23].  \nInstead of using Euclidean norm as the similarity measure, \nKFCM adopts a more robust kernel induced similarity \nmeasure. This will help the resultant KFCM-FSVM algorithm \neffectively deal with noises and outliers and achieve better \nclassification accuracy than FCM-FSVM algorithm does. The \nnumeric experiments results of cases studies (Section 6) will \nshow that the KFCM-FSVM is more robust than FCM-FSVM \nin transformer fault diagnosis based on DGA measurements. \nKFCM minimizes the following objective function [22]: \n \nMinimize ( )( ) ( ) ( ) ( )\n2\n1 1\nvxVU, ; x ij\nmc\ni\nN\nj j iuJ ξξξ −= ∑\n=\n∑\n=\n  (12)  \n       Subject to      Nju\nc\ni j i ,... 2 , 1, 1\n1\n=∀=∑\n=\n                           (13) \n \nwhere ( )xx : ξξ →  defines a nonlinear transformation \nfrom original input space to a high dimensional feature space. \nIn Equation (12), the norm is in the form of [22, 23]: \n  \n( ) ( ) ( ) ( ) ( )i ji ij jij KKK v , x2v , vx , xvx\n2\n−+=− ξξ   (14) \n \nwhere \n( ) ( ) ( ) yxy x, ξξ\nT\nK =  is a kernel function. In this paper \nthe Gaussian kernel ( ) ( )22\nyxexp y x, σ−−=K  is adopted, \nwhich has the property of ( ) 1x x, =K . By adopting the \nGaussian kernel, the above objective function, i.e. Equation \n(12) can be rewritten as:  \n( )( ) ( ) ( )( ) i j\nmc\ni\nN\nj j i KuJ v , x12VU, ; xˆ\n1 1\n−= ∑\n=\n∑\n=\nξ       (15) \nBy adopting Lagrange multipliers, the iterations for \ncalculating the partition matrix U and group centers V are: \n \n( )( )( ) ( )\n( )( )( ) ( )\nNjci\nK\nK\nc\nk\nm\nk j\nm\ni j\nj i ≤≤≤≤\n−\n−=\n∑\n=\n−\n−\n1 ,1 ,\nv , x1 1\nv , x1 1\n1\n11\n11\nµ  (16)        \n( )\n( )\nci\nK\nK\nN\nj ij\nm\nj i\nN\nj jij\nm\nj i\ni ≤≤=\n∑\n=\n∑\n=\n1 ,\nv , x\nxv , x\nv\n1\n1\nµ\nµ\n            (17) \n \nThe KFCM algorithm starts with a random partition matrix. \nThe partition matrix and group centers are then updated \niteratively until the termination value \nε is reached.  \n4.3 FCM-FSVM AND KFCM-FSVM ALGORITHM \nAfter forming clusters (i.e. determining the cluster centers \nand assigning samples to the clusters based on their \nmembership values), the sample weights ( )kxρ  of FSVM is \ncomputed as an exponentially decaying function [24]: \n \n( )\n( )\n[ ]1 , 0\nexp 1\n2x ∈\n+\n= β\nβ\nρ\ncen \nk\nk\nd\n            (18) \n \n \nwhere \n2 1\nvx ik\ncen \nkd −= is the Euclidean distance between \nthe sample kx and its cluster center iv , and β  determines the \nsteepness of the decay. According to equation (18), the \nsamples closer to the cluster center are treated as more \ncredible and assigned higher weights while the samples away \nfrom the center are treated as less credible and assigned lower \nweights. Once the weight is computed for each sample in the \ntraining dataset, the FSVM is invoked to obtain a nonlinear \nclassifier and finally can make fault classification on the \ntransformer of interest (e.g. testing dataset). The overall \nprocedure of FCM-FSVM and KFCM-FSVM is depicted in \nFigure 3. \n \n \n \nFigure 3. Flowchart of FCM-FSVM and KFCM-FSVM algorithm. \n \n5 DEALING WITH CLASS IMBALANCE \nBoth FCM-FSVM and KFCM-FSVM can deal with the \nproblems of noises and outliers in training dataset. However, \nthe performance of both algorithms could still be jeopardized \nby the effect of class imbalance in training dataset. FCM-\nFSCM and KFCM-FSVM may generate a biased model, \nwhich is in favor of the majority classes (i.e. the fault types \nhaving large number of samples) and provides lower \nclassification accuracy on the minority classes (i.e. the fault \ntypes having small number of samples).  \nThe problem of training algorithms using the class \nimbalanced dataset has attracted more attentions and many \napproaches have been proposed [15, 24]. This paper adopts \nbetween-class weighting and random oversampling approach \nfor handling the class imbalance problem. These two \napproaches will be integrated with FCM-FSVM and KFCM-\nFSVM to provide these two algorithms with the capability to \nreduce the effect of class imbalance without significantly \naffecting their capabilities in dealing with noises and outliers.  \n \n5.1 BETWEEN-CALSS WEIGHTING \nIn this approach, a between-class weight is introduced to \ncombat the class imbalance problem: a lower weight is \nassigned to all samples in the majority class and a higher \nweight is assigned to all samples in the minority class. These \nweights should not be confused with the weights discussed in \nthe previous section, which are assigned to samples based on \ntheir trustworthiness with the class to which they are \nbelonged.   \nThe assignment of between-class weights can be \nexemplified with a following two classes classification. Let \n+η represents the between-class weight for all samples in the \nminority class and −η represents the between-class weight for \nall samples in the majority class. The between-class weights \ncan be assigned as 1=+η  and ηη 1=− , where \nclass minority in  samples  of number \ncalss majority in  samples  of number =η .  \nCombing the above between-class weights with the weights \ncomputed using FCM or KFCM algorithm (refer to Section 4), \nthe overall weights for each sample in the training dataset \nbecomes: \n( ) ++ = ηρω kk x                               (19) \n( ) −− = ηρω kk x                               (20) \n \nwhere \n( )kxρ  is the weights computed with either FCM or \nKFCM. +\nkω and −\nkω are the overall sample weights for the \nsamples in the minority and majority class, respectively. \nBy integrating the above overall weights with FCM-FSVM \nand KFCM-FSVM (simply replacing kρ in equations (1) – (3) \nwith +\nkω and −\nkω ), the FCM-FSVM and KFCM-FSVM can \nreduce the effect of class imbalance in training dataset. The \nresultant FCM-FSVM and KFCM-FSVM integrated with the \nbetween-class weighting are termed as FCM-FSVM-W and \nKFCM-FSVM-W algorithm respectively in the remaining of \nthe paper. \n5.2 RANDOM OVERSAMPLING  \nAnother approach adopted in this paper to handle the class \nimbalance problem is the random oversampling [15]. In this \napproach, the samples in the minority class are randomly \nredrawn with the replacement to create the new samples. Each \nnew sample has the probability ( )\n1−\nmi N of being equally \nselected to any one of the original samples in the minority \nclass, where mi N  is the samples number of the minority class. \nThe above generated samples will be added into the original \nsamples to form a new minority class. \nAdopting this approach for transformer fault diagnosis \nusing a DGA dataset with the imbalanced class, the re-\nsampling is firstly performed on the minority class to increase \nthe sample number of the minority class. And then FCM-\nFSVM or KFCM-FSVM is invoked for fault classification. In \nthe remaining of this paper, the FCM-FSVM and KFCM-\nFSVM integrated with random oversampling are termed as \nFCM-FSVM-S and KFCM-FSVM-S algorithm, respectively.  \n \n\n \n6 CASE STUDIES \nThis section provides case studies to evaluate the algorithms \nimplemented in this paper, including FCM-FSVM and \nKFCM-FSVM, FCM-FSVM-W and KFCM-FSVM-W (i.e. \nFCM-FSVM and KFCM-FSVM integrated with between-class \nweighting), FCM-FSVM-S and KFCM-FSVM-S (i.e. FCM-\nFSVM and KFCM-FSVM integrated with random \noversampling). For the purpose of comparison, SVM, SVM-W \n(i.e. SVM integrated with between-class weighting), and \nSVM-S (i.e. SVM integrated with random oversampling) are \nalso implemented for the case studies. In algorithmic \nimplementations some software routines from the available \ntoolbox have been adopted with necessary modifications and \nextensions [25].  \n6.1 PARAMETERS SELECTION \nIn the above algorithms, several common parameters are set \nas follows: the fuzziness parameter (equations 8 and 12) is set \nas m =2, the iterative termination value in FCM and KFCM is \nset as ε=0.00001 \n and the decay parameter (Equation 18) is set \nas β=0.5. For both FCM and KFCM, a random partition matrix \nis adopted for the initialization. Moreover, the radial basis \nfunction (RBF) kernel ( ) ( ) 2x x\nx , x\nj i\neK ji\n−−\n=\nγ (γ is the \nvariance parameter) is adopted for all the algorithms evaluated \nin the case studies. \nIn the case studies, the original DGA dataset is randomly \nsplit into two parts: a training dataset (consisting of about 70% \nsamples of the original dataset) for deciding the hyperplane \nthat can separate the samples into different classes (i.e. \ndifferent fault types) and a testing dataset (consisting of about \n30% of the original dataset) for verifying the classification \naccuracy of the algorithms. Note that the samples distribution \namong different classes in both training and testing dataset are \nkept as the same as that in the original dataset. To decide the \noptimal values of parameters C and γ in finding the separation \nhyperplane for the training dataset, a grid search with ten folds \n(or five folds) cross-validations are conducted on the training \ndataset for each of the above algorithms. As explained earlier, \nC  is the regularization parameter in FSVM to balance the \nmargin maximization and classification violation (equation \n(1)) and γ is the variance parameter of the RBF kernel.  \nOnce the best set of parameters C and γ is found for each \nalgorithm, each individual algorithm will be trained with its \nbest set of parameters using the training dataset. Finally, the \ntrained models (classifiers) are applied to assess the condition \nof transformers in the testing dataset. Each transformer in the \ntesting dataset will be assigned a class label to indicate its \ncondition, i.e. in normal operating condition or in any fault \ncondition. Before running any algorithm, the data are \nnormalized to [0, 1] interval. Two case studies are presented in \nthe following. \n6.2 CASE STUDY I \nThe DGA dataset in the first case study is digested from \nDuval’s IEC database [3] which consists of 70 DGA records. \nThe faults occurred in the corresponding transformers include: \ndischarge of low energy, discharge of high energy, low and \nmedium temperature thermal fault, high temperature thermal \nfault, and partial discharge. As shown in Table 1, this dataset \nis randomly split into a training dataset (with 51 records) and a \ntesting dataset (with 19 records).  \nIn the dataset, most fault types have almost the same \nnumber of samples. Though the high energy discharge fault \nhas more samples than other types of faults, the number \ndifference is not much significant. Therefore, the above \ndataset will be treated as class balanced dataset and only \nSVM, FCM-FSVM and KFCM-FSVM algorithm are \nevaluated for transformer fault diagnosis based on this dataset. \nIn these three algorithms, Duval’s criteria are used as inputs, \ni.e. the input is the relative portion of three gases as follows: \n%C \n2H 2=x/(x+y+z), %C 2H 4=y/(x+y+z), and %CH 4=z/(x+y+z), \nwhere x, y, and z are gas concentrations of C 2H 2, C 2H 4, and \nCH 4, respectively. \n \n \nTable 1.  DGA dataset I – total 70 records. \nFault type Training \nSamples \nTesting \nSamples \nLow energy discharge fault (D1) 7 3 \nHigh energy discharge fault (D2) 20 8 \nLower/medium range thermal fault (T1) 9 3 \nHigh range thermal fault (T2) 9 3 \nPartial discharge (PD) 6 2 \nTotal 51 19 \n \nThe procedure of dataset splitting, training and testing as \ndescribed above was repeated ten times for SVM, FCM-\nFSVM and KFCM-FSVM. The overall classification \naccuracy, which is defined as: \nsamples   total of number \n  samples  classified correctly  of number Accuracy  Overall =   (21) \n \nis used as the performance indicator in this case study. Table 2 \nsummarizes the accuracy on the testing dataset of the ten trials \nfor each of the three algorithms.  \nIn Table 2, the values of parameters C  (regularization \nparameter in FSVM) and γ (variance parameter of radial basis \nfunction) are decided by conducting a grid search with ten \nfolds cross-validation on the training dataset, where the pair \n(C, γ) is in the range of C ={2 \n0, 2 1, … 2 10 } and  γ ={2 -8, 2 -7, … \n28}. Moreover, since the Dataset I contain five faults types, the \nnumber of clusters in FCM and KFCM is set to five.  \nIt can be seen from Table 2 that the averaged overall \nclassification accuracy over ten trials of the three algorithms \nare KFCM-FSVM (92.6%), FCM-FSVM (90.0%), and SVM  \n(87.9%); and the best overall classification accuracy in ten \ntrials of the three algorithms are KFCM-FSVM (100%), FCM-\nFSVM (94.7%), and SVM  (94.7%). This indicates that \nKFCM-FSVM and FCM-FSVM outperform SVM, which is \ndue to the introduction of weight for each sample in the \ntraining dataset in both KFCM-FSVM and KCM-FSVM to \nreduce the reverse effects of noises and outliers. Moreover, \nKFCM-FSVM is more robust and has tendency to attain \n \nhigher classification accuracy than FCM-FSVM. This is \nbecause KFCM-FSVM algorithm maps the samples into a \nhigher dimensional space before performing the clustering \ntask.  \n \nTable 2. Classification accuracy of ten trials of algorithms ( Dataset I). \nAlgorithms Trial \nNo. C γ   Overall Classification \nAccuracy (%) \n 1 512 0.25 94.7 \n 2 64 1 89.5 \n 3 2 8 89.5 \nSVM 4 4 8 89.5 \n 5 64 0.25 78.9 \n 6 16 1 89.5 \n 7 1024 0.0078125 84.2 \n 8 4 2 84.2 \n 9 256 1 84.2 \n 10 1 8 94.7 \n Average overall accuracy: 87.9 (±5.0) \n 1 512 0.125 94.7 \n 2 16 0.5 89.5 \n 3 64 1 89.5 \n 4 8 8 89.5 \n 5 128 8 84.2 \nFCM-FSVM 6 16 4 89.5 \n 7 4 1 89.5 \n 8 64 8 89.5 \n 9 512 0.03125 89.5 \n 10 1024 4 94.7 \n Average overall accuracy: 90.0 (±3.0) \n 1 256 0.125 100 \n 2 32 8 94.7 \n 3 8 1 89.5 \n 4 8 1 89.5 \nKFCM-FSVM 5 4 16 84.2 \n 6 4 16 94.7 \n 7 8 1 89.5 \n 8 16 8 89.5 \n 9 1 4 94.7 \n 10 4 8 100 \n Average overall accuracy: 92.6 (±5.1) \n \nTo test the significance of results of three algorithms as \npresented in Table 2, the t-test is conducted [26]. The t-test \nresults show that the accuracy of three algorithms at \nsignificant level α=0.05 are SVM 85.2%, FCM-FSVM 88.4%, \nand KFCM-FSVM 89.9%. \nTable 3 shows the typical classification results of both \nFCM-FSVM and KFCM-FSVM algorithm on 15 transformers. \nDue to the complexity of the ageing mechanism of insulation \nmaterial, different fault types might co-exist in one \ntransformer. Thus it would be useful to estimate the \nprobabilities for each type of fault. In Table 3 the algorithm \nindicates the probability of each fault as a percentage. For \nexample, KFCM-FSVM classification result shows \ntransformer TX11 (Table 3) has about 63% probability of \nhaving T2 (high range thermal fault) and has about 25% \nprobability of having T1 (low to medium range thermal fault). \nSuch interpretation will be beneficial for understanding the \noverall condition of a power transformer. \n \n \nTable 3 . KFCM-FSVM and FCM-KFCM Classification Results on 15  \nTransformers. \n[D1 D2 T1 T2 PD] is the probability of each class, where D1- low energy \ndischarge, D2- high energy discharge, T1- lower to medium range thermal fault, T2 \n- high range thermal fault, and PD - Partial discharge. [D1 D2 T1 T2 PD]1 is the \nclassification result of KFCM-FSVM algorithm and [D1 D2 T1 T2 PD] 2 is the \nclassification result of FCM-FSVM algorithm. \nTransformer Probability \nof Each Class (%) \nAlgorithms \nResults \nOn-site \nInspection \nTX1 [0.71 0.20 0.03 0.04 0.02] 1 \n[0.79 0.06 0.05 0.05 0.05]  2 \nD1 \nD1 D1 \nTX2 [0.72 0.17 0.04 0.04 0.03]  1 \n[0.53 0.38 0.03 0.03 0.03]  2 \nD1 \nD1 D1 \nTX3 [0.93 0.01 0.02 0.02 0.02]  1 \n[0.50 0.41 0.03 0.03 0.03]  2 \nD1 \nD1 D1 \nTX4 [0.06 0.88 0.02 0.03 0.01]  1 \n[0.09 0.80 0.05 0.03 0.03]  2 \nD2 \nD2 D2 \nTX5 [0.09 0.66 0.12 0.08 0.05]  1 \n[0.23 0.71 0.02 0.02 0.02]  2 \nD2 \nD2 D2 \nTX6 [0.07 0.46 0.23 0.20 0.04]  1 \n[0.32 0.61 0.02 0.02 0.03]  2 \nD2 \nD2 D2 \nTX7 [0.04 0.76 0.07 0.11 0.02]  1 \n[0.02 0.90 0.03 0.03 0.02]  2 \nD2 \nD2 D2 \nTX8 [0.06 0.90 0.01 0.02 0.01]  1 \n[0.03 0.83 0.04 0.06 0.04]  2 \nD2 \nD2 D2 \nTX9 [0.04 0.07 0.61 0.22 0.06]  1 \n[0.05 0.06 0.47 0.36 0.06]  2 \nT1 \nT1 T1 \nTX10 [0.04 0.07 0.44 0.40 0.05]  1 \n[0.04 0.05 0.67 0.09 0.15]  2 \nT1 \nT1 T1 \nTX11 [0.04 0.06 0.25 0.63 0.02]  1 \n[0.05 0.06 0.21 0.62 0.06]  2 \nT2 \nT2 T2 \nTX12 [0.02 0.02 0.06 0.90 0.00]  1 \n[0.06 0.11 0.13 0.65 0.05]  2 \nT2 \nT2 T2 \nTX13 [0.03 0.03 0.09 0.84 0.01]  1 \n[0.11 0.11 0.14 0.55 0.09]  2 \nT2 \nT2 T2 \nTX14 [0.05 0.05 0.26 0.06 0.58]  1 \n[0.05 0.06 0.26 0.08 0.55]  2 \nPD \nPD PD \nTX15 [0.05 0.05 0.26 0.06 0.58]  1 \n[0.06 0.06 0.25 0.07 0.56]  2 \nPD \nPD PD \n \n6.3 CASE STUDY II \nThis case study is to evaluate the performance of SVM, \nFCM-FSVM, KFCM-FSVM and their variants on a class \nimbalanced dataset. The DGA dataset used in the second case \nstudy consists of 368 DGA records, which are provided by a \nutility company. The corresponding transformers are either in \nnormal operating condition or low to medium range thermal \nfault or partial discharge fault (Table 4).  \n \nTable 4. DGA dataset II – total 368 records. \nTransformer \nCondition Training Samples Testing Samples \nNormal 141 60 \nThermal Fault 95 40 \nPartial Discharge 23 9 \nTotal 259 109 \n \nAs shown in Table 4, this dataset exhibits unequal sample \ndistribution among different fault types and can be regarded as \na class imbalanced dataset: the samples in the class of normal \ncondition significantly outnumber the samples in the class of \npartial discharge fault. For a class imbalance dataset, the \noverall classification accuracy (equation (21)), which is the \nportion of correctly classified samples, may not be a suitable \n \nperformance indicator [24]. Instead, this paper adopts the \ngeometric mean as the performance indicator for the class \nimbalanced dataset:  \n \nSP SE G m ×=                              (22) \n \nclass majority in  samples   total of number \n class majority in  samples  classified correctly  of number SE = (23) \n \nclass minority  in    samples   of number  \n class minority  in    samples   classified  correctly   of number  SP = (24) \n \nIn this case study the procedure of dataset splitting, training \nand testing as mentioned earlier was repeated ten times on \ndataset II. The gas concentrations C \n2H 2, C 2H 4, C 2H 6, CH 4 and \nH 2 in volume are used as inputs in the algorithms. Total nine \nalgorithms including SVM, FCM-FSVM, KFCM-FSVM, \nSVM-W, FCM-FSVM-W, KFCM-FSVM-W, SVM-S, FCM-\nFSVM-S, and KFCM-FSVM-S are evaluated using the above \ndataset in the case study. \nIn SVM-W, FCM-FSVM-W and KFCM-FSVM-W (i.e. \nSVM, FCM-FSVM, and KFCM-FSVM integrated with \nbetween-class weighting respectively), each sample in partial \ndischarge fault class in training dataset is assigned with \nbetween-class weight of one, each sample in thermal fault \nclass is assigned with between-class weight of 23/95=0.24, \nand each sample in normal condition class is assigned with \nbetween-class weight of 23/141=0.16.  \nIn SVM-S, FCM-FSVM-S and KFCM-FSVM-S (i.e. SVM, \nFCM-FSVM, and KFCM-FSVM integrated with random \noversampling respectively), the number of samples in partial \ndischarge class in both training and testing dataset is increased \nby three times using random oversampling. After random \noversampling, the samples in each class are as follows: \ntraining dataset (normal class 141, thermal fault class 95, and \npartial discharge fault class 69); testing dataset (normal class \n60, thermal fault 40, and partial discharge class 27). The \nreason of only increasing the samples in partial discharge class \nby three fold is to minimize the noises and outliers, which \nmight be introduced by the added samples. Table 5 \nsummarizes the classification results (averaged over ten trials) \nof the above nine algorithms. \nIt can be seen from Table 5 that the overall classification \naccuracy could not accurately reflect the algorithm’s \nperformance on the class imbalanced dataset and may mislead \nthe algorithm’s validation. For example, for SVM-S, FCM-\nFSVM-S, and KFCM-FSVM-S algorithm, the overall \nclassification accuracy is 91.5%, 92%, and 92.2% \nrespectively. However, the SP (accuracy for the minority \nclass) of these three algorithms is 73.8%, 80%, and 82.6% \nrespectively. In contrast, the geometric mean G \nm  of these three \nalgorithms is 84.2%, 86.5%, and 87.9% respectively, which \ncould provide a reasonable performance indicator for the \nclassification on both minority and majority class. \n \nTable 5.  Classification Results of Algorithms (Averaged over Ten  \n                Trials). \nAlgorithm \nOverall \nClassification \nAccuracy \n \nSE SP G m  \nSVM 92.6 95.4 67.8 79.8 \nFCM-FSVM 93.5 96.2 65.6 78.9 \nKFCM-FSVM 94.4  96.3 73.4 83.8 \nSVM-W 93.5 95 70 81.4 \nFCM-FSVM-W 93.9 96.4 68.9 81.3 \nKFCM-FSVM-W 94.6 96.7 74.5 84.5 \nSVM-S 91.5 95.1 73.8 84.2 \nFCM-FSVM-S 92 94.6 80 86.5 \nKFCM-FSVM-S 92.2 93.8 82.6 87.9 \n \nTable 5 shows that KFCM-FSVM, KFCM-FSVM-W, and \nKFCM-FSVM-S consistently outperform the corresponding \nSVM and FCM-FSVM, SVM-W and FCM-FSVM-W, and \nSVM-S and FCM-FSVM-S. This proves the consistence of the \ndiagnosis capability of KFCM clustering based FSVM in the \npresence of measurement originated uncertainties. KFCM-\nFSVM algorithm and its variants have the consistent tendency \nin achieving higher faults classification accuracy in the \npresence of noise and outliers as well as the class imbalance in \nthe training dataset. Table 5 also indicates that the algorithms \nintegrated with random oversampling approach can achieve \nbetter performance \n(in terms of geometric mean) than the \nalgorithms integrated with between-class weighting approach \nin dealing with the effect of class imbalance in training \ndataset. \n7 CONCLUSIONS \nThis paper presented a number of clustering based fuzzy \nsupport vector machine algorithms for DGA based transformer \nfault diagnosis under measurement originated uncertainties \ndue to (a) the presence of noises and outliers and (b) the effect \nof class imbalance in a training dataset. The case studies \npresented in this paper indicate that compared to the standard \nsupport vector machine (SVM), the kernel fuzzy c-means \nfuzzy support vector machine (KFCM-FSVM) and its variants \nhave consistent tendency to attain higher faults classification \naccuracy in power transformer fault diagnosis using DGA \nmeasurements. \nREFERENCES \n[1]  IEEE Guide for the Interpretation of Gases Generated in Oil-Immersed \nTransformers, IEEE Standard C57.104-2008, 2009. \n[2]  M. Duval, “Dissolved gas analysis: it can save your transformer,” IEEE \nElectr. Insul. Mag., Vol.5, No.6, pp. 22-27, 1989. \n[3]  M. Duval and A. dePablo, “Interpretation of Gas-In-Oil Analysis Using \nNew IEX Publication 60599 and IEC TC 10 Databases”, IEEE Electr. \nInsul. Mag., Vol. 17, No.2, pp. 31-41, 2001. \n[4]  C.F. Lin, J.M. Ling, and C.L. Huang, “An Expert Sys tem for \nTransformer Fault Diagnosis Using Dissolved Gas Analysis”, IEEE \nTrans. Power Delivery, Vol. 8, pp.1836-1841, 1993.   \n[5]  M. Islam, T. Wu, and G. Ledwich, “A novel fuzzy logic approach to \ntransformer fault diagnosis”, IEEE Trans. Dielectr. Electr. Insul., Vol. 7, \npp. 177-186, 2000.  \n[6] K. Tomsovic, M. Tapper, and T. Ingvarsson, “A Fuzzy Information\nApproach to Integrating Different Transformer Diagnostic Methods”, \nIEEE Trans. Power Delivery, Vol. 11, pp. 1836-1841, 1996. \n[7] J.L. Guardado, J.L. Naredo, P. Moreno, and C.R. Fuerte, “A \nComparative Study of Neural Network Efficiency in Power \nTransformers Diagnosis Using Dissolved Gas Analysis”, IEEE Trans. \nPower Delivery, Vol. 16, pp. 643- 647, 2001. \n[8] Y. Zhang, X. Ding, Y. Liu, and P.J. Griffin, “An Artificial Neural \nApproach to Transformer Diagnostic Methods”, IEEE Trans. Power \nDelivery, Vol. 8, pp. 1638-1644, 1993.\n[9] V. Miranda and A.R.G. Castro, “Improving the IEC Table for\nTransformer Failure Diagnosis With Knowledge Extraction From Neural \nNetwork”, IEEE Trans. Power Delivery, Vol. 20, pp. 2509-2516, 2005. \n[10] Z.Y. Wang, Y.L. Liu and P.J. Griffin, “A Combined ANN and Expert \nSystem Tool for Transformer Fault Diagnosis”,\n IEEE Trans. Power \nDelivery, Vol. 13, pp. 1224-1229, 1998. \n[11] R. Naresh, V. Sharma, and M. Vashisth, “An Integrated Neural Fuzzy\nApproach for Fault Diagnosis of Transformers”, IEEE Trans. Power\nDelivery, Vol. 23, pp. 2017-2024, 2008. \n[12] Y.-C. Huang, H.-T. Huang, and C.-L. Huang, “Developing a New \nTransformer Fault Diagnosis System through Evolutionary Fuzzy logic”, \nIEEE Trans. Power Delivery, Vol. 12, pp.761-767, 1997. \n[13] Y.-C. Huang, “Evolving Neural Nets for Fault Diagnosis of Power\nTransformers”, IEEE Trans. Power Delivery, Vol. 18, pp. 843-848, \n2003. \n[14] H. Ma, T. K. Saha, and C. Ekanayake, “Intelligent Framework and \nTechniques for Power Transformer Insulation Diagnosis”, IEEE Power\nand Energy Society, General Meeting, Paper No. PESGM2009-000425, \nCalgary, Alberta, Canada, 2009. \n[15] H. He and E.A. Garcia, “Learning from Imbalanced Data”, IEEE Trans. \nKnowledge and Data Eng., Vol.21, pp. 1263-1284, 2002. \n[16] V. Vapnik, The Nature of Statistical Learning Theory, Springer, 1995. \n[17] N. Cristianini and J. Shawe-Taylor, An Introduction of Support Vector \nMachines and Other Kernel-Based Learning Methods, Cambridge\nUniversity Press, 2001. \n[18] C.-F. Lin and S.-D. Wang, “Fuzzy Support Vector Machines”, IEEE\nTrans. Neural Networks, Vol.13, pp. 464-471, 2002. \n[19] Y. Wang, S. Wang, and K.K. Lai, “A New Fuzzy Support Vector\nMachine to Evaluate Credit Risk,” IEEE Trans. Fuzzy Systems, Vol.13,\npp. 820-831, 2005. \n[20] R. Babuska, P.J. van der Veen, and U. Kaymak, “Improved Covariance \nEstimation for GustafsonKessel Clustering”, IEEE Int’l. Conf. Fuzzy \nSystem, Honolulu, pp. 1081-1085, USA, 2002. \n[21] M. Filippone, F. Camastra, F. Masulli, and S. Rovetta, “A Survey of\nKernel and Spectral Methods for Clustering”, Pattern Recognition, Vol.\n41, pp. 176-190, 2008. \n[22] D. Graves and W. Pedrycz, “Kernel-based Fuzzy Clustering and Fuzzy \nClustering: A Comparative Experimental Study”, Fuzzy Sets and \nSystems, Vol. 161, pp. 522-543, 2010. \n[23] X. Yang, G. Zhang, J. Lu, and J. Ma, “A Kernel Fuzzy c-Means \nClustering-Based Fuzzy Support Vector Machine Algorithm for\nClassification Problems with Outliers or Noises”, IEEE Trans. Fuzzy \nSyst., Vol. 19, pp. 105-115, 2011. \n[24] R. Batuwita and V. Palade, “FSVM-CIL: Fuzzy Support Vector\nMachines for Class Imbalanced Learning”, IEEE Trans. Fuzzy Systems, \nVol. 18, pp. 558-571, 2010. \n[25] C.-C. Chang and C.-J. Lin, “LIBSVM: a library for support vector\nmachines,” available: http://www.csie.ntu.edu.tw/~cjlin/libsvm. \n[26] D.V. Moore and G.P. Mccabe, Introduction to the Practice of Statistics,\n5\nth Edition, W.H. Freemand and Company, 2006. \nHui Ma  (M’95) received the B.Eng. and M.Eng. \ndegrees from Xi’an Jiaotong University, China in \n1991 and 1994, respectively, the M.Eng (by \nresearch) degree from Nanyang Technological \nUniversity, Singapore in 1998, and the Ph.D. degree \nfrom the University of Adelaide, Adelaide, Australia \nin 2008. Currently Dr. Ma is a research fellow in the \nSchool of Information Technology and Electrical \nEngineering, the University of Queensland, \nAustralia. Prior to joining the University of \nQueensland, Dr. Ma had many years of research and \ndevelopment experience. From 1994 to 1995, he was a researcher in Xi’an \nJiaotong University, China. From 1997 to 1999, he worked as a firmware \ndevelopment engineer in CET Technologies Pte. Ltd., Singapore. He was with \nSingapore Institute of Manufacturing Technology as a research engineer from \n1999 to 2003. His research interests include industrial informatics, condition \nmonitoring and diagnosis, power systems, wireless sensor networks, and \nsensor signal processing. \nChandima Ekanayake (M’00) received the \nB.Sc.Eng.(Hons) degree in 1999 from the University \nof Peradeniya, Sri Lanka. He obtained the Tech. Lic. \nand Ph.D. degrees from Chalmers University of \nTechnology Sweden in 2003 and 2006, respectively. \nCurrently he is a lecturer  in the School of \nInformation Technology and Electrical Engineering, \nthe University of Queensland (UQ), Brisbane, \nAustralia. Before joining UQ he was with the \nUniversity of Peradeniya Sri Lanka as a Senior \nlecturer. During his Ph.D. degree studies he was \nworking for a European Union Project called REDIATOOL where he engaged \nin research related to diagnostics of transformer insulation from dielectric \nresponse measurements. From 2001, he has been involving on condition \nmonitoring of transformers installed at Ceylon Electricity Board, Sri Lanka. \nHe was the Chair of IEEE Sri Lanka Section in year 2006 and 2007. His \nresearch interests are condition monitoring of power apparatus, alternatives \nfor insulating oil, transient studies on power systems and energy related \nstudies.   \nTapan Kumar Saha  (M’93-SM’97) was born in \nBangladesh in 1959 and immigrated to Australia in 1989. \nHe received the B.Sc. Eng. (electrical and electronic) in \n1982 from the Bangladesh University of Engineering & \nTechnology, Dhaka, Bangladesh, M. Tech (electrical \nengineering) in1985 from the Indian Institute of \nTechnology, New Delhi, India and the Ph.D. degree in \n1994 from the University of Queensland, Brisbane, \nAustralia. Tapan is currently Professor of Electrical \nEngineering in the School of Information Technology \nand Electrical Engineering, University of Queensland, \nAustralia. Previously he has had visiting appointments for a semester at both the \nRoyal Institute of Technology (KTH), Stockholm, Sweden and at the University of \nNewcastle (Australia). He is a Fellow of the Institution of Engineers, Australia. His \nresearch interests include condition monitoring of electrical plants, power systems \nand power quality. \n",
  "topic": "Outlier",
  "concepts": [
    {
      "name": "Outlier",
      "score": 0.7485036849975586
    },
    {
      "name": "Cluster analysis",
      "score": 0.6178911924362183
    },
    {
      "name": "Computer science",
      "score": 0.5722118616104126
    },
    {
      "name": "Support vector machine",
      "score": 0.5397215485572815
    },
    {
      "name": "Fuzzy logic",
      "score": 0.520098090171814
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5186557173728943
    },
    {
      "name": "Dissolved gas analysis",
      "score": 0.5084213614463806
    },
    {
      "name": "Weighting",
      "score": 0.5069773197174072
    },
    {
      "name": "Anomaly detection",
      "score": 0.5012717247009277
    },
    {
      "name": "Data mining",
      "score": 0.500840425491333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.493429571390152
    },
    {
      "name": "Transformer",
      "score": 0.3821403384208679
    },
    {
      "name": "Engineering",
      "score": 0.10690507292747498
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Transformer oil",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165143802",
      "name": "The University of Queensland",
      "country": "AU"
    }
  ],
  "cited_by": 58
}