{
  "title": "ReFormer: The Relational Transformer for Image Captioning",
  "url": "https://openalex.org/W3184581815",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2177825436",
      "name": "Yang Xue-wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2304588178",
      "name": "Liu Yingru",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1837861352",
      "name": "Wang Xin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963758027",
    "https://openalex.org/W2479423890",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W3107503524",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2989377923",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3103651098",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963088515",
    "https://openalex.org/W2963260436",
    "https://openalex.org/W68733909",
    "https://openalex.org/W2962779575",
    "https://openalex.org/W3107848485",
    "https://openalex.org/W3107492437",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963938081",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2579549467",
    "https://openalex.org/W3035017890",
    "https://openalex.org/W2963565375",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3110157234",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3034316193",
    "https://openalex.org/W2795151422",
    "https://openalex.org/W2481240925",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2963686907",
    "https://openalex.org/W2939888942",
    "https://openalex.org/W2963360627",
    "https://openalex.org/W2963536419",
    "https://openalex.org/W3016211260"
  ],
  "abstract": "Image captioning is shown to be able to achieve a better performance by using scene graphs to represent the relations of objects in the image. The current captioning encoders generally use a Graph Convolutional Net (GCN) to represent the relation information and merge it with the object region features via concatenation or convolution to get the final input for sentence decoding. However, the GCN-based encoders in the existing methods are less effective for captioning due to two reasons. First, using the image captioning as the objective (i.e., Maximum Likelihood Estimation) rather than a relation-centric loss cannot fully explore the potential of the encoder. Second, using a pre-trained model instead of the encoder itself to extract the relationships is not flexible and cannot contribute to the explainability of the model. To improve the quality of image captioning, we propose a novel architecture ReFormer -- a RElational transFORMER to generate features with relation information embedded and to explicitly express the pair-wise relationships between objects in the image. ReFormer incorporates the objective of scene graph generation with that of image captioning using one modified Transformer model. This design allows ReFormer to generate not only better image captions with the bene-fit of extracting strong relational image features, but also scene graphs to explicitly describe the pair-wise relation-ships. Experiments on publicly available datasets show that our model significantly outperforms state-of-the-art methods on image captioning and scene graph generation",
  "full_text": "ReFormer: The Relational Transformer for Image Captioning\nXuewen Yang*\nxuewen.yang@innopeaktech.com\nInnoPeak Technology, Inc.\nPalo Alto, CA, USA\nYingru Liu\nliu2231665@hotmail.com\nStony Brook University\nStony Brook, NY, USA\nXin Wang\nx.wang@stonybrook.edu\nStony Brook University\nStony Brook, NY, USA\nABSTRACT\nImage captioning is shown to be able to achieve a better perfor-\nmance by using scene graphs to represent the relations of objects in\nthe image. The current captioning encoders generally use a Graph\nConvolutional Net (GCN) to represent the relation information and\nmerge it with the object region features via concatenation or con-\nvolution to get the final input for sentence decoding. However, the\nGCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as\nthe objective (i.e., Maximum Likelihood Estimation) rather than a\nrelation-centric loss cannot fully explore the potential of the encoder.\nSecond, using a pre-trained model instead of the encoder itself to\nextract the relationships is not flexible and cannot contribute to the\nexplainability of the model. To improve the quality of image cap-\ntioning, we propose a novel architecture ReFormer- a RElational\ntransFORMER to generate features with relation information em-\nbedded and to explicitly express the pair-wise relationships between\nobjects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified\nTransformer model. This design allows ReFormer to generate not\nonly better image captions with the benefit of extracting strong rela-\ntional image features, but also scene graphs to explicitly describe the\npair-wise relationships. Experiments on publicly available datasets\nshow that our model significantly outperforms state-of-the-art meth-\nods on image captioning and scene graph generation.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computer vision; Image repre-\nsentations.\nKEYWORDS\nimage captioning, graph neural networks, transformers, sequence to\nsequence modeling\nACM Reference Format:\nXuewen Yang, Yingru Liu, and Xin Wang. 2022. ReFormer: The Relational\nTransformer for Image Captioning. In Proceedings of the 30th ACM In-\nternational Conference on Multimedia (MM â€™22), October 10â€“14, 2022,\n*The author is currently a senior research scientist at InnoPeak Technology,\nInc. He was previously a PhD student in Stony Brook University, using email\nxuewen.yang@stonybrook.edu.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9203-7/22/10. . . $15.00\nhttps://doi.org/10.1145/3503161.3548409\nLisboa, Portugal. ACM, New York, NY , USA, 9 pages. https://doi.org/10.\n1145/3503161.3548409\n1 INTRODUCTION\nResearch on image captioning to generate textual descriptions of im-\nages has made a great progress in recent years thanks to the introduc-\ntion of encoder-decoder architectures [3, 4, 17, 18, 28, 40, 44, 50].\nExisting models are generally trained and evaluated on datasets cre-\nated for image captioning like COCO [ 6, 23] and Flickr [15] that\nonly contain generic object categories but not pair-wise relations of\nthe objects in the image.\nTo equip the captioning model with relation information, some\nmore recent studies resort to the scene graph generation [ 43, 53]\nto provide the graph representations of real-world images with the\nsemantic summaries of objects and their pair-wise relationships. For\nexample, the graph in Figure 1 encodes the key objects in the image\nsuch as people (â€˜manâ€™), their possessions (â€˜hairâ€™ and â€˜shirtâ€™, both\npossessed by the man), and their activities (the man is â€˜holdingâ€™ a\nâ€˜racketâ€™). The graph representation has been applied to improve the\nimage related tasks that involve natural language [37, 52]. When it\ncomes to the task of image captioning, recent studies [ 41, 47, 51]\npropose to first use a scene graph generation model well-trained on\nVisual Genome [20] dataset to predict the pair-wise relationships\nexisting in the COCO image and then use a Graph Convolutional\nNet (GCN) to encode the relation information. Typically, the object\nregion features and the relation representations are then merged\ntogether via concatenation or convolution to feed into a decoder for\ngenerating a sentence using the Maximum Likelihood Estimation\n(MLE). These methods typically suffer from at least one of three\nmain weaknesses: (i) There are mis-alignments between the image\nobjects and the relation labels, because the regions containing the\nobjects do not correspond to those used to predict the relations;\n(ii) Given that the goal of using a GCN is to extract the relation\ninformation, the training of model for GCN is less effective by only\nusing the objective to optimize the captioning without considering\nthe object relationship; (iii) The encoder itself cannot extract the\nrelations between objects but relying on other pre-trained models\nto do it, which makes the captioning less explainable. As another\nobservation, recent studies [1, 5, 9, 11, 34] have pointed out that good\nmetric scores can be achieved with a strong decoder, without the need\nof underlying encoder to truly understand the visual content. Thus,\nit becomes less likely to determine if the models are really learning\nsome important relationships through the encoder or they just follow\nsome language rules by the decoder. To be more concrete, for a\ngenerated sentence like â€˜a man is riding a bikeâ€™, can the model really\ntell the difference among â€˜ridingâ€™, â€˜rollingâ€™ or â€˜onâ€™ or it just follows\nsome language expression rules ( i.e., â€˜ridingâ€™ is more commonly\nused than â€˜rollingâ€™ and â€˜onâ€™)?\narXiv:2107.14178v2  [cs.CV]  14 Jul 2022\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Xuewen Y ang, Yingru Liu, and Xin Wang\nman\nhair\nshirt\nhandracket\nballwith holdinginholdingwearing\nFigure 1: A scene graph containing entities, such as man, hair\nor racket, that are localized in the image with bounding boxes\nand the relationships between those entities, such as with,\nwearing and holding.\nRegularizing the encoder with a relation-centric objective is essen-\ntial since it can not only guide the encoder to learn representations\nwith relation information embedded, but also explicitly express the\npair-wise relationships and explain the generation of some relational\nwords. In this paper, we propose ReFormer: the RElational trans-\nFORMER that learns a scene graph to express the object relation-\nships in the process of decoding a sentence description. ReFormer\nincorporates both image captioning and scene graph generation com-\nponents via a novel transformer encoder. Different from conventional\nTransformer [38] that only uses the image captioning as the final\nobjective to train both the encoder and the decoder, ReFormer uses a\nscene graph generation objective to guide the encoder to learn better\nrelational representations. Since the image captioning and scene\ngraph generation are two distinct tasks, directly using the Multi-Task\nLearning paradigm is non-trivial. We propose a sequential training\nalgorithm that guides the ReFormer to learn both tasks step by step.\nOur work has three main contributions. (i) We propose to generate\nscene graphs as a way to enrich the captions that they together\ncan better describe the images; (ii) We design a novel relational\nTransformer (ReFormer) that can better learn the image features\nfor captioning with the relationships embedded via an auxiliary\nscene graph generation task; (iii) We propose a sequential training\nalgorithm that guides the ReFormer to accomplish both tasks in\nthree consecutive steps. Experimental results show that ReFormer\ncan achieve better performance than state-of-the-art methods on\nboth image caption generation and scene graph generation. We will\nrelease the source code.\n2 BACKGROUND AND RELATED WORK\nIn this section, we first introduce the background knowledge of\nscene graph generation, and then discuss the related work on image\ncaptioning and the application of scene graphs in image captioning.\n2.1 Scene Graph Generation\nA scene graph, ğº, as shown in Figure 1, is a structural representation\nof the semantic contents in an image [20]. It consists of:\nâ€¢a set ofbounding boxes ğµ = {ğ‘1,...,ğ‘ ğ‘š}, ğ‘ğ‘– = (ğ‘¥ğ‘–1,ğ‘¦ğ‘–1,ğ‘¥ğ‘–2,ğ‘¦ğ‘–2)1.\n1ğ‘¥ğ‘–1,ğ‘¦ğ‘–1 are the top-left coordinates of ğ‘ğ‘–, while ğ‘¥ğ‘–2,ğ‘¦ğ‘–2 are the bottom-right\ncoordinates.\nâ€¢a corresponding set of objects ğ‘‚ = {ğ‘œ1,...,ğ‘œ ğ‘š}, where ğ‘œğ‘– âˆˆ\nCis the class label assigned to the bounding box ğ‘ğ‘– and Cis\nthe set containing all label categories.\nâ€¢a set of pair-wise relationships ğ‘… = {...,ğ‘Ÿ ğ‘–â†’ğ‘—,... }, with\nğ‘Ÿğ‘–â†’ğ‘— âˆˆR representing the relationship between a start node\n(ğ‘ğ‘–,ğ‘œğ‘–)and an end node (ğ‘ğ‘—,ğ‘œğ‘—). Ris the set of relation types,\nincluding the â€˜backgroundâ€™ predicate, which indicates that\nthere is no edge between the specified objects.\nScene graph [ 35, 36, 53] is often generated with a few proce-\ndures: object detection (detecting ğ‘ğ‘–), classification (classifying ğ‘œğ‘–)\nand predicate (relation label) prediction to determine ğ‘Ÿğ‘–â†’ğ‘— given\n(ğ‘ğ‘–,ğ‘œğ‘–)and (ğ‘ğ‘—,ğ‘œğ‘—). Most of the methods on scene graph generation\nhave been developed on the Visual Genome [ 20] dataset, which\nprovides annotated scene graphs for 100K images, consisting of\nover 1M instances of objects and 600K relations. Since only a small\nportion of images in this data set also exist in the COCO captioning\ndataset [6], directly using a multi-task learning scheme on both tasks\nis challenging.\n2.2 Image Captioning\nState-of-the-art approaches [3, 10, 12, 17, 24, 25, 33, 42, 44, 49, 50]\nmainly use encoder-decoder frameworks with attention to generate\ncaptions for images. Xu et al. [44â€“46, 48] developed soft and hard\nattention mechanisms to focus on different regions in the image\nwhen generating different words. Similarly, Anderson et al. [3]\nused a Faster R-CNN [31] to extract regions of interest that can be\nattended to. Yang et al. [50] used self-critical sequence training for\nimage captioning.\nVarious Transformer-based [38] models have achieved promising\nsuccess on the image captioning task [7, 12, 13, 21]. Cornia et al. [7]\nproposed a meshed-memory transformer that learns a multi-level\nrepresentation of the image regions, and uses a mesh-like connec-\ntivity at decoding stage to exploit low- and high-level features. Li\net al. [21] introduced the entangled attention that enables the Trans-\nformer to exploit semantic and visual information simultaneously.\nHe et al. [12] introduced the image transformer, which consists of a\nmodified encoding transformer and an implicit decoding transformer\nto adapt to the structure of images. Herdade et al. [13] introduced\nthe object transformer, that explicitly incorporates information about\nthe spatial relationship between detected objects through geometric\nattention.\nSome research studies have been using scene graphs for image\ncaptioning [26, 41, 51, 54]. Zhong et al. [54] proposes a scene graph\ndecomposition method that decomposes a scene graph into a set of\nsub-graphs, with each sub-graph capturing a semantic component\nof the input image. By selecting important sub-graphs, different\ntarget sentences are decoded. Wang et al. [41] uses two encoders,\none is a ResNet image encoder, the other is a GCN encoder for the\nrelation labels of the objects. The two encoders are then attached with\nan LSTM decoder and combined together using attention. Yao et\nal. [51] proposes to use two GCNs to encode the spatial and semantic\nrelations in an image. Then the features from the two encoders are\nmerged together via attention to get the final feature. Guo et al. [26]\nproposed to explicitly model the object interactions in semantics and\ngeometry based on Graph Convolutional Networks (GCNs). Yanget\nal. [47] proposed the scene graph auto-encoding technique to learn\nReFormer: The Relational Transformer for Image Captioning MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\na dictionary that helps to encode the desired language prior, which\nguides the encoding-decoding pipeline.\nReFormer differs from the previous methods in two aspects: (i)\nReFormer can not only generate a caption but also a scene graph\nto capture the relationships between the objects without using an\nexternal scene graph generator. (ii) ReFormer integrates the scene\ngraph generation with image captioning using a sequential training\nalgorithm to better learn the relational image features step by step.\n3 DESIGN OF REFORMER\nIn this section, we first discuss the problems of existing schemes\nand propose the basic architecture to construct the ReFormer that\ncombines the scene graph generation with image captioning to learn\nrelational features of images. We then present the scene graph gener-\nation task which is used to first pre-train the encoder of ReFormer\nand later used as an auxiliary objective to generate captions. Fi-\nnally, we describe the image captioning task as well as the training\nalgorithm to train on both tasks.\n3.1 A Relational Encoding Learning Idea\nIn a standard image captioning model based on encoder-decoder\nstructure, the decoder directly predicts the target sequence y con-\nditioned on the source input x. The captioning probability ğ‘ƒ(y|x)\nis modeled directly using the probability of each target word yğ‘– at\nthe time step ğ‘– conditioned on the source input sequence x and the\ncurrent partial target sequence y1:ğ‘–âˆ’1 as follows:\nğ‘ƒ(y|x; ğœ½)=\nğ‘Ã–\nğ‘–=1\nğ‘ƒ(yğ‘–|x,y1:ğ‘–âˆ’1; ğœ½) (1)\nwhere ğœ½ denotes the parameters of the model2.\nIn general, x are image features that can be obtained by feeding an\nimage ğ‘‹ to a pre-trained CNN (e.g. ResNet) encoder x = ğ¶ğ‘ğ‘ (ğ‘‹).\nTo integrate relational information into the image features, some\nstate-of-the-art methods first use a well-trained scene graph genera-\ntion model to extract a graph ğ‘”from the same image and then use\na Graph Convolutional Net (GCN) to encode ğ‘”to vectors, shown\nin Figure 2a. Then the image features and the relational features\nare merged together x = [ğ¶ğ‘ğ‘ (ğ‘‹);ğºğ¶ğ‘(ğ‘”)], where [Â·; Â·]can be\nconcatenation, attention or convolution. This straightforward way of\nintegrating relational information may suffer from a few problems.\nFirst, training a GCN with the objective of getting the image caption\n(Maximum Likelihood Estimation, i.e., MLE) might be less effective.\nThe goal of using a GCN is to capture the relational information\nexisting in the image, while the likelihood function used to estimate\nthe probability distribution is not directly relevant to the relation of\nobjects in an image. Second, if the model no longer has access to\nthe pre-trained scene graph generator (e.g., using a different dataset),\nthe caption generation might not be feasible any more. Third, recent\nstudies show that good metric scores can be obtained with a strong\ndecoder, without the underlying encoder to truly understand the vi-\nsual content. This indicates that the caption objective alone cannot\neffectively guide the training of the encoder to accurately extract the\nrelationships between the objects.\nThe above analysis motivates us to design a model that can gener-\nate scene graphs together with the learning of image caption using\n2Through-out this paper, we omit ğœ½ for simplicity.\nimage decoder\nencoderGCN\nCNN caption\n(a) Two-encoder captioning model.\nimage decoder\nencoderReFormercaption\nscene graph\n(b) ReFormer: Our Relational Transformer model.\nFigure 2: Models trying to encode relation information: (a) A widely-\nused two-encoder captioning model that one encoder is a GCN used\nto encode the scene graphs into relational features and the other is a\npre-trained CNN (i.e. ResNet) for images. (b) ReFormer, our Relational\nTransformer that generates a scene graph and a corresponding caption.\nthe same dataset without the need of an external scene graph gener-\nator, but with the objective of producing the good scene graph. In\nthis way, we can also better train the encoder to well understand\nthe visual contents without being misled by the good results from a\nstronger decoder. We propose a second objective ğ‘ƒ(ğ‘”|ğ‘‹)for good\nscene graph generation and apply it to the encoder learning. We use\na Transformer model where the encoder is used to generate scene\ngraphs and the decoder is applied to generate captions. By incorpo-\nrating this â€˜RElationalâ€™ objective, our transFORMER can also better\nlearn the image features with relation information embedded. The\nsimplified model structure of ReFormer is shown in Figure 2b.\n3.2 Scene Graph Generation\nWe first lay out the mathematical formulation of the scene graph\ngeneration problem. As introduced in Section 2.1, for an image\nof ğ‘š objects, its visually grounded scene graph consists of tuples\u0000ğ‘Ÿğ‘–â†’ğ‘—,(ğ‘ğ‘–,ğ‘œğ‘–),(ğ‘ğ‘—,ğ‘œğ‘—)\u0001ğ‘š\nğ‘–,ğ‘—=1,ğ‘–â‰ ğ‘—\n3, with ğ‘ğ‘– being the bounding box\nof the object ğ‘–, ğ‘œğ‘– the label and ğ‘Ÿğ‘–â†’ğ‘— the predicate (relation label)\nbetween objects ğ‘– and ğ‘—. Thus, the scene graph generation objective\ncan be derived as follows:\nğ‘ƒ(ğ‘”|ğ‘‹)=\nğ‘šÃ–\n(ğ‘–,ğ‘—)\nğ‘ƒ(ğ‘Ÿğ‘–â†’ğ‘—|(ğ‘ğ‘–,ğ‘œğ‘–),(ğ‘ğ‘—,ğ‘œğ‘—),ğ‘‹)\nÃ—\nğ‘šÃ–\nğ‘˜\nğ‘ƒ(ğ‘œğ‘˜|ğ‘ğ‘˜,ğ‘‹)ğ‘ƒ(ğ‘ğ‘˜|ğ‘‹)\n(2)\nTo simplify the learning process and improve the effectiveness of\ntraining, we conveniently use Faster-RCNN to model ğ‘ƒ(ğ‘œğ‘˜|ğ‘ğ‘˜,ğ‘‹)\nand ğ‘ƒ(ğ‘ğ‘˜|ğ‘‹)together. The negative log-likelihood of the modelâ€™s\nparameters computed on the training data is then given by:\nLğ‘” = âˆ’log ğ‘ƒ(ğ‘”|ğ‘‹)= âˆ’\nğ‘šâˆ‘ï¸\n(ğ‘–,ğ‘—)\nlog ğ‘ƒ(ğ‘Ÿğ‘–â†’ğ‘—|(ğ‘ğ‘–,ğ‘œğ‘–),(ğ‘ğ‘—,ğ‘œğ‘—),ğ‘‹)\nâˆ’\nğ‘šâˆ‘ï¸\nğ‘˜\n[log ğ‘ƒ(ğ‘ğ‘˜|ğ‘‹)+ log ğ‘ƒ(ğ‘œğ‘˜|ğ‘ğ‘˜,ğ‘‹)]\n(3)\n3ğ‘– âˆˆ1 ...ğ‘š , ğ‘— âˆˆ1 ...ğ‘š , but ğ‘– â‰  ğ‘—.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Xuewen Y ang, Yingru Liu, and Xin Wang\n< man withhair >< man holdingracket >\nmanhandracketballhairbox1box3box5box4box2\nFaster R-CNN\nEncoder Layer 1\nğ‘¦\":$â‹¯Ã—ğ‘š\nâ‹¯Ã—ğ‘š\nâ‹¯Ã—ğ‘š(ğ‘šâˆ’1)â¨‚â¨‚ â‹¯Ã—ğ‘š(ğ‘šâˆ’1)\nEncoder Layer ğ‘.\nPost-Processing Layer\nPre-Processing Layerğ‘ƒ(ğ‘¦$|ğ‘‹,ğ‘¦\":$)\nğ‘ƒ(ğ‘Ÿ|ğ‘,ğ‘œ,ğ‘‹)\na man with long hair holding a ball and tennis racket.\nobject detection\nimage captioningâ‹¯\nDecoder Layer 1Decoder Layer ğ‘6â‹¯\nscene graph generation\n(a) (b) (c)\nğ‘ƒ(ğ‘|ğ‘‹)ğ‘ƒ(ğ‘œ|ğ‘‹)\nman\nhairshirt handracket\nballwithholding\ninholdingwearing\na generated graph\n(d)\nFigure 3: The architecture of ReFormer. ReFormer consists of a (a) Faster R-CNN object detection model to provideğ‘šbounding boxes, object labels\nand object region features, a (b) Transformer encoder to generate ğ‘šÃ—(ğ‘šâˆ’1)pair-wise relations for the objects in the image and a (c) Transformer\ndecoder for generating captions. The generated scene graph is shown in (d).\nWe model ğ‘ƒ(ğ‘ğ‘˜|ğ‘‹)and ğ‘ƒ(ğ‘œğ‘˜|ğ‘ğ‘˜,ğ‘‹)with object detector Faster R-\nCNN [31] to automatically generate a set of bounding boxes ğ‘ğ‘˜ and\nthe corresponding object labels ğ‘œğ‘˜ from an image ğ‘‹ (Figure 3(a)).\nIn practice, training a Faster R-CNN is highly non-trivial. To sim-\nplify the training of our proposed ReFormer, we first train a Faster\nR-CNN till its convergence. Then with its parameters learnt, we train\nthe model ğ‘ƒ(ğ‘Ÿğ‘–â†’ğ‘—|(ğ‘ğ‘–,ğ‘œğ‘–),(ğ‘ğ‘—,ğ‘œğ‘—),ğ‘‹)(Figure 3(b)) with (ğ‘ğ‘–,ğ‘œğ‘–),\n(ğ‘ğ‘—,ğ‘œğ‘—)and ğ‘‹ as the input. In theory, if the groundtruth ğ‘ğ‘˜ and\nğ‘œğ‘˜ are provided, we can eliminate Faster R-CNN. To feed inğ‘ğ‘˜, ğ‘œğ‘˜\nand ğ‘‹, we use a Pre-Processing Layer (Section 3.3.1) which converts\nthe three inputs into vectors and then concatenate them together to\nget the final input features.\nModeling ğ‘ƒ(ğ‘Ÿğ‘–â†’ğ‘—|(ğ‘ğ‘–,ğ‘œğ‘–),(ğ‘ğ‘—,ğ‘œğ‘—),ğ‘‹)is actually a predicate clas-\nsification task. We use the ReFormer encoder (Section 3.3.2) to\nfirst encode the input into high-level features and then apply a Post-\nProcessing Layer (Section 3.3.3) to get the final output for classifi-\ncation. The features right before the Post-Procesing layer contain\nuseful relational information and are used as the input to the decoder\nfor caption generation (Section 3.4). We talk about more details\nabout the model architecture in Section 3.3.\n3.3 Encoder Architecture\nThe encoder takes as input a set ofğ‘šimage region tuples,(ğ‘ğ‘–,ğ‘œğ‘–,vğ‘–)ğ‘š\nğ‘–=1,\nwhere vğ‘– is defined as the mean-pooled convolutional feature from\nregion ğ‘– with dimension 2048 and the number of region varies for\ndifferent images. It consists of three main components which pro-\ncess the input consecutively: (i) a Pre-Processing Layer that takes\nthe bounding boxes, object labels and image region features as in-\nput and linearizes them to form the input vector; (ii) the Encoder\nLayers further process the features with Multi-Head Attention to\ncreate a contextualized representation of each object; and (iii) the\nPost-Processing Layer where the object features are paired to make\npredictions of their relationships.\n3.3.1 Pre-Processing Layer. Different from conventional image\ncaptioning model where only image region features vğ‘– are used as\nthe input, we also use object labels ğ‘œğ‘– as well as bounding boxes ğ‘ğ‘–\nas the input to encode both object information and spatial relation\ninformation.\nGiven the bounding box ğ‘ = (ğ‘¥1,ğ‘¦1,ğ‘¥2,ğ‘¦2), to better represent\nits location as well as size in the image, we normalize it with\nthe size of the image and convert it into a 9-dimensional vector\n(ğ‘ğ‘¥\nğ‘Š,ğ‘ğ‘¦\nğ» , ğ‘¤\nğ‘Š, â„\nğ»,ğ‘¥1\nğ‘Š, ğ‘¦1\nğ» ,ğ‘¥2\nğ‘Š, ğ‘¦2\nğ» , ğ‘¤â„\nğ‘Šğ» ), where(ğ‘ğ‘¥,ğ‘ğ‘¦)is the coordinate\nof the bounding box center, (ğ‘¤,â„)and (ğ‘Š,ğ» )are the width and\nheight of the bounding box and the image respectively. We use a\ntwo-layer feed-forward net to encode it into vector vğ‘ of dimension\nğ‘‘ğ‘.\nAs object labels (e.g. â€˜manâ€™) are meaningful words from natural\nlanguages, we use Glove [30] features, which contain pre-trained\nfeatures to capture fine-grained semantic and syntactic regularities\nin natural languages. The dimension of the object label feature vğ‘™ is\ndenoted as ğ‘‘ğ‘™.\nThus, the final image feature is represented as v = ğ‘“([vğ‘–; vğ‘; vğ‘™]),\nwhere [Â·; Â·; Â·]is the concatenation operation, and ğ‘“(Â·) is a feed-\nforward network to map the feature to dimension ğ‘‘â„.\n3.3.2 Transformer Encoder Layer.Given a set of image features\nv = {v1,..., vğ‘š}extracted using the Pre-Processing Layer, we use a\nstack of ğ‘ğ¸ Transformer [38] encoder layers to obtain a permutation\ninvariant encoding. Each encoder layer consists of a multi-head\nself-attention layer followed by a small feed-forward network. The\nReFormer: The Relational Transformer for Image Captioning MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nmulti-head self-attention layer itself consists of â„ identical heads.\nEach attention head first calculates the queries Q, keys K and values\nV for the ğ‘šinput features as follows:\nQ = XWğ‘„,K = XWğ¾,V = XWğ‘‰, (4)\nwhere X contains all the input vectors x1,..., xğ‘š stacked into a\nmatrix and Wğ‘„, Wğ¾, Wğ‘‰ are learned projection matrices. The\nimage features are used as the input to the first self-attentive layer.\nFor the following layers, we use the output of the previous encoder\nlayer as the input to the current layer. The output of each head are\nthen computed via scaled dot-product attention without using any\nrecurrence:\nâ„ğ‘’ğ‘ğ‘‘(X)= ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (Q,K,V)= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (QKğ‘‡\nâˆš\nğ‘‘\n)V (5)\nwhere ğ‘‘ is a scaling factor. Eq. 4 and 5 are calculated for every head\nindependently. The output of all â„heads are then concatenated to\none output vector and multiplied with a learned projection matrix\nWğ‘‚:\nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ (X)= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘‘1,...,â„ğ‘’ğ‘ğ‘‘ â„)Wğ‘‚ (6)\nThe multihead attention is then fed into a point-wise feed-forward\nnetwork (FFN), which is applied to each output of the attention\nlayer:\nh = ğ¹ğ¹ğ‘ (x)= max(0,xW1 +b1)W2 +b2 (7)\nwhere W1, b1 and W2, b2 are the weights and biases of two fully\nconnected layers. In addition, skip-connections and layer-norm are\napplied to the outputs of the self-attention and the feed-forward\nlayer.\nOther choices like LSTM or convolutional layers are also feasible.\nIn this paper, we choose self-attention layers because self-attention\noperation can be seen as a way of encoding pair-wise relationships\nbetween input features given that the self attention weights depend\non the pair-wise similarities between the input features. This is\ncoherent to the objective of the paper: to model the relationships\nfrom the input objects.\n3.3.3 Post-Processing Layer. For an image of ğ‘šobjects, there\nare ğ‘š(ğ‘šâˆ’1)possible relationships. For each possible relationship\nbetween object ğ‘– and ğ‘—, we compute the probability of the rela-\ntionship of label ğ‘Ÿğ‘–â†’ğ‘—. Since the output of the last Transformer\nencoder layer h contains ğ‘šfeatures h1,..., hğ‘š, directly predicting\nthe relationships using these features is impossible. We thus use a\nPost-Processing Layer that maps ğ‘šobject features into ğ‘š(ğ‘šâˆ’1)\npair-wise features. We use a Linear layer to map hğ‘– of dimension\nğ‘‘â„ into rğ‘– of dimension 2ğ‘‘â„. By doubling the dimension of hğ‘–, we\ncan then equally split rğ‘– into two parts, head râ„\nğ‘– and tail rğ‘¡\nğ‘–, with\nhead standing for the relationship starts from this node while tail\nstands for the relationship ends at this node. Thus, for each pair-wise\nrelationship representation, we can have:\nrğ‘–â†’ğ‘— = (Wğ‘Ÿ[râ„\nğ‘– ; rğ‘¡\nğ‘—])âŠ™[ vğ‘–; vğ‘—] (8)\nwhere âŠ™is point-wise multiplication operation, [Â·; Â·]is concatenation\nand Wğ‘Ÿ is a trainable parameter. The distribution is:\nğ‘ƒ(ğ‘Ÿğ‘–â†’ğ‘—|(ğ‘ğ‘–,ğ‘œğ‘–),(ğ‘ğ‘—,ğ‘œğ‘—),ğ‘‹)= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (rğ‘–â†’ğ‘—) (9)\n3.4 Weighted Decoder for Image Captioning\nAs discussed in Section 3.1, the objective of our model for caption\ngeneration is to minimize the negative log-likelihood of the correct\ncaption using the maximum likelihood estimation:\nLğ‘ = âˆ’\nğ‘›âˆ‘ï¸\nğ‘–=1\nlog ğ‘(yğ‘–|y1:ğ‘›âˆ’1,x) (10)\nThe decoder that is used to model Eq. 10 consists of a stack of\nğ‘ğ· decoder layers (Figure 3(c)). For each layer, to calculate the\ndistribution for the word at the time step ğ‘–, it takes as input: the\nembeddings of all previously generated words y0:ğ‘–âˆ’1 and the context\nembeddings x from the encoder.\nFor conventional Transformer [38], x = hğ‘ğ¸\n1:ğ‘š, which means the\ndecoder layers only take the output of the final layer (i.e. the ğ‘ğ¸-th)\nas input. This might omit some of the useful features from the lower\nencoder layers. In this paper, given the outputs from all the encoder\nlayers, {h1:ğ‘ğ¸\n1:ğ‘š }, we take a weighted sum across all layers to obtain\nthe final image feature as:\nxğ‘– = ğ›¼ğ‘™\nğ‘ğ¸âˆ‘ï¸\nğ‘™=1\nhğ‘™\nğ‘– (11)\nwhere ğ›¼ğ‘™ are weights obtained using a softmax layer.\n3.5 Sequential Training with Inferred Labels\nTraining ReFormer involves three steps: (i) training the Faster R-\nCNN object detector on Visual Genome dataset; (ii) the trained\nFaster R-CNN is applied to train the encoder with Eq. 3 on Visual\nGenome dataset; (iii) when the encoder is well trained, it is further\ntrained with the joint objective of scene graph generation and image\ncaptioning following Eq. 12 on COCO dataset. As the relation labels\nare not available on COCO dataset, we infer the labels for objects in\nCOCO dataset using the encoder trained in step (ii).\nThe overall loss function in step (iii) is:\nL= Lğ‘ +ğœ†Lğ‘Ÿ (12)\nwith ğœ†being a hyper-parameter.\nOne possible variant of this training algorithm is to only use Lğ‘\nwithout Lğ‘Ÿ as the training loss, as done in the literature work. How-\never, our Ablation studies in Tab. 3 show that this variant cannot\nachieve as good results as those using the proposed training algo-\nrithm. The main purpose of incorporating Lğ‘Ÿ is to ensure that the\nrelational features learned do not vary much in step (iii) while being\nused to generate an accurate scene graph.\n4 EXPERIMENTS\n4.1 Datasets\nWe evaluate ReFormer on two large-scale publicly available datasets:\nMS COCO [23] which is an image captioning dataset and Visual\nGenome [20] which is a scene graph generation dataset.\nCOCO. The dataset is the most popular benchmark for image\ncaptioning, which contains 82,783 training images and 40,504 vali-\ndation images. There are 5 human annotated descriptions per image.\nAs the annotations of the official test set are not publicly available,\nwe follow the widely used splits provided by [18], where 5,000 im-\nages are used for validation,5,000 for testing and the rest for training.\nWe convert all the descriptions in the training set to lower case and\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Xuewen Y ang, Yingru Liu, and Xin Wang\ndiscard rare words which occur less than 5 times, resulting in the\nfinal vocabulary with 10,201 unique words in the COCO dataset.\nVisual Genome. The dataset is a large-scale image dataset for\nmodeling the relationships between objects, which contains 108K\nimages with densely annotated objects, attributes, and relations. To\npre-train the Faster R-CNN object detector, we take98K for training,\n5K for validation and 5K for testing. As part of images (about 50K)\nin the Visual Genome are also found in COCO, the split of the Visual\nGenome is carefully selected to avoid contamination of the COCO\nvalidation and test sets. We perform extensive cleaning and filtering\nof training data, and train Faster R-CNN over the selected 1,600\nobject classes. To pre-train the encoder of ReFormer on the scene\ngraph generation task, we adopt the same data split for training the\nobject detector. Moreover, we select the top-50 frequent predicates in\ntraining data. The semantic relation detection model is thus trained\nover the 50 relation classes plus a non-relation class.\n4.2 Methods & Metrics\nWe compare against three types of baselines. (i) The CNN-LSTM [14]\nbased models: Up-Down [3] which uses attention over regions of\ninterest, NBT [28] that first generates a sentence â€˜templateâ€™ and then\nfill in by visual concepts identified by object detectors, Att2all [32]\nthat uses self-critical sequence training for image captioning, and\nAoA [16] which uses attention on attention for encoding image\nregions and an LSTM language model; (ii) Transformer-based mod-\nels: M2-T [7] which uses a mesh-like connectivity to learn prior\nknowledge, Image-T [12], an image transformer, Object-T [13] that\nmodels the spatial relationship between objects, and ETA [21] which\nproposes the entangled attention mechanism; (iii) The GCN-LSTM\nbased models: VSUA [26] that uses GCNs to model the semantic\nand geometric interactions of the objects, GCN [51] which exploits\npairwise relationships between image regions through a GCN, and\nSGAE [47] which instead uses auto-encoding scene graphs.\nFor the caption generation evaluation, we follow the other base-\nlines and use the BLEU-1 and BLEU-4 [ 29], ROUGE [22], ME-\nTEOR [8], CIDEr [39] and SPICE [2] metrics.\nTo evaluate the Scene Graph Generation, we divide it into three\nsub-tasks [53]: (i) Predicate Classification (PredCls), to predict ğ‘Ÿğ‘–â†’ğ‘—\nwith (ğ‘ğ‘–,ğ‘œğ‘–)and (ğ‘ğ‘—,ğ‘œğ‘—)given; (ii) Scene Graph Classification (SG-\nCls), to predict the object labels ğ‘œğ‘– and ğ‘œğ‘— and the relationship ğ‘Ÿğ‘–â†’ğ‘—\nwith ğ‘ğ‘– and ğ‘ğ‘— given; (iii) Scene Graph Detection (SGDet), to di-\nrectly predict (ğ‘ğ‘–,ğ‘œğ‘–), (ğ‘ğ‘—,ğ‘œğ‘—)and ğ‘Ÿğ‘–â†’ğ‘— from an image without the\ngroundtruth bounding boxes or object labels.\nThe evaluation metrics we report are recall @ ğ‘¥, where ğ‘¥ =\n20,50,100. Recall @ ğ‘¥ computes the fraction of times the correct\nrelationship is predicted in the top ğ‘¥ confident relationship predic-\ntions. It was first proposed in [27] and then widely adopted in other\npapers [36, 36, 53]. We notice that mean average precision (mAP) is\nanother widely used metric. However, mAP is a pessimistic evalu-\nation metric because we can not exhaustively annotate all possible\nrelationships in an image. We thus do not report the results using\nthis metric.\n4.2.1 Implementation Details. To represent image regions, we\nuse Faster R-CNN with ResNet-101 finetuned on the Visual Genome\ndataset, thus obtaining a 2048-dimensional feature vector for each\nregion. To represent words, we use one-hot vectors and linearly\nMethod B-1 B-4 M R C S\nUp-Down [3] 79.8 36.3 27.7 56.9 120.1 21.4\nAtt2all [32] â€“ â€“ 34.2 26.7 55.7 114.0 â€“ â€“\nNBT [28] 75.5 34.7 27.1 54.7 107.2 20.1\nAoA [16] 80.2 38.9 29.2 58.8 129.8 22.4\nETA [21] 81.5 39.3 28.8 58.9 126.6 22.7\nObject-T [13] 80.5 38.6 28.7 58.4 128.3 22.6\nImage-T [12] 80.8 39.5 29.1 59.0 130.8 22.8\nM2-T [7] 80.8 39.1 29.2 58.6 131.2 22.6\nGCN [51] 80.5 38.2 28.5 58.3 127.6 22.0\nSGAE [47] 80.8 38.4 28.4 58.6 127.8 22.1\nVSUA [26] â€“ â€“ 38.4 28.5 58.4 128.6 22.0\nReFormer 82.3 39.8 29.7 59.8 131.9 23.0\nTable 1: Results on COCO dataset. We only report the single model\nresults on the â€˜Karpathyâ€™ test split. We highlight the best model in bold.\nproject them to the input dimensionality of 512. The dimension of\nthe encoded bounding box, object label and the final image feature\nare ğ‘‘ğ‘ = 100, ğ‘‘ğ‘™ = 300, and ğ‘‘â„ = 512 respectively. We set the\ndimension of each layer toğ‘‘ = 512 and the number of heads toâ„= 8.\nWe use the same number of encoders and decoders, thus having\nğ‘ğ¸ = ğ‘ğ· = 3. We employ dropout with a probability 0.9 after each\nattention and feed-forward layer. Training with the overall objective\nfunction (Eq. 12) is done following the learning rate scheduling\nstrategy of [38] with a warmup equal to 10K iterations. Then, during\nCIDEr optimization, we use a fixed learning rate of 5 Ã—10âˆ’6. We\ntrain all models using the Adam [19] optimizer. We use Glove [30]\nembedding to initialize word embedding layer. The total number of\nobjects in one image varies from 10 to 50, depending on the IOU\nthreshold that is set to 0.3. After some parameter tuning, we fix\nğœ†= 0.1 at which ReFormer provides the best CIDEr score.\n4.3 Evaluation\n4.3.1 General Caption Generation. We first evaluate our model\nwith the general caption generation metrics. We first compare the\nperformances of our ReFormer with those of several recent proposals\nfor image captioning on the COCO â€˜Karpathyâ€™ test split. As shown in\nTab. 1, in general, the Transformer-based models outperforms other\ntwo types of baselines: the models using pre-trained CNN to encode\nimage information and the LSTM decoder with attention to decode\na caption and those using GCN to encode scene graph information\nand the LSTM decoder to generate a sentence. This proves that\nTransformer can be used to better learn high-level image features\nand is capable of decoding sentences with a higher quality. The GCN-\nbased models outperform the CNN-based ones but perform worse\nthan the ones using the Transformer, which indicates that the scene\ngraph information is useful for better learning the image features but\nstill not well explored. Our proposed ReFormer outperforms all other\nmodels. For example, it provides an improvement of 1.5, 0.7, 0.5,\n1.2, 0.7 and 0.4 points over baseline model M2-T [7] on 6 metrics\nrespectively. This demonstrates the effectiveness of concurrently\nexploiting the Transformer structure and scene graphs in extracting\nthe relational image features.\nWe evaluate the model on the COCO online test server, composed\nof 40775 images for which annotations are not made publicly avail-\nable. The MSCOCO online testing results are listed in Tab. 2, our\nReFormer: The Relational Transformer for Image Captioning MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nGT:a male tennis player in white shirt is playing tennis.ğ‘´ğŸ-T:a man playing with a tennis racket.ReFormer:a player wearing white shirt isplaying tennis.playerhat\nshirt    hand\nwearingholdingwearing\nlogo\nhashas\nleg\nracketinholding\ncourt\nGT:a cheesy pizza sitting on top of a table.ğ‘´ğŸ-T:a pizza with cheese on it.ReFormer:a pizza with cheese and herb on a table.pizzaarm\nherb    cheese\nwith tablewith\nglassonon\nFigure 4: Examples of captions generated by ReFormer and theM2-T\nmodel, as well as the corresponding ground-truths. ReFormer generates\na scene graph to show the pair-wise relationships.\nReFormer outperforms previous transformer based model on several\nevaluation metrics.\n4.3.2 Ablation. We first do ablation studies on the number of\nTransformer layers. We start from the vanilla Transformer without\nusing any other techniques proposed in the paper. We vary the num-\nber of encoder and decoder layers from 2 to 6. As shown in Tab. 3,\nthe Transformer model with 3 layers achieves the best results. To\nevaluate the importance of keeping Lğ‘ in step (iii) of Section 3.5,\nwe conduct two ablation experiments. We first keep the parameters\nof the encoder fixed, which means Lğ‘Ÿ is not used to update the\nparameters of the encoder. We denote this variant as ReFormer âˆ—.\nWe find that the CIDEr score drops from 131.2 to 128.9. We then\nremove Lğ‘Ÿ and only use Lğ‘ to update the parameters of the encoder\nand the decoder. We denote this case as ReFormerâˆ’Lğ‘Ÿ. We find that\nthe CIDEr score drops as well. We can thus conclude that using Lğ‘Ÿ\nhelps to improve the quality of caption generation.\n4.3.3 Scene Graph Generation Evaluation.Generating scene\ngraphs is essential not only because it provides a way of explaining\nthe relationships between the objects in the image, but also the\nresource of the scene graphs when there is no other state-of-the-art\nscene graph generator available. To evaluate the proposed ReFormer\nin generating scene graphs, we compare it with other state-of-the-art\nscene graph generation methods. IMP [43] solves the scene graph\ninference problem using standard RNNs and learns to iteratively\nimproves its predictions via message passing. MOTIFS [ 53] is a\nstacked bi-directional LSTM architecture designed to capture higher\norder motifs in scene graphs. VCTree [36] is a dynamic tree structure\nthat places the objects in an image into a visual context which\nhelps to improve the scene graph generation task. The results are\nshown in Tab. 4. ReFormer achieves better results than all other\nthree baselines on all the three tasks: SGGen, SGCls and PredCls.\nThis demonstrates the capability of our ReFormer to exploit the\nrelationships between the objects in the image.\n4.3.4 Qualitative Evaluation. In Figure 4, we show the image,\ngroundtruth caption and the caption generated by M2 Transformer\nand ReFormer. Our model is able to not only generate a meaningful\nand more accurate caption than the baseline, but also generate a\nscene graph showing the relationships between the objects in the\nimage. With the graphs generated, captions become more expressive\nand explainable. Interestingly, the graphs usually contain useful\ninformation to describe the image. For instance, in the first example,\nthe scene graph generated by ReFormer tells us that the player\nis wearing a shirt and he is holding a racket. While in the second\nexample, the graph shows that there is a glass on the table and an arm\nfrom a person. These information cannot be inferred from only the\ncaption generated. Since the average number of words in a sentence\nin the COCO dataset is â‰ˆ10, it becomes very difficult for a captioner\nto describe details from the image using such short sentences. Thus,\na scene graph can be a good complementing component to the\ncaption.\n5 CONCLUSION\nExploring object relationships for image captioning is a challenging\ntask, because it not only requires a strong encoder-decoder model to\ngenerate accurate captions but also a scheme to embed the relational\ninformation in the encoder. To effectively improve the image caption\nquality, we propose the use of ReFormer to integrate the extraction\nof object relationship and caption generation into the same learning\nframework that the encoder can be more accurately trained. Our\nmethod achieves significant gains over COCO dataset compared to\nthe state-of-the-art models. However, there is still a room to improve\nthe quality of image captioning. For example, to get more accurate\nscene graphs, one might use the online crowd-sourcing tools like\nAmazon Mechanical Turk to manually annotate the COCO dataset\nwith relational labels.\n6 ACKNOWLEDGEMENTS\nThis work is supported in part by the National Science Foundation\nunder Grants NSF 2134840 and NIH R01EB032218. We thank the\nreviewers for valuable discussions and feedbacks.\nREFERENCES\n[1] Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the Behavior\nof Visual Question Answering Models. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing.\n[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016.\nSPICE: Semantic Propositional Image Caption Evaluation. In ECCV.\n[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,\nStephen Gould, and Lei Zhang. 2018. Bottom-Up and Top-Down Attention for\nImage Captioning and Visual Question Answering. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\n[4] J. Aneja, A. Deshpande, and A. G. Schwing. 2018. Convolutional Image Caption-\ning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition.\n[5] Ozan Caglayan, Pranava Madhyastha, Lucia Specia, and LoÃ¯c Barrault. 2019.\nProbing the Need for Visual Context in Multimodal Machine Translation. In Pro-\nceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics.\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta,\nPiotr DollÃ¡r, and C. Lawrence Zitnick. 2015. Microsoft COCO Captions: Data\nCollection and Evaluation Server. ArXiv abs/1504.00325 (2015).\n[7] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020.\nMeshed-Memory Transformer for Image Captioning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition.\n[8] Michael Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific\nTranslation Evaluation for Any Target Language. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Xuewen Y ang, Yingru Liu, and Xin Wang\nMethod B-1 B-4 M R C\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nUp-Down [3] 80.2 95.2 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nAtt2all [32] 78.1 93.7 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7\nAoA [16] 81.0 95.0 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nETA [21] 81.2 95.0 38.9 70.2 28.6 38.0 58.6 73.9 122.1 124.4\nImage-T [12] 81.2 95.4 39.6 71.5 29.1 38.4 59.2 74.5 127.4 129.6\nM2-T [7] 81.6 96.0 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nGCN [51] 80.8 95.9 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSGAE [47] â€“ â€“ â€“ â€“ 37.8 68.7 28.1 37.0 58.2 73.1 122.7 125.5\nVSUA [26] 79.9 94.7 37.4 68.3 28.2 37.1 57.9 72.8 123.1 125.5\nReFormer 82.0 96.7 40.1 73.2 29.8 39.5 59.9 75.2 129.9 132.8\nTable 2: Results on COCO dataset. We report the single model results on the COCO online test server. We highlight the best model in bold.\nMethod B-4 M R C S\nTrans.-2 35.7 27.4 56.4 121.3 20.5\nTrans.-3 36.5 27.8 57.0 123.6 21.1\nTrans.-4 36.3 27.6 56.5 121.5 20.8\nTrans.-5 36.1 27.5 56.8 121.9 20.7\nWeighted Trans. 37.3 28.4 57.5 125.4 21.7\nReFormer 39.8 29.7 59.8 131.2 23.0\nReFormer âˆ— 38.5 28.7 58.3 128.9 22.1\nReFormer âˆ’Lğ‘Ÿ 38.9 28.8 58.7 129.4 22.3\nReFormer âˆ’Weighted 39.3 29.2 58.9 130.3 22.5\nTable 3:Ablation study and comparison of ReFormer variants. Results\nare reported on the â€˜Karpathyâ€™ test split. âˆ—denotes that we fix the en-\ncoder during the caption training. We highlight the best model in bold.\nMethod SGGen SGCls PredCls\nR@20 R@50 R@100 R@20 R@50 R@100 R@20 R@50 R@100\nIMP 14.6 20.7 24.5 31.7 34.6 35.4 52.7 59.3 61.3\nMOTIFS 21.4 27.2 30.3 32.9 35.8 36.5 58.5 65.2 67.1\nVCTree 22.0 27.9 31.3 35.2 38.1 38.8 60.1 66.4 68.1\nReFormer 25.4 33.0 37.2 36.6 40.1 41.1 60.5 66.7 68.1\nTable 4: Scene graph generation evaluation results on Visual Genome\ndataset. We highlight the best model in bold.\n[9] jacob devlin, saurabh gupta, ross girshick, margaret mitchell, and lawrence c\nzitnick. 2015. Exploring Nearest Neighbor Approaches for Image Captioning.\nCoRR (2015).\n[10] Z. Feng, Q. Zhou, J. Zhang, P. Jiang, and X. Yang. 2015. A Target Guided Subband\nFilter for Acoustic Event Detection in Noisy Environments Using Wavelet Packets.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing23 (2015),\n361â€“372.\n[11] Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2019. Making the V in VQA Matter: Elevating the Role\nof Image Understanding in Visual Question Answering. Int. J. Comput. Vision\n(2019).\n[12] Sen He, Wentong Liao, Hamed Rezazadegan Tavakoli, Michael Ying Yang, Bodo\nRosenhahn, and Nicolas Pugeault. 2020. Image Captioning through Image Trans-\nformer. In Proceedings of the European Conference on Computer Vision (ECCV).\n[13] Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. 2019. Image\nCaptioning: Transforming Objects into Words. In Advances in Neural Information\nProcessing Systems 32.\n[14] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation (1997).\n[15] Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image\nDescription as a Ranking Task: Data, Models and Evaluation Metrics. Journal of\nArtificial Intelligence Research (2013).\n[16] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. 2019. Attention on\nAttention for Image Captioning. In International Conference on Computer Vision.\n[17] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. DenseCap: Fully Convolu-\ntional Localization Networks for Dense Captioning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition.\n[18] Andrej Karpathy and Li Fei-Fei. 2017. Deep Visual-Semantic Alignments for\nGenerating Image Descriptions. IEEE Trans. Pattern Anal. Mach. Intell.(2017).\n[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic\nOptimization.\n[20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua\nKravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language\nand Vision Using Crowdsourced Dense Image Annotations. Int. J. Comput. Vision\n(2017).\n[21] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. 2019. Entangled Transformer for\nImage Captioning. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV).\n[22] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out.\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Com-\nmon Objects in Context. In ECCV 2014.\n[24] Yingru Liu, Yucheng Xing, Xuewen Yang, Xin Wang, Jing Shi, Di Jin, Zhaoyue\nChen, and Jacqueline Wu. 2021. Continuous-Time Stochastic Differential Net-\nworks for Irregular Time Series Modeling. In Neural Information Process-\ning, Teddy Mantoro, Minho Lee, Media Anugerah Ayu, Kok Wai Wong, and\nAchmad Nizar Hidayanto (Eds.). Springer International Publishing, Cham, 343â€“\n351.\n[25] Yingru Liu, Xuewen Yang, Dongliang Xie, Xin Wang, Li Shen, Haozhi Huang,\nand Niranjan Balasubramanian. 2019. Adaptive Activation Network and Func-\ntional Regularization for Efficient and Flexible Deep Multi-Task Learning. CoRR\nabs/1911.08065 (2019). arXiv:1911.08065 http://arxiv.org/abs/1911.08065\n[26] Guo Longteng, Liu Jing, Tang Jinhui, Li Jiangwei, Guo Wei, and Lu Hanqing.\n2019. Aligning Linguistic Words and Visual Semantic Units for Image Captioning.\nIn Proceedings of the ACM Multimedia.\n[27] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. 2016. Visual Rela-\ntionship Detection with Language Priors. In European Conference on Computer\nVision.\n[28] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2018. Neural Baby Talk.\n2018 IEEE Conference on Computer Vision and Pattern Recognition(2018).\n[29] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nMethod for Automatic Evaluation of Machine Translation. In Proceedings of the\n40th Annual Meeting of the Association for Computational Linguistics.\n[30] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. InProceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP).\n[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: To-\nwards Real-Time Object Detection with Region Proposal Networks. In Advances\nin Neural Information Processing Systems 28.\n[32] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaib-\nhava Goel. 2017. Self-Critical Sequence Training for Image Captioning. IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)(2017).\nReFormer: The Relational Transformer for Image Captioning MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\n[33] Fawaz Sammani and Luke Melas-Kyriazi. 2020. Show, Edit and Tell: A Frame-\nwork for Editing Image Captions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR).\n[34] Ravi Shekhar, Ece Takmaz, Raquel FernÃ¡ndez, and Raffaella Bernardi. 2019. Eval-\nuating the Representational Hub of Language and Vision Models. In Proceedings\nof the 13th International Conference on Computational Semantics.\n[35] Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. 2020.\nUnbiased Scene Graph Generation from Biased Training. In Conference on Com-\nputer Vision and Pattern Recognition.\n[36] Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. 2019.\nLearning to Compose Dynamic Tree Structures for Visual Contexts. InConference\non Computer Vision and Pattern Recognition.\n[37] D. Teney, L. Liu, and A. Van Den Hengel. 2017. Graph-Structured Representations\nfor Visual Question Answering. In 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\nNeed. In Advances in Neural Information Processing Systems 30.\n[39] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr:\nConsensus-based image description evaluation. In CVPR.\n[40] Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond J.\nMooney, Trevor Darrell, and Kate Saenko. 2017. Captioning Images with Diverse\nObjects. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (2017), 1170â€“1178.\n[41] Dalin Wang, Daniel Beck, and Trevor Cohn. 2019. On the Role of Scene Graphs\nin Image Captioning. In Proceedings of the Beyond Vision and LANguage: inTE-\ngrating Real-world kNowledge.\n[42] Zeyu Wang, Berthy Feng, Karthik Narasimhan, and Olga Russakovsky. 2020.\nTowards Unique and Informative Captioning of Images. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV).\n[43] Danfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei. 2017. Scene Graph Gen-\neration by Iterative Message Passing. In Computer Vision and Pattern Recognition\n(CVPR).\n[44] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan\nSalakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell:\nNeural Image Caption Generation with Visual Attention. In Proceedings of the\n32nd International Conference on Machine Learning.\n[45] Xuewen Yang, Svebor Karaman, Joel Tetreault, and Alejandro Jaimes. 2021.\nJournalistic Guidelines Aware News Image Captioning. InProceedings of the 2021\nConference on Empirical Methods in Natural Language Processing. Association\nfor Computational Linguistics, Online and Punta Cana, Dominican Republic,\n5162â€“5175. https://doi.org/10.18653/v1/2021.emnlp-main.419\n[46] Xuewen Yang, Yingru Liu, Dongliang Xie, Xin Wang, and Niranjan Balasubra-\nmanian. 2019. Latent Part-of-Speech Sequences for Neural Machine Translation.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong\nKong, China, 780â€“790. https://doi.org/10.18653/v1/D19-1072\n[47] X. Yang, K. Tang, H. Zhang, and J. Cai. 2019. Auto-Encoding Scene Graphs\nfor Image Captioning. In 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR).\n[48] Xuewen Yang, Dongliang Xie, and Xin Wang. 2020. Crossing-Domain Generative\nAdversarial Networks for Unsupervised Multi-Domain Image-to-Image Transla-\ntion. CoRR abs/2008.11882 (2020). arXiv:2008.11882 https://arxiv.org/abs/2008.\n11882\n[49] Xuewen Yang, Dongliang Xie, Xin Wang, Jiangbo Yuan, Wanying Ding, and\nPengyun Yan. 2020. Learning Tuple Compatibility for Conditional OutfitRecom-\nmendation. CoRR abs/2008.08189 (2020). arXiv:2008.08189 https://arxiv.org/\nabs/2008.08189\n[50] X. Yang, H. Zhang, D. Jin, Yingru Liu, Chi-Hao Wu, Jianchao Tan, Dongliang\nXie, Jue Wang, and Xin Wang. 2020. Fashion Captioning: Towards Generating\nAccurate Descriptions with Semantic Rewards. In Proceedings of the European\nConference on Computer Vision (ECCV).\n[51] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring Visual Rela-\ntionship for Image Captioning. In Proceedings of the European Conference on\nComputer Vision (ECCV).\n[52] Xuwang Yin and Vicente Ordonez. 2017. Obj2Text: Generating Visually Descrip-\ntive Language from Object Layouts. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing.\n[53] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. 2018. Neural\nMotifs: Scene Graph Parsing with Global Context. In Conference on Computer\nVision and Pattern Recognition.\n[54] Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and Yin Li. 2020. Compre-\nhensive Image Captioning via Scene Graph Decomposition. In ECCV.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9694184064865112
    },
    {
      "name": "Computer science",
      "score": 0.7552355527877808
    },
    {
      "name": "Transformer",
      "score": 0.6974912285804749
    },
    {
      "name": "Scene graph",
      "score": 0.5666453242301941
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5356829166412354
    },
    {
      "name": "Encoder",
      "score": 0.5310224890708923
    },
    {
      "name": "Graph",
      "score": 0.48896491527557373
    },
    {
      "name": "Relation (database)",
      "score": 0.4480415880680084
    },
    {
      "name": "Sentence",
      "score": 0.44205451011657715
    },
    {
      "name": "Decoding methods",
      "score": 0.4311748147010803
    },
    {
      "name": "Image (mathematics)",
      "score": 0.38780829310417175
    },
    {
      "name": "Computer vision",
      "score": 0.3613828122615814
    },
    {
      "name": "Natural language processing",
      "score": 0.3287245035171509
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2491987943649292
    },
    {
      "name": "Data mining",
      "score": 0.19969013333320618
    },
    {
      "name": "Algorithm",
      "score": 0.18268966674804688
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.15551772713661194
    },
    {
      "name": "Voltage",
      "score": 0.08816462755203247
    },
    {
      "name": "Engineering",
      "score": 0.07258099317550659
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    }
  ],
  "cited_by": 8
}