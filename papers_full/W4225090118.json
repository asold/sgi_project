{
    "title": "Testing the Ability of Language Models to Interpret Figurative Language",
    "url": "https://openalex.org/W4225090118",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2977616260",
            "name": "Emmy Liu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2224729464",
            "name": "Chenxuan Cui",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2292292114",
            "name": "Kenneth Zheng",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3212496002",
        "https://openalex.org/W2071479885",
        "https://openalex.org/W3045533700",
        "https://openalex.org/W582755466",
        "https://openalex.org/W1964761760",
        "https://openalex.org/W1497756321",
        "https://openalex.org/W3173281684",
        "https://openalex.org/W2805668658",
        "https://openalex.org/W2806685047",
        "https://openalex.org/W2515860461",
        "https://openalex.org/W3100307207",
        "https://openalex.org/W4221154698",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W4253159630",
        "https://openalex.org/W2126530744",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W2806556772",
        "https://openalex.org/W1525668021",
        "https://openalex.org/W2806273110",
        "https://openalex.org/W3202282051",
        "https://openalex.org/W2798651446",
        "https://openalex.org/W174355806",
        "https://openalex.org/W2507978491",
        "https://openalex.org/W2059179731",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W2806806808",
        "https://openalex.org/W1912242145",
        "https://openalex.org/W2963272610",
        "https://openalex.org/W4298323777",
        "https://openalex.org/W2803972085",
        "https://openalex.org/W1843033958",
        "https://openalex.org/W2889468083",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2953320089",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3180295840",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W2070702913",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3211499990",
        "https://openalex.org/W3171213613",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3198711991",
        "https://openalex.org/W2521924953"
    ],
    "abstract": "Emmy Liu, Chenxuan Cui, Kenneth Zheng, Graham Neubig. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4437 - 4452\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nTesting the Ability of Language Models\nto Interpret Figurative Language\nEmmy Liu, Chenxuan Cui, Kenneth Zheng, Graham Neubig\nLanguage Technologies Institute\nCarnegie Mellon University\n{mengyan3,cxcui,kzheng2,gneubig}@cs.cmu.edu\nAbstract\nFigurative and metaphorical language are com-\nmonplace in discourse, and ﬁgurative expres-\nsions play an important role in communica-\ntion and cognition. However, ﬁgurative lan-\nguage has been a relatively under-studied area\nin NLP, and it remains an open question to\nwhat extent modern language models can inter-\npret nonliteral phrases. To address this ques-\ntion, we introduce Fig-QA, a Winograd-style\nnonliteral language understanding task consist-\ning of correctly interpreting paired ﬁgurative\nphrases with divergent meanings. We evalu-\nate the performance of several state-of-the-art\nlanguage models on this task, and ﬁnd that al-\nthough language models achieve performance\nsigniﬁcantly over chance, they still fall short\nof human performance, particularly in zero- or\nfew-shot settings. This suggests that further\nwork is needed to improve the nonliteral rea-\nsoning capabilities of language models.1\n1 Introduction\nAll our words are but crumbs that fall down from\nthe feast of the mind(Gibran, 1926). When humans\nread such a metaphorical phrase, how do they inter-\npret it? Conceptual metaphors structure our every-\nday language and are used to map everyday phys-\nical experiences and emotions onto abstract con-\ncepts (Lakoff and Johnson, 1981). They allow us\nto communicate complex ideas, to emphasize emo-\ntions, and to make humorous statements (Fussell\nand Moss, 2008). However, despite relating words\nin a way that differs from their accepted deﬁni-\ntion, these phrases are readily interpreted by human\nlisteners, and are common in discourse (Shutova,\n2011), occurring on average every three sentences\n(Mio and Katz, 1996; Fussell and Moss, 2008)\nThe ability to interpret ﬁgurative language has\nbeen viewed as a bottleneck in natural language un-\n1Code and data are available at https://github.com/\nnightingal3/Fig-QA, and a leaderboard can be found at https:\n//explainaboard.inspiredco.ai/leaderboards?dataset=ﬁg_qa\nderstanding, but it has not been studied as widely as\nliteral language (Shutova, 2011; Tong et al., 2021).\nFigurative language often relies on shared common-\nsense or cultural knowledge, and in some cases may\nbe difﬁcult to solve using language statistics. This\npresents a challenge to language models (LMs), as\nstrong LMs trained only on text may not be able\nto make sense of the physical world, nor the social\nor cultural knowledge that language is grounded in\n(Bender and Koller, 2020; Bisk et al., 2020).\nMost previous work on ﬁgurative language fo-\ncuses on metaphor detection, where a model is\ntrained to identify the existence of metaphors in\ntext (Tsvetkov et al., 2014; Stowe and Palmer,\n2018; Leong et al., 2020), with datasets consisting\nmostly of conventionalized metaphors and idioms\nin wide use. However, identifying these common\nmetaphors that already appear often in language\nmay be an easy task for LMs, and not fully test\ntheir ability to interpret ﬁgurative language. The\nlittle work that exists on metaphor interpretation\nframes it as a task linking metaphorical phrases\nto literal rewordings, either through paraphrase de-\ntection (Bizzoni and Lappin, 2018) or paraphrase\ngeneration (Shutova, 2010; Su et al., 2017; Mao\net al., 2018) (details in § 7) Another line of work\nprobes for metaphorical understanding in LMs, but\nthis is similar to the metaphor detection task, in that\nthe LM is not actually asked to choose an interpre-\ntation for the metaphor (Pedinotti et al., 2021; Ag-\nhazadeh et al., 2022). While interesting, this work\ndoes not take into account the fact that metaphors\nare rich with different implications that may vary\ndepending on the context.\nIn this work, we ask whether or not LMs can\ncorrectly make inferences regarding creative, rela-\ntively novel metaphors generated by humans. This\ntask is harder for two reasons: (1) inference is\nharder than identiﬁcation or paraphrasing, as it\nrequires understanding the underlying semantics,\nand (2) the metaphors in our dataset are novel cre-\n4437\nations, and many may not appear even once in the\nLMs’ training data. We propose a minimal task\ninspired by the Winograd schema (Levesque et al.,\n2012), where LMs are tasked with choosing the\nentailed phrase from two opposite metaphorical\nphrases. An example of a paired sentence is \"Her\ncommitment is as sturdy as (plywood/oak)\". The\ncorrect answer would be either \"She was (commit-\nted/uncommitted)\". This can also be seen as an\nentailment task, where input xis the premise, and\nthe output yis the hypothesis.2\nWe crowdsource a benchmark Fig-QA, consist-\ning of 10,256 such metaphors and implications\n(§ 2), which can be used to evaluate the nonlit-\neral reasoning abilities of LMs or for more broad\nstudies of ﬁgurative language in general (we pro-\nvide preliminary analyses in § 3). Through exten-\nsive experiments over strong pre-trained LMs (§ 4),\nwe ﬁnd that although they can be ﬁne-tuned to do\nreasonably well, their few-shot performance falls\nsigniﬁcantly short of human performance (§ 5). An\nin-depth analysis (§ 6) uncovers several insights:\n(1) LMs do not make use of the metaphorical con-\ntext well, instead relying on the predicted proba-\nbility of interpretations alone, (2) the task of asso-\nciating a metaphor with an interpretation is more\ndifﬁcult than the reverse, (3) even strong models\nsuch as GPT-3 make inexplicable errors that are\nnot well-aligned with human ones, indicating that\nfurther work is needed to properly model nonliteral\nlanguage.\n2 Dataset Creation and Validation\n2.1 Crowdsourcing Task\nWe crowdsourced data from workers on Amazon\nMechanical Turk ( details in Appendix A). Workers\nwere asked to generate paired metaphors with dif-\nferent meanings, as well as literal implications of\nthe two metaphors in context. We instructed work-\ners to try to generate rare or creative metaphors,\nnamely “metaphors that would not appear often in\ntext on the internet, books, social media, or news\nsites, but that can still be easily understood by peo-\nple.” Workers were given examples of valid pairs\nthat ﬁt the format, and examples of invalid ones\nto discourage errors. Some examples of generated\npairs are displayed in Table 1.\n2The opposing meanings help to avoid ambiguity in the\ncorrect answer, make the task intuitive for human annotators,\nand help prevent annotation artifacts that have plagued other\nNLI datasets (Gururangan et al., 2018).\nIn order to help workers, we employ therandom-\nness as genesis and narrow limits of change prin-\nciples of Cognitive Load Theory (Sweller, 2006).\nTo add soft constraints, we generate 3 different ran-\ndom words to be shown to each batch of workers.\nHowever, workers were not required to use these\nwords, as we wanted to encourage maximal diver-\nsity. In order to ensure that the random words were\nmetaphorically rich, we selected them based on\nmetaphorical frames in Lakoff and Johnson (1981).\n2.2 Data Validation\nThe dataset was manually validated by three au-\nthors of this paper. Each author covered roughly\none-third, evenly split between training, validation,\nand test. Examples were excluded if they (a) did\nnot make sense given the ﬁgurative expression, (b)\nhad grammar or spelling errors that rendered them\nunintelligible, or (c) did not follow the format of the\ntask. Examples of excluded samples are included\nin Appendix B. We collected 13,324 sentences and\ninterpretations from the crowdsourcing task, and\n10,256 sentences remained after ﬁltering.\n2.3 Final Dataset\nThe release version of our dataset contains the\nnamed data splits in Table 2. The medium train,\ndev, and test splits were generated from partitioning\nthe ﬁrst stage of data collected. The large train split\nadditionally contains all the new examples from the\nsecond collection stage, and the small train split is\na small random sample.\n3 Figurative Language Typologies\nIn this sample, we perform an analysis of the col-\nlected data to demonstrate its trends and categorize\nexamples for further error analysis. Speciﬁcally,\nwe examine (a) subjects, objects, and relations, and\n(b) types of common-sense knowledge needed to\ninterpret the metaphor.\n3.1 Figurative Language Structure\nWe note that most metaphors and similes can\nbe characterized by three components, (S,R,O ),\nwhere Sis a subject, Ris a relation, and Ois an ob-\nject. For instance, \"Her commitment is as sturdy as\nplywood\" can be written (Her commitment, sturdy,\nplywood). Interpretation involves inferring an at-\ntribute of the subject by extracting a relational at-\ntribute from the object (Fauconnier and Turner,\n2003). In a simile, Ris explicit, whereas it is usu-\nally implicit in a metaphor. The most common\n4438\nPaired sentences Possible answers\nThe pilot ﬂew like a ballet dancer The pilot ﬂew in a (restrained way| creative way)\nThe pilot ﬂew like a modern dancer The pilot ﬂew in a (restrained way |creative way)\nThe meteor was as bright as New York City The meteor was (very bright| not bright at all)\nThe meteor was as bright as coal The meteor was (very bright |not bright at all)\nThe atom is like a solar system Electrons (orbit the nucleus| are in probability densities)\nThe atom is like a cloud Electrons (orbit the nucleus |are in probability densities)\nHe hustles like rent was due three days ago He (hustles hardcore. | doesn’t hustle at all.)\nHe hustles like he’s a billionaire’s son. He (hustles hardcore |doesn’t hustle at all)\nLife is as easy as kindergarten for a high school senior Life is (basic| beyond comprehension)\nLife is as easy as kindergarten for a newborn Life is (basic |beyond comprehension)\nTable 1: Example sentences from the dataset\nTrain Dev TestS M L\n200 1,458 8,016 1,094 1,146\nTable 2: Examples in each data split\nFigure 1: Visualization of 25 most frequent subjects,\nrelations, and objects in the medium train set.\nsubjects, relations, and objects in the medium train\ndataset are shown in Figure 1. These were obtained\nby ﬁrst segmenting the phrases with syntactic pat-\nterns constructed from observation, followed by\nlemmatization and removal of punctuation and de-\nterminers \"the\", \"an\", \"a\" and \"that\". There are 441\nunique subjects, 646 unique relations, and 1,198\nunique objects in the medium training set.\n3.2 Common-sense Knowledge Types\nNext, we examined the test set to determine\nthe types of commonsense knowledge needed to\ninterpret metaphors. Through thematic analy-\nsis, we devised 4 categories based on common-\nsense knowledge, which are not mutually exclu-\nsive: common-sense object knowledge, visual\nmetaphors, common-sense social understanding,\nand cultural knowledge. The same 3 paper authors\nannotated the test set for these categories, with an-\nnotators responsible for separate categories.\nCommon-sense object knowledge consisted of\nmetaphors that made reference to properties of com-\nmon objects and animals, such as volume, height or\nmass of objects, or properties of materials. 68.35%\nof the test-set was found to require common-sense\nobject knowledge.\nVisual metaphors were a subset of common-\nsense object metaphors, primarily relying on the\nvisual modality, including attributes such as bright-\nness or colour. Some visual metaphors also\nsketched a vivid visual scene. These examples\ncomprised 14.73% of the test set.\nCommon-sense social understanding exam-\nples required knowing about how humans would re-\nact in different circumstances, or required knowing\nabout human emotions. These examples comprised\n27.55% of the test set.\nCultural metaphors required knowing cultural\ntraditions, works of art/artefacts, or religion. Due\nto crowdworkers being recruited from the US, these\nare centered around US culture. These examples\ncomprised 16.56% of the test set.\n4 Baseline Models and Evaluation\n4.1 Auto-regressive Language Models\nAuto-regressive LMs generate a probability distri-\nbution of the next token given all preceding tokens.\n4439\nType of knowledge required Paired sentences\nCommon-sense (objects) The new mattress is just as comfortable as sleeping on a (cloud/rocks outside)\nVisual The professor’s argument had the clarity of a (crystal glass/marine fog)\nCommon-sense (social) She is as embarrassed as a kid that (forgot homework/got an A)\nCultural The construction was as disastrous as the (1981 musical Cats/The 2019 ﬁlm based on the musical Cats)\nTable 3: Metaphor types based on types of knowledge required (not mutually exclusive)\nAs such, we can directly compute the probability\nof a sentence by multiplying the conditional proba-\nbility of each token at every time step.\n˜P(w1...wN ) =p(w1)\nN∏\ni=2\np(wi|w1...wi−1)\nThe ability to directly extract probabilities en-\nables the zero-shot reasoning of these LMs. For a\npair of metaphorical expressions x1 and x2 with\ntwo corresponding interpretations y1 and y2, we\nfeed in the concatenation of the metaphor and the\ninterpretation to the pretrained model without ﬁne-\ntuning. We deﬁne “forward” and “backward” prob-\nabilities assigned to interpretations and ﬁgurative\nlanguage expressions, respectively. For the for-\nward probability, for ﬁgurative phrase xi and cor-\nrect answer yi, we take\nP(yi|xi) = P(xi,yi)\nP(xi,yi) +P(xi,yj)\nsince there are only two answer options. From\nthis, we can calculate accuracy when we taking\nthe indicator of P(yi|xi) >0.5. Similarly for the\nbackward probability (predicting phrase based\non answer), we take\nP(xi|yi) = P(xi,yi)\nP(xi,yi) +P(xj,yi)\nwith analogous backward accuracy.3\nWe examine three state-of-the-art large\ntransformer-based LMs of this category: GPT-2\n(with 117M parameters, trained on 40GB of\ntext), GPT-neo (with 1.3B parameters, trained on\n800GB of text) and GPT-3 (4 variants between\n350M and 175B parameters, trained on 45TB\non text) (Radford et al., 2019; Black et al.,\n3In actuality, we use the length-normalized probability\nthat a model assigns to a sentence as a heuristic for the to-\ntal probability, to minimize the effect that the length of a\nsentence has on the decision (though this is not the prob-\nability of the sequence in a strict sense): P(w1...wN) =\nexp(− 1\nN log ˜P(w1...wN)). Initial experimentation showed\nmarginal differences in accuracy when using these two meth-\nods, so we used normalized probabilities by default.\n2021; Brown et al., 2020). We also examine the\nperformance of these models after ﬁnetuning on\nthe training data. GPT-2 and GPT-neo were trained\nwith a batch size of 8, with early stopping on the\nmedium dataset with a patience of 1 epoch, and\na minimal hyperparameter search was done with\nlearning rates 1e-5 to 5e-5. GPT-3 was trained with\nthe default parameters of the GPT-3 ﬁnetuning\nAPI.\n4.2 Masked Language Models\nWe also evaluate the performance of masked LMs\non this task. Unlike auto-regressive LMs, masked\nLMs cannot directly output the probability of a\nsentence, so it is not possible to directly test the\nzero-shot performance of these models. Instead,\nwe test the transfer performance by ﬁrst ﬁnetun-\ning them in two ways: on WinoGrande, which\nis also a binary choice task based on common-\nsense reasoning, and on several NLI datasets, in-\ncluding SNLI, MNLI, FEVER-NLI and ANLI\n(Nie et al., 2020; Sakaguchi et al., 2020). The\ninput to the model trained on WINOGRANDE\nis formatted as [CLS][metaphor][SEP]\n[answer1][SEP][answer2], and we use an\nextra linear layer on the [CLS] token embedding\nto perform the classiﬁcation. In addition to the\ntransfer performance, we also use contrastive ﬁne-\ntuning by feeding in each metaphor along with both\nanswer choices, and training the model with our\ndataset to classify which answer is correct. For the\nNLI model, we examine accuracy using all three\nlabels the model was originally trained with (entail-\nment, neutral, and contradiction), as well as using\na forced binary choice paradigm in which the log-\nits for the contradiction label are subtracted from\nthe logits for the entailment label, and the higher\n\"entailment score\" is the ending the model pre-\ndicts. We treat these two conditions as the analog\nof “zero-shot\" for these models.\nWe examine two masked LMs that are com-\nmonly used as baselines on many NLP tasks:\nBERT (Devlin et al., 2019), a transformer-based\nLM jointly trained on the masked LM and next sen-\n4440\ntence prediction objectives, and RoBERTa (Liu\net al., 2019), an improved variant of BERT which\nconsistently outperforms BERT across most tasks.\nWe use the large variant of both models (350M pa-\nrameters). BERT and RoBERTa were ﬁnetuned on\nthe medium dataset for 8 epochs with batch size\n8, following the setting in (Sakaguchi et al., 2020).\nA hyperparameter search was done with learning\nrates 5e-6 to 2e-5. Both BERT and RoBERTa were\nused for the Winogrande experiments, while only\nRoBERTa was used for the NLI experiment.\n4.3 Forced-choice Paradigm\nDue to the inherent creativity of metaphors, there\nmay be different interpretations of the same\nmetaphor. For instance, in Table 1, the exam-\nple \"he hustles like he’s a billionaire’s son\" could\nalso be interpreted in other ways, for instance \"he\nuses his father’s contacts and social privileges to\nmake money\". In a structural-mapping context, the\nforced choice between two answers constrains the\npossible meaning of the metaphor to be along one\naxis (Gentnder and Bowdle, 2008). In this case,\nit would be whether or not he is required to work\nhard.\nOf course, many of these metaphors have other\nvalid interpretations. In the \"billionaire’s son\" ex-\nample, another valid axis of interpretation could\nbe the manner in which he works. For instance,\nthe alternative pair could be \"he hustles like he’s\na (billionaire’s son | single mother working three\njobs)\" with answers \"he (uses his contacts and so-\ncial privileges to make money | works extremely\nlong hours with multiple ventures to make money)\".\nIt is possible that LMs could come up with other\nvalid interpretations that are not the ones originally\nintended, motivating us to also look at generation\nperformance in section § 5.2.\n4.4 Human Performance\nTo estimate the expected human performance on\nthis task, we ran a benchmark on the test set with 10\nhuman volunteers who were not authors of the pa-\nper. The human annotators were not shown any\ntraining examples, so this would be equivalent\nto the zero-shot setting for models. Participants\nranged from 20 to 29 years old, and there were 5\nmale and 5 female participants. 5 each were native-\nand non-native English speakers respectively. Par-\nticipants were mainly graduate student volunteers.\nWe shufﬂed the test set and split it into 10 par-\ntitions of ≈115 examples for each annotator. The\nModel Zero-shot Tuned (L) Tuned (XL)\nGPT-2 53.93 54.80 62.65\nGPT-neo 1.3B 56.89 69.98 72.00\nGPT-3 Ada 59.08 69.17 73.56\nGPT-3 Babbage 62.91 73.97 77.31\nGPT-3 Curie 65.35 79.04 81.94\nGPT-3 Davinci 68.41 - -\nBERT 58.14 83.16 85.69\nRoBERTa 66.184 89.22 90.32\nHuman 94.42 - -\nHuman (conﬁdent)95.39 - -\nTable 4: Zero-shot and ﬁnetuned test accuracies (%),\nﬁnetuned is averaged across 5 seeds. Dev set accura-\ncies can be found on the leaderboard under the \"valida-\ntion\" split.\nexamples were presented with pairs shufﬂed and\nseparated, in order to create a better comparison\nwith model performance.\nDue to differences in vocabulary or cultural back-\nground, we instructed participants to mark exam-\nples where they weren’t conﬁdent, such as those\nthat contained words or cultural references they\ndidn’t understand.\n5 Results\n5.1 Inference Results\nThe ﬁrst question is whether strong LMs can in-\nterpret metaphors at all when presented with\ntwo opposing meanings, in zero-shot or super-\nvised settings. These results are presented in Ta-\nble 4. The results for masked language models\nare higher than those for autoregressive language\nmodels, and ﬁne-tuning signiﬁcantly improves per-\nformance for all models.\nZero-shot Performance For the zero-shot set-\nting, we examine the test accuracy based on zero-\nshot forward probabilities for the GPT models, and\nthe pseudo \"zero-shot\" transfer performance for\nBERT and RoBERTa using models pretrained on\nthe WinoGrande task (Sakaguchi et al., 2020). As\nshown, the GPT-3 models outperform the GPT-2\nand GPT-neo models. Among the GPT-3 mod-\nels, there is a clear correlation between model size\nand performance, with the largest model (GPT-3\nDavinci) achieving the highest zero-shot test ac-\ncuracy. BERT and RoBERTa achieve accuracies\n4This is the accuracy score when transferring from Wino-\ngrande. Pretrained NLI results were 50.47 when using original\nlabels (entailment/contradiction/neutral), and 66.32 when forc-\ning a binary decision.\n4441\nwithin the range of GPT-3 models. While our mod-\nels mostly perform much better than chance in the\nzero-shot setting, there is still a large gap of 26 per-\ncentage points between our best model and human\nlevel performance.\nFine-tuned Performance For the ﬁne-tuned set-\nting, all listed models are ﬁne-tuned on the small\ndataset split. GPT models were trained with lan-\nguage modeling loss, whereas BERT and RoBERTa\nare trained with contrastive loss. We did not eval-\nuate ﬁne-tuning of GPT-3 Davinci due to bud-\nget. Overall, ﬁne-tuning improved accuracy sig-\nniﬁcantly for all models, with GPT-3 models uni-\nformly improving by about 13 percentage points,\nand BERT/RoBERTa improving by about 25 points.\nOur best model after ﬁne-tuning is RoBERTa,\nwhich reaches within 5% of our human perfor-\nmance.\nPrompting We also experiment with prompting\nmethods. Firstly, we use a simple sufﬁx prompt-\ning method, where we simply append the phrase\n\"that is to say\" between the metaphor and the in-\nterpretation, which we hypothesized may \"explain\"\nto the LM that the previous statement is ﬁgura-\ntive. We also evaluate the effectiveness of the ex-\namples method, by appending k random correct\nmetaphor/interpretation pairs before the actual pair\nwe are testing. The results of these experiments\ncan be seen in Figure 2. We found that the suf-\nﬁx method provided a small (1-2%) improvement\nover the baseline, while the example method was\ngenerally ineffective.\nBackward accuracies Note the accuracies re-\nported in this section are for the forward direc-\ntion, and the backward direction is reported in Ap-\npendix C. Backward accuracies are lower, with\nGPT-3 Curie for example having a 7% reduction\nin accuracy in the zero-shot case. This suggests\nthat selecting a metaphorical expression to match a\nliteral phrase is more challenging than the reverse\nfor LMs.\nPaired Evaluation Because our dataset is for-\nmatted as a Winograd schema, we can take ad-\nvantage of group scoring to evaluate models more\nstringently (Elazar et al., 2021). We found that\nperformance for autoregressive models plummeted\nunder this evaluation scheme, while masked lan-\nguage models also suffered in accuracy. The hu-\nman scores were least affected. Details are in Ap-\npendix D. This is most likely related to the phe-\nnomenon found in § 6.1.\nGPT -2 GPT -neo 1.3B GPT -3 Ada\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\nAccuracy\nZero-shot\nPrompting (k = 3)\nPrompting (k = 5)\nPrompting (k = 10)\nPrompting (suffix)\nFigure 2: Comparison of prompting methods with au-\ntoregressive models\n5.2 Generation Results\nNext, we examine if models can generate sensi-\nble interpretations for metaphors. Given the dif-\nﬁculty of evaluating text generation, compounded\nby the difﬁculty of ﬁgurative language, we opted\nfor manual evaluation of one tenth of the test\ndataset using generations of the strongest auto-\nregressive model: GPT-3 Davinci (≈175B parame-\nters).\nThe metaphor was given as input to the model,\nand 4 completions were generated for each\nmetaphor, with a maximum length of 100 tokens.\nCompletions were also truncated to the ﬁrst sen-\ntence, as initial experiments showed that contra-\ndictory statements (e.g. \"he was talented. But he\nwas not very talented\") were often generated across\nsubsequent sentences. Sufﬁx prompting was also\nused because of the lack of context, with \"That is\nto say, \" appended to each metaphor. Only the ﬁrst\nsentence of the output was evaluated. The tem-\nperature parameter was determined through grid\nsearch through values [0.2, 0.4, 0.6, 0.8, 1] on a\nsmall separate set of metaphors. A human annota-\ntor inspected the generated completions and found\nthat a temperature of 0.4 produced the most correct\nresults.\nThree paper authors labelled completions gener-\nated by GPT-3 Davinci as either correct, incorrect,\nor literal. In some cases, there were valid inter-\npretations that were not the same as the answer\ngiven by crowdworkers, which were also marked\ncorrect. If the model simply restated the metaphor\nwith no interpretation, the completion was marked\nas literal. Because some metaphors are ambiguous\nwhen presented without context, those examples\n4442\nwere not counted. The inter-rater reliability was\nmoderate due to differing standards for correctness\n(Krippendorff’s α = 0.5567). The majority vote\nwas taken between annotators’ judgments.\nGPT-3 Davinci’s accuracy, counting literalized\nmetaphors as incorrect, was 50.8%. Not count-\ning literalized metaphors, accuracy was 63.9%. In\n37.7% of cases, GPT-3 generated contradictory\ncompletions among the 4 completions. There was\nat least one correct completion for 78.1% of the\nmetaphors, but only 19.3% of metaphors had all\ncompletions correct. Examples of annotated gener-\nations can be found in Appendix G.\n6 Performance and Error Analysis\nWith these results in mind, we examinewhat kinds\nof errors models make, and what factors make\nthe task difﬁcult. . This is covered in § 6. We\nﬁnd that autoregressive models rely on the pre-\ndicted probability of each answer by itself to pre-\ndict the answer, and that this holds true for all\nmodels, before and after training. We ﬁnd that\nmodels have difﬁculty in interpreting \"sarcastic\"\nmetaphors, and sometimes inexplicably interpret\nvery simple metaphors wrong. We also examine\nerror typology according to the commonsense ty-\npology of § 3.2 and ﬁnd that models improve signif-\nicantly on object, visual and social commonsense\nwhen trained, but not on cultural commonsense.\n6.1 Reliance on Probability of Answers\nWe ﬁnd that models often rely solely on the pre-\ndicted probability of answers y1 and y2 to make\ntheir ﬁnal predictions, regardless of the context.\nThis led models to make the same prediction for\nthe paired sentences in many cases. Figure 3 and\nTable 5 show that this trend improves with ﬁne-\ntuning, and that GPT-3 is best able to disentangle\nthe probability ofyi and the probability ofP(yi|xi),\nbut all three models show a heavy tendency to pre-\ndict based on the relative probability of an answer\nalone.\nWe hypothesize that this may be one reason why\nBERT and RoBERTa achieve the best ﬁnetuned per-\nformance; they use a contrastive ﬁnetuning strategy\nproviding both the correct and incorrect options as\ninput to the model. On the other hand, the GPT\nmodels were ﬁnetuned with only the correct option,\nmaking the comparison unfair. One way to ﬁne-\ntune GPT models contrastively is to include both\noptions into a cleverly engineered prompt, but we\nModel r p\nUntrained\nGPT-2 0.8128 6.700 × 10−136\nGPT-neo 0.7891 6.075 × 10−123\nGPT-3 0.7392 4.329 × 10−100\nTrained\nGPT-2 0.6765 6.700 × 10−78\nGPT-neo 0.6689 1.456 × 10−75\nGPT-3 0.4157 2.598 × 10−25\nTable 5: Spearman r-values and p-values between\nP(yi|xi) and P(yi)\nleave this as a direction for future work.\nFigure 3: Models over-rely on probability of the answer\nto do their predictions. y-axis is probability of the ﬁrst\ninterpretation (answer) given metaphor while x-axis is\nlog odds of the ﬁrst interpretation.\n6.2 Other Factors Inﬂuencing Correctness\nWe also examined the inﬂuence of several other fac-\ntors on correctness. The point-biserial correlation\nbetween length of the context phrase and the bi-\nnary correctness value was -0.1544 with a p-value\nof 1.50 ×10−7, indicating that longer phrases are\nharder to interpret correctly. The point-biserial cor-\nrelation between answer probability and binary cor-\nrectness was 0.1840, with a p-value of3.50×10−10,\nindicating that examples where the answer was\nalready more probable were more likely to be an-\n4443\nswered correctly, in line with our ﬁndings that mod-\nels tended to predict the answer that was already\nmore plausible alone.\nFurthermore, we conducted an analysis on sub-\njects, objects, and relations as deﬁned in § 3.1. We\nexamined accuracy by part of speech patterns in\neach part of the metaphor, as well as by wordnet\nhypernyms present in each part of the metaphor.\nThis is detailed in Appendix E and Appendix F\n(Fellbaum, 1998). We used NLTK for POS tagging\n(Loper and Bird, 2002).\n6.3 Qualitative Analysis of Error Trends\nCommon Sense Knowledge We ﬁrst examine\nthe error tendencies by the type of common sense\nknowledge described in § 3.2. Table 6 summa-\nrizes accuracies for these types of commonsense\nquestions compared to humans.\nModel Obj Vis Soc Cul\nUntrained\nGPT-2 52.17 52.07 55.38 58.42\nGPT-neo 56.38 55.62 56.01 62.10\nGPT-3 Curie 75.00 71.00 72.47 78.42\nTrained\nGPT-2 53.57 51.48 57.91 57.37\nGPT-neo 70.15 72.78 68.67 70.00\nGPT-3 Curie 87.50 84.62 83.86 83.16\nBERT 87.37 92.31 84.18 77.37\nRoBERTa 91.20 94.08 89.56 83.68\nHuman 95.41 96.45 93.99 90.00\nTable 6: The performance of models across different\ncommonsense categories, in terms of accuracy on ex-\namples annotated with that category (%). The strongest\ncategory of each model is highlighted.\nWe ﬁnd that both humans and trained models\ntend to ﬁnd object commonsense and visual com-\nmonsense metaphors easier to interpret. We ﬁnd\nthat as models improve, most of the performance\ngain comes from the object, visual and social com-\nmonsense categories. Interestingly, the untrained\nmodels do quite well on cultural examples, but do\nnot improve much on the culture category when\ntrained. This makes sense, as the cultural examples\ntend to be quite disparate, so training would not\nhelp as much with other examples.\nSarcastic Metaphors For both humans and\nLMs, many of the errors are \"sarcastic\" metaphors,\nsuch as saying \"the girl was as bubbly as still water\"\nto mean \"the girl was bland\", rather than \"the girl\nwas vivacious\". These sentences can be difﬁcult if\nthe model or human focuses on simple word asso-\nciation (bubbly with vivacious) without reading the\nentire sentence to understand the sarcasm.\nInexplicable Errors We examined the errors\nmade by GPT-3 Curie (trained) and found that there\nwas little overlap with mistakes made by humans.\nOf the 64 human errors, 13 were also errors made\nby GPT-3. GPT-3 made many more \"obvious\" er-\nrors, such as predicting \"The ball is a big red sun\"\nto mean \"the ball is small\" rather than \"the ball is\nbig and red\" This is in contrast to the sentences in\nwhich humans made errors, which often contained\nrare vocabulary or unfamiliar cultural references.\n7 Related work\n7.1 Figurative Language Identiﬁcation\nMost existing work focuses on identifying ﬁgura-\ntive language at the word level. The VU Amster-\ndam Metaphor Corpus (VUA) is the largest avail-\nable corpus of metaphorical language, annotated by\nhumans (Steen et al., 2010). Two shared tasks on\nmetaphor identiﬁcation have been run (Leong et al.,\n2018, 2020). Both have utilized the VUA corpus,\nand the latter also introduced the TOEFL corpus,\nsampled from essays written by non-native English\nspeakers (Leong et al., 2020; Beigman Klebanov\net al., 2018). Most participants in the shared tasks\nused neural models, notably BERT, RoBERTa, and\nBi-LSTMs (Leong et al., 2020; Bizzoni and Gha-\nnimifard, 2018; Gao et al., 2018; Pramanick et al.,\n2018). These models are generally improved when\naugmented with semantic data, such as concrete-\nness, and multimodal information.\nAnother line of work focuses on probing mod-\nels to determine the extent of metaphor recogni-\ntion. For instance, BERT assigns higher pseudo-\nlog-likelihood scores to metaphors than nonsense\nexpressions, and its contextualized representations\nshow some signs of contextualizing the object do-\nmain (Pedinotti et al., 2021). Another study uses\nlinear probes trained on layers of BERT to predict\nwhether a word usage is literal or nonliteral, and\nﬁnds that this can be done effectively, especially\nusing middle layers as a representation (Aghazadeh\net al., 2022),\nDespite the utility of these tasks and datasets,\nthey have drawbacks. Most of the metaphor use is\nconventional, so this task does not capture novel\nmetaphors well. The word-level annotation also\ndoes not lend itself well to capturing extended con-\n4444\nceptual metaphors. Finally, metaphor interpretation\nmay be a more difﬁcult, although less studied, task.\n7.2 Figurative Language Interpretation\nRecent studies mostly focus on metaphor para-\nphrases, either through identiﬁcation (Bizzoni and\nLappin, 2018) or generation (Shutova, 2010; Su\net al., 2017; Mao et al., 2018). However, there has\nnot been as much work done on interpretation as\non detection, and framing metaphor interpretation\nas a paraphrase task may not capture the emergent\nmeaning of metaphors, such as the intended emo-\ntion, or the interaction of subject, relation and ob-\nject in the metaphor (Tong et al., 2021; Mohammad\net al., 2016).\nOther work has focused on interpreting ﬁgurative\nlanguage in narratives in context, based on plau-\nsible continuations of ﬁgurative language such as\nidioms and similes from stories (Chakrabarty et al.,\n2021a) or dialogues (Jhamtani et al., 2021). This\nrepresents a promising direction, and our work fo-\ncuses on expanding our understanding of LMs’ abil-\nity to interpret non-conventionalized metaphors.\n7.3 Other Figurative Language Datasets\nWe note that there are several other challenging\nNLI datasets available which contain ﬁgurative lan-\nguage, including the DNC corpus, and the RTE\ndataset (Poliak et al., 2018; Chakrabarty et al.,\n2021b). Other datasets, such as RiddleSense, ex-\nplicitly test models through difﬁcult commonsense\ninference, involving ﬁgurative language (Lin et al.,\n2021).\nOur work is distinguished by the Winograd\nschema format, as this format provides a better\nguard against the possibility that models have sim-\nply memorized common word associations that oc-\ncur in ﬁgurative language. Additionally, we specif-\nically instructed crowdworkers to be creative, and\nthis resulted in longer ﬁgurative phrases which re-\nquire more detailed commonsense knowledge. It is\nlikely that a fair number of these ﬁgurative phrases\nhave never appeared in any training corpus. How-\never, our ﬁgurative phrases also differ from riddles,\nas they are not supposed to be difﬁcult to reason\nabout, given that the source, relation and object are\nproperly contextualized.\n7.4 Human Language Processing\nHumans typically do not have any more difﬁculty\nprocessing metaphorical statements in context com-\npared to literal statements (Fussell and Moss, 2008;\nGlucksberg, 2003). This may be because certain\nwords serve as adual reference, which is to say they\nrefer simultaneously to a physical referent and an\nabstract superordinate category (Glucksberg, 2003).\nFor instance, \"shark\" may refer to literal sharks, as\nwell as anything that is considered vicious, leading\nto utterances such as \"that lawyer is a shark\".\nMetaphorical language processing has also been\nstudied in second-language learners, in the case of\nidioms. In most cases, the meaning of an unfamiliar\nidiom is inferred from the context or from word\nassociation (Cooper, 1999; Carston and Wearing,\n2011; Wolff and Gentner, 2000).\nAs LMs excel at word-association based tasks,\nthis is an encouraging ﬁnding. However, there is\nstill a gap between LM and human performance\neven in our task, in which one answer is obviously\nwrong when the input is correctly understood.\nWe take into account that these results are for\nconventionalized ﬁgurative language and that some\nof the more creative phrases in this dataset may take\na longer time to process for humans as well. This\nis especially true for non-native English speakers.\nHowever, the high human accuracy on this task\nwith half the participants being non-native English\nspeakers suggests that this was not a major barrier.\n8 Conclusion\nWe present a Winograd-like benchmark task to test\nthe ability of LMs to reason about ﬁgurative lan-\nguage, based on large-scale collection of creative\nmetaphors written by humans. We ﬁnd a large gap\nbetween LM zero-shot and human performance on\nthis dataset, but show that models can be ﬁne-tuned\nto perform well on this particular task.\nWe hope that this work will encourage further\nstudy of nonliteral reasoning in LMs, especially\nin few-shot settings. Given that metaphorical rea-\nsoning may play a role in problem-solving and\nlinguistic creativity, the development of models,\ntraining methods, or datasets that enable metaphor-\nical reasoning may improve models’ abilities to\nreason creatively and draw analogies between sit-\nuations that may appear to be different on the sur-\nface. One avenue we hope to investigate is multi-\nmodal metaphors, as this dataset currently includes\nonly text-based metaphors. Nonliteral expres-\nsions also remain understudied cross-linguistically,\nbut further work on identifying and interpreting\nmetaphors in other languages may also improve\nthe abilities of multilingual models.\n4445\n9 Ethical Considerations\n9.1 Potential Risks\nFigurative language has the potential to be used in\na harmful way, especially against minority and his-\ntorically disadvantaged groups. Such language is\noften emotionally charged or used to insult others,\nso we took care to remove any examples that were\npotentially offensive, especially toward protected\ngroups. We acknowledge that this was based on\nour own judgment, and generically insulting lan-\nguage (for instance, a metaphor that implies that\nsomeone is ugly) was not removed because it was\nnot insulting toward any particular individual.\nAll examples from Fig-QA are also in English,\nas it is the language that all authors speak, and\nthis was a preliminary dataset, being the ﬁrst of its\ntype that the authors have worked on. However,\nﬁgurative language is not just important in English,\nand we leave investigation of ﬁgurative language\nin other languages as future work.\n9.2 Terms of Use of Artefacts Used\nAdditional datasets we used were the Winogrande\ndataset, SNLI, MNLI, FEVER-NLI and ANLI.\nWinogrande is licensed under the Apache 2.0 li-\ncense, which allows modiﬁcation and distribution,\nﬁtting our use case. SNLI is licensed under a Cre-\native Commons Attribution ShareAlike 4.0 Interna-\ntional license, which allows us to share and adapt\nthe work as long as we give attribution. The ma-\njority of MNLI is licensed under OANC, which\nallows free use. The ﬁction section of this dataset\nconsists mostly of works in the public domain, but\nseveral stories are licensed: Seven Swords is avail-\nable under a Creative Commons Share-Alike 3.0\nUnported License, while Living History and Pass-\nword Incorrect are available under Creative Com-\nmons Attribution 3.0 Unported Licenses. These\nlicenses allow sharing and adaptation with attri-\nbution. FEVER-NLI is licensed under an MIT\nlicense, which also allows modiﬁcation, distribu-\ntion, and reuse. ANLI is licensed under Creative\nCommons Attribution-NonCommercial 4.0 Inter-\nnational, which also allows sharing and reuse as\nlong as we give attribution.\nModels used were GPT-2, GPT-neo, GPT-3,\nBERT and RoBERTa. GPT-2 and GPT-neo are\nlicensed under an MIT license, which does not\nplace any restrictions on its use. BERT is licensed\nunder an Apache License 2.0, which allows modiﬁ-\ncation and distribution. RoBERTa is licensed under\na GNU General Public License v2.0. This ﬁts our\nuse case as we are only running and studying the\nmodel. GPT-3 is licensed by Microsoft, and we\nused the public API to receive output.\n9.3 Computational Infrastructure and\nComputing Budget\nTo run our computational experiments, we had ac-\ncess to a compute cluster, but minimal compute is\nneeded to run the experiments in this paper. We\ngenerally did not use more than 2 GPUs at a time.\nThe only models that required GPU parallelism\nwere the GPT-neo models. An estimated 20 GPU\nhours are required.\nOur computing budget was roughly 100 USD.\nWe also used roughly 20 USD on credits for the\nGPT-3 API.\nAcknowledgements\nWe thank Pengfei Liu, Lyuyang Hu, and Chih-Hao\nWang for helping us set up the leaderboard for this\ndataset on Explainaboard. We also thank Pengfei\nLiu for helping run GPT-3, Danish Pruthi for guid-\nance on setting up the MTurk task, and all partic-\nipants who contributed to the human benchmark.\nLastly, we thank all the Turkers who contributed\nmetaphors to the dataset.\nThis work was supported in part by a CMU Pres-\nidential Fellowship and National Science Founda-\ntion Award No. 1761548.\nReferences\nEhsan Aghazadeh, Mohsen Fayyaz, and Yadollah\nYaghoobzadeh. 2022. Metaphors in pre-trained lan-\nguage models: Probing and generalization across\ndatasets and languages.\nBeata Beigman Klebanov, Chee Wee (Ben) Leong, and\nMichael Flor. 2018. A corpus of non-native written\nEnglish annotated for metaphor. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 86–91, New Orleans, Louisiana. Asso-\nciation for Computational Linguistics.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\n4446\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718–8735,\nOnline. Association for Computational Linguistics.\nYuri Bizzoni and Mehdi Ghanimifard. 2018. Bigrams\nand BiLSTMs two neural networks for sequential\nmetaphor detection. In Proceedings of the Workshop\non Figurative Language Processing, pages 91–101,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYuri Bizzoni and Shalom Lappin. 2018. Predicting hu-\nman metaphor paraphrase judgments with deep neu-\nral networks. In Proceedings of the Workshop on\nFigurative Language Processing, pages 45–55, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorﬂow.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nRobyn Carston and Catherine Wearing. 2011.\nMetaphor, hyperbole and simile: A pragmatic\napproach. Language and Cognition, 3(2):283–312.\nTuhin Chakrabarty, Yejin Choi, and Vered Shwartz.\n2021a. It’s not rocket science : Interpreting ﬁgura-\ntive language in narratives. ArXiv, abs/2109.00087.\nTuhin Chakrabarty, Debanjan Ghosh, Adam Poliak,\nand Smaranda Muresan. 2021b. Figurative language\nin recognizing textual entailment. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021 , pages 3354–3361, Online. Associa-\ntion for Computational Linguistics.\nThomas C. Cooper. 1999. Processing of idioms by l2\nlearners of english. TESOL Quarterly, 33:233–262.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nYanai Elazar, Hongming Zhang, Yoav Goldberg, and\nDan Roth. 2021. Back to square one: Artifact detec-\ntion, training and commonsense disentanglement in\nthe winograd schema.\nGilles Fauconnier and Mark Turner. 2003. Conceptual\nblending, form and meaning.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database. Bradford Books.\nSusan Fussell and Mallie Moss. 2008. Figurative lan-\nguage in emotional communication.\nGe Gao, Eunsol Choi, Yejin Choi, and Luke Zettle-\nmoyer. 2018. Neural metaphor detection in context.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n607–613, Brussels, Belgium. Association for Com-\nputational Linguistics.\nD Gentnder and B Bowdle. 2008. Metaphor as\nstructure-mapping. In The Cambridge handbook of\nmetaphor and thought , pages 109–128. Cambridge\nUniversity Press.\nKahlil Gibran. 1926. Sand and Foam; a book of apho-\nrisms. A.A Knopf.\nSam Glucksberg. 2003. The psycholinguistics of\nmetaphor. Trends in cognitive sciences, 7:92–96.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nHarsh Jhamtani, Varun Gangal, Eduard Hovy, and Tay-\nlor Berg-Kirkpatrick. 2021. Investigating robustness\nof dialog models to popular ﬁgurative language con-\nstructs. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7476–7485, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nG. Lakoff and M. Johnson. 1981. Metaphors we Live\nBy. University of Chicago Press.\nChee Wee (Ben) Leong, Beata Beigman Klebanov,\nChris Hamill, Egon Stemle, Rutuja Ubale, and Xi-\nanyang Chen. 2020. A report on the 2020 VUA\nand TOEFL metaphor detection shared task. In Pro-\nceedings of the Second Workshop on Figurative Lan-\nguage Processing, pages 18–29, Online. Association\nfor Computational Linguistics.\nChee Wee (Ben) Leong, Beata Beigman Klebanov, and\nEkaterina Shutova. 2018. A report on the 2018 VUA\nmetaphor detection shared task. In Proceedings of\nthe Workshop on Figurative Language Processing ,\npages 56–66, New Orleans, Louisiana. Association\nfor Computational Linguistics.\n4447\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nProceedings of the Thirteenth International Confer-\nence on Principles of Knowledge Representation\nand Reasoning, KR’12, page 552–561. AAAI Press.\nBill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee,\nand Xiang Ren. 2021. Riddlesense: Reasoning\nabout riddle questions featuring linguistic creativity\nand commonsense knowledge.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nEdward Loper and Steven Bird. 2002. Nltk: The natu-\nral language toolkit. CoRR, cs.CL/0205028.\nRui Mao, Chenghua Lin, and Frank Guerin. 2018.\nWord embedding and WordNet based metaphor iden-\ntiﬁcation and interpretation. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1222–1231, Melbourne, Australia. Association for\nComputational Linguistics.\nJ. S Mio and A. N Katz. 1996. Metaphor: Implications\nand Applications. Psychology Press.\nSaif Mohammad, Ekaterina Shutova, and Peter Tur-\nney. 2016. Metaphor as a medium for emotion: An\nempirical study. In Proceedings of the Fifth Joint\nConference on Lexical and Computational Seman-\ntics, pages 23–33, Berlin, Germany. Association for\nComputational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nPaolo Pedinotti, Eliana Di Palma, Ludovica Cerini,\nand Alessandro Lenci. 2021. A howling success or\na working sea? testing what BERT knows about\nmetaphors. In Proceedings of the Fourth Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , pages 192–204, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J. Ed-\nward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. 2018. Collecting diverse nat-\nural language inference problems for sentence rep-\nresentation evaluation. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 67–81, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMalay Pramanick, Ashim Gupta, and Pabitra Mi-\ntra. 2018. An LSTM-CRF based approach to\ntoken-level metaphor detection. In Proceedings of\nthe Workshop on Figurative Language Processing ,\npages 67–75, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In AAAI.\nEkaterina Shutova. 2010. Automatic metaphor inter-\npretation as a paraphrasing task. In Human Lan-\nguage Technologies: The 2010 Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics , pages 1029–1037,\nLos Angeles, California. Association for Computa-\ntional Linguistics.\nEkaterina Shutova. 2011. Computational approaches\nto ﬁgurative language.\nGerard J Steen, Alleta G Dorst, Berenike Herrmann,\nAnna A Kaal, Tina Krennmayr, and Trynetje Pasma.\n2010. A Method for Linguistic Metaphor Identiﬁca-\ntion: From MIP to MIPVU. John Benjamins.\nKevin Stowe and Martha Palmer. 2018. Leverag-\ning syntactic constructions for metaphor identiﬁca-\ntion. In Proceedings of the Workshop on Figurative\nLanguage Processing, pages 17–26, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nChang Su, Shuman Huang, and Yijiang Chen. 2017.\nAutomatic detection and interpretation of nominal\nmetaphor based on the theory of meaning. Neuro-\ncomputing, 219:300–311.\nJohn Sweller. 2006. Discussion of ’emerging topics in\ncognitive load research: Using learner and informa-\ntion characteristics in the design of powerful learn-\ning environments’. Applied Cognitive Psychology -\nAPPL COGNITIVE PSYCHOL, 20:353–357.\nXiaoyu Tong, Ekaterina Shutova, and Martha Lewis.\n2021. Recent advances in neural metaphor process-\ning: A linguistic, cognitive and social perspective.\nIn NAACL 2021.\nYulia Tsvetkov, Leonid Boytsov, Anatole Gershman,\nEric Nyberg, and Chris Dyer. 2014. Metaphor detec-\ntion with cross-lingual model transfer. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 248–258, Baltimore, Maryland. Associ-\nation for Computational Linguistics.\nP. Wolff and Dedre Gentner. 2000. Evidence for role-\nneutral initial processing of metaphors. Journal\nof experimental psychology. Learning, memory, and\ncognition, 26 2:529–41.\n4448\nA Crowdsourcing Details\nWe crowdsource metaphorical expressions and\ntheir interpretations through Amazon Mechanical\nTurk. Workers were recruited from the United\nStates and were limited to those who had a >98%\napproval rating on the platform, and who had also\ncompleted more than 1000 Human Intelligence\nTasks (HITs). Data collection was split into two\nstages: in the ﬁrst stage, 1458 train examples, and\nall the dev and test examples were collected. In the\nsecond stage, the remaining 6558 training examples\nwere collected. We identiﬁed some workers who\ncreated especially good examples in the ﬁrst stage,\nand recruited them back for more examples in the\nsecond stage. Workers were paid $0.33 for each\npair of sentences and were asked to generate 3 pairs\nat a time. An author of this paper wrote an initial\npilot set of sentences, and timed themselves while\nwriting some sentences. They found that each pair\ntook around 1 minute to write, though this varied\n(less creative examples took less time, while more\ncreative examples took more time). This extrap-\nolates to an hourly rate of 19.80 USD, which is\nabove the minimum wage in all US states, where\nworkers were located.\nOur HIT task was structured as follows: At the\ntop of the page, the workers are shown the follow-\ning instructions: \"Your task is to generate three\npairs of sentences with opposite or very differ-\nent meanings, both of which contain rare/creative\nmetaphors, which means metaphors that would not\nappear often in text on the internet, books, social\nmedia or news sites, but that can still be easily un-\nderstood by people. For each metaphor, you should\nalso provide a literal (non-metaphorical) sentence\nwith the same meaning.\" Then, we display one ex-\nample of a valid sentence pair. There is a button that\nopens a modal with more detailed instructions and\nsome more valid/invalid examples for reference.\nBelow that, we display three random words, which\nworkers are encouraged to use in their sentences\nif they get stuck. Finally, we display three sets of\n5 text ﬁelds for workers to ﬁll in: one for the start\nphrase, two for each metaphorical phrase, and two\nfor each literal interpretation. As the user types\nin each start phrase, we prepend a copy of their\nphrase before the corresponding metaphor ﬁelds in\nthe UI using some embedded JavaScript, which we\nfound helped reduce confusion and resulted in less\nimproperly formatted responses.\nWe launched many batches of these HITs until\nwe had collected the desired quantity of data. Then,\nwe converted the form responses into sentence pairs\nand validated each pair by hand before adding it to\nour dataset.\nB Invalid Examples\nFigurative language examples collected from\ncrowdworkers were excluded if they (a) did not\nmake sense given the meaning and the metaphori-\ncal expression, (b) had grammar or spelling errors\nthat rendered them unintelligible, or (c) did not\nfollow the format speciﬁed by the task template.\nExamples are given below:\n1. Do not make sense given the meaning and the\nmetaphorical expression\nPaired sentences Possible answers\nHe was resourceful like toilet paper He was very resourceful.He was resourceful like a mess He wasn’t resourceful at all\nThe night was as long as a spool of thread The night is longThe night was as long as a winding road The night dragged on\nthe concert of the lession is a main and a major we concert everyonethe concert of the lession features we concert our loved one\nTable 7: Examples that were rejected due to being non-\nsensical.\n2. Signiﬁcant grammar or spelling errors\nPaired sentences Possible answersfallten data are very much trusted fallten are nicefallten data are very valuable ﬂatten are safe\nCAR IS BIRD FEATHEAR CAR SITE IS ROUGHCAR IS COTTON CAR SITE IS HARD\nInﬂation is as natural as Minnesota rainfall in June Inﬂation is perfectly naturalInﬂation is as natural as Minnesota snowfall in June Patient is in a natural result of other things\nTable 8: Examples that were rejected due to having sig-\nniﬁcant spelling or grammar errors.\n3. Do not follow format\nPaired sentences Possible answers\nThis attack is as weak as a feather The attack is uselessThis attack is as weak as a breeze The attack doesn’t work\nMy car motor is dusty like old cave Car motor is very rustyMy car motor is dusty like abandon building car motor is very dusty\nthe writer is stuck between a rock And another hard place He is just stuck doesnt have a choicethe writer is stuck between a rock And a pebble The writer can get over the pebble\nTable 9: Examples that were rejected due to not follow-\ning the speciﬁed format.\nEfforts were made to ensure that the ﬁnal dataset\ncontains no offensive content or personally iden-\ntiﬁable information. WorkerID and other poten-\ntailly personally identifying information were not\nincluded.\n4449\nC Backward accuracies\nModel Zero-shot Fine-tuned (L)\nGPT-2 52.18 52.00\nGPT-neo 1.3B 54.36 63.44\nGPT-3 Curie 58.46 74.83\nTable 10: Zero-shot and ﬁnetuned backward auto-\nregressive model accuracies on the test set\nD Paired accuracies\nModel\nAccuracy\n(pairs correct)\nGPT-2 zero-shot 6.63\nGPT-2 ﬁnetuned 5.06\nGPT-neo zero-shot 10.3\nGPT-neo ﬁnetuned 10.3\nGPT-3 Curie zero-shot 17.4\nGPT-3 Curie ﬁnetuned 50.0\nBERT ﬁnetuned 70.6\nRoBERTa ﬁnetuned 80.4\nHuman 89.7\nTable 11: Accuracy for models on the test set, counted\nin terms of pairs of sentences in which both are correct\n(%). Results are from one run.\nE Accuracy breakdown by\nPart-of-Speech\nE.1 Subject\nPart of speech Accuracy Frequency\nNN 0.8569 538\nPRP 0.8526 156\nPRP$ NN 0.9 110\nNN NN 0.8889 63\nDT NN 0.8182 44\nNN NN NN 0.9375 32\nJJ NN 0.9167 12\nTable 12: Accuracy breakdown and frequency of parts\nof speech in metaphor subjects. Only part-of-speech\npatterns with greater than 10 occurrences are shown.\nE.2 Relation\nPart of speech Accuracy Frequency\nVBZ NN IN 0.8421 152\nVBD RB JJ IN 0.8904 146\nVBZ RB JJ IN 0.8889 99\nVBZ 0.8352 91\nVBD NN IN 0.8806 67\nVBD 0.9180 61\nVBN IN 0.9545 22\nNN IN 0.8636 22\nVBD JJ IN 0.9048 21\nNNS IN 0.8889 18\nVBD IN 0.8462 13\nVBZ IN 1.0 13\nVBD RB VBN IN 0.8182 11\nTable 13: Accuracy breakdown and frequency of parts\nof speech in metaphor relations. Only part-of-speech\npatterns with greater than 10 occurrences are shown.\nE.3 Object\nPart of speech Accuracy Frequency\nNN 0.8788 429\nNN NN 0.8992 129\nJJ NN 0.8352 91\nNN IN NN 0.8372 43\nJJ NN NN 0.8710 31\nNN NN NN 0.9130 23\nVBG NN 0.9545 22\nNN IN JJ NN 0.6154 13\nPRP$ NN 1.0 11\nJJ 0.6364 11\nNN IN NN NN 0.8182 11\nTable 14: Accuracy breakdown and frequency of parts\nof speech in metaphor objects. Only part-of-speech pat-\nterns with greater than 10 occurrences are shown.\nF Accuracy breakdown by hypernyms\nF.1 Subject\n4450\nSynset Accuracy Frequency\nadult.n.01 0.8736 182\nmale.n.02 0.8684 152\nwoman.n.01 0.7391 46\nfemale.n.02 0.9130 46\nshow.n.03 0.875 24\nproduct.n.02 0.8636 22\nmotor_vehicle.n.01 0.9048 21\nactivity.n.01 0.8421 19\nemotion.n.01 0.6667 18\npublication.n.01 0.8333 18\nfeline.n.01 0.9375 16\nbeing.n.01 0.7143 14\nperformer.n.01 0.8333 12\ncanine.n.02 12\nbody_covering.n.01 0.8333 12\nvessel.n.03 0.8333 12\nsound.n.01 1.0 12\ndomestic_animal.n.01 0.9167 12\nperson.n.01 0.8 10\nscheme.n.01 0.9 10\ncontestant.n.01 1.0 10\nTable 15: Accuracy breakdown and frequency of Word-\nNet hypernyms in metaphor subjects. Only hypernyms\nwith 10 or greater occurrences are shown.\nF.2 Object\nSynset Accuracy Frequency\ntime_period.n.01 0.85 20\nnatural_object.n.01 0.8947 19\nperson.n.01 0.8824 17\nlarge_integer.n.01 0.9286 14\nadult.n.01 1.0 14\nsolid.n.01 0.9167 13\nmale.n.02 1.0 13\nchild.n.02 0.8333 12\nbody_of_water.n.01 0.75 12\nbody_covering.n.01 0.8333 12\ndigit.n.01 0.9167 12\nregion.n.01 0.8182 11\nbeverage.n.01 0.8182 11\njuvenile.n.01 0.8182 11\ncontainer.n.01 0.9 10\nrodent.n.01 0.9 10\nfeline.n.01 0.7 10\nbuilding.n.01 0.8 10\ntime_unit.n.01 0.9 10\ntravel.v.01 0.7 10\nTable 16: Accuracy breakdown and frequency of Word-\nNet hypernyms in metaphor objects. Only hypernyms\nwith 10 or greater occurrences are shown.\nG Generation examples\nGeneration examples can be found in Table 17.\n4451\nStartphrase Completion Label\nHis temper was a marshmallow. That is to say, he was very easily angered, and once he was, he was very hard to cool down Incorrect\nHe’s got the swimming ability of a pack of dolphins. That is to say, he’s got none Incorrect\nThe villain is as beautiful as the Joker. That is to say, he’s not Correct\nThe child has the energy of a sloth. That is to say, he is lazy Correct\nThe girl moved as fast as a turtle. That is to say, she moved at a turtle’s pace Literal\nThe sadness of her death was a ﬂea.That is to say, It was a ﬂea that was a sadness Literal\nTable 17: Examples of completions generated by GPT-3 Davinci.\n4452"
}