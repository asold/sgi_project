{
  "title": "Leveraging Concept-Enhanced Pre-Training Model and Masked-Entity Language Model for Named Entity Disambiguation",
  "url": "https://openalex.org/W3024457643",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3025942920",
      "name": "Zizheng Ji",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109287883",
      "name": "Lin Dai",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2132525939",
      "name": "Jin Pang",
      "affiliations": [
        "State Grid Corporation of China (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2001754203",
      "name": "Tingting Shen",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3025942920",
      "name": "Zizheng Ji",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109287883",
      "name": "Lin Dai",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2132525939",
      "name": "Jin Pang",
      "affiliations": [
        "State Grid Corporation of China (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2001754203",
      "name": "Tingting Shen",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250333922",
    "https://openalex.org/W6692591614",
    "https://openalex.org/W1746680969",
    "https://openalex.org/W1990280147",
    "https://openalex.org/W2295227292",
    "https://openalex.org/W192510948",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W1988157164",
    "https://openalex.org/W2019484125",
    "https://openalex.org/W6632289321",
    "https://openalex.org/W165252003",
    "https://openalex.org/W2048110207",
    "https://openalex.org/W6682141183",
    "https://openalex.org/W2768645204",
    "https://openalex.org/W6600479677",
    "https://openalex.org/W2479758238",
    "https://openalex.org/W6755009524",
    "https://openalex.org/W6745910440",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2776054229",
    "https://openalex.org/W2963510636",
    "https://openalex.org/W2101210369",
    "https://openalex.org/W2507868973",
    "https://openalex.org/W2055518963",
    "https://openalex.org/W2436001372",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2026810221",
    "https://openalex.org/W2127289991",
    "https://openalex.org/W1784290353",
    "https://openalex.org/W2624743818",
    "https://openalex.org/W6712569459",
    "https://openalex.org/W6691288088",
    "https://openalex.org/W2505055145",
    "https://openalex.org/W2122865749",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2177768736",
    "https://openalex.org/W2138605095",
    "https://openalex.org/W2249777566",
    "https://openalex.org/W2605149786",
    "https://openalex.org/W2777205766",
    "https://openalex.org/W6629486361",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6761910064",
    "https://openalex.org/W2963855739",
    "https://openalex.org/W6755539384",
    "https://openalex.org/W2896675016",
    "https://openalex.org/W2612773933",
    "https://openalex.org/W2963691861",
    "https://openalex.org/W2791164948",
    "https://openalex.org/W2963863453",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6691016536",
    "https://openalex.org/W2980945086",
    "https://openalex.org/W6685530852",
    "https://openalex.org/W6718683173",
    "https://openalex.org/W3011170682",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2153225416",
    "https://openalex.org/W11298561",
    "https://openalex.org/W2260776682",
    "https://openalex.org/W2897059942",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2181629536",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2397058297",
    "https://openalex.org/W2151048449",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2250750514",
    "https://openalex.org/W1538632262",
    "https://openalex.org/W2963541420",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1491611863",
    "https://openalex.org/W2252137719",
    "https://openalex.org/W2980073313"
  ],
  "abstract": "Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in an input-text sequence to their correct references in a knowledge graph. We tackle NED problem by leveraging two novel objectives for pre-training framework, and propose a novel pre-training NED model. Especially, the proposed pre-training NED model consists of: (i) concept-enhanced pre-training, aiming at identifying valid lexical semantic relations with the concept semantic constraints derived from external resource Probase; and (ii) masked entity language model, aiming to train the contextualized embedding by predicting randomly masked entities based on words and non-masked entities in the given input-text. Therefore, the proposed pre-training NED model could merge the advantage of pre-training mechanism for generating contextualized embedding with the superiority of the lexical knowledge (e.g., concept knowledge emphasized here) for understanding language semantic. We conduct experiments on the CoNLL dataset and TAC dataset, and various datasets provided by GERBIL platform. The experimental results demonstrate that the proposed model achieves significantly higher performance than previous models.",
  "full_text": "Received April 7, 2020, accepted April 24, 2020, date of publication May 12, 2020, date of current version June 9, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.2994247\nLeveraging Concept-Enhanced Pre-Training\nModel and Masked-Entity Language Model for\nNamed Entity Disambiguation\nZIZHENG JI\n1, LIN DAI\n 1, JIN PANG\n 2, AND TINGTING SHEN\n1\n1School of Computer, Beijing Institute of Technology, Beijing 100081, China\n2State Grid Corporation of China, Beijing 100031, China\nCorresponding author: Zizheng Ji (jzz6891@163.com)\nABSTRACT Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity\nmentions in an input-text sequence to their correct references in a knowledge graph. We tackle NED problem\nby leveraging two novel objectives for pre-training framework, and propose a novel pre-training NED model.\nEspecially, the proposed pre-training NED model consists of: (i) concept-enhanced pre-training, aiming\nat identifying valid lexical semantic relations with the concept semantic constraints derived from external\nresource Probase; and (ii) masked entity language model, aiming to train the contextualized embedding\nby predicting randomly masked entities based on words and non-masked entities in the given input-text.\nTherefore, the proposed pre-training NED model could merge the advantage of pre-training mechanism for\ngenerating contextualized embedding with the superiority of the lexical knowledge (e.g., concept knowledge\nemphasized here) for understanding language semantic. We conduct experiments on the CoNLL dataset and\nTAC dataset, and various datasets provided by GERBIL platform. The experimental results demonstrate that\nthe proposed model achieves signiﬁcantly higher performance than previous models.\nINDEX TERMS Named entity disambiguation, pre-training, lexical knowledge.\nI. INTRODUCTION\nNamed Entity Disambiguation (NED) is important for vari-\nous Natural Language Processing (NLP) tasks such as ques-\ntion answering and dialog systems [1]–[3]. Although current\nneural network based approaches have advanced the state-of-\nthe-art results on NED task [4]–[8], they however failed to\nmodel the complex semantic relationships, and multiple sig-\nnals (i.e., words and entities etc.,) can not be fully interplayed\nin their architectures. On the other hand, language model\npre-training has been shown to be effective for improving\nmany NLP tasks [9]–[11], relying on its ability of repre-\nsenting complex context. Hence, this study tries to test the\neffectiveness of the pre-training contextualized embeddings\nfor NED task. In this paper, we describe a novel unsu-\npervised pre-training model for words and entities towards\nNED task.\nConventional unsupervised pre-training models have been\nshown to facilitate a wide range of downstream applica-\ntions, however still encode only the distributional knowledge,\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Wenbing Zhao\n.\nincorporated through masked language modeling objectives.\nTo enhance the representation ability of pre-training mecha-\nnism, this paper introduces extra lexical knowledge, which\nhas been proved to be effective in helping understanding\nsemantic in many NLP tasks [12]–[14] and could be com-\nbined with the distributional knowledge easily. Especially,\nwe couple BERT’s Masked Language Model (MLM) objec-\ntive [9] with a novel Concept Correlation Prediction (CCP)\nobjective available from lexical knowledge graph Probase\n[12], [15], through which we inject prior lexical knowledge\n(i.e., concept knowledge from Probase [12], [16]) into the\nbasic BERT. Wherein, CCP could be viewed as an addi-\ntional classifying task, which aims at identifying valid lexi-\ncal semantic relations with the concept semantic constraints\nderived from external resource Probase.\nFurthermore, we leverage another objective for the afore-\nmentioned concept-enhanced BERT, to release our ultimate\npre-training model towards NED task. Concretely, inspired\nby Masked Language Model (MLM) objective [9], we pro-\npose the Masked-Entity Language Model (MELM), which\nis a novel classifying task that aims to train the embed-\nding model by predicting randomly masked entities based\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 100469\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\non words and non-masked entities in the given input-text\nsequence.\nIn conclusion, the proposed pre-training model for NED\ntask, feeds two novel objectives, i.e., Concept Correlation\nPrediction objective and Masked-Entity Language Model\nobjective, to the traditional pre-training framework (i.e.,\nBERT [9] utilized in this paper). Therefore, the proposed\npre-training NED model could merge the advantage of pre-\ntraining mechanism for generating contextualized embedding\nwith the superiority of the lexical knowledge (e.g., concept\nknowledge emphasized here) for understanding language\nsemantic.\nFor evaluation, we train the proposed pre-training NED\nmodel using texts and their entity annotations retrieved from\nWikipedia and news articles. Finally, we test the proposed\nmodel using two standard NED datasets and the GERBIL\nplatform, which is a benchmark platform that includes many\nstate-of-the-art NED models’ results as well as quite a few\ndatasets. The experimental results reveal that our model out-\nperforms state-of-the-art models on both datasets. The con-\ntributions of our paper are concluded as follows:\n(i) We leverage extra lexical knowledge (i.e., concept\nknowledge emphasized in this paper) for supplementing\nunsupervised pre-training models, by coupling masked\nlanguage model objective with a novel concept corre-\nlation prediction (CCP) objective. To the best of our\nknowledge, it is the ﬁrst work for adapting the concept\nrelation semantic (e.g., comparative concept correlation)\ninto pre-training framework.\n(ii) A novel masked-entity language model (MELM) is\nintroduced here, aiming to train the contextualized\nembedding model by predicting randomly masked enti-\nties based on the words and non-masked entities in the\ngiven input-text.\n(iii) With efforts above, towards named entity disambigua-\ntion (NED) task, this paper proposes a novel unsuper-\nvised pre-training model for words and entities, combing\nthe advantages of both pre-training mechanism and prior\nlexical knowledge.\n(iv) Finally, the experimental results demonstrate that, not\nonly the overall proposed pre-training NED model but\nalso the separately concept-enhanced BERT, yields per-\nformance gains over current state-of-the-art models on\nNED task.\nII. RELATED WORK AND MOTIVATION\nA. UNSUPERVISED PRE-TRAINING APPROACHES\nLanguage model pre-training, such as ELmo [10], GPT/GPT2\n[11], [17] and BERT [9], has been shown to be effective\nfor improving many Natural Language Processing (NLP)\ntasks. ELMo [10] generalized traditional word embedding\nresearch along a different dimension, and proposed to extract\ncontext-sensitive features from a language model. GPT [17]\nenhanced the context-sensitive embedding by adapting the\nTransformer [18]. Like traditional static embedding models,\nunsupervised pre-training models also relied only on large\ntext corpora. E.g., the basic BERT model [9], which was\ndesigned to pre-training deep bidirectional representations\nfrom unlabeled text by jointly conditioning on both left and\nright context in all layers, consumed only the distributional\ninformation, ignoring the extra lexical knowledge. There has\nexisted research trying to introduce prior semantic knowledge\nto pre-training scheme. E.g., [19] introduced an entity-level\nmasking strategy to ensure that all of the words in the same\nentity were masked during word representation training,\ninstead of only one word or character being masked; [20]\nupdated contextual word representations via a form of word-\nto-entity attention, by inserting prior knowledge into a deep\nneural model. On the other hand, previous work has demon-\nstrated that, leveraging extra lexical knowledge (e.g., concept\netc.,) can signiﬁcantly boost the efﬁciency of contextualized\nembeddings for word [15], entity [21], relation [22], sentence\n[23], and so on. Overall, the lexicon-enhanced contextualized\nembedding representation produced by these models has\nproduced substantial gains in a number of downstream NLP\ntasks.\nSHORTAGE: Almost current pre-training models still\nencode only the distributional knowledge (provided by word\nsurface), ignoring the beneﬁcial lexical semantic (e.g., con-\ncept information) from extra high-quality knowledge source\nfor understanding language.\nMOTIVATION:Therefore, this paper aims at investigating\nwhether the supplement of extra lexical knowledge could\nalso produce substantial improvement for unsupervised pre-\ntraining models. The goal is to inform the pre-training model\non the relation of comparative concept correlation among\nwords: according to prior work on static word embeddings,\nthe sets of extra semantic information (i.e., concepts) are\nuseful to boost the model’s ability to capture true semantic\nrelation, which in turn have a positive effect on downstream\nlanguage understanding applications. Especially, we lever-\nage concept information for enhancing the conventional\npre-training model’s representation ability, and propose the\nconcept-enhanced pre-training, details in Section IV-B and\nexperimental comparison in Section V-D2.\nB. NAMED ENTITY DISAMBIGUATION\nDisambiguating among similar entities is a crucial feature\nin extracting relevant information from texts—Ambiguous\ninformation needs to be resolved, requiring additional steps\nthat go beyond grammatical parsing. Early Named Entity\nDisambiguation (NED) models addressed the problem as a\nwell-studied word sense disambiguation problem [24]–[26],\nprimarily aiming at modeling the similarity of textual context,\nor modeling the coherence among disambiguated entities in\nthe same context [27]–[29].\nMore recently, neural network based approaches have\nadvanced the state-of-the-art results on NED task, which\nfocused on learning the representations of entities or words.\nReference [30] used random walks (RW) on knowledge\ngraphs to construct vector representations of entities and\n100470 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\ndocuments to address the problem of NED. Reference [27]\nused an iterative heuristic to remove unpromising mention-\nentity edges. Reference [31] presented a graph-based disam-\nbiguation approach based on Personalized PageRank (PPR)\n[32] with combination of local and global evidence for disam-\nbiguation. Reference [5] combined beneﬁts of deep learning\nwith more traditional approaches such as graphical models\n[33] and probabilistic mention-entity maps. Reference [34]\nproposed a method to map entities into the word embedding\nspace [35] by using entity descriptions in the knowledge\ngraph and applied it for NED task. Reference [4] created a\nstate-of-the-art NED system that models entity-context sim-\nilarity with word embeddings and entity embeddings trained\nusing the Skip-Gram model [36]. Similarly, deep neural net-\nwork is introduced for computing representations of entities\nand contexts of mentions directly from the knowledge graph\nin [37], and modeling representations of entity-mentions,\ncontexts of entity-mentions, and entities in [38]. Reference\n[39] also leveraged deep neural networks to learn entity rep-\nresentations such that the consequent pairwise entity relat-\nedness was more suitable than of a standard method (i.e.,\nWikipedia Link-based Measure which was a method to mea-\nsure entity relatedness based on its link structure) for NED\ntask. Similarly, [40] utilized an extra Wikipedia Link-based\ndataset to improve the performance of NED. Furthermore,\nfor model coherence, in [39], hierarchical information in the\nknowledge graph was utilized to generate embedding and\napplied to model coherence; in [29], a novel coherence model\nwas introduced with an attention-like mechanism, wherein\nthe score for each entity candidate only depended on a small\nsubset of entity-mentions. Reference [6] proposed a compre-\nhensive approach for short-text entity recognition and linking,\nby using the concepts of entities as ﬁne-grained topics to\nexplicitly represent the context and model topic coherence.\nRecently, many research investigated the end-to-end strat-\negy for NED task, e.g., [7] exploited the effectiveness of\nboth knowledge-based and corpus-based semantic similarity\nmethods for NED, and [8] treated relations as latent variables\nand induced the relations without any supervision while opti-\nmizing the NED method in an end-to-end strategy.\nSHORTAGE: To be brief, previous neural network based\nmethods have recently achieved strong results on NED. Gen-\nerally, the key component of these models is an embedding\nmodel of words and entities trained using a large knowledge\nbase. These models are typically based on conventional word\nembedding models that assign a ﬁxed embedding to each\nword and entity. Although neural network based approaches\nhave advanced the state-of-the-art results on NED task, they\nhowever failed to model and represent complex relation-\nships and context, and multiple signals (i.e., words, enti-\nties and context etc.,) can not be fully interplayed in their\narchitectures.\nMOTIVATION: Hence, we introduce a novel concept-\nenhanced pre-training mechanism for NED task, because:\n(i) language model pre-training has been shown to be effec-\ntive for improving many NLP tasks, relying on its ability of\nmodeling and representing complex semantic relationships\nand context. Hence, in this study, we aim to test the effective-\nness of the pre-training contextualized embeddings for NED\ntask. (ii) prior work on concept-enhanced contextualized\nembedding (for word, entity and sentence etc.,) has shown\nthat, steering distributional models towards capturing robust\nsemantic representation has a positive impact on natural lan-\nguage understanding. Accordingly, unlike these NED meth-\nods, our proposed model involves a novel concept-enhanced\npre-training NED models for words and entities, including\na novel masked-entity language model (MELM) objective\n(details in Section IV-C and comparison in Section V-D1) and\na novel concept-enhanced pre-training mechanism (details in\nSection IV-Band experimental comparison in Section V-D2),\nand hence enables the accurate representation ability for NED\ntask. The proposed model based on pre-training could enable\nthe signals (i.e., the words, the entities and the concepts) to\nfully interplay to derive the NED results for given text.\nIII. NOTATION AND DEFINITION\nThis paper represents vectors with lowercase letters and\nmatrices with uppercase letters. Let x∈Rk be vector of length\nk, i.e., the embedding dimensionality is k. In this paper,\nthe notation ‘‘token’’ refers to word (denoted as w) or entity\n(denoted as e) occurring in the input-text sequence. V indi-\ncates the 30K-WordPiece [41] vocabulary for word, and\naccordingly |V |indicates the number of all the words in the\nvocabulary. In this paper, a ‘‘word’’ can refer to a linguistic\nword and a sub-word. Let E and R represent the set of entities\nand relations respectively, and accordingly |E|indicates the\nnumber of entities and |R|indicates the number of types of\nrelations. In this sense, E could be reviewed as the vocab-\nulary for entities in this paper, and actually E is equivalent\nto the entity set KB [29] described latter in Section V-C1.\nThe embedding matrix is denoted as M, and the projec-\ntion (weighted) matrix is denoted as W here. Therefore,\nthe matrices of the 30K-WordPiece word token embeddings\nand entity token embeddings are represented as MV ∈R|V |×k\nand ME ∈R|E|×k , respectively. A knowledge graph (KG)\nis denoted as G, consisting of a large number of ‘‘fact’’s.\nGenerally, the ‘‘fact’’ is formed as (h, r,t), and acts as the\nbasic unit of KG, wherein h ∈E is the head entity, r ∈R is\nthe relation, and t ∈E is the tail entity, meaning that there\nexists relation r between head entity h and tail entity t [42].\nGiven a knowledge graph G, it contains |E|entities and |R|\ntypes of relations.\nIV. METHODOLOGY\nIn this section, we will introduce the proposed pre-training\nmodel for NED task in details. First, we sketch the overall\ngeneral framework (Section IV-A). Then, we describe the\ndetails of the proposed pre-training NED model, which con-\nsists of: (i) concept-enhanced pre-training (Section IV-B),\naiming at identifying valid lexical semantic relations with the\nconcept semantic constraints derived from external resource\nProbase [12], [15]; and (ii) masked-entity language model\nVOLUME 8, 2020 100471\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\n(Section IV-C), aiming to train the contextualized embedding\nby predicting randomly masked entities based on the words\nand non-masked entities in the given input-text.\nIn conclusion, the proposed pre-training model for NED\ntask, feeds two novel objectives, i.e., Concept Correlation\nPrediction objective (Section IV-B) and Masked-Entity Lan-\nguage Model objective (Section IV-C), to the traditional pre-\ntraining framework (i.e., BERT [9] utilized in this paper).\nTherefore, the proposed pre-training NED model could\nmerge the advantage of pre-training mechanism for gen-\nerating contextualized embedding with the superiority of\nthe lexical knowledge (e.g., prior knowledge about concept\nemphasized here) for understanding language semantic.\nA. SKETCH OF OVERALL FRAMEWORK\nThis section details the proposed unsupervised pre-training\nmodel for NED task, mainly consisting of two additional\noptimization objectives: (i) Concept Correlation Prediction\nobjective (details in Section IV-B and shown as green letters\nin Figure 1); and (ii) Masked-Entity Language Model objec-\ntive (details in Section IV-C and shown as blue letters in Fig-\nure 1). Figure 1 sketches the architecture of the proposed\nmodel, which takes a sequence of words, constraints (derived\nfrom Probase directly) and entities contained in the input-\ntext, and then generates embedding representations for each\nword and entity. Following [18], a multi-layer bi-directional\ntransformer encoder is adopted here: Given a sequence of\ntokens (words/entities), the proposed model ﬁrst represents\neach token in the sequence as input-embedding (i.e., sum of\ntoken-self embedding (yellow rectangle in Figure 1), token-\ntype embedding (green rectangle in Figure 1) and token-\nposition embedding (gray rectangle in Figure 1)), and then\ngenerates a output-embedding for each token (blue rectangle\nin Figure 1). Following [9], the proposed model inserts the\nspecial indication token [CLS] at the beginning of the input-\ntext sequence as the ﬁrst token, and inserts [SEP] at the end\nof the input-text sequence as the separator token.\nThe reason why we choose Probase [12] as our extra lexical\nknowledge source in this paper, is discussed as follows. As\ndescribed above, this paper aims at investigating understand-\ning language by introducing lexical knowledge source, which\nconsists of concept information. DBpedia [43] etc., belongs\nto the encyclopedic knowledge graph, and both Probase and\nWordNet [44] belong to the lexical knowledge graph. Prior\nwok has demonstrated that, it is essential to utilize lexical\nknowledge graphs to help the machine to understand the\nworld facts and the semantics. That is, the knowledge of\nthe language should be used. The encyclopedic knowledge\ngraphs contain facts such as Barack Obama’s birthday and\nbirthplace, while the lexical knowledge graphs could def-\ninitely indicate that birthplace and birthday are properties\nof a person. Besides, the reason why we choose Probase\nrather than WordNet is that: Probase is widely used in\nrecent research about text understanding, text conceptualiza-\ntion, text representation, information retrieval, and knowl-\nedge graph completion [13], [14], [23], [45]. Probase uses an\nautomatic and iterative procedure to extract concept knowl-\nedge from 1.68 billion Web pages. It contains 2.36 millions of\nopen domain terms, and each term is a concept, an instance\n(respect to an entity-mention occurring in given text in this\nstudy), or both. Meanwhile, it provides around 14 millions\nrelationships with two kinds of important prior knowledge\nrelated to concepts: concept-attribute co-occurrence (isAttr-\nbuteOf) and concept-instance co-occurrence (isA). Moreover,\nProbase provides huge number of high-quality and robust\nconcepts without builds [12]. Therefore, lexical knowledge\ngraph Probase is utilized in this paper for leveraging lexical\nsemantics for boosting NED’s efﬁciency.\nB. CONCEPT-ENHANCED PRE-TRAINING MODEL\n1) BASIC PRE-TRAINING MODEL\nReference [9] proposes Bidirectional Encoder Representa-\ntions from Transformers (BERT), and adopts a Masked Lan-\nguage Model (MLM) pre-training objective, which randomly\nmasks some of the tokens from the input-text. This objective\nis to predict the original vocabulary ID of the masked word\nbased only on its context. Moreover, BERT learns to predict\nwhether two sentences are adjacent. This task tries to model\nthe relationship between two sentences, which is not captured\nby the traditional language models. There are two main steps\nin the BERT framework: (i) pre-training, wherein the model\nis trained on unlabeled data over different pre-training tasks;\nand (ii) ﬁne-tuning, wherein all of the parameters are ﬁne-\ntuned using labeled data from the downstream tasks. Con-\nsequently, this particular pre-training scheme helps BERT to\noutperform state-of-the-art techniques by a large margin on\nvarious key NLP datasets.\nOverall, BERT’s model architecture is a multi-layer\nbidirectional transformer encoder based on the original\nimplementation described in [18]. 1 The input-text is usually\ntokenized to word tokens by using the BERT’s sub-word\ntokenizer (such as 30K-WordPiece [41]). Especially, in order\nto train a deep bidirectional representation, BERT simply\nmasks some percentage of the input word tokens at random,\nand then predicts those masked tokens, then the ﬁnal hidden\nvectors corresponding to the masked tokens are fed into an\noutput softmax over the vocabulary V . This procedure is\ncalled ‘‘Masked Language Model (MLM)’’ in BERT. How-\never, we argue that current techniques restrict the power of\nthe pre-training representations. The major limitation is that\nstandard masked language models rely only on distributional\nknowledge, ignoring the beneﬁcial supplement of extra lexi-\ncal knowledge (e.g., concept) which has been proved to be\neffective in helping understanding semantic in many NLP\ntasks [12]–[14], [22].\n2) CONCEPT-ENHANCED PRE-TRAINING\nTo enhance the traditional BERT with extra lexical\nknowledge, we propose a novel pre-training model, i.e.,\n1The proposed pre-training NED model inherits the successful multi-layer\nbidirectional transformer encoder architecture utilized in basic BERT [9].\n100472 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nFIGURE 1. Architecture of the proposed pre-training NED model, which consists of concept-enhanced pre-training model and masked-entity\nlanguage model.\nConcept-Enhanced BERT, which leverages concept con-\nstraints for strengthening language understanding. The goal is\nto enhance the BERT model based on the concept constraints,\naccording to prior work on short-text conceptualization, text\nrepresentation learning, knowledge graph completion and\nnatural language understanding [15], [21], [23]. The sets of\nconcept constraints utilized here are based on comparative\nconcept correlation relation, and are useful to boost the\nbasic pre-training model’s ability to capture optimal semantic\ncorrelation and representation.\nConcretely, as mentioned above, basic BERT feeds input\ntoken representations to Masked Language Model (MLM)\nclassiﬁer, i.e., predicting the masked tokens. We introduce\nanother pre-training objective classiﬁer: it predicts whether\nan encoded word triple represents a valid lexical relation (i.e.,\ncomparative concept correlation relation: a positive triple\nexample consisting of three words wherein the ﬁrst word\nis more related to the second word than the third word on\nconcept-level) or not. Note that, other kinds of lexical relation\nconstraints (e.g., synonym, hypernym-hyponym and so on),\ncould be adapted here because of our model’s generality and\nﬂexibility.\nWe derive the aforementioned word triple as concept con-\nstraints from high-quality lexical knowledge graph Probase\n[12], with the following form: T = {(w1,w2,w3)}. Espe-\ncially, following the successful work on short-text under-\nstanding and disambiguation [14], [15], [23], this paper select\nabove-mentioned comparative concept correlation as our con-\nstraint for intensifying the original pre-training mechanism:\nGiven a triple t = (w1,w2,w3) ∈ T , word w1 shares\nmore concepts with word w2 than word w3, wherein instance\nconceptualization algorithm [16], [46] is adopted here to\ngenerate concepts for w1, w2 and w3 from Probase respec-\ntively. We then extract such lexicon-semantic relations to\nconstruct concept constraint set T . Concretely, we introduce\nsemantic similarity based on concept [21], for measuring\nthe comparative concept correlation, here: (i) Given word\nw1, we denote its concept-set as Cw1 , consisting of the cor-\nresponding concepts deriving from Probase by leveraging\ninstance conceptualization algorithm 2 [16], [46]. E.g., given\n2https://concept.research.microsoft.com/Home/API\nword ‘‘microsoft’’, the instance conceptualization algorithm\nreturns its corresponding concept distribution and differ-\nent scores , 3 and then generates its concept-set as follows:\nCmicrosoft = {COMPANY,VENDOR, CLIENT, FIRM,\nORGANIZATION,CORPORATION,BRAND,···}. Simi-\nlarly, the concept-set for other words (w 2 and w3) in this triple\ncould be constructed in the same way. (ii) The semantic simi-\nlarity between w1 and w2, to measure the distinction of word’s\nconcept semantics with the concept information, is deﬁned\nas sim(w1,w2) =\n|Cw1 ∪Cw2 |\n|Cw1 | , and similarly sim(w1,w3) =\n|Cw1 ∪Cw3 |\n|Cw1 | . (iii) Hence, for triple t = (w1,w2,w3) ∈ T ,\nsim(w1,w2) ≥ sim(w1,w3). Intuitively, each triple in the\naforementioned T could be viewed as a ‘‘comparative con-\ncept correlation’’ constraint.\nSince each constraint t =(w1,w2,w3) corresponds to a\ntrue lexical semantic relation, it could be viewed as a positive\ntraining example for the model, and hence its corresponding\nnegative examples is created in the following way: (i) We\nﬁrst group positive constraints from T in mini-batches Bp; (ii)\nFor each positive example t =(w1,w2,w3), we create two\nnegative instances ˜t1 =(w1,˜w2,w3) and ˜t2 =(w1,w2,˜w3)\nsuch that ˜w2 is the word sampled from batch Bp (other than\nw2) closest to w2 (i.e., with sim(w2,˜w2) below preset thresh-\nold), and similarly ˜w3 is the word (other than w3) closest to\nw3, respectively, in terms of the concept correlation of their\nconcepts in Probase.\nNext, given a text s = {w1,w2,··· ,wn}, we make the\npotential triple-wise combinations among all these tokens to\ngenerate triples. If the triple belongs to T (constructed above),\nwe select and retain it into the constraint-set Ts for current\ntext. Finally, we transform each constraint t =(w1,w2,w3) ∈\nTs into a ‘‘BERT-compatible’’ input format, as follows: We\ninsert the special beginning token [CLS] before w1, and then\ninsert the special end token [SEP] after tokens of each of\n{w1,w2,w3}, as shown in this example for the constraint t =\n(w1,w2,w3): [CLS] w1 [SEP] w2 [SEP] w3 [SEP]. Then\nwe transform each instance into a ‘‘BERT-compatible’’ for-\nmat, i.e., into a sequence of the widely-used 30K-WordPiece\n3For efﬁciency, we ignore a concept if its score is less than 0.001 for\ncomputational efﬁciency.\nVOLUME 8, 2020 100473\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\n[9], [41] tokens: We split w1, w2 and w3 into 30K-WordPiece\ntokens. For generating input representation of multi-layer\nbidirectional transformer, we also sum the {token-self embed-\nding, token-type embedding, token-position embedding} of\neach token, similar to [9], as shown in Figure 1. Especially,\n(i) we assign the token-type ID of 1 to all w1 tokens, and\n[CLS] token and [SEP] token before and after w1 tokens;\n(ii) we assign the token-type ID of 2 to all w2 tokens, and\nthe [SEP] token after it; and (iii) we assign the token-type\nID of 3 to all w3 tokens, and the [SEP] token after it,\nas shown in Figure 1. Take Figure 1 as an example, given text\n‘‘microsoft unveils ofﬁce for apple ipad’’, constraint triple\n(apple,microsoft,ipad) exists in current context and then we\ntransform this constraint into following BERT-compatible\ninput format—[CLS] apple [SEP] microsoft [SEP] ipad\n[SEP], as shown in middle part in Figure 1. Note that,\nthe other constraints in Ts are all processed with the same\nmanner above, and then sequentially added into the sequence.\nTherefore, the ellipsis illustrated in Figure 1 represents other\nconstraints with ‘‘BERT-compatible’’ format in this context.\nThen, we feed these prepared token input representa-\ntions into a novel objective, i.e., Concept Correlation Pre-\ndiction (abbreviated as CCP here), which is introduced as\nanother classiﬁer objective into the original BERT. Overall,\nfor triple t = (w1,w2,w3) ∈ Ts, let oCCP−CLS ∈ Rk\nbe the transformed vector representation of the concept-\nconstraint sequence beginning token [CLS] that encodes\nthe whole comparative concept correlation (w 1,w2,w3) (as\nshown in Figure 1). Then, our Concept Correlation Prediction\n(CCP) objective could also be deﬁned by the widely-used\nsoftmax strategy:\nˆyCCP =SoftMax(WCCP ·oCCP−CLS +bCCP) (1)\nwherein, WCCP ∈R2×k and bCCP ∈R2 indicate the weighted\nmatrix and output bias for Concept Correlation Prediction\nobjective, respectively, which are parameters to be trained.\nWith efforts above, the training loss function of the pro-\nposed concept-enhanced pre-training model, is the sum of the\nlog-likelihood of original Masked Language Model (MLM)\nobjective [9] and the log-likelihood of the proposed Concept\nCorrelation Prediction (in Eq. (1)) objective. Overall, we train\nall the parameters of the proposed model, by optimizing the\naforementioned training loss function.\nC. MASKED-ENTITY LANGUAGE MODEL\nAs discussed in the beginning section, we leverage another\nobjective for the aforementioned concept-enhanced pre-\ntraining (Section IV-B) —Masked-Entity Language Model\n(MELM) objective, which specialises in Named Entity\nDisambiguation (NED) task and could combine with\nthe concept-enhanced pre-training mentioned-above easily\n(details in the following Section IV-C2). This novel objec-\ntive will be described in details in this section. Note that,\nour Masked-Entity Language Model also inherits the multi-\nlayer bidirectional transformer encoder in both BERT and\nour concept-enhanced BERT mentioned above, which could\nbe viewed as a supplement for each of them, and that’s the\nreason why the proposed Masked-Entity Language Model\nimplements the seamless connection with each of them (as\nshown in Figure 2 (a)). Actually, the concept-enhanced BERT\n(Section IV-B) and this Masked-Entity Language Model,\nultimately constitute our powerful pre-training NED model,\nshown in green part in Figure 2 (a).\nThe input-text is also tokenized to words by using the\nsame 30K-WordPiece [41] vocabulary as BERT and other\nkinds of variant pre-training models (such as [19]). Given\na sequence consisting of words and entities, our Masked-\nEntity Language Model ﬁrst represents this sequence as a\nsequence of input embeddings (Section IV-C1), and then\ngenerates a contextualized output embedding for each token.\nNext, the model randomly masks some entities in the input-\ntext (shown in Figure 2 (b)), and then trains the embeddings\nand parameters to predict these masked entities based on the\nwords and other non-masked entities (Section IV-C2). Lastly,\nthe inference procedure towards NED task, is described in\nSection IV-C3.\n1) INPUT REPRESENTATION GENERATION\nAs shown in Figure 1, in the NED task we represent the\nwords and entities of input-text as a single packed sequence.\nTherefore, the proposed model generates the input embed-\nding representation for: (i) word, (ii) ‘‘comparative concept\ncorrelation’’ constraints (deﬁned in Section IV-B), (iii) entity-\nmention with only one word, and (iv) entity-mention with\nmultiple words, respectively.\nFirstly, following [9] and [19], for the word in i-th position\nin current input-text sequence, denoted as wi, its input rep-\nresentation is constructed by summing the following embed-\ndings:\n- Token-self embedding, which is the word embedding of\nthe corresponding word, denoted as wi ∈Rk (derived\nfrom 30K-WordPiece word token embedding matrix\nMV , i.e., wi ⊂MV );\n- Token-type embedding represents the type of token,\nnamely ‘‘word-type’’, and we assign token-type ID of\n0 to each token;\n- Token-position embedding, which represents the posi-\ntion of wi, denoted as pV\ni ∈Rk .\nSecondly, for each concept constraint triple (i.e., ‘‘compar-\native concept correlation’’ constraint described in Section IV-\nB), the word in i-th position in current sequence is denoted as\nwi, and its input representation is constructed by summing the\nfollowing embeddings:\n- Token-self embedding, which is also the 30K-\nWordPiece embedding of the corresponding word,\ndenoted as wi ∈Rk (apparently, wi ⊂MV );\n- Token-type embedding, which represents ‘‘concept-\nconstraint type’’, is set as discussed in Section IV-B2:\nwe assign the token-type ID of 1, 2 and 3 to each token\nin the concept constraint triple sequentially;\n100474 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\n- Token-position embedding, which represents the posi-\ntion of wi, denoted as pV\ni ∈Rk .\nThirdly, for the entity-mention in i-th position containing\nonly one word, denoted as ei, its input representation is\nconstructed by summing the following embeddings:\n- Token-self embedding, which is the entity embedding\nof the corresponding entity, denoted as ei ∈Rk (derived\nfrom entity token embedding matrix ME , i.e., ei ⊂ME );\n- Token-type embedding represents the type of token,\nnamely ‘‘entity-mention type’’, and we assign token-\ntype ID of 4 to each token;\n- Token-position embedding, which represents the posi-\ntion of ei, denoted as pE\ni ∈Rk , and apparently pE\ni =pV\ni .\nLastly, for the entity-mention containing multiple words\nwith span from i-th position to j-th position, denoted as ei, its\ninput representation is constructed by summing the following\nembeddings:\n- Token-self embedding, which is the entity embedding of\nthe corresponding entity, denoted as ei ∈Rk (apparently,\nei ⊂ME );\n- Token-type embedding represents the type of token,\nnamely ‘‘entity-mention type’’, and we assign token-\ntype ID of 4 to each token;\n- Token-position embedding, which represents the posi-\ntion of ei, denoted as pE\ni ∈Rk , and can be computed by\naveraging from i to j as\n∑j\nk=i pE\nk\nj−i+1 . Hence, all of the words\nin the same entity are considered together, like [19].\nIn conclusion, for the given word or entity in the input-text,\nits input representation for the proposed pre-training NED\nmodel is constructed by summing the following embeddings\n[9], [19]: (i) Token-self embedding, which is the embed-\nding of the corresponding word or entity (yellow rectangle\nin Figure 1); (ii) Token-type embedding, which represents\nthe type of token (green rectangle in Figure 1) ; and (iii)\nToken-position embedding, which represents the position of\nthe given token in the sequence (gray rectangle in Figure 1).\n2) TRAINING PROCEDURE\nThis section describes the details about the procedure of\nthe proposed Masked-Entity Language Model for training\nthe embeddings of words and entities. The proposed model\nrandomly masks some entities in the input-text sequence\n(shown in Figure 2 (b)), and then trains the embeddings to\npredict these masked entities based on the words and other\nnon-masked entities, similar to the strategy used in [15], [35],\n[36] and [9], wherein some word(s) are masked (shown and\ncompared with the proposed Masked-Entity Language Model\nin Figure 2 (b)). Note that, the entities be masked are known as\n‘‘truth-entities’’ in this paper, and represented by the special\nindication token [MASK] .4\nEspecially, let oMELM−ME represent the entity output\nembedding representation of the marked entity. Hence,\n4[MASK] token doesn’t appear during the ﬁne-tuning stage.\nthe mask vector mMELM ∈Rk could be deﬁned as follows:\nmMELM =τ[η(WMELM ·oMELM−ME +bMELM )] (2)\nwherein, WMELM ∈ Rk×k denotes to the weight matrix\nand bMELM ∈ Rk is the bias vector, for the pro-\nposed Masked-Entity Language Model objective. Appar-\nently, {WMELM ,bMELM }is part of parameters to be trained.\nBesides, η(·) represents the activation function, wherein\nGELUs (Gaussian Error Linear Units) activation function\n[47] could be adopted here, and τ(·) indicates the layer nor-\nmalization function.\nHence, we could predict the ‘‘truth-entity’’ of this masked\nentity by leveraging the softmax function over all entities in\nE deﬁned in the knowledge graph G, in the following way as\nsimilar to the format of Eq. (1):\nˆyMELM−TRAIN =SoftMax(ME ·mMELM +bo) (3)\nwherein, ME ∈ R|E|×k indicates the matrix of the entity\ntoken embeddings (as deﬁned in Section III), and bo ∈R|E|\nindicates the output bias. Similarly, {ME ,bo}in Eq. (3) are\nparameters to be trained. Apparently, ˆyMELM−TRAIN ∈R|E|.\nOverall, the training loss function of the our overall per-\ntraining NED model, is the sum of (i) the log-likelihood\nof original Masked Language Model (MLM) objective [9];\n(ii) the log-likelihood of the proposed Concept Correlation\nPrediction (CCP, in Eq. (1) in Section IV-B) objective; and\n(iii) the log-likelihood of the Masked-Entity Language Model\n(MELM, in Eq. (3)) objective. The relation among these three\nobjectives are sketched in Figure 2 (a), illustrated in gray\nrectangle (MLM), green rectangle (CCP) and blue rectangle\n(MELM) respectively. Overall, we trained all the parameters\nof the proposed per-training NED model, by optimizing the\naforementioned training loss function.\n3) INFERENCE PROCEDURE FOR NAMED ENTITY\nDISAMBIGUATION\nThis section sketches the procedure of Named Entity Disam-\nbiguation (NED) based on the aforementioned pre-training\nNED model. For each entity-mention u with its Top-N entity-\ncandidates occurring in the given input-text sequence, 5 the\nproposed model creates an input sequence consisting of:\n(i) a masked entity corresponding to this entity-mention u,\n(ii) words occurring in the given input-text, and (iii) entity\ncandidate for each of other entity-mentions derived randomly\nfrom their corresponding Top-N entity candidates.\nThen the proposed model takes the input-text sequence\nmentioned above, and generates the mask vector mu ∈Rk\ncorresponding to this entity-mention u according to Eq. (2).\nThen, the proposed model predicts the optimal entity for this\nentity-mention u, by leveraging the softmax function (i.e.,\nEq. (3)) over its N entity candidates:\nˆyMELM−INFERENCE =SoftMax(M′\nE ·mu +b′\no) (4)\n5Parameter N indicates the number of entity candidates, and the various\nchoices for the parameter N will be discussed in the following experimental\nsection.\nVOLUME 8, 2020 100475\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nwherein, M′\nE ∈ RN×k is the entity embedding matrix for\nu’s Top-N candidate entities, and apparently M′\nE ⊂ME (as\ndeﬁned in Section III). From another perspective, M′\nE could\nbe viewed as classiﬁcation layer weights where N is the\nnumber of labels. Similarly, b′\no ∈RN indicates the output\nbias corresponding to these N entity candidates, and b′\no is\nthe subset of bo in Eq. (3). Apparently, ˆyMELM−INFERENCE ∈\nRN , and each dimensionality of ˆyMELM−INFERENCE denotes\nto the score of the corresponding candidate entity among u’s\nTop-N candidate entities. Note that, [19] introduced entity-\nlevel masking strategy to ensure that all of the words in the\nsame entity are masked during word representation training,\ninstead of only one word or character being masked. There\nexist essential differences between our model and [19]: (i) the\nentity-level masking in [19] is still token-level while ours\nare in reality entity-level; (ii) entity-level masking in [19]\ncould be reviewed as a constraint and it inherits the objective\nof BERT, while the masked entity model in ours is a novel\nobjective (as shown in Figure 2). The detailed discussion will\nbe provided in the following experimental section.\nV. EXPERIMENTS\nWe test the proposed model using two standard NED datasets\nand benchmark GERBIL platform. Firstly, datasets and com-\nparative models are listed in Section V-A and Section V-B.\nThen we describe the experimental settings in Section V-C,\nwhich is widely used in domain of NED, and the experimental\nresults in Section V-D. Finally, performance on GERBIL plat-\nform comparing with 8 baselines on 10 datasets, is discussed\nin Section V-E.\nA. DATASETS\nTo evaluate efﬁciency of the proposed pre-training NED\nmodel, we conduct experiments on the dataset CoNLL [27]\nand dataset TAC [48], which are commonly-used by [4], [5],\n[8], [29], [49].\n(i) CoNLL [27] is a dataset based on Reuters, created by\nhand-annotating the CoNLL 2003 Named Entity Recog-\nnition task dataset with YAGO [50] entities. It contains\n1,393 documents split into train set (946 documents),\ntest-a set (216 documents) and test-b set (231 docu-\nments), and 34K mentions.\n(ii) TAC [48] is also news-based NED dataset constructed\nfor the Text Analysis Conference (TAC). It is based\non news articles from various agencies and Web log-\ndata, and consists of a training and a test set containing\n1,043 and 1,013 documents, respectively.\nIn addition, we use several data sources to train our model:\n(i) We use a Wikipedia snapshot, and hypothesize that\nretrieving from a large and high-ﬁdelity corpus will\nprovide cleaner language. Therefore, we construct a\nWikipedia dataset (denoted as Wiki dataset) for train-\ning the proposed model. We preprocess the Wikipedia\narticles with the following rules. First, we remove the\narticles less than 100 words, as well as the articles less\nthan 10 links. Then we remove all the category pages and\ndisambiguation pages. Moreover, we move the content\nto the right redirection pages. Finally we obtain about\n3.74 billion Wikipedia articles and 21.3 million entity\nannotations for indexing and training.\n(ii) We consider training our model by performing a retrieval\non large auxiliary corpora, and the news documents are\nalso used here. Therefore, we construct a news dataset\n(denoted as News dataset) for training the proposed pre-\ntraining NED model. The news articles are extracted\nfrom a large news corpus, which contains approximately\n2.6 billion articles and 13.5 million entity annotations\nsearched from Reuters, New York Time and so on.\nFor all datasets, this paper utilizes the standard\nentity candidate set, i.e., KB+YAGO (details in following\nSection V-C1), with their associated prior probabilities\n( ˆP(entity|entity-mention)) following [5], and only Top-\n30 entity candidates based on ˆP(entity|entity-mention) are\nconsidered (i.e., N =30 in Section IV-C3). For each kind of\ndata source, we generate input-embeddings (Section IV-C1)\nby splitting the content of each article into sequences consist-\ning of less than 256 words and their entity annotations. There-\nfore, we trained the proposed model by utilizing the texts and\ntheir corresponding entity annotations retrieved from Wiki\ndataset and News dataset. Note that, only mentions referring\nto valid entities in Wiki dataset and News dataset can be\nconsidered.\nB. BASELINES\nFor evaluating the NED task, we compare the proposed model\nwith the following comparative models:\n(Hoffart et al., 2011): [27] is a graph-based model aiming\nat ﬁnding a dense sub-graph of entities in an input-text to\naddress the problem of NED.\n(Cai et al., 2013): [37] utilizes deep neural-networks to\nderive the representations of entities and entity-mention con-\ntexts, and then applies them to NED.\n(Chisholm et al., 2015): [40] utilizes an extra Wiki-links\ndataset to improve the performance of NED.\n(Pershina et al., 2015): [31] presents a graph-based disam-\nbiguation approach based on Personalized PageRank (PPR)\n[32] that combines local and global evidence for disambigua-\ntion.\n(Globerson et al., 2016): [29] proposes a novel coherence\nmodel with an attention-like mechanism, where the score for\neach candidate only depends on a small subset of entity-\nmentions.\n(Yamada et al., 2016): [4] creates a state-of-the-art\nNED system that models entity-context similarity with word\nembeddings and entity embeddings trained using the Skip-\nGram model [36].\n(Chen et al., 2018): [6] proposes a comprehensive\napproach for short-text entity recognition and linking,\nby using the concepts of entities as ﬁne-grained topics to\nexplicitly represent the context and model topic coherence.\n100476 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nFIGURE 2. Differences between basic BERT and the proposed pre-training NED model (i.e., NED+BERT and NED+CeBERT).\n(Ganea et al., 2017): [5] combines beneﬁts of deep learn-\ning with more traditional approaches such as graphical mod-\nels [33] and probabilistic mention-entity maps.\n(Zhu et al., 2018): [7] exploits the effectiveness of\nboth knowledge-based and corpus-based semantic similarity\nmethods for NED. Besides, a joint-learning framework for\nword and category embedding is proposed in this work.\n(Le et al., 2018): [8] treats relations as latent variables, and\ninduces the relations without any supervision while optimiz-\ning the NED method in an end-to-end strategy.\nC. EXPERIMENT SETTINGS\n1) KB AND ALIAS-ENTITY MAPPING\nAs discussed above, for all datasets, this paper utilizes the\nstandard entity candidate set, denoted as KB+YAGO, which\nis widely used by NED research [5], [29]. This standard\nentity candidate set KB+YAGO consists of two components:\n(i) an entity set KB, and (ii) a mapping scheme YAGO.\nFollowing previous work [29], the referent KB is derived\nfrom the Wikipedia subset of Freebase [51]. We also collect\nalias counts from Wikipedia page titles (including redirects\nand disambiguation pages), Freebase aliases, and Wikipedia\nanchor text. Besides, we optionally use the mapping from\naliases to candidate entities released by [27], obtained by\nextending the ‘‘means’’ relations of YAGO [52]. Every anno-\ntated entity-mention could be mapped to at least one entity,\nand the set of entities included the ‘‘gold’’ entity. However,\nchanges in canonical Wikipedia URLs, accented characters\nand unicode usually result in mention losses over time, as not\nall URLs can be mapped to the KB [53].\nTable 1 summaries the statistics of the alias-entity map-\npings on the CoNLL test-b dataset and TAC dataset (restricted\nto non-NIL mentions), respectively, for the YAGO+KB alias-\nentity mapping, reported in [29]. Wherein, ‘‘Mention-Recall’’\nindicates the percentage of entity-mentions with at least\none known entity, ‘‘Gold-Recall’’ represents the percentage\nof entity-mentions where the gold-entity is included in the\nentity-candidates, and ‘‘Unique’’ aliases map to exactly one\nentity. Besides, ‘‘Avg.’’ is the number of entity-candidates\naveraged over entity-mentions.\n2) ENTITY CANDIDATE SELECTION\nWe make use of a mention-entity prior ˆP(entity|entity-\nmention) [5] for entity candidate selection (Section IV-C3),\nwhich could be viewed as the context-independent probabil-\nity of selecting entity conditioned only on entity-mention.\nThis probability is computed by averaging probabilities from\ntwo indexes build from mention entity hyperlink statistics\nfrom Wikipedia and a large Web corpus [54], plus the\nYAGO index of [27] (with uniform prior) [5], [8]. As dis-\ncussed before, for all datasets, this paper utilizes KB+YAGO\n(Section V-C1), with their associated prior probabilities\n( ˆP(entity|entity-mention)) following [5], to generate entity\ncandidates used in Section IV-C3, and only Top-30 entity can-\ndidates based on ˆP(entity|entity-mention) are considered (i.e.,\nN =30). Note that, similar to previous work, we refrain from\nannotating mentions without any entity-candidate, implying\nthat precision and recall can be different.\n3) PARAMETER SETTINGS\nFor dataset CoNLL, we also use the development set (i.e.,\nits test-a set) for the parameter tuning described above.\nFollowing [27], we only use 27,816 entity-mentions with\nvalid entries in the KB+YAGO and report the standard\nmicro- (aggregates over all entity-mentions) accuracy of the\ntop-ranked candidate entities to assess disambiguation per-\nformance. For dataset TAC, following [37], [40], this experi-\nmental section utilizes entity-mentions only with a valid entry\nin the KB+YAGO, and reports the micro-accuracy score of the\ntop-ranked candidate entities. Consequently, we evaluate our\nmodel on 1,020 entity-mentions contained in the test set.\nFor the proposed model, the traditional model conﬁgura-\ntion in the BERT model [9] is adopted here, although many\nvariants exist: (i) the vector dimension k is 1024, and the\nVOLUME 8, 2020 100477\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nTABLE 1. Statistics of alias-entity mapping onCoNLL dataset andTAC dataset.\nmaximum word length in an input-text is set as 256; (ii) for the\nbidirectional transformer encoder, the number of layers (i.e.,\ntransformer blocks) is 24, and the number of self-attention\nheads is 16; (iii) the feed-forward/ﬁlter size is 4096; (iv)\nthe dropout probability is set as 0.1 for all layers; (v) the\nlearning rate is 2e-5 (among 5e-5, 4e-5, 3e-5, and 2e-5), and\nthe batch size is set to 252; (vi) the masked percentage is set to\n35%, which releases the optimal experimental result (detailed\ncomparative evaluation of the masked percentage will be\ndiscussed in Section V-D3); and (vii) only Top-30 entity\ncandidates based on ˆP(entity|entity-mention) are introduced\ninto our model ( N =30 is widely adopted in current work for\nthe same datasets, and moreover different choices of Top-N\nwill be compared in Section V-D3). Other hyper-parameters\nare set to the values reported by [9].\nDuring the masked-entity language model’s training pro-\ncedure (Section IV-C2), if the i-th entity-mention is chosen\nto be masked, we replace the i-th entity-mention with (i)\nthe [MASK] token 80% of the time (e.g., in Figure 2 (b)),\n(ii) a random entity-mention 10% of the time, and (iii) the\nunchanged i-th entity-mention 10% of the time. Besides,\nthe same 30K-WordPiece vocabulary [41] and the sub-word\ntokenizer of BERT are adopted with |V | =30,000 for\nstemming. The reason why 30K-WordPiece is utilized here,\nis that we want to transform each instance into a ‘‘BERT-\ncompatible’’ format, i.e., into a sequence of WordPiece\ntokens. Since, the main architecture used here is pre-training\nmodel. We think that, sharing 30K-WordPieces helps our\ntask because lexical-semantic relationships are similar for\nwords composed of the same morphemes. GELUs (Gaussian\nError Linear Units) activation function is adopted in Eq. (2).\nWe obtain the set of concept-constraints T for the Concept\nCorrelation Prediction (details in Section IV-B) from the\nprevious work about instance conceptualization [14], [16],\n[46], and actually we obtained 154.2 million concept triples\nfrom Probase6 [12].\nBesides, the statistical t-test [55], [56] is employed here:\nTo decide whether the improvement by model A over model\nB is signiﬁcant, the t-test calculates a value p∗based on the\nperformance of model A and model B. The smaller p∗ is,\nthe more signiﬁcant the improvement is. If the p∗is small\nenough (p ∗ < 0.05), we conclude that the improvement is\nstatistically signiﬁcant.\nD. EXPERIMENTAL RESULTS AND ANALYSIS\n1) PERFORMANCE SUMMARY\nTable 2 compares the proposed pre-training NED model with\nrecent competitive models on CoNLL test-b dataset in terms\n6https://concept.research.microsoft.com/\nTABLE 2. Accuracy scores (%) of the proposed model and the\nstate-of-the-art models onCoNLL dataset. The superscript† and ‡\nrespectively denote statistically significant improvements over\nstate-of-the-art (Le et al., 2018)[8] and(Yamada et al., 2016)[4]\n(p∗< 0.05).\nof micro accuracy (i.e., mention-averaged accuracy). Like\nmost previous work, we report performance on the 231 test-b\ndocuments. Besides, similar to past work, we retrieve possible\nentity-mention surfaces of an entity from: (i) the title of the\nentity; (ii) the title of another entity redirecting to the entity;\nand (iii) the names of anchors pointing to the entity [4]. From\nthe experimental results in Table 2, we could observe that, our\nmodel, NED+CeBERT, outperforms all the state-of-the-art\nmethods on CoNLL dataset. Especially, compared with state-\nof-the-art (Yamada et al., 2016)[4], (Ganea et al., 2017)\n[5] and (Le et al., 2018)[8], the proposed NED+CeBERT\nimproves the micro accuracy score by 5.16%, 4.34% and\n3.38%, respectively. This result veriﬁes that the proposed\nunsupervised pre-training mechanism, enhanced with prior\nconcept semantic (from extra lexical knowledge graph, e.g.,\nProbase) in Section IV-B and masked-entity language model\nin Section IV-C, is beneﬁcial for NED task. It should be noted\nthat all the other models use, at least partially, engineered\nfeatures. The merit of our proposed model is to show that,\nwith the exception of the ˆP(entity|entity-mention) feature,\na pre-training mechanism is able to learn the best (implicit)\nfeatures for NED without requiring extra expert input.\nTable 3 shows our results for the TAC dataset, where we\nalso utilize the YAGO+KB entity-alias mapping mechanism\n(details in Section V-C1) for all our experiments. Note that,\nthe competition evaluation for TAC dataset includes NIL\nentities, referring to the entities that are not in the knowl-\nedge graph, and TAC’s participants are required to cluster\nNIL mentions across documents so that all mentions of\neach unknown entity are assigned a unique identiﬁer. For\nentity-mentions that cannot be linked to our KB+YAGO,\nwe simply use the mention string as the [NIL] identiﬁer,\nsimilarly to previous work [29]. Following past work [4],\n100478 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nTABLE 3. Accuracy scores (%) of the proposed model and the\nstate-of-the-art models onTAC dataset. The superscript† and ‡\nrespectively denote statistically significant improvements over\nstate-of-the-art (Le et al., 2018)[8] and(Yamada et al., 2016)[4]\n(p∗< 0.05).\n[37], [40], we use entity-mentions only with a valid entry in\nthe KB+YAGO, and report the micro-accuracy score of the\ntop-ranked candidate entities in Table 3. Furthermore, exper-\nimental results show that, the proposed pre-training NED\nmodel signiﬁcantly improves performance on the CoNLL\ndataset, however it only slightly contributes to performance\non the TAC dataset. E.g., the proposed model without concept\nsemantic boosting (denoted as NED+BERT in Table 3) only\nwins by a neck from the best result (Le et al., 2018)[8]. We\nargue that, this is because of the difference in the density of\nentity-mentions between the datasets. Especially, the CoNLL\ndataset contains approximately 20 entity-mentions per docu-\nment, however the TAC dataset only contains approximately\none mention per document. Surprisingly, we attain results\ncomparable with those of some state-of-the-art models on the\nboth datasets by only using original pre-training mechanism\n(i.e., NED+BERT). We observe signiﬁcant improvement\nwhen adding the proposed concept-enhanced mechanism,\ni.e., NED+CeBERT. It also further validates intuitions that,\ndeep contextualized embeddings trained using unsupervised\nlanguage modeling (e.g., BERT [9]) are successful in a wide\nrange of NLP tasks. Moreover, (Ganea et al., 2017) [5],\n(Chen et al., 2018)[6] and the proposed model all utilize con-\ncept information, while (Ganea et al., 2017)[5] introduces\nimplicit concept and others use explicit concept informa-\ntion deﬁned in high-quality lexical knowledge base Probase.\nExperimental results in Table 2 and Table 3 shows (Ganea\net al., 2017) falls behind the other two, which highlights\nthe importance of extra prior knowledge and corroborates\nour motivation. Furthermore, compared with (Chen et al.,\n2018) [6], the proposed NED+CeBERT improves the micro\naccuracy score by 4.11% and 1.88%, on dataset CoNLL and\ndataset TAC respectively, while both of them utilize the same\nextra lexical knowledge resource. We attribute this success\nto the context-aware representation ability from pre-training\nscheme.\nMoreover, [19] also introduced an entity-level masking\nstrategy, masking the entity-mentions (essentially, the words)\nwhich are usually composed of multiple words, and ensured\nthat all of the words in the same entity-mention were masked\nduring word representation training, instead of only one\nword or character being masked. The proposed NED+BERT\nand NED+CeBERT also have the characteristics to do so. To\nsum up, there exist essential differences between our model\nand [19] (as shown in Table 4): (i) [19] proposed three mask-\ning strategies, i.e., basic-level masking, phrase-level mask-\ning and entity-level masking. However, all of them masked\nwords (tokenized by 30K-WordPiece [41]) and trained only\nword embeddings. E.g., in its entity-level masking strategy,\n[19] masked entity-mentions (i.e., words) but not entities.\nTherefore, what [19] predicted were still words, and what [19]\ntrained were only word embeddings. In contrast, our masked-\nentity language model (MELM) masks the entities and trains\nthe entity embeddings. Hence, the ‘‘entity-level’’ masking in\n[19] is still word-level while ours is entity-level. (ii) entity-\nlevel masking in [19] could be reviewed as a constraint and\n[19] inherited the conventional objective of BERT, while the\nmasked-entity language model in ours is a novel objective\n(as discussed in Section IV-C). (iii) In both [19] and ours,\nall of the words in the same entity are considered together.\nWe all take an entity as one unit, which is usually composed\nof several words. All of the words in the same unit are\nmasked during representation training, instead of only one\nword being masked. For example, in the proposed masked-\nentity language model (MELM), if an entity contains multiple\nwords, we compute its position embedding by averaging the\nembeddings of the corresponding positions.\n2) EFFORTS OF CONCEPT CORRELATION PREDICTION\nCurrent unsupervised pre-training models almost rely on\n(masked) language modeling objectives that exploit the\nknowledge encoded in large corpus. This paper complements\nthe encoded distributional knowledge with external lexical\nknowledge (i.e., Probase), and proposes a novel classiﬁcation\nobjective—Concept Correlation Prediction (denoted as CCP\nand details in Section IV-B2). To evaluate the effects of our\ninfection of prior lexical knowledge, this section trains the\nsame model from scratch—with our CCP (which is a concept-\nenhanced BERT, with ‘‘-CeBERT’’ sufﬁx, shown as the blue\npart in Figure 2 (a)) and without it (i.e., original BERT with\n‘‘-BERT’’ sufﬁx, shown as the green part in Figure 2 (a)), for\nNED tasks. To isolate the effects of injecting lexical knowl-\nedge into BERT (Section IV-B), we train basic NED-BERT\nand our NED-CeBERT in the same settings, with excep-\ntion that we additionally update NED-CeBERT’s parameters\nbased on the sum of Masked Language Model (MLM) likeli-\nhood, Masked-Entity Language Model (MELM) likelihood\nand the proposed CCP likelihood (Eq. (1)), while NED-\nBERT optimizes only Masked Language Model likelihood\nand Masked-Entity Language Model likelihood. Different\noptimization schemes mentioned above, are illustrated in Fig-\nure 2 (a). The comparability results are shown in Table 2\nand Table 3, respectively. The experimental results show\nthe additional performance improvement when leveraging\nconcept constraints. Especially, the relative micro accuracy\nVOLUME 8, 2020 100479\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nTABLE 4. Comparison analysis between the proposed model (NED+CeBERT/NED+BERT) and [19], which also introduced an entity-masking strategy.\nimprovements over NED+BERTon CoNLL dataset and TAC\ndataset, are 2.34% and 1.33%, respectively. This demon-\nstrate that, supplementing beneﬁcial extra lexical knowledge\nwith clean linguistic information from structured external\nresources may also lead to unsupervised pre-training models\n(such as BERT [9] etc.,) to improve NED’s performance.\nNote that, as described in Section IV-B2, although ‘‘compar-\native concept correlation’’ relation based constraint is utilized\nhere, other kinds of lexical relation constraints could also be\nadapted here according to the generality and ﬂexibility of\nour model. Moreover, it also further validates intuitions from\nprior work on concept enhanced contextualized embedding\n(for word [15], entity [21] and sentence [23] etc.,) that lever-\naging concept prior for capturing robust semantic representa-\ntion, has a positive impact on natural language understanding.\nMoreover, some output results released by different com-\nparative algorithms are shown in Table 5. Wherein, word\nunderlined in ‘‘text context’’ is the entity-mentions. Com-\nparing NED+BERT, which removes Concept Correlation\nPrediction, and NED+CeBERT, the efﬁciency of Concept\nCorrelation Prediction is proved, especially in text context\nIV . Wherein, contextual word ‘‘watch’’ indicates signiﬁcantly\nthat the entity-mention ‘‘Harry Potter’’ prefers to a movie,\nwith help of extra instance conceptualization algorithm for\nconstructing concept correlation constraints.\n3) OPTIMIZATION OF PARAMETERS\nIn this section, we analyze the robustness of the parameter\nsetting in the proposed model, which could affect the overall\nperformance: (i) the masked percentage; and (ii) different\nchoices of Top-N . All these experiments in this section are\nrun on CoNLL dataset and TAC dataset.\nThe masked percentage is ratio of marked entities of all\nthe entities occurred in the given input-text sequence. When\noptimizing our Masked-Entity Language Model (MELM)\nobjective in Section IV-C, we randomly mask some enti-\nties in the input-text sequence, and then train the embed-\ndings and parameters to predict these masked entities based\non the words and other non-masked entities. We evaluate\nincreasing value of this parameter and the results are reported\nin Figure 3. The sufﬁx ‘‘-CoNLL’’ and sufﬁx ‘‘-TAC’’ indi-\ncates the experimental results evaluated on CoNLL dataset\nand TAC dataset, respectively. Overall, we ﬁnd that the\nmasked percentage considerably affects performance. When\nsetting masked percentage around 35%, both the proposed\nFIGURE 3. Effect of the increasing masked percentage onCoNLL dataset\nand TAC dataset.\nFIGURE 4. Effect of the increasing number of candidate entities (Top-N)\non CoNLL dataset.\nNED+CeBERT and NED+BERT could reach their opti-\nmal micro accuracy scores, while larger value will result in\na performance drop as it loses beneﬁcial semantic signals\nexcessively and leads to many hardships for name entity\nprediction. Moreover, from the experimental results from Fig-\nure 3, we could observe that, the proposed NED+CeBERT\nis almost insulated from masked percentage ﬂuctuations, and\nit proves the effective use of concept correlation prediction\nobjective (Section IV-B).\nFigure 4 and Figure 5 show the sensitivity of our model\nand other comparative models to the parameter N, i.e., the\nnumber of candidate entities, on CoNLL dataset and TAC\ndataset, respectively. We can have a very good prediction\nresults with a small number of candidate entities, compared\n100480 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nTABLE 5. Examples of output results from different algorithms.\nFIGURE 5. Effect of the increasing number of candidate entities (Top-N)\non TAC dataset.\nwith other state-of-the-art models. According to these ﬁg-\nures, by increasing the number of candidate entities, the per-\nformance is improved. N = 30 is the best value for the\nparameter N. In fact, by sequentially increasing the number\nof candidate entities, the performance slightly decreases. The\nperformances of (Ganea et al., 2017)and (Le et al., 2018)are\nslightly inﬂuenced by the number of candidate entities (i.e.,\nparameter N), while (Yamadaet al., 2016)is impressionable.\nWherein, (Ganea et al., 2017) releases the best stability\nperformance. The reason could be related to the second ﬁl-\nter strategy utilized in both (Ganea et al., 2017) and (Le\net al., 2018), wherein they invariably select only 7 candidate\nentities from the Top-30 candidate entities based on some\nextra heuristic. Interestingly, the proposed NED+CeBERT\nand NED-BERT could achieve the similar stability although\nwe don’t introduce any extra heuristic for second ﬁltering.\nE. PERFORMANCE ON GERBIL PLATFORM\nIn order to produce a fair comparison, this section describes\nthe comparisons with AGDISTIS [57], Babelfy [58], Spot-\nlight [59], Dexter [60], KEA [61], NERD-ML [62],\nTagMe 2[63], WAT[63] and benchmarking results by the\nstandard platform GERBIL 7 [64]. GERBIL is a general\n7http://aksw.org/Projects/GERBIL.html\nentity annotation system, which offers an easy-to-use web-\nbased platform for the agile comparison of algorithms using\nmultiple datasets and uniform measuring approaches.\nEspecially, we compare the proposed model with 8 other\nstate-of-the-art NED models over 10 different datasets , 8\nby implementing a web-based API for our model that is\ncompatible with GERBIL. Details about the aforementioned\nbaselines and datasets can be found in [64]. The strong anno-\ntation task (D2KB) is considered in this section, wherein\nthe comparative models are given an input-text containing\na number of marked tokens, and must return the entity\nmodel associates with each marked token. We evaluate the\nproposed model using the aforementioned datasets available\nby default in GERBIL, which are primarily based on news\narticles, RSS feeds, and tweets. Table 6 and Table 7 show\nthe micro-F1 scores (aggregated across models) and macro-\nF1 scores (aggregated across texts), respectively. The exper-\nimental results show that the proposed model (denoted as\nNED+CeBERT) could defeat the state-of-the-art baselines\nprovided by GERBIL in most cases, and also provide con-\nvincing evidence that the proposed model works well on\nseveral types of datasets.\nVI. DISCUSSION\nPrevious neural network based methods have recently\nachieved strong results on NED task. Generally, the key\ncomponent of these methods is an embedding model of\nwords or entities trained using a large knowledge base. These\nmodels are typically based on conventional word embedding\nmodels that assign a ﬁxed embedding to each word and entity.\nAlthough these methods have advanced the state-of-the-art\nresults on NED task, they however failed to model and repre-\nsent complex relationships and context, and multiple signals\n(i.e., words, entities and context etc.,) can not be fully inter-\nplayed in their architectures. To overcome these problem,\nin this study, we aim to test the effectiveness of the pre-trained\ncontextualized embeddings for NED task. Because, On the\n8Note that, the datasets provided by GERBIL contain dataset CoNLL\n[27], which has been evaluated in previous experiment (Section V-D) and\nits comparison results are not reported here.\nVOLUME 8, 2020 100481\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nTABLE 6. Micro-F1 scores of the proposed model (denoted asNED+CeBERT) and the state-of-the-art models provided by GERBIL platform.\nTABLE 7. Macro-F1 scores of the proposed model (denoted asNED+CeBERT) and the state-of-the-art models provided by GERBIL platform.\nother hand, language model pre-training has been shown to\nbe effective for improving many NLP tasks, relying on its\nability of modeling the complex context. Moreover, previous\npre-training work relied only on the distributional knowledge\nwhile ignoring lexical semantic information for understand-\ning language. Therefore, this paper also investigates how\nto enhanced pre-training model’s ability by leveraging prior\nconcept information.\nOverall, this paper tackles NED problem by leveraging two\nnovel objectives for pre-training framework, and proposes a\nnovel pre-training NED model. Especially, the proposed pre-\ntraining NED model consists of: (i) concept-enhanced pre-\ntraining, aiming at identifying valid lexical semantic relations\nwith the concept semantic constraints derived from external\nresource Probase (details in Section IV-B); and (ii) masked-\nentity language model, aiming to train the contextualized\nembedding by predicting randomly masked entities based on\nthe words and non-masked entities in the given input-text\n(details in Section IV-C).\n(i) Concept-Enhanced Pre-Training: The proposed\nconcept-enhanced pre-training model successfully blends\nextra lexical knowledge with distributional learning sig-\nnals. Moreover, other kinds of linguistic information such\nas synonymy etc., could be adopted here because of the\ngenerality of the proposed model. The experimental results in\nSection V-D2 suggest that, available extra lexical knowledge\ncan be used to supplement unsupervised pre-training mod-\nels, with useful information which cannot be fully captured\nsolely using plain text although the scale of corpus is huge\nenough.\n(ii) Masked-Entity Language Model: The intuition behind\nthe proposed masked-entity language model is that, following\nthe strategy hold in original BERT that certain percentage of\nwords is masked and needs to be predicted, therefore we try\nto mask a certain percentage of entitys in the similar way for\ntraining pre-training model for NED task, because both of the\naforementioned conditions are at token-level.\nVII. CONCLUSION\nIn the task of Named Entity Disambiguation (NED), this\npaper proposes a novel unsupervised pre-training model for\nwords and entities, which is enhanced by external lexical\nknowledge (i.e., concept knowledge from Probase). Espe-\ncially, a novel masked-entity language model is introduced\nhere, aiming to train the contextualized embedding model by\npredicting randomly masked entities based on the words and\nnon-masked entities in the given input-text sequence. Exper-\nimental results show that, our pre-training NED model suc-\ncessfully achieves enhanced performance on both datasets,\nas well as the benchmark GERBIL platform.\nREFERENCES\n[1] T. Liu, K. Wang, L. Sha, B. Chang, and Z. Sui, ‘‘Table-to-text generation\nby structure-aware seq2seq learning,’’ CoRR, 2018.\n[2] A. Laha, P. Jain, A. Mishra, and K. Sankaranarayanan, ‘‘Scalable micro-\nplanned generation of discourse from structured data,’’ CoRR, 2018.\n[3] A. Cetoli, M. Akbari, S. Bragaglia, A. D. O’Harney, and M. Sloan,\n‘‘Named entity disambiguation using deep learning on graphs,’’ 2018,\narXiv:1810.09164. [Online]. Available: http://arxiv.org/abs/1810.09164\n[4] I. Yamada, H. Shindo, H. Takeda, and Y . Takefuji, ‘‘Joint learning of the\nembedding of words and entities for named entity disambiguation,’’ in\nProc. CoNLL, 2016, pp. 250–259.\n[5] O.-E. Ganea and T. Hofmann, ‘‘Deep joint entity disambiguation with local\nneural attention,’’ in Proc. EMNLP, 2017, pp. 2619–2629.\n[6] L. Chen, J. Liang, C. Xie, and Y . Xiao, ‘‘Short text entity linking with ﬁne-\ngrained topics,’’ in Proc. CIKM, Oct. 2018, pp. 457–466.\n[7] G. Zhu and C. A. Iglesias, ‘‘Exploiting semantic similarity for named\nentity disambiguation in knowledge graphs,’’ Expert Syst. Appl., vol. 101,\npp. 8–24, Jul. 2018.\n[8] P. Le and I. Titov, ‘‘Improving entity linking by modeling latent relations\nbetween mentions,’’ in Proc. ACL, 2018, pp. 1595–1604.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘Bert: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL-HLT, 2019, pp. 1–16.\n100482 VOLUME 8, 2020\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\n[10] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365\n[11] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog,\nvol. 1, no. 8, p. 9, 2019.\n[12] W. Wu, H. Li, H. Wang, and K. Q. Zhu, ‘‘Probase: A probabilistic taxon-\nomy for text understanding,’’ in Proc. ACM SIGMOD Int. Conf. Manage.\nData, 2012, pp. 481–492.\n[13] Z. Yu, H. Wang, X. Lin, and M. Wang, ‘‘Understanding short texts through\nsemantic enrichment and hashing,’’ IEEE Trans. Knowl. Data Eng., vol. 28,\nno. 2, pp. 566–579, Feb. 2016.\n[14] Y . Wang, H. Huang, and C. Feng, ‘‘Query expansion based on a feedback\nconcept model for microblog retrieval,’’ in Proc. Int. Conf. World Wide\nWeb, Apr. 2017, pp. 559–568.\n[15] H. Huang, Y . Wang, C. Feng, Z. Liu, and Q. Zhou, ‘‘Leveraging con-\nceptualization for short-text embedding,’’ IEEE Trans. Knowl. Data Eng.,\nvol. 30, no. 7, pp. 1282–1295, Jul. 2018.\n[16] Y . Song, H. Wang, Z. Wang, H. Li, and W. Chen, ‘‘Short text conceptu-\nalization using a probabilistic knowledgebase,’’ in Proc. Int. Joint Conf.\nArtif. Intell., 2011, pp. 2330–2336.\n[17] A. Radford, ‘‘Improving language understanding by generative pre-\ntraining,’’ Tech. Rep., 2018.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. NIPS,\n2017, pp. 5998–6008.\n[19] Y . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang, X. Tian,\nD. Zhu, H. Tian, and H. Wu, ‘‘ERNIE: Enhanced representation through\nknowledge integration,’’ 2019, arXiv:1904.09223. [Online]. Available:\nhttp://arxiv.org/abs/1904.09223\n[20] M. E. Peters, M. Neumann, R. Logan, R. Schwartz, V . Joshi, S. Singh, and\nN. A. Smith, ‘‘Knowledge enhanced contextual word representations,’’ in\nProc. EMNLP-IJCNLP, 2019, pp. 43–54.\n[21] Y . Wang, Y . Liu, H. Zhang, and H. Xie, ‘‘Leveraging lexical semantic infor-\nmation for learning concept-based multiple embedding representations for\nknowledge graph completion,’’ inProc. APWeb/WAIM, 2019, pp. 382–397.\n[22] S. Ma, J. Ding, W. Jia, K. Wang, and M. Guo, ‘‘Transt: Type-based\nmultiple embedding representations for knowledge graph completion,’’ in\nProc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases, 2017,\npp. 717–733.\n[23] Y . Wang, H. Huang, C. Feng, Q. Zhou, J. Gu, and X. Gao, ‘‘CSE: Concep-\ntual sentence embeddings based on attention model,’’ in Proc. 54th Annu.\nMeeting Assoc. Comput. Linguistics, 2016, pp. 505–515.\n[24] D. Yarowsky, ‘‘Unsupervised word sense disambiguation rivaling super-\nvised methods,’’ in Proc. Annu. Meeting Assoc. Comput. Linguistics, 1995,\npp. 189–196.\n[25] R. Navigli, ‘‘Word sense disambiguation: A survey,’’ ACM Comput. Surv.,\nvol. 41, no. 2, pp. 1–69, 2009.\n[26] E. Agirre and A. Soroa, ‘‘Personalizing PageRank for word sense disam-\nbiguation,’’ in Proc. EACL, 2009, pp. 33–41.\n[27] J. Hoffart, M. A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol,\nB. Taneva, S. Thater, and G. Weikum, ‘‘Robust disambiguation of named\nentities in text,’’ in Proc. EMNLP, 2011, pp. 782–792.\n[28] L.-A. Ratinov, D. Roth, D. Downey, and M. Anderson, ‘‘Local and\nglobal algorithms for disambiguation to wikipedia,’’ in Proc. ACL, 2011,\npp. 1375–1384.\n[29] A. Globerson, N. Lazic, S. Chakrabarti, A. Subramanya, M. Ringaard,\nand F. Pereira, ‘‘Collective entity resolution with multi-focal attention,’’\nin Proc. ACL, 2016, pp. 621–631.\n[30] Z. Guo and D. Barbosa, ‘‘Entity linking with a uniﬁed semantic represen-\ntation,’’ in Proc. WWW, 2014, pp. 1305–1310.\n[31] M. Pershina, Y . He, and R. Grishman, ‘‘Personalized page rank for named\nentity disambiguation,’’ in Proc. HLT-NAACL, 2015, pp. 238–243.\n[32] S. Chakrabarti, ‘‘Dynamic personalized pagerank in entity-relation\ngraphs,’’ in Proc. WWW, 2007, pp. 571–580.\n[33] M. I. Jordan, Learning in Graphical Models(NATO ASI Series). 1998.\n[34] R. Blanco, G. Ottaviano, and E. Meij, ‘‘Fast and space-efﬁcient entity\nlinking for queries,’’ in Proc. WSDM, 2015, pp. 179–188.\n[35] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\nword representations in vector space,’’ Comput. Sci., 2013.\n[36] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ Adv.\nNeural Inf. Process. Syst., vol. 26, pp. 3111–3119, 2013.\n[37] Z. He, S. Liu, M. Li, M. Zhou, L. Zhang, and H. Wang, ‘‘Learning entity\nrepresentation for entity disambiguation,’’ in Proc. ACL, 2013, pp. 30–34.\n[38] Y . Sun, L. Lin, D. Tang, N. Yang, Z. Ji, and X. Wang, ‘‘Modeling mention,\ncontext and entity with neural networks for entity disambiguation,’’ in\nProc. IJCAI, 2015, pp. 1333–1339.\n[39] Z. Hu, P. Huang, Y . Deng, Y . Gao, and E. P. Xing, ‘‘Entity hierarchy\nembedding,’’ in Proc. ACL, 2015, pp. 1292–1300.\n[40] A. Chisholm and B. Hachey, ‘‘Entity disambiguation with Web links,’’\nTrans. Assoc. Comput. Linguistics, vol. 3, pp. 145–156, Dec. 2015.\n[41] Y . Wu et al., ‘‘Google’s neural machine translation system: Bridging the\ngap between human and machine translation,’’ 2016, arXiv:1609.08144.\n[Online]. Available: http://arxiv.org/abs/1609.08144\n[42] Y . Wang, H. Zhang, G. Shi, Z. Liu, and Q. Zhou, ‘‘A model of text-\nenhanced knowledge graph representation learning with mutual attention,’’\nIEEE Access, vol. 8, pp. 52895–52905, 2020.\n[43] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and\nS. Hellmann, ‘‘DBpedia—A crystallization point for the Web of data,’’\nJ. Web Semantics, vol. 7, no. 3, pp. 154–165, Sep. 2009.\n[44] G. A. Miller, ‘‘WordNet: A lexical database for English,’’ Commun. ACM,\nvol. 38, no. 11, pp. 39–41, 1995.\n[45] Y . Wang, ‘‘Short-text conceptualization based on a co-ranking framework\nvia lexical knowledge base,’’ in Proc. CCL, 2019, pp. 281–293.\n[46] Z. Wang, K. Zhao, H. Wang, X. Meng, and J.-R. Wen, ‘‘Query understand-\ning through knowledge-based conceptualization,’’ in Proc. Int. Conf. Artif.\nIntell., 2015, pp. 3264–3270.\n[47] D. Hendrycks and K. Gimpel, ‘‘Bridging nonlinearities and stochastic\nregularizers with Gaussian error linear units,’’ 2017, arXiv:1606.08415.\n[Online]. Available: https://arxiv.org/abs/1606.08415\n[48] H. Ji, R. Grishman, H. T. Dang, K. Grifﬁtt, and J. Ellis, ‘‘Overview of the\nTAC 2010 knowledge base population track,’’ in Proc. 3rd Text Anal. Conf.\n(TAC), 2010, p. 3.\n[49] Y . Eshel, N. Cohen, K. Radinsky, S. Markovitch, I. Yamada, and O. Levy,\n‘‘Named entity disambiguation for noisy text,’’ in Proc. CoNLL, 2017,\npp. 58–68.\n[50] F. M. Suchanek, G. Kasneci, and G. Weikum, ‘‘Yago: A core of semantic\nknowledge,’’ in Proc. Int. Conf. World Wide Web, 2007, pp. 697–706.\n[51] K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, ‘‘Freebase:\nA collaboratively created graph database for structuring human knowl-\nedge,’’ in Proc. SIGMOD Conf., 2008, pp. 1247–1250.\n[52] J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum, ‘‘YAGO2:\nA spatially and temporally enhanced knowledge base from wikipedia,’’\nArtif. Intell., vol. 194, pp. 28–61, Jan. 2013.\n[53] F. Hasibi, K. Balog, and S. E. Bratsberg, ‘‘On the reproducibility of the\nTAGME entity linking system,’’ in Proc. ECIR, 2016, pp. 436–449.\n[54] V . I. Spitkovsky and A. X. Chang, ‘‘A cross-lingual dictionary for English\nWikipedia concepts,’’ in Proc. LREC, 2012, pp. 1–8.\n[55] A. A. Ding, C. Chen, and T. Eisenbarth, ‘‘Simpler, faster, and more robust\nt-test based leakage detection,’’ in Proc. COSADE, 2015, pp. 163–183.\n[56] K. R. B. Jankowski, K. J. Flannelly, and L. T. Flannelly, ‘‘The t-test:\nAn inﬂuential inferential tool in chaplaincy and other healthcare research,’’\nJ. Health Care Chaplaincy, vol. 24, no. 1, p. 30, 2018.\n[57] R. Usbeck, A.-C. N. Ngomo, M. Röder, D. Gerber, S. Coelho, S.\nAuer, and A. Both, ‘‘AGDISTIS—Graph-based disambiguation of named\nentities using linked data,’’ in Proc. Int. Semantic Web Conf., 2014,\npp. 457–471.\n[58] A. Moro, A. Raganato, and R. Navigli, ‘‘Entity linking meets word sense\ndisambiguation: A uniﬁed approach,’’ Trans. Assoc. Comput. Linguistics,\nvol. 2, pp. 231–244, Dec. 2014.\n[59] J. Daiber, M. Jakob, C. Hokamp, and P. N. Mendes, ‘‘Improving efﬁciency\nand accuracy in multilingual entity extraction,’’ in Proc. I-SEMANTICS,\n2013, pp. 121–124.\n[60] D. Ceccarelli, C. Lucchese, S. Orlando, R. Perego, and S. Trani, ‘‘Dexter:\nAn open source framework for entity linking,’’ in Proc. ESAIR, 2013,\npp. 17–20.\n[61] N. Steinmetz and H. Sack, ‘‘Semantic multimedia information retrieval\nbased on contextual descriptions,’’ in Proc. ESWC, 2013, pp. 382–396.\n[62] G. Rizzo, M. van Erp, and R. Troncy, ‘‘Benchmarking the extraction and\ndisambiguation of named entities on the semantic Web,’’ in Proc. LREC,\n2014, pp. 4593–4600.\n[63] P. Ferragina and U. Scaiella, ‘‘Fast and accurate annotation of short texts\nwith Wikipedia pages,’’ IEEE Softw., vol. 29, no. 1, pp. 70–75, Jan. 2012.\n[64] M. Röder, R. Usbeck, and A.-C. N. Ngomo, ‘‘GERBIL—Benchmarking\nnamed entity recognition and linking consistently,’’ Semantic Web, vol. 9,\nno. 5, pp. 605–625, Aug. 2018.\nVOLUME 8, 2020 100483\nZ. Jiet al.: Leveraging Concept-Enhanced Pre-Training Model and MELM for NED\nZIZHENG JI is currently pursuing the Ph.D.\ndegree in computer science with the Beijing Insti-\ntute of Technology, Beijing, China. His current\nresearch interests include learning algorithms,\nmachine learning, and knowledge graph.\nLIN DAI is currently a Professor and a Doctoral\nTutor at the School of Computer, Beijing Insti-\ntute of Technology. His current research interest\nmainly focuses on natural language processing.\nJIN PANGreceived the M.Eng. degree from North\nChina Electric Power University, Beijing, in 2016.\nHer current research interests include network\nsecurity and attack and defense technology.\nTINGTING SHEN received the M.S. degree\nfrom the Beijing Institute of Technology, Bei-\njing, China, in 2016. Her current research interest\nincludes artiﬁcial intelligence.\n100484 VOLUME 8, 2020",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9110394716262817
    },
    {
      "name": "Entity linking",
      "score": 0.746129035949707
    },
    {
      "name": "Natural language processing",
      "score": 0.7171604633331299
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6396329402923584
    },
    {
      "name": "Knowledge graph",
      "score": 0.5824657678604126
    },
    {
      "name": "Language model",
      "score": 0.5512113571166992
    },
    {
      "name": "Merge (version control)",
      "score": 0.5508105158805847
    },
    {
      "name": "Embedding",
      "score": 0.5369844436645508
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5056881904602051
    },
    {
      "name": "Sequence labeling",
      "score": 0.4821578860282898
    },
    {
      "name": "Named entity",
      "score": 0.44437074661254883
    },
    {
      "name": "Task (project management)",
      "score": 0.405738890171051
    },
    {
      "name": "Information retrieval",
      "score": 0.31614017486572266
    },
    {
      "name": "Knowledge base",
      "score": 0.1903507113456726
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}