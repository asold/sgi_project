{
    "title": "Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification",
    "url": "https://openalex.org/W3035740499",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2098377681",
            "name": "Hao Tang",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2031222755",
            "name": "Donghong Ji",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2105534490",
            "name": "Chenliang Li",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2479215446",
            "name": "Qiji Zhou",
            "affiliations": [
                "Wuhan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2911752708",
        "https://openalex.org/W2529550020",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963168371",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964164368",
        "https://openalex.org/W2963428430",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2296071000",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2251089481",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W2954278700",
        "https://openalex.org/W2600702321",
        "https://openalex.org/W2963909901",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950404230",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2971220558",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2962808042",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2251124635",
        "https://openalex.org/W2799119017",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2892094955",
        "https://openalex.org/W2465099471",
        "https://openalex.org/W2963355447",
        "https://openalex.org/W2963240575",
        "https://openalex.org/W2979860911",
        "https://openalex.org/W2740567223",
        "https://openalex.org/W2251648804",
        "https://openalex.org/W2251294039",
        "https://openalex.org/W2265846598",
        "https://openalex.org/W2605145284",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2951701153",
        "https://openalex.org/W2962946486",
        "https://openalex.org/W1902237438"
    ],
    "abstract": "Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But the improvement is limited due to the noise and instability of dependency trees. To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner. Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6578–6588\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n6578\nDependency Graph Enhanced Dual-transformer Structure for\nAspect-based Sentiment Classiﬁcation\nHao Tang1 , Donghong Ji1∗, Chenliang Li1 , Qiji Zhou1\n1Key Laboratory of Aerospace Information Security and Trusted Computing,\nMinistry of Education, School of Cyber Science and Engineering, Wuhan University, China\n{tanghaopro,dhji,cllee,qiji.zhou}@whu.edu.cn\nAbstract\nAspect-based sentiment classiﬁcation is a pop-\nular task aimed at identifying the correspond-\ning emotion of a speciﬁc aspect. One sentence\nmay contain various sentiments for different\naspects. Many sophisticated methods such\nas attention mechanism and Convolutional\nNeural Networks (CNN) have been widely\nemployed for handling this challenge. Re-\ncently, semantic dependency tree implemented\nby Graph Convolutional Networks (GCN) is\nintroduced to describe the inner connection\nbetween aspects and the associated emotion\nwords. But the improvement is limited due to\nthe noise and instability of dependency trees.\nTo this end, we propose a dependency graph\nenhanced dual-transformer network (named\nDGEDT) by jointly considering the ﬂat repre-\nsentations learnt from Transformer and graph-\nbased representations learnt from the corre-\nsponding dependency graph in an iterative\ninteraction manner. Speciﬁcally, a dual-\ntransformer structure is devised in DGEDT to\nsupport mutual reinforcement between the ﬂat\nrepresentation learning and graph-based repre-\nsentation learning. The idea is to allow the\ndependency graph to guide the representation\nlearning of the transformer encoder and vice\nversa. The results on ﬁve datasets demonstrate\nthat the proposed DGEDT outperforms all\nstate-of-the-art alternatives with a large mar-\ngin.\n1 Introduction\nAspect-based or aspect-level sentiment classiﬁca-\ntion is a popular task with the purpose of identify-\ning the sentiment polarity of the given aspect (Yang\net al., 2017; Zhang and Liu, 2017; Zeng et al.,\n2019). The goal is to predict the sentiment po-\nlarity of a given pair (sentence, aspect). Aspects in\nour study are mostly noun phrases appearing in the\n∗Corresponding author.\ninput sentence. As shown in Figure 1, where the\ncomment is about the laptop review, the sentiment\npolarities of two aspects battery life and memory\nare positive and negative, respectively. Giving a\nspeciﬁc aspect is crucial for sentiment classiﬁcation\nowing to the situation that one sentence sometimes\ncontains several aspects, and these aspects may\nhave different sentiment polarities.\nModern neural methods such as Recurrent Neu-\nral Networks (RNN), Convolutional Neural Net-\nworks (CNN) (Dong et al., 2014; V o and Zhang,\n2015) have already been widely applied to aspect-\nbased sentiment classiﬁcation. Inspired by the\nwork (Tang et al., 2016a) which demonstrates the\nimportance of modeling the semantic connection\nbetween contextual words and aspects, RNN aug-\nmented by attention mechanism (Bahdanau et al.,\n2015; Luong et al., 2015; Xu et al., 2015) is widely\nutilized in recent methods for exploring the poten-\ntially relevant words with respect to the given as-\npect (Yang et al.,2017; Zhang and Liu, 2017; Zeng\net al., 2019; Wang et al., 2016). CNN based atten-\ntion methods (Xue and Li , 2018; Li et al., 2018)\nare also proposed to enhance the phrase-level rep-\nresentation and achieved encouraging results.\nAlthough attention-based models have achieved\npromising performance on several tasks, the limita-\ntion is still obvious because attention module may\nhighlight the irrelevant words owing to the syntac-\ntical absence. For example, given the sentence “it\nhas a bad memory but a great battery life. ” and\naspect “battery life”, attention module may still\nassign a large weight to word “bad” rather than\n“great”, which adversely leads to a wrong sentiment\npolarity prediction.\nTo take advantages of syntactical information\namong aspects and contextual words, Zhang et al.\n(2019) proposed a novel aspect-based GCN method\nwhich incorporates dependency tree into the at-\ntention models. Actually, using GCN (Kipf and\n6579\nAspect: memory   Sentiment: Negative\nAspect: battery life   Sentiment: Positive\nFigure 1: A typical utterance sample of aspect-based sentiment classiﬁcation task with a proper dependency tree,\nnotice that different aspects may have different sentiment polarities.\nWelling, 2017) to encode the information conveyed\nby a dependency tree has already been investigated\nin several ﬁelds, e.g., modeling document-word re-\nlationships (Yao et al., 2019) and tree structures\n(Marcheggiani and Titov,2017; Zhang et al., 2018).\nAs shown in Figure 1, an annotated dependency\ntree of original sentence is provided, and we can\nobserve that word-aspect pairs (bad, memory) and\n(great, battery life) are well established.\nDirect application of dependency tree has two\nobvious shortcomings: (1) the noisy information\nis inevitably introduced through the dependency\ntree, due to imperfect parsing performance and the\ncasualness of input sentence; (2) GCN would be\ninherently inferior in modeling long-distance or\ndisconnected words in the dependency tree. It is\nreported that lower performance is achieved even\nwith the golden dependency tree, by comparing\nagainst using only the ﬂat structure (Zhang et al.,\n2019).\nTo address these two challenges, we propose\na dependency graph enhanced dual-transformer\nnetwork (named DGEDT) for aspect-based sen-\ntiment classiﬁcation. DGEDT consists of a tra-\nditional transformer (Vaswani et al., 2017) and a\ntransformer-like structure implemented via a de-\npendency graph based bidirectional GCN (BiGCN).\nSpeciﬁcally, a dual-transformer structure is intro-\nduced in DGEDT to fuse the ﬂat representations\nlearnt by the transformer and the graph-based rep-\nresentations learnt based on the dependency graph.\nThese two kinds of representations are jointly re-\nﬁned through a mutual BiAfﬁne transformation pro-\ncess, where the dependency graph can guide and\npromote the ﬂat representation learning. The ﬁnal\nﬂat representations derived by the transformer is\nthen used with an aspect-based attention for senti-\nment classiﬁcation. We have conducted extensive\nexperiments over ﬁve benchmark datasets. The ex-\nperimental results demonstrate that the proposed\nDGEDT achieves a large performance gain over\nthe existing state-of-the-art alternatives.\nTo the best of our knowledge, the proposed\nDGEDT is the ﬁrst work that jointly considers\nthe ﬂat textual knowledge and dependency graph\nempowered knowledge in a uniﬁed framework. Fur-\nthermore, unlike other aspect-based GCN models,\nwe aggregate the aspect embeddings from multi-\nple aspect spans which share the same mentioned\naspect before feeding these embeddings into sub-\nmodules. We also introduce an aspect-modiﬁed\ndependency graph in DGEDT.\n2 Related Work\nEmploying modern neural networks for aspect-\nbased sequence-level sentiment classiﬁcation task,\nsuch as CNNs (Kim, 2014; Johnson and Zhang,\n2015), RNNs (Castellucci et al., 2014; Tang et al.,\n2016a), Recurrent Convolutional Neural Networks\n(RCNNs) (Lai et al., 2015), have already achieved\nexcellent performance in several sentiment analysis\ntasks. Many attention-based RNN or CNN meth-\nods (Yang et al., 2017; Zhang and Liu, 2017; Zeng\net al., 2019) are also proposed to handle sequence\nclassiﬁcation tasks. Tai et al. (2015) proposed a\ntree-LSTM structure which is enhanced with de-\npendency trees or constituency trees, which outper-\nforms traditional LSTM. Dong et al. (2014) pro-\nposed an adaptive recursive neural network using\ndependency trees. Since being ﬁrstly introduced\nin (Kipf and Welling, 2017), GCN has recently\nshown a great ability on addressing the graph struc-\nture representation in Natural Language Process-\ning (NLP) ﬁeld. Marcheggiani and Titov (2017)\nproposed a GCN-based model for semantic role\nlabeling. Vashishth et al. (2018) and Zhang et al.\n6580\n(2018) used GCN over dependency trees in docu-\nment dating and relation classiﬁcation, respectively.\nYao et al. (2019) introduced GCN to text classiﬁ-\ncation task with the guidance of document-word\nand word-word relations. Furthermore, Zhang et al.\n(2019) introduced aspect-based GCN to cope with\naspect-level sentiment classiﬁcation task using de-\npendency graphs. On the other hand, Chen and\nQian (2019) introduced and adapted Capsule Net-\nworks along with transfer learning to improve the\nperformance of aspect-level sentiment classiﬁca-\ntion. Gao et al. (2019) introduced BERT into a\ntarget-based method, and Sun et al. (2019) con-\nstructed BERT-based auxiliary sentences to further\nimprove the performance.\n3 Preliminaries\nSince Transformer (Vaswani et al.,2017) and GCN\nare two crucial sub-modules in DGEDT, here we\nbrieﬂy introduce these two networks and illustrate\nthe fact that GCN can be considered as a special-\nized Transformer.\nAssume that there are three input matrices Q∈\nRn×dk ,K ∈Rm×dk ,V ∈Rm×dv , which repre-\nsent the queries, keys and values respectively. n\nand mare the length of two inputs.\nQ′= Attention(Q,K,V )\n= softmax( QKT\n√dk\n)V,\n(1)\nwhere Q′ ∈Rn×dv , dk and dv are the dimension\nsize of keys and values, respectively. Actually,\nTransformer adopts multi-head attention mecha-\nnism to further enhance the representative ability\nas follows:\nhi = Attention(QWQ\ni ,KW K\ni ,VW V\ni ), (2)\nQ′= Concat([h1,...])WO, (3)\nwhere i ∈ [1,H], H is the head size, WQ\ni ∈\nRdk×dk/H,WK\ni ∈Rdk×dk/H,WV\ni ∈Rdv×dv/H\nand WO ∈Rdv×dv , and hi is the i-th head embed-\nding. Then, two normalization layers are employed\nto extract higher-level features as follows:\nQ′\n1 = Norm(Q′+ Q), (4)\nQ′\n2 = Norm(Q′\n1 + FFN (Q′\n1)), (5)\nwhere FFN (x) =Relu(xW1 + b1)W2 + b2 is a\ntwo-layer multi-layer perceptron (MLP) with the\nactivation function Relu, Norm is a normalization\nDual-transformer Structure\nMax-Pooling\nClassify\nAttention \nModule\nDependency \nGraph (Aspect-\nmodified)\nAspect \nRepresentation\nAspect SpanAspect Span\nSUM SUM\nAspect \nRepresentation\nBiLSTM/\nBERT\nInput\nFigure 2: An overall demonstration of our proposed\nDGEDT. Aspect representation is accumulated from\nthe embeddings in its aspect span, thus the attention\nmodule is also aspect-sensitive.\nlayer, Q′\n2 is the output vector of this transformer\nlayer. Equations (1)-(5) can be repeated forT times.\nNote that if Q = K = V, this operation can be\nconsidered as self alignment.\nAs for GCN, the computation can be conducted\nas follows when the adjacent matrix of each word\nin the input is explicitly provided.\nQ′= Norm(Q+ Relu( 1\n|Aadj|AadjQW)), (6)\nwhere Aadj ∈Rn×n is the adjacent matrix formed\nfrom the dependency graph, n is the number of\nwords, Q ∈Rn×dk ,W ∈Rdk×dk . 1\n|Aadj|Aadj is\nsimilar to softmax( QKT\n√dk\n) which is denoted as a\ngenerated alignment matrix, except for the main\ndifference that Aadj is ﬁxed and discrete. It is ob-\nvious that Equation (6) can be decomposed into\nEquations (1)-(4), and it can be also repeated for\nT times. In our perspective, GCN is a specialized\nTransformer with the head size set to one and the\ngenerated alignment matrix replaced by a ﬁxed ad-\njacent matrix.\n4 DGEDT\nThe network architecture of our proposed DGEDT\nis shown in Figure 2. For a given input text, we\n6581\nInput\nEmbedding\nSelf\nAttention\nFeed\nForward\nAdd&Norm\nAdd&Norm\nBiGCN\nAdd&Norm\nFlat\n(with graph)\nGraph\n(with flat)\nMutual\nBiaffine\nAdd&Norm Add&Norm\nT  \nFigure 3: A simpliﬁed demonstration of dual-\ntransformer structure, which consists of two sub-\nmodules, one is a standard transformer, another is\na transformer-like structure implemented by BiGCN\nwith the supervision of dependency graph.\nﬁrst utilize BiLSTM or Bidirectional Encoder Rep-\nresentations from Transformers (BERT) (Devlin\net al., 2019) as the aspect-based encoder to ex-\ntract hidden contextual representations. Then these\nhidden representations are fed into our proposed\ndual-transformer structure, with the guidance of\naspect-modiﬁed dependency graph. At last, we\naggregate all the aspect representations via max-\npooling and apply an attention module to align\ncontextual words and the target aspect. In this way,\nthe model can automatically select relevant aspect-\nsensitive contextual words with the dependency\ninformation for sentiment classiﬁcation.\n4.1 Aspect-based Encoder\nWe use wk to represent the k-th word embedding.\nBidirectional LSTMs (Schuster and Paliwal, 1997;\nHochreiter and Schmidhuber, 1997) (BiLSTM) are\napplied for the encoder if we do not use BERT.\nh1,... = Encoder([w1,...]), (7)\nwhere hk ∈ Rh is the k-th output of Encoder\n(BERT or BiLSTM), k ∈[1,N] and his the hid-\nden size, and N is the text length. Note that for a\ngiven aspect, there may existMaspect mentions re-\nferring to the same aspect in the text. Also, each as-\npect mention could contain more than one word. To\nease aspect-level representation in the later stage,\nwe choose to collapse each aspect mention as a sin-\ngle word. The summation of the representations of\neach constituent word within the mention works as\nits hidden representation. We also develop a span\nset spanwith the size Ns. Each span records the\nstart and end position of the given aspect. spanj\ndenotes the j-th aspect span in original text. Note\nthat for non-aspect words, spans involved in the\ncomputation are their original positions with the\nlength as one.\nsj = SUM([hspanj ]), (8)\nwhere j ∈[1,Ns], Ns <= N denotes the number\nof words after aspect-based sum operation. sj is\nthe j-th output of the aspect-based encoder layer.\nThis process can be illuminated by an example\ntransforming ‘It has a bad memory but a great\nbattery life’ to ‘It has a bad memory but a great\n[battery life]’.N is ten and Ns is nine in this case.\n4.2 Dual-transformer Structure\nAfter obtaining the contextual hidden representa-\ntions from the aspect-based encoder, we develop\na dual-transformer structure to fuse the ﬂat textual\nknowledge and dependency knowledge in a mu-\ntual reinforcement manner. Speciﬁcally, as demon-\nstrated in Figure 3, dual-transformer structure con-\nsists of a multi-layer Transformer and a multi-layer\nBiGCN.\nBidirectional GCN: We design a BiGCN by\nconsidering the direction of each edge in the depen-\ndency graph. Note that dependency graph is con-\nstructed on the word-level. Hence, similar to aspect-\nlevel representation performed in Section 4.1, we\nmerge the edges corresponding to the constituent\nword of the given aspect in the adjacent matrix,\nresulting in an aspect-level adjacent matrix. Then,\nwe derive the graph-based representations for the\ninput text as follows:\nQt\nout = Relu( 1\n|Aout\nadj|Aout\nadjQtWout), (9)\nQt\nin = Relu( 1\n|Ain\nadj|Ain\nadjQtWin), (10)\nQt+1 = Norm(Qt + Relu([Qt\nout\n,Qt\nin]WO + bO)), (11)\nQt+1 = BiGCN(Qt,Aout\nadj,Ain\nadj), (12)\nwhere Aout\nadj\nand Ain\nadj\nare outgoing and incoming\naspect-level adjacent matrices gathered from the de-\npendency graph respectively. Here, we concatenate\n6582\nthe representations of two directions to produce the\nﬁnal output in each iteration, while other similar\nmethods conduct the merging only in the last itera-\ntion. BiGCN represents Equations (9)-(11). We\nuse a simple method to merge the adjacent matrix\nof the words in the same aspect span as follows:\nA′\nadji = MIN(⃗1,SUM ([Aadjspani ])), (13)\nwhere Aadj can be replaced by Aout\nadj and Ain\nadj\n, and\nwe can thus get Aout\nadj\n′\nand Ain\nadj\n′\n. Each span records\nthe start and end position of the given aspect.spani\ndenotes the i-th span in original text.\nBiAfﬁne Module: Assume that there are two\ninputs S1 ∈Rn×h and S2 ∈Rn′×h, we introduce\na mutual BiAfﬁne transformation process to inter-\nchange their relevant features as follows:\nA1 = softmax(S1W1ST\n2 ), (14)\nA2 = softmax(S2W2ST\n1 ), (15)\nS′\n1 = A1S2, (16)\nS′\n2 = A2S1, (17)\nS′\n1,S′\n2 = Biaffine(S1,S2), (18)\nwhere W1,W2 ∈Rh×h. Here, S′\n1 can be consid-\nered as a projection from S2 to S1, and S′\n2 follows\nthe same principle. Biaffine represents Equa-\ntions (14)-(17). A1 and A2 are temporary align-\nment matrices projecting from S2 to S1 and S1 to\nS2, respectively.\nThe Whole Procedure: We can then assemble\nall the sub-modules mentioned above to construct\nour proposed dual-transformer structure, and the\ndetailed procedures are listed below:\nSTr ′\nt = Transfomer(STr\nt ), (19)\nSG′\nt = BiGCN(SG\nt ,Aout\nadj\n′,Ain\nadj\n′\n), (20)\nSTr ′′\nt ,SG′′\nt = Biaffine(STr ′\nt ,SG′\nt ), (21)\nSTr\nt+1 = Norm(STr ′\nt + STr ′′\nt ), (22)\nSG\nt+1 = Norm(SG′\nt + SG′′\nt ), (23)\nwhere STr\n0 = SG\n0 = H, and H ∈RNs×h denotes\nthe contextual hidden representations{s1,...}from\nthe aspect-based encoder. Transfomer repre-\nsents the process denoted by Equations (1)-(5).\nEquations (19)-(23) can be repeatedly calculated\nfor\nT times and t ∈[0,T]. We choose STr\nT (ﬂat\n(with graph) in Figure 3) as the last representation,\nbecause SG\nT (graph (with ﬂat) in Figure 3) heavily\ndepends on the dependency graph.\n4.3 Aspect-based Attention Module\nGiven M aspect representations can be obtained\nthrough the above mentioned procedure, we can de-\nrive the ﬁnal aspect representation by Max-Pooling\noperation. Here, we utilize an attention mechanism\nto identify relevant words with respect to the aspect.\nHowever, these would be\nM aspect representations\nwhich are all highly relevant to the aggregated as-\npect representation. To avoid that these aspect men-\ntions from being assigned with too high weight,\nwe utilize a mask mechanism to explicitly set the\nattention values of aspect mentions to zeros. Let I\nbe the index set of these M aspect mentions, we\nform Mask vector as follows:\nMaski =\n{\n−inf, if i∈I;\n0, if other. (24)\nWe then calculate the probability distribution pof\nthe sentiment polarity as follows:\nhf = MaxPooling([STr\nT i|i∈I]), (25)\naf = softmax(hf W3STr\nT\nT\n+ Mask), (26)\nh′f = Relu([hf ,af STr\nT ]W′+ b′), (27)\np= softmax(h′f Wp + bp), (28)\nwhere W3,W′,Wp and b′,bp are learnable weights\nand biases, respectively.\n4.4 Loss Function\nThe proposed DGEDT is optimized by the stan-\ndard gradient descent algorithm with the cross-\nentropy loss and L2-regularization:\nLoss= −\n∑\n(d,yp)∈D\nlog(pyp) +λ||θ||2, (29)\nwhere D denotes the training dataset, yp is the\nground-truth label and pyp means the yp-th element\nof p. θrepresents all trainable parameters, and λis\nthe coefﬁcient of the regularization term.\n5 Experiments\n5.1 Datasets\nOur experiments are conducted on ﬁve datasets,\nincluding one (Twitter) which is originally built\nby Dong et al. (2014), and the other four datasets\n(Lap14, Rest 14, Rest 15, Rest16) are respectively\nfrom SemEval 2014 task 4 (Pontiki et al., 2014),\nSemEval 2015 task 12 (Pontiki et al.,2015) and Se-\nmEval 2016 task 5 (Hercig et al., 2016), consisting\n6583\nDataset Category Pos Neu Neg\nTwitter Train 1561 3127 1560\nTest 173 346 173\nLap14 Train 994 464 870\nTest 341 169 128\nRest14 Train 2164 637 807\nTest 728 196 196\nRest15 Train 912 36 256\nTest 326 34 182\nRest16 Train 1240 69 439\nTest 469 30 117\nTable 1: Detailed statistics of ﬁve datasets in our exper-\niments.\nof data from two categories: laptop and restaurant.\nThe statistics of datasets are demonstrated in Ta-\nble 1.\n5.2 Experiment Setup\nWe compare the proposed DGEDT ∗with a line of\nbaselines and state-of-the-art alternatives, includ-\ning LSTM, MemNet (Tang et al., 2016b), AOA\n(Huang et al., 2018), IAN (Ma et al., 2017), TNet-\nLF (Li et al., 2018), CAPSNet (Chen and Qian,\n2019), Transfer-CAPS (Chen and Qian, 2019), TG-\nBERT (Gao et al., 2019), AS-CNN (Zhang et al.,\n2019) and AS-GCN (Zhang et al., 2019). We con-\nduct the experiments with our proposed DGEDT\nwith BiLSTM as the aspect-based encoder, and\nDGEDT +BERT with BERT as the aspect-based\nencoder. Several simpliﬁed variants of DGEDT\nare also investigated: DGEDT(Transformer) de-\nnotes that we keep standard Transformer and re-\nmove the BiGCN part, DGEDT(BiGCN) denotes\nthat we keep BiGCN and remove the Transformer\npart. The layer number or iteration number (i.e.,\nT) of all available models is set to three for both\nTransformer and GCN. We use Spacy toolkit†to\ngenerate dependency trees.\n5.3 Parameter Settings\nWe use BERT-base English version (Devlin et al.,\n2019), which contains 12 hidden layers and 768\nhidden units for each layer. We use Adam (Kingma\nand Ba, 2014) as the optimizer for BERT and our\nmodel with the learning rate initialized by 0.00001\nand 0.001 respectively, and decay rate of learning\nis set as 0.98. Except for the inﬂuence of decay\nrate, the learning rate decreases dynamically ac-\ncording to the current step number. Batch shufﬂing\n∗available at https://github.com/tomsonsgs/DGEDT-senti-\nmaster.\n†available at https://spacy.io/\nis applied to the training set. The hidden size of\nour basic BiLSTM is 256 and the size of all em-\nbeddings is set as 100. The vocab size of BERT\nis 30,522. The batch size of all model is set as\n32. As for regularization, dropout function is ap-\nplied to word embeddings and the dropout rate is\nset as 0.3. Besides, the coefﬁcient\nλ for the L2-\nnorm regularization is set as 0.0001. We train our\nmodel up to 50 epochs and conduct the same ex-\nperiment for 10 times with random initialization.\nAccuracy and Macro-Averaged F1 are adopted as\nthe evaluation metrics. We follow the experimental\nsetup in (Zhang et al., 2019; Chen and Qian, 2019)\nand report the average maximum value for all met-\nrics on testing set. If the model is not equipped\nwith BERT, then we use word vectors that were\npre-trained from Glove (Pennington et al., 2014).\n5.4 Overall Results\nAs shown in Table 2, our model DGEDT out-\nperforms all other alternatives on all ﬁve dataset.\nBERT makes further improvement on the per-\nformance especially in Twitter, Rest14 and Rest\n15. We can conclude that traditional Trans-\nformer DGEDT(Transformer) obtains better perfor-\nmance than DGEDT(BiGCN) in the most datasets.\nDGEDT employs and combines two sub-modules\n(traditional Transformer and dependency graph\nenhanced GCN) and outperforms any single sub-\nmodule. Using dependency tree indeed contributes\nto the performance when acting as a supplement\nrather than a single decisive module.\n5.5 Ablation Study\nNote that the performance of individual modules\nis already reported in Table 2. As shown in Ta-\nble 3, we investigate and report four typical abla-\ntion conditions. ‘–Mask’ denotes that we remove\nthe aspect-based attention mask mechanism, and\n‘–MultiAspect’ denotes that we only use the as-\npect representation of the ﬁrst aspect mention in-\nstead of MaxPooling them. We can see that these\ntwo procedures provide slight improvement. ‘–\nBiGCN(+GCN)’ means that we remove the bidi-\nrectional connection and only use original GCN,\nthe results show that bidirectional GCN outper-\nforms original GCN owing to the adequate con-\nnection information. ‘–BiAfﬁne’ indicates that\nwe remove the BiAfﬁne process and use all the\noutputs of dual-transformer structure, we can thus\nconclude that BiAfﬁne process is critical for our\nmodel, and utilizing simple concatenation of the\n6584\nModel Twitter Lap14 Rest14 Rest15 Rest16\nAcc F1 Acc F1 Acc F1 Acc F1 Acc F1\nLSTM 69.6 67.7 69.3 63.1 78.1 67.5 77.4 55.2 86.8 63.9\nMemNet 71.5 69.9 70.6 65.2 79.6 69.6 77.3 58.3 85.4 66.0\nAOA 72.3 70.2 72.6 67.5 80.0 70.4 78.2 57.0 87.5 66.2\nIAN 72.5 70.8 72.1 67.4 79.3 70.1 78.6 52.7 84.7 55.2\nTNet 73.0 71.4 74.6 70.1 80.4 71.0 78.5 59.5 89.1 70.4\nAS-CNN 71.1 69.5 72.6 66.7 81.7 73.1 78.5 58.9 87.4 64.6\nCAPSNet – – 72.7 68.8 78.8 69.7 – – – –\nTransfer-CAPS – – 73.9 70.2 79.3 70.9 – – – –\nAS-GCN 72.2 70.4 75.6 71.1 80.8 72.0 79.9 61.9 89.0 67.5\nDGEDT(Transformer) 74.1 72.7 76.0 71.4 82.8 73.9 81.0 64.9 90.0 72.6\nDGEDT(BiGCN) 72.8 71.0 76.2 71.8 81.8 72.5 80.4 62.9 89.4 70.4\nDGEDT 74.8 73.4 76.8 72.3 83.9 75.1 82.1 65.9 90.8 73.8\nTG-BERT 76.7 74.3 78.9 74.4 85.1 78.4 – – – –\nDGEDT-BERT 77.9 75.4 79.8 75.6 86.3 80.0 84.0 71.0 91.9 79.0\nTable 2: Overall performance of accuracy and F1 on ﬁve datasets, AS means aspect-based.\nAblation Twitter\nLap14 Rest14 Rest15 Rest16\nAcc Acc\nAcc Acc Acc\nDGEDT 74.8 76.8\n83.9 82.1 90.8\n–Mask 74.5 76.7 83.5 82.0 90.5\n–MultiAspect 74.5 76.4 83.4 81.8 90.4\n–BiGCN\n(+GCN) 74.3 76.2 83.2 81.4 90.2\n–BiAfﬁne 73.0 75.4 82.4 81.0 89.6\nTable 3: Overall ablation results of accuracy on ﬁve\ndatasets.\n(a) Lap14 Dataset.\n (b) Rest14 Dataset.\nFigure 4: A demonstration of accuracy-T curves on\nLap14 and Rest 14 datasets respectively: T is the it-\neration number.\noutputs of Transformer and BiGCN is worse than\nDGEDT(Transformer).\n5.6 Impact of Iteration Number\nAs shown in Figure 4, we ﬁnd that three is the best\niteration number for Lap14 and Rest14. Depen-\ndency information will not be fully broadcasted\nwhen the iteration number is too small. The model\nwill suffer from over-ﬁtting and redundant informa-\ntion passing, which results in the performance drop\nwhen iteration number is too large. So, numerous\nexperiments need to be conducted to ﬁgure out a\nproper iteration number.\n5.7 Case Study and Attention Distribution\nExploration\nAs shown in Figure 5, DGEDT and\nDGEDT(BiGCN) output correct prediction\nNegative while DGEDT(Transformer) fails for\nthe sentence The management was less than\naccommodating. To ﬁgure out the essential cause,\nwe demonstrate the attention of self alignment\nin Figure 5. We can see that for the aspect man-\nagement, DGEDT(Transformer) mainly focuses\non accommodating, which is a positive word at\ndocument level. Thus, DGEDT(Transformer)\nobtains an incorrect prediction Positive. In the\ndependency tree, less which is often regarded as a\nnegative word has a more related connection with\naspect management, so DGEDT(BiGCN) outputs\nright sentiment Negative. With the assistance of\nsupplementary dependency graph, DGEDT also\nobtains right prediction Negative owing to the high\nattention value between management and less.\nAs shown in Figure 6, DGEDT and\nDGEDT(Transformer) output correct predic-\ntion Positive while DGEDT(BiGCN) fails for the\nsentence This little place is wonderfully warm\nwelcoming. To ﬁgure out the essential cause, we\ndemonstrate the attention of self alignment and\ndependency tree in Figure 6. We can see that for\nthe aspect place, DGEDT(Transformer) mainly\nfocuses on wonderfully, which is a positive word\nat document level. Thus, DGEDT(Transformer)\nobtains a correct prediction Positive. In the\ndependency tree, little which is often regarded as\na negative word has a more related connection\nwith aspect place, so DGEDT(BiGCN) outputs\nincorrect sentiment Negative. With the disturbance\nof inappropriate dependency tree, DGEDT still\n6585\nAspect: management\nGolden: Negative\nDGEDT(Transformer): Positive\nDGEDT(BiGCN): Negative\nDGEDT: Negative\n(a) The attention matrix of self alignment by\nDGEDT(Transformer).\n (b) The attention matrix of self alignment by DGEDT.\nFigure 5: Case Study 1: A testing example demonstrates that the information of dependency tree contributes\nto the classiﬁcation performance, our dual-transformer model generates a proper attention distribution with the\nassistance of dependency tree. Darker cell color indicates higher attention value, the aspect is management and\ngolden sentiment is Negative.\nAspect: place\nGolden: Positive\nDGEDT(Transformer): Positive\nDGEDT(BiGCN): Negative\nDGEDT: Positive\n(a) The attention matrix of self alignment by\nDGEDT(Transformer).\n (b) The attention matrix of self alignment by DGEDT.\nFigure 6: Case Study 2: A testing example demonstrates that the information of dependency tree may be harmful\nfor the classiﬁcation performance, and our dual-transformer model still obtains a proper attention distribution.\nDarker cell color indicates higher attention value, the aspect is place and golden sentiment is Positive.\n6586\nobtains right prediction Positive owing to the high\nattention value between place and wonderfully.\nWe can see from two examples above that\nDGEDT is capable of achieving the proper bal-\nance between dependency graph enhanced BiGCN\nand traditional Transformer according to different\nsituations.\n6 Conclusion\nRecently neural structures with syntactical infor-\nmation such as semantic dependency tree and con-\nstituent tree are widely employed to enhance the\nword-level representation of traditional neural net-\nworks. These structures are often modeled and\ndescribed by TreeLSTMs or GCNs. To introduce\nTransformer into our task and diminish the error\ninduced by incorrect dependency trees, we propose\na dual-transformer structure which considers the\nconnections in dependency tree as a supplementary\nGCN module and a Transformer-like structure for\nself alignment in traditional Transformer. The re-\nsults on ﬁve datasets demonstrate that dependency\ntree indeed promotes the ﬁnal performance when\nutilized as a sub-module for dual-transformer struc-\nture.\nIn future work, we can further improve our\nmethod in the following aspects. First, the edge\ninformation of the dependency trees needs to be\nexploited in later work. We plan to employ an edge-\naware graph neural network considering the edge\nlabels. Second and last, domain-speciﬁc knowl-\nedge can be incorporated into our method as an\nexternal learning source.\nAcknowledgments\nWe thank the reviewers for their valuable com-\nments. This work is supported through the\ngrants from National Natural Science Founda-\ntion of China (NSFC-61772378), the National\nKey research and Development Program of China\n(No.2017YFC1200500) and the Major Projects of\nthe National Social Science Foundation of China\n(No.11&ZD189).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nGiuseppe Castellucci, Simone Filice, Danilo Croce,\nand Roberto Basili. 2014. UNITOR: aspect based\nsentiment analysis with structured learning. In\nProceedings of the 8th International Workshop\non Semantic Evaluation, SemEval@COLING 2014,\nDublin, Ireland, August 23-24, 2014, pages 761–767.\nThe Association for Computer Linguistics.\nZhuang Chen and Tieyun Qian. 2019. Transfer cap-\nsule network for aspect level sentiment classiﬁcation.\nIn Proceedings of the 57th Conference of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 547–556. Association for Com-\nputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming\nZhou, and Ke Xu. 2014. Adaptive recursive neural\nnetwork for target-dependent twitter sentiment clas-\nsiﬁcation. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2014, June 22-27, 2014, Baltimore, MD, USA,\nVolume 2: Short Papers, pages 49–54. The Associa-\ntion for Computer Linguistics.\nZhengjie Gao, Ao Feng, Xinyu Song, and Xi Wu.\n2019. Target-dependent sentiment classiﬁcation\nwith BERT. IEEE Access, 7:154290–154299.\nTom’avs Hercig, Tom´as Brychc´ın, Luk´as Svoboda, and\nMichal Konkol. 2016. UWB at semeval-2016 task 5:\nAspect based sentiment analysis. In Proceedings of\nthe 10th International Workshop on Semantic Evalu-\nation, SemEval@NAACL-HLT 2016, San Diego, CA,\nUSA, June 16-17, 2016, pages 342–349. The Associ-\nation for Computer Linguistics.\nS Hochreiter and J Schmidhuber. 1997. Long short-\nterm memory. Neural Computation, 9(8):1735–\n1780.\nBinxuan Huang, Yanglan Ou, and Kathleen M. Car-\nley. 2018. Aspect level sentiment classiﬁcation with\nattention-over-attention neural networks. In Social,\nCultural, and Behavioral Modeling - 11th Interna-\ntional Conference, SBP-BRiMS 2018, Washington,\nDC, USA, July 10-13, 2018, Proceedings, volume\n10899 of Lecture Notes in Computer Science, pages\n197–206. Springer.\nRie Johnson and Tong Zhang. 2015. Semi-supervised\nconvolutional neural networks for text categoriza-\ntion via region embedding. In Advances in Neu-\nral Information Processing Systems 28: Annual\n6587\nConference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec,\nCanada, pages 919–927.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL, pages 1746–1751. ACL.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text\nclassiﬁcation. In Proceedings of the Twenty-Ninth\nAAAI Conference on Artiﬁcial Intelligence, January\n25-30, 2015, Austin, Texas, USA, pages 2267–2273.\nAAAI Press.\nXin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018.\nTransformation networks for target-oriented senti-\nment classiﬁcation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 946–\n956. Association for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015, pages 1412–1421. The\nAssociation for Computational Linguistics.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017. Interactive attention networks for\naspect-level sentiment classiﬁcation. In Proceed-\nings of the Twenty-Sixth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI 2017, Mel-\nbourne, Australia, August 19-25, 2017, pages 4068–\n4074. ijcai.org.\nDiego Marcheggiani and Ivan Titov. 2017. Encoding\nsentences with graph convolutional networks for se-\nmantic role labeling. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2017, Copenhagen, Den-\nmark, September 9-11, 2017, pages 1506–1515. As-\nsociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2014, October 25-29, 2014,\nDoha, Qatar, A meeting of SIGDAT, a Special Inter-\nest Group of the ACL, pages 1532–1543. ACL.\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemeval-2015 task 12: Aspect based sentiment anal-\nysis. In Proceedings of the 9th International Work-\nshop on Semantic Evaluation, SemEval@NAACL-\nHLT 2015, Denver, Colorado, USA, June 4-5, 2015,\npages 486–495. The Association for Computer Lin-\nguistics.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos,\nHarris Papageorgiou, Ion Androutsopoulos, and\nSuresh Manandhar. 2014. Semeval-2014 task 4: As-\npect based sentiment analysis. In Proceedings of the\n8th International Workshop on Semantic Evaluation,\nSemEval@COLING 2014, Dublin, Ireland, August\n23-24, 2014, pages 27–35. The Association for Com-\nputer Linguistics.\nMike Schuster and Kuldip K. Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE Trans. Sig-\nnal Processing, 45(11):2673–2681.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Uti-\nlizing BERT for aspect-based sentiment analysis via\nconstructing auxiliary sentence. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 380–385. Associa-\ntion for Computational Linguistics.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Nat-\nural Language Processing of the Asian Federation\nof Natural Language Processing, ACL 2015, July\n26-31, 2015, Beijing, China, Volume 1: Long Pa-\npers, pages 1556–1566. The Association for Com-\nputer Linguistics.\nDuyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.\n2016a. Effective lstms for target-dependent senti-\nment classiﬁcation. In COLING 2016, 26th Inter-\nnational Conference on Computational Linguistics,\nProceedings of the Conference: Technical Papers,\nDecember 11-16, 2016, Osaka, Japan, pages 3298–\n3307. ACL.\nDuyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect\nlevel sentiment classiﬁcation with deep memory net-\nwork. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2016, Austin, Texas, USA, November 1-4,\n2016, pages 214–224. The Association for Compu-\ntational Linguistics.\nShikhar Vashishth, Shib Sankar Dasgupta,\nSwayambhu Nath Ray, and Partha P. Talukdar.\n6588\n2018. Dating documents using graph convolution\nnetworks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 1605–\n1615. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nDuy-Tin V o and Yue Zhang. 2015. Target-dependent\ntwitter sentiment classiﬁcation with rich automatic\nfeatures. In Proceedings of the Twenty-Fourth Inter-\nnational Joint Conference on Artiﬁcial Intelligence,\nIJCAI 2015, Buenos Aires, Argentina, July 25-31,\n2015, pages 1347–1353. AAAI Press.\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and\nLi Zhao. 2016. Attention-based LSTM for aspect-\nlevel sentiment classiﬁcation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2016, Austin, Texas,\nUSA, November 1-4, 2016, pages 606–615. The As-\nsociation for Computational Linguistics.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun\nCho, Aaron C. Courville, Ruslan Salakhutdinov,\nRichard S. Zemel, and Yoshua Bengio. 2015. Show,\nattend and tell: Neural image caption generation\nwith visual attention. In Proceedings of the 32nd In-\nternational Conference on Machine Learning, ICML\n2015, Lille, France, 6-11 July 2015, volume 37\nof JMLR Workshop and Conference Proceedings,\npages 2048–2057. JMLR.org.\nWei Xue and Tao Li. 2018. Aspect based sentiment\nanalysis with gated convolutional networks. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2018, Mel-\nbourne, Australia, July 15-20, 2018, Volume 1: Long\nPapers, pages 2514–2523. Association for Computa-\ntional Linguistics.\nMin Yang, Wenting Tu, Jingxuan Wang, Fei Xu, and\nXiaojun Chen. 2017. Attention based LSTM for tar-\nget dependent sentiment classiﬁcation. In Proceed-\nings of the Thirty-First AAAI Conference on Artiﬁ-\ncial Intelligence, February 4-9, 2017, San Francisco,\nCalifornia, USA, pages 5013–5014. AAAI Press.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nGraph convolutional networks for text classiﬁcation.\nIn The Thirty-Third AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2019, The Thirty-First Innova-\ntive Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2019, The Ninth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 7370–7377. AAAI Press.\nJiangfeng Zeng, Xiao Ma, and Ke Zhou. 2019.Enhanc-\ning attention-based LSTM with position context for\naspect-level sentiment classiﬁcation. IEEE Access,\n7:20462–20471.\nChen Zhang, Qiuchi Li, and Dawei Song. 2019.\nAspect-based sentiment classiﬁcation with aspect-\nspeciﬁc graph convolutional networks. CoRR,\nabs/1909.03477.\nYue Zhang and Jiangming Liu. 2017. Attention mod-\neling for targeted sentiment. In Proceedings of the\n15th Conference of the European Chapter of the\nAssociation for Computational Linguistics, EACL\n2017, Valencia, Spain, April 3-7, 2017, Volume 2:\nShort Papers, pages 572–577. Association for Com-\nputational Linguistics.\nYuhao Zhang, Peng Qi, and Christopher D. Manning.\n2018. Graph convolution over pruned dependency\ntrees improves relation extraction. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, Brussels, Belgium, Octo-\nber 31 - November 4, 2018, pages 2205–2215. Asso-\nciation for Computational Linguistics."
}