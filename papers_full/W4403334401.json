{
  "title": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs",
  "url": "https://openalex.org/W4403334401",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5085867822",
      "name": "Wanli Qian",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5078227948",
      "name": "Chenfeng Gao",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5038950679",
      "name": "Anup Sathya",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5103097262",
      "name": "Ryo Suzuki",
      "affiliations": [
        "University of Calgary",
        "University of Colorado Boulder"
      ]
    },
    {
      "id": "https://openalex.org/A5016319946",
      "name": "Ken Nakagaki",
      "affiliations": [
        "University of Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385714563",
    "https://openalex.org/W4213449333",
    "https://openalex.org/W1980950036",
    "https://openalex.org/W2145826504",
    "https://openalex.org/W2135643110",
    "https://openalex.org/W2295685962",
    "https://openalex.org/W1986841213",
    "https://openalex.org/W2347183436",
    "https://openalex.org/W4391210549",
    "https://openalex.org/W4396833067",
    "https://openalex.org/W2402656541",
    "https://openalex.org/W2798949618",
    "https://openalex.org/W2143131345",
    "https://openalex.org/W2294959121",
    "https://openalex.org/W2028187927",
    "https://openalex.org/W2082727067",
    "https://openalex.org/W2033597984",
    "https://openalex.org/W2396345583",
    "https://openalex.org/W3162898761",
    "https://openalex.org/W4321162272",
    "https://openalex.org/W4238155858",
    "https://openalex.org/W2015637289",
    "https://openalex.org/W2079512759",
    "https://openalex.org/W2130615539",
    "https://openalex.org/W6910018007",
    "https://openalex.org/W4313041309",
    "https://openalex.org/W4294959213",
    "https://openalex.org/W2921465830",
    "https://openalex.org/W2624397982",
    "https://openalex.org/W2408357557",
    "https://openalex.org/W2787712888",
    "https://openalex.org/W2403260819",
    "https://openalex.org/W2796014682",
    "https://openalex.org/W2981649310",
    "https://openalex.org/W2535447426",
    "https://openalex.org/W2897263022",
    "https://openalex.org/W2142304282",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W2010166572",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W4307472388",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W2230143564"
  ],
  "abstract": "This paper introduces text-to-shape-display, a novel approach to generating\\ndynamic shape changes in pin-based shape displays through natural language\\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\\napproach allows users to author shape-changing behaviors on demand through text\\nprompts without programming. We describe the foundational aspects necessary for\\nsuch a system, including the identification of key generative elements\\n(primitive, animation, and interaction) and design requirements to enhance user\\ninteraction, based on formative exploration and iterative design processes.\\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\\n24 x 24 shape display, which translates the user's textual command into\\nexecutable code and allows for quick exploration through a web-based control\\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\\nhighlight the ability to facilitate rapid ideation of a wide range of\\nshape-changing behaviors with AI. However, the findings also expose\\naccuracy-related challenges and limitations, prompting further exploration into\\nrefining the framework for leveraging AI to better suit the unique requirements\\nof shape-changing systems.\\n",
  "full_text": "SHAPE-IT: Exploring Text-to-Shape-Display for Generative\nShape-Changing Behaviors with LLMs\nWanli Qian∗\nUniversity of Chicago\nChicago, USA\nmichaelq@uchicago.edu\nChenfeng Gao∗\nUniversity of Chicago\nChicago, USA\njessegao7@uchicago.edu\nAnup Sathya\nUniversity of Chicago\nChicago, USA\nanups@uchicago.edu\nRyo Suzuki\nUniversity of Colorado Boulder\nBoulder, USA\nUniversity of Calgary\nCalgary, Canada\nryo.suzuki@colorado.edu\nKen Nakagaki\nUniversity of Chicago\nChicago, USA\nknakagaki@uchicago.edu\nFigure 1: SHAPE-IT lets users to input text to author shape displays. A: SHAPE-IT’s GUI. B: a user typing prompts. C: example\noutcome from our study participant for movement, D: another example for interaction.\nABSTRACT\nThis paper introduces text-to-shape-display, a novel approach\nto generating dynamic shape changes in pin-based shape displays\nthrough natural language commands. By leveraging large language\nmodels (LLMs) and AI-chaining, our approach allows users to au-\nthor shape-changing behaviors on demand through text prompts\nwithout programming. We describe the foundational aspects neces-\nsary for such a system, including the identification of key generative\n∗Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0628-8/24/10. . . $15.00\nhttps://doi.org/10.1145/3654777.3676348\nelements (primitive, animation, and interaction) and design require-\nments to enhance user interaction, based on formative exploration\nand iterative design processes. Based on these insights, we develop\nSHAPE-IT, an LLM-based authoring tool for a 24 x 24 shape display,\nwhich translates the user’s textual command into executable code\nand allows for quick exploration through a web-based control in-\nterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study\nconclusions highlight the ability to facilitate rapid ideation of a wide\nrange of shape-changing behaviors with AI. However, the findings\nalso expose accuracy-related challenges and limitations, prompting\nfurther exploration into refining the framework for leveraging AI\nto better suit the unique requirements of shape-changing systems.\nCCS CONCEPTS\n• Human-centered computing →Interaction devices.\nKEYWORDS\nShape Display, Text-based Authoring, LLMs, Code-Generation\narXiv:2409.06205v1  [cs.HC]  10 Sep 2024\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\nACM Reference Format:\nWanli Qian, Chenfeng Gao, Anup Sathya, Ryo Suzuki, and Ken Naka-\ngaki. 2024. SHAPE-IT: Exploring Text-to-Shape-Display for Generative\nShape-Changing Behaviors with LLMs. In The 37th Annual ACM Sym-\nposium on User Interface Software and Technology (UIST ’24), October 13–\n16, 2024, Pittsburgh, PA, USA. ACM, New York, NY, USA, 29 pages. https:\n//doi.org/10.1145/3654777.3676348\n1 INTRODUCTION\nWhat if our physical environment could dynamically transform\nthrough natural language commands? Imagine a shape-changing ta-\nble that brings a pen or generates a bookstand by simply asking for\nit or a floor that transforms itself into a chair or furniture just by say-\ning so. While such ideas have been partially illustrated in research\nvisions [21, 23] or demonstrations [ 63] to create shape changes\nbased on user intent, most of these dynamic shape configurations\ncurrently rely on pre-programmed behaviors. How can we create\nthese dynamic shape changes on-demand without programming?\nAs a first step towards this vision, this paper explores text-\nto-shape-display, the concept of generating dynamic shapes for\npin-based shape displays through natural language commands by\nleveraging large language models (LLMs). The underlying idea is\nto employ LLMs to generate executable code that controls each pin\nof the shape display, allowing users to quickly generate a dynamic\nshape by simply asking for it. Although the underlying concept\nis simple, several important questions still remain, such as • what\ncommon elements text-to-shape-display systems need to gener-\nate, • what limitations exist when applying basic LLM-based code\ngeneration to program shape displays, and • how to design such\na system from a user-centered perspective. To address these ques-\ntions, this paper makes three key contributions: 1) identification of\nthree key generative elements that emerged from formative explo-\nration, 2) system requirements and design guidelines identified via\nan iterative design process, and 3) a system design and evaluation\nof a proof-of-concept prototype.\nFirst, we identify Primitive , Animation , and Interaction as the\nkey elements for the design of text-to-shape-displays. To identify\nthis, we conducted a formative exploration in which we collected\n49 representative shape display video clips and recruited 100 crowd-\nsourced participants to investigate how end-users might command\nthe generation of various shape display outputs. Our findings re-\nveal that typical shape display outputs described by users consist of\nthree main components: 1) Primitive which defines base geometry,\n2) Animation which describes the movement of primitive objects,\nand 3) Interaction which specifies how user inputs influence primi-\ntive objects. Given this result, we propose that text-to-shape-display\nsystems should focus on generating these elements to support a\nbroad spectrum of shape-changing behaviors.\nSecond, we identified several design requirements for text-to-\nshape display, including support for iterative refinement through\nstep-by-step authoring, transparency regarding non-deterministic\nLLM results, and support for real-time exploration via parame-\nter manipulation. These guidelines were established through the\ndevelopment and evaluation of an early prototype, in which we\ndeveloped a simple LLM-based shape display authoring system,\nand then conducted a pilot study with ten participants.\nBased on these insights, we developed SHAPE-IT, the first text-to-\nshape-display system that can control a 24 x 24 pin-based shape dis-\nplay based on text instructions. Our system leverages AI-chaining,\nwhich connects multiple AI models or tasks in sequence to perform\ncomplex operations by leveraging the strengths of each compo-\nnent.. First, the prompt-helper module interprets user input and\ntranslates it into three key components: Primitive , Animation ,\nand Interaction . Second, the code-generating modules take the re-\nsult of these three decomposed inputs and then generate executable\ncode for each element. This AI-chaining architecture improves the\nreliability of code generation through explicit interpretation of user\nintents. In addition, our frontend interface with a conversational UI\nand shape display simulator allows users to quickly review and mod-\nify the shape display behaviors through parameter manipulation\nor iterative command execution.\nWe evaluate our system through two methods: 1) performance\nevaluation and 2) user evaluation. First, we measure the perfor-\nmance of our system, which results in 82% compilation success rate,\nbased on 50 random samples gathered from our crowdsourcing\nstudy. Second, we invited 10 participants to attend a user study,\nwhich included a series of design tasks using our system. We col-\nlected qualitative data with semi-structured interviews. Partici-\npants’ feedback validates the usability and capability of our system.\nTheir creative shape-changing interaction design indicates that\nSHAPE-IT could facilitate rapid ideation of tangible interaction.\nWe also uncovered the limitations of our system, learned the need\nfor the enhancement of human-AI collaboration in tangible author-\ning, and the generation of interpretable results. Based on these\ninsights, we discuss how future research could address these issues\nto broaden this concept to broader shape-changing interfaces.\nFinally, our contributions include:\n•Requirements, and guidelines for designing text-to-shape\ndisplays through formative exploration and iterative devel-\nopment.\n•SHAPE-IT 1, the first LLM-based authoring tool for shape\ndisplays uses AI-Chaining, which connects multiple AI mod-\nels or tasks in sequence to perform complex operations by\nleveraging the strengths of each component, enabling the\nauthoring of dynamic shapes, motions, and interactions.\n•Evaluation results and insights highlighting the benefits and\nchallenges of integrating LLMs with shape displays.\n2 RELATED WORK\n2.1 Authoring and control methods for\npin-based Shape Display\nTo allow lay users to flexibly and intuitively configure shapes, mo-\ntion, and interaction on pin-based shape displays, researchers have\nexplored various software and hardware implementations aimed\nat empowering users to design and configure tangible shapes and\nmotions on pin-based shape displays. These approaches include\npre-planning GUI [14, 26, 65], gestural interactions [6, 16, 16, 22, 34],\nand tangible instruction based on touch or ‘scanning’ physical ob-\njects [34, 42, 43]. SHAPE-IT enhances the process of designing for\npin-based shape displays by enabling users to input instructions in\n1Our project code: https://github.com/AxLab-UofC/SHAPE-IT\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nnatural language, either by typing or speaking. Unlike traditional\nmethods that are confined topredetermined options, our system uses\nLarge Language Models (LLMs) to convert these instructions into\ncode, thereby controlling the display’s behavior. This innovation\nsignificantly broadens the scope of possible behaviors, moving be-\nyond fixed options and allowing for limitless creativity in shaping\ndisplay outcomes.\n2.2 Text Authoring and Generative AI in HCI\nResearch in using text input to control and author computing sys-\ntems is a fundamental research realm in HCI, and it is now part of\nour everyday computing interface, for example, voice assistants [50].\nNatural language input is one of the most accessible methods for\nuser interaction, and it has great potential for users to achieve\ncomplex task goals without needing to remember specific UI ele-\nments or buttons [32], or even programming [5]. Text input has\nbeen employed in various applications, often combined with other\nmodalities, such as image editing [32] general point-and-speech UI\ninteraction [7], and data visualization [56]. The accessibility and\nexpressiveness of natural language make it a powerful tool for users\nto communicate their intentions and achieve desired outcomes in\ninteractive systems [49], as well as to create and modify interactive\nMixed Reality experiences in real-time [15].\nThe recent mind-blowing advancements in generative AI using\ntext inputs to generate content have opened up new possibilities\nfor user interaction systems. These tools, which include text-based\n(ChaptGPT [47], Claude[3]), image-based (DALL-E [46]), and even\nvideo-based models (Sora [ 9]) are becoming publically available\nand used world-wide.\nFollowing this trend, in the domain of HCI, LLM-based AI gen-\neration systems actively being investigated to design novel user\ninteraction harnessing the capability of translating natural language\ndescriptions into generative outputs, including data visualization\nauthoring [56], interactive diagrams [27], UI prototyping [33], writ-\ning assistance [67], and end-user programming [61], creative cod-\ning [2], and code generation [28, 62]. To our knowledge, though\nemploying LLM to generate 3D digital models has been explored in\nthe graphics community[38, 41, 45], translating text into outputs\nfor physical shape-changing devices has not been explored. We find\ngreat research opportunities and potential to bridge the realms of\nhuman-generative-AI interaction and shape-changing interfaces to\ncreate a new thread of research that turns natural language into\ntangible, dynamic, and responsive shapes.\nOur work advances LLM-based code generation by facilitating\ndynamic script creation for shape-altering displays and addressing\nchallenges in Generative AI systems. We tackle the discrepancy\nbetween user text instructions and the formulation of effective\nprompts for code generation. As highlighted by “Why Johnny Can’t\nPrompt” [66], not all users can craft prompts that elicit the desired\nresponse from AI systems. Further, Khalid, et al. [40] suggest that\nstructured prompts, akin to pseudo-code, enhance code generation\naccuracy—a principle applicable to text-to-shape display systems\nrequiring precise interpretation of user intent. Despite the growing\npopularity of AI in code generation, achieving flawless outcomes\nremains elusive [ 17], underscoring the need to minimize errors\nin such systems. Drawing inspiration from the concept of LLM-\nChaining and its potential to improve complex systems [ 64], we\nintroduce an architecture that preprocesses user commands via\nLLM-chaining. This approach converts user inputs into structured\ncode instructions based on user intent, thereby enhancing the ac-\ncuracy of subsequent code generation.\n3 FORMATIVE EXPLORATION\nThis section describes our formative exploration aimed at informing\nthe design of text-to-shape-display systems. To understand how\nusers construct commands for creating shape-changing behaviors,\nwe conducted a crowdsourced elicitation study. First, we compiled\nrepresentative video clips of shape displays (N = 49) from previ-\nous research papers (N = 21). For each video collected, we asked\ncrowdsourced participants (N = 100) to provide a prompt that could\ngenerate the observed motion. By analyzing these user-generated\ncommands (N = 314), we identified common language patterns and\nkey elements of shape-changing behaviors, which will guide the\ndesign of our system.\n3.1 Method\nDataset. For the elicitation study, we collected 49 video clips that\nrepresent various behaviors of shape displays. The collection pro-\ncess was conducted in two steps: 1) searching for and collecting\nrelevant research papers, and 2) extracting diverse interactions\nfrom video demonstrations. First, we gathered existing research\non shape displays by conducting a manual keyword search for\n“pin-based shape display” in the ACM Digital Library, from which\nwe curated those that primarily focused on tangible information\ndisplays or shape-changing interactions, excluding works centered\non themes such as haptics and actuation techniques. After the\nselection process (see Appendix A), we compiled a set of 21 pa-\npers [13, 16, 18, 19, 22, 24–26, 34–36, 42–44, 52, 54, 55, 57–59, 63, 65]\nfor our video materials and we extracted short video clips from\nthese research videos. After assembling this pool of clips, we ap-\nplied several criteria to further filter the data (see Appendix A.2).\nThese criteria were designed to ensure the essential aspects of the\nshape-changing behaviors are captured and minimize the presence\nof external information that could potentially bias or influence\nparticipants’ decisions. This process resulted in a final set of 49 rep-\nresentative video clips2, each with an average length of 4.8 seconds.\nParticipants. Participants for this study were recruited through\nProlific, an online platform that provides academic researchers with\naccess to a diverse pool of participants. During recruitment, we\ndid not apply strict demographic criteria or target specific user\ngroups, with the exception of requiring English as the primary\nlanguage. This requirement ensured that participants could provide\ndetailed text inputs in English, which was essential for our study.\nWe recruited a total of 100 participants, compensating them at an\naverage rate of $12 per hour, which amounted to an average of\n$1.60 per participant. For detailed procedures, see Appendix A.2.\nTask. We asked each participant to generate a prompt for each of\nfour video clips. The task was divided into two parts: First, during\nthe introduction and training phase, participants were presented\n2A link to the playlist can be found: study materials\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\nFigure 2: Primitive, Animation, and Interaction: Fundamental shapes, motion behaviors, and user-controlled triggers for\ntext-to-shape-display approach learned from prior pin-based shape display research.\nwith a video (a heart shape scaling up and moving from the top right\nto the middle in the TRANSFORM [ 63] video) and two example\nprompts to ensure they understood that the task was to provide\ninstructions rather than mere descriptions. Second, in the main\ntask, participants viewed four randomly selected video clips from\nthe curated 49 videos. They were then asked to type their own\ninstructions for each clip. From 100 participants, a total of 314\nvalid3 user instructions were collected from 400 raw data points.\nCoding Analysis. Given the 314 user-generated prompts, two of\nthe authors conducted a thorough thematic analysis [ 8] to iden-\ntify commonly used language patterns and recurring themes. The\nanalysis began with an initial coding phase, where each author\n3prompts like “I don’t know how to describe” or it doesn’t focus on shape-changing\neffect are treated as invalid.\nindependently examined a subset of the prompts and assigned\npreliminary codes to capture the salient features and concepts ex-\npressed by the users. After several rounds of analysis, refinement,\nand discussion within the research team, we reached a consensus\non the final categorization scheme, which consisted of three main\ngenerative elements: primitive, animation, and interaction. Fig-\nure 2 illustrates a set of example videos, associated user-generated\nprompts, and color-coding of these key elements. In the follow-\ning sections, we describe each element in detail. These identified\nelements inform the design of our system, detailed in Section 5.\n3.2 Primitive\nWe observed that participants often referenced a specific ’object’\non the shape display to describe shape-changing effects. We define\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nPrimitive as the basic geometry of that constitutes target shape\ndisplay behavior, which can be either basic shapes, geometries, or\nmovement patterns. These primitives exclude added animations\nor interactions, considering them accessory features. Primitives\nmay include motion, such as a “wave” pattern. Categories of prim-\nitives identified are Basic Shapes (geometric shapes like circles,\nsquares, and triangles), Complex Shapes (more detailed forms like\nletters, icons, and landscapes), and Patterns or Basic Layouts\n(specified arrangements or effects, often described when precise\nshapes cannot be articulated). These serve as foundational elements\nfor constructing more intricate forms and behaviors.\n3.3 Animation\nAnimation elements are defined as enhancements to primitives,\naltering geometry parameters to achieve dynamic and continuous\nmotion. These elements introduce various motion behaviors to\nprimitives, including basic motion (e.g., translations like rising,\nfalling, and pulsating movements), transformations (shape or\nstate changes such as morphing and revealing), and speed and\ntiming (adjustments from fast to slow). Additionally, Repetition\nand Speed are crucial, with users frequently requesting repetitive\nor looping animations, dictating the rhythm and periodicity of the\nmotion. These animation features, applied atop primitives, facilitate\ndynamic shape transformations.\n3.4 Interaction\nThe Interaction element focuses on how users expect to interact\nwith and control the primitive shape or pattern to achieve the\ndesired shape-changing behaviors. Such as trigger: Users specify\nvarious triggers that initiate or influence the shape-changing effects,\nincluding physical touch, pressing on the shape displays, hovering\nabove the shape displays; mapping and control : Users describe\ndifferent mapping mechanisms for continuously interacting with\nthe shapes. This includes direct and indirect manipulation and\nparameterization. These interaction control techniques allow users\nto manipulate and customize the shape-changing effects by being\napplied to primitives.\n4 EARLY PROTOTYPE AND PILOT STUDY\nOur goal is to identify the challenges and potential benefits of\nan LLM-based text-to-shape-display system. To understand the\nuser experience of such a system, we developed an initial working\nprototype to conduct a pilot study, based on Buchenau’s experience\nprototyping protocol [ 10]. This helps us gain insights from the\nuser’s firsthand experiences by allowing them to directly engage\nwith functional systems.\n4.1 Early Prototype\nOur initial prototype consisted of two main components: 1) a fron-\ntend shape display simulator built with Unity, and 2) a backend\nLLM-based code generator built with Python. The backend code\ngenerator leverages the GPT-4 API to translate users’ textual in-\nstructions into executable Unity code. To ensure the generation of\nexecutable code, we employ few-shot prompting [51] techniques4.\n4Few-shot prompting in the context of Large Language Models (LLMs) involves pro-\nviding the model with a small set of examples (typically one to five) to guide it on a\nFor example, each prompt provides a collection of example com-\nmands, such as “create a heart shape” or “Created a ball shape and\na button to control toggling appear and disappear of the ball shape” ,\nalong with their corresponding author-prepared Unity programs\nthat are designed to control the height of each pin in a 30 x 30 shape\ndisplay simulator (see Appendix B for more prompt details). The\nfrontend interface features simple and basic UI elements, including\na text input box, an area for displaying AI responses, and a shape\ndisplay simulator. After receiving a response from GPT-4 API, the\nbackend system sends the output to the frontend interface with\nthe JSON format, so that the system can run the GPT-generated\nprogram to animate the 30 x 30 shape display simulator with a\nsingle input text box.\nFigure 3: Early UI prototype (A) for creating basic geometries\n(B), and interaction (C).\n4.2 Pilot Study\nWe conducted a pilot study with 10 participants (6 males, 4 fe-\nmales), including 4 experts in programming, 5 in design, and 1 in\nproduct management, with their ages ranging from 22 to 33 (av-\nerage age 26). First, participants were given a brief introduction\nand a walkthrough of the system. They were then tasked with\nperforming a free-form ideation activity (40 minutes) to develop\ntheir desired concepts using our system, followed by a short in-\nterview (20 minutes) to discuss the system’s usability and gather\nsuggestions. Throughout the study, we observed the participants’\ninteractions and collected feedback. Each session lasted approxi-\nmately 60 minutes, and each participant was compensated $15 for\ntheir participation.\n4.3 Findings in Challenges and Informing\nFeature Improvments\nOverall, participants recognized the potential of the text-to-shape-\ndisplay approach, as all of them could construct certain shape-\ndisplay behaviors using texts easily. However, we also found multi-\nple key problems that have to be resolved to build text-to-shape-\ndisplay system, that is easy to use, employ and iterate repeatedly.\nWe elaborate on them in five key insights below, which we incor-\nporated for SHAPE-IT implementation, detailed in section 5.\n• Enabling Micro-Adjustments: The only option for users to\ncreate or modify the shape display behavior was prompting with the\npilot frontend UI. Even very simple minor adjustments (e.g., tuning\nthe height of a rectangle or controlling the wave motion speed)\nspecific task. These examples, comprising input-output pairs, act as a mini-dataset for\nthe model to adapt its responses accordingly.\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\nrequired re-prompting, which required typing a follow-up prompt\nand waiting approximately 40-50 seconds for GPT agent processing,\nonly to get another result which may still not satisfy user’s intent.\nTo address this issue of difficulty of quick micro-adjustments , we\npropose to make parameter generation, and adjustments via\nGUI as one of the required features for the SHAPE-IT system. In\nsuch a system the LLM agents can generate a set of parameters that\ncan be tuned and adjusted by the users to for quick modifications.\n• Accessing Conversation History: Participants also expressed\nfrustration with the inability to review and compare and revert to\npreviously generated results , especially when the newly created ones\nwere worse than before. This motivated us to implement a feature\nof conversation history that allows users not only to view the\nprior prompts and generated codes, but also to revert to any of the\nhistory results, and re-prompt based on that prior result.\n• Transparency of AI generation: During the study, partici-\npants expressed concerns that the process between user input and\nAI output appeared as a “black box, ” leading to feelings of distrust\nand frustration, particularly when the system failed to meet their\nexpectations. This feedback highlights the essential need to develop\nan AI system that transparently communicates its reasoning\nprocess, provides feedback, and reports errors to users. Such im-\nprovements are crucial for fostering a more effective human-AI\ninteraction by enhancing transparency throughout the generation\nprocess.\n• Token Economy5 and Context management: the pilot sys-\ntem uses a naive context management strategy that compounds\nuser inputs and code responses in LLM conversations in each user\ninteraction to enable context-based editing which leads to a qua-\ndratic increase in input token costs during the conversation. Given\neach iteration the context is compounded with a code result which\nhas massive content, this indicates poor token economy and ineff-\ncient context preservation strategy. This issue is worsened by feed-\ning multiple few-shot prompting examples to the system, further\ninflating input length and model context capacity after 3-5 interac-\ntion rounds for participants during the study. Hence, in SHAPE-IT\nframework, we utilize Retrieval-Augmented Generation (RAG)\n6 and Multi-Agent framework to provide a better token economy\nwhile maintaining necessary contexts for code generation ensuring\nconsistent user experience.\n• The Need for Component Segmentation: Our pilot study\nhighlighted that users often request additional features without\nintending to modify existing elements. For example, requests like\n“make it move left and right” or “create a button to control the\nheight” are common. However, our initial system, lacking compo-\nnent segmentation, generated new scripts that not only addressed\nthese requests but also included comprehensive definitions extend-\ning to the user’s entire context. This usually resulted in unintended\nchanges to the primitive elements, and high variability exists in code\n5In the context of Large Language Models (LLMs), we define “token economy” as\nefficient management and utilization of the input and output tokens within the con-\nstraints of the model’s processing capacity. Tokens, in this context, are the basic units\nof text that the model processes, which can be words, parts of words, or punctuation\nmarks, depending on the tokenization process used by the model.\n6RAG enhances language models by integrating information retrieval(fetching relevant\ndocuments in response to queries and using these to guide output.), allowing for more\naccurate and information-rich text generation.\nresults. This issue underscores the need to separate primitives, ani-\nmations, and interactions within our system. Our formative study\nfindings align with this insight that animations and interaction\nenhancements should be built on and added to existing primitives,\nfacilitating incremental development based on an existing base.\nIn Section 5, we discuss how we apply the proposed features\nlearned from the Pilot Study in detail.\n5 SHAPE-IT\nWe developed SHAPE-IT, a system that turns user instructions into\nexecutable code for creating and modifying shapes. Similar to the\ninitial prototype, our system has two components: 1) a frontend\ninterface for the shape display simulator built with JavaScript and\n2) a backend LLM-based code generator built with Python. Our\nsystem is designed to address all of the five design requirements\nidentified in the early prototype. In this section, we describe the\nsystem design and implementation of SHAPE-IT.\n5.1 System Walkthrough\nSHAPE-IT not only allows users to provide instructions in an incre-\nmental manner, where users are able to do follow-up commands to\nadd extra behaviors or make edits on current creation, but also can\nunderstand a composite prompt to create dynamic and interactive\nshape-changing behaviors at once. we describe how our system\nworks through the following example prompt: “create a heart shape\nwith a pulsing animation, and move the shape left and right with two\nbuttons. ”. Our system generates the interactive animation as seen\nin Figure 4. Below, we detail how our LLM pipeline produces the\nresulting outcome based on this user input.\nStep 1. Segmentation: Decompose User Input into Three\nKey Elements. First, the system decomposes the user input\ninto three key elements: Primitive , Animation , and Interaction ,\ninformed by our formative exploration (Figure 5 A, B1). In\nthis example, the system segments the inputs as follows: 1)\nPrimitive: “Create a heart shape on the display” , 2) Animation:\n“Implement a pulsing animation to simulate the heart beating” , and\n3) Interaction: “Create two buttons, one to move the heart shape to t-\nhe left and another to move it to the right across the display” .\nStep 2. Parameter Generation: Identify Key Shape Properties\nfor Dynamic Motion. In our system, dynamic motion is achieved\nby updating the shape properties of primitive objects. To this end,\nthe system identifies the necessary parameters for the specified an-\nimation and interaction (Figure 5 B2-3). For instance, in the current\nexample, the system identifies four key parameters: 1) positionX\nof the heart shape, required for horizontal movement, 2) positionY\nof the heart shape, required for vertical movement, 2) scale of the\nheart shape, which is necessary for the pulsing animation, and 4)\nheight of the heart shape to control the visibility. The system uses\nthese properties as controllable parameters for both the animation\nand interaction.\nStep 3. Parameter Validation: Ensure integrity of parameter\ncontrol on the auxiliary components. SHAPE-IT ensures the\nintegrity of parameter control within the animation and interac-\ntion segments by incorporating an additional parameter validation\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nFigure 4: SHAPE-IT UI with an example prompt; (A) The feedback message communicates the generating steps to users,\nindicating Primitive , an Animation , and an Interaction scripts are generated. (B) the Parameter Control Panel; (C) Loaded\nscripts appear as Script Card UIs, where they can be toggled on/off.\nstep. This step specifically assesses whether the current primitive\nparameters can fulfill the objectives outlined in the animation and\ninteraction segments. This is crucial for managing scenarios where\na user’s subsequent actions may necessitate an animation or inter-\naction that implies the need for a parameter not included in the\ninitial primitive setup. However, in this context, the code generation\nprocess in Step 2 has already taken into account the requirements\nof all three segments. Consequently, it has generated a list of pa-\nrameters that encompasses all necessary implications, leading to\nsuccessful validation.\nStep 4. Update Code Instruction: Prepare Proper Code In-\nstructions by Incorporating Extracted Parameters. Next, the\nsystem constructs three code instructions by incorporating ex-\ntracted parameters (Figure 5 B4). For instance, in the current ex-\nample, the resulting prompts are as follows, where [params] are\nidentified parameters: 1) Primitive: “Create a heart shape on the\ndisplay by setting its initial position with [heartPositionX] and [heart-\nPositionY], scaling it with [heartScale], and establishing its height\nwith [heartHeight] for the 2.5D effect. ” . 2) Animation: “Implement\na pulsing animation to simulate the heart beating by periodically\nchanging [heartScale]” . 3) Interaction: “Create two buttons: one that\ndecreases [heartPositionX] to move the heart shape to the left, and\nanother that increases [heartPositionX] to move it to the right across\nthe display. ”. These instructions will be sent to the frontend as well,\nnotifying the users of the creating tasks(shown in Figure 4 A).\nStep 5. Script Generation: Generate Executable Code for\nPrimitive, Animation, and Interaction. Given the updated\nprompt, the system generates the script for each component. The\nsystem includes three independent LLM modules, each responsible\nfor constructing a script for primitive, animation, and interaction\n(Figure 5 C). These LLM modules generate executable code that\ncontrols the height of each pin in the shape display. The prompt\nengineering for this module combines three techniques: 1) rule-\nbased prompting, 2) retrieval augmented generation (RAG), and 3)\nstructuring of input and output, which will be described in a later\nsection.\nStep 6. Code Execution: Run the Generated Script for Shape\nConstruction and Animation. Once the backend system gener-\nates the executable script, it sends the information to the frontend\nin JSON format, which generates a dynamic shape in a Three.js\nsimulator which will be synced to a physical 24 x 24 shape display\n(Figure 5 D). The frontend system first processes the primitive script\nto construct static primitive shapes. Then, it executes the animation\nand interaction scripts to make the dynamic motion. The extracted\nparameters are also displayed as a list of slider values in the user\ninterface. When the animation or interaction alters these param-\neters, the slider values are updated accordingly. Simultaneously,\nthe frontend interface shows the previous prompt, generated code,\nand the system’s explanations of how the provided prompt was\ninterpreted.\nStep 7. Interaction and Modification: Interact with the Shape\nDisplay and Update Results. Once the code is executed, the user\ncan see the result, interact with the generated shape, or refine the\noutcomes by adjusting the generated parameters. The slider values\nare not only useful for the animation and interaction modules but\nalso enable users to interactively modify the shapes without having\nto issue new commands. Users can also update the result by entering\na new command. For example, if a user says, “instead of moving the\nposition, I want it to rotate when I click the button” , the system adapts\nby adding [heartRotation] parameters in the next round, where the\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\nFigure 5: SHAPE-IT System Architecture. (A) Text Input is handed to Prompt Helper, (B) Prompt Helper processes it to provide\ncode instructions; (C) Script Generators generate the Javascript codes based on the instructions; (D) the generated codes are\nexecuted in the frontend, controlling the shape display and creating UI elements for users to adjust parameters.\nsystem modifies the code for the existing primitive and animation\nobjects accordingly.\n5.2 Implementation\nBackend System. The backend of the system consists of two\ncomponents: a prompt helper module (Steps 1-3, Figure 5 A-B) and\na script generator module (Step 4, Figure 5 D). We provide our\nprompts for both prompt helper and script generator modules in\nAppendix C.\n5.2.1 Prompt Helper. We developed a prompt helper module to\nconvert natural language inputs into appropriate code instruc-\ntions. The prompt helper is built upon GPT-4-Turbo and utilizes\nAI-chaining method [64] that sequences multiple language mod-\nels to progressively transform user instructions into executable\ncode directives. This approach breaks down the overarching task\ninto smaller, specialized steps. In our implementation, this module\nfunctions through four sub-chains: 1) segmentation, 2) parameter\ngeneration, 3) parameter validation, and 4) code instruction prompt\ngeneration. After generating segments and parameters, the param-\neter validation chain assesses whether the identified parameters\nalign with the user’s intent. If not, it initiates a subroutine to ad-\njust the existing parameters and correct the command based on\nthe revised parameters. These parameters are then used to refine\nthe segmentation results, culminating in the formation of code\ninstructions derived from the generated parameters.\n5.2.2 Script Generator. Script generator modules take updated\nprompts as input and generate executable code as output. There\nare three script generator modules for primitives, animations, and\ninteractions, all built with GPT-3.5-Turbo-0125. We selected this\nmodel to achieve faster code generation. The prompt engineering\nfor the script generator is based on the following three techniques:\n1) rule-based prompting, 2) retrieval-augmented generation (RAG),\nand 3) input/output structuring.\n(1) Rule-Based Prompting: Rule-based prompting establishes clear\nguidelines for the generated results. It emphasizes functional code\nthat aligns with our specific requirements by providing the fol-\nlowing four instructions: structured code templating through the\nuse of parameter lists for initialization, coding practices to ensure\nmodularity and system compatibility, detailed instructions for error\nhandling, and a focus on goal orientation to enhance code complex-\nity and efficiency.\n(2) Retrieval Augmented Generation: We employ a retrieval-\naugmented generation (RAG)[29] to match instruction-code pairs\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nrelevant to the given instruction. We have developed three dis-\ntinct input-output collections categorized under primitives, anima-\ntions, and interactions. These collections are stored within separate\nchroma vector stores for efficient retrieval. During the retrieval\nphase, we use the semantic similarity example selector [30] from the\nLangchain framework[1] to identify the three most semantically re-\nlated instructions in our database. These are then incorporated into\nthe final generation template. This method ensures that the gener-\nated code leverages relevant examples and adheres to the formatting\nand structural guidelines established by our rule-based prompt-\ning. Each input-output collection for Primitive , Animation , and\nInteraction (detailed in Appendix C.2.2, C.3.2, C.4.2) was built un-\nder our formative study results (section 3.2). This formative study\ncategorization guided the development of multiple code examples\nfor each category. Additionally, we included new examples based\non observations of frequently attempted shape primitives by par-\nticipants in our pilot study.\n(3) Input/Output Structuring: For JSON output, we utilize the\nstructured output parser [31] within the same LangChain framework.\nThe output JSON follows this structure: 1) user input, 2) parameters\nneeded for code generation, 3) the category of the script generator,\n4) a system-generated explanation for interpretation, and 5) the\nexecutable code generated by the script generator.\n5.2.3 Context Management. SHAPE-IT introduces a method for\nmanaging context that addresses the challenges related to the to-\nken economy observed in our pilot system and, in the meantime,\nfacilitates continuous conversations between users and the system.\nOur framework employs code generation agents to hold onto a\nsingle history of output code as short-term memory. This allows\nfor consistent and incremental user modifications. Additionally,\nwe incorporate a Retrieval-Augmented Generation (RAG) mecha-\nnism within each code generator, enabling the selective retrieval of\nrelevant database examples, which diminishes the necessity for ex-\ntensive few-shot examples. Furthermore, the Prompt Helper Agent\nis responsible for managing long-term memory, archiving all user\ninputs along with their associated code instructions, thereby pre-\nserving the comprehensive context of the interaction. Despite the\nquadratic increase in token cost for the Prompt Helper Agent, it\nis much more compact than code results. Yet, by doing this, we\nallow code-generator agents to sustain a linear growth in token cost.\nTherefore, we offer an improved context management strategy with\na better token economy than the pilot system without breaching\nthe model context limitation.\nFrontend Sytem. The frontend system consists of 1) a shape dis-\nplay simulator and control interface, 2) conversation history, 3) an\nerror handling console, and 4) hardware communication.\n5.2.4 Shape Display Simulator and Control Interface.The frontend\ninterface renders 24 x 24 shape display simulator built with ThreeJS,\nalong with sliders for the generated parameters. The initial values\nof these parameters are scaled up by a factor of three and reduced by\na third to establish the upper and lower bounds of the slider range.\nThrough real-time manipulation of these sliders, users can directly\ncontrol the primitives. For interactions, interactive elements like\nbuttons identified in the interaction script are highlighted in orange.\nEach button element features a dropdown list for selecting its size\n(1x1 or 2x2), as well as sliders for adjusting its position (posX and\nposY) on the display and height.\n5.2.5 Conversation History. The UI records and displays the en-\ntire history of interactions between the user and the system. Each\ngenerated script is represented as a card with a textual description,\ncolor-coded based on their types. Users can navigate back to any\nprevious card to continue editing from that point, facilitated by the\nbackend’s ability to roll back to a specific history state. This feature\nnot only provides a comprehensive overview of the authoring ses-\nsion but also enables users to reference previous commands, inputs,\nand outcomes.\n5.2.6 Error Handling Console. When the front end encounters com-\npilation errors while loading the generated scripts, it requests the\nbackend to regenerate the scripts again.\nSpecifically, when the front-end encounters a compilation error,\nit sends a ’compile error message’ to the backend LLM modules\nfor each module to regenerate the code. Since each code generator\nretains the memory of the previously generated result, the modules\ncan adaptively correct the errors. This is a common practice for\nhandling errors in code generation. [11].\n5.2.7 Hardware Communication. For the physical shape display,\nwe replicated a 24x24 (434× 434mm display area, with 100mm stroke\nlength) inFORM shape display, based on an existing design [60]. For\nbi-directional data transmission, we utilized the MQTT (Message\nQueuing Telemetry Transport) protocol. The MQTT server, hosted\non our university’s server, was built using RabbitMQ. Our frontend\nJavaScript application can publish and subscribe to MQTT topics,\nwhich convey the target heights of pins based on the generated\nscript. It also receives real-time height information of the pins from\nOpenFrameworks, which is used to control the movements of hard-\nware motors. All these hardware development and communication\ndetails are elaborated in prior publications [19, 44].\n6 PERFORMANCE EVALUATION\n6.1 Method\nTo assess the performance of our system, we conducted an eval-\nuation using a subset of data from our crowdsourcing study in\nformative exploration. Specifically, we randomly selected 50 data\npoints from a total of 314 user-generated prompts (shown in Appen-\ndix D). These prompts were then input into our system to determine\nits success rates, defined by the system’s ability to attach generated\ncode to the scene without resulting in compile errors. The system\narchitecture listed for comparison marks the progression from our\ninitial system from the pilot study to the current SHAPE-IT system.\nOur hypothesis is that the addition of modules transitioning from\nthe baseline to the SHAPE-IT system should not negatively influ-\nence the code compilation success rate. Maintaining or increasing\nthe compilation success rate validates this hypothesis while also\nachieving the goal of a more intuitive user-generation flow for\nSHAPE-IT. This evaluation method was inspired by LLMR [15].\nThe success rate was calculated using the following formula:\n¯𝑆 = 1\n𝑛\n𝑛∑︁\n𝑖=1\n©­\n«\n1\n𝑚𝑖\n𝑚𝑖∑︁\n𝑗=1\n𝑠𝑖 𝑗\nª®\n¬\n, 𝑛= 50\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\nIn this formula,𝑠𝑖 𝑗represents the success or failure of each segment\nwithin a sample as a binary outcome (1 for success, 0 for failure).\nThe average success for each segment yields the sample’s success\nrate. We then computed the overall average success rate by taking\nthe mean of these rates across all samples.\nOur evaluation spanned four models, two of which is a baseline\nsingle code-generator architecture without segmentation based on\n(primitive, animation and interaction) intent segmentation, and the\nlatter two is Multi code-generator architecture that involves intent\nsegmentation.7:\n•The Baseline model, which utilizes a Single code-generator\nwith few-shot prompting. Where we feed the user input di-\nrectly to the code generator with few-shot selected examples\nfrom the combination of Primitive, Animation, and Interac-\ntion Agents.\n•Baseline+RAG, Single code-generator baseline, replacing\nfew-shot prompting with Retriever-Augmented Generation\n(RAG). with RAG retrieval collection built with combined\nexample of Primitive Animation and Interaction Examples\n•SHAPE-IT Segmentation , Multi-code generator frame-\nwork which does simple segmentation without parameter\ngeneration and inference. Similar architecture with current\nsystem Figure 5 with lesser version of PromptHelper (Chain\n1 only).\n•The SHAPE-IT System, our current system Figure 5 which\nbuilds upon the Segmentation model by adding a parameter\npipeline and code instruction.\nAdditionally, we evaluated model latency to inform our choice\nof code-generating agent, using the baseline system as a reference.\nIn evaluating the latency of code generation, we compared\nthree widely used large language models (LLMs) tailored for code-\ngeneration tasks: gpt-3.5-turbo-0125, gpt-4-1106-preview, and gpt-\n4-turbo-preview. Our findings, illustrated in Figure 6 A., revealed\nthat gpt-3.5-turbo-0125 exhibited significantly lower latency, aver-\naging 8.01 seconds across 50 samples. This was compared to 24.41\nseconds for gpt-4-1106-preview and 19.27 seconds for gpt-4-turbo-\npreview. Consequently, we selected gpt-3.5-turbo-0125 as the base\nmodel for our code-generation agents due to its efficiency.\nFrom Figure6 B., the Baseline system achieved a success rate of\n78%, and Baseline+RAG achieved 80%. Suggesting moderate compi-\nlation success for our baseline model. The SHAPE-IT Segmentation\nsystem has a lower success rate of 65%, due to the lack of coherece\nin construction of segments. While the segmentation results reason-\nably reflect user’s intent, we observe that the failure cases mainly\ncomes from the animation/interaction segments doesn’t provide\na coherent instruction with the primitive. SHAPE-IT system ad-\ndresses this problem by introducing parameter inference after the\nsegmentation process to allow code-instruction to be built for the\nthree segments leveraging the inferred parameters, this approach\nallows each segments to be interweaved by parameter instructions\ntherefore provides coherence structuring of the segments It achieves\nthe highest success rate of 82%, suggesting that the compilation of\nthe code is no longer effected by this incoherence.\n7Baseline and Baseline+RAG models, are single code-generator architecture with-\nout segmentation, 𝑚𝑖 = 1. In contrast, the SHAPE-IT Segmentation and SHAPE-IT\nsegments user prompts into three distinct segments—primitive, animation, and inter-\naction—thus 𝑚𝑖 = 3 for these systems.\nFigure 6: A.Model Runtime Comparison. B.System Success\nRate Comparison\n7 USER EVALUATION\nWe conducted a study involving 10 participants who have never in-\nteracted with shape displays to assess the effectiveness of SHAPE-IT.\nOur aim was to identify both its strengths and limitations, inform-\ning potential areas for improvement through example interaction\nprocesses and results.\n7.1 Participants\nOur participants, comprising 6 males and 4 females with an average\nage of 28.89, were non-experts in shape displays but had familiarity\nwith LLMs. Coding proficiency varied, with 7 out of 10 participants\nreporting proficiency, while 4 were skilled in design and 8 were\nfamiliar with prompting strategies. Each participant spent 1 hour\nin the study and received $15 for their time.\n7.2 Procedures and Tasks\nParticipants underwent an introductory session (10 mins) where\none of the authors explained the study procedures followed by three\nmain tasks (35 mins).\nTask 1: Participants were tasked with creating a simple primitive\nshape and then enhancing it by incorporating additional animation\nor interaction behaviors. This task tests the system’s ability to\nenable user input with the intention to author animation/interaction\nbased on an existing primitive.\nTask 2: Participants were asked to provide composite instructions,\ncombining multiple commands into a single sentence. This task\naimed to test the system’s ability to parse and execute complex\ninstructions.\nTask 3: Participants engaged in rapid idea exploration, where they\nwere free to utilize any instructions to create the desired shape-\nchanging behaviors. This task allowed for creative exploration of\nthe system’s capabilities.\nFinally, we conducted a semi-structured interview (15 mins) to\ngather insights from the participants’ perspectives.\n7.3 Results\nUsing the video recordings and the transcripts from the interviews,\nwe synthesize a few qualitative insights. Some of these insights are\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nFigure 7: Study setup and examples of participants’ creations: P8 created an interaction to use a button to move the position of\na star shape. P9 generated an animated mountain terrain from a movie. P4 created a spinning fan. P10 animated a growing\nmushroom. P6 made a dancing stick figure. P10 designed a controllable wave animation with buttons.\nspecifically applicable to our system and others represent ongoing\nchallenges with generative AI as a whole.\n7.3.1 Overall Generated Results. Based on participants’ instruc-\ntions, SHAPE-IT successfully created basic shapes (e.g., circles,\ncubes, triangles) and some simple animations (e.g., Star Twinkle,\nRaining effect). Impressively, it can create a flower with 8 petals,\nwhere the size of the petals can be controlled by sliders. It also\nachieved a chasing animation, where participants could use the UI\nto control the position of the chased object, while another object\nactively chased it. However, more complex shapes (e.g., Four-Wheel\nCar, Cat Face) and animations (e.g., Morphing, Eye Blinking) of-\nten failed. Interestingly, the system worked for prompts like “cir-\ncling around”, but failed with “Orbiting”. Simple interactions (e.g.,\nbutton-triggered text) were generally successful, while intricate\ninteractions (e.g., controlling speed) frequently failed. Addition-\nally, we have included a table in the Appendix E summarizing all\nattempted prompts by participants, with a subjective assessment\nof the results (Success, Partial Success, Failure). This provides a\nglanceable overview of the user study outcomes.\n7.3.2 Overall impressions. The study indicated strong interest in\nthe system, with notable praise for the user interface. Participants\ncommended the system’s adeptness at interpreting language inputs\nand offering granular control over parameters. They observed that\nthe system’s segmentation of primitives, animations, and interac-\ntions corresponded closely with their expectations, also suggesting\nthat the system’s initial feedback output accurately reflected their\nintentions. However, this raises expectations for the system’s sub-\nsequent visual outputs(via code results) towards which participants\ncommonly expressed a desire for greater accuracy.\nParticipants highly valued the ability to adjust parameters along-\nside 3D content and found the physical, interactive buttons on the\nshape display to be a novel experience. However, in some instances,\nparticipants found the parameters to be overly complex – such as\nwhen attempting to create a human face – where the adjustable pa-\nrameters were overwhelmingly extensive. This provokes questions\nabout potentially limiting the number of adjustable parameters in\nthe interface to reduce cognitive overload.\n7.3.3 Concrete vs Abstract Creations. During Task 3, participants\nencountered a tension between concrete shapes (squares, cubes,\nletters, emojis, and mathematical surfaces) and more abstract shapes\n(zombies, cats, and birds). When abstract shapes were desired, they\noften found the quality of the generated shapes to be unsatisfactory.\nThis tension is an expected challenge inherent in generative AI. For\nexample, while imagining a cube typically yields a consistent mental\nimage among a broad population, abstract shapes like zombies or\ncats vary widely in interpretation. In our case, this challenge is\nfurther compounded by the resolution limitations of the shape\ndisplay itself. We recommend that future researchers develop code\nexamples for various primitive shape categories. This approach will\nhelp GPT achieve better code generation by following predefined\ncode structures, thereby improving the quality of the desired shapes.\n7.3.4 Apparent Learning Curve. Participants noted that the con-\nceptual models of primitives, animations, and interactions didn’t\nalways seamlessly align with their use cases. For instance, the in-\nability to spawn a primitive through interaction is a functionality\nour system does not support because it considers animations and\ninteractions as secondary to a single primitive. They expressed a\nrecurring desire for more transparency about the system’s structure\nduring prompting, acknowledging the challenge of understanding\nthese supposed constraints up front, especially for a novice user.\nWhile there’s a common belief that natural language input requires\nminimal training, this underscores the presence of a learning curve\nassociated with such interfaces, especially when the system con-\ntains additional mechanisms that improve the experience in the\nlong term (such as parameter adjustment).\n7.3.5 Prompting vs. Coding. When comparing user preferences\nbetween the coding interface and LLM authoring, feedback varied\nsignificantly based on individuals’ coding proficiency. Participants\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\nwithout coding experience found the LLM tool beneficial for intu-\nitive exploration and design iteration. However, participants with\ncoding skills preferred using both the coding interface and the LLM\nsystem together. This hybrid approach allowed for precise adjust-\nments, especially when initial outputs didn’t match their intentions.\nNotably, one proficient coder viewed the LLM tool skeptically, pre-\nferring a more hands-on coding approach. This highlights the chal-\nlenge of accommodating both novice and expert users in authoring\ntools [2, 4, 12]. Future systems could offer options for expert users\nto directly edit and refine the generated code, as indicated by the\nstudy findings.\n7.3.6 The Importance of AI Interpretability.Participants value the\nstep-by-step feedback from the AI system, which enhances under-\nstanding of its decision-making process. They appreciate how ad-\njustments and feedback influence the resulting 3D content, leading\nto a sense of achievement when they get the result they intended.\nConversely, discrepancies between the AI-generated content and\nparticipants’ expectations can cause confusion and frustration. Par-\nticipants prefer upfront recognition of system limitations and trans-\nparent communication to set realistic expectations and mitigate\ndisappointment. This highlights the importance of advances in AI\ninterpretability.\n7.3.7 Future Prospects for AI Authoring: Text-to-Shape Display vs.\nOther LLM Tools.During our discussions on the evolution of AI\nauthoring tools, participants expressed optimism about the future\nof these innovations. A common thread in these conversations was\nthe admiration for the tangible aspects of AI-generated content, an\narea where many current AI authoring tools fall short. The ability\nto physically interact with AI-generated 3D objects was highlighted\nas a novel and valuable feature, setting our text-to-shape display\nsystem apart from other LLMs.\nMany users recognized the unique strengths of various AI author-\ning tools, but they particularly appreciated the tangible interaction\noffered by our system. One user eloquently states, “Compared to\nSORA, I can physically feel the 3D creations and even use tangible\nbuttons for control, which is cool. This could be incredibly beneficial\nfor 3D designers. ” This feedback underscores the potential impact\nand utility of tangibility in AI authoring, suggesting a promising\ndirection for further development in making digital creations more\naccessible and interactive.\n8 POTENTIAL APPLICATION SPACES\nBy allowing users to author dynamic shape-changing behaviors on\npin-based shape displays through simple text instructions. Here, we\nbriefly introduce how SHAPE-IT could be employed in applications\nsuch as adaptive tabletop furniture and gaming & storytelling. The\nresults shown in Figure 8 were actually generated from our SHAPE-\nIT system.\nAdaptive Tabletop Furniture: Inspired by the vision of adaptive\nand dynamic furniture presented in TRANSFORM [ 63], where\nshape-changing tabletop surfaces dynamically react to users,\nSHAPE-IT brings this vision close to reality by enabling users\nto create customizable and interactive tabletop features only by\nspeech. For instance, as illustrated in Figure 8 A, a user can create\na cup holder with a hidden button that allows for adjusting the\nholder’s position. They can also flexibly ask the display to create a\nroom interior, as B represents a shape-changing clock generated\nbased on language instruction.\nFigure 8: Potential applications that are enabled by SHAPE-\nIT: A: pass a beverage across the table by a hidden button;\nB: tangible clock displaying 7:02; C: a button that acts as\na trigger to trap a character; D: flush a character by host\nmanipulation of the scene of an emerging wave;\nGaming and Storytelling: SHAPE-IT enables users to provide\nreal-time instruction, which can expand the entertainment and\nexpressive capability of the shape displays. For example, board\ngames like Dungeon & Dragons require the host to use static game\nsheets and verbal communication as a way to bring the player an\nimmersive game experience. SHAPE-IT understands such a natural\nlanguage-based instruction and creates on-demand dynamic scenes\non shape displays that align with host descriptions, which enhance\ntheir gaming experiences. As illustrated in Figure 8C, the host can\nauthor a trap mechanism that is triggered when a player figure steps\non a specific trigger, forming a cage that traps the frog character,\nwhich could be flexibly generated based on a player’s narrative\ndesign. The game host can also dynamically alter the game terrain\nbased on the storyline, providing a customizable gaming experience.\nFigure 8D showcases an example where the frog character sitting\non the island is caught off-guard by a rising tide. By not requiring\nany programming experience, SHAPE-IT allows players to plan\nand implement game mechanisms or tangible story design via text\ninput.\n9 LIMITATION & FUTURE WORK\nIn this paper, we explored text-to-shape-display system to allow\nusers to author shape display behavior through natural language\ninstruction, using LLMs. Our proof-of-concept system has shed\nlight on numerous research opportunities, as a first attempt to\nconfigure shape-changing interfaces using text-based instruction.\nThis section reflects on our research, discussing the limitations and\nfuture work. It also includes system/UI design recommendations for\nfuture researchers in both the AI/LLM domain and shape-changing\ninterface domain to explore this novel research area.\n9.1 Prototype Limitation & Potential\nImprovements\n9.1.1 Flexible Authoring Framework. Our existing system is struc-\ntured to facilitate the generation of a single primitive, supplemented\nby a framework that supports animation and interaction enhance-\nments. This design choice stems primarily from the inherent limi-\ntations in our code’s ability to bind animation and interaction func-\ntionalities exclusively to specific primitives. Through user feedback,\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nit became evident that such a setup is restrictive, especially when\nusers wish to generate primitives through interactive or animated\nprocesses. To cater to a wider array of design possibilities, we recog-\nnize the imperative need to develop a more versatile framework. By\nallowing animations and interactions to be freely associated with\nany element, thereby enabling the construction of complex, non-\nlinear scenes, we can enhance the system’s utility and flexibility\nfor diverse design applications.\n9.1.2 Parameter Bound Control Issue. In our prototype, we ac-\nknowledged the absence of a robust mechanism for inferring pa-\nrameter bounds. To achieve a sophisticated inference of parameter\nbounds, it is essential to conduct a meticulous analysis of system\nconstraints, map out the dependencies between parameters, and\nutilize data-driven insights alongside machine learning techniques.\nThis approach will facilitate the accurate determination of param-\neter limits in line with operational constraints and dependencies.\nIncorporating robust validation methods and a feedback loop is\ncrucial for refining the accuracy of these inferences over time. Fur-\nthermore, designing this system with scalability and flexibility in\nmind will ensure it can adapt to evolving system designs and ex-\nternal factors, significantly bolstering the prototype’s functionality\nand reliability in diverse operational contexts.\n9.1.3 Deploying and Improving the Prototype as Usable Tools.Our\nintent for our prototype system is to be an accessible, generalizable\ntool for HCI researchers, designers, or even novices to easily plan\nand customize shape display projects as we open-source the code.\nTo this end, there are other improvements we could make to the\nsystem to be further generalizable, for example, to adapt to shape\ndisplays with different configurations (resolution, number of pins,\npin stroke, etc.), as our prototype was confined for the specific 24\nx 24 shape display hardware. By making it accessible online with\nany browser, we consider our tools useful not only for designing\nbehavior on shape display hardware but also for informing people\nconsidering/planning to develop shape display to identify their\nhardware requirements by testing target behaviors interactively\nvia natural languages.\n9.1.4 Physical Constraints of the System.The physical constraints\nof our system are largely divided into two types of geometries:\nImpossible Geometries: These include geometries that are not\nfeasible due to hardware limitations, such as floating or overhanging\nstructures.\nIntractable Geometries: These refer to complex geometries that\nare challenging to handle, such as the animation of an object rotat-\ning along the lateral axis (as opposed to the vertical axis).\nWhile the constraints in category (a) are purely due to hardware\nlimitations that the LLM cannot handle, those in category (b) can be\naddressed by improving the LLM’s prompting capabilities beyond\nsimple ’x-y array’ interpretations. This includes incorporating a 3D\nmesh renderer/shader and enhancing the code structure to manage\ncomplex geometries more effectively.\n9.2 Potential Future Direction and Implication\nfor the General Approach\n9.2.1 Limitation of Code Generation, and Potentials of other LLM\nTools. In light of the previously discussed limitation, while the\nFigure 9: Text-to-point-cloud prototype’s example result\ncode-generation LLM approach in our system is very good at gen-\nerating numerically describable motion and geometries (e.g., wave,\nsquare, cone), it suffers from making semantic shapes and motion\n(e.g., car model, human face, etc.) In the computer graphics com-\nmunity, AI-assisted LLM-based geometry generation systems are\nwell-explored [37, 39, 41, 53]\nIn fact, in the early stage of our project, we experimented with\nemploying the text-to-point-cloud LLM tool, Point-E [45], to gen-\nerate a 3D model using text to be rendered on the shape displays\n(Figure 9). While this prototype was capable of generating semantic\nmodels, we found the AI system not interpreting the shape dis-\nplay configuration (vertically moving arrays of pins) had an issue\nwhen representing some shapes (e.g. granular detail shapes). Also,\nit is limited in creating motion and interaction. Hence, we have\nemployed a code generation approach for our final approach.\nWe believe the future text-to-shape-display system should inte-\ngrate multiple types of LLM tools to mitigate each pros and cons.\nIn such a future system, code-generation LLM should have access\nto generate geometric 3D models that can be employed in the exe-\ncuted code to supplement the weakness of creating semantic shapes.\nOther LLM approaches, such as generating images or videos, which\ncan be translated into pin height of shape displays, could be another\noption, as image and video generation tools are actively advancing\nto flexibly create images in a fast manner [48]. Still, we consider\ncode-generation LLM should be central for shape-changing displays\n(or shape-changing interfaces, in general), as the behavior of these\nhardware is usually confined to computationally controlled code.\n9.2.2 Incoprating Multi-Modal Interactions. Additional future di-\nrection for the system should be in incorporating rich multi-modal\ninteractions, to harness the tangibility and embodied affordance of\nthe shape display. Beyond text input, such a system could incorpo-\nrate gesture and speech interaction [7], allowing users to point at a\nlocation of the display and summon a shape. Tangible interaction\ncould be incorporated to edit (or manipulate) the behavior gener-\nated by the system to author them in a physical manner, fusing the\ninteraction technique explored in prior works [6, 19, 34]. Such an\napproach should require an additional AI agent in our system so it\ncan flexibly interpret the gestures and tangible cues. Such a direc-\ntion would help us reach to the vision of reconfigurable material\nor clay [20, 23], that reconfigure based on user intent expressed in\ntangible, gestural, and speech interaction.\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n10 CONCLUSION\nIn this paper, we introduced SHAPE-IT, a text-to-shape-display\nsystem, harnessing multiple LLM agents (GPT) to generate code\nto control pin-based shape display based on text inputs by users.\nThe system allows users to provide natural language instruction to\nauthor shape, motion, and interaction of the hardware. Our early ex-\nploration, based on the crowdsourcing study and an early prototype,\nemploying a single LLM agent, informed us how people describe\nbehaviors on shape displays using texts and how our system should\nbe designed, including UI features and multi-agent architecture.\nOur implemented system was evaluated through a technical evalu-\nation, which reported the code-compilation success rate, and a user\nstudy, which revealed the usability of the system, as well as diverse\ntangible and shape-changing results created from participants’ text\ninput. We consider this paper to be the first step in opening up a\nnovel research realm in AI-infused shape-changing UIs that can\ndynamically shift shapes by flexibly understanding users’ intent.\nACKNOWLEDGMENTS\nWe acknowledge the CERES program and Prof. Andrew Chien at\nthe University of Chicago for their support. We thank Richard Liu\nand Rana Hanocka from 3DL at the University of Chicago for their\nsupport for the initial exploration of this research.\nWe also appreciate the help of Actuated Experience Lab members\nwith the project, specifically Ran Zhou, for advising the User Study\nmethods. Lastly, we thank everyone who helped with the assembly\nprocess of the inForm shape display (originally designed in MIT’s\nTangible Media Group), including Chenfeng’s mom, Lilith Yu, You\nLi, Yifan Zhou, Willa Yang, Matthew Jeung, Emilie Faracci, Harrison\nDong, Raymond Qian, Ramarko Bhattacharya, and Skylar Liu.\nREFERENCES\n[1] 2024. Langchain - Your AI Language Model Partner. https://www.langchain.com/.\nAccessed: 2024-04-02.\n[2] Tyler Angert, Miroslav Suzara, Jenny Han, Christopher Pondoc, and Hariharan\nSubramonyam. 2023. Spellburst: A Node-based Interface for Exploratory Creative\nCoding with Natural Language Prompts. In Proceedings of the 36th Annual ACM\nSymposium on User Interface Software and Technology . 1–22.\n[3] Anthropic. 2023. Introducing Claude 3: A New Generation of AI by Anthropic.\nhttps://www.anthropic.com and https://docs.anthropic.com. Accessed: 2024-04-\n03.\n[4] Gilles Bailly, Mehdi Khamassi, and Benoît Girard. 2023. Computational model of\nthe transition from novice to expert interaction techniques. ACM Transactions\non Computer-Human Interaction 30, 5 (2023), 1–33.\n[5] MP Barnett and WM Ruhsam. 1968. A natural language programming system for\ntext processing. IEEE transactions on engineering writing and speech 11, 2 (1968),\n45–52.\n[6] Matthew Blackshaw, Anthony DeVincenzi, David Lakatos, Daniel Leithinger, and\nHiroshi Ishii. 2011. Recompose: direct and gestural interaction with an actuated\nsurface. In CHI’11 Extended Abstracts on Human Factors in Computing Systems .\n1237–1242.\n[7] Richard A Bolt. 1980. “Put-that-there” Voice and gesture at the graphics interface.\nIn Proceedings of the 7th annual conference on Computer graphics and interactive\ntechniques. 262–270.\n[8] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\nQualitative research in psychology 3, 2 (2006), 77–101.\n[9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David\nSchnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and\nAditya Ramesh. 2024. Video generation models as world simulators. (2024).\nhttps://openai.com/research/video-generation-models-as-world-simulators\n[10] Marion Buchenau and Jane Fulton Suri. 2000. Experience prototyping. In Pro-\nceedings of the 3rd conference on Designing interactive systems: processes, practices,\nmethods, and techniques . 424–433.\n[11] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching\nlarge language models to self-debug. arXiv preprint arXiv:2304.05128 (2023).\n[12] Andy Cockburn, Carl Gutwin, Joey Scarr, and Sylvain Malacria. 2014. Supporting\nnovice to expert transitions in user interfaces. ACM Computing Surveys (CSUR)\n47, 2 (2014), 1–36.\n[13] Aubrey Colter, Patlapa Davivongsa, Donald Derek Haddad, Halla Moore, Brian\nTice, and Hiroshi Ishii. 2016. SoundFORMS: Manipulating sound through touch.\nIn Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in\nComputing Systems . 2425–2430.\n[14] Sida Dai, Brygg Ullmer, and Winifred Elysse Newman. 2024. MorphMatrix: A\nToolkit Facilitating Shape-Changing Interface Design. In Proceedings of the Eigh-\nteenth International Conference on Tangible, Embedded, and Embodied Interaction .\n1–12.\n[15] Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-\nFahey, Judith Amores Fernandez, and Jaron Lanier. 2024. Llmr: Real-time prompt-\ning of interactive worlds using large language models. In Proceedings of the CHI\nConference on Human Factors in Computing Systems . 1–22.\n[16] Aluna Everitt, Faisal Taher, and Jason Alexander. 2016. ShapeCanvas: an ex-\nploration of shape-changing content generation by members of the public. In\nProceedings of the 2016 CHI Conference on Human Factors in Computing Systems .\n2778–2782.\n[17] Yunhe Feng, Sreecharan Vanam, Manasa Cherukupally, Weijian Zheng, Meikang\nQiu, and Haihua Chen. 2023. Investigating code generation performance of\nChatGPT with crowdsourcing social data. In 2023 IEEE 47th Annual Computers,\nSoftware, and Applications Conference (COMPSAC) . IEEE, 876–885.\n[18] Daniel Fitzgerald and Hiroshi Ishii. 2018. Mediate: A spatial tangible interface\nfor mixed reality. In Extended Abstracts of the 2018 CHI Conference on Human\nFactors in Computing Systems . 1–6.\n[19] Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii.\n2013. inFORM: dynamic physical affordances and constraints through shape and\nobject actuation.. In Uist, Vol. 13. Citeseer, 2501–988.\n[20] Seth Copen Goldstein, Jason D Campbell, and Todd C Mowry. 2005. Pro-\ngrammable matter. Computer 38, 6 (2005), 99–101.\n[21] Seth C Goldstein and Todd C Mowry. 2004. Claytronics: A scalable basis for\nfuture robots. (2004).\n[22] John Hardy, Christian Weichel, Faisal Taher, John Vidler, and Jason Alexander.\n2015. Shapeclip: towards rapid prototyping with shape-changing displays for\ndesigners. In Proceedings of the 33rd Annual ACM Conference on Human Factors\nin Computing Systems . 19–28.\n[23] Hiroshi Ishii, Dávid Lakatos, Leonardo Bonanni, and Jean-Baptiste Labrune. 2012.\nRadical atoms: beyond tangible bits, toward transformable materials. interactions\n19, 1 (2012), 38–51.\n[24] Hiroshi Ishii, Daniel Leithinger, Sean Follmer, Amit Zoran, Philipp Schoessler,\nand Jared Counts. 2015. TRANSFORM: Embodiment of\" Radical Atoms\" at\nMilano Design Week. InProceedings of the 33rd Annual ACM Conference Extended\nAbstracts on Human Factors in Computing Systems . 687–694.\n[25] Sungjune Jang, Lawrence H Kim, Kesler Tanner, Hiroshi Ishii, and Sean Follmer.\n2016. Haptic edge display for mobile tactile interaction. In Proceedings of the 2016\nCHI conference on human factors in computing systems . 3706–3716.\n[26] Seungwoo Je, Hyunseung Lim, Kongpyung Moon, Shan-Yuan Teng, Jas Brooks,\nPedro Lopes, and Andrea Bianchi. 2021. Elevate: A walkable pin-array for large\nshape-changing terrains. In Proceedings of the 2021 CHI Conference on human\nFactors in Computing Systems . 1–11.\n[27] Peiling Jiang, Jude Rayan, Steven P Dow, and Haijun Xia. 2023. Graphologue:\nExploring Large Language Model Responses with Interactive Diagrams. arXiv\npreprint arXiv:2305.11473 (2023).\n[28] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David\nWeintrop, and Tovi Grossman. 2023. Studying the Effect of AI Code Generators\non Supporting Novice Learners in Introductory Programming. In Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg,\nGermany) (CHI ’23) . Association for Computing Machinery, New York, NY, USA,\nArticle 455, 23 pages. https://doi.org/10.1145/3544548.3580919\n[29] Langchain. 2023. Retrieval - Expression Language Cookbook.\n[30] Langchain. 2023. Semantic Similarity Example Selector - Langchain Core Docu-\nmentation.\n[31] Langchain. 2023. Structured Output Parsers - Langchain Python Documen-\ntation. https://python.langchain.com/docs/modules/model_io/output_parsers/\ntypes/structured. Accessed: 2024-04-02.\n[32] Gierad P Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang, Aseem Agar-\nwala, Jason Linder, and Eytan Adar. 2013. Pixeltone: A multimodal interface\nfor image editing. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems . 2185–2194.\n[33] Jeeeun Lee, Jinmo Kim, Geehyuk Ko, Minsuk Suh, and Uichin Lee. 2022. Illumi-\nnating the Design Space of Glanceable Artificial Intelligence in Mobile Intelligent\nPersonal Assistant Applications. In Proceedings of the 2022 CHI Conference on\nHuman Factors in Computing Systems . ACM.\n[34] Daniel Leithinger, Sean Follmer, Alex Olwal, and Hiroshi Ishii. 2014. Physical\ntelepresence: shape capture and display for embodied, computer-mediated remote\ncollaboration. In Proceedings of the 27th annual ACM symposium on User interface\nsoftware and technology . 461–470.\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n[35] Daniel Leithinger, Sean Follmer, Alex Olwal, Samuel Luescher, Akimitsu Hogge,\nJinha Lee, and Hiroshi Ishii. 2013. Sublimate: state-changing virtual and physical\nrendering to augment interaction with shape displays. In Proceedings of the\nSIGCHI conference on human factors in computing systems . 1441–1450.\n[36] Daniel Leithinger and Hiroshi Ishii. 2010. Relief: a scalable actuated shape display.\nIn Proceedings of the fourth international conference on Tangible, embedded, and\nembodied interaction . 221–222.\n[37] Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-\nHornung. 2022. Ganimator: Neural motion synthesis from a single sequence.\nACM Transactions on Graphics (TOG) 41, 4 (2022), 1–12.\n[38] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2021.\nText2Mesh: Text-Driven Neural Stylization for Meshes. arXiv:2112.03221 [cs.CV]\n[39] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2022.\nText2Mesh: Text-Driven Neural Stylization for Meshes. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 13492–\n13502.\n[40] Mayank Mishra, Prince Kumar, Riyaz Ahmad Bhat, Vishwas Rudramurthy, Danish\nContractor, and Srikanth G. Tamilselvam. 2023. Prompting with Pseudo-Code\nInstructions. In Conference on Empirical Methods in Natural Language Processing .\nhttps://api.semanticscholar.org/CorpusID:258823202\n[41] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. 2022.\nCLIP-Mesh: Generating textured meshes from text using pretrained image-text\nmodels. In SIGGRAPH Asia 2022 Conference Papers . 1–8.\n[42] Ken Nakagaki, Daniel Fitzgerald, Zhiyao (John) Ma, Luke Vink, Daniel Levine,\nand Hiroshi Ishii. 2019. InFORCE: Bi-Directional ‘Force’ Shape Display for\nHaptic Interaction. In Proceedings of the Thirteenth International Conference on\nTangible, Embedded, and Embodied Interaction (Tempe, Arizona, USA) (TEI ’19) .\nAssociation for Computing Machinery, New York, NY, USA, 615–623. https:\n//doi.org/10.1145/3294109.3295621\n[43] Ken Nakagaki, Udayan Umapathi, Daniel Leithinger, and Hiroshi Ishii. 2017.\nAnimaStage: hands-on animated craft on pin-based shape displays. InProceedings\nof the 2017 Conference on Designing Interactive Systems . 1093–1097.\n[44] Ken Nakagaki, Luke Vink, Jared Counts, Daniel Windham, Daniel Leithinger,\nSean Follmer, and Hiroshi Ishii. 2016. Materiable: Rendering dynamic material\nproperties in response to direct physical touch with shape changing interfaces. In\nProceedings of the 2016 CHI Conference on Human Factors in Computing Systems .\n2764–2772.\n[45] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.\n2022. Point-E: A System for Generating 3D Point Clouds from Complex Prompts.\narXiv preprint arXiv:2212.08751 (2022).\n[46] OpenAI. 2021. DALL·E: Creating Images from Text. https://openai.com/dall-e.\nAccessed: 2024-04-02.\n[47] OpenAI. 2023. ChatGPT: Language Models are Few-Shot Learners. https://openai.\ncom/chatgpt/. Accessed: 2024-04-02.\n[48] OpenAI. 2023. DALL-E2. Retrieved April 3, 2024 from https://openai.com/\nproduct/dall-e-2\n[49] Sharon Oviatt, Björn Schuller, Philip Cohen, Daniel Sonntag, and Gerasimos\nPotamianos. 2017. The handbook of multimodal-multisensor interfaces, volume\n1: Foundations, user modeling, and common modality combinations . Morgan &\nClaypool.\n[50] Martin Porcheron, Joel E Fischer, Stuart Reeves, and Sarah Sharples. 2018. Voice\ninterfaces in everyday life. In proceedings of the 2018 CHI conference on human\nfactors in computing systems . 1–12.\n[51] Prompting Guide. 2023. Few-Shot Learning Techniques. https://www.\npromptingguide.ai/techniques/fewshot. Accessed: 2023-04-03.\n[52] Simon Robinson, Céline Coutrix, Jennifer Pearson, Juan Rosso, Matheus Fernan-\ndes Torquato, Laurence Nigay, and Matt Jones. 2016. Emergeables: Deformable\ndisplays for continuous eyes-free mobile interaction. In Proceedings of the 2016\nCHI Conference on Human Factors in Computing Systems . 3793–3805.\n[53] Meitar Shechter, Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or.\n2022. NeuralMLS: Geometry-Aware Control Point Deformation. (2022).\n[54] Alexa F. Siu, Eric J. Gonzalez, Shenli Yuan, Jason B. Ginsberg, and Sean Follmer.\n2018. ShapeShift: 2D Spatial Manipulation and Self-Actuation of Tabletop Shape\nDisplays for Tangible and Haptic Interaction. In Proceedings of the 2018 CHI\nConference on Human Factors in Computing Systems (Montreal QC, Canada)\n(CHI ’18) . Association for Computing Machinery, New York, NY, USA, 1–13.\nhttps://doi.org/10.1145/3173574.3173865\n[55] Alexa F. Siu, Son Kim, Joshua A. Miele, and Sean Follmer. 2019. ShapeCAD:\nAn Accessible 3D Modelling Workflow for the Blind and Visually-Impaired Via\n2.5D Shape Displays. In Proceedings of the 21st International ACM SIGACCESS\nConference on Computers and Accessibility (Pittsburgh, PA, USA) (ASSETS ’19) .\nAssociation for Computing Machinery, New York, NY, USA, 342–354. https:\n//doi.org/10.1145/3308561.3353782\n[56] Arjun Srinivasan, Bongshin Lee, and John Stasko. 2021. InChorus: Designing\nConsistent Multimodal Interactions for Data Visualization on Tablet Devices. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems .\nACM.\n[57] Evan Strasnick and Sean Follmer. 2016. Applications of switchable permanent\nmagnetic actuators in shape change and tactile display. In Adjunct Proceedings\nof the 29th Annual ACM Symposium on User Interface Software and Technology .\n123–125.\n[58] Ryo Suzuki, Junichi Yamaoka, Daniel Leithinger, Tom Yeh, Mark D Gross, Yoshi-\nhiro Kawahara, and Yasuaki Kakehi. 2018. Dynablock: Dynamic 3d printing for\ninstant and reconstructable shape formation. In Proceedings of the 31st annual\nACM symposium on user interface software and technology . 99–111.\n[59] Faisal Taher, John Hardy, Abhijit Karnik, Christian Weichel, Yvonne Jansen,\nKasper Hornbæk, and Jason Alexander. 2015. Exploring interactions with phys-\nically dynamic bar charts. In Proceedings of the 33rd annual acm conference on\nhuman factors in computing systems . 3237–3246.\n[60] MIT Tangible Media Group. 2014. inFORM at Cooper Hewitt . Retrieved April 3,\n2024 from https://tangible.media.mit.edu/project/inform-at-cooper-hewitt\n[61] Kayla Thayer and Brad A. Myers. 2022. End-User Programming with Natural\nLanguage: A Systematic Review and Research Agenda. In Proceedings of the 2022\nCHI Conference on Human Factors in Computing Systems . ACM.\n[62] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs.&nbsp;Experience: Evaluating the Usability of Code Generation Tools Powered\nby Large Language Models. In Extended Abstracts of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA ’22) .\nAssociation for Computing Machinery, New York, NY, USA, Article 332, 7 pages.\nhttps://doi.org/10.1145/3491101.3519665\n[63] Luke Vink, Viirj Kan, Ken Nakagaki, Daniel Leithinger, Sean Follmer, Philipp\nSchoessler, Amit Zoran, and Hiroshi Ishii. 2015. Transform as adaptive and\ndynamic furniture. In Proceedings of the 33rd Annual ACM Conference Extended\nAbstracts on Human Factors in Computing Systems . 183–183.\n[64] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent\nand controllable human-ai interaction by chaining large language model prompts.\nIn Proceedings of the 2022 CHI conference on human factors in computing systems .\n1–22.\n[65] Kentaro Yasu. 2022. MagneShape: A Non-electrical Pin-Based Shape-Changing\nDisplay. In Proceedings of the 35th Annual ACM Symposium on User Interface\nSoftware and Technology . 1–12.\n[66] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny can’t prompt: how non-AI experts try (and fail) to design\nLLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1–21.\n[67] Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, and Toby Jia-Jun Li. 2023. VISAR:\nA Human-AI Argumentative Writing Assistant with Visual Programming and\nRapid Draft Prototyping. arXiv preprint arXiv:2304.07810 (2023).\nA FORMATIVE EXPLORATION MATERIALS\nA.1 Inclusion Criteria for Prior Pin-Based\nShape Display Research Selection\nThis inclusive approach allowed us to capture a broader range\nof shape-changing behaviors and applications while maintaining\na focus on displays capable of rendering rich visual information\nbeyond solely haptic or tactile feedback. To ensure a cohesive and\ntargeted analysis, we defined the following inclusion criteria(C) for\nthe reviewed papers:\nC.1 Device Specifications:\n•Actuation Mechanism: Devices must utilize a pin-based shape\ndisplay with linear motion along the z-axis. Exclusions: inflat-\nable matrices (e.g., Flat Panel Haptics), devices with bi-stable\nmovement (e.g., certain braille displays), devices with additional\ndegrees of freedom (e.g., PolySurface, TiltDisplay), and modular\nshape displays (e.g., ShapeClip, ShapeBots, HapticBots).\n•Device Orientation and Mapping: The shape display must be\noriented on a flat x-y plane. Exclusions: devices like HaptiVec,\nReal-Time Physical Prototyping Tools based on Shape-Changing\nDisplays, and PoCoPo. Only vertically moving pin-based displays\nare considered.\n•Size and Resolution: Devices should not be excessively small\n(e.g., fingertip or watch size) or overly large (e.g., room-scale).\nExclusions: devices simplistically display shapes with few pins\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n(e.g., LiftTiles, TilePop). Inclusions: devices with higher resolution\ncapable of detailed shape representation (e.g., Elevate).\nC.2 Research Purpose:\n•Focus of Research: Exclusions: studies primarily centered on\nhaptic, tactile display, or visual haptic illusion technologies (e.g.,\nTextureTouch). Inclusions: research utilizing hardware for dis-\nplaying shapes for affordance design (e.g., HapticEdgeDisplay).\n•Primary Subject Matter: Research must primarily investigate\nthe shape display technology itself. Exclusions: studies focus-\ning solely on moving objects on shape displays (e.g., TransDoc,\nKineticBlocks, AnimaStage).\nA.2 Crowdsourcing Study Details\nA.2.1 Study Materials. Our aim was to provide participants with\na focused viewing experience that allowed them to perceive and\ndescribe the shape-changing based solely on the physical shape\ndisplay itself, without undue distractions or contextual cues. Here\nare the rules(R) we applied for selecting the video pool:\nR1 Respect raw data: The video clips must be presented in their\noriginal, unedited form without any modifications or re-editing.\nIt is crucial to respect the integrity of the original data and\npresent the clips exactly as captured without altering any seg-\nments.\nR2 Focus on shape-changing itself: The video clips should not\ninclude scenarios where the shape-changing display is used to\nmanipulate or interact with external objects (e.g., moving a ball,\nsliding a phone in inFORM). The focus should be on describing\nthe shape-changing behavior itself rather than describing the\nshape display as a means of manipulating or interacting with\nother objects.\nR3 Avoid external visual cues: The video clips should have\nminimal visual distractions or cues that could influence or\nguide participants’ descriptions of the shape-changing behav-\nior. Specifically, clips should exclude any projected annotations\n(e.g., turtle shape in Materiable, hand shape in Physical Telep-\nresence), video subtitles (e.g., rain animation in ShapeCanvas),\nlighting indicators (e.g., turn on/off lights in Emerge), or other\nvisual elements that explicitly highlight or draw attention to\nthe shape changes. The goal is for participants to perceive and\ndescribe the shape transformations based solely on the physical\nshape Display itself, without external visual cues potentially\nbiasing their interpretations.\nR4 Avoid potential bias of mechanism: Exclude clips that show\nthe sequential process or rendering of the shape formation, as\nthis could lead participants to describe the underlying mecha-\nnisms rather than the final shape-changing behavior itself (e.g.,\nlandscape creation in Elevate).\nR5 Avoid flaws scenario: The shape display featured in the video\nclips should clearly and cleanly represent the intended shape-\nchanging behavior without obvious mechanical flaws or im-\nperfections that could distract from or misrepresent the shape\ntransformations. We considered this would mislead partici-\npants into noticing the unwanted parts (e.g., one stuck pin in\nHapticEdgeDisplay).\nA.2.2 Procedures. Participants were presented with an informed\nconsent form detailing the study’s purpose, what their participation\nentails, the voluntary nature of their involvement, and the mea-\nsures taken to protect their privacy and data. This consent process\nwas facilitated through Qualtrics and Prolific, ensuring that ethical\nstandards were met before participants proceeded to the main study\ntasks. After completing the study, participants were redirected to\nProlific, where their submissions would await approval for com-\npensation. For our study, we have chosen an average rate of $12\nper hour, which translates to $1.6 per participant.\nB PROMPTS DETAILS IN EARLY PROTOTYPE\nThe Early prototype contains the following three aspects of Few-\nShots Promoting integration: 1)Context Initialization Listing 1: in-\nforming the system about the hardware constraints, such as the\nspecifics of the InFORM-30x30. 2)Objective and Guidelines List-\ning 1: setting the groundwork for how the system should respond.\nWe defined context on the kind of action or response in ’type’, AI-\ngenerated human-readable explanation or feedback in ’message’,\npayload with code in ’content’. 3)Examples and Demonstrations\nListing 2: providing templates for the system to adapt and respond\nto user requests efficiently.\n1 {\n2 \" role \": \" system \",\n3 \" content \": \" You are a AI software control system\nwhich highly skilled C# and with expertise in Unity .\nYour primary responsibility is to generate\nbehaviors ( including shape , motion and Interactions )\non shape display hardwares used within a Unity\nenvironment by controlling individual pins .\"\n4 },\n5 {\n6 \" role \": \" user \",\n7 \" content \": \"\"\" The default hardware you will be\nworking with is a 30 x30 pin display called InFORM ,\nvirtually implemented in Unity .\n8 There are three other types of hardware you may be\nworking with : a 12 x24 pin display called ShapeShift ,\na 24 x24 version pin display called InFORM , and a 5\nx10 version pin display called InFORCE .\n9 during this conversation , I will notify you of when\nto change the hardware type . I will be talking in\nthe context of 5 x10 pin display , and you should\ngenerate code that is compatible with the hardware\ntype I specify .\n10\n11 You should always keep in mind the hardware\nconstraints for specific hardware when generating\ncode and NEVER forget this at any point of this\nconversation . The common constraints for all\nhardware are :\n12 The common constraints for all hardware are :\n13 1: Each Pin on shape display can only move in the y\ndirection for a certain range , which means you can\nonly modify transform . position .y not x and z.\n14\n15 Specific to InFORCE hardware :\n16 1: Each Pin has a square shape cross - section with\n0.018 m in diameter . and of length 0.12 m.\n17 2: Assuming the start position of each pin is 0m, the\nrange of motion for each of them is only at the y\naxis for 0 -0.1 m.\n18\n19 Specific to ShapeShift hardware :\n20 1: Each Pin has a square shape cross - section with\n0.00485 m in diameter . and of length 0.152 m.\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n21 2: Assuming the start position of each pin is 0m, the\nrange of motion for each of them is only at the y\naxis for 0 -0.05 m.\n22\n23 Specific to InFORM -24 x24 hardware :\n24 1: Each Pin has a square shape cross - section with\n0.00386 m in diameter . and of length 0.12 m.\n25 2: Assuming the start position of each pin is 0m, the\nrange of motion for each of them is only at the y\naxis for 0 -0.1 m.\n26\n27 Specific to InFORM -30 x30 hardware :\n28 1: Each Pin has a square shape cross - section with\n0.00386 m in diameter . and of length 0.12 m.\n29 2: Assuming the start position of each pin is 0m, the\nrange of motion for each of them is only at the y\naxis for 0 -0.1 m.\n30\n31 Now , I will be talking in the context of InFORM , The\nmain object in the Unity hierarchy that you 'll\nattach scripts to is named 'Pins ' object . This\nobject contains 900 child objects representing pins ,\nnamed from 'Pin0 ' to 'Pin899 '.\n32\n33 Next , I will be giving you guidelines and\ndemonstration to you as trainings for you to be\nready to go as a system .\n34 \"\"\"\n35 },\n36 {\n37 \" role \": \" assistant \",\n38 \" content \": \" Understood . I 'll focus on generating C#\ncode snippets and advice suitable for managing a 30\nx30 pin display named InFORM within a Unity\nenvironment . I 'll pay special attention to the\nconstraints you 've provided : Pins can only move in\nthe y- direction within a specific range . For the\nInFORM hardware , the range of motion for each pin on\nthe InFORCE display is 0 -0.1 m on the y- axis . The\nUnity object hierarchy you 've described consists of\na main object named 'Pins ', which contains 900 child\nobjects representing individual pins ( named from '\nPin0 ' to 'Pin899 '). Let 's proceed . Would you like to\nstart with example code for basic pin movement , or\ndid you have a specific behavior or interaction in\nmind ?\"\n39 },\n40 # Objective & Guidelines Definition\n41 {\n42 \" role \": \" user \",\n43 \" content \": \"\"\"\n44 To generate the desired output effectively , please\nadhere to the following guidelines :\n45 1. The output should be straightforward with no\npreamble or introductions .\n46 2. Ensure there are no errors in the generated code .\n47 *3*. You generated code is used to control the\nindividual pins to create shapes and motions and any\ninteractions with them , again the the hardware\nconstraint is that the pin display is of 5*10\ndimension and the range of motion for each of them\nis only at the y axis for 0 -0.1 f. You should\ngenerate shape , motion or interactions as naturally\non the shape display as possible while leveraging\nthe hardware constraints . No matter what shape ,\nmotion or interactions you generate , the pins should\nalways conform with the hardware constraints at ALL\nTIME .\n48 5. Exclusively output in JSON format . Formatted as\n49 {\n50 \" type \": ... ,\n51 \" message \": ... ,\n52 \" content \": ...\n53 }\n54 6. If the output type is a script , name it '\nDynamicScript '.\n55 7. When generating shapes or motions , consider :\n56 a. The method to display the shape on the Shape\ndisplay .\n57 b. The control protocol to allow user interaction\nwith the shape .\n58 c. How users can bind custom widgets or controls\nto these shapes .\n59 8. DO NOT escape anything in the script portion of\nthe JSON output , like '\"'.\n60 Remember , any given example is not for rote\nreplication . Understand the underlying principle and\nproduce generalized results .\n61 Next , I will be giving you some example interactions\nbetween user and the system for you to learn how to\nbehave properly under various scenarios\n62 \"\"\"\n63 },\n64 {\n65 \" role \": \" assistant \",\n66 \" content \": \" Got it. I will ensure that the generated\noutput adheres to these guidelines . Specifically , I '\nll provide straightforward and error - free code ,\noffer outputs in the required JSON format , save to\nthe specified location , and factor in the\nconsiderations for shape and motion design . Please\nproceed with the specific task or example you 'd like\nme to start with .\"\n67 },\nListing 1: Pilot System Prompting\n1 {\n2 \" role \": \" user \",\n3 \" content \": \"\"\"\n4 ** Scenario : Creating Static Shape **\n5\n6 user : Create a heart shape\n7\n8 chatgpt :\n9 {\n10 \" type \": \" script \",\n11 \" message \": \" Created a heart shape \",\n12 \" content \": \\\"\\\"\\\"\n13 using UnityEngine ;\n14 public class DynamicScript : MonoBehaviour\n15 {\n16 private GameObject [] pins ;\n17 static private int rows = 30;\n18 static private int cols = 30;\n19 private float scale = 0.2 f; // to fit the\nheart shape in the grid\n20\n21 void Start ()\n22 {\n23 pins = new GameObject [ rows * cols ];\n24 for ( int i = 0; i < rows * cols ; i ++)\n25 {\n26 pins [i] = GameObject . Find (\" Pin \" + i.\nToString ());\n27 }\n28 }\n29\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n30 void Update ()\n31 {\n32 CreateHeartShape ();\n33 }\n34\n35 private void CreateHeartShape ()\n36 {\n37 for ( int row = 0; row < rows ; row ++)\n38 {\n39 for ( int col = 0; col < cols ; col ++)\n40 {\n41 int index = row * cols + col ;\n42 GameObject pin = pins [ index ];\n43 float x = ( col - cols / 2f) *\nscale ;\n44 float y = ( rows / 2f - row ) *\nscale ;\n45 float equation = Mathf . Pow (x, 2)\n+ Mathf . Pow (y - Mathf . Sqrt ( Mathf . Abs (x)), 2) - 1;\n46\n47 if ( equation < 0)\n48 {\n49 pin . transform . position = new\nVector3 ( pin . transform . position .x, 0.1f, pin .\ntransform . position .z);\n50 }\n51 else\n52 {\n53 pin . transform . position = new\nVector3 ( pin . transform . position .x, 0, pin . transform .\nposition .z);\n54 }\n55 }\n56 }\n57 }\n58 }\n59\n60 \\\"\\\"\\\"\n61 }\n62 For this scenario , user is creating a static heart\nshape on the shape display , be careful that the\ngenerated shape should be of a reasonable size and\nstays within the boundry of the shape display and\nuser might want to adjust the level of the raised\npins for the shape .\n63\n64 \"\"\"\n65 },\n66 {\n67 \" role \": \" assistant \",\n68 \" content \": \"\"\"\n69 Understood , this example interaction gives me a good\nidea of how to generate code that creates a static\nheart shape on the shape display . The code carefully\nrespects the hardware constraints and allows for\nfuture modifications to the height of the pins . The\ngenerated code is included as a script in the JSON\noutput , conforming to the user request and\nspecifications provided . The C# script essentially\nloops through each pin to decide its height based on\na heart shape equation , then adjusts the pin 's\nheight accordingly . Now , I 'm ready for the next\nexample interaction scenario you 'll be showing me.\n70 \"\"\"\n71 },\n72\n73 {\n74 \" role \": \" user \",\n75 \" content \": \"\"\"\n76 ** Scenario : Making basic position and size change to\nthe static shape **\n77\n78 user : I want to move this heart shape left and right\nand repeat , but it stays in the shape display\nboundaries .\n79\n80 chatgpt :\n81 {\n82 \" type \": \" script \",\n83 \" message \": \" Created a ball shape and a button to\ncontrol toggling appear and disappear of the ball\nshape .\" ,\n84 \" content \": \\\"\\\"\\\"\n85\n86 using UnityEngine ;\n87\n88 public class DynamicScript : MonoBehaviour\n89 {\n90 private GameObject [] pins ;\n91 static private int rows = 30;\n92 static private int cols = 30;\n93 private float scale = 0.2 f; // to fit the\nheart shape in the grid\n94 private float offsetX = 0.0 f; // Horizontal\noffset for moving the heart shape\n95 private float speed = 0.05 f; // Speed for\nmoving the heart shape left and right\n96\n97 void Start ()\n98 {\n99 pins = new GameObject [ rows * cols ];\n100 for ( int i = 0; i < rows * cols ; i ++)\n101 {\n102 pins [i] = GameObject . Find (\" Pin \" + i.\nToString ());\n103 }\n104 }\n105\n106 void Update ()\n107 {\n108 MoveHeartShape ();\n109 CreateHeartShape ();\n110 }\n111\n112 private void MoveHeartShape ()\n113 {\n114 offsetX += speed ;\n115 if ( offsetX >= 5.0 f || offsetX <= -5.0f)\n116 {\n117 speed = -speed ;\n118 }\n119 }\n120\n121 private void CreateHeartShape ()\n122 {\n123 for ( int row = 0; row < rows ; row ++)\n124 {\n125 for ( int col = 0; col < cols ; col ++)\n126 {\n127 int index = row * cols + col ;\n128 GameObject pin = pins [ index ];\n129 float x = ( col - cols / 2f +\noffsetX ) * scale ;\n130 float y = ( rows / 2f - row ) *\nscale ;\n131 float equation = Mathf . Pow (x, 2)\n+ Mathf . Pow (y - Mathf . Sqrt ( Mathf . Abs (x)), 2) - 1;\n132\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n133 if ( equation < 0)\n134 {\n135 pin . transform . position = new\nVector3 ( pin . transform . position .x, 0.1f, pin .\ntransform . position .z);\n136 }\n137 else\n138 {\n139 pin . transform . position = new\nVector3 ( pin . transform . position .x, 0, pin . transform .\nposition .z);\n140 }\n141 }\n142 }\n143 }\n144 }\n145 \\\"\\\"\\\"\n146 }\n147 In this scenario , the user authorized an repeating\nleft and right moving animation of the heart shape\ncreated in the previous scenario . Bear in mind that\nthere are other motions that user might create , like\nchanging the scale of the shape , and also adjusting\nthe height of the shape ( meaning the height of the\nraised pins )\n148 \"\"\"\n149 },\n150 {\n151 \" role \": \" assistant \",\n152 \" content \": \"\"\"\n153 Understood , the example interaction for this scenario\ninvolves modifying the existing static heart shape\nto move it left and right within the shape display\nboundaries . The generated code now includes an\nUpdate function that both moves and redraws the\nheart shape , respecting hardware constraints . The\nconcept of an \" offset \" is introduced in this code to\nachieve the motion . The offset is increased or\ndecreased over time within limits to make the shape\nmove left and right . The MoveHeartShape function\nhandles the logic for this . I see that the user\nmight also be interested in other kinds of motion ,\nlike resizing the shape or adjusting the height of\nthe raised pins . These user intentions will require\nadditional parameters and controls in the code .\n154 I'm now ready for the next example interaction\nscenario .\n155 \"\"\"\n156 },\nListing 2: Pilot System Few-shot Example\nC APPENDIX C\nC.1 PromptHelper Prompting\nC.1.1 Segmentation Chain. Prompting for Segmentation Chain\n1 \"\"\"\n2 You are a smart prompt helper agent who can intelligently\nsegment prompt into three seperate segments which\nwill later be dispatched to three script writing AI\nagents ( primitiveAgent , animationAgent , and\nInteractionAgent ); as well as differentiating\nwhether a user prompt is a followup\n3 ** Input & output Rules **\n4 Input : A user defined description of the desired\ncreations including implicit or explicit references\nof shape / motion / interactions\n5 Output : A JSON object with the following field\n6 \" is_followup \": \" Boolean \",\n7 \" Authoring Primitive Shape / Motion \": \" String \",\n8 \" Authoring Animation \": \" String \",\n9 \" Authoring Interaction \": \" String \"\n10\n11 1. ** Contextual Awareness **:\n12 - Determine if a prompt is a new request or a follow -up\nby looking for references to previous interactions .\nIf unclear , seek clarification or assume it 's a new\nrequest .\n13\n14 2. ** Prompt Segmentation **:\n15 - ** Primitive Shape / Motion **: Always include a detailed\ndescription of shapes , motions , and relevant\nfeatures . This is essential for every new command .\n16 - ** Animation **: Describe changes or movements over time\nthat occur without user interaction . Include only if\nthe prompt specifies animation .\n17 - ** Interaction **: Focus on user - initiated events , like\npressing buttons made from pins . Include this\nsegment only when button creation is explicitly\nmentioned .\n18\n19 3. ** Distinct Functionalities & Clear Demarcation Between\nStatic and Dynamic Descriptions **:\n20 - Keep animation and interaction functionalities separate\n: Animation is for automatic visual / motion changes ;\nInteraction is strictly for user - initiated events .\n21 - In the Primitive Shape / Motion segment , focus solely on\nthe static aspects of the creation , such as shape ,\nsize , and spatial orientation . Avoid implying\ndynamic capabilities or movements that suggest\nanimation . Descriptions of movement , changes over\ntime , or interactive behaviors should be exclusively\nreserved for the Animation and Interaction segments\n, respectively .\n22\n23 4. ** Pin - Based Interactions **:\n24 - Clearly describe how buttons or triggers are made and\nfunction using pins or pin groups .\n25\n26 5. ** Clarity and Detail **:\n27 - Each segment should be clear and detailed enough to\navoid further clarification and accurately reflect\nthe user 's intent .\n28\n29 6. ** Managing Ambiguities **:\n30 - Resolve ambiguous commands at your discretion , ensuring\naccurate segmentation .\n31\n32 7. **2.5 D shape display capability for rendering **:\n33 - Since the hardware is 2.5 D shape display , we cannot\nrender color , material hardness , etc . Only\nGeometries , so Exclude those references since the\ndisplay can only show 2.5 D elements .\n34\n35 8. ** User Intent Fidelity **:\n36 - Stay true to the user 's original intent without adding\nunnecessary elements .\n37\n38 9. ** Intelligent Translation and Segmentation **:\n39 - Translate and segment user prompts based on their\nintended actions and specifications .\n40\n41 10. ** Handling Redos **:\n42 - For commands indicating a redo , repeat the last\noutput exactly .\n43\n44 11. ** Instructions for Script Agents **:\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n45 - At the end of animation and interaction segments ,\ninclude instructions for agents to find parameters\n46 in scripts .\n47\n48 12. ** Primitive Treatment of Waves **:\n49 - Treat wave motions as primitives , including their\nmovement in the shape / motion segment .\n50\n51 13. **2.5 D Display ability **:\n52 - Since our system will eventually need to create\nconstructs on 2.5 D shape display , so avoid\nexplicitly say '3D ', as the code generator produce\nunrecognizable shapes if '3D ' is specified\n53\n54 14. ** Followup animation / interaction reiteration **:\n55 - In case of a followup , when user intends to edit on\nan existing primitive , you should reiterate the\nexisting animation and interaction from previous\ncontexts\n56\n57 15. ** Interaction intentions **:\n58 - whenever user intends to create button to control\nsomething , that should automatically be intrepreted\nas interaction segment , since our system offer\nbutton as a way of control .\n59 And If user doesn 't specify button , but indicates\nthat they want to interact with something , you need\nto mention the creation of a button to control\nsomething in your interaction segment .\n60 \"\"\"\nListing 3: Segmentation Chain Prompting\nC.1.2 Parameter Generation Chain. Prompting for Parameter Gen-\neration Chain\n1 \"\"\"\n2 You are an AI specifically trained to analyze and\ngenerate parameters for code constructions that deal\nwith graphical primitives , animations , and\ninteractions based on user inputs . Your role\ninvolves understanding the requirements from three\nkey segments of input provided by the user :\n3\n4 1. ** Authoring Primitive Shape / Motion **: This segment is\nalways present and describes the basic shape or\nmotion to be created . It serves as the foundation\nfor the other segments .\n5\n6 2. ** Authoring Animation **: This segment describes how\nthe primitive is supposed to be animated . It\noutlines the motion or transformation that the\nprimitive undergoes over time .\n7\n8 3. ** Authoring Interaction **: This segment details any\ninteractive behavior that should be applied to the\nprimitive , such as changes in response to user\nactions like clicks or mouse movements .\n9\n10 Your task is to infer and generate a comprehensive list\nof parameters that are necessary to implement the\nrequirements described in these segments . These\nparameters should be specifically tailored to\nsupport the functionalities described in both the\nanimation and interaction segments .\n11\n12 ### Input\n13 Your input will be a JSON object containing three fields\ncorresponding to the segments mentioned above :\n14 - ** Authoring Primitive Shape / Motion **\n15 - ** Authoring Animation **\n16 - ** Authoring Interaction **\n17\n18 ### Output\n19 Your output should be a string representation of JSON\nobject containing only one field :\n20 ** parameters **: A list of parameter necessary for\nimplementing the described primitive shape / motion ,\nanimation , and interaction .\n21\n22 ### Goal\n23 The ultimate goal is to bridge the gap between high - level\ndescriptions of graphical features and the specific\n, technical details required for their\nimplementation . The parameter you generated will be\nsent to specifically construct primitive , this\ninvolves identifying the necessary parameters for\ncode construction for primitive .\n24 However , considering the fact that animation and\ninteraction will attempts to achieve what is\ndescribed in the segment by manipulating primitive\nparameters , so you also need to provide necessary\nparameter in primitive construction to account for\nanimations and interactions .\n25\n26 ### Guidelines :\n27 0. Parameter Scope : since the rendering device we have is\na 24 x24 shape diplay , so it only have the ability\nto render shape geometry and motion related\nparameters , do not include parameter such as color ,\nor roughness which shape display cannot render .\n28 * You should always priortize creating parameters that is\ngeometery related , and can be used in\nMathematically forming the primitive , that means , if\nsomething abstract is defined in animation or\ninteraction segment , you should also try to come up\nwith parameters that can be leveraged mathematically\n, that resolves these segments\n29\n30 1. Height parameter : Make sure the you include a height\nparameter to adjust the height of the primitive\n31\n32 2. parameter as numbers : The idea of the generated\n33 parameter is for a later scriptGeneration AI to use the\nparameter you provide to parametrically contruct\nprimitive , hence all parameter should indicate ONLY\nnumber , anything else like : landscapeMesh , or\n34 weatheringStyle , that might indicate something other than\nnumber is strictly prohibited .\n35\n36 3. Treat Composite Objects as Single Entities : Whenever\npossible , group related elements (e.g., letters\nforming a word ) into a single object to minimize the\nnumber of parameters needed . This simplifies both\nthe creation and animation of complex shapes or\nmotions .\n37\n38 4: Minimize Parameter Redundancy : Avoid creating separate\nparameters for aspects of the animation or\ninteraction that can be controlled by a single , well\n- thought - out parameter . This reduces the complexity\nof the code and the effort needed to manage these\nparameters .\n39\n40 5: Focus on Essential Parameters : Identify and generate\nonly those parameters that are absolutely necessary\nfor the primitive 's creation , its animation , and any\nspecified interactions . This ensures clarity and\nefficiency in the parameter list .\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n41\n42 6: Logical Grouping of Parameters : Organize parameters in\n43 a way that reflects their role in the construction ,\nanimation , and interaction of the primitive .\nGrouping related parameters together can aid in\nunderstanding and managing them .\n44\n45 7: Focus on the mathematical foundations of the\nprimitives : Devise parameters that naturally\nfacilitate animation or interaction through their\ninherent mathematical properties , avoiding any\ndirect or explicit references to the animation or\ninteraction processes themselves .\n46\n47 8: Adapt Parameter Design to User Requirements : Tailor\nthe parameters to the specific needs of the\nanimation and interaction as described by the user .\nEnsure that the parameters provided can adequately\nsupport the described behaviors without unnecessary\ncomplexity .\n48\n49 9: Clarify Parameter Usage in Descriptions : In the\nmodified segments for animation and interaction ,\nexplicitly mention how the parameters are to be used\n. This clarifies their purpose and ensures that the\nimplementation aligns with the intended design .\n50\n51 10: Efficiency in Parameter Usage : Strive for an\nimplementation strategy that uses the fewest\nparameters possible while still achieving the\ndesired outcome . This often involves creative\nproblem - solving to find the most efficient way to\ncontrol the primitive 's behavior .\n52\n53 11: Simplify Interaction / Animation Handling : When\ninteractions / animation are specified , consider how\nthey can be managed using existing parameters or\nwith minimal additional parameters . This keeps the\ninteraction implementation straightforward and\nintegrated with the overall design .\n54\n55 12: Adhere to 2.5 D Spatial Constraints :\n56 Recognize that shape display is limited to a 2.5 D\n57 surface , involving only X ( horizontal ), Y ( vertical ),\nand height ( pin elevation ) dimensions .\n58 Avoid generating parameters that specify spatial\nparameters beyond X, Y, and height , to prevent\n59 confusion with traditional 3D space representation .\n60\n61 13: Relationship between Primitive , Animation and\nInteraction : Primitive is the main landing 3D\ncontent of the scene , and animation and interaction\nis a dynamic manipulation of these primitive\nparameters . The parameters you generated are for\nPrimitives , hence , you need to come up with the\nparameter needed for primitive that accounts for\nwhat specified in animation / interaction segments ,\nparameter like \" animationSpeed \" is undesired because\nit is not associated with the primitive , and the\ncode generation AI will not be able to recognize\nthis parameter when paired by the primitive segment .\n62\n63 14: Intelligently Design Parameters for Animation and\nInteraction :\n64 When setting up parameters for animations and\ninteractions , it 's crucial to adopt a strategic\napproach . Rather than directly naming parameters\nafter the specific animation or interaction (e.g., \"\npulse \" for a pulsing heart ), consider how you can\nachieve the desired effect through mathematical\nmeans . For instance , if you 're animating a heart to\n65 pulse , think about the underlying mathematical principle\nthat can mimic pulsing . In this scenario , varying\nthe scale increasing and decreasing effectively\nsimulates a pulsing motion . Thus , a parameter like \"\nheartScale \" should be introduced to control this\naspect , embodying a thoughtful , indirect approach to\nparameterization that supports animation and\ninteraction without explicitly naming them .\n66\n67 15: Handling abstract or conceptual shapes :\n68 For abstract or conceptual shapes such as heart , come up\nwith a simplified construction of parameters ( for\nexample positionx , positiony , scale , is already\nsufficient ), so that the code generation agent can\neasily form code based on your parameters . The goal\nis to create these conceptual shapes visible , not to\nmake it most realistic .\n69 \"\"\"\nListing 4: Parameter Generation Chain Prompting\nC.1.3 Parameter Inference Chain. Prompting for Parameter Infer-\nence Chain\n1 \"\"\"\n2 You are a smart agent tasked with evaluating whether a\nuser - described animation or interaction can be\nimplemented using a provided list of parameters . You\nwill receive input as a JSON string containing\nparentparam and prompt fields . Your output should\nconsist of two key components :\n3\n4 success : A boolean value indicating whether the\nparentparam is sufficient to accommodate the user 's\nprompt .\n5 message : A detailed explanation offering recommendations\nto the user on how to achieve the described\ninteraction or animation using the available\nparentparam . This should include suggestions for\ncreative use of the parameters and , if necessary ,\nrecommendations for additional parameters that are\nnot included in parentparam but would be required to\nfully realize the prompt .\n6 Important Note :\n7 Your evaluation should focus on creatively utilizing the\nparentparam to achieve the desired outcome , even if\nthe solution is not immediately apparent or if the\nimplementation might only partially fulfill the\nprompt . The relationship between parentparam and the\nprompt may not always be straightforward . For\ninstance , if the user 's prompt involves controlling\nhorizontal left and right movement and the\nparentparam includes a positionX parameter , this\nscenario should be considered a success . This is\nbecause manipulating positionX can indeed result in\nhorizontal movement , demonstrating an implicit but\neffective relationship between the parameter and the\ndesired animation or interaction . You must think\ncritically and explore the potential of each\nparameter to meet the requirements of the prompt ,\nemphasizing the importance of not overlooking the\nimplicit capabilities of the provided parameters .\n8\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n9 Rules :\n10\n11 1. ** Input Understanding **: Recognize input in the form\nof a JSON string that details `parentparam ` and `\nprompt `. Understand these fields as the basis for\nyour evaluation .\n12\n13 2. ** Success Determination **: Output a boolean value `\nsuccess ` indicating whether the provided `\nparentparam ` sufficiently accommodates the user 's\nprompt . True signifies adequacy , while false\nindicates insufficiency .\n14\n15 3. ** Message Formulation **: Provide a `message ` that\nincludes :\n16 - A clear explanation of how the `parentparam ` can or\ncannot fulfill the prompt .\n17 - Creative recommendations for using available parameters\nto achieve the described animation or interaction .\n18 - Suggestions for additional parameters not included in `\nparentparam ` but necessary for full realization if\nthe existing set falls short .\n19\n20 4. ** Creative Utilization **: Emphasize the innovative use\nof `parentparam ` to achieve the prompt 's goals .\nEven if a direct solution seems absent , explore and\nhighlight indirect or partial methods that can serve\nthe intended purpose .\n21\n22 5. ** Implicit Relationships **: Acknowledge and capitalize\non the less obvious connections between `\nparentparam ` and the prompt . Consider a parameter 's\npotential beyond its explicit function if it can\ncontribute to fulfilling the request .\n23\n24 6. ** Updated Parameters Inclusion **: For scenarios deemed\nfailures , include an `updatedParams ` field in your\noutput . This field should list the original\nparameters plus any additional ones needed to make\nthe implementation possible , providing a\nconstructive path forward for achieving the\n25 desired animation or interaction .\n26\n27 7. ** Critical Thinking **: Apply careful analysis and\ncreative problem - solving to bridge gaps between\navailable parameters and the animation or\ninteraction described in the prompt . Avoid\ndismissing a prompt as unachievable without\nthoroughly exploring alternative approaches .\n28\n29 8. ** Clear Communication **: Ensure your explanations are\nunderstandable , offering actionable insights and\nsteps that users can follow to implement your\nrecommendations .\n30\n31 9. ** Feedback Loop **: Incorporate a mechanism or suggest\na method for users to refine their prompts or\nparameters based on your feedback , encouraging\niterative improvement towards achieving the desired\noutcome .\n32 \"\"\"\nListing 5: Parameter Inference Chain\nC.1.4 Code Instruction Chain. Prompting for Code Instruction\nChain\n1 \"\"\"\n2 You are an AI specifically trained to analyze the given\nprimitives , animations , and interactions segments as\nwell as a list of parameters . Your role involves\nunderstanding the given segments and based on the\nparameter given how to come up with code\ninstructions for these three segments :\n3\n4 1. ** Authoring Primitive Shape / Motion **: This segment is\nalways present and describes the basic shape or\nmotion to be created . It serves as the foundation\nfor the other segments .\n5\n6 2. ** Authoring Animation **: This segment describes how\nthe primitive is supposed to be animated . It\noutlines the motion or transformation that the\nprimitive undergoes over time .\n7\n8 3. ** Authoring Interaction **: This segment details any\ninteractive behavior that should be applied to the\nprimitive , such as changes in response to user\nactions like clicks or mouse movements .\n9\n10 Your task is to infer and generate a comprehensive list\nof parameters that are necessary to implement the\nrequirements described in these segments . These\nparameters should be specifically tailored to\nsupport the functionalities described in both the\nanimation and interaction segments . Moreover , you\nare required to modify the descriptions of the\nanimation and interaction segments to explicitly\nindicate which parameters are to be used for\nimplementing the described behaviors .\n11\n12 ### Input\n13 Your input will be a JSON object containing three fields\ncorresponding to the segments mentioned above as\nwell as parameters :\n14 - ** Authoring Primitive Shape / Motion **\n15 - ** Authoring Animation **\n16 - ** Authoring Interaction **\n17 - ** parameters **: A list of strings , where each string is\nthe name of a parameter necessary for implementing\nthe described primitive shape / motion , animation , and\ninteraction .\n18\n19 ### Output\n20 Your output should be a JSON object containing three\nfields :\n21 1. ** Authoring Primitive Shape / Motion **: This section\nmust enumerate each parameter outlined in the input\nand describe its role in creating or defining the\nprimitive shape or motion . This description should\nbe technical , indicating how each parameter affects\nthe geometry , positioning , or initial state of the\nprimitive in the code .\n22\n23 2/3. ** Modified Authoring Animation and Authoring\nInteraction **: These fields should contain the\noriginal descriptions but modified to include\nspecific references to the parameters you 've\nidentified . This modification should make it clear\nwhich parameters are involved in the animation and\ninteraction functionalities .\n24 For animation segment : you need to inform the code\ngeneration agent how to create the code for\nanimation by manipulation of ( what parameters )\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n25 For interaction segment : you need to inform the code\ngeneration agent to create button to control ( which\nparameter ) in what way , for example , if there is\nPosX parameter , and the user wants to create\ninteraction that move the object left , inform the\nbutton functionality to decrease PosX when button is\npressed\n26\n27 ### Goal\n28 The primary goal is to convert the provided input\nsegments into clear , actionable coding instructions\nthat precisely use the specified parameters . This\nprocess is designed to bridge the conceptual and\npractical implementation gap , ensuring a smooth\ntransition from visual and interactive designs to\nfully functional code . A detailed explanation of\neach parameter 's role is crucial , as it allows\ndevelopers to easily grasp and execute the provided\ninstructions , leading to the creation of dynamic and\ninteractive visual elements .\n29\n30 ### Guidelines :\n31 1: Handling 'None ' Inputs : If the input for any segment\n( Authoring Primitive Shape / Motion , Authoring\nAnimation , or Authoring Interaction ) is \"None ,\" or\nsuggesting that it is empty , your output for that\nparticular segment should be \" None .\" This indicates\nthat no action or code is required for that segment .\n32\n33 2: 2.5 D Shape Display Considerations : The code\ninstructions you generate for the primitive shape or\nmotion are intended for use in creating 3D\nconstructs visualized on 2.5 D pin - based shape\ndisplays . This unique display medium requires\nspecific consideration in how shapes are represented\n, particularly in their depth and interaction with\nlight to achieve the desired 3D effect on a 2.5 D\nsurface .\n34\n35 3: Incorporating the 'Height ' Parameter : Given the 2.5 D\nnature of the shape display , it 's important to\ninclude and properly utilize a 'Height ' parameter .\nThis parameter is crucial for controlling the\n36 perceived height or depth of the 3D constructs on the\nshape display . In your code instructions , make sure\nto articulate how the 'Height ' parameter influences\nthe overall representation of the construct .\n37\n38 4: Utility of Every Parameter in Primitive Creation : When\ndetailing the creation of a primitive shape or\nmotion , emphasize that every listed parameter must\n39 serve a specific purpose . There should be no\nassumption of redundancy among the parameters nor\nthey should be assumed to be ignored in the initial\nconfiguration . For each parameter , provide clear\ninstructions on how it contributes to the\nconstruction of the primitive , whether it affects\nits size , position , rotation , or any other attribute\n. Make sure that you create instructions that\naccounts for ALL of the parameters listed in the\ninput .\n40\n41 5: Handling the update of code instructions for primitive\n: There are cases where you need to update the\nprimitive instructions , this is signifed by a input\nof primitive code instruction containing directives\nof how a subset of input parameters are utilized ,\nnote that here because we have updated the\nparameters so that the current code instruction didn\n't cover the new parameters , you need to incorporate\nthese uncovered parameters properly and form a new\ncode instruction .\n42\n43 6: If you see a segment which contains already a\nsatisfying code instruction covering all of the\nparameters you can output that segment as is.\n44 \"\"\"\nListing 6: Code Instruction Chain\nC.2 Primitive Agent\nC.2.1 Prompting. Prompting for primitive Agent\n1 \"\"\"\n2 ### AI Software Writing System : JavaScript Expertise\nRequired\n3\n4 ** Objective :** Generate functions that create primitives\n( static shapes and motions ) on a 24 x24 pin - based\nshape display hardware , controlling individual pins .\n5\n6 ** Training :** You will learn through few - shot examples to\nidentify patterns in code and generate new scripts\nbased on user prompts .\n7\n8 ### Rules and Guidance :\n9\n10 1. ** Input Prompt :** You will receive a user prompt\ndescribing the intended shape or motion , as well as\na parameter list which you need to use for creating\nthat primitive . Your task is to generate a JSON\noutput containing `type `, `message `, and `content ` (\nthe script ), where `type ` must be `primitive `.\n11\n12 2. ** Output Format :** `type ` will always be \" primitive \",\nand `message ` will be the response message to users ;\nProduce a script as the `content ` in the JSON\noutput .\n13\n14 3. ** Examples :** Learn from provided examples to create\nprimitive script structures for shape displays . Note\n: Assume no pre - existing functions ; define your own\nas needed .\n15\n16 4. ** Parameter Definitions :** Define these parameters ,\nwhich given in the parameters in input , within the `\ninitializeParams ` function , clearly stating their\npurpose , and make sure you utilize all of them\neffectively in the construction of the primitive ,\nall of the parameters should be assigned a value in\ninitializeParams and should only have number as\nvalues .\n17\n18 5. ** How to Produce code **: Begin by using the list of\nparameters in the input exactly by their names in a\nfunction named `initializeParams () `. Use these\nparameters to construct the shape / motion , then\nreturn the parameters . And dynamicScript ( make sure\nyou exactly have the name `dynamicScript `) function\nacting as the main loop , create other helper\nfunction as needed .\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n19\n20 6. ** Code Structure :** For dynamic shapes (e.g., waves ,\nanimated patterns ), encapsulate logic in a `\ndynamicScript ` function using an IIFE to maintain\nstate privately without global scope pollution . And\nmake sure that to make dynamicScript function as the\nmain logic function\n21\n22 7. ** Code Content :** Ensure the generated code is\nfunctional , with no placeholders .\n23\n24 8. ** ShapeDisplay Object :** Utilize the following\npredefined library objects and functions :\n25 - `ShapeDisplay ` Object :\n26 - `grid_x `: Horizontal grid length (24) .\n27 - `grid_y `: Vertical grid length (24) .\n28 - `Pins `: Storage for all 24 x24 pin objects .\n29 - `getPin ( int index ) `: Returns the pin object at\nthe specified index .\n30\n31 - `Pin ` Object :\n32 - `setPos ( float height ) `: Sets the pin 's height .\n33\n34 9. ** Adhere to 2.5 D Spatial Constraints **:\n35 Recognize that shape display is limited to a 2.5 D\nsurface , involving only X ( horizontal ), Y ( vertical )\n, and height ( pin elevation ) dimensions .\n36 try to make scripts that makes the user creation\nbecome visible in this 2.5 D surface , and prevent\nconfusion with traditional 3D space representation .\n37\n38 10. ** Geometry construction **: For geometrical shapes ,\nyou should try to contruct the shapes mathematically\n, and leverage the parameters defined in the input .\n39\n40 11. ** main loop definition **: In the code you will need\nto define the main loop as the exact name *\ndynamicScript *, as we will have external reference\nto this exact function name .\n41\n42 12. ** Function Definition within the script **: You should\ndefine all the functions within the script , and don\n't assume a function exists without defining them .\n43\n44 ### Goal : Generate working JavaScript code based on user\nprompts , adhering to the specified rules and\nstructure for creating primitives on a shape display\n45 \"\"\"\nListing 7: Primitive Agent Prompting\nC.2.2 Code Example in Collection. Code Example\n1 {\n2 \" input \":\n3 \"\"\"\n4 {\n5 \" Prompt \": \" Generate a customizable square\nshape \",\n6 \" parameters \":[ squareScale , squarePosX ,\nsquarePosY , squareRotation , squareHeight ]\n7 }\n8 \"\"\" ,\n9 \" output \": \"\"\"\n10 {\n11 \" type \": \" primitive \",\n12 \" message \": \" Created a square shape \",\n13 \" content \": \\\"\\\"\\\"\n14 // Defines initial setup values such as scale\n, position , rotation , and height of the square\n15 function initializeParams () {\n16 return {\n17 squareScale : 0.5 , // Scale factor for\nthe square size relative to the display grid\n18 squarePosX : Math . floor ( ShapeDisplay .\ngrid_x / 2) , // X position of the square 's center\n19 squarePosY : Math . floor ( ShapeDisplay .\ngrid_y / 2) , // Y position of the square 's center\n20 squareRotation : 0, // Initial\nrotation angle of the square\n21 squareHeight : 25 , // Height of the\nsquare pins\n22 };\n23 }\n24\n25 // Calculates the new position of a point\nafter rotation around the origin\n26 function calculateRotatedPosition (x, y,\nrotation ) {\n27 return {\n28 rotatedX : x * Math . cos (- rotation ) - y\n* Math . sin (- rotation ), // X coordinate after\nrotation\n29 rotatedY : x * Math . sin (- rotation ) + y\n* Math . cos (- rotation ), // Y coordinate after\nrotation\n30 };\n31 }\n32\n33 // Determines if a point is within the\ndefined square boundaries after rotation\n34 function checkInBounds ( rotatedX , rotatedY ,\nmaxDimension_x , maxDimension_y ) {\n35 return (\n36 rotatedX >= - maxDimension_x / 2 &&\n37 rotatedX <= maxDimension_x / 2 &&\n38 rotatedY >= - maxDimension_y / 2 &&\n39 rotatedY <= maxDimension_y / 2\n40 );\n41 }\n42\n43 // Main function to orchestrate the dynamic\nscript\n44 // Uses initialized parameters to set the\ndisplay according to the square pattern\n45 function dynamicScript ( deltaTime , params ) {\n46 const {\n47 squareScale ,\n48 squarePosX ,\n49 squarePosY ,\n50 squareRotation ,\n51 squareHeight ,\n52 } = params ;\n53 const maxDimension_x = ShapeDisplay .\ngrid_x * squareScale ; // Max width of the square\n54 const maxDimension_y = ShapeDisplay .\ngrid_y * squareScale ; // Max height of the square\n55\n56 // Iterate over all pins to set their\npositions based on the square pattern\n57 ShapeDisplay . Pins . forEach (( pin , index ) =>\n{\n58 let x = ( index % ShapeDisplay . grid_x )\n- squarePosX ;\n59 let y = Math . floor ( index /\nShapeDisplay . grid_x ) - squarePosY ;\n60\n61 // Calculate rotated position for\neach pin\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n62 const { rotatedX , rotatedY } =\ncalculateRotatedPosition (\n63 x,\n64 y,\n65 squareRotation\n66 );\n67\n68 // Check if the point falls within\nthe bounds and set pin height if true\n69 if ( checkInBounds ( rotatedX , rotatedY ,\nmaxDimension_x , maxDimension_y )) {\n70 pin . setPos ( squareHeight );\n71 }\n72 });\n73 }\n74 \\\"\\\"\\\"\n75 }\n76 \"\"\"\n77 },\nListing 8: Primitive Agent Vector Store Example\nC.3 Animation Agent\nC.3.1 Prompting. Prompting for Animation Agent\n1 \"\"\"\n2 ### AI Animation Script Generator : JavaScript Expertise\nfor 3D Design\n3\n4 ** Objective :** Use the parameters within an existing\nscript that construct a primitive shape to generate\nanimations . Utilize JavaScript to manipulate these\nparameters according to user prompts , creating\ndynamic animations on a shape display .\n5\n6 ** Training :** Through few - shot examples , identify\npatterns in existing code and learn to generate new\nanimation scripts based on user prompts .\n7\n8 ### Rules and Guidance :\n9\n10 1. ** Input Prompt :** Receive user prompts describing the\ndesired animation . And parentparams indicate that a\nlist of parameter exposed to you for the purpose of\nmaking the animation by manipulating them .\n11\n12 2. ** Output Format :** You should output a JSON that has\nthree fields : type : which is always animation ,\nmessage : the message feedback of the system , and\ncontent which is the script itself that you are\ngoing to create , for script , you should manipulate\nparentparams 's value provided and adjust them to\nachieve the intended animation .\n13\n14 3. ** Important Note :** In generating the animation script\n, ** do not start from scratch **. An existing script\nand its parent parameters are already provided . Your\ntask is to analyze the user prompt and manipulate\nthese parent parameters to create the intended\nanimation .\n15\n16 4. ** Examples :** Study provided examples to understand\nhow to manipulate parameters for animations\neffectively . Assume the existence of certain\nfunctions and objects as outlined below but do not\ncopy parameters verbatim .\n17\n18 5. ** Explicit Function Naming :** Adhere to naming\nconventions seen in examples . Use clear and\ndescriptive names for any new functions or\nparameters you introduce .\n19\n20 6. ** Parameter Manipulation :** Directly manipulate\nexisting parameters ( parentparams ) to achieve the\nanimation . Do not introduce unnecessary global\nvariables or functions .\n21\n22 7. ** External Libraries and Objects :**\n23 - ** ShapeDisplay Object :**\n24 - `grid_x `: Horizontal grid length , set to 24.\n25 - `grid_y `: Vertical grid length , set to 24.\n26 - Use this object to understand the display 's\ndimensions and manipulate the primitive 's position\nand scale accordingly .\n27\n28 8. ** Code Content :** Ensure the generated code is\nfunctional and executable , with no placeholders . You\nneed to complete the code as described in the input\n, place holders are absolutely prohibited .\nDynamically adjust parameters over time described in\nthe input if necessary .\n29\n30 9. ** Learning from Examples :** Pay close attention to the\nstructure and logic of the example scripts . Your\ntask is to apply similar patterns to new prompts ,\nadapting the logic to fit the requested animations .\n31\n32 10. ** No Assumption of Function Existence :** While\ncertain objects and their properties are given , do\nnot assume the existence of additional functions\noutside of those specified or exemplified . Define\nany new logic needed to achieve the animation .\n33\n34 11. ** deltaTime usage :** deltaTime is in seconds not in\nmiliseconds , so use deltaTime as is no need to\ndivide by 1000\n35\n36 12. ** parentParams reference :** remember that to\nreference the parameters given in the input you need\nto use parentparams .( parameters in the input list )\n37\n38 ### Goal : Produce JavaScript code that dynamically\nanimates primitives on a shape display , adhering to\nthe user 's prompt and following the provided\nguidelines and examples .\n39 \"\"\"\nListing 9: Animation Agent Prompting\nC.3.2 Code Example in Collection. Code Example\n1 {\n2 \" input \": \"\"\"\n3 {\n4 \" Prompt \": \" create a left and right repeat\nanimation for the square shape \",\n5 \" parentparams \":[ squareScale : 0.5 , squarePosX :\nMath . floor ( ShapeDisplay . grid_x / 2) , squarePosY :\nMath . floor ( ShapeDisplay . grid_y / 2) , squareRotation :\n0, squareHeight : 25]\n6 }\n7 \"\"\" ,\n8 \" output \":\n9 \"\"\"\n10 {\n11 \" type \": \" animation \",\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n12 \" message \": \" Created left and right repeat\nanimation for the square shape \",\n13 \" content \":\n14 \\\"\\\"\\\"\n15 // Function to initialize and return the\nparameters used by the dynamic script\n16 function initializeParams () {\n17 return {\n18 speed : 2, // Speed of the\nmovement , defined as 2 units . Adjust this value to\nincrease or decrease the speed . Note that the\nanimation parameter should NOT repeat the parameter\nin primitive scripts , but rather control parameters\nfor the animation\n19 };\n20 }\n21\n22 // Define a function that encapsulates\nits own state using a closure\n23 const dynamicScript = ( function () {\n24 let direction = 1; // Initialize\ndirection : 1 signifies moving right , -1 signifies\nmoving left\n25\n26 // Return a function that updates the\nposition based on parameters\n27 return function ( deltaTime , params ,\nparentparams ) {\n28 const { speed } = params ; //\nDestructure speed from params for easy access\n29\n30 // Conditional check to reverse\ndirection when hitting boundaries\n31 if (\n32 parentparams . squarePosX >=\nShapeDisplay . grid_x || // Right boundary check\n33 parentparams . squarePosX <= 0\n// Left boundary check\n34 ) {\n35 direction *= -1; // Reverse\ndirection upon hitting a boundary\n36 }\n37\n38 // Update the square 's position\non the X axis based on direction , speed , and elapsed\ntime\n39 parentparams . squarePosX +=\ndirection * speed * deltaTime ;\n40 };\n41 }) ();\n42 \\\"\\\"\\\"\n43 }\n44 \"\"\"\n45 },\nListing 10: Animation Agent Vector Store Example\nC.4 Interaction Agent\nC.4.1 Prompting. Prompting for Interaction Agent\n1 \"\"\"\n2 ### AI Software Writing System : JavaScript Expertise\nRequired\n3 ** Objective :**\n4 You are tasked with acting as a smart 3D design coder\nagent . Your role is to write interaction scripts\nbased on an existing script that describes a\nprimitive and a user command aiming to create some\ninteraction with that primitive .\n5\n6 ** Important Note :**\n7 - You 'll be working with an existing script where certain\nparameters ( `parentparams `) are exposed for\nmanipulating through a button to achieve the\nintended interaction .\n8\n9 ** Input :**\n10 - User prompt\n11 - `parentparams `: Parameters from the existing script\nthat are exposed for authoring interaction ( button )\nto control .\n12\n13 ** Output :**\n14 - Generate a JSON response containing the following keys :\n15 - `type `: Must be \" interaction \".\n16 - `message `: A descriptive message about the interaction .\n17 - `content `: The interaction script .\n18\n19 ** Rules and Guidance :**\n20 1. Consider the primitives as already created . Your task\nis to write an interaction script to author a button\nthat controls the existing primitive (e.g.,\nmovement , deformation ).\n21 2. You will be given examples to learn from . Ensure you\nunderstand how to construct responses and generate\nscripts from these examples .\n22 3. Follow explicit function naming conventions as shown\nin the examples , especially for object functions and\nparameters .\n23 4. Clearly define all parameters within the interaction\nscript , specifying their purposes .\n24 5. The interaction script should only author buttons as\ninteraction to manipulate parameters defined in the\n`parentparams `\n25\n26 ** Code Writing Guidelines :**\n27 - Begin with a function named `\ninitializeInteractionParameters () ` that includes a\nlist of buttons and other parameters . Each button\nshould be a dictionary containing exactly four\nparameters :\n28 - `id `: A unique identifier for the button group .\n29 - `size `: Button size (1 for a single unit , 2 for a\nlarger 2x2 button ).\n30 - `position `: Calculated to place the button at the\ncenter of the grid , with `x` ( horizontal index 0 -11)\nand `y` ( vertical index 0 -11) .\n31 - `init_height `: Initial z- height of the button in the\ngrid .\n32 - Reference to the parameters given in the input using `\nparentparams . ` followed by the parameter name .\n33\n34 ** Externally Defined Functions and Objects :**\n35 - `initializeButtons ( params ) `: Initialize buttons using\nthe defined parameters at the start . Always call\nthis at the start of the `dynamicInteraction `\nfunction .\n36 - `ShapeDisplay ` Object :\n37 - `grid_x `: Grid horizontal length (24) .\n38 - `grid_y `: Grid vertical length (24) .\n39 - `Pin ` Object :\n40 - `isButton `: Boolean value indicating if a pin is a\nbutton .\n41 - `isPressing `: Boolean value indicating if a pin is\ncurrently being pressed .\n42 - `buttonGroup_id `: The ID of the button you assigned in\nparameters .\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\n43 \"\"\"\nListing 11: Interaction Agent Prompting\nC.4.2 Code Example in Collection. Code Example\n1 {\n2 \" input \": \"\"\"\n3 {\n4 \" Prompt \": \" create two buttons that controls\nleft and right movement of the button \",\n5 \" parentparams \": [ squareScale : 0.5 , squarePosX\n: Math . floor ( ShapeDisplay . grid_x / 2) , squarePosY :\nMath . floor ( ShapeDisplay . grid_y / 2) , squareRotation :\n0, squareHeight : 25]\n6 }\n7 \"\"\" ,\n8 \" output \":\n9 \"\"\"\n10 {\n11 \" type \": \" interaction \",\n12 \" message \": \" created two buttons that controls\nleft and right movement of the button \",\n13 \" content \":\\\"\\\"\\\"\n14 // Initializes interaction parameters\nwith button configurations and movement speed\n15 function initializeInteractionParameters\n() {\n16 return {\n17 // Always make button as a list ,\nwhich contains all button instantiations and declare\nit as \" button :[...]\"\n18 buttons : [ // Array of button\nconfigurations\n19 {\n20 id: 1, // Unique\nidentifier for the first button\n21 size : 1, // Size of the\nbutton , 1 indicates a single unit button\n22 position : [ // Position\nof the first button , calculated to be on the right\nside\n23 Math . floor ((2 *\nShapeDisplay . grid_x ) / 3) ,\n24 Math . floor (\nShapeDisplay . grid_y - 4) ,\n25 ],\n26 init_height : 50 , //\nInitial height of the button above the baseline\n27 },\n28 {\n29 id: 2, // Unique\nidentifier for the second button\n30 size : 1, // Size of the\nbutton , also a single unit button\n31 position : [ // Position\nof the second button , calculated to be on the left\nside\n32 Math . floor (\nShapeDisplay . grid_x / 3) ,\n33 Math . floor (\nShapeDisplay . grid_y - 4) ,\n34 ],\n35 init_height : 50 , //\nInitial height of the button above the baseline\n36 },\n37 ],\n38 moveSpeed : 0.1 , // Speed at which\nthe square will move when a button is pressed\n39 };\n40 }\n41\n42 // Main interaction logic , processes\nbutton presses and adjusts the square 's position\naccordingly\n43 function dynamicInteraction ( deltaTime ,\nparams , parentParams ) {\n44 initializeButtons ( params ); //\nInitializes the buttons at the start\n45\n46 // Iterates over all pins to process\nbutton presses and move the square\n47 ShapeDisplay . Pins . forEach (( pin ) => {\n48 if ( pin . isButton ) {\n49 processButtonPress (pin ,\nparams , parentParams ); // Processes button press for\nmovement\n50 }\n51 });\n52 }\n53\n54 // Processes button presses to move the\nsquare left or right based on the button pressed\n55 function processButtonPress (pin , params ,\nparentParams ) {\n56 if ( pin . isPressing ) { // Checks if\nthe button ( pin ) is being pressed\n57 if ( pin . buttonGroup_id == 1) {\n58 parentParams . squarePosX +=\nparams . moveSpeed ; // Moves the square to the right\nfor button 1\n59 } else if ( pin . buttonGroup_id ==\n2) {\n60 parentParams . squarePosX -=\nparams . moveSpeed ; // Moves the square to the left\nfor button 2\n61 }\n62 }\n63 }\n64 \\\"\\\"\\\"\n65 }\n66 \"\"\"\n67 },\nListing 12: Interaction Agent Vector Store Example\nD TECHNICAL EVALUATION INPUT\nPROMPTS\n1 Please create a grid of long rods that move up and down\ndepending on the location of the product .\n2 pins move from center to exterior in a wave .\n3 Create a shape changing effect where the shapes undulate\nfrom the center as if a rock was dropped in a pond .\n4 Create pins that correspond to movement above it.\n5 Can you create a sine wave that continues from right to\nleft with a large amplitude located at the far lower\nhalf ?\n6 create a serious of pegs that rise randomly\n7 Create a series of blocks in staircase format and end\nwith blocks upright with two blocks in a row from\nthe lower bottom of the screen to the top of the\nscreen\n8 Staircase shaped blocks spawn out of nowhere and move to\nthe left\nUIST ’24, October 13–16, 2024, Pittsburgh, PA, USA Qian and Gao, et al.\n9 When I press the top left square on the left side , make\nboth the vertical line on the left and horizontal\nline on the bottom taller and longer while making\nthe square shape grow bigger overall and growing\ntowards the top right .\n10 make the letter t appear out of the surface , then e, then\ni\n11 Create a shape changing effect that shifts away from the\nball as I move it around the screen .\n12 Start with a medium wedge shape . As an object is moved\nalong the surface behind it , the wedge should shift\nin the same direction , staying roughly the same size\n.\n13 Have sticks move up and down across one after the other\ntwo create a wave like effect .\n14 lift up slides up from left to right\n15 Show a person scrolling on their phone , zoomed in so you\ncan see the right border of the phone , and show pins\nmoving up and down correlating with the scrolling .\n16 Create a line of pins on the side of the smartphone that\nproduces a wave like motion parallel to the\ndirection the smartphone is scrolling .\n17 Create a ring where a ball can roll around in a circle in\n, and then create an arrow in the middle pointing\naway from where the ball currently is in the ring .\n18 Display Alphabet as I motion left to right .\n19 Create a large square on a table that creates a tidal\nwave effect from a cell phone placed in the upper\nright - hand corner\n20 Create a shape that can hold a phone or a similar gadget\n21 I want the lighted objects to protrude upward randomly .\nThe motion does not need to be in sync with each\nother . There can be more than one object that\nprotrudes at the same time .\n22 Create a visual effect of a square grid of 1 inch cubes\nthat are raised up in a torus , with a depression in\nthe middle and circular depression on the outside\nperimeter of the torus . When a sphere is placed in\nthe center of the torus lay all of the cubes at the\nsame level .\n23 Can you make the pins pop to replicate a ripple effect of\nwater , starting from the middle and flowing\noutwards .\n24 Can you show me blocks that rise and fall when touched to\ncreate a waterdrop effect ?\n25 make a heart get bigger and smaller with a pop background\n26 lift cubes up in rows to form a square and change it rows\nand put the others level down\n27 make a jagged sawtooth waveform constantly moving right\nto left with new sawtooths coming into view on the\nright as others leave to the left . When a hand drops\nin front of a sawtooth , change to a series of tall\ntowers moving from right to left .\n28 create a ripple effect coming from the bottom right\ncorner , then again from the top left corner\n29 The shapes need to wave outwards .\n30 When you press your hand on the pieces , make them form a\nripple effect , moving outward away from where you\npressed .\n31 Create a circle with a smaller circle inside and only\nallow one to pop up at once\n32 make the table appear to undulate in a wave from left to\nright\n33 Make the cubes run in a wave motion from left to right\n34 create graph lines that move from shortest to longest\nwhen the long line is touched\n35 Construct a pattern of cubes that fellow the input from a\nred spherical shape . Raise cubes with input from\nthe sphere on the opposite side of the space .\n36 Make the shapes appear in as a moving wave , from right to\nleft . The waves are identical and have a peak to\nthe right with a gradual decline to the left .\n37 Back left rises , back 2nd rises but not as high , back\nright rises a little bit higher than back 2, then\nthey all go down . Back left rises again , then down ,\nthen back up again and stays up. Back 2 rises again ,\nthen back right , and they all go down again just\nlike the first time .\n38 Create 3D objects to move with motion or direction of\nhands or any sensor\n39 Draw a scaled pattern of blocks from right to left on a\nconveyor\n40 create a bowl for the oranges using the pins . make it\nshaped to fit\n41 Use the pin graph to display the letters T, E, and I, one\nat a time and in upper case , filling the entire\nboard with each letter .\n42 Some of the pieces rise up , around the inner rectangle .\nThey form a ripple effect . Pieces toward the outer\nedge rise up as well then they all form a pulsating\nmovement .\n43 raise pins in the middle to create a hollow light bulb\nshape , inverse the pins , then raise the pins again\nto a filled light bulb shape\n44 Create a shape that creates a ripple effect on blocks\nwhereever it is touched\n45 Can you make a hill mountain type design that follows the\nred ball\n46 Create 3 vertically long areas that contain multiple\nlines that move together from left to right to form\nwave shapes .\n47 make 4 imges that are different from each other in sizes\nand colors , and have them move down a line 1 at a\ntime .\n48 Create three 3d rectangles displaying approximately 10\nrows of moving waves , using a grids of rectangles .\n49 Can you show me an animation of rectangular blocks that\nmove in groups , starting from the right side of the\nscreen to the left ?\n50 On the far side of the surface , create an effect in which\nthe pins rise and fall in a steady cadence , like\nthat of a sewing machine , with the pins alternating\nbetween extending and retracting from left to right .\nThe row of pins directly in front of this effect\nshould remain stationary in a pattern of two pins\nextended 1/3 of the way and one pin extended fully .\nIn the middle of the surface , form a wave shape\nusing one row of pins . The shape should start in the\nmiddle of the surface , then split into two waves\nmoving away from the middle towards the left and\nright sides of the surface . On the nearest side of\nthe surface , form a wave shape that moves from right\nto left .\nListing 13: TechEval Prompts\nE SUMMARY OF USER CREATIONS DURING\nUSER STUDY\nSHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs UIST ’24, October 13–16, 2024, Pittsburgh, PA, USA\nCategory Subcategory Keywords of Attempted Creation Result\nPrimitive\nBasic shapes\nCircle Success\nCube Success\nTriangle Success\n3D and complex shape\nSphere Success\nFirework Success\nFan Success\nMushroom Success\nPiano Keyboard Success\nVolcano Success\nCity Block Success\nDune-like Mountain Success\n’hello’ & ’tiger’ text Success\nTetrahedral Partial Success\nMoon and Earth Partial Success\nFour-Wheel Car Partial Success\nBird Head Partial Success\nEyes Partial Success\nCat Face Failed\nZombies Failed\nWhale Failed\nLung Shape Failed\nHand Failed\nIconic shape\nStar Success\nDaisy with 8 Petals Success\nStick Figure Success\nSmiley Face Partial Success\nPattern\nRipple in Pool Success\nWater Drop Partial Success\nHeartbeat Graph Failed\nAnimation\nBasic motion\nStar Twinkle Success\nCircling around Success\nChasing Success\nRaining effect Success\nRunning Bird&Man Partial Success\nOrbiting Failed\nTransformation\nEye blinking Failed\nMorphing Circle to Square Failed\nSimulate pressing Piano Keyboard Failed\nInteraction\nTrigger\nButton to Appear Text Success\nButton for Firework Animation Success\nButton to Move Circle randomly Failed\nButton to Change Direction/Angle Failed\nButton to Jump Circle Failed\nMapping and control\nButton to Rotate Star Success\nButtons to move Star&Circle Success\nButton to Control Skyscraper Height Success\nButton to Control Speed Failed\nTable 1: Summary of user study results",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.6115614175796509
    },
    {
      "name": "Computer science",
      "score": 0.5840397477149963
    },
    {
      "name": "Generative model",
      "score": 0.46637633442878723
    },
    {
      "name": "Artificial intelligence",
      "score": 0.418865442276001
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.3765535056591034
    },
    {
      "name": "Human–computer interaction",
      "score": 0.34741324186325073
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I168635309",
      "name": "University of Calgary",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    }
  ]
}