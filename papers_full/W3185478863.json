{
  "title": "Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation",
  "url": "https://openalex.org/W3185478863",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100362614",
      "name": "Wenhao Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100410326",
      "name": "Hong Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5038044561",
      "name": "Runwei Ding",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100705472",
      "name": "Mengyuan Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042680345",
      "name": "Pichao Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026184280",
      "name": "Wenming Yang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2886552305",
    "https://openalex.org/W2963598138",
    "https://openalex.org/W2908684875",
    "https://openalex.org/W3119588134",
    "https://openalex.org/W2895689136",
    "https://openalex.org/W3117576675",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2805088284",
    "https://openalex.org/W2799211965",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W2134704262",
    "https://openalex.org/W3173811519",
    "https://openalex.org/W2099333815",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2984280075",
    "https://openalex.org/W2969450957",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3106838237",
    "https://openalex.org/W2913397815",
    "https://openalex.org/W3106882556",
    "https://openalex.org/W3034581612",
    "https://openalex.org/W2120051910",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3136525061",
    "https://openalex.org/W3118802333",
    "https://openalex.org/W2612706635",
    "https://openalex.org/W3175199633",
    "https://openalex.org/W3098612954",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3092900809",
    "https://openalex.org/W3098473649",
    "https://openalex.org/W3034448411",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2964016027",
    "https://openalex.org/W2962824791",
    "https://openalex.org/W2809890486",
    "https://openalex.org/W3159460161",
    "https://openalex.org/W3169891778",
    "https://openalex.org/W2293220651",
    "https://openalex.org/W2972662547",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W2963441822",
    "https://openalex.org/W2963225971",
    "https://openalex.org/W2798646183",
    "https://openalex.org/W3097623574",
    "https://openalex.org/W2981660954",
    "https://openalex.org/W2554247908",
    "https://openalex.org/W2963076553",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Despite the great progress in 3D human pose estimation from videos, it is still an open problem to take full advantage of a redundant 2D pose sequence to learn representative representations for generating one 3D pose. To this end, we propose an improved Transformer-based architecture, called Strided Transformer, which simply and effectively lifts a long sequence of 2D joint locations to a single 3D pose. Specifically, a Vanilla Transformer Encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce the redundancy of the sequence, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively shrink the sequence length and aggregate information from local contexts. The modified VTE is termed as Strided Transformer Encoder (STE), which is built upon the outputs of VTE. STE not only effectively aggregates long-range information to a single-vector representation in a hierarchical global and local fashion, but also significantly reduces the computation cost. Furthermore, a full-to-single supervision scheme is designed at both full sequence and single target frame scales applied to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision and hence helps produce smoother and more accurate 3D poses. The proposed Strided Transformer is evaluated on two challenging benchmark datasets, Human3.6M and HumanEva-I, and achieves state-of-the-art results with fewer parameters. Code and models are available at \\url{https://github.com/Vegetebird/StridedTransformer-Pose3D}.",
  "full_text": "1\nExploiting Temporal Contexts with Strided\nTransformer for 3D Human Pose Estimation\nWenhao Li, Hong Liu †, Runwei Ding, Mengyuan Liu, Pichao Wang, and Wenming Yang\nAbstract—Despite the great progress in 3D human pose es-\ntimation from videos, it is still an open problem to take full\nadvantage of a redundant 2D pose sequence to learn repre-\nsentative representations for generating one 3D pose. To this\nend, we propose an improved Transformer-based architecture,\ncalled Strided Transformer, which simply and effectively lifts\na long sequence of 2D joint locations to a single 3D pose.\nSpeciﬁcally, a Vanilla Transformer Encoder (VTE) is adopted to\nmodel long-range dependencies of 2D pose sequences. To reduce\nthe redundancy of the sequence, fully-connected layers in the\nfeed-forward network of VTE are replaced with strided convolu-\ntions to progressively shrink the sequence length and aggregate\ninformation from local contexts. The modiﬁed VTE is termed\nas Strided Transformer Encoder (STE), which is built upon the\noutputs of VTE. STE not only effectively aggregates long-range\ninformation to a single-vector representation in a hierarchical\nglobal and local fashion, but also signiﬁcantly reduces the com-\nputation cost. Furthermore, a full-to-single supervision scheme\nis designed at both full sequence and single target frame scales\napplied to the outputs of VTE and STE, respectively. This scheme\nimposes extra temporal smoothness constraints in conjunction\nwith the single target frame supervision and hence helps produce\nsmoother and more accurate 3D poses. The proposed Strided\nTransformer is evaluated on two challenging benchmark datasets,\nHuman3.6M and HumanEva-I, and achieves state-of-the-art\nresults with fewer parameters. Code and models are available\nat https://github.com/Vegetebird/StridedTransformer-Pose3D.\nIndex Terms—3D human pose estimation, Transformer, Strided\nconvolution.\nI. I NTRODUCTION\n3\nD human pose estimation is a classic computer vision\ntask that aims to estimate 3D joint locations of a human\nbody from images or videos. This task has drawn tremendous\nattention in the past decades [1]–[4] since it plays a signif-\nicant role in wide applications, such as clinic [5], computer\nanimation [6], action recognition [7]–[16], and human-robot\ninteraction [17], [18]. Many state-of-the-art approaches adopt\na two-stage pipeline ( i.e., 2D-to-3D lifting method) [19]–[21],\n†Corresponding author.\nW. Li, H. Liu, and R. Ding are with Key Laboratory of Machine Perception,\nShenzhen Graduate School, Peking University, Beijing 100871, China. E-\nmail: {wenhaoli, hongliu, dingrunwei }@pku.edu.cn. M. Liu is with School\nof Intelligent Systems Engineering, Sun Yat-sen University, China. E-mail:\nnkliuyifang@gmail.com. P. Wang is with Alibaba Group, Bellevue, W A,\n98004, USA. E-mail: pichao.wang@alibaba-inc.com. W. Yang is with the\nShenzhen Key Lab of Information Science and Technology, Shenzhen En-\ngineering Lab of IS&DRM, Department of Electronic Engineering, Graduate\nSchool at Shenzhen, Tsinghua University, Shenzhen 518055, China. E-mail:\nyang.wenming@sz.tsinghua.edu.cn\nThis work is supported by National Key R&D Program of China (No.\n2020AAA0108904), Basic and Applied Basic Research Foundation of Guang-\ndong (No. 2020A1515110370), Science and Technology Plan of Shenzhen\n(Nos. JCYJ20190808182209321, JCYJ20200109140410340).\n…\n…\n…\nSTE\nVTE\nFig. 1: Our Strided Transformer Encoder (STE) takes the\noutputs of Vanilla Transformer Encoder (VTE) as input (yel-\nlow) and generates a 3D pose for the target frame as output\n(top). The self-attention mechanism (blue) concentrates on\nglobal context and the strided convolution (green) aggregates\ninformation from local contexts.\nwhich ﬁrst estimates 2D keypoints and then lifts them to 3D\nspace. Although the 2D-to-3D lifting method beneﬁts from the\nreliable performance of 2D pose detectors, it is still a highly\nill-posed problem due to the inherent ambiguity in depth, since\nmultiple 3D interpretations can be projected to the same 2D\npose in the image space.\nTo alleviate this problem, temporal context information has\nbeen investigated by many researchers. Some methods [22]–\n[24] leverage past and future data in the sequence to predict\nthe 3D pose of the target frame. For instance, Cai et al. [24]\npresented a local-to-global graph convolutional network to\nexploit spatio-temporal relations to estimate 3D keypoints\nfrom a 2D pose sequence. However, these approaches have\nsmall temporal receptive ﬁelds and limited temporal cor-\nrelation windows, thus suffering from modeling long-range\ndependencies.\nVanilla Transformer [25] is developed for exploiting long-\nrange dependencies and achieves tremendous success in natu-\nral language processing [26], [27] and computer vision [28]–\n[32]. It consists of a self-attention module and a position-\nwise feed-forward network (FFN). The self-attention module\ncomputes pairwise dot-product among all input elements to\narXiv:2103.14304v8  [cs.CV]  11 Jan 2022\n2\n…\n…\n…\n…\n…\n…\n0 520 ms\nFig. 2: Example of 2D pose sequences of 27 consecutive frames (520 ms) on Human3.6M dataset (captured from 50 Hz\ncameras). It contains huge redundant information as nearby poses are same. The rectangle denotes the center frame.\ncapture global-context information, and the FFN acts as pattern\ndetectors over the input across all layers [33]. Such a design\nlooks like a good choice for the 2D-to-3D pose lifting method\nto capture long-range dependencies. However, there are several\nshortcomings in the Vanilla Transformer Encoder (VTE) [25]:\n(i) The full-length sequence in the forward pass across all\nlayers contains signiﬁcant redundancy for video-based pose\nestimation as nearby poses are quite similar, as illustrated in\nFig. 2. (ii) The time and memory complexity of the attention\noperation grows quadratically with the input length, making it\nvery expensive to process long sequences. Thus, the receptive\nﬁeld may be forced to decrease in real-time applications,\nwhereas a large receptive ﬁeld is important to enhance the\nestimation consistency [34]. (iii) The VTE architecture is less\ncapable to extract ﬁne-grained local feature patterns, which\nis well-known to be crucial for computer vision tasks. To\nmitigate these issues, we propose to gradually merge nearby\nposes to shrink the sequence length until one representation\nof the target pose is acquired. An alternative is to perform\npooling operation after the FFN [27]. However, lots of valuable\ninformation will be lost using pooling operation, and the\nlocal information can not be well exploited. Motivated by the\nprevious methods [20], [34] that are able to elegantly han-\ndle variable-length sequences via temporal convolutions, we\npropose to replace fully-connected layers in FFN with strided\nconvolutions to progressively reduce the sequence length. The\nmodiﬁed Transformer is dubbed Strided Transformer Encoder\n(STE), as shown in Fig. 1. With the proposed STE, we can\nmodel both global and local information in a hierarchical\narchitecture, and the computation in FFN can be traded off\nfor constructing a deeper model to boost the model capacity.\nAlthough the STE can aggregate long-range information to\na single-pose representation, it remains a question whether this\nsingle representation is enough to represent a long sequence\nand how to make this representation work in improving the\nperformance. We observe that directly supervising the model at\na single target frame scale always breaks temporal smoothness\namong video frames, while only supervising at a full sequence\nscale cannot explicitly learn a speciﬁc representation for the\ntarget frame. These observations encourage us to develop a\nmethod that can effectively embed both scales into a learnable\nframework. Therefore, based on the outputs of VTE and STE,\na full-to-single supervision scheme is designed at both full and\nsingle scales, which can impose extra temporal smoothness\nconstraints at the full sequence scale and reﬁne the estimation\nat the single target frame scale. This scheme brings great\nbeneﬁts in producing smoother and more accurate 3D poses.\nThe proposed architecture is called Strided Transformer,\nas shown in Fig. 3. Extensive experiments are conducted\non two standard 3D human pose estimation datasets, i.e.,\nHuman3.6M [35] and HumanEva-I [36]. Experimental results\nshow that the proposed method achieves state-of-the-art per-\nformance.\nOur contributions are summarized as follows:\n• We propose a new Transformer-based architecture for 3D\nhuman pose estimation called Strided Transformer, which\ncan simply and effectively lift a long 2D pose sequence\nto a single 3D pose.\n• To reduce the sequence redundancy and computation cost,\nStrided Transformer Encoder (STE) is introduced to grad-\nually reduce the temporal dimensionality and aggregate\nlong-range information into a single-vector representation\nof pose sequences in a hierarchical global and local\nfashion.\n• A full-to-single supervision scheme is designed to impose\nextra temporal smoothness constraints during training at\nthe full sequence scale and further reﬁne the estimation\nat the single target frame scale.\n• State-of-the-art results are achieved with fewer parame-\nters on two commonly used benchmark datasets, making\nour method a strong baseline for Transformer-based 3D\npose estimation.\nII. R ELATED WORK\nAt the early stage of applying deep neural networks on\n3D pose estimation task, many methods [37]–[40] learned\nthe direct mapping from RGB images to 3D poses ( i.e.,\none-stage pose estimation). However, these methods require\nsophisticated architectures with high computation costs, which\nare impractical in realistic applications.\nTwo-stage pose estimation. Two-stage methods formulate\nthe problem of 3D human pose estimation as 2D keypoint\ndetection followed by 2D-to-3D lifting estimation [19], [41],\n[42]. Recent works show that 3D locations of body joints\ncan be efﬁciently and effectively recovered using detected 2D\nposes from state-of-the-art 2D pose detectors, and this 2D-\nto-3D pose lifting method outperforms one-stage approaches.\nFor example, Martinez et al. [19] lifted 2D joint locations\nto 3D space via a fully-connected residual network. Fang et\nal. [41] proposed a pose grammar model to encode the human\n3\nInput Frames\n2D Pose Sequences\n3D Pose Sequences 3D Pose\nGT Frames GT \nTake Center Frame\nT+1\nT-1\nT T\n3D Human Pose Estimaiton with Strided Transformer\nSupervision\nVanilla\nTransformer\nEncoder\nStrided\nTransformer\nEncoder \nFig. 3: Overview of our proposed Strided Transformer for predicting the 3D joint locations of the target frame (center frame)\nfrom the estimated 2D pose sequences. It mainly consists of a Vanilla Transformer Encoder (VTE) and a Strided Transformer\nEncoder (STE). The network ﬁrst models long-range information via VTE and then aggregates the information into one target\npose representation from the proposed STE. The model is trained end-to-end at both full sequence and single target frame\nscales.\nbody conﬁguration of human poses from 2D space to 3D\nspace. To improve the generalization of the trained 2D-to-\n3D pose estimator, Gong [43] introduced a pose augmentation\nframework that is differentiable. We also follow this two-stage\npipeline because it is widely adopted among the state-of-the-\nart methods in this domain.\nVideo pose estimation. Recently, many approaches tried to\nexploit temporal information [20], [23], [24], [44] to improve\nthe accuracy and the smoothness of the estimated 3D pose se-\nquence. To predict temporally consistent 3D poses, Hossain et\nal. [23] designed a sequence-to-sequence network with LSTM.\nPavllo et al. [20] introduced a fully convolutional model based\non dilated temporal convolutions. Cai et al. [24] directly chose\nthe 3D pose of the target frame from the outputs of the\nproposed graph-based method and then fed it to a reﬁnement\nmodel. To produce smoother 3D sequences, Wang et al. [44]\ndesigned an U-shaped graph convolutional network and in-\nvolved motion modeling into learning. However, the tempo-\nral connectivity of these architectures is inherently limited\nand is mainly constrained to simple sequential correlations.\nDifferent from most existing works that employed LSTM-\nbased [23], graph-based [24], [44], or temporal convolutional\nnetworks [20], [34], [45] to exploit temporal information,\nwe propose a Transformer-based architecture to capture long-\nrange dependencies from input 2D pose sequences. Further-\nmore, compared with previous methods [24], [44] that either\nutilize a reﬁnement model or use a motion loss to improve\nestimations, we design a full-to-single supervision scheme that\nreﬁnes the intermediate predictions to produce smoother and\nmore accurate estimations.\nVisual Transformers. Transformer models ﬁrst proposed\nin [25] are commonly used in various language tasks. Recently,\nTransformers have shown promising performance in many\ncomputer vision tasks, such as object detection [46], [47]\nand image classiﬁcation [48], [49]. DETR [46] presented\na new Transformer-based design for object detection sys-\ntems. ViT [48] proposed to apply a standard Transformer\narchitecture directly to sequential image patches for image\nclassiﬁcation. METRO [50] introduced a Transformer frame-\nwork to reconstruct 3D human pose and mesh from a single\nimage. However, METRO focused on the one-stage pose\nestimation and ignores the temporal information across frames.\nUnlike DETR [46], ViT [48], or METRO [50] that directly\napply Transformer to images, we utilize a Transformer-based\narchitecture to effectively map 2D keypoints to 3D poses.\nAdditionally, efﬁcient strided convolutions are incorporated\ninto Transformer models to address the redundancy problem\nfor the video-based 3D pose estimation task.\nIII. M ETHOD\nIn this section, we ﬁrst present an overview of the proposed\nStrided Transformer for 3D human pose estimation from a\n2D video stream, and then show how our Transformer-based\narchitecture learns a representative single-pose representation\nfrom redundant sequences resulting in an enhanced estimation.\nFinally, the complexity analysis of our network is presented.\nA. Overview\nThe overall framework of our proposed method is illustrated\nin Fig. 3. Given a sequence of the estimated 2D poses\nP = {p1,...,p T }from videos, we aim at reconstructing 3D\njoint locations X ∈RJ×3 for a target frame (center frame),\nwhere pt ∈RJ×2 denotes the 2D joint locations at frame t,\nT is the number of video frames, and J is the number of\njoints. The network contains a Vanilla Transformer Encoder\n(VTE) followed by a Strided Transformer Encoder (STE),\nwhich is trained in a full-to-single prediction scheme at both\nfull sequence and single target frame scales. Speciﬁcally, VTE\nis ﬁrst used to model long-range information and is supervised\nby the full sequence scale to enforce temporal smoothness.\nThen, the proposed STE aggregates the information to generate\none target pose representation and is supervised by the single\ntarget frame scale to produce more accurate estimations.\nB. Strided Transformer Encoder\nDespite the substantial performance gains achieved by\nTransformers [25] in many computer vision tasks, the full-\nlength token representation makes it unsuitable for many\n4\nvideo-based vision tasks that only require a single-vector\nrepresentation of a sequence. To this end, STE is proposed to\ngradually compress the sequence of hidden states and model\nboth global and local information in a hierarchical architecture.\nEach layer of the proposed STE consists of a multi-head self-\nattention (MSA) and a convolutional feed-forward network\n(CFFN).\n1) Multi-head self-attention: The core mechanism of the\nTransformer model is MSA [25]. Suppose there are a set of\nqueries (Q), keys (K), and values (V) of dimension dm. Then\nthe MSA can be computed as:\nheadi = Self-Attn\n(\nQWQ\ni ,KW K\ni ,VW V\ni\n)\n, (1)\nMSA(Q,K,V ) = Concat (head1,...,head h) WO, (2)\nwhere Self-Attn(Q,K,V ) = softmax\n(\nQKT /√dk\n)\nV and\nWQ\ni ∈ Rdm×dk ,WK\ni ∈ Rdm×dk ,WV\ni ∈ Rdm×dv , and\nWO ∈Rhdv×dm are parameter matrices. The hyperparameter\nhis the number of multi-attention heads, dm is the dimension\nof the model, and dk = dv = dm/h in our implementation.\n2) Convolutional feed-forward network: In the existing\nfully-connected (FC) layers in the FFN of VTE (Eq. (3)), it\nalways maintains a full-length sequence of hidden representa-\ntions across all layers with a high computation cost. It contains\nsigniﬁcant redundancy for video-based pose estimation, as\nnearby poses are quite similar. However, to reconstruct more\naccurate 3D body joints of the target frame, crucial information\nshould be extracted from the entire pose sequences. Therefore,\nit requires selectively aggregating useful information.\nTo tackle this issue, inspired by the previous works [20],\n[34] that employ temporal convolutions to effectively shrink\nthe sequence length, we make modiﬁcations to the generic\nFFN. Given the input feature vector Z ∈ RT×Din with\nT sequences and Din channels to generate an output of\n( ˜T,Dout) features, the operation performed by FC in FFN\ncan be formulated as:\nFCt,dout(z) =\nDin∑\ni\nwdout,i ∗zt,i . (3)\nIf 1D convolution is considered with kernel size K and\nstrided factor S, a strided convolution in CFFN can be\ncomputed as:\nConvS(t),cout(z) =\nDin∑\ni\nK∑\nk\nwdout,i,k ∗zS(t−K−1\n2 +k),i . (4)\nIn this way, fully-connected layers in FFN of VTE are\nreplaced with strided convolutions. The modiﬁed VTE is\ntermed as Strided Transformer Encoder (STE), which can be\nrepresented as:\nˆZn−1 = Zn−1 + MSA(LN(Zn−1)), (5)\nZn = MaxPool( ˆZn−1) + CFFN(LN(ˆZn−1)), (6)\nwhere LN(·) denotes the layer normalization, MaxPool(·)\ndenotes the max pooling operation, and n ∈ [1,...,N ] is\nthe index of STE layers.\nThe STE is a hierarchical global and local architecture,\nwhere the self-attention mechanism models global context\nPosition\nEmbedding\nPose Embedding\nMulti-Head\nAttention\nRegression Head\nMulti-Head\nAttention\nRegression Head\nPosition\nEmbedding\nN1× ×N2 \nLinear, df 1D Conv, kf, sf, df\nLinear, dm 1D Conv, km, sm, dm\nMaxPool\nSTE\nFFN CFFN\nReLU, Dropout ReLU, Dropout\nVTE\nLayerNorm\nLayerNorm\n… …\n… …\n……\nLayerNorm\nLayerNorm\nFig. 4: The network architecture of our proposed Strided\nTransformer. The left is the VTE and the right is the STE.\nHere, N1 and N2 denote the number of layers of the two\nmodules, respectively. The hyperparameters k, s, dm and df\nare the kernel size, the strided factor, the dimension, and the\nnumber of hidden units. The max pooling operation is applied\nto the residuals to match the temporal dimensions.\nand the strided convolution helps capture local contexts, as\npresented in Fig. 4 (right). It gradually merges the nearby\nposes to a short sequence length representation, as illustrated\nin Fig. 5. Importantly, through such a hierarchical design, the\nredundancy of the sequence and the computation cost can be\nreduced.\nC. Network Architecture\nIn this section, we describe how to use the proposed\nTransformer-based network architecture to estimate 3D human\nposes from a sequence of 2D poses. As shown in Fig. 5, the\nproposed network is composed of four components: a pose\nembedding, a Vanilla Transformer Encoder (VTE), a Strided\nTransformer Encoder (STE), and a regression head.\n1) Pose embedding: Given a sequence of the estimated 2D\nposes P ∈RT×J×2, the pose embedding ﬁrst concatenates\n(x,y) coordinates of the J joints for each frame to tokens P′∈\nRT×(J·2), and then embeds each token to a high dimensional\nfeature Z0 ∈ RT×dm using a 1D convolutional layer with\ndm channels, followed by batch normalization, dropout, and a\nReLU activation.\n2) Vanilla Transformer Encoder: Suppose that the VTE\nconsists of N1 layers, the learnable position embedding E1 ∈\nRT×dm is used before the ﬁrst layer of VTE, which can be\nformulated as follows:\nZ0\n1 = Z0 + E1. (7)\n5\n1D Conv, 1, 1, 51\nVTE Layer\nVTE Layer\nVTE Layer\nPosition Embedding\n(27, 34)\n(1, 51)\n1D Conv, 1, 1, 256\nBatchNorm 1D\nReLU, Dropout\nBatchNorm 1D\nPose Embedding VTE Regression Head\nSTE Layer\nLayerNorm\nAttention Block\n1D Conv, 1, 1, 512\nReLU, Dropout\n1D Conv, 3, 3, 256\nLayerNorm\n(27, 256) (9, 256)\nPosition Embedding\nLayerNorm\nAttention Block\n1D Conv, 1, 1, 512\nReLU, Dropout\n1D Conv, 3, 3, 256\nLayerNorm\n(3, 256) (1, 256)\nSTE Layer\nPosition Embedding\nLayerNorm\nAttention Block\n1D Conv, 1, 1, 512\nReLU, Dropout\n1D Conv, 3, 3, 256\nLayerNorm\n(9, 256) (3, 256)\nSTE Layer\nPosition Embedding\nMaxPoolMaxPool MaxPool\nFig. 5: An instantiation of the proposed Strided Transformer network. It reconstructs the target 3D body joints by progressively\nreducing the sequence length. The input consists of 2D keypoints for a receptive ﬁeld of 27 frames with J = 17 joints.\nConvolutional feed-forward networks are in blue where (3,3,256) denotes kernels of size 3 with strided factor 3 and 256\noutput channels. The tensor sizes are shown in parentheses, e.g., (27,34) denotes 27 frames and 34 channels. Due to strided\nconvolutions, the max pooling operation is applied to the residuals to match the shape of subsequent tensors.\nThen, given the embedded feature Z0\n1 , the VTE layers can\nbe represented as:\nˆZn−1\n1 = Zn−1\n1 + MSA(LN(Zn−1\n1 )), (8)\nZn\n1 = ˆZn−1\n1 + FFN(LN( ˆZn−1\n1 )), (9)\nwhere n ∈[1,...,N 1] is the index of VTE layers. It can be\nexpressed by using a function of a VTE layer VTE(·):\nZn\n1 = VTE(Zn−1\n1 ). (10)\n3) Strided Transformer Encoder: For the STE, it is built\nupon the outputs of VTE and takes the ZN1\n1 ∈RT×dm as\ninput. The learnable position embeddings E2 ∈ RS(t)×dm\nwith strided factor S are used for every layer of STE due\nto the different sequence lengths. Then, the STE layers can be\nrepresented as follows:\nZn\n2 = STE(Zn−1\n2 + En\n2 ), (11)\nwhere n∈[1,...,N 2] is the index of STE layers, Z0\n2 = ZN1\n1 ,\nand STE(·) denotes the function of an STE layer whose details\ncan be found in Eq. (5) and Eq. (6).\n4) Regression head: In order to perform the regression,\na batch normalization and a 1D convolutional layer are ap-\nplied to the outputs of VTE and STE, ZN1\n1 ∈RT×dm and\nZN2\n2 ∈R1×dm, respectively. Finally, the outputs of 3D pose\nprediction are ˜X and X, where ˜X ∈RT×J×3 and X ∈RJ×3\nare predictions of the 3D pose sequence and the 3D joint\nlocations of the target frame, respectively.\nD. Full-to-Single Prediction\nThe iterative reﬁnement scheme, aimed at producing pre-\ndictions in multiple processing stages, is effective for 3D\npose estimation [24], [37]. Motivated by the success of such\niterative processing, we also consider a reﬁnement scheme.\nA full-to-single scheme is proposed to incorporate both full\nsequence and single target frame scales constraints into the\nframework. This scheme further reﬁnes the intermediate pre-\ndictions to produce more accurate estimations rather than using\na single component with a single output. More precisely, the\nfull sequence scale can enforce temporal smoothness and the\nsingle target frame scale helps learn a speciﬁc representation\nfor the target frame.\n1) Full sequence scale: The ﬁrst step is to supervise at\nfull sequence scale by imposing extra temporal smoothness\nconstraints during training from the output of VTE followed\nby a regression head. A sequence loss Lf is used to improve\nupon single frame predictions for temporal consistency over\na sequence. This loss ensures that the estimated 3D pose\nsequences ˜X ∈RT×J×3 coincide with the ground truth 3D\njoint sequences Y ∈RT×J×3:\nLf =\nT∑\nt=1\nJ∑\ni=1\nYt\ni − ˜Xt\ni\n\n2\n, (12)\nwhere ˜Xt\ni and Yt\ni represent the sequence of estimated 3D\nposes and ground truth 3D joint locations of joint i at frame\nt, respectively.\n2) Single target frame scale: In the second step, the su-\npervision is adopted on the output of STE followed by a\nregression head. A single-frame loss Ls is used to reﬁne the\nestimation at the single target frame scale. It minimizes the\ndistance between the estimated 3D pose X ∈RJ×3 and the\ntarget ground truth 3D joint annotation Y ∈RJ×3:\nLs =\nJ∑\ni=1\n∥Yi −Xi∥2 , (13)\nwhere Xi and Yi represent the target frame’s estimated 3D\npose and ground truth 3D joint locations of joint i, respec-\ntively.\n3) Loss function: In our implementation, the model is\nsupervised at both full sequence scale and single target frame\nscale. We train the entire network in an end-to-end manner\nwith the total loss:\nL= λf Lf + λsLs, (14)\nwhere λf and λs are weighting factors.\nE. Complexity Analysis\nIn this section, we use ﬂoating-point operations (FLOPs) to\nmeasure the computational cost and analyze the compression\nratio of our proposed Strided Transformer network. Given the\nsequence length t, dimension dm = df /2 = d, strided factor\n6\ns, and kernel size k, the FLOPs of a VTE layer Fn\nV TE and\nan STE layer Fn\nSTE can be computed by:\nFn\nV TE(t,d) = Fn\nMSA (t,d) + Fn\nFFN (t,d)\n= 8td2 + 2t2d, (15)\nFn\nSTE (t,d,s ) = Fn\nMSA (t,d,s ) + Fn\nCFFN (t,d,s )\n= (6 + 2s−1k)td2 + 2t2d, (16)\nwhere Fn\nMSA , Fn\nFFN , and Fn\nCFFN are the FLOPs of the\nMSA, FFN, and CFFN, respectively.\nThen if we consider N layers of VTE and STE with\ninput sequence length T, dimension D, strided factor S, and\nkernel size K, the encoder-wise FLOPs of VTE FV TE can be\nformulated as:\nFV TE = NFn\nV TE = N(8TD2 + 2T2D), (17)\nthe encoder-wise FLOPs of STE FSTE can be formulated as:\nFSTE =\nN∑\nn=1\nFn\nSTE\n=\nN∑\nn=1\n[\n(6 + 2KS−1\nSn−1 )TD2 + 2\nS2(n−1) T2D\n]\n.\n(18)\nFor our 27-frame Strided Transformer, which contains N1\nVTE layers and N2 STE layers with N1 = N2 = N = 3 ,\nS = 3, and K = 3. In this case, the compression ratio α can\nbe computed by:\nα= 2FV TE\nFV TE+ FSTE\n= 2\n1 + β, (19)\nwhere\nβ = FSTE\nFV TE\n= 468D+ 91T\n972D+ 243T. (20)\nWe have limD→∞α = 1 .35 with a ﬁxed T. Thus, the\ncompression ratio α of our 27-frame Strided Transformer is\n1.35.\nIV. E XPERIMENTS\nA. Datasets and Evaluation\nThe proposed method is evaluated on two challenging\nbenchmark datasets, i.e., Human3.6M [35] and HumanEva-\nI [36]. Human3.6M dataset is the largest publicly available\ndataset for 3D human pose estimation, which consists of\n3.6 million images captured from 4 synchronized cameras\nwith 50 Hz. There are 7 professional subjects performing 15\ndaily activities such as “Waiting”, “Smoking”, and “Posing”.\nFollowing the standard protocol in prior works [20], [56], [57],\n5 subjects (S1, S5, S6, S7, S8) are used for training and 2\nsubjects (S9 and S11) are used for evaluation. The frames\nfrom all views are trained by a single model for all actions.\nHumanEva-I is a much smaller dataset with fewer subjects\nand actions compared to Human3.6M. Following [20], [22],\nour model is trained for all subjects (S1, S2, S3) and all actions\n(Walk, Jog, Box).\nThree standard evaluation protocols are used in the exper-\niments. The mean per joint position error (MPJPE) is the\naverage Euclidean distance between the ground truth and\npredicted positions of the joints, which is referred to as\nprotocol #1 in many works [41], [58]. Procrustes analysis\nMPJPE (P-MPJPE) is adopted, where the estimated 3D pose\nis aligned to the ground truth in translation, rotation, and\nscale. This protocol is referred to as protocol #2 [19], [23].\nFollowing [20], [44], [45], we also report the mean per joint\nvelocity error (MPJVE) corresponding to the MPJPE of the\nﬁrst derivative of the 3D pose sequences. This metric measures\nthe smoothness of predictions over time and is vital for video-\nbased 3D pose estimation.\nB. Implementation Details\nIn our experiments, the proposed Strided Transformer con-\ntains N1 = N2 = 3 encoder layers, h = 8 attention heads,\ndm = 256 dimensions, and df = 512 hidden units for both\nVTE and STE. The kernel sizes kf and km are set to 1 and\n3 in all STE layers, respectively. The strided factor sf is set\nto 1, and sm is set to {3,3,3}for the receptive ﬁeld of 27\nframes, {9,3,3}for 81, {3,9,9}for 243, and {3,9,13}for\n351. The weighting factors λf and λs are set to 1.\nAll experiments are conducted on the PyTorch framework\nwith one GeForce GTX 3090 GPU. The network is trained\nusing Amsgrad optimizer. An initial learning rate of 0.001 is\nused with a shrink factor of 0.95 applied after each epoch.\nThe same reﬁne module as [24], [44] is adopted. We only\napply horizontal ﬂip augmentation during training/test stages.\nThe 2D poses can be obtained by performing any classic 2D\npose detections or directly using the 2D ground truth. Follow-\ning [20], [59], the cascaded pyramid network (CPN) [60] is\nused for Human3.6M and Mask R-CNN [61] is adopted for\nHumanEva-I to obtain 2D poses for a fair comparison.\nC. Comparison with State-of-the-art Results\nOur method is compared with previous state-of-the-art ap-\nproaches on Human3.6M dataset. The performance of our\n351-frame model with CPN input is reported in Table I.\nOur method outperforms the state-of-the-art methods on Hu-\nman3.6M under all metrics (43.7 mm on protocol #1 and 35.2\nmm on protocol #2).\nTable II compares the computational complexity, MPJPE,\nand frame per second (FPS) with several state-of-the-art meth-\nods in different receptive ﬁelds on Human3.6M. Our model is\nlightweight and the number of parameters hardly increases\nwith the increased receptive ﬁelds, which is practical for\nreal-time applications. Compared with temporal convolutional\nnetworks [20], [45], our proposed Transformer-based network\nrequires fewer total parameters with competitive performance\nfor 3D pose estimation in videos. Besides, even though the\ninference speed of the proposed model is lower than [20],\n[45], it still has an acceptable FPS for real-time inference.\nFig. 7 shows some qualitative comparisons with state-of-the-\nart methods [20], [34], which indicates that our methods can\nproduce more accurate 3D predictions.\nTo further explore the upper bound of our method, the\nresults from 2D ground truth inputs are reported in Table III.\nIt can be seen that our method achieves the best result\n(28.5 mm in MPJPE), outperforming all other methods. This\n7\nTABLE I: Quantitative comparisons on Human3.6M under protocol #1 and protocol #2, where †indicates the temporal\ninformation used in each method. Best in bold, second-best underlined.\nProtocol #1 Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nMartinezet al. [19] ICCV’17 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9\nFanget al. [41] AAAI’18 50.1 54.3 57.0 57.1 66.6 73.3 53.4 55.7 72.8 88.6 60.3 57.7 62.7 47.5 50.6 60.4\nLee et al. [22] ECCV’18† 40.2 49.2 47.8 52.6 50.1 75.0 50.2 43.0 55.8 73.9 54.1 55.6 58.2 43.3 43.3 52.8\nXu et al. [42] CVPR’21 45.2 49.9 47.5 50.9 54.9 66.1 48.5 46.3 59.7 71.5 51.4 48.6 53.9 39.9 44.1 51.9\nGonget al. [43] CVPR’21 - - - - - - - - - - - - - - - 50.2\nCai et al. [24] ICCV’19† 44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8\nPavlloet al. [20] CVPR’19† 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8\nLin et al. [51] BMVC’19† 42.5 44.8 42.6 44.2 48.5 57.1 42.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1 46.6\nXu et al. [52] CVPR’20† 37.4 43.5 42.7 42.7 46.6 59.7 41.3 45.1 52.7 60.2 45.8 43.1 47.7 33.7 37.1 45.6\nLiu et al. [34] CVPR’20† 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1\nZenget al. [53] ECCV’20† 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6 44.8\nWanget al. [44] ECCV’20† 40.2 42.5 42.6 41.1 46.7 56.7 41.4 42.3 56.2 60.4 46.3 42.2 46.2 31.7 31.0 44.5\nChenet al. [45] TCSVT’21† 41.4 43.5 40.1 42.9 46.6 51.9 41.7 42.3 53.9 60.2 45.4 41.7 46.0 31.5 32.7 44.1\nOurs† 40.3 43.3 40.2 42.3 45.6 52.3 41.8 40.5 55.9 60.6 44.2 43.0 44.2 30.0 30.2 43.7\nProtocol #2 Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nMartinezet al. [19] ICCV’17 39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4 49.2 45.0 49.5 38.0 43.1 47.7\nPavlakoset al. [54] CVPR’18 34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8 42.6 39.6 43.9 32.1 36.5 41.8\nLiu et al. [55] ECCV’20 35.9 40.0 38.0 41.5 42.5 51.4 37.8 36.0 48.6 56.6 41.8 38.3 42.7 31.7 36.2 41.2\nGonget al. [43] CVPR’21 - - - - - - - - - - - - - - - 39.1\nCai et al. [24] ICCV’19† 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 33.2 39.0\nLin et al. [51] BMVC’19† 32.5 35.3 34.3 36.2 37.8 43.0 33.0 32.2 45.7 51.8 38.4 32.8 37.5 25.8 28.9 36.8\nPavlloet al. [20] CVPR’19† 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5\nXu et al. [52] CVPR’20† 31.0 34.8 34.7 34.4 36.2 43.9 31.6 33.5 42.3 49.0 37.1 33.0 39.1 26.9 31.9 36.2\nLiu et al. [34] CVPR’20† 32.3 35.2 33.3 35.8 35.9 41.5 33.2 32.7 44.6 50.9 37.0 32.4 37.0 25.2 27.2 35.6\nWanget al. [44] ECCV’20† 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5\nOurs† 32.7 35.5 32.5 35.4 35.9 41.6 33.0 31.9 45.1 50.1 36.3 33.5 35.1 23.9 25.0 35.2\nTABLE II: Quantitative comparisons with state-of-the-art\nmethods in different receptive ﬁelds on Human3.6M. The\ncomputational complexity, MPJPE, and frame per second\n(FPS) are reported. FPS is computed on a single GeForce GTX\n2080 Ti GPU.\nModel T Param (M) FLOPs (G) MPJPE (mm) FPS\nPavllo et al. [20] 27 8.56 0.017 48.8 1492\nPavllo et al. [20] 81 12.75 0.025 47.7 1121\nPavllo et al. [20] 243 16.95 0.033 46.8 863\nChen et al. [45] 27 31.88 0.061 45.3 410\nChen et al. [45] 81 45.53 0.088 44.6 315\nChen et al. [45] 243 59.18 0.116 44.1 264\nOurs (27 frames) 27 4.01 0.128 46.9 118\nOurs (81 frames) 81 4.06 0.392 45.4 112\nOurs (243 frames) 243 4.23 1.372 44.0 108\nOurs (351 frames) 351 4.34 2.142 43.7 105\ndemonstrates if a more robust 2D pose detection is available,\nour Strided Transformer can produce more accurate 3D poses.\nAs shown in Table IV, with the supervision of full sequence\nscale, our method reduces the MPJVE by 15.4% (from 2.6\nmm to 2.2 mm), achieving smoother predictions with lower\nMPJVE than other models. It indicates that the full-to-single\nsupervision scheme can enhance temporal smoothness and\nproduce vastly smoother poses.\nTo evaluate the generalizability of our model to smaller\ndatasets, experiments are conducted on HumanEva-I based on\nMask R-CNN 2D detections and 2D ground truth. The results\nin Table V demonstrate that our method achieves promising\nresults on all kinds of actions.\nD. Ablation Studies\nInput sequence length. The MPJPE results of our model\nwith different sequence lengths (between 1 and 351) on\nHuman3.6M are shown in Fig. 6 (a). It can be seen that\nour proposed method obtains larger gains under both 2D\npose inputs (CPN and GT) with more input frames used for\npredictions, but the error saturates past a certain point. This\nis expected since directly lifting 3D poses from disjointed\n2D poses leads to temporally incoherent outputs [62]. It is\nworth mentioning that our method gets a better result with\nT = 351 (43.7 mm) than T = 243 (44.0 mm), while\nthe performance decreases with longer inputs ( T > 243)\nin [34]. This indicates that our method equipped with the\nglobal self-attention mechanism is powerful in modeling long-\nrange dependencies. Meanwhile, with the help of STE, our\nmethod can learn the representative representation from long\nsequences. Next, we choose T = 27 on Human3.6M in the\nfollowing ablation experiments as a compromise between the\naccuracy and computational complexity.\n2D detections. For the 2D-to-3D pose lifting task, the\naccuracy of the 2D detections directly inﬂuences the results\nof 3D pose estimation [19]. To show the effectiveness of our\nmethod on different 2D pose detectors, we carry out exper-\niments with the detections from Stack Hourglass (SH) [63],\nDetectron [20], and CPN [60]. Moreover, to test the tolerance\nof our method to different levels of noise, we also train\nour network by 2D ground truth (GT) with various levels of\nadditive Gaussian noises. The results are shown in Fig. 6 (b).\nIt can be observed that the MPJPE of 3D poses increases\nlinearly with the two-norm errors of 2D poses. Besides, our\nmethod performs well on different 2D inputs, indicating the\neffectiveness and robustness of our method.\nModel hyperparameters. As shown in Table VI, we ﬁrst\n8\nTABLE III: Quantitative comparisons of MPJPE in millimeter on Human3.6M under protocol #1, using ground truth 2D joint\nlocations as input. †means the method utilizing temporal information. Best in bold.\nProtocol #1 Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nMartinezet al. [19] ICCV’17 37.7 44.4 40.3 42.1 48.2 54.9 44.4 42.1 54.6 58.0 45.1 46.4 47.6 36.4 40.4 45.5\nLee et al. [22] ECCV’18† 32.1 36.6 34.3 37.8 44.5 49.9 40.9 36.2 44.1 45.6 35.3 35.9 30.3 37.6 35.5 38.4\nPavlloet al. [20] CVPR’19† 35.2 40.2 32.7 35.7 38.2 45.5 40.6 36.1 48.8 47.3 37.8 39.7 38.7 27.8 29.5 37.8\nCai et al. [24] ICCV’19† 32.9 38.7 32.9 37.0 37.3 44.8 38.7 36.1 41.0 45.6 36.8 37.7 37.7 29.5 31.6 37.2\nXu et al. [42] CVPR’21 35.8 38.1 31.0 35.3 35.8 43.2 37.3 31.7 38.4 45.5 35.4 36.7 36.8 27.9 30.7 35.8\nLiu et al. [34] CVPR’20† 34.5 37.1 33.6 34.2 32.9 37.1 39.6 35.8 40.7 41.4 33.0 33.8 33.0 26.6 26.9 34.7\nChenet al. [45] TCSVT’21† - - - - - - - - - - - - - - - 32.3\nZenget al. [53] ECCV’20† 34.8 32.1 28.5 30.7 31.4 36.9 35.6 30.5 38.9 40.5 32.5 31.0 29.9 22.5 24.5 32.0\nOurs 27.1 29.4 26.5 27.1 28.6 33.0 30.7 26.8 38.2 34.7 29.1 29.8 26.8 19.1 19.8 28.5\nTABLE IV: Results show the velocity error (MPJPV) of our methods and other state-of-the-arts on Human3.6M. Here, ∗\ndenotes our result without the supervision of full sequence scale. Best in bold.\nMPJPV Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nPavlloet al. [20] CVPR’19 3.0 3.1 2.2 3.4 2.3 2.7 2.7 3.1 2.1 2.9 2.3 2.4 3.7 3.1 2.8 2.8\nLin et al. [51] BMVC’19 2.7 2.8 2.1 3.1 2.0 2.5 2.5 2.9 1.8 2.6 2.1 2.3 3.7 2.7 3.1 2.7\nChen et al. [45] TCSVT’21 2.7 2.8 2.0 3.1 2.0 2.4 2.4 2.8 1.8 2.4 2.0 2.1 3.4 2.7 2.4 2.5\nWanget al. [44] ECCV’20 2.3 2.5 2.0 2.7 2.0 2.3 2.2 2.5 1.8 2.7 1.9 2.0 3.1 2.2 2.5 2.3\nOurs ∗ 2.8 2.8 2.1 3.2 2.2 2.5 2.6 2.8 1.8 2.4 2.1 2.3 3.5 3.0 2.6 2.6\nOurs 2.4 2.5 1.8 2.8 1.8 2.2 2.2 2.5 1.5 2.0 1.8 1.9 3.2 2.5 2.1 2.2\nTABLE V: Quantitative results on HumanEva-I dataset under\nprotocol #2. Best in bold, second-best underlined. (MRCNN)\n- Mask-RCNN; (GT) - 2D ground truth.\nWalk Jog Box\nS1 S2 S3 S1 S2 S3 S1 S2 S3 Avg.\nMartinez et al. [19] 19.7 17.4 46.8 26.9 18.2 18.6 - - - -\nPavlakos et al. [37] 22.3 19.5 29.7 28.9 21.9 23.8 - - - -\nLee et al. [22] 18.6 19.9 30.5 25.7 16.8 17.7 42.8 48.1 53.4 30.3\nPavllo et al. [20] 13.9 10.2 46.6 20.9 13.1 13.8 23.8 33.7 32.0 23.1\nOurs (T = 27 MRCNN) 14.0 10.0 32.8 19.5 13.6 14.2 22.4 21.6 22.5 18.9\nOurs (T = 27 GT) 9.7 7.6 15.8 12.3 9.4 11.2 14.8 12.9 16.5 12.2\nanalyze the effect of the number of VTE layers. Empirically, it\ncan be found that the performance cannot be improved when\nnaively stacking multiple standard Transformer encoder layers.\nNotably, our model equipped with STE is more accurate at the\nsame number of Transformer encoder layers and comparable\nmodel parameters. For example, our method ( N1 = 3 and\nN2 = 3) has better performance and fewer FLOPs than the\nmodel of N1 = 6 at the same dm = 256 and df = 512 (46.9\nmm vs. 47.9 mm, 0.128G vs. 0.174G). In addition, our STE\n(N2 = 3, 0.041G) also has fewer FLOPs than standard Trans-\nformer encoder ( N1 = 3 , 0.087G) with similar parameters,\nwhich achieves 2.1×less computation. It veriﬁes the effec-\ntiveness of our proposed STE in reducing computation cost\nand boosting performance. Then, we investigate the inﬂuence\nof various hyperparameters combinations to ﬁnd the optimal\nnetwork architecture. It can be observed that using 3 encoder\nlayers of both VTE and STE modules, 256 dimensions, and\n512 hidden units achieves the best performance.\nStrided factor. We observe that the strided factor of STE\nused in our Strided Transformer has an impact on the esti-\nmation performance. Here, we study the inﬂuence of using\ndifferent design choices of strided factor of STE. The exper-\nimental results are depicted in Table VII. It shows that using\na strided factor sm = {3,3,3}has the best performance. This\n19 27 81 135 189 243 297 351\nSequence length\n30\n35\n40\n45\n50MPJPE (mm)\nCPN\nGT\n(a)\n0 5 10 15 20 25\nTwo-norm errors of 2D Estimator\n35\n40\n45\n50\n55MPJPE (mm)\nGT\nGT+(0, 5)\nGT+(0, 10)\nGT+(0, 15)\nGT+(0, 20)\nCPN\nDetectron\nSH (b)\nFig. 6: (a) Ablation studies on different sequence lengths\nof our method on Human3.6M with the MPJPE metric. (b)\nThe impact of 2D detections on Human3.6M. Here, N(0,σ2)\nrepresents the Gaussian noise with mean zero and σ is the\nstandard deviation. (CPN) - Cascaded Pyramid Network; (SH)\nStack Hourglass; (GT) - 2D ground truth.\ndemonstrates the beneﬁt of gradually reducing the temporal\ndimensionality with a small strided factor.\nPrediction scheme. We further examine the proposed pre-\ndiction scheme of full sequence scale and single target frame\nscale by using ﬁve different designs: (i) Full: the STE of our\nproposed method is replaced with VTE, and the new architec-\nture is only supervised by the full sequence scale (the sequence\nloss). (ii) Single: the proposed method is only supervised by\nthe single target frame scale (single-frame loss). (iii) Full-\nto-full: the architecture consists of six VTE layers, whose\nﬁrst three layers and ﬁnal three layers are both supervised\nby the sequence loss. (iv) Single-to-single: VTE and STE of\nthe proposed method are both supervised by the single-frame\nloss. (v) Full-to-single: our proposed method. In Table VIII,\nit can be observed that the schemes of considering only one\nprediction manner (i, ii, iii, iv) decay performance, and our\nfull-to-single prediction scheme (v) is the best. The empirical\nresults indicate that our proposed full-to-single mechanism is\n9\nGround Truth Ground TruthOursInput Input OursTCN TCNATTN-TCN ATTN-TCN\n46.2 mm50.9 mm56.6 mm 97.2 mm 86.8 mm 57.3 mm\n79.1 mm 69.4 mm 36.7 mm 85.6 mm 56.9 mm 42.0 mm\nS9 Directions S9 Photo\nS11 SitD. S11 SitD.\nFig. 7: Qualitative comparisons with the previous state-of-the-art methods, TCN [20] and ATTN-TCN [34] on Human3.6M\ndataset. Wrong estimations are highlighted by red circles.\nTABLE VI: Ablation study on the hyperparameters of our\nmodel on Human3.6M under protocol #1. N1 and N2 are the\nnumber of VTE and STE layers, respectively. dm and df are\nthe dimensions and the number of hidden units.\nN1 N2 dm df Param (M) FLOPs (G) MPJPE (mm)\n2 - 512 2048 6.36 0.342 47.9\n3 - 512 2048 9.51 0.514 47.8\n4 - 512 2048 12.66 0.685 48.0\n5 - 512 2048 15.82 0.856 48.4\n6 - 512 2048 18.97 1.028 49.3\n2 - 256 512 1.08 0.058 47.8\n3 - 256 512 1.61 0.087 47.6\n4 - 256 512 2.13 0.116 47.8\n5 - 256 512 2.66 0.145 47.7\n6 - 256 512 3.19 0.174 47.9\n- 3 256 512 2.42 0.041 48.0\n2 3 256 512 3.48 0.099 47.4\n3 3 256 512 4.01 0.128 46.9\n2 3 512 2048 22.18 0.589 47.4\n3 3 512 2048 25.33 0.761 47.3\nTABLE VII: Ablation study on the strided factor of STE with\nthe receptive ﬁeld T = 3 ×3 ×3 = 27 . The evaluation is\nperformed on Human3.6M under protocol #1.\nLayers Strided factor MPJPE (mm)\n3 3, 3, 3 46.9\n3 3, 9, 1 47.5\n3 9, 3, 1 47.3\n2 3, 9 47.2\n2 9, 3 47.1\n1 27 47.7\ncrucial for performance improvement.\nModel components. As shown in Table IX, an ablation\nstudy is performed to assess the effectiveness of different\ncomponents of our method. We select the center frame of\nintermediate predictions from VTE as ﬁnal results, which\nincreases the MPJPE by 1.2 mm (from 46.9 mm to 48.1\nmm). It proves that the scheme of intermediate supervision can\nfurther improve estimation accuracy. Next, we perform pooling\noperation after FFN of VTE following [27] and then replace\nSTE of our proposed method with it. The new architecture\nis termed as Pooling Transformer, and its error increases by\n0.4 mm, which highlights that our STE can preserve more\nvaluable information than Pooling Transformer by exploiting\nTABLE VIII: Ablation study on different prediction schemes.\nThe evaluation is performed on Human3.6M under protocol\n#1. ∆ represents the performance gap between the methods\nand ours.\nPrediction scheme MPJPE (mm) ∆\nFull 47.9 1.0\nSingle 48.3 1.4\nFull-to-full 47.4 0.5\nSingle-to-single 48.5 1.6\nFull-to-single 46.9 -\nTABLE IX: Ablation study on each component of our network\narchitecture on Human3.6M under protocol #1.\nMethod MPJPE (mm)\nOurs, proposed 46.9\nOurs, intermediate predictions 48.1\nOurs, Pooling Transformer 47.3\nw/o VTE 48.0\nw/o STE 47.6\nlocal contexts to aggregate information. Removing VTE (only\ntrained with single-frame loss) leads to a 1.1 mm increase\nin MPJPE error. Besides, removing STE (only trained with\nsequence loss) increases the MPJPE to 47.6 mm. These results\nvalidate the importance of both VTE and STE modules in our\nStrided Transformer, where VTE mainly models long-range\ninformation and STE focuses on aggregating information in a\nhierarchical global and local fashion.\nE. Qualitative Results\nAttention visualization. Our method is easily interpretable\nthrough visualizing the attention score across frames to explain\nwhat the target frame relies on. Visualization results of the\nmulti-head attention maps of the ﬁrst attention layers from\nVTE and STE (243-frame model) are shown in Fig. 8. The\nleft map shows strong attention close to the input frames [64],\n[65], while the right map mainly pays strong attention to the\ncenter frame across all the sequences. This is expected since\nthe proposed full-to-single strategy enables the VTE and STE\nmodules to learn different representations: (i) VTE selectively\nidentiﬁes important sequences that are close to the input\nframes and enforces temporal consistency across frames. (ii)\nSTE learns a speciﬁc representation from the input sequences\n10\n(a) VTE\n (b) STE\nFig. 8: Multi-head attention maps ( h = 8) from VTE and STE of our 243-frame model. It illustrates that the self-attention\nmechanism systematically assigns a weight distribution to frames, all of which might beneﬁt the inference. Brighter color\nindicates higher attention score.\nInput Ours ATTN-TCN TCN GCN\nFig. 9: Qualitative comparisons on challenging in-the-\nwild videos with previous state-of-the-art methods, ATTN-\nTCN [34], TCN [20], and GCN [24]. The last row shows\nthe failure case, where the 2D detector has failed badly.\nusing both past and future data, improving the representation\nability of features to reach an optimal inference for the target\nframe. Note that a few attention head maps are sparse due to\nthe different temporal patterns or semantics.\n3D reconstruction visualization. We further evaluate our\nmethod on challenging in-the-wild videos from YouTube.\nFig. 9 shows the qualitative comparisons with the previous\nstate-of-the-art methods [20], [24], [34]. We use the same\n2D detector (cascaded pyramid network [60]) to obtain 2D\nposes and then feed them to the models for a fair comparison.\nDespite the challenging samples with complex actions and fast\nmovements, the proposed method can produce realistic and\nstructurally plausible 3D predictions outperforming previous\nworks. This demonstrates our method is robust to partial\nocclusions and tolerant to depth ambiguity. The last row shows\nthe failure case caused by a big 2D detection error.\nV. C ONCLUSION\nIn this work, we investigate the suitableness of applying a\nTransformer-based network to the task of video-based 3D hu-\nman pose estimation. From the proposed Strided Transformer\nwith Strided Transformer Encoder (STE) and full-to-single\nsupervision scheme, we show how the representative single-\npose representation can be learned from redundant sequences.\nThe key is to reasonably use strided convolutions in the\nTransformer architecture to aggregate long-range information\ninto a single-vector pose in a hierarchical global and local\nfashion. Meanwhile, the computation cost can be reduced\nsigniﬁcantly. Moreover, our full-to-single supervision scheme\nenhances temporal smoothness and further reﬁnes the repre-\nsentation for the target frame. Comprehensive experiments on\ntwo benchmark datasets demonstrate that our method achieves\nsuperior performance compared with state-of-the-art methods.\nAlthough our method can reduce the computation cost of\nTransformers, the computational complexity and runtime cost\nof our method are still larger than temporal convolutional\nnetworks [20], [45], indicated in Table II. It is well acknowl-\nedged that the strong performance of Transformers comes at\nhigh computational costs. Note that the scope of this paper\nonly targets improving FFN in the Transformer model. Future\nworks may include designing a more efﬁcient self-attention\nmechanism and extending our Strided Transformer to solve\nmulti-view 3D human pose estimation. In addition, we hope\nthat our approach would bring inspiration to the ﬁeld of\nskeleton-based representation learning, e.g., action recognition,\nmotion prediction, pose tracking, and so on.\nREFERENCES\n[1] I. Radwan, A. Dhall, and R. Goecke, “Monocular image 3d human\npose estimation under self-occlusion,” in Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) , 2013, pp. 1888–\n1895.\n11\n[2] S. Li and A. B. Chan, “3d human pose estimation from monocular\nimages with deep convolutional neural network,” in Asian Conference\non Computer Vision (ACCV) , 2014, pp. 332–347.\n[3] T. Zhao, S. Li, K. N. Ngan, and F. Wu, “3-d reconstruction of human\nbody shape from a single commodity depth camera,” IEEE Transactions\non Multimedia, vol. 21, no. 1, pp. 114–123, 2018.\n[4] P. Hu, E. S.-l. Ho, and A. Munteanu, “3dbodynet: Fast reconstruction\nof 3d animatable human body shape from a single commodity depth\ncamera,” IEEE Transactions on Multimedia , 2021.\n[5] A. Kadkhodamohammadi and N. Padoy, “A generalizable approach for\nmulti-view 3d human pose regression,” Machine Vision and Applica-\ntions, vol. 32, no. 1, pp. 1–14, 2021.\n[6] K. Pullen and C. Bregler, “Motion capture assisted animation: Textur-\ning and synthesis,” in Proceedings of the 29th annual conference on\nComputer graphics and interactive techniques , 2002, pp. 501–508.\n[7] P. Wang, W. Li, Z. Gao, C. Tang, and P. O. Ogunbona, “Depth\npooling based large-scale 3-d action recognition with convolutional\nneural networks,” IEEE Transactions on Multimedia , vol. 20, no. 5, pp.\n1051–1061, 2018.\n[8] M. Liu, H. Liu, and C. Chen, “Robust 3d action recognition through\nsampling local appearances and global distributions,” IEEE Transactions\non Multimedia, vol. 20, no. 8, pp. 1932–1947, 2017.\n[9] M. Liu and J. Yuan, “Recognizing human actions as the evolution of pose\nestimation maps,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2018, pp. 1159–1168.\n[10] P. Wei, H. Sun, and N. Zheng, “Learning composite latent structures for\n3d human action representation and recognition,” IEEE Transactions on\nMultimedia, vol. 21, no. 9, pp. 2195–2208, 2019.\n[11] Y .-F. Song, Z. Zhang, C. Shan, and L. Wang, “Constructing stronger and\nfaster baselines for skeleton-based action recognition,” arXiv preprint\narXiv:2106.15125, 2021.\n[12] T. Chen, D. Zhou, J. Wang, S. Wang, Y . Guan, X. He, and E. Ding,\n“Learning multi-granular spatio-temporal graph network for skeleton-\nbased action recognition,” in Proceedings of the 29th ACM International\nConference on Multimedia (ACMMM) , 2021, pp. 4334–4342.\n[13] C. Li, C. Xie, B. Zhang, J. Han, X. Zhen, and J. Chen, “Memory atten-\ntion networks for skeleton-based action recognition,” IEEE Transactions\non Neural Networks and Learning Systems , 2021.\n[14] D. Yang, Y . Wang, A. Dantcheva, L. Garattoni, G. Francesca, and\nF. Bremond, “Unik: A uniﬁed framework for real-world skeleton-based\naction recognition,” arXiv preprint arXiv:2107.08580 , 2021.\n[15] B. Zhang, Y . Yang, C. Chen, L. Yang, J. Han, and L. Shao, “Action\nrecognition using 3d histograms of texture and a multi-class boosting\nclassiﬁer,” IEEE Transactions on Image processing , vol. 26, no. 10, pp.\n4648–4660, 2017.\n[16] C. Chen, M. Liu, H. Liu, B. Zhang, J. Han, and N. Kehtarnavaz, “Multi-\ntemporal depth motion maps-based local binary patterns for 3-d human\naction recognition,” IEEE Access, vol. 5, pp. 22 590–22 604, 2017.\n[17] M. Garcia-Salguero, J. Gonzalez-Jimenez, and F.-A. Moreno, “Human\n3d pose estimation with a tilting camera for social mobile robot\ninteraction,” Sensors, vol. 19, no. 22, p. 4943, 2019.\n[18] L. Gui, K. Zhang, Y . Wang, X. Liang, J. M. Moura, and M. Veloso,\n“Teaching robots to predict human motion,” in Proceedings of the IEEE\nInternational Conference on Intelligent Robots and Systems (IROS) ,\n2018, pp. 562–567.\n[19] J. Martinez, R. Hossain, J. Romero, and J. J. Little, “A simple yet\neffective baseline for 3d human pose estimation,” in Proceedings of the\nIEEE International Conference on Computer Vision (ICCV) , 2017, pp.\n2640–2649.\n[20] D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, “3d human pose\nestimation in video with temporal convolutions and semi-supervised\ntraining,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2019, pp. 7753–7762.\n[21] G. Hua, W. Li, Q. Zhang, R. Ding, and H. Liu, “Weakly-\nsupervised cross-view 3d human pose estimation,” arXiv preprint\narXiv:2105.10882, 2021.\n[22] K. Lee, I. Lee, and S. Lee, “Propagating lstm: 3d pose estimation based\non joint interdependency,” in Proceedings of the European Conference\non Computer Vision (ECCV) , 2018, pp. 119–135.\n[23] M. Rayat Imtiaz Hossain and J. J. Little, “Exploiting temporal infor-\nmation for 3d human pose estimation,” in Proceedings of the European\nConference on Computer Vision (ECCV) , 2018, pp. 68–84.\n[24] Y . Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, and N. M. Thalmann,\n“Exploiting spatial-temporal relationships for 3d pose estimation via\ngraph convolutional networks,” inProceedings of the IEEE International\nConference on Computer Vision (ICCV) , 2019, pp. 2272–2281.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems (NIPS), 2017, pp. 5998–6008.\n[26] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efﬁcient transformers:\nA survey,” arXiv preprint arXiv:2009.06732 , 2020.\n[27] D. Zihang, L. Guokun, Y . Yiming, and Q. L. V ., “Funnel-transformer:\nFiltering out sequential redundancy for efﬁcient language processing,”\nin Advances in Neural Information Processing Systems (NIPS) , 2020.\n[28] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al. , “A survey on visual transformer,” arXiv preprint\narXiv:2012.12556, 2020.\n[29] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang,\n“Transreid: Transformer-based object re-identiﬁcation,” arXiv preprint\narXiv:2102.04378, 2021.\n[30] X. Li, Y . Hou, P. Wang, Z. Gao, M. Xu, and W. Li, “Trear:\nTransformer-based rgb-d egocentric action recognition,” arXiv preprint\narXiv:2101.03904.\n[31] L. Han, P. Wang, Z. Yin, F. Wang, and H. Li, “Exploiting better feature\naggregation for video object detection,” in Proceedings of the 28th ACM\nInternational Conference on Multimedia (ACMMM) , 2020, pp. 1469–\n1477.\n[32] X. Li, Y . Hou, P. Wang, Z. Gao, M. Xu, and W. Li, “Transformer guided\ngeometry model for ﬂow-based unsupervised visual odometry,” Neural\nComputing and Applications , pp. 1–12, 2021.\n[33] M. Geva, R. Schuster, J. Berant, and O. Levy, “Transformer feed-forward\nlayers are key-value memories,” arXiv preprint arXiv:2012.14913, 2020.\n[34] R. Liu, J. Shen, H. Wang, C. Chen, S.-c. Cheung, and V . Asari, “Atten-\ntion mechanism exploits temporal contexts: Real-time 3d human pose\nreconstruction,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2020, pp. 5064–5073.\n[35] C. Ionescu, D. Papava, V . Olaru, and C. Sminchisescu, “Human3.6m:\nLarge scale datasets and predictive methods for 3d human sensing\nin natural environments,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 36, no. 7, pp. 1325–1339, 2013.\n[36] L. Sigal, A. O. Balan, and M. J. Black, “Humaneva: Synchronized video\nand motion capture dataset and baseline algorithm for evaluation of\narticulated human motion,” International Journal of Computer Vision ,\nvol. 87, no. 12, pp. 4–27, 2010.\n[37] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Coarse-to-ﬁne\nvolumetric prediction for single-image 3d human pose,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017, pp. 7025–7034.\n[38] X. Sun, B. Xiao, F. Wei, S. Liang, and Y . Wei, “Integral human pose\nregression,” in Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 529–545.\n[39] L. Zhao, X. Peng, Y . Tian, M. Kapadia, and D. N. Metaxas, “Seman-\ntic graph convolutional networks for 3d human pose regression,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019, pp. 3425–3435.\n[40] J. Liu, H. Ding, A. Shahroudy, L.-Y . Duan, X. Jiang, G. Wang, and\nA. C. Kot, “Feature boosting network for 3d pose estimation,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 42,\nno. 2, pp. 494–501, 2019.\n[41] H.-S. Fang, Y . Xu, W. Wang, X. Liu, and S.-C. Zhu, “Learning pose\ngrammar to encode human body conﬁguration for 3d pose estimation,”\nin Thirty-Second AAAI Conference on Artiﬁcial Intelligence , 2018.\n[42] T. Xu and W. Takano, “Graph stacked hourglass networks for 3d human\npose estimation,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2021, pp. 16 105–16 114.\n[43] K. Gong, J. Zhang, and J. Feng, “Poseaug: A differentiable pose\naugmentation framework for 3d human pose estimation,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021, pp. 8575–8584.\n[44] J. Wang, S. Yan, Y . Xiong, and D. Lin, “Motion guided 3d pose\nestimation from videos,” arXiv preprint arXiv:2004.13985 , 2020.\n[45] T. Chen, C. Fang, X. Shen, Y . Zhu, Z. Chen, and J. Luo, “Anatomy-\naware 3d human pose estimation with bone-based pose decomposition,”\nIEEE Transactions on Circuits and Systems for Video Technology, 2021.\n[46] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision (ECCV) , 2020, pp. 213–229.\n[47] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,”arXiv preprint\narXiv:2010.04159, 2020.\n[48] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n12\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[49] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan, “Tokens-to-token vit: Training vision transformers from scratch\non imagenet,” arXiv preprint arXiv:2101.11986 , 2021.\n[50] K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh\nreconstruction with transformers,” arXiv preprint arXiv:2012.09760 ,\n2020.\n[51] J. Lin and G. H. Lee, “Trajectory space factorization for deep video-\nbased 3d human pose estimation,” arXiv preprint arXiv:1908.08289 ,\n2019.\n[52] J. Xu, Z. Yu, B. Ni, J. Yang, X. Yang, and W. Zhang, “Deep kinematics\nanalysis for monocular 3d human pose estimation,” inProceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2020, pp. 899–908.\n[53] A. Zeng, X. Sun, F. Huang, M. Liu, Q. Xu, and S. Lin, “Srnet: Improving\ngeneralization in 3d human pose estimation with a split-and-recombine\napproach,” in European Conference on Computer Vision (ECCV) , 2020,\npp. 507–523.\n[54] G. Pavlakos, X. Zhou, and K. Daniilidis, “Ordinal depth supervision\nfor 3d human pose estimation,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2018, pp. 7307–\n7316.\n[55] K. Liu, R. Ding, Z. Zou, L. Wang, and W. Tang, “A comprehensive study\nof weight sharing in graph networks for 3d human pose estimation,” in\nProceedings of the European Conference on Computer Vision (ECCV) ,\n2020, pp. 318–334.\n[56] X. Chen, K.-Y . Lin, W. Liu, C. Qian, and L. Lin, “Weakly-supervised\ndiscovery of geometry-aware representation for 3d human pose estima-\ntion,” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019, pp. 10 895–10 904.\n[57] D. Tome, M. Toso, L. Agapito, and C. Russell, “Rethinking pose in 3d:\nMulti-stage reﬁnement and recovery for markerless motion capture,” in\n2018 International Conference on 3D Vision (3DV) , 2018, pp. 474–483.\n[58] M. Kocabas, S. Karagoz, and E. Akbas, “Self-supervised learning of 3d\nhuman pose using multi-view geometry,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2019,\npp. 1077–1086.\n[59] Y . Cheng, B. Yang, B. Wang, W. Yan, and R. T. Tan, “Occlusion-aware\nnetworks for 3d human pose estimation in video,” in Proceedings of the\nIEEE International Conference on Computer Vision (ICCV) , 2019, pp.\n723–732.\n[60] Y . Chen, Z. Wang, Y . Peng, Z. Zhang, G. Yu, and J. Sun, “Cascaded\npyramid network for multi-person pose estimation,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018, pp. 7103–7112.\n[61] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in\nProceedings of the IEEE international conference on computer vision\n(ICCV), 2017, pp. 2961–2969.\n[62] R. Dabral, A. Mundhada, U. Kusupati, S. Afaque, A. Sharma, and\nA. Jain, “Learning 3d human pose from structure and motion,” in\nProceedings of the European Conference on Computer Vision (ECCV) ,\n2018, pp. 668–683.\n[63] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for\nhuman pose estimation,” in Proceedings of the European Conference\non Computer Vision (ECCV) , 2016, pp. 483–499.\n[64] Z. Wu, Z. Liu, J. Lin, Y . Lin, and S. Han, “Lite transformer with\nlong-short range attention,” in International Conference on Learning\nRepresentations (ICLR), 2020.\n[65] Z. Jiang, W. Yu, D. Zhou, Y . Chen, J. Feng, and S. Yan, “Convbert:\nImproving bert with span-based dynamic convolution,” in Advances in\nNeural Information Processing Systems (NIPS) , 2020.\nVI. A PPENDIX\n13\n3D Pose Ground Truth\n 3D Pose Ground Truth\nFig. 10: Visual results of our proposed method on Human3.6M dataset (ﬁrst 3 rows) and HumanEva-I dataset (last 2 rows).\n420\n420\n260\n260\n190\n190\n150\n150\n140\n140\n040\n040\n400\n400\n 420\n420\n 440\n440\n 460\n460\nc\n480\nc\n480\n080\n080\n500\n500\n380\n380\n190\n190\n175\n175\n160\n160\n145\n145\n130\n130\nc\n100\n100\n 115\n115\nFig. 11: Qualitative results on challenging wild videos. The number is the frame index of input videos.",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.712838888168335
    },
    {
      "name": "Transformer",
      "score": 0.711024284362793
    },
    {
      "name": "Computer science",
      "score": 0.6280105113983154
    },
    {
      "name": "Computation",
      "score": 0.4706936180591583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4124603569507599
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3931695222854614
    },
    {
      "name": "Algorithm",
      "score": 0.29225850105285645
    },
    {
      "name": "Voltage",
      "score": 0.08023890852928162
    },
    {
      "name": "Engineering",
      "score": 0.06961935758590698
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}