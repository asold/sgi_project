{
  "title": "Grammatical versus Spelling Error Correction: An Investigation into the Responsiveness of Transformer-Based Language Models Using BART and MarianMT",
  "url": "https://openalex.org/W4393069008",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5041852639",
      "name": "Rohit Raju",
      "affiliations": [
        "Amrita Vishwa Vidyapeetham",
        "University of Colorado Boulder"
      ]
    },
    {
      "id": "https://openalex.org/A5085779756",
      "name": "Peeta Basa Pati",
      "affiliations": [
        "Amrita Vishwa Vidyapeetham"
      ]
    },
    {
      "id": "https://openalex.org/A5092001013",
      "name": "SA Gandheesh",
      "affiliations": [
        "Amrita Vishwa Vidyapeetham"
      ]
    },
    {
      "id": "https://openalex.org/A5093344984",
      "name": "Gayatri Sanjana Sannala",
      "affiliations": [
        "Amrita Vishwa Vidyapeetham"
      ]
    },
    {
      "id": "https://openalex.org/A5007941443",
      "name": "KS Suriya",
      "affiliations": [
        "Amrita Vishwa Vidyapeetham"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798685342",
    "https://openalex.org/W2971162405",
    "https://openalex.org/W4285160092",
    "https://openalex.org/W593223840",
    "https://openalex.org/W2805583812",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2745673470",
    "https://openalex.org/W2885213066",
    "https://openalex.org/W3093981321",
    "https://openalex.org/W3173175058",
    "https://openalex.org/W3103161287",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W2971332944",
    "https://openalex.org/W3138323210",
    "https://openalex.org/W4213189167",
    "https://openalex.org/W2912581782",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W4377700701",
    "https://openalex.org/W2785385789",
    "https://openalex.org/W4285324709",
    "https://openalex.org/W3119920397",
    "https://openalex.org/W2001642682",
    "https://openalex.org/W3209132994",
    "https://openalex.org/W2092911974",
    "https://openalex.org/W3132905760",
    "https://openalex.org/W4220861476",
    "https://openalex.org/W3176140329",
    "https://openalex.org/W3214077018",
    "https://openalex.org/W3173712076",
    "https://openalex.org/W3015001695",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963881719",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "Text continues to remain a relevant form of representation for information. Text documents are created either in digital native platforms or through the conversion of other media files such as images and speech. While the digital native text is invariably obtained through physical or virtual keyboards, technologies such as OCR and speech recognition are utilised to transform the images and speech signals into text content. All these variety of mechanisms of text generation also introduce errors into the captured text. This project aims at analysing different kinds of errors that occur in text documents. The work employs two of the advanced deep neural network-based language models, namely, BART and MarianMT, to rectify the anomalies present in the text. Transfer learning of these models with available dataset is performed to finetune their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, BART can handle spelling errors far better (24.6%) than grammatical errors (8.8%).",
  "full_text": "Grammatical vs Spelling error correction: An \ninvestigation into the responsiveness of \nTransformer based language models using \nBART and MarianMT \nRohit Raju1,2, Peeta Basa Pati*,2, SA Gandheesh2, Gayatri Sanjana Sannala2 & Suriya KS2 \n1University of Colorado Boulder, CO, US, e-mail: rohit.raju@colorado.edu \n2Department of Computer Science & Engineering, Amrita School of Computing Bengaluru, \nAmrita Vishwa Vidyapeetham, INDIA  \n*Corresponding Author: bp_peeta@blr.amrita.edu, ORC ID: 0000-0003-2376-4591  \nAbstract \nText continues to remain a relevant form of representation for information. Text documents are \ncreated either in digital native platforms or through conversion of other media files such as images \nand speech. While the digital native text is invariably obtained through physical or virtual \nkeyboards, technologies such as OCR & speech recognition are u tilized to transform the images \nand speech signals to text content. All these variety of mechanisms of text generation also \nintroduce error into the captured text. \nThis project aims at analyzing different kinds of errors that occurs in text documents. The work \nemploys two of the advanced deep neural network based language models, namely, BART and \nMarianMT, for rectifying the anomalies present in text.  Transfer learning of these models with \navailable dataset is performed to finetune their capacity for error correction. A comparative study \nis conducted to investigate the effectiveness of these models in handling each of the defined error \ncategories. It is observed that while both the models are able to bring down the erroneous sentences \nby 20+%, BART is able to handle spelling errors far better (24.6%) than grammatical errors \n(8.8%). \nKeywords: BART, MarianMT, text enhancement, spelling error correction, error category. \nI. Introduction \nText is a natural representation of all the existing languages in the world. Texts help one \nexpress and communicate with others. Handwritten texts have been part of the history for ages, \nwhile digital texts have evolved to keep up with the rapidly growing technology in day to day lives. \nIt is due to texts that one can extend from their knowledge and memory beyond their body into the \nenvironment around [1]. Text is available in various forms, from handwritten manuscripts to \nThis is a pre-print version of the paper. The print version is available at:\nhttps://doi.org/10.1142/S0219649224500370\ndigitally written blogs, from stone carvings to printed posters. Texts can be utilized for personal \nreasons such as diary entry, blog, etc., as well as for professional purposes like advertising, \nsurveying, etc. Right from the newspaper one reads in the mo rning to the social media scrolling \nbefore going to bed, people are surrounded by text. \nIt is human nature to categorize any kind of data they receive. As there is so much text \navailable around, it is obvious that humans tend to inspect and review the text they require. Thus, \nthe origin of text analysis. It is the process of scanning the text ual data in order to derive some \nmeaning and store information. \nMost businesses rely on text analysis to extract valuable insights from various raw sources. \nThe feedback received from these sources such as emails, chat messages, social media posts, \ncomments & statements and survey responses help them in their decision-making strategies. Text \ncapturing can be complex and tend to introduce new errors based on its source and capture \ntechnology.  \nBesides digital native documents, text may be obtained from other forms of media such as \nimages, video, speech or voice. OCR extracts text present in images, thereby enabling editing and \nreviewing of the content [2]. The captured textual information may contain errors due to a variety \nof reasons. Some of these errors are pre -existing in the input images due to incorrect typing or \neven inaccurate language knowledge. Moreover, the models used to recognize the text in images \nmay introduce some errors, since al l OCR engines are inherently error -prone. These recognition \nerrors may occur due to noisy and unclear images, poor handwriting, formatting and spacing \nissues. Speech recognition [3] is a speech -to-text technology that recognizes spoken words and \ninto its t ext equivalent. Auto -transcription of speech may introduce errors while phoneme to \ngrapheme conversion is carried out. These errors may be attributed to factors such as substitution \nof a word with a different, yet similar sounding word, deletion of incoher ent words and insertion \nof contextually irrelevant words. Technologies such as web scraping [4], email -readers and file \nhandlers may also introduce errors while capturing text due to format incompatibility.  \nThus, there are mainly two distinct error types available in text sentences, namely, spelling \nor typographic error and grammatical error. Both these kinds of error may be attributed to errors \nin symbol recognition, mis -typing and incorrect language knowled ge. Existence of these errors \nmay lead to undesirable consequences, especially in fields such as courtrooms & hospitals. For \nexample, the sentences “The accused fled from the crime” and “The accused bled from the crime” \ncan lead to very different outcomes for the case based on just one spelling mistake. The sentence \n“He are healthy” is grammatically incorrect, yet highly possible to be created by someone who \nlacks proper knowledge of English grammar.  \nThere are several methods to eradicate these errors. While one can utilize manual labor to \nindividually pick out the errors, it can be time consuming and can add some human errors.  Rule \nbased error correction [5] is also a solution to eliminate error. In such a system, first a large number \nof documents are studied to identify the common error patterns based one which the rules are \ndefined. These defined rules are, subsequently, employed to bring corrections to errors in future \ndocuments. Efficiency of such error correction is based on the qu ality of defined rules as well as \nordering and properties of the rules. Though it is useful in reducing the errors in text, it is sensitive \nto newer language structures. Rule Based error correction is often time consuming, without any \nlearning capacity and  is very unidimensional, thereby, making it inefficient. To overcome these \nshortcomings, advanced deep neural network based Natural Language Processing (NLP) models \nare used  for error correction. These NLP models use Artificial Intelligence (AI) and enable \nmachines to read, understand & analyze the meaning as well as context of the text sentences. Since \ntext is a sequential data consisting of a sequence of words, Recurrent Ne ural Network (RNN) \narchitectures are utilized to analyze and capture the information storing them as language models. \nLong Short -Term Memory (LSTM) networks are advanced RNN architectures which allow \ninformation to persist for relatively longer duration in the model’s network. The encoder-decoder \narchitecture of the LSTM carries the capability to learn the context of a sentence and store \ninformation in the model, thereby making it a perfect tool for error correction [6]. This paper \nspeaks about two such deep learning NLP models, Bidirectional Auto-Regressive Transformers \n(BART) [7] and MarianMT [39]. \nBART and MarianMT models have an encoder -decoder architecture, they work using \nsequence-to-sequence modeling of language data, and are memory -based. During training, these \nmodels pick up on the traits and rules of the grammar. The noisy text is fed as input to these \nlanguage models, which produce accurate sentences as output. This helps to improve the accuracy \nof generated text. Both these models accomplish the stated task by comprehending th e grammar \nand context of used words in a sentence. Token masking, token detection, text infill & sentence \npermutation are tasks that the BART and MarianMT decoders are trained to perform. \nBART is an advanced language model and has been reported for its efficacy for various \nlanguage processing tasks.  MarianMT is yet to be thoroughly explored in these aspects. For the \npurpose of this work, both these models are employed to correct various errors present in the input \nsentence. Many researchers have already employed such language models for accuracy \nenhancements and reported success with the approach. However, most of these works deal with \nstudy of the aggregated quantitative impact, namely, ac curacy improvement, error reduction etc. \nIt’s equally important to find the pattern of improvement brought in by these models. The outcome \nof such a study shall help in identification of error scenarios and find the right model to be \nemployed.  \nIn this work, an error category definition is developed & utilized to categorize all the input \nsentences in the dataset as well as the corresponding predicted outputs. While it's ideal to assume \nthat all input sentences shall be predicted as fully correcte d, in practice predicted sentences may \nalso have errors. Thus, it's important to study the shift pattern of the error categories which is the \nmain focus of this work. Observing and analyzing the shift in error categories provides insights \ninto the model’s behavior as well as capacity for capturing the context and meaning of the \nsentences. Such analysis also helps to develop effective strategies to engage such models for \nappropriate correction based on the types of error available in input sentences. This w ork aligns \nwith United Nations’ Sustainable Development Goals of “ Quality Education (SDG -4)” and \n“Decent Work and Economic Growth (SDG-8)”.  \nThe Novelties of this work are as follows: \n• Error Category Definition. \n• Employ different NLP models and evaluate their capability for category specific error \ncorrection and quantity of error present. \n• Analysis of percentage of error category shift to understand the capabilities and \nlimitations of the model. \n• New subset of data created using manual correction. \nThis paper is organized as follows: the literature survey is provided in Section II, while \nSection III provides a definition of various error categories (one of our novelties of this work). The \nDataset is discussed in detail in Section IV, Model Design is provided in Section V, analysis of \nobtained results in Section VI, followed by Conclusion & future scope of the work in Section VII. \nII. Literature Survey \nLanguage models have been employed in numerous research studies on a variety of \ndocuments to get a wide range of results. Following are some of the noteworthy research \ncontributions. \nA system to improve handwritten paper recognition was proposed by Kumar and Pati [8] \nusing pre-trained models like MarianMT, BERT, and BART. The accuracy of the NLP models \nwas measured using the C4 (trained on 3,000 sentences), Block, Word, and AVV 40 datasets. The \nauthors performed a comparative examination of progress on various language models for various \ndatasets and noted that BART provided the most uplift. \nThe authors Alikaniotis and Raheja [9] propose that state -of-the-art language models can \nbe used to achieve competitive performance on the task of grammatical error correction (GEC) \nwithout the need for annotated training data. The authors reported using pre-trained Transformer \nlanguage models like BERT, GPT, and GPT-2 to perform GEC & demonstrated that these models \ncreate tough baselines to beat.  Their work proved that transformer based language-models are \neffective and robust for the task of GEC, even in absence of annotated training data. \nA detailed survey work by Zhang et al. [10] reports about the sensitivity of language models \nbuilt on deep neural networks towards Adversarial Attacks (the text is bombarded with \nimperceptible words). Pruithi et al. [11] employed BERT & RNN  models to handle Adversarial \nSpelling Mistakes and reported accuracies of 90.3% & 75%, respectively. However, a combined \nBERT and RNN model  proved to be quite ineffective for Adversarial attacks and the accuracy \ndrastically dropped to 45.8%.  \nERNIE is a model proposed by Zhang et al. [12], which expands the existing abilities of \nBERT by training the model with knowledge graphs. These knowledge graphs capture  rich \nstructured knowledge facts for better language understanding. The model’s novelty lies in accurate \ntraining with smaller datasets. Kantor et al. [13] proposed a system that combines multiple GEC \nsystems using a black box approach. This work detects t he strength of a system or a combination \nof several systems per error type. The proposed approach has proven that such a system \noutperforms the average accuracy of ensemble of existing RNN models. \nAn open-source platform called NeuSpell was created by Jayanthi et al. [14] to simplify \ncorrection of spelling mistakes. A wide variety of models, including BERT, SC -LSTM, CHAR-\nCNN-LSTM, and CHAR -LSTM-LSTM, are available on th is platform. Hangaragi et al. [16] \nutilized the BERT, SC-LSTM, CHAR -CNN-LSTM and CHAR -LSTM-LST models from the \nNeuSpell to achieve error correction for recognized output from handwritten document images. In \ntheir work, Google Vision API (OCR) is used on handwritten documents from the I AM dataset \n[17], to generate the text data. They reported  that BERT provides the best improvement ( 9.2% at \ncharacter level) to erroneous text when compared to other models. \nErrors introduced by non-native English speakers are a big challenge in Grammatical error \ncorrection as there is change in the meaning of the sentences. Liang et al. 's [15] proposed a solution \nto overcome this problem. They first defined the categories of  noise in an English sentence and \nthen injected targeted noise into sentences to build training sets. Subsequently, they fine -tuned \nBERT with the training set which outperformed various state -of-the-art language models such as \nLSTM and CNN. \nSaluja et al. [18] developed an LSTM-based model with a fixed latency that can learn, \ndetect, and correct OCR errors. Three studies were carried out: error detection (using various \nevaluation), error correction, and suggestion generation. The results showed that their LTSM \nmodel provided the best error correction for Hindi and Malayalam compared to the prior reported \nmodels. Introduction of the fixed delay to LSTM is a novelty, different from the standard LSTM \nmodels, that enables learning from the subsequent sequence of characters. \nTan et al .’s [19] study focused mostly on BERT for  Mandarin spelling error correction. \nThe BERT model, which was trained on a Chinese dataset, is used in the paper to fix incorrect \nstrings. The detection network and the correction network were kept as two distinct parts of the \nmodel during design. The task of the detection network is to identify incorrect text (such as \nmisspelled words) while the correction network rectifies the incorrect text. The model's precision, \nrecall, and F1 score s were recorded and compared with those of Kenlm, RNNLM, and B ERT-\nFintune. An average improvement of 13% for each of the se evaluation metrics was observed.  \nBERT, additionally, has been employed for many other text processing tasks such as \nsummarization of braille documents [41], dependency parsing for Tamil langauge [42] & aspect \nterm extraction for sentiment analysis [43. \nChinese Spelling Check (CSC) is a challenging task due to the complex characteristics of \nChinese characters. The most common errors present in Chinese language are phonological or \nvisual errors. To overcome these problems, Huang et al. [20] proposed a novel end-to-end trainable \nmodel called PHMOSpell, which promotes the performance of CSC with multi-modal information. \nPinyin and glyph graphical representations are derived which are then integrated into a pre-trained \nlanguage model by a well-designed adaptive gating mechanism. The reported model significantly \noutperformed all previous state-of-the-art models on precision, recall and f1 score metrics.  \nExisting state -of-the-art methods either only use a pre -trained language model or \nincorporate phonological information as external knowledge. To overcome this drawback, Zhang \net al. [21] proposed an end -to-end Chinese spelling correction (CSC) model that integrates \nphonetic features. Initially, the words were replaced with phonetic features and the ir sound-alike \nwords. Then these words were jointly trained for error correction and detection. The transformer \nmodel was trained on SIGHANI15 dataset and significantly outperformed previous state -of-the-\nart methods with a precision of 77.5%, recall of 83.1% and f1-score of 80.2%.  \nXu1 et al. [22] proposed a Chinese spell checker model called ReaLiSe by directly \nleveraging the multimodal information of the Chinese characters. The ReaLiSe model tackles the \nCSC task by first capturing the semantic, phonetic, & graphic information of the input characters \nand then selectively mixing the information in these modalities to predict the correct output. The \nperformance of ReaLiSe model was reported with an accuracy of 84.7%, precision of 77.3%, recall \nof 81.3% and f1-score of 79.3%.  \nKai Fu et al. [23] in this paper detail the approach taken by  them to build a Chinese \nGrammatical Error Correction system. Essays written by the non-native Chinese speakers were \nused as data and the error correction was carried out in various stages. At the f irst stage they \nemployed a spelling error correction model which removed the spelling errors.  This also acts as a \npre-processing step which reduces perturbation at later stages. In the second stage , they cast the \ngrammatical error correction problem as a machine translation task. Here, a sequence-to-sequence \nmodel is employed . To achieve this, they experimented with  several models with different \nconfigurations.  \n Existing GEC systems suffer from not having enough labeled training data to achieve high \naccuracy. To overcome this problem, Zhang et al. [24] proposed a copy -augmented architecture \nfor the GEC task by copying the unchanged words from the source sentence to the target sentence. \nThe copy-augmented architecture was pre-trained with unlabeled One Billion Benchmark dataset. \nThis is followed by comparisons between the fully transferred learnt model and a pretrained model. \nA copying mechanism was applied on the GEC system, which enables the model to copy tokens \nfrom the source sentence. The model was evaluated against CoNLL -2014 and JFLEG dataset s. \nThe copy-augmented model reported an aggregated precision of 68.48%, recall of 33.10%, f1-\nscore of 56.42% and GLEU score of 59.48 with respect to CoNLL-2014 and GLEU score of 59.48 \nwith respect to JFLEG. The authors reported an increase in the evaluation metrics when the model \nis combined with denoising auto -encoders. A precision of 71.57%, recall of 38.65%, f1 -score of \n61.15% with respect to CoNLL -2014 and GLEU score of 61.00 with respect to JFLEG was \nachieved with such a combination.  \nMounika et al. [25] have experimented with both pre-trained and fine-tuned T5 models for \nlowering the word error rate  (WER) [26] in text generated by recognition of speech samples of \nmathematical equations. On both the models, GloVe & FastText embeddings were used to increase \nthe accuracy. The WER was successfully lowered from 36% to 16% by the proposed system.  \nBryant and Briscoe [27] discuss the use of language models (LM) in GEC . The work \nproposes a simple 5-step approach that relies on very little annotated data and can be used for any \nlanguage. The approach involves calculating the probability of input sentences, building a \nconfusion set for each token, rescoring the sentence, applying t he best correction, and iterating. \nThe work is concluded by highlighting the potential of LM -based approaches for GEC and their \nusefulness as a baseline for future research.  \nSreevidhya & Narayanan [28] have employed three vectorization techniques, namely, \nLatent Semantic Indexing (LSI), Sentence -BERT & Word2Vec to evaluate the answers by \nstudents. These answers are compared against the model answer using these embedding \ntechniques. They employ Cosine similarity to measure the resemblance of the student answer to \nthe model answer.  \nBased on the survey of the related works, it is inferred that models such as BERT & BART \nare used not only for various kinds of text processing works but has also been studied for their \neffectiveness in correction of errors in text.  MarianMT is also an enhanced version of BERT and \nis demonstrated to be effective for spelling error correction. Rohit et al. [40] developed a system \nto correct anomalies present in English statements using MarianMT and BART. The reported a \nWER reduction of 34% with MarianMT. Finally, they suggested a systematic analysis to identify \nthe patterns of corrections being performed by each of these models. Therefore, BART and \nMarianMT are chosen for this work. Additionally, while most reported works have studied the \neffectiveness of various models for error correction of one type of error  (either grammatical or \nspelling), this work tries to simultaneously handle both these error types. This work, an extension \nof the work by Rohit et al. [40], spotlights the error shifts occurring with the model predictions to \nunderstands the models’ capabilities  under differing scenarios of anomalies. This helps identify \nthe models’ strengths and weaknesses.  \nIII. Error category definition & shift analysis \nHossain et al. [29] studied the different errors in textual documents and presented the list \nof eight error types. These error types are: Typographic error, Cognitive error, Visual error, Run -\non error, Split -word error, Non-word error, Real -word error. Subsequently, Mounika et al. [25] \nstudied the error types present in automatically speech recognized (ASR) text in their work. Their \nwork provided a list of errors observed in sentences. Both the works of Hossain et al. & Mounika \net al. dealt with errors at w ord level. In this study, these categories of errors are abstracted to \nsentence level and four distinct categories of errors are proposed. The detailed definitions of these \nfour proposed error categories, namely, Cat A, Cat B, Cat C and Cat D, are provided below. These \nerror categories, each with a short description and example sentences, are tabulated in Table 1. \n1. Cat A: This is a no -error category. Here, the input sentence matches perfectly with the \ntarget sentence. \n2. Cat B: This category deals with sentences containing errors such as grammatical, word \nomission, capitalization. The input sentence s do not  match the target perfectly but all \nconstituent words of the input sentence are valid words. Here, the erroneous words are of \nreal-word error types as described by Hossain et al. Additionally, new words may be added \nor some words may be missing in the sent ence. The various caused which leads to \nformation of this error category are listed below. \n• Some words of the input sentence do not match to words in target; all such words in \nthe input sentence are valid dictionary words. \n• New words (valid dictionary words) are added or some words are missing in the input \nsentence. \n• Change in position of words leading to improper & grammatically inaccurate \nsentences. \n• Changing the sentence formation from direct to indirect speech or vice -versa; change \nof speech in sentences from first person to third person. \n3. Cat C:  The constituent words present in sentences contain spelling errors or typographic \nerrors. The input sentences contain non-word errors and other tokens which are not found \nin dictionary. This category may contain words from languages other than English as well. \n4. Cat D:  This category or error consists of input sentences containing both Cat B & Cat C \ntype errors. Thus, the sentences contain both non -word and real -word errors. For a \nlanguage model, this is the most complex type of error to deal with due to the presence of  \nboth Cat B & C type errors. Such errors are very confusing to humans as well. \nTable 1: Different Categories of Errors. \nError Category Description Example Sentence Example Target \nCat A No errors detected in \nthe text \nThese are cars. These are cars. \nCat B Grammatical errors \ndetected in the text. \nThese are car. These are cars. \nCat C Spelling errors \ndetected in the text. \nThese are rars. These are cars. \nCat D Sentence formation \nerrors are introduced. \nThess are car. These are cars. \nError Category Shift Causal Analysis  \nThere may be a mutlitude of reasons which causes a shift between any two categories of \nerror. A systematic study has led to identification of these reasons which are tabulated in Table 2. \nIt may be noted that these causes are identified based on study of the available dataset. \nTable 2: List of causes leading to error category shift of sentences. \n \n  \nInput \nCat A Cat B Cat C Cat D \nPredict\ned \nCat A • No Change • Grammatical errors are \ncorrected \n• Spelling errors are \ncorrected \n• Both, spelling & \ngrammatical errors are \ncorrected \nCat B \n• New words \nintroduced \n• Words deleted \n• Sequence change \n• Existing correct \nword altered but \nno spelling error \n• No change \n• New words introduced \n• Words deleted \n• Sequence changes of \nexisting words \n• Existing correct word \naltered but no spelling \nerror \n• New words introduced \n• Words deleted \n• Sequence changes of \nexisting words \n• Existing correct word \naltered but no spelling \nerror \n• Spelling errors \ncorrected to non-\nmatching words \n• New words introduced \nalong with correction of \nerroneous words. \n• Erroneous words \ndeleted \n• Sequence changes with \nspelling correction. \n• Existing correct word \naltered but no spelling \nerror \n• Spelling errors \ncorrected to non-\nmatching words \n• Spelling errors \ncorrected \nCat C \n• New words with \nspelling errors \n• Existing correct \nword altered but \nno grammatical \nerror \n• New words with \nspelling errors \nintroduced with \ncorrected grammar \n• Sequence changed \nwith correct grammar \nbut spelling error \n• No change. \n• New spelling errors \nintroduced. \n• Existing spelling \nerrors corrected with \nnew spelling errors \nintroduced. \n• New words with \nspelling errors with \ncorrected grammar \n• Existing errors \ncorrected but new \nwords spelling errors \n• Existing grammatical \nerrors got corrected. \n• Existing words \ncorrected but \nmisspelled \n• Existing errors \ncorrected but other \nexisting words \nmisspelled \nCat D \n• New misspelt \nwords and \ngrammatical errors \n• Existing words \naltered for spelling \nand grammatical \nerrors \n• Word deletion and \nexisting correct \nwords altered with \nspelling errors \n• Sequence changed \n& existing correct \nwords altered with \nspelling errors \n• New misspelt words  \n• Some existing words \nmisspelled  \n• Existing grammatical \nerror corrected but \nnew grammatical and \nspelling error \nintroduced \n• New words with \ngrammatical and / or \nspelling errors  \n• Existing correct words \nsequence changed  \n• Existing spelling error \ncorrected but new \ngrammatical & \nspelling error \nintroduced  \n• No change. \n• New spelling OR / \nAND grammatical error \nadded \n• New misspelt words \nwith corrected existing \nspelling error \n• New grammatical error \nwith corrected existing \ngrammatical error \nIV. Dataset \nC4 Dataset is an Open-Source dataset obtained with Common Crawl web scrape [30]. This \ndataset contains millions of collected sentences along with their target sentences. The collected \nstatements, referred to as input sentences in this work, belong to one of the categories defined in \nSec. III. The input sentences are manually typed chat messages sent by users across the internet. \nThese target sentences are manually inspected corrected input sentences by human experts. These \ntarget sentences act as the ground truth for this work. Due to constraints of available computing \ninfrastructure, only a million sentences from this dataset are chosen for this work. \nThese one million sentences were analyzed for the error category distribution. Fig.1 \nrepresents the distribution pattern of the dataset chosen for this work. It may be observed that most \nof these input sentences are error free and belong to Cat A. \nThese one million sentences were split into train, validation, and test sets with 50:20:30 \nratio. 500,000 sentences were used for training of the models while 200,000 sentences were used \nas validation sets (refer Table 3). A closer inspection of the test s et sentences revealed that some \nof the target sentences contained errors (improper job in correcting the input sentences by human \nexperts). Since the existence of such error affects the evaluation of the models, an exercise to \ninspect and correct the targets was undertaken. Such an exercise demands huge investment of time. \nHence, a limited set of 25,738 sentences were inspected and corrected in this work. This set, called \nas ‘UpdTest’ is used for all testing and result reporting in this work. \nFig. 1: Error category distribution of the input sentences in the dataset used for this work.  \nTo understand the impact of target sentence inaccuracy on the model training, a study was \nconducted on the 25,738 samples in the UpdTest set. The study involved identification of sentences \nwhich passed as error free during the quality inspection process. It was observed that ~90% target \nsentences were error free. Since, most of the target sentences are error -free, it was assumed that \nonly a negligible influence shall be exerted by the rest 10% of the erroneous sentences during the \nmodel training. So, a decision to continue with the training set (500,000 sentences) and validation \nset (200,000 sentences) was taken.  \nTable 3: Count of sentence records in Train, Test & Validation sets. \nParameter Value \nTrain 500,000 \nValidation 200,000 \nTest 300,000 \nUpdTest 25,738 \nV. Methodology \na. Model Design \nSequence to Sequence (Seq2Seq) [31] models are an industry standard & used for various \nkinds of text analytics and natural language processing (NLP) tasks. Seq2Seq model employs \nadvanced deep learning algorithms with elaborate encoder -decoder architectures to perform such \ntasks. The encoder in Seq2Seq models reads the input sequence and summarizes it into a fixed \nlength representation called state or context vector. The decoder uses this context vector to \ngenerate an output which is task specific. The choic e of architecture for each Seq2Seq model \ndepends on the task at hand. \n\nSeq2Seq models achieve their intended target tasks by conversion of textual data from one \ndomain to another. Such conversion operations include machine translation [32], text \nsummarization [33], image captioning [34], chatbot interactions [35] & query engi nes [36]. \nGenerating error -free sentences from erroneous text is another such task which may be \naccomplished by Seq2Seq modeling [31]. This error correction is achieved by analysis of the \ncontext, neighborhood and meaning of the sentence. Two such Seq2Seq models, namely, \nBidirectional Auto-Regressive Transformers (BART) and MarianMT, are utilized in this work. \nBART is a self-supervised denoising autoencoder that aids in the recognition of words and \nphrases. It is a pre -trained model that combines the advantages of auto -regressive transformers \nwith bidirectional traversal. BART’s working consists of two sequential steps: (i) adding noise to \nthe input text, & (ii) reconstructing the correct output. The first step is achieved either by corrupting \na predetermined character or sequence, or by using appropriate noise -generating functions. The \nreconstruction of the correct output sequence is achieved with usage of a language model with a \nSeq2Seq architecture that aids in learning and reconstruction of the output string by substituting \nvalid tokens for noise [37]. BART’s architecture allows for selection of different nosing functions \nfor effective learning. The BART model from the SimpleTransformers library [38] is utilized for \nthis work. \nMarianMT [39], a sophisticated neural translation framework that uses a sequence -to-\nsequence model, is mostly employed for text translation. They are C++ translational frameworks \nthat are effective and independent. The foundation of MarianMT is also a typical encoder-decoder \ndesign. In the current work, MarianMT is used to transform incorrect English sentences into their \ncorrected equivalents. It’s assumed that such a transformation is equivalent to language translation \ntask where incorrect sentences form a variation of the English language. The MarianMT model \nemployed in this work uses pre -trained models from the SimpleTransformers library [38]. The \ndecoder, \"Helsinki-NLP/opus-mt-NORTH EU-NORTH EU,\" is t rained with transfer learning  in \nthis work for converting sentences from erroneous English language back to corrected English, \nthereby, reducing textual error. \nBoth the pretrained models, BART and MarianMT, are trained on a train set of 500,000 \nrecords with a batch size of 32 sentences in each batch. The model employs beam search with a \nbeam width of 5 for the prediction of tokens from sequence of words. A fivefold cross validation \nwas also employed during the training process. The training lasted for ~27 hours on a GTX Titan \nBlack GPU with 12 GB RAM to complete one epoch. \nb. Error Category Analysis Algorithm \nTo analyze the error category for the input sentences, an algorithm is developed. The \nsentences along with the constituent word tokens of the input-target pairs are utilized to accomplish \nthe category determination task. A pseudo-code for the algorithm is presented in Fig. 2 while Fig. \n3 provides a flowchart demonstration. \nAt the first stage of the algorithm, the input sentence is compared against the target to \ndetermine for their match. If the input matches the target sentence, then it belongs to Cat A. For \nnon-match scenarios, both the input and target sentences are tokenized to extract their constituent \nwords. The constituent words of the input sentence are searched for their existence in the target \nlist as well as a dictionary. The input sentence word tokens which are not found in the target \nsentence are referred to as m is-matched words. If all the mis-matched words from the input are \nfound to exist in the dictionary, then the input sentence is categorized as Cat B, considering it as a \nmere grammatical error. However, if some of these words are found to exist in the dictionary, then \nthe sentence is declared as Cat D. If none of the mis -matched words are found in the dictionary, \nthe mis-matched words in the input sentence are replaced wit h words corresponding to the index \nof the target sentence. If the target now matches th e input, it is declared as Cat D else the input \nsentence is assumed to be containing pure typographic errors and declared as Cat C. \nFig. 2: Pseudo-code for the Error Analysis \n\n \nFig. 3: Error analysis algorithm flow diagram \nVI. Result and discussion \nBoth the BART and MarianMT pre-trained models are trained with available trained data. \nSince the pretrained models are trained on billions of text samples, they already are capable of \nhandling grammatical errors to varied degrees. Most of the prior works employ transfer learning \nfor spelling error correction . Therefore, it is assumed that the model’s ability to handle spelling \nerrors is not adequate and transfer learning, with the training dataset, is performed to enhance these \nmodels’ capacity in handling spelling errors. \na. BART confusion matrix \nThe predictions of the BART models for the test set (25,738 sentences) were analyzed with \nthe error category detection module. The categories of error were recorded and tabulated as seen \n\nin Table 4. An analysis of error correction for different categories of input sentences by the BART \nmodel reveals that the model behaves quite differently while responding to each of them. While \n99.8% of Cat A sentences were predicted correctly (sentences predicted as-is), only 9% of Cat B, \n22% of Cat C and a miniscule of 3% of Cat D were fully corrected. Looking at this pattern, it may \nbe inferred that BART is able to handle spelling error corrections better than grammatical errors. \nTable 4:  Confusion matrix for BART  \n \nInput BART \nCat A Cat B Cat C Cat D Total \n \nPredicted \nCat A 10975 1229 123 54 12381 \nCat B 13 11169 66 449 11697 \nCat C 0 2 324 24 350 \nCat D 0 26 22 1262 1310 \nTotal 10988 12426 535 1789  \nSimilarly, for input sentences from Cat B, 89.9% of the sentences did not have any category \nshift. Only 1229 (9.9%) of sentences got corrected and moved to Cat A, while 28 (remaining 0.2%) \nsentences had spelling errors introduced, thereby pushing them to Cat C or D. This shift is due to \nthe new spelling errors being induced. Thus, its inferred that the model introduces spelling errors, \nthough no plausible explanation exists, currently, for such an introduction. \nIt may be observed from Table 4 that 123 (22.9%) sentences from Cat C were shifted to \nCat A with complete elimination of spelling errors . The spelling errors of 66 (12.3%) sentences \nwere corrected and these sentences were shifted to Cat B. This scenario happens when there are \nno spelling errors but the sentences contain grammatical errors. While category shift did not occur \nfor 324 (60.5%) sentences (remained with Cat C as-is), predictions for 22 (4.1%) sentences moved \nthem to Cat D. Various cases of movement from Cat C to Cat B or D are listed in Table 2. \nWith a 3% complete correction rate for Cat D, 54 sentences are predicted completely error \nfree (a shift from Cat D to Cat A). Spelling errors are removed for 449 (25.2%) sentences \n(additionally, new grammatical errors might have been introduced), thereby shifting them to Cat \nB from Cat D. Furthermor e, 24 (1.3%) sentences are moved from Cat D to Cat C. This occurs \nwhen either the grammatical mistakes are corrected or the new words responsible for grammatical \ncorrection are introduced with spelling errors. 1262 (70.5%) sentences still contained both \ngrammatical and spelling errors remaining in Cat D. The various causes for such movements are \nlisted down in Table 2.  \nFurthermore, Cat D to Cat B shift would mean an additional increase in the percentage of \ntypographic errors being corrected. Similarly, Cat D to Cat C shift would mean an additional \nincrease in the percentage of grammatical errors being corrected. Hence, total percentage of errors \nbeing corrected for each of the se categories are listed in Table 5. It may be noted from this table \nthat the BART model has performed significantly well for spelling error correction when compared \nto grammatical error correction. This is in contrast to the observations by Alikaniotis & Raheja [9] \nwhere they state that such models are suited to grammatical error correction (GEC). \nTable 5: Error percentage for BART.  \nError Formula Calculation Percentage \nTotal Spelling Error \nCorrection Percentage \n((Cat D -> Cat B) + (Cat C -> Cat A))/ (Sum of \nCat D transitions + Sum of Cat C transitions) \n572 / 2324 24.6% \nTotal Grammatical Error \nCorrection Percentage \n((Cat D -> Cat C) + (Cat B -> Cat A)) / (Sum of \nCat D transitions + Sum of Cat B transitions) \n1253 / 14215 8.8% \nTotal Mixed Error \nCorrection Percentage \n(Cat D -> Cat A) / (Sum of Cat D transitions) 54 / 1789 3.1% \nb. MarianMT confusion matrix \nSimilar to BART, MarianMT models were also analyzed with UpdTest set sentences (25, \n738) for error categorization, and the results are presented in Table 6. A total of 10956 (99.7%) \nsentences are predicted as -is error free & remain in Cat A. However , grammatical error was \nintroduced in a tiny number of 32 sentences with 0.3% shift to Cat B. This could have occurred \ndue to new words added by the model, deletion of words or sequence change of the sentence. \nTable 6:  Confusion matrix MarianMT \n  \nInput MarianMT \nCat A Cat B Cat C Cat D Total \nPredicted \nCat A 10956 669 93 26 11744 \nCat B 32 11617 24 390 12063 \nCat C 0 6 363 18 387 \nCat D 0 134 55 1355 1544 \nTotal 10988 12426 535 1789  \nWhile 5.4% of sentences with grammatical error were corrected and 669 sentences shifted \nfrom Cat B to Cat A, 11617 (93.5%) of sentences remained in Cat B as -is. A minute 0.04% (6 \nsentences) were shifted to Cat C with only spelling errors in them. 134 (1.1%) sentences with \ngrammatical errors got additional spelling errors induced and shifted from Cat B to Cat D. \nFor input sentences in Cat C, 93 (17.4%) sentences got spelling errors rectified and shifted \nto Cat A while 363 (67.9%) sentences were predicted as -is & remained in Cat C. Spelling errors \nwere corrected for 24 (4.5%) sentences to grammatical errors making a shift from Cat C to Cat B \nand 55 (10.3%) of the sentences shifted from Cat C to Cat D. \nWith a closer inspection, it may be noted from Table 6 that 26 (1.5%) sentences containing \nboth grammatical and spelling errors were completely rectified and shifted to Cat A from Cat D. \nWhile 390 (21.8%) sentences were predicted spelling error free with shift to Cat B , 18 (1%) \nsentences were predicted grammatical error free and shifted to Cat C from Cat D. The reasons for \nsuch shifts are mentioned in Table 2. Finally, 1355 (75.7%) sentences remained as-is in Cat D. \nSimilar to BART, if we conduct an analysis on the Cat D to Cat B jump and Cat D to Cat \nC jump for MarianMT to assess the amount of grammatical and spelling errors being corrected , \nrespectively. The results of the analysis are presented in Table 7. It may be noted from Table 7 that \nthe total amount of corrections performed from MarianMT is less than from BART model. \nHowever, a similar pattern of correction with maximum corrections coming from spelling errors \nis observed for MarianMT as well. \nTable 7: Error percentage for BART.  \nError Formula Calculation Percentage \nTotal Spelling Error \nCorrection Percentage \n((Cat D -> Cat B) + (Cat C -> Cat A))/ (Sum of \nCat D transitions + Sum of Cat C transitions) \n483/2324 20.8% \nTotal Grammatical Error \nCorrection Percentage \n((Cat D -> Cat C) + (Cat B -> Cat A)) / (Sum of \nCat D transitions + Sum of Cat B transitions) \n687/14215 4.8% \nTotal Mixed Error \nCorrection Percentage \n(Cat D -> Cat A) / (Sum of Cat D transitions) 26/1789 1.4% \nc. Error Shift Analysis  \nThis subsection conducts a detailed analysis for all different categories of error shifts.  \nWhile granular quantitative analysis for each cause for such shifts is not carried out, general pattern \nanalysis, for multiple causes for such shifts, is performed here.  \nThe errors introduced, change the structure and context of the sentence. The main idea \nbehind analyzing the error and shift in error category is to study and identify the patterns associated \nwith the shifts and understand the weak points of the model s. This understanding may help, in \nfuture works, to increase the performance and accuracy of the models. \nError Shifts for Cat A Input \nThis category deals with input sentences which match perfectly to the target sentences. A \nshift from Cat A to any other  category indicates that the predicted sentence contains error. Such \nchange is possible due to one or more of the following causes: (i) errors, spelling or grammatical, \nexist in target while the predicted text is error-free, and (ii) models’ predicted text is correct but an \nalternate form of sentence (output & target text mismatch but correct). \nIt may be observed in Table 4 & Table 6 that both BART and MarianMT have shifted \nsentences from Cat A to Cat B. Additionally, it may be recalled from the earlier discussion that \nmost of the input sentences stayed put in the same category, thereby, confirmi ng the assumption \nthat both the chosen models are fairly resistant to inducing error for accurate input sentences.  \nCategory A -> Category B \n This subsection examines the cases of shifts from Cat A to Cat B with examples. \nTable 8: Error category shift for Cat A to Cat B for BART and MarianMT. \n \nExample 1 Example 2 Example 3 Example 4 \ntarget Because it \nsmells \nartistic. \nEnjoy this rather short ( 1' 40'' ) : \nIt is the impressive story of a \nbrave teenager rescuing dolphins. \nThe way of learning: \nreading vs. experience vs. \nexperience of someone \nI listened to an audio \nbook `` Doctor WHO \n`` \ninput Because it \nsmells \nartistic. \nEnjoy this rather short ( 1' 40'' ) : It \nis the impressive story of a brave \nteenager rescuing dolphins. \nThe way of learning : \nreading vs. experience vs. \nexperience of someone \nI listened to an audio \nbook `` Doctor WHO \n`` \nBART Because it \nsmell like \nart. \nEnjoy this rather short ( 1'40'' ) : It \nis the impressive story of a brave \nteenager rescuing dolphins. \nThe way of learning : \nreading, experience vs. \nexperience of someone \nI listened to an audio \nbook `` Doctor WHO \n`` \nMarian \nMT \nBecause it \nsmell art. \nEnjoy this rather short ( 1'40'' ) : It \nis the impressive story of a brave \nteenager rescuing dolphins.  \nThe way of learning : \nreading vs. experience vs. \nexperience of someone \nI listened to an audio \nbook  Doctor WHO \nExample 1 in Table 8  contains a sentence which starts with a conjunction, namely, \n“because”. S ince the input begin with a conjunction, it’s extremely difficult to determine the \ncontext for the sentence. This poses a serious challenge in front of  both BART and MarianMT to \narrive at the intended meaning of the sentence, leading to failure of both these models. Example 2 \npresents a sentence which contains data in Degree, Minutes and Seconds (DMS) format. While \ndefinite symbolism for representation of such data exists, there is no standard framework which \nspecifies usage of space between values for such data. Thus, data of this format exists both with \nand without spaces. Possibly, pre -trained models of both BART & MarianMT were trained on \nformat of no-space representation for DMS. This led to both these models removing the space in \nthe data. While the predicted sentences are not erroneous, yet they don’t match the target, leading \nto shift in error category. A closer look at example 3 gives us the understanding that the BART \nmodel has introduced error by removing the word ‘vs’ whereas MarianMT's prediction has \nmatched the target. BART predicted the sentence in an accurate, yet different representation. This \ncan be attributed to the context understanding of the given input sentence by BART model . In \nExample 4, the input sentence contains a par of the sentence within 2 single quotes instead of a \ndouble quote. While this form of representation is not quite standard, its not completely erroneous \neither. BART has left the sentence untouched while MarianMT has removed the 2 single quotes. \nSo, MarianMT generated a greater error in such prediction leading to a category shift. \nTo summarise, the change in error category is observed due the model s’ failure to \nunderstand the context of the sentence. Additionally, error-shifts are also observed for incomplete \nor improperly formed sentences.  \nCategory B Error Shifts: \nIn the Category B error input text is erroneous as it consists of grammatical errors which \nchange the structure and context of the sentence. These errors include omission or addition of \nwords/phrases, change in parts of fundamental grammar.  The following subsections examine, with \nexamples, the cases of shifts for input statements in Cat B.  \nCategory B -> Category A \nIn Example 1 of Table 9, “it” has been added by both the models to complete the sentences \nand make it error free. Similarly in example 2, the models added the article “the” to predict correct \nsentences. In example 3, BART corrects “I had been” to “I have been ” to make the sentence \ncontextually right. In example 4, in “most of people” “of” is omitted by MarianMT to predict a \ncorrect sentence. \nTable 9: Error category shift for Cat B to Cat A for BART and MarianMT. \n \nExample \n1 \nExample 2 Example 3 Example 4 \ntarget_new I enjoyed \nit. \nAbout 30 people joined the \nsame company together. \nSo now I have been \nforcing myself to do it. \nMost people think that Poland \nis a backward country. \nInput I enjoyed. About 30 people joined same \ncompany together. \nSo now I had been \nforcing myself to do it. \nMost of people think that \nPoland is a backward country. \nBART I enjoyed \nit. \nAbout 30 people joined the \nsame company together. \nSo now I have been \nforcing myself to do it. \nMost of people think that \nPoland is a backward country. \nMarianMT I enjoyed \nit. \nAbout 30 people joined the \nsame company together. \nSo now I had been \nforcing myself to do it. \nMost people think that Poland \nis a backward country. \nFrom the above examples, it can be observed that both the models understand the \ngrammatical rules well enough to add articles wherever necessary. The models also interpret the \ncorrect tenses for simple input sentences. However, each model also have their f ailure scenarios \nthough no specific pattern is observed for such failures.  \nCategory B -> Category C \nThis subsection examines the cases of shifts from Cat B to Cat C with examples.  In Table \n10-Example 1, BART grammatically corrects “as a” to “as an” but misspells “orthodontist”. \nSimilarly in Example 2, “videos” is added for the subject verb agreement, but “imaginative” is \nmisspelled as “imaginationful” by BART. MarianMT misspells “visit” as “visite” in Example 3. \nIn Example 4, both the models changed “broadcast” to  “broadcasted” and predicted sentences. \nHowever, its quite difficult to state if the models predicted wrongly given the constitution of the \ninput sentence. \nTable 10:  Error category shift for Cat B to Cat C for BART and MarianMT. \n \nExample 1 Example 2 Example 3 Example 4 \ntarget_new I think it's difficult for me \nto work as an \northodontist. \nTheir music videos are so \nCOOL and they are very \nimaginative. \nI would like to \nvisit the USA. \nIt was about the \nNetherlands. \ninput I think it's difficult for me \nto work as a \northodontist. \nTheir music video are so \nCOOL and they are very \nimagination. \nI would like to \nvisiting USA. \nIt broadcast about \nthe Netherlands. \nBART I think it's difficult for me \nto work as an orthodist. \nTheir music videos are so \nCOOL and they are very \nimaginationful. \nI would like to \nvisit the USA. \nIt was broadcasted \nin the Netherlands. \nMarianMT I think it's difficult for me \nto work as a \northodontist. \nTheir music video are so \nCOOL and they are very \nimaginated. \nI would like to \nvisite the USA. \nIt broadcasted \nabout the \nNetherlands. \nThe models are observed to have learnt well on grammatical rules like adding articles. But \nwhen they encounter possibly new words such as “orthodontist”,  spelling errors arise. This pattern \nof spelling error injection is also observed for small, incomplete and context-absent sentences.  \nCategory B -> Category D \nThis subsection examines the shifts from Cat B to Cat D, with examples. Table 11 presents \nsome example cases of this category shift. As may be observed in Example 1, “to a” was omitted \nin the input & the same was added by BART . Additionally, BART inserted a space between \n“Shijo” & “Karasuma” in the input word “ShijoKarasuma” which is a right thing to do. However, \ndue to error in the target sentence, this is considered as an erroneous category shift. On the other \nhand, MarianMT not only misses to include “to a” into the sentence, it also incorrectly converts \n“ShijoKarasuma” to “Shijo”. In Example 2, BART predicts “detail” as “TheDetail” as one word \nmaking it a spelling error and does not make “detail” plural. Except for the missing space, the \nsentence predicted by BART is correct, but a mismatch against the target. In Example 3, “Boeing” \nis misspelt as “teaching” and “byborg” . The tense is also predicted wrong by MarianMT  for the \nsame sentence . Example 4  presents a case where both the models have failed to capture the \ngrammar from the contest. Additionally,  MarianMT predicts “IBT” as “VIT” which is change of \nthe proper noun. \nTable 11: Error category from Category B to Category D for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget The day before yesterday, my \nold classmates and I went to a \nsushi bar at ShijoKarasuma. \nDetails are in \nthe following \nsite. \nThe Boeing 787, a new \naircraft manufactured by \nBoeing, came to Japan \ntoday. \nHas anyone \ntaken the IBT \ntoefl before? \ninput The day before yesterday, my \nold classmates and I went sushi \nbar at ShijoKarasuma. \nDetail is in \nfollowing site. \nBoeing 787, a new air craft \nmanufactured by Boeing, \ncome to Japan today. \nIs anyone take \nthe IBT toefl \nbefore? \nBART The day before yesterday, my \nold classmates and I went to a \nsushi bar at Shijo Karasuma.. \nTheDetail is in \nthe following \nsite. \nBoeing 787, a new air craft \nmanufactured by Boeing, \ncame to Japan today. \nIs anyone take \nthe IBT toefl \nbefore? \nMarianMT The day before yesterday, my \nold classmates and I went sushi \nbar at Shijo \nin the following \nwebsite. \nteaching 787, a new air \ncraft manufactured byborg, \ncome to Japan today. \nIs anyone take \nthe VIT toefl \nbefore? \nIn the above examples, the models predict correct sentences but do not match the target, or \ngrammatically erroneous sentences due to lack of context. Additionally, these models add spelling \nerrors with proper nouns, as they haven’t learnt these words earlier.  \nCategory C Error Shifts: \nIn the Category C error input text is erroneous as it consists of spelling errors which change \nthe structure and context of the sentence. The spelling errors include words that cannot be found \nin the English dictionary, capitalization of proper nouns and abbreviation. \nCategory C -> Category A \nTable 12: Error category from Category C to Category A for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget Good evening! I want to talk to \nher \nSo, I will study hard \ntomorrow. \nOur daily life is getting useful day \nby day. \ninput Good \neveninng! \nI wnat to talk to \nher \nSo, I will study hard \ntommorow. \nOur daily life is getting usuful day \nby day. \nBART Good evening! I want to talk to \nher \nSo, I will study hard \ntomorrow. \nOur daily life is getting usuful day \nby day. \nMarianMT Good evening! I want to talk to \nher \nSo, I will study hard \ntommorow. \nOur daily life is getting useful day \nby day. \nThis subsection examines the cases of shifts from Cat C to Cat A with examples. Table 12 \npresents some representative samples for such error category shift. In Example 1 and Example 2, \n'evening' is misspelt as 'eveninng' and 'want' is misspel t as 'wnat', respectively. Both BART and \nMarianMT are able to handle these errors effectively and correct the sentences. In Example 3, \nBART corrects “tommorow” as “tomorrow” while MarianMT fails to achieve the same. Similarly \nin Example 4, MarianMT spell corrects “usuful” to “useful” and BART is not able to correct this \nspelling error. \nBoth BART and MarianMT models have been able to correct spelling errors in the above \nmentioned examples. This may be due the fact that the input sentences are simple, complete and \nwith minimal spelling errors. Once again, there is no pattern to the observed failures. \nCategory C -> Category B \nThis subsection examines the cases of shifts from Cat C to Cat B with examples. Table 13 \npresents example cases of such shift. As shown in Example 1, BART corrected “it raind” to “it’s \nbeen raining”. This change makes the predicted sentence a mismatch with the target, thereby, \ndeclaring it as a grammatical error. MarianMT, on the other hand, changes “raind” to “rains” which \nmakes an inaccurate as well as grammatically incorrect statement. In Example 2, both the models \ncorrected “everydays” as “every day”, treating it as two different words. Since both “everyday” \nand “every day” are valid forms of usage, the correction is appropriate, yet mismatches the target. \nIn Example 3, BART puts “motivation” instead of “exercise” which not only alters the context of \nthe sentence, but also marks it as grammatical error. Both models have failed here. Similarly in \nExample 4, MarianMT changes “sennd” to “send” instead of “second” which does not match the \ntarget while BART is able to correct the error. \nTable 13: Error category from Category C to Category B for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget Today, it rained for a \nlong time. \nI use it for studying \neveryday. \nI need more \nexercise. \nThe second one is the Chinese \nPavilion in the EXPO. \ninput Today, it raind for a \nlong time. \nI use it for studying \neverydays. \nI need more \nexcersise. \nThe sennd one is the Chinese \nPavilion in the EXPO. \nBART Today, it's been raining \nfor a long time. \nI use it for studying \nevery day. \nI need more \nmotivation. \nThe second one is the Chinese \nPavilion in the EXPO. \nMarianMT Today, it rains for a long \ntime. \nI use it for studying \nevery day. \nI need more \nexcersise. \nThe send one is the Chinese \nPavilion in the EXPO. \nBoth the models are fairly able to capture and correct spelling errors in sentences with \nadequate context. Sometimes in the process of correction, especially for shorter sentences lacking \ncontextual information, the corrections lead to formation of either alternate forms of sentences or \ndifferent sentences.  \n \nCategory C -> Category D \nThis subsection examines the cases of shifts from Cat C to Cat D with examples. Table 14 presents \nselected examples from such category shift. \nTable 14: Error category from Category C to Category D for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget_new I sometimes felt sea \nsickness. \nFortunately everyone \nwas on time! \nit's even offensive \nin Japan. \nBut I'll try to keep writing \njournals regularly from now on. \ninput I sometimes felt sea \nsichness. \nFortunately eveyone \ncames on time! \nit's even offence \nin Japan. \nBut I'll try to keep writng \njounal regularly from now on. \nBART I sometimes felt the \nsea sichness. \nFortunately eveyone \ncame on time! \nit's even an \noffence in Japan. \nBut I'll try to keep writing \njounal regularly from now on. \nMarianMT I sometimes felt \nseakelness. \nFortunately eveyone \ncame on time! \nit's even offence \nin Japan. \nBut I'll try to keep writing a \njounal regularly from now on. \nAs shown in Example 1, BART failed to correct “sichness” to “sickness”. It also added an \nunwanted article “the”. Similarly in Example 3, BART added the article “an” making the sentence \ngrammatically correct but a mis-match against the target.  In Example 2, both the models corrected \n“cames” to “came”  making the sentence correct. A closer examination suggests the target to be \ninaccurate representation for the input sentence. In Example 4, both BART & MarianMT corrects \nthe spelling error in “writng” to “writing” but fails to handle the same for “jounal”. \nThough the models have failed to correct the spelling errors in this scenario, they may have \ngenerated better version s of the sentence s in a grammatical sense . This could be due to lack of \ncontext or incomplete sentences given as input.  \nCategory D Error Shifts: \nIn the Category D error input text is erroneous as it consists of both spelling errors and \ngrammatical errors which change the structure and context of the sentence. \nCategory D -> Category A \nThis subsection examines the cases of shifts from Cat D to Cat A with examples. As seen \nin Example 1 of Table 15, both the models correct the spelling of “people” and change “problem” \nto its plural form to generate an error free sentence. Similarly, in Example 2, “of course” is \ncorrected by both the models. In example 3, BART corrects the spelling of “visit” and adds “the” \nto give a completely correct sentence. MarianMT on the other hand has been able to add the article \n“the” but fails to correct the spelling error. In Example 4, MarianMT has been able to correct the \nspelling error in “communicating” as well as convert the “foreigner” to its plural form. BART, \nhere on the other hand, has failed on both the accounts. \nThe models have learnt well to adjust the words in singular or plural according to the \ncontext as well as to add required articles like “the” wherever required. Additionally, the models \nalso made spelling corrections. \nTable 15: Error category from Category D to Category A for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget So younger people in Japan \nshould have more interest in \nthese problems  \nOf course I should \nspeak in English \nthere! \nI will visit the \nUSA this winter! \nThe Internet helps us with \ncommunicating with \nforeigners. \ninput So younger peopole in Japan \nshould have more interest in \nthese problem  \nOfcourse I should \nspeak in English \nthere! \nI will visite USA \nin this winter! \nThe Internet helps us with \ncommunicatig with \nforeigner. \nBART So younger people in Japan \nshould have more interest in \nthese problems  \nOf course I should \nspeak in English \nthere! \nI will visit the \nUSA this winter! \nThe Internet helps us with \ncommunicatig with \nforeigner. \nMarianMT So younger people in Japan \nshould have more interest in \nthese problems  \nOf course I should \nspeak in English \nthere! \nI will visite the \nUSA in this \nwinter! \nThe Internet helps us with \ncommunicating with \nforeigners. \n \nCategory D -> Category B \nThis subsection examines the cases of shifts from Cat D to Cat B with examples. Table 16 presents \nsome chosen examples to illustrate the patterns of shifts observed in this category. \nTable 16: Error category from Category D to Category B for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget He meets her \neveryday. \nSo I can speak a little \nFrench. \nLuckily, I've found this \nwebsite. \nToday my sister came to \nsee me. \ninput He meet her \nevryday. \nSo I can speak French a \nlittele. \nLuckly, I find this web. Today my sisiter come to \nsee me. \nBART He met her \nyesterday. \nSo I can speak French a \nlittle. \nLuckily, I found this \nwebsite. \nToday my sisiter came to \nsee me. \nMarianMT He meet her \neveryday. \nSo I can speak French a \nlittle. \nLuckily, I find this \nwebsite. \nToday my sister come to \nsee me. \nAs seen in Example 1, MarianMT has been able to correct the spelling error but fails to \ncorrect the grammatical error. BART, on the other hand, predicts a perfectly correct but different \nstatement with interpretation of “evryday” as “yesterday”. I n Example 3, BART predicted the \nsentence is contextually right, but a mismatch to the target. MarianMT corrects “website” but the \nsentence presents a meaning which is different from the intended one. In Example 2, both the \nmodels correct little’s spelling, but produce sentences with grammatical error. In example 4, \nMarianMT corrects sister’s spelling but the tense used is wrong. \nThe above examples demonstrate that the models predict sentences that remain the same, that are \ncontextually correct but do not match the target or the model fails to learn the context and produces \na different and wrong sentence.  It’s observed that BART performs superior corrections when \nlengthier sentences with better context is presented. \nCategory D -> Category C \nThis subsection examines the cases of shifts from Cat D to Cat C with examples. Table 17 presents \nexample cases to illustrate patterns for this category shift. \nTable 17: Error category from Category D to Category C for BART and MarianMT  \n \nExample 1 Example 2 Example 3 Example 4 \ntarget_new I was very \nsurprised. \nIf you learn Japanese, I will \nhelp you. \nJapanese believe that it is a \nlucky symbol. \nIt will be a nice \ntime. \ninput I was suprised very \nmuch. \nIf you learn Japanease, I \nhelp you. \nJapanese believes that it is a \nluckey symbol. \nIt will be naice \ntime. \nBART I was very suprised. If you learn Japanase, I will \nhelp you. \nJapanese believe that it is a \nluckey symbol. \nIt will be naice \ntime. \nMarianMT I was very suprised. If you learn Japanese, I \nhelp you. \nJapanese believes that it is a \nluckey symbol. \nIt will be a naice \ntime. \nAs seen in Example 1 “surprised” spelling was not corrected  by both the models. In \nExample 2, BART is unable to make corrections to spelling error present in the proper noun \n“Japanese” while MarianMT handles this properly. On the other hand, MarianMT is not efficiently \nable to handle the error in grammar while BART is able to correct the grammar error. In example \n3, BART corrects “believe” to “believes” but does correct “ luckey”. In example 4, MarianMT \nmakes the grammatical correction by adding “a”, but “nice” spelling remains erroneous.  \nThese models fail to rectify spelling errors because the words may be proper nouns or the \nmodel encounters these words for the first time, hence fails to understand its context and usage.  \nVII. Conclusion \nIt can be concluded that both BART and MarianMT trained better for spelling error \ncorrection than grammatical error correction, with BART outperforming MarianMT. Both the \nmodels established some new errors while rectifying existing errors hence moving from  \ngrammatical error category to spelling error category and vi ce versa, it can be inferred that \nalthough these models correct the error present in the sentence there is a possibility that a new type \nof error be introduced. During the error category jump ana lysis, interesting scenarios were \nobserved. Though the models did not predict the exact target sentence, they generated English \nsentences that retained the context of the statement. The most significant achievement of this \nanalysis is the shift from Cat D to Cat A. While this shift happened only on 3% of the data, it can \nbe concluded that BART and MarianMT has the ability to correct a mixed category of errors \nmaking it a robust model. A crucial observation was that the shift from Cat D to Cat B and Cat D \nto Cat C are more common, while the shift to Cat A is rare.  \nWhile understanding the analysis and conclusions drawn in this work, it can be extended \nfurther by conducting a detailed study of shifts in categories and comprehending the reason behind \nthe shift in each sentence. The grammatical error category can be further split into multiple \ncategories such as words omission, words addition, sequence change, grammar change, and an \nanalysis can be performed to study the performance of the models better . A test to check the \nbreaking point of the model can be conducted, to realize the limit of the model’s ability to correct \nerrors. Finally, this work can be expanded further to test with other language models to learn their \nstrong points. \nDECLARATIONS \nAcknowledgement \nThe authors thank their university for the needed infrastructure support for this research work & \nmanuscript preparation. \nEthical Approval \nNot Applicable. \nAvailability of supporting data \nThe C4 dataset , used for the work reported in this paper , is available at \nhttps://www.tensorflow.org/datasets/catalog/c4. \nCompeting interests \nThe authors declare that they have no competing interest for this work. \nFunding \nNo external funding was received for this work. \nReferences  \n[1] Barber K (2007) The Anthropology of Texts, Persons and Publics (New Departures in Anthropology). Cambridge: Cambridge \nUniversity Press. doi:10.1017/CBO9780511619656. \n[2] Smith R (2007) An Overview of the Tesseract OCR Engine - 1995. Proc. Ninth Int. Conference on Document Analysis and \nRecognition (ICDAR), IEEE Computer Society, pp. 629-633. \n[3] Nassif A B, Shahin I, Attili I, Azzeh M, Shaalan K (2019) Speech Recognition Using Deep Neural Networks: A Systematic \nReview. IEEE Access, vol. 7, pp. 19143-19165, doi: 10.1109/ACCESS.2019.2896880.  \n[4] Singrodia V, Mitra A, Paul S (2019) A Review on Web Scraping and its Applications. Int. Conf. on Computer \nCommunication and Informatics (ICCCI), Coimbatore, India, pp. 1-6, doi: 10.1109/ICCCI.2019.8821809.  \n[5] Sewwandi U, Ranathunga L, Wijethunge S (2021) A Rule Based Approach for Detection and Correction of Grammar Errors \nin Written Active Voice Sinhala Sentences. 21st International Conference on Advances in ICT for Emerging Regions (ICter), \nColombo, Sri Lanka, pp. 159-164, doi: 10.1109/ICter53630.2021.9774800.  \n[6] Sutskever I, Vinyals O, Le Q V (2014) Sequence to Sequence Learning with Neural Networks. In: New York, US, Cornell \nUniversity. https://arxiv.org/abs/1409.3215v3. \n[7] Lewis M, Li, Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L (2019) BART: Denoising \nSequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. New York, US, Cornell \nUniversity. https://arxiv.org/abs/1910.13461v1. \n[8] Kumar A, Pati P B (2022) Offline HWR Accuracy Enhancement with Image Enhancement and Deep Learning Techniques. \nInt. Conf. on Machine Learning & Data Engineering. doi:10.1016/j.procs.2022.12.399. \n[9] Alikaniotis D, Raheja V (2019) The unreasonable effectiveness of transformer language models in grammatical error \ncorrection. ACL 2019 - Innovative Use of NLP for Building Educational Applications, BEA 2019 - Proceedings of the 14th \nWorkshop. \n[10] Zhang W E, Sheng Q Z, Alhazmi A, Li C (2020) Adversarial Attacks on Deep-learning Models in Natural Language \nProcessing: A Survey. ACM Trans. Intell. Syst. Technol. 11, 3, Article 24, 41 pages. https://doi.org/10.1145/3374217.  \n[11] Pruthi D, Dhingra B, Lipton Z C (2019) Combating Adversarial Misspellings with Robust Word Recognition. Proceedings \nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 5582–5591, Florence, Italy. Association for \nComputational Linguistics.  \n[12] Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q (2019) ERNIE: Enhanced Language Representation with Informative \nEntities. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1441–1451, Florence, \nItaly. Association for Computational Linguistics. \n[13] Kantor Y, Katz Y, Choshen L, Cohen-Karlik E, Liberman N, Toledo A, Menczel A, Slonim N (2019) Learning to combine \nGrammatical Error Corrections. Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational \nApplications, pages 139–148, Florence, Italy. Association for Computational Linguistics. \n[14] Jayanthi S M, Pruthi D, Neubig G (2020) Neuspell: A neural spelling correction toolkit. Proc. Conf. on Empirical Methods \nin Natural Language Processing: System Demonstrations (EMNLP) - 2020, pp.158–164. \n[15] Liang Z, Youssef A (2020) Performance Benchmarking of Automated Sentence Denoising using Deep Learning. IEEE \nInternational Conference on Big Data (Big Data), Atlanta, GA, USA, 2020, pp. 2779-2784, doi: \n10.1109/BigData50022.2020.9377985.  \n[16] Hangaragi S, Pati P B, Neelima N (2022) Accuracy Comparison of Neural Models for Spelling Correction in Handwriting \nOCR Data. 4th International Conference on Communication, Computing and Electronics Systems (ICCCES), doi:10.1007/978-\n981-19-7753-4_18.  \n[17] IAM Dataset: fki.tic.heia-fr.ch/databases/iam-handwriting-databasehttps://fki.tic.heia-fr.ch/databases/iam-handwriting-\ndatabase. (2019) \n[18] Saluja R, Adiga D, Chaudhuri P, Ramakrishnan G, Carman M (2017) Error Detection and Corrections in Indic OCR Using \nLSTMs. 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), Kyoto, Japan, 2017, pp. 17-22, \ndoi: 10.1109/ICDAR.2017.13.  \n[19] Tan M, Chen D, Li Z, Wang P (2020) Spelling Error Correction with BERT based on Character-Phonetic. IEEE 6th \nInternational Conference on Computer and Communications (ICCC), China, 2020, pp. 1146-1150, doi: \n10.1109/ICCC51575.2020.9345276. \n[20] Huang L, Li J, Jiang W, Zhang Z, Chen M, Wang S, Xiao J (2021) PHMOSpell Phonological and morphological knowledge \nguided Chinese spelling check. ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and \nthe 11th Int. Joint Conf. on Natural Language Processing, Proceedings of the Conference. \n[21] Zhang R, Pang C, Zhang C, Wang S, He Z, Sun Y, Wu H, Wang H (2021) Correcting Chinese Spelling Errors with Phonetic \nPre-training. Findings of the Association for Computational Linguistics: ACL-IJCNLP, pages 2250–2261, Online. Association \nfor Computational Linguistics.  \n[22] Xu H D, Li Z, Zhou Q, Li C, Wang Z, Cao Y, Huang H, Mao X L (2021) Read, Listen, and See: Leveraging Multimodal \nInformation Helps Chinese Spell Checking. Findings of the Association for Computational Linguistics: ACL-IJCNLP, pages \n716–728, Online. Association for Computational Linguistics.  \n[23] Fu K, Huang J, Duan Y (2018) Youdaos Winning Solution to the NLPCC Task Challenge: A Neural Machine Translation \nApproach to Chinese Grammatical Error Correction. Natural Language Processing and Chinese Computing. NLPCC. Lecture \nNotes in Computer Science, vol 11108. Springer, Cham. https://doi.org/10.1007/978-3-319-99495-6_29.  \n[24] Zhao W, Wang L, Shen K, Jia R, Liu J (2019) Improving Grammatical Error Correction via Pre-Training a Copy-\nAugmented Architecture with Unlabeled Data. Proceedings of the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 156–165, \nMinneapolis, Minnesota. Association for Computational Linguistics.  \n[25] Mounika Y, Tarakaram Y, Prasanna Y L, Gupta D, Pati P B (2022) Automatic Correction of Speech Recognized \nMathematical Equations using Encoder-Decoder Attention Mode. India Council International Conference (INDICON, IEEE \nComputer Society.  \n[26] Ali A, Renals S (2018) Word Error Rate Estimation for Speech Recognition: e-WER. Proceedings of the 56th Annual \nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 20–24, Melbourne, Australia. \n[27] Bryant C, Briscoe T (2018) Language Model Based Grammatical Error Correction without Annotated Training Data. \nProceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 247–253, New \nOrleans, Louisiana. Association for Computational Linguistics.  \n[28] Sreevidhya V, Narayanan J (2021) Short descriptive answer evaluation using word-embedding techniques. 12th Int. \nConference on Computing Communication and Networking Technologies (ICCCNT), pp. 1-4.  \n[29] Ahmed H, Hoque M, Alam M D M (2018) Nature of errors and mistakes in the English writings of graduating students in \nBangladesh: A case study. IIUC Studies. 15. 11-22. 10.3329/iiucs.v15i0.49341.  \n[30] C4 Data: www.tensorflow.org/datasets/catalog/c4. (2019) \n[31] Sutskever I, Vinyals O, Le Q V (2014) Sequence to sequence learning with neural networks. Proceedings of the 27th \nInternational Conference on Neural Information Processing Systems - Volume 2 (NIPS'14). MIT Press, Cambridge, MA, USA, \n3104–3112. \n[32] Cho K, Merriënboer B V, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y (2014) Learning Phrase \nRepresentations using RNN Encoder–Decoder for Statistical Machine Translation. Proceedings of the 2014 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar. Association for Computational \nLinguistics.  \n[33] Shi T, Keneshloo Y, Ramakrishnan N, Reddy C K (2021) Neural Abstractive Text Summarization with Sequence-to-\nSequence Models. ACM/IMS Trans. Data Sci. 2, 1, Article 1 (February 2021), 37 pages. https://doi.org/10.1145/3419106.  \n[34] Malla A, Jafar M A, Ghneim N (2022) Image captioning model using attention and object features to mimic human image \nunderstanding. J Big Data 9, 20. https://doi.org/10.1186/s40537-022-00571-w.  \n[35] Ali A, Amin M (2019) Conversational AI Chatbot Based on Encoder-Decoder Architectures with Attention Mechanism. \nArtificial Intelligence Festival 2.0, NED University of Engineering and Technology, Karachi, Pakistan \n10.13140/RG.2.2.12710.27204.  \n[36] Dehghani M, Rothe S, Alfonseca E, Fleury P (2017) Learning to Attend, Copy, and Generate for Session-Based Query \nSuggestion. Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM '17). Association \nfor Computing Machinery, New York, NY, USA, 1747–1756. https://doi.org/10.1145/3132847.3133010.  \n[37] Subramaniam L V, Roy S, Faruquie T A, Negi S (2009) A survey of types of text noise and techniques to handle noisy text. \nACM International Conference Proceeding Series. 115-122. 10.1145/1568296.1568315.  \n[38] Transformers: simpletransformers.ai/docs/usage/. (2021) \n[39] Junczys-Dowmunt M, Grundkiewicz R, Dwojak T, Hoang H, Heafield K, Neckermann T, Seide F, Germann U, Aji A F, \nBogoychev N, Martins A F T, Birch A (2018) Marian: Fast Neural Machine Translation in C++. Proceedings of ACL 2018, \nSystem Demonstrations, pages 116–121, Melbourne, Australia.  \n[40] Rohit R, Gandheesh S A, Suriya K S, Pati P B (2023) System for Enhancing Accuracy of Noisy Text using Deep Network \nLanguage Models. The 2023 International Conference for Convergence in Technology (I2CT), Bombay, India. \n[41] Yamuna K, Shriamrut V, Singh D, Gopalasamy V and Menon N (2021) Bert-based Braille Summarization of Long \nDocuments. 12th International Conference on Computing Communication and Networking Technologies (ICCCNT), pp. 1-6, \ndoi: 10.1109/ICCCNT51525.2021.9579748. \n[42] Ayush Kumar C S, Maharana A, Murali S, Premjith B, Soman KP (2022) BERT-Based Sequence Labelling Approach for \nDependency Parsing in Tamil. Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian \nLanguages, (Dublin, Ireland, 1-8), DOI: 10.18653/v1/2022.dravidianlangtech-1.1 \n[43] Venugopalan M and Gupta D (2022) An enhanced guided LDA model augmented with BERT based semantic strength for \naspect term extraction in sentiment analysis. Knowledge-Based Systems, 246, 108668, DOI: 10.1016/j.knosys.2022.108668 ",
  "topic": "Spelling",
  "concepts": [
    {
      "name": "Spelling",
      "score": 0.9003258943557739
    },
    {
      "name": "Computer science",
      "score": 0.7041255831718445
    },
    {
      "name": "Transformer",
      "score": 0.6611844897270203
    },
    {
      "name": "Natural language processing",
      "score": 0.6172963380813599
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4919126033782959
    },
    {
      "name": "Error detection and correction",
      "score": 0.46929436922073364
    },
    {
      "name": "Linguistics",
      "score": 0.38808590173721313
    },
    {
      "name": "Algorithm",
      "score": 0.08354240655899048
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I81556334",
      "name": "Amrita Vishwa Vidyapeetham",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    }
  ]
}