{
    "title": "The Devil in Linear Transformer",
    "url": "https://openalex.org/W4385573415",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2096947334",
            "name": "Zhen Qin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100530648",
            "name": "Xiaodong Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116596018",
            "name": "Weixuan Sun",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2096889750",
            "name": "Dongxu Li",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2154593323",
            "name": "Lingpeng Kong",
            "affiliations": [
                "Hong Kong Metropolitan University",
                "University of Hong Kong",
                "Shanghai Artificial Intelligence Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2100194327",
            "name": "Nick Barnes",
            "affiliations": [
                "Australian National University"
            ]
        },
        {
            "id": "https://openalex.org/A2274110869",
            "name": "Yiran Zhong",
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225525539",
        "https://openalex.org/W4313123347",
        "https://openalex.org/W3123615524",
        "https://openalex.org/W2526800167",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W4221151441",
        "https://openalex.org/W4223530498",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4301581299",
        "https://openalex.org/W4221153789",
        "https://openalex.org/W4380520356",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W4309793872",
        "https://openalex.org/W4226261765",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W4296197038",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W3006983028",
        "https://openalex.org/W3181262653",
        "https://openalex.org/W4206662200",
        "https://openalex.org/W4288804157",
        "https://openalex.org/W2606101940",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W2981040094"
    ],
    "abstract": "Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nThe Devil in Linear Transformer\n⋆Zhen Qin1, ⋆Xiaodong Han1, Weixuan Sun2,3, Dongxu Li2,\nLingpeng Kong4,5, Nick Barnes2, \u0000 Yiran Zhong4\n1SenseTime Research, 2Australian National University,\n3OPPO Research Institute, 4Shanghai AI Laboratory, 5The University of Hong Kong\nhttps://github.com/OpenNLPLab/Transnormer\nAbstract\nLinear transformers aim to reduce the quadratic\nspace-time complexity of vanilla transformers.\nHowever, they usually suffer from degraded\nperformances on various tasks and corpora. In\nthis paper, we examine existing kernel-based\nlinear transformers and identify two key is-\nsues that lead to such performance gaps: 1)\nunbounded gradients in the attention computa-\ntion adversely impact the convergence of lin-\near transformer models; 2) attention dilution\nwhich trivially distributes attention scores over\nlong sequences while neglecting neighbouring\nstructures. To address these issues, we first\nidentify that the scaling of attention matrices is\nthe devil in unbounded gradients, which turns\nout unnecessary in linear attention as we show\ntheoretically and empirically. To this end, we\npropose a new linear attention that replaces\nthe scaling operation with a normalization to\nstabilize gradients. For the issue of attention\ndilution, we leverage a diagonal attention to\nconfine attention to only neighbouring tokens\nin early layers. Benefiting from the stable gra-\ndients and improved attention, our new linear\ntransformer model, TRANS NORMER , demon-\nstrates superior performance on text classifica-\ntion and language modeling tasks, as well as on\nthe challenging Long-Range Arena benchmark,\nsurpassing vanilla transformer and existing lin-\near variants by a clear margin while being sig-\nnificantly more space-time efficient. The code\nis available at TRANS NORMER .\n1 Introduction\nTransformer models show great performance on\na wide range of natural language processing and\ncomputer vision tasks (Qin et al., 2022; Sun et al.,\n2022b; Cheng et al., 2022a,b; Zhou et al., 2022).\nOne issue of the vanilla transformer model lies in\nits quadratic space-time complexity with respect\n⋆Equal contribution. \u0000 The corresponding author (Email:\nzhongyiran@gmail.com). This work was done when Weixuan\nSun and Yiran Zhong were in the SenseTime Research.\n2 4 6 8 10 12 14 16 18\nSpeed (steps per sec)\n54\n56\n58\n60\n62\n64Long-Range Arena Score\nTransformer\nFLASH_quad\nFLASH\nLS\nPerformer\ncosFormer\nLinformer\nNystorm\nReformer\nTransNormer T1\nTransNormer T2\na\nFigure 1: TRANS NORMER has smaller memory foot-\nprints (circle sizes) and produces clearly favorable speed\n(x-axis) and overall scores (y-axis), when evaluated on\nthe challenging Long-Range Arena benchmark than the\nvanilla transformer and other competing methods.\nto the input length. Various prior works attempt\nto alleviate this inefficiency (Zaheer et al., 2020;\nBeltagy et al., 2020; Tay et al., 2020a; Kitaev et al.,\n2020; Child et al., 2019; Liu et al., 2022; Sun et al.,\n2022b). In this work, we focus on a particular sub-\nset of these methods, known as kernel-based lin-\near transformers (Choromanski et al., 2020; Wang\net al., 2020; Katharopoulos et al., 2020; Peng et al.,\n2020; Qin et al., 2022) considering their desirable\nlinear space-time complexity.\nDespite their space-time efficiency, linear trans-\nformers are not always in favor for practical adop-\ntion, largely due to the degraded performance than\nthe vanilla model. To address this issue, we take\na close look at existing kernel-based linear trans-\nformers and identify two deficiencies that lead to\nsuch a performance gap.\nUnbounded gradients. Most existing linear trans-\nformers inherit attention formulation from the\nvanilla transformer, which scales attention scores\nto ensure they are bounded within [0,1]. However,\nwe theoretically show that such a scaling strategy\nrenders unbounded gradients for linear transformer\nmodels. As a result, the unbounded gradients em-\n7025\npirically lead to unstable convergence as our pre-\nliminary experiments suggest.\nAttention dilution. Previous works (Titsias, 2016;\nJang et al., 2016; Gao and Pavel, 2017; Qin et al.,\n2022; Sun et al., 2022b,a) suggest that in vanilla\ntransformer, softmax attention maps tend to be lo-\ncal. In contrast, as shown in Fig 2, we observe that\nlinear transformers often trivially distribute atten-\ntion scores over the entire sequence even in early\nlayers. Due to this issue, which we refer as atten-\ntion dilution, important local information is less\nwell preserved in linear models, resulting in infe-\nrior performance. This negative impact of attention\ndilution is also evidenced by the performance drop\nin our controlled experiments if partly replacing\nvanilla attention in transformer layers with linear\nattention ones.\nTo mitigate these issues, we propose a linear\ntransformer model, called TRANS NORMER , which\nshows better performance than vanilla transformer\non a wide range of task while being significantly\nfaster during runtime, as shown in Fig. 1.\nTo avoid the unbounded gradients, we introduce\nNORM ATTENTION , which gets rid of scaling over\nattention matrices while appending an additional\nnormalization only after the attention layer. The\nchoice of the normalization operator is unrestricted,\nfor example, LayerNorm (Ba et al., 2016) or RM-\nSNorm (Zhang and Sennrich, 2019) both serve the\npurpose. We show empirical results demonstrat-\ning that with NORM ATTENTION , the gradients are\nmore stable during training, which in turn leads to\nmore consistent convergence.\nTo alleviate the attention dilution issue, we mod-\nify the vanilla attention and allow each token to\nonly attend to its neighbouring tokens, resulting\nin a diagonal attention. To mimic the behaviors\non local semantics of the vanilla transformer, we\nemploy the diagonal attention on early layers while\nusing NORM ATTENTION for later ones. In this\nway, we encourage the model to capture both local\nand global language context. Note that our diag-\nonal attention can be efficiently computed such\nthat the overall linear space-time complexity of\nTRANS NORMER is preserved.\nWe perform extensive experiments on standard\ntasks, where TRANS NORMER demonstrates lower\nlanguage modeling perplexities on WikiText-103\nand overall higher text classification accuracy on\nGLUE than vanilla model and other competing\nmethods. In addition, on the challenging Long-\nRange Arena benchmark, TRANS NORMER also\nshows favorable results while beingfaster and more\nscalable with longer inputs during both training\nand inference time.\n2 Background and related work\nWe first briefly review vanilla transformer (Vaswani\net al., 2017) and its efficient variants. The key com-\nponent of transformers is the self-attention, which\noperates on query Q, key K and value V matrices;\neach of them is the image of a linear projection\ntaking X ∈Rn×d as input:\nQ = XWQ,K = XWK,V = XWV ∈Rn×d, (1)\nwith nthe input length, dthe hidden dimension.\nThe output O ∈Rn×d is formulated as:\nO = Softmax(QKT/\n√\nd)V, (2)\nwhere the Softmax(·) step renders quadratic space-\ntime complexity with respect to the input length,\nmaking it prohibitive for vanilla transformer to\nscale to long input sequences. To address this\nissue, numerous efficient transformers have been\nexplored in the literature. These methods can be\ngenerally categorized into two families,i.e., pattern\nbased methods and kernel based methods.\nPattern based methods (Zaheer et al., 2020; Belt-\nagy et al., 2020; Tay et al., 2020a; Kitaev et al.,\n2020; Child et al., 2019) sparsify the attention cal-\nculation with handcrafted or learnable masking pat-\nterns. Kernel-based methods adopt kernel functions\nto decompose softmax attention, which reduces the\ntheoretical space-time complexity to linear. In this\npaper, we refer the kernel-based variants as linear\ntransformers for simplicity.\nIn the kernel-based methods (Choromanski et al.,\n2020; Katharopoulos et al., 2020; Peng et al., 2020;\nQin et al., 2022; Zheng et al., 2022; Wang et al.,\n2020), a kernel functionϕ(·) maps queries and keys\nto their hidden representations. Then the output of\nthe linear attention can be rewritten as:\nO = ∆−1ϕ(Q)[ϕ(K)TV],\n∆ = diag(ϕ(Q)[ϕ(K)T1n]).\n(3)\nwhere the product of keys and values are com-\nputed to avoid the quadratic n×n matrix. Ex-\nisting methods mainly differ in the design of kernel\nfunctions. For example, Choromanski et al. (2020)\nand Katharopoulos et al. (2020) adopt activation\nfunction 1 +eluto process query and key. Wang\n7026\net al. (2020) assumes attention matrices are low-\nrank. Peng et al. (2020) and Zheng et al. (2022)\napproximate softmax under constrained theoretical\nbounds. Qin et al. (2022) propose a linear alterna-\ntive to the attention based on empirical properties\nof the softmax function.\nThese methods focus on either approximating\nor altering the softmax operator while preserving\nits properties. Compared with the vanilla trans-\nformer, these methods often trade performance for\nefficiency, usually resulting in worse task perfor-\nmance. In this paper, we argue that there are two\nessential reasons leading to such a performance\ngap, discussed in detail as follows.\n3 The devil in linear attention\nIn this section, we motivate the design principles of\nTRANS NORMER by providing theoretical evidence\nfor the unbounded gradients, and empirical results\nshowing the adverse influence of attention dilution.\n3.1 Unbounded gradients\nFew work on linear transformers analyzes their gra-\ndients during training. Our first key observation is\nthat kernel-based linear attention suffer from un-\nbounded gradients, causing unstable convergence\nduring training. In the following, we highlight the\nmain theoretical results while referring readers to\nAppendix D for the full derivation.\nConsider a self-attention module, either vanilla\nor linear attention. Its attention matrix P ∈Rn×n\ncan be represented in the following unified form1:\npij = f(sij)∑n\nk=1 f(sik),f : R →R. (4)\nVanilla and linear attention differ mainly in their\ncomputation of token-wise similarities sij2. In\nvanilla attention, sij is computed as:\nsij = qT\ni kj/\n√\nd,f(x) = exp(x), (5)\nwhile for linear attentions, sij can be decomposed\nusing a kernel function ϕ, such that:\nsij = ϕ(qi)Tϕ(kj),f(x) =x. (6)\nGiven the above definitions, the gradients of the\nattention matrix P is derived as:\n∂pij\n∂sik\n= f′(sik)\nf(sik) (1j=kpij −pijpik) (7)\n1 Here we assume that f(sij) ≥0, the conclusion is satisfied\nin most cases.\n2 Note that sij is not directly computed in linear attention, but\ncan still be represented in this unified form, see Appendix D\nfor more detailed derivation\nTherefore, for the vanilla attention, the partial\nderivative ∂pij\n∂sik\nis:\nf′(x) = exp(x) =f(x)\n∂pij\n∂sik\n= 1j=kpij −pijpik\n=\n{\npik −pikpik ∈[0,1/4] j = k\n−pijpik ∈[−1/4,0] j ̸= k\n(8)\nand it is bounded as:\n⏐⏐⏐⏐\n∂pij\n∂sik\n⏐⏐⏐⏐≤1\n4. (9)\nHowever, for linear attentions, we have:\nf′(x) = 1\n∂pij\n∂sik\n= 1\nsik\n(1j=kpij −pijpik)\n=\n{\n1\nsik\n(pik −pikpik) j = k\n1\nsik\n(−pijpik) j ̸= k\n(10)\nand3 ⏐⏐⏐⏐\n∂pij\n∂sik\n⏐⏐⏐⏐≤ 1\n4|sik|. (11)\nSince |sik|−1 = |ϕ(qi)ϕ(kj)T|−1 can be arbitrar-\nily large, the gradient of linear attention has no\nupper bound. On the other hand, we can also show\nthat the gradient of linear attention has no lower\nbound4:\nProposition 3.1. ∀M >0, there exists qi,kj ∈\nRd,j = 1,...,n , such that:\n⏐⏐⏐⏐\n∂pij\n∂sik\n⏐⏐⏐⏐>M. (12)\nThe unbounded gradients lead to less stable op-\ntimization and worse convergence results in our\npreliminary studies.\n3.2 Attention dilution\nIt is a known property of vanilla attention to em-\nphasize on neighbouring tokens (Titsias, 2016;\nQin et al., 2022). However, this property does not\ndirectly inherit to the linear transformer variants.\nTo quantify the attention dilution issue, we intro-\nduce a metric called locally accumulated attention\nscore, which measures how much attention scores\nare distributed within the local neighbourhood of a\nparticular token.\nFor an input sequence of length N, consider\na local neighbourhood {xstart,...,x i...,xend}cen-\ntering around token xi of total length r·N, with r\n3 A detailed proof of the upper bound can be found at Ap-\npendix B.\n4 The proof can be found in Appendix C.\n7027\nVanillaLinearTransNormer\n(a) (b)\nFigure 2: (a): Comparison of locally accumulated attention scores of different transformer variants. The x-axis\ndenotes ratio of neighbourhood size relative to the input length; the y-axis denotes accumulated attention scores\ninside this neighbourhood for the centering token. The curve for the vanilla transformer model increases more\nsharply, indicating that the attention scores are more concentrated. Our model greatly alleviates the attention dilution\nissue for linear models. (b): Qualitative comparison of attention matrices in early model layers. The proposed\nTRANS NORMER produces more similar patterns to the original vanilla transformer, benefiting to better capture\nlocal-global language context, while the linear model suffers clearly from the issue of attention dilution and gets\ndistracted by distant tokens in early layers.\nthe ratio relative to the total input, the locally accu-\nmulated attention score for token xi is defined as\nl(i,r,N ) =pi,start + ...+ pi,end. A higher score\nindicates the particular attention layer concentrates\non the local neighbourhood, while a lower score\ntends to indicate the issue of attention dilution,\nwhere scores are distributed more evenly to local\nand distant tokens. For example, l(i,0.4,N) = 0.6\nmeans that that 40% of the neighbors around i’th\ntoken contribute 60% of the attention score.\nIn Fig. 2 (a), we compare locally accumulated\nattention scores (y-axis) for vanilla transformer and\nlinear transformer, with varying sizes of neighbour-\nhood by ratio (x-axis). We show the average score\nover each position across the entire sequence. It\ncan be seen that the area under the vanilla model\ncurve is significantly larger than that of the lin-\near model. This provides evidence that the vanilla\nattention is more concentrated locally, while the\nlinear transformer suffers from the issue of atten-\ntion dilution. This is further qualitatively supported\nby Fig. 2 (b), where the attention maps for vanilla\nmodel are more concentrated than the linear model.\n4 Method\nBased on the aforementioned observations, we\npropose a new linear transformer network called\nTRANS NORMER that addresses the above two lim-\nitations of current linear transformers. The overall\narchitecture is shown in Fig. 3.\n4.1 The overall architecture\nVanilla attention suffers less in attention dilution\nwhile linear attention is more efficient and scalable\non longer sequences. This motivate us to design a\nmethod that exploits the best of the both worlds by\nusing these mechanisms in combined.\nSpecifically, our network consists of two types\nof attention: DIAG ATTENTION for the early stage\nof the model and NORM ATTENTION for the later\nstage. The former addresses the attention dilution\nissue and the later aims to stabilize training gradi-\nents. Note that by properly reshaping the inputs,\nthe diagonal attention can be efficiently computed\nin linear space-time, thus preserving the overall\nlinear complexity.\n4.2 N ORM ATTENTION\nTable 1: Ablation of linear attention with scaling\noperation. Directly removing scaling operation i.e., the\ndenominator in Eq. 4, leads to significant performance\ndrop. Our normalization strategy achieves better result.\nmethod ppl(val)\n1 +elu 4.98\n1 +eluw/o scaling 797.08\nNORM ATTENTION 4.94\nAs proved in Sec. 3, the scaling operation, i.e.,\nthe denominator in Eq. 4, in the linear transformers\nhinder the optimization due to the unbounded gradi-\nents. To solve this issue, we propose to remove the\nscaling operation in the linear transformers. How-\never, as shown in Table. 1, directly removing the\nscaling operation leads to critical performance drop\nsince the attention map becomes unbounded in the\nforward pass. Therefore, an alternative is required\nto bound both attention maps during forward and\ntheir gradients during backward passes in linear\n7028\nattentions.\nOur proposed solution is simple yet effective.\nGiven a linear attention, the attention without scal-\ning can be formulated as:\nO = Q(KTV). (13)\nWe empirically find that we can apply an arbitrary\nnormalization on this attention to bound it, which\nleads to our NORM ATTENTION as:\nOnorm = XNorm(Q(KTV)), (14)\nwhere the XNorm can be Layernorm(Ba et al.,\n2016) or RMSNorm (Zhang and Sennrich, 2019)\nand etc. We use the RMSNorm in our experiments\nas it is slightly faster than other options.\nIt can be proved that the gradients of NORM AT-\nTENTION is bounded by5:\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐≤3c1c2d\n2√ϵ <∞, (15)\nwhere Lis the loss function, ϵ is the small con-\nstant that used in RMSNorm, dis the embedding\ndimension and\nc1 =\nn\nmax\ni=1\n∥∇Oi L∥2 <∞\nc2 =\nn\nmax\ni=1\n∥Vi∥2 <∞\n(16)\nTo demonstrate the gradients stability of the\nNORM ATTENTION , we compare the relative stan-\ndard deviation of gradients during each training\niterations to other linear transformers and vanilla\ntransformer. Specifically, we train our model for\n50k iterations with RoBERTa architecture on the\nWikiText103 (Merity et al., 2017) and obtain the\nrelative standard deviation of all iterations’ gradi-\nents. As shown in Table 2, existing linear methods\n(Choromanski et al., 2020; Katharopoulos et al.,\n2020) have substantially higher deviations com-\npared to vanilla attention, which leads to inferior\nresults. The NORM ATTENTION produces more sta-\nble gradients, which validates the effectiveness of\nour method.\n4.3 D IAG ATTENTION\nTo better understand the design principles, we show\nin Table 3 that by replacing partial layers of lin-\near transformers with vanilla attention, the perfor-\nmance on language modeling is evidently improved.\nThe results also suggest that capturing more local\ninformation in early layers are more helpful than\notherwise.\n5 The full derivation can be found in Appendix D.\nTable 2: Relative standard deviation of training gradi-\nents over 50k iterations. Our proposed NORM ATTEN -\nTION provides more stable gradients which are closer to\nvanilla transformer.\nmethod Relative Standard\nDeviation\n1 +elu(Katharopoulos et al., 2020) 0.58\nPerformer(Choromanski et al., 2020) 0.47\nVanilla(Vaswani et al., 2017) 0.25\nNORM ATTENTION 0.20\nFigure 3: Architecture overview of the proposed\nTRANS NORMER . In the early stages, we leverage DIA-\nGATTENTION , where attention is only calculated inside\nthe blocks to enforce neighbouring focus. In late stages,\nNORM ATTENTION assists to obtain a more stable gra-\ndients in linear complexity.\nTo this end, we leverage none-overlapped block-\nbased strategy to reduce the space-time complexity\nof the vanilla attention. Based on the observation in\nFig. 2, we utilize a strict diagonal blocked pattern\nto constraint the attention in a certain range. Since\nthe attentions are calculated inside each block, the\ncomputation complexity of our diagonal attention\nis O(nwd), where n is sequence length , w is\nthe block size and dis feature dimension. When\nd≪n, the complexity scales linearly respect to the\nsequence length n. In subsequent sections, we use\nDIAG ATTENTION to refer to Diagonal attention.\nWe empirically find that applying DIAG ATTEN -\nTION to the later stages of a model hurts the perfor-\nmance as shown in Table. 9. It indicates that the\n7029\nTable 3: Ablation on attention dilution issue. We\nimplement all structures under the same setting: Vanilla\n(Vaswani et al., 2017), 1 +elu (Katharopoulos et al.,\n2020).\nEarly layers Later layers ppl (val)\n1 +elu 1 +elu 4.98\n1 +elu Vanilla 3.90\nVanilla 1 +elu 3.76\nmodel requires a global field of view in the later\nlayers, which also justifies our choices of NOR-\nMATTENTION in later layers of TRANS NORMER .\n5 Experiments\nIn this section, we compare our method to other lin-\near transformers and the vanilla transformer on au-\ntoregressive language modeling, bidirectional lan-\nguage modeling as well as the Long Range Arena\nbenchmark (Tay et al., 2020b). We also provide an\nextensive ablation study to vindicate our choice in\ndesigning the TRANS NORMER .\nWe validate our method on two variants of the\nTRANS NORMER . The TRANS NORMER T1 uses\nthe ReLA attention (Zhang et al., 2021) in the DIA-\nGATTENTION and the elu as the activation function\nin the NORM ATTENTION . The TRANS NORMER\nT2 uses the Softmax attention (Vaswani et al.,\n2017) in the DIAG ATTENTION and the 1+elu as\nthe activation function in the NORM ATTENTION .\nFor experiments, we first study the autoregres-\nsive language modeling on WikiText-103 (Merity\net al., 2017) in section 5.2. Then in section 5.2 we\ntest our method on bidirectional language model-\ning, which is pre-trained on WikiText-103 (Merity\net al., 2017) and then fine-tuned on several down-\nstream tasks from the GLUE benchmark (Wang\net al., 2018). Finally, we test TRANS NORMER\non the Long-Range Arena benchmark (Tay et al.,\n2020b) to evaluate its ability in modeling long-\nrange dependencies and efficiencies in section 5.2.\n5.1 Settings\nWe implement our models in the Fairseq frame-\nwork (Ott et al., 2019) and train them on 8 V100\nGPUS. We use the same training configuration\nfor all competitors and we list detailed hyper-\nparameters in Appendix F. We choose the FLASH-\nquad, FLASH (Hua et al., 2022), Transformer-LS\n(Zhu et al., 2021), Performer (Choromanski et al.,\n2020), 1+elu (Katharopoulos et al., 2020) as our\nmain competing methods.\nFor the autoregressive language modeling,\nwe use 6 decoder layers (10 layers for the\nFlASH/FLASH-quad) as our base model and all\nmodels are trained on the WikiText-103 dataset\n(Merity et al., 2017) for 100K steps with a learning\nrate of 0.005. We use the perplexity (PPL) as the\nevaluation metric.\nFor the bidirectional language modeling, we\nchoose the RoBERTa base (Liu et al., 2019) for\nall methods. It consists of 12 encoder layers (24\nlayers for the FLASH and FLASH-quad to match\nthe number of parameters). All models are pre-\ntrained on the WikiText-103 (Merity et al., 2017)\nfor 50K steps with lr=0.005 and fine-tuned on the\nGLUE dataset (Wang et al., 2018). We use dif-\nferent learning rates among 1e-5, 3e-5, 6e-5, 1e-4\nand choosing the best result after fine-tuning for 3\nepochs.\nFor the Long-Range Arena benchmark, to make\nsure it reflect the practical speed in Pytorch plat-\nform, we re-implement the benchmark in Pytorch.\nWe adopt the same configuration from the Sky-\nformer (Chen et al., 2021) and make sure all mod-\nels have a similar parameter size. We use the same\ntraining hyper parameters for all models as well.\nTable 4: Quantitative results in autoregressive lan-\nguage modeling. The best result is highlighted with\nbold and the second with underlined. The smaller the\nbetter for the PPL metric. LS stands for transformer-LS.\nMethod PPL (val) PPL (test) Params (m)\nVanilla 29.63 31.01 156.00\nLS 32.37 32.59 159.46\nFLASH-quad 31.88 33.50 153.51\nFLASH 33.18 34.63 153.52\n1+elu 32.63 34.25 156.00\nPerformer 75.29 77.65 156.00\nTRANS NORMER T1 29.89 31.35 155.99\nTRANS NORMER T2 29.57 31.01 155.99\n5.2 Results\nAutoregressive language modeling We report\nthe results in Table 4. It can be found that both\nTRANS NORMER variants get comparable or better\nperplexity to the vanilla attention and outperform\nall existing linear models with a clear margin. For\nexample, compared to previous state-of-the-art lin-\near methods on validation set(Hua et al., 2022)\nand test set(Zhu et al., 2021), TRANS NORMER T2\nachieves substantially lower perplexity by 2.31 and\n1.58 respectively. It demonstrates the effectiveness\nof our method in causal models.\n7030\nTable 5: Quantitative results of the GLUE benchmark. MNLI is reported by the match/mismatch splits. MRPC\nis reported by F1 score. CoLA is reported by Matthews correlation coefficient. All the other tasks are measured by\nthe accuracy. LS stands for transformer-LS. The best result is highlighted with bold and the second with underlined.\nThe larger the better for all metrics. \"-\" means unconverged.\nMethod MNLI QNLI QQP SST-2 MRPC CoLA A VG Params (m)\nVanilla 79.37/79.07 87.79 88.04 90.25 88.35 38.63 78.79 124.70\nFLASH-quad 78.71/79.43 86.36 88.95 90.94 81.73 41.28 78.20 127.11\nFLASH 79.45/80.08 87.10 88.83 90.71 82.50 29.40 76.87 127.12\nLS 77.01/76.78 84.86 86.85 90.25 82.65 40.65 77.01 128.28\nPerformer 58.85/59.52 63.44 79.10 81.42 82.11 19.41 63.41 124.70\n1+elu 74.87/75.37 82.59 86.9 87.27 83.03 - 70.00 124.0\nTRANS NORMER T1 79.06/79.93 87.00 88.61 91.17 84.50 45.38 79.38 124.67\nTRANS NORMER T2 77.28/78.53 85.39 88.56 90.71 85.06 45.90 78.78 124.67\nTable 6: Quantitative results on the Long-Range Arena benchmark. The best result is highlighted with bold and\nthe second with underlined. The larger the better for all metrics.\nModel Text ListOps Retrieval Pathfinder Image A VG.\nTransformer 61.95 38.37 80.69 65.26 40.57 57.37\nKernelized Attention 60.22 38.78 81.77 70.73 41.29 58.56\nNystromformer 64.83 38.51 80.52 69.48 41.30 58.93\nLinformer 58.93 37.45 78.19 60.93 37.96 54.69\nInformer 62.64 32.53 77.57 57.83 38.10 53.73\nPerformer 64.19 38.02 80.04 66.30 41.43 58.00\nReformer 62.93 37.68 78.99 66.49 48.87 58.99\nBigBird 63.86 39.25 80.28 68.72 43.16 59.05\nSkyformer 64.70 38.69 82.06 70.73 40.77 59.39\nLS 66.62 40.30 81.68 69.98 47.60 61.24\ncosFormer 67.70 36.50 83.15 71.96 51.23 62.11\nFLASH-quad 64.10 42.20 83.00 63.28 48.30 60.18\nFLASH 64.10 38.70 86.10 70.25 47.40 61.31\nTRANS NORMER T1 66.90 41.03 83.11 75.92 51.60 63.71\nTRANS NORMER T2 72.20 41.60 83.82 76.80 49.60 64.80\nTable 7: Speed comparison on Long-Range Arena benchmark. We mark it with a dash if a method exhausts\nGPU memory. The higher the better for all metrics. The 1K,...,5K represent the input sequence length.\nInference Speed(steps per sec) Train Speed(steps per sec)\nmodel 1K 2K 3K 4K 5K 1K 2K 3K 4K 5K\nTransformer 39.06 10.05 - - - 15.34 3.05 - - -\nFLASH-quad 44.64 16.45 9.40 6.54 5.39 19.84 8.47 5.19 3.59 2.92\nFLASH 40.32 23.15 16.89 14.04 13.16 20.49 11.06 8.47 7.23 6.93\nLS 32.05 17.36 12.14 10.16 9.06 15.43 8.68 6.28 5.24 4.76\nPerformer 104.17 56.82 42.37 33.78 31.25 28.41 16.23 12.02 10.04 9.06\ncosFormer 86.21 46.30 32.47 27.47 25.00 22.94 12.82 9.19 7.79 7.14\nLinformer 104.17 58.14 40.32 31.25 26.32 27.17 15.63 11.26 8.77 7.42\nReformer 78.13 38.46 26.04 19.84 16.23 20.16 10.87 7.46 5.69 4.70\nNystorm 58.14 38.46 29.07 23.81 20.33 14.12 9.62 7.46 6.11 5.26\nTRANS NORMER T1 113.64 65.79 46.30 39.06 35.71 28.41 17.12 12.76 10.87 10.12\nTRANS NORMER T2 119.05 65.79 47.17 39.68 36.23 29.41 17.24 12.95 10.96 10.16\nBidirectional language modeling We show our\nbidirectional results on the GLUE benchmark in\nTable. 5. Our method achieves superior per-\nformance to all the competing methods in aver-\nage. On three tasks, i.e., SST-2, MRPC, CoLA,\nTRANS NORMER reports comprehensively better\nresults than all competing linear methods, such as\n4.62 higher on CoLA. Further, one of our variants\ni.e., TRANS NORMER T1, even outperforms the\nvanilla attention with a notable margin. It proves\nthe effectiveness of our method in bidirectional\nlanguage modeling.\nLong Range Arena Benchmark The results be-\nfore the transformer Long-short (abbr. LS) are\ntaken from the Skyformer (Chen et al., 2021). As\nshown in Table. 6, we achieve either first or sec-\nond places across all five tasks. In terms of over-\nall results, both TRANS NORMER variants (T1,T2)\noutperform all other competing methods including\nvanilla transformer (Vaswani et al., 2017), which\n7031\nvalidates our capability to encode long sequences.\n5.3 Speed comparison\nWe compare the training and inference speed of the\nTRANS NORMER with other methods. For a fair\nand comprehensive comparison, we follow exactly\nthe same configurations of the Skyformer(Chen\net al., 2021) and report step per second under dif-\nferent sequence lengths. Timing is conducted on a\nNvidia A6000 GPU with 48G GPU memory. Ta-\nble. 7 suggests that the vanilla transformer is sub-\nstantially slow and exhausts GPU memory with\nsequence longer than 3k. Compared to other effi-\ncient transformers, our TRANS NORMER achieves\nfaster speed with comparable GPU memory foot-\nprints, while competing efficient methods all report\nworse results compared to our TRANS NORMER .\nFor instance, compared to FLASH-quad (Hua et al.,\n2022) that achieves previous best linear results on\nboth autoregressive and bidirectional benchmarks,\nour model performs over 300% faster during train-\ning and 150% faster during inference.\n5.4 Ablation study\nIn this section, we justify our design choice of the\nTRANS NORMER , including , the selection of the\nFFN module, and the size of the attention block\nin DIAG ATTENTION . We use the PPL from the\nRoberta pre-training stage as our evaluation metric.\nTable 8: Ablation of the proportion of the attentions.\nWe empirically find that the balanced structure achieves\nthe best result. We abbreviate the DIAG ATTENTION as\nBlockAtt and NORM ATTENTION as NormAtt.\nEarly stage\nBlockAtt\nLater stage\nNormAtt T1 ppl(val) T2 ppl(val)\n0 12 4.23 4.48\n3 9 4.13 3.83\n6 6 3.82 3.81\n9 3 3.87 3.86\n12 0 4.75 4.66\nTable 9: Ablation of the order of two proposed at-\ntention. Using DIAG ATTENTION in the early stage\nachieves better results than using it on later stage.\nEarly stage Later stage T1 ppl(val) T2 ppl(val)\nNormAtt BlockAtt 4.13 4.21\nBlockAtt NormAtt 3.82 3.81\nStructure design As aforementioned, we empiri-\ncally choose the first 6 layers as the early stage of\nthe model and the rest as the later stage. We provide\nthe designing ground for this choice in Table. 8. It\ncan be also observed that either choosing the DIA-\nGATTENTION or NORM ATTENTION for the entire\nmodel will lead to inferior performance. We also\nprovide the ablation results of swapping the order\nof the DIAG ATTENTION and the NORM ATTEN -\nTION in Table. 9. Using DIAG ATTENTION in the\nearly stage achieves significantly better results than\nusing it on later stage. It further proves our claim\nthat the early stage focuses on neighbouring tokens\nwhile the later stage needs long-range attentions.\nTable 10: Ablation of the selection of the FFN mod-\nules. The GLU leads to better results.\nFFN type T1 ppl(val) T2 ppl(val)\nFFN 3.93 3.93\nGLU(ours) 3.82 3.81\nFFN module We ablate the selection of the\nFFN modules in Table. 10. Compared with\nthe traditional FFN (Vaswani et al., 2017), the\nGLU (Shazeer, 2020) achieves better results.\nTable 11: Ablation of on block sizes in the DIAG AT-\nTENTION . The larger block size the better results.\nBlock size T1 ppl(val) T2 ppl(val)\n32 3.92 3.90\n64 3.82 3.81\n128 3.72 3.69\nBlock size From the Table. 11, we observe clear\nperformance improvements with increased block\nsizes. However, since the complexity of the DIA-\nGATTENTION is O(nwd), larger block sizewleads\nto heavier computational overhead. We choose a\nblock size as 64 as a trade-off between performance\nand computational cost.\nCombination of attentions Finally, we study the\neffect that whether we should use both attentions\nin one layer. In particular, we compare either to\n1) use DIAG ATTENTION and NORM ATTENTION\nsequentially in a layer with different orders; or to\n2) use them in parallel in each attention layer and\nthen concatenate their embedding output. Table. 12\nshows that we should not use these attentions se-\nquentially within a layer and apply them in parallel\nwill double the computation complexities without\nimproving the performance.\n6 Conclusion\nIn this paper, we identified two key issues that\ncause the inferior performance of existing linear\ntransformer models: 1) unbounded gradients; 2)\n7032\nTable 12: Ablation of the combination of two pro-\nposed attention. In first two rows, the two attention\nlayers appear in an interleaved manner. D for the DIA-\nGATTENTION and N for the NORM ATTENTION .\napproach T1 ppl(val) T2 ppl(val)\naltering D→N 4.19 4.23\naltering N→D 4.11 4.21\nparallel 3.77 3.82\nTRANS NORMER 3.82 3.81\nattention dilution. For the former issue, we pro-\nposed a new NORM ATTENTION to stabilize the\ntraining gradients. For the latter, we develop DIA-\nGATTENTION to force the model concentrate atten-\ntion in neighbouring tokens. The resultant model\nTRANS NORMER marries the strength of the vanilla\ntransformers and the linear transformers, outper-\nforming competing linear transformers on both au-\ntoregressive and bidirectional language modeling,\ntext classification tasks and the challenging Long-\nrange arena benchmark.\nLimitations\nIn this paper, we identified two main issues of cur-\nrent linear transformers and provided a comprehen-\nsive analysis in natural language processing tasks.\nHowever, with the booming development of vision\ntransformers, whether they share the same issues\nof linear NLP transformers is yet to be discovered.\nWe will validate our method on the linear vision\ntransformers in our future work.\nEthics Statement\nThe proposed technique is beneficial to develop\nlarge-scale environment-friendly language models\nby reducing computing resource demand. Corpus\nused to train the model is from public web sources,\nwhich may contain biased, explicit or improper\ncontent. Further assessment and regulation have to\nbe in-place before deploying the model in practice.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nYifan Chen, Qi Zeng, Heng Ji, and Yun Yang. 2021.\nSkyformer: Remodel self-attention with gaussian\nkernel and nyström method. In Advances in Neural\nInformation Processing Systems 35: Annual Confer-\nence on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual.\nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran\nZhong, Mehrtash Harandi, Tom Drummond, and\nZongyuan Ge. 2022a. Implicit motion handling for\nvideo camouflaged object detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13864–13873.\nXuelian Cheng, Yiran Zhong, Mehrtash Harandi, Tom\nDrummond, Zhiyong Wang, and Zongyuan Ge.\n2022b. Deep laparoscopic stereo matching with\ntransformers. In International Conference on Medi-\ncal Image Computing and Computer-Assisted Inter-\nvention, pages 464–474. Springer.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention with\nperformers. arXiv preprint arXiv:2009.14794.\nBolin Gao and Lacra Pavel. 2017. On the properties\nof the softmax function with application in game\ntheory and reinforcement learning. arXiv preprint\narXiv:1704.00805.\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V\nLe. 2022. Transformer quality in linear time. arXiv\npreprint arXiv:2202.10447.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categori-\ncal reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156–5165. PMLR.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. arXiv\npreprint arXiv:2001.04451.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan\nSun, Jiacheng Xu, and Yiran Zhong. 2022. Neu-\nral architecture search on efficient transformers and\nbeyond. arXiv preprint arXiv:2207.13955.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. 5th International Conference on Learning Rep-\nresentations, ICLR, Toulon, France.\n7033\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for se-\nquence modeling. arXiv preprint arXiv:1904.01038.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2020.\nRandom feature attention. In International Confer-\nence on Learning Representations.\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yun-\nshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong,\nand Yiran Zhong. 2022. cosformer: Rethinking soft-\nmax in attention. In International Conference on\nLearning Representations.\nNoam Shazeer. 2020. Glu variants improve transformer.\narXiv preprint arXiv:2002.05202.\nJingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang\nLi, and Yiran Zhong. 2022a. Locality matters: A\nlocality-biased linear attention for automatic speech\nrecognition. arXiv preprint arXiv:2203.15609.\nWeixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang,\nYi Zhang, Kaihao Zhang, Nick Barnes, Stan\nBirchfield, Lingpeng Kong, and Yiran Zhong.\n2022b. Vicinity vision transformer. arXiv preprint\narXiv:2206.10552.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020a. Sparse sinkhorn attention.\nIn International Conference on Machine Learning,\npages 9438–9447. PMLR.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se-\nbastian Ruder, and Donald Metzler. 2020b. Long\nrange arena: A benchmark for efficient transformers.\nIn International Conference on Learning Representa-\ntions.\nMichalis K Titsias. 2016. One-vs-each approximation\nto softmax for scalable estimation of probabilities.\narXiv preprint arXiv:1609.07410.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. In NeurIPS.\nBiao Zhang and Rico Sennrich. 2019. Root Mean\nSquare Layer Normalization. In Advances in Neu-\nral Information Processing Systems 32, Vancouver,\nCanada.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2021.\nSparse attention with linear units. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 6507–6520, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nLin Zheng, Chong Wang, and Lingpeng Kong. 2022.\nLinear complexity randomized self-attention mecha-\nnism. arXiv preprint arXiv:2204.04667.\nJinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan\nSun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng\nKong, Meng Wang, and Yiran Zhong. 2022. Audio-\nvisual segmentation. In European Conference on\nComputer Vision.\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad\nShoeybi, Tom Goldstein, Anima Anandkumar, and\nBryan Catanzaro. 2021. Long-short transformer: Ef-\nficient transformers for language and vision. In Ad-\nvances in Neural Information Processing Systems.\nAppendix\nA Mathematical Notations\nWe use bold uppercase letters for matrices(M),\nbold lowercase letters for vectors(m), and low-\nercase letters for scalars(mij). We represent all\nvectors as column vectors and denote the ith\nrow of matrix M by m⊤\ni or Mi. We use ∥.∥2\nto denote the l2 norm and ∥.∥F to denote the\nFrobenius norm of the matrix and the vector.\nThe main mathematical symbols are input\nX ∈ Rn×d, Q (Query), K (Key) and V\n7034\n(Value), which has the following form:\nX =\n\n\nxT\n1\n...\nxT\nn\n\n∈Rn×d,\nQ =\n\n\nqT\n1\n...\nqT\nn\n\n= XWQ =\n\n\nxT\n1 WQ\n...\nxT\nnWQ\n\n∈Rn×d,\nK =\n\n\nkT\n1\n...\nkT\nn\n\n= XWK =\n\n\nxT\n1 WK\n...\nxT\nnWK\n\n∈Rn×d,\nV =\n\n\nvT\n1\n...\nvT\nn\n\n= XWV =\n\n\nxT\n1 WV\n...\nxT\nnWV\n\n∈Rn×d,\n(17)\nwhere WQ,WK,WV ∈Rd×d.\nB Proof of gradients’ upper bound\nIn this part, we will proof the bound in (8) and\n(10), all we need to prove is:\n0 ≤pik(1 −pik) ≤1\n4,0 ≤pijpik ≤1\n4. (18)\nWe adopt the theorem that geometric mean is\nbounded by arithmetic mean, i.e.,\n√\nab≤a+ b\n2 ⇐⇒ab≤\n(a+ b\n2\n)2\n,∀a,b ≥0.\n(19)\nWe take a= pik,b = 1−pik to complete the\nproof. The first bound can be proven by:\n0 ≤pik(1 −pik) ≤\n(pik + 1−pik\n2\n)2\n= 1\n4.\n(20)\nFor the second bound, we first use the fact that:\n0 ≤pij + pik ≤1 ⇒pij ≤1 −pik. (21)\nSo we have:\n0 ≤pijpik ≤(1 −pik)pik ≤1\n4. (22)\nC Proof of Proposition 3.1\nProof of Proposition 3.1. ∀ϵ > 0 and kernel\nfunction ϕ, let6:\nqi = kj = ϕ−1(x0),\n0 <∥x0∥2 ≤√ϵ,i,j = 1,...,n. (23)\n6 We assume that the image of ϕcontains vectors arbitrary\nclose to 0, which is a common case in kernel function.\nThen\nϕ(qi) =ϕ(kj) =x0,i,j = 1,...,n. (24)\nSo\nsij = ϕ(qi)Tϕ(kj) =x0\nTx0 ∈(0,ϵ], (25)\nand\npij = sij∑n\nk=1 sik\n= x0Tx0∑n\nk=1 x0Tx0\n= 1\nn. (26)\nAccording to (10), we have:\n∂pij\n∂sik\n=\n{\n1\nx0Tx0\n1\nn(1 −1\nn) j = k\n− 1\nx0Tx0\n1\nn2 j ̸= k,\n⏐⏐⏐⏐\n∂pij\n∂sik\n⏐⏐⏐⏐=\n{\n1\nx0Tx0\n1\nn(1 −1\nn) j = k\n1\nx0Tx0\n1\nn2 j ̸= k\n≥\n{\n1\nϵn(1 −1\nn) j = k\n1\nϵn2 j ̸= k.\n(27)\nLet ϵ →0+, then 1\nϵn2 , 1\nϵn(1 −1\nn) →∞, so⏐⏐⏐\n∂pij\n∂sik\n⏐⏐⏐→∞.\nD Analyze the gradient of each method\nIn this section, let’s consider a one-layer Trans-\nformer, for a multi-layer Transformer, we can\nprove our conclusion using induction.\nWe begin this section by introducing some\nmathematical notations.\nD.1 Notations\nIn vanilla attention, we have:\nS = QKT ∈Rn×n,\nP = Softmax(S) ∈Rn×n,\nO = PV ∈Rn×d.\n(28)\nIn linear attention, we have:\nS = ϕ(Q)ϕ(K)T ∈Rn×n,\n∆ = diag(S1n) ∈Rn×n,\nP = ∆−1S ∈Rn×n,\nO = PV ∈Rn×d.\n(29)\nAlthough this term is not calculated in linear\nattention, we discuss it conceptually. Note that\n7035\nthe above formulations can be unified into the\nfollowing form 7:\nS = f(ψ(Q)ψ(K)T) ∈Rn×n,\n∆ = diag(S1n) ∈Rn×n,\nP = ∆−1S ∈Rn×n,\nO = PV ∈Rn×d,\n(30)\nwhere in vanilla attention, we have:\nψ(x) =x,f(x) = exp(x), (31)\nand in linear attention, we have:\nψ(x) =ϕ(x),f(x) =x. (32)\nIn NORM ATTENTION , we have:\nS = ϕ(Q)ϕ(K)T ∈Rn×n,\nT = SV ∈Rn×d,\nO = RMSNorm(T)\n≜\n\n\nRMSNorm(t1)T\n...\nRMSNorm(tn)T\n\n∈Rn×d,\n(33)\nwhere RMSNorm is defined as follows:\nDefinition D.1.\nRMSNorm(x) = x√\nσ2 + ϵ,\nσ2 =\n∑d\ni=1 x2\ni\nd ,\nϵ> 0,\nx ∈Rd.\n(34)\nIn the subsequent discussion, we define gra-\ndient ∇MLas:\nDefinition D.2.\n[∇ML]ij = ∂L\n∂mij\n, (35)\nwhere Lstands for loss function, M is a pa-\nrameter matrix.\nThen we define the mapping has:\nDefinition D.3.\nh: Rn×m →R,h(X) =\nn\nmax\ni=1\n∥Xi∥2,\nX ∈Rn×m.\n(36)\n7 Here, the function f(X) is applied element-wise to the ma-\ntrix X ∈Rn×m, that is, [f(X)]ij = [f(xij)]\nThe mapping hhas the following property:\nProposition D.4. ∀X ∈Rn×m,Y ∈Rr×m,\nwe have:\nh(XY⊤) ≤√rh(X)h(Y). (37)\nProof. Since\n[XY⊤]ij = Xi[Yj]⊤\n≤∥Xi∥2∥Yj∥2\n≤h(X)h(Y),\n(38)\nso\n∥[XY⊤]i∥2 =\n√\nr∑\nj=1\n([XY⊤]ij)2\n≤\n√\nr(h(X)h(Y))2\n= √rh(X)h(Y),\nh(XY⊤) =\nr\nmax\ni=1\n[XY⊤]i\n\n2\n≤√rh(X)h(Y).\n(39)\nD.2 Gradient analysis\nD.2.1 Preliminary\nGiven gradient ∇OL∈ Rn×d, let’s compute\n∇SLin every situation.\nWe first define:\nc1 = h(∇OL)\n=\nn\nmax\ni=1\n∥∇OiL∥2,\nc2 = h(V)\n=\nn\nmax\ni=1\n∥Vi∥2 <∞,\nc3 = min\ni,j\n|sij|≥ 0.\n(40)\nBefore we get started, we have the following\npropositions. The proof can be found in Ap-\npendix D.3.\nProposition D.5. c1 <∞.\nProposition D.6. ∀X ∈Rn×m, we have:\n∥X∥2 ≤√nh(X). (41)\nTake X = V, we get:\n∥V∥2 ≤√nh(V) =√nc2. (42)\n7036\nD.2.2 Vanilla/Linear attention\nAccording to (30), we can discuss vanilla and\nlinear attention under one formula:\n∇PL= [∇OL]VT ∈Rn×n. (43)\nThen define matrix U(i) ∈Rn×n:\n[U(i)]jk = ∂pik\n∂sij\n. (44)\nAccording to (9), in vanilla attention, we have:\n⏐⏐[U(i)]jk\n⏐⏐≤1\n4, (45)\nwhile in linear attention, we have:\n⏐⏐[U(i)]jk\n⏐⏐≤ 1\n4|sij|≤ 1\n4c3\n. (46)\nSince:\n∂L\n∂sij\n=\nn∑\nk=1\n∂L\n∂pik\n∂pik\n∂sij\n= (∇PiL)(U(i)\nj )⊤\n= (∇OiL)VT(U(i)\nj )⊤.\n(47)\nSo we have:\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐≤∥(∇OiL)VT∥2\nU(i)\nj\nT\n2\n≤∥∇OiL∥2∥VT∥2∥U(i)\nj ∥2\n≤c1 ×√nc2 ×1\n4t\n=\n√nc1c2\n4t ,\n(48)\nwhere t= 1in vanilla attention and t= c3 in\nlinear attention.\nOn the other hand, according to Appendix\nC, in linear attention, there exist qi,kj, such\nthat:\n∂pik\n∂sij\n= 1\n∥x0Tx0∥tijk,\ntijk =\n{\n1\nn(1 −1\nn) j = k\n−1\nn2 j ̸= k.\n(49)\nThen\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐=\n⏐⏐⏐⏐⏐\nn∑\nk=1\n∂L\n∂pik\n∂pik\n∂sij\n⏐⏐⏐⏐⏐\n= 1\n∥x0Tx0∥\n⏐⏐⏐⏐⏐\nn∑\nk=1\n∂L\n∂pik\ntijk\n⏐⏐⏐⏐⏐\n≥1\nϵ\n⏐⏐⏐⏐⏐\nn∑\nk=1\n∂L\n∂pik\ntijk\n⏐⏐⏐⏐⏐.\n(50)\nLet ϵ→0+, then\n⏐⏐⏐∂L\n∂sik\n⏐⏐⏐→∞. This means that\nthe gradient in linear attention is unbounded.\nD.2.3 N ORM ATTENTION\nWe first define the second-moment of i’th row\nof T:\nσ2\ni =\n∑d\nj=1 t2\nij\nd . (51)\nThen ∂oij\n∂tik\nis as follows:\n∂oij\n∂tik\n= 1√\nσ2\ni + ϵ\n[\n1{j = k}− 1\nd\ntijtik\nσ2\ni + ϵ\n]\n.\n(52)\nNotice that we have the following upper bound:\n⏐⏐⏐⏐\n∂oij\n∂tik\n⏐⏐⏐⏐\n= 1√\nσ2\ni + ϵ\n[\n1{j = k}− 1\nd\ntijtik\n∑d\ns=1 t2\nis\nd + ϵ\n]\n= 1√\nσ2\ni + ϵ\n[\n1{j = k}+ tijtik\n∑d\ns=1 t2\nis + dϵ\n]\n≤ 1√\nσ2\ni + ϵ\n[\n1{j = k}+ 1\n2\nt2\nij + t2\nik\n∑d\ns=1 t2\nis\n]\n≤ 1√\nσ2\ni + ϵ\n[\n1 +1\n2\n]\n≤ 3\n2\n√\nσ2\ni + ϵ\n.\n(53)\nLet’s define matrix R(i) ∈Rd×d as follows:\n[R(i)]jk = ∂oik\n∂tij\n. (54)\nSince\n∂L\n∂tij\n=\nn∑\nk=1\n∂L\n∂oik\n∂oik\n∂tij\n= (∇OiL)(R(i)\nj )⊤.\n(55)\n7037\nThen we can get:\n∇TiL= (∇OiL)(R(i))T ∈R1×d. (56)\nAccording to (53), we have:\n∥R(i)∥2 ≤∥R(i)∥F\n≤\n√\nd∑\nj=1\nd∑\nk=1\n[∂oij\n∂tik\n]2\n≤ 3d\n2\n√\nσ2\ni + ϵ\n≤ 3d\n2√ϵ.\n(57)\nFinally, we get:\n∇SiL= (∇TiL)VT\n= (∇OiL)(R(i))TVT ∈R1×n,\n∂L\n∂sij\n= (∇OiL)(R(i))TVj,\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐=\n⏐⏐(∇OiL)(R(i))TVj\n⏐⏐\n≤∥∇OiL∥2∥R(i)Vj∥2\n≤∥∇OiL∥2∥R(i)∥2∥Vj∥2\n≤c1 × 3d\n2√ϵ ×c2\n= 3c1c2d\n2√ϵ .\n(58)\nLet’s summarize the previous results.\nIn vanilla attention, we have:\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐≤\n√nc1c2\n4 <∞. (59)\nIn linear attention, there exist qi,kj, such that:\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐→∞. (60)\nIn NORM ATTENTION , we have:\n⏐⏐⏐⏐\n∂L\n∂sij\n⏐⏐⏐⏐≤3c1c2d\n2√ϵ <∞. (61)\nSo ∂L\n∂sij\nis bounded in vanilla attention and\nNORM ATTENTION , while it’s unbounded in\nlinear attention. This makes the training of\nlinear transformer unstable.\nD.3 Proof of the proposition\nProof of Proposition D.5. Let’s consider a\none layer Transformer for classification tasks.\nThe input is X ∈ Rn×d, the label is Y ∈\nRn×m, where m is the number of categories\nand Yi is one-hot vector,. f1,f2 are the activa-\ntion functions, here we take f1 = f2 = ReLU\nas an example. The parameters of the model\nare:\nW1 ∈Rd×d1 ,W2 ∈Rd1×d,\nW3 ∈Rd×m. (62)\nThe forward pass of the model is8:\n• X1 = XAttention(X) ∈Rn×d.\n• X2 = f1(X1W1) ∈Rn×d1 .\n• X3 = f2(X2W2) ∈Rn×d.\n• O = TW3 ∈Rn×m.\n• P = Softmax(O) ∈Rn×m.\n• L= corss_entropy(P,Y) ∈R.\nThe backward pass of the model is:\n1. ∇OL= P −Y ∈Rn×m.\n(a) The upper bound is:\nh(∇OL)\n= max{\nm∑\ni=1\np2\ni −2p1 + 1,\npi ≥0,\nm∑\ni=1\npi = 1}\n≜a0\n<∞.\n(63)\n2. ∇X3 L= (∇OL)W⊤\n3 ∈Rn×d.\n(a) The upper bound is:\nh(∇X3 L)\n≤\n√\ndh(∇OL) h(W3)\n≤\n√\nda0h(W3)\n≜a1 <∞.\n(64)\n8 XAttention stands for vanilla/norm attention.\n7038\n3. ∇X2 L = (f′\n2(X2W2) ⊙∇X3 L) W⊤\n2 ∈\nRn×d.\n(a) The upper bound is:\nh(∇X2 L)\n≤\n√\nd1h(f′\n2(X2W2) ⊙∇X3 L) h(W2)\n≤\n√\nd1a1h(W2)\n≜a2\n<∞.\n(65)\n4. ∇X1 L = (f′\n1(X1W1) ⊙∇X2 L) W⊤\n1 ∈\nRn×d1 .\n(a) The upper bound is:\nh(∇X1 L)\n≤\n√\ndh(f′\n1(X1W1) ⊙∇X2 L) h(W1)\n≤\n√\nda2h(W2)\n≜a3\n<∞.\n(66)\nSo the gradient passed to XAttention module\nis bounded, i.e., c1 = a3 <∞.\nProof of Proposition D.6.\n∥X∥2 ≤∥X∥F\n=\n√\nn∑\ni=1\n∥Xi∥2\n2\n≤\n√\nn∑\ni=1\n[h(X)]2\n= √nh(X).\n(67)\nE Experiment configs\nIn this section, we will introduce detailed train-\ning hyperparameters. We introduce the con-\nfigurations for autoregressive/bidirectional lan-\nguage model in table F. For LRA benchmark,\nwe use the same configuration as Skyformer,\nwhich use 2-layer transformer model with 64\nhidden dimensions, 2 attention heads, 85 GLU\ndimensions, Swish as GLU activation function.\nFor batch size and learning rate , we use 16,1e-\n4 for Text Classification, 32,1e-4 for ListOps,\n16,2e-4 for Document Retrieval, 128,2e-4 for\nPathfinder, 256,1e-4 for Image Classification,\nthe same as Skyformer.\n7039\nF Pseudocode for visualization.\nIn this section, we provide pseudo codes for\nthe 4th column of Figure 2 in Python:\nimport torch\ndef get_curve(w):\nn, m = w.shape\nnum = 100\nP = torch.linspace(0, 1, num)\ncnts = torch.zeros(num)\nfor i in range(n):\ncnt = torch.zeros(num)\nw1 = w[i].clone()\ncenter = i % m\ns = w1[center].item()\nL = 1\nl = center - 1\nr = center + 1\nj = 1\nl_thre = 0\nr_thre = m\nflag = 0\nwhile L < m and j < num:\nif (s >= P[j].item()):\ncnt[j] = L\nj += 1\ncontinue\nif flag == 1:\nif r != r_thre:\ns += w1[r].item()\nr = min(r_thre, r + 1)\nflag = 0\nelse:\nif l != l_thre:\ns += w1[l].item()\nl = max(l_thre, l - 1)\nflag = 1\nL = min(r - l + 1, m)\nif L >= m:\nfor u in range(j, num):\ncnt[u] = min(L, m)\ncnt[0] = 0\ncnts += cnt\ncnts = cnts / n / m\nplt.plot(cnts, P)\nreturn cnts\n7040\nTable 13: Detailed configurations used in our experiments. “Total batch size” means batch_per_gpu ×\nupdate_freq ×num_gpus. “Attention dropout” is only used for vanilla attention. “ALM”: autoregressive Language\nModel. “BLM”: bidirectional Language Model.\nAML BLM\nData WikiText-103 WikiText-103\nTokenizer method BPE BPE\nSrc V ocab size 267744 50265\nEncoder layers 0 12\nDecoder layers 6 0\nHidden dimensions 512 768\nNumber of heads 8 12\nGLU dimensions 2048 1365\nGLU activation function Swish Swish\nSequence length 512 512\nTotal batch size 128 512\nNumber of updates 100k 50k\nWarmup steps 8k 3k\nPeak learning rate 5e-4 5e-4\nLearning rate scheduler Inverse sqrt Polynomial decay\nOptimizer Adam Adam\nAdam ϵ 1e-8 1e-6\nAdam (β1,β2) (0.9, 0.98) (0.9, 0.98)\nWeight decay 0.01 0.01\nGradient clipping 0.0 0\nHidden dropout 0.1 0.1\nAttention dropout 0 0.1\n7041"
}