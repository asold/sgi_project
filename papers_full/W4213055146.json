{
  "title": "GePpeTto Carves Italian into a Language Model",
  "url": "https://openalex.org/W4213055146",
  "year": 2020,
  "authors": [
    {
      "id": null,
      "name": "Mattei, Lorenzo De",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226382021",
      "name": "Cafagna, Michele",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281502918",
      "name": "Dell'Orletta, Felice",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2782368733",
      "name": "Nissim, Malvina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752517397",
      "name": "Guerini, Marco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287476380",
      "name": "Monti, Johanna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2736191205",
      "name": "Tamburini Fabio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2155870214",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W2604799547"
  ],
  "abstract": "In the last few years, pre-trained neural architectures have provided impressive improvements across several NLP tasks. Still, generative language models are available mainly for English. We develop GePpeTto, the first generative language model for Italian, built using the GPT-2 architecture. We provide a thorough analysis of GePpeTto’s quality by means of both an automatic and a human-based evaluation. The automatic assessment consists in (i) calculating perplexity across different genres and (ii) a profiling analysis over GePpeTto’s writing characteristics. We find that GePpeTto’s production is a sort of bonsai version of human production, with shorter but yet complex sentences. Human evaluation is performed over a sentence completion task, where GePpeTto’s output is judged as natural more often than not, and much closer to the original human texts than to a simpler language model which we take as baseline.",
  "full_text": "University of Groningen\nGePpeTto Carves Italian into a Language Model\nMattei, Lorenzo De; Cafagna, Michele; Dell'Orletta, Felice; Nissim, Malvina; Guerini, Marco\nPublished in:\nProceedings of the Seventh Italian Conference on Computational Linguistics, CLiC-it 2020, Bologna, Italy, March\n1-3, 2021\nIMPORTANT NOTE: You are advised to consult the publisher's version (publisher's PDF) if you wish to cite from\nit. Please check the document version below.\nDocument Version\nPublisher's PDF, also known as Version of record\nPublication date:\n2020\nLink to publication in University of Groningen/UMCG research database\nCitation for published version (APA):\nMattei, L. D., Cafagna, M., Dell'Orletta, F., Nissim, M., & Guerini, M. (2020). GePpeTto Carves Italian into a\nLanguage Model. In J. Monti, F. Dell'Orletta, & F. Tamburini (Eds.), Proceedings of the Seventh Italian\nConference on Computational Linguistics, CLiC-it 2020, Bologna, Italy, March 1-3, 2021 (Vol. 2769).\nCEUR-WS.org.\nCopyright\nOther than for strictly personal use, it is not permitted to download or to forward/distribute the text or part of it without the consent of the\nauthor(s) and/or copyright holder(s), unless the work is under an open content license (like Creative Commons).\nThe publication may also be distributed here under the terms of Article 25fa of the Dutch Copyright Act, indicated by the “Taverne” license.\nMore information can be found on the University of Groningen website: https://www.rug.nl/library/open-access/self-archiving-pure/taverne-\namendment.\nTake-down policy\nIf you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately\nand investigate your claim.\nDownloaded from the University of Groningen/UMCG research database (Pure): http://www.rug.nl/research/portal. For technical reasons the\nnumber of authors shown on this cover page is limited to 10 maximum.\nDownload date: 05-11-2025\nGePpeTto Carves Italian into a Language Model\nLorenzo De Mattei•⋄⋆†, Michele Cafagna†, Felice Dell’Orletta⋆, Malvina Nissim⋄, Marco Guerini‡\n•Department of Computer Science, University of Pisa, Italy\n⋄Center for Language and Cognition Groningen, University of Groningen, The Netherlands\n⋆ItaliaNLP Lab, Istituto di Linguistica Computazionale “Antonio Zampolli”, Pisa, Italy\n†Aptus.AI, Pisa, Italy\n‡Fondazione Bruno Kessler, Trento, Italy\nlorenzo.demattei@di.unipi.it, michele@aptus.ai,\nfelice.dellorletta@ilc.cnr.it, m.nissim@rug.nl, guerini@fbk.eu\nAbstract\nIn the last few years, pre-trained neural ar-\nchitectures have provided impressive improve-\nments across several NLP tasks. Still, gen-\nerative language models are available mainly\nfor English. We develop GePpeTto, the ﬁrst\ngenerative language model for Italian, built\nusing the GPT-2 architecture. We provide\na thorough analysis of GePpeTto’s quality\nby means of both an automatic and a human-\nbased evaluation. The automatic assessment\nconsists in (i) calculating perplexity across dif-\nferent genres and (ii) a proﬁling analysis over\nGePpeTto’s writing characteristics. We ﬁnd\nthat GePpeTto’s production is a sort of bon-\nsai version of human production, with shorter\nbut yet complex sentences. Human evaluation\nis performed over a sentence completion task,\nwhere GePpeTto’s output is judged as natu-\nral more often than not, and much closer to\nthe original human texts than to a simpler lan-\nguage model which we take as baseline.\n1 Introduction\nLanguage Models (LMs) based on pre-trained ar-\nchitectures such as BERT (Devlin et al., 2019) and\nGPT-2 (Radford et al., 2019) have provided impres-\nsive improvements across several NLP tasks. While\nfor BERT-based architectures several monolingual\nmodels other than English have been developed,\nlanguage-speciﬁc implementations of generative\npre-trained transformer based models, such as GPT-\n2, are not widely available yet. As a contribution\nto ﬁll this gap, we developed GePpeTto, the ﬁrst\ngenerative language model for Italian, using the\noriginal GPT-2 as a blueprint.\nCopyright © 2020 for this paper by its authors. Use\npermitted under Creative Commons License Attribution 4.0\nInternational (CC BY 4.0).\nThe evaluation of generated text is known to be\nintrinsically difﬁcult (Gatt and Krahmer, 2018); we\nadopt here an encompassing approach, performing\nboth automatic and human-based evaluations. The\nautomatic assessment consists in two strategies: the\nﬁrst involves calculating perplexity across different\nlanguage models trained on various datasets repre-\nsenting different genres. This serves to understand\nhow good GePpeTto is as a language model, and\nhow much it captures the various genres. The sec-\nond one is a proﬁling analysis where, by means\nof a series of linguistic features, we capture some\nof GePpeTto’s writing characteristics, and com-\npare them to those of the data it was trained on.\nFinally, the human evaluation is performed over\na sentence completion task where GePpeTto is\nevaluated against gold standard sentences as well\nas a simple Markov-based baseline.\nWe make the model available to the community:\nhttps://github.com/LoreDema/GePpeTto.\n2 GePpeTto\nGePpeTto was trained using the original settings\nof GPT-2 on a collection of Italian texts amount-\ning to almost 13GB. Details on data and model’s\nparameters are provided in the following sections.\n2.1 Data\nThe training set comprises two main sources. The\nﬁrst one is a dump of Italian Wikipedia (Novem-\nber 2019), consisting of 2.8GB of text. The content\nwas extracted using the Wikiextractor tool (Attardi,\n2012). The second one is the ItWac corpus (Ba-\nroni et al., 2009), which amounts to 11GB of web\ntexts. This collection provides a mix of standard\nand less standard Italian, on a rather wide chrono-\nlogical span, with older texts than the Wikipedia\n\ndump (the latter stretches only to the late 2000s).\nMinimal processing was applied to the texts.\nAll Wikipedia documents were preﬁxed by the\ntoken “Wikipedia” followed by the page’s title\nwords. All ItWac texts were introduced by the\ntoken “Links” followed by the webpage address\nthe text was coming from. For all texts in both\ncollections, end of document was marked with the\nstring <|endoftext|>, as done for the original\nGPT-2 training set (Radford et al., 2019).\n2.2 Model\nGePpeTto was trained on 4 GPUs Tesla T4, us-\ning the PyTorch implementation available on hug-\ngingface.1 The model’s size corresponds to GPT-2\nsmall, with 12 layers and 117M parameters (Rad-\nford et al., 2019). The vocabulary size is 30,000,\nthe batch size is 32 and the block size is 100. We\nuse 10,000 warmup steps, the adam optimiser, and\nan initial learning rate of 5e-5. We train the model\nfor 620,000 steps.\n2.3 Examples\nThrough some selected examples, we illustrate here\nGePpeTto’s generation behaviour. We prompt it\nwith different inputs, and show how the generated\ntext is indeed style-compliant with the prompts.\nIn the ﬁrst example, we start the prompt with the\n‘keyword’ “Wikipedia”, primingGePpeTto to re-\nproduce text in the style and format of Wikipedia\npages (recall that the tag “Wikipedia” was preposed\nto all Wikipedia training data).\n“Wikipedia La Repubblica del Burmini \\n\\nLa\nRepubblica del Burmini (in francese “La Re-\npubblica del Burginais”), originariamente repub-\nblica di Francia, venne creata nel 1859 insieme\nal Regno d’Italia, con capitale Bourges, per pro-\nmuovere l’uniﬁcazione dei due territori della\nFrancia. Le citt `a della Francia si unirono nel\n1877 ed i cinque stati facenti parte dell’impero\nerano Francia, Italia e Savoia.”\nprompt: “Wikipedia La Repubblica del\nBurmini”\nIn the second example, we prompt\nGePpeTto with the same text “ La Repub-\nblica del Burmini ” but preposing to it the\n‘keyword’ “Links” followed by the web address\nof an Italian newspaper, priming GePpeTto to\n1https://huggingface.co.\nreproduce a text in the style of a news article.\nInterestingly the output is in news style but the\nprompt is interpreted as the header of the news,\ni.e., ‘place and date’.\n“Links http://www.corriere.it La Repubblica del\nBurmini - 14 gennaio 2003. Nota: Per gli ambiti\nregionali, le Regioni dovranno determinare in un\nsolo anno le modalit `a di trasporto, tra gli altri,\ndegli studenti del secondo ciclo, e le modalit `a\nper pagare eventuali supplementi a carico del\nservizio sanitario nazionale. Si tratta, in altre\nparole, di un decreto legislativo con un semplice\nriferimento alla Regione Lombardia.\nprompt: “Links http://www.corriere.it La\nRepubblica del Burmini”\nIn the following three examples we can see\nthat even without using any speciﬁc keywords,\nGePpeTto’s is easily style-primed by clear\nprompts. In the ﬁrst case, we use as input a string\nprompting forum-like language, in the second one,\nwe use legal terms in the prompt, and in the last\none, we prompt GePpeTto with more sophisti-\ncated language, which the model appears to readily\npick up in its generated text.\n“Piero DM wrote: >Una domanda sul lavoro\na tempo pieno dovrebbe far sorridere...... <<\nScusate, ma vorrei avere una vostra opinione.”\nprompt: “Piero DM wrote:”\n“Visto l’articolo 4 del dpcm 29 dicembre 1999,\ncon il quale viene disposto l’inquadramento tra\nil personale interessato alle operazioni e le RSU\nper l’applicazione del quarto comma del predetto\narticolo 4, paragrafo 1, lettera a) del presente\naccordo,”\nprompt: “Visto l’articolo 4 del dpcm:”\n“l’intricato esistenzialismo diventa il motore di\nquella riﬂessione metaﬁsico-ﬁlosoﬁca di cui il\nprimo pensatore si avvale. L’altruismo di fondo\nappartiene quasi completamente alla ﬁlosoﬁa;\nl’acuto verso la razionalit`a senza limite.\nprompt: “l’intricato esistenzialismo”\n3 Automatic Evaluation\nGePpeTto is trained as a language model for\nItalian. To assess its closeness to actual Ital-\n2\nian texts, we calculate perplexity on a variety of\nsources, including a small leave out test set (1%) of\nGePpeTto s training corpus (Section 3.1). In addi-\ntion, we explore GePpeTto’s linguistic proﬁle by\ncomparing its production with human-written texts\nalong a series of linguistic features (Section 3.2).\n3.1 Perplexity\nAs a ﬁrst evaluation, we are interested in under-\nstanding the quality of GePpeTto as a language\nmodel in its own training domain. As a second\nevaluation we want test its performance at zero-\nshot domain transfer (i.e. language modeling of a\ndifferent domain). We use perplexity as a measure\nof language modelling performance. The different\ndomains we consider, and the relative corpora we\nuse, are as follows:\n• own domains: Wikipedia and ItWac;\n• legal domain: a corpus of Italian laws scraped\nfrom EUR-Lex2 (tables excluded);\n• news: a corpus of articles from the online ver-\nsions of two newspapers, i.e., la Repubblica3\nand Il Giornale4 (De Mattei et al., 2020);\n• social media: a corpus of forum comments\n(Maslennikova et al., 2019).\nTo compute the perplexity scores (Table 1) we used\na random sample of 4M tokens for each corpus.\nAs expected, GePpeTto performs better on its\nown domains. Although ItWac is four times big-\nger than Wikipedia, the lower performance on the\nformer might be due to ItWac being open domain\nwith a large diversity of styles, while Wikipedia is\nmore ‘standardised’. Consistently with this hypoth-\nesis, we observe a similar trend in ‘out-of-domain’\ntesting, where GePpeTto performs better on do-\nmains with a well coded style, namely legal docu-\nments. On domains with less coded styles, such as\nnews and especially forum comments, we observe\na performance drop.\nIf we compare perplexity scores with the orig-\ninal English GPT-2 small model, we see that\nGePpeTto’s results are slightly worse on the own\ndomain corpora, which could be due to the smaller\nsize of the training set. Out-of-domain perplexity\nscores are comparable between the two models.\n3.2 Linguistic Proﬁling\nFor our second evaluation, we used Proﬁling-UD\n(Brunato et al., 2020), a tool for the automatic\n2https://eur-lex.europa.eu/\n3https://www.repubblica.it\n4https://www.ilgiornale.it/\nDOMAIN PERPLEXITY\nWikipedia 26.4910\nItWac 30.9698\nLegal 39.6087\nNews 48.3468\nSocial Media 131.3812\nTable 1: Perplexity of GePpeTto over several in-\ndomain and out-of-domain corpora.\nOriginal GePpeTto\nFeature µ std µ std\nCPT 4.809 0.959 4.750 1.127\nTPS 32.302 28.322 20.382 11.127\nTPC 12.393 11.504 10.711 8.529\nLLmax 13.290 13.370 8.922 6.112\nLLavg 2.555 1.002 2.373 0.676\nTable 2: Main linguistic features considered in our anal-\nysis. CPT = chars per token, TPS = token per sentence,\nTPC = tokens per clause, LL = links length.\nanalysis of texts that extracts several linguistic fea-\ntures of varying complexity. These features range\nfrom raw text properties, such as average length of\nwords and sentences, to lexical, morpho-syntactic,\nand syntactic properties, such as part-of-speech\n(POS) distribution and inﬂectional properties of\nverbs. More complex aspects of sentence structure\nare derived from syntactic annotation, and model\nglobal and local properties of parsed tree structure,\nsuch as the order of subjects/objects with respect\nto the verb, the distribution of syntactic relations,\nand the use of subordination.\nIn our analysis we focus on two macro aspects\nof GePpeTto’s output, namely lexical complexity\nand syntactic complexity, and compare them to hu-\nman productions. To do so, we rely on a selection\nof Proﬁling-UD’s features which we use as proxies\nfor the macro-aspects that we consider.\nWe run the proﬁling analysis on a sample of both\ngold and generated texts. For gold, we randomly\nsample the test set for a total of about 19k sentences.\nFor GePpeTto, we pick the ﬁrst token from each\nof the 19k gold sentences, and use it as a prompt to\nthe model. We proﬁle these generated texts.\nLexical complexity. We proxy lexical complex-\nity with the number of characters per word, overall\nfrequency of tokens, also with reference to an ex-\n3\nternal dictionary, and POS distribution.\nThe number of characters per token (CPT),\nwhich indicates whether shorter (usually more com-\nmon) or longer (usually more complex/specialised)\nwords are used, is completely comparable across\nthe original (4.80, std=0.96) and GePpeTto’s\n(4.75, std=1.13) language models – see Table 2.\nThis suggests that the complexity of the used vo-\ncabulary is not that different.\nWe compute a reference dictionary of token fre-\nquency on ItWac (≈1.5 billion tokens), and com-\npare observed token frequency in both gold and\ngenerated text to this reference. We observe that\nin gold sentences, each token has a probability of\n0.912 to be in the top 5‰ of most frequent tokens.\nIn the generated sentences, the probability grows to\n0.935, suggesting that GePpeTto is more likely\nto use more frequent words rather than rarer ones.\nThis observation is in line with previous research\nwhich showed that for Nucleus Sampled texts, such\nas those produced by GPT-2, all tokens come from\nthe top-p%, since the long tail is cut off, while for\nhuman produced texts, the probability of all tokens\nbeing drawn from the top-p% of the language dis-\ntribution goes to zero as document length increases\n(Gehrmann et al., 2019; Zellers et al., 2019).\nRegarding POS distribution, we observe that\nwhile for most POS tags usage is comparable, for\na few others the two language models differ. The\nlatter are, speciﬁcally, auxiliaries and proper nouns,\nwhich GePpeTto tends to overgenerate in com-\nparison to the original model, and adjectives, which\nGePpeTto instead uses less than in the original\ntexts. This is seen also for nouns and verbs, but the\ndifferences are relatively minimal. Conjunctions\nare also overall less frequent in GePpeTto. A\ndetailed table will be included in the ﬁnal version.\nSyntactic complexity. At the level of syntax, we\nproxy complexity by the number of tokens per sen-\ntence, and the number of tokens per clause. We\nalso look at the length of a dependency link, that\nis calculated as the number of words occurring lin-\nearly between the syntactic head and its dependent\n(excluding punctuation dependencies). The value\nassociated with this feature corresponds to the av-\nerage value extracted for all dependencies in a text.\nThis information is complemented with the feature\nMaximum dependency link corresponding to the\nlongest dependency link for each sentence.\nWhen comparing the number of tokens per sen-\ntence (TPS, Table 2), we see that it’s much lower\nfor GePpeTto’s production rather than for hu-\nman texts (20.4 tokens per sentence on average for\nGePpeTto vs 32.3 for gold texts),indicating that\nGePpeTto generates shorter sentences. Contextu-\nally, we also observe that GePpeTto’s generated\nsentences exhibit less variation in length (smaller\nSTD) than human sentences (larger STD).\nThe difference in number of tokens at the clause\nlevel is relatively smaller, with clauses of length\n12.4 in human texts vs 10.7 in GePpeTto (TPC,\nsee Table 2). Considering that a clause is proxied\nby the presence of a verbal/copular head, it seems\nthat sentences produced by GePpeTto, though\nshorter, are similar in complexity given the propor-\ntional distribution of verbal heads.\nThe above values taken together might suggest\nthat while complexity at the macro level (sentence\nlength) is higher for natural sentences, at the micro\nlevel (clause length) complexity of GePpeTto’s\ngenerations and human texts is more similar. While\nthis intuition will require further linguistic analysis,\nobserving the length of syntactic links seems to\nsupport it. This feature proxies quite well syntac-\ntic complexity, since it indicates how maximally\nfar (and how far on average) a dependent and its\nhead are within a sentence. Both the maximum\nlength and the average length are higher for human\ntexts (LLmax and LLavg, see Table 2). However, if\nwe look at them proportionally to sentence length,\nwe ﬁnd that they are comparable: normalising the\nlongest link by the number of tokens per sentence\n(LLmax/TPS), we obtain similar values for gold\n(0.411) and for GePpeTto (0.438). This suggests\nthat GePpeTto produces somewhat shorter sen-\ntences, but their internal complexity relatively cor-\nresponds to the internal complexity of the longer\nsentences produced by humans.\n4 Human evaluation\nWe also test GePpeTto’s ability to generate Ital-\nian texts through a sentence completion task. The\nautomatically generated sentences are presented to\nhuman subjects for evaluation on perceived natural-\nness and compared to gold ones and to a baseline.\nWhile the original (gold) texts represent an\nupperbound for GePpeTto, we do not actually\nhave a lowerbound against which the quality of\nGePpeTto can be assessed. To provide a com-\nparison, we train a simple Markov model that\nwould be able to generate text and use it as our\nbaseline. Since the size of a Markov model dra-\n4\nmatically grows with its vocabulary size, we use\n1 million randomly sampled sentences from the\nsame training-set used for GePpeTto. We train a\nMarkov chain generator using the markovify5\nimplementation with state size 2, then we generate\nsynthetic texts starting from the last 2 tokens of\nsame prompts used for GePpeTto.\n4.1 Tasks\nHuman subjects are asked to perform two evalu-\nation tasks. One is a comparative ranking task,\nwhere subjects are asked to rank three portions\nof text (produced by gold, GePpeTto, baseline)\naccording to perceived naturalness. The other is\na classiﬁcation task, where subjects are asked to\ntell, according to their intuition, if a portion of text,\nseen in isolation, is automatically generated ( yes,\nno, can’t tell).\nExperimental design. The experiment includes\n12 conditions of the stimulus material in a 4x3\ndesign. One level (A) with three conditions is\ngiven by {gold,GePpeTto, baseline}. The second\nlevel (B) is the prompt+completion combina-\ntion that results in 4 conditions {5+5, 5+10, 10+5,\n10+10}. We use 100 different prompts (randomly\nselected gold sentences truncated at 5 and 10 to-\nkens). Each of the 100 prompts enters each of the\n12 conditions of the 4x3 design, for a total of 12\ndifferent stimuli. Basically, each 5 or 10 tokens\nprompt is completed with 5 or 10 tokens coming ei-\nther from gold, GePpeTto, or the baseline model.\nTable 3 shows an example of all the stimuli deriving\nfrom the same 5- or 10-token prompt.\nEach subject is assigned either to the ranking or\nto the classiﬁcation task.\nIn ranking, we opt for a between subject evalua-\ntion set up by assigning each subject to one of the\n(B) conditions and offer the three versions of (A)\nto be ranked. For example, one subject is asked to\nevaluate all the 100 prompts in the 5+5 conﬁgura-\ntion (dimension B) for the three realisations, i.e.,\ngold, GePpeTto, and baseline (dimension A).\nFor the classiﬁcation experiments, we again opt\nfor a between subject evaluation set up, this time\nby assigning each subject to one of the 12 con-\nditions, randomly picked up for each prompt. In\nother words, we make sure that each subject is\nexposed to only one completion per prompt, ran-\ndomising prompt order. By seeing only one (out\nof 12) realisation per prompt, each subject sees a\n5https://github.com/jsvine/markovify.\ngiven prompt only once and we can therefore avoid\ncross-comparison effects of different completions\nof the same prompt, which could otherwise poten-\ntially lead again to an implicit ranking task.\nMaterial. The materials are prepared as follows:\nwe have selected 100 random documents/sentences\nand have cut them at their 5 ﬁrst tokens and also\ntheir 10 ﬁrst tokens. Each 5-token and 10-token\nprompt was given to GePpeTto and baseline so\nthat the models could continue the text.\nFor each prompt, we obtain one single generated\ntext by the two automatic models and chop them\nat 5 or at 10 tokens. In other words, each chopped\nversion is derived from the same generated output\nwhich is just cut at different lengths.\nWe cut the sentences (including the original one)\nto control for the effect of text length. Indeed,\nwe observed in Section 3.2 that GePpeTto gener-\nates shorter sentences than humans, which could\nrepresent a strong bias in evaluation. In Ta-\nble 3, we show examples of all the possible\nstimulus material conﬁgurations according to the\nprompt+completion conditions of level (B).\nInstructions and subjects. For both the ranking\nand classiﬁcation experiments, subjects were told\nthat they will have to evaluate excerpts of text along\na ‘more natural vs. more artiﬁcial’ dimension. All\nstimuli used in both scenarios are the same.\nFor the ranking scenario, subjects were asked to\n“rank the given examples from the most natural to\nthe most artiﬁcial”, where the inputs are three texts\n(gold, GePpeTto, baseline), all starting with the\nsame prompt, thus the same ﬁve or ten tokens.\nFor the classiﬁcation scenario, subjects saw in-\nstead the portions of text in isolation, and could\nanswer yes, no, or can’t tell to the question “ ac-\ncording to your intuition is this sentence written by\nan artiﬁcial intelligence?”.\nA total of 24 unique subjects (12 females) carried\nout the tasks using Google Forms. Twelve subjects\n(6 females) were assigned to Task 1 and the others\nto Task 2. Each subject evaluated 100 cases, and\neach case was evaluated by three different subjects.\n4.2 Results\nFirst, we discuss the results of our human evalu-\nation separately, with observations related to the\nranking task and observations related to the classiﬁ-\ncation task. Subsequently, we knit together the two\noutcomes to draw a wider picture of how humans\nassess the quality of GePpeTto’s output.\n5\n5 token prompt: Mentre per quanto riguarda gli\n10 token prompt: Mentre per quanto riguarda gli accordi per la fornitura di\nGold\n5+5 Mentre per quanto riguarda gli accordi per la fornitura di\n5+10 Mentre per quanto riguarda gli accordi per la fornitura di latte, in scadenza questa\n10+5 Mentre per quanto riguarda gli accordi per la fornitura di latte, in scadenza questa\n10+10 Mentre per quanto riguarda gli accordi per la fornitura di latte, in scadenza questa settimana,\nAlemanno ha detto\nGePpeTto\n5+5 Mentre per quanto riguarda gli emendamenti, fa presente che il\n5+10 Mentre per quanto riguarda gli emendamenti, fa presente che il suo gruppo non ha sottoscritto\n10+5 Mentre per quanto riguarda gli accordi per la fornitura di beni e servizi, i fatti\n10+10 Mentre per quanto riguarda gli accordi per la fornitura di beni e servizi, i fatti in suo possesso\nhanno come\nMarkov-based baseline\n5+5 Mentre per quanto riguarda gli aspetti pi `u signiﬁcativi del mondo\n5+10 Mentre per quanto riguarda gli aspetti pi`u signiﬁcativi del mondo editoriali, con priorit`a di\nsviluppo\n10+5 Mentre per quanto riguarda gli accordi per la fornitura di biciclette elettriche a 48 bit\n10+10 Mentre per quanto riguarda gli accordi per la fornitura di biciclette elettriche a 48 bit (281,5\ntrilioni di operazioni e\nTable 3: Example outputs (stimuli) for different prompt lengths of the same original sentence.\nRanking Overall, results show that the most fre-\nquently chosen completion is the gold one, fol-\nlowed by GePpeTto and then the Markov base-\nline, but the baseline is far more distant from\nGePpeTto than GePpeTto from gold (Figure 1).\nIf we look at results in more detail (see Table 4),\nbased on the variable that we have considered in\nthe experimental set up, namely length of input\nand continuation as well as overall sentence length,\nwe observe that the order of preference for gold is\n10+10, then 5+10, then 10+5, and lastly 5+5, while\nfor the automatic models the order is 5+5, 10+5,\n5+10, and then 10+10, suggesting the following.\nFirst, the shortest the sentence, the hardest it is\nto discriminate between gold and generated text;\nindeed, the 5+5 condition is the one that results\nbest for the two models and worst for gold.\nSecond, when the sentence is the longest\n(10+10), it is easiest for the subjects to discrim-\ninate the gold from the generated sentences. It is\nalso interesting to note that in this condition we\nobserve the largest gap between the two generation\nmodels, with GePpeTto getting ranked higher\nthan Markov more than in the other conditions.\ngold geppetto markov\n0\n20\n40\n60\n80\n100percentage\nrank\n1st\n2nd\n3rd\nFigure 1: Ranking results for the three models\nThird, at equal sentence length (15 tokens) the\nsituation is a bit more fuzzy, but we can observe a\nslight tendency where it is easier to spot as automat-\nically generated the 5+10 rather than 10+5 cases.\nThis, in combination with the previous observation,\nseems to imply that the longer the generated text,\nthe easier it is to ﬁgure out which texts are automat-\nically produced, which makes sense, since there is\nmore ‘space’ for the models to make mistakes.\n6\nmodel 5+5 5+10 10+5 10+10\n1st 2nd 3rd 1st 2nd 3rd 1st 2nd 3rd 1st 2nd 3rd\nGold 54 30 16 62 31 7 60 27 13 70 21 9\nGePpeTto 34 43 23 30 46 24 33 43 24 23 59 18\nMarkov 12 27 61 8 23 69 7 30 63 7 20 73\nTable 4: Percentages of ranking results according to the various stimulus material conditions.\ngold geppetto markov\n0\n20\n40\n60\n80\n100percentage\nartificial\nNo\nYes\nCan't tell\nFigure 2: Classiﬁcation results for the three models\nClassiﬁcation Overall, results show that across\nall conditions, gold sentences are most often rightly\nidentiﬁed as not automatically generated (68% of\n“no” to the question whether the output was pro-\nduced by an artiﬁcial intelligence), followed by\nGePpeTto (54%), and lastly by the Markov base-\nline (26%), indicating, as expected, that the latter\nproduces the least natural outputs. Figure 2 reports\nthe distribution over the various answers. Also\nin this case the distance between GePpeTto and\ngold is lower than GePpeTto and the baseline\n(double in percentage points), indicating that the\nproduction of GePpeTto is approaching natural\nlanguage. It is also interesting to see that the\nhighest percentage of “can’t tell” is recorded for\nGePpeTto, meaning that for this model it was\nharder than for baseline and gold to decide whether\nthe text was automatic or not.\nLet us look at results in more detail (Table 5),\nfocusing again on length of input and continuation.\nRegarding continuation, we observe that *+5 con-\nditions are better than *+10 conditions for both au-\ntomatic models, indicating that the least generated\ntext, the more natural the fragment is perceived.\nRegarding input length, we see that for\nGePpeTto a longer prompt yields better results\n(10+5 is better than 5+5, and 10+10 is better than\n5+10). With 10-token prompts, GePpeTto gen-\nerates text that is (i) assessed as natural as much\nas the original text when completed with 5 tokens\n(62% GePpeTto, 63% original), and (ii) judged as\nnatural 50% of the times when completed with 10\ntokens. This seems to suggests that a longer input\ncontext is beneﬁcial to GePpeTto when comple-\ntion size is kept constant. However, we may wonder\nwhether GePpeTto is evaluated as more natural\nbecause the generated text is actually better given\nthe more context to start with, or simply because\nthere is more gold text in the stimulus. If it were\njust for the contribution of a longer gold portion\nin the stimulus, we should see a similar behaviour\nfor the baseline. Instead, we see that prompt size\ndoesn’t matter for the baseline, at least for the 5\ntoken completion case (33% in both 5+5 and 10+5).\nIn the 10-completions (5+10 and 10+10), the larger\namount of gold data in the stimulus probably does\nalleviate a little the very low naturalness induced\nby the generated text. While we can tentatively pos-\ntulate that GePpeTto generates better text when\nmore input is provided, further investigation is re-\nquired to provide more solid evidence.\nSummary of Results. Intersecting the observa-\ntions from the two experimental setups provides\nus with a complete picture. In ranking (thus\nwhen the models are directly compared), both\nGePpeTto and the baseline perform best in the\n5+5 and 10+5 conditions, suggesting that automatic\ngeneration can easily be spotted when compared\nside by side with human text. In other words, the\nleast generated material, the better.\nHowever, looking at classiﬁcation, where each\ntextual material is evaluated in isolation, we see\nthat the two models behave in fact very differ-\nently. First, there is a much larger proportion of\ncases produced by GePpeTto that are deemed\n“natural” (54%) compared to Markov (26%). Sec-\nond, the margin of uncertainty when judging\nGePpeTto is higher than for the baseline and\n7\nmodel 5+5 5+10 10+5 10+10\nyes no ct yes no ct yes no ct yes no ct\nGold 26 66 8 27 68 5 32 63 5 28 71 1\nGePpeTto 32 55 13 48 46 6 32 62 6 42 50 8\nMarkov 62 33 5 80 13 7 61 33 6 71 19 10\nTable 5: Percentages of classiﬁcation results according to the various stimulus material conditions.\nIs the text automatically generated? {yes, no, can’t tell (ct)}.\nfor original text. Lastly, given the same completion\nsize, GePpeTto performs better when its prompt\nis longer. Whether this is an effect of a larger pro-\nportion of gold data in the stimulus or it has to\ndo with providing the model with a larger input\ncontext is left to future investigation.\n5 Conclusion\nGePpeTto is the ﬁrst GPT-2-based language\nmodel for Italian. Through both automatic and\nmanual evaluation we assessed its quality on a va-\nriety of texts and in comparison to gold data as\nwell as another statistical generation model. Re-\nsults show that GePpeTto is able to produce text\nwhich is much closer to human quality rather than\nto the text generated by the other generation model\nwe have used. Linguistic analysis also highlights\nthat GePpeTto’s production is quite similar to hu-\nman production, though in a sort of bonsai version,\nsince its sentences are on average shorter than the\noriginal texts, but with similar complexity.\nThe availability of GePpeTto opens up sub-\nstantial possibilities. In the same way that GPT-2\nis changing the approach to several NLP English\ntasks, we can expect GePpeTto to serve a similar\npurpose in Italian language processing.\nReferences\nGiuseppe Attardi. 2012. Wikiextractor. http://\nattardi.github.io/wikiextractor.\nMarco Baroni, Silvia Bernardini, Adriano Ferraresi,\nand Eros Zanchetta. 2009. The WaCky wide web: a\ncollection of very large linguistically processed web-\ncrawled corpora. Language resources and evalua-\ntion, 43(3):209–226.\nDominique Brunato, Andrea Cimino, Felice\nDell’Orletta, Simonetta Montemagni, and Giu-\nlia Venturi. 2020. Proﬁling-UD: a Tool for\nLinguistic Proﬁling of Texts. In Proceedings of\nthe Twelfth International Conference on Language\nResources and Evaluation (LREC 2020) , Marseille,\nFrance. European Language Resources Association\n(ELRA).\nLorenzo De Mattei, Michele Cafagna, Felice\nDell’Orletta, and Malvina Nissim. 2020. Invis-\nible to People but not to Machines: Evaluation\nof Style-aware Headline Generation in Absence\nof Reliable Human Judgment. In Proceedings of\nthe Twelfth International Conference on Language\nResources and Evaluation (LREC 2020) , Marseille,\nFrance. European Language Resources Association\n(ELRA).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlbert Gatt and Emiel Krahmer. 2018. Survey of the\nstate of the art in natural language generation: Core\ntasks, applications and evaluation. Journal of Artiﬁ-\ncial Intelligence Research, 61:65–170.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M Rush. 2019. Gltr: Statistical detection\nand visualization of generated text. arXiv preprint\narXiv:1906.04043.\nAleksandra Maslennikova, Paolo Labruna, Andrea\nCimino, and Felice Dell’Orletta. 2019. Quanti anni\nhai? Age Identiﬁcation for Italian. In Proceedings of\nthe Sixth Italian Conference on Computational Lin-\nguistics (CLiC-it 2019), Bari, Italy. CEUR Proceed-\nings 2481.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Process-\ning Systems, pages 9051–9062.\n8",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9408134818077087
    },
    {
      "name": "Computer science",
      "score": 0.8044507503509521
    },
    {
      "name": "Generative grammar",
      "score": 0.7208298444747925
    },
    {
      "name": "Natural language processing",
      "score": 0.630571186542511
    },
    {
      "name": "Sentence",
      "score": 0.6156282424926758
    },
    {
      "name": "sort",
      "score": 0.5819570422172546
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5807011127471924
    },
    {
      "name": "Task (project management)",
      "score": 0.5099993944168091
    },
    {
      "name": "Language model",
      "score": 0.5047622919082642
    },
    {
      "name": "Profiling (computer programming)",
      "score": 0.4664537310600281
    },
    {
      "name": "Generative model",
      "score": 0.4577665627002716
    },
    {
      "name": "Architecture",
      "score": 0.45419836044311523
    },
    {
      "name": "Programming language",
      "score": 0.13887685537338257
    },
    {
      "name": "Engineering",
      "score": 0.0814875066280365
    },
    {
      "name": "History",
      "score": 0.07905048131942749
    },
    {
      "name": "Information retrieval",
      "score": 0.07590767741203308
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}