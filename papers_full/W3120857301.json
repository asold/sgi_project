{
  "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
  "url": "https://openalex.org/W3120857301",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2389021690",
      "name": "Zheng Sixiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281858129",
      "name": "Lu, Jiachen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3171936339",
      "name": "Zhao, Hengshuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176328991",
      "name": "Zhu, Xiatian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3044990543",
      "name": "Luo Zekun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2737475498",
      "name": "Wang, Yabiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2685942508",
      "name": "Fu, Yanwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2369082204",
      "name": "Feng Jianfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100117188",
      "name": "Xiang, Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221998561",
      "name": "Torr, Philip H. S.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051190893",
      "name": "Zhang, Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3034345703",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2965391153",
    "https://openalex.org/W2963108253",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2948080074",
    "https://openalex.org/W3034502973",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034355852",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3023001672",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3096678291",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W2895340641",
    "https://openalex.org/W2598666589",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2124592697",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2799213142",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2993235622",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3107113572",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2997564896",
    "https://openalex.org/W3022478135",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W1610060839",
    "https://openalex.org/W2963727650"
  ],
  "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.",
  "full_text": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective\nwith Transformers\nSixiao Zheng1* Jiachen Lu1 Hengshuang Zhao2 Xiatian Zhu3 Zekun Luo4 Yabiao Wang4\nYanwei Fu1 Jianfeng Feng1 Tao Xiang3, 5 Philip H.S. Torr2 Li Zhang1†\n1Fudan University 2University of Oxford 3University of Surrey\n4Tencent Youtu Lab 5Facebook AI\nhttps://fudan-zvg.github.io/SETR\nAbstract\nMost recent semantic segmentation methods adopt\na fully-convolutional network (FCN) with an encoder-\ndecoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic\nvisual concepts with larger receptive ﬁelds. Since context\nmodeling is critical for segmentation, the latest efforts have\nbeen focused on increasing the receptive ﬁeld, through ei-\nther dilated/atrous convolutions or inserting attention mod-\nules. However, the encoder-decoder based FCN architec-\nture remains unchanged. In this paper, we aim to provide\nan alternative perspective by treating semantic segmenta-\ntion as a sequence-to-sequence prediction task. Speciﬁcally,\nwe deploy a pure transformer (i.e., without convolution and\nresolution reduction) to encode an image as a sequence of\npatches. With the global context modeled in every layer of\nthe transformer, this encoder can be combined with a simple\ndecoder to provide a powerful segmentation model, termed\nSEgmentation TRansformer (SETR). Extensive experiments\nshow that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and com-\npetitive results on Cityscapes. Particularly, we achieve the\nﬁrst position in the highly competitive ADE20K test server\nleaderboard on the day of submission.\n1. Introduction\nSince the seminal work of [36], existing semantic seg-\nmentation models have been dominated by those based on\nfully convolutional network (FCN). A standard FCN seg-\nmentation model has an encoder-decoder architecture: the\nencoder is for feature representation learning, while the de-\ncoder for pixel-level classiﬁcation of the feature representa-\n*Work done while Sixiao Zheng was interning at Tencent Youtu Lab.\n†Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with\nSchool of Data Science, Fudan University.\ntions yielded by the encoder. Among the two, feature rep-\nresentation learning (i.e., the encoder) is arguably the most\nimportant model component [8, 28, 57, 60]. The encoder,\nlike most other CNNs designed for image understanding,\nconsists of stacked convolution layers. Due to concerns\non computational cost, the resolution of feature maps is re-\nduced progressively, and the encoder is hence able to learn\nmore abstract/semantic visual concepts with a gradually in-\ncreased receptive ﬁeld. Such a design is popular due to two\nfavorable merits, namely translation equivariance and local-\nity. The former respects well the nature of imaging pro-\ncess [58] which underpins the model generalization ability\nto unseen image data. Whereas the latter controls the model\ncomplexity by sharing parameters across space. However, it\nalso raises a fundamental limitation that learning long-range\ndependency information, critical for semantic segmentation\nin unconstrained scene images [2,50], becomes challenging\ndue to still limited receptive ﬁelds.\nTo overcome this aforementioned limitation, a number\nof approaches have been introduced recently. One approach\nis to directly manipulate the convolution operation. This in-\ncludes large kernel sizes [40], atrous convolutions [8, 22],\nand image/feature pyramids [60]. The other approach is to\nintegrate attention modules into the FCN architecture. Such\na module aims to model global interactions of all pixels in\nthe feature map [48]. When applied to semantic segmenta-\ntion [25, 29], a common design is to combine the attention\nmodule to the FCN architecture with attention layers sitting\non the top. Taking either approach, the standard encoder-\ndecoder FCN model architecture remains unchanged. More\nrecently, attempts have been made to get rid of convolutions\naltogether and deploy attention-alone models [47] instead.\nHowever, even without convolution, they do not change the\nnature of the FCN model structure: an encoder downsam-\nples the spatial resolution of the input, developing lower-\nresolution feature mappings useful for discriminating se-\nmantic classes, and the decoder upsamples the feature rep-\n1\narXiv:2012.15840v3  [cs.CV]  25 Jul 2021\nresentations into a full-resolution segmentation map.\nIn this paper, we aim to provide a rethinking to the se-\nmantic segmentation model design and contribute an alter-\nnative. In particular, we propose to replace the stacked con-\nvolution layers based encoder with gradually reduced spa-\ntial resolution with a pure transformer [45], resulting in\na new segmentation model termed SEgmentation TRans-\nformer (SETR). This transformer-alone encoder treats an\ninput image as a sequence of image patches represented\nby learned patch embedding, and transforms the sequence\nwith global self-attention modeling for discriminative fea-\nture representation learning. Concretely, we ﬁrst decom-\npose an image into a grid of ﬁxed-sized patches, forming a\nsequence of patches. With a linear embedding layer applied\nto the ﬂattened pixel vectors of every patch, we then obtain\na sequence of feature embedding vectors as the input to a\ntransformer. Given the learned features from the encoder\ntransformer, a decoder is then used to recover the original\nimage resolution. Crucially there is no downsampling in\nspatial resolution but global context modeling at every layer\nof the encoder transformer, thus offering a completely new\nperspective to the semantic segmentation problem.\nThis pure transformer design is inspired by its tremen-\ndous success in natural language processing (NLP) [15,45].\nMore recently, a pure vision transformer or ViT [17] has\nshown to be effective for image classiﬁcation tasks. It thus\nprovides direct evidence that the traditional stacked convo-\nlution layer (i.e., CNN) design can be challenged and image\nfeatures do not necessarily need to be learned progressively\nfrom local to global context by reducing spatial resolution.\nHowever, extending a pure transformer from image classi-\nﬁcation to a spatial location sensitive task of semantic seg-\nmentation is non-trivial. We show empirically that SETR\nnot only offers a new perspective in model design, but also\nachieves new state of the art on a number of benchmarks.\nThe following contributions are made in this paper: (1)\nWe reformulate the image semantic segmentation problem\nfrom a sequence-to-sequence learning perspective, offer-\ning an alternative to the dominating encoder-decoder FCN\nmodel design. (2) As an instantiation, we exploit the trans-\nformer framework to implement our fully attentive feature\nrepresentation encoder by sequentializing images. (3) To\nextensively examine the self-attentive feature presentations,\nwe further introduce three different decoder designs with\nvarying complexities. Extensive experiments show that\nour SETR models can learn superior feature representa-\ntions as compared to different FCNs with and without at-\ntention modules, yielding new state of the art on ADE20K\n(50.28%), Pascal Context (55.83%) and competitive results\non Cityscapes. Particularly, our entry is ranked the1st place\nin the highly competitive ADE20K test server leaderboard.\n2. Related work\nSemantic segmentation Semantic image segmentation has\nbeen signiﬁcantly boosted with the development of deep\nneural networks. By removing fully connected layers, the\nfully convolutional network (FCN) [36] is able to achieve\npixel-wise predictions. While the predictions of FCN are\nrelatively coarse, several CRF/MRF [6, 35, 62] based ap-\nproaches are developed to help reﬁne the coarse predictions.\nTo address the inherent tension between semantics and lo-\ncation [36], coarse and ﬁne layers need to be aggregated for\nboth the encoder and decoder. This leads to different vari-\nants of the encoder-decoder structures [2, 38, 42] for multi-\nlevel feature fusion.\nMany recent efforts have been focused on addressing\nthe limited receptive ﬁeld/context modeling problem in\nFCN. To enlarge the receptive ﬁeld, DeepLab [7] and Di-\nlation [53] introduce the dilated convolution. Alterna-\ntively, context modeling is the focus of PSPNet [60] and\nDeepLabV2 [9]. The former proposes the PPM module to\nobtain different region’s contextual information while the\nlatter develops ASPP module that adopts pyramid dilated\nconvolutions with different dilation rates. Decomposed\nlarge kernels [40] are also utilized for context capturing.\nRecently, attention based models are popular for capturing\nlong range context information. PSANet [61] develops the\npointwise spatial attention module for dynamically captur-\ning the long range context. DANet [18] embeds both spatial\nattention and channel attention. CCNet [26] alternatively\nfocuses on economizing the heavy computation budget in-\ntroduced by full spatial attention. DGMN [57] builds a dy-\nnamic graph message passing network for scene modeling\nand it can signiﬁcantly reduce the computational complex-\nity. Note that all these approaches are still based on FCNs\nwhere the feature encoding and extraction part are based on\nclassical ConvNets like VGG [43] and ResNet [20]. In this\nwork, we alternatively rethink the semantic segmentation\ntask from a different perspective.\nTransformer Transformer and self-attention models have\nrevolutionized machine translation and NLP [14,15,45,51].\nRecently, there are also some explorations for the usage\nof transformer structures in image recognition. Non-local\nnetwork [48] appends transformer style attention onto the\nconvolutional backbone. AANet [3] mixes convolution and\nself-attention for backbone training. LRNet [24] and stand-\nalone networks [41] explore local self-attention to avoid\nthe heavy computation brought by global self-attention.\nSAN [59] explores two types of self-attention modules.\nAxial-Attention [47] decomposes the global spatial atten-\ntion into two separate axial attentions such that the com-\nputation is largely reduced. Apart from these pure trans-\nformer based models, there are also CNN-transformer hy-\nbrid ones. DETR [5] and the following deformable version\n2\n... \n... ... \nLinear Projection\nTransformer Layer\nTransformer Layer\nMLP HeadDecoder \n24x\n+ + + + \nLayer Norm\nLayer Norm\nMLP\nMulti-Head\nAttention\nPosition\n Embedding\nPatch\n Embedding + \n(a)\nconv→2xreshape\n conv→2xconv→2xconv→2x\n(b)\nconv-conv-4x \nconv-4x \nZ 24 \nZ 18 \nZ 12 \nZ 6 \nreshape-conv \n(c)\nFigure 1. Schematic illustration of the proposedSEgmentation TRansformer(SETR) (a). We ﬁrst split an image into ﬁxed-size patches,\nlinearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. To\nperform pixel-wise segmentation, we introduce different decoder designs: (b) progressive upsampling (resulting in a variant called SETR-\nPUP); and (c) multi-level feature aggregation (a variant called SETR-MLA).\nutilize transformer for object detection where transformer\nis appended inside the detection head. STTR [32] and\nLSTR [34] adopt transformer for disparity estimation and\nlane shape prediction respectively. Most recently, ViT [17]\nis the ﬁrst work to show that a pure transformer based image\nclassiﬁcation model can achieve the state-of-the-art. It pro-\nvides direct inspiration to exploit a pure transformer based\nencoder design in a semantic segmentation model.\nThe most related work is [47] which also leverages at-\ntention for image segmentation. However, there are several\nkey differences. First, though convolution is completely re-\nmoved in [47] as in our SETR, their model still follows the\nconventional FCN design in that spatial resolution of feature\nmaps is reduced progressively. In contrast, our sequence-to-\nsequence prediction model keeps the same spatial resolution\nthroughout and thus represents a step-change in model de-\nsign. Second, to maximize the scalability on modern hard-\nware accelerators and facilitate easy-to-use, we stick to the\nstandard self-attention design. Instead, [47] adopts a spe-\ncially designed axial-attention [21] which is less scalable to\nstandard computing facilities. Our model is also superior in\nsegmentation accuracy (see Section 4).\n3. Method\n3.1. FCN-based semantic segmentation\nIn order to contrast with our new model design, let us\nﬁrst revisit the conventional FCN [36] for image semantic\nsegmentation. An FCN encoder consists of a stack of se-\nquentially connected convolutional layers. The ﬁrst layer\ntakes as input the image, denoted asH×W ×3 with H×W\nspecifying the image size in pixels. The input of subse-\nquent layer i is a three-dimensional tensor sized h ×w ×d,\nwhere h and w are spatial dimensions of feature maps, and\nd is the feature/channel dimension. Locations of the ten-\nsor in a higher layer are computed based on the locations of\ntensors of all lower layers they are connected to via layer-\nby-layer convolutions, which are deﬁned as their receptive\nﬁelds. Due to the locality nature of convolution operation,\nthe receptive ﬁeld increases linearly along the depth of lay-\ners, conditional on the kernel sizes (typically 3 ×3). As\na result, only higher layers with big receptive ﬁelds can\nmodel long-range dependencies in this FCN architecture.\nHowever, it is shown that the beneﬁts of adding more layers\nwould diminish rapidly once reaching certain depths [20].\nHaving limited receptive ﬁelds for context modeling is thus\nan intrinsic limitation of the vanilla FCN architecture.\nRecently, a number of state-of-the-art methods [25, 56,\n57] suggest that combing FCN with attention mechanism\nis a more effective strategy for learning long-range contex-\ntual information. These methods limit the attention learn-\ning to higher layers with smaller input sizes alone due to its\nquadratic complexity w.r.t. the pixel number of feature ten-\nsors. This means that dependency learning on lower-level\nfeature tensors is lacking, leading to sub-optimal represen-\ntation learning. To overcome this limitation, we propose\na pure self-attention based encoder, named SEgmentation\nTRansformers (SETR).\n3\n3.2. Segmentation transformers (SETR)\nImage to sequence SETR follows the same input-output\nstructure as in NLP for transformation between 1D se-\nquences. There thus exists a mismatch between 2D image\nand 1D sequence. Concretely, the Transformer, as depicted\nin Figure 1(a), accepts a 1D sequence of feature embeddings\nZ ∈RL×C as input, L is the length of sequence, C is the\nhidden channel size. Image sequentialization is thus needed\nto convert an input image x ∈RH×W×3 into Z.\nA straightforward way for image sequentialization is to\nﬂatten the image pixel values into a 1D vector with size of\n3HW . For a typical image sized at 480(H) ×480(W) ×3,\nthe resulting vector will have a length of 691,200. Given\nthe quadratic model complexity of Transformer, it is not\npossible that such high-dimensional vectors can be handled\nin both space and time. Therefore tokenizing every single\npixel as input to our transformer is out of the question.\nIn view of the fact that a typical encoder designed for\nsemantic segmentation would downsample a 2D image x ∈\nRH×W×3 into a feature map xf ∈ R\nH\n16 ×W\n16 ×C, we thus\ndecide to set the transformer input sequence length L as\nH\n16 ×W\n16 = HW\n256 . This way, the output sequence of the trans-\nformer can be simply reshaped to the target feature mapxf .\nTo obtain the HW\n256 -long input sequence, we divide an\nimage x ∈RH×W×3 into a grid of H\n16 ×W\n16 patches uni-\nformly, and then ﬂatten this grid into a sequence. By\nfurther mapping each vectorized patch p into a latent C-\ndimensional embedding space using a linear projection\nfunction f: p −→e ∈RC, we obtain a 1D sequence of\npatch embeddings for an image x. To encode the patch spa-\ncial information, we learn a speciﬁc embeddingpi for every\nlocation i which is added toei to form the ﬁnal sequence in-\nput E = {e1 + p1, e2 + p2, ··· , eL + pL}. This way, spa-\ntial information is kept despite the orderless self-attention\nnature of transformers.\nTransformer Given the 1D embedding sequence E as\ninput, a pure transformer based encoder is employed to\nlearn feature representations. This means each transformer\nlayer has a global receptive ﬁeld, solving the limited re-\nceptive ﬁeld problem of existing FCN encoder once and\nfor all. The transformer encoder consists of Le layers of\nmulti-head self-attention (MSA) and Multilayer Perceptron\n(MLP) blocks [46] (Figure 1(a)). At each layer l, the in-\nput to self-attention is in a triplet of (query, key, value)\ncomputed from the input Zl−1 ∈RL×C as:\nquery = Zl−1WQ, key = Zl−1WK, value = Zl−1WV , (1)\nwhere WQ/WK/WV ∈RC×d are the learnable parameters\nof three linear projection layers and d is the dimension of\n(query, key, value). Self-attention (SA) is then formu-\nlated as:\nSA(Zl−1) =Zl−1 + softmax(Zl−1WQ(ZWK)⊤\n√\nd\n)(Zl−1WV ).\n(2)\nMSA is an extension with m independent SA operations\nand project their concatenated outputs: MSA (Zl−1) =\n[SA1(Zl−1); SA2(Zl−1); ··· ; SAm(Zl−1)]WO, where\nWO ∈Rmd×C. d is typically set to C/m. The output of\nMSA is then transformed by an MLP block with residual\nskip as the layer output as:\nZl = MSA(Zl−1) +MLP (MSA(Zl−1)) ∈ RL×C. (3)\nNote, layer norm is applied before MSA and MLP\nblocks which is omitted for simplicity. We denote\n{Z1, Z2, ··· , ZLe}as the features of transformer layers.\n3.3. Decoder designs\nTo evaluate the effectiveness of SETR’s encoder feature\nrepresentations Z, we introduce three different decoder de-\nsigns to perform pixel-level segmentation. As the goal of\nthe decoder is to generate the segmentation results in the\noriginal 2D image space (H ×W), we need to reshape\nthe encoder’s features (that are used in the decoder), Z,\nfrom a 2D shape of HW\n256 ×C to a standard 3D feature map\nH\n16 ×W\n16 ×C. Next, we brieﬂy describe the three decoders.\n(1) Naive upsampling (Naive) This naive decoder ﬁrst\nprojects the transformer feature ZLe to the dimension of\ncategory number ( e.g., 19 for experiments on Cityscapes).\nFor this we adopt a simple 2-layer network with architec-\nture: 1 ×1 conv + sync batch norm (w/ ReLU) + 1 ×1\nconv. After that, we simply bilinearly upsample the out-\nput to the full image resolution, followed by a classiﬁcation\nlayer with pixel-wise cross-entropy loss. When this decoder\nis used, we denote our model as SETR-Na¨ıve.\n(2) Progressive UPsampling (PUP) Instead of one-step\nupscaling which may introduce noisy predictions, we con-\nsider a progressive upsampling strategy that alternates conv\nlayers and upsampling operations. To maximally mitigate\nthe adversarial effect, we restrict upsampling to 2×. Hence,\na total of 4 operations are needed for reaching the full res-\nolution from ZLe with size H\n16 ×W\n16 . More details of this\nprocess are given in Figure 1(b). When using this decoder,\nwe denote our model as SETR-PUP.\n(3) Multi-Level feature Aggregation (MLA)The third\ndesign is characterized by multi-level feature aggregation\n(Figure 1(c)) in similar spirit of feature pyramid network\n[27, 33]. However, our decoder is fundamentally differ-\nent because the feature representations Zl of every SETR’s\nlayer share the same resolution without a pyramid shape.\nSpeciﬁcally, we take as input the feature representations\n{Zm}(m ∈ {Le\nM , 2Le\nM , ··· , MLe\nM }) from M layers uni-\nformly distributed across the layers with step Le\nM to the de-\ncoder. M streams are then deployed, with each focusing on\n4\nModel T-layers Hidden size Att head\nT-Base 12 768 12\nT-Large 24 1024 16\nTable 1. Conﬁguration of Transformer backbone variants.\nMethod Pre Backbone #Params 40k 80k\nFCN [39] 1K R-101 68.59M 73.93 75.52\nSemantic FPN [39] 1K R-101 47.51M - 75.80\nHybrid-Base R T-Base 112.59M 74.48 77.36\nHybrid-Base 21K T-Base 112.59M 76.76 76.57\nHybrid-DeiT 21K T-Base 112.59M 77.42 78.28\nSETR-Na¨ıve 21K T-Large 305.67M 77.37 77.90\nSETR-MLA 21K T-Large 310.57M 76.65 77.24\nSETR-PUP 21K T-Large 318.31M 78.39 79.34\nSETR-PUP R T-Large 318.31M 42.27 -\nSETR-Na¨ıve-Base 21K T-Base 87.69M 75.54 76.25\nSETR-MLA-Base 21K T-Base 92.59M 75.60 76.87\nSETR-PUP-Base 21K T-Base 97.64M 76.71 78.02\nSETR-Na¨ıve-DeiT 1K T-Base 87.69M 77.85 78.66\nSETR-MLA-DeiT 1K T-Base 92.59M 78.04 78.98\nSETR-PUP-DeiT 1K T-Base 97.64M 78.79 79.45\nTable 2. Comparing SETR variants on different pre-training\nstrategies and backbones. All experiments are trained on\nCityscapes train ﬁne set with batch size 8, and evaluated using the\nsingle scale test protocol on the Cityscapes validation set in mean\nIoU (%) rate. “Pre” denotes the pre-training of transformer part.\n“R” means the transformer part is randomly initialized.\none speciﬁc selected layer. In each stream, we ﬁrst reshape\nthe encoder’s featureZl from a 2D shape of HW\n256 ×C to a\n3D feature map H\n16 ×W\n16 ×C. A 3-layer (kernel size 1 ×1,\n3 ×3, and 3 ×3) network is applied with the feature chan-\nnels halved at the ﬁrst and third layers respectively, and the\nspatial resolution upscaled 4×by bilinear operation after\nthe third layer. To enhance the interactions across differ-\nent streams, we introduce a top-down aggregation design\nvia element-wise addition after the ﬁrst layer. An additional\n3 ×3 conv is applied after the element-wise additioned fea-\nture. After the third layer, we obtain the fused feature from\nall the streams via channel-wise concatenation which is then\nbilinearly upsampled 4×to the full resolution. When using\nthis decoder, we denote our model as SETR-MLA.\n4. Experiments\n4.1. Experimental setup\nWe conduct experiments on three widely-used semantic\nsegmentation benchmark datasets.\nCityscapes [13] densely annotates 19 object categories in\nimages with urban scenes. It contains 5000 ﬁnely annotated\nimages, split into 2975, 500 and 1525 for training, valida-\ntion and testing respectively. The images are all captured at\na high resolution of 2048 ×1024. In addition, it provides\n19,998 coarse annotated images for model training.\nMethod Pre Backbone ADE20K Cityscapes\nFCN [39] 1K R-101 39.91 73.93\nFCN 21K R-101 42.17 76.38\nSETR-MLA 21K T-Large 48.64 76.65\nSETR-PUP 21K T-Large 48.58 78.39\nSETR-MLA-DeiT 1K T-Large 46.15 78.98\nSETR-PUP-DeiT 1K T-Large 46.24 79.45\nTable 3. Comparison to FCN with different pre-trainingwith\nsingle-scale inference on the ADE20K val and Cityscapes val set.\nADE20K [63] is a challenging scene parsing benchmark\nwith 150 ﬁne-grained semantic concepts. It contains 20210,\n2000 and 3352 images for training, validation and testing.\nPASCAL Context [37] provides pixel-wise semantic la-\nbels for the whole scene (both “thing” and “stuff” classes),\nand contains 4998 and 5105 images for training and valida-\ntion respectively. Following previous works, we evaluate on\nthe most frequent 59 classes and the background class (60\nclasses in total).\nImplementation details Following the default setting\n(e.g., data augmentation and training schedule) of public\ncodebase mmsegmentation [39], (i) we apply random resize\nwith ratio between 0.5 and 2, random cropping (768, 512\nand 480 for Cityscapes, ADE20K and Pascal Context re-\nspectively) and random horizontal ﬂipping during training\nfor all the experiments; (ii) We set batch size 16 and the to-\ntal iteration to 160,000 and 80,000 for the experiments on\nADE20K and Pascal Context. For Cityscapes, we set batch\nsize to 8 with a number of training schedules reported in Ta-\nble 2, 6 and 7 for fair comparison. We adopt a polynomial\nlearning rate decay schedule [60] and employ SGD as the\noptimizer. Momentum and weight decay are set to 0.9 and\n0 respectively for all the experiments on the three datasets.\nWe set initial learning rate 0.001 on ADE20K and Pascal\nContext, and 0.01 on Cityscapes.\nAuxiliary loss As [60] we also ﬁnd the auxiliary seg-\nmentation loss helps the model training. Each aux-\niliary loss head follows a 2-layer network. We add\nauxiliary losses at different Transformer layers: SETR-\nNa¨ıve (Z10, Z15, Z20), SETR- PUP (Z10, Z15, Z20, Z24),\nSETR-MLA (Z6, Z12, Z18, Z24). Both auxiliary loss and\nmain loss heads are applied concurrently.\nMulti-scale test We use the default settings ofmmsegmen-\ntation [39]. Speciﬁcally, the input image is ﬁrst scaled to\na uniform size. Multi-scale scaling and random horizontal\nﬂip are then performed on the image with a scaling factor\n(0.5, 0.75, 1.0, 1.25, 1.5, 1.75). Sliding window is adopted\nfor test (e.g., 480 ×480 for Pascal Context). If the shorter\nside is smaller than the size of the sliding window, the im-\nage is scaled with its shorter side to the size of the sliding\n5\nFigure 2. Qualitative results on ADE20K:SETR (right column)\nvs. dilated FCN baseline (left column) in each pair. Best viewed\nin color and zoom in.\nMethod Pre Backbone #Params mIoU\nFCN (160k, SS) [39] 1K ResNet-101 68.59M 39.91\nFCN (160k, MS) [39] 1K ResNet-101 68.59M 41.40\nCCNet [25] 1K ResNet-101 - 45.22\nStrip pooling [23] 1K ResNet-101 - 45.60\nDANet [18] 1K ResNet-101 69.0M 45.30\nOCRNet [54] 1K ResNet-101 71.0M 45.70\nUperNet [49] 1K ResNet-101 86.0M 44.90\nDeeplab V3+ [11] 1K ResNet-101 63.0M 46.40\nSETR-Na¨ıve(160k, SS) 21K T-Large 305.67M 48.06\nSETR-Na¨ıve(160k, MS) 21K T-Large 305.67M 48.80\nSETR-PUP (160k, SS) 21K T-Large 318.31M 48.58\nSETR-PUP (160k, MS) 21K T-Large 318.31M 50.09\nSETR-MLA (160k, SS) 21K T-Large 310.57M 48.64\nSETR-MLA (160k, MS) 21K T-Large 310.57M 50.28\nSETR-PUP-DeiT (160k, SS) 1K T-Base 97.64M 46.34\nSETR-PUP-DeiT (160k, MS) 1K T-Base 97.64M 47.30\nSETR-MLA-DeiT (160k, SS) 1K T-Base 92.59M 46.15\nSETR-MLA-DeiT (160k, MS) 1K T-Base 92.59M 47.71\nTable 4. State-of-the-art comparison on the ADE20K dataset.\nPerformances of different model variants are reported. SS: Single-\nscale inference. MS: Multi-scale inference.\nwindow (e.g., 480) while keeping the aspect ratio. Synchro-\nnized BN is used in decoder and auxiliary loss heads. For\ntraining simplicity, we do not adopt the widely-used tricks\nsuch as OHEM [55] loss in model training.\nBaselines We adopt dilated FCN [36] and Semantic\nFPN [27] as baselines with their results taken from [39].\nOur models and the baselines are trained and tested in the\nsame settings for fair comparison. In addition, state-of-the-\nart models are also compared. Note that the dilated FCN is\nwith output stride 8 and we use output stride 16 in all our\nmodels due to GPU memory constrain.\nSETR variants Three variants of our model with differ-\nent decoder designs (see Sec. 3.3), namely SETR- Na¨ıve,\nSETR-PUP and SETR- MLA. Besides, we use two vari-\nants of the encoder “T-Base” and “T-Large” with 12 and 24\nlayers respectively (Table 1). Unless otherwise speciﬁed,\nwe use “T-Large” as the encoder for SETR- Na¨ıve, SETR-\nPUP and SETR-MLA. We denote SETR-Na¨ıve-Baseas the\nmodel utilizing “T-Base” in SETR-Na¨ıve.\nThough designed as a model with a pure transformer\nencoder, we also set a hybrid baseline Hybrid by using a\nFigure 3. Qualitative results on Pascal Context:SETR (right\ncolumn) vs. dilated FCN baseline (left column) in each pair. Best\nviewed in color and zoom in.\nMethod Pre Backbone mIoU\nFCN (80k, SS) [39] 1K ResNet-101 44.47\nFCN (80k, MS) [39] 1K ResNet-101 45.74\nDANet [18] 1K ResNet-101 52.60\nEMANet [31] 1K ResNet-101 53.10\nSVCNet [16] 1K ResNet-101 53.20\nStrip pooling [23] 1K ResNet-101 54.50\nGFFNet [30] 1K ResNet-101 54.20\nAPCNet [19] 1K ResNet-101 54.70\nSETR-Na¨ıve(80k, SS) 21K T-Large 52.89\nSETR-Na¨ıve(80k, MS) 21K T-Large 53.61\nSETR-PUP (80k, SS) 21K T-Large 54.40\nSETR-PUP (80k, MS) 21K T-Large 55.27\nSETR-MLA (80k, SS) 21K T-Large 54.87\nSETR-MLA (80k, MS) 21K T-Large 55.83\nSETR-PUP-DeiT (80k, SS) 1K T-Base 52.71\nSETR-PUP-DeiT (80k, MS) 1K T-Base 53.71\nSETR-MLA-DeiT (80k, SS) 1K T-Base 52.91\nSETR-MLA-DeiT (80k, MS) 1K T-Base 53.74\nTable 5. State-of-the-art comparison on the Pascal Context\ndataset. Performances of different model variants are reported.\nSS: Single-scale inference. MS: Multi-scale inference.\nResNet-50 based FCN encoder and feeding its output fea-\nture into SETR. To cope with the GPU memory constraint\nand for fair comparison, we only consider ‘T-Base” in Hy-\nbrid and set the output stride of FCN to 1/16. That is, Hy-\nbrid is a combination of ResNet-50 and SETR-Na¨ıve-Base.\nPre-training We use the pre-trained weights provided by\nViT [17] or DeiT [44] to initialize all the transformer lay-\ners and the input linear projection layer in our model. We\ndenote SETR-Na¨ıve-DeiT as the model utilizing DeiT [44]\npre-training in SETR- Na¨ıve-Base. All the layers without\npre-training are randomly initialized. For the FCN en-\ncoder of Hybrid, we use the initial weights pre-trained on\nImageNet-1k. For the transformer part, we use the weights\npre-trained by ViT [17], DeiT [44] or randomly initialized.\nWe use patch size 16 ×16 for all the experiments. We\nperform 2D interpolation on the pre-trained position em-\nbeddings, according to their location in the original image\nfor different input size ﬁne-tuning.\nEvaluation metric Following the standard evaluation pro-\ntocol [13], the metric of mean Intersection over Union\n(mIoU) averaged over all classes is reported. For ADE20K,\n6\nFigure 4. Qualitative results on Cityscapes:SETR (right column) vs. dilated FCN baseline (left column) in each pair. Best viewed in\ncolor and zoom in.\nMethod Backbone mIoU\nFCN (40k, SS) [39] ResNet-101 73.93\nFCN (40k, MS) [39] ResNet-101 75.14\nFCN (80k, SS) [39] ResNet-101 75.52\nFCN (80k, MS) [39] ResNet-101 76.61\nPSPNet [60] ResNet-101 78.50\nDeepLab-v3 [10] (MS) ResNet-101 79.30\nNonLocal [48] ResNet-101 79.10\nCCNet [25] ResNet-101 80.20\nGCNet [4] ResNet-101 78.10\nAxial-DeepLab-XL [47] (MS) Axial-ResNet-XL 81.10\nAxial-DeepLab-L [47] (MS) Axial-ResNet-L 81.50\nSETR-PUP (40k, SS) T-Large 78.39\nSETR-PUP (40k, MS) T-Large 81.57\nSETR-PUP (80k, SS) T-Large 79.34\nSETR-PUP (80k, MS) T-Large 82.15\nTable 6. State-of-the-art comparison on the Cityscapes valida-\ntion set. Performances of different training schedules ( e.g., 40k\nand 80k) are reported. SS: Single-scale inference. MS: Multi-\nscale inference.\nadditionally pixel-wise accuracy is reported following the\nexisting practice.\n4.2. Ablation studies\nTable 2 and 3 show ablation studies on(a) different vari-\nants of SETR on various training schedules, (b) compari-\nson to FCN [39] and Semantic FPN [39], (c) pre-training\non different data, (d) comparison with Hybrid, (e) com-\npare to FCN with different pre-training. Unless otherwise\nspeciﬁed, all experiments on Table 2 and 3 are trained on\nCityscapes train ﬁne set with batch size 8, and evaluated\nusing the single scale test protocol on the Cityscapes vali-\ndation set in mean IoU (%) rate. Experiments on ADE20K\nalso follow the single scale test protocol.\nFrom Table 2, we can make the following observations:\n(i) Progressively upsampling the feature maps, SETR-\nMethod Backbone mIoU\nPSPNet [60] ResNet-101 78.40\nDenseASPP [50] DenseNet-161 80.60\nBiSeNet [52] ResNet-101 78.90\nPSANet [61] ResNet-101 80.10\nDANet [18] ResNet-101 81.50\nOCNet [55] ResNet-101 80.10\nCCNet [25] ResNet-101 81.90\nAxial-DeepLab-L [47] Axial-ResNet-L 79.50\nAxial-DeepLab-XL [47] Axial-ResNet-XL 79.90\nSETR-PUP (100k) T-Large 81.08\nSETR-PUP‡ T-Large 81.64\nTable 7. Comparison on the Cityscapes test set.‡: trained on\nﬁne and coarse annotated data.\nPUP achieves the best performance among all the vari-\nants on Cityscapes. One possible reason for inferior per-\nformance of SETR- MLA is that the feature outputs of dif-\nferent transformer layers do not have the beneﬁts of reso-\nlution pyramid as in feature pyramid network (FPN) (see\nFigure 5). However, SETR- MLA performs slightly better\nthan SETR-PUP, and much superior to the variant SETR-\nNa¨ıve that upsamples the transformers output feature by\n16×in one-shot, on ADE20K val set (Table 3 and 4). (ii)\nThe variants using “T-Large” (e.g., SETR-MLA and SETR-\nNa¨ıve) are superior to their “T-Base” counterparts, i.e.,\nSETR-MLA-Base and SETR-Na¨ıve-Base, as expected. (iii)\nWhile our SETR- PUP-Base (76.71) performs worse than\nHybrid-Base (76.76), it shines (78.02) when training with\nmore iterations (80k). It suggests that FCN encoder design\ncan be replaced in semantic segmentation, and further con-\nﬁrms the effectiveness of our model.(iv) Pre-training is crit-\nical for our model. Randomly initialized SETR- PUP only\ngives 42.27% mIoU on Cityscapes. Model pre-trained with\nDeiT [44] on ImageNet-1K gives the best performance on\nCityscapes, slightly better than the counterpart pre-trained\n7\nFigure 5. Visualization of output feature of layerZ1, Z9, Z17, Z24\nof SETR trained on Pascal Context. Best viewed in color.\nFigure 6. Examples of attention maps from SETR trained on Pas-\ncal Context.\nwith ViT [17] on ImageNet-21K. (v) To study the power\nof pre-training and further verify the effectiveness of our\nproposed approach, we conduct the ablation study on the\npre-training strategy in Table 3. For fair comparison with\nthe FCN baseline, we ﬁrst pre-train a ResNet-101 on the\nImagenet-21k dataset with a classiﬁcation task and then\nadopt the pre-trained weights for a dilated FCN training for\nthe semantic segmentation task on ADE20K or Cityscapes.\nTable 3 shows that with ImageNet-21k pre-training FCN\nbaseline experienced a clear improvement over the variant\npre-trained on ImageNet-1k. However, our method out-\nperforms the FCN counterparts by a large margin, veri-\nfying that the advantage of our approach largely comes\nfrom the proposed sequence-to-sequence modeling strategy\nrather than bigger pre-training data.\n4.3. Comparison to state-of-the-art\nResults on ADE20K Table 4 presents our results on\nthe more challenging ADE20K dataset. Our SETR-\nMLA achieves superior mIoU of 48.64% with single-scale\n(SS) inference. When multi-scale inference is adopted, our\nmethod achieves a new state of the art with mIoU hitting\n50.28%. Figure 2 shows the qualitative results of our model\nand dilated FCN on ADE20K. When training a single model\non the train+validation set with the default 160,000 itera-\ntions, our method ranks 1st place in the highly competitive\nADE20K test server leaderboard.\nResults on Pascal ContextTable 5 compares the segmen-\ntation results on Pascal Context. Dilated FCN with the\nResNet-101 backbone achieves a mIoU of 45.74%. Us-\ning the same training schedule, our proposed SETR sig-\nniﬁcantly outperforms this baseline, achieving mIoU of\n54.40% (SETR- PUP) and 54.87% (SETR- MLA). SETR-\nMLA further improves the performance to 55.83% when\nmulti-scale (MS) inference is adopted, outperforming the\nnearest rival APCNet with a clear margin. Figure 3 gives\nsome qualitative results of SETR and dilated FCN. Fur-\nther visualization of the learned attention maps in Figure 6\nshows that SETR can attend to semantically meaningful\nforeground regions, demonstrating its ability to learn dis-\ncriminative feature representations useful for segmentation.\nResults on CityscapesTables 6 and 7 show the compara-\ntive results on the validation and test set of Cityscapes re-\nspectively. We can see that our model SETR- PUP is su-\nperior to FCN baselines, and FCN plus attention based ap-\nproaches, such as Non-local [48] and CCNet [25]; and its\nperformance is on par with the best results reported so far.\nOn this dataset we can now compare with the closely related\nAxial-DeepLab [12, 47] which aims to use an attention-\nalone model but still follows the basic structure of FCN.\nNote that Axial-DeepLab sets the same output stride 16 as\nours. However, its full input resolution ( 1024 ×2048) is\nmuch larger than our crop size 768 ×768, and it runs more\nepochs (60k iteration with batch size 32) than our setting\n(80k iterations with batch size 8). Nevertheless, our model\nis still superior to Axial-DeepLab when multi-scale infer-\nence is adopted on Cityscapes validation set. Using the ﬁne\nset only, our model (trained with 100k iterations) outper-\nforms Axial-DeepLab-XL with a clear margin on the test\nset. Figure 4 shows the qualitative results of our model and\ndilated FCN on Cityscapes.\n5. Conclusion\nIn this work, we have presented an alternative perspec-\ntive for semantic segmentation by introducing a sequence-\nto-sequence prediction framework. In contrast to existing\nFCN based methods that enlarge the receptive ﬁeld typi-\ncally with dilated convolutions and attention modules at the\ncomponent level, we made a step change at thearchitectural\nlevel to completely eliminate the reliance on FCN and ele-\ngantly solve the limited receptive ﬁeld challenge. We imple-\nmented the proposed idea with Transformers that can model\nglobal context at every stage of feature learning. Along\nwith a set of decoder designs in different complexity, strong\nsegmentation models are established with none of the bells\nand whistles deployed by recent methods. Extensive ex-\nperiments demonstrate that our models set new state of the\nart on ADE20, Pascal Context and competitive results on\nCityscapes. Encouragingly, our method is ranked the 1st\nplace in the highly competitive ADE20K test server leader-\nboard on the day of submission.\n8\nAcknowledgments\nThis work was supported by Shanghai Municipal Sci-\nence and Technology Major Project (No.2018SHZDZX01),\nZJLab, and Shanghai Center for Brain Science and Brain-\nInspired Technology.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention\nﬂow in transformers. arXiv preprint, 2020. 11\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegnet: A deep convolutional encoder-decoder architecture\nfor image segmentation. TPAMI, 2017. 1, 2\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In ICCV, 2019. 2\n[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In ICCV workshops, 2019. 7\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 2\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Semantic image seg-\nmentation with deep convolutional nets and fully connected\nCRFs. In ICLR, 2015. 2\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Semantic image seg-\nmentation with deep convolutional nets and fully connected\nCRFs. In ICLR, 2015. 2\n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. TPAMI, 2018. 1\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected crfs. TPAMI, 2018. 2\n[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. arXiv preprint, 2017. 7\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 6\n[12] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-deeplab: A simple, strong, and fast baseline for\nbottom-up panoptic segmentation. In CVPR, 2020. 8\n[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016. 5, 6\n[14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc V . Le, and Ruslan Salakhutdinov. Transformer-XL:\nAttentive language models beyond a ﬁxed-length context. In\nACL, 2019. 2\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n2\n[16] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and\nGang Wang. Semantic correlation promoted shape-variant\ncontext for segmentation. In CVPR, 2019. 6\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 2, 3,\n6, 8\n[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu.\nDual attention network for scene segmentation. In CVPR,\n2019. 2, 6, 7\n[19] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu\nQiao. Adaptive pyramid context network for semantic seg-\nmentation. In CVPR, 2019. 6\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 2, 3\n[21] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint, 2019. 3\n[22] Matthias Holschneider, Richard Kronland-Martinet, Jean\nMorlet, and Ph Tchamitchian. A real-time algorithm for\nsignal analysis with the help of the wavelet transform. In\nWavelets, 1990. 1\n[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng.\nStrip pooling: Rethinking spatial pooling for scene parsing.\nIn CVPR, 2020. 6\n[24] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 2\n[25] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 1, 3, 6,\n7, 8\n[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 2\n[27] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll´ar. Panoptic feature pyramid networks. In CVPR, 2019.\n4, 6\n[28] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping\nShi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong. Improv-\ning semantic segmentation via decoupled body and edge su-\npervision. In ECCV, 2020. 1\n[29] Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan\nYang, and Yunhai Tong. Global aggregation then local dis-\ntribution in fully convolutional networks. In BMVC, 2019.\n1\n[30] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, and\nKuiyuan Yang. Gff: Gated fully fusion for semantic seg-\nmentation. In AAAI, 2020. 6\n9\n[31] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen\nLin, and Hong Liu. Expectation-maximization attention net-\nworks for semantic segmentation. In CVPR, 2019. 6\n[32] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H\nTaylor, and Mathias Unberath. Revisiting stereo depth esti-\nmation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint, 2020. 3\n[33] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017. 4\n[34] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-\nto-end lane shape prediction with transformers. In WACV,\n2020. 3\n[35] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and\nXiaoou Tang. Semantic image segmentation via deep parsing\nnetwork. In ICCV, 2015. 2\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nCVPR, 2015. 1, 2, 3, 6\n[37] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan Yuille. The role of context for object detection and se-\nmantic segmentation in the wild. In CVPR, 2014. 5\n[38] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmentation.\nIn ICCV, 2015. 2\n[39] OpenMMLab. mmsegmentation. https://github.\ncom/open-mmlab/mmsegmentation, 2020. 5, 6, 7\n[40] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and\nJian Sun. Large kernel matters — improve semantic seg-\nmentation by global convolutional network. In CVPR, 2017.\n1, 2\n[41] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019. 2\n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nMICCAI, 2015. 2\n[43] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015. 2\n[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint, 2020. 6, 7\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 2\n[46] Petar Veli ˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. In ICLR, 2018. 4\n[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 1, 2, 3, 7, 8\n[48] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 1, 2, 7,\n8\n[49] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Uniﬁed perceptual parsing for scene understand-\ning. In ECCV, 2018. 6\n[50] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan\nYang. Denseaspp for semantic segmentation in street scenes.\nIn CVPR, 2018. 1, 7\n[51] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V . Le. XLNet: General-\nized autoregressive pretraining for language understanding.\nIn NeurIPS, 2019. 2\n[52] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. Bisenet: Bilateral segmenta-\ntion network for real-time semantic segmentation. In ECCV,\n2018. 7\n[53] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-\ntion by dilated convolutions. ICLR, 2016. 2\n[54] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-\ncontextual representations for semantic segmentation. In\nECCV, 2020. 6\n[55] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-\nwork for scene parsing. arXiv preprint, 2018. 6, 7\n[56] Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yun-\nhai Tong, and Philip HS Torr. Dual graph convolutional net-\nwork for semantic segmentation. In BMVC, 2019. 3\n[57] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dy-\nnamic graph message passing networks. In CVPR, 2020. 1,\n2, 3\n[58] Richard Zhang. Making convolutional networks shift-\ninvariant again. In ICML, 2019. 1\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020. 2\n[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017. 1, 2, 5, 7\n[61] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen\nChange Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise\nspatial attention network for scene parsing. In ECCV, 2018.\n2, 7\n[62] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-\nParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang\nHuang, and Philip H. S. Torr. Conditional random ﬁelds as\nrecurrent neural networks. In ICCV, 2015. 2\n[63] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Semantic understanding of\nscenes through the ade20k dataset. arXiv preprint, 2016. 5\n10\nAppendix\nA. Visualizations\nPosition embedding Visualization of the learned position\nembedding in Figure 7 shows that the model learns to en-\ncode distance within the image in the similarity of position\nembeddings.\nFeatures Figure 9 shows the feature visualization of our\nSETR-PUP. For the encoder, 24 output features from the 24\ntransformer layers namely Z1 −Z24 are collected. Mean-\nwhile, 5 features ( U1 −U5) right after each bilinear inter-\npolation in the decoder head are visited.\nAttention maps Attention maps (Figure 10) in each trans-\nformer layer catch our interest. There are 16 heads and 24\nlayers in T-large. Similar to [1], a recursion perspective into\nthis problem is applied. Figure 8 shows the attention maps\nof different selected spatial points (red).\nFigure 7. Similarity of position embeddings of SETR-PUP trained\non Pascal Context. Tiles show the cosine similarity between the\nposition embedding of the patch with the indicated row and col-\numn and the position embeddings of all other patches.\nFigure 8. The ﬁrst and third columns show images from Pascal\nContext. The second and fourth columns illustrate the attention\nmap of the picked points (red).\n11\nFigure 9. Visualization of output feature of layer Z1 −Z24 and U1 −U5 of SETR-PUP trained on Pascal Context. Best view in color.\nFirst row:The input image. Second row:Layer Z1-Z12. Third row:Layer Z13-Z24. Fourth row:Layer U1 −U5.\nFigure 10. More examples of attention maps from SETR-PUP trained on Pascal Context.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7441865801811218
    },
    {
      "name": "Segmentation",
      "score": 0.7257722020149231
    },
    {
      "name": "Encoder",
      "score": 0.6938841342926025
    },
    {
      "name": "Transformer",
      "score": 0.6397437453269958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6023629307746887
    },
    {
      "name": "Pascal (unit)",
      "score": 0.5666319727897644
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4163432717323303
    },
    {
      "name": "Computer vision",
      "score": 0.4046400189399719
    },
    {
      "name": "Engineering",
      "score": 0.06579798460006714
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}