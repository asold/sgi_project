{
  "title": "General Video Game AI: A Multitrack Framework for Evaluating Agents, Games, and Content Generation Algorithms",
  "url": "https://openalex.org/W2789184669",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2229561851",
      "name": "Diego Pérez Liébana",
      "affiliations": [
        "Queen Mary University of London"
      ]
    },
    {
      "id": "https://openalex.org/A2125685373",
      "name": "Jialin Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2004892426",
      "name": "Ahmed Khalifa",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2580756099",
      "name": "Raluca D. Gaina",
      "affiliations": [
        "Queen Mary University of London"
      ]
    },
    {
      "id": "https://openalex.org/A319365700",
      "name": "Julian Togelius",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2104275215",
      "name": "Simon M. Lucas",
      "affiliations": [
        "Queen Mary University of London"
      ]
    },
    {
      "id": "https://openalex.org/A2229561851",
      "name": "Diego Pérez Liébana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125685373",
      "name": "Jialin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2004892426",
      "name": "Ahmed Khalifa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2580756099",
      "name": "Raluca D. Gaina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A319365700",
      "name": "Julian Togelius",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104275215",
      "name": "Simon M. Lucas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2903764415",
    "https://openalex.org/W2913821476",
    "https://openalex.org/W1925516709",
    "https://openalex.org/W7046902500",
    "https://openalex.org/W2588233657",
    "https://openalex.org/W2559225160",
    "https://openalex.org/W2911683482",
    "https://openalex.org/W2602193277",
    "https://openalex.org/W2964088926",
    "https://openalex.org/W2589169860",
    "https://openalex.org/W1915195482",
    "https://openalex.org/W2591200008",
    "https://openalex.org/W2589349991",
    "https://openalex.org/W1910077957",
    "https://openalex.org/W2090255638",
    "https://openalex.org/W1944144827",
    "https://openalex.org/W6713499454",
    "https://openalex.org/W2790436215",
    "https://openalex.org/W2964262220",
    "https://openalex.org/W2767707542",
    "https://openalex.org/W2902987394",
    "https://openalex.org/W2896539953",
    "https://openalex.org/W6733698688",
    "https://openalex.org/W6740208863",
    "https://openalex.org/W2896794443",
    "https://openalex.org/W2037790718",
    "https://openalex.org/W6639581174",
    "https://openalex.org/W2569196563",
    "https://openalex.org/W2028208918",
    "https://openalex.org/W6639645198",
    "https://openalex.org/W2125693710",
    "https://openalex.org/W1993158562",
    "https://openalex.org/W2602390993",
    "https://openalex.org/W2582685648",
    "https://openalex.org/W959464167",
    "https://openalex.org/W2590143935",
    "https://openalex.org/W2765689447",
    "https://openalex.org/W2590481545",
    "https://openalex.org/W2963672598",
    "https://openalex.org/W1893470618",
    "https://openalex.org/W6656872355",
    "https://openalex.org/W2766774971",
    "https://openalex.org/W6741437182",
    "https://openalex.org/W1923890037",
    "https://openalex.org/W2099295505",
    "https://openalex.org/W6744645968",
    "https://openalex.org/W2896966958",
    "https://openalex.org/W2963260084",
    "https://openalex.org/W2750681837",
    "https://openalex.org/W2897422953",
    "https://openalex.org/W6727060998",
    "https://openalex.org/W1899406468",
    "https://openalex.org/W2963987545",
    "https://openalex.org/W6747825032",
    "https://openalex.org/W2896445362",
    "https://openalex.org/W6754521519",
    "https://openalex.org/W6756394905",
    "https://openalex.org/W6780559895",
    "https://openalex.org/W1849147390",
    "https://openalex.org/W6674622777",
    "https://openalex.org/W6754676675",
    "https://openalex.org/W2150468603",
    "https://openalex.org/W6721634521",
    "https://openalex.org/W2767922802",
    "https://openalex.org/W6723310229",
    "https://openalex.org/W2766162456",
    "https://openalex.org/W2766680193",
    "https://openalex.org/W2765848027",
    "https://openalex.org/W6731878110",
    "https://openalex.org/W2790464696",
    "https://openalex.org/W2765692642",
    "https://openalex.org/W6740701745",
    "https://openalex.org/W2867408011",
    "https://openalex.org/W2598636128",
    "https://openalex.org/W2590549092",
    "https://openalex.org/W6750238590",
    "https://openalex.org/W2897242117",
    "https://openalex.org/W788042400",
    "https://openalex.org/W2757176154",
    "https://openalex.org/W2794293226",
    "https://openalex.org/W2490603777",
    "https://openalex.org/W3038038686",
    "https://openalex.org/W2025400977",
    "https://openalex.org/W2902221828",
    "https://openalex.org/W2522413982",
    "https://openalex.org/W2891945572",
    "https://openalex.org/W2891790128",
    "https://openalex.org/W2739691807",
    "https://openalex.org/W3103780890",
    "https://openalex.org/W2727836376",
    "https://openalex.org/W2574252542",
    "https://openalex.org/W2611473408"
  ],
  "abstract": "General Video Game Playing (GVGP) aims at designing an agent that is capable of playing multiple video games with no human intervention. In 2014, The General Video Game AI (GVGAI) competition framework was created and released with the purpose of providing researchers a common open-source and easy to use platform for testing their AI methods with potentially infinity of games created using Video Game Description Language (VGDL). The framework has been expanded into several tracks during the last few years to meet the demand of different research directions. The agents are required either to play multiple unknown games with or without access to game simulations, or to design new game levels or rules. This survey paper presents the VGDL, the GVGAI framework, existing tracks, and reviews the wide use of GVGAI framework in research, education and competitions five years after its birth. A future plan of framework improvements is also described.",
  "full_text": "General Video Game AI: a Multi-Track Framework\nfor Evaluating Agents, Games and Content\nGeneration Algorithms\nDiego Perez-Liebana, Member, IEEE, Jialin Liu*, Member, IEEE, Ahmed Khalifa, Raluca D. Gaina, Student\nMember, IEEE, Julian Togelius, Member, IEEE, and Simon M. Lucas, Senior Member, IEEE\nAbstract—General Video Game Playing (GVGP) aims at\ndesigning an agent that is capable of playing multiple video\ngames with no human intervention. In 2014, The General Video\nGame AI (GVGAI) competition framework was created and\nreleased with the purpose of providing researchers a common\nopen-source and easy to use platform for testing their AI\nmethods with potentially inﬁnity of games created using Video\nGame Description Language (VGDL). The framework has been\nexpanded into several tracks during the last few years to meet the\ndemand of different research directions. The agents are required\neither to play multiple unknown games with or without access\nto game simulations, or to design new game levels or rules.\nThis survey paper presents the VGDL, the GVGAI framework,\nexisting tracks, and reviews the wide use of GVGAI framework\nin research, education and competitions ﬁve years after its birth.\nA future plan of framework improvements is also described.\nIndex Terms—Computational intelligence, artiﬁcial intelli-\ngence, games, general video game playing, GVGAI, video game\ndescription language\nI. I NTRODUCTION\nGame-based benchmarks and competitions have been used\nfor testing artiﬁcial intelligence capabilities since the inception\nof the research ﬁeld. Since the early 2000s a number of\ncompetitions and benchmarks based on video games have\nsprung up. So far, most competitions and game benchmarks\nchallenge the agents to play a single game, which leads to\nan overspecialization, or overﬁtting, of agents to individual\ngames. This is reﬂected in the outcome of individual com-\npetitions – for example, over the more than ﬁve years the\nSimulated Car Racing Competition [1] 1 ran, submitted car\ncontrollers got better at completing races fast, but incorporated\nmore and more game-speciﬁc engineering and arguably less\nof general AI and machine learning algorithms. Therefore,\nthis trend threatens to negate the usefulness of game-based\nD. Perez-Liebana, R. Gaina and S. M. Lucas are with the Department\nof Electrical Engineering and Computer Engineering (EECS), Queen Mary\nUniversity of London, London E1 4NS, UK.\nJ. Liu is with the Shenzhen Key Laboratory of Computational Intelligence,\nUniversity Key Laboratory of Evolving Intelligent Systems of Guangdong\nProvince, Department of Computer Science and Engineering, Southern Uni-\nversity of Science and Technology, Shenzhen 518055, China. She was with\nEECS, Queen Mary University of London, London E1 4NS, UK.\n*J. Liu is the corresponding author.\nA. Khalifa and J. Togelius are with the Department of Computer Science\nand Engineering, New York University, New York 11201, USA.\n1We cite Yannakakis [1] and Russell [2] as standard references for Games\nand AI (respectively) to reduce the number of non GVGP references.\nAI competitions for spurring and testing the development of\nstronger and more general AI.\nThe General Video Game AI (GVGAI) competition [3] was\nfounded on the belief that the best way to stop AI researchers\nfrom relying on game-speciﬁc engineering in their agents\nis to make it impossible. Researchers would develop their\nagents without knowing what games they will be playing, and\nafter submitting their agents to the competition all agents are\nevaluated using an unseen set of games. Every competition\nevent requires the design of a new set of games, as reusing\nprevious games would make this task impossible.\nWhile the GVGAI competition was initially focused on\nbenchmarking AI algorithms for playing the game, the compe-\ntition and its associated software has multiple uses. In addition\nto the competition tracks dedicated to game-playing agents,\nthere are now tracks focused on generating game levels or\nrules. There is also the potential to use GVGAI for game\nprototyping, with a rapidly growing body of research using\nthis framework for everything from building mixed-initiative\ndesign tools to demonstrating new concepts in game design.\nThe objective of this paper is to provide an overview of\nthe different efforts from the community on the use of the\nGVGAI framework (and, by extension, of its competition) for\nGeneral Game Artiﬁcial Intelligence. This overview aims at\nidentifying the main approaches that have been used so far for\nagent AI and procedural content generation (PCG), in order to\ncompare them and recognize possible lines of future research\nwithin this ﬁeld. The paper starts with a brief overview of the\nframework and the different competition tracks, for context\nand completeness, which summarizes work published in other\npapers by the same authors. The bulk of the paper is centered\nin the next few sections, which are devoted to discussing\nthe various kinds of AI methods that have been used in the\nsubmissions to each track. Special consideration is given to\nthe single-player planning track, as it has existed for longest\nand received the most submissions up to date. This is followed\nby a section cataloguing some of the non-competition research\nuses of the GVGAI software. The ﬁnal few sections provide a\nview on the future use and development of the framework and\ncompetition: how it can be used in teaching, open research\nproblems (speciﬁcally related to the planning tracks), and the\nfuture evolution of the competition and framework itself.\narXiv:1802.10363v4  [cs.AI]  22 Feb 2019\nFig. 1. Examples of VGDL games. From top to bottom, left to right:\nButterﬂies, Escape, Crossﬁre and Wait for Breakfast.\nII. T HE GVGAI F RAMEWORK\nEbner et al. [4] and Levine et al. [5] ﬁrst described the need\nand interest for such a framework that could accommodate a\ncompetition for researchers to tackle the challenge of General\nVideo Game Playing (GVGP). The authors proposed the idea\nof the Video Game Description Language (VGDL), which\nlater was developed by Schaul [6], [7] in a Python framework\nfor model-based learning and released the ﬁrst game engine\nin 2013. Years later, Perez-Liebana et al. [3] implemented a\nversion of Schaul’s initial framework in Java and organized\nthe ﬁrst General Video Game AI (GVGAI) competition in\n2014 [8], which employed games developed in VGDL. In the\nfollowing years, this framework was extended to accommodate\ntwo-player games [9], [10], level [11], rule [12] generation,\nand real-world physics games [13]. These competition tracks\naccumulate hundreds of submissions. Furthermore, the GV-\nGAI Framework and Competition have been used as tools for\nresearch and education around the globe, including their usage\nin taught modules, MSc and PhD dissertation projects (see\nSection XI).\nVGDL is a text description language that allows for the\ndeﬁnition of two-dimensional, arcade, grid-based physics and\n(generally) stochastic games and levels. Originally designed\nfor single-player games, the language now admits 2-player\nchallenges. VGDL permits the deﬁnition of sprites (objects\nwithin the game) and their properties (from speed and behavior\nto images or animations) in the Sprite Set. Thus this set\ndeﬁnes the type of sprites that can take part in the game.\nTheir interactions are regulated in the Interaction Set, which\ndeﬁnes the rules that govern the effects of two sprites colliding\nwith each other. This includes the speciﬁcation of score for\nthe games. The Termination Set deﬁnes how the game ends,\nwhich could happen due to the presence or absence of certain\nsprites or due to timers running out. Levels in which the games\ncan be played are deﬁned also in text ﬁles. Each character\ncorresponds to one or more sprites deﬁned in the Sprite\nSet, and the correspondence between sprites and characters\nis established in the Mapping Set. At the moment of writing,\nthe framework counts on 120 single-player and 60 two-player\ngames. Examples of VGDL games are shown in Figure 1.\nVGDL game and level ﬁles are parsed by the GVGAI\nframework, which deﬁnes the ontology of sprite types and\ninteractions that are allowed. The benchmark creates the game\nthat can be played either by a human or a bot. For the latter, the\nframework provides an API that bots (or agents, or controllers)\ncan implement to interact with the game - hence GVGAI\nbots can play any VGDL game provided. All controllers\nmust inherit from an abstract class within the framework and\nimplement a constructor and three different methods: INIT ,\ncalled at the beginning of every game; ACT, called at every\ngame tick and must return the next action of the controller;\nand RESULT , called at the end of the game with the ﬁnal state.\nThe agents do not have access to the rules of the game (i.e.\nthe VGDL description) but can receive information about the\ngame state at each tick. This information is formed by the\ngame status - winner, time step and score -, state of the player\n(also referred to in this paper as avatar) - position, orientation,\nresources, health points -, history of collisions and positions of\nthe different sprites in the game identiﬁed with a unique type\nid. Additionally, sprites are grouped in categories attending to\ntheir general behavior: Non-Player Characters (NPC), static,\nmovable, portals (which spawn other sprites in the game, or\nbehave as entry or exit point in the levels) and resources (that\ncan be collected by the player). Finally, each game has a\ndifferent set of actions available (a subset of left, right, up,\ndown, use and nil), which can also be queried by the agent.\nIn the planning settings of the framework (single- [8] and\ntwo-player [10]), the bots can also use a Forward Model. This\nallows the agent to copy the game state and roll it forward,\ngiven an action, to reach a potential next game state. In these\nsettings, controllers have 1 second for initialization and 40ms\nat each game tick as decision time. If the action to execute\nin the game is returned between 40 and 50 milliseconds, the\ngame will play the move nil as a penalty. If the agent takes\nmore than 50 milliseconds to return an action, the bot will be\ndisqualiﬁed. This is done in order to keep the real-time aspect\nof the game. In the two-player case, games are played by two\nagents in a simultaneous move fashion. Therefore, the forward\nmodel requires the agents to also supply an action for the other\nplayer, thus facilitating research in general opponent modeling.\nTwo-player games can also be competitive or cooperative, a\nfact that is not disclosed to the bots at any time.\nThe learning setting of the competition changes the infor-\nmation that is given to the agents. The main difference with\nthe planning case is that no Forward Model is provided, in\norder to foster research by learning to play in an episodic\nmanner [14]. This is the only setting in which agents can\nbe written not only in Java, but also in Python, in order to\naccommodate for popular machine learning libraries written in\nthis language. Game state information (same as in the planning\ncase) is provided in a Json format and the game screen can be\nobserved by the agent at every game tick. Since 2018, Torrado\net al. [15] interfaced the GVGAI framework to the OpenAI\nGym environment.\nThe GVGAI framework can also be used for procedural\ncontent generation (PCG). In the level generation setting [11],\nthe objective is to program a generator that can create playable\nlevels for any game received. In the rule generation case [12],\nthe goal is to create rules that allow agents to play in any level\nreceived. The framework provides, in both cases, access to the\nforward model so agents can be used to test and evaluate the\ncontent generated.\nWhen generating levels, the framework provides the gener-\nator with all the information needed about the game such as\ngame sprites, interaction set, termination conditions and level\nmapping. Levels are generated in the form of 2d matrix of\ncharacters, with each character representing the game sprites at\nthe speciﬁc location determined by the matrix. The challenge\nalso allows the generator to replace the level mapping with\na new one. When generating rules, the framework provides\nthe game sprites and a certain level. The generated games are\nrepresented as two arrays of strings. The ﬁrst array contains the\ninteraction set, while the second array contains the termination\nconditions.\nAs can be seen, the GVGAI framework offers an AI\nchallenge at multiple levels. Each one of the settings (or\ncompetition tracks) is designed to serve as benchmark for\na particular type of problems and approaches. The planning\ntracks provide a forward model, which favors the use of\nstatistical forward planning and model-based reinforcement\nlearning methods. In particular, this is enhanced in the two-\nplayer planning track with the challenge of player modeling\nand interaction with other another agent in the game. The\nlearning track promotes research in model-free reinforcement\nlearning techniques and similar approaches, such as evolution\nand neuro-evolution. Finally, the level and rule generation\ntracks focus on content creation problems and the algorithms\nthat are traditionally used for this: search-based (evolution-\nary algorithms and forward planning methods), solver (SAT,\nAnswer Set Programming), cellular automata, grammar-based\napproaches, noise and fractals.\nIII. T HE GVGAI C OMPETITION\nFor each one of the settings described in the previous\nsection, one or more competitions have been run. All GV-\nGAI competition tracks follow a similar structure: games\nare grouped in different sets ( 10 games on each set, with\n5 different levels each). Public sets of games are included\nin the framework and allow participants to train their agents\non them. For each year, there is one validation and one\ntest set. Both sets are private and stored in the competition\nserver2. Participants can submit their entries any time before\nthe submission deadline to all training and validation sets, and\npreliminary rankings are displayed in the competition website\n(the names of the validation set games are anonymous).\nA. Game Playing Tracks\nIn the game playing tracks (planning and learning set-\ntings), the competition rankings are computed by ﬁrst sorting\nall entries per game according to victory rates, scores and\ngame lengths, in this order. These per-game rankings award\npoints to the ﬁrst 10 entries, from ﬁrst to tenth position:\n25,18,15,12,10,8,6,4,2 and 1. The winner of the com-\npetition is the submission that sums more points across all\ngames in the test set. For a more detailed description of the\n2www.gvgai.net; Intel Core i5 machine, 2.90GHz, and 4GB of memory.\nTABLE I\nWINNERS OF ALL EDITIONS OF THE GVGAI P LANNING COMPETITION .\n2P INDICATES 2-P LAYER TRACK . HYBRID DENOTES 2 OR MORE\nTECHNIQUES COMBINED IN A SINGLE ALGORITHM . HYPER -HEURISTIC\nHAS A HIGH LEVEL DECISION MAKER TO DECIDES WHICH SUB -AGENT\nMUST PLAY (SEE SECTION IV). T ABLE EXTENDED FROM [16].\nContest Leg Winner Type Section\nCIG-14 OLETS Tree Search Method IV-B [8]\nGECCO-15 YOLOBOT Hyper-heuristic IV-E [17]\nCIG-15 Return42 Hyper-heuristic IV-E [16]\nCEEC-15 YBCriber Hybrid IV-D [18]\nGECCO-16 YOLOBOT Hyper-heuristic IV-E [17]\nCIG-16 MaastCTS2 Tree Search Method IV-B [19]\nWCCI-16 (2P) ToV o2 Hybrid V-A [10]\nCIG-16 (2P) Number27 Hybrid V-B [10]\nGECCO-17 YOLOBOT Hyper-heuristic IV-E [17]\nCEC-17 (2P) ToV o2 Hybrid V-A [10]\nWCCI-18 (1P) YOLOBOT Hyper-heuristic IV-E [17]\nFDG-18 (2P) OLETS Tree Search Method IV-B [10]\ncompetition and its rules, the reader is referred to [8]. All\ncontrollers are run on the test set after the submission deadline\nto determine the ﬁnal rankings of the competition, executing\neach agent multiple times on each level.\n1) Planning tracks: The ﬁrst GVGAI competition ever held\nfeatured the Single-player Planning track in 2014. A full\ndescription of this competition can be found at [8]. 2015\nfeatured three legs in a year-long championship, each one of\nthem with different validation and test sets. The Two-player\nPlanning track [9] was added in 2016, with the aim of testing\ngeneral AI agents in environments which are more complex\nand present more direct player interaction [10]. Since then, the\nsingle and two-player tracks have run in parallel until 2018.\nTable I shows the winners of all editions up to date, along\nwith the section of this survey in which the method is included\nand the paper that describes the approach more in depth.\n2) Learning track: The GVGAI Single-Player learning\ntrack has run for two years: 2017 and 2018, both at the IEEE\nConference on Computational Intelligence and Games (CIG).\nIn the 2017 edition, the execution of controllers was divided\ninto two phases: learning and validation. In the learning phase,\neach controller has a limited amount of time, 5 minutes, for\nlearning the ﬁrst 3 levels of each game. The agent could play as\nmany times as desired, choosing among these 3 levels, as long\nas the 5 minutes time limit is respected. In the validation phase,\nthe controller plays 10 times the levels 4 and 5 sequentially.\nThe results obtained in these validation levels are the ones used\nin the competition to rank the entries. Besides the two sample\nrandom agents written in Java and Python and one sample\nagent using Sarsa written in Java, the ﬁrst GVGAI single-\nplayer learning track received three submissions written in Java\nand one in Python [20]. The winner of this track is a naive\nimplementation of Q-Learning algorithm (Section VI-A4).\nThe 2018 edition featured, for the ﬁrst time, the integration\nof the framework with the OpenAI Gym API [15], which\nresults as GVGAI Gym 3. This edition also ran with some\nrelaxed constraints. Firstly, only 3 games are used for the\n3https://github.com/rubenrtorrado/GVGAI GYM\nTABLE II\nSCORE AND RANKING OF THE SUBMITTED AGENTS IN THE 2018’S\nGVGAI L EARNING COMPETITION . †DENOTES A SAMPLE CONTROLLER .\nGame Game 1 Game 2 Game 3 RankingLevel 3 4 5 3 4 5 3 4 5\nfraBot-RL-Sarsa -2 1 -1 0 0 0 2 3 2 1\nfraBot-RL-QLearning -2 -1 -2 0 0 0 1 0 2 2\nRandom†† -0.5 0.2 -0.1 0 0 0 3.5 0.7 2.7 3\nDQN† 61.5 -1 0.3 0 0 0 - - - -\nPrioritized Dueling DQN † 36.8 -1 -2 0 0 0 - - - -\nA2C† 8.1 -1 -2 0 0 0 - - - -\nOLETS Planning Agent 41.7 48.6 3.1 0 0 2.2 4.2 8.1 14 -\ncompetition, and they are made public. Only 2 levels for each\nare provided to the participants for training purposes, while\nthe other 3 are kept secret and used for computing the ﬁnal\nresults. Secondly, each agent has an increased decision time\nof 100ms. Thirdly, the participants were free to train their\nagent by themselves using as much time and computational\nresources as they want before the submission deadline.\nThis edition of the competition received only 2 entries,\nfraBot-RL-QLearning and fraBot-RL-Sarsa, submitted by the\nsame group of contributors from the Frankfurt University of\nApplied Science. The results of the entries and sample agents\n(random, DQN, Prioritized Dueling DQN and A2C [15]) are\nsummarized in Table II. For comparison, the planning agent\nOLETS (with access to the forward model) is included. DQN\nand Prioritized Dueling DQN are outstanding on level 3 (test\nlevel) of the game 1, because the level 3 is very similar to\nthe level 2 (training level). Interestingly, the sample learning\nagent DQN outperformed OLETS on the third level of game\n1. DQN, Prioritized Dueling DQN and A2C are not applied\nto the game 3, due to the different game screen dimensions of\ndifferent levels. We would like to refer the readers to [15] for\nmore about the GVGAI Gym.\nB. PCG Tracks\nIn the PCG tracks, participants develop generators for levels\nor rules that are adequate for any game or level (respectively)\ngiven. Due to the inherent subjective nature of content genera-\ntion, the evaluation of the entries is done by human judges who\nattend the conference where the competition takes place. For\nboth tracks, during the competition day, judges are encouraged\nto try pairs of generated content and select which one they\nliked (one, both, or neither). Finally, the winner was selected\nbased on the generator with more votes.\n1) Level Generation Track: The ﬁrst level generation com-\npetition was held at the International Joint Conference on Arti-\nﬁcial Intelligence (IJCAI) in 2016. This competition received 4\nparticipants. Each one of them was provided a month to submit\na new level generator. Three different level generators were\nprovided in order to help the users get started with the system\n(see Section VII for a description of these). Three out of the\nfour participants were simulation-based level generators while\nthe remaining was based on cellular automata. The winner of\nthe contest was the Easablade generator, a cellular automata\ndescribed in Section VII-A4. The competition was run again\non the following year at IEEE CIG 2017. Unfortunately, only\none submission was received, hence the the competition was\ncanceled. This submission used a n-gram model to generate\nnew constrained levels using a recorded player keystrokes.\n2) Rule Generation Track: The Rule Generation track [12]\nwas introduced and held during CIG 2017. Three different\nsample generators were provided (Section VIII) and the con-\ntest ran over a month’s period. Unfortunately, no submissions\nwere received for this track.\nIV. M ETHODS FOR SINGLE PLAYER PLANNING\nThis section describes the different methods that have been\nimplemented for Single Player Planning in GVGAI. All the\ncontrollers that face this challenge have in common the possi-\nbility of using the forward model to sample future states from\nthe current game state, plus the fact that they have a limited\naction-decision time. While most attempts abide by the 40ms\ndecision time imposed by the competition, other efforts in the\nliterature compel their agents to obey a maximum number of\ncalls of the forward model.\nSection IV-A brieﬂy introduces the most basic methods\nthat can be found within the framework. Then Section IV-B\ndescribes the different tree search methods that have been\nimplemented for this settings by the community, followed by\nEvolutionary Methods in Section IV-C. Often, more than one\nmethod is combined into the algorithm, which gives place to\nHybrid methods (Section IV-D) or Hyper-heuristic algorithms\n(Section IV-E). Further discussion on these methods and their\ncommon take-aways has been included in Section X.\nA. Basic Methods\nThe GVGAI framework contains several agents aimed at\ndemonstrating how a controller can be created for the single-\nplayer planning track of the competition [8]. Therefore, these\nmethods are not particularly strong.\nThe simplest of all methods is, without much doubt,doNoth-\ning. This agent returns the action nil at every game tick without\nexception. The next agent in complexity is sampleRandom,\nwhich returns a random action at each game tick. Finally,\nonesteplookahead is another sample controller that rolls the\nmodel forward for each one of the available actions in order\nto select the one with the highest action value, determined\nby a function that tries to maximize score while minimizing\ndistances to NPCs and portals.\nB. Tree Search Methods\nOne of the strongest and inﬂuential sample controllers is\nsampleMCTS, which implements the Monte Carlo Tree Search\n(MCTS) algorithm for real-time games. Initially implemented\nin a closed loop version (the states visited are stored in the\ntree node, without calling forward model during the tree policy\nphase of MCTS), it achieved the 3rd position (out of 18\nparticipants) in the ﬁrst edition of the competition.\nThe winner of that edition, Cou ¨etoux, implemented Open\nLoop Expectimax Tree Search (OLETS), which is an open\nloop (states visited are never stored in the associated tree\nnode) version of MCTS which does not include rollouts and\nuses Open Loop Expectimax (OLE) for the tree policy. OLE\nsubstitutes the empirical average reward by rM , a weighted\nsum of the empirical average of rewards and the maximum of\nits children rM values [8].\nSchuster, in his MSc thesis [21], analyzes several enhance-\nments and variations for MCTS in different sets of the GV-\nGAI framework. These modiﬁcations included different tree\nselection, expansion and play-out policies. Results show that\ncombinations of Move-Average Sampling Technique (MAST)\nand n-Gram Selection Technique (NST) with Progressive\nHistory provided an overall higher rate of victories than\ntheir counterparts without these enhancements, although this\nresult was not consistent across all games (with some simpler\nalgorithms achieving similar results).\nIn a different study, Soemers [19], [22] explored multiple\nenhancements for MCTS: Progressive History (PH) and NST\nfor the tree selection and play-out steps, tree re-use (by starting\nat each game tick with the subtree grown in the previous frame\nthat corresponds to the action taken, rather than a new root\nnode), bread-ﬁrst tree initialization (direct successors of the\nroot note are explored before MCTS starts), safety pre-pruning\n(prune those nodes with high number of game loses found),\nloss avoidance (MCTS ignores game lose states when found\nfor the ﬁrst time by choosing a better alternative), novelty-\nbased pruning (in which states with features rarely seen are\nless likely to be pruned), knowledge based evaluation [23]\nand deterministic game detection. The authors experimented\nwith all these enhancements in 60 games of the framework,\nshowing that most of them improved the performance of\nMCTS signiﬁcantly and their all-in-one combination increased\nthe average win rate of the sample agent in 17 percentage\npoints. The best conﬁguration was the winner of one of the\neditions of the 2016 competitions (see Table I).\nF. Frydenberg studied yet another set of enhancements\nfor MCTS [24]. The authors showed that using MixMax\nbackups (weighing average and maximum rewards on each\nnode) improved the performance in only some games, but\nits combination with reversal penalty (to penalize visiting the\nsame location twice in a play-out) offers better results than\nvanilla MCTS. Other enhancements, such as macro-actions (by\nrepeating an action several times in a sequence) and partial\nexpansion (a child node is considered expanded only if its\nchildren have also been expanded) did not improve the results\nobtained.\nPerez-Liebana et al. [23] implemented KB-MCTS, a version\nof MCTS with two main enhancements. First, distances to\ndifferent sprites were considered features for a linear combi-\nnation, where the weights were evolved to bias the MCTS\nrollouts. Secondly, a Knowledge Base (KB) is kept about\nhow interesting for the player the different sprites are, where\ninteresting is a measure of curiosity (rollouts are biased\ntowards unknown sprites) and experience (a positive/negative\nbias for getting closer/farther to beneﬁcial/harmful entities).\nThe results of applying this algorithm to the ﬁrst set of games\nof the framework showed that the combination of these two\ncomponents gave a boost in performance in most games of\nthe ﬁrst training set.\nThe work in [23] has been extended by other researchers in\nthe ﬁeld, which also put a special effort on biasing the Monte\nCarlo (MC) simulations. In [25], the authors modiﬁed the\nrandom action selection in MCTS rollouts by using potential\nﬁelds, which bias the rollouts by making the agent move in\na direction akin to the ﬁeld. The authors showed that KB-\nMCTS provides a better performance if this potential ﬁeld\nis used instead of the Euclidean distance between sprites\nimplemented in [23]. Additionally, in a similar study [26],\nthe authors substituted the Euclidean distance for a measure\ncalculated by a path-ﬁnding algorithm. This addition achieved\nsome improvements over the original KB-MCTS, although the\nauthors noted in their study that using path-ﬁnding does not\nprovide a competitive advantage in all games.\nAnother work by Park and Kim [27] tackles this challenge\nby i) determining the goodness of the other sprites in the\ngame; ii) computing an Inﬂuence Map (IM) based on this; and\niii) using the IM to bias the simulations, in this occasion by\nadding a third term to the Upper Conﬁdence Bound (UCB)\nequation [1] for the tree policy of MCTS. Although not\ncompared with KB-MCTS, the resultant algorithm improves\nthe performance of the sample controllers in several games of\nthe framework, albeit performing worse than these in some of\nthe games used in the study.\nBiasing rollouts is also attempted by dos Santos et al. [28],\nwho introduced Redundant Action Avoidance (RAA) and Non-\nDefeat Policy (NDP); RAA analyzes changes in the state to\navoid selecting sequences of actions that do not produce any\nalteration on position, orientation, properties or new sprites in\nthe avatar. NDP makes the recommendation policy ignore all\nchildren of the root node who found at least one game loss in\na simulation from that state. If all children are marked with\na defeat, normal (higher number of visits) recommendation is\nfollowed. Again, both modiﬁcations are able to improve the\nperformance of MCTS in some of the games, but not in all.\nde Waard et al. [29] introduced the concept of options of\nmacro-actions in GVGAI and designed Option MCTS (O-\nMCTS). Each option is associated with a goal, a policy and\na termination condition. The selection and expansion steps in\nMCTS are modiﬁed so the search tree branches only if an\noption is ﬁnished, allowing for a deeper search in the same\namount of time. Their results show that O-MCTS outperforms\nMCTS in games with small levels or a few number of sprites,\nbut loses in the comparison to MCTS when the games are\nbigger due to these options becoming too large.\nIn a similar line, Perez-Liebana et al. [13] employed macro-\nactions for GVGAI games that used continuous (rather than\ngrid-based) physics. These games have a larger state space,\nwhich in turn delays the effects of the player’s actions and\nmodiﬁes the way agents navigate through the level. Macro-\nactions are deﬁned as a sequence or repetition of the same\naction during M steps, which is arguably the simplest kind of\nmacro-actions that can be devised. MCTS performed better\nwithout macro-actions on average across games, but there\nare particular games where MCTS needs macro-actions to\navoid losing at every attempt. The authors also concluded that\nthe length M of the macro-actions impacts different games\ndistinctly, although shorter ones seem to provide better results\nthan larger ones, probably due to a more ﬁne control in the\nmovement of the agents.\nSome studies have brought multi-objective optimization\nto this challenge. For instance, Perez-Liebana et al. [30]\nimplemented a Multi-objective version of MCTS, concretely\nmaximizing score and level exploration simultaneously. In\nthe games tested, the rate of victories grew from 32.24%\n(normal MCTS) to 42.38% in the multi-objective version,\nshowing great promise for this approach. In a different study,\nKhalifa et al. [31] applied multi-objective concepts to evolving\nparameters for a tree selection conﬁdence bounds equation. A\nprevious work by Bravi [32] (also discussed in Section IV-D)\nprovided multiple UCB equations for different games. The\nwork in [31] evolved, using S-Metric Selection Evolutionary\nMulti-objective Optimization Algorithm (SMS-EMOA), the\nlinear weights of a UCB equation that results of combining\nall from [32] in a single one. All these components respond\nto different and conﬂicting objectives, and their results show\nthat it is possible to ﬁnd good solutions for the games tested.\nA signiﬁcant exception to MCTS with regards to tree search\nmethods for GVGAI is that of Geffner and Geffner [18] (win-\nner of one of the editions of the 2015 competition, YBCriber,\nas indicated in Table I), who implemented Iterated Width (IW;\nconcretely IW(1)). IW(1) is a breadth-ﬁrst search with a crucial\nalteration: a new state found during search is pruned if it does\nnot make true a new tuple of at most 1 atom, where atoms\nare Boolean variables that refer to position (and orientations\nin the case of avatars) changes of certain sprites at speciﬁc\nlocations. The authors found that IW(1) performed better than\nMCTS in many games, with the exception of puzzles, where\nIW(2) (pruning according to pairs of atoms) showed better\nperformance. This agent was declared winner in the CEEC\n2015 edition of the Single-Player Planning Track [3].\nBabadi [33] implemented several versions of Enforced Hill\nClimbing (EHC), a breadth-ﬁrst search method that looks for\na successor of the current state with a better heuristic value.\nEHC obtained similar results to KB-MCTS in the ﬁrst set of\ngames of the framework, with a few disparities in speciﬁc\ngames of the set.\nNelson [34] ran a study on MCTS in order to investigate if,\ngiving a higher time budget to the algorithm (i.e. increasing\nthe number of iterations), MCTS was able to master most\nof the games. In other words, if the real-time nature of\nthe GVGAI framework and competition is the reason why\ndifferent approaches fail to achieve a high victory rate. This\nstudy provided up to 30 times more budget to the agent, but\nthe performance of MCTS only increased marginally even at\nthat level. In fact, this improvement was achieved by means\nof losing less often rather than by winning more games. This\npaper concludes that the real-time aspect is not the only factor\nin the challenge, but also the diversity in the games. In other\nwords, increasing the computational budget is not the answer\nto the problem GVGAI poses, at least for MCTS.\nFinally, another study on the uses of MCTS for single\nplayer planning is carried out by Bravi et al. [35]. In this\nwork, the focus is set on understanding why and under\nwhich circumstances different MCTS agents make different\ndecisions, allowing for a more in-depth description and be-\nhavioral logging. This study proposes the analysis of different\nmetrics (recommended action and their probabilities, action\nvalues, consumed budget before converging on a decision,\netc.) recorded via a shadow proxy agent, used to compare\nalgorithms in pairs. The analysis described in the paper shows\nthat traditional win-rate performance can be enhanced with\nthese metrics in order to compare two or more approaches.\nC. Evolutionary Methods\nThe second big group of algorithms used for single-player\nplanning is that of evolutionary algorithms (EA). Concretely,\nthe use of EAs for this real-time problem is mostly imple-\nmented in the form of Rolling Horizon EAs (RHEA). This\nfamily of algorithms evolves sequences of actions with the\nuse of the forward model. Each sequence is an individual of\nan EA which ﬁtness is the value of the state found at the end\nof the sequence. Once the time budget is up, the ﬁrst action of\nthe sequence with the highest ﬁtness is chosen to be applied\nin that time step.\nThe GVGAI competition includes SampleRHEA as a sample\ncontroller. SampleRHEA has a population size of 10, individual\nlength of 10 and implements uniform crossover and mutation,\nwhere one action in the sequence is changed for another one\n(position and new action chosen uniformly at random) [8].\nGaina et al. [36] analyzed the effects of the RHEA parame-\nters on the performance of the algorithm in 20 games, chosen\namong the existent ones in order to have a representative\nset of all games in the framework. The parameters analyzed\nwere population size and individual length, and results showed\nthat higher values for both parameters provided higher victory\nrates. This study motivated the inclusion of Random Search\n(SampleRS) as a sample in the framework, which is equivalent\nto RHEA but with an inﬁnite population size (i.e. only\none generation is evaluated until budget is consumed) and\nachieves better results than RHEA in some games. [36] also\ncompared RHEA with MCTS, showing better performance for\nan individual length of 10 and high population sizes.\nSantos et al. [37] implemented three variants for RHEA\nwith shifted buffer (RHEA-SB) by (i) applying the one-\nstep-look-ahead algorithm after the buffer shifting phase; (ii)\napplying a spatial redundant action avoidance policy [28]; and\n(iii) applying both techniques. The experimental tests on 20\nGVGAI single-player games showed that the third variant of\nRHEA-SB achieved promising results.\nSantos and Bernardino [38] applied the avatar-related in-\nformation, spacial exploration encouraging and knowledge\nobtained during game playing to the game state evaluation\nof RHEA. These game state evaluation enhancements have\nalso been tested on an MCTS agent. The enhancements\nsigniﬁcantly increased the win rate and game score obtained\nby RHEA and MCTS on 20 tested games.\nA different type of information was used by Gaina et al. [39]\nto dynamically adjust the length of the individuals in RHEA:\nthe ﬂatness of the ﬁtness landscape is used to shorten or\nlengthen the individuals in order for the algorithm to better\ndeal with sparse reward environments (using longer rollouts\nfor identiﬁcation of further away rewards), while not harming\nperformance in dense reward games (using shorter rollouts for\nfocus on immediate rewards). However, this had a detrimental\neffect in RHEA, while boosting MCTS results. Simply in-\ncreasing the rollout length proved to be more effective than\nthis initial attempt at using the internal agent state to affect\nthe search itself.\nA different Evolutionary Computation agent was proposed\nby Jia et al. [40], [41], which consists of a Genetic Pro-\ngramming (GP) approach. The authors extract features from a\nscreen capture of the game, such as avatar location and the\npositions and distances to the nearest object of each type.\nThese features are inputs to a GP system that, using arithmetic\noperands as nodes, determines the action to execute as a result\nof three trees (horizontal, vertical and action use). The authors\nreport that all the different variations of the inputs provided to\nthe GP algorithm give similar results to those of MCTS, on\nthe three games tested in their study.\nD. Hybrids\nThe previous studies feature techniques in which one tech-\nnique is predominant in the agent created, albeit they may\ninclude enhancements which can place them in the boundary\nof hybrids. This section describes those approaches that, in the\nopinion of the authors, would in their own right be considered\nas techniques that mix more than one approach in the same,\nsingle algorithm.\nAn example of these approaches is presented by Gaina\net al. [42], which analyzed the effects of seeding the initial\npopulation of a RHEA using different methods. Part of the\ndecision time budget is dedicated to initialize a population with\nsequences that are promising, as determined by onesteplooka-\nhead and MCTS agents. Results show that both seeding options\nprovide a boost in victory rate when population size and\nindividual length are small, but the beneﬁts vanish when these\nparameters are large.\nOther enhancements for RHEA proposed in [43] are incor-\nporating a bandit-based mutation, a statistical tree, a shifted\nbuffer and rollouts at the end of the sequences. The bandit-\nbased mutation breaks the uniformity of the random mutations\nin order to choose new values according to suggestions given\nby a uni-variate armed bandit. However, the authors reported\nthat no improvement on performance was noticed. A statistical\ntree, previously introduced in [44], keeps the visit count and\naccumulated rewards in the root node, which are subsequently\nused for recommending the action to take at that time step.\nThis enhancement produced better results with smaller indi-\nvidual length and smaller population sizes. The shifted buffer\nenhancement provided the best improvement in performance,\nwhich consist of shifting the sequences of the individuals of\nthe population one action to the left, removing the action from\nthe previous time step. This variation, similar to keeping the\ntree between frames in MCTS, combined with the addition of\nrollouts at the end of the sequences provided an improvement\nin victory rate ( 20 percentile points over vanilla RHEA) and\nscores.\nA similar (and previous) study was conducted by Horn et\nal. [45]. In particular, this study features RHEA with rollouts\n(as in [43]), RHEA with MCTS for alternative actions (where\nMCTS can determine any action with the exception of the one\nrecommended by RHEA), RHEA with rollouts and sequence\nplanning (same approach as the shifted buffer in [43]), RHEA\nwith rollouts and occlusion detection (which removes not\nneeded actions in a sequence that reaches a reward) and\nRHEA with rollouts and NPC attitude check (which rewards\nsequences in terms of proximity to sprites that provide a\npositive or negative reward). Results show that RHEA with\nrollouts improved performance in many games, although all\nthe other variants and additions performed worse than the\nsample agents. It is interesting to see that in this case the\nshifted buffer did not provide an improvement in the victory\nrate, although this may be due to the use of different games.\nSchuster [21] proposed two methods that combine MCTS\nwith evolution. One of them, (1 +1)-EA as proposed by [23],\nevolves a vector of weights for a set of game features in order\nto bias the rollouts towards more interesting parts of the search\nspace. Each rollout becomes an evaluation for an individual\n(weight vector), using the value of the ﬁnal state as ﬁtness.\nThe second algorithm is based on strongly-typed GP (STGP)\nand uses game features to evolve state evaluation functions\nthat are embedded within MCTS. These two approaches join\nMAST and NST (see Section IV-B) in a larger comparison,\nand the study concludes that different algorithms outperform\nothers in distinct games, without an overall winner in terms\nof superior victory rate, although superior to vanilla MCTS in\nmost cases.\nThe idea of evolving weight vectors for game features\nduring the MCTS rollouts introduced in [23] (KB-MCTS 4)\nwas explored further by van Eeden in his MSc thesis [46]. In\nparticular, the author added A* as a path-ﬁnding algorithm to\nreplace the euclidean distance used in KB-MCTS for a more\naccurate measure and changing the evolutionary approach.\nWhile KB-MCTS used a weight for each pair feature-action,\nbeing the action chosen at each step by the Softmax equation,\nthis work combines all move actions on a single weight and\npicks the action using Gibbs sampling. The author concludes\nthat the improvements achieved by these modiﬁcations are\nmarginal, and likely due to the inclusion of path-ﬁnding.\nAdditional improvements on KB-MCTS are proposed by\nChu et al. [47]. The authors replace the Euclidean distance\nfeature to sprites with a grid view of the agent’s surroundings,\nand also the (1+1)-EA with a Q-Learning approach to bias the\nMCTS rollouts, making the algorithm update the weights at\neach step in the rollout. The proposed modiﬁcations improved\nthe victory rate in several sets of games of the framework\nand also achieved the highest average victory rate among the\nalgorithms it was compared with.\n˙Ilhan and Etaner-Uyar [48] implemented a combination of\nMCTS and true online Sarsa (λ). The authors use MCTS roll-\nouts as episodes of past experience, executing true online Sarsa\nat each iteration with a ϵ-greedy selection policy. Weights are\nlearnt for features taken as the smallest euclidean distance\nto sprites of each type. Results showed that the proposed\n4This approach could also be considered an hybrid. Given its inﬂuence in\nother tree approaches, it has also been partially described in Section IV-B\napproaches improved the performance on vanilla MCTS in\nthe majority of the 10 games used in the study.\nEvolution and MCTS have also been combined in different\nways. In one of them, Bravi et al. [49] used a GP system\nto evolve different tree policies for MCTS. Concretely, the\nauthors evolve a different policy for each one of the 5 games\nemployed in the study, aiming to exploit the characteristics\nof each game in particular. The results showed that the tree\npolicy plays a very important role on the performance of the\nMCTS agent, although in most cases the performance is poor -\nnone of the evolved heuristics performed better than the default\nUCB in MCTS.\nFinally, Sironi et al. [50] designed three Self-Adaptive\nMCTS (SA-MCTS) that tuned the parameters of MCTS (play-\nout depth and exploration factor) on-line, using Naive Monte-\nCarlo, an ( λ, µ)-Evolutionary Algorithm and the N-Tuple\nBandit Evolutionary Algorithm (NTBEA) [51]. Results show\nthat all tuning algorithms improve the performance of MCTS\nwhere vanilla MCTS performs poorly, while keeping a similar\nrate of victories in those where MCTS performs well. In a\nfollow-up study, however, Sironi and Winands [52] extend\nthe experimental study to show that online parameter tuning\nimpacts performance in only a few GVGP games, with NT-\nBEA improving performance signiﬁcantly in only one of them.\nThe authors conclude that online tuning is more suitable for\ngames with longer budget times, as it struggles to improve\nperformance in most GVGAI real-time games.\nE. Hyper-heuristics / Algorithm Selection\nSeveral authors have also proposed agents that use several\nalgorithms, but rather than combining them into a single one,\nthere is a higher level decision process that determines which\none of them should be used at each time.\nRoss, in his MSc thesis [53] proposes an agent that is a\ncombination of two methods. This approach uses A* with\nEnforced Hill Climbing to navigate through the game at a\nhigh level and switches to MCTS when in close proximity\nto the goal. The work highlights the problems of computing\npaths in the short time budget allowed, but indicate that goal\ntargeting with path-ﬁnding combined with local maneuvering\nusing MCTS does provide good performance in some of the\ngames tested.\nJoppen et al. [17] implemented YOLOBOT, arguably the\nmost successful agent for GVGAI up to date, as it has won\nseveral editions of the competition. Their approach consists\nof a combination of two methods: a heuristic Best First\nSearch (BFS) for deterministic environments and MCTS for\nstochastic games. Initially, the algorithm employs BFS until\nthe game is deemed stochastic, an optimal solution is found\nor a certain game tick threshold is reached, extending through\nseveral consecutive frames if needed for the search. Unless the\noptimal sequence of actions is found, the agent will execute\nan enhanced MCTS consistent of informed priors and rollout\npolicies, backtracking, early cutoffs and pruning. The resultant\nagent has shown consistently a good level of play in multiple\ngame sets of the framework.\nAnother hyper-heuristic approach, also winner of one of\nthe 2015 editions of the competition ( Return42, see Table I),\ndetermines ﬁrst if the game is deterministic or stochastic. In\ncase of the former, A* is used to direct the agent to sprites of\ninterest. Otherwise, random walks are employed to navigate\nthrough the level [16].\nAzaria et al. [54] applied GP to evolve hyper-heuristic-based\nagents. The authors evolved 3 step-lookahead agents, which\nwere tested on the 3 game sets from the ﬁrst 2014 GVGAI\ncompetition. The resultant agent was able to outperform the\nagent ranked at 3rd place in the competition (sample MCTS).\nThe fact that this type of portfolio agents has shown very\npromising results has triggered more research into hyper-\nheuristics and game classiﬁcation. The work by Bontrager\net al. [55] used K-means to cluster games and algorithms\nattending to game features derived from the type of sprites\ndeclared in the VGDL ﬁles. The resulting classiﬁcation seemed\nto follow a difﬁculty pattern, with 4 clusters that grouped\ngames that were won by the agents at different rates.\nMendes et al. [56] built a hyper-agent which selected\nautomatically an agent from a portfolio of agents for playing\nindividual game and tested it on the GVGAI framework. This\napproached employed game-based features to train different\nclassiﬁers (Support Vector Machines - SVM, Multi-layer Per-\nceptrons, Decision Trees - J48, among others) in order to select\nwhich agent should be used for playing each game. Results\nshow that the SVM and J48 hyper-heuristics obtained a higher\nvictory rate than the single agents separately.\nHorn et al. [45] (described before in Section IV-D) also\nincludes an analysis on game features and difﬁculty estimation.\nThe authors suggest that the multiple enhancements that are\nconstantly attempted in many algorithms could potentially be\nswitched on and off depending on the game that is being\nplayed, with the objective of dynamically adapting to the\npresent circumstances.\nAshlock et al. [16] suggest the possibility of creating a\nclassiﬁcation of games, based on the performance of multiple\nagents (and their variations: different enhancements, heuristics,\nobjectives) on them. Furthermore, this classiﬁcation needs\nto be stable, in order to accommodate the ever-increasing\ncollection of games within the GVGAI framework, but also\nﬂexible enough to allow an hyper-heuristic algorithm to choose\nthe version that better adapts to unseen games.\nFinally, Gaina et al. [57] gave a ﬁrst step towards algorithm\nselection from a different angle. The authors trained several\nclassiﬁers on agent log data across 80 games of the GVGAI\nframework, in particular obtained only from player experience\n(i.e. features extracted from the way search was conducted,\nrather than potentially human-biased game features), to deter-\nmine if the game will be won or not at the end. Three models\nare trained, for the early, mid and late game, respectively, and\ntested in previously not seen games. Results show that these\npredictors are able to foresee, with high reliability, if the agent\nis going to lose or win the game. These models would there-\nfore allow to indicate when and if the algorithm used to play\nthe game should be changed. A visualization of these agent\nfeatures, including win prediction, displayed live while playing\ngames, is available through the VertigØ tool [58], which means\nto offer better agent analysis for deeper understanding of the\nagents’ decision making process, debugging and game testing.\nV. M ETHODS FOR TWO-PLAYER PLANNING\nThis section approaches agents developed by researchers\nwithin the Two-Player Planning setting. Most of these entries\nhave been submitted to the Two-Player Planning track of the\ncompetition [9]. Two methods stood out as the base of most\nentries received so far, Monte Carlo Tree Search (MCTS) and\nEvolutionary Algorithms (EA) [10]. On the one hand, MCTS\nperformed better in cooperative games, as well as showing the\nability to adapt better to asymmetric games, which involved a\nrole switch between matches in the same environment. EAs,\non the other hand, excelled in games with long lookaheads,\nsuch as puzzle games, which rely on a speciﬁc sequence of\nmoves being identiﬁed.\nCounterparts of the basic methods described in Section IV-A\nare available in the framework as well, the only difference\nbeing in the One Step Lookahead agent which requires an\naction to be supplied for the opponent when simulating game\nstates. The opponent model used by the sample agent assumes\nthey will perform a random move (with the exception of those\nactions that would cause a loss of the game).\nA. Tree Search methods\nMost of the competition entries in the ﬁrst 3 seasons\n(2016-2018) were based on MCTS (see Section IV-B). It is\ninteresting to note that the 2016 winner won again in 2018\n- highlighting the difﬁculty of the challenge and showing the\nneed for more research focus on multi-player games for better\nand faster progress.\nSome entries employed an Open Loop version of MCTS,\nwhich would only store statistics in the nodes of the trees\nand not game states, therefore needing to simulate through\nthe actions at each iteration for a potentially more accurate\nevaluation of the possible game states. Due to this being\nunnecessarily costly in deterministic games, some entries such\nas MaasCTS2 and YOLOBOT switched to Breadth-First Search\nin such games after an initial analysis of the game type, a\nmethod which has shown ability to ﬁnding the optimal solution\nif the game lasts long enough.\nEnhancements brought to MCTS include generating value\nmaps, either regarding physical positions in the level, or\nhigher-level concepts (such as higher values being assigned to\nstates where the agent is closer to objects it hasn’t interacted\nwith before; or interesting targets as determined by controller-\nspeciﬁc heuristics). The winner of the 2016 WCCI and 2017\nCEC legs, ToVo2, also employed dynamic Monte Carlo roll-out\nlength adjustments (increased with the number of iterations to\nencourage further lookahead if budget allows) and weighted\nroll-outs (the weights per action generated randomly at the\nbeginning of each roll-out).\nAll agents use online learning in one way or another\n(the simplest form being the base Monte Carlo Tree Search\nbackups, used to gather statistics about each action through\nmultiple simulations), but only the overall 2016 and 2018\nChampionship winner, adrienctx, uses ofﬂine learning on the\ntraining set supplied to tune the parameters in the Stochastic\nGradient Descent function employed, learning rate and mini\nbatch size.\nB. Evolutionary methods\nTwo of the 2016 competition entries used an EA tech-\nnique as a base as an alternative to MCTS: Number27 and\nCatLinux [10].\nNumber27 was the winner of the CIG 2016 leg, the con-\ntroller placing 4th overall in the 2016 Championship. Num-\nber27 uses a Genetic Algorithm (GA), with one population\ncontaining individuals which represent ﬁxed-length action\nsequences. The main improvement it features on top of the\nbase method is the generation of a value heat-map, used to\nencourage the agent’s exploration towards interesting parts of\nthe level. The heat-map is initialized based on the inverse\nfrequency of each object type (therefore a lower value the\nhigher the object number) and including a range of inﬂuence\non nearby tiles. The event history is used to evaluate game\nobjects during simulations and to update the value map.\nCatLinux was not a top controller on either of the individual\nlegs run in 2016, but placed 5th overall in the Championship.\nThis agent uses a Rolling Horizon Evolutionary Algorithm\n(RHEA). A shift buffer enhancement is used to boost per-\nformance, speciﬁcally keeping the population evolved during\none game tick in the next, instead of discarding it, each action\nsequence is shifted one action to the left (therefore removing\nthe previous game step) and a new random action is added at\nthe end to complete the individual to its ﬁxed length.\nNo ofﬂine learning was used by any of the EA agents,\nalthough there could be scope for improvement through pa-\nrameter tuning (ofﬂine or online).\nC. Opponent model\nMost agents submitted to the Two-Player competition use\ncompletely random opponent models. Some entries have\nadopted the method integrated within the sample One Step\nLookahead controller, choosing a random but non-losing ac-\ntion. In the 2016 competition, webpigeon assumed the oppo-\nnent would always cooperate, therefore play a move beneﬁcial\nto the agent. MaasCTS2 used the only advanced model at\nthe time: it remembered Q-values for the opponent actions\nduring simulations and added them to the statistics stored\nin the MCTS tree nodes; an ϵ-greedy policy was used to\nselect opponent actions based on the Q-values recorded. This\nprovided a boost in performance on the games in the WCCI\n2016 leg, but it did not improve the controller’s position in\nthe rankings for the following CIG 2016 leg. Most entries in\nthe 2017 and 2018 seasons employed simple random opponent\nmodels.\nOpponent models were found to be an area to explore\nfurther in [10] and Gonzalez and Perez-Liebana looked at 9\ndifferent models integrated within the sample MCTS agent\nprovided with the framework [59]. Alphabeta builds a tree\nincrementally, returning the best possible action in each time\ntick, while Minimum returns the worst possible action. Average\nuses a similar tree structure, but it computes the average\nreward over all the actions and it returns the action closest\nto the average. Fallible returns the best possible action with\na probability p = 0 .8 and the action with the minimum\nreward otherwise. Probabilistic involved ofﬂine learning over\n20 games in the GVGAI framework in order to determine\nthe probability of an MCTS agent to select each action,\nand then using these to determine the opponent action while\nplaying online. Same Action returns the same action the agent\nplays, while Mirror returns its opposite. Finally, LimitedBuffer\nrecords the last n = 20 actions performed by the player\nand builds probabilities of selecting the next action based on\nthis data, while UnlimitedBuffer records the entire history of\nactions during the game. When all 9 opponent models were\ntested in a round robin tournament against each other, the\nprobabilistic models achieve the highest win rates and two\nmodels, Probabilistic and UnlimitedBuffer outperforming a\nrandom opponent model.\nFinally, the work done on two-player GVGAI has in-\nspired other research on Mathematical General Game Playing.\nAshlock et al. [60] implemented general agents for three\ndifferent mathematical coordination games, including the Pris-\noner’s Dilemma. Games were presented at once, but switching\nbetween them at certain points, and experiments show that\nagents can learn to play these games and recognize when the\ngame has changed.\nVI. M ETHODS FOR SINGLE -PLAYER LEARNING\nThe GVGAI framework has also been used from an agent\nlearning perspective. In this setting, the agents do not use the\nforward model to plan ahead actions to execute in the real\ngame. Instead, the algorithms learn the games by repeatedly\nplaying them multiple times (as episodes in Reinforcement\nLearning), ideally improving their performance progressively.\nThis section describes ﬁrst the approaches that tackled the\nchallenge set in the single-player learning track of the 2017\nand 2018 competitions, to then move to other approaches.\nA. Competition entries\n1) Random agent: A sample random agent, which selects\nan action uniformly at random at every game tick, is included\nin the framework (in both Java and Python) for the purposes\nof testing. This agent is also meant to be taken as a baseline:\na learner is expected to perform better than an agent which\nacts randomly and does not undertake any learning.\n2) Multi-armed bandit algorithm: DontUnderestima-\nteUchiha by K. Kunanusont is based on two popular\nMulti-Armed Bandit (MAB) algorithms, ϵ-Decreasing Greedy\nAlgorithm and Upper Conﬁdence Bounds (UCB). At any\ngame tick T, the current best action with probability 1 −ϵT is\npicked, otherwise an action is uniformly randomly selected.\nThe best action at time T is determined using UCB with\nincrement of score as reward. This is a very interesting\ncombination, as the UCB-style selection and the ϵ-Decreasing\nGreedy Algorithm both aim at balancing the trade-off between\nexploiting more the best-so-far action and exploring others.\nAdditionally, ϵ0 is set to 0.5 and it decreases slowly along\ntime, formalized as ϵT = ϵ0 −0.0001T. According to the\ncompetition setting, all games will last longer than 2,000\ngame ticks, so ∀T ∈{1,..., 2000}, 0.5 ≥ϵT ≥0.3. As a\nresult, random decisions are made for approximately 40%\ntime.\n3) Sarsa: sampleLearner , ercumentilhan and fraBot-RL-\nSarsa are based on the State-Action-Reward-State-Action\n(Sarsa) algorithm [2]. The sampleLearner and ercumentilhan\nuse a subset of the whole game state information to build a\nnew state to reduce the amount of information to be saved and\nto take into account similar situations. The main difference is\nthat the former uses a square region with ﬁxed size centered at\nthe avatar’s position, while the latter uses a ﬁrst-person view\nwith a ﬁxed distance. fraBot-RL-Sarsa uses Sarsa, and it uses\nthe entire screenshot of the game screen as input provided by\nGVGAI Gym. The agent has been trained using 1000 episodes\nfor each level of each game, and the total training time was\n48 hours.\n4) Q-learning: kkunan , by K. Kunanusont, is a simple Q-\nlearning [2] agent using most of the avatar’s current infor-\nmation as features, which a few exceptions (such as avatar’s\nhealth and screen size, as these elements that vary greatly\nfrom game to game). The reward at game tick t+ 1is deﬁned\nas the difference between the score at t+ 1 and the one at\nt. The learning rate α and discounted factor γ are manually\nset to 0.05 and 0.8. During the learning phase , a random\naction is performed with probability ϵ = 0.1, otherwise, the\nbest action is selected. During the validation phase , the best\naction is always selected. Despite it’s simplicity, it won the the\nﬁrst track in 2017. fraBot-RL-QLearning uses the Q-Learning\nalgorithm. It has been trained using 1000 episodes for each\nlevel of each game, and the total training time was 48 hours.\n5) Tree search methods: YOLOBOT is an adaption of\nthe YOLOBOT planning agent (as described previously in\nSection IV-E). As the forward model is no more accessible\nin the learning track, the MCTS is substituted by a greedy\nalgorithm to pick the action that minimizes the distance to\nthe chosen object at most. According to the authors, the poor\nperformance of YOLOBOT in the learning track, contrary to\nits success in the planning tracks, was due to the collision\nmodel created by themselves that did not work well.\nB. Other learning agents\nOne of the ﬁrst works that used this framework as a learning\nenvironment was carried out by Samothrakis et al. [61], who\nemployed Neuro-Evolution in 10 games of the benchmark.\nConcretely, the authors experimented with Separable Natural\nEvolution Strategies (S-NES) using two different policies ( ϵ-\ngreedy versus softmax) and a linear function approximator\nversus a neural network as a state evaluation function. Features\nlike score, game status, avatar and other sprites information\nwere used to evolve learners during 1000 episodes. Results\nshow that ϵ-greedy with a linear function approximator was\nthe better combination to learn how to maximize scores on\neach game.\nBraylan and Miikkulainen [62] performed a study in which\nthe objective was to learn a forward model on 30 games. The\nobjective was to learn the next state from the current one\nplus an action, where the state is deﬁned as a collection of\nattribute values of the sprites (spawns, directions, movements,\netc.), by means of logistic regression. Additionally, the authors\ntransfer the learnt object models from game to game, under\nthe assumption that many mechanics and behaviours are\ntransferable between them. Experiments showed the effective\nvalue of object model transfer in the accuracy of learning\nforward models, resulting in these agents being stronger at\nexploration.\nAlso in a learning setting, Kunanusont et al. [63], [64]\ndeveloped agents that were able to play several games via\nscreen capture. In particular, the authors employed a Deep\nQ-Network in 7 games of the framework of increasing com-\nplexity, and included several enhancements to GVGAI to\ndeal with different screen sizes and a non-visualization game\nmode. Results showed that the approach allowed the agent to\nlearn how to play in both deterministic and stochastic games,\nachieving a higher winning rate and game score as the number\nof episodes increased.\nApeldoorn and Kern-Isberner [65] proposed a learning agent\nwhich rapidly determines and exploits heuristics in an un-\nknown environment by using a hybrid symbolic/sub-symbolic\nagent model. The proposed agent-based model learned the\nweighted state-action pairs using a sub-symbolic learning ap-\nproach. The proposed agent has been tested on a single-player\nstochastic game, Camel Race , from the GVGAI framework,\nand won more than half of the games in different levels\nwithin the ﬁrst 100 game ticks, while the standard Q-Learning\nagent never won given the same game length. Based on\n[65], Dockhorn and Apeldoorn [66] used exception-tolerant\nHierarchical Knowledge Bases (HKBs) to learn the approx-\nimated forward model and tested the approach on the 2017\nGVGAI Learning track framework, respecting the competition\nrules. The proposed agent beats the best entry in the learning\ncompetition organized at CIG-17 [66], but still performed far\nworse than the best planning agents, which have access to the\nreal forward models.\nUsing the new GVGAI Gym, Torrado et al. [15] compared 3\nimplemented Deep Reinforcement Learning algorithms of the\nOpenAI Gym, Deep Q-Network (DQN), Prioritized Dueling\nDQN and Advance Actor-Critic (A2C), on 8 GVGAI games\nwith various difﬁculties and game rules. All the three RL\nagents perform well on most of the games, however, DQNs\nand A2C perform badly when no game score is given during\na game playing (only win or loss is given when a game\nterminates). These three agents have been used as sample\nagents in the learning competition organized at CIG-18.\nFinally, Justesen et al. [67] implemented A2C within the\nGVGAI-Gym interface in a training environment that allows\nlearning by procedurally generating new levels. By varying\nthe levels in which the agent plays, the resulting learning is\nmore general and does not overﬁt to speciﬁc levels. The level\ngenerator creates levels at each episode, producing them in a\nslowly increasing level of difﬁculty in response to the observed\nagent performance.\nC. Discussion\nThe presented agents differ between each other in the input\ngame state (Json string or screen capture), the amount of\nlearning time, the algorithm used. Additionally, some of the\nagents have been tested on a different set of games and\nsometimes using different game length (i.e., maximal number\nof game ticks allowed). None of the agents, which were\nsubmitted to the 2017 learning competition, using the classic\nGVGAI framework, have used screen capture.\nThe Sarsa-based agents performed surprisingly bad in the\ncompetition, probably due to the arbitrarily chosen parameters\nand very short learning time. Also, learning 3 levels and testing\non 3 more difﬁcult levels given only 5 minutes learning time\nis a difﬁcult task. An agent should take care of the learning\nbudget distribution and decide when to stop learning a level\nand to proceed the next one.\nThe learning agent using exception-tolerant HKBs [66]\nlearns fast. However, when longer learning time is allowed, it\nis dominated by Deep Reinforcement Learning (DRL) agents.\nOut of the 8 games tested by Torrado et al. [15], none of the\ntested 3 DRL algorithms outperformed the planning agents on\n6 games. However, on the heavily stochastic game Seaquest,\nA2C achieved almost double score than the best planning\nagent, MCTS.\nVII. M ETHODS FOR LEVEL GENERATION\nDifferent researchers used different approaches to generate\nlevels for the GVGAI framework. The following subsection\ndescribes all known generators either included in the frame-\nwork or developed during the competition.\nA. Constructive methods\nConstructive generators are designed to generate levels\nbased on general knowledge. For example: enemies should\nbe away from the avatar, walls shouldn’t divide the world into\nislands, etc. Based on the game the generator adjusts a couple\nof parameters and rules to ﬁt the game as, for example, the\nnumber of non-playable characters (NPCs) in the generated\nlevel. Constructive generators don’t need any simulations after\ngenerating the level. The following are the known constructive\ngenerators.\n1) Sample random generator: This is the most naive\nmethod to generate a level. The generator ﬁrst identiﬁes\nsolid sprites. Solid sprites block the avatar and all NPCs\nfrom moving and don’t react to anything. The generator adds\na selected solid sprite as a border for the generated level\nto prevent sprites from wandering outside the game screen.\nFollowed by adding one of each character in the level mapping\nsection to a random location in the level. This step ensures the\ngame is playable. Finally, it adds a random amount of random\nsprites from the level mapping to random locations in the level.\n2) Sample constructive generator: This generator uses\nsome general game knowledge to generate the level. First, the\ngenerator calculates the level dimensions and the number of\nsprites in the level, then labels game sprites based on their\ninteractions and sprite types. After that, it constructs a level\nlayout using the solid sprites, to later add the avatar to a\nrandom empty location. After knowing the avatar position, the\ngenerator adds harmful sprites (those that can kill the avatar)\nin a far location from the avatar and adds other sprites at any\nrandom free locations. Finally, the generator makes sure that\nthe number of goal sprites is sufﬁcient to prevent winning or\nlosing automatically when the game starts.\n3) Easablade constructive generator: This is the winner\ngenerator for the ﬁrst level generator competition. The gener-\nator is similar to the sample constructive generator but it uses\ncellular automata to generate the level instead of layering the\nobjects randomly. The cellular automata is run on multiple\nlayers. The ﬁrst layer is to design the map obstacles, followed\nby the exit and the avatar, then the goal sprites, harmful sprites,\nand others.\n4) N-Gram constructive generator: This generator uses a\nn-gram model to generate the level. The generator records\nthe player actions from a previous play-through. This action\nsequence is used to generate the levels using predeﬁned rules\nand constraints. For example, if the player uses the USE action\nquite often, the generator will include more enemies in the\nlevel. The n-gram is used to specify the rules. Instead of re-\nacting to each separate action, the model reacts to a n-sequence\nof actions. During the generation process, the algorithm keeps\ntrack of the number and position of every generated object to\nensure the generated sprites do not overpopulate the level. A\nsingle avatar sprite is placed in the lower half of the level.\n5) Beaupre’s constructive pattern generator: In this work,\nBeaupre et al. [68] automatically analyzed 97 different games\nfrom the GVG-AI framework using a 3x3 sliding window over\nall the provided GVG-AI levels. They constructed a dictionary\nof all the different patterns (they discovered 12,941 unique\npatterns) with labels about the type of objects in them. The\nconstructive generator starts by checking if the game contain\nsolid sprites (sprites that doesn’t allow player to pass through\nthem). If that was the case, the generator ﬁlls the edges using\nborder patterns (patterns that contain solid sprites and exists\non the edge of the maps). The rest of the game area is ﬁlled\nby random selecting of patterns that maintain the following\ntwo heuristics: 1) only one avatar sprite should be found in\nthe level; and 2) all non solid game areas area connected.\nB. Search-based methods\nSearch-based generators use simulations to make sure the\ngenerated level is playable and better than just placing random\nobjects. The following are the known search-based generators.\n1) Sample genetic generator: This is a search-based level\ngenerator based on the Feasible Infeasible 2 Population Ge-\nnetic Algorithm (FI2Pop). FI2Pop is a GA which uses 2\npopulations, one for feasible chromosomes and the other\nfor infeasible chromosomes. The feasible population tries\nto increase the difference between the OLETS agent (see\nSection IV-B) and one-step look ahead, while the infeasible\npopulation tries to decrease the number of chromosomes that\nviolate the problem constraints (i.e. at least one avatar in the\ngame, the avatar must not die in the ﬁrst 40 steps, etc.). Each\npopulation evolves on its own, where the children can transfer\nbetween the two populations. This generator initializes the\npopulation using sample constructive generator.\n2) Amy12 genetic generator: This generator is built on top\nof the sample genetic generator. The main idea is to generate a\nlevel that ﬁts a certain suspense curve. Suspense is calculated\nat each point in time, by calculating the number of actions that\nleads to death or tie using the OLETS agent. The algorithm\nmodiﬁes the levels to make sure the suspense curve is not\nconstant during the life time of the game. Good generators\nare aimed at producing 3 suspense peeks with values of 50%\n(where half of the actions, on average lead to losing the game).\nOne of the advantages of using this technique that it makes\nsure that the generated level is winnable. Games that are hard\nto win will have a higher peak in the suspense curve, which\nis not valued highly by the generator.\n3) Jnicho genetic generator: This generator [69] uses a\nstandard GA with similar crossover and mutation operators\nto the sample GA. The ﬁtness function used is a combination\nbetween the score difference and the constraints speciﬁed in\nthe sample genetic generator. The score difference is calculated\nbetween an Monte Carlo Tree Search agent and One Step Look\nAhead agent. The score difference is normalized between 0\nand 1 to make sure it won’t overshadow the constraint values.\n4) Number13 genetic generator: This is a modiﬁed version\nof the sample genetic generator. These modiﬁcations includes\nusing adaptive crossover mechanism, adaptive mutation rate,\na better agent than OLETS, and allowing crossover between\nfeasible and infeasible population, which is not allowed in the\nsample genetic generator.\n5) Sharif’s pattern generator: This generator is still work\nin progress. Sharif et al. [70] identiﬁed 23 different patterns by\nanalyzing the grouping of different game sprites from several\nGVG-AI games. They are working now on using these design\npatterns as a ﬁtness function for a search based generator.\n6) Beaupre’s evolutionary pattern generator: Similar to\nBeaupre’s constructive pattern generator in section VII-A5,\nthey used the constructed dictionary for designing a search\nbased generator. They modiﬁed the sample genetic generator\nprovided with the framework to work using patterns instead of\nusing game sprites. They also initialized the generator using\nthe constructive version to speed up the generation process.\nC. Constraint-based methods\n1) ASP generator: This generator [71] uses Answer Set\nProgramming (ASP) to generate levels. The main idea is to\ngenerate ASP rules that generate suitable levels for the current\ngame. The generated rules consists of three different types. The\nﬁrst type are basic rules, which are based on speciﬁc decisions\nto keep the levels simple (for instance, levels can only have one\nsprite per tile). The second type are game speciﬁc rules, which\nare extracted from the game description ﬁle. An example is\nthe identiﬁcation of singleton sprites that should only have\none sprite in the level. The last type are additional rules to\nminimize the search space. These rules limit the minimum and\nmaximum number of each sprite type. All the rules are evolved\nusing evolutionary strategy with the algorithm performance\ndifference between sampleMCTS and a random agent as the\nﬁtness function.\nD. Discussion\nThe presented generators differ in the amount of time\nneeded to generate a level and the features of the generated\ncontent. The constructive generators take the least amount of\ntime to generate a single level without a guarantee that the\ngenerated level is beatable. On the other hand, both search-\nbased and constraint-based generators take longer time but\ngenerate challenging beatable levels as they use automated\nplaying agents as a ﬁtness function. The constraint-based\ngenerator only takes long time to ﬁnd an ASP generator which\ncould be used to generate many different levels as fast as the\nconstructive generators, while search-based generators take a\nlong time to ﬁnd a group of similar looking levels.\nFor the generators that participated in the GVG-AI level\ngeneration competition (Easablade, Amy12, Jnicho, and Num-\nber13), they have been evaluated during IJCAI 2016 by asking\nthe conference delegates to play two randomly selected levels\nand choose a preferred one. Each generator was used to\ngenerate 3 levels for 4 different games (The Snowman, Free-\nway, Run, and Butterﬂies). Easablade was chosen most often\n(78.4%), followed by Number13, amyP2 and jnicho ( 40.3%,\n39.13% and 34.54%, respectively). The winner, Easablade,\ngenerated fewer objects than the opponents and nice looking\nlayouts produced by the cellular automata, which is likely\nis the main reason behind its victory. Most of the generated\nlevels by Easablade, however, were either unbeatable or easy\ncompared to the other generators.\nVIII. M ETHODS FOR RULE GENERATION\nThis section describes the different algorithms that are\nincluded in the framework or have been found in the litera-\nture [72] toward generating rules for the GVG-AI framework.\nA. Constructive methods\nConstructive methods are algorithms that generate the rules\nin one pass without the need to play the game. The constructive\nmethods often incorporate knowledge about game design to\ngenerate more interesting games.\n1) Sample random generator: This is the simplest generator\nprovided with the framework. The main idea is to gener-\nate a game that compiles with no errors. For example, the\ngame shouldn’t contain interactions such as killing the end\nof screen (EOS) sprite. The algorithm starts by generating\na random number of interactions by selecting two random\nsprites (including EOS) and a random interaction rule one\nby one. The algorithm checks that every interaction is valid\nbefore adding it to the generated game. After generating the\nrandom interactions, the algorithm generates two termination\nconditions, one for winning and one for losing. The losing\ncondition is ﬁxed to the avatar being killed, while the winning\nis either winning the game after a random amount of frames\nor winning the game when certain sprite count reaches zero.\n2) Sample constructive generator: This is a more complex\ngenerator that utilizes knowledge about VGDL language and\nlevel design to generate more interesting games. The algorithm\nstarts by classifying the game sprites into different categories,\nsuch as wall sprites (those that surround the level), col-\nlectible/harmful sprites (immovable sprites that cover around\n10% of the level), spawner sprites (sprites that spawn another),\netc. For each type of sprite, the algorithm has rules to generate\ninteractions based on them. For example, harmful sprites kill\nthe avatar on collision, wall sprites either prevent any movable\nobject from passing through or kill the movable object upon\ncollision, etc. For more details about the rules, the reader is\nreferred to [12]). After the game interactions are generated,\ntwo termination conditions are generated, one for winning and\none for losing. The losing condition is ﬁxed to the avatar’s\ndeath, while the winning condition depends on the current\nsprites. For example: if collectible sprites exist in the current\ndeﬁnition, the winning condition is set to collect them all.\nB. Search-based methods\nSearch-based methods use a search based algorithm to ﬁnd a\ngame based on certain criteria that ensure the generated game\nhave better rules than just randomly choosing them.\n1) Sample genetic generator: Similar to the level genera-\ntion track, the search based algorithm uses FI2Pop to evolve\nnew games. As discussed before, FI2Pop keeps two popula-\ntions one for feasible games and the other for infeasible games.\nThe infeasible games tries to become feasible by satisfying\nmultiple constraints such as minimizing the number of bad\nframes (frames contains sprites outside the level boundaries)\nunder certain threshold, the avatar doesn’t die in the ﬁrst 40-\nframes, etc. On the other hand, the feasible chromosomes try\nto maximize its ﬁtness. The ﬁtness consists of two parts, the\nﬁrst part is to maximize the difference in performance between\nthe OLETS and MCTS agents, and the difference between\nMCTS and random agent. The second part is to maximize the\nnumber of interaction rules that ﬁres during the simulation of\nthe generated game.\n2) Thorbjrn generator: This generator [72] is similar to the\nsample genetic generator. It tries to maximize the difference\nbetween the performance of different algorithms. This gener-\nator uses evolutionary strategies with mutation and crossover\noperators to generate an entire game instead of an interaction\nset and termination conditions.\nC. Discussion\nSimilar to the level generators, the difference between the\ndifferent generators is the time used in creation and the\nfeatures in the output game. The constructive methods take less\ntime but do not guarantee different games or playability, while\nthe search-based generators take long time to generate one\ngame, attempting to satisfy the playability constraints using\nautomated playing agents. Thorbjorn is the only generator that\ncreates the whole game, not only the interaction rules and\ntermination conditions, which makes it harder to compare to\nthe rest of the generators.\nThe remaining ones are the 3 sample generators that come\nwith the framework, which are compared to each other by\ndoing a user study on the generated games [12]. The generators\nare used to generate 3 new games for 3 different levels\n(Aliens, Boulderdash, and Solarfox). The participants in the\nstudy were subjected to two generated games by two randomly\nselected generators and asked to pick the one they prefer. The\nconstructive generator was the preferred one (chosen 76.38%\nof the time), followed by the genetic ( 44.73%) and random\n(24.07%) generators. An explanation for the low preference\nshown for the genetic generator could be its ﬁtness function:\nit incorporates a constraint that tries to make sure that the\ngame sprites are always in the playing area. This constraint\ncaused the GA in the current allocated time to favor games\nthat limit considerably the movement of the sprites.\nIX. R ESEARCH THAT BUILDS ON GVGAI\nA. Learn the domain knowledge\nBeside the work relevant to the learning competition, there\nare some other research work around Reinforcement Learning\nusing the GVGAI framework. Narasimhan et al. [73] combined\na differentiable planning module and a model-free component\nto a two-part representation, obtained by mapping the collected\nannotations for game playings to the transitions and rewards, to\nspeed up the learning. The proposed approach has been tested\non 4 GVGAI single-player games and shown its effectiveness\non both transfer and multi-task scenarios on the tested games.\nThe GVGAI Learning Competition proposes to use a screen-\nshot of the game screen (at pixel level) at every game tick to\nrepresent the current game state. Instead of directly using the\nscreen-shot, Woof and Chen [74] used an Object Embedding\nNetwork (OEN), which extracted the objects in the game state\nand compressed object feature vectors (e.g., position, distance\nto the nearest sprite, etc.) into one single ﬁxed-length feature\nvector. The DRL agent based on OEN has been evaluated\non 5 of the GVGAI single-player games and showed various\nperformance levels on the tested games [74].\nB. AI-assisted game design\nMachado et al. [75] implemented a recommender system\nbased on the VGDL to recommend game elements, such as\nsprites and mechanics. Then, the recommender system was\nexpanded to Cicero [76], [77], an AI-assisted game design\nand debugging tool built on top of the GVGAI. Cicero has a\nstatistics tool of the interactions to help ﬁguring out the unused\ngame rules; a visualization system to illustrate the information\nabout game objects and events, a mechanics recommender, a\nquery system [78] for in-game data, a playtrace aggregator,\na heatmap-based game analysis system and a retrospective\nanalysis application SeekWhence [79]. The gameplay sessions\nby human players or AI agents can be recorded and every\nsingle frame at every game tick can be easily extracted for\nfurther study and analysis.\nRecently, Liu et al. [80] applied a simple Random Mutation\nHill Climber (RMHC) and a Multi-Armed Bandit RMHC\ntogether with resampling methods to tune game parameters\nautomatically. Games instances with signiﬁcant skill-depth\nhave been evolved using GVGAI agents. Furthermore, Ku-\nnanusont et al. [51] evolved simultaneously the GVGAI agents\nas part of the game (opponent models).\nGuerrero et al. [81] explored ﬁve GVGAI agents using\nfour different heuristics separately on playing twenty GVGAI\ngames, allowing different behaviors according to the diverse\nscenarios presented in the games. In particular, this work\nexplored heuristics that were not focused on winning the game,\nbut to explore the level or interact with the different sprites\nof the games. These agents can be used to evaluate generated\ngames, thus help evolve them with preferences to particular\nbehaviors.\nKhalifa et al. [82] modiﬁed MCTS agents by editing the\nUCT formula used in the agent. Human playing data has\nbeen used for modeling to make the modiﬁed agents playing\nin a human-like way. Primary results showed that one of\nthe studied agents achieved a similar distribution of repeated\nactions to the one by human players. The work was then\nextended by Bravi et al. [49], in which game-play data have\nbeen used to evolve effective UCT alternatives for a speciﬁc\ngame. The MCTS agents using new formulas, with none\nor limited domain information, are compared to a standard\nimplementation of MCTS (the sampleMCTS agent of GVGAI)\non the game Missile Command. Applying the UCT alternatives\nevolved using game-playing data to a standard MCTS signif-\nicantly improved its performance.\nBesides designing games and the agents used in them,\nthe automatic generation of video game tutorials (aimed at\nhelping players understanding how to play a game) is also\nan interesting sub-ﬁeld of study. Green et al. [83] pointed\nout that the GVGAI Framework provides an easy testbed for\ntutorial generation. The game rules in GVGAI are deﬁned\nin VGDL, therefore the tutorial generation can be easily\nachieved by reading and translating VGDL ﬁles. Further Green\net al. [84] build a system (AtDelﬁ) that generates tutorials\nusing the VGDL ﬁle and automated AI agents. AtDelﬁ reads\nthe VGDL ﬁle and build a graph of interactions between\nthe game sprites. AtDelﬁ analyzes the graph to identify the\nwinning path (sequence of nodes starting from player sprite\nthat leads to the winning condition in the graph), losing paths\n(sequence of nodes starting from the losing condition till there\nis no dependency), and score path (sequence of nodes starting\nfrom player sprite that leads to score change in the graph).\nThese paths are represented as text and videos that explain to\nthe user how to play the game. The text is generated using\na string replacement method to generate a human readable\ninstructions, while the videos are recorded using a group of\nautomated agents that won the General Video Game Playing\nCompetition [8] and record every group of frames that cause\none of the interactions on the path to trigger.\nA more recent work by Anderson et al. [85] focused on\ndesigning deceptive games to deceive AI agents and lead\nthe agents away from a globally optimal policy. Designing\nsuch games helps understand the capabilities and weaknesses\nof existing AI agents and can serve at a preparation step\nfor designing a meta-agent for GVGP which combines the\nadvantages of different agents. The authors categorized the de-\nceptions and imported various types of deception to the exist-\ning GVGAI games by editing the corresponding VGDL ﬁles.\nThe agents submitted to the GVGAI single-player planing\ncompetition have been tested on the new games. Interestingly,\nthe ﬁnal ranking of the agents on each of the games differed\nsigniﬁcantly from the rankings in the GVGAI competition.\nThe new designed deceptive games successfully explored the\nweaknesses of agents which have performed well on the test\nset of the ofﬁcial competition.\nFinally, C. Guerrero-Romero et al., in a vision paper [86],\nproposed a methodology that consists of the use of a team\nof general AI agents with differentiated skill levels and goals\n(winning, exploring, eliminating sprites, collecting items, etc.).\nThe methodology is aimed at aiding game design by analyzing\nthe performance of this team of agents as a whole and the\nprovision of logged and visual information that shows the\nagent experience through the game.\nC. Game generation with RAPP\nNielsen et al. [87] proposed Relative Algorithm Perfor-\nmance Proﬁle (RAPP) as a measure of relative performance\nof agents and tested their approach on different general game-\nplaying AI agents using GVGAI framework. The authors\nshowed that well-designed games have clear skill-depth, thus\nbeing able to distinct good or bad players. In other words,\na strong agent or human player should perform signiﬁcantly\nbetter than a weak agent or human player over multiple\nplayings on well-designed games. For instance, a skillful agent\nis expected to perform better than a random agent, or one that\ndoes not move.\nThen, Nielsen et al. [72] integrated the differences of\naverage game scores and win rate between any agent and a\nrandom agent to the evaluation of new games either randomly\ngenerated or generated by editing existing GVGAI games.\nThough most of the resulted games are interesting to play,\nthere are some exceptions, in which the core challenge of the\ngame has been removed. For instance, the enemy can not heart\nthe player, which makes it no more an enemy. But it still\nprovides useful starting points for human designers.\nKunanusont et al. [51] extended the idea of RAPP. Five\nGVGAI agents and a deterministic agent designed for the\ntested Space Battle Game are used as the candidate opponent,\nwhich is considered as part of the game to be evolved.\nTwo GVGAI agents, One Step Look Ahead (weak), MCTS\n(strong) and the deterministic agent (mediocre), are used to\nplay multiple times the evolved game for evaluation. The\nevaluation function is deﬁned as the minimum of the difference\nof game scores between the strong and mediocre agents, and\nthe difference of game scores between the mediocre and weak\nagents, aiming at generating games that can clearly distinguish\nstronger agents and weak agents.\nRecently, Kunanusont et al. [88] used the NTBEA to evolve\ngame parameters in order to model player experience within\nthe game. The authors were able to ﬁnd parameterizations of\nthree games that, when played by MCTS and RHEA agents,\nproduce predeﬁned and different score trends.\nD. Robustness testing\nPerez-Liebana et al. [89] ran a study on the winners of\nthe 2014 and 2015 editions of the single player planning\ncompetition in order to analyze how robust they were to\nchanges in the environment with regards to actions and re-\nwards. The aim of this work was to analyze a different type\nof generality: controllers for this framework are developed\nto play in multiple games under certain conditions, but the\nauthors investigated which could be the effect of breaking\nthose compromises: an inaccurate Forward Model, an agent\nthat does not execute the move decided by the algorithm or\nscore penalties incurred by performing certain actions.\nAn interesting conclusion on this study is that, once the\nconditions have been altered, sample agents climb up to the\ntop of the rankings and the good controllers behave worse.\nAgents that rely on best ﬁrst search or A* (such as YOLOBOT\nor Return42, already described in this paper) handled noise\nvery badly. MCTS also showed to be quite robust in this\nregard, above other Rolling Horizon agents that could not\ncope so well with these changes. This work also reinforced\nthe idea that the GVGAI framework and competition are also\nrobust. Despite the changes in the performance of the agents,\nsome controllers do better than others under practically all\nconditions. The opposite (rankings depending only on noise\nfactors, for instance) would mean that the framework is fragile.\nMore recently, Stephenson et al. [90] have pointed out that\nthe selection of a proper subset of games for comparing a\nnew algorithm with others is critical, as using a non-suitable\nrepresentative subset may have a bias to some algorithms.\nMore general, the questions is, given a set of sample problems,\nhow to sample a subset as fair as possible for the algorithms to\nbe tested, and to avoid the bias to any of the algorithms. The\nauthors use an information-theoretic method in conjunction\nwith game playing data to assist in the selection of GVGAI\ngames. Games with higher information gains are used for\ntesting a new agent.\nX. D ISCUSSION AND OPEN RESEARCH PROBLEMS ON\nSINGLE - AND TWO -PLAYER PLANNING\nThe single- and two-player planning versions of GVGAI\nare the ones that have received most attention and research.\nDespite their popularity and efforts, the best approaches rarely\nsurpass an approximately 50% victory rate in competition\ngame sets, with very low victory rate in a great number\nof games. Similarly, different MCTS and RHEA variants\n(including many of the enhancements studied in the literature)\nstruggle to achieve a higher than 25% victory rate in all (more\nthan a hundred) single-player games of the framework. There-\nfore, increasing performance in a great proportion of games is\nprobably the most challenging problem at the moment.\nLiterature shows multiple enhancements on algorithms and\nmethods aiming to improve this performance, but in the vast\nmajority of cases the improvements only affect a subset of\ngames or certain conﬁgurations of the algorithms. While this\nis understandable due to the nature of general video game\nplaying, it also shows that the current approaches does not\nwork in order to reach truly general approaches that work\nacross board.\nThe work described in this survey has shown, however,\ninteresting insights that can point us in the right direction. For\ninstance, several studies show that using more sophisticated\n(i.e. with A* or other methods such as potential ﬁelds)\ndistances to sprites as features works better than Euclidean\ndistances. The downside is that computing these measurements\ntake an important part of the decision time budget, which can’t\nbe used in case it is needed for some games or states where\nthe best action to take is not straight-forward.\nIn general, one could say that one of the main points to\naddress is how to use the decision time more wisely. Some\napproaches tried to make every use of the forward model\ncount, like those agents that attempt to learn facts about the\ngame during the rollouts of MCTS. Again, some attempts in\nthis direction have provided marginal improvements, but the\nproblem may be trying to design a general feature extractor.\nIn other words, what we try to learn is inﬂuenced by what\nwe know about existing games (i.e. some sprites are good,\nother are bad, some spawn other entities and there are sprites\n- resources - that the avatar can collect). Some games may\nrequire features that have not been thought of, especially\nbecause the challenge itself presents games that have not been\nseen before.\nAnother improvement that has been tried in several studies\nis the use of macro-actions (in most cases, a repetition of an\naction during several consecutive steps) to i) make the action\nspace coarser; and ii) make a better use of the time budget.\nAgain, these modiﬁcations have improved performance in\ncertain games (including some that had not been won by any\nalgorithm previously) but they either did not have an impact\nin others, or they made the performance worse. It is likely\nthat different games can beneﬁt from different macro-action\nlengths (so work could be done on trying to automatically and\ndynamically adapt the number of times the action is repeated)\nbut also of more complex structures that allow for high-level\nplanning. In fact, games that require high-level planning are\nstill an open problem to be solved in this setting.\nGames classiﬁcation and the use of hyper-heuristics are also\nan interesting area for research. Some of the best approaches\nup to date, as YOLOBOT, do make a differentiation between\nstochastic and deterministic games to later use one or another\nalgorithm. An open challenge is how to make this classiﬁcation\nmore accurate and detailed, so an approach could count on a\nportfolio of (more than 2) algorithms that adapt to every game.\nAttempts have been made to classify with game features, but\nresults suggest that these classiﬁcations and the algorithms\nused are not strong enough. Devising more general features\nfor this, maybe focused on the agent game-play experience\nrather than game features, is a line of future research.\nAll these unsolved issues apply to both single- and two-\nplayer settings, although the latter case adds the difﬁculty of\nhaving an opponent to compete or collaborate with. There are\ntwo open problems that arise from this: ﬁrst, no study has\nbeen made that tries to identify the game and behavior of the\nopponent as collaborative or competitive. Analysis of the other\nplayer’s intentions can be seen as a sub-ﬁeld on its own, only\nthat in this case we add the general game playing component\nto it. Secondly, some advancements have been done in using\nopponent models that go beyond random, but investigation in\nmore complicated opponent models that better capture and\nlearn the behavior of the other player could potentially yield\nbetter results.\nBeside the development of agents for game playing, AI-\nassisted game design, automatic game testing and game\ndebugging using GVGAI agents have attracted researchers’\nattention. Some work around evolving game skill-depth using\nrelative performance between GVGAI agents have been done\nrecently, and most of this work has been focused on Relative\nAlgorithm Performance Proﬁles (RAPP), where performance\nis measured in terms of how well the agents play the given\ngames. However, it is sensible to explore other aspects of agent\ngame-play to inﬂuence game design. Factors like the amount\nof level explored by different agents (so a generator favors\nthose levels or games that allow for a wider exploration, or\nmaybe a progressive one), their decisiveness [91] on selecting\nthe best action to take or the entropy of their moves can also\nbe used to this end.\nXI. E DUCATIONAL USE OF GVGAI\nThe GVGAI framework has been used to provide engaging\nassignments for taught modules, and as the basis for many\nMSc dissertation projects. The descriptions below give an idea\nof the educational uses of GVGAI but are not intended to be\nan exhaustive list.\nA. Taught Modules\nGVGAI has been used in at least two distinct ways within\ntaught modules. The most typical way is to use design speciﬁc\naspects of the course around the framework, teaching the\nstudents about the main concepts of GVGAI with examples\nof how to write agents for the selected tracks. This is then\nfollowed up with an assignment, where a signiﬁcant weight is\ngiven to how well each student or group’s entry performs in the\nleague. Several institutions have run private leagues for this,\nincluding Otto V on Guericke Universit¨at Magdeburg, Univer-\nsity of Essex, University of Muenster, Universidad Carlos III\nde Madrid, Universidad de Malaga and New York University.\nRunning a private league means the course supervisor has full\ncontrol over the setup of the league, including when students\ncan enter and how thoroughly the entries are evaluated, and\nthe set of games to evaluate them on. For the 2-player\ntrack, this also allows control over the opponents chosen.\nThe Southern University of Science and Technology and the\nNanjing University have also used GVGAI framework in their\nAI modules, without running a private league, as assignments\nwhen teaching search or reinforcement learning methods.\nAnother use-case in taught modules is to teach the VGDL\npart of framework, then set the development of novel and\ninteresting games as the assignment. This was done to good\neffect at IT University of Copenhagen, where the students\nproduced a number of challenging puzzle games that were later\nused in the training and validation sets of the planning track. A\nsimilar approach was taken in a module on AI-Assisted Game\nDesign at the University of Essex, where planning track games\nwere also produced.\nB. MSc Dissertation Projects\nGVGAI offers an extensive range of interesting research\nchallenges, some of which have been addressed in MSc\ndissertation projects. The majority of the ones we are aware of\nhave focused on the single-player planning track, but this is not\nsurprising as it was the ﬁrst track to be developed. The single-\nplayer planning track also has the beneﬁt of providing some\ngood sample agents as starting points for further work either\nin the sense of extending the sample agents to achieve higher\nperformance, or using the sample agents as a useful source\nof comparison. A good example is the work on MCTS with\noptions, in which options refer to action sequences designed\nfor speciﬁc subgoals. The version with options signiﬁcantly\noutperformed the sample MCTS agent on most of the games\nstudied: as with many cases what began as an MSc thesis was\nlater published as a conference paper [29]. In our experience\nthis usually provides an excellent educational experience for\nthe student. Other planning track thesis include [21], the real-\ntime enhancements of [22], knowledge-based variants [46] and\ngoal-oriented approaches [53].\nBeyond the planning tracks, other examples (already de-\nscribed in this survey) include applying Answer-Set Program-\nming (ASP) [92] or GAs [69] to the level generation track and\nlearning from game screen capture [63]. [63] was essentially a\nlearning track approach before the learning track was running.\nFinally, another approach is to extend the framework in some\nway, such as developing the two-player learning track [93].\nXII. F UTURE DIRECTIONS\nThe GVGAI framework and competition are in constant\ndevelopment. The opportunities that this benchmark provides\nfor different lines of research and education are varied, and\nthis section outlines the future directions planned ahead for\nthe following years.\nA. New tracks\nAs new challenges are proposed, the possibility of organiz-\ning them as competition tracks arise. Below are listed some\npossible new tracks that can attract interesting research areas.\n1) Automatic game design: The game design involves,\nbut not limited to, game generation, level generation, rule\ngeneration and play-testing (playing experience, game feeling,\nfun, etc.), study of game market, user interface design and\naudio design. The automatic game design becomes an active\nresearch topic since the late 2000’s. A review of the state of\nthe art in automatic game design can be found in [80].\nA Game Generation track would aim at providing AI\ncontrollers which automatically generate totally new games or\ngame instances by varying the game parameters, i.e., parameter\ntuning. How to achieve the former is an open question. The\nstraight-forward way would be providing a particular theme, a\ndatabase of game objects, or searching spaces of game rules,\nwith which the participants can generate new games. The ideal\ncase would be that the controllers automatically create totally\nnew games from nothing. Though there is a yawning gulf\nbetween aspiration and reality, an interdisciplinary ﬁeld com-\nbining automatic game design and domain-speciﬁc automatic\nprogramming is expected. The latter, automatic game tuning,\nis relatively easier. Some search-based and population-based\nmethods have been applied to game parameter optimization\naiming at maximizing the depth of game variants [80] or\nﬁnding more playable games.\n2) Multi-Player GVGAI: Multi-agent games has drawn\npeople’s attention, for instance, real time strategy games (e.g.\nStarCraft) and board games (e.g. Mahjong). The study of\nmulti-agent GVGAI is a fruitful research topic. Atari games\ncan also be extended to multi-agent games. In particular,\nthe Pac-Man can be seen as a multi-agent game and related\ncompetitions have been held since 2011. The most recent\nMs Pac-Man vs Ghost Team Competition [1], which included\npartial observability, was held at CIG in 2016. Nevertheless,\na more general multi-agent track is favorable.\nThe interface of the Two-player Planning Track was initially\ndeveloped for two or more players, so it has the potential to\nbe expanded to a Multi-player Planning Track, in which an\nagent is allowed to control more than one player or each of\nthe players is controlled by a separate agent. This future track\ncan be expanded again as a multi-agent learning framework,\nproviding a two-or-more-player learning track.\n3) Turing Test GVGAI: Determining if an agent that is\nplaying a game is a human or a bot is a challenge that has\nbeen subject of study for many years [1], and the idea of\napplying it to a general video game setting is not new [94].\nThis offers an interesting opportunity to extend the framework\nto having a Turing Test Track where participants create AI\nagents that play like humans for any game that is given. Albeit\nthe understandable difﬁculty of this problem, the interest for\nresearch in this area is signiﬁcant: what are the features that\ncan make an agent play like a human in any game?\nB. General directions\nThere are several improvements and additions to the frame-\nwork that can be done and would potentially affect all existent\nand future competition tracks. One of these continuous mod-\niﬁcations is the constant enlargement of the games library.\nNot only new games are added for each new edition of the\ncompetition, but the work done on automatic game design\nusing the GVGAI framework has the potential to create inﬁnite\nnumber of games that can be integrated into the framework.\nAdding more games can also be complemented with com-\npatibility with other systems. Other general frameworks like\nOpenAI Gym [95], Arcade Learning Environment (ALE) [96]\nor Microsoft Malm ¨o [97] count on a great number of single-\nor multi-player, model-free or model-based tasks. Interfacing\nwith these systems would greatly increase the number of\navailable games which all GVGAI agents could play via a\ncommon API. This would also open the framework to 3D\ngames, an important section of the environments the current\nbenchmark does not cover.\nWith regards to the agents, another possibility is to provide\nthem with a wider range of available actions. For instance,\nthe player could be able to apply more than one action\nsimultaneously, or these actions could form a continuous\naction space (i.e. pressing a throttle in a range between 0 and\n1). This would enhance the number of legal combinations for\nthe agent to choose from at each decision step.\nBeside the framework itself, the website for GVGAI could\nalso be improved to provide better and faster feedback to\nthe competition participants. More data analysis features can\nbe added, such as visualization of the score changes during\nthe game playing, the action entropy and the exploration of\nthe game world (heat-map of visited positions). A related\nwork is to provide better and more ﬂexible support for game\nplay metric logging, better support for data mining of results\ntogether with visualization, and better data saving, which will\nhelp enabling to upload replays (i.e., action logs) from AI\nagents and human play-throughs.\nAnother envisaged feature is being able to play the game\nin a web browser (without any download or installation) by\nan AI agent or human, and visualize the analyzed features\nduring the game playing in real time. A bonus will be the\neasy parameterization options for games, thus a player or\nan AI agent can easily set up the parameters and rules to\ndeﬁne the desired game by inserting values directly or generate\npseudo-randomly a level to play using some pre-implemented\nautomatic game tuning techniques given some particular goals\nor features.\nXIII. C ONCLUSIONS\nThe GVGAI framework offers the most comprehensive\nsystem to date for evaluating the performance of general\nvideo game playing agents, and for testing general purpose\nalgorithms for creating new games and creating new content\nfor novel games. The framework has been used in multiple\ninternational competitions, and has been used to evaluate the\nperformance of hundreds of general video game agents.\nThe agent tracks cater for planning agents able to exploit a\nfast forward model, and learning agents that must learn to react\nsensibly without the beneﬁts of a forward model. The planning\ntrack already comes in single- and two-player versions; the\nlearning track is currently single-player only, but with a two-\nplayer version envisaged. Although long-term learning may\nalso be used within the planning track, the best-performing\nagents have, as far as we know, not yet done this. Recent\nsuccesses in Go indicate what can be achieved by combining\nlearning and planning, so applying a similar system within\nGVGAI is an interesting prospect. In fact, the combination of\ndifferent approaches into one is an interesting avenue of future\nresearch. And example is the work described in this survey that\nmixes learning and procedural level generation [67], but one\ncould imagine further synergies such as content generation and\nlearning for two-player games.\nThe main alternative to GVGAI is the ALE [96]. At the time\nof writing, ALE offers higher-quality games than GVGAI as\nthey were home-console commercial games of a few decades\nago. In GVGAI terms, ALE offers just two tracks: single-\nplayer learning and planning, with the learning track being\nthe more widely used. For future work on machine learning in\nvideo games, we predict that the two-player tracks will become\nthe most important, as they offer open-ended challenges based\non an arms race of intelligence as new players are developed,\nand are also outside of the current scope of ALE. Although\nALE has had so far a greater uptake within some sectors of\nthe machine learning community, GVGAI beneﬁts from being\nmuch more easily extensible than ALE: it is easy to create new\nVGDL games, easy to create new levels for these games, and\neasy to create level generators for them as well. It is also easy\nto automatically generate variations on existing VGDL games\nand their levels. This allows for training on arbitrarily large\nsets of game variations and level variations. In contrast, agents\ntrained on ALE games run a serious risk of overﬁtting to the\ngame and level they are trained on. An immediate priority is to\ntest the rich set of ALE agents on the equivalent GVGAI-tracks\nto gain a sense of the relative difﬁculty of each environment\nand to learn more of the relative challenges offered by each.\nThe content creation tracks offer an extremely hard chal-\nlenge: creating rules or levels for unseen games. Promising\ndirections include the further development and exploitation of\na range of general game evaluation measures [91], and greater\nuse of the best GVGAI agents to perform the play-testing of\nthe novel rules and levels.\nThe VGDL has been an important part of GVGAI to date,\nsince it makes it possible to rapidly and concisely specify\nnew games. However, it is also a source of limitation, as its\nlimited expressiveness makes it hard to make games which\nare fun for humans to play. VGDL also limits the ease with\nwhich complex game mechanics can be embedded in games,\nwhich in turn limits the depth of challenge that can be posed\nfor the GVGAI agents. Hence an important future direction\nis the authoring of GVGAI-compatible games in any suitable\nlanguage which conform to the necessary GVGAI API in order\nto ensure compatibility with the desired GVGAI track.\nFinally, while the above discussion provides a compelling\ncase for the future of GVGAI as a tool for academic study, we\nalso believe that when it reaches a higher level of maturity it\nwill provide an important tool for game designers. The vision\nis to provide an army of intelligent agents with a range of\nplay-testing abilities, and a diverse set of metrics with which\nto analyze a range of important functional aspects of a game.\nACKNOWLEDGEMENTS\nThe authors would like to thank the participants of all tracks\nof the competition for their work and submitted controllers\nand generators. This work was partially supported by the\nEPSRC CDT in Intelligent Games and Game Intelligence\n(IGGI) EP/L015846/1, the Shenzhen Peacock Plan (Grant\nNo. KQTD2016112514355531), the Science and Technol-\nogy Innovation Committee Foundation of Shenzhen (Grant\nNo. ZDSYS201703031748284) and the Program for Uni-\nversity Key Laboratory of Guangdong Province (Grant No.\n2017KSYS008).\nREFERENCES\n[1] G. N. Yannakakis and J. Togelius, Artiﬁcial Intelligence and Games .\nSpringer, 2018.\n[2] S. J. Russell and P. Norvig, Artiﬁcial Intelligence: a Modern Approach .\nMalaysia; Pearson Education Limited,, 2016.\n[3] D. Perez-Liebana, S. Samothrakis, J. Togelius, S. M. Lucas, and\nT. Schaul, “General Video Game AI: Competition, Challenges and\nOpportunities,” in Thirtieth AAAI Conference on Artiﬁcial Intelligence ,\n2016, pp. 4335–4337.\n[4] M. Ebner, J. Levine, S. M. Lucas, T. Schaul, T. Thompson, and\nJ. Togelius, “Towards a Video Game Description Language,” Dagstuhl\nFollow-Ups, vol. 6, 2013.\n[5] J. Levine, C. B. Congdon, M. Ebner, G. Kendall, S. M. Lucas, R. Mi-\nikkulainen, T. Schaul, and T. Thompson, “General Video Game Playing,”\nDagstuhl Follow-Ups, vol. 6, 2013.\n[6] T. Schaul, “A Video Game Description Language for Model-based or In-\nteractive Learning,” in IEEE Conference on Computational Intelligence\nin Games (CIG) , 2013, pp. 1–8.\n[7] ——, “An Extensible Description Language for Video Games,” IEEE\nTransactions on Computational Intelligence and AI in Games , vol. 6,\nno. 4, pp. 325–331, 2014.\n[8] D. Perez, S. Samothrakis, J. Togelius, T. Schaul, S. Lucas, A. Cou ¨etoux,\nJ. Lee, C.-U. Lim, and T. Thompson, “The 2014 General Video Game\nPlaying Competition,”IEEE Transactions on Computational Intelligence\nand AI in Games , vol. 8, pp. 229–243, 2015.\n[9] R. D. Gaina, D. Prez-Libana, and S. M. Lucas, “General Video Game\nfor 2 Players: Framework and Competition,” in 8th Computer Science\nand Electronic Engineering (CEEC) . IEEE, Sep. 2016, pp. 186–191.\n[10] R. D. Gaina, A. Cou ¨etoux, D. J. Soemers, M. H. Winands, T. V odopivec,\nF. Kirchgessner, J. Liu, S. M. Lucas, and D. Perez-Liebana, “The 2016\ntwo-player gvgai competition,” IEEE Transactions on Games , vol. 10,\nno. 2, pp. 209–220, June 2018.\n[11] A. Khalifa, D. Perez-Liebana, S. M. Lucas, and J. Togelius, “General\nVideo Game Level Generation,” in Proceedings of the 2016 Annual\nConference Genetic and Evolutionary Computation Conference . ACM,\n2016, pp. 253–259.\n[12] A. Khalifa, M. C. Green, D. P ´erez-Li´ebana, and J. Togelius, “General\nVideo Game Rule Generation,” in Computational Intelligence and\nGames (CIG), 2017 IEEE Conference on . IEEE, 2017, pp. 170–177.\n[13] D. P ´erez-Li´ebana, M. Stephenson, R. D. Gaina, J. Renz, and S. M.\nLucas, “Introducing Real World Physics and Macro-actions to General\nVideo Game AI,” inComputational Intelligence and Games (CIG), 2017\nIEEE Conference on . IEEE, 2017, pp. 248–255.\n[14] J. Liu, D. Perez-Liebana, and S. M. Lucas, “The Single-\nPlayer GVGAI Learning Framework - Technical Manual,”\n2017. [Online]. Available: http://www.liujialin.tech/publications/\nGVGAISingleLearning manual.pdf\n[15] R. R. Torrado, P. Bontrager, J. Togelius, J. Liu, and D. Perez-Liebana,\n“Deep Reinforcement Learning in the General Video Game AI frame-\nwork,” in Conference on Computational Intelligence and Games , 2018,\np. to appear.\n[16] D. Ashlock, D. P ´erez-Li´ebana, and A. Saunders, “General Video Game\nPlaying Escapes the No Free Lunch Theorem,” in Conference on\nComputational Intelligence and Games . IEEE, 2017, pp. 17–24.\n[17] T. Joppen, M. U. Moneke, N. Schr ¨oder, C. Wirth, and J. F ¨urnkranz,\n“Informed Hybrid Game Tree Search for General Video Game Playing,”\nIEEE Transactions on Games , vol. 10, no. 1, pp. 78–90, 2018.\n[18] T. Geffner and H. Geffner, “Width-based Planning for General Video-\nGame Playing,” inEleventh Artiﬁcial Intelligence and Interactive Digital\nEntertainment Conference, 2015, pp. 23–29.\n[19] D. Soemers, C. Sironi, T. Schuster, and M. Winands, “Enhancements for\nReal-Time Monte-Carlo Tree Search in General Video Game Playing,”\nin Computational Intelligence and Games (CIG), 2016 IEEE Conference\non, 2016, pp. 1–8.\n[20] J. Liu. GVGAI Single-Player Learning Competition at IEEE\nCIG17. [Online]. Available: https://www.slideshare.net/ljialin126/\ngvgai-singleplayer-learning-competition-at-ieee-cig17\n[21] T. Schuster, “MCTS Based Agent for General Video Games,” Master’s\nthesis, Maastricht University, 2015.\n[22] D. Soemers, “Enhancements for Real-Time Monte-Carlo Tree Search in\nGeneral Video Game Playing,” Master’s thesis, Maastricht Univ., 2016.\n[23] D. Perez, S. Samothrakis, and S. Lucas, “Knowledge-based Fast Evo-\nlutionary MCTS for General Video Game Playing,” in Conference on\nComputational Intelligence and Games . IEEE, 2014, pp. 1–8.\n[24] F. Frydenberg, K. R. Andersen, S. Risi, and J. Togelius, “Investigating\nMCTS Mmodiﬁcations in General Video Game Playing,” in Conference\non Computational Intelligence and Games . IEEE, 2015, pp. 107–113.\n[25] C. Y . Chu, T. Harada, and R. Thawonmas, “Biasing Monte-Carlo\nRollouts with Potential Field in General Video Game Playing,” in IPSJ\nKansai-Branch Convention, 2015, pp. 1–6.\n[26] C. Y . Chu, H. Hashizume, Z. Guo, T. Harada, and R. Thawonmas,\n“Combining Pathfmding Algorithm with Knowledge-based Monte-Carlo\nTree Search in General Video Game Playing,” in Conference on Com-\nputational Intelligence and Games . IEEE, 2015, pp. 523–529.\n[27] H. Park and K.-J. Kim, “MCTS with Inﬂuence Map for General Video\nGame Playing,” in Computational Intelligence and Games (CIG), 2015\nIEEE Conference on . IEEE, 2015, pp. 534–535.\n[28] E. H. dos Santos and H. S. Bernardino, “Redundant Action Avoidance\nand Non-Defeat Policy in the Monte Carlo Tree Search Algorithm for\nGeneral Video Game Playing,” inProceedings do XVI Simpsio Brasileiro\nde Jogos e Entretenimento Digital , 2017.\n[29] M. d. Waard, D. M. Roijers, and S. C. Bakkes, “Monte Carlo Tree\nSearch with Options for General Video Game Playing,” in 2016 IEEE\nConference on Computational Intelligence and Games (CIG) . IEEE,\n2016, pp. 47–54.\n[30] D. Perez-Liebana, S. Mostaghim, and S. M. Lucas, “Multi-objective Tree\nSearch Approaches for General Video Game Playing,” in Congress on\nEvolutionary Computation (CEC) . IEEE, 2016, pp. 624–631.\n[31] A. Khalifa, M. Preuss, and J. Togelius, “Multi-objective Adaptation of a\nParameterized GVGAI Agent Towards Several Games,” in International\nConference on Evolutionary Multi-Criterion Optimization . Springer,\n2017, pp. 359–374.\n[32] I. Bravi, A. Khalifa, C. Holmg ˚ard, and J. Togelius, “Evolving UCT Al-\nternatives for General Video Game Playing,” in The IJCAI-16 Workshop\non General Game Playing , 2016, p. 63.\n[33] A. Babadi, B. Omoomi, and G. Kendall, “EnHiC: An Enforced Hill\nClimbing Based System for General Game Playing,” in Computational\nIntelligence and Games (CIG), 2015 IEEE Conference on. IEEE, 2015,\npp. 193–199.\n[34] M. J. Nelson, “Investigating Vanilla MCTS Scaling on the GVG-AI\nGame Corpus,” in 2016 IEEE Conference on Computational Intelligence\nand Games (CIG) . IEEE, 2016, pp. 402–408.\n[35] I. Bravi, D. Perez-Liebana, S. M. Lucas, and J. Liu, “Shallow Decision-\nmaking Analysis in General Video Game Playing,” in Computational\nIntelligence and Games (CIG), 2018 IEEE Conference on , 2018, p. to\nappear.\n[36] R. D. Gaina, J. Liu, S. M. Lucas, and D. P ´erez-Li´ebana, “Analysis of\nVanilla Rolling Horizon Evolution Parameters in General Video Game\nPlaying,” in European Conference on the Applications of Evolutionary\nComputation. Springer, 2017, pp. 418–434.\n[37] B. S. Santos, H. S. Bernardino, and E. Hauck, “An Improved Rolling\nHorizon Evolution Algorithm with Shift Buffer for General Game\nPlaying,” in Brazilian Symposium on Computer Games and Digital\nEntertainment, 2018.\n[38] B. S. Santos and H. S. Bernardino, “Game State Evaluation Heuristics\nin General Video Game Playing,” in Brazilian Symposium on Computer\nGames and Digital Entertainment , 2018.\n[39] R. D. Gaina, S. M. Lucas, and D. Perez-Liebana, “Tackling Sparse Re-\nwards in Real-Time Games with Statistical Forward Planning Methods,”\nin AAAI Conference on Artiﬁcial Intelligence (AAAI-19) , 2019, p. to\nappear.\n[40] B. Jia, M. Ebner, and C. Schack, “A GP-based Video Game Player,” in\nProceedings of the 2015 Annual Conference on Genetic and Evolution-\nary Computation. ACM, 2015, pp. 1047–1053.\n[41] B. Jia and M. Ebner, “A Strongly Typed GP-based Video Game Player,”\nin Computational Intelligence and Games (CIG), 2015 IEEE Conference\non. IEEE, 2015, pp. 299–305.\n[42] R. D. Gaina, S. M. Lucas, and D. P ´erez-Li´ebana, “Population Seeding\nTechniques for Rolling Horizon Evolution in General Video Game\nPlaying,” in Evolutionary Computation (CEC), 2017 IEEE Congress on .\nIEEE, 2017, pp. 1956–1963.\n[43] ——, “Rolling Horizon Evolution Enhancements in General Video\nGame Playing,” in Computational Intelligence and Games (CIG), 2017\nIEEE Conference on . IEEE, 2017, pp. 88–95.\n[44] D. Perez Liebana, J. Dieskau, M. Hunermund, S. Mostaghim, and\nS. Lucas, “Open Loop Search for General Video Game Playing,” in\nProceedings of the 2015 Annual Conference on Genetic and Evolution-\nary Computation. ACM, 2015, pp. 337–344.\n[45] H. Horn, V . V olz, D. P´erez-Li´ebana, and M. Preuss, “MCTS/EA Hybrid\nGVGAI Players and Game Difﬁculty Estimation,” in Computational\nIntelligence and Games (CIG), 2016 IEEE Conference on. IEEE, 2016,\npp. 1–8.\n[46] J. van Eeden, “Analysing and Improving the Knowledge-based Fast\nEvolutionary MCTS Algorithm,” Master’s thesis, Utrecht University,\n2015.\n[47] C.-Y . Chu, S. Ito, T. Harada, and R. Thawonmas, “Position-based Re-\ninforcement Learning Biased MCTS for General Video Game Playing,”\nin Conference on Computational Intelligence and Games (CIG) . IEEE,\n2016, pp. 444–451.\n[48] E. Ilhan and A. S ¸. Uyar, “Monte Carlo Tree Search with Temporal-\nDifference Learning for General Video Game Playing,” in Computa-\ntional Intelligence and Games (CIG), 2017 IEEE Conference on. IEEE,\n2017, pp. 317–324.\n[49] I. Bravi, A. Khalifa, C. Holmg ˚ard, and J. Togelius, “Evolving Game-\nSpeciﬁc UCB Alternatives for General Video Game Playing,” in Eu-\nropean Conference on the Applications of Evolutionary Computation .\nSpringer, 2017, pp. 393–406.\n[50] C. F. Sironi, J. Liu, D. Perez-Liebana, R. D. Gaina, I. Bravi, S. M.\nLucas, and W. M. H. M, “Self-Adaptive MCTS for General Video Game\nPlaying,” in European Conference on the Applications of Evolutionary\nComputation. Springer, 2018, p. to appear.\n[51] K. Kunanusont, R. D. Gaina, J. Liu, D. Perez-Liebana, and S. M. Lucas,\n“The n-tuple bandit evolutionary algorithm for automatic game improve-\nment,” in Evolutionary Computation (CEC), 2017 IEEE Congress on .\nIEEE, 2017, pp. 2201–2208.\n[52] C. F. Sironi and M. H. M. Winands, “Analysis of Self-adaptive Monte\nCarlo Tree Search in General Video Game Playing,” in Conference on\nComputational Intelligence and Games , 2018, p. to appear.\n[53] B. Ross, “General Video Game Playing with Goal Orientation,” Master’s\nthesis, University of Strathclyde, 2014.\n[54] I. Azaria, A. Elyasaf, and M. Sipper, Evolving Artiﬁcial General\nIntelligence for Video Game Controllers. Cham: Springer International\nPublishing, 2018, pp. 53–63. [Online]. Available: https://doi.org/10.\n1007/978-3-319-97088-2 4\n[55] P. Bontrager, A. Khalifa, A. Mendes, and J. Togelius, “Matching\nGames and Algorithms for General Video Game Playing,” in Artiﬁcial\nIntelligence and Interactive Digital Entertainment Conference, 2016, pp.\n122–128.\n[56] A. Mendes, J. Togelius, and A. Nealen, “Hyperheuristic General Video\nGame Playing,” in Computational Intelligence and Games (CIG), 2016\nIEEE Conference on . IEEE, 2016, pp. 1–8.\n[57] R. D. Gaina, S. M. Lucas, and D. Perez-Liebana, “General Win\nPrediction from Agent Experience,” in Computational Intelligence and\nGames (CIG), 2018 IEEE Conference on , 2018, p. to appear.\n[58] ——, “VERTIGO: Visualisation of Rolling Horizon Evolutionary Al-\ngorithms in GVGAI,” in The 14th AAAI Conference on Artiﬁcial\nIntelligence and Interactive Digital Entertainment , 2018, pp. 265–267.\n[59] J. M. Gonzalez-Castro and D. Perez-Liebana, “Opponent Models Com-\nparison for 2 Players in GVGAI Competitions,” in 9th Computer Science\nand Electronic Engineering Conference . IEEE, 2017, pp. 151–156.\n[60] D. Ashlock, E.-Y . M. Kim, and D. Perez-Liebana, “Toward General\nMathematical Game Playing Agents,” in Computational Intelligence and\nGames (CIG), 2018 IEEE Conference on , 2018, p. to appear.\n[61] S. Samothrakis, D. Perez-Liebana, S. M. Lucas, and M. Fasli, “Neu-\nroevolution for General Video Game Playing,” in Conference on Com-\nputational Intelligence and Games . IEEE, 2015, pp. 200–207.\n[62] A. Braylan and R. Miikkulainen, “Object-Model Transfer in the General\nVideo Game Domain,” in Twelfth Artiﬁcial Intelligence and Interactive\nDigital Entertainment Conference , 2016.\n[63] K. Kunanusont, “General Video Game Artiﬁcial Intelligence: Learning\nfrom Screen Capture,” Master’s thesis, University of Essex, 2016.\n[64] K. Kunanusont, S. M. Lucas, and D. P ´erez-Li´ebana, “General Video\nGame AI: Learning from Screen Capture,” in Evolutionary Computation\n(CEC), 2017 IEEE Congress on . IEEE, 2017, pp. 2078–2085.\n[65] D. Apeldoorn and G. Kern-Isberner, “An Agent-Based Learning Ap-\nproach for Finding and Exploiting Heuristics in Unknown Environ-\nments,” in COMMONSENSE, 2017.\n[66] A. Dockhorn and D. Apeldoorn, “Forward Model Approximation for\nGeneral Video Game Learning,” in Computational Intelligence and\nGames (CIG), 2018 IEEE Conference on , 2018, p. to appear.\n[67] N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius, and\nS. Risi, “Illuminating Generalization in Deep Reinforcement Learning\nthrough Procedural Level Generation,” arXiv:1806.10729, 2018.\n[68] S. Beaupre, T. Wiles, S. Briggs, and G. Smith, “A Design Pattern\nApproach for Multi-Game Level Generation,” in Artiﬁcial Intelligence\nand Interactive Digital Entertainment . AAAI, 2018.\n[69] J. Nichols, “The Use of Genetic Algorithms in Automatic Level Gener-\nation,” Master’s thesis, University of Essex, 2016.\n[70] M. Sharif, A. Zafar, and U. Muhammad, “Design Patterns and General\nVideo Game Level Generation,” Intl. Journal of Advanced Computer\nScience and Applications , vol. 8, no. 9, pp. 393–398, 2017.\n[71] X. Neufeld, S. Mostaghim, and D. Perez-Liebana, “Procedural Level\nGeneration with Answer Set Programming for General Video Game\nPlaying,” in 7th Computer Science and Electronic Engineering (CEEC) .\nIEEE, 2015, pp. 207–212.\n[72] T. S. Nielsen, G. A. Barros, J. Togelius, and M. J. Nelson, “Towards\nGenerating Arcade Game Rules with VGDL,” in Conference on Com-\nputational Intelligence and Games . IEEE, 2015, pp. 185–192.\n[73] K. Narasimhan, R. Barzilay, and T. S. Jaakkola, “Deep Transfer\nin Reinforcement Learning by Language Grounding,” CoRR, vol.\nabs/1708.00133, 2017.\n[74] W. Woof and K. Chen, “Learning to Play General Video-Games via an\nObject Embedding Network,” in Computational Intelligence and Games\n(CIG), 2018 IEEE Conference on , 2018, p. to appear.\n[75] T. Machado, I. Bravi, Z. Wang, A. Nealen, and J. Togelius, “Shopping\nfor game mechanics,” in Proceedings of the FDG Workshop on Proce-\ndural Content Generation , 2016.\n[76] T. Machado, A. Nealen, and J. Togelius, “Cicero: A mixed-initiative\nAI-assisted game design tool,” in Proceedings of the 12th International\nConference on the Foundations of Digital Games , ser. FDG’17. New\nYork, NY , USA: ACM, 2017.\n[77] T. Machado, D. Gopstein, A. Nealen, O. Nov, and J. Togelius, “AI-\nassisted Game Debugging with Cicero,” in IEEE Congress on Evolu-\ntionary Computation (CEC), 2018 IEEE Conference on . IEEE, 2018,\np. to appear.\n[78] T. Machado, D. Gopstein, A. Nealen, and J. Togelius, “Kwiri - What,\nWhen, Where and Who: Everything you ever wanted to know about\nyour game but didnt know how to ask,” in Knowledge Extraction From\nGames Workshop. AAAI, 2019, p. to appear.\n[79] T. Machado, A. Nealen, and J. Togelius, “SeekWhence a Retrospective\nAnalysis Tool for General Game Design,” in Proceedings of the 12th\nInternational Conference on the Foundations of Digital Games , ser.\nFDG ’17. New York, NY , USA: ACM, 2017, pp. 4:1–4:6. [Online].\nAvailable: http://doi.acm.org/10.1145/3102071.3102090\n[80] J. Liu, J. Togelius, D. P ´erez-Li´ebana, and S. M. Lucas, “Evolving Game\nSkill-Depth using General Video Game AI Agents,” in IEEE Congress\non Evolutionary Computation (CEC), 2017 IEEE Conference on. IEEE,\n2017, pp. 2299–2307.\n[81] C. Guerrero-Romero, A. Louis, and D. P ´erez-Li´ebana, “Beyond Playing\nto Win: Diversifying Heuristics for GVGAI,” in Conference on Compu-\ntational Intelligence and Games . IEEE, 2017, pp. 118–125.\n[82] A. Khalifa, A. Isaksen, J. Togelius, and A. Nealen, “Modifying MCTS\nfor Human-Like General Video Game Playing,” in IJCAI, 2016, pp.\n2514–2520.\n[83] M. C. Green, A. Khalifa, G. A. Barros, and J. Togelius, ““Press\nSpace To Fire”: Automatic Video Game Tutorial Generation,” in Fourth\nExperimental AI in Games Workshop , 2017.\n[84] M. C. Green, A. Khalifa, G. A. Barros, T. Machado, A. Nealen, and\nJ. Togelius, “AtDelﬁ: Automatically Designing Legible, Full Instructions\nFor Games,” in Foundation of Digital Games , 2018.\n[85] D. Anderson, M. Stephenson, J. Togelius, C. Salge, J. Levine, and\nJ. Renz, “Deceptive Games,” arXiv preprint arXiv:1802.00048 , 2018.\n[86] C. Guerrero-Romero, S. M. Lucas, and D. Perez-Liebana, “Using a\nTeam of General AI Algorithms to Assist Game Design and Testing,” in\nComputational Intelligence and Games (CIG), 2018 IEEE Conference\non, 2018, p. to appear.\n[87] T. S. Nielsen, G. A. Barros, J. Togelius, and M. J. Nelson, “General\nVideo Game Evaluation using Relative Algorithm Performance Proﬁles,”\nin European Conference on the Applications of Evolutionary Computa-\ntion. Springer, 2015, pp. 369–380.\n[88] K. Kunanusont, S. M. Lucas, and D. Perez-Liebana, “Modelling Player\nExperience with the N-Tuple Bandit Evolutionary Algorithm,” in Arti-\nﬁcial intelligence and Interactive Digital Entertainment (AIIDE) , 2018.\n[89] D. P ´erez-Li´ebana, S. Samothrakis, J. Togelius, T. Schaul, and S. M.\nLucas, “Analyzing the Robustness of General Video Game Playing\nAgents,” in Computational Intelligence and Games (CIG), 2016 IEEE\nConference on, 2016, pp. 1–8.\n[90] M. Stephenson, D. Anderson, A. Khalifa, J. Levine, J. Renz, J. Togelius,\nand C. Salge, “A Continuous Information Gain Measure to Find the\nMost Discriminatory Problems for AI Benchmarking,” arXiv preprint\narXiv:1809.02904, 2018.\n[91] V . V olz, D. Ashlock, S. Colton, S. Dahlskog, J. Liu, S. Lucas, D. Perez-\nLiebana, and T. Thompson, “Gameplay Evaluation Measures,” Dagstuhl\nFollow-Ups, 2017.\n[92] X. Neufeld, “Procedural level generation with answer set programming\nfor general video game playing,” Master’s thesis, University of Magde-\nburg, 2016.\n[93] R. D. Gaina, “The 2 Player General Video Game Playing Competition,”\nMaster’s thesis, University of Essex, 2016.\n[94] J. Lehman and R. Miikkulainen, “General Video Game Playing as\na Benchmark for Human-Competitive AI,” in AAAI-15 Workshop on\nBeyond the Turing Test, 2015.\n[95] G. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schul-\nman, J. Tang, and W. Zaremba, “Openai Gym,” arXiv preprint\narXiv:1606.01540, 2016.\n[96] M. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling, “The Arcade\nLearning Environment: An evaluation platform for general agents,” J.\nArtif. Intell. Res.(JAIR) , vol. 47, pp. 253–279, 2013.\n[97] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell, “The Malmo\nPlatform for Artiﬁcial Intelligence Experimentation,” in IJCAI, 2016,\npp. 4246–4247.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7351042032241821
    },
    {
      "name": "Video game",
      "score": 0.6270717978477478
    },
    {
      "name": "Video game design",
      "score": 0.6174831390380859
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.5620077252388
    },
    {
      "name": "Game design",
      "score": 0.5542843341827393
    },
    {
      "name": "Game mechanics",
      "score": 0.5328619480133057
    },
    {
      "name": "Multimedia",
      "score": 0.5155088901519775
    },
    {
      "name": "Game Developer",
      "score": 0.46359604597091675
    },
    {
      "name": "Turns, rounds and time-keeping systems in games",
      "score": 0.43593668937683105
    },
    {
      "name": "Track (disk drive)",
      "score": 0.4343891441822052
    },
    {
      "name": "Competition (biology)",
      "score": 0.4310719072818756
    },
    {
      "name": "Video game development",
      "score": 0.4235702455043793
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 63
}