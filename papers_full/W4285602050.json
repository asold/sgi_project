{
  "title": "Contrastive Graph Transformer Network for Personality Detection",
  "url": "https://openalex.org/W4285602050",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3008021805",
      "name": "Yangfu Zhu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2164292348",
      "name": "Linmei Hu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2999862266",
      "name": "Xinkai Ge",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2884984836",
      "name": "Wanrong Peng",
      "affiliations": [
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A1970514160",
      "name": "Bin Wu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3176540316",
    "https://openalex.org/W3177080218",
    "https://openalex.org/W2997212048",
    "https://openalex.org/W3173710861",
    "https://openalex.org/W2806246579",
    "https://openalex.org/W3173644040",
    "https://openalex.org/W3127917488",
    "https://openalex.org/W1985449126",
    "https://openalex.org/W2963248507",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W2064394365",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2894536868",
    "https://openalex.org/W2767879018",
    "https://openalex.org/W2763585929",
    "https://openalex.org/W2784934784",
    "https://openalex.org/W3035156228",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W4250304879",
    "https://openalex.org/W2423024114",
    "https://openalex.org/W3130967929"
  ],
  "abstract": "Personality detection is to identify the personality traits underlying social media posts. Most of the existing work is mainly devoted to learning the representations of posts based on labeled data. Yet the ground-truth personality traits are collected through time-consuming questionnaires. Thus, one of the biggest limitations lies in the lack of training data for this data-hungry task. In addition, the correlations among traits should be considered since they are important psychological cues that could help collectively identify the traits. In this paper, we construct a fully-connected post graph for each user and develop a novel Contrastive Graph Transformer Network model (CGTN) which distills potential labels of the graphs based on both labeled and unlabeled data. Specifically, our model first explores a self-supervised Graph Neural Network (GNN) to learn the post embeddings. We design two types of post graph augmentations to incorporate different priors based on psycholinguistic knowledge of Linguistic Inquiry and Word Count (LIWC) and post semantics. Then, upon the post embeddings of the graph, a Transformer-based decoder equipped with post-to-trait attention is exploited to generate traits sequentially. Experiments on two standard datasets demonstrate that our CGTN outperforms the state-of-the-art methods for personality detection.",
  "full_text": "Contrastive Graph Transformer Network for Personality Detection\nYangfu Zhu1y\n, Linmei Hu1y\u0003\n, Xinkai Ge1 , Wanrong Peng2 , Bin Wu1\u0003\n1Beijing Key Laboratory of Intelligence Telecommunication Software and Multimedia, Beijing\nUniversity of Posts and Telecommunications, Beijing, China\n2Medical Psychological Center, the Second Xiangya Hospital, Central South University, Changsha, China\n{zhuyangfu, hulinmei,gexinkai2021,wubin}@bupt.edu.cn, wanrongpeng@csu.edu.cn\nAbstract\nPersonality detection is to identify the personality\ntraits underlying social media posts. Most of the\nexisting work is mainly devoted to learning the rep-\nresentations of posts based on labeled data. Yet the\nground-truth personality traits are collected through\ntime-consuming questionnaires. Thus, one of the\nbiggest limitations lies in the lack of training data\nfor this data-hungry task. In addition, the correla-\ntions among traits should be considered since they\nare important psychological cues that could help\ncollectively identify the traits. In this paper, we\nconstruct a fully-connected post graph for each user\nand develop a novel Contrastive Graph Transformer\nNetwork model (CGTN) which distills potential la-\nbels of the graphs based on both labeled and un-\nlabeled data. Speciﬁcally, our model ﬁrst explores\na self-supervised Graph Neural Network (GNN) to\nlearn the post embeddings. We design two types of\npost graph augmentations to incorporate different\npriors based on psycholinguistic knowledge of Lin-\nguistic Inquiry and Word Count (LIWC) and post\nsemantics. Then, upon the post embeddings of the\ngraph, a Transformer-based decoder equipped with\npost-to-trait attention is exploited to generate traits\nsequentially. Experiments on two standard datasets\ndemonstrate that our CGTN outperforms the state-\nof-the-art methods for personality detection.\n1 Introduction\nPersonality refers to the characteristic pattern in a person’s\nthinking, feeling, and decision-making [Kaushal and Pat-\nwardhan, 2018]. Personality detection is an emerging topic\nin user proﬁle research, which aims to identify one’s per-\nsonality traits from online texts he/she creates and has ex-\npanded to massive applications such as recommendation sys-\ntem [Shen et al., 2020], dialogue system [Yang et al., 2021b;\nWen et al., 2021 ] and computer game design [Lang et al.,\n2019].\nyEqual contribution.\n\u0003Corresponding authors.\nWith the blossoming of social media, users yield con-\nsiderable posts containing their mental activities every day,\noffering new possibilities for automatically inferring per-\nsonality traits [ˇStajner and Yenikent, 2020 ]. Earlier re-\nsearchers mainly used two sources of lexical features, Lin-\nguistic Inquiry and Word Count (LIWC) [Tausczik and Pen-\nnebaker, 2010] and Medical Research Council (MRC) [Colt-\nheart, 1981] to identify personality from user-generated posts\n[Mairesse et al., 2007 ]. To overcome manual feature engi-\nneering, deep neural networks (DNNs) were applied in the\npersonality detection task to obtain the representations of\nposts. However, understanding the hidden personality traits\nbehind the posts is non-trivial. Most recent works have been\ndevoted to reﬁning post representations from the perspective\nof the post structure including [Lynn et al., 2020 ], [Yang et\nal., 2021c ],and [Yang et al., 2021a ]. Despite the consid-\nerable improvements achieved in personality detection, the\nexisting models are likely to suffer from the scarcity of per-\nsonality tags as the ground-truth personality traits are usually\ncollected from professional questionnaires, which are often\nresource-intensive and time-consuming. Hence, such pre-\ncious personality tags are hard to collect, which becomes a\nlimitation for training deep neural networks and makes it dif-\nﬁcult to infer personality from posts.\nIn addition, personality is deﬁned in terms of different di-\nmensions (traits) and these traits often co-occur with a non-\nnegligible correlation, which has been conﬁrmed in empiri-\ncal psychological researches[John et al., 2008; Sharpe et al.,\n2011]. For example, neurotic people are more likely to be\nextroverted, like Trump. However, such implicit trait corre-\nlations are rarely exploited, which should have been the key\npsychological cues to be considered for personality detection.\nTaking both the problem of data scarcity and the corre-\nlations among traits into consideration, we model the user-\ngenerated posts as a fully-connected post graph, and pro-\npose a novel Contrastive Graph Transformer Network model\n(CGTN) for personality detection, which distills potential la-\nbels of the graphs based on both labeled and unlabeled data.\nSpeciﬁcally, CGTN consists of a contrastive post graph en-\ncoder and a trait sequence decoder. In post graph encoder,\ntwo types of graph augmentations are designed to incorpo-\nrate different priors based on psycholinguistic knowledge of\nLIWC and post semantics. To be precise, LIWC can be uti-\nlized to extract psycholinguistic features while post semantics\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4559\nare able to capture the semantic relations among the posts. A\nself-supervised paradigm is deﬁned to maximize the agree-\nment over the representations of the augmented graphs that\ncome from the same user. This contrastive strategy allows\nus to learn the post embeddings without using any labeled\ndata. In trait sequence decoder, we view the multi-trait de-\ntection task as a trait sequence generation problem and apply\na transformer-based decoder to model the correlations among\ntraits. In addition, we use the post-to-trait attention to ensure\nthat crucial posts are selected for trait generation.\nIn summary, our main contributions are as follows:\n•To our best knowledge, this is the ﬁrst effort to explore\ncontrastive self-supervised learning to distill auxiliary\nsignals for personality detection, providing a new per-\nspective for alleviating the data scarcity for personality\ndetection.\n•We propose a novel Contrasive Graph Transformer Net-\nwork (CGTN) model, for which we design two types\nof graph augmentations to incorporate priors based on\nLIWC, and post semantic knowledge, and explicitly in-\ntroduce trait correlations by exploiting a sequence gen-\neration model.\n•The experimental results demonstrate the outperfor-\nmance of our model over the baselines including the\nstate-of-the-art methods, which shows the effectiveness\nof our model.\n2 Related Work\nAs an emerging interdisciplinary study, personality detec-\ntion has attracted the attention of both computer scientists\nand psychologists [Xue et al., 2018; Mehta et al., 2020;\nYang et al., 2021c].\nEarlier studies mainly exploit psychologically statistical\nfeatures to detect personality [Mairesse et al., 2007 ], such\nas LIWC [Tausczik and Pennebaker, 2010] and MRC [Colt-\nheart, 1981]. Nonetheless, the statistical analysis cannot ef-\nfectively represent the original semantics of the posts. With\nthe rapid development of deep learning, a series of Deep Neu-\nral Networks (DNNs) are applied to personality detection task\nand have achieved great success, including CNN [Xue et al.,\n2018], LSTM [Tandera et al., 2017 ], etc. Recently, person-\nality detection has beneﬁted from large-scale pre-trained lan-\nguage models, such as BERT [Devlin et al., 2018], and thus\nget improved[Mehta et al., 2020; Ren et al., 2021 ]. Based\non these pre-trained models, latest works focus on reﬁning\npost representations from the perspective of post structure.\n[Lynn et al., 2020 ] designed SN+Attn which introduces a\nhierarchical attention network to obtain user document rep-\nresentations in a bottom-up manner from the word-grained\nlevel to the post-grained level, arguing that not every post\ncontributes equally. In order to avoid introducing post-order\nbias, Transformer-MD [Yang et al., 2021a] considers differ-\nent posts to be unrelated to each other. TrigNet [Yang et al.,\n2021c], however, holds a different view that there is a psy-\ncholinguistic structure between posts and constructs a hetero-\ngeneous graph for each user and aggregates post information\nfrom a psychological perspective.\nHowever, the above methods mainly focus on obtaining\nthe representations of the user’s posts by the supervised\nparadigms. For personality detection task, human-provided\nlabels are hard to collect, thus the model tends to overﬁt the\ntraining data and performs poorly on test data. In this work,\nto address the issue, we develop a novel contrastive graph\ntransformer model for personality detection, which fully ex-\nploits both labeled and unlabeled data through contrastive\nself-supervised learning.\n3 Preliminaries\nPersonality detection can be phrased as a multi-document\nmulti-label classiﬁcation task [Lynn et al., 2020; Yang et al.,\n2021a]. Formally, given a set P = {p1;p2:::pn}of N posts\nfrom a user, where pi = {w1\ni;w2\ni:::wk\ni}is i-th post with k\ntokens, our goal is to predict t-dimensional personality traits\nfrom the trait-speciﬁc label space Y ={y1;y2;···yt}, e.g.,t\n= 4 in the MBTI taxonomy, t= 5 in the Big-Five taxonomy.\nIn this paper, we model a user-generated document as graph\nover posts. For each user with nposts, we construct a fully-\nconnected original graph G = (V; E), where V consists of\nnpost nodes and the edges Ecapture the correlations among\nthe posts. The BERT is employed to obtain the initial embed-\ndings of each post node. And then based on the post graph,\nwe propose a Contrastive Graph Transformer Network model\n(CGTN) for personality detection.\n4 Contrastive Graph Transformer Network\nFigure 1 presents the overall architecture of the proposed\nCGTN, which consists of a contrastive post graph encoder\nand a trait sequence decoder. The encoder aims to learn\nrich post representations via self-discrimination on post graph\nwhile the decoder is to uncover psychological cues contained\nin the personality correlations. In the following subsections,\nwe detail the contrastive post graph encoder and trait se-\nquence decoder.\n4.1 Contrastive Post Graph Encoder\nIn contrastive post graph encoder, we design two types of\ngraph augmentations based on psycholinguistic knowledge\nof LIWC and post semantics. Thereafter, contrastive self-\nsupervised learning is exploited on augmentations graph to\nlearn post representation by judging whether two augmented\ngraphs are from the same user.\nPost Graph Augmentation\nThe core of personality detection is to understand a collec-\ntion of user-generated posts. Previous works demonstrated\nthat digging the inherent patterns in the structure of post is\nhelpful for representation. Self-supervised learning allows us\nto exploit the “unlabeled” data via making disturbs on the in-\nput data. Naturally, we can construct the “unlabeled” data by\ngenerating multi-view post graphs for each user. Speciﬁcally,\nLIWC is used to construct psycholinguistic view graphs G\u000b\n[Yanget al., 2021c]. The LIWC dictionary divides words into\npsychology-related categories C = {c1;c2 ···;cn}which can\nbe taken as bridges to connect different post nodes. Two post\nnodes are connected if they contain the words of the same\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4560\nFigure 1: An overview of our CGTN, which consists of a contrastive post graph encoder and a trait sequence decoder.\ncategories. For the semantic view graphs G\f, We build the\nedges between the posts if their semantic similarity is larger\nthan a given threshold. The semantic similarity is computed\nas the cosine similarity based on the initial post embeddings.\nGraph Contrastive Self-supervised Learning\nContrastive self-supervised learning offers a simple way to\nlearn invariant representations by local disturbs in the input\ndata without using any labeled data. In our task, we randomly\nsample a batch of U users and any pair of augmented graphs\n(G\u000b;G\f) that comes from the same user is considered as a\npositive pair. Otherwise, they are labeled as negative. We\nlearn to predict whether two augmented graphs originate from\nthe same user or not. In the following, we ﬁrst introduce how\nwe learn the representation of a graph and then illustrate the\ncontrastive loss.\nSpeciﬁcally, to obtain the graph representation, we ﬁrst\nuse GNN to capture the structural information within nodes’\nneighborhoods [Xu et al., 2018 ]. The L-th layer GNN up-\ndates the post node embeddings hp as:\nhL\np = GNN(hL−1\np0 ); (1)\nwhere p′is the neighbour node of post node p on the given\naugmented graph. where hL\np is the embedding of the node p\nat the Llayer. After obtaining the post node embeddings with\nfused neighbor information, we pass them through an average\npooling layer and a two-layer MLP to obtain the entire graph\nrepresentation. Formlly,\nzu = MLP(Avg(hL\np));u ∈U: (2)\nBased on the above graph embedding, the psycholinguistic\naugmentation graph and the semantic augmentation graph of\nuser u are represented as z\u000b\nu and z\f\nu, respectively. Given a\npositive pair (z\u000b\nu;z\f\nu) and a negative pair (z\u000b\nu;z\f\nv), which is\nsampled from the augmented graphs of other users v within\nthe same batch. The contrastive loss Lcl is deﬁned to max-\nimize the consistency between positive pairs compared with\nnegative pairs:\nLcl =\nX\nu∈U\n−log exp(sim(z\u000b\nu;z\f\nu)=\u001c)\nP\nv∈U\nexp(sim(z\u000b\nu;z\f\nv)=\u001c); (3)\nwhere sim() denotes the cosine similarity and \u001c is a temper-\nature hyperparameter.\n4.2 Trait Sequence Decoder\nUnlike single-trait classiﬁcation where only one label is as-\nsigned to each sample, a decoder with Transformer [Vaswani\net al., 2017] backbones is designed to capture the correlations\nof traits by the sequence generation architecture. In addition\nwe design post-to-trait attention to select the key posts for\ntrait generation. Formally, the trait generation can be mod-\neled as ﬁnding an optimal trait sequence y∗that maximizes\nthe conditional probability:\nP(y|H\u000b\nu;H\f\nu) =\nTY\nt=1\np(yt|y1;y2;···;yt−1; H\u000b\nu;H\f\nu); (4)\nwhere H\u000b\nu = [h\u000b\np1\n; h\u000b\np2\n;···;h\u000b\npn\n] is post sequence based\non psycholinguistic view graph G\u000b, similarly, H\f\nu is post se-\nquence based on semantic view graph G\f.\nThe decoder as shown in the right part of Figure 1 is com-\nposed of M identical blocks, where each block contains a\nmulti-head self-attention layer, a post-to-trait attention layer\nand a feed-forward layer. Formally, the output of the ﬁrst sub-\nlayer Cm, the second sub-layer Dm, and the third sub-layer\nEmat m-th decoding block are sequentially calculated as:\nCm = LN(SATT(Em−1) +Em−1); (5)\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4561\nDm = LN((PTATT(Cm;Hu) +Cm)); (6)\nEm = LN(FFN(Dm) +Dm); (7)\nwhere LN( ·) denotes layer normalization, SATT(·) denotes\nmulti-head self-attention mechanism, PTATT( ·) is post-to-\ntrait attention layer we inserted, and FFN( ·) is feed-forward\nnetwork, Hu = {H\u000b\nu;H\f\nu}denotes two post sequences, re-\nspectively.\nPost-to-trait Attention\nWe design post-to-trait attention sub-layer to select crucial\nposts from the two augmented views for generating traits.\nThis inserted sub-layer includes two steps: ﬁrst, two view-\nspeciﬁc post sequences H\u000b\nu and H\f\nu are fed into the decoding\nmodule simultaneously. For each decoding step, the decoder\nprocesses each view independently and obtains two contex-\ntual sequences (Cm\np→t)\u000b and (Cm\np→t)\f. Formally:\nCm\np→t = ATT(Cm;Hu); (8)\nSubsequently, we leverage cross-view self-attention over two\nsequences to control different contributions of different views\nat each step. Formally:\nPTATT(·) =SATT((Cm\np→t)\u000b;(Cm\np→t)\f); (9)\nTrait Generation\nFinally, the output of the last layer of the decoderEmis used\nto detect the personality via linear and softmax layer. The\ngeneration of the t-th trait by the decoder can be formalized\nas\nbyt = softmax(WEm+ It); (10)\nwhere It is the mask vector at decoding step t that is used\nto prevent the decoder from detecting the repeated trait. In\ninferring stage, byt is further used as input token of the next\ngeneration step to detect the (t+ 1)-th trait:\n(It)t0 =\n\u001a−∞ if the t′-th traits has been detected\n0 otherwise ;\n(11)\nFollowing SGM [Yang et al., 2018 ], we use beam search to\nﬁnd the top-ranked prediction path at generation time. The\nﬁnal output is trained using the mean binary cross-entropy\nover all traits. Given true binary label vector yt and predicted\nlabels byt, the detection loss is:\nLdet = −\nUX\nu\nYX\nt=1\n(ytlog (^yt) + (1−yt) log (1−^yt)):\n(12)\n4.3 Model Training\nWe apply two training strategies including pre-training and\njoint learning. For the pre-training strategy, the model is\ntrained in a two-stage paradigm. Given a collection of un-\nlabeled post graphs, a direct contrastive method is to predict\nwhether two augmented graphs are similar. After training, we\nﬁnetune the pre-trained graph embeddings in the downstream\ntrait generation task. For the joint learning strategy, an aux-\niliary self-supervised task is included to help learn the super-\nvised detection task, and two tasks share the same graph en-\ncoder. Our training objective is to minimize the cross-entropy\nloss and contrastive loss corresponding to the tasks of per-\nsonality detection and post graph contrastive self-supervised\nlearning, respectively. Formally, the objective function is de-\nﬁned as follows:\nL= Ldet + \u0015Lcl: (13)\nwhere \u0015 is a trade-off parameter to control the strengths of\ncontrastive learning Lcl.\n5 Experiments\n5.1 Dataset\nFollowing previous studies, we conduct experiments on the\nKaggle1 with MBTI taxnomy and Essays datasets with Big-\nFive taxonomy. The Kaggle dataset is collected from Per-\nsonalityCafe, where people share their personality types and\ndaily communications, with a total of 8675 users and 45-50\nposts for each user. The traits for Kaggle dataset, namely,\nMBTI taxonomy, include Introversion / Extroversion, Sens-\ning / Ntuition, Think / Feeling, and Perception / Judging.\nThe Essays [Pennebaker and King, 1999 ] is a well-known\ndataset of stream-of-consciousness texts which contains 2468\nanonymous users with approximately 50 sentences recorded\nfor each user. Each user is tagged with a binary label of the\nBig Five taxonomy, including Openness, Conscientiousness,\nExtroversion, Agreeableness, and Neuroticism. Two datasets\nare randomly divided into 6:2:2 for training, validation, and\ntesting, respectively. The F1 metric is adopted to evaluate in\nthe Essays dataset. The Macro-F1 is adopted to evaluate the\nperformance in each personality trait since the Kaggle dataset\nis imbalanced. Note that, due to the privacy and high ex-\npenses for data collection, available personality datasets with\nstandard labels are rare. In 2018, the MyPersonality dataset 2\nstopped sharing as the world’s largest personality dataset due\nto privacy breach.\n5.2 Baselines\nWe compare our model with several baselines, which can be\ncategorized as follows.\n•BiLSTM [Tandera et al., 2017 ] is a sequence model\nﬁrstly employed to encode each post, and then the aver-\naged post representation is used for user representation.\n•AttRCNN [Xue et al., 2018] is a hierarchical structure,\nin which CNN-based aggregator is employed to obtain\nthe user representations.\n•BERT is a pre-trained language model, [Mehta et al.,\n2020; Ren et al., 2021 ] perform extensive experiments\nto arrive at the optimal conﬁguration for personality de-\ntection.\n•SN+Attn [Lynn et al., 2020 ] is a hierarchical network,\nin which the GRU with attention is used to encode both\nsequences of words and posts for user representations.\n1kaggle.com/datasnaek/mbti-type\n2http://mypersonality.org./\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4562\nMethods Kaggle Essays\nI/E S/N T/F P/J Average r OPN CON EXT AGR NEU Average r\nBiLSTM 57.82 57.87 69.97 57.01 60.67 - 63.32 62.47 63.54 65.97 56.30 62.32 -\nBERTﬁnetune 63.57 62.15 76.41 63.04 66.29 - 65.13 64.55 67.12 68.14 60.51 65.09 -\nAttRCNN 59.74 64.08 78.77 66.44 67.25 - 67.84 63.46 71.50 71.92 62.36 67.42 -\nSN+Attn 65.43 62.15 78.05 63.92 67.39 - 68.50 64.19 72.25 70.82 68.10 68.77 -\nTransformer-MD 66.08 69.10 79.19 67.50 70.47 - 70.47 68.50 72.79 71.07 69.76 69.51 -\nTrigNet 69.54 67.17 79.06 67.69 70.86 6.81 69.52 68.27 70.01 73.12 69.34 70.05 11.26\nCGTNpretrain 71.66 69.43 80.14 69.90 72.78 2.95 72.28 74.75 76.21 76.01 73.77 74.60 6.46\nCGTNjoint 71.12 70.44 80.22 72.64 73.61 2.52 72.17 76.21 78.78 77.12 70.87 75.03 3.25\nTable 1: Overall results of CGTN family and baselines in Macro-F1(%) score of kaggle dataset and F1(%) score of Essays dataset, where r\ndenotes difference between training score and testing score.\nMethods Kaggle Essays\nI/E S/N T/F P/J Average OPN CON EXT AGR NEU Average\nCGTNw/o CL 67.34 68.37 77.29 69.27 70.56 71.42 72.13 72.51 74.92 71.70 72.53\nCGTNw/o TC 69.83 70.42 79.55 71.21 72.74 71.59 72.84 74.63 74.20 71.13 72.96\nCGTNjoint 71.12 70.44 80.22 72.64 73.61 72.17 76.21 78.78 77.12 70.87 75.03\nTable 2: Results of ablation study in Macro-F1 (%) score on the Kaggle dataset and F1 (%) score on the Essays dataset, where “w/o” means\nremoval of a component from the original CGTN.\n•TrigNet [Yang et al., 2021c ] is a novel ﬂow tripar-\ntite graph attention network, which aggregates different\nposts of each user from a psychological perspective.\n•Transformer-MD[Yang et al., 2021a] is a novel multi-\ndocument Transformer, which aggregates different posts\nto depict a personality proﬁle for each user without in-\ntroducing post orders.\n5.3 Implementation Details\nFollowing previous works [Yang et al., 2021c; Yang et al.,\n2021a], we set the max number of posts as 50 for each user\nand the max length as 70 for each post. For pretraining, the\ninitial learning rate is searched in {1e−2, 1e−3, 1e−4}and\nto optimize the contrastive loss on different datasets. The\nmini-batch size is set as 64. The temperature \u001c is set as\n0.15. We adopt early stopping when the validation loss stops\ndecreasing by 10 epochs. For joint learning, we search the\ntrade-off parameter\u0015in {1, 0.1, 0.01, 0.001, 0.0001}for dif-\nferent datasets. The initial learning rate is also searched in\n{1e−2, 1e−3, 1e−4}. The settings of batch size, patience for\nearly stopping, and temperature are the same as the pretrain-\ning strategy3.\n5.4 Overall Results\nThe overall results are presented in Table 1. The major ﬁnd-\nings can be summarized as follows,First, we can observe that\nour ﬁnal model CGTNjoint achieves the highest scores for both\ndatasets, signiﬁcantly outperforming the current state-of-the-\nart model (TrigNet) by 2.75 with t-test p<0.01 in Kaggle\ndataset and 4.98 with t-test p<0.01 in Essays dataset. What’s\nmore, with pre-training strategy, our model CGTNpretrain also\nachieves signiﬁcant breakthrough compared to the current\nSOTA model TrigNet. The results verify the effectiveness\n3The code available at https://github.com/yangpu06/CGTN\nof our model in personality detection. We believe the rea-\nsons are two fold: (1) Our model CGTN uses contrastive\nself-supervised learning to learn better post representations\nwhich reduces the risk of overﬁtting on a small training\nset. (2) Trait correlations are well captured, which injected\nsome psychological clues into for personality detection. Sec-\nond, CGTNpretrain and CGTNjoint performs better on Essays\ndataset compared with baseline, we measure the difference\nbetween the corresponding scores of the training and test-\ning sets compared to that of the supervised paradigm ap-\nproach TrigNet on each datasets and ﬁnd that the train-test\ndifference under CGTN pretrain, joint is smaller than that under\nTrigNet. This is even more obvious on Essay dataset which\nindicates that our method can better mitigate overﬁtting under\nsmall datasets. Third, CGTN joint generally performs better\nthan CGTN pretrain. As shown in Table 1, the train-test dif-\nference under CGTNjoint is smaller than CGTN pretrain on two\ndatasets, which indicates that the ﬁne-tuned representations\nare still at risk of bias towards overﬁtting. Joint learning strat-\negy is probably the better option since the representations in\nthe main and auxiliary tasks are mutually enhanced with each\nother. Fourth, TrigNet and Transformer-MD achieve greater\nperformance compared to the DNNs models, which further\nimplies that making full use of post structure information is\nessential for personality understanding.\n5.5 Ablation Study\nWe conduct an ablation study of our CGTNjoint model on both\ndatasets by removing trait correlation component, represented\nby CGTNw/o TC , and contrasive learning component, repre-\nsented by CGTN w/o CL , to investigate their contributions re-\nspectively. As shown in Table 2, we observe that CGTN joint\noutperforms CGTNw/o TC , suggesting the effectiveness of our\napproach in modeling trait correlations. In particular, the per-\nformance improvement on the Essays dataset is higher than\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4563\n(a) Kaggle\n (b) Essays\nFigure 2: Performance curves for different Training set ratios.\nFigure 3: Performance curves for different trade-off parameter.\non the Kaggle. We guess that it might be the correlations\nof Big Five personality traits are slightly higher than that of\nMBTI indicators. In addition, the performance of CGTN joint\nis also supreior to that of CGTN w/o CL , especially on Essays\ndataset, which shows contrasive learning is helpful for in-\ncreasing the generalization ability of the model, especially on\nsmall datasets.\n5.6 Impact of Number of Training Samples\nWe compare our model with two baseline methods with the\nbest performances: Transformer-MD and TrigNet, to study\nthe impact of the ratio of training set. Particularly, we vary\nthe number of training samples and compare their perfor-\nmance on the Kaggle and Essays dataset. We run each method\n10 times and report the average performance. As shown in\nFigure 2, with the increase of training data, all the methods\nachieve better results in terms of Macro-F1 and F1 on both\ndatasets. Generally, our method outperforms all the other\nmethods consistently. When fewer training data are provided,\nthe baselines exhibit obvious performance drop, while our\nmodel still achieves relatively high performance. It demon-\nstrates that our method can more effectively take advantage of\nthe limited labeled data for personality detection. We believe\nour model beneﬁts from auxiliary signals distilled through\ncontrastive self-supervised learning for personality detection.\n5.7 Effect of Trade-off Parameter\nFigure 3 demonstrates how Macro-F1 and F1 values change\nwhen the trade-off parameter \u0015in CGTNjoint increases. from\nwhich we can observe that the score ﬁrst rises as the trade-\noff parameter \u0015 rises and then begins to drop when \u0015 is\nlarger than 0.1. This is because a bigger value imposes a\n(a) Kaggle\n (b) Essays\nFigure 4: Training curves of CGTNjoint and CGTNw/o CL .\nstronger regularization impact, which helps to reduce overﬁt-\nting. However, if \u0015gets too high, the score will drop because\nexcessive regularization impact outweighs the detection loss.\n5.8 Training Efﬁciency\nWe investigate the effect of self-supervised contrastive learn-\ning on training efﬁciency. Figure 4 shows the training\ncurves of CGTNjoint and CGTNw/o CL on Kaggle and Essays\ndatasets. Obviously, CGTN joint converges much faster than\nCGTNw/o CL on both datasets. In particular, early stop oc-\ncurs at the 35-th epochs and arrives at the best performance\nfor CGTNjoint, while it takes more epochs for CGTN w/o CL\non Kaggle dataset. It demonstrates that contrastive learning\ntask speeds up the detection progress and helps to learn a bet-\nter model. The Essays dataset shows the same trend, and\nCGTNjoint has a lower training loss. The above results ver-\nify that the proposed contrasive self-supervised paradigm is\neffective for such a data-hungry task.\n6 Conclusion\nIn this paper, we proposed a novel Contrastive Graph Trans-\nformer Network model (CGTN) for personality detection.\nCGTN aims to introduce a new learning paradigm to alle-\nviate the data scarcity inherent to personality detection tasks.\nFor this purpose, we designed two types of graph augmenta-\ntions based on LIWC and post semantics and learned post em-\nbeddings from graph self-supervised contrasive learning. Be-\nsides, Transformer-based trait generation architecture is de-\nsigned to exploit correlations among personality traits. More-\nover, we used post-to-trait attention to select the vital posts for\ntrait generation. In the end, extensive experimental results on\nKaggle and Essays datasets demonstrate the effectiveness and\nefﬁciency of our model.\nAcknowledgments\nThis work is supported by the NSFC-General Technology\nBasic Research Joint Funds under Grant (U1936220), the\nNational Natural Science Foundation of China under Grant\n(61972047) and the National Key Research and Development\nProgram of China (2018YFC0831500).\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4564\nReferences\n[Coltheart, 1981] Max Coltheart. The mrc psycholinguistic\ndatabase. The Quarterly Journal of Experimental Psychol-\nogy Section A, 33(4):497–505, 1981.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[John et al., 2008] Oliver P John, Laura P Naumann, and\nChristopher J Soto. Paradigm shift to the integrative big\nﬁve trait taxonomy: History, measurement, and conceptual\nissues. Handbook of personality: Theory and research,\npages 114–158, 2008.\n[Kaushal and Patwardhan, 2018] Vishal Kaushal and Man-\nasi Patwardhan. Emerging trends in personality identiﬁ-\ncation using online social networks—a literature survey.\nACM Transactions on Knowledge Discovery from Data,\n12(2):1–30, 2018.\n[Lang et al., 2019] Yining Lang, Wei Liang, Yujia Wang,\nand Lap-Fai Yu. 3d face synthesis driven by personality\nimpression. In AAAI, volume 33, pages 1707–1714, 2019.\n[Lynn et al., 2020] Veronica Lynn, Niranjan Balasubrama-\nnian, and H Andrew Schwartz. Hierarchical modeling for\nuser personality prediction: The role of message-level at-\ntention. In ACL, pages 5306–5316, 2020.\n[Mairesse et al., 2007] Franc ¸ois Mairesse, Marilyn A\nWalker, Matthias R Mehl, and Roger K Moore. Using\nlinguistic cues for the automatic recognition of personality\nin conversation and text. Journal of artiﬁcial intelligence\nresearch, 30:457–500, 2007.\n[Mehta et al., 2020] Yash Mehta, Samin Fatehi, Amirmo-\nhammad Kazameini, Clemens Stachl, Erik Cambria, and\nSauleh Eetemadi. Bottom-up and top-down: Predicting\npersonality with psycholinguistic and language model fea-\ntures. In ICDM, pages 1184–1189. IEEE, 2020.\n[Pennebaker and King, 1999] James W Pennebaker and\nLaura A King. Linguistic styles: language use as an in-\ndividual difference. Journal of personality and social psy-\nchology, 77(6):1296, 1999.\n[Ren et al., 2021] Zhancheng Ren, Qiang Shen, Xiaolei\nDiao, and Hao Xu. A sentiment-aware deep learning ap-\nproach for personality detection from text. Information\nProcessing & Management, 58(3):102532, 2021.\n[Sharpe et al., 2011] J Patrick Sharpe, Nicholas R Martin,\nand Kelly A Roth. Optimism and the big ﬁve factors of\npersonality: Beyond neuroticism and extraversion. Per-\nsonality and Individual Differences, 51(8):946–951, 2011.\n[Shen et al., 2020] Tiancheng Shen, Jia Jia, Yan Li, Yihui\nMa, Yaohua Bu, Hanjie Wang, Bo Chen, Tat-Seng Chua,\nand Wendy Hall. Peia: Personality and emotion integrated\nattentive model for music recommendation on social me-\ndia platforms. In AAAI, volume 34, pages 206–213, 2020.\n[ˇStajner and Yenikent, 2020] Sanja ˇStajner and Seren\nYenikent. A survey of automatic personality detection\nfrom texts. In ACL, pages 6284–6295, 2020.\n[Tandera et al., 2017] Tommy Tandera, Derwin Suhartono,\nRini Wongso, Yen Lina Prasetio, et al. Personality pre-\ndiction system from facebook users. Procedia computer\nscience, 116:604–611, 2017.\n[Tausczik and Pennebaker, 2010] Yla R Tausczik and\nJames W Pennebaker. The psychological meaning of\nwords: Liwc and computerized text analysis methods.\nJournal of language and social psychology, 29(1):24–54,\n2010.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998–6008, 2017.\n[Wen et al., 2021] Zhiyuan Wen, Jiannong Cao, Ruosong\nYang, Shuaiqi Liu, and Jiaxing Shen. Automatically se-\nlect emotion for response via personality-affected emotion\ntransition. In ACL, pages 5010–5020, 2021.\n[Xu et al., 2018] Keyulu Xu, Weihua Hu, Jure Leskovec, and\nStefanie Jegelka. How powerful are graph neural net-\nworks? In ICLR, 2018.\n[Xue et al., 2018] Di Xue, Lifa Wu, Zheng Hong, Shize Guo,\nLiang Gao, Zhiyong Wu, Xiaofeng Zhong, and Jianshan\nSun. Deep learning-based personality recognition from\ntext posts of online social networks. Applied Intelligence,\n48(11):4232–4246, 2018.\n[Yang et al., 2018] Pengcheng Yang, Xu Sun, Wei Li, Shum-\ning Ma, Wei Wu, and Houfeng Wang. Sgm: sequence gen-\neration model for multi-label classiﬁcation. In COLING,\npages 3915–3926, 2018.\n[Yang et al., 2021a] Feifan Yang, Xiaojun Quan, Yunyi\nYang, and Jianxing Yu. Multi-document transformer for\npersonality detection. In AAAI, volume 35, pages 14221–\n14229, 2021.\n[Yang et al., 2021b] Runzhe Yang, Jingxiao Chen, and\nKarthik Narasimhan. Improving dialog systems for nego-\ntiation with personality modeling. InACL, pages 681–693,\n2021.\n[Yang et al., 2021c] Tao Yang, Feifan Yang, Haolan Ouyang,\nand Xiaojun Quan. Psycholinguistic tripartite graph net-\nwork for personality detection. In ACL, pages 4229–4239,\n2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4565",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.675334632396698
    },
    {
      "name": "Transformer",
      "score": 0.5863567590713501
    },
    {
      "name": "Personality",
      "score": 0.563328206539154
    },
    {
      "name": "Big Five personality traits",
      "score": 0.5501998066902161
    },
    {
      "name": "Artificial intelligence",
      "score": 0.548532247543335
    },
    {
      "name": "Trait",
      "score": 0.5170912742614746
    },
    {
      "name": "Graph",
      "score": 0.4970257580280304
    },
    {
      "name": "Machine learning",
      "score": 0.4839361310005188
    },
    {
      "name": "Natural language processing",
      "score": 0.47060972452163696
    },
    {
      "name": "Ground truth",
      "score": 0.45065733790397644
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3358560800552368
    },
    {
      "name": "Theoretical computer science",
      "score": 0.27151137590408325
    },
    {
      "name": "Psychology",
      "score": 0.17422649264335632
    },
    {
      "name": "Social psychology",
      "score": 0.09158375859260559
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210153856",
      "name": "Second Xiangya Hospital of Central South University",
      "country": "CN"
    }
  ],
  "cited_by": 13
}