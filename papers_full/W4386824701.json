{
  "title": "Salient Object Detection in Optical Remote Sensing Images Driven by Transformer",
  "url": "https://openalex.org/W4386824701",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4221525481",
      "name": "Li, Gongyang",
      "affiliations": [
        "Shanghai University"
      ]
    },
    {
      "id": "https://openalex.org/A2160140499",
      "name": "Bai Zhen",
      "affiliations": [
        "Shanghai University"
      ]
    },
    {
      "id": "https://openalex.org/A2000682113",
      "name": "Liu Zhi",
      "affiliations": [
        "Shanghai University"
      ]
    },
    {
      "id": "https://openalex.org/A367327854",
      "name": "Zhang Xinpeng",
      "affiliations": [
        "Shanghai University"
      ]
    },
    {
      "id": "https://openalex.org/A2747133689",
      "name": "Ling, Haibin",
      "affiliations": [
        "Stony Brook University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2793668851",
    "https://openalex.org/W1772076007",
    "https://openalex.org/W3010616503",
    "https://openalex.org/W2323509952",
    "https://openalex.org/W2981374717",
    "https://openalex.org/W2971137300",
    "https://openalex.org/W3115654959",
    "https://openalex.org/W3175617055",
    "https://openalex.org/W3199185814",
    "https://openalex.org/W4296913506",
    "https://openalex.org/W1978479866",
    "https://openalex.org/W2912465472",
    "https://openalex.org/W4315631877",
    "https://openalex.org/W3108948422",
    "https://openalex.org/W2967085153",
    "https://openalex.org/W4221138999",
    "https://openalex.org/W4206947033",
    "https://openalex.org/W4223896192",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3217306379",
    "https://openalex.org/W3166714471",
    "https://openalex.org/W3188023301",
    "https://openalex.org/W3179147540",
    "https://openalex.org/W3084740725",
    "https://openalex.org/W3208937872",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W6840571137",
    "https://openalex.org/W4214561053",
    "https://openalex.org/W3212645988",
    "https://openalex.org/W3207668590",
    "https://openalex.org/W6810188477",
    "https://openalex.org/W6810081451",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W4289752563",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W6761855798",
    "https://openalex.org/W3162418282",
    "https://openalex.org/W3106587394",
    "https://openalex.org/W4205288538",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W3203699578",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2807746031",
    "https://openalex.org/W2939217524",
    "https://openalex.org/W2990984982",
    "https://openalex.org/W2997316506",
    "https://openalex.org/W3035422681",
    "https://openalex.org/W3035290198",
    "https://openalex.org/W3107944836",
    "https://openalex.org/W3109623941",
    "https://openalex.org/W3136838953",
    "https://openalex.org/W3112885960",
    "https://openalex.org/W3029368604",
    "https://openalex.org/W3173382343",
    "https://openalex.org/W4307778795",
    "https://openalex.org/W3132018008",
    "https://openalex.org/W3122006940",
    "https://openalex.org/W4285215809",
    "https://openalex.org/W3135874576",
    "https://openalex.org/W2961348656",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2963868681",
    "https://openalex.org/W2100470808",
    "https://openalex.org/W2963529609",
    "https://openalex.org/W3019728440",
    "https://openalex.org/W3003121299",
    "https://openalex.org/W3010722397",
    "https://openalex.org/W4285161446",
    "https://openalex.org/W2985335644",
    "https://openalex.org/W3102864715",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3153906112",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2567978322",
    "https://openalex.org/W3177004386",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3190340754",
    "https://openalex.org/W4309968609",
    "https://openalex.org/W4287204411",
    "https://openalex.org/W4226537900",
    "https://openalex.org/W4387085913",
    "https://openalex.org/W2945164022",
    "https://openalex.org/W4297801414",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W4312373555",
    "https://openalex.org/W3125703990",
    "https://openalex.org/W4213078714",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W4362654014",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Existing methods for Salient Object Detection in Optical Remote Sensing Images (ORSI-SOD) mainly adopt Convolutional Neural Networks (CNNs) as the backbone, such as VGG and ResNet. Since CNNs can only extract features within certain receptive fields, most ORSI-SOD methods generally follow the local-to-contextual paradigm. In this paper, we propose a novel Global Extraction Local Exploration Network (GeleNet) for ORSI-SOD following the global-to-local paradigm. Specifically, GeleNet first adopts a transformer backbone to generate four-level feature embeddings with global long-range dependencies. Then, GeleNet employs a Direction-aware Shuffle Weighted Spatial Attention Module (D-SWSAM) and its simplified version (SWSAM) to enhance local interactions, and a Knowledge Transfer Module (KTM) to further enhance cross-level contextual interactions. D-SWSAM comprehensively perceives the orientation information in the lowest-level features through directional convolutions to adapt to various orientations of salient objects in ORSIs, and effectively enhances the details of salient objects with an improved attention mechanism. SWSAM discards the direction-aware part of D-SWSAM to focus on localizing salient objects in the highest-level features. KTM models the contextual correlation knowledge of two middle-level features of different scales based on the self-attention mechanism, and transfers the knowledge to the raw features to generate more discriminative features. Finally, a saliency predictor is used to generate the saliency map based on the outputs of the above three modules. Extensive experiments on three public datasets demonstrate that the proposed GeleNet outperforms relevant state-of-the-art methods. The code and results of our method are available at https://github.com/MathLee/GeleNet.",
  "full_text": "IEEE TRANSACTIONS ON IMAGE PROCESSING 1\nSalient Object Detection in Optical Remote Sensing\nImages Driven by Transformer\nGongyang Li, Zhen Bai, Zhi Liu, Senior Member, IEEE, Xinpeng Zhang, Member, IEEE,\nand Haibin Ling, Fellow, IEEE\nAbstract‚ÄîExisting methods for Salient Object Detection in\nOptical Remote Sensing Images (ORSI-SOD) mainly adopt Convo-\nlutional Neural Networks (CNNs) as the backbone, such as VGG\nand ResNet. Since CNNs can only extract features within certain\nreceptive fields, most ORSI-SOD methods generally follow the\nlocal-to-contextual paradigm. In this paper, we propose a novel\nGlobal Extraction Local Exploration Network (GeleNet) for ORSI-\nSOD following the global-to-local paradigm. Specifically, GeleNet\nfirst adopts a transformer backbone to generate four-level feature\nembeddings with global long-range dependencies. Then, GeleNet\nemploys a Direction-aware Shuffle Weighted Spatial Attention\nModule (D-SWSAM) and its simplified version (SWSAM) to\nenhance local interactions, and a Knowledge Transfer Module\n(KTM) to further enhance cross-level contextual interactions. D-\nSWSAM comprehensively perceives the orientation information\nin the lowest-level features through directional convolutions\nto adapt to various orientations of salient objects in ORSIs,\nand effectively enhances the details of salient objects with an\nimproved attention mechanism. SWSAM discards the direction-\naware part of D-SWSAM to focus on localizing salient ob-\njects in the highest-level features. KTM models the contextual\ncorrelation knowledge of two middle-level features of different\nscales based on the self-attention mechanism, and transfers the\nknowledge to the raw features to generate more discriminative\nfeatures. Finally, a saliency predictor is used to generate the\nsaliency map based on the outputs of the above three modules.\nExtensive experiments on three public datasets demonstrate\nthat the proposed GeleNet outperforms relevant state-of-the-art\nmethods. The code and results of our method are available at\nhttps://github.com/MathLee/GeleNet.\nIndex Terms‚ÄîSalient object detection, optical remote sensing\nimage, transformer, directional convolution, shuffle weighted\nspatial attention.\nI. I NTRODUCTION\nS\nALIENT Object Detection (SOD) focuses on finding and\nlocating the most visually prominent objects/regions in a\nscene [1]‚Äì[3]. It is a common pre-processing step for many\ntasks in computer vision, such as quality assessment [4],\n[5], object segmentation [6]‚Äì[10], video compression [11],\nand object tracking [12]. Recently, SOD in Optical Remote\nGongyang Li, Zhen Bai, Zhi Liu, and Xinpeng Zhang are with Key Labo-\nratory of Specialty Fiber Optics and Optical Access Networks, Joint Interna-\ntional Research Laboratory of Specialty Fiber Optics and Advanced Commu-\nnication, Shanghai Institute for Advanced Communication and Data Science,\nShanghai University, Shanghai 200444, China, and School of Communication\nand Information Engineering, Shanghai University, Shanghai 200444, China.\nGongyang Li and Zhi Liu are also with Wenzhou Institute of Shang-\nhai University, Wenzhou 325000, China (email: ligongyang@shu.edu.cn;\nbz536476@163.com; liuzhisjtu@163.com; xzhang@shu.edu.cn).\nHaibin Ling is with the Department of Computer Science, Stony Brook\nUniversity, Stony Brook, NY 11794 USA (email: hling@cs.stonybrook.edu).\nCorresponding authors: Zhi Liu and Xinpeng Zhang.\nORSI Ground truth Ours ACCoNet CorrNet ERPNet\nFig. 1. Saliency maps produced by our method and three state-of-the-\nart ORSI-SOD methods, including ACCoNet [16], CorrNet [17], and ERP-\nNet [18]. Please zoom in for details, especially the first row.\nSensing Images (ORSI-SOD) [13]‚Äì[15], as an emerging topic,\nhas attracted the attention of many researchers, and has been\nwidely used in agriculture, forestry, environmental science, and\nsecurity surveillance.\nWith the rapid development of deep learning, Convolutional\nNeural Networks (CNNs) [19] have dominated the field of\ncomputer vision with their powerful feature representation\ncapabilities. Many effective CNN-based solutions for ORSI-\nSOD are proposed [15]‚Äì[18], [20]‚Äì[23]. While a few methods\nfollow the local-to-local paradigm [18], [20], most methods\nadopt the local-to-contextual paradigm 1 [15]‚Äì[17], [21]‚Äì[25].\nBoth paradigms first use a CNN backbone, such as VGG [26]\nand ResNet [27], to extract basic feature embeddings. The\nlocal-to-local paradigm focuses on exploring valuable infor-\nmation in single-level feature embeddings. Differently, since\nthe local-to-contextual one considers that CNNs only extract\nfeatures within certain receptive fields, it focuses on designing\nspecific modules to mine the contextual information between\nfeature embeddings at multiple levels. The above paradigms\npromote the development of ORSI-SOD and achieve promis-\ning performance.\nHowever, due to the characteristics of ORSI scenes, such\nas variation in object orientation, scale, and category, the\nabove paradigms suffer from obvious limitations. The local-\nto-local paradigm ignores contextual information that is useful\nfor handling the above scenes. The contextual information\ncaptured by the local-to-contextual paradigm is still based\non convolution layers with limited receptive fields, which is\nalso insufficient to handle challenging scenes of ORSIs. For\n1Here, the first ‚Äúlocal‚Äù in both paradigms specifically refers to using CNN\nbackbones to extract features with limited receptive fields.\narXiv:2309.08206v1  [cs.CV]  15 Sep 2023\nIEEE TRANSACTIONS ON IMAGE PROCESSING 2\nintuitive understanding, we show the saliency maps generated\nby typical methods for both paradigms in Fig. 1, where\nACCoNet [16] and CorrNet [17] belong to the local-to-\ncontextual paradigm, and ERPNet [18] belongs to the local-\nto-local paradigm. We find that these methods suffer from\norientation insensitivity, incomplete detection, and missing\nsalient objects.\nInspired by the above observations, in this paper, we pro-\npose to design new solutions following the global-to-local\nparadigm. Our main idea is to replace the CNN backbone\nwith a transformer one that can establish global relationships\n(i.e., changing the first ‚Äúlocal‚Äù in two existing paradigms to\n‚Äúglobal‚Äù) and to perform local enhancement on the extracted\nglobal features. With this idea, we build a novel Global\nExtraction Local Exploration Network (GeleNet) for ORSI-\nSOD with a transformer backbone. Transformers [28]‚Äì[31]\nare known to be good at modeling the global long-range\ndependencies between feature patches. This unique ability of\ntransformer enables GeleNet to deal with complex scenes and\nchangeable objects in ORSIs. Furthermore, in GeleNet, we\nfocus on local and cross-level contextual interactions, which\nare beneficial for highlighting salient objects in ORSIs.\nIn particular, we adopt the popular PVT [32], [33] as the\nbackbone of our GeleNet. To alleviate the orientation insensi-\ntivity issue of previous methods, we propose aDirection-aware\nShuffle Weighted Spatial Attention Module (D-SWSAM), and\nassign it to the lowest-level features to adequately identify the\norientation of objects through directional convolutions with\nfour directions. D-SWSAM is also equipped with an improved\nattention mechanism to outline the details of salient objects.\nSince high-level features contain location information rather\nthan orientation and texture information, we extract the cor-\nresponding part containing the improved attention mechanism\nfrom D-SWSAM, i.e., SWSAM, and assign it to the highest-\nlevel features to determine the location of salient objects.\nThe above modules can well enhance local interactions of\nintra-level features. In addition, we propose a Knowledge\nTransfer Module (KTM) for the remaining adjacent features\nto explore contextual interactions between inter-level features\nand transfer the specific knowledge of salient objects between\nadjacent features to the raw features. In this way, the proposed\nGeleNet can generate saliency maps with accurate orientations\nand complete objects, as illustrated in the third column of\nFig. 1, and consistently outperforms compared methods on\nthree datasets.\nOur main contributions are summarized in three aspects:\n‚Ä¢ We propose a transformer-based ORSI-SOD solution,\nGeleNet, with the global-to-local paradigm, which is\ndifferent from the local-to-contextual paradigm followed\nby most existing CNN-based methods. To the best of our\nknowledge, this is the first transformer-driven ORSI-SOD\nsolution.\n‚Ä¢ We propose the D-SWSAM and its variant SWSAM\nto enhance local interactions of the extracted global\nfeature embeddings. D-SWSAM can tackle the problem\nof objects with various orientations in ORSIs and enhance\nthe details of salient objects in the lowest-level features,\nwhile SWSAM can locate salient objects in the highest-\nlevel features.\n‚Ä¢ We propose the KTM to enhance contextual interactions\nof two middle-level features. In KTM, we model the\ncontextual correlation knowledge of two types of com-\nbinations ( i.e., product and sum) of these features, and\ntransfer the knowledge to the raw features to generate\nmore discriminative features.\nThe rest of this paper is arranged as follows. In Sec. II, we\nreview the related work. In Sec. III, we describe the details of\nthe proposed GeleNet. In Sec. IV, we conduct comprehensive\nexperiments and ablation studies. In Sec. V, we present the\nconclusion.\nII. R ELATED WORK\nA. Salient Object Detection in Optical Remote Sensing Images\nSalient object detection in optical remote sensing images\nplays an important role in understanding ORSIs. Recently,\nwith the successive construction of the three datasets [14],\n[15], [22], numerous ORSI-SOD methods are proposed. Here\nwe focus on CNN-based methods, which dominate this topic\nand achieve promising performance.\nExisting CNN-based ORSI-SOD methods mainly follow\ntwo paradigms, i.e., the local-to-local paradigm and the local-\nto-contextual paradigm. The local-to-local paradigm typi-\ncally extracts feature embeddings containing local informa-\ntion through the CNN backbone, and then explores valuable\ninformation in single-level feature embeddings. For example,\nin [18], Zhou et al. extracted multi-level features through the\nCNN backbone, and performed edge extraction and feature\nfusion on each level of features in two parallel decoders.\nLi et al . [20] explored the complementarity of foreground,\nedge, background, and the global image-level content of\nsingle-level features, and aimed at generating complete salient\nobjects. They focused on the extraction of various specific\ninformation on single-level features ( i.e., local features), ig-\nnoring the contextual interactions between local features at\ndifferent levels.\nThe local-to-contextual paradigm, by contrast, explores\ncontextual information between local feature embeddings at\ndifferent levels, and is therefore popularly adopted by recent\nsolutions. For example, Li et al . [15] extracted multi-level\nfeatures from multiple inputs, and employed nested connec-\ntions to aggregate them. Similarly, Zhou et al. [23] proposed\na cascaded feature fusion module to fuse multi-level features\nfrom different branches. In [21], Huang et al. aggregated three\nhigh-level features to produce contextual semantic information\nto approximately locate salient objects. Li et al. [17] proposed\na correlation module for continuous semantic features, gener-\nating an initial coarse saliency map for location guidance of\nlow-level features. Tu et al . [22] proposed two decoders to\naggregate three adjacent features twice with salient boundary\nfeatures. Li et al. [16] designed a specific module for adjacent\nfeatures, aiming at coordinating cross-scale interactions and\nmining valuable contextual information.\nDespite great progress achieved by the local-to-contextual\nparadigm, the explored contextual interactions only mine\nIEEE TRANSACTIONS ON IMAGE PROCESSING 3\ninteractions between features at different levels through\nconvolution-based modules. In this paper, inspired by the\npopular transformer [28]‚Äì[32], we propose the global-to-local\nparadigm that first models the global long-range dependencies\nbetween feature patches and then enhances the local and\ncontextual interactions, and build a novel GeleNet for ORSI-\nSOD. Benefiting from the global view of the transformer\nand the local enhancement of our proposed modules, our\nGeleNet can better perceive salient objects with numerous\nscales, diverse types, and multiple numbers in ORSIs.\nB. Salient Object Detection with Transformer\nTransformer was first proposed for Natural Language Pro-\ncessing (NLP) [28], which is good at modeling global long-\nrange dependencies between word vectors. Following its suc-\ncess in NLP, researchers have extended it into computer\nvision and achieved remarkable progress on numerous tasks,\nespecially on dense prediction tasks [31]‚Äì[33].\nHere, we introduce some representative works on\ntransformer-based SOD, involving SOD in Natural Scene\nImages (NSI-SOD) [34], [35], RGB-D/T SOD [36]‚Äì[38], co-\nsaliency detection [39], and video SOD [39]. In general,\ntransformer-based SOD methods can be roughly divided into\nthree types depending on where the transformer is used. The\nfirst type of method adopted transformer as the feature extrac-\ntor in the encoding phase. For instance, Liu et al . [36] used\nSwin Transformer [31] to extract basic features from RGB-D/T\npairs, and aligned cross-modality features through attention\nmechanism to generate discriminative features. Liu et al. [34]\nachieved effective context modeling using the same back-\nbone as [36] for NSI-SOD. The second type of method\nadopted transformer to develop modules in the decoding phase.\nLiu et al . [37] proposed a triplet transformer embedding\nmodule to enhance high-level features by learning long-range\ndependencies across layers. In [39], Su et al . proposed a\nunified transformer framework for co-saliency detection and\nvideo SOD, which is equipped with two transformer blocks\nto capture the long-range dependencies among a group of fea-\ntures from different images/frames. Fang et al. [38] proposed a\nmultiple transformer module to learn the common information\nof cross-modality and cross-scale features. The last type of\nmethod utilized the pure transformer architecture to achieve\nSOD. Liu et al. [35] adopted T2T-ViT [30] as the backbone,\nand proposed a multi-task transformer decoder to jointly detect\nsalient objects and boundaries.\nThe above transformer-based SOD methods achieve impres-\nsive results on specific SOD tasks. Therefore, we introduce\nthe transformer into the ORSI-SOD task, and propose the\nfirst transformer-driven ORSI-SOD method, i.e., GeleNet. Our\nmethod belongs to the first type of method, and adopts\nPVT [32], [33] as the backbone to extract long-range depen-\ndency features from input ORSIs.\nC. Attention Mechanism\nAttention mechanism is widely used in computer vision and\nimage analysis. In general, it includes channel attention [40],\nspatial attention [41], and self-attention [28], [42]. SENet [40]\nwas a classic channel attention model, which explicitly repre-\nsents dependencies between channels to adaptively recalibrate\nfeatures. ECANet [43] developed an extremely lightweight\nchannel attention module through a fast 1D convolution.\nMoreover, CBAM [41] additionally introduced spatial atten-\ntion, and inferred attention maps along channel and spatial\ndomain in turn for adaptive feature enhancement. Li et al. [44]\nproposed the Spatial Group-wise Enhance (SGE), which first\nsplits features into several sub-features, then extracts specific\nsemantics from each sub-feature, and finally adjusts the im-\nportance of semantics of each sub-feature by an attention\nfactor. Zhang et al. [45] proposed a lightweight shuffle atten-\ntion, which also first splits features into several groups, then\nperforms channel attention and spatial attention in parallel,\nand finally introduces channel shuffle to allow information\ncommunication along channels.\nBoth SGE [44] and shuffle attention [45] consider only the\nattention of each sub-feature, but ignore the consistency of\nattention between different sub-features, which is not friendly\nto SOD. In addition, since the global features extracted by\nthe transformer lack channel interaction, it is unreasonable for\nshuffle attention to put the shuffle operation at the end. There-\nfore, we propose an improved spatial attention module, namely\nSWSAM, which focuses on enhancing the channel interactions\nof global features and improving the effectiveness of spatial\nattention to highlight salient regions more accurately. Notably,\nwe further integrate SWSAM and directional convolutions, and\npropose D-SWSAM to adapt to various orientations of salient\nobjects in ORSIs. Moreover, we also propose a self-attention-\nbased KTM to model and transfer the contextual knowledge\nto generate more discriminative features.\nIII. P ROPOSED METHOD\nIn this section, we elaborate on the proposed transformer-\ndriven GeleNet. In Sec. III-A, we depict the network overview.\nIn Sec. III-B and Sec. III-C, we introduce D-SWSAM and\nKTM, respectively. In Sec. III-D, we present the saliency\npredictor and loss function.\nA. Network Overview\nAs illustrated in Fig. 2, the proposed GeleNet follows the\npopular three-stage structure [46], [47] in SOD, including a\nfeature extractor for basic feature generation, three modules\n(i.e., D-SWSAM, KTM, and SWSAM) for feature interac-\ntion/enhancement, and a saliency predictor for saliency map\ngeneration.\nConcretely, we use the Pyramid Vision Transformer\n(PVT) [33] as the backbone, whose input size is set to\n3 √ó 352 √ó 352. PVT consists of four transformer encoder\nblocks denoted as T i (i ‚àà {1, 2, 3, 4}), and can generate four-\nlevel basic global features denoted as ÀÜfi\nt ‚àà Rci√óhi√ówi , where\nci ‚àà {64, 128, 320, 512}, and hi/wi = 352\n2i+1 . To improve the\ncomputational efficiency, we unify the channel number of ÀÜfi\nt\n(i ‚àà {1, 3, 4}) to 32 by the channel normalization ( i.e., a\nconvolution layer), generating fi\nt ‚àà Rc√óhi√ówi , where c is 32.\nNotably, for ÀÜf2\nt , we not only reduce its channel number to\n32, but also adjust its resolution from 44 √ó44 to 22 √ó22 for\nIEEE TRANSACTIONS ON IMAGE PROCESSING 4\nTransformer-based Feature Extractor‚ÄîPVT\nT1\n64 √ó88 √ó88\nT1\n64 √ó88 √ó88\nT2\n128 √ó44 √ó44\nT2\n128 √ó44 √ó44\nT3\n320 √ó22 √ó22\nT3\n320 √ó22 √ó22\nT4\n512 √ó11 √ó11\nT4\n512 √ó11 √ó11\nConv.\nSupervision\n‚äöConcatenation\nS: 352 √ó 352GT\nUpsamplingU\nùêøtotal\nU Saliency PredictorSaliency Predictor\nORSI\n3 √ó352 √ó352\nChannel Normalization32√ó88√ó88 32√ó22√ó22 32√ó22√ó22 (c√óh√ów) 32√ó11√ó11\nDirectional Conv.\n‚äö\nSWSAM\nChannel Shuffle\nFeature Split\nSA SA SA SA\nùíÇ1‚àôùúî1 ùíÇ2‚àôùúî2 ùíÇ3‚àôùúî3 ùíÇ4‚àôùúî4\n‚äï\nS\n‚äï\n‚äó\nD-SWSAM\nChannel Shuffle\nFeature Split\nSA SA SA SA\nùíÇ1‚àôùúî1 ùíÇ2‚àôùúî2 ùíÇ3‚àôùúî3 ùíÇ4‚àôùúî4\n‚äï\nS\n‚äï\n‚äóSWSAM\nS\n‚äï ‚äó\n‚äõ(hw)√ó(c/2)\nR&T KQ R\nR&T Reshape and Transpose\n(c/2)√ó(hw)\nSoftmax\n(hw)√ó(hw)\n(c/2)√óh√ów (c/2)√óh√ówc√óh√ów\nR\n‚äõ ‚äõ\nV1\nc√ó(hw)\nR\n‚®Å\nc√óh √ów\nR\nV2\nc√ó(hw)\n‚®Å\n‚äïKTM\nR\n‚äõ Matrix Multiplication\nElement-wise Summation / Multiplication‚äï/‚äó\nSigmoid\nResidual Connection\nSA Spatial Attention\nùíát\n1 ùíát\n2 ùíát\n3 ùíát\n4\nùíádswsa ùíáktm\nùíáswsa\nFig. 2. Pipeline of the proposed transformer-driven GeleNet, which consists of a feature extractor, three modules, and a saliency predictor. First, we adopt a\ntransformer-based feature extractor PVT-v2-b2 [33] to extract four-level basic feature embeddings with global long-range dependencies. Then, we employ the\nDirection-aware Shuffle Weighted Spatial Attention Module(D-SWSAM), the Knowledge Transfer Module, and the variant of D-SWSAM (i.e., SWSAM) to deal\nwith the corresponding features, respectively. Specifically, in D-SWSAM, we perform four directional convolutions with different directions ( i.e., horizontal,\nvertical, leading diagonal, and reverse diagonal) on the lowest-level features to extract specific orientation information, and then use SWSAM to outline the\ndetails regions. We also adopt SWSAM to enhance the location of salient objects in the highest-level features. In KTM, we model the contextual correlation\nknowledge of two types of combinations ( i.e., product and sum) of two middle-level features, and transfer the knowledge to the raw features to generate more\ndiscriminative features. Finally, we use a saliency predictor to generate a saliency map from the outputs of the above modules.\nsubsequent processing in KTM, generating f2\nt ‚àà R32√ó22√ó22.\nFor the lowest-level features f1\nt and the highest-level features\nf4\nt , we adopt an improved spatial attention mechanism for\nlocal enhancement. According to the characteristics of features\nat different levels, we adopt D-SWSAM for f1\nt to extract\norientation information and achieve local detail enhancement,\ngenerating fdswsa. While we adopt SWSAM for f4\nt to achieve\nlocal location enhancement, generating fswsa. Moreover, we\nadopt KTM to activate cross-level contextual interactions of\nf2\nt and f3\nt , generating discriminative features fktm. Taking\nadvantage of PVT and these three novel modules, we infer\nsalient objects in the saliency predictor, which is a variant of\nthe effective partial decoder [48].\nB. Direction-aware Shuffle Weighted Spatial Attention Module\nSince the basic features extracted by PVT is with global\nlong-range dependencies, we want to explore their local en-\nhancements to complement their global information and adapt\nto complex scenes in ORSIs. To be precise, we hope to con-\nsistently highlight salient regions in features across different\nchannels, which is important for SOD. Traditional spatial\nattention [41] is known to be effective way to achieve this\ngoal, however, it generates the spatial attention map in a global\nmanner. Specifically, it performs global max pooling and\nglobal average pooling on all channels, which may produce\nan insufficient spatial attention map. Differently, SGE [44],\nas a grouping attention, splits features into several subsets\nand generates a specific spatial attention map from each sub-\nfeature for individual enhancement. While considering only\nthe attention of each sub-feature, SGE ignores the consistency\nof attention between different sub-features, resulting in the\nlack of consistency in the group-enhanced features, which\nis not friendly to SOD. Inspired by [41], [44], we propose\nan effective grouping spatial attention mechanism for SOD,\ni.e., the Shuffle Weighted Spatial Attention Module (SWSAM),\nwhich first generates the local spatial attention map from each\nsub-feature, and then adopts the weighted fusion operation to\nproduce the final spatial attention map for consistent enhance-\nment.\nIn addition, salient objects in ORSIs usually have various\norientations, as shown in Fig. 1 and Fig. 2, which often bring\ntroubles to existing methods using the traditional convolutions.\nTo solve this issue, we specifically introduce directional con-\nvolutions with different directions [49] into SWSAM, and pro-\npose D-SWSAM to explicitly extract orientation information\nof salient objects and achieve local enhancement. Moreover,\nwe arrange D-SWSAM to deal with f1\nt . The detailed structure\nof D-SWSAM is presented in the left part of Fig. 2. In the\nfollowing, we elaborate D-SWSAM in three parts, i.e., the\ndirectional convolution unit, the channel shuffle and feature\nsplit, and the weighted spatial attention, of which the latter\ntwo parts constitute SWSAM.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 5\n1) Directional Convolution Unit . The directional convolu-\ntion unit takes into account the four basic directions, and is\ncomposed of four directional convolution layers with horizon-\ntal (h), vertical (v), leading diagonal (ld), and reverse diagonal\n(rd) directions [49]. We parallelize these four directional\nconvolution layers to simultaneously mine different orientation\ninformation of f1\nt , and concatenate the results for information\nintegration. We formulate the above process as follows:\nfori = convh(f1\nt ) ‚äö convv(f1\nt ) ‚äö convld(f1\nt ) ‚äö convrd(f1\nt ),\n(1)\nwhere fori ‚àà R32√ó88√ó88 denotes the output orientation features,\n‚äö is the concatenation, convx(¬∑) is the directional convolution\nlayer with the direction x ‚àà {h, v, ld, rd}. Considering the\ninput feature size and computational efficiency, here we set\nthe kernel size and the output channel of four directional con-\nvolutions to 5 and 8, respectively. To show the extracted ori-\nentation information intuitively, we expand fori across chan-\nnel as [f1\nh , ...,f8\nh , f1\nv , ...,f8\nv , f1\nld, ...,f8\nld, f1\nrd, ...,f8\nrd], where\nfx ‚àà R1√ó88√ó88 is a single-channel feature and we omit its\nsuperscript, and each directional convolution layer generates\nan eight-channel feature.\n2) Channel Shuffle and Feature Split . Inspired by Shuf-\nfleNet [50] and shuffle attention [45], which shuffle features to\nachieve information communication along channels, we shuffle\nfori with four groups to evenly disperse the orientation infor-\nmation, achieving fshuf ‚àà R32√ó88√ó88, which can be expanded\nas [f1\nh , f1\nv , f1\nld, f1\nrd, ...,f4\nh , f4\nv , f4\nld, f4\nrd, ...,f8\nh , f8\nv , f8\nld, f8\nrd].\nThen, we split fshuf into four feature subsets along\nchannel, generating {f1\ns‚àíshuf, f2\ns‚àíshuf, f3\ns‚àíshuf, f4\ns‚àíshuf} ‚àà\nR8√ó88√ó88, where fn\ns‚àíshuf (n ‚àà {1, 2, 3, 4}) can be ex-\npanded as [f2n‚àí1\nh , f2n‚àí1\nv , f2n‚àí1\nld , f2n‚àí1\nrd , f2n\nh , f2n\nv , f2n\nld , f2n\nrd ].\nThe above operations activate the interaction between fea-\ntures of different orientations, so that each sub-feature evenly\ncontains orientation information in four directions, which is\nconducive to generating an accurate spatial attention map for\neach sub-feature.\n3) Weighted Spatial Attention. We then apply the traditional\nspatial attention [41] to the above sub-features fn\ns‚àíshuf, gener-\nating corresponding spatial attention maps an ‚àà (0, 1)1√ó88√ó88\nas follows:\nan = SA(fn\ns‚àíshuf), (2)\nwhere SA(¬∑) is the spatial attention operation. These four\nspatial attention maps can extract salient regions in local sub-\nfeatures comprehensively without neglecting salient regions in\nthe original complete fori.\nNext, we design a learnable attention fusion approach, that\nis, set a learnable parameter wn ‚àà [0, 1] for each spatial\nattention map an and aggregate them as follows:\naori = sigmoid(conv(\n4X\nn=1\nwn ¬∑ an)), (3)\nwhere aori ‚àà (0, 1)1√ó88√ó88 is the aggregated spatial attention\nmap, wn is initialized as 0.25 and gradually converges to\nappropriate weights, P4\nn=1 wn = 1, conv(¬∑) is the normal\nconvolution layer, and sigmoid(¬∑) is the sigmoid activation\nfunction. In this way, we can obtain a comprehensive and\norientation-sensitive spatial attention map aori. We adopt aori\nto achieve consistent detail enhancement, generating the output\nfeature of D-SWSAM fdswsa ‚àà R32√ó88√ó88 as follows:\nfdswsa = (aori ‚äó fshuf) ‚äï fshuf, (4)\nwhere ‚äó is the element-wise multiplication and ‚äï is the\nelement-wise summation. Notably, here we perform detail\nenhancement on fshuf rather than fori, which continues to\nmaintain valid channel interactions.\n4) Applying SWSAM for Location Enhancement . As shown\nin Fig. 2, instead of D-SWSAM, we apply SWSAM on the\nhighest-level features f4\nt for location enhancement. This is\nbecause f4\nt mainly contains location information, rather than\ndetail information such as orientation information and texture\ninformation, which means that the directional convolution unit\nin D-SWSAM is superfluous. Therefore, we abandon this unit.\nIn addition, f4\nt is extracted using PVT which focuses on mod-\neling the long-range dependencies between feature patches\nand inevitably ignores feature interactions between channels.\nSo we maintain the channel shuffle operation in SWSAM to\nexplicitly increase the channel interaction. In this way, we can\nobtain the output feature of SWSAM fswsa ‚àà R32√ó11√ó11.\nIn summary, our D-SWSAM and SWSAM are designed ac-\ncording to specific characteristics of extracted global features\nof ORSIs to better enhance local interactions. We believe our\nD-SWSAM can effectively assist GeleNet to adapt to salient\nobjects with various orientations in ORSIs, and our SWSAM\ncan assist GeleNet to accurately locate all salient objects in\nORSIs.\nC. Knowledge Transfer Module\nFor the lowest-level and highest-level features, we design\nspecial modules to process them to achieve local interactions\naccording to their respective characteristics. However, it is\ninsufficient to consider only local enhancement, we enhance\ncross-level contextual interactions on two middle-level features\n(i.e., f2\nt and f3\nt ) to explore the discriminative information of\nsalient objects. Inspired by the self-attention mechanism [28],\n[42], we propose a knowledge transfer module to achieve\nthe goal. The detailed structure of KTM is presented in the\nmiddle part of Fig. 2. In the following, we introduce the two\nKTM components, i.e., the contextual correlation knowledge\nmodeling and the knowledge transfer.\n1) Contextual Correlation Knowledge Modeling . In SOD,\nthe product of two features can reveal the significant infor-\nmation co-existing in both features, which is conducive to\ncollaboratively identifying objects. The sum of two features\ncan comprehensively capture the information contained in both\nfeatures without omission, which is conducive to elaborating\nobjects. In particular for our framework, the product and sum\nof f2\nt and f3\nt are complementary to a certain extent. Therefore,\nwe adopt self-attention [28], [42] to model the contextual\ncorrelation knowledge between the product and sum of f2\nt\nand f3\nt .\nAs stated in Sec. III-A, we have unified the size of f2\nt and\nf3\nt to 32√ó22√ó22. For convenience, we denote the size of f2\nt\nand f3\nt to c √óh √ów, as shown in Fig. 2. Here, we denote\nIEEE TRANSACTIONS ON IMAGE PROCESSING 6\nthe product and sum of f2\nt and f3\nt as fpro ‚àà Rc√óh√ów and\nfsum ‚àà Rc√óh√ów, respectively. As the KTM illustrated in Fig. 2,\nto reduce the computational cost, we perform a convolution\nlayer with the channel number of c/2 on fpro and fsum to\ngenerate two new features { Àúfpro, Àúfsum} ‚ààR(c/2)√óh√ów. Then,\nwe reshape and transpose Àúfsum to obtain fQ ‚àà R(hw)√ó(c/2),\nand reshape Àúfpro to obtain fK ‚àà R(c/2)√ó(hw). After that we\nmodel the contextual correlation knowledge C ‚àà R(hw)√ó(hw)\nbetween fQ and fK as follows:\nC = softmax(fQ ‚äõ fK), (5)\nwhere softmax(¬∑) is the softmax activation function and ‚äõ is\nthe matrix multiplication. In this way, we model the pixel-to-\npixel dependencies between the co-existing significant infor-\nmation of fpro and the comprehensive information of fsum,\nwhich are effective to avoid missing salient regions/objects in\nORSIs.\n2) Knowledge Transfer . Meanwhile, we use a convolution\nlayer on f2\nt and f3\nt to generate two new features { Àúf2\nt , Àúf3\nt } ‚àà\nRc√óh√ów, and then reshape them to obtain {fV1 , fV2 } ‚àà\nRc√ó(hw). After that we transfer the modeled knowledge C to\nfV1 and fV2 to generate the informative transferred features\n{f1\ntsf, f2\ntsf} ‚ààRc√óh√ów as follows:\nf1\ntsf = R(fV1 ‚äõ T(C)),\nf2\ntsf = R(fV2 ‚äõ T(C)), (6)\nwhere R(¬∑) and T(¬∑) mean reshape and transpose, respectively.\nFollowing [42], we introduce a trainable weight to adaptively\nfuse f1\ntsf and raw f2\nt through residual connection, and do the\nsame for f2\ntsf and raw f3\nt , generating { Àúf1\ntsf, Àúf2\ntsf} ‚ààRc√óh√ów.\nFinally, we adopt an element-wise summation and a convolu-\ntion layer to integrate the cross-level Àúf1\ntsf and Àúf2\ntsf, generating\nthe discriminative output feature of KTM fktm ‚àà Rc√óh√ów.\nIn summary, fktm inherits the properties of two combi-\nnations of f2\nt and f3\nt , so it has the ability to simultaneously\nidentify and elaborate salient objects. In addition, compared to\nfdswsa and fswsa, fktm is more contextual, which is beneficial\nfor our GeleNet to combine with local enhanced features\n(i.e., fdswsa and fswsa) for better salient object inference.\nD. Saliency Predictor\nTo make better use of the informative output features of D-\nSWSAM, KTM and SWSAM, i.e., fdswsa, fktm and fswsa,\nwe adopt the effective partial decoder [48] as our saliency pre-\ndictor to generate the saliency map. Normally, the resolutions\nof input features in the original partial decoder are 1√ó, 2√ó, and\n4√ó. However, the resolutions of input features of our saliency\npredictor are 32 √ó 11 √ó 11 (fswsa), 32 √ó 22 √ó 22 (fktm), and\n32√ó88√ó88 (fdswsa). Therefore, we make a small modification\nto the original partial encoder, i.e., modify the upsampling\nrate, to adapt to the resolutions of our input features. In this\nway, our saliency predictor can generate an initial saliency\nmap s ‚àà [0, 1]1√ó88√ó88. We restore its resolution to the same\nresolution as the input ORSI by a 4 √ó upsampling operation,\nand obtain the final saliency map S ‚àà [0, 1]1√ó352√ó352.\nDuring the training phase, we train the proposed Ge-\nleNet with a hybrid loss function [67], [68], including the\nintersection-over-union (IoU) loss and the binary cross-entropy\n(BCE) loss. We formulate the total loss function Ltotal as\nfollows:\nLtotal = ‚Ñìiou(S, G) +‚Ñìbce(S, G), (7)\nwhere ‚Ñìiou(¬∑) and ‚Ñìbce(¬∑) are IoU loss and BCE loss, respec-\ntively, and G ‚àà {0, 1}1√ó352√ó352 is the ground truth (GT).\nIV. E XPERIMENTS\nA. Experimental Setup\n1) Datasets. We conduct experiments on the ORSSD [15],\nEORSSD [14], and ORSI-4199 [22] datasets. The ORSSD\ndataset is the first public dataset for ORSI-SOD, and contains\n800 images and corresponding pixel-level GTs, of which 600\nimages are used for training and 200 images for testing. The\nEORSSD dataset contains 2,000 images and corresponding\nGTs, of which 1,400 images are used for training and 600 im-\nages for testing. The ORSI-4199 dataset is the biggest dataset\nfor ORSI-SOD, and contains 4,199 images and corresponding\nGTs, of which 2,000 images are used for training and 2,199\nimages for testing. Following [14], [17], [23], we train our\nGeleNet on the training set of each dataset and test it on the\ntest set of each dataset.\n2) Network Implementation. All experiments are conducted\non PyTorch [69] with an NVIDIA Titan X GPU (12GB\nmemory). To balance the effectiveness and efficiency, we adopt\nPVT-v2-b2 [33] as the backbone, and initialize it with the\npre-trained parameters. Newly added layers are all initialized\nwith the ‚ÄúKaiming‚Äù method [70]. We adopt rotation and a\ncombination of flipping and rotation for data augmentation,\nand resize the input image and GT to 352 √ó352. Our GeleNet\nis trained using Adam optimizer [71] for 45 epochs with a\nbatch size of 8 and a base learning rate of 1e‚àí4 which will\ndecay to 1/10 every 30 epochs.\n3) Evaluation Metrics. We adopt some widely used eval-\nuation metrics to quantitatively evaluate the performance of\nour method and all compared methods on three datasets,\nincluding S-measure ( SŒ±, Œ± = 0.5) [72], F-measure ( FŒ≤, Œ≤2\n= 0.3) [73], E-measure ( EŒæ) [74], mean absolute error (MAE,\nM), precision-recall (PR) curve, and F-measure curve. Here\nwe adopt the evaluation tool (Matlab version) 2 for convenient\nevaluation.\nB. Comparison with State-of-the-arts\nWe compare our GeleNet with state-of-the-art NSI-SOD and\nORSI-SOD methods on the EORSSD and ORSSD datasets,\nincluding R3Net [51], PoolNet [52], EGNet [53], GCPA [54],\nMINet [55], ITSD [56], GateNet [57], CSNet [58], SAM-\nNet [59], HVPNet [60], SUCA [61], PA-KRN [62], VST [35],\nDPORTNet [63], DNTD [64], ICON [65] with PVT back-\nbone, LVNet [15], DAFNet [14], SARNet [21], MJRBM [22],\nEMFINet [23], ERPNet [18], ACCoNet [16], CorrNet [17],\nMCCNet [20], and HFANet [66]. The saliency maps for the\nabove methods are obtained from authors and public bench-\nmarks3,4 [14], [15], or by running public source codes. For the\n2https://github.com/MathLee/MatlabEvaluationTools\n3https://li-chongyi.github.io/proj optical saliency.html\n4https://github.com/rmcong/DAFNet TIP20\nIEEE TRANSACTIONS ON IMAGE PROCESSING 7\nTABLE I\nQUANTITATIVE COMPARISONS WITH STATE -OF-THE -ART NSI-SOD AND ORSI-SOD METHODS ON EORSSD AND ORSSD DATASETS . ‚Üì INDICATES\nTHAT THE LOWER THE BETTER , WHILE ‚Üë THE OPPOSITE . WE MARK THE TOP TWO RESULTS IN RED AND BLUE , RESPECTIVELY .\nMethodsType EORSSD [14] ORSSD [15]\nSŒ± ‚Üë Fmax\nŒ≤ ‚Üë Fmean\nŒ≤ ‚Üë Fadp\nŒ≤ ‚Üë Emax\nŒæ ‚Üë Emean\nŒæ ‚Üë Eadp\nŒæ ‚Üë M ‚ÜìSŒ± ‚Üë Fmax\nŒ≤ ‚Üë Fmean\nŒ≤ ‚Üë Fadp\nŒ≤ ‚Üë Emax\nŒæ ‚Üë Emean\nŒæ ‚Üë Eadp\nŒæ ‚Üë M ‚Üì\nR3Net18 [51] CN .8184 .7498 .6302 .4165 .9483 .8294 .6462 .0171 .8141 .7456 .7383 .7379 .8913 .8681 .8887 .0399\nPoolNet19 [52] CN .8207 .7545 .6406 .4611 .9292 .8193 .6836 .0210 .8403 .7706 .6999 .6166 .9343 .8650 .8124 .0358\nEGNet19 [53] CN .8601 .7880 .6967 .5379 .9570 .8775 .7566 .0110 .8721 .8332 .7500 .6452 .9731 .9013 .8226 .0216\nGCPA20 [54] CN .8869 .8347 .7905 .6723 .9524 .9167 .8647 .0102 .9026 .8687 .8433 .7861 .9509 .9341 .9205 .0168\nMINet20 [55] CN .9040 .8344 .8174 .7705 .9442 .9346 .9243 .0093 .9040 .8761 .8574 .8251 .9545 .9454 .9423 .0144\nITSD20 [56] CN .9050 .8523 .8221 .7421 .9556 .9407 .9103 .0106 .9050 .8735 .8502 .8068 .9601 .9482 .9335 .0165\nGateNet20 [57] CN .9114 .8566 .8228 .7109 .9610 .9385 .8909 .0095 .9186 .8871 .8679 .8229 .9664 .9538 .9428 .0137\nCSNet20 [58] CN .8364 .8341 .7656 .6319 .9535 .8929 .8339 .0169 .8910 .8790 .8285 .7615 .9628 .9171 .9068 .0186\nSAMNet21 [59] CN .8622 .7813 .7214 .6114 .9421 .8700 .8284 .0132 .8761 .8137 .7531 .6843 .9478 .8818 .8656 .0217\nHVPNet21 [60] CN .8734 .8036 .7377 .6202 .9482 .8721 .8270 .0110 .8610 .7938 .7396 .6726 .9320 .8717 .8471 .0225\nSUCA21 [61] CN .8988 .8229 .7949 .7260 .9520 .9277 .9082 .0097 .8989 .8484 .8237 .7748 .9584 .9400 .9194 .0145\nPA-KRN21 [62] CN .9192 .8639 .8358 .7993 .9616 .9536 .9416 .0104 .9239 .8890 .8727 .8548 .9680 .9620 .9579 .0139\nVST21 [35] TN .9208 .8716 .8263 .7089 .9743 .9442 .8941 .0067 .9365 .9095 .8817 .8262 .9810 .9621 .9466 .0094\nDPORTNet22 [63] CN .8960 .8363 .7937 .7545 .9423 .9116 .9150 .0150 .8827 .8309 .8184 .7970 .9214 .9139 .9083 .0220\nDNTD22 [64] CN .8957 .8189 .7962 .7288 .9378 .9225 .9047 .0113 .8698 .8231 .8020 .7645 .9286 .9086 .9081 .0217\nICON23 [65] TN .9185 .8622 .8371 .8065 .9687 .9619 .9497 .0073 .9256 .8939 .8671 .8444 .9704 .9637 .9554 .0116\nLVNet19 [15] CO .8630 .7794 .7328 .6284 .9254 .8801 .8445 .0146 .8815 .8263 .7995 .7506 .9456 .9259 .9195 .0207\nDAFNet21 [14] CO .9166 .8614 .7845 .6427 .9861 .9291 .8446 .0060.9191 .8928 .8511 .7876 .9771 .9539 .9360 .0113\nSARNet21 [21] CO .9240 .8719 .8541 .8304 .9620 .9555 .9536 .0099 .9134 .8850 .8619 .8512 .9557 .9477 .9464 .0187\nMJRBM22 [22] CO .9197 .8656 .8239 .7066 .9646 .9350 .8897 .0099 .9204 .8842 .8566 .8022 .9623 .9415 .9328 .0163\nEMFINet22 [23] CO .9290 .8720 .8486 .7984 .9711 .9604 .9501 .0084 .9366 .9002 .8856 .8617 .9737 .9671 .9663 .0109\nERPNet22 [18] CO .9210 .8632 .8304 .7554 .9603 .9401 .9228 .0089 .9254 .8974 .8745 .8356 .9710 .9566 .9520 .0135\nACCoNet22 [16] CO .9290 .8837 .8552 .7969 .9727 .9653 .9450 .0074 .9437 .9149 .8971 .8806 .9796 .9754 .9721 .0088\nCorrNet22 [17] CO .9289 .8778 .8620 .8311 .9696 .9646 .9593 .0083 .9380 .9129 .9002 .8875 .9790 .9746 .9721 .0098\nMCCNet22 [20] CO .9327 .8904 .8604 .8137 .9755 .9685 .9538 .0066 .9437 .9155 .9054 .8957 .9800 .9758 .9735 .0087\nHFANet22 [66] TO .9380 .8876 .8681 .8365 .9740 .9679 .9644 .0070 .9399 .9112 .8981 .8819 .9770 .9712 .9722 .0092\nOurs-VGGCO .9241 .8721 .8616 .8382 .9723 .9636 .9622 .0080 .9252 .9023 .8932 .8806 .9744 .9651 .9655 .0130\nOurs-Res CO .9271 .8723 .8621 .8481 .9692 .9651 .9644 .0071 .9307 .9042 .8934 .8826 .9774 .9714 .9709 .0098\nOurs-SwinTTO .9259 .8774 .8649 .8528 .9794 .9752 .9713 .0055 .9410 .9203 .9093 .9038 .9829 .9779 .9805 .0080\nOurs-PVT TO .9376 .8923 .8781 .8641 .9828 .9766 .9750 .0064.9469 .9254 .9128 .9035 .9860 .9815 .9786 .0079\nCN: CNN-based NSI-SOD method, TN: Transformer-based NSI-SOD method, CO: CNN-based ORSI-SOD method, TO: Transformer-based ORSI-SOD method.\nORSI-4199 dataset, we compare our GeleNet with 19 of the\nabove 26 methods, whose saliency maps on the ORSI-4199\ndataset are available, and additional five NSI-SOD methods\n(i.e., PiCANet [75], BASNet [68], CPD [48], RAS [76],\nENFNet [77]) provided by the public benchmark 5 [22]. Here,\nfor a comprehensive comparison, in addition to GeleNet with\nthe backbone of PVT-v2-b2 ( i.e., Ours-PVT), we also provide\nthree variants of our GeleNet with backbones of VGG, ResNet,\nand Swin Transformer, named Ours-VGG, Ours-Res, and\nOurs-SwinT, respectively.\n1) Quantitative Comparison on the EORSSD and ORSSD\nDatasets. We report the quantitative comparison results of\nour method and other 26 compared methods on the EORSSD\nand ORSSD datasets in Tab. I. We observe that Ours-PVT\noutperforms all compared methods on both datasets, except\nfor SŒ±, Emax\nŒæ and M on the EORSSD dataset. Concretely, on\nthe EORSSD dataset, Ours-PVT greatly surpasses the second-\nbest method by 1.00%, 2.76%, and 1.06% in terms of Fmean\nŒ≤ ,\nFadp\nŒ≤ , and Eadp\nŒæ , respectively. In Emax\nŒæ and M, Ours-PVT is\nmarginally lower than the best method by 0.33% and 0.0004,\n5https://github.com/wchao1213/ORSI-SOD\nrespectively. On the ORSSD dataset, Ours-PVT is better than\nthe second-best method in terms of SŒ± (0.9469 v.s. 0.9437),\nFmax\nŒ≤ (0.9254 v.s. 0.9155), Emax\nŒæ (0.9860 v.s. 0.9810), and\nM (0.0079 v.s. 0.0087). Notably, Ours-PVT is the only one\nwhose Fadp\nŒ≤ exceeds 0.9, i.e., 0.9035. In addition, we plot the\nPR curve and F-measure curve of Ours-PVT and the compared\nmethods on the EORSSD and ORSSD datasets in Fig. 3 (a-\nb). We can find that under different thresholds, Ours-PVT\nmaintains its superiority and consistently achieves excellent\nperformance.\nMoreover, Ours-SwinT achieves competitive performance\non the EORSSD dataset, and outperforms 26 compared meth-\nods in Fadp\nŒ≤ , Emean\nŒæ , Eadp\nŒæ , and M. Ours-SwinT ranks first out\nof seven metrics and second out of one metric compared to 26\ncompared methods on the ORSSD dataset. Since our modules\nare designed specifically for the global features of transformer,\nthe performance of our two CNN-based variants, i.e., Ours-\nVGG and Ours-Res, is inferior to that of Ours-SwinT and\nOurs-PVT, and is comparable to that of ERPNet, EMFINet,\nand CorrNet.\n2) Quantitative Comparison on the ORSI-4199 Dataset. Due\nto slight differences in the comparison methods, we report\nIEEE TRANSACTIONS ON IMAGE PROCESSING 8\n0 0.2 0.4 0.6 0.8 1\nRecall\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nCorrNet\nACCoNet\nMCCNet\nHFANet\nOurs-PVT\n0 0.2 0.4 0.6 0.8 1\nRecall\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nVST\nCorrNet\nACCoNet\nMCCNet\nOurs-PVT\n0 50 100 150 200 250\nThreshold\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9F-measure\nACCoNet\nMCCNet\nCorrNet\nHFANet\nOurs-PVT\n0 50 100 150 200 250\nThreshold\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95F-measure\nACCoNet\nHFANet\nCorrNet\nMCCNet\nOurs-PVT\n0 0.2 0.4 0.6 0.8 1\nRecall\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nMCCNet\nSUCA\nVST\nICON\nOurs-PVT\n0 50 100 150 200 250\nThreshold\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9F-measure\nACCoNet\nMCCNet\nHFANet\nICON\nOurs-PVT\n(a) EORSSD [14] (b) ORSSD [15] (c) ORSI-4199 [22]\nFig. 3. Quantitative comparison on PR curve (the first row) and F-measure curve (the second row) in three datasets. We show the top five methods in different\ncolors and the other compared methods in gray.\nORSI GT Ours-PVT MCCNet CorrNet ERPNet MJRBM VST PA-KRN SUCA HVPNet\nFig. 4. Visual comparisons with eight representative state-of-the-art methods on three datasets.\nthe quantitative comparison results of Ours-PVT and other\n24 compared methods on the ORSI-4199 dataset separately\nin Tab. II. The ORSI-4199 dataset is the biggest and the most\nchallenging dataset for ORSI-SOD. The performance of Ours-\nPVT on this dataset is impressive, outperforming the second-\nbest method by 0.23% ‚àº1.63% in terms of S-measure, F-\nmeasure, and E-measure. And the MAE score of Ours-PVT is\nonly 0.0264, which is one of only three methods with the MAE\nscore below 0.03. The advantage of Ours-PVT is easier to spot\non the PR curve and F-measure curve, especially the latter one,\nas plotted in Fig. 3 (c). The above excellent performance on\nthe challenging ORSI-4199 dataset strongly demonstrates the\neffectiveness of Ours-PVT. But to be honest, there is still a\nlot of room for improvement on the ORSI-4199 dataset.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 9\nTABLE II\nQUANTITATIVE COMPARISONS WITH STATE -OF-THE -ART NSI-SOD AND\nORSI-SOD METHODS ON THE ORSI-4199 DATASET. WE MARK THE TOP\nTWO RESULTS IN RED AND BLUE , RESPECTIVELY .\nMethodsType ORSI-4199 [22]\nSŒ±‚Üë FmaxŒ≤ ‚Üë FmeanŒ≤ ‚Üë Fadp\nŒ≤ ‚Üë EmaxŒæ ‚Üë EmeanŒæ ‚Üë Eadp\nŒæ ‚Üë M ‚Üì\nR3Net18[51] CN .8142 .7847 .7790 .7776 .8880 .8722 .8645 .0401\nPiCANet18[75] CN .7114 .6461 .5684 .5933 .7946 .6927 .7511 .0974\nPoolNet19[52] CN .8271 .8010 .7779 .7382 .8964 .8676 .8531 .0541\nEGNet19[53] CN .8464 .8267 .8041 .7650 .9161 .8947 .8620 .0440\nBASNet19[68] CN .8341 .8157 .8042 .7810 .9069 .8881 .8882 .0454\nCPD19[48] CN .8476 .8305 .8169 .7960 .9168 .9025 .8883 .0409\nRAS20[76] CN .7753 .7343 .7141 .7017 .8481 .8133 .8308 .0671\nCSNet20[58] CN .8241 .8124 .7674 .7162 .9096 .8586 .8447 .0524\nSAMNet21[59] CN .8409 .8249 .8029 .7744 .9186 .8938 .8781 .0432\nHVPNet21[60] CN .8471 .8295 .8041 .7652 .9201 .8956 .8687 .0419\nENFNet21[77] CN .7766 .7285 .7177 .7271 .8370 .8107 .8235 .0608\nSUCA21[61] CN .8794 .8692 .8590 .8415 .9438 .9356 .9186 .0304\nPA-KRN21[62] CN .8491 .8415 .8324 .8200 .9280 .9168 .9063 .0382\nVST21[35] TN .8790 .8717 .8524 .7947 .9481 .9348 .8997 .0281\nDPORTNet22[63] CN .8094 .7789 .7701 .7554 .8759 .8687 .8628 .0569\nDNTD22[64] CN .8444 .8310 .8208 .8065 .9158 .9050 .8963 .0425\nICON23[65] TN .8752 .8763 .8664 .8531 .9521 .9438 .9239 .0282\nMJRBM22[22] CO .8593 .8493 .8309 .7995 .9311 .9102 .8891 .0374\nEMFINet22[23] CO .8675 .8584 .8479 .8186 .9340 .9257 .9136 .0330\nERPNet22[18] CO .8670 .8553 .8374 .8024 .9290 .9149 .9024 .0357\nACCoNet22[16] CO .8775 .8686 .8620 .8581 .9412 .9342 .9167 .0314\nCorrNet22[17] CO .8623 .8560 .8513 .8534 .9330 .9206 .9142 .0366\nMCCNet22[20] CO .8746 .8690 .8630 .8592 .9413 .9348 .9182 .0316\nHFANet22[66] TO .8767 .8700 .8624 .8323 .9431 .9336 .9191 .0314\nOurs-VGGCO .8540 .8444 .8374 .8345 .9283 .9098 .9086 .0391\nOurs-ResCO .8670 .8601 .8549 .8516 .9383 .9284 .9178 .0329\nOurs-SwinTTO .8828 .8806 .8734 .8681 .9537 .9482 .9261 .0264\nOurs-PVTTO .8862 .8842 .8785 .8755 .9544 .9478 .9265 .0264\nOurs-SwinT consistently outperforms all compared methods\nin all eight metrics on the ORSI-4199 dataset, and achieves\nthe best performance in Emean\nŒæ and M, even compared to\nOurs-PVT. Similar to the performance on the EORSSD and\nORSSD datasets, the performance of Ours-VGG and Ours-\nRes is relatively average on the ORSI-4199 dataset, which\nfurther confirms that our modules is specifically designed for\nthe global features of transformer.\nIn addition, Ours-PVT and two other transformer-based\nmethod (i.e., VST and ICON) perform almost the best among\ntheir respective types of methods, i.e., ORSI-SOD method\nand NSI-SOD method, on three datasets. This means that\ntransformer-based methods can continue to drive the devel-\nopment of ORSI-SOD. The performance of the specialized\nORSI-SOD method is generally better than that of NSI-SOD\nmethod on three datasets, which motivates us to develop better\nspecialized ORSI-SOD solutions.\n3) Visual Comparison. We show the visual comparison of\nOurs-PVT and eight representative state-of-the-art methods in\nFig. 4. There are eight cases in Fig. 4 belonging to four\ntypical and challenging ORSI scenes from three datasets.\nThe first scene is objects with various orientations, which\nis unique to ORSIs, as in the first two cases of Fig. 4. We\nobserve that only our method accurately highlights salient\nobjects without including background. In contrast, another\ntransformer-based method, i.e., VST, incorrectly highlights\nsome background regions, and all CNN-based methods fail\nto fully highlight objects. This is attributed to the directional\nTABLE III\nABLATION RESULTS OF EVALUATING THE INDIVIDUAL CONTRIBUTION OF\nEACH MODULE IN GELE NET. THE BEST ONE IN EACH COLUMN IS BOLD .\nNo. Baseline D-SWSAM KTM SWSAM EORSSD [14]\nSŒ± ‚Üë Fmax\nŒ≤ ‚Üë Emax\nŒæ ‚Üë\n1 ! 0.9249 0.8717 0.9712\n2 ! ! 0.9305 0.8827 0.9764\n3 ! ! 0.9301 0.8812 0.9778\n4 ! ! 0.9309 0.8836 0.9775\n5 ! ! ! 0.9350 0.8872 0.9796\n6 ! ! ! 0.9328 0.8871 0.9786\n7 ! ! ! 0.9339 0.8863 0.9791\n8 ! ! ‚àó ! 0.9355 0.8879 0.9798\n9 ! ! ! ‚àó 0.9366 0.8911 0.9802\n10 ! ! ! ! 0.9376 0.8923 0.9828\n!‚àó: using this module to enhance the lowest- and highest-level features.\nconvolution unit of D-SWSAM. The second scene contains\nmultiple salient objects, as in the third and fourth cases of\nFig. 4. Most methods only locate some of these objects and\ntheir saliency maps are relatively rough, but our method finely\nsegments all salient objects. This is due to the precise location\ncapability of SWSAM. The third scene contains objects with\nfine structure, as in the fifth and sixth cases of Fig. 4. Our\nmethod successfully delineates the same fine structure of\nsalient objects as GTs, such as the islands in the river and the\nshape of the playground. The last scene is low contrast, where\nthe color of foreground and background is similar, as in the\nlast two cases of Fig. 4. Due to the global modeling capability\nof PVT and the local enhancement of proposed modules, our\nmethod accurately distinguishes white vehicles in both cases\nwithout the interference of white zebra crossings. While other\nmethods are confused by the white zebra crossing and wrongly\nhighlight them.\nC. Ablation Studies\nWe conduct comprehensive ablation studies on the EORSSD\ndataset to evaluate the effectiveness of each module of our Ge-\nleNet and each component of our three modules. Accordingly,\nwe analyze 1) the individual contribution of three modules,\n2) the effectiveness of each component in D-SWSAM, 3) the\nrationality of the way of modeling knowledge in KTM, and 4)\nthe effectiveness of each component in SWSAM. We conduct\nthese ablation studies on the GeleNet with the backbone of\nPVT-v2-b2, and adopt the same parameter settings and dataset\npartitioning as in Sec. IV-A for all variants.\n1) Individual Contribution of Three Modules . To investigate\nthe individual contribution of the proposed three modules,\ni.e., D-SWSAM, KTM, and SWSAM, we design various\ncombinations of the above three modules for a total of\nseven variants: 1) Baseline, in which we remove all proposed\nmodules and adopt element-wise summation to fuse f2\nt and\nf3\nt , 2) Baseline+D-SWSAM, 3) Baseline+KTM, 4) Base-\nline+SWSAM, 5) Baseline+KTM+SWSAM, 6) Baseline+D-\nSWSAM+SWSAM, and 7) Baseline+D-SWSAM+KTM. We\nreport the quantitative results in Tab. III.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 10\nORSI GT B (1) B+D (2) B+S (4) B+D+S (6) Ours (10)\nFig. 5. Visual comparisons of different variants. B, D, and S are Baseline,\nD-SWSAM, and SWSAM, respectively. The numbers in parentheses are the\nordinal numbers of these variants in Tab. III.\nTABLE IV\nABLATION RESULTS OF EVALUATING THE EFFECTIVENESS OF EACH\nCOMPONENT OF THE PROPOSED THREE MODULES . THE BEST ONE IN EACH\nCOLUMN IS BOLD . D-SW. MEANS D-SWSAM, AND SWSA. MEANS\nSWSAM.\nModels EORSSD [14]\nSŒ± ‚Üë Fmax\nŒ≤ ‚Üë Emax\nŒæ ‚Üë\nGeleNet (Ours) 0.9376 0.8923 0.9828\nD-SW.\nw/o DirConv 0.9366 0.8911 0.9802\nw/o SWSAM 0.9346 0.8886 0.9800\nKTM\nw/ sum 0.9353 0.8886 0.9808\nw/ product 0.9334 0.8876 0.9813\nSWSA.\nw/o shuffle 0.9320 0.8895 0.9798\nw/o weights 0.9319 0.8872 0.9788\nFrom the first four rows in Tab. III, we can find that each\nmodule can individually improve ‚ÄúBaseline‚Äù by around 0.5%\nin SŒ±, around 1.0% in Fmax\nŒ≤ , and around 0.6% in Emax\nŒæ , which\ndirectly proves that the proposed three modules are effective.\nThe fifth to seventh rows of Tab. III present the performance\nof pairwise cooperation of modules. We can conclude that\nthe cooperation of different modules can further improve the\nrobustness of our method, resulting in better performance.\nTherefore, with all three modules working together, our full\nmodel significantly outperforms ‚ÄúBaseline‚Äù by 1.27% in SŒ±,\n2.06% in Fmax\nŒ≤ , and 1.16% in Emax\nŒæ .\nWe provide two variants to prove the necessity of enhancing\nthe lowest-level and highest-level features with different mod-\nules: 8) using D-SWSAM to enhance both lowest-level and\nhighest-level features, and 9) using SWSAM to enhance both\nlowest-level and highest-level features. As shown in Tab. III,\nwe observe that the performance of the above two variants\nis not as good as our method with different enhancements.\nThis means that enhancing the lowest-level and highest-level\nfeatures with the same module is suboptimal, and our different\nenhancements to the lowest-level and highest-level features are\nnecessary.\nFurthermore, we show the saliency maps for the first,\nsecond, fourth, sixth variants, and our full model in Fig. 5\nto visually illustrate the role of modules. Without the help\nof any modules, ‚ÄúBaseline‚Äù performs badly, and its saliency\nmaps have the problems of wrongly highlighting, introducing\nbackground, and incomplete highlighting. With the addition\nof D-SWSAM which can perceive the orientation information\nand perform local enhancement, the saliency maps generated\nTABLE V\nCOMPARING THE PROPOSED SWSA WITH TWO CLASSIC ATTENTION\nOPERATIONS , i.e., THE TRADITIONAL SPATIAL ATTENTION [41] AND\nSGE [44]. T HE BEST ONE IN EACH COLUMN IS BOLD .\nModels EORSSD [14]\nSŒ± ‚Üë Fmax\nŒ≤ ‚Üë Emax\nŒæ ‚Üë\nw/ SWSAM (Ours) 0.9376 0.8923 0.9828\nw/ SA 0.9293 0.8850 0.9784\nw/ SGE 0.9324 0.8843 0.9791\nby ‚ÄúB+D‚Äù successfully highlight the salient objects with the\ncorrect direction ( i.e., the first and last cases) and suppress\nthe background ( i.e., the second case). Since SWSAM is\nresponsible for location enhancement in the highest-level\nfeatures, the salient objects in the saliency maps generated\nby ‚ÄúB+S‚Äù are highlighted correctly and completely. Naturally,\nthe combination of D-SWSAM and SWSAM, i.e., ‚ÄúB+D+S‚Äù,\ninherits all the advantages of the two modules. With the\nadditional help of KTM, the saliency maps generated by our\nfull model are visually indistinguishable from GTs. The above\nanalysis proves that the proposed three modules are effective\nand play their respective functions.\n2) Effectiveness of Each Component in D-SWSAM. D-\nSWSAM consists of a directional convolution unit and\nSWSAM. Here, we provide two variants of D-SWSAM to\ninvestigate the effectiveness of the above components: 1)\nremoving directional convolution unit (i.e., w/o DirConv which\nis the same as No.9 in Tab. III), and 2) removing SWSAM\n(i.e., w/o SWSAM ). As shown in the second and third rows\nof Tab. IV, removing either component reduces detection\naccuracy, which demonstrates both components are necessary\nfor D-SWSAM. Notably, the performance of w/o SWSAM\ndegrades more than that of w/o DirConv , indicating that\nSWSAM is more important in D-SWSAM.\n3) Rationality of the Way of Modeling Knowledge in KTM.\nDue to the product and sum of f2\nt and f3\nt are complementary,\nwe model the contextual correlation knowledge between the\nproduct and sum of f2\nt and f3\nt in KTM. To investigate the\nrationality of this way of modeling knowledge, we design\ntwo alternative modeling strategies: 1) removing product then\nmodeling knowledge only from sum ( i.e., w/ sum ), and 2)\nremoving sum then modeling knowledge only from product\n(i.e., w/ product ). As shown in the fourth and fifth rows\nof Tab. IV, w/ sum and w/ product perform worse. As\ndetailed in Sec. III-C, due to the complementarity between\nthe product and sum of f2\nt and f3\nt , the contextual correlation\nknowledge modeled from both is more conducive to inferring\nsalient objects. Modeling knowledge from only one of them\nis suboptimal.\n4) Effectiveness of Each Component in SWSAM. SWSAM\nplays an important role in our GeleNet. We use it twice in\nour GeleNet on the lowest-level and highest-level features.\nHere, we provide two variants of SWSAM to investigate the\neffectiveness of its components: 1) removing channel shuffle\n(i.e., w/o shuffle ), and 2) removing learnable parameter wn\nin Eq. 3 ( i.e., w/o weights ). Notably, these two variants are\nIEEE TRANSACTIONS ON IMAGE PROCESSING 11\nORSI GT w/ SWSAM (Ours) w/ SA w/ SGE\nFig. 6. Visual comparisons of different attention mechanisms.\napplied in SWSAM of D-SWSAM and SWSAM of the highest\nlevel. As shown in the last two rows of Tab. IV, w/o shuffle\nand w/o weights are almost the worst of all variants in Tab. IV,\nwhich illustrates the importance of both operations. Actually,\nchannel shuffle in SWSAM serves two different purposes. w/o\nshuffle lets SWSAM in D-SWSAM to generate four spatial\nattention maps directly from four sub-features with single\ndirection instead of sub-features with uniform four directions,\nand weakens the channel communication of the global highest-\nlevel features. w/o weights does not take into account the\ndifferences between different spatial attention maps and simply\nfuses spatial attention maps. Therefore, the performance of\nboth variants is degraded.\nIn addition, we compare the proposed SWSAM with two\nclassic attention mechanisms, i.e., the traditional spatial atten-\ntion [41] and SGE, to further investigate the effectiveness of\nour SWSAM. We provide two variants: 1) replacing SWSAM\nwith the traditional spatial attention ( i.e., w/ SA ), and 2)\nreplacing SWSAM with SGE ( i.e., w/ SGE ). As reported in\nTab. V, the effectiveness of these two attention mechanisms\nis lower than that of our SWSAM, i.e., w/ SWSAM , for\nORSI-SOD. Moreover, in Fig. 6, we show the saliency maps\ngenerated by w/ SWSAM , w/ SA , and w/ SGE for the visual\ncomparison. The first case is that some background regions are\nsimilar to salient objects. Traditional spatial attention generates\nthe attention map in a global manner ( i.e., from all channels),\nwhich leads to the omission of valid information and is not\nconducive to generating an accurate attention map. Therefore,\nw/ SA incorrectly highlights background regions similar to\nsalient objects. The second case is the scene with the irrelevant\nobject. Since SGE extracts specific semantics from each sub-\nfeature and does not adopt the same consistent attention map\nfor enhancement, w/ SGE mistakenly highlights the irrelevant\nobject in the scene. The last case is the elevated highway with\ncars. Since the comprehensive valid information in traditional\nspatial attention is omitted, w/ SA only highlights cars on the\nelevated highway instead of the elevated highway. w/ SGE\ntakes into account the semantics of different sub-features, so it\nhighlights more regions than w/ SA, but meanwhile introduces\nother background regions. Differently, our SWSAM aggregates\nmultiple attention maps generated from different sub-features\nin an adaptive way, resulting in a comprehensive attention map\nfor consistent enhancement. Therefore, our w/ SWSAM can\neffectively handle the above cases.\nV. C ONCLUSION\nIn this paper, we propose the first transformer-driven ORSI-\nSOD solution, namely GeleNet. GeleNet mainly follows the\nglobal-to-local paradigm, while also considering cross-level\ncontextual interactions. GeleNet employs PVT to extract\nglobal features, SWSAM and D-SWSAM to achieve local en-\nhancement, and KTM to activate cross-level contextual inter-\nactions. Specifically, SWSAM is an improved spatial attention\nmodule, which is responsible for location enhancement in the\nhighest-level features. To adapt to various object orientations\nin ORSIs, directional convolutions are used in D-SWSAM to\nexplicitly perceive orientation information of the lowest-level\nfeatures, followed by SWSAM to achieve detail enhancement.\nKTM is built on the self-attention mechanism, and models\nthe complementary information between the product and the\nsum of two middle-level features to generate discriminative\nfeatures. The cooperation of components makes GeleNet a\nsuccessful salient object detector for ORSIs. Extensive com-\nparisons and ablation studies demonstrate the superiority of\nGeleNet and the effectiveness of the three proposed modules.\nMoreover, the proposed D-SWSAM and SWSAM can be used\nas plug-and-play modules for related tasks [1], [2], [77]‚Äì[79].\nREFERENCES\n[1] R. Cong, J. Lei, H. Fu, M.-M. Cheng, W. Lin, and Q. Huang, ‚ÄúReview of\nvisual saliency detection with comprehensive information,‚Äù IEEE Trans.\nCircuits Syst. Video Technol., vol. 29, no. 10, pp. 2941‚Äì2959, Oct. 2019.\n[2] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, ‚ÄúSalient object detection:\nA benchmark,‚Äù IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706‚Äì\n5722, Dec. 2015.\n[3] G. Li, Z. Liu, and H. Ling, ‚ÄúICNet: Information conversion network for\nRGB-D based salient object detection,‚Äù IEEE Trans. Image Process. ,\nvol. 29, pp. 4873‚Äì4884, Mar. 2020.\n[4] K. Gu, S. Wang, H. Yang, W. Lin, G. Zhai, X. Yang, and W. Zhang,\n‚ÄúSaliency-guided quality assessment of screen content images,‚Äù IEEE\nTrans. Multimedia, vol. 18, no. 6, pp. 1098‚Äì1110, Jun. 2016.\n[5] S. Yang, Q. Jiang, W. Lin, and Y . Wang, ‚ÄúSGDNet: An end-to-end\nsaliency-guided deep neural network for no-reference image quality\nassessment,‚Äù in Proc. ACM MM , Oct. 2019, pp. 1383‚Äì1391.\n[6] G. Li, Z. Liu, R. Shi, and W. Wei, ‚ÄúConstrained fixation point based\nsegmentation via deep neural network,‚Äù Neurocomputing, vol. 368, pp.\n180‚Äì187, Nov. 2019.\n[7] G. Li et al. , ‚ÄúPersonal fixations-based object segmentation with object\nlocalization and boundary preservation,‚Äù IEEE Trans. Image Process. ,\nvol. 30, pp. 1461‚Äì1475, Jan. 2021.\n[8] N. Liu, W. Zhao, L. Shao, and J. Han, ‚ÄúSCG: Saliency and contour\nguided salient instance segmentation,‚Äù IEEE Trans. Image Process. ,\nvol. 30, pp. 5862‚Äì5874, Jun. 2021.\n[9] Q. En, L. Duan, and Z. Zhang, ‚ÄúJoint multisource saliency and exemplar\nmechanism for weakly supervised video object segmentation,‚Äù IEEE\nTrans. Image Process., vol. 30, pp. 8155‚Äì8169, Sept. 2021.\n[10] G. Li, Y . Wang, Z. Liu, X. Zhang, and D. Zeng, ‚ÄúRGB-T semantic\nsegmentation with location, activation, and sharpening,‚Äù IEEE Trans.\nCircuits Syst. Video Technol., vol. 33, no. 3, pp. 1223‚Äì1235, Mar. 2023.\n[11] H. Hadizadeh and I. V . Baji¬¥c, ‚ÄúSaliency-aware video compression,‚ÄùIEEE\nTrans. Image Process., vol. 23, no. 1, pp. 19‚Äì33, Jan. 2014.\n[12] W. Feng, R. Han, Q. Guo, J. Zhu, and S. Wang, ‚ÄúDynamic saliency-\naware regularization for correlation filter-based object tracking,‚Äù IEEE\nTrans. Image Process., vol. 28, no. 7, pp. 3232‚Äì3245, Jul. 2019.\n[13] G. Li et al. , ‚ÄúLightweight salient object detection in optical remote-\nsensing images via semantic matching and edge alignment,‚Äù IEEE Trans.\nGeosci. Remote Sens. , vol. 61, pp. 1‚Äì12, 2023.\n[14] Q. Zhang et al. , ‚ÄúDense attention fluid network for salient object de-\ntection in optical remote sensing images,‚Äù IEEE Trans. Image Process.,\nvol. 30, pp. 1305‚Äì1317, 2021.\n[15] C. Li, R. Cong, J. Hou, S. Zhang, Y . Qian, and S. Kwong, ‚ÄúNested\nnetwork with two-stream pyramid for salient object detection in optical\nremote sensing images,‚Äù IEEE Trans. Geosci. Remote Sens. , vol. 57,\nno. 11, pp. 9156‚Äì9166, Nov. 2019.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 12\n[16] G. Li, Z. Liu, D. Zeng, W. Lin, and H. Ling, ‚ÄúAdjacent context\ncoordination network for salient object detection in optical remote\nsensing images,‚Äù IEEE Trans. Cybern. , vol. 53, no. 1, pp. 526‚Äì538,\nJan. 2023.\n[17] G. Li, Z. Liu, Z. Bai, W. Lin, and H. Ling, ‚ÄúLightweight salient object\ndetection in optical remote sensing images via feature correlation,‚Äù IEEE\nTrans. Geosci. Remote Sens. , vol. 60, pp. 1‚Äì12, 2022.\n[18] X. Zhou, K. Shen, L. Weng, R. Cong, B. Zheng, J. Zhang, and C. Yan,\n‚ÄúEdge-guided recurrent positioning network for salient object detection\nin optical remote sensing images,‚Äù IEEE Trans. Cybern., vol. 53, no. 1,\npp. 539‚Äì552, Jan. 2023.\n[19] Y . LeCun and others., ‚ÄúBackpropagation applied to handwritten zip code\nrecognition,‚Äù Neural Comput., vol. 1, no. 4, pp. 541‚Äì551, Dec. 1989.\n[20] G. Li, Z. Liu, W. Lin, and H. Ling, ‚ÄúMulti-content complementation\nnetwork for salient object detection in optical remote sensing images,‚Äù\nIEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1‚Äì13, 2022.\n[21] Z. Huang, H. Chen, B. Liu, and Z. Wang, ‚ÄúSemantic-guided attention\nrefinement network for salient object detection in optical remote sensing\nimages,‚Äù Remote Sens., vol. 13, no. 11, p. 2163, May 2021.\n[22] Z. Tu, C. Wang, C. Li, M. Fan, H. Zhao, and B. Luo, ‚ÄúORSI salient\nobject detection via multiscale joint region and boundary model,‚Äù IEEE\nTrans. Geosci. Remote Sens. , vol. 60, pp. 1‚Äì13, 2022.\n[23] X. Zhou, K. Shen, Z. Liu, C. Gong, J. Zhang, and C. Yan, ‚ÄúEdge-\naware multiscale feature integration network for salient object detection\nin optical remote sensing images,‚Äù IEEE Trans. Geosci. Remote Sens. ,\nvol. 60, pp. 1‚Äì15, 2022.\n[24] C. Li et al. , ‚ÄúA parallel down-up fusion network for salient object\ndetection in optical remote sensing images,‚Äù Neurocomputing, vol. 415,\npp. 411‚Äì120, Nov. 2020.\n[25] R. Cong, Y . Zhang, L. Fang, J. Li, Y . Zhao, and S. Kwong, ‚ÄúRRNet:\nRelational reasoning network with parallel multi-scale attention for\nsalient object detection in optical remote sensing images,‚Äù IEEE Trans.\nGeosci. Remote Sens. , vol. 60, pp. 1‚Äì11, 2022.\n[26] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for\nlarge-scale image recognition,‚Äù in Proc. ICLR, May 2015, pp. 1‚Äì14.\n[27] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in Proc. IEEE CVPR , Jun. 2016, pp. 770‚Äì778.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Proc.\nNeurIPS, Dec. 2017, pp. 6000‚Äì6010.\n[29] A. Dosovitskiy et al. , ‚ÄúAn image is worth 16x16 words: Transformers\nfor image recognition at scale,‚Äù in Proc. ICLR, 2021.\n[30] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. H. Tay, J. Feng,\nand S. Yan, ‚ÄúTokens-to-token ViT: Training vision transformers from\nscratch on imagenet,‚Äù in Proc. IEEE ICCV , Oct. 2021, pp. 538‚Äì547.\n[31] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n‚ÄúSwin Transformer: Hierarchical vision transformer using shifted win-\ndows,‚Äù in Proc. IEEE ICCV , Oct. 2021, pp. 9992‚Äì10 002.\n[32] W. Wang et al., ‚ÄúPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,‚Äù in Proc. IEEE ICCV, Oct. 2021,\npp. 548‚Äì558.\n[33] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, ‚ÄúPVT v2: Improved baselines with pyramid vision transformer,‚Äù\nComput. Vis. Media , vol. 8, pp. 415‚Äì424, Sept. 2022.\n[34] Y . Mao, J. Zhang, Z. Wan, Y . Dai, A. Li, Y . Lv, X. Tian, D.-P. Fan,\nand N. Barnes, ‚ÄúTransformer transforms salient object detection and\ncamouflaged object detection,‚Äù arXiv preprint arXiv:2104.10127 , 2021.\n[35] N. Liu, N. Zhang, K. Wan, L. Shao, and J. Han, ‚ÄúVisual saliency\ntransformer,‚Äù in Proc. IEEE ICCV , Oct. 2021, pp. 4702‚Äì4712.\n[36] Z. Liu, Y . Tan, Q. He, and Y . Xiao, ‚ÄúSwinNet: Swin Transformer drives\nedge-aware RGB-D and RGB-T salient object detection,‚Äù IEEE Trans.\nCircuits Syst. Video Technol., vol. 32, no. 7, pp. 4486‚Äì4497, Jul. 2022.\n[37] Z. Liu, Y . Wang, Z. Tu, Y . Xiao, and B. Tang, ‚ÄúTriTransNet: RGB-D\nsalient object detection with a triplet transformer embedding network,‚Äù\nin Proc. ACM MM, Oct. 2021, pp. 4481‚Äì4490.\n[38] X. Fang, J. Zhu, X. Shao, and H. Wang, ‚ÄúGroupTransNet: Group\ntransformer network for RGB-D salient object detection,‚Äù arXiv preprint\narXiv:2203.10785, 2022.\n[39] Y . Su, J. Deng, R. Sun, G. Lin, and Q. Wu, ‚ÄúA unified trans-\nformer framework for group-based segmentation: Co-segmentation, co-\nsaliency detection and video salient object detection,‚Äù arXiv preprint\narXiv:2203.04708, 2022.\n[40] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, ‚ÄúSqueeze-and-excitation\nnetworks,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 42, no. 8, pp.\n2011‚Äì2023, Aug. 2020.\n[41] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, ‚ÄúCBAM: Convolutional\nblock attention module,‚Äù in Proc. ECCV, Sept. 2018, pp. 3‚Äì19.\n[42] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, ‚ÄúDual attention\nnetwork for scene segmentation,‚Äù in Proc. IEEE CVPR , Jun. 2019, pp.\n3146‚Äì3154.\n[43] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, ‚ÄúECA-Net: Efficient\nchannel attention for deep convolutional neural networks,‚Äù inProc. IEEE\nCVPR, Jun. 2020, pp. 11 531‚Äì11 539.\n[44] X. Li, X. Hu, and J. Yang, ‚ÄúSpatial group-wise enhance: Improving\nsemantic feature learning in convolutional networks,‚Äù arXiv preprint\narXiv:1905.09646, 2019.\n[45] Q.-L. Zhang and Y .-B. Yang, ‚ÄúSA-Net: Shuffle attention for deep\nconvolutional neural networks,‚Äù in Proc. IEEE ICASSP , Jun. 2021, pp.\n2235‚Äì2239.\n[46] G. Li, Z. Liu, L. Ye, Y . Wang, and H. Ling, ‚ÄúCross-modal weighting\nnetwork for RGB-D salient object detection,‚Äù inProc. ECCV, Aug. 2020,\npp. 665‚Äì681.\n[47] Z. Bai, Z. Liu, G. Li, and Y . Wang, ‚ÄúAdaptive group-wise consistency\nnetwork for co-saliency detection,‚Äù IEEE Trans. Multimedia, vol. 25, pp.\n764‚Äì776, Mar. 2023.\n[48] Z. Wu, L. Su, and Q. Huang, ‚ÄúCascaded partial decoder for fast and\naccurate salient object detection,‚Äù in Proc. IEEE CVPR , Jun. 2019, pp.\n3902‚Äì3911.\n[49] J. Mei, R.-J. Li, W. Gao, and M.-M. Cheng, ‚ÄúCoANet: Connectivity\nattention network for road extraction from satellite imagery,‚Äù IEEE\nTrans. Image Process., vol. 30, pp. 8540‚Äì8552, 2021.\n[50] X. Zhang, X. Zhou, M. Lin, and J. Sun, ‚ÄúShuffleNet: An extremely\nefficient convolutional neural network for mobile devices,‚Äù inProc. IEEE\nCVPR, Jun. 2018, pp. 6848‚Äì6856.\n[51] Z. Deng, X. Hu, L. Zhu, X. Xu, J. Qin, G. Han, and P.-A. Heng, ‚ÄúR 3Net:\nRecurrent residual refinement network for saliency detection,‚Äù in Proc.\nIJCAI, Jul. 2018, pp. 684‚Äì690.\n[52] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, ‚ÄúA simple pooling-\nbased design for real-time salient object detection,‚Äù inProc. IEEE CVPR,\nJun. 2019, pp. 3912‚Äì3921.\n[53] J. Zhao, J.-J. Liu, D.-P. Fan, Y . Cao, J. Yang, and M.-M. Cheng, ‚ÄúEGNet:\nEdge guidance network for salient object detection,‚Äù in Proc. IEEE\nICCV, Oct. 2019, pp. 8779‚Äì8788.\n[54] Z. Chen, Q. Xu, R. Cong, and Q. Huang, ‚ÄúGlobal context-aware\nprogressive aggregation network for salient object detection,‚Äù in Proc.\nAAAI, Feb. 2020, pp. 10 599‚Äì10 606.\n[55] Y . Pang, X. Zhao, L. Zhang, and H. Lu, ‚ÄúMulti-scale interactive network\nfor salient object detection,‚Äù in Proc. IEEE CVPR, Jun. 2020, pp. 9410‚Äì\n9419.\n[56] H. Zhou, X. Xie, J.-H. Lai, Z. Chen, and L. Yang, ‚ÄúInteractive two-\nstream decoder for accurate and fast saliency detection,‚Äù in Proc. IEEE\nCVPR, Jun. 2020, pp. 9138‚Äì9147.\n[57] X. Zhao, Y . Pang, L. Zhang, H. Lu, and L. Zhang, ‚ÄúSuppress and\nbalance: A simple gated network for salient object detection,‚Äù in Proc.\nECCV, Aug. 2020, pp. 35‚Äì51.\n[58] S.-H. Gao, Y .-Q. Tan, M.-M. Cheng, C. Lu, Y . Chen, and S. Yan, ‚ÄúHighly\nefficient salient object detection with 100k parameters,‚Äù in Proc. ECCV,\nAug. 2020, pp. 702‚Äì721.\n[59] Y . Liu et al., ‚ÄúSAMNet: Stereoscopically attentive multi-scale network\nfor lightweight salient object detection,‚Äù IEEE Trans. Image Process. ,\nvol. 30, pp. 3804‚Äì3814, 2021.\n[60] Y . Liu, Y .-C. Gu, X.-Y . Zhang, W. Wang, and M.-M. Cheng,\n‚ÄúLightweight salient object detection via hierarchical visual perception\nlearning,‚Äù IEEE Trans. Cybern. , vol. 51, no. 9, pp. 4439‚Äì4449, Sept.\n2021.\n[61] J. Li, Z. Pan, Q. Liu, and Z. Wang, ‚ÄúStacked U-shape network with\nchannel-wise attention for salient object detection,‚Äù IEEE Trans. Multi-\nmedia, vol. 23, pp. 1397‚Äì1409, 2021.\n[62] B. Xu, H. Liang, R. Liang, and P. Chen, ‚ÄúLocate globally, segment\nlocally: A progressive architecture with knowledge review network for\nsalient object detection,‚Äù in Proc. AAAI, Feb. 2021, pp. 3004‚Äì3012.\n[63] Y . Liu, D. Zhang, N. Liu, S. Xu, and J. Han, ‚ÄúDisentangled capsule\nrouting for fast part-object relational saliency,‚Äù IEEE Trans. Image\nProcess., vol. 31, pp. 6719‚Äì6732, 2022.\n[64] C. Fang, H. Tian, D. Zhang, Q. Zhang, J. Han, and J. Han, ‚ÄúDensely\nnested top-down flows for salient object detection,‚Äù Sci. China Inf. Sci. ,\nvol. 65, no. 8, pp. 1‚Äì14, Aug. 2022.\n[65] M. Zhuge, D.-P. Fan, N. Liu, D. Zhang, D. Xu, and L. Shao, ‚ÄúSalient\nobject detection via integrity learning,‚Äù IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 45, no. 3, pp. 3738‚Äì3752, Mar. 2023.\n[66] Q. Wang, Y . Liu, Z. Xiong, and Y . Yuan, ‚ÄúHybrid feature aligned\nnetwork for salient object detection in optical remote sensing imagery,‚Äù\nIEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1‚Äì15, 2022.\nIEEE TRANSACTIONS ON IMAGE PROCESSING 13\n[67] G. Li, Z. Liu, M. Chen, Z. Bai, W. Lin, and H. Ling, ‚ÄúHierarchical\nalternate interaction network for RGB-D salient object detection,‚Äù IEEE\nTrans. Image Process., vol. 30, pp. 3528‚Äì3542, Mar. 2021.\n[68] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, and M. Jagersand,\n‚ÄúBASNet: Boundary-aware salient object detection,‚Äù in Proc. IEEE\nCVPR, Jun. 2019, pp. 7479‚Äì7489.\n[69] A. Paszke et al., ‚ÄúPyTorch: An imperative style, high-performance deep\nlearning library,‚Äù in Proc. NeurIPS, Dec. 2019, pp. 8024‚Äì8035.\n[70] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDelving deep into rectifiers:\nSurpassing human-level performance on ImageNet classification,‚Äù in\nProc. IEEE ICCV , Dec. 2015, pp. 1026‚Äì1034.\n[71] D. P. Kingma and J. L. Ba, ‚ÄúAdam: A method for stochastic optimiza-\ntion,‚Äù in Proc. ICLR, May 2015, pp. 1‚Äì15.\n[72] D.-P. Fan, M.-M. Cheng, Y . Liu, T. Li, and A. Borji, ‚ÄúStructure-measure:\nA new way to evaluate foreground maps,‚Äù in Proc. IEEE ICCV , Oct.\n2017, pp. 4548‚Äì4557.\n[73] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, ‚ÄúFrequency-tuned\nsalient region detection,‚Äù in Proc. IEEE CVPR , Jun. 2009, pp. 1597‚Äì\n1604.\n[74] D.-P. Fan, C. Gong, Y . Cao, B. Ren, M.-M. Cheng, and A. Borji,\n‚ÄúEnhanced-alignment measure for binary foreground map evaluation,‚Äù\nin Proc. IJCAI, Jul. 2018, pp. 698‚Äì704.\n[75] N. Liu, J. Han, and M.-H. Yang, ‚ÄúPiCANet: Pixel-wise contextual\nattention learning for accurate saliency detection,‚Äù IEEE Trans. Image\nProcess., vol. 29, pp. 6438‚Äì6451, Apr. 2020.\n[76] S. Chen, X. Tan, B. Wang, H. Lu, X. Hu, and Y . Fu, ‚ÄúReverse attention-\nbased residual network for salient object detection,‚Äù IEEE Trans. Image\nProcess., vol. 29, pp. 3763‚Äì3776, Jan. 2020.\n[77] Z. Tu, Y . Ma, C. Li, J. Tang, and B. Luo, ‚ÄúEdge-guided non-local fully\nconvolutional network for salient object detection,‚Äù IEEE Trans. Circuits\nSyst. Video Technol., vol. 31, no. 2, pp. 582‚Äì593, Feb. 2021.\n[78] Z. Tu, Z. Li, C. Li, and J. Tang, ‚ÄúWeakly alignment-free RGBT salient\nobject detection with deep correlation network,‚Äù IEEE Trans. Image\nProcess., vol. 31, pp. 3752‚Äì3764, May 2022.\n[79] J. Tang, D. Fan, X. Wang, Z. Tu, and C. Li, ‚ÄúRGBT salient object\ndetection: Benchmark and a novel cooperative ranking approach,‚Äù IEEE\nTrans. Circuits Syst. Video Technol. , vol. 30, no. 12, pp. 4421‚Äì4433,\nDec. 2020.",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.7268311977386475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6602303385734558
    },
    {
      "name": "Object detection",
      "score": 0.6171097755432129
    },
    {
      "name": "Computer science",
      "score": 0.6130578517913818
    },
    {
      "name": "Salient",
      "score": 0.5795561075210571
    },
    {
      "name": "Transformer",
      "score": 0.4348137378692627
    },
    {
      "name": "Remote sensing",
      "score": 0.4169844388961792
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3479766845703125
    },
    {
      "name": "Geography",
      "score": 0.1408088207244873
    },
    {
      "name": "Engineering",
      "score": 0.14035958051681519
    },
    {
      "name": "Voltage",
      "score": 0.09179946780204773
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I113940042",
      "name": "Shanghai University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I146620803",
      "name": "Wenzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    }
  ]
}