{
  "title": "Applying the Transformer to Character-level Transduction",
  "url": "https://openalex.org/W3153224075",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2130569026",
      "name": "Shijie Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2148165152",
      "name": "Ryan Cotterell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2177549942",
      "name": "Mans Hulden",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3100160869",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2625720409",
    "https://openalex.org/W629021723",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2266080033",
    "https://openalex.org/W2962680795",
    "https://openalex.org/W2889368791",
    "https://openalex.org/W2166084679",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W1580142630",
    "https://openalex.org/W3101954944",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2740599975",
    "https://openalex.org/W2963715460",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2888539709",
    "https://openalex.org/W2916993268",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2808154809",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2740149041",
    "https://openalex.org/W4293864276",
    "https://openalex.org/W2950176361",
    "https://openalex.org/W619445499",
    "https://openalex.org/W2983032163",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2945958591",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2508815538",
    "https://openalex.org/W2963635689",
    "https://openalex.org/W2970963828",
    "https://openalex.org/W4394150413",
    "https://openalex.org/W2950825792",
    "https://openalex.org/W2896509746",
    "https://openalex.org/W2951357757",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2271671850",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2982630078",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The transformer has been shown to outperform recurrent neural network-based sequence-to-sequence models in various word-level NLP tasks. Yet for character-level transduction tasks, e.g. morphological inflection generation and historical text normalization, there are few works that outperform recurrent models using the transformer. In an empirical study, we uncover that, in contrast to recurrent sequence-to-sequence models, the batch size plays a crucial role in the performance of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1901–1907\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n1901\nApplying the Transformer to Character-level Transduction\nShijie WuZ Ryan CotterellQ,6 Mans HuldenX\nZJohns Hopkins University 6University of Cambridge\nQETH Z¨urich XUniversity of Colorado Boulder\nshijie.wu@jhu.edu ryan.cotterell@inf.ethz.ch mans.hulden@colorado.edu\nAbstract\nThe transformer (Vaswani et al., 2017) has\nbeen shown to outperform recurrent neural\nnetwork-based sequence-to-sequence models\nin various word-level NLP tasks. Yet for\ncharacter-level transduction tasks, e.g. mor-\nphological inﬂection generation and histori-\ncal text normalization, there are few works\nthat outperform recurrent models using the\ntransformer. In an empirical study, we un-\ncover that, in contrast to recurrent sequence-\nto-sequence models, the batch size plays a\ncrucial role in the performance of the trans-\nformer on character-level tasks, and we show\nthat with a large enough batch size, the trans-\nformer does indeed outperform recurrent mod-\nels. We also introduce a simple technique\nto handle feature-guided character-level trans-\nduction that further improves performance.\nWith these insights, we achieve state-of-the-art\nperformance on morphological inﬂection and\nhistorical text normalization. We also show\nthat the transformer outperforms a strong base-\nline on two other character-level transduction\ntasks: grapheme-to-phoneme conversion and\ntransliteration.\n1 Introduction\nThe transformer (Vaswani et al., 2017) has become\na popular architecture for sequence-to-sequence\ntransduction in NLP. It has achieved state-of-the-\nart performance on a range of common word-level\ntransduction tasks: neural machine translation (Bar-\nrault et al., 2019), question answering (Devlin et al.,\n2019) and abstractive summarization (Dong et al.,\n2019). In addition, the transformer forms the back-\nbone of the widely-used BERT (Devlin et al., 2019).\nYet for character-level transduction tasks like mor-\nphological inﬂection, the dominant model has re-\nmained a recurrent neural network-based sequence-\nCode will be available at https://github.com/\nshijie-wu/neural-transducer.\n16 32 64 128 256 512\nBatch Size\n76\n78\n80\n82\n84\n86\n88\n90ACC\nWu and Cotterell (2019)\nWu and Cotterell (2019) (Our Eval)\nWu and Cotterell (2019) + LR Warmup\nVanilla Transformer\nFeature Invariant Transformer\nFigure 1: Development set accuracy for 5 languages\non morphological inﬂection with different batch sizes.\nWe evince our two primary contributions: (1) we set the\nnew state of the artmorphological inﬂection using the\ntransformer and (2) we demonstrate the transformer’s\ndependence on the batch size.\nto-sequence model with attention (Cotterell et al.,\n2018). This is not for lack of effort—but rather, it is\nthe case that the transformer has consistently under-\nperformed in experiments on average (Tang et al.,\n2018b).1 As anecdotal evidence of this, we note\nthat in the 2019 SIGMORPHON shared task on\ncross-lingual transfer for morphological inﬂection,\nno participating system was based on the trans-\nformer (McCarthy et al., 2019).\nCharacter-level transduction models are often\ntrained with less data than their word-level coun-\nterparts: In contrast to machine translation, where\nmillions of training samples are available, the 2018\nSIGMORPHON shared task (Cotterell et al., 2018)\nhigh-resource setting only provides ≈ 10k training\nexamples per language. It is also not obvious that\nnon-recurrent architectures such as the transformer\n1This claim is also based on the authors’ personal commu-\nnication with other researchers in morphology in the corridors\nof conferences and through email.\n1902\nraemsPSTV V.PTCP<s> </s>\nVanilla\nraemsPSTV V.PTCP<s> </s>\nFeature Invariant\nToken\nPosition\nType\n0 1 2 3 4 5 6 7 8 9 0 0 0 0 1 2 3 4 5 6\nF F F C C C C C\n+ + + + + + + + + + + + + + + + + + + +\n+ + + + + + + +\nFigure 2: Handling of feature-guided character-level transduction with special position and type embeddings in the\nencoder. F denotes features while C denotes characters. We use morphological inﬂection as an example, inﬂecting\nsmear into its past participle form, smeared.\nshould provide an advantage at many character-\nlevel tasks: For instance, Gehring et al. (2017) and\nVaswani et al. (2017) suggest that transformers (and\nconvolutional models in general) should be better\nat remembering long-range dependencies. In the\ncase of morphology, none of these considerations\nseem relevant: inﬂecting a word (a) requires little\ncapacity to model long-distance dependencies and\nis largely a monotonic transduction; (b) it involves\nno semantic disambiguation, the tokens in question\nbeing letters; (c) it is not a task for which paral-\nlelization during training appears to help, since\ntraining time has never been an issue in morphol-\nogy tasks.2\nIn this work, we provide state-of-the-art num-\nbers for morphological inﬂection and historical\ntext normalization, a novel result in the litera-\nture. We also show the transformer outperforms\na strong recurrent baseline on two other character-\nlevel tasks: grapheme-to-phoneme (g2p) conver-\nsion and transliteration. We ﬁnd that a single hy-\nperparameter, batch size, is largely responsible for\nthe previous poor results. Despite having fewer pa-\nrameters, the transformer outperforms the recurrent\nsequence-to-sequence baselines on all four tasks.\nWe conduct a short error analysis on the task of\nmorphological inﬂection to round out the paper.\n2 The Transformer for Characters\nThe Transformer. The transformer, originally\ndescribed by Vaswani et al. (2017), is a self-\nattention-based encoder-decoder model. The en-\ncoder has N layers, consisting of a multi-head self-\nattention layer and a two-layer feed-forward layer\nwith ReLU activation, both equipped with a skip\nconnection. The decoder has a similar structure\nas the encoder except that, in each decoder layer\n2Many successful CoNLL–SIGMORPHON shared task\nparticipants report training their models on laptop CPUs.\nbetween the self-attention layer and feed-forward\nlayer, a multi-head attention layer attends to the\noutput of the encoder. Layer normalization (Ba\net al., 2016) is applied to the output of each skip\nconnection. Sinusoidal positional embeddings are\nused to incorporate positional information without\nthe need for recurrence or convolution. Here, we\ndescribe two modiﬁcations we make to the trans-\nformer for character-level tasks.\nA Smaller Transformer. As the dataset sizes in\ncharacter-level transduction tasks are signiﬁcantly\nsmaller than in machine translation, we employ a\nsmaller transformer with N = 4encoder-decoder\nlayers. We use 4 self-attention heads. The em-\nbedding size is dmodel = 256 and the hidden size\nof the feed-forward layer is dFF = 1024 . In\nthe preliminary experiments, we found that using\nlayer normalization before self-attention and the\nfeed-forward layer performed slightly better than\nthe original model. It is also the default setting\nof a popular implementation of the transformer\n(Vaswani et al., 2018). The transformer alone has\naround 7.37M parameters, excluding character em-\nbeddings and the linear mapping before the softmax\nlayer. We decode the model left to right in a greedy\nfashion.\nFeature Invariance. Some character-level trans-\nduction is guided by features. For example, in\nthe case of morphological reinﬂection, the task re-\nquires a set of morphological attributes that control\nwhat form a citation form is inﬂected into (see\nFig. 2 for an example). However, the order of the\nfeatures is irrelevant. In a recurrent neural network,\nfeatures are input in some predeﬁned order as spe-\ncial characters and pre- or postpended to the input\ncharacter sequence representing the citation form.\nThe same is true for a vanilla transformer model, as\nshown on the left-hand side of Fig. 2. This leads to\n1903\nLS β2 Vanilla Feature Invariant\n0 0.999 89.34 89.80\n0 0.98 89.62 89.92\n0.1 0.999 89.48 90.02\n0.1 0.98 89.98 90.28\nTable 1: Average development accuracy on morpho-\nlogical inﬂection with different LS and β2, which de-\nnote hyperparameter of label smoothing and Adam op-\ntimizer respectively.\ndifferent relative distances between a character and\na set of features.3 To avoid such an inconsistency,\nwe propose a simple remedy: We set the positional\nencoding of features to 0 and only start counting\nthe positions for characters. Additionally, we add\na special token to indicate whether a symbol is a\nword character or a feature. The right-hand side\nof Fig. 2 evinces how we have the same relative\ndistance between characters and features.\n3 Empirical Findings\nTasks. We consider four character-level transduc-\ntion tasks: morphological inﬂection, grapheme-to-\nphoneme conversion, transliteration, and historical\ntext normalization. For morphological inﬂection,\nwe use the 2017 SIGMORPHON shared task data\n(Cotterell et al., 2017) with 52 languages. The\nperformance is evaluated by accuracy (ACC) and\nedit distance (Dist). For the g2p task, we use the\nunstressed CMUDict (Weide, 1998) and NETtalk\n(Sejnowski and Rosenberg, 1987) resources. We\nuse the splits from Wu et al. (2018). We evaluate un-\nder word error rate (WER) and phoneme error rate\n(PER). For transliteration, we use the NEWS 2015\nshared task data (Zhang et al., 2015).4 For histori-\ncal text normalization, we follow Bollmann (2019)\nand use datasets for Spanish (S ´anchez-Mart´ınez\net al., 2013), Icelandic and Swedish (Pettersson\net al., 2013), Slovene (Scherrer and Erjavec, 2013,\n2016; Ljubeˇsic et al., 2016), Hungarian and Ger-\nman (Pettersson, 2016).5 We evaluate using accu-\nracy (ACC) and character error rate of incorrect\nprediction (CERi).\nOptimization. We use Adam (Kingma and Ba,\n2014) with a learning rate of 0.001 and an inverse\n3While the features could be encoded with a binary vector\nfollowed by MLP, it introduces a representation bottleneck for\nencoding features.\n4We do not have access to the test set.\n5We do not include English due to licensing issues.\nFigure 3: Distribution of incorrectly inﬂected forms in\nthe test set of the inﬂection task over all 52 languages\ngrouped by desired output word length.\nsquare root learning rate scheduler (Vaswani et al.,\n2017) with 4k steps during the warm-up. We train\nthe model for 20k gradient updates and save and\nevaluate the model every 400 gradient updates. We\nselect the best model out of 50 checkpoints based\non development set accuracy. The number of gradi-\nent updates and checkpoints are roughly the same\nas Wu and Cotterell (2019), the single model state\nof the art on the 2017 SIGMORPHON dataset. We\nuse their model as a baseline model. For all experi-\nments, we use a single predeﬁned random seed.\n3.1 A Controlled Hyperparameter Study\nTo demonstrate the importance of hyperparame-\nter tuning for the transformer on character-level\ntasks, we perform a small controlled hyperparame-\nter study. This is important since researchers had\npreviously failed to achieve high-performing re-\nsults with the transformer on character-level tasks.\nHere, we look at morphological inﬂection on the\nﬁve languages in the 2017 SIGMORPHON dataset\nwhere submitted systems performed the worst:\nLatin, Faroese, French, Hungarian, and Norwegian\n(Nynorsk). We set the dropout to 0.3, β2 of Adam\nto 0.999 (the default value), and do not use label\nsmoothing. We do not tune any other hyperparam-\neter except the following three hyperparameters.\nThe Importance of Batch Size. While recurrent\nmodels like Wu and Cotterell use a batch size of 20,\nhalving the learning rate when stuck and employ-\ning early stopping, we ﬁnd that a less aggressive\nlearning rate scheduler, allowing the model to train\nlonger, outperforms these hyperparameters. Fig. 1\nshows that the signiﬁcant impact of batch size on\nthe transformer. The transformer performance in-\n1904\nACC Dist\nSilfverberg et al. (2017)* 92.97 0.170\nWu et al. (2018) 93.60 0.128\nWu and Cotterell (2019) 94.40 0.113\nWu and Cotterell (2019) (Our eval) 94.81 0.123\nMakarov et al. (2017)* 95.12 0.100\nBergmanis et al. (2017)* 95.32 0.100\nTransformer (Dropout = 0.3) 95.59 0.088\nTransformer (Dropout = 0.1) 95.56 0.090\nTable 2: Average test performance on morphological\ninﬂection of Transformer against models from the liter-\nature. ∗ denotes model ensembling.\ncreases steadily as the batch size is increased, sim-\nilarly to what Popel and Bojar (2018) observe for\nmachine translation. The transformer only outper-\nforms the recurrent baseline when the batch size is\nat least 128, which is much larger than batch size\ncommonly used in recurrent models.6 Note that the\nmodel of Wu and Cotterell has 8.66M parameters,\n17% more than the transformer model. To get an\napples-to-apples comparison, we apply the same\nlearning rate scheduler to Wu and Cotterell; this\ndoes not yield similar improvements and underper-\nforms with respect to the traditional learning rate\nscheduler. Our feature invariant transformer also\noutperforms the vanilla transformer model. We\nset the batch size to 400 for our main experiments.\nNote the batch size of 400 is especially large (4%\nof training data) considering the training size is\nonly 10k.\nOther Hyperparameters. Vaswani et al. (2017)\napplies label smoothing (Szegedy et al., 2016) of\n0.1 to the transformer model and shows that it hurts\nperplexity, but improves BLEU scores for machine\ntranslation. Instead of the default 0.999 β2 for\nAdam, Vaswani et al. (2017) uses 0.98 and we ﬁnd\nthat both choices beneﬁt character-level transduc-\ntion tasks as well (see Tab. 1).\n3.2 New State-of-the-Art Results\nWe train our feature invariant transformer on the\nfour character-level tasks, exhibiting state-of-the-\nart results on morphological inﬂection and histori-\ncal text normalization.\n6It is also large in the context of character-level tasks,\nwhich typically have around 10k training examples. Batch\nsize of 400 would imply approximately 4% of training data in\na single gradient update.\nACC CERi ACCs CERsi\nLjubeˇsi´c et al. (2016) 91.78 0.392 90.37 0.360\nLjubeˇsi´c et al. (2016) (LM) 91.56 0.399 89.93 0.368\nBollmann (2018) 91.27 0.381 89.73 0.350\nTang et al. (2018a) 91.67 0.389 90.32 0.358\nFlachs et al. (2019) - - 90.06 -\nTransformer (Dropout = 0.3) 91.300.340 89.99 0.330\nTransformer (Dropout = 0.1)91.85 0.352 90.61 0.334\nTable 3: Average test performance on historical text\nnormalization of Transformer against models from the\nliterature. s denote subset of dataset as Flachs et al.\n(2019) only experiment with subset of languages.\nWER PER ACC MFS\nWu et al. (2018) 28.20 0.068 41.10 0.894\nWu and Cotterell (2019) 28.20 0.069 41.20 0.895\nTransformer (Dropout = 0.3) 28.08 0.07043.39 0.897\nTransformer (Dropout = 0.1)27.63 0.069 41.35 0.891\nTable 4: Average test performance on Grapheme-to-\nPhoneme and dev performance on Transliteration of\nTransformer against models from the literature.\nMorphological Inﬂection. As shown in Tab. 2,\nthe feature invariant transformer produces state-of-\nthe-art results on the 2017 SIGMORPHON shared\ntasks, improving upon ensemble-based systems by\n0.27 points. We observe that as the dataset de-\ncreases in size, a model with a larger dropout value\nperforms slightly better. A brief tally of phenomena\nthat are difﬁcult to learn for many machine learn-\ning models, categorized along typical linguistic\ndimensions (such as word-internal sound changes,\nvowel harmony, circumﬁxation, ablaut, and umlaut\nphenomena) fail to reveal any consistent pattern of\nadvantage to the transformer model. In fact, errors\nseem to be randomly distributed with an overall ad-\nvantage of the transformer model. Curiously, errors\ngrouped along the dimension of word length reveal\nthat as word forms grow longer, the transformer\nadvantage shrinks (Fig. 3).\nHistorical Text Normalization. Tab. 3 shows\nthat the transformer model with dropout of 0.1, as\nin the case of morphological inﬂection, improves\nupon the previous state of the art, although the\nmodel with a dropout of 0.3 yields a slightly better\nCERi.\nG2P and Transliteration. Tab. 4 shows that\nthe transformer outperforms previously published\nstrong recurrent models on two tasks despite hav-\ning fewer parameters. A dropout rate of 0.3 yields\n1905\nsigniﬁcantly better performance on the translitera-\ntion task while a dropout rate of 0.1 is stronger on\nthe g2p task. This shows that transformers can and\ndo outperform recurrent transducers on common\ncharacter-level tasks when properly tuned.\n4 Related Work\nCharacter-level transduction is largely dominated\nby attention-based LSTM sequence-to-sequence\n(Luong et al., 2015) models (Cotterell et al., 2018).\nCharacter-level transduction tasks usually involve\ninput-output pairs that share large substrings and\nalignments between these are often monotonic.\nModels that address the task tend to focus on ex-\nploiting such structural bias. Instead of learning\nthe alignments, Aharoni and Goldberg (2017) use\nexternal monotonic alignments from the SIGMOR-\nPHON 2016 shared task baseline Cotterell et al.\n(2016). Makarov et al. (2017) use this approach\nto win the CoNLL-SIGMORPHON 2017 shared\ntask on morphological inﬂection (Cotterell et al.,\n2017). Wu et al. (2018) shows that explicitly model-\ning alignment (hard attention) between source and\ntarget characters outperforms soft attention. Wu\nand Cotterell (2019) further shows that enforcing\nmonotonicity in a hard attention model improves\nperformance.\n5 Conclusion\nUsing a large batch size and feature invariant input\nallows the transformer to achieve strong perfor-\nmance on character-level tasks. However, it is un-\nclear what linguistic errors the transformer makes\ncompared to recurrent models on these tasks. Fu-\nture work should analyze the errors in detail as\nGorman et al. (2019) does for recurrent models.\nWhile Wu and Cotterell shows that the monotonic-\nity bias beneﬁts character-level tasks, it is not evi-\ndent how to enforce monotonicity on multi-headed\nself-attention. Future work should consider how\nto best incorporate monotonicity into the model,\neither by enforcing it strictly (Wu and Cotterell,\n2019) or by pretraining the model to copy (Anasta-\nsopoulos and Neubig, 2019).\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Morphologi-\ncal inﬂection generation with hard monotonic atten-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2004–2015, Vancouver,\nCanada. Association for Computational Linguistics.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nﬂection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n984–996, Hong Kong, China. Association for Com-\nputational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nLo¨ıc Barrault, Ond ˇrej Bojar, Marta R. Costa-juss `a,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias M ¨uller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine transla-\ntion (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nToms Bergmanis, Katharina Kann, Hinrich Sch ¨utze,\nand Sharon Goldwater. 2017. Training data aug-\nmentation for low-resource morphological inﬂection.\nIn Proceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 31–39, Vancouver. Association for Computa-\ntional Linguistics.\nMarcel Bollmann. 2018. Normalization of historical\ntexts with neural network models . Ph.D. thesis,\nBochum, Ruhr-Universit¨at Bochum.\nMarcel Bollmann. 2019. A large-scale comparison of\nhistorical text normalization systems. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 3885–3898, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Arya D.\nMcCarthy, Katharina Kann, Sebastian Mielke, Gar-\nrett Nicolai, Miikka Silfverberg, David Yarowsky,\nJason Eisner, and Mans Hulden. 2018. The CoNLL–\nSIGMORPHON 2018 shared task: Universal mor-\nphological reinﬂection. In Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task: Univer-\nsal Morphological Reinﬂection , pages 1–27, Brus-\nsels. Association for Computational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Patrick\nXia, Manaal Faruqui, Sandra K ¨ubler, David\nYarowsky, Jason Eisner, and Mans Hulden. 2017.\nCoNLL-SIGMORPHON 2017 shared task: Univer-\nsal morphological reinﬂection in 52 languages. In\n1906\nProceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 1–30, Vancouver. Association for Computa-\ntional Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016. The SIGMORPHON 2016 shared Task—\nMorphological reinﬂection. In Proceedings of the\n14th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphol-\nogy, pages 10–22, Berlin, Germany. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042–13054.\nSimon Flachs, Marcel Bollmann, and Anders Søgaard.\n2019. Historical text normalization with delayed\nrewards. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1614–1619, Florence, Italy. Association\nfor Computational Linguistics.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR.\nKyle Gorman, Arya D. McCarthy, Ryan Cotterell,\nEkaterina Vylomova, Miikka Silfverberg, and Mag-\ndalena Markowska. 2019. Weird inﬂects but OK:\nMaking sense of morphological generation errors.\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL) , pages\n140–151, Hong Kong, China. Association for Com-\nputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNikola Ljubeˇsic, Katja Zupan, Darja Fi ˇser, and Tomaz\nErjavec. 2016. Normalising Slovene data: historical\ntexts vs. user-generated content. In Proceedings of\nthe 13th Conference on Natural Language Process-\ning (KONVENS 2016), pages 146–155.\nNikola Ljubeˇsi´c, Katja Zupan, Darja Fi ˇser, and Tomaˇz\nErjavec. 2016. Normalising Slovene data: historical\ntexts vs. user-generated content. In Proceedings of\nthe 13th Conference on Natural Language Process-\ning (KONVENS 2016), pages 146–155.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nPeter Makarov, Tatiana Ruzsics, and Simon Clematide.\n2017. Align and copy: UZH at SIGMORPHON\n2017 shared task for morphological reinﬂection. In\nProceedings of the CoNLL SIGMORPHON 2017\nShared Task: Universal Morphological Reinﬂection,\npages 49–57, Vancouver. Association for Computa-\ntional Linguistics.\nArya D. McCarthy, Ekaterina Vylomova, Shijie Wu,\nChaitanya Malaviya, Lawrence Wolf-Sonkin, Gar-\nrett Nicolai, Christo Kirov, Miikka Silfverberg, Se-\nbastian J. Mielke, Jeffrey Heinz, Ryan Cotterell, and\nMans Hulden. 2019. The SIGMORPHON 2019\nshared task: Morphological analysis in context and\ncross-lingual transfer for inﬂection. In Proceedings\nof the 16th Workshop on Computational Research in\nPhonetics, Phonology, and Morphology, pages 229–\n244, Florence, Italy. Association for Computational\nLinguistics.\nEva Pettersson. 2016. Spelling normalisation and lin-\nguistic analysis of historical text for information ex-\ntraction. Ph.D. thesis, Acta Universitatis Upsalien-\nsis.\nEva Pettersson, Be ´ata Megyesi, and J ¨org Tiedemann.\n2013. An SMT approach to automatic annotation\nof historical text. In Proceedings of the workshop\non computational historical linguistics at NODAL-\nIDA 2013; May 22-24; 2013; Oslo; Norway. NEALT\nProceedings Series 18, 087, pages 54–69. Link¨oping\nUniversity Electronic Press.\nMartin Popel and Ond ˇrej Bojar. 2018. Training tips\nfor the transformer model. The Prague Bulletin of\nMathematical Linguistics, 110(1):43–70.\nFelipe S ´anchez-Mart´ınez, Isabel Mart ´ınez-Sempere,\nXavier Ivars-Ribes, and Rafael C. Carrasco. 2013.\nAn open diachronic corpus of historical Spanish:\nannotation criteria and automatic modernisation of\nspelling. arXiv preprint arXiv:1306.3692.\nYves Scherrer and Tomaˇz Erjavec. 2013. Modernizing\nhistorical Slovene words with character-based smt.\nIn BSNLP 2013-4th Biennial Workshop on Balto-\nSlavic Natural Language Processing.\nYves Scherrer and Tomaˇz Erjavec. 2016. Modernising\nhistorical Slovene words. Natural Language Engi-\nneering, 22(6):881–905.\nTerrence J. Sejnowski and Charles R. Rosenberg. 1987.\nParallel networks that learn to pronounce English\ntext. Complex Systems, 1.\n1907\nMiikka Silfverberg, Adam Wiemerslage, Ling Liu, and\nLingshuang Jack Mao. 2017. Data augmentation for\nmorphological reinﬂection. In Proceedings of the\nCoNLL SIGMORPHON 2017 Shared Task: Univer-\nsal Morphological Reinﬂection , pages 90–99, Van-\ncouver. Association for Computational Linguistics.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 2818–2826.\nGongbo Tang, Fabienne Cap, Eva Pettersson, and\nJoakim Nivre. 2018a. An evaluation of neural\nmachine translation models on historical spelling\nnormalization. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\npages 1320–1331, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nGongbo Tang, Mathias M¨uller, Annette Rios, and Rico\nSennrich. 2018b. Why self-attention? a targeted\nevaluation of neural machine translation architec-\ntures. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4263–4272, Brussels, Belgium. Association\nfor Computational Linguistics.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, et al. 2018. Tensor2tensor for neural ma-\nchine translation. arXiv preprint arXiv:1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nR.L. Weide. 1998. The Carnegie Mellon pronouncing\ndictionary.\nShijie Wu and Ryan Cotterell. 2019. Exact hard mono-\ntonic attention for character-level transduction. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1530–\n1537, Florence, Italy. Association for Computational\nLinguistics.\nShijie Wu, Pamela Shapiro, and Ryan Cotterell. 2018.\nHard non-monotonic attention for character-level\ntransduction. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4425–4438, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMin Zhang, Haizhou Li, Rafael E. Banchs, and A Ku-\nmaran. 2015. Whitepaper of NEWS 2015 shared\ntask on machine transliteration. In Proceedings of\nthe Fifth Named Entity Workshop , pages 1–9, Bei-\njing, China. Association for Computational Linguis-\ntics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7433167099952698
    },
    {
      "name": "Computer science",
      "score": 0.6590677499771118
    },
    {
      "name": "Transduction (biophysics)",
      "score": 0.6376451253890991
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5051630139350891
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.4873441159725189
    },
    {
      "name": "Recurrent neural network",
      "score": 0.47562816739082336
    },
    {
      "name": "Speech recognition",
      "score": 0.4047909677028656
    },
    {
      "name": "Natural language processing",
      "score": 0.39914634823799133
    },
    {
      "name": "Artificial neural network",
      "score": 0.30903857946395874
    },
    {
      "name": "Engineering",
      "score": 0.12818709015846252
    },
    {
      "name": "Voltage",
      "score": 0.07946497201919556
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}