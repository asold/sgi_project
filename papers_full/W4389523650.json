{
  "title": "Text encoders bottleneck compositionality in contrastive vision-language models",
  "url": "https://openalex.org/W4389523650",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5014130591",
      "name": "Amita Kamath",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5043614405",
      "name": "Jack Hessel",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5087096372",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W1923162067",
    "https://openalex.org/W4312206006",
    "https://openalex.org/W4312261477",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W1525482321",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4389520792",
    "https://openalex.org/W4312090938",
    "https://openalex.org/W3162926177",
    "https://openalex.org/W3110019360",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4386076015",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4380715531",
    "https://openalex.org/W1909455558",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4382323313",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3163542683",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4312381519",
    "https://openalex.org/W2963800628"
  ],
  "abstract": "Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP’s text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multimodal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of fine-grained compositional images and captions. Specifically, our results suggest text-only recoverability is a necessary (but not sufficient) condition for modeling compositional factors in contrastive VL models. We release our datasets and code.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4933–4944\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nText encoders bottleneck compositionality\nin contrastive vision-language models\nAmita Kamath1 Jack Hessel2 Kai-Wei Chang1\n1 University of California, Los Angeles\n2 Allen Institute for AI\n{kamatha, kwchang}@cs.ucla.edu, jackh@allenai.org\nAbstract\nPerformant vision-language (VL) models like\nCLIP represent captions using a single vec-\ntor. How much information about language\nis lost in this bottleneck? We first curate\nCompPrompts, a set of increasingly composi-\ntional image captions that VL models should\nbe able to capture (e.g., single object, to ob-\nject+property, to multiple interacting objects).\nThen, we train text-only recovery probes that\naim to reconstruct captions from single-vector\ntext representations produced by several VL\nmodels. This approach does not require im-\nages, allowing us to test on a broader range of\nscenes compared to prior work. We find that: 1)\nCLIP’s text encoder falls short on more compo-\nsitional inputs, including object relationships,\nattribute-object association, counting, and nega-\ntions; 2) some text encoders work significantly\nbetter than others; and 3) text-only recovery\nperformance predicts multimodal matching per-\nformance on ControlledImCaps: a new evalu-\nation benchmark we collect and release consist-\ning of fine-grained compositional images and\ncaptions. Specifically, our results suggest text-\nonly recoverability is a necessary (but not suf-\nficient) condition for modeling compositional\nfactors in contrastive VL models. We release\nour datasets and code.\n1 Introduction\n“A penguin on Mars wearing a spacesuit and walking\na robot dog next to Santa Claus. ”Riedl (2022)’s\ntext-to-image query is the type that modern multi-\nmodal models should be able to support. It is spa-\ntially precise (the dog is next to Santa, not in front),\ncompositional (robot dog, but notrobot Santa), and\nimaginative (it is unlikely such an image exists al-\nready). However, several recent works have shown\nthat a variety of multimodal models (despite achiev-\ning strong benchmark performance) are frequently\nunable to reason about even simple spatial rela-\ntions or attribute attachments (Gokhale et al., 2022;\nThrush et al., 2022; Yuksekgonul et al., 2023).\nVerbs …\ncan the model do image-textmatching? (§4.1)\nIncorrect\nA person after opening an umbrellaA person before opening an umbrella\ncan a probereconstruct the input? (§3) predicts(§4.2-4.5)\na cheetah before eatinga bride before flyinga businessperson after drivingan instructor after sitting and a presidenta politician after helping a ridera dog after carrying a stallion\na cheetah before eatinga bride before her flighta businessperson while drivinga president and after sitting on instructora rider after helping politiciana stallion dog after carrying a dog\nContrastive VL modelContrastive VL text encoder\nTemporal RelationsSpatial Relations\nCompPromptsControlledImCaps\nFigure 1: We present CompPrompts, a dataset of 18,100\ntext prompts, and ControlledImCaps, a dataset of 600\nimage pairs+captions that differ by only one word. The\ntwo datasets are grouped by the same set of caption\nproperties, e.g., temporal/spatial relations. Experiments\non CompPrompts quantify the information loss of a text\nencoder; experiments on ControlledImCaps illustrate\nthat information loss correlates with multimodal errors.\nUnderlying several popular multimodal mod-\nels like CLIP (Radford et al., 2021), DALL-E 2\n(Ramesh et al., 2022) and ALIGN (Jia et al., 2021)\nis a pooled text encoder,i.e., a text representation\nmodel that outputs a single vector for a given input\ncaption.1 In this work, we use this representational\nbottleneck as an interface to ask: how precise are\ntextual representations of visually-descriptive lan-\nguage in these modern multimodal models?\n1Pooled text encoders (c.f., bidirectional multimodal en-\ncoders) are used for a variety of practical reasons: e.g., for\nguided diffusion (Dhariwal and Nichol, 2021), for fast k-NN\nqueries over billions of images (Schuhmann et al., 2022), for\ncontrastive objectives dependent on large batch sizes like Rad-\nford et al. (2021)’s 32K example “mini”-batch, etc.\n4933\nCLIP text encoder\nCLIP image encoder\nan iguana\nT5text decoder(finetuned)\na happy dinosaura surfer carrying a lifeguard\nan orangutan eating and an officer flying\nan iguanaan amusing dinosaura lifeguard carrying a surferan orangutan and an officer eating an orangutan\n❄\n✗✗✓✓\n(unused)\nFigure 2: We probe the representations of single-vector\ntext encoders used in popular VL models. Using a\ncorpus of increasingly compositional image captions,\nCompPrompts, we attempt to generatively decode the\noriginal input sentence. Text encoders of popular mod-\nels like CLIP fail to effectively encode precise aspects\nof their captions like attribute attachments and object\nrelationships (real examples shown, as in Figure 1).\nOur strategy is as follows: for a fixed pooled\ntext encoder T : x →y, which maps from cap-\ntions x to vectors y ∈Rd, we test how accurately\nx can be recovered by an expressive generative de-\ncoder given y, i.e., P(x|T(x)). In an ideal case,\nT should result in no information loss, i.e., an ex-\nact reconstruction of the original caption should\nbe possible, to account for specific visual factors.\nHowever, we hypothesize that if a specific visu-\nally descriptive property (e.g., a spatial relation)\ncannot be accurately decoded from y (using a de-\ncoder trained with significant supervision), then it\nis unlikely a multimodal model can effectively use\nthat property of x using T. Different from exist-\ning probes, ours does not require images, enabling\nexploration of a broader range of captions, e.g., cre-\native text-to-image queries for which there may be\nno associated image (like “A penguin on Mars... ”).\nWe execute our probe using an increasingly-\ncompositional hierarchy of image captions we cu-\nrate, CompPrompts, which covers cases ranging\nfrom a single object with no attributes (e.g. “a\ncat”) to multiple objects with attributes and rela-\ntions (e.g. “an orange cat to the left of a dog”). We\nalso test counting (e.g. “three cats”) (Seguí et al.,\n2015; Parcalabescu et al., 2021) and negations (e.g.\n“a cat that is not yawning”). We compare five text\nencoders, and find that top contrastive VL models:\n(1) are broadly ineffective at textually encoding\nspatial relations, numbers, and negations; (2) fre-\nquently cannot match attributes to their correspond-\ning objects; and (3) fail more as inputs grow more\ncompositional. While some text encoders perform\nsignificantly better than others, all underperform a\nproof-of-concept model which demonstrates that\nour prompts can indeed be compressed into single\nvectors with little information loss.\nIn order to verify that our text-only probe pre-\ndicts performance in multimodal settings, we cu-\nrate an evaluation set of image-caption pairs, Con-\ntrolledImCaps, which operationalizes the compo-\nsitional factors of CompPrompts in a multimodal\nsetting. Results on this corpus suggest our text-\nonly probe gives a necessary condition: if the\ntext-only recovery probe fails to recover a text-\nonly property on CompPrompts, then the associ-\nated multimodal model also performs poorly for\nthat property on ControlledImCaps. However,\nour results also suggest that text-only recover-\nability is not a sufficient condition: a model can\nachieve low text-only information loss on a par-\nticular prompt type but not fully solve it on Con-\ntrolledImCaps. To facilitate future probing experi-\nments, we release our code alongside the newly col-\nlected CompPrompts and ControlledImCaps cor-\npora at https://github.com/amitakamath/vl_\ntext_encoders_are_bottlenecks.\n2 Evaluation Corpora\n2.1 CompPrompts\nWe create an evaluation dataset of 18,100 text\nprompts describing potential visual scenes with\nvarying degrees of specificity and composition.\nOur starting point is animate nouns with corre-\nsponding verbs and adjectives from the Web10K\ndataset (Kamath et al., 2022). We remove some\nsynonyms to prevent ambiguity in the prompt (e.g.\n“a rhino to the left of a rhinoceros”).\nThe prompts are increasingly compositional:\nThey have 1 or 2 unique nouns, and 0, 1, or 2\nattributes, of which there are 4 types: adjective,\nverb, spatial, and temporal. Nouns are randomly\nmatched to generate prompts with two unique\nnouns — this results in unusual and imaginative\ntext inputs that cannot be guessed based on priors\nlearned during model pre-training (e.g., “a crab lift-\n4934\ning a rhino”). The verb and spatial attributes can\nhave either one associated noun (i.e. intransitive,\ne.g. “a koala yawning”, “a policeman on the left”)\nor two (i.e. transitive, e.g. “a poet chasing a rabbit”,\n“a dinosaur left of a tiger”). We also test multiples\nand negations in the one-attribute setting.\nPrompt examples of each type are given in Ta-\nbles 2 and 3. There are 300-500 examples of each\nprompt type in the dataset.\n2.2 ControlledImCaps\nWe create a second evaluation dataset to evaluate\nthe overall vision-language model, rather than the\ntext encoder specifically: where CompPrompts con-\ntains text prompts alone, ControlledImCaps con-\ntains 600 pairs of images, along with a correspond-\ning caption for each image.\nThe images are sourced from the COCO valida-\ntion set (Lin et al., 2014), and the captions are hand-\nwritten to study one of six specific fine-grained\ntextual properties: spatial relations with one asso-\nciated noun, spatial relations with two associated\nnouns, temporal relations, verbs with one associ-\nated noun, verbs with two associated nouns, or\nadjectives. For spatial relations, we evaluate only\n“left” and “right” (unlikeCompPrompts, which eval-\nuates also “above”, “under”, “in front of”, and “be-\nhind”), due to insufficient presence of other spatial\nrelations clearly depicted in the COCO data.\nA key property ofControlledImCaps is that only\none word changes between the two captions associ-\nated with a given image pair, such that the relation\nis changed or inverted: e.g., the caption pair “a per-\nson before opening an umbrella”, “a person after\nopening an umbrella”, along with the correspond-\ning images for each (as in Figure 1) tests the overall\nmodel’s understanding of temporal relations alone,\nwithout conflating any other types of reasoning.\n3 Text-only Recovery\nFor a given text encoderT, our first step is to obtain\na training corpus of representations to fit a decoding\nprobe P(x|T(x)). We use (just the text of) Con-\nceptual Captions 3M (Sharma et al., 2018) (CC3M)\nsplit into a 90/10 train/val set; this corpus consists\nof cleaned alt-texts from web images, and thus is\nsimilar to the pretraining corpora of many VL mod-\nels. For P, we use T5-large: specifically, we con-\ndition the decoder on T(x), followed by a linear\ntransformation and layer normalization. We train\nusing Adafactor (Shazeer and Stern, 2018) with\nT(x) Embed.\nsize Avg. EM (%)\nCLIP ViT-B/32 512 13.2\nCLIP ViT-L/14 768 28.5\nnegCLIP ViT-B/32 512 28.6\nRoBERTaCLIP ViT-B/32 512 28.9\nSBERT 768 41.6\nProof-of-concepT5 1024 92.9\nTable 1: Average EM performance of each text encoder\non CompPrompts, not including multiples and negations\n(reported in Table 3).\na batch size of 512 for 4 epochs over CC3M; we\nselect checkpoints with the lowest val loss. Mod-\nels are trained using 4xA6000 GPUs with 48GB of\nmemory using Transformers (Wolf et al., 2019) and\naccelerate2. At evaluation time, we generate cap-\ntions for CompPrompts set using beam=5 search.\nText Models. We evaluate several T models:\nCLIP ViT-B/32 (12 layers, 512 dim) and ViT-\nL/14 (12 layers, 768 dim) (Radford et al., 2021),\nCLIP with a RoBERTa-pretrained text encoder (Liu\net al., 2019; Ilharco et al., 2021), and Yuksek-\ngonul et al. (2023)’s more-order-aware CLIP en-\ncoder finetuned with hard negatives, negCLIP. For\ncomparison, we also consider the uni-modal Sen-\ntenceBERT (Reimers and Gurevych, 2019) model\nall-mpnet-base-v2, which is trained on several\nsentence similarity datasets including COCO cap-\ntions (Lin et al., 2014).\nProof-of-concepT5 We also consider a T5-large\ntext encoder that produces a single vector output\nvia mean pooling over the token embeddings. In\ncontrast to the other fixed encoders, we fine-tune\nthis model on CC3M, like an autoencoder3. Then,\nwe use the resulting encoder as a feature extractor,\nand hand a dimension-shuffled version of the re-\nsulting embeddings to the probe. This “proof of\nconcept” encoder is specifically optimized to gener-\nate a vector from which a T5 model can decode the\nfull sentence, and serves to validate that our probe\nsetup is even possible.\nEvaluation. We evaluate using exact match\n(EM). While we report BLEU scores in the Ap-\npendix, for our high-precision setting, partial credit\nmetrics are too generous, e.g., generating “a re-\n2https://github.com/huggingface/accelerate\n3There is no overlap between CC3M and CompPrompts.\n4935\n0 attributes 1 attribute 2 attributes\n1 adj. 1 spatial1 1-objverb1 2-objverb 2 adj. 1 adj +1 spatial\n1 adj +1 1-objverb\n1 adj +1 2-objverb\n1 spatial+ 1 1-objverb\n1 spatial+ 1 2-objverb 2 spatial1 temp. +1 1-objverb\n1 temp. +1 2-objverb 2 verbs\n1 uniqueobject a cat an orangecat a cat onthe lefta catyawning- an orangeand spottedcat\nan orangecat onthe left\nan orangecatyawning- a cat onthe leftyawning- - a catbeforeyawning- -\nCLIP ViT-B/32 3.0 38.2 34.6 15.4 14.4 47.0 36.4 15.6 17.0CLIP ViT-L/14 33.0 81.8 53.2 71.6 23.2 43.8 62.6 6.2 58.6negCLIP ViT-B/32 2.3 42.2 57.2 20.6 20.8 63.6 50.4 22.8 42.4RoBERTa-CLIP ViT-B/32 1.3 17.0 89.4 30.0 42.2 83.4 59.8 5.6 28.4SBERT 54.0 91.8 91.8 78.4 35.6 85.6 76.8 21.2 57.6\n2 uniqueobjects a cat anda dog\nan orangecat and adog / a catand abrown dog\na cat tothe leftof a dog\na catyawningand adog\na catchasinga dog\nan orangecat and abrown dog\nan orangecat to theleft of adog\na catyawningand abrown dog\na catchasinga browndog / anorange catchasinga dog\na catyawningto theleft of adog\na catchasing adog on theleft\na cat onthe rightand a dogon the left\na catbeforeyawningand a dog\na catbeforechasinga dog\na catyawningand a dogstretching\nCLIP ViT-B/32 13.4 15.8 6.2 1.6 6.6 17.8 6.2 0.8 7.2 8.8 4.2 1.2 1.6 3.4 1.8CLIP ViT-L/14 48.4 30.2 17.8 5.0 37.2 23.6 14.0 1.4 21.0 16.6 7.8 0.6 0.2 21.6 4.8negCLIP ViT-B/32 52.8 41.4 36.2 12.2 39.4 35.6 24.8 4.8 24.2 28.6 16.4 12.4 7.4 19.2 8.8RoBERTa-CLIP ViT-B/32 47.6 29.0 34.0 14.6 48.8 24.0 20.7 8.0 29.8 30.5 39.2 5.6 0.4 26.8 1.6SBERT 56.0 41.8 44.8 11.2 48.8 32.8 30.3 8.0 30.2 31.9 23.8 3.0 4.4 4.8 10.8\nTable 2: Prompt example and exact match (% EM) score of reconstruction from all models, averaged over several\nhundred instances each. As inputs become more compositional, vision-language text encoders perform increasingly\npoorly on text reconstruction.\nporter on top of a penguin” as “a penguin on top\nof a hill” scores 48 BLEU-4 points. Similarly for\nBERT-Score (Zhang et al., 2020), where generat-\ning “two rabbits and three shrimps” as “four of the\nshrimps and a rabbit” scores 0.91 F1.\n3.1 Text-only Recovery Results\nTable 1 presents the average exact match of each\nmodel over the corpus of prompts in CompPrompts,\nexcluding negations and multiples, which are re-\nported in Table 3. The proof-of-concepT5 model’s\nhigh performance illustrates that it is possible in\ntheory to nearly exactly decode all captions in\nCompPrompts using T5-large, given the “right” en-\ncoding4. Beyond proof-of-concepT5, the best per-\nforming model is SBERT; and the best performing\nmultimodal model is RoBERTa-CLIP.\n3.2 Fine-Grained Results on Different Prompt\nTypes\nTable 2 contains EM results of all models on the\nvarious types of prompts in CompPrompts. A sep-\narate study on multiples and negations in Table 3\nshows that text encoders struggle to encode those\nas well. These results show that it is fairly difficult\nto decode input sentences from text representations\nfor most VL models, with increasingly composi-\ntional categories proving more difficult (e.g., “an\norange cat” to “an orange cat yawning” to “an or-\nange cat chasing a dog”).\nSpatial relations. Text encoders of VL models\nstruggle to represent spatial relations (average 23.7\n4Most errors made by proof-of-concepT5 are minor e.g.,\n“two physicians on the right” →“two physician on the right”.\nEM), particularly those between two objects (aver-\nage 13.8 EM). SBERT, in comparison, scores 36.9\nand 22.3 EM, respectively.\nTemporal relations. VL models perform poorly\non temporal relations, scoring on average 17.1 EM.\nIn comparison, SBERT scores 29.6 EM — likely\nbecause temporal relations appear more frequently\nin language than in web alt-text.\nTransitive vs intransitive verbs and prepositions.\nOn transitive verbs (e.g., “chasing”), CLIP ViT-\nB/32 and ViT-L/14 do worse by an average of 21\nEM than vs. intransitive verbs (e.g., “yawning”),\nwhereas negCLIP and RoBERTa-CLIP do better by\nan averaged 18.7 points. On transitive prepositions\n(“to the left of”) instead of intransitive (“on the\nleft”), all models do worse by an averaged 35 EM.\nNegations and multiples. Models perform\npoorly on negations (average EM 13.0) and multi-\nples (average EM 5.1). This agrees with previous\nobservations that VL models struggle with count-\ning (Seguí et al., 2015; Parcalabescu et al., 2021).\nPrompts where word order matters. VL text\nencoders struggle to capture word order: on\nprompts where word order matters less (e.g., “a cat\nand a dog”), they score an average of 34 EM, but\nwhere word order matters more, they score an aver-\nage of 15.8 EM. The failure cases are often caused\nby assigning attributes to nouns incorrectly, as high-\nlighted in the Appendix. This extends Thrush et al.\n(2022)’s and Yuksekgonul et al. (2023)’s finding\nthat contrastive VL models can behave like bags-\nof-words — this issue manifests just in the text\nencoder as well.\n4936\n0 attributes 1 attribute\n1 adj. 1 spatial1 1-objverb 1 2-objverb\n1 uniqueobject +multiplestwo catstwo orangecats two cats onthe left two catsyawning-\nCLIP ViT-B/32 0.3 15.0 9.6 9.4CLIP ViT-L/14 3.0 14.8 10.8 16.8negCLIP ViT-B/32 1.0 9.4 11.2 12.8RoBERTa CLIP ViT-B/32 0.7 4.6 16.0 6.4SBERT 37.3 56.6 48.4 33.22 uniqueobjects +multiples\ntwo catsandfour dogs- two cats tothe left offour dogs- two catschasingfour dogsCLIP ViT-B/32 0.0 0.0 0.0CLIP ViT-L/14 0.0 0.0 0.0negCLIP ViT-B/32 0.0 0.0 0.2RoBERTa CLIP ViT-B/32 0.6 0.2 0.6SBERT 0.0 0.0 0.0\n1 uniqueobject +negation - a cat thatis notorange\na cat thatis not onthe left\na cat thatis notyawning -\nCLIP ViT-B/32 18.0 7.4 6.8CLIP ViT-L/14 10.4 6.4 7.8negCLIP ViT-B/32 1.8 2.2 14.2RoBERTa CLIP ViT-B/32 43.8 50.4 58.4SBERT 32.8 4.6 19.4\n2 uniqueobjects +negation - -\na cat thatis not tothe leftof a dog\n-\na cat thatis notchasinga dogCLIP ViT-B/32 0.6 0.8CLIP ViT-L/14 0.4 0.4negCLIP ViT-B/32 2.6 1.8RoBERTa CLIP ViT-B/32 12.6 13.6SBERT 2.6 2.6\nTable 3: All models’ EM on prompts that contain multi-\nples or negations. Text recovery of these inputs is very\npoor, likely because multiples and negations tend to be\ninfrequent in image captions.\nAdjectives and verbs. VL models perform rel-\natively well in the basic one-object, one-attribute\nsetting on both adjectives (average EM 44.8) and\nverbs (average EM 34.5): even higher than the\nzero-attribute setting, where error analysis reveals\nthey tend to hallucinate information (“a tarantula”\n→“a tarantula in a hand”). While these numbers\nare well behind SBERT (EM 91.8 and 78.4 respec-\ntively), they agree with previous observations that\nVL models exhibit good visual recognition of basic\nadjectives and actions (Radford et al., 2021).\nCompositionality. Text encoders struggle with\nincreasingly compositional information, e.g., the\nprobe decodes SBERT(“a dentist after examining\nan ape”) →“an ape after examining a dentist”. On\naverage, performance on two unique objects drops\nby 49% from their performance on one unique ob-\nject (for CLIP ViT-B/32, it drops 71%). VL model\nperformance drops on average by 35% when the\nprompt contains two attributes compared to one.\n3.3 Fine-Grained Results for Different Model\nDesigns\nPre-training the text encoder helps, especially on\nnegations. The average EM of RoBERTa-CLIP\non prompts without multiples or negations is 15.7\npoints higher than CLIP ViT-B/32. However, on\nthe prompts that do include negations, its average\nEM is 29 points higher. This provides evidence that\ntext pre-training the text encoder helps negations,\npresumably because negations are less likely in\nalt-texts compared to other settings.\nIncreasing model size helps overall, but not on\nspatial relations. The average EM of CLIP ViT-\nL/14 on prompts that do not include spatial rela-\ntions is 20.7 points higher than CLIP ViT-B/32.\nHowever, on the prompts that do include spatial re-\nlations, its average EM is only 4 points higher. The\nmodest increase of text encoder size in the CLIP\ntraining regime appear less reliable for encoding\nspatial relations than text pre-training or hard neg-\natives (though, more significant scaling could be\nbeneficial, as in Imagen (Saharia et al., 2022)).\nHard negatives from Yuksekgonul et al. (2023)\nhelp, especially where word order matters. On\naverage, negCLIP does 15.4 points better than\nCLIP. On prompts where word order matters (e.g.\n“a cat chasing a dog”), it scores 16.3 points higher;\non prompts where word order does not matter (e.g.\n“a cat and a dog”), it scores 12.8 points higher.\n3.4 Incorrect Model Predictions\nWe manually inspect models’ incorrect predictions.\nDecoded VL text encoder predictions often come\nclose (e.g. “three shrimps” →“three of shrimp” is\na pattern shown by CLIP ViT-B/32, CLIP ViT-L/14\nand negCLIP), whereas SBERT’s incorrect decod-\nings fall further afield (e.g. “three gardeners” →\n“three gardeners and a third man.”). Thus, while the\nsuperior results of the unimodal SBERT compared\nto the VL text encoders when evaluated in the same\nfrozen-encoder setting (including CLIP ViT-L/14,\nwhich has the same text embedding size) show that\nthere is significant room for improvement for VL\ntext encoders, the types of errors made by each\nmodel may not be fully captured by EM. Nonethe-\nless, EM remains an appropriate metric for our\nhigh-precision setting, as discussed in Section 3.\n4 Experiments and Results in the\nMulti-modal Setting\nWe investigate the hypothesis that if a textual prop-\nerty cannot be decoded from the VL text encoder’s\nvector representation with a highly expressive de-\ncoder (like T5-Large), then it also cannot be readily\nmodeled in the multimodal setting. Controlled-\nImCaps studies the attributes from CompPrompts in\nthe multimodal setting. We then compare the text\n4937\nTypeComparable Prompts in                  xExample from                      x\nSpatial-11 unique object + 1 1-obj preposition (Left/Right only)\nSpatial-22 unique objects + 1 2-obj preposition (Left/Right only)\nTemporal\n1 unique object + temporal relation of a 1-obj verb,2 unique objects + temporal relation of a 1-obj verb,2 unique object + temporal relation of a 2-obj verb\nVerb-1 1 unique object + 1 1-obj verb\nVerb-2 2 unique objects + 1 2-obj verb\nAdjective1 unique object + 1 adjective\na cat on the righta cat on the left\na person to the right of a horse\na person to the left of a horse\na dog beforecatchingafrisbeea dog after catching a frisbee\na bird sittinga bird flying\na person feeding an elephanta person ridinganelephant\na blue fire hydranta white fire hydrant\nFigure 3: Each attribute in ControlledImCaps, with\ncomparable prompts in CompPrompts and an example.\nencoder performance of a VL model on a particular\nprompt type in CompPrompts with the performance\nof the overall VL model on that prompt type in\nControlledImCaps. As discussed in Section 2.2,\nthe two captions in every example differ by only\none word which changes or inverts the relation, al-\nlowing us to perform fine-grained analyses in con-\ntrolled settings without conflating multiple types of\ncompositional reasoning. Figure 3 depicts the six\ntypes of attributes studied in ControlledImCaps,\ntheir corresponding prompt type in CompPrompts,\nand an example of each.\nVL Models. We evaluate the same VL models\nas in Section 3: CLIP ViT-B/32, CLIP ViT-L/14,\nCLIP with a RoBERTa-pretrained text encoder (Liu\net al., 2019; Ilharco et al., 2021), and negCLIP\n(Yuksekgonul et al., 2023). Each of these mod-\nels can return a score when given an image and a\ncaption, representing how well they match.\nEvaluation. We follow the evaluation scheme\nfrom Winoground (Thrush et al., 2022): for a given\npair of images with corresponding captions, we\nmeasure both a text score, the fraction of instances\nwhere a model scores the correct caption higher\nPrompt Type EM on\nCompPrompts\nCIC\nImage\nscore\nCIC\nText\nscore\nSpatial 1-obj L/R 28.5 4.0 15.0\nSpatial 2-obj L/R 4.4 4.0 8.0\nTemporal 26.8 7.0 30.0\nVerb 1-obj 71.6 84.0 89.0\nVerb 2-obj 37.2 46.0 63.0\nAdjectives 81.8 65.0 85.0\nTable 4: Performance of CLIP ViT-L/14 text encoder\n(%) on the equivalent prompts inCompPrompts, and per-\nformance of CLIP ViT-L/14 full model on Controlled-\nImCaps (CIC). On the types of prompts where the text\nencoder performs poorly, so too does the overall model.\nthan the incorrect caption when given an image,\nand an image score, the fraction of instances where\na model scores the correct image higher than the\nincorrect image when given a caption.\n4.1 Multi-modal Results\nTable 4 presents the results of CLIP ViT-L/14\non both CompPrompts and ControlledImCaps (all\nmodel results in Appendix). The CompPrompts re-\nsults correspond to the prompt type(s) most closely\nmatching the captions in ControlledImCaps (spec-\nified in Figure 3). For the spatial relations, for this\ntable alone, we calculate the EM on the data points\nin CompPrompts containing “left” and “right” spa-\ntial relations only due to lack of sufficient support\nin COCO for other spatial relations, as discussed\nin Section 2.2. On prompt types where the text\nencoder performance on CompPrompts is poor, the\noverall model performance on ControlledImCaps\nis also poor: showing that the text encoder does\nindeed bottleneck VL models’ compositionality.\nWe see similar findings per prompt type and\nmodel design as those discussed in Section 3.3.\n4.2 Fine-Grained Results on Different Prompt\nTypes\nWe discuss findings on the prompt types in Con-\ntrolledImCaps, with 95% confidence intervals.\nModels do poorly on spatial relations. On aver-\nage, VL models perform poorly on spatial relations,\nachieving an average image|text score of 2.5|12.4\n(±2.2 |3.7). Their text encoder performance on\nthe corresponding prompts in CompPrompts was\nsimilarly poor, with an average EM of 27.8. This\nagrees with Kamath et al. (2023), which shows that\nVL models struggle with spatial relations.\n4938\nModels do poorly on temporal relations. VL\nperforms poorly on temporal relations, with an av-\nerage image |text score of 5.3 |30.8 (±2.7 |4.8).\nTheir text encoder performance on CompPrompts\ntemporal reasoning was similarly low at 18.9 EM.\nModels do well on verbs and adjectives. VL\nmodels perform well on verbs (average image|text\nscore 65.4 |78.1, ±5.0 |4.8) and even better on\nadjectives (average image|text score 78.5|89.0, ±\n7.0 |3.5), mirroring their text encoder performance\non CompPrompts, where the average EM for verbs\nand adjectives were 33.7 and 44.8 respectively.\nTwo-object verbs are more difficult than one-\nobject verbs. We find that for all models, two-\nobject verbs are harder than one-object verbs, with\nthe former achieving an image |text score of 52.3\n|68.5 and the latter 78.5 |87.8 (with p < 0.05\nunder the Wilcoxon signed-rank test). This follows\nperformance on CompPrompts for ViT-B/32 and\nViT-L/14, but not for negCLIP and RoBERTa-CLIP,\nhinting that ability to reconstruct is necessary but\nnot sufficient, as discussed in Section 4.5.\n4.3 Fine-grained results on different model\ndesign choices\nWe discuss findings on the model designs in Con-\ntrolledImCaps. All findings are statistically signif-\nicant at p <0.05 using the Wilcoxon signed-rank\ntest to compare models.\nPre-training the text encoder improves text\nscore on verbs. RoBERTa-CLIP obtains a higher\ntext score than CLIP ViT-B/32 (78.0 vs 68.0), as\nwell as a higher EM on the prompts in Comp-\nPrompts corresponding to verbs (39.4 vs 11.0).\nIncreasing model size does not help on spa-\ntial or temporal reasoning. On both spatial and\ntemporal reasoning inputs, ViT-L/14 performance\non ControlledImCaps was not statistically signifi-\ncantly higher than that of ViT-B/32.\nHard negatives from Yuksekgonul et al. (2023)\nhelp where word order matters. On prompts\nwhere word order matters, negCLIP scores an im-\nage |text score of 36.5 |50.0 and a CompPrompts\nEM of 27.2, and other models score an average im-\nage |text score of 24.8 |37.5 and a CompPrompts\nEM of 21.0. negCLIP also outperforms ViT-B/32\non all prompts on average.\n4.4 Text reconstruction appears to be\nnecessary...\nTo study the relationship between text reconstruc-\ntion and overall model performance beyond Table 4,\nwe evaluate text reconstruction on ControlledIm-\nCaps. Specifically, we use the trained T5 decoders\nfrom Section 3 and try to reconstruct the input\nwhen ControlledImCaps text inputs are evaluated.\nOn the cases where the reconstruction is incorrect\naccording to human evaluation 5 on either of the\ntwo text inputs, the overall model Image Score on\nControlledImCaps for CLIP ViT-L/14 is zero 96%\nof the time, and the Text Score is zero 83% of\nthe time. This text reconstruction vs. multimodal\nmatching correlation is more direct compared to\nthe similar correlation reported in Table 4 because\nwe compare on the same instances.\n4.5 ... but insufficient.\nConversely, just because a model performs well\non the CompPrompts probe does not mean it will\nperform well on ControlledImCaps. For exam-\nple, ViT-L/14 outperformed ViT-B/32 overall on\nCompPrompts, but not (statistically significantly)\non ControlledImCaps. Also, RoBERTa-CLIP out-\nperforms ViT-B/32 on temporal relations onComp-\nPrompts, but achieves a worse text score on Con-\ntrolledImCaps. When we evaluate text reconstruc-\ntion on ControlledImCaps, on cases where the re-\nconstruction is correct for both text inputs, the over-\nall model Image Score on ControlledImCaps for\nViT-L/14 is zero 59% of the time, and the Text\nScore is zero 47% of the time. This suggests that\ntext recoverability is a necessary but insufficient\ncondition for overall model performance. The in-\nsufficiency is intuitive, as multimodal errors could\npotentially stem from the image encoder.\n4.6 A Note on Winoground\nWe evaluate our four VL models on the\nWinoground dataset (Thrush et al., 2022). They\nperform poorly, with an average image |text score\nof 10.3 |30.8, where random chance is 25.0 |25.0.\nHowever, on shorter inputs (5 words or less) which\nexhibit fewer compositional concepts on average,\ne.g., “a bird eats a snake” |“a snake eats a bird”,\nthe four models achieve higher scores of 20.4 |\n47.2 on average. On longer (over 10 words), more\n5For the simple inputs of ControlledImCaps, we found\nhuman evaluation by the authors tractable, with the added\nadvantage of not penalizing minor errors as EM does.\n4939\ncompositional inputs, e.g. “in the stadium, the per-\nson wearing gray outperformed the one wearing\nblue” |“in the stadium, the person wearing blue out-\nperformed the one wearing gray”, models achieve\na much lower score of 3.4 |18.5. This mirrors\nour finding on CompPrompts that VL text encoders\nstruggle with increasingly compositional inputs.\n5 Related work\nBuilding models capable of reasoning jointly about\nvisual and textual inputs is a long-standing goal of\nAI (Winograd, 1971), with potential applications in\nthe fields of vision-language navigation (Anderson\net al., 2018), human-robot interaction (Matuszek\net al., 2012), accessible image captioning (Gurari\net al., 2020), etc.\nRecent challenge datasets have been designed\nto probe the capacity of multimodal models to rep-\nresent descriptions of precise visual compositions\n(Johnson et al., 2017; Suhr et al., 2019; Hudson\nand Manning, 2019; Thrush et al., 2022). Yuksek-\ngonul et al. (2023) and Yamada et al. (2022) study\nCLIP specifically, demonstrating its shortcomings\n(and some potential fixes) in terms of modeling\nsyntax. Ma et al. (2022) study OpenCLIP models\nfor various types of compositional reasoning, with\nprogrammatically sourced hard negatives. Differ-\nent from these works, our textual probe does not\nrequire access to images.\nOur image-and-text evaluation most closely re-\nsembles Thrush et al. (2022). However, we stratify\nthe examples based on type of input (e.g., tem-\nporal relations) to provide more detailed insights.\nWe also keep our prompts relatively simple, never\nhaving more than two objects or two attributes in\nthe input. We believe this is a more realistic goal\nfor our current vision-language models. The word\norder shuffling aspect is also discussed in Yuksek-\ngonul et al. (2023). However, as their proposed\nbenchmark does not provide pairs of images with\ncorresponding captions, it is possible to achieve\nstate-of-the-art with a text-only model (specifically,\n2-shot ChatGPT6 (Ouyang et al., 2022), details in\nAppendix and the recent Hsieh et al. (2023)). While\nthis does not detract from their finding that vision-\nlanguage models ignore word order, our bench-\nmarks have an additional advantage of being insen-\nsitive to text-only priors.\n6https://platform.openai.com/docs/\napi-reference/chat, using the gpt-3.5-turbo model\n6 Conclusion and Discussion\nWe present probing results that suggest significant\ninformation loss upon text encoding of composi-\ntional inputs in vision and language models. This\ninformation loss is quantified using CompPrompts,\na test set of increasingly compositional image de-\nscriptions, and ControlledImCaps, a test set that\nwe use to verify that this information loss affects\nthe performance of multimodal models on compo-\nsitional inputs. Harder negatives, more text pre-\ntraining, and larger models all improve encoder\nquality, but information is still lost even for the\nmost performant models, compared to the uni-\nmodal SBERT as well as a T5-based auto-encoder.\nGoing forward, even more difficult test sets than\nCompPrompts and ControlledImCaps might be re-\nquired to analyze and evaluate vision-language\nmodel capabilities. Returning to Riedl (2022)’s\ntweet from the intro, “A penguin on Mars wear-\ning a spacesuit and walking a robot dog next to\nSanta Claus. ”, even our highly accurate proof-of-\nconcepT5 model struggles, predicting: “compul-\nsory penguin onexposition wearing a spacesuit and\nwalking a dog robot next tohoc”. To support imagi-\nnative text-to-image generation queries (for images\nthat may not exist yet), future work would be well-\nsuited to design text encoders that can generalize\nto captions that contain compositions never-before-\nseen in web alt-text corpora.\nOur probing results suggest two future modeling\ndirections: (1) Modifying contrastive VL models’\ntraining objectives to additionally encourage abil-\nity to reconstruct the text input, either through an\nadditional reconstruction loss on the text encoder\nduring finetuning, or through the addition of even\nharder negatives than Yuksekgonul et al. (2023)\nand Ma et al. (2022), would be an exciting avenue\nfor future work. Alternatives to contrastive train-\ning, such as captioning, have also shown promise\nin recent work (Tschannen et al., 2023); and (2)\nexplicitly encouraging linear recovery with a modi-\nfied loss function: while the gap between SBERT\nand the VL Text encoders can be partially explained\nby the superior pooling method and training data,\nSBERT’s training objective does not require linear\nrecoverability (whereas CLIP’s dot product inter-\naction term might): explicitly encouraging linear\ntext-text recoverability might improve multimodal\nperformance. Finally, we hope that ControlledIm-\nCaps can facilitate research beyond single-vector\nbottleneck VL models.\n4940\nLimitations\nFirst, our probing method involves a pre-trained T5\ndecoder. It is possible that language biases from\nthe pre-training emerge while decoding from the\nVL text embedding, e.g., predicting “a dog chasing\na cat” instead of “a cat chasing a dog” because the\nformer is more likely under the T5 decoder’s priors\nfrom pre-training. However, as the methodology is\nthe same across all models we evaluate, we believe\nthat the evaluation is fair. Second, we evaluate\nwith only one probe, whereas probing with com-\nplementary methods (e.g., especially deterministic\nones, like a convex linear probe) could reveal more\ninsights. Third, text encoders that do well on our\nevaluation may not perform well if directly plugged\ninto a contrastive VL model like CLIP, if the text\nencoders were not trained to encode the informa-\ntion in a manner that is linearly recoverable.\nAcknowledgements\nThe authors thank John Hewitt, Akhila Yerukola,\nand anonymous reviewers for useful discussion and\nfeedback. This work was funded by the Allen In-\nstitute for AI. AK was additionally supported by\nthe UCLA Computer Science Department First-\nYear Fellowship. KC was supported in part by\nU.S. DARPA MCS Program under contract num-\nber N660011924032, U.S. DARPA ECOLE Pro-\ngram No. HR00112390060, and ONR N00014-23-\n1-2780, and a Sloan Fellowship. The views and\nconclusions contained herein are those of the au-\nthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, of DARPA, or the U.S. Government.\nReferences\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko Sünderhauf, Ian Reid, Stephen\nGould, and Anton Van Den Hengel. 2018. Vision-\nand-language navigation: Interpreting visually-\ngrounded navigation instructions in real environ-\nments. In CVPR.\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffu-\nsion models beat gans on image synthesis. NeurIPS.\nTejas Gokhale, Hamid Palangi, Besmira Nushi, Vib-\nhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral,\nand Yezhou Yang. 2022. Benchmarking spatial rela-\ntionships in text-to-image generation. arXiv preprint\narXiv:2212.10015.\nDanna Gurari, Yinan Zhao, Meng Zhang, and Nilavra\nBhattacharya. 2020. Captioning images taken by\npeople who are blind. In ECCV. Springer.\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha\nKembhavi, and Ranjay Krishna. 2023. Sugarcrepe:\nFixing hackable benchmarks for vision-language\ncompositionality. In Thirty-Seventh Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track.\nDrew A Hudson and Christopher D Manning. 2019.\nGQA: A new dataset for real-world visual reasoning\nand compositional question answering. In CVPR,\npages 6700–6709.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman,\nCade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John\nMiller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-\nwig Schmidt. 2021. Openclip.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In International Conference\non Machine Learning.\nJustin Johnson, Bharath Hariharan, Laurens Van\nDer Maaten, Li Fei-Fei, C Lawrence Zitnick, and\nRoss Girshick. 2017. CLEVR: A diagnostic dataset\nfor compositional language and elementary visual\nreasoning. In CVPR.\nAmita Kamath, Christopher Clark, Tanmay Gupta, Eric\nKolve, Derek Hoiem, and Aniruddha Kembhavi.\n2022. Webly supervised concept expansion for gen-\neral purpose vision models. ECCV.\nAmita Kamath, Jack Hessel, and Kai-Wei Chang. 2023.\nWhat’s “up”’ with vision-language models? inves-\ntigating their struggle with spatial reasoning. In\nEMNLP.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In European conference\non computer vision, pages 740–755. Springer.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nZixian Ma, Jerry Hong, Mustafa Omer Gul, Mona\nGandhi, Irena Gao, and Ranjay Krishna. 2022. Crepe:\nCan vision-language foundation models reason com-\npositionally? In CVPR.\nCynthia Matuszek, Nicholas FitzGerald, Luke Zettle-\nmoyer, Liefeng Bo, and Dieter Fox. 2012. A joint\nmodel of language and perception for grounded at-\ntribute learning. In ICML.\n4941\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke E.\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul Francis Christiano, Jan Leike, and Ryan J.\nLowe. 2022. Training language models to follow\ninstructions with human feedback. NeurIPS.\nLetitia Parcalabescu, Albert Gatt, Anette Frank, and\nIacer Calixto. 2021. Seeing past words: Testing\nthe cross-modal capabilities of pretrained V&L mod-\nels on counting tasks. In Proceedings of the 1st\nWorkshop on Multimodal Semantic Representations\n(MMSR).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with CLIP latents.\narXiv preprint arXiv:2204.06125.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nMark Riedl. 2022. A penguin on mars wearing a space-\nsuit and walking a robot dog next to santa claus.\nTweet ID 1511745781870514176.\nChitwan Saharia, William Chan, Saurabh Saxena,\nLala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Raphael Gontijo-Lopes,\nBurcu Karagol Ayan, Tim Salimans, Jonathan Ho,\nDavid J. Fleet, and Mohammad Norouzi. 2022. Pho-\ntorealistic text-to-image diffusion models with deep\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade W Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton\nMullis, Mitchell Wortsman, Patrick Schramowski,\nSrivatsa R Kundurthy, Katherine Crowson, Lud-\nwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.\n2022. LAION-5b: An open large-scale dataset for\ntraining next generation image-text models. InThirty-\nsixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track.\nSanti Seguí, Oriol Pujol, and Jordi Vitria. 2015. Learn-\ning to count with deep object features. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops, pages 90–96.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In ACL.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596–4604. PMLR.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. 2022. Winoground: Probing vision and lan-\nguage models for visio-linguistic compositionality.\nIn CVPR.\nMichael Tschannen, Manoj Kumar, Andreas Steiner,\nXiaohua Zhai, Neil Houlsby, and Lucas Beyer. 2023.\nImage captioners are scalable vision learners too.\nTerry Winograd. 1971. Procedures as a representation\nfor data in a computer program for understanding\nnatural language. Technical report.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nYutaro Yamada, Yingtian Tang, and Ilker Yildirim. 2022.\nWhen are lemons purple? the concept association\nbias of clip. arXiv preprint arXiv:2212.12043.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou. 2023. When and why\nvision-language models behave like bag-of-words\nmodels, and what to do about it? ICLR.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\n4942\nA Additional Results\nTable 5 contains average BLEU-4 scores of the\nmodels. Table 6 contains a study of model per-\nformance on object-attribute association in Comp-\nPrompts. Table 7, Table 8 and Table 9 con-\ntain results of other models on ControlledImCaps\nin comparison to CompPrompts (ViT-L/14 is dis-\ncussed in Table 4). Table 10 discusses text-only\nresults on the ARO benchmark (Yuksekgonul et al.,\n2023).\nT(x) Embed.\nsize Avg. BLEU-4\nCLIP ViT-B/32 512 33.3\nCLIP ViT-L/14 768 46.0\nnegCLIP ViT-B/32 512 50.2\nRoBERTaCLIP ViT-B/32 512 52.0\nSBERT 768 56.7\nTable 5: Average BLEU-4 performance of each text\nencoder on CompPrompts, not including multiples and\nnegations. The trend correlates with EM %, but the eval-\nuation itself is too lenient for our purposes, as described\nin the main text.\nT(x) Embed.\nsize Shuffled%(↓)\nCLIP ViT-B/32 512 51.8\nCLIP ViT-L/14 768 55.5\nnegCLIP ViT-B/32 512 37.2\nRoBERTaCLIP ViT-B/32 512 62.8\nSBERT 768 44.2\nTable 6: Of the times the model gets the words in the\nprediction correct, Shuffled % is the percentage of when\nit gets the word order incorrect (in the prompts where\nword order matters, unlike “cat and dog” — specifically,\nwhere attributes must be associated with the correct\nobject). Clearly, negCLIP having been trained with hard\nnegatives involving word order shuffling allows it to\nperform the best. All models suffer from poor object\nattribute association.\nPrompt Type EM on\nCompPrompts\nCIC\nImage\nscore\nCIC\nText\nscore\nSpatial 1-obj L/R 27.2 1.0 10.0\nSpatial 2-obj L/R 0.6 4.0 10.0\nTemporal 7.3 4.0 35.0\nVerb 1-obj 15.4 71.0 77.0\nVerb 2-obj 6.6 43.0 59.0\nAdjectives 38.2 74.0 92.0\nTable 7: Performance of CLIP ViT-B/32 text encoder\n(%) on the equivalent prompts inCompPrompts, and per-\nformance of CLIP ViT-B/32 full model onControlled-\nImCaps (CIC).\nPrompt Type EM on\nCompPrompts\nCIC\nImage\nscore\nCIC\nText\nscore\nSpatial 1-obj L/R 26.0 1.0 13.0\nSpatial 2-obj L/R 15.0 3.0 13.0\nTemporal 23.0 8.0 35.0\nVerb 1-obj 20.6 84.0 94.0\nVerb 2-obj 39.4 70.0 87.0\nAdjectives 42.2 95.0 92.0\nTable 8: Performance of negCLIP ViT-B/32 text en-\ncoder (%) on the equivalent prompts in CompPrompts,\nand performance of negCLIP ViT-B/32 full model on\nControlledImCaps (CIC).\nPrompt Type EM on\nCompPrompts\nCIC\nImage\nscore\nCIC\nText\nscore\nSpatial 1-obj L/R 92.3 1.0 10.0\nSpatial 2-obj L/R 28.3 2.0 20.0\nTemporal 18.5 2.0 23.0\nVerb 1-obj 30 75.0 91.0\nVerb 2-obj 48.8 50.0 65.0\nAdjectives 17 80.0 87.0\nTable 9: Performance of RoBERTa-CLIP ViT-B/32\ntext encoder (%) on the equivalent prompts in Comp-\nPrompts, and performance of RoBERTa-CLIP ViT-B/32\nfull model on ControlledImCaps (CIC).\n4943\nDataset negCLIP ChatGPT 2-shot\nVG-Relation 0.81 0.90\nVG-Attribution 0.71 0.80\nFlickr30k-PRC 0.91 0.86\nCOCO-PRC ViT-B/32 0.86 0.86\nTable 10: Performance of ChatGPT 2-shot on the ARO\nbenchmark (Yuksekgonul et al., 2023). While the\ndataset was designed to test VL models’ sensitivity to\nword order shuffling (which is orthogonal to text-only\nperformance on the same data), the textual priors that\nexist in ARO (e.g., “horse eating grass” is more likely\nthan “grass eating horse”) are less relevant to the prob-\ning experiments for CompPrompts (because the probe\nmust reconstruct any given caption in CompPrompts, in-\ncluding unusual ones, e.g., “five teenagers riding three\nbutterflies\") and do not exist in ControlledImCaps due\nto the paired-image construction.\n4944",
  "topic": "Principle of compositionality",
  "concepts": [
    {
      "name": "Principle of compositionality",
      "score": 0.8347882628440857
    },
    {
      "name": "Computer science",
      "score": 0.8199448585510254
    },
    {
      "name": "Natural language processing",
      "score": 0.6508474349975586
    },
    {
      "name": "Encoder",
      "score": 0.6418688297271729
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5741287469863892
    },
    {
      "name": "Object (grammar)",
      "score": 0.5478009581565857
    },
    {
      "name": "Pooling",
      "score": 0.5466232895851135
    },
    {
      "name": "Bottleneck",
      "score": 0.5436798930168152
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5278826951980591
    },
    {
      "name": "Language model",
      "score": 0.46753475069999695
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4615386426448822
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4446846544742584
    },
    {
      "name": "Programming language",
      "score": 0.1733091175556183
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ]
}