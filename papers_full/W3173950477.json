{
  "title": "Transformer-Style Relational Reasoning with Dynamic Memory Updating for Temporal Network Modeling",
  "url": "https://openalex.org/W3173950477",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5068433690",
      "name": "Dongkuan Xu",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5101484374",
      "name": "Junjie Liang",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5037724644",
      "name": "Wei Cheng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100777770",
      "name": "Hua Wei",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5100456786",
      "name": "Haifeng Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5060725887",
      "name": "X. D. Zhang",
      "affiliations": [
        "Pennsylvania State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970246438",
    "https://openalex.org/W2415243320",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2808087697",
    "https://openalex.org/W1801278145",
    "https://openalex.org/W2901645117",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W6729448088",
    "https://openalex.org/W2804713552",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2623187518",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W6948116018",
    "https://openalex.org/W7009330471",
    "https://openalex.org/W2472819217",
    "https://openalex.org/W6752675014",
    "https://openalex.org/W6712081154",
    "https://openalex.org/W6736262870",
    "https://openalex.org/W6807384801",
    "https://openalex.org/W3004366655",
    "https://openalex.org/W2964811671",
    "https://openalex.org/W2795735740",
    "https://openalex.org/W2808908091",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2954691982",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W2963870701",
    "https://openalex.org/W2963935808",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2898104583",
    "https://openalex.org/W2154851992",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W3127433878",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2998116985",
    "https://openalex.org/W2952165242",
    "https://openalex.org/W2806983170",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2393319904",
    "https://openalex.org/W2604942799",
    "https://openalex.org/W2905224888",
    "https://openalex.org/W3161012769",
    "https://openalex.org/W3045255111",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2904900486",
    "https://openalex.org/W4394656990",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W2991612931",
    "https://openalex.org/W4297571622",
    "https://openalex.org/W2963460174",
    "https://openalex.org/W2964015378"
  ],
  "abstract": "Network modeling aims to learn the latent representations of nodes such that the representations preserve both network structures and node attribute information. This problem is fundamental due to its prevalence in numerous domains. However, existing approaches either target the static networks or struggle to capture the complicated temporal dependency, while most real-world networks evolve over time and the success of network modeling hinges on the understanding of how entities are temporally connected. In this paper, we present TRRN, a transformer-style relational reasoning network with dynamic memory updating, to deal with the above challenges. TRRN employs multi-head self-attention to reason over a set of memories, which provides a multitude of shortcut paths for information to flow from past observations to the current latent representations. By utilizing the policy networks augmented with differentiable binary routers, TRRN estimates the possibility of each memory being activated and dynamically updates the memories at the time steps when they are most relevant. We evaluate TRRN with the tasks of node classification and link prediction on four real temporal network datasets. Experimental results demonstrate the consistent performance gains for TRRN over the leading competitors.",
  "full_text": "Transformer-Style Relational Reasoning with Dynamic Memory Updating for\nTemporal Network Modeling\nDongkuan Xu1, Junjie Liang1, Wei Cheng2, Hua Wei1, Haifeng Chen2, Xiang Zhang1\n1The Pennsylvania State University\n2NEC Laboratories America, Inc.\nfdux19, jul672, hzw77, xzz89g@psu.edu\nfweicheng, haifengg@nec-labs.com\nAbstract\nNetwork modeling aims to learn the latent representations\nof nodes such that the representations preserve both network\nstructures and node attribute information. This problem is fun-\ndamental due to its prevalence in numerous domains. However,\nexisting approaches either target the static networks or strug-\ngle to capture the complicated temporal dependency, while\nmost real-world networks evolve over time and the success\nof network modeling hinges on the understanding of how\nentities are temporally connected. In this paper, we present\nTRRN, a transformer-style relational reasoning network with\ndynamic memory updating, to deal with the above challenges.\nTRRN employs multi-head self-attention to reason over a set\nof memories, which provides a multitude of shortcut paths\nfor information to ﬂow from past observations to the current\nlatent representations. By utilizing the policy networks aug-\nmented with differentiable binary routers, TRRN estimates the\npossibility of each memory being activated and dynamically\nupdates the memories at the time steps when they are most\nrelevant. We evaluate TRRN with the tasks of node classiﬁca-\ntion and link prediction on four real temporal network datasets.\nExperimental results demonstrate the consistent performance\ngains for TRRN over the leading competitors.\nIntroduction\nNetwork modeling is a fundamental problem due to its preva-\nlence in numerous domains, such as knowledge base, social\nmedia and bioinformatics (Wu et al. 2020). Network model-\ning aims to learn the latent representations of nodes, which\npreserve both the structure properties and the node attribute\ninformation. Such representations beneﬁt various applica-\ntions, such as node classiﬁcation, link prediction and commu-\nnity detection (Zhang, Cui, and Zhu 2018; Zhou et al. 2018),\netc. Over the years, numerous efforts are made to improve the\nperformance of network modeling, such as DeepWalk (Per-\nozzi, Al-Rfou, and Skiena 2014), GCN (Kipf and Welling\n2017), GAT (Veliˇckovi´c et al. 2018), GraphSAGE (Veliˇckovi´c\net al. 2018), which model the static networks.\nHowever, in many real-world applications, networks are\noften dynamic and may evolve over time. To model tempo-\nral network, a single (static) observation is not sufﬁcient to\nlearn the latent representations, such as using GNNs (Figure\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ne\nm\nf\ne\nm\ne\nm1\nP\ne\nm4\nm3\nm2\nm1\nx\ne\nf1\ne\nm4\nm3\nm2\nm1\nm4\nm3\nm2\nm1\nx1 x3\nx2\nP\nf2\nf3\nf1 f2\nf3\nf1 f2\nf3\nm2\nm3\nm1\nm2\nm3\n(a) Graph NNs\ne\nm\nf\ne\nm\ne\nm1\nP\ne\nm4\nm3\nm2\nm1\nx\ne\nf1\ne\nm4\nm3\nm2\nm1\nm4\nm3\nm2\nm1\nx1 x3\nx2\nP\nf2\nf3\nf1 f2\nf3\nf1 f2\nf3\nm2\nm3\nm1\nm2\nm3 (b) Gated RNNs\ne\nm\nf\ne\nm\ne\nm1\nP\ne\nm4\nm3\nm2\nm1\nx\ne\nf1\ne\nm4\nm3\nm2\nm1\nm4\nm3\nm2\nm1\nx1 x3\nx2\nP\nf2\nf3\nf1 f2\nf3\nf1 f2\nf3\nm2\nm3\nm1\nm2\nm3\n(c) Spatio-temporal NNs\ne\nm\nf\ne\nm\ne\nm1\nP\ne\nm4\nm3\nm2\nm1\nx\ne\nf1\ne\nm4\nm3\nm2\nm1\nm4\nm3\nm2\nm1\nx1 x3\nx2\nP\nf2\nf3\nf1 f2\nf3\nf1 f2\nf3\nm2\nm3\nm1\nm2\nm3 (d) The proposed TRRN\nFigure 1: Four architectures for temporal network modeling.\nffigrepresent different factors that inﬂuence node behaviors,\nsuch as network topology, attribute information of nodes\nand edges, etc. e is the latent representation of node to be\nlearned. fmkgare memories. Circles are aggregation layers.\nTrapeziums are gate layers. The hexagon with letter p inside\nis the policy network.\n1(a)). It makes more sense to reason over the history of past\nobservations such that sufﬁcient information can be extracted\nfor current time step decision-making. For this problem, an\nintuitive choice is to apply the gated RNNs (Hochreiter and\nSchmidhuber 1997) (Figure 1(b)) which maintain the infor-\nmation from past observations via the recurrent state vectors.\nHowever, it would ignore the network structure properties.\nSome recent progresses have been made on jointly modeling\nthe spatial and temporal contextual information (He et al.\n2019; Xu et al. 2019a,b) (Figure 1(c)). Despite their success,\ntheir capacity for learning complicated temporal dependency\nis limited. To successfully model temporal network, it is de-\nsirable to understand how entities are connected temporally.\nFor this problem, we argue that the relational reasoning abil-\nity plays an essential role (Battaglia et al. 2018), which helps\ncomprise a capacity to compare and contrast information\nobserved at different time steps.\nIn this work, we focus on the problem of temporal net-\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n4546\nwork modeling. Given a sequence of network snapshots for\nthe same set of nodes, where each node is coupled with at-\ntributes and has a unique class label over time. Both network\ntopology and node attributes evolve over time. The task is to\nlearn the latent representation for each node that considers\ntemporal patterns of both topological structures and node at-\ntributes. We evaluate the learned representations on different\ndownstream tasks, such as node classiﬁcation and link pre-\ndiction. The problem is challenging mainly for three reasons.\nFirst, capturing the complicated temporal dependency is hard.\nFor temporal networks, the evolution distributes in different\ntime steps. How to effectively compartmentalize and relate\nthe information of different time steps is extremely interest-\ning but at the same time quite challenging. Second, learning\ncontextualized representations of nodes is also challenging.\nNode behaviors are often determined by various factors as\ndenoted by ffigin Figure 1, such as node attributes and net-\nwork topology structures, etc. Different factors at different\ntime periods inﬂuence node representations diversely. How\nto embed them into node representations and differentiate\ntheir inﬂuence on node representations remains a challenging\nproblem. Third, we highlight the sparse dynamics in the evo-\nlution of temporal networks. Dynamic systems in real-world\nare often characterized by independent and sparsely interact-\ning dynamic processes (Goyal et al. 2019). It is desirable and\nchallenging to capture the most relevant ones at time steps\nand update the dynamic processes sparsely (Bengio 2017).\nIn an effort to better model temporal networks, we propose\nTRRN, a transformer-style relational reasoning network with\ndynamic memory updating. Speciﬁcally, similar to memory-\naugmented architectures (Santoro et al. 2016, 2018), TRRN,\nas illustrated in Figure 1(d), adopts a set of memories to\nenhance temporal capacity. It applies Transformer-style self-\nattention to allow for interactions between memories over\ntime, which helps compartmentalize and relate information\nof different time steps explicitly. To learn the contextualized\nrepresentations of nodes, TRRN feeds different factors along\nwith the updated memories into a gated recurrent network,\nfrom which the generated representations is able to consider\nthe variety of contexts (factors) that the node involves. The\ninﬂuence of different factors are reﬂected in the attention\nmatrix. Moreover, to model the sparse dynamics of networks,\nTRRN takes inspiration from conditional computation (Ben-\ngio, L´eonard, and Courville 2013) and employs the policy\nnetworks to selectively activate and update the most relevant\nmemories at each time step, on a per node basis. However,\nas the decisions determining which memories need to be\nactivated are non-differentiable, we introduce differentiable\nbinary routers based upon the Gumbel-softmax reparameteri-\nzation (Jang, Gu, and Poole 2017) to train policy networks.\nWe validate the proposed model on node classiﬁcation and\nlink prediction tasks, using four real-world temporal network\ndatasets. Experimental results show that TRRN outperforms\nthe leading competitors by a large margin. We demonstrate\nthe beneﬁts of dynamically updating memories through an\nablation study and visualize the activation decision vectors\ntogether with the attention weights to provide interpretability\nof results. We summarize our contributions as follows.\n• We analyze major challenges for temporal network model-\nMethod Relational Relational Model Different Sparse\nBias Reasoning Interp. Factors Dynamics\nDeepWalk S. \u0002 \u0002 \u0002 \u0002\nGAT S. \u0002 \u0002 \u0003 \u0002\nGCN S. \u0002 \u0002 \u0003 \u0002\nGraphSAGE S. \u0002 \u0002 \u0003 \u0002\nnode2vec S. \u0002 \u0002 \u0002 \u0002\nLSTM T. \u0002 \u0002 \u0002 \u0002\nGRU T. \u0002 \u0002 \u0002 \u0002\nDynGEM S.+T. \u0002 \u0002 \u0002 \u0002\nDynAERNN. S.+T. \u0002 \u0002 \u0002 \u0002\nDANE S.+T. \u0002 \u0002 \u0003 \u0002\nDySAT S.+T. \u0003 \u0003 \u0003 \u0002\nSTAR S.+T. \u0003 \u0003 \u0003 \u0002\nOur Work S.+T. X X X X\nTable 1: A comparison of published approaches for tempo-\nral network modeling. S. and T. represent spatial relational\nbias and temporal relational bias respectively. Model Interp\ndenotes model interpretability. X, \u0003, \u0002represent true, not\nexactly true and false, respectively.\ning, including the complicated temporal dependency, the\ncontextualized representations of nodes and the sparse dy-\nnamics of network evolution, which encourages us to ﬁnd\nthe complementarity between memory-augmented archi-\ntectures and conditional computation.\n• We extend the strength of multi-head self-attention via\npolicy networks, proposing TRRN, in which complicated\ntemporal dependency is learned, nodes in temporal net-\nworks can be contextualized ﬂexibly and sparse dynamics\nof network evolution is captured.\n• We summarize the existing network modeling approaches\nin a uniﬁed manner and conduct extensive experiments\non four real datasets. The results show that TRRN outper-\nforms the leading competitors in accuracy, and provides\nbetter interpretability as well.\nProblem Deﬁnition\nA temporal network, denoted by G = (G1, G2, \u0001\u0001\u0001 , GT ), is a\ncollection of snapshots of a network at different time steps.\nGt = (V, At, Xt) is the snapshot at time step t. Vis the\nset of nodes that is ﬁxed for all time steps. Each node has\na consistent label across\nT time steps. At 2RN\u0002N is the\nadjacency matrix. Xt 2RN\u0002d is the node attribute matrix.\nUnlike some previous methods that do not consider node\nattributes and assume links can only be added over time, we\nallow for node attributes and support the removal of links\nover time. The goal of temporal network modeling is to learn\nthe latent representation et\nv for every node v 2V at time\nstep t2f1,2,\u0001\u0001\u0001 ,Tg, such that et\nv\npreserves both network\nstructures and node attributes related to node v.\nRelated Work\nA comparison of the leading approaches that can be used to\nmodel temporal network is illustrated in Table 1.\nStatic Network Modeling Network modeling has drawn\nextensive research attention in recent years (Perozzi, Al-Rfou,\n4547\nand Skiena 2014; Grover and Leskovec 2016; Wang, Cui, and\nZhu 2016; Kipf and Welling 2017; Wang et al. 2017; Hamil-\nton, Ying, and Leskovec 2017; Veliˇckovi´c et al. 2018). For\nexample, a biased random walk is utilized to learn richer\nnode representations by exploring diverse local information\n(Grover and Leskovec 2016). An attention is applied to gener-\nate node representations by learning the relationship between\na node and its neighbors (Veliˇckovi´c et al. 2018).\nTemporal Network Modeling However, many real-world\nnetworks are temporal. Some recent progresses have been\nmade on modeling temporal networks (Zhu et al. 2018; Du\net al. 2018; Goyal, Chhetri, and Canedo 2018; Ma et al. 2018;\nZuo et al. 2018; Xu et al. 2019b; Sankar et al. 2020). An\nofﬂine way to learn node representations in terms of network\ntopology and node attributes is introduced (Li et al. 2017).\nA method based on autoencoder is proposed to generate the\nembedding of snapshot at time tfrom the embedding at time\nt-1 (Goyal et al. 2018). DySAT (Sankar et al. 2020) models\nnetwork by appling self-attention along the two dimensions of\nstructural neighborhood and temporal dynamics. STAR (Xu\net al. 2019b) is a spatio-temporal attentive RNN, proposed to\nlearn node representations in temporal networks. However,\nthe capacity of these approaches for learning complicated\ntemporal dependency is limited.\nMemory-Based Architectures Many neural network ap-\nproaches that successfully model sequential data utilize mem-\nory systems. However, the standard memory architectures\nlike LSTM might struggle when tasks are involved in under-\nstanding how entities are related. Some memory-augmented\narchitectures (Sukhbaatar et al. 2015; Santoro et al. 2016;\nGraves et al. 2016; Santoro et al. 2018; Goyal et al. 2019)\nare further proposed to address this issue. For example,\nRMC (Santoro et al. 2018) and RIMs (Goyal et al. 2019) ap-\nply self-attention on a set of memories similar to our TRRN.\nHowever, RMC updates all memories at each time and RIMs\nupdates kmemories. In contrast, TRRN selectively activates\nmemories and update them dynamically. In addition, TRRN\nis proposed for network modeling, while these methods are\nproposed for reinforcement learning tasks.\nThe TRRN Model\nThe architecture of TRRN is shown in Figure 1(d). We ﬁrst\nintroduce how to determine the memories to be activated,\nfollowed by how to update the selected memories. Then we\nelaborate how to generate the contextualized representations,\nand last we give the learning objective.\nActivating Memory Selectively\nFigure 2 illustrates how to activate memories selectively,\nwhich allows each node to have its own memory activation\npolicy at each time step. TRRN applies Gumbel Softmax\nsampling to jointly train the policy network and the target\nlearning task. The output of policy network is sampled to pro-\nduce activation decisions of which memories to be activated.\nExtraction of Network Factors To generate contextual-\nized representations, we mainly consider two factors, node\nInactivate\nActivate\nInactivate\ni.i.d. Gumbel \nsamples\nSoftmax\nArgmax\nFactor matrix Memory matrix\nDecision vector\nPolicy network\nForward\nBackward\nFigure 2: Illustration of activating memories selectively.\nattributes and network topology, which are believed to in-\nﬂuence node representations most. Other factors like edge\nweights can be easily utilized by our method.\nTo extract node attribute factor, we feed node attributes\ninto a fully-connected layer. The output vector is used as the\nattribute factor. How to extract network topology factor is\nﬂexible and we use random walk with restart (RWR) (Cao,\nLu, and Xu 2016) in this work. Given a snapshot\nGt = (V,\nAt, Xt) and a node v, the r-step RWR vector is deﬁned as\np(r) = cp(r\u00001)[(D\u00001)At] + (1\u0000c)p(0) 2R1\u0002N\n+ ; (1)\nwhere p(r)\nu represents the probability of node utransitions\nfrom v after r steps. p(0) is the initial vector with p(0)\nv =1\nand all other entries are 0. D is a diagonal matrix with\nDii=PN\nj=1At\nij. 1-cis the probability that the random walker\nwill restart from v. Thus, we use at = PR\nr=1 p(r) to represent\nthe topology information for node vat time step t, where R\nindicates the number of steps. After feeding at into a fully-\nconnected layer, we use the output as the topology factor.\nThe Policy Network The policy network aims to estimate\nthe probability of each memory being activated for each node.\nIts output is a matrix B 2RnM\u00022, of which rows correspond\nto nM memories and columns represent two categories (ac-\ntivate and inactivate). The output B is further fed into the\nrouters to produce the binary activation decisions.\nAt each time step, TRRN receives a memory matrix M\n2RnM\u0002dM that stores the information from previous time\nsteps and a factor matrix F 2RnF \u0002dF that represents an\nobservation of nF factors. TRRN concatenate each memory\nand the transformation of F. The policy network further takes\nthe concatenation as input. The form of policy networks is\nﬂexible and we opt to use a two-layer MLP as the policy\nnetwork. Conceptually, it is deﬁned as\nB = PolicyNetwork (\n2\n664\nm>\n1 \b~f>\nm>\n2 \b~f>\n\u0001\u0001\u0001\nm>\nnM \b~f>\n3\n775) 2RnM\u00022; (2)where \bis the concatenation operator, mk 2RdM is the k-th\nmemory, and ~f 2R~dF is generated by ﬁrst ﬂattening F and\n4548\nthen applying a transformation matrix W 2RnF dF \u0002~dF . The\npolicy network is jointly trained with other parts of TRRN.\nIts simple architecture makes the estimation of the probability\nmatrix fast and efﬁcient, which is similar to the design of\nthe policy network in (Guo et al. 2019) and the design of\nDecision-Learner in (Ahmed and Torresani 2019).\nDifferentiable Binary Routers Given the output B 2\nRnM\u00022, a router is designed to produce the binary activation\ndecisions for each memory. An intuitive way to achieve this\ngoal is to select the position with maximum value ofbk = fb0\nk,\nb1\nkg\n, where k= f1,2,\u0001\u0001\u0001 ,nM g. However, this approach is non-\ndifferentiable. In this work, we adopt the Gumbel-Softmax\nsampling approach (Jang, Gu, and Poole 2017; Maddison,\nMnih, and Teh 2017) that allows us to propagate gradients\nthrough the discrete nodes.\nSpeciﬁcally, we have two categories (activate and inacti-\nvate). The outputs fb0\nk, b1\nkg\nare considered as the log proba-\nbilities flog(p0\nk\n), log(p1\nk\n)gof the two categories for the k-th\nmemory. Based on Gumbel-Max trick (Maddison, Mnih, and\nTeh 2017), we can draw samples from a Bernoulli distribution\nparameterized by class probabilities fp0\nk\n, p1\nkg\nin the follow-\ning way: we ﬁrst draw i.i.d samples fg0\nk, g1\nkgfrom a Gumbel\ndistribution described by:\ngi\nk = \u0000log(\u0000log(x)) \u0018Gumbel;i 2f0; 1g; (3)\nwhere x\u0018Uniform(0, 1). Then produce the discrete sample\nby adding gi\nk to introduce stochasticity:\nzk = arg max\ni\n[bi\nk + gi\nk];i 2f0; 1g: (4)\nThus, we get a binary decision vector z = (z1, z2, znM )>2\nRnM , in which the values indicate the memory is activated\n(zk=1) or frozen (zk=0).\nHowever, the arg maxoperation is non-differentiable. We\ncan use the softmax as a continuous differentiable approxi-\nmation to it, and generate a two-dimensional vector \fk:\n\fi\nk = exp ((bi\nk + gi\nk)=\u001c)\nP1\n^i=0 exp ((b^i\nk + g^i\nk)=\u001c)\n;i 2f0; 1g; (5)\nwhere \u001c is the temperature to control the discreteness. Thus,\nwe use the arg maxto make the binary activation decision on\nthe forward pass, while approximate it with softmax on the\nbackward pass, which is called the straight-through estimator\n(Jang, Gu, and Poole 2017).\nUpdating Memory Dynamically\nTransformer-Style Self-Attention Transformer-style dot\nproduct self-attention (Vaswani et al. 2017) operates on a set\nof typed interchangeable objects and aims to relate different\nobjects in order to compute representations of the same set.\nIt has been shown to be useful in various task (Vaswani et al.\n2017). Speciﬁcally, it applies a scaled dot-product of a query\nmatrix Q to a key matrixK, of which the result is normalized\nvia a softmax to produce a set of weights. The resulting\nrepresentations are the weighted average of a value matrix V\nbased on the produced weights, which are computed as\nAttention(Q;K;V) =softmax\n\u0012QK>\np\nd\n\u0013\nV; (6)\nwhere the softmax is applied to each row of its argument\nmatrix and d is the size of a key vector used as a scaling\nfactor. Notably, the size dcan be split into multiple heads\nand each head operates independently.\nIntuitively, we can utilize transformer-style self-attention\n(generating Q/K/V via applying linear projections to M) to\nallow memories to interact and update their content based\nupon the attended information. However, it is not capable to\nupdate the activated memories without ignoring the informa-\ntion from various factors that inﬂuence node behaviors.\nUpdating Activated Memories We propose to update the\nmemories as follows.\nQ = (M \u0003z) Wq; (7)\nK =\n\u0002\nM; f>\n1 ; f>\n2 ; \u0001\u0001\u0001 ; f>\nnF\n\u0003\nWs; (8)\nV =\n\u0002\nM; f>\n1 ; f>\n2 ; \u0001\u0001\u0001 ; f>\nnF\n\u0003\nWv; (9)\n~M = softmax\n\u0012QK>\npds\n\u0013\nV + M \u0003(1 \u0000z) +M; (10)\nwhere \u0012= (Wq;Ws;Wv;\u0012z). z is the binary decision vec-\ntor.\n\u0002\nM; f>\n1 ; f>\n2 ; f>\nnF\n\u0003\ndenotes the row-wise concatenation. fj\nis the observation of the j-th factor.\nIn particular, to only update the activated memories, we\nuse z 2RnM to mask inactivated memories as shown in\nEquation (7), where M \u0003z is deﬁned by\nM \u0003z = (m1z1;\u0001\u0001\u0001 ;mnM znM )>: (11)\nIn Equations (8)-(9), to consider the inﬂuence from dif-\nferent factors, we incorporate the vector representations of\nfactors into new memory matrix .\nIn Equation (10), softmax\n\u0010\nQK>\npds\n\u0011\nV represents the up-\ndated activated memories, M \u0003(1 \u0000z) represents the kept\ninactivated memories, and M is a residual connection.\nNotably, we do not mask the inactivated memories when\ngenerating the key and value matrices (Equations (8)-(9)).\nThis is because we allow the activated memories to read\nfrom all memories. The intuition is that inactivated memories\nmight still store contextual information related to the acti-\nvated memories, though they are not related to the current\ninput factors.\nLearning Contextualized Representations\nDifferent factors at different time periods inﬂuence node\nrepresentations diversely. To consider the observations of\ndifferent factors at each time step, we introduce the following\nrecurrence to generate the contextualized representations.\nThe recurrence of each memory is independent, but shares the\nsame set of parameters with others. Implementing recurrence\nis ﬂexible and we opt to embed it into an LSTM.\n2\n4\ngt\nk\nit\nk\not\nk\n3\n5=\n\"\u001b\n\u001b\n\u001b\n#\nfW[et\u00001\nk \bft\n1 \bft\n2 \bft\nnF ] +bg; (12)\nmt\nk = gt\nk \fmt\u00001\nk + it\nk \f ( ~mt\nk); (13)\net\nk = ot\nk \ftanh(mt\nk); (14)\net = [et\n1 \bet\n2 \bet\nnM ]; (15)\n4549\nwhere et 2RnMdM is the contextualized representation of a\nnode and et\nk 2RdM is generated from the k-th memory. \f\ndenotes the element-wise multiplication operator.\nEquation (12) describes how to generate the forget, input\nand output gates for the k-th memory, which considers the\nprevious node representation and all the factor observations.\nEquation (13) describes how to generate a new memory,\nwhere the updated memory ~mt\nk\nis used as a candidate.  \nis a transformation function and we use a two-layer MLP\nwith layer normalization. Equation (14) describes how to\ngenerate a part of contextualized representation from thek-th\nmemory. Equation (15) concatenates the representations from\nall memories as the contextualized representation of a node.\nObjective Functions\nTwo Tasks Given the node representations fe1\nv, e2\nv\n, \u0001\u0001\u0001 ,\neT\nv g\nfor node v2V, we consider tasks of node classiﬁcation\nand link prediction to evaluate the representation quality. The\ntwo tasks share a similar objective function:\nJ = Lce + \u0015Pnn; (16)\nwhere Lce = - 1\nN\nPy log (~y) is the cross-entropy loss, Pnn\nis the penalization term for parameters to address over-ﬁtting,\nand \u0015is a hyper-parameter.\nFor node classiﬁcation, y is the class label. ~y is the esti-\nmate, which is produced by ~y = softmax(WoeT\nv + bo). Wo\n2Rc\u0002nMdM and bo 2Rc are parameters. cis the number\nof classes. For link prediction, y is the binary label, indi-\ncating whether there is a link between two nodes. ~y is the\nestimate, which is produced by a logistic regression classi-\nﬁer, i.e., ~y = fh(et\nv;et\nu);1\n- h(et\nv;et\nu) g\n, where h(et\nv;et\nu)\n=\n\u001b((Woet\nv)>(Woet\nu)) and Wo 2Rdo\u0002nMdM is a parameter.\nSparsity Constrains\nWe add the sparsity constraint on the\nbinary activation decision vector z = (z1, z2, znM )>2RnM\nof each time step. We want TRRN to avoid activating every\nmemory and capture the most relevant ones at each time step,\nwhich reﬂects the sparse dynamics in the evolution of tempo-\nral networks. Inspired by (Ke et al. 2018), we add a penalty\nterm \u0015s\nPT\nt=1 S(zt) into the objective function. S(zt) rep-\nresents the penalty that TRRN needs to pay for activating\nmemories at time step t. The penalty term is deﬁned as\n\u0015s\nTX\nt=1\nS(zt) =\u0015s\nTX\nt=1\nReLU((\nnMX\nk=1\nzt\nk) \u0000\rnM ); (17)\nwhere \u0015s and \rare hyper-parameters. nM is the number of\nmemories. \ris the proportion of memories that are without\nbeing penalized when they are activated. Intuitively, each\nactivated memory above the threshold \rnM is penalized.\nComplexity Analysis TRRN is local in time and the in-\nput sequence length does not affect its storage require-\nments. The complexity per parameter is O(1) for each\ntime step. Thus, the complexity of TRRN per time step\nis proportional to the number of parameters. The param-\neters of TRRN are from the policy network, memory up-\ndating, contextualized representation learning and the task\nlayers. Based on the four parts, the complexity of TRRN is\nStatistics Task-fMRI DBLP5 Epinions Reddit\n# Nodes 5000 6606 16025 8291\n# Edges 1955488 42815 1144258 264050\n# Attributes 20 100 20 20\n# Time Steps 12 10 11 10\n# Node Categories 10 5 10 4\nTable 2: Dataset description.\nO(dF dM +cdM nM +nF d2\nF +nM d2\nM\n) for node classiﬁcation\nor O(dF dM +dM donM +nF d2\nF\n+nM d2\nM\n) for link prediction.\nBecause O(dM )=O(dF ) and O(nM )=O(nF ), the complexity\nof per time step of TRRN is derived to O(cdM nM +nM d2\nM\n)\nor O(dM donM +nM d2\nM ).\nExperiments\nTo analyze the node representation quality from different\nperspectives, we propose three research questions:\n(RQ1) How does TRRN compare with the leading network\nmodeling methods on node classiﬁcation task?\n(RQ2) How does TRRN compare with the competitors on\nlink prediction task?\n(RQ3) Are the proposed memory activating and updating\nmechanism in TRRN effective and interpretable?\nExperimental Setup\nDatasets We use four real temporal network datasets as\nshown in Table 2. Task-fMRI is a brain temporal network\ndataset, where nodes represent tidy cubes of brain tissue and\nare categorized into ten groups. Two nodes are connected\nif they show similar degree of activation during a time pe-\nriod (Gonzalez-Castillo et al. 2015). This dataset is extracted\nfrom the task based functional magnetic resonance imaging\n(fMRI) data1, which is collected when the subject conducts\ndifferent tasks successively. We apply PCA to the fMRI data\nof a time period to generate the node attributes of a network\nsnapshot. DBLP5 is a co-author temporal network dataset,\nwhere nodes represent authors. The dataset is extracted from\nthe bibliography website DBLP. The node attributes in a net-\nwork snapshot are extracted from the titles and abstracts of\nthe corresponding author’s publications during a time period\nby word2vec (Mikolov et al. 2013). The authors in DBLP5\nare from ﬁve areas. Epinions is a who-trust-whom temporal\nnetwork dataset which is extracted from the product review\nwebsite Epinions.com, where users decide whether to trust\nothers to seek advice. Nodes represent users and two nodes\nare connected if one of them seeks advice from the other.\nNode attributes are generated from the reviews by word2vec.\nWe select ten categories of products to construct the dataset.\nReddit is a post temporal network dataset (reddit.com), where\nnodes represent posts. Two nodes are connected if their cor-\nresponding posts contain similar keywords. We apply the\nword2vec approach to the comments of a post to generate its\nnode attributes. We select four categories of the posts.\n1https://tinyurl.com/y4hhw8ro\n4550\nMethod Task-fMRI DBLP5 Epinions Reddit\nACC AUC ACC AUC ACC AUC ACC AUC\nDeepWalk 71.4\u00061.3 97.2\u00061.0 35.4\u00061.2 61.0\u00061.8 30.1\u00061.6 68.4\u00061.8 47.5\u00061.7 71.9\u00062.4\nGAT 43.8\u00062.5 86.2\u00063.4 32.5\u00062.4 48.6\u00062.9 22.5\u00061.5 63.1\u00061.8 29.6\u00061.9 52.4\u00062.6\nGCN 65.0\u00061.4 86.7\u00060.9 33.7\u00061.3 50.0\u00061.2 20.9\u00060.7 62.4\u00061.4 27.7\u00060.8 54.0\u00061.0\nGraphSAGE 69.4\u00062.6 96.7\u00062.1 71.0\u00061.1 90.7\u00061.9 24.5\u00062.9 63.9\u00062.0 42.5\u00062.1 66.8\u00062.5\nnode2vec 71.0\u00061.5 96.8\u00061.8 36.9\u00061.1 64.2\u00061.1 32.8\u00061.5 70.2\u00061.6 48.0\u00061.3 72.2\u00061.1\nLSTM 83.6\u00061.8 98.6\u00061.5 74.1\u00060.6 91.4\u00060.8 17.9\u00061.0 61.5\u00061.2 40.2\u00061.4 66.5\u00061.6\nGRU 80.4\u00061.7 98.2\u00061.7 75.6\u00061.0 91.5\u00061.1 17.3\u00061.1 61.7\u00061.6 42.1\u00061.4 67.2\u00061.7\nDynGEM 71.0\u00062.7 97.2\u00062.7 52.3\u00063.2 59.0\u00063.4 31.6\u00062.4 54.6\u00062.5 39.9\u00063.5 66.2\u00062.8\nDANE 85.2\u00061.3 94.8\u00062.9 82.5\u00061.7 92.3\u00061.0 31.8\u00061.8 67.1\u00061.8 45.7\u00061.9 70.0\u00061.6\nSTAR 89.2\u00061.2 99.2\u00060.8 80.3\u00061.5 95.5\u00060.7 32.6\u00061.6 67.4\u00061.5 50.8\u00061.3 75.0\u00061.7\nTRRN 91.5\u00061.0 99.8\u00060.8 88.9\u00061.5 97.6\u00061.8 34.6\u00061.3 72.8\u00061.2 52.0\u00061.7 79.2\u00061.8\nTable 3: Node classiﬁcation results (%).\nBaselines We compare TRRN with competitive base-\nlines. DeepWalk (Perozzi, Al-Rfou, and Skiena 2014),\nGAT (Veliˇckovi´c et al. 2018), GCN (Kipf and Welling 2017),\nGraphSAGE (Veliˇckovi´c et al. 2018) and node2vec (Grover\nand Leskovec 2016) are well-known network modeling meth-\nods proposed for static networks. They are good at extract-\ning spatial structure information, but ignore the temporal\ndependency of temporal networks. LSTM and GRU (Cho\net al. 2014) are proposed to model temporal dependency and\nfamous for their gate mechanism to address the vanishing\n(and exploding) gradient issues. DANE (Li et al. 2017), Dyn-\nGEM (Goyal et al. 2018), DynAERNN (Goyal, Chhetri, and\nCanedo 2018) and STAR (Xu et al. 2019b) are temporal net-\nwork modeling methods. DANE and STAR are capable of\nconsidering both network topology and node attributes. To\ngain insights about TRRN, we study its variants. TRRN-All\nupdates all memories at each time step. TRRN-Fix updates\na ﬁxed number of memories (half of the total) at each time\nstep. TRRN-S does not apply the sparsity constraint. More\ndetails of these methods are summarized in Table 1.\nOther Settings In our experiments, \u0015 and \u0015s are set to\nthe same, 10\u00003. They are determined by grid-search from\nf0, 2\u000210\u00004, 5\u000210\u00004, 1\u000210\u00003, 2\u000210\u00003g. \r is set to 0.5,\nwhich is from f0.25, 0.5, 0.75g. Rand nF are set to 4 and\n2 respectively. nM is set to 4, which is from f2,3,4,5g. dM ,\ndF and ~dF are set to the same, 20. The sizes of query, key,\nvalue are set to same, ds=20 (we use three attention heads).\nThey are determined from f10, 20, 30, 40 g. We randomly\nselect 4/5 of all training examples as the training set and 1/5\nas the test set. We randomly select 1/10 of the training set as\nthe validation set to determine the best hyper-parameters. 10-\nfold cross-validation is applied. TRRN is implemented with\nPyTorch and optimized by Adam (Kingma and Ba 2014)2.\nNode Classiﬁcation Comparison (RQ1)\nIn this section, we conduct experiments on node classiﬁcation.\nAll models are trained on G = (G1, G2, \u0001\u0001\u0001 , GT ) to learn the\nnode representations fe1\nv, e2\nv\n, \u0001\u0001\u0001 , eT\nv g\nfor v 2V. Given\nthese representations and the labels of a subset of nodes VL,\nnode classiﬁcation aims to classify the nodes in subset VU\nwhose labels are unknown, where V = VL [VU . For the\n2Data and codes can be found in the authors’ website.\nstatic network modeling methods, we ﬁrst apply them to each\nnetwork snapshot to generate the node representation at each\ntime step. Then we concatenate the representations of all time\nsteps into a vector. A fully-connected layer with softmax is\napplied to the vector to predict the node label.\nTable 3 shows the node classiﬁcation results. We observe\nthat TRRN achieves consistent gains over the baselines.\nLSTM and GRU perform well on Task-fMRI and DBLP,\nbut show low performance on Epinions and Reddit. This is\nbecause the node representations of Task-fMRI and DBLP\nare dominated by node attributes, but they are dominated by\nnetwork topology in Epinions and Reddit. DeepWalk and\nnode2vec show high performance on Epinions and Reddit be-\ncause of their advanatges of extracting topology information.\nSTAR DANE and DynGEM show high performance in gen-\neral, which is mainly because of their ability of utilizing both\nnode attribute and network topology information. We conjec-\nture that allowing relational reasoning on a set of memories\nhelps TRRN capture the evolution of temporal networks and\nthus is responsible for achieving the superior performance.\nLink Prediction Comparison (RQ2)\nLink prediction is another widely used task to evaluate the\nquality of learned node representations (Wu et al. 2020). The\ntraining examples (node pairs) are created for time step tby\nsampling the links in Gt and an equal number of randomly\nsampled non-links. We split the training examples of each\ntime step into training set and test set. All models are trained\non training sets and tested on test sets. The node representa-\ntions at time stept-1 are used to predict the links at time stept.\ni.e., classifying a node pair in test set into links and non-links.\nWe evaluate the models at time steptfor t2f2,3,\u0001\u0001\u0001 ,Tg.\nTable 4 shows the link prediction results. TRRN shows\nthe consistently superior performance. Notably, GAT shows\ncomparable performance compared to STAR. One possible\nreason is that GAT has a good ability to extract local network\nstructure information, which plays an important role on link\nprediction. TRRN outperforms STAR, which indicates the\nbeneﬁt of reasoning over a set of memories that provides\nmultiple paths for information to ﬂow from past to current.\nIn addition, we compare the prediction results at each time\nstep as shown in Figure 3. It is observed that TRRN shows\nmore stable performance compared to GAT. This is because\n4551\nMethod Task-fMRI DBLP5\nACC AUC ACC AUC\nGAT 72.2\u00061.5 75.6\u00060.9 63.9\u00062.0 69.0\u00061.7\nLSTM 68.1\u00062.7 71.3\u00060.7 59.7\u00061.4 65.9\u00061.7\nSTAR 71.9\u00060.9 73.4\u00061.7 63.9\u00061.5 67.4\u00061.5\nTRRN 75.3\u00061.9 78.0\u00060.9 66.5\u00060.7 70.9\u00061.4\nTable 4: Link prediction results (%).\nMethod Task-fMRI DBLP5\nACC AUC ACC AUC\nTRRN-All 88.7\u00061.9 97.6\u00061.1 86.0\u00061.8 94.3\u00061.6\nTRRN-Fix. 90.1\u00062.3 98.4\u00061.6 85.2\u00061.9 95.2\u00061.8\nTRRN-S 90.4\u00061.2 97.7\u00060.9 86.2\u00061.5 95.2\u00061.6\nTRRN 91.5\u00061.0 99.8\u00060.8 88.9\u00061.5 97.6\u00061.8\nTable 5: Ablation study on dynamic memory updating.\nthe static network modeling methods ignore the temporal\ndependency of network evolution.\nAnalysis of Memory Activating & Updating (RQ3)\nSince memory activating and updating play critical role in\nTRRN, we conduct exploratory analysis of: (effectiveness)\nhow they inﬂuence the performance of TRRN, and (inter-\npretability) whether the activation decisions are interpretable.\nEffectiveness We conduct an ablation study by comparing\nTRRN with its variants TRRN-All, TRRN-Fix and TRRN-S.\nTRRN-All updates all memories by setting the activation de-\ncision vector z to 1. TRRN-Fix updates half of the memories.\nIt feeds the concatenation of input factors and memories to an\nattention layer and the memories with the top half attention\nvalues are selected. TRRN-S removes the sparsity constrain.\nTable 5 shows the results (node classiﬁcation). TRRN outper-\nforms TRRN-All and TRRN-S, indicating the importance of\nselective memory activation and sparsity constrain on model-\ning network evolution. By comparing TRRN with TRRN-Fix,\nwe see the beneﬁt of updating memories dynamically.\nInterpretability We visualize the (averaged) activation de-\ncision vectors. Figure 4(a) shows the results of two categories\nof nodes from Task-fMRI. It is observed the two categories\nof nodes activate different memories. However, they share\nmore decision vector patterns at the ﬁrst and last few periods.\nThis is because both of the categories of nodes are active\nwhen the subject conduct the tasks during these periods.\nWe also visualize the self-attention matrix, i.e., softmax\n(QK>=pds) in Eq. (10). Figure 4(b) shows the results. We\nobserve memories are activated dynamically in different peri-\nods. Moreover, node attribute factor (f1) shows higher atten-\ntion values (on activated memories) than network topology\n(f2), which indirectly veriﬁes attribute information is domi-\nnant in DBLP5. Thus, TRRN is able to provide effective and\nmore importantly, interpretable evolution modeling results.\nConclusion\nIn this paper, we propose a method TRRN to model temporal\nnetworks. To capture the complicated temporal dependency,\n(a) Task-fMRI\n(b) DBLP5\nFigure 3: Link prediction results at each time step.\nm1 m2 m3 m4 f1 f2\nm1\nm2\nm3\nm4\nBody \nmovement\n2 4 6 8 10 12\nTime step\nCognitive \ncontrol\nm1\nm2\nm3\nm4\nTime step 1Time step 12\nMemory Factor\nm1\nm2\nm3\nm4\nm1\nm2\nm3\nm4\n(a) Activation decisions.\nm1 m2 m3 m4 f1 f2\nm1\nm2\nm3\nm4\nBody \nmovement\n2 4 6 8 10 12\nTime step\nCognitive \ncontrol\nm1\nm2\nm3\nm4\nTime step 1Time step 10\nMemory Factor\nm1\nm2\nm3\nm4\nm1\nm2\nm3\nm4\nInactivated\nInactivated\nInactivated\nInactivated\n(b) Self-attention matrices.\nFigure 4: (a) The visualization of (averaged) activation deci-\nsion vectors of two categories of nodes from Task-fMRI. (b)\nThe visualization of self-attention weight matrices of a node\nfrom DBLP5 at two time steps .\nTRRN employs transformer-style self-attention to reason\nover a set of memories. With the policy network augmented\nwith differentiable routers, TRRN updates memories dynami-\ncally at the time steps where they are more relevant. Contex-\ntualized node representations are generated by considering\nboth updated memories and different factors that inﬂuence\nnode behaviors. Experimental results show TRRN outper-\nforms the competitive baselines and provides interpretability\nfor the modeling results.\n4552\nAcknowledgements\nThis project was partially supported by NSF projects IIS-\n1707548 and CBET-1638320.\nReferences\nAhmed, K.; and Torresani, L. 2019. STAR-Caps: Capsule\nnetworks with straight-through attentive routing. In NeurIPS,\n9098–9107.\nBattaglia, P. W.; Hamrick, J. B.; Bapst, V .; Sanchez-Gonzalez,\nA.; Zambaldi, V .; Malinowski, M.; Tacchetti, A.; Raposo, D.;\nSantoro, A.; Faulkner, R.; et al. 2018. Relational inductive\nbiases, deep learning, and graph networks. arXiv preprint\narXiv:1806.01261 .\nBengio, Y . 2017. The consciousness prior.arXiv:1709.08568\n.\nBengio, Y .; L´eonard, N.; and Courville, A. 2013. Estimat-\ning or propagating gradients through stochastic neurons for\nconditional computation. arXiv:1308.3432 .\nCao, S.; Lu, W.; and Xu, Q. 2016. Deep neural networks for\nlearning graph representations. In AAAI.\nCho, K.; Van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learn-\ning phrase representations using RNN encoder-decoder for\nstatistical machine translation. In EMNLP.\nDu, L.; Wang, Y .; Song, G.; Lu, Z.; and Wang, J. 2018. Dy-\nnamic Network Embedding: An Extended Approach for Skip-\ngram based Network Embedding. In IJCAI, 2086–2092.\nGonzalez-Castillo, J.; Hoy, C. W.; Handwerker, D. A.; Robin-\nson, M. E.; Buchanan, L. C.; Saad, Z. S.; and Bandettini, P. A.\n2015. Tracking ongoing cognition in individuals using brief,\nwhole-brain functional connectivity patterns. PNAS 112(28):\n8762–8767.\nGoyal, A.; Lamb, A.; Hoffmann, J.; Sodhani, S.; Levine, S.;\nBengio, Y .; and Sch¨olkopf, B. 2019. Recurrent independent\nmechanisms. arXiv preprint arXiv:1909.10893.\nGoyal, P.; Chhetri, S. R.; and Canedo, A. 2018. dyn-\ngraph2vec: Capturing network dynamics using dynamic\ngraph representation learning. arXiv:1809.02657 .\nGoyal, P.; Kamra, N.; He, X.; and Liu, Y . 2018. Dyngem:\nDeep embedding method for dynamic graphs. arXiv preprint\narXiv:1805.11273 .\nGraves, A.; Wayne, G.; Reynolds, M.; Harley, T.; Danihelka,\nI.; Grabska-Barwi´nska, A.; Colmenarejo, S. G.; Grefenstette,\nE.; Ramalho, T.; Agapiou, J.; et al. 2016. Hybrid comput-\ning using a neural network with dynamic external memory.\nNature 538(7626): 471–476.\nGrover, A.; and Leskovec, J. 2016. node2vec: Scalable fea-\nture learning for networks. In SIGKDD, 855–864. ACM.\nGuo, Y .; Shi, H.; Kumar, A.; Grauman, K.; Rosing, T.; and\nFeris, R. 2019. SpotTune: transfer learning through adaptive\nﬁne-tuning. In CVPR, 4805–4814.\nHamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive\nrepresentation learning on large graphs. In NeurIPS, 1024–\n1034.\nHe, B.; Zhou, D.; Xiao, J.; Liu, Q.; Yuan, N. J.; Xu, T.; et al.\n2019. Integrating graph contextualized knowledge into pre-\ntrained language models. arXiv preprint arXiv:1912.00147\n.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation9(8): 1735–1780.\nJang, E.; Gu, S.; and Poole, B. 2017. Categorical reparame-\nterization with gumbel-softmax. In ICLR.\nKe, N. R.; Zolna, K.; Sordoni, A.; Lin, Z.; Trischler, A.;\nBengio, Y .; Pineau, J.; Charlin, L.; and Pal, C. 2018. Focused\nhierarchical rnns for conditional sequence processing. In\nICML.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for stochas-\ntic optimization. arXiv:1412.6980 .\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Classiﬁ-\ncation with Graph Convolutional Networks. In ICLR.\nLi, J.; Dani, H.; Hu, X.; Tang, J.; Chang, Y .; and Liu, H. 2017.\nAttributed network embedding for learning in a dynamic\nenvironment. In CIKM, 387–396. ACM.\nMa, Y .; Guo, Z.; Ren, Z.; Zhao, E.; Tang, J.; and Yin, D. 2018.\nStreaming Graph Neural Networks. arXiv:1810.10627 .\nMaddison, C. J.; Mnih, A.; and Teh, Y . W. 2017. The con-\ncrete distribution: A continuous relaxation of discrete random\nvariables. ICLR .\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Ef-\nﬁcient estimation of word representations in vector space.\narXiv:1301.3781 .\nPerozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk:\nOnline learning of social representations. In SIGKDD, 701–\n710. ACM.\nSankar, A.; Wu, Y .; Gou, L.; Zhang, W.; and Yang, H. 2020.\nDySAT: Deep Neural Representation Learning on Dynamic\nGraphs via Self-Attention Networks. In WSDM, 519–527.\nSantoro, A.; Bartunov, S.; Botvinick, M.; Wierstra, D.; and\nLillicrap, T. 2016. Meta-learning with memory-augmented\nneural networks. In International conference on machine\nlearning, 1842–1850.\nSantoro, A.; Faulkner, R.; Raposo, D.; Rae, J.; Chrzanowski,\nM.; Weber, T.; Wierstra, D.; Vinyals, O.; Pascanu, R.; and\nLillicrap, T. 2018. Relational recurrent neural networks. In\nNeurIPS, 7299–7310.\nSukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-end\nmemory networks. In NeurIPS, 2440–2448.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser,\nŁ.; and Polosukhin, I. 2017. Attention\nis all you need. In NeurIPS, 5998–6008.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2018. Graph attention networks. InICLR.\nWang, D.; Cui, P.; and Zhu, W. 2016. Structural deep network\nembedding. In SIGKDD, 1225–1234. ACM.\nWang, X.; Cui, P.; Wang, J.; Pei, J.; Zhu, W.; and Yang, S.\n2017. Community preserving network embedding. In AAAI.\n4553\nWu, Z.; Pan, S.; Chen, F.; Long, G.; Zhang, C.; and Philip,\nS. Y . 2020. A comprehensive survey on graph neural net-\nworks. IEEE transactions on neural networks and learning\nsystems .\nXu, D.; Cheng, W.; Luo, D.; Gu, Y .; Liu, X.; Ni, J.; Zong,\nB.; Chen, H.; and Zhang, X. 2019a. Adaptive neural network\nfor node classiﬁcation in dynamic networks. In ICDM, 1402–\n1407. IEEE.\nXu, D.; Cheng, W.; Luo, D.; Liu, X.; and Zhang, X. 2019b.\nSpatio-Temporal Attentive RNN for Node Classiﬁcation in\nTemporal Attributed Graphs. In IJCAI, 3947–3953.\nZhang, Z.; Cui, P.; and Zhu, W. 2018. Deep learning on\ngraphs: A survey. arXiv preprint arXiv:1812.04202.\nZhou, J.; Cui, G.; Zhang, Z.; Yang, C.; Liu, Z.; Wang, L.; Li,\nC.; and Sun, M. 2018. Graph neural networks: A review of\nmethods and applications. arXiv preprint arXiv:1812.08434.\nZhu, D.; Cui, P.; Zhang, Z.; Pei, J.; and Zhu, W. 2018. High-\norder proximity preserved embedding for dynamic networks.\nTKDE 30(11): 2134–2144.\nZuo, Y .; Liu, G.; Lin, H.; Guo, J.; Hu, X.; and Wu, J. 2018.\nEmbedding Temporal Network via Neighborhood Formation.\nIn SIGKDD, 2857–2866. ACM.\n4554",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8111491799354553
    },
    {
      "name": "Transformer",
      "score": 0.4897925853729248
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46408721804618835
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.41961801052093506
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}