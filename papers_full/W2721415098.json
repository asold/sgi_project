{
    "title": "A Neural Network approach for mixing language models",
    "url": "https://openalex.org/W2721415098",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5014456106",
            "name": "Youssef Oualil",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5008875255",
            "name": "Dietrich Klakow",
            "affiliations": [
                null,
                "Saarland University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6713098461",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2158199200",
        "https://openalex.org/W1965154800",
        "https://openalex.org/W6686651311",
        "https://openalex.org/W2963315049",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2251640092",
        "https://openalex.org/W6631943919",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W2437096199",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W6600417804",
        "https://openalex.org/W6674571003",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2097333193",
        "https://openalex.org/W2950682695",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2185726469",
        "https://openalex.org/W10704533",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2951559648"
    ],
    "abstract": "The performance of Neural Network (NN)-based language models is steadily\\nimproving due to the emergence of new architectures, which are able to learn\\ndifferent natural language characteristics. This paper presents a novel\\nframework, which shows that a significant improvement can be achieved by\\ncombining different existing heterogeneous models in a single architecture.\\nThis is done through 1) a feature layer, which separately learns different\\nNN-based models and 2) a mixture layer, which merges the resulting model\\nfeatures. In doing so, this architecture benefits from the learning\\ncapabilities of each model with no noticeable increase in the number of model\\nparameters or the training time. Extensive experiments conducted on the Penn\\nTreebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a\\nsignificant reduction of the perplexity when compared to state-of-the-art\\nfeedforward as well as recurrent neural network architectures.\\n",
    "full_text": "A NEURAL NETWORK APPROACH FOR MIXING LANGUAGE MODELS\nYoussef Oualil, Dietrich Klakow\nSpoken Language Systems (LSV)\nCollaborative Research Center on Information Density and Linguistic Encoding\nSaarland University, Saarbr¨ucken, Germany\n{firstname.lastname}@lsv.uni-saarland.de\nABSTRACT\nThe performance of Neural Network (NN)-based language models is\nsteadily improving due to the emergence of new architectures, which\nare able to learn different natural language characteristics. This pa-\nper presents a novel framework, which shows that a signiﬁcant im-\nprovement can be achieved by combining different existing hetero-\ngeneous models in a single architecture. This is done through 1)\na feature layer, which separately learns different NN-based models\nand 2) a mixture layer, which merges the resulting model features.\nIn doing so, this architecture beneﬁts from the learning capabilities\nof each model with no noticeable increase in the number of model\nparameters or the training time. Extensive experiments conducted on\nthe Penn Treebank (PTB) and the Large Text Compression Bench-\nmark (LTCB) corpus showed a signiﬁcant reduction of the perplexity\nwhen compared to state-of-the-art feedforward as well as recurrent\nneural network architectures.\nIndex Terms— Neural networks, mixture models, language\nmodeling\n1. INTRODUCTION\nFor many language technology applications such as speech recogni-\ntion [1] and machine translation [2], a high quality Language Model\n(LM) is considered to be a key component to success. Tradition-\nally, LMs aim to predict probable sequences of predeﬁned linguistic\nunits, which are typically words. These predictions are guided by\nthe semantic and syntactic properties that are encoded by the LM.\nThe recent advances in neural network-based approaches for\nlanguage modeling led to a signiﬁcant improvement over the stan-\ndard N-gram models [3, 4]. This is mainly due to the continuous\nword representations they provide, which typically overcome the ex-\nponential growth of parameters that N-gram models require. The\nNN-based LMs were ﬁrst introduced by Bengio et al. [5], who pro-\nposed a Feedforward Neural Network (FNN) model as an alternative\nto N-grams. Although FNNs were shown to perform very well for\ndifferent tasks [6, 7], their ﬁxed context (word history) size con-\nstraint was a limiting factor for their performance. In order to over-\ncome this constraint, Mikolov et al. [8, 9] proposed a Recurrent Neu-\nral Network (RNN), which allows context information to cycle in the\nnetwork. Investigating the inherent shortcomings of RNNs led to the\nLong-Short Term Memory (LSTM)-based LMs [10], which explic-\nitly control the longevity of context information in the network. This\nchain of novel NN-based LMs continued with more complex and ad-\nvanced models such as Convolutional Neural Networks (CNN) [11]\nand autoencoders [12], to name a few.\nThis research was funded by the German Research Foundation (DFG) as\npart of SFB 1102.\nLMs performance has been shown to signiﬁcantly improve us-\ning model combination. This is typically done by either 1) designing\ndeep networks with different architectures at the different layers, as\nit was done in [11], which combines LSTM, CNN and a highway\nnetwork, or by 2) combining different models at the output layer, as\nit is done in the maximum entropy RNN model [13], which uses di-\nrect N-gram connections to the output layer, or using the classical\nlinear interpolation [14]. While the former category, requires a care-\nful selection of the architectures to combine for a well-suited feature\ndesign, and can be difﬁcult/slow to train, the second category knows\na signiﬁcant increase in the number of parameters when combining\nmultiple models.\nMotivated by the work in [13], we have recently proposed a Se-\nquential Recurrent Neural Network (SRNN) [15], which combines\nFFN information and RNN. In this paper, we continue along this line\nof work by proposing a generalized framework to combine different\nheterogeneous NN-based architectures in a single mixture model.\nMore particularly, the proposed architecture uses 1) a hidden fea-\nture layer to, separately, learn each of the models to be combined,\nand 2) a hidden mixture layer, which combines the resulting model\nfeatures. Moreover, this architecture uses a single word embedding\nmatrix, which is learned from all models, and a single output layer.\nThis framework is, in principle, able to combine different NN-based\nLMs (e.g., FNN, RNN, LSTM, etc.) with no direct constraints on\nthe number of models to combine or their conﬁgurations.\nWe proceed as follows. Section 2 presents an overview of the\nbasic NN-based LMs. Section 3 introduces the proposed neural mix-\nture model. Then, Section 4 evaluates the proposed network in com-\nparison to different state-of-the-art language models for perplexity\non the PTB and the LTCB corpus. Finally, we conclude in Section 5.\n2. NEURAL NETWORK LANGUAGE MODELS\nThe goal of a language model is to estimate the probability distribu-\ntion p(wT\n1 ) of word sequences wT\n1 = w1,··· ,wT . Using the chain\nrule, this distribution can be expressed as\np(wT\n1 ) =\nT∏\nt=1\np(wt|wt−1\n1 ) (1)\nLet U be a word embedding matrix and let W be the hidden-to-\noutput weights. NN-based LMs (NNLMs), that consider word em-\nbeddings as input, approximate each of the terms involved in this\nproduct in a bottom-up evaluation of the network according to\nHt = M(P,Rt−1,U) (2)\nOt = g\n(\nHt ·W\n)\n(3)\narXiv:1708.06989v1  [cs.CL]  23 Aug 2017\nwhere Mrepresents a particular NN-based model, which can be a\ndeep architecture, Pdenotes its parameters and Rt−1 denotes its\nrecurrent information at time t. g(·) is the softmax function.\nThe rest of this section brieﬂy introduces M, Pand Rt−1 for\nthe basic architectures, namely FNN, RNN and LSTM, which were\ninvestigated and evaluated as different components in the proposed\nmixture model. The proposed architecture, however, is general and\ncan include all NNLMs that consider world embeddings as input.\n2.1. Feedforward Neural Networks\nSimilarly to N-gram models, FNN uses the Markov assumption of\norder N −1 to approximate (1). That is, the current word depends\nonly on the last N −1 words. Subsequently, Mis given by\nEt−i = Xt−i ·U , i = N −1,··· ,1 (4)\nHt = f\n(N−1∑\ni=1\nEt−i ·Vi\n)\n(5)\nXt−i is a one-hot encoding of the wordwt−i. Thus, Et−i is the con-\ntinuous representation of the word wt−i. f(·) is an activation func-\ntion. Hence, for an FNN model M, P= {Vi}N−1\ni=1 and Rt−1 = ∅.\n2.2. Recurrent Neural Networks\nRNN attempts to capture the complete history in a context vectorht,\nwhich represents the state of the network and evolves in time. There-\nfore, RNN approximates each term in (1) as p(wt|wT\n1 ) ≈p(wt|ht).\nAs a result, Mfor an RNN is given by\nHt = f\n(\nXt−1 ·U + Ht−1 ·V\n)\n(6)\nThus, for an RNN model M, P= V and Rt−1 = Ht−1.\n2.3. Long-Short Term Memory Networks\nIn order to alleviate the rapidly changing context issue in standard\nRNNs and control the longevity of the dependencies modeling in the\nnetwork, the LSTM architecture [10] introduces an internal memory\nstate Ct, which explicitly controls the amount of information, to for-\nget or to add to the network, before estimating the current hidden\nstate. Formally, an LSTM model Mis given by\nEt−1 = Xt−1 ·U (7)\n{i,f,o }t = σ\n(\nVi,f,o\nw ·Et−1 + Vi,f,o\nh ·Ht−1\n)\n(8)\n˜Ct = f\n(\nVc\nw ·Et−1 + Vc\nh ·Ht−1)\n(9)\nCt = ft ⊙Ct−1 + it ⊙ ˜Ct (10)\nHt = ot ⊙f\n(\nCt)\n(11)\nwhere ⊙is the element-wise product, ˜Ct is the memory candidate,\nwhereas it,ft and ot are the input, forget and output gates of the net-\nwork, respectively. Hence, for an LSTM modelM, Rt = {Ht,Ct}\nand P= {Vi,f,o,c\nw ,V i,f,o,c\nh }.\n3. NEURAL NETWORK MIXTURE MODELS\nOn the contrary to a large number of research directions on improv-\ning or designing (new) particular neural architectures for language\nmodeling, the work presented in this paper is an attempt to design\na general architecture, which is able to combine different types of\nexisting heterogeneous models rather than investigating new ones.\n3.1. Model Combination for Language Modeling\nThe work presented in this paper is motivated by recent research\nshowing that model combination can lead to a signiﬁcant improve-\nment in LM performance [14]. This is typically done by either 1)\ndesigning deep networks with different architectures at the different\nlayers, as it was done in [11]. This category of model combination,\nhowever, requires a careful selection of the architectures to combine\nfor a well-suited feature design, as it can be difﬁcult/slow to train,\nwhereas the second category 2) combines different models at the\noutput layer, as it is done in the maximum entropy RNN model [13]\nor using the classical linear interpolation [14]. This category typ-\nically leads to a signiﬁcant increase in the number of parameters\nwhen combining multiple models.\nIn a ﬁrst attempt to circumvent these problems, we have recently\nproposed an SRNN model [15], which combines FFN information\nand RNN through additional sequential connections at the hidden\nlayer. Although SRNN was successful and did not noticeably suffer\nfrom the aforementioned problems, it was solely designed to com-\nbine RNN and FNN and is, therefore, not well-suited for other archi-\ntectures. This paper continues along this line of work by proposing a\ngeneral architecture to combine different heterogeneous neural mod-\nels with no direct constraints on the number or type of models.\n3.2. Neural Network Mixture Models\nThis section introduces the mathematical formulation of the pro-\nposed mixture model. Let {Mm}M\nm=1 be a set of M models to\ncombine, and let {Pm,Rt\nm}M\nm=1 be their corresponding model pa-\nrameters and recurrent information at time t, respectively. For the\nbasic NNLMs, namely FNN, RNN and LSTM, Mm, Pm and Rt\nm\nwere introduced in Section 2.\nLet U be the shared word embedding matrix, which is learned\nduring training from all models in the mixture. The mixture model\nis given by the following steps (see illustration in Fig. 1):\n1) Feature layer: update each model and calculate its features\nHt\nm = Mm(Pm,Rt−1\nm ,U), m = 1,··· ,M (12)\n2) Mixture layer: combine the different features\nHt\nmixture = fmixture\n( M∑\nm=1\nHt\nm ·Sm\n)\n(13)\n3) Output layer: calculate the output using a softmax function\nOt = g\n(\nHt\nmixture ·W\n)\n(14)\nfmixture is a non-linear mixing function, whereas Sm, m=\n1,··· ,M are the mixture weights (matrices).\nAlthough the experiments conducted in this work mainly include\nFNN, RNN and LSTM, the set of possible model selection for Mm\nis not restricted to these but includes all NN-based models that take\nword embeddings as input.\nThe proposed mixture model uses a single word embedding ma-\ntrix and a single output layer with predeﬁned and ﬁxed sizes. The\nlatter are independent of the sizes of the mixture models. In doing\nso, this model does not suffer from the signiﬁcant parameter growth\nwhen increasing the number of models in the mixture. We can also\nsee that this architecture does not impose any direct constraints on\nthe number of models to combine, their size or their type. Hence,\nwe can combine, for instance, models of the same type but with dif-\nferent sizes/conﬁgurations, as we can combine heterogeneous mod-\nels such as recurrent and non-recurrent models, in a single mixture.\nMoreover, the mixture models can also be deep architectures with\nmultiple hidden layers.\nFig. 1: Neural Mixture Model (NMM) architecture. Red (back) ar-\nrows show the error propagation during training.\n3.3. Training of Neural Mixture Models\nNMM training follows the standard back-propagation algorithm\nused to train neural architectures. More particularly, the error at the\noutput layer is propagated to all models in the mixture. At this stage,\neach model receives a network error, updates its parameters, and\npropagates its error to the shared word embedding (input) layer. We\nshould also mention here that recurrent models can be “unfolded” in\ntime, independently of the other models in the mixture, as it is done\nfor standard networks. Once each model is updated, the continu-\nous word representations are then updated as well while taking into\naccount the individual network errors emerging from the different\nmodels in the mixture (see illustration in Fig. 1).\nThe joint training of the mixture models is expected to lead to a\n“complementarity” effect. We mean by “complementarity” that the\nmixture models perform poorly when evaluated separately but lead\nto a much better performance when tested jointly. This is typically\na result of the models learning and modeling, eventually, different\nfeatures. Moreover, the joint learning is also expected to lead to a\nricher and more expressive word embeddings.\n3.4. Model Dropout\nIn order to 1) enforce models co-training and 2) avoid network over-\nﬁtting when the number of models in the mixture is large. We\nuse a model dropout technique, which is inspired by the standard\ndropout regularization [16] that is widely used to train neural net-\nworks. The idea here is to have “models” replace “neurons” in the\nstandard dropout. Therefore, for each training example, a model is\nto be dropped with a probability pd. Then, only models that are\nselected contribute to the mixture and have their parameters and\nmixing weights Sm updated. Similarly to standard dropout, model\ndropout is applied only to non-recurrent models in the mixture.\n4. EXPERIMENTS AND RESULTS\n4.1. Experimental Setup\nWe evaluated the proposed architecture on two different benchmark\ntasks. The ﬁrst set of experiments was conducted on the Penn Tree-\nbank (PTB) corpus using the standard division, e.g. [9, 17]; sections\n0-20 are used for training while sections 21-22 and 23-24 are used\nfor validation and testing. The vocabulary was limited to the 10k\nmost frequent words while the remaining words were all mapped\nto the token <unk>. In order to evaluate how the proposed ap-\nproach scales to large corpora, we run a set of experiments on the\nLarge Text Compression Benchmark (LTCB) [18]. This corpus is\nbased on the enwik9 dataset which contains the ﬁrst 109 bytes of\nenwiki-20060303-pages-articles.xml. We adopted the same training-\ntest-validation data split and pre-processing from [17]. The vocabu-\nlary was limited to the 80k most frequent words. Details about the\nsizes of these two corpora and the percentage of Out-Of-V ocabulary\n(OOV) words that were mapped to <unk>can be found in Table 1.\nTable 1: Corpus size in number of words and <unk>rate.\nTrain Dev Test\nCorpus #W <unk> #W <unk> #W <unk>\nPTB 930K 6.52% 82K 6.47% 74K 7.45%\nLTCB 133M 1.43% 7.8M 2.15% 7.9M 2.30%\nThe results reported below compare the proposed Neural Mix-\nture Model (NMM) approach to the baseline NNLMs. In particular,\nwe compare our model to the FNN-based LM [5], the full RNN [9]\n(without classes) as well as RNN with maximum entropy (RN-\nNME) [13]. We also report results for the LSTM architecture [10],\nand the recently proposed SRNN model [15].\nAlthough the proposed approach was not designed for a partic-\nular mixture of models, we only report results for different com-\nbinations of FNN, RNN and LSTM, which are considered to be\nthe baseline NNLMs. For clarity, an NMM result is presented\nas F\nN1,··· ,Nf\nS1,··· ,Sf + RS1,··· ,Sr + LS1,··· ,Sl , where f is the number of\nFNNs in the mixture, Sm,m = 1,··· ,f are their correspond-\ning hidden layer sizes (that are fed to the mixture) and Nm,m =\n1,··· ,f are their ﬁxed history sizes. The same notation holds for\nRNN and LSTM, whererand lare the number of RNNs and LSTMs\nin the mixture, respectively, and Nr,Nl = 1. The number of mod-\nels in the mixture is given by f + r + l. Moreover, the notation\nFNb−Ne\nSf means that this model combines Ne −Nb + 1consecutive\nFNN models with respective history sizesNb,Nb +1,··· ,Ne, with\nall models having the same hidden layer size Sf .\n4.2. PTB Experiments\nFor the PTB experiments, all models have a hidden layer size of 400,\nwith FFNN and SRNN using the Rectiﬁed Linear Unit (ReLu) i.e.,\nf(x) = max(0,x) as activation function and having 2 hidden lay-\ners. ReLu is also used as activation function for the mixture layer in\nNMMs, which use a single hidden layer. The embedding size is 100\nfor SRNN and NMMs, whereas it is set to 400 for RNN and 200 for\nFNN and LSTM. The training is performed using the stochastic gra-\ndient descent algorithm with a mini-batch size of 200. the learning\nrate is initialized to 0.4, the momentum is set to 0.9, the weight decay\nis ﬁxed at 4x10−5, the model dropout is set to 0.4 and the training\nis done in epochs. The weights initialization follows the normalized\ninitialization proposed in [19]. Similarly to [8], the learning rate is\nhalved when no signiﬁcant improvement in the log-likelihood of the\nvalidation data is observed. The BPTT was set to 5 time steps for\nall recurrent models. In the tables below, the results are reported\nin terms of perplexity (PPL), Number of model Parameters (NoP)\nand the Parameter Growth (PG) for NMM, which is deﬁned as the\nrelative increase in the number of parameters of NMM w.r.t. the\nbaseline model in the table. In order to demonstrate the power of\nthe joint training, we also report the perplexity PPL and NoP of the\nLinearly Interpolated (LI) models in the mixture after training them\nseparately. In this case, each model learns its own word embedding\nand output layer.\nTable 2: LMs performance on the PTB test set.\nmodel PPL NoP PG PPL(LI) NoP(LI)\nFNN (N=5) 114 6.49M — — —\nF2,3\n200 117 5.27M -18.80% 120.0 6.10M\nF2−5\n200 110 5.61M -13.56% 112.0 12.28M\nLSTM 105 6.97M — — —\nL100 + F2\n200 102 5.25M -24.68% 114 5.12M\nL100 + R100 102 5.18M -25.68% 118 4.09M\nRNN 117 8.17M — — —\nR100 + F2\n200 109 5.18M -36.60% 119 5.05M\nR100 + F2−6\n200 105 5.86M -28.27% 108 17.41M\nRNNME 117 10G — — —\nWD-SRNN 104 6.33M — — —\nWI-SRNN 104 5.33M — — —\nThe PTB results reported in Table 2 show clearly that combin-\ning different small-size models with a reduced word embedding size\nresults in a better perplexity performance compared to the baseline\nmodels, with a signiﬁcant decrease in the NoP required by the mix-\nture. More particularly, we can see that adding a single FNN model\nto a small size LSTM or RNN is sufﬁcient to outperform the base-\nline models while reducing the number of parameters by 24% and\n36%, respectively. The same conclusion can be drawn when com-\nbining an RNN with an LSTM. We can also see that adding more\nFNN models to each of these mixtures leads to additional improve-\nments while keeping the number of parameters signiﬁcantly small.\nTable 2 also shows that training the small size models (in the mix-\nture) separately, and then linearly interpolating them, results in a\nslightly worse performance compared to the mixture model with a\nnoticeable increase in the NoP. This conclusion emphasizes the im-\nportance of the joint training. Moreover, we can also see that mixing\nRNN and FNNs leads to a comparable performance to SRNN, which\nwas particularly designed to enhance RNN with FNN information.\nThe proposed approach, however, does not particularity encode the\nindividual characteristics of the models in the mixture, which re-\nﬂects its ability to include different types of NNLMs. We can also\nconclude that combining FNN with recurrent models leads to a more\nsigniﬁcant improvement when compared to mixtures of FNNs. This\nconclusion shows, similarly to other work e.g. [15, 13], that recurrent\nmodels can be further improved using N-gram/feedforward informa-\ntion, given that they model different linguistic features.\nFig. 2 is an extension of Table 2, which shows the change in the\nperplexity and NoP of different NMMs when iteratively adding more\nFFN models to the mixture. This ﬁgure conﬁrms that combining het-\nerogeneous models (combining LSTM or RNN with FNNs) achieves\na better performance compared to combining only FNN models. We\ncan also conclude from this ﬁgure that the improvement becomes\nvery slow after adding 4 FNN models to each mixture.\n4.3. LTCB Experiments\nThe LTCB experiments use the same PTB setup with minor changes.\nThe results shown in Table 3 follow the same experimental setup\nused in [15]. More precisely, these results were obtained without\nusage of momentum, model dropout or weight decay whereas the\n0 2 4 6 8\n100\n110\n120\n130\n140\n150\n160\n170\nHistory Size (N−1)\nPPL\n \n \n0 2 4 6 8\n3\n4\n5\n6\n7\n8\nHistory Size (N−1)\nNoP (M)\nStandard FNN\nF200\n{1,...,(N−1)} \nL100+F200\n{1,...,(N−1)} \nR 100+F200\n{1,...,(N−1)} \nFig. 2: Perplexity vs parameter growth of different mixture models\nwhile iteratively adding more FNN models to the mixture.\nmini-batch size was set to 400. The FNN architecture contains 2\nhidden layers of size 600 whereas RNN, LSTM, SRNN and NMM\nhave a single hidden layer of size 600.\nTable 3: LMs Perplexity on the LTCB test set.\nmodel PPL NoP PG\nFNN[4*200]-600-600-80k 110 64.92M —\nF2−4\n600 102 66.24M 2.03%\nF2−7\n100 92 64.98M 0.09%\nRNN[600]-600-80k 85 96.44M —\nR200 + F4\n400 84 64.80M -32.81%\nR200 + F2−4\n600 77 66.40M -31.15%\nLSTM[600]-600-80k 66 66.00M —\nL400 + R200 64 65.44M -1.51%\nL300 + F4\n300 64 65.28M -1.75%\nL600 + F4\n600 58 67.21M 1.16%\nWI-SRNN[4*200]-600-80k 77 64.56M —\nWD-SRNN[4*200]-600-80k 72 80.56M —\nThe LTCB results shown in Table 3 generally conﬁrm the PTB\nconclusions. In particular, we can see that combining recurrent mod-\nels, with half (third for RNN) their original size, with a single FNN\nmodel leads to a comparable performance to the baseline models.\nMoreover, increasing the mixture models size (for LSTM) or in-\ncreasing the number of FNNs (for RNN) improves the performance\nfurther with no noticeable increase in the NoP. Similarly to the PTB,\nwe can also see that NMM achieves the same performance as the WI-\nSRNN model with a NoP reduction of31% compared to the original\nRNN model. The FFN mixture results show a more signiﬁcant im-\nprovement when combining multiple small-size (100) models com-\npared to mixing few large models (600). This conclusion shows that\nthe strength of mixture models lies in their ability to combine the\nlearning capabilities of different models, even with small sizes.\n5. CONCLUSION AND FUTURE WORK\nWe have presented a neural mixture model which is able to combine\nheterogeneous NN-based LMs in a single architecture. Experiments\non PTB and LTCB corpora have shown that this architecture sub-\nstantially outperforms many state-of-the-art neural systems, due to\nits ability to combine learning capabilities of different architectures.\nFurther gains could be made using a more advanced model selec-\ntion or feature combination at the mixing layer instead of the simple\nmodel weighting. These will be investigated in future work.\n6. REFERENCES\n[1] Slava M. Katz, “Estimation of probabilities from sparse data\nfor the language model component of a speech recognizer,”\nIEEE Transactions on Acoustics, Speech, and Signal Process-\ning, vol. 35, no. 3, pp. 400–401, Mar. 1987.\n[2] Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent\nJ. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L.\nMercer, and Paul S. Roossin, “A statistical approach to ma-\nchine translation,” Computational Linguistics, vol. 16, no. 2,\npp. 79–85, Jun. 1990.\n[3] Ronald Rosenfeld, “Two decades of statistical language mod-\neling: where do we go from here?,” Proceedings of the IEEE,\nvol. 88, no. 8, pp. 1270–1278, Aug. 2000.\n[4] Reinhard Kneser and Hermann Ney, “Improved backing-\noff for m-gram language modeling,” in IEEE International\nConference on Acoustics, Speech, and Signal Processing\n(ICASSP), Detroit, Michigan, USA, May 1995, pp. 181–184.\n[5] Yoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Chris-\ntian Jauvin, “A neural probabilistic language model,” Journal\nof Machine Learning Research , vol. 3, pp. 1137–1155, Mar.\n2003.\n[6] Holger Schwenk and Jean-Luc Gauvain, “Training neural net-\nwork language models on very large corpora,” in Human\nLanguage Technology Conference and Conference on Empir-\nical Methods in Natural Language Processing (EMNLP), Oct.\n2005, pp. 201–208.\n[7] Joshua Goodman, “A bit of progress in language modeling,\nextended version,” Tech. Rep. MSR-TR-2001-72, Microsoft\nResearch, 2001.\n[8] Tomas Mikolov, Martin Karaﬁ ´at, Luk´as Burget, Jan Cernock´y,\nand Sanjeev Khudanpur, “Recurrent neural network based\nlanguage model,” in 11th Annual Conference of the Interna-\ntional Speech Communication Association (INTERSPEECH) ,\nMakuhari, Chiba, Japan, Sep. 2010, pp. 1045–1048.\n[9] Tomas Mikolov, Stefan Kombrink, Luk ´as Burget, Jan Cer-\nnock´y, and Sanjeev Khudanpur, “Extensions of recurrent neu-\nral network language model,” in IEEE International Confer-\nence on Acoustics, Speech, and Signal Processing (ICASSP) ,\nPrague, Czech Republic, May 2011, pp. 5528–5531.\n[10] Martin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney,\n“LSTM neural networks for language modeling,” in 13th An-\nnual Conference of the International Speech Communication\nAssociation (INTERSPEECH) , Portland, Oregon, USA, Sep.\n2012, pp. 194–197.\n[11] Kim Yoon, Jernite Yacine, Sontag David, and Rush Alexander\nM., “Character-aware neural language models,” in 30th AAAI\nConference on Artiﬁcial Intelligence, 2016.\n[12] Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh\nKhapra, Balaraman Ravindran, Vikas C Raykar, and Amrita\nSaha, “An autoencoder approach to learning bilingual word\nrepresentations,” in Advances in Neural Information Process-\ning Systems 27, pp. 1853–1861. 2014.\n[13] Tomas Mikolov, Anoop Deoras, Daniel Povey, Luk ´as Bur-\nget, and Jan Cernock ´y, “Strategies for training large scale\nneural network language models,” in IEEE Workshop on\nAutomatic Speech Recognition & Understanding (ASRU) ,\nWaikoloa, Hawaii, USA, Dec. 11-15, 2011, pp. 196–201.\n[14] Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Luk ´as Bur-\nget, and Jan Cernock´y, “Empirical evaluation and combination\nof advanced language modeling techniques,” in 12th Annual\nConference of the International Speech Communication Asso-\nciation (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011,\npp. 605–608.\n[15] Youssef Oualil, Clayton Greenberg, Mittul Singh, and Diet-\nrich Klakow, “Sequential recurrent neural network for lan-\nguage modeling,” in 17th Annual Conference of the Interna-\ntional Speech Communication Association (INTERSPEECH) ,\nSan Francisco, California, USA, Sep. 2016.\n[16] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov, “Dropout: a simple way\nto prevent neural networks from overﬁtting.,” Journal of Ma-\nchine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n[17] Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, and\nLi-Rong Dai, “The ﬁxed-size ordinally-forgetting encoding\nmethod for neural network language models,” in 53rd Annual\nMeeting of the Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural Language\nProcessing of the Asian Federation of Natural Language Pro-\ncessing ACL, July 2015, vol. 2, pp. 495–500.\n[18] Matt Mahoney, “Large text compression benchmark,” 2011.\n[19] Xavier Glorot and Yoshua Bengio, “Understanding the difﬁ-\nculty of training deep feedforward neural networks,” in Pro-\nceedings of the Thirteenth International Conference on Artiﬁ-\ncial Intelligence and Statistics (AISTATS), Chia Laguna Resort,\nSardinia, Italy, May 2010, pp. 249–256."
}