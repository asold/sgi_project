{
    "title": "Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech",
    "url": "https://openalex.org/W4385570946",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2807696482",
            "name": "Flor Miriam Plaza del Arco",
            "affiliations": [
                "Bocconi University"
            ]
        },
        {
            "id": "https://openalex.org/A2203505503",
            "name": "Debora Nozza",
            "affiliations": [
                "Bocconi University"
            ]
        },
        {
            "id": "https://openalex.org/A310222905",
            "name": "Dirk Hovy",
            "affiliations": [
                "Bocconi University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3198943295",
        "https://openalex.org/W2954226438",
        "https://openalex.org/W2740168486",
        "https://openalex.org/W2595653137",
        "https://openalex.org/W4302012739",
        "https://openalex.org/W3185909895",
        "https://openalex.org/W3034282334",
        "https://openalex.org/W2791170418",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4385573793",
        "https://openalex.org/W3091315987",
        "https://openalex.org/W3210129272",
        "https://openalex.org/W3088784738",
        "https://openalex.org/W2887782043",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3173380736",
        "https://openalex.org/W3010933031",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W4297899443",
        "https://openalex.org/W4285152678",
        "https://openalex.org/W3176580738",
        "https://openalex.org/W4296155197",
        "https://openalex.org/W2908323419",
        "https://openalex.org/W2906979176",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3185578183",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W4297633153",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W2970200208",
        "https://openalex.org/W4287888350",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W3175487198",
        "https://openalex.org/W4287257982",
        "https://openalex.org/W2737223426",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W4205216562",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2971150411",
        "https://openalex.org/W2473555522",
        "https://openalex.org/W3174882277"
    ],
    "abstract": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",
    "full_text": "The 7th Workshop on Online Abuse and Harms (WOAH), pages 60–68\nJuly 13, 2023c⃝2023 Association for Computational Linguistics\nRespectful or Toxic?\nUsing Zero-Shot Learning with Language Models to Detect Hate Speech\nFlor Miriam Plaza-del-Arco, Debora Nozza, Dirk Hovy\nBocconi University\nVia Sarfatti 25\nMilan, Italy\n{flor.plaza, debora.nozza, dirk.hovy}@unibocconi.it\nAbstract\nHate speech detection faces two significant\nchallenges: 1) the limited availability of la-\nbeled data and 2) the high variability of hate\nspeech across different contexts and languages.\nPrompting brings a ray of hope to these chal-\nlenges. It allows injecting a model with task-\nspecific knowledge without relying on labeled\ndata. This paper explores zero-shot learning\nwith prompting for hate speech detection. We\ninvestigate how well zero-shot learning can de-\ntect hate speech in 3 languages with limited\nlabeled data. We experiment with various large\nlanguage models and verbalizers on 8 bench-\nmark datasets. Our findings highlight the im-\npact of prompt selection on the results. They\nalso suggest that prompting, specifically with\nrecent large language models, can achieve per-\nformance comparable to and surpass fine-tuned\nmodels, making it a promising alternative for\nunder-resourced languages. Our findings high-\nlight the potential of prompting for hate speech\ndetection and show how both the prompt and\nthe model have a significant impact on achiev-\ning more accurate predictions in this task.\n1 Introduction\nThe rising prevalence of online hate speech and its\nharmful effects have made hate speech detection a\ncentral task in natural language processing (NLP).\nDespite progress, the prevalent supervised learning\napproaches encounter significant challenges: many\nlanguages or contexts have little or no labeled data\n(Poletto et al., 2021). Hate speech is also subjective\nand context-dependent, as it is influenced by factors\nsuch as demographics, social norms, and cultural\nbackgrounds (Talat and Hovy, 2016).\nTo overcome these challenges, approaches like\nzero-shot learning (ZSL) and prompting of large\nlanguage models (LLMs) have emerged. 1 Both\n1Note that ZSL could be used with various models,\nwhereas prompting is specific to LLMs. Here, we use ZSL\nto prompt LLMs without additional labeled examples in the\nprompt (few-shot learning), but only the target sentence.\nuse a template to process the original text and the\nclass labels as verbalizers. This approach lever-\nages the LLM’s knowledge to predict the like-\nlihood of the (class) verbalizers in the template.\nThese verbalizers guide the model’s understanding\nof a specific task. For binary hate speech detec-\ntion, the template might be “< text>. This text\nis <verbalizer>”, where <verbalizer> can be\n“hateful” or “ non-hateful”. For the input, “I\nhate you. This text is”, the LLM should associate\na higher likelihood with the verbalizer completion\n“hateful”. By picking the more likely comple-\ntion, this approach requires no training data. It\nhas shown promising results in various NLP ap-\nplications (Zhao et al., 2023; Su et al., 2022; Wei\net al., 2022; Brown et al., 2020).However, to date,\nits effectiveness for hate speech detection remains\nlargely unexplored.\nWe comprehensively evaluate ZSL with prompt-\ning for hate speech detection to better understand\nits capabilities. The choice of appropriate verbaliz-\ners is a key factor in the effectiveness of prompting\n(Plaza-del-Arco et al., 2022; Liu et al., 2023). To\nthis end, we systematically compare various ver-\nbalizers across multiple models. We evaluate the\nperformance of conventional transformer models\nand more recent instruction fine-tuned LLMs on 8\nbenchmark datasets to assess their robustness. Fur-\nthermore, we test our approach on two languages\nwith limited labeled data (Italian and Spanish). Our\nresults show that ZSL with prompting matches or\nsurpasses the performance of fine-tuned models,\nparticularly in instruction fine-tuned models.\nContributions 1) We investigate the effective-\nness of ZSL with prompting for hate speech detec-\ntion 2) We conduct a systematic exploration and\ncomparison of various verbalizers across 5 models\n3) We extend our investigation to two languages\nwith limited labeled data. Our code is publicly\navailable at https://github.com/MilaNLProc/\nprompting_hate_speech.\n60\n2 Datasets\nWe compare our results on 8 benchmark datasets\nusing binary classification. See Table 1 for details.\nThey differ in terms of size, corpus source, and\nlabels. More details are in Appendix A.\nDataset Size Source\nDAVIDSON 24,802 Twitter\nDYNABENCH 41,255 Synthetic\nGHC 27,665 Gab\nHATEVAL 13,000 Twitter\nHATEXPLAIN 20,148 Twitter and Gab\nMHS 50,000 Youtube, Twitter and Reddit\nMLMA 5,647 Twitter\nHSHP 16,914 Twitter\nTable 1: Datasets used in our experiments.\n3 Prompting for Zero-Shot Hate Speech\nClassification\nWe use ZSL with prompting to evaluate the models’\nability to detect hate speech. First, we test various\nencoder models to select the best verbalizers. We\nthen test those verbalizers on recent instruction\nfine-tuned LLMs and compare to encoder models.\nEncoder-based Language Models For our ex-\nperiments, we use the following prompt template:\n“<text> This text is < verbalizers>”. We then\ncheck the LLM likelihood of hateful and non-\nhateful verbalizers and select the most probable\ncompletion as final prediction. We test all 25\npossible pairs from the following lists. For hate:\nharmful, abusive, offensive, hateful,\ntoxic, and for non-hate respectful, kind,\npolite, neutral, positive.\nWe compare three different language models:\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019), and DeBERTa (He et al., 2020). We use\nOpenPrompt (Ding et al., 2022), a standard frame-\nwork for prompt learning over pre-trained language\nmodels.\nInstruction Fine-tuned Language Models We\nexperiment with recent instruction fine-tuned lan-\nguage models. They are fine-tuned on a large set of\nvaried instructions that use an intuitive description\nof the downstream task to answer natural language\nprompts. In this approach, we formulate the prompt\ntemplate as “Classify this text as <verbnon-hate> or\n<verbhate>. < text>. Answer:”, for the verbaliz-\ners (verbnon-hate, verbhate) we consider the best\npair obtained with the encoder models, and for the\nprompt models, we use the Fine-tuned Language\nNet (FLAN-T5) model (Chung et al., 2022) and\nmT0 (Muennighoff et al., 2022). Note that FLAN-\nT5 has been trained for toxic language detection.\nBaseline We used (1) a RoBERTa model fine-\ntuned with supervised training on each hate speech\ndataset and (2) the commercial Perspective API.2\n4 Results\n4.1 Encoder models\nTable 2 shows the results of several encoder mod-\nels on multiple hate speech detection benchmark\ndatasets. Overall, the best-performing encoder\nmodel across different datasets is RoBERTaLARGE\nobtained the best macro-F1 score in 5 out of\n8 datasets. Regarding the verbalizers, the pair\npositive and polite yield the best results in\nidentifying non-hateful speech, while hateful and\ntoxic prove best for detecting hate speech. This\nhighlights the need for careful selection of verbal-\nizers to achieve optimal performance in this task.\nIdentifying Best Verbalizers To select the best\npair of verbalizers that work well across models and\ndatasets for hate speech detection, we averaged the\ndifferent performance metrics by model and dataset\nacross all folds. As shown in Table 3, the best-\nperforming verbalizer pair is respectful-toxic,\nwhich achieves the highest macro-F1 score of\n42.74. The verbalizers most commonly associated\nwith the non-hate speech class arerespectful and\npolite, while toxic and hateful are more com-\nmonly associated with hate speech. We select the\nbest verbalizer pair ( respectful-toxic) to con-\nduct additional experiments.\n4.2 Encoder vs. Instruction Fine-tuned LLMs\nIn this section, we compare the results obtained by\nprompting the encoder-based models and the in-\nstruction fine-tuned models. The results are shown\nin Table 4. These models are prompted using the\nbest pair of verbalizers we found in the encoder-\nbased models, which is respectful-toxic. In\ngeneral, the recent models mT0 and FLAN-T5 out-\nperform the encoder-based models by a large mar-\ngin showing an average improvement of 39.75%\nand 65.33% over the encoder models, respectively.\nIn particular, FLAN-T5 exhibits remarkable per-\nformance in detecting hate speech across various\ndatasets, which can be attributed to its prior fine-\ntuning for toxic detection. This suggests that the\n2https://www.perspectiveapi.com/\n61\nDataset Model Verbnon-hate Verbhate F1non-hate F1hate Macro-F1\nDAVIDSON RoBERTaLARGE positive hateful 41.38 69.15 55.26\nDYNABENCH RoBERTaLARGE positive harmful 52.96 57.36 55.16\nGHC RoBERTaLARGE positive hateful 45.03 68.85 56.94\nHATEVAL BERTBASE-uncased polite toxic 61.52 58.05 59.78\nHATEXPLAIN RoBERTaLARGE polite toxic 24.36 86.23 55.30\nMHS RoBERTaLARGE positive hateful 66.91 73.68 70.30\nMLMA DeBERTaV3-BASE polite hateful 12.32 93.53 52.93\nHSHP RoBERTaBASE positive hateful 73.79 54.64 64.21\nTable 2: Class and macro-F1 score of encoder models on different benchmark datasets.\nVerb-nh Verb-h F1-nh F1-h Macro-F1\nrespectful toxic 27.28 58.19 42.74\npolite hateful 24.37 59.42 41.89\npositive hateful 34.58 48.84 41.71\npositive offensive 19.37 63.94 41.66\nneutral toxic 31.17 52.11 41.64\nrespectful hateful 18.60 63.91 41.25\npolite toxic 28.30 53.79 41.04\nTable 3: Verbalizer pairs across encoder models and\ndatasets by Macro-F1 score.\nknowledge learned from detecting toxic language\nis transferable and can be leveraged to improve\nhate speech detection in other datasets. In addition,\nwe conduct a comparison between the supervised\nlearning upper bound, a fine-tuned RoBERTaBASE\nmodel, and the instruction fine-tuned models in\nour ZSL experiments. Our findings show that the\ninstruction fine-tuned models achieve comparable\nperformance, and FLAN-T5 even surpasses the\nRoBERTaBASE fine-tuned model in some datasets,\nsuch as GHC , HATEXPLAIN , and MLMA . Overall,\nthe DAVIDSON dataset achieves the highest perfor-\nmance among all the datasets, with a macro-F1\nscore of 83.30. In contrast, the MLMA dataset ob-\ntains the lowest macro-F1 score of 54.35, which\nis expected given its complexity arising from the\nlow inter-annotator agreement. Notably, the per-\nformance on the HATEVAL dataset (65.38) exhibits\nan improvement over the participant results’ mean\n(44.84) in the competition (Basile et al., 2019). On\nthe DYNABENCH dataset, the FLAN-T5 model’s\nresult (58.08) is similar to that of fine-tuning the\nRoBERTaBASE fine-tuned model (61.76), despite\nthe dataset’s complexity with a large number of\nchallenging perturbations that make it harder for\nmodels to detect hate speech accurately. Finally,\nwe compared our approach with Perspective API,\nthe most popular commercial tool for toxicity detec-\ntion. FLAN-T5 is outperforming it in 6 cases out of\n8, demonstrating prompting to be a more accurate\nsolution. While the varying degrees of difficulty\nacross datasets in hate speech detection is demon-\nstrated in these results, the potential of instruction\nfine-tuned models to achieve state-of-the-art per-\nformance on various benchmarks without requiring\nfine-tuning on a specific dataset is highlighted. This\ninsight is especially valuable for subjective tasks\nlike hate speech, where the complex nature of la-\nbeling this phenomenon can make it challenging to\nfind labeled datasets.\n5 Results on Multi-Lingual Datasets\nWe also investigated the effectiveness of ZSL with\nprompting in a multilingual context, which is of-\nten more challenging due to the scarcity or un-\navailability of data. We present the outcomes\nachieved by multilingual models: multilingual\nXLM-R (Conneau et al., 2020) as encoder model\nand mT0 and FLAN-T5 as instruction fined-tuned\nmodels. The prompt has been written in English\nfollowing the same templates presented in Sec-\ntion 3 and using the best-performing verbalizer\npair respectful-toxic. We use the experimen-\ntal settings adopted in Nozza (2021), comparing\nour method with their fine-tuned XLM-R model.\nThus, the dataset comprises English (EN), Spanish\n(ES), and Italian ( IT). The HatEval (Basile et al.,\n2019) shared task dataset on hate speech against\nimmigrants and women on Twitter is adopted for\nEnglish and Spanish. For Italian, two different\ncorpora proposed for Evalita shared tasks (Caselli\net al., 2018) are considered: the automatic misog-\nyny identification challenge (AMI) (Fersini et al.,\n2018) for hate speech towards women, and the hate\nspeech detection shared task on Facebook and Twit-\nter (HaSpeeDe) (Bosco et al., 2018) for hate speech\ntowards immigrants.\nThe results are shown in Table 5. Regarding the\nZSL approaches, the instruction fine-tuned mod-\nels outperform XLM-R, with FLAN-T5 achieving\nthe highest macro-F1 score on all languages. The\n62\nDataset ZSL Prompting API Fine-tuning\nRoBERTaB RoBERTaL BERTB DeBERTaB DeBERTaL mT0 FLAN-T5 Perspective API RoBERTaB\nDAVIDSON 42.46 40.87 52.33 46.67 25.99 54.46 83.30 79.20 91.28\nDYNABENCH 36.68 36.08 45.87 51.38 37.57 54.11 58.08 55.50 61.76\nGHC 42.02 41.43 53.13 50.13 35.36 56.07 61.53 62.35 59.59\nHATEVAL 31.89 29.90 59.69 55.82 36.68 57.76 65.38 60.77 70.98\nHATEXPLAIN 49.38 46.11 48.93 51.67 20.88 56.68 67.11 58.86 60.34\nMHS 44.60 36.16 62.23 57.38 43.29 74.70 79.38 87.90 90.50\nMLMA 47.90 47.65 49.47 49.10 28.23 44.97 54.35 43.91 47.47\nHSHP 27.50 24.77 43.10 44.17 40.37 53.97 64.36 56.30 76.82\nAvg.%↑ — — — — — 39.75 ↑ 65.33↑ — —\nTable 4: Macro-F1 scores for different models on benchmark datasets using respectful-toxic verbalizer. B = base\nmodel, L = large model. Best model in bold, second-best underlined. Last row shows the average improvement of\nFlan-T5 and mT0 over encoder models.\nLang XLM-R mT0 FLAN-T5 Nozza (2021)\nEN 29.80 57.85 65.34 41.6\nES 29.42 53.75 62.61 75.2\nIT 31.34 43.25 57.29 80.4\nTable 5: Macro-F1 scores on different languages. Best\nmodel in bold, second-best underlined.\nZSL models, as expected, did not outperform the\nfine-tuned XLM-R. However, the results obtained\nfrom the ZSL models are still considered adequate.\nSpanish, in particular, achieves comparable results\nwith FLAN-T5 to the fine-tuned XLM-R. FLAN-\nT5 achieves better results in English because it is\nnot affected by overfitting issues that arise during\ntraining (Nozza, 2021). These findings suggest\nthat prompting with instruction fine-tuned LLMs\nis a promising method for hate speech detection\nin both mono and multilingual settings, without\nlanguage-specific fine-tuning.\n6 Related Work\nHate speech classification received increased atten-\ntion in recent years. Supervised learning methods\nare the most common (Poletto et al., 2021; Fortuna\net al., 2022). Among these methods, fine-tuning\ntransformer-based LLMs emerged as the dominant\nparadigm (Plaza-del-Arco et al., 2020; Sarkar et al.,\n2021; Singh and Li, 2021; Caselli et al., 2021; Kirk\net al., 2022, inter alia). However, they face sig-\nnificant challenges, like the limited availability of\nlabeled data, especially in languages other than En-\nglish (Schmidt and Wiegand, 2017; Fortuna and\nNunes, 2018), and the subjective nature of hate\nspeech, which varies based on cultural background,\npersonal experiences, and individual beliefs.\nLLMs have led to innovative techniques like\nprompting (Liu et al., 2023) that use zero-shot and\nfew-shot learning paradigms without needing la-\nbeled data. Recent works have explored these new\ntechniques for hate speech detection. Chiu et al.\n(2021) use the prompts “Is the following text sex-\nist? Answer yes or no” and “Classify the following\ntexts into racist, sexist, or neither” to detect hate\nspeech, with GPT-3 showing that LLMs have a\nrole to play in hate speech detection. Schick et al.\n(2021) explore toxicity in LLMs using comparable\nprompts to self-diagnose toxicity during the de-\ncoding. They use the RealToxicityPrompts dataset\n(Gehman et al., 2020). (Goldzycher and Schneider,\n2022) develop NLI-based zero-shot hate speech\ndetection approaches using prompts as a hypothe-\nsis as proposed by Yin et al. (2019). Their results\noutperform fine-tuned models. Our work ZSL for\nhate speech classification differs from previous ap-\nproaches as follows. (1) We provide a comprehen-\nsive evaluation of ZSL with prompting on multiple\nbenchmark datasets, offering new insights into the\neffectiveness of this technique. (2) We explore the\nimpact of the selection of verbalizers and models\nfor the task, and (3) we compare the performance\nof encoder models with the recent LLMs based on\ninstruction fine-tuning.\n7 Conclusion\nThis paper presents a comprehensive evaluation of\nZSL with prompting for hate speech classification.\nWe have compared both encoder and instruction\nfine-tuned LLMs. Our experiments across different\nbenchmark data sets showed that ZSL with prompt-\ning is a promising option to address the challenges\npresented in supervised learning systems. However,\nit also highlights the importance of carefully select-\ning the model and appropriate verbalizers, as they\ncan significantly affect performance. Our results\nalso show that recent LLMs based on instruction\n63\nfine-tuning play an essential role in hate speech de-\ntection. Further exploration of prompt formulation\ncould lead to their continued growth in this area.\nAdditionally, our multilingual experiments show\nthat our proposed methods can be applied to other\nlanguages with comparable results.\nFuture research could investigate the bias pres-\nence (Dixon et al., 2018; Attanasio et al., 2022)\nand robustness (Röttger et al., 2021, 2022) of ZSL\nprompting for hate speech detection models, also\nin multilingual settings.\nLimitations\nWhile promising, our work presents limitations\nthat need to be acknowledged. Firstly, we did not\nexplore the best verbalizers for instruction fine-\ntuned language models, which could have further\nenhanced the performance of the models explored\nin this study, due to computational cost and the\nspecific goals of the research. Secondly, we se-\nlected benchmark datasets based on their popular-\nity and diversity, which might not be representative\nof all possible datasets in hate speech detection.\nWe also acknowledge that, in addition to the lan-\nguages examined in this paper, there are a number\nof other languages that may present unique chal-\nlenges and characteristics for detecting hate speech.\nOur decision as to which languages to include in\nthe multilingual experiment was based on a direct\ncomparison with state-of-the-art research. Finally,\nwe utilized the latest open-source language models\nfor our experiments, but we did not explore other\nrecent language models, such as the GPT family,\nprimarily because they are not open and reason-\nably reproducible3, and therefore the community\nmay encounter challenges in replicating our results.\nThese limitations provide directions for future re-\nsearch to improve and expand upon our work.\nEthics Statement\nTo ensure data privacy and protection, we use pub-\nlicly available benchmark datasets for hate speech\ndetection and do not collect any personal or sen-\nsitive information. Additionally, we acknowledge\nthat the detection of hate speech can be a sensi-\ntive topic; therefore, we report the results of our\nexperiments in a responsible and appropriate man-\nner. Lastly, we acknowledge that language models\ntrained on large datasets have the potential to per-\npetuate bias and discrimination, and we strive to\n3https://hackingsemantics.xyz/2023/\nclosed-baselines/\nmitigate these risks by carefully selecting and eval-\nuating our models and verbalizers to ensure fairness\nand impartiality.\nAcknowledgements\nThis project has in part received funding from Fon-\ndazione Cariplo (grant No. 2020-4288, MONICA).\nThe authors are members of the MilaNLP group\nand the Data and Marketing Insights Unit of the\nBocconi Institute for Data Science and Analysis.\nReferences\nGiuseppe Attanasio, Debora Nozza, Dirk Hovy, and\nElena Baralis. 2022. Entropy-based attention regu-\nlarization frees unintended bias mitigation from lists.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 1105–1119, Dublin,\nIreland. Association for Computational Linguistics.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela Sanguinetti.\n2019. SemEval-2019 task 5: Multilingual detection\nof hate speech against immigrants and women in\nTwitter. In Proceedings of the 13th International\nWorkshop on Semantic Evaluation, pages 54–63, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\nCristina Bosco, Dell’Orletta Felice, Fabio Poletto,\nManuela Sanguinetti, and Tesconi Maurizio. 2018.\nOverview of the EV ALITA 2018 hate speech de-\ntection task. In Proceedings of the Sixth Eval-\nuation Campaign of Natural Language Process-\ning and Speech Tools for Italian. Final Workshop\n(EVALITA 2018), volume 2263, pages 1–9, Turin,\nItaly. CEUR.org.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language Models are Few-Shot\nLearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTommaso Caselli, Valerio Basile, Jelena Mitrovi´c, and\nMichael Granitzer. 2021. HateBERT: Retraining\nBERT for abusive language detection in English. In\nProceedings of the 5th Workshop on Online Abuse\nand Harms (WOAH 2021), pages 17–25, Online. As-\nsociation for Computational Linguistics.\nTommaso Caselli, Nicole Novielli, Viviana Patti, and\nPaolo Rosso. 2018. EV ALITA 2018: Overview of the\n6th Evaluation Campaign of Natural Language Pro-\ncessing and Speech Tools for Italian. In Proceedings\nof Sixth Evaluation Campaign of Natural Language\nProcessing and Speech Tools for Italian. Final Work-\nshop (EVALITA 2018), Turin, Italy. CEUR.org.\n64\nKe-Li Chiu, Annie Collins, and Rohan Alexander. 2021.\nDetecting Hate Speech with GPT-3. arXiv preprint\narXiv:2103.12407.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nThomas Davidson, Dana Warmsley, Michael W. Macy,\nand Ingmar Weber. 2017. Automated Hate Speech\nDetection and the Problem of Offensive Language. In\nInternational Conference on Web and Social Media.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Haitao Zheng, and Maosong Sun. 2022.\nOpenPrompt: An open-source framework for prompt-\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 105–113, Dublin, Ire-\nland. Association for Computational Linguistics.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classification. In Proceed-\nings of the 2018 AAAI/ACM Conference on AI, Ethics,\nand Society, AIES ’18, page 67–73, New York, NY ,\nUSA. Association for Computing Machinery.\nElisabetta Fersini, Debora Nozza, and Paolo Rosso.\n2018. Overview of the EV ALITA 2018 task on auto-\nmatic misogyny identification (AMI). Proceedings\nof the 6th evaluation campaign of Natural Language\nProcessing and Speech tools for Italian (EVALITA\n2018), 12:59.\nPaula Fortuna, Monica Dominguez, Leo Wanner, and\nZeerak Talat. 2022. Directions for NLP Practices Ap-\nplied to Online Hate Speech Detection. In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11794–11805,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on\nautomatic detection of hate speech in text. 51(4).\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nJanis Goldzycher and Gerold Schneider. 2022. Hypoth-\nesis engineering for zero-shot hate speech detection.\nIn Proceedings of the Third Workshop on Threat, Ag-\ngression and Cyberbullying (TRAC 2022), pages 75–\n90, Gyeongju, Republic of Korea. Association for\nComputational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Wang, Weizhu\nLi, Yelong Liu, and Jianfeng Chen. 2020. DeBERTa:\nDecoding-enhanced BERT with Disentangled Atten-\ntion. arXiv preprint arXiv:2006.03654.\nBrendan Kennedy, Mohammad Atari,\nAida Mostafazadeh Davani, Leigh Yeh, Ali\nOmrani, Yehsong Kim, Kris Coombs, Shreya\nHavaldar, Gwenyth Portillo-Wightman, Elaine\nGonzalez, et al. 2022. Introducing the Gab Hate\nCorpus: defining and applying hate-based rhetoric to\nsocial media posts at scale. Language Resources and\nEvaluation, pages 1–30.\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Da-\nvani, Morteza Dehghani, and Xiang Ren. 2020a. Con-\ntextualizing hate speech classifiers with post-hoc ex-\nplanation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5435–5442, Online. Association for Computa-\ntional Linguistics.\nChris J Kennedy, Geoff Bacon, Alexander Sahn, and\nClaudia von Vacano. 2020b. Constructing interval\nvariables via faceted Rasch measurement and multi-\ntask deep learning: a hate speech application. arXiv\npreprint arXiv:2009.10277.\nHannah Kirk, Bertie Vidgen, and Scott Hale. 2022.\nIs more data better? re-thinking the importance\nof efficiency in abusive language detection with\ntransformers-based active learning. In Proceedings\nof the Third Workshop on Threat, Aggression and\nCyberbullying (TRAC 2022), pages 52–61, Gyeongju,\nRepublic of Korea. Association for Computational\nLinguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\n65\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam,\nChris Biemann, Pawan Goyal, and Animesh Mukher-\njee. 2021. HateXplain: A Benchmark Dataset for\nExplainable Hate Speech Detection. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 35, pages 14867–14875.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-\nham Fikri Aji, Khalid Almubarak, Samuel Albanie,\nZaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. 2022. Crosslingual generalization\nthrough multitask finetuning.\nDebora Nozza. 2021. Exposing the limits of zero-shot\ncross-lingual hate speech detection. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 907–914, Online.\nAssociation for Computational Linguistics.\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang,\nYangqiu Song, and Dit-Yan Yeung. 2019. Multi-\nlingual and multi-aspect hate speech analysis. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4675–\n4684, Hong Kong, China. Association for Computa-\ntional Linguistics.\nFlor Miriam Plaza-del-Arco, María-Teresa Martín-\nValdivia, and Roman Klinger. 2022. Natural lan-\nguage inference prompts for zero-shot emotion clas-\nsification in text across corpora. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 6805–6817, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nFlor-Miriam Plaza-del-Arco, M. Dolores Molina-\nGonzález, L. Alfonso Ureña López, and M. Teresa\nMartín-Valdivia. 2020. Detecting Misogyny and\nXenophobia in Spanish Tweets Using Language Tech-\nnologies. ACM Trans. Internet Technol., 20(2).\nFabio Poletto, Valerio Basile, Manuela Sanguinetti,\nCristina Bosco, and Viviana Patti. 2021. Resources\nand benchmark corpora for hate speech detection: a\nsystematic review. Language Resources and Evalua-\ntion, 55:477–523.\nPaul Röttger, Haitham Seelawi, Debora Nozza, Zeerak\nTalat, and Bertie Vidgen. 2022. Multilingual Hate-\nCheck: Functional tests for multilingual hate speech\ndetection models. In Proceedings of the Sixth Work-\nshop on Online Abuse and Harms (WOAH) , pages\n154–169, Seattle, Washington (Hybrid). Association\nfor Computational Linguistics.\nPaul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak Ta-\nlat, Helen Margetts, and Janet Pierrehumbert. 2021.\nHateCheck: Functional tests for hate speech detec-\ntion models. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 41–58, Online. Association for Computational\nLinguistics.\nDiptanu Sarkar, Marcos Zampieri, Tharindu Ranas-\ninghe, and Alexander Ororbia. 2021. fBERT: A neu-\nral transformer for identifying offensive content. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 1792–1798, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-Diagnosis and Self-Debiasing: A Proposal for\nReducing Corpus-Based Bias in NLP. Transactions\nof the Association for Computational Linguistics ,\n9:1408–1424.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the Fifth International\nWorkshop on Natural Language Processing for So-\ncial Media, pages 1–10, Valencia, Spain. Association\nfor Computational Linguistics.\nSumer Singh and Sheng Li. 2021. Exploiting auxiliary\ndata for offensive language detection with bidirec-\ntional transformers. In Proceedings of the 5th Work-\nshop on Online Abuse and Harms (WOAH 2021) ,\npages 1–5, Online. Association for Computational\nLinguistics.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, and Tao Yu. 2022.\nSelective annotation makes language models better\nfew-shot learners. arXiv preprint arXiv:2209.01975.\nZeerak Talat and Dirk Hovy. 2016. Hateful symbols or\nhateful people? predictive features for hate speech\ndetection on Twitter. In Proceedings of the NAACL\nStudent Research Workshop, pages 88–93, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nBertie Vidgen, Tristan Thrush, Zeerak Talat, and Douwe\nKiela. 2021. Learning from the worst: Dynamically\ngenerated datasets to improve online hate detection.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1667–1682, Online. Association for Computational\nLinguistics.\n66\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022.\nEmergent abilities of large language models. arXiv\npreprint arXiv:2206.07682.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019.\nBenchmarking zero-shot text classification: Datasets,\nevaluation and entailment approach. CoRR,\nabs/1909.00161.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nA Dataset Details\nVidgen et al. (2021) (DYNABENCH ) introduced a\nnovel framework for dynamically creating bench-\nmark corpora. The task assigned to the annotators\ninvolved identifying adversarial examples, which\nare instances that would be classified incorrectly by\nthe target model and are particularly challenging\nto detect. The dataset contains a significant propor-\ntion of hateful entries, accounting for 54% of the\ndataset.\nKennedy et al. (2020b) (MHS ) gathered a large\ncollection of comments from diverse social media\nplatforms (YouTube, Twitter, and Reddit). To label\nthe comments, they used a crowdsourcing platform\nwhere four different ratings were given to each com-\nment. To ensure a comprehensive assessment, the\nauthors made certain that every annotator evaluated\ncomments that spanned the entire hate speech scale.\nSince the dataset is annotated with a continuous\nhate score, we used a threshold set to binarise the\nproblem: if value < -1 → 0 and if value > 0.5 →\n1.\nKennedy et al. (2022) (GHC ) presented the Gab\nHate Corpus, a multi-label English dataset of posts\nsourced from gab.com, a social networking plat-\nform. To label the comments, at least three an-\nnotators labeled them under one of the following\ncategories: Call for Violence, Assault on Human\nDignity, or Not Hateful. Following Kennedy et al.\n(2020a), we aggregate the first two for obtaining\nthe hateful class.\nBasile et al. (2019) (HATEVAL ) created the Hat-\nEval corpus for the HatEval campaign in SemEval.\nThe dataset consists of tweets that were manually\nannotated via crowdsourcing for hate speech. To\ncollect the tweets, they follow three different strate-\ngies: (1) monitoring potential victims of hate ac-\ncounts, (2) downloading the history of identified\nhaters, and (3) filtering Twitter streams with key-\nwords, i.e., words, hashtags, and stems. The corpus\ncontains a total of 24,802 tweets.\nTalat and Hovy (2016) (HSHP ) provided a dataset\nconsisting of 16,914 tweets that were collected us-\ning Twitter’s streaming API and filtered using a\nset of hate speech-related keywords related to reli-\ngious, sexual, gender, and ethnic minorities. The\ntweets were then manually annotated by two anno-\ntators for the presence of hate speech.\nDavidson et al. (2017) ( DAVIDSON ) created a\ndataset of 24,802 tweets annotated for the pres-\nence of hate speech and offensive language. The\ntweets were crawled using keywords related to a\nhate speech lexicon. Each tweet was labeled by\nthree or more people into one of three categories:\nhate speech, offensive language, or neither. We\naggregate the first two for obtaining the hateful\nclass.\nMathew et al. (2021) (HATEXPLAIN ) collected\nEnglish posts from Twitter and Gab social media\nplatforms. Afterward, a crowdsourcing platform\nwas employed to categorize each post into three\ncategories: hate speech, offensive speech, or nor-\nmal speech. In addition to this, the annotators were\ntasked with identifying the target communities men-\ntioned in the posts, as well as the specific portions\nof the post which formed the basis of their labeling\ndecision. Finally, the majority voting decision was\nused to determine the final label. By combining\nthe hate and offensive targets, the hateful class was\nformed. We combine the hate and offensive posts\nto obtain the hateful class.\nOusidhoum et al. (2019) ( MLMA ) presented a\nmultilingual multi-aspect hate speech dataset com-\nprising English, French, and Arabic tweets that en-\ncompass various targets and hostility types. Each\ntweet is labeled by 5 annotators, and then the ma-\njority vote is used to decide the final label. The av-\nerage Krippendorff scores for inter-annotator agree-\nment (IAA) are 0.153, 0.244, and 0.202 for English,\nFrench, and Arabic, respectively.\nB Implementation Details\nWe implement the fine-tuned version of\nRoBERTaBASE with the following hyperpa-\nrameter configuration for training: epochs are set\nto 3, batch size to 8, and the number of epochs\n67\nto 3. For the ZSL models, we used the default\nhyperparameters presented in Hugging Face. We\nfine-tune RoBERTaBASE for three epochs. We\nperform 5-fold partitions and report the results on\nthe test set.\nHugging Face model cars BERTBASE-uncased4,\nRoBERTaBASE5, RoBERTa LARGE6,\nDeBERTaV3-BASE7, DeBERTa V3-LARGE8, XLM-\nRoBERTaLARGE9, mT010, and FLAN-T511.\nComputing Infrastructure We run the experi-\nments on one machine with the following charac-\nteristics: it is equipped with three NVIDIA RTX\nA6000 and has 48GB of RAM.\n4https://huggingface.co/bert-base-uncased\n5https://huggingface.co/roberta-base\n6https://huggingface.co/roberta-large\n7https://huggingface.co/microsoft/\ndeberta-v3-base\n8https://huggingface.co/microsoft/\ndeberta-v3-large\n9https://huggingface.co/xlm-roberta-large\n10https://huggingface.co/bigscience/mT0-xxl\n11https://huggingface.co/google/flan-t5-xl\n68"
}