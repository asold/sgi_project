{
  "title": "Large Language Model–Assisted Risk-of-Bias Assessment in Randomized Controlled Trials Using the Revised Risk-of-Bias Tool: Evaluation Study",
  "url": "https://openalex.org/W4410096786",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2236919384",
      "name": "Jiajie Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3027002599",
      "name": "Honghao Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153724856",
      "name": "Weilong Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893407040",
      "name": "Danni Xia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2429496208",
      "name": "Chun-Yang Bai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2313411897",
      "name": "Mingyao Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103633185",
      "name": "Jianing Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106477905",
      "name": "Jiayi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154764302",
      "name": "Pan Bei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2420727107",
      "name": "Jinhui Tian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2165231689",
      "name": "Long Ge",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3156775206",
    "https://openalex.org/W2104675861",
    "https://openalex.org/W2098923148",
    "https://openalex.org/W3036957975",
    "https://openalex.org/W3201352787",
    "https://openalex.org/W4296821927",
    "https://openalex.org/W3080187696",
    "https://openalex.org/W4321435202",
    "https://openalex.org/W4398201968",
    "https://openalex.org/W4407009455",
    "https://openalex.org/W2793387368",
    "https://openalex.org/W2160796802",
    "https://openalex.org/W2155638685",
    "https://openalex.org/W3211291193",
    "https://openalex.org/W1841320001",
    "https://openalex.org/W1790460690",
    "https://openalex.org/W2511667219",
    "https://openalex.org/W2593331700",
    "https://openalex.org/W3027630905",
    "https://openalex.org/W3008397146",
    "https://openalex.org/W1801725150",
    "https://openalex.org/W2132002786",
    "https://openalex.org/W2200887429",
    "https://openalex.org/W2976709398",
    "https://openalex.org/W1985555749",
    "https://openalex.org/W2948930187",
    "https://openalex.org/W3042270788",
    "https://openalex.org/W2159488334",
    "https://openalex.org/W3006643038",
    "https://openalex.org/W3127620132",
    "https://openalex.org/W2124994119",
    "https://openalex.org/W2175884377",
    "https://openalex.org/W2088248135",
    "https://openalex.org/W2031615497",
    "https://openalex.org/W3181725617",
    "https://openalex.org/W2076947446",
    "https://openalex.org/W2155388082",
    "https://openalex.org/W2126356892",
    "https://openalex.org/W4309438079",
    "https://openalex.org/W2036326371",
    "https://openalex.org/W2117904842",
    "https://openalex.org/W1983955745",
    "https://openalex.org/W2048616866",
    "https://openalex.org/W2409417393",
    "https://openalex.org/W2039675881",
    "https://openalex.org/W2783914054",
    "https://openalex.org/W2968753132",
    "https://openalex.org/W2916756916",
    "https://openalex.org/W2135105002",
    "https://openalex.org/W1958095205",
    "https://openalex.org/W2089640735",
    "https://openalex.org/W2512196444",
    "https://openalex.org/W2098812160",
    "https://openalex.org/W2142190824",
    "https://openalex.org/W2990159075",
    "https://openalex.org/W2162350507"
  ],
  "abstract": "Background The revised Risk-of-Bias tool (RoB2) overcomes the limitations of its predecessor but introduces new implementation challenges. Studies demonstrate low interrater reliability and substantial time requirements for RoB2 implementation. Large language models (LLMs) may assist in RoB2 implementation, although their effectiveness remains uncertain. Objective This study aims to evaluate the accuracy of LLMs in RoB2 assessments to explore their potential as research assistants for bias evaluation. Methods We systematically searched the Cochrane Library (through October 2023) for reviews using RoB2, categorized by interest in adhering or assignment. From 86 eligible reviews of randomized controlled trials (covering 1399 RCTs), we randomly selected 46 RCTs (23 per category). In addition, 3 experienced reviewers independently assessed all 46 RCTs using RoB2, recording assessment time for each trial. Reviewer judgments were reconciled through consensus. Furthermore, 6 RCTs (3 from each category) were randomly selected for prompt development and optimization. The remaining 40 trials established the internal validation standard, while Cochrane Reviews judgments served as external validation. Primary outcomes were extracted as reported in corresponding Cochrane Reviews. We calculated accuracy rates, Cohen κ, and time differentials. Results We identified significant differences between Cochrane and reviewer judgments, particularly in domains 1, 4, and 5, likely due to different standards in assessing randomization and blinding. Among the 20 articles focusing on adhering, 18 Cochrane Reviews and 19 reviewer judgments classified them as “High risk,” while assignment-focused RCTs showed more heterogeneous risk distribution. Compared with Cochrane Reviews, LLMs demonstrated accuracy rates of 57.5% and 70% for overall (assignment) and overall (adhering), respectively. When compared with reviewer judgments, LLMs’ accuracy rates were 65% and 70% for these domains. The average accuracy rates for the remaining 6 domains were 65.2% (95% CI 57.6-72.7) against Cochrane Reviews and 74.2% (95% CI 64.7-83.9) against reviewers. At the signaling question level, LLMs achieved 83.2% average accuracy (95% CI 77.5-88.9), with accuracy exceeding 70% for most questions except 2.4 (assignment), 2.5 (assignment), 3.3, and 3.4. When domain judgments were derived from LLM-generated signaling questions using the RoB2 algorithm rather than direct LLM domain judgments, accuracy improved substantially for Domain 2 (adhering; 55-95) and overall (adhering; 70-90). LLMs demonstrated high consistency between iterations (average 85.2%, 95% CI 85.15-88.79) and completed assessments in 1.9 minutes versus 31.5 minutes for human reviewers (mean difference 29.6, 95% CI 25.6-33.6 minutes). Conclusions LLMs achieved commendable accuracy when guided by structured prompts, particularly through processing methodological details through structured reasoning. While not replacing human assessment, LLMs demonstrate strong potential for assisting RoB2 evaluations. Larger studies with improved prompting could enhance performance.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.8059695959091187
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.5771032571792603
    },
    {
      "name": "Psychology",
      "score": 0.37156298756599426
    },
    {
      "name": "Medicine",
      "score": 0.35339051485061646
    },
    {
      "name": "Computer science",
      "score": 0.34278979897499084
    },
    {
      "name": "World Wide Web",
      "score": 0.2539130449295044
    },
    {
      "name": "Internal medicine",
      "score": 0.11937832832336426
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76214153",
      "name": "Lanzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I58200834",
      "name": "Southern Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210092839",
      "name": "Gansu University of Traditional Chinese Medicine",
      "country": "CN"
    }
  ]
}