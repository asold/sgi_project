{
    "title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
    "url": "https://openalex.org/W4392669714",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3150917519",
            "name": "Christoph Leiter",
            "affiliations": [
                "Bielefeld University"
            ]
        },
        {
            "id": "https://openalex.org/A2555239629",
            "name": "Juri Opitz",
            "affiliations": [
                "Heidelberg University",
                "University of Haifa"
            ]
        },
        {
            "id": "https://openalex.org/A2342646302",
            "name": "Daniel Deutsch",
            "affiliations": [
                "Google (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2065795670",
            "name": "Gao Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2549192832",
            "name": "Rotem Dror",
            "affiliations": [
                "University of Haifa",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2073897212",
            "name": "Steffen Eger",
            "affiliations": [
                "University of Haifa",
                "Bielefeld University",
                "Heidelberg University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3174724858",
        "https://openalex.org/W4287889965",
        "https://openalex.org/W4385571232",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W2888442053",
        "https://openalex.org/W4392669920",
        "https://openalex.org/W4324134461",
        "https://openalex.org/W4323359867",
        "https://openalex.org/W4385750253",
        "https://openalex.org/W4206256378",
        "https://openalex.org/W3093871477",
        "https://openalex.org/W4392669845",
        "https://openalex.org/W3205309080",
        "https://openalex.org/W3173360659",
        "https://openalex.org/W3037013468",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W4327487298",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W3039695075",
        "https://openalex.org/W4386185625",
        "https://openalex.org/W4388553912",
        "https://openalex.org/W4392669730",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W1979354511",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4380687266",
        "https://openalex.org/W4389523995",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4379539933",
        "https://openalex.org/W4392454577",
        "https://openalex.org/W1654441844",
        "https://openalex.org/W4389519402",
        "https://openalex.org/W4378447159",
        "https://openalex.org/W4385571140",
        "https://openalex.org/W4392669856",
        "https://openalex.org/W4363671827",
        "https://openalex.org/W3211758383",
        "https://openalex.org/W3159259047",
        "https://openalex.org/W3035252911",
        "https://openalex.org/W4307934016",
        "https://openalex.org/W4318899036",
        "https://openalex.org/W3119878165",
        "https://openalex.org/W4295682935",
        "https://openalex.org/W4367000491",
        "https://openalex.org/W3119881489",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4381797953",
        "https://openalex.org/W3211986039",
        "https://openalex.org/W4392669734",
        "https://openalex.org/W4385849424",
        "https://openalex.org/W3105214104",
        "https://openalex.org/W4392669836",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2102065370",
        "https://openalex.org/W2806532810",
        "https://openalex.org/W4319793767",
        "https://openalex.org/W4387323454",
        "https://openalex.org/W4322760121",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W3094342783",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3159892921",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W3035628162",
        "https://openalex.org/W3099942180",
        "https://openalex.org/W4392669762",
        "https://openalex.org/W4368367885",
        "https://openalex.org/W4283462007",
        "https://openalex.org/W2295598076"
    ],
    "abstract": "Generative large language models (LLMs) have seen many breakthroughs over the last year. With an increasing number of parameters and pre-training data, they have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Strategies employed in this context differ in the choice of input prompts, the selection of samples for demonstration, and the methodology used to construct scores grading the generations. Approaches often differ in the input prompts, the samples that are selected for demonstration and the construction process of scores from the output. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore such approaches for machine translation evaluation and summarization eval- uation. Specifically, we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We test the approaches of the participants on a new reference-free test-set spanning 3 language pairs for machine transla- tion as well as a summarization dataset. Further, we present an overview of the approaches taken by the participants, present their results on the test set and analyze paths for future work. Fi- nally, as a separate track, we perform a human evaluation of the plausibility of explanations given by the LLMs and its effect on model performance. We make parts of our code and datasets available.",
    "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 117–138\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nThe Eval4NLP 2023 Shared Task on\nPrompting Large Language Models as Explainable Metrics\nChristoph Leiter∗, Juri Opitz†, Daniel Deutsch‡, Yang Gao⋄\nRotem Dror††, Steffen Eger∗\n∗Bielefeld University, Germany †Heidelberg University, Germany\n‡Google, US ⋄Google Research, UK ††University of Haifa, Israel\nchristoph.leiter@uni-bielefeld.de opitz.sci@gmail.com\ndandeutsch@google.com gaostayyang@google.com\nrdror@is.haifa.ac.il steffen.eger@uni-bielefeld.de\nAbstract\nGenerative large language models (LLMs) have\nseen many breakthroughs over the last year.\nWith an increasing number of parameters and\npre-training data, they have shown remarkable\ncapabilities to solve tasks with minimal or no\ntask-related examples. Notably, LLMs have\nbeen successfully employed as evaluation met-\nrics in text generation tasks. Approaches often\ndiffer in the input prompts, the samples that are\nselected for demonstration and the construc-\ntion process of scores from the output. Within\nthis context, we introduce the Eval4NLP 2023\nshared task that asks participants to explore\nsuch approaches for machine translation eval-\nuation and summarization evaluation. Specifi-\ncally, we select a list of allowed LLMs and dis-\nallow fine-tuning to ensure a focus on prompt-\ning. We evaluate the approaches of the partici-\npants on a new reference-free test-set spanning\n3 language pairs for machine translation as well\nas a summarization dataset. Further, we present\nan overview of the approaches taken by the par-\nticipants, present their results on the test set\nand analyze paths for future work. Finally, as a\nseparate track, we perform a small-scale human\nevaluation of the plausibility of explanations\ngiven by the LLMs. We make parts of our code\nand datasets available.1\n1 Introduction\nThe ChatGPT revolution in late 2022 has ignited\na wide public and scientific debate about the pos-\nsibilities (and limitations) of generative AI in var-\nious fields and application scenarios (Leiter et al.,\n1https://github.com/eval4nlp/SharedTask2023/tree/main\n2023b; Eger et al., 2023), including education (Ha-\nlaweh, 2023), logic (Liu et al., 2023a), medicine\n(Dave et al., 2023), math (Frieder et al., 2023),\nprogramming (Rozière et al., 2023) and science\n(Belouadi et al., 2023).\nThe immense research interest has also triggered\nthe exploration of numerous approaches that lever-\nage generative large language models (LLMs) as\nevaluation metrics (Kocmi and Federmann, 2023;\nLiu et al., 2023b; Fu et al., 2023; Xu et al., 2023b;\nFernandes et al., 2023) for natural language genera-\ntion (NLG) tasks like machine translation (MT) and\nsummarization. Recent LLM based approaches dif-\nfer, for example, in their prompting strategies, e.g.,\nin the way that natural language instructions are\nused to trigger the LLM to compute metric scores.\nFor example, GEMBA (Kocmi and Federmann,\n2023) uses zero-shot prompting to directly predict\nscores or quality labels in the output. In contrast,\nAutoMQM (Fernandes et al., 2023) instructs LLMs\nto predict fine-grained error labels and uses these\nto compute the final scores. These works have con-\ntributed to the exploration of prompting for NLG\nevaluation, but an exhaustive exploration of ap-\nproaches remains unaddressed. Further, many ap-\nproaches leverage closed source LLMs while much\nfewer use open source LLMs. Those approaches\nrelying on open source LLMs put a large focus on\nacquiring training data (e.g. Xu et al., 2023b) and\nfine-tune models to specific tasks. Given this typi-\ncal focus on fine-tuning and motivated by promis-\ning work on prompting techniques2 (e.g. Wei et al.,\n2Various websites track the development of prompt-\ning techniques, e.g. https://www.promptingguide.\n117\nFigure 1: Using a generative LLM as MT evaluation metric. In this example, the metric is reference-free. I.e. it\ngrades the translated sentence based on its source sentence. The input sentences are wrapped into a prompt that\nis given to an LLM. The LLM generates an output and a final score could for example be constructed from this\ntextual output or from other values involved in the process. The red borders indicate the focus of our shared task.\nParticipants should evaluate the best prompts and the best approaches to construct scores from model output.\n2022; Yao et al., 2023; Wang et al., 2023; Zhou\net al., 2023), we notice a research gap in the thor-\nough examination of prompting and score compo-\nsition in the domain of NLG metrics, especially for\nopen-source generative LLMs.\nThe Eval4NLP 2023 shared tasks aims to fill this\ngap by disallowing participants to fine-tune mod-\nels and by restricting model usage to a fixed list\nof LLMs (see Figure 1). Hence, participants may\nonly vary how models are prompted, how scores\nare extracted, and how models are used in combina-\ntion. To make the task more inclusive, we consider\nlarge and small(er) LLM’s in two separate tracks.\nThis is different from shared tasks without model\nrestriction, where the largest models often perform\nbest, for example, the WMT metrics shared task\n(e.g. Freitag et al., 2022).\nThe goal of the shared task is to design evalu-\nation metrics for MT and summarization, which\nwe select as sub-tasks of NLG, while adhering to\nthe model restrictions. Our contributions are the\nfollowing:\n• We design a novel, restricted evaluation set-\nting that allows to focus on prompting and\nai/, https://github.com/promptslab/\nAwesome-Prompt-Engineering, https://github.com/\nDukeLuo/awesome-awesome-prompts, https://github.\ncom/snwfdhmp/awesome-gpt-prompt-engineering ,\nhttps://github.com/dqxiu/ICL_PaperList, https:\n//github.com/EgoAlpha/prompt-in-context-learning\nscore extraction in building evaluation met-\nrics. This might aid inexpensive development\nof new metrics without fine-tuning or could\nbenefit the selection of metric architectures\nwith fine-tuning.\n• We organized a CodaLab (Pavao et al., 2023)\n/ Codabench (Xu et al., 2022) competition\nwhere participants could submit their system\nscores in a dev- and test-phase. The dev-phase\nhas received 44 participant registrations, of\nwhich 9 teams have submitted contributions to\nthe test-phase leaderboard and system papers.\nThis paper summarizes their approaches and\nfindings and presents their final ranking.\n• We collect a novel dataset from Wikipedia arti-\ncles created past the 15.07.2023 with the goal\nof minimizing the use of data that has been\nused to pre-train LLaMA2 (Touvron et al.,\n2023) released on 17.07.2023. This is because\nsome of the allowed models are fine-tuned ver-\nsions of LLaMA2.\n• In line with the Eval4NLP 2021 shared task\n(Fomicheva et al., 2021), we consider the ex-\nplainability of the designed metrics. The gen-\nerative nature of LLMs allows to return nat-\nural language or formatted explanations of\nits output. While these explanations are not\nnecessarily faithful, they also offer value if\n118\nthey are plausible (Leiter et al., 2023a) or\nmight support the generation process itself\n(Wei et al., 2022).\nOur paper is structured into 8 sections. §2 gives\nan overview of how our shared task is related to\nother competitions. §3 describes the competition\nsetup and §4 / §5 describe the datasets and annota-\ntion process for the test phase respectively. In §6,\nwe highlight the approaches tested by the partici-\npants, especially those for the test set submissions.\n§7 presents the final scores of the participants on\nthe test set and further analyses. Finally, §8 dis-\ncusses future work and provides a conclusion.\n2 Related Work\nIn this paragraph, we describe other work that is\nrelated to our shared task. In specific, we give a\nbrief overview of evaluation metrics, highlight the\nrecent development on metrics that are based on\ngenerative LLMs and describe related shared tasks.\nNLG evaluation metrics The evaluation of NLG\nsystems is necessary to compare them to other Sys-\ntems and generally evaluate their applicability in\nintended scenarios. Manual/human evaluation is\nexpensive, time consuming and often infeasible for\nlarger datasets. Hence, automatic metrics are con-\nstructed. Many early metrics like BLEU (Papineni\net al., 2002) and ROUGE (Lin, 2004) measure the\nlexical overlap between the generation and a human\nwritten reference. Metrics that use manually anno-\ntated references are called reference-based, while\nmetrics that evaluate the generation quality based\non the source text are called reference-free (in MT\nalso Quality Estimation, QE). The early metrics\nthat are based on lexical overlap have limitations\nin their ability to capture semantics of generated\ntext (e.g. Reiter, 2018). For example, a generation\nmight not be graded as good if it uses paraphrases\nof the reference texts. Newer metrics are usually\nbased on language models that are able to embed\nthe meanings of tokens (e.g. Zhang et al., 2020;\nZhao et al., 2019; Sellam et al., 2020; Rei et al.,\n2020). These metrics achieve strong correlations to\nhuman judgments of generation quality (e.g. Fre-\nitag et al., 2022). Embedding based metrics have\nalso enabled reference-free evaluation. This has\nthe added benefit of no longer needing human ref-\nerence generations and therefore enables further\nuse cases, such as checking generation quality on\nthe fly (e.g. Zerva et al., 2022), training with met-\nrics as supervision signal (e.g. Wu et al., 2018) and\nusing metrics during decoding (Fernandes et al.,\n2022). However, the usage of black-box systems in\nthe evaluation process also poses new challenges.\nFor example, it can be difficult to understand why\nmetrics exhibit certain behavior, they might lack\nrobustness and fail in unexpected scenarios and\nthey might show social biases (e.g. Leiter et al.,\n2023a). Surveys on NLG metrics are presented by\n(e.g. Celikyilmaz et al., 2021; Sai et al., 2022).\nGeneration-based evaluation metrics Related\nwork includes other generation-based metrics. Be-\nginning with PRISM (Thompson and Post, 2020)\nand BARTScore (Yuan et al., 2021), generation-\nbased metrics have shown strong performance.\nThese two metrics use the generation probabil-\nity of paraphrases or translations as metric scores.\nNewer work that follows the same principle with\nmore high-performing LLMs has shown improved\nscores (e.g. Fu et al., 2023). Another branch of\ngeneration-based metrics has originated with recent\nGPT models and shows that models can directly\nperform the task of grading machine generated text\nfrom in-context task descriptions (e.g. Kocmi and\nFedermann, 2023; Chiang and Lee, 2023; Fu et al.,\n2023; Xu et al., 2023b; Yang et al., 2023; Lu et al.,\n2023). We will refer to these metrics as output-\nbased. Here, the rating is usually returned directly\nin the generated output text or constructed from it.\nAnother branch of these models employs genera-\ntive LLMs for ranking between better and worse\ngenerations (Zheng et al., 2023; Shen et al., 2023;\nJi et al., 2023).\nThis recent surge of approaches has motivated\nour shared task. During the runtime of the shared\ntask, other state-of-the-art approaches have been\npublished (e.g. Fernandes et al., 2023). The sys-\ntems submitted to our competition are different\nfrom most generation-based metrics in thoroughly\nexploring the usage of fixed recent open-source\nLLMs since ChatGPT without the usage of fine-\ntuning.\nEvaluation Shared Tasks Our shared task is also\nrelated to other shared tasks that consider the evalu-\nation of evaluation metrics for NLG, especially for\nMT and summarization. For MT, the established\nWMT workshop comprises multiple shared tasks\non MT evaluation. Especially, the WMT metrics\nshared task (e.g. Mathur et al., 2020; Freitag et al.,\n2021b, 2022) and the WMT shared task on quality\nestimation (e.g. Specia et al., 2020, 2021; Zerva\net al., 2022) are related to ours. The main track of\n119\nFigure 2: Schematic overview of possible approaches to compute scores from a generative LLM. Zero-shot\napproaches do not present examples in the prompt, while few-shot approaches present them. Chain-of-though (Wei\net al., 2022) approaches trigger the LLM to generate an explanation of its process before returning the final score.\nFine-grained approaches, e.g. Fernandes et al. (2023), first construct a detailed error analysis and then construct a\nfinal score from them. Translation probability approaches, e.g. Fu et al. (2023), use the probability of generating\na paraphrase as a translation. In a majority vote approach the results from multiple prompts could be combined.\nSelf-refinement approaches could trigger a model multiple times to refine its output.\nthe WMT metrics shared task considers the system-\nand segment-level evaluation quality of MT metrics\n— that is, how well can metrics reflect the quality\nof whole MT systems or single segment transla-\ntions. Recent years also put a focus on evaluating\nthe robustness of metrics towards certain linguistic\nphenomena. The main track of the WMT metrics\nshared task consists of a reference-based evalua-\ntion, i.e., metrics compare the machine translation\nto human-written reference translations. Recent\neditions also contain a track for reference-free eval-\nuation, where submitted metrics should directly\ncompare the machine translation to its source text.\nSince 2021, the WMT metrics shared task has ac-\nquired its test data using the fine-grained MQM\nevaluation scheme (Lommel et al., 2014; Freitag\net al., 2021a) that has been shown to be more accu-\nrate than crowd-sourced direct assessment annota-\ntions. The WMT shared task on quality estimation\nsets its main focus on the reference-free evaluation\nof machine translations. In recent years, their test\nsets are also annotated with MQM. Additionally,\nthe quality estimation workshop has, for example,\nconducted tasks on word-level error prediction and\nspan-level error severity prediction.\nLike the WMT QE shared task, our task is the\nreference-free evaluation of machine translations.\nThe biggest difference of our shared task is that we\nfix the allowed models. That means, participants\nmay only use models from a list we provide to them.\nHence, participants have to focus on a thorough ex-\nploration of prompting and score extraction rather\nthan fine-tuning and dataset creation. A second\ndifference is that we include summarization as a\nsubtask. As a third difference, our shared task has\na subtrack to evaluate explanations that are created\nas a byproduct of scoring with generative LLM’s\nfor plausibility. This last point offers parallels to\nthe Eval4NLP 2021 shared task (Fomicheva et al.,\n2021) and its successor subtask at the WMT 2022\nshared task (Zerva et al., 2022) on quality estima-\ntion. These tasks treated human word-level error\nannotations as explanations of translation quality\nand evaluated their correlations to manual anno-\ntations. In our subtask, we allow for any kind of\nexplanation. Background information on explain-\n120\nability for machine translation metrics can be found\nin Leiter et al. (2023a).\n3 Shared Task Setup\nAs described in §1, the goal of our shared task is to\nleverage generative LLMs as (explainable) metrics\nfor MT and summarization.3 Thereby, participants\nare not allowed to fine-tune their models and only\ncertain models are allowed. Figure 1 shows the\ngeneral setup of using generative LLMs as metrics,\nillustrated with an example from MT. The figure\nshows that final scores could be constructed from\nthe generated model output or from other variables\ninvolved in the inference process. Specifically, re-\ncent work on prompting and metrics offer a wide\nrange of possibilities to influence score construc-\ntion even without fine-tuning. Some of them are\nshown in Figure 2.\nLLM sizes We organize two tracks based on the\nmodel sizes. Models smaller than 25B parameters\nare considered as small, and models bigger than\n25B parameters as large. Table 1 gives an overview\nof the allowed models. We mainly choose these\nmodels based on their good average performance\non the Huggingface Open LLM Leaderboard.4 For\nPlatypus2, Guanaco and WizardLM, we use 4-bit\nquantized versions with GPTQ (Frantar et al., 2023)\nto lower the system requirements to run them. Of\nthese models, only the Guanaco model was explic-\nitly fine-tuned with multilingual data. The models\nWizard, Nous and Guanaco were allowed for use\nfrom the start of the competition, while the other\n3 models were added to the list 20 days later. In\nanother track, we explore the explanatory value of\nexplanations created as a byproduct of the scoring\nprocess (see §7).\nPhases Our shared task was conducted in two\nphases. First, we hosted a dev-phase on CodaLab5\n(Pavao et al., 2023) from 07.08.23 to 30.09.23. In\nthis phase, participants were developing their ap-\nproaches and could already evaluate their scores\non a leaderboard. While the standing in the dev-\nphase does not influence the ranking of the shared\ntask, the phase aided the creation of a competi-\ntive atmosphere, acted as an advertisement for the\ncompetition and allowed us to gauge the number of\n3We treat MT and summarization as separate tracks.\n4https://huggingface.co/spaces/HuggingFaceH4/\nopen_llm_leaderboard\n5https://codalab.lisn.upsaclay.fr/\ncompetitions/15072\ninterested participants. The main part of the compe-\ntition was the test-phase conducted from 26.09.23\nto 01.10.23. Due to performance problems and\nunforeseen issues with extending the competition\nsetup on CodaLab, the test phase was migrated to\nits successor Codabench 6 (Xu et al., 2022). Sub-\nmissions to the dev-phase and test-phase both had\nto contain at least a file with newline separated\nscores that grade each sample of our datasets. The\ntest-phase additionally required to enter a team\nname, to indicate the track for each submission\nand to provide additional files with (1) a short sys-\ntem description, (2) newline separated prompts for\neach input, and (3) optionally newline separated\nexplanations.\nWe describe the shared task datasets in §4.\n4 Datasets\nDuring the dev-phase of our shared task, we pro-\nvided participants with a train- and a dev-set. For\nthe test-phase, we further added a test-set.\nTrain- & Dev-set Our train- and dev-sets are\nconstructed from two datasets. For MT, we select\nthe en-de and zh-en MQM partitions of the WMT\n2022 metrics shared task (Freitag et al., 2022). For\nsummarization, we select SummEval (Fabbri et al.,\n2021). We conduct our task in a reference-free\nsetting, that is, we do not provide human written\nreference translations or summaries. Hence, we\nremove the references provided with WMT and\nSummEval. SummEval has separate scores for rel-\nevance, factuality, coherence and consictency for\neach sample. We construct a single score per ex-\nample by averaging these separate scores. Further\nchanges to the original datasets include the split\ninto train- and dev-partitions as well as shuffling.\nIn the dev phase participants could experiment with\ngeneralizable (prompting) approaches.\nTest-set We collect a novel test set for the test-\nphase of our shared task. It consists of 3 language\npairs for MT: en-de, en-es, en-zh and a summa-\nrization part. We only choose high-resource lan-\nguages, as the LLaMA(2)-based models have seen\nlimited multilingual data during their pre-training\nand fine-tuning. Hence, high-resource languages\ncan indicate an upper bound of what these models\ncan achieve without further fine-tuning. To reduce\nthe possibility that our chosen LLMs were trained\n6https://www.codabench.org/competitions/1359/\n121\nMode Release Date Track\nPlatypus2-70B-Instruct-GPTQ7 (Lee et al., 2023a) 11.08.23 Large\nGuanaco-65B-GPTQ8 (Dettmers et al., 2023) 25.05.23 Large\nWizardLM-13B-V1.1-GPTQ9 (Xu et al., 2023a) 07.07.23 Small\nNous-Hermes-13b10 03.06.23 Small\nOpenOrca-Platypus2-13B11 (Lee et al., 2023b; Mukherjee et al., 2023) 11.08.23 Small\norca_mini_v3_7b12 (Mathur, 2023; Mukherjee et al., 2023) 07.08.23 Small\nTable 1: Generative LLMs whose usage was allowed in the Eval4NLP 2023 shared task.\non parts of the test set, we gather Wikipedia articles\ncreated after 15.07.23 as source texts.13\nFigure 3 shows the score distributions of our\ndatasets. We can see that all language pairs exhibit\na pattern of centering around values divisible by\n5. This makes sense, as MQM weighs major er-\nrors with 5 points. Also, in en-es, samples have\ngenerally received a higher score; i.e., fewer major\nerrors were annotated. Finally, our summarization\ndataset, which uses a combined annotation scheme\n(see §5) does not show this pattern.\n5 Annotation\nIn this section, we describe the annotation process\nof our dataset. For MT annotation, we hire one\nannotator per language pair: one Master student\nwho speaks Spanish as mother tongue with English\ncertifications, one NLP Bachelor student, who is a\nnative English speaker that lives in Germany since\nmany years, and one data and discourse studies\nMaster student, who is a native Chinese speaker\nwho uses English on a daily basis. For summariza-\ntion annotation, we hire one NLP Bachelor student\nas well as a data and discourse studies Master stu-\ndent with a prior master in linguistics. Both an-\nnotators annotated the same data. All annotators\ndemonstrated their suitability for the role in initial\ntest rounds with further applicants. The distribution\nof our final MT dataset is shown in Table 3. The\ntotal annotation costs were ca. 5000 C.\nWe use Google’s Anthea14 as annotation tool,\nbecause of its support for MQM annotations (Lom-\nmel et al., 2014; Freitag et al., 2021a). As we\nmostly annotate single sentences for MT, we mod-\nify Anthea to provide context via a Wikipedia URL\nthat can be consulted if annotators are unsure about\na translation. For summarization, annotations were\n13Limitations of this approach are discussed in §8\n14https://github.com/google-research/\ngoogle-research/tree/master/anthea\nconducted in a modified version of Anthea with a\nnew template (we show a screenshot of the UI in\nAppendix C).\nFor both data sets, we perform fine-grained an-\nnotations. In MT this has been shown to yield\nmore reliable human annotations than other anno-\ntation schemes (Freitag et al., 2021a). Also, the\nfine-grained annotations could be used later-on to\nverify automatically generated explanations. As we\nonly received 2 submissions for the explainability\ntrack, we do not consider apply this in this report.\nMT We construct the MT dataset from random\nsource sentences with a minimum length of 110\ncharacters, as tokenized by the NLTK sentence tok-\nenizer15. In a few cases, multiple sentences are con-\ncatenated due to missing spaces between dots. We\nobtain machine translations with 4 different trans-\nlation models (see Table 2). Further, we use MQM\nas annotation scheme and conducted the annotation\nprocess in multiple batches to allow for corrections\nin subsequent batches. The batch sizes varied be-\ntween 200 and 600 samples. For the first batch, we\nchanged parts of the process during the annotation.\nSpecifically, we had accidentally chosen an incor-\nrect tokenization for the first few samples of the\nfirst batch.16 This may have led to coarser annota-\ntion and to ignoring some punctuation issues. We\nstill use these samples, as punctuation errors only\nhave a very small weight in MQM and a coarser\nannotation does not change the severity assigned\nto errors. Hence, we assume that the impact on the\nMQM scores is minimal. Another change between\nannotation versions is that the first batch contains\n15https://www.nltk.org/api/nltk.tokenize.html\n16For the evaluation phase, we keep the annotations of the\nfirst batch, as small issues in source sentences should not\ninvalidate the possibility of creating good translations; instead,\nwe remove every sentence from the final dataset that has at\nleast one major source error. We do this as major source errors\nmight cause ambiguity in the annotation process. For example,\nif the source is unreadable, it is unclear which quality should\nbe expected from the translation.\n122\n/uni00000015/uni00000018\n/uni00000015/uni00000013\n/uni00000014/uni00000018\n/uni00000014/uni00000013\n/uni00000018\n/uni00000013\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000018/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000015/uni00000018/uni00000013/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n(a) en-de\n/uni00000015/uni00000018\n/uni00000015/uni00000013\n/uni00000014/uni00000018\n/uni00000014/uni00000013\n/uni00000018\n/uni00000013\n/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000016/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000018/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013\n/uni0000001a/uni00000013/uni00000013/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057 (b) en-es\n/uni00000015/uni00000018\n/uni00000015/uni00000013\n/uni00000014/uni00000018\n/uni00000014/uni00000013\n/uni00000018\n/uni00000013\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000015/uni00000013/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n(c) en-zh\n/uni00000015/uni00000013\n/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000014/uni00000015/uni00000013\n/uni00000014/uni00000017/uni00000013\n/uni00000014/uni00000019/uni00000013/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057 (d) summarization\nFigure 3: Score distributions of our datasets. The annotation process is described in §5.\nunordered sentences, while in the second version,\nall translations of a single source follow each other\n(in a random order). This has majorly improved\nthe annotation speed as annotators do not need to\nreread the source sentences anymore. Further, the\nannotators commented on difficult source texts in\nthe first batch. Therefore, in the following batches,\nwe pre-filter the Wikipedia source articles by their\nquality classes17 and keep only c-class and better\narticles. Furthermore, we employ languagetool18 to\nfilter for the grammatical correctness of the source\nsentences.\nTo verify the quality of the dataset, members of\nour team who are native speakers of the respective\n17https://en.wikipedia.org/wiki/Wikipedia:\nContent_assessment\n18https://languagetool.org/de\ntarget languages have annotated small subsets of\n30-50 samples of the datasets. Table 4 shows the\nagreement on these subsets. For en-es, either the\nMT models were more performant, the annotator\nmight have been missing some errors or annotating\nthem less strictly, as suggested by Figure 3.\nSummarization We select random sections from\nWikipedia that have a length of 150 to 800 tokens\nas measured by the tokenizer of bart-large-cnn.\nThe summarization models we use are listed in\nTable 2. To create a dataset that offers as much\nexplanatory value on the summary quality as possi-\nble, we perform a fine-grained evaluation inspired\nby MQM. However, we cannot simply reuse all\ncriteria of the MQM commonly used in MT, as\ninstead of fulfilling the criteria of adequacy, sum-\n123\nMT Models19 Summarization Models\nmbart50_en2m (Fan et al., 2021) sshleifer/distilbart-cnn-12-620 (Shleifer and Rush, 2020)\nmbart50_m2m (Fan et al., 2021) facebook/bart-large-cnn21 (Lewis et al., 2020)\nm2m_100_418M (Tang et al., 2021) google/bigbird-pegasus-large-bigpatent22 (Zaheer et al., 2020)\nm2m_100_1.2B (Tang et al., 2021) facebook/bart-large-xsum23 (Lewis et al., 2020)\nmT5_multilingual_XLSum24 (Hasan et al., 2021)\nTable 2: An overview of the translation and summarization models we have used to created our datasets.\nType Train Dev Test\nen-de 11046 7364 1425\nen-es - - 1834\nen-zh - - 1161 (1297)\nzh-en 15750 10500 -\nsummarization 320 1280 671 (825)\nTable 3: Number of samples in our datasets. In the case\nof the brackets, we filtered out potentially malformed\nexamples after the test phase was conducted.\nType Agreement\nen-de 0.458\nen-es 0.239\nen-zh 0.480\nsummarization 0.625\nTable 4: Kendall agreement between annotators. For\nMT, the agreement was calculated on 30-50 samples.\nFor summarization, it was calculated on 373 examples.\nmaries need to capture the most relevant facts (rele-\nvance) and only represent correct facts (factuality).\nSpecifically, we orient ourselves on the quality cri-\nteria for summaries by Dang (2005); Fabbri et al.\n(2021): relevance, factuality, and readability, where\nreadability includes the property of coherence and\nfluency. We note that readability is already cov-\nered to a large degree by the MT MQM annotation\nguidelines. We change them by removing adequacy\nand adding coherence. Coherence has the follow-\ning sub-categories: referential clarity, redundancy,\nstructure, and meaning. The meaning category\nrefers to cases where the summary changes the\nmeaning of the source text without hallucinating,\ne.g., by concatenating facts in the wrong order.\nOne common approach to determine the rele-\nvance and factuality of summaries is the pyramid\napproach (Nenkova and Passonneau, 2004). Here,\nsmall atomic facts of many human written refer-\nences are collected and ordered in a pyramid, based\non their occurrence count. Instead we introduce\na more resource efficient approach, where we use\na reference-free method for annotating the sum-\nmaries’ relevance and factuality. Inspired by Liu\net al. (2023c), who manually split the source text\ninto atomic facts, we leverage the NLTK sentence\ntokenizer to split the source text into enumerated\nsentences. In some cases, sentences were not split\ncorrectly. In sentences of the final test set, we have\ncorrected them manually. We treat each sentence as\na single fact.25 Next, we annotate the relevance of\neach of these facts, i.e., how likely would the anno-\ntator use the fact in a given sentence if they should\nwrite a summary themselves. Then, we annotate\nwhich source sentence is reflected in which part\nof the summary. By doing so, we can weigh the\nrelevance of each fact that appears in the summary.\nFinally, we annotate each fact not represented in\nthe original source text as a hallucination. Based\non these components, we build a heuristic that is\nnegative for bad summaries and positive for good\nsummaries. The equation is shown in Figure 4. α,\nβand γcan be chosen to determine the influence\nof each sub-score for relevance, hallucinations and\nreadability, respectively. There are many design\nchoices regarding the weighting of each component\nand different normalization approaches. We find\nthat these generally only have a small impact on\nthe final ranking of our shared task (see Appendix\nA). Longer summaries can contain more facts and\nwould hence receive higher scores in this heuris-\ntic. We address this issue by generating summaries\nof similar lengths using max token settings. The\nexample in Figure 5 shows this annotation process.\nLike with MT, we annotated in several batches.\nAfter the first batch, as for MT, we took measures to\nimprove the source quality and ordered the sources\nto allow for faster annotations. After a check on the\nannotation quality, some misunderstandings of the\n25Splitting each sentence into more granular facts, might\nfurther improve the fine-grained score composition but would\nrequire more effort in determining distinct facts.\n124\n∑\ni∈Facts in Summary\nα∗relevance(i) +β∗ |Hallucinated Characters|\n|Characters in the summary|+ γ∗MQM (1)\nFigure 4: A heuristic for fine-grained reference-free evaluation of summaries. We set α= 3, β = 5and γ = 1.\nFigure 5: An example of the summarization annotation process.\nannotation classes were uncovered and discussed.\nIn the final evaluation, we drop all examples labeled\nbefore this discussion, such that we keep a total\nof 671 samples. Further, one annotator showed\na larger annotation speed and a more consistent\nunderstanding of the task. In the test set, we use\nthe annnotations of this annotator.\nTable 4 shows the agreement between the an-\nnotators. It is high for relevance and factuality\nannotations and lower for the MQM part.\nEvaluation\nFollowing earlier WMT tasks on segment-level\nevaluation, we compute Kendall’s tau correlation\n(KENDALL, 1945) to compare the system gener-\nated scores to human scores. We further report the\nSpearman and Pearson correlations.26 Future work\n26For these evaluations of correlations, we use the imple-\nmentations of the python scipy library: https://scipy.org/\n125\ncould explore if the usage of other and possibly\nmore suited variants of Kendall, as suggested by\nDeutsch et al. (2023), might affect the rankings of\nour competition.\n6 Shared Task Approaches\nThe test phase of our shared task received submis-\nsions from 12 different teams, 9 of which submitted\nsystem papers. Here, we summarize the approaches\nof these 9 systems and announce their final stand-\nings. Table 5 gives an overview of the participating\nteams and of the tracks they are participating in.27\nThis table can be used as a mapping for the scores\nreported in §7.\nWe divide the approaches taken by the partic-\nipants into probability-based, output-based and\nagent-based.28 Besides their final approaches, the\nparticipants have explored a large number of possi-\nble variations. Afterwards, we introduce the base-\nline approaches, we compare the participants with.\nProbability-based Probability-based approaches\ncalculate how likely a paraphrase or translation\nof an input is generated with an LLM. Probabil-\nity based approaches are explored by Zhang et al.\n(2023) and Pradhan and Todi (2023). Zhang et al.\n(2023) define 10 different prompts to translate a\nsource sentence with an LLM. They combine this\napproach with demonstrating samples in the in-\nput prompt selected by (among others) SBERT\n(Reimers and Gurevych, 2019). Further, they use\nensembles to recombine the scores of multiple\nprompts and models. Pradhan and Todi (2023) use\nthe probability-based approach with own prompts\nand prompts designed by the authors of GPTScore\n(Fu et al., 2023).\nOutput-based All submitted papers explore the\ndirect usage of an LLM’s natural language output\nas score. Zhang et al. (2023) test the same sam-\nple selection and ensembling strategies described\nabove with 4 different prompts in an output-based\nsetting. Larionov et al. (2023) follow a simi-\nlar approach to Zhang et al. (2023) and retrieve\ndemonstration examples by finding similar exam-\nples with LABSE (Feng et al., 2022) embeddings in\nan output-based setting. Pradhan and Todi (2023)\n27While the first and last authors of Larionov et al. (2023)\nare members of the NLLG group, we did not share any inter-\nnal details that would have given them an advantage. They\ndeveloped their approach independently.\n28View §2 for the distinction of probability-based and\noutput-based.\ntry one approach in which they present a prompt\nthat triggers the prediction of a single score and one\napproach that triggers the model to first rate sum-\nmary qualities for consistency, coherence, fluency\nand relevancy. Then they aggregate these scores\nin 3 different ways. Baswani et al. (2023) quan-\ntize Orcamini themselves to run an even smaller\nmodel (which is close to violating the allowed set-\ntings of the shared task). They provide a detailed\nexplanation to their model that triggers it to pro-\nduce fine-grained scores and a combined score in\nthe same output. Kim et al. (2023) choose rating\nguidelines from related work — concretely, the hu-\nman guidelines (HG) for SummEval, the machine\nguidelines for G-Eval (Liu et al., 2023b) and eval-\nuation steps generated by GPT4 (OpenAI, 2023).\nThey test various adaptations to this prompt, ex-\nplore the usage of examples in the prompt and the\nusage of coarse-grained vs. fine-grained and aggre-\ngated scores. On the test set, they add a shortcut\nfor very bad summarizations and employ bucket-\ning for their scores. Akkasi et al. (2023) explore\nevaluating 6 different criteria over all model com-\nbinations. Kotonya et al. (2023) explore 8 prompt\ntypes: 3 base prompts and their extensions with\nchain-of-though (Wei et al., 2022), zero-shot and\nfew-shot settings. Mahmoudi (2023) explores vari-\nous zero-shot and few-shot settings with Orcamini.\nFinally, Mahmoudi (2023); Baswani et al. (2023)\ngenerate explanations as an additional request to\ntheir model.\nAgent-based While they also use an output-\nbased setup, we place Lu and Yu-Ting (2023) in\na separate group. They define 4 characters that\nshould be played by a model and a list of 10 prop-\nerties. For example they define “Internet Troll” as\na critical character or “Teacher” as more knowl-\nedgeable character, with the intention that different\nviewpoints can help to judge generation quality\nbetter. Then, they evaluate the combined 40 set-\ntings and use XGBoost (Chen and Guestrin, 2016)\nto combine their scores. While they did not add\ntheir top submissions to the final leaderboard they\npresent their reasonably good final scores in their\npaper.\nBaselines As baselines, we use the widely used\nmetrics BERTScore (with XLMR-large embed-\ndings) (Zhang et al., 2020), SBERT (Reimers and\nGurevych, 2019) cosine-similarity (with XLMR-\nlarge embeddings), SUPERT (Gao et al., 2020),\nGEMBA (Kocmi and Federmann, 2023) and\n126\nTeam Authors Tracks\nPradhan/Todi (Pradhan and Todi, 2023) S, SU\nKotonya et. al. (Kotonya et al., 2023) S, SU\nDSBA (Kim et al., 2023) S, L, SU\nHIT-MI&T Lab (Zhang et al., 2023) S, MT\nIUST_NLP_Lab (Mahmoudi, 2023) S, SU, E\nLTRC (Baswani et al., 2023) S, MT, SU, E\nNLLG (Larionov et al., 2023) L, MT, SU\nTaiwanSenior (Lu and Yu-Ting, 2023) S, MT\niML (Akkasi et al., 2023) S, L, SU\nTable 5: Overview of shared task submissions. The letters are abbreviations for the following tracks: S(mall model\ntrack), L(arge model track), M(achine)T(ranslation track), SU(mmarization track), E(xplainability track).\nComet-Kiwi-XXL (Rei et al., 2023). Further, we\ninclude one baseline for every allowed model that\nuses the DA score prompt of GEMBA (Kocmi and\nFedermann, 2023) (with a slight modification for\nsummarization). The models are further specified\nin Appendix D.\n7 Results and Analysis\nIn this section, we first report statistics of the shared\ntask. Then we will present and discuss the final\nsystem ranking. Note that we include submissions\nof participants on the test-set-leaderboard that did\nnot submit a system paper. However, we do not\ndescribe their approaches in §5. Lastly, we will\ndiscuss the implications of these results on the de-\nvelopment of generation-based metrics.\nStatistics The dev-phase on CodaLab has re-\nceived 44 registrations, 13 of which have submit-\nted their scores. In total, there have been 1048\nsubmissions on the dev-set suggesting that some\nparticipants might have optimized their method on\nthe dev-set. Especially, one participant submitted\n417 submissions on the dev set. The test-phase\non Codabench has received 21 registrations and\n248 submissions from 11 participants. We have\nrestricted the number of allowed submissions per\nday to 10. Allowing a higher number would en-\nable participants to optimize their approaches on\nthe test-set too much, such that the results would\nnot reflect the generalization capability anymore.\nOn the other hand, we wanted to give participants\nthe option to try out multiple approaches they de-\nsigned. Further, Codabench would sometimes fail\nto compute scores and still deduct one submission.\nHence, 10 submissions per day allows us to con-\ntinue in these cases. Two participants have used\nup a contingent of ≈50 submissions. Of the 11\ntest-phase participants, 9 have submitted a system\npaper. The first authors are from China, India (2),\nKorea, Taiwan, Canada, Iran, Germany and the\nUnited Kingdoms. That means, many authors are\nfrom developing countries. Also, many authors\nare students. Hence, their resource availability was\nlimited, leading many of them to opting for smaller\nmodels.\nCorrelation with humans Here, we present the\nresults that the participants achieve on the test sets.\nA mapping between team names and authors can\nbe found in Table 5. Table 6 shows the final rank-\ning of the small MT subtask. Compared to the\nother participants, Zhang et al. (2023) leads by a\nlarge margin on all correlation measures. Even sig-\nnificantly outperforming the recent COMET-kiwi-\nXXL and only being matched by GEMBA with\nGPT-4. This is surprising, as the scores they report\non the dev-set are not this strong. However, also on\nthe dev-set they beat the large model baselines that\nuse the 6 models we allow in the shared task. The\ntest-set approach that Zhang et al. (2023) report\nin their paper builds on ensembling probability-\nbased scores from prompts to OpenOrca-Platypus.\nThese prompts contain 3 up to the maximum num-\nber of possible example demonstrations. Future\nwork should explore whether their approach can\nuphold its strong performance across other datasets\nand settings. The ranking is then followed by vari-\nous baseline models and team LTRC.\nTable 7 shows the final ranking of the large MT\nsubtask. For this subtask, the baselines have not\nbeen beaten. Table 8 shows the final ranking of\nthe small summarization subtask. Kim et al. (2023)\nand Akkasi et al. (2023) lead this track. Both use\n127\nKendall Pearson Spearman\nTeam de zh es de zh es de zh es\nbaselineGEMBA 0.492 0.384 0.409 0.506 0.356 0.251 0.625 0.496 0.512\nHIT-MI&T Lab 0.491 0.375 0.417 0.655 0.528 0.453 0.656 0.511 0.553\nbaselineCometKiwiXXL 0.421 0.345 0.288 0.562 0.443 0.331 0.583 0.484 0.403\nbaselineBertscore 0.239 0.174 0.221 0.344 0.236 0.179 0.344 0.252 0.312\nbaselineSBERT 0.209 0.167 0.226 0.246 0.210 0.081 0.304 0.242 0.320\nLTRC 0.194 0.144 0.112 0.232 0.133 0.031 0.233 0.173 0.132\nbaselineNous 0.189 0.011 0.112 0.183 0.044 0.045 0.230 0.013 0.136\nbaselineOrcaPlaty 0.189 0.011 0.112 0.183 0.044 0.045 0.230 0.013 0.136\nseanstilwell 0.120 NaN NaN 0.164 NaN NaN 0.152 NaN NaN\nbaselineWizard 0.101 0.065 0.079 0.047 0.057 0.026 0.121 0.077 0.093\nbaselineOrcaMini 0.073 0.188 0.065 0.030 0.102 0.009 0.088 0.225 0.077\nTaiwanSenior 0.041 NaN NaN -0.037 NaN NaN 0.051 NaN NaN\nTable 6: Results of the small model track for MT. For our main metric Kendall, we write results that are significantly\nbetter than the following, with p≤0.05, as measured by a permute-both significance test (Deutsch et al., 2021).\nGEMBA was not included in the significance test. Teams with paper submissions are bolded.\nKendall Pearson Spearman\nTeam de zh es de zh es de zh es\nbaselinePlaty_large 0.362 0.293 0.264 0.312 0.270 0.129 0.445 0.364 0.320\nbaselineGuanaco_large 0.350 0.219 0.241 0.344 0.176 0.125 0.445 0.273 0.300\nNLLG 0.245 0.139 0.179 0.257 0.196 0.155 0.335 0.190 0.238\nkaiwalya_large 0.174 0.113 0.125 0.161 0.141 0.052 0.209 0.138 0.147\nTable 7: Results of the large model track for MT. For our main metric Kendall, we write results that are significantly\nbetter than the following, with p≤0.05, as measured by a permute-both significance test (Deutsch et al., 2021).\nTeams with paper submissions are bolded.\n128\nTeam kd ps sp\nDSBA 0.633 0.783 0.782\niML 0.615 0.763 0.772\nbaselineBertscore 0.578 0.771 0.765\nIUST_NLP_Lab 0.573 0.722 0.722\nbaselineOrcaMini 0.560 0.681 0.706\nbaselineSupertMpnet2 0.554 0.736 0.747\nbaselineOrcaPlaty 0.552 0.666 0.674\nbaselineNous 0.552 0.666 0.674\nKotonya et. al. 0.546 0.680 0.682\nLTRC 0.531 0.691 0.679\nbaselineSupertFull 0.516 0.686 0.706\nbaselineSupert5 0.492 0.654 0.678\nbaselineSBERT 0.465 0.625 0.645\nPradhan/Todi 0.436 0.032 0.610\nbaselineWizard 0.411 0.534 0.536\nHaaland 0.221 0.514 0.280\nTable 8: Results of thesmall model track for summariza-\ntion. kd stands for Kendall, ps stands for Pearson and\nsp stands for Spearman. For our main metric Kendall,\nwe write results that are significantly better than the\nfollowing, with ≤0.05, as measured by a permute-both\nsignificance test (Deutsch et al., 2021). Teams with\npaper submissions are bolded.\nTeam kd ps sp\niML 0.612 0.738 0.768\nDSBA 0.603 0.756 0.766\nbaselinePlaty_large 0.600 0.740 0.753\nNLLG 0.471 0.643 0.638\nbaselineGuanaco_large 0.402 0.492 0.504\nTable 9: Results of the large model track for summariza-\ntion. kd stands for Kendall, ps stands for Pearson and\nsp stands for Spearman. For our main metric Kendall,\nwe write results that are significantly better than the\nfollowing, with ≤0.05, as measured by a permute-both\nsignificance test (Deutsch et al., 2021). Teams with\npaper submissions are bolded.\ncarefully crafted prompts to achieve their results.\nTable 9 shows the final ranking of thelarge sum-\nmarization subtask. Here, Akkasi et al. (2023) is\nthe winning team. Interestingly, for MT and sum-\nmarization, the small models have beaten the large\nmodels. One potential reason might be that the\nlarge models take much longer to run and therefore\nthey could not be examined with the same care.\nFurther, it is interesting that the OrcaMini baseline\nand Mahmoudi (2023) beats many other models\ndespite its parameter count being the lowest of the\nallowed models’. Generally, many teams opted for\nthe usage of small models. Some teams only use\nthe OrcaMini model, due to resource constraints.\nThis highlights the importance of the inclusiveness\nof research in the metrics domain. We show a fur-\nther analysis of the impact of the summarization\nsubcategories in Appendix B.\nPerformance The best performing approaches\nof the participants achieve a similar Kendall corre-\nlation as our team members when we were testing\nthe inter-annotator agreement on a small subset\nof samples (see §3). This suggests that these ap-\nproaches are already close to the performance of\nnative speakers with little training with the annota-\ntion process (as compared to our main annotators\nwith a strong language background and more anno-\ntation experience on the task). This is an intriguing\nfinding and highlights the potential of current open\nsource models with and without fine-tuning. Es-\npecially, as many prompting approaches, like tree-\nof-thoughts or self-refinement still remain to be\nexplored. Further, it shows that for closed source\nmodels like ChatGPT or GPT4 similar opportu-\nnities may exist and lead to new state-of-the-art\nmetrics. The results also show that comparably\nsmall hardware can already be enough to create\nstrong new metrics.\nExplainability Only 2 participants (Baswani\net al., 2023; Mahmoudi, 2023) have submitted en-\ntries with complementary explanations to the Cod-\nabench leaderboard. Both directly prompted the\nmodel to give reasoning for the model’s decision.\nThus, we perform the human experiment on ex-\nplainability only on a small scale of 50 annotations\nfor randomly selected samples of our summariza-\ntion dataset. Two annotators of our team were pre-\nsented with source, summary, MQM annotations\n(to help to identify problems), the scores of the par-\nticipants and the explanations of the participants.\nThey annotated which of two explanations they pre-\n129\nfer. One annotator preferred explanations of one\nsystem, lets call it A, in 27 cases and explanations\nof the other in 23 cases. The other annotator pre-\nferred system A in 24 cases and the other system in\n26 cases. In these annotations the annotators agree\nin 56% of cases. These findings show that the anno-\ntators did not have a clear preference between the\nsystems. Also, we notice that many explanations\ntend to be vague and return texts such as “The sum-\nmary has a good coherence and fluency”. In some\ncases, the explanations correctly describe problems.\nWe show one example explanation of Baswani et al.\n(2023) in Table 10. Here, the explanation correctly\ncaptures the word repetition.\n8 Conclusion\nWe discuss future work and then summarize the\nshared task in a conclusion.\nFuture Work We have considered high resource\nlanguages for the MT task. Future work could\nevaluate low-resource languages, especially once\nmore generative LLMs are released that are trained\nacross a wide range of languages. Also, if this\nshared task topic is repeated in the future, we might\nencourage and set rewards for pipeline-based solu-\ntions. In other words, currently most approaches\nof the shared task are based on single prompts or\nprobability outputs; instead many interesting ap-\nproaches like tree of thoughts (Yao et al., 2023)\nexplore pipelines in which the output is generated\niteratively or in parallel. Future work might also\ncreate larger or more diverse datasets for our evalua-\ntion scheme. Another point is that our current work\nonly contains a small analysis of explainability that\nremained indecisive on the explanation quality be-\ntween two participants. This could be extended in\nfuture work.\nConclusion This work describes the Eval4NLP\n2023 shared task on prompting LLMs as explain-\nable metrics. We have constructed a fine-grained\ndataset for MT and summarization evaluation, with\na novel annotation scheme for the latter. Further,\nwe have organized a competition following the\nnovel restriction to specify allowed models and\ndisallow fine-tuning in a MT and summarization\nevaluation setting. By running a small and a large\nmodel track, we have enabled participation for par-\nticipants with fewer resources, leading to an inclu-\nsive shared task setting.\nThe top scores of the participants highlight a\nnumber of interesting findings that we summarize\nhere:\n• Small Models: The results on the test set\nshow that the best solutions built on small\nmodels outperform those that are built on\nlarger models. This is contradicting usual\npatterns and an interesting finding for metric\nefficiency.\n• Probability-based vs. Output based: The\nMT ranking is lead by a probability-based\nmethod, while the summarization ranking is\nlead by two prompt-based methods. For MT,\nthis could be caused by the models’ under-\nstanding of other languages being smaller than\nits capability of translation, therefore favoring\nparaphrasation based methods.\n• Simplicity helps: Many baseline systems\nachieved high ranks, despite using a simple\nprompting approach. Participants often report\nthat demonstrating examples reduced their per-\nformance. Hence, lean metrics are easier to\ndesign and can still be very powerful. The\nbest ranked systems, however, explore more\nintricate prompts.\nThe contributions of our participants highlight\nonce more how current LLMs can achieve state-of-\nthe-art performance, even without any task-specific\nfine-tuning.\nAcknowledgements\nWe thank our participants for the active contribu-\ntion and discussion. Further, we thank our an-\nnotators for their effort in creating our test sets.\nChristoph Leiter is financed by the BMBF project\n“Metrics4NLG”. Steffen Eger is financed by DFG\nHeisenberg grant EG 375/5–1.\nLimitations\nOne potential limitation of our work lies in the us-\nage of data from Wikipedia after 15.07. While the\nselected articles were indeed selected after 15.07,\ntexts could still be copied from other places, some\ntexts were automatically translated from other lan-\nguages were an entry existed and some texts might\neven be generated. Another issue of our work lies\nin the comparably small dataset with low agree-\nments for the small test conducted on the Spanish\nannotations. Due to time restrictions, we could\nnot do further evaluations. Still we believe that\n130\nSource Summary Explanation\nIn 1918, Wimble built a small ship at Hastings\nwith help from a friend and sailed to the West\nIndies to seek his fortune after his family faced\nfinancial hardship. In 1922, he acquired land in\nthe Bahamas which enabled him to begin trad-\ning with the English colonies in mainland North\nAmerica. He also acquired land in North Car-\nolina, which was formally granted to him by\nGeorge Burrington’s council on August 4, 1723.\nWimble later moved to Boston, Massachusetts\nwhere he married Rebecca Waters, the daughter\nof a prominent local, on March 26, 1724. Their\nfirst son, James, was born on December 20, 1724.\nHe owned land in the South End which he pre-\nsumably operated as a distillery. While in Boston,\nhe continued his business of moving trade goods\nbetween North Carolina and various British trade\nposts in the West Indies. This business enabled\nhim to increase his land holdings in North Car-\nolina and purchase a brigantine, which he named\n\"Rebecca\" after his wife. In 1932, Wimble lost\nhis ship and all of its cargo to a hurricane after\nbeing forced by Governor Woodes Rogers of the\nBahamas to use his ship to protect vessels and\nsalt ponds in Rum Cay. Wimble was forced to\nsell a portion of his belongings, land, and slaves\nto cover the loss and began the process of trying\nto collect damages from Woodes Rogers’ com-\nmandeering of his ship.29\nWimble is the subject of\nthis patent application.\nWimble is the son of\nWimble. Wimble is the\nson of Wimble. Wim-\nble is the son of Wimble.\nWimble is the son of\nWimble. Wimble is the\nson of Wimble. Wim-\nble is the son of Wim-\nble. Wimble is the son\nof Wimble. Wimble is\nthe son of Wimble.\nExplanation: The sum-\nmary is not relevant\nto the source text, as\nit contains no informa-\ntion about Wimble’s life,\nfamily, or business ven-\ntures. Instead, it repeats\nthe name \"Wimble\" mul-\ntiple times, which is not\na summary of the source\ntext.\nTable 10: Explanation generated with the approach by Baswani et al. (2023)\n. It correctly identifies the issue of the word Wimble repeating often.\n131\nour annotators were capable in their languages and\nthorough with their analysis of the samples. As\nanother limitation, pre-filtering with language tool\nand later on sorting out severe source errors, might\nmiss out on more subtle errors causing problems in\nthe test set.\nReferences\nAbbas Akkasi, Kathleen C. Fraser, and Majid Komeili.\n2023. Reference-free summarization evaluation with\nlarge language models. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nPavan Baswani, Ananya Mukherjee, and Manish Shri-\nvastava. 2023. Ltrc_iiith’s 2023 submission for\nprompting large language models as explainable met-\nrics task. In Proceedings of the 4th Workshop on\nEvaluation and Comparison for NLP systems.\nJonas Belouadi, Anne Lauscher, and Steffen Eger. 2023.\nAutomatikz: Text-guided synthesis of scientific vec-\ntor graphics with tikz.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2021. Evaluation of text generation: A survey.\nTianqi Chen and Carlos Guestrin. 2016. XGBoost. In\nProceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning. ACM.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631, Toronto,\nCanada. Association for Computational Linguistics.\nHoa Trang Dang. 2005. Overview of duc 2005.\nTirth Dave, Sai Anirudh Athaluri, and Satyam Singh.\n2023. Chatgpt in medicine: an overview of its ap-\nplications, advantages, limitations, future prospects,\nand ethical considerations. Frontiers in Artificial\nIntelligence, 6.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms.\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2021. A\nstatistical analysis of summarization evaluation met-\nrics using resampling methods. Transactions of the\nAssociation for Computational Linguistics, 9:1132–\n1146.\nDaniel Deutsch, George Foster, and Markus Freitag.\n2023. Ties matter: Modifying kendall’s tau for mod-\nern metric meta-evaluation.\nSteffen Eger, Christoph Leiter, Jonas Belouadi, Ran\nZhang, Aida Kostikova, Daniil Larionov, Yanran\nChen, and Vivian Fresen. 2023. Nllg quarterly arxiv\nreport 06/23: What are the most influential current ai\npapers? ArXiv, abs/2308.04889.\nAlexander R. Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. SummEval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Michael Auli, and Ar-\nmand Joulin. 2021. Beyond english-centric multilin-\ngual machine translation. Journal of Machine Learn-\ning Research, 22(107):1–48.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n878–891, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein,\nParker Riley, André F. T. Martins, Graham Neubig,\nAnkush Garg, Jonathan H. Clark, Markus Freitag,\nand Orhan Firat. 2023. The devil is in the errors:\nLeveraging large language models for fine-grained\nmachine translation evaluation.\nPatrick Fernandes, António Farinhas, Ricardo Rei,\nJosé G. C. de Souza, Perez Ogayo, Graham Neubig,\nand Andre Martins. 2022. Quality-aware decoding\nfor neural machine translation. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1396–1412,\nSeattle, United States. Association for Computational\nLinguistics.\nMarina Fomicheva, Piyawat Lertvittayakumjorn, Wei\nZhao, Steffen Eger, and Yang Gao. 2021. The\nEval4NLP shared task on explainable quality esti-\nmation: Overview and results. In Proceedings of\nthe 2nd Workshop on Evaluation and Comparison\nof NLP Systems , pages 165–178, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021a.\nExperts, errors, and context: A large-scale study of\nhuman evaluation for machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1460–1474.\n132\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, George Foster, Alon Lavie, and Ondˇrej\nBojar. 2021b. Results of the WMT21 metrics shared\ntask: Evaluating metrics with expert-based human\nevaluations on TED and news domain. In Proceed-\nings of the Sixth Conference on Machine Translation,\npages 733–774, Online. Association for Computa-\ntional Linguistics.\nSimon Frieder, Luca Pinchetti, Alexis Chevalier,\nRyan-Rhys Griffiths, Tommaso Salvatori, Thomas\nLukasiewicz, Philipp Christian Petersen, and Julius\nBerner. 2023. Mathematical capabilities of chatgpt.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nYang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT:\nTowards new frontiers in unsupervised evaluation\nmetrics for multi-document summarization. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1347–\n1354, Online. Association for Computational Linguis-\ntics.\nMohanad Halaweh. 2023. Chatgpt in education: Strate-\ngies for responsible implementation. Contemporary\nEducational Technology.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nYunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun,\nDongyu Pan, Baochang Ma, and Xiangang Li. 2023.\nExploring chatgpt’s ability to rank content: A prelim-\ninary study on consistency with human preferences.\nM. G. KENDALL. 1945. THE TREATMENT OF TIES\nIN RANKING PROBLEMS. Biometrika, 33(3):239–\n251.\nJoongHoon Kim, Sangmin Lee, Seung Hun, Saeran\nPark, Jiyoon Lee, Kiyoon Jeong, and Pilsung Kang.\n2023. Which is better? exploring prompting strategy\nfor llm-based metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nTom Kocmi and Christian Federmann. 2023. Large lan-\nguage models are state-of-the-art evaluators of trans-\nlation quality. In Proceedings of the 24th Annual\nConference of the European Association for Machine\nTranslation, pages 193–203, Tampere, Finland. Euro-\npean Association for Machine Translation.\nNeema Kotonya, Saran Krishnasamy, Joel R. Tetreault,\nand Alejandro Jaimes. 2023. Little giants: Exploring\nthe potential of small llms as evaluation metrics in\nsummarization in the eval4nlp 2023 shared task. In\nProceedings of the 4th Workshop on Evaluation and\nComparison for NLP systems.\nDaniil Larionov, Vasiliy Viskov, George Kokush,\nAlexander Panchenko, and Steffen Eger. 2023. Team\nnllg submission for eval4nlp 2023 shared task:\nRetrieval-augmented in-context learning for nlg eval-\nuation. In Proceedings of the 4th Workshop on Eval-\nuation and Comparison for NLP systems.\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023a.\nPlatypus: Quick, cheap, and powerful refinement of\nllms.\nAriel N. Lee, Cole J. Hunter, Nataniel Ruiz, Bleys\nGoodson, Wing Lian, Guan Wang, Eugene Pent-\nland, Austin Cook, Chanvichet V ong, and \"Teknium\".\n2023b. Openorcaplatypus: Llama2-13b model\ninstruct-tuned on filtered openorcav1 gpt-4 dataset\nand merged with divergent stem and logic dataset\nmodel. https://huggingface.co/Open-Orca/\nOpenOrca-Platypus2-13B.\nChristoph Leiter, Piyawat Lertvittayakumjorn,\nM. Fomicheva, Wei Zhao, Yang Gao, and Steffen\nEger. 2023a. Towards explainable evaluation metrics\nfor machine translation. ArXiv, abs/2306.13041.\nChristoph Leiter, Ran Zhang, Yanran Chen, Jonas Be-\nlouadi, Daniil Larionov, Vivian Fresen, and Steffen\nEger. 2023b. Chatgpt: A meta-analysis after 2.5\nmonths.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023a. Evaluating the logical\nreasoning ability of chatgpt and gpt-4.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\n133\nYixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Liny-\nong Nan, Ruilin Han, Simeng Han, Shafiq Joty,\nChien-Sheng Wu, Caiming Xiong, and Dragomir\nRadev. 2023c. Revisiting the gold standard: Ground-\ning summarization evaluation with robust human\nevaluation. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4140–4170, Toronto,\nCanada. Association for Computational Linguistics.\nArle Lommel, Aljoscha Burchardt, and Hans Uszkor-\neit. 2014. Multidimensional quality metrics (mqm):\nA framework for declaring and describing transla-\ntion quality metrics. Tradumàtica: tecnologies de la\ntraducció, 0:455–463.\nQingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang,\nTom Kocmi, and Dacheng Tao. 2023. Error analysis\nprompting enables human-like translation evaluation\nin large language models: A case study on chatgpt.\nYuan Lu and Lin Yu-Ting. 2023. Characterised llms\naffect its evaluation of summary and translation. In\nProceedings of the 4th Workshop on Evaluation and\nComparison for NLP systems.\nGhazaleh Mahmoudi. 2023. Exploring prompting large\nlanguage models as explainable metrics. In Proceed-\nings of the 4th Workshop on Evaluation and Compar-\nison for NLP systems.\nNitika Mathur, Johnny Wei, Markus Freitag, Qingsong\nMa, and Ondˇrej Bojar. 2020. Results of the WMT20\nmetrics shared task. In Proceedings of the Fifth Con-\nference on Machine Translation, pages 688–725, On-\nline. Association for Computational Linguistics.\nPankaj Mathur. 2023. orca_mini_v3_7b: An ex-\nplain tuned llama2-7b model. https://https://\nhuggingface.co/psmathur/orca_mini_v3_7b.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4.\nAni Nenkova and Rebecca Passonneau. 2004. Evaluat-\ning content selection in summarization: The pyramid\nmethod. In Proceedings of the Human Language\nTechnology Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHLT-NAACL 2004, pages 145–152, Boston, Mas-\nsachusetts, USA. Association for Computational Lin-\nguistics.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAdrien Pavao, Isabelle Guyon, Anne-Catherine Letour-\nnel, Dinh-Tuan Tran, Xavier Baro, Hugo Jair Es-\ncalante, Sergio Escalera, Tyler Thomas, and Zhen\nXu. 2023. Codalab competitions: An open source\nplatform to organize scientific challenges. Journal of\nMachine Learning Research, 24(198):1–6.\nAbhishek Pradhan and Ketan Kumar Todi. 2023. Under-\nstanding large language model based metrics for text\nsummarization. In Proceedings of the 4th Workshop\non Evaluation and Comparison for NLP systems.\nRicardo Rei, Nuno M. Guerreiro, José Pombal, Daan\nvan Stigt, Marcos Treviso, Luisa Coheur, José G. C.\nde Souza, and André F. T. Martins. 2023. Scaling\nup cometkiwi: Unbabel-ist 2023 submission for the\nquality estimation shared task.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nEhud Reiter. 2018. A structured review of the validity of\nBLEU. Computational Linguistics, 44(3):393–401.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish\nBhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-\nhan Xiong, Alexandre Défossez, Jade Copet, Faisal\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\nThomas Scialom, and Gabriel Synnaeve. 2023. Code\nllama: Open foundation models for code.\nAnanya B. Sai, Akash Kumar Mohankumar, and\nMitesh M. Khapra. 2022. A survey of evaluation\nmetrics used for nlg systems. ACM Comput. Surv.,\n55(2).\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nChenhui Shen, Liying Cheng, Yang You, and Lidong\nBing. 2023. Are large language models good evalua-\ntors for abstractive summarization?\nSam Shleifer and Alexander M. Rush. 2020. Pre-trained\nsummarization distillation.\n134\nLucia Specia, Frédéric Blain, Marina Fomicheva, Er-\nick Fonseca, Vishrav Chaudhary, Francisco Guzmán,\nand André F. T. Martins. 2020. Findings of the WMT\n2020 shared task on quality estimation. In Proceed-\nings of the Fifth Conference on Machine Translation,\npages 743–764, Online. Association for Computa-\ntional Linguistics.\nLucia Specia, Frédéric Blain, Marina Fomicheva,\nChrysoula Zerva, Zhenhao Li, Vishrav Chaudhary,\nand André F. T. Martins. 2021. Findings of the WMT\n2021 shared task on quality estimation. In Proceed-\nings of the Sixth Conference on Machine Translation,\npages 684–725, Online. Association for Computa-\ntional Linguistics.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2021. Multilingual translation from de-\nnoising pre-training. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 3450–3466, Online. Association for Computa-\ntional Linguistics.\nBrian Thompson and Matt Post. 2020. Automatic ma-\nchine translation evaluation in many languages via\nzero-shot paraphrasing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 90–121, Online.\nAssociation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-\nYan Liu. 2018. A study of reinforcement learning\nfor neural machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3612–3621, Brussels,\nBelgium. Association for Computational Linguistics.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a. Wizardlm: Empowering large lan-\nguage models to follow complex instructions.\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao\nSong, Markus Freitag, William Yang Wang, and Lei\nLi. 2023b. Instructscore: Explainable text generation\nevaluation with finegrained feedback.\nZhen Xu, Sergio Escalera, Adrien Pavão, Magali\nRichard, Wei-Wei Tu, Quanming Yao, Huan Zhao,\nand Isabelle Guyon. 2022. Codabench: Flexible,\neasy-to-use, and reproducible meta-benchmark plat-\nform. Patterns, 3(7):100543.\nHao Yang, Min Zhang, Shimin Tao, Minghan Wang,\nDaimeng Wei, and Yanfei Jiang. 2023. Knowledge-\nprompted estimator: A novel approach to explainable\nmachine translation assessment.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n17283–17297. Curran Associates, Inc.\nChrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat\nLertvittayakumjorn, José G. C. de Souza, Steffen\nEger, Diptesh Kanojia, Duarte Alves, Constantin\nOr˘asan, Marina Fomicheva, André F. T. Martins, and\nLucia Specia. 2022. Findings of the WMT 2022\nshared task on quality estimation. In Proceedings\nof the Seventh Conference on Machine Translation\n(WMT), pages 69–99, Abu Dhabi, United Arab Emi-\nrates (Hybrid). Association for Computational Lin-\nguistics.\nRui Zhang, Fuhai Song, Hui Huang, Jinghao Yuan,\nMuyun Yang, and Tiejun Zhao. 2023. Hit-mi&t\nlab’s submission to eval4nlp 2023 shared task. In\nProceedings of the 4th Workshop on Evaluation and\nComparison for NLP systems.\n135\nteam s_kd s_ps s_sp\nDSBA 0.623 0.675 0.772\niML 0.602 0.642 0.757\nIUST_NLP_Lab 0.566 0.678 0.712\nbertscore 0.546 0.711 0.729\nbaselineOrcaMini 0.545 0.640 0.684\nKotonya et.al. 0.543 0.745 0.675\nbaselineOrcaPlaty 0.527 0.589 0.650\nbaselineNous 0.527 0.589 0.650\nLTRC 0.522 0.655 0.666\nbaselineSBERT 0.438 0.524 0.611\nPradhan/Todi 0.424 0.030 0.594\nbaselineWizard 0.408 0.489 0.531\nHaaland 0.265 0.732 0.332\ncometXXL -0.009 0.091 -0.015\nbaselineSUPERT -0.028 -0.040 -0.040\nTable 11: Results of the small model track for summa-\nrization with Equation 6.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers.\nA Impact of the summarization heuristic\nHere, we consider the impact of using alternative\nheuristics for summarization, by studying their ef-\nfect on the ranking of summarization systems. The\nresults for Equation 6 are shown in Table 11. The\nresults for Equation 7 are shown in Table 12. We\ncan see that the top rankings remain the same.\nteam s_kd s_ps s_sp\nDSBA 0.551 0.490 0.695\niML 0.533 0.454 0.687\nISUT_NLP_Lab 0.512 0.546 0.649\nbertscore 0.497 0.569 0.663\nbaselineOrcaMini 0.485 0.517 0.612\nKotonya et.al. 0.480 0.690 0.604\nLTRC 0.476 0.534 0.609\nbaselineOrcaPlaty 0.462 0.446 0.581\nbaselineNous 0.462 0.446 0.581\nPradhan/Todi 0.422 0.023 0.591\nbaselineSBERT 0.384 0.371 0.539\nbaselineWizard 0.361 0.381 0.478\nHaaland 0.295 0.800 0.368\ncometXXL 0.015 0.159 0.021\nbaselineSUPERT 0.003 -0.018 0.004\nTable 12: Results of the small model track for summa-\nrization with Equation 7.\nB Impact of subcategories\nWe also study the impact of subcategories on the\nfinal ranking of summarization. That means, we\ncalculate the ranking with each of α, β, γ set to\n1, while the others are 0. The results are shown in\nTables 13, 14 and 15. Intriguingly, when only the\nMQM score is evaluated, the model by Haaland\nhas the highest correlation. However, they did not\nsubmit a system description or a system paper. Fur-\nther, all baselines in this setting perform relatively\nweak. The best baseline is comet, potentially as it\nhas been trained on MQM scores. The results for\nrelevance and hallucinations are rather unsurpris-\ning with one time DSBA being the winning team\nand the other time iML.\nC Screenshot of the annotation interface\nFigure 8 shows a screenshot of the Anthea annota-\ntion interface.\nD Model Details\nFor SBert, we use embeddings of XLM-R to in-\nclude multilinguality30. For SUPERT we report\nthe standard metric using bert-large-nli-stsb-mean-\ntokens31 with 5 and all source sentences as pseudo-\nreferences. Further, we upgrade SUPERT to use all-\n30https://huggingface.co/sentence-transformers/\nstsb-xlm-r-multilingual\n31https://huggingface.co/sentence-transformers/\nbert-large-nli-stsb-mean-tokens\n136\n∑\ni∈Facts in Summary\nα∗relevance(i) +β∗ |Hallucinated Characters|\n|Characters in the summary|+ γ∗MQM (2)\nFigure 6: A heuristic for fine-grained reference-free evaluation of summaries. Alternatively, we set α= 1, β = 1\nand γ = 1.\n∑\ni∈Facts in Summary α∗relevance(i)\n|Facts in Source| + β∗ |Hallucinated Characters|\n|Characters in the summary|+ γ∗MQM (3)\nFigure 7: An alternative heuristic for fine-grained reference-free evaluation of summaries. We set α= 1, β = 1and\nγ = 1. Further, we divide the relevance part by the number of facts in the source as normalization.\nFigure 8: The modified anthea annotation interface for summarization.\nteam s_kd s_ps s_sp\nHaaland 0.334 0.796 0.379\nDSBA 0.172 0.401 0.210\nKotonya et. al. 0.166 0.642 0.200\nIUST_NLP_LAB 0.164 0.472 0.200\ncometXXL 0.163 0.184 0.215\nPradhan/Todi 0.158 0.022 0.205\nLTRC 0.154 0.462 0.191\niML 0.146 0.362 0.174\nbaselineWizard 0.133 0.327 0.163\nbaselineOrcaMini 0.126 0.447 0.155\nbaselineOrcaPlaty 0.100 0.370 0.120\nbaselineNous 0.100 0.370 0.120\nbertscore 0.097 0.481 0.130\nbaselineSBERT 0.071 0.293 0.094\nbaselineSUPERT 0.023 -0.013 0.030\nTable 13: Results of the small model track for summa-\nrization, when only predicting MQM.\nteam s_kd s_ps s_sp\nDSBA 0.600 0.730 0.727\niML 0.596 0.720 0.722\nbertscore 0.562 0.687 0.724\nIUST_NLP_LAB 0.553 0.637 0.677\nbaselineOrcaMini 0.549 0.595 0.669\nbaselineOrcaPlaty 0.536 0.606 0.638\nbaselineNous 0.536 0.606 0.638\nKotonya et. al. 0.522 0.525 0.634\nLTRC 0.511 0.608 0.635\nbaselineSBERT 0.464 0.594 0.616\nPradhan/Todi 0.397 0.023 0.543\nbaselineWizard 0.393 0.479 0.491\nHaaland 0.164 0.280 0.197\nbaselineSUPERT -0.041 -0.059 -0.056\ncometXXL -0.065 -0.083 -0.092\nTable 14: Results of the small model track for summa-\nrization, when only predicting relevance.\n137\nteam s_kd s_ps s_sp\niML 0.516 0.599 0.606\nbertscore 0.471 0.480 0.595\nDSBA 0.454 0.576 0.537\nbaselineOrcaPlaty 0.432 0.483 0.506\nbaselineNous 0.432 0.483 0.506\nPradhan/Todi 0.414 0.041 0.532\nbaselineOrcaMini 0.406 0.417 0.487\nbaselineSBERT 0.403 0.477 0.525\nIUST_NLP_LAB 0.391 0.421 0.469\nLTRC 0.353 0.382 0.429\nKotonya et.al. 0.348 0.220 0.417\nbaselineWizard 0.267 0.331 0.323\nbaselineSUPERT -0.031 -0.043 -0.041\nHaaland -0.067 -0.127 -0.077\ncometXXL -0.198 -0.212 -0.265\nTable 15: Results of the small model track for summa-\nrization, when only predicting hallucinations.\nmpnet-base-v232, which improves its performance.\nFor COMET, we use comet-kiwi-xxlm 33, which\nachieved strong results on reference-free evalua-\ntion. Fort GEMBA we use the GEMBA library34\nand make small modifications to support GPT-4\nrequests. Finally, for BERTScore, we use xlm-\nroberta-large35.\n32https://huggingface.co/sentence-transformers/\nall-mpnet-base-v2\n33https://huggingface.co/Unbabel/\nwmt23-cometkiwi-da-xxl\n34https://github.com/MicrosoftTranslator/GEMBA\n35https://huggingface.co/xlm-roberta-large\n138"
}