{
  "title": "Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring",
  "url": "https://openalex.org/W3173781631",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2095590488",
      "name": "Yichi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160072504",
      "name": "Joyce Chai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963854351",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3182006211",
    "https://openalex.org/W4289364796",
    "https://openalex.org/W2114441423",
    "https://openalex.org/W2963367210",
    "https://openalex.org/W3099130708",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W2964164804",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2890902815",
    "https://openalex.org/W2963726321",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962684798",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2963447367",
    "https://openalex.org/W3108144224",
    "https://openalex.org/W2962744691",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3030193665",
    "https://openalex.org/W2981601849",
    "https://openalex.org/W2951973805",
    "https://openalex.org/W2100988719",
    "https://openalex.org/W2986357018",
    "https://openalex.org/W3034984114",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W3126325318",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1986014385",
    "https://openalex.org/W2885804027",
    "https://openalex.org/W2774005037",
    "https://openalex.org/W3092516542",
    "https://openalex.org/W2974759213",
    "https://openalex.org/W2740888779",
    "https://openalex.org/W4293566037",
    "https://openalex.org/W2565552376",
    "https://openalex.org/W3120543233",
    "https://openalex.org/W2805984364",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2111362669",
    "https://openalex.org/W3112356180",
    "https://openalex.org/W2963575117",
    "https://openalex.org/W2620626054",
    "https://openalex.org/W2950697717",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W2979727876",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W3100909505",
    "https://openalex.org/W2909303996",
    "https://openalex.org/W2295722319"
  ],
  "abstract": "Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem.On the AL-FRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%.To address this issue, this paper takes a closer look at task learning.In a departure from a widely applied end-toend architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT 1 (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure.On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability.In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art.The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4202‚Äì4213\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n4202\nHierarchical Task Learning from Language Instructions with UniÔ¨Åed\nTransformers and Self-Monitoring\nYichi Zhang Joyce Y. Chai\nComputer Science and Engineering\nUniversity of Michigan\nAnn Arbor, MI, USA\n{zhangyic, chaijy}@umich.edu\nAbstract\nDespite recent progress, learning new tasks\nthrough language instructions remains an ex-\ntremely challenging problem. On the AL-\nFRED benchmark for task learning, the pub-\nlished state-of-the-art system only achieves a\ntask success rate of less than 10% in an un-\nseen environment, compared to the human per-\nformance of over 90%. To address this issue,\nthis paper takes a closer look at task learning.\nIn a departure from a widely applied end-to-\nend architecture, we decomposed task learning\ninto three sub-problems: sub-goal planning,\nscene navigation, and object manipulation;\nand developed a model HiTUT1 (stands for\nHierarchical Tasks via UniÔ¨Åed Transformers)\nthat addresses each sub-problem in a uni-\nÔ¨Åed manner to learn a hierarchical task struc-\nture. On the ALFRED benchmark, HiTUT has\nachieved the best performance with a remark-\nably higher generalization ability. In the un-\nseen environment, HiTUT achieves over 160%\nperformance gain in success rate compared to\nthe previous state of the art. The explicit rep-\nresentation of task structures also enables an\nin-depth understanding of the nature of the\nproblem and the ability of the agent, which\nprovides insight for future benchmark develop-\nment and evaluation.\n1 Introduction\nAs physical agents (e.g., robots) start to emerge as\nour assistants and partners, it has become increas-\ningly important to empower these agents with an\nability to learn new tasks by following human lan-\nguage instructions. Many benchmarks have been\ndeveloped to study the agent‚Äôs ability to follow\nnatural language instructions in various domains\nincluding navigation (Anderson et al., 2018; Chen\net al., 2019), object manipulation (Misra et al.,\n1Source code available at https://github.com/\n594zyc/HiTUT\nPut(Mug, Sink)   TurnOn(Faucet)  TurnOff(Faucet)\nGoal Directive\nPlace a clean mug in \nthe coffee machine.\nGoto(Mug)\nPickup(Mug)\nGoto(Sink)\nClean(Mug)\nGoto(\nCoffeeMachine)\nPut (Mug, \nCoffeemachine)\nRotateLeft RotateLeft MoveAhead MoveAhead\n‚Ä¶\nPickup(Mug)     End\nSub-Goal Instruction: Go back towards the table.\nSub-Goal Instruction: Pick up the dirty mug.\n‚Ä¶\nSub-Goal Instruction: Wash the mug in the sink\n‚Ä¶\n‚Ä¶\n‚Ä¶\nSub-Goal Planning\nScene Navigation\nObject Manipulation\nFigure 1: An example task in ALFRED.\n2017; Zhu et al., 2017) and embodied reasoning\n(Das et al., 2018a; Gordon et al., 2018). Despite re-\ncent progress, learning new tasks through language\ninstructions remains an extremely challenging prob-\nlem as it touches upon almost every aspect of AI\nfrom perception, reasoning, to planning and actions.\nFor example, on the ALFRED benchmark for task\nlearning (Shridhar et al., 2020), the state-of-the-art\nsystem only achieves less than 10% task success\nrate in an unseen environment (Singh et al., 2020),\ncompared to the human performance of over 90%.\nMost previous works apply an end-to-end neural ar-\nchitecture (Shridhar et al., 2020; Singh et al., 2020;\nStorks et al., 2021) which attempt to map language\ninstructions and visual inputs directly to actions.\nWhile striving to top the leader board for end task\nperformance, these models are opaque, making it\ndifÔ¨Åcult to understand the nature of the problem\nand the ability of the agent.\nTo address this issue, this paper takes a closer\nlook at task learning using the ALFRED bench-\nmark. In a departure from an end-to-end ar-\n4203\nchitecture, we have developed an approach to\nlearn the hierarchical structure of task composi-\ntions from language instructions. As shown in\nFigure 1, a high-level goal directive (‚Äúplace a\nclean mug in the coffee machine‚Äù) can be de-\ncomposed to a sequence of sub-goals. Some sub-\ngoals involve navigation in space (e.g., Goto(Mug),\nGoto(Sink)) and others require manipulation of\nobjects (e.g., Pickup(Mug), Clean(Mug)). These\nsub-goals can be further decomposed into naviga-\ntion actions such as RotateLeft and MoveAhead,\nand manipulation actions such asPut(Mug, Sink),\nTurnOn(Faucet). In fact, such hierarchical struc-\nture is similar to Hierarchical Task Network (HTN)\nwidely used in AI planning (Erol et al., 1994).\nWhile this hierarchical structure is explicit and has\nseveral advantages in planning and making models\ntransparent, how to effectively learn such structure\nremains a key challenge.\nMotivated by recent work in multi-task learn-\ning (Liu et al., 2019a), we decomposed task learn-\ning in ALFRED into three sub-problems: sub-goal\nplanning, scene navigation, and object manipula-\ntion; and developed a model called HiTUT (stands\nfor Hierarchical Tasks via UniÔ¨Åed Transformers)\nthat addresses each sub-problem in a uniÔ¨Åed man-\nner to learn a hierarchical task structure. On the\nALFRED benchmark, HiTUT has achieved the best\nperformance with a remarkably higher generaliza-\ntion ability. In the unseen environment, HiTUT\nachieves over 160% performance gain in success\nrate compared to the previous state of the art.\nThe contributions of this work lie in the follow-\ning two aspects.\nAn explainable model achieving the new state-of-\nthe-art performance.By explicitly modeling a hi-\nerarchical structure, our model offers explainability\nand allows the agent to monitor its own behaviors\nduring task execution (e.g., what sub-goals are com-\npleted and what to accomplish next). When a failed\nattempt occurs, the agent can backtrack to previ-\nous sub-goals for alternative plans to execute. This\nability of self-monitoring and backtracking offers\nÔ¨Çexibility to dynamically update sub-goal planning\nat the inference time to cope with exceptions and\nnew situations. It has led to a signiÔ¨Åcantly higher\ngeneralization ability in unseen environments.\nA de-composable platform to support more in-\ndepth evaluation and analysis.The decomposi-\ntion of task learning into sub-problems not only\nmakes it easier for an agent to learn, but also pro-\nvides a tool for an in-depth analysis of task com-\nplexity and the agent‚Äôs ability. For example, one\nof our observations from the ALFRED benchmark\nis that the agent‚Äôs inability to navigate is a major\nbottleneck in task completion. Navigation actions\nare harder to learn than sub-goal planning and ma-\nnipulation actions. For manipulation actions, the\nagent can learn action types and action arguments\npredominantly based on sub-goals and the history\nof actions, while language instructions do not con-\ntribute signiÔ¨Åcantly to learning. The success of\nmanipulation actions also largely depends on the\nagent‚Äôs ability in detecting and grounding action\narguments to corresponding objects in the environ-\nment. These Ô¨Åndings allow a better understanding\nof the nature of the tasks in ALFRED and provide\ninsight to address future opportunities and chal-\nlenges in task learning.\n2 Related Work\nRecent years have seen an increasing amount of\nwork on in the intersection of language, vision and\nrobotics. One line of work particularly focuses on\nteaching robots new tasks through demonstration\nand instruction (Rybski et al., 2007; Mohseni-Kabir\net al., 2018). Originated in the robotics community,\nlearning from demonstration (LfD) (Thomaz and\nCakmak, 2009; Argall et al., 2009) enables robots\nto learn a mapping from world states to robots‚Äô\nmanipulations based on human‚Äôs demonstration of\ndesired robot behaviors. More recent work has also\nexplored the use of natural language and dialogue\ntogether with demonstration to teach robots new\nactions (Mohan and Laird, 2014; Scheutz et al.,\n2017; Liu et al., 2016; She and Chai, 2017; Chai\net al., 2018; Gluck and Laird, 2018).\nTo facilitate task learning from natural lan-\nguage instructions, several benchmarks using sim-\nulated physical environment have been made avail-\nable (Anderson et al., 2018; Misra et al., 2018;\nBlukis et al., 2019; Shridhar et al., 2020). In par-\nticular, the vision and language navigation (VLN)\nbenchmark (Anderson et al., 2018) has received a\nlot of attention. Many models have been developed,\nsuch as the Speaker-Follower model (Fried et al.,\n2018), the Self-Monitoring Navigation Agent(Ma\net al., 2019a; Ke et al., 2019), the Regretful Agent\n(Ma et al., 2019b), and the environment drop-out\nmodel (Tan et al., 2019). The VLN benchmark\nis further extended to study the Ô¨Ådelity of instruc-\ntion following (Jain et al., 2019) and examined\n4204\nto understand the bias of the benchmark (Zhang\net al., 2020). Beyond navigation, there are also\nbenchmarks that additionally incorporate object\nmanipulation to broaden research on vision and\nlanguage reasoning, such as embodied question an-\nswering (Das et al., 2018a; Gordon et al., 2018).\nThe work closest to ours is the Neural Modular\nControl (NMC) (Das et al., 2018b), which also\ndecomposes high-level tasks into sub-tasks and ad-\ndresses each sub-task accordingly. However, self-\nmonitoring and backtracking between sub-tasks is\nnot explored in NMC.\nThe ALFRED benchmark consists of high-level\ngoal directives such as ‚Äúplace a clean mug in the\ncoffee machine‚Äù and low level language instruc-\ntions such as ‚Äúrinse the mug in the sink‚Äù and ‚Äúturn\nright and walk to the coffee machine‚Äù to accom-\nplish these goals. In addition to language instruc-\ntions, it also comes with expert demonstrations\nof task execution in an interactive visual environ-\nment. We choose this dataset because its unique\nchallenges are closer to the real world, which re-\nquire the agent to not only learn to ground language\nto visual perception but also learn to plan for and\nexecute actions for both navigation and object ma-\nnipulation.\n3 Hierarchical Tasks via UniÔ¨Åed\nTransformers\nAs discussed in Section 1, task structures are inher-\nently hierarchical, which compose of goals and sub-\ngoals. Different sub-goals involve tasks of different\nnature. For example, navigation focuses on path\nplanning and movement trajectories, while manip-\nulation concerns more about interactions with con-\ncrete objects. Instead of end-to-end mapping from\nlanguage instructions to primitive actions (Shrid-\nhar et al., 2020; Singh et al., 2020; Storks et al.,\n2021), we decomposed task learning into three sep-\narate but connected sub-problems: sub-goal plan-\nning, scene navigation, and object manipulation,\nand developed a model called HiTUT (stands for\nHierarchical Tasks via UniÔ¨Åed Transformers) to\ntie these sub-problems together to form a hierarchi-\ncal task structure.\n3.1 Task Decomposition\nWe Ô¨Årst introduce some notations to describe the\ntask and the model. There are three types of infor-\nmation:\n- Language (L). We use G to denote a high-level\ngoal directive, e.g., ‚Äúplace a clean mug in the coffee\nmachine‚Äù and Ii to refer to a speciÔ¨Åc low-level\nlanguage instruction.\n- Vision (V). It captures the visual representation\nof the environment.\n- Predicates (P). Symbolic representations are de-\nÔ¨Åned to capture three types of predicates: sub-\ngoals ( sg), navigation actions ( an), and manip-\nulation actions ( am). Each sg has two parts\n(sgtype, sgarg) where sgtype is the type (e.g.,\nGoto) and sgarg is the argument (e.g., Knife).\nEach an speciÔ¨Åes a type ( an type) of action,\nfrom {RotateLeft, RotateRight, MoveAhead,\nLookUp, LookDown}. Each am has also two parts\n(am type, am arg) where am type is the action type\n(e.g., TurnOn); am arg is the action argument (e.g.,\nFaucet).\nSub-Goal Planning. Sub-goal planning acquires\na sequence of sub-goals sg1, ¬∑¬∑¬∑ , sgn to accom-\nplish the high-level goal G. We predict the type\nsgtype\ni and argument sgarg\ni separately to avoid the\ncombinatorial expansion of the output space. Previ-\nous work (Jansen, 2020) models sub-goal planning\nmerely from high-level goal directives without vi-\nsual grounding. These plans are Ô¨Åxed and thus not\nrobust to potential failures during execution and\nvariations of the visual environment. To overcome\nthese drawbacks, our sub-goal planning is done on\nthe Ô¨Çy after the previous sub-goal is executed in\nthe environment. More speciÔ¨Åcally, our sub-goal\nplanning objective is to learn a model ( Msg) that\ntakes the visual observation at the current step (vt),\nthe high-level goal directive (G), and a complete\nsub-goal history prior to the current step (sg<i) to\npredict the current sub-goal as follows:\nsgi ‚âú (sgtype\ni , sgarg\ni ) =Msg(vt, G, sg<i)\nThe predicted sub-goals serve as a bridge between\nthe high-level goal and the low-level predictions of\nnavigation actions and/or manipulation actions.\nScene Navigation. Navigation sub-goals only re-\nquire predictions for the types of navigation actions.\nThe objective is to learn a model for navigation\n(Mn) which takes the current visual observation\n(vt), current sub-goal ( sgi), language instruction\n(Ii), and the navigation action history up to the\ncurrent step ( an\n<j) to predict the next navigation\naction:\nan\nj ‚âú an type\nj = Mn(vt, Ii, sgi, an\n<j)\n4205\nBERT\nWord Embedding\nFC & LN FC & LN\n FC & LN\nFC & \nSoftmax\nType Arg\nPredicate to Word\nLN\nùí´ùí´\nPredicate History\nMask Selection\nQK       K K\nSum over heads \n& Softmax\nObject Detector Tokenizer\nclass\n‚Ñí\nLanguage Instruction\nEmb\nposition\nFC\nPosture\nFeatures\n(Navi. only)\nObject Detector\nMask\nùí±ùí±\nVisual Observation \nSub-Goal Planning     \nScene Navigation\nObject Manipulation\nùë†ùë†ùë†ùë†ùëñùëñ\nùë°ùë°ùë°ùë°ùë°ùë°ùë°ùë°, ùë†ùë†ùë†ùë†ùëñùëñ\nùëéùëéùëéùëéùëéùëé = ùêªùêªùêªùêªùêªùêªùêªùêªùêªùêª ùë£ùë£ùë°ùë°, ùê∫ùê∫, ùë†ùë†ùë†ùë†<ùëñùëñ\nùëéùëéùëóùëó\nùëõùëõ_ùë°ùë°ùë°ùë°ùë°ùë°ùë°ùë° = ùêªùêªùêªùêªùêªùêªùêªùêªùêªùêª(ùë£ùë£ùë°ùë°, ùêºùêºùëñùëñ, ùë†ùë†ùë†ùë†ùëñùëñ ‚äïùëéùëé<ùëóùëó\nùëõùëõ )\nùëéùëéùëóùëó\nùëöùëö_ùë°ùë°ùë°ùë°ùë°ùë°ùë°ùë°, ùëéùëéùëóùëó\nùëöùëö_ùëéùëéùëéùëéùëéùëé, ùëöùëöùëóùëó = ùêªùêªùêªùêªùêªùêªùêªùêªùêªùêª(ùë£ùë£ùë°ùë°, ùêºùêºùëñùëñ, ùë†ùë†ùë†ùë†ùëñùëñ ‚äïùëéùëé<ùëóùëó\nùëöùëö )\nùí±ùí±, ‚Ñí , ùí´ùí´\nParadigm\nFigure 2: The structure of HiTUT.\nObject Manipulation. For a manipulation sub-\ngoal, in addition to the type and argument of the\naction, the model ( Mm) also needs to generate a\nsegmentation mask (mj) on the current visual ob-\nservation to indicate which object to interact with\n(i.e., which object the argument is grounded to):\n(am\nj , mj) ‚âú (am type\nj , am arg\nj , mj)\n= Mm(vt, Ii, sgi, am\n<j)\nThe mask prediction is crucial because the action\nwill not be successfully executed with an incorrect\ngrounding even if am\nj is correctly predicted.\nAs described above, although the context of the\nthree sub-problems varies, each model has simi-\nlar input components from the space of ‚ü®V, L, P‚ü©.\nThis similarity inspires us to design an uniÔ¨Åed\nmodel to solve three sub-problems simultaneously.\n3.2 UniÔ¨Åed Transformers\nWe leverage the effective self-attention based\nmodel (Vaswani et al., 2017) to capture the cor-\nrespondence of different input sources as shown in\nFigure 2. We Ô¨Årst project the input from different\nmodalities into the language embedding space, and\nadopt a transformer to integrate the information\ntogether. Multiple prediction heads are constructed\non top of the transformer encoder to make pre-\ndictions for the sub-goal type and argument, the\naction type and argument, and object masks respec-\ntively. As the three sub-problems share the similar\ninput form, we solve them all together using a uni-\nÔ¨Åed model based on multi-task learning (Liu et al.,\n2019a).\nOur model differs from previous works (Shrid-\nhar et al., 2020; Singh et al., 2020) in the following\naspects. First, we do not apply recurrent state tran-\nsitions, but feed the prediction history as the input\nto each subsequent prediction. This may help better\ncapture correlations between predicates and other\nmodalities. Second, we do not use dense visual fea-\ntures from the scene, but rather the object detection\nresults. By doing this, we map different modalities\nto the word embedding space before feeding them\ninto the transformer encoder, thus taking advantage\nof the pre-trained language models. Third, we use a\npredicate embedding to share linguistic knowledge\nbetween predicate symbols and word embeddings.\nPredicate Embedding. We use the term predi-\ncates to refer to symbolic representations including\nsub-goal types, action types, and their arguments.\nWe map symbols to their corresponding natural\nlanguage phrases (e.g., AppleSliced is mapped to\na sliced apple). We then tokenize and embed the\ntokens using word embeddings, and take the sum\nof the embeddings to obtain the representation of\neach predicate.\nVision Encoding. We use a pre-trained object\ndetector (Mask R-CNN (He et al., 2017)) to en-\ncode visual information. Instead of dense features,\nwe simply use the detection results (class labels,\nbounding box coordinates and conÔ¨Ådence scores)\nas visual features. SpeciÔ¨Åcally, we use the top K\ndetected objects with a conÔ¨Ådence score higher\nthan 0.4 to form the visual features. The object\nclass labels share the same space with object argu-\nments, thus can be embedded into the same space.\nThe position information of an object is encoded\nby a 7-dimensional vector consisting of its coordi-\nnates, width and height of the bounding box and its\nconÔ¨Ådential score. This vector is Ô¨Årst mapped to\nthe same dimension as word embeddings by a liner\ntransformation, then added to the class embedding\nto form the Ô¨Ånal object representation.\nObject Grounding. HiTUT does not generate\nmasks by itself. Instead it chooses an object from\nthe K input objects and uses the corresponding\nmask generated by the object detector. This method\nmakes use of the strong prior learned from object\ndetection pre-training, so the model can focus on\n4206\nGoal Directive ùê∫ùê∫\nNavi. Actions ùëéùëéùëõùëõ Sub-Goals ùë†ùë†ùë†ùë†Observations ùë£ùë£ùë°ùë°\nModels ùëÄùëÄùë†ùë†ùë†ùë†, ùëÄùëÄùëõùëõ, ùëÄùëÄùëöùëöTemporal Transitions \nInstr. ùêºùêºùëñùëñ‚àí1 Instr. ùêºùêºùëñùëñ Instr. ùêºùêºùëñùëñ+1\nùëÄùëÄùëõùëõ ùëÄùëÄùëöùëö ùëÄùëÄùëõùëõ\nùëÄùëÄùë†ùë†ùë†ùë†\nMani. Actions ùëéùëéùëöùëö\nFigure 3: Overview of HiTUT where uniÔ¨Åed transformers\nfor sub-programs are integrated together .\nlearning the grounding task. A drawback is that the\nobject detector cannot be improved during training,\nand the performance of the detector determines\nthe upper bound of our model‚Äôs grounding ability.\nWe leave the exploration of more robust grounding\nmethod for future work.\nPosture Feature We use an additional posture\nfeature to assist scene navigation, which includes\nthe agent‚Äôs rotation (N, S, E, W) and its angle of\nsight horizon (discretized by 15 degree). The po-\nsitions are embedded and summed up to form the\nposture feature representation. The agent maintains\nits own posture in the form of a relative change to\nits initial posture instead of the absolute posture in\nthe environment, thus avoid using additional sen-\nsory data.\n3.3 Self-Monitoring and Backtracking\nThese uniÔ¨Åed transformers trained for sub-\nproblems are integrated together as shown in Fig-\nure 3. One important advantage of intermedi-\nate sub-goal representations is to facilitate self-\nmonitoring and backtracking which allows the\nagent to dynamically adjust the plan to cope with\nfailures during execution. As shown in Section 4,\nthis feature brings out the most remarkable perfor-\nmance gain compared to the state of the art.\nSelf-Monitoring. The world is full of uncer-\ntainties, and mistakes are inevitable. Based on\nthe learned model, the agent should be able\nto monitor its own behaviors and dynamically\nupdate its plan when the situation arises. Our\nexplicit representation of sub-goals allows the\nagent to self-check whether some sub-goals are\naccomplished. Particularly for manipulation\nsub-goals, it is feasible for the agent to detect\ntheir failures by simply monitoring whether all the\nTrain Validation Test\nSeen Unseen Seen Unseen\n#Scenes 108 88 4 107 8\n#Demonstrations 6,574 251 255 483 488\n#Annotations 21,023 820 821 1,533 1,529\n#Sub-goals 162k 6.4k 6.0k - -\n#Navi. Actions 983k 39k 35k - -\n#Mani. Actions 209k 8.3k 8.1k - -\nTable 1: Statistics of data distribution in ALFRED. The\nnumber of annotations is equivalent to the number of\ntasks in each split.\nmanipulation actions are successfully executed.\nFor example, Clean(Mug) cannot succeed if\nany of the actions along the path Put(Mug,\nSink), TurnOn(Facuet), TurnOff(Facuet),\nPickup(Mug) fail. When the agent detects the fail-\nure of a subgoal, for example, as shown in Figure 4\nthe manipulation sub-goal Pickup(Mug) fails, it\ncan reason about whether the previous sub-goal\n(i.e., Goto(Mug)) is successfully achieved.\nBacktracking. In classical AI, backtracking is\nthe technique to go back and try an alternative path\nthat can potentially lead to the goal. As shown in\nFigure 4, when Pickup(Mug) fails, the agent back-\ntracks to Goto(Mug) and tries a different sequence\nof primitive actions to accomplish this sub-goal.\nIn ALFRED, only based on the visual information\nwithout other sensory information (e.g., only ob-\nserving a mug without knowing how far it is), is it\ndifÔ¨Åcult to check whether a navigation sub-goal is\nsuccessfully achieved (e.g. whether a Mug is reach-\nable). So every time after trying a different path\nfor Goto(Mug), the agent will check whether the\nsubsequent manipulation action Pickup(Mug) is\nsuccessful. If it‚Äôs successful, the agent will move\non to the next sub-goal; otherwise the agent will\ncontinue to backtrack until a limit on the maximum\nnumber of attempts is reached. Our explicit rep-\nresentation of sub-goals makes this backtracking\npossible and has led to a signiÔ¨Åcant performance\ngain in unseen environments.\n4 Experiments\n4.1 Setting and Implementation\nDataset. We follow the train/validation/tests data\npartition proposed in ALFRED, where validation\nand test sets are further split into seen and unseen\nbased on whether the scene is shown to the model\nduring training. Each sub-goal planning step or a\nprimitive prediction step forms a data instance for\n4207\nPickup(Mug)Goto(Mug) Goto(Mug)\nRotateRight MoveAheadMoveAhead End\n Pickup(Mug)\n RotateLeft MoveAhead Pickup(Mug)\nPickup(Mug)\nEnd\nBacktracking\n‚Ä¶\nFigure 4: Illustration of self-monitoring and backtracking.\nModel\nValidation Seen Validation Unseen Test Seen Test Unseen\nSuccess Goal-Cond Success Goal-Cond Success Goal-Cond Success Goal-Cond\nSeq2Seq 3.70 (2.10) 10.00 (7.00) 0.00 (0.00) 6.90 (5.10) 3.98 (2.02) 9.42 (6.27) 0.39 (0.80) 7.03 (4.26)\nHAM - - - - 12.40 (8.20) 20.68 (18.79) 4.50 (2.24) 12.34 (9.44)\nMOCA 19.15 ( 13.60) 28.50 (22.30) 3.78 (2.00) 13.40 (8.30)22.05(15.10) 28.29 (22.05) 5.30 (2.72) 14.28 (9.99)\nHiTUT 25.24(12.20)34.85(18.52)12.44(6.85) 23.71(11.98) 21.27 (11.10)29.97(17.41)13.87(5.86) 20.31(11.51)\nHiTUT (Gonly) 18.41 (7.59) 25.27 (12.55) 10.23(4.54) 20.71 (9.56) 13.63 (5.57) 21.11 (11.00) 11.12 (4.50) 17.89 (9.77)\nHuman - - - - - - 91.00 (85.80) 94.50 (87.60)\nTable 2: Task and Goal-Condition success rates. The path length weighted version is in parentheses. The highest values per\ncolumn are in bold. ‚Äù-‚Äù denotes scores that are not reported. G only denotes only using the goal directive during evaluation\nwithout any sub-goal instructions.\nthe corresponding sub-problem. The number of\ndata instances are shown in Table 1.\nPre-training. We employ the pre-training fol-\nlowed by Ô¨Åne-tuning paradigm for both the object\ndetector and the main model. For the object detec-\ntor, we use a Mask R-CNN (He et al., 2017) model\npre-trained on MSCOCO (Lin et al., 2014), and\nÔ¨Åne-tune it on 50K images collected by replaying\nthe expert trajectories in the ALFRED train split.\nAs we observe that the model struggles on detecting\nsmall objects together with large receptacles, we\ntrain two networks to detect movable objects and\nbig receptacles separately. We use the pre-trained\nRoBERTa (Liu et al., 2019b) model to initialize the\ntransformer encoder.\nTraining. We perform imitation learning (super-\nvised learning) on the expert demonstrations. The\nground-truth labels of sub-goals and primitive ac-\ntions are obtained from the metadata. Different\ninput and output labels are organized for each sub-\nproblem respectively as described in Section 3. We\nuse the mask proposal that overlaps the most with\nthe ground truth mask as the mask selection la-\nbel if the intersection-of-union is above 50%. If\nthere is no valid mask proposals, the label is as-\nsigned to 0 as an indicator of non-valid grounding.\nWe optimize the cross-entropy loss between model\npredictions and the ground truth. We follow the\nmulti-task training schema in Liu et al. (2019a)\nwhere for each iteration, a batch is randomly sam-\npled among all the sub-problems, and the model is\nupdated according to the corresponding objective.\nMore details are in Appendix.\nEvaluation Metrics. ALFRED leverages an in-\nteractive evaluation in the AI2-THOR environment\n(Kolve et al., 2017). A task is considered success-\nful if all the goal conditions (e.g. the target object\nis placed on a correct receptacle and in a requested\nstate such as heated or cleaned etc.) are met. Three\nmeasures are used: (1) success rate (the ratio of\nsuccessfully completed tasks), (2) goal-condition\nrate (ratio of completed goal conditions), and (3) a\nweight version of these two rates which takes into\naccount of the length difference between the pre-\ndicted action sequence and the expert demonstrated\naction sequence (Shridhar et al., 2020).\nBaselines. We compare HiTUT to: (1) Seq2Seq -\nan LSTM-based baseline model with progress mon-\nitoring proposed in Shridhar et al. (2020); (2) HAM\n- a hierarchical attention model over enriched visual\ninputs (Nguyen and Okatani, 2020), and (3) MOCA\n- a modular approach which also uses a Mask R-\nCNN for mask generation (Singh et al., 2020) and\nachieved previous state-of-the-art performance.\n4208\nModel PickPut CoolHeatCleanSliceToggleAvg.\nSeen\nSeq2Seq 32 81 88 85 81 25 100 70\nMOCA 53 62 87 84 79 51 93 73\nHiTUT 81 77 95 100 83 81 97 88\nUnseen\nSeq2Seq 21 46 92 89 57 12 32 50\nMOCA 44 39 38 86 71 55 11 49\nHiTUT 71 69 100 97 91 78 58 81\nTable 3: Success rates of manipulation sub-goals on valida-\ntion sets. The highest values per fold are in bold.\n4.2 Evaluation Results\n4.2.1 Overall Performance of HiTUT\nWe Ô¨Årst evaluate the overall performance of the\nproposed framework as shown in Table 2. On\nthe testing data reported by the leader board, in\nseen environments, HiTUT achieves comparable\nperformance as MOCA. However in unseen en-\nvironments, HiTUT outperforms MOCA by over\n160% on success rate. This demonstrates our hi-\nerarchical task modeling approach has higher gen-\neralization ability compared to end-to-end models.\nSelf-monitoring and backtracking enabled by hier-\narchical task structures allows the agent to better\nhandle new situations. Remarkably, only based on\nhigh-level goal directives (i.e., HiTUT (G Only)) with-\nout using any sub-goal instructions, is HiTUT able\nto obtain a success rate of 11% in unseen environ-\nment, achieving 110% performance gain compared\nto MOCA. This result indicates that HiTUT can\nlearn prior task knowledge from the hierarchical\nmodeling process and apply that directly in new\nenvironment with some success. Nevertheless, our\nresults are far from human performance and there\nis still huge room for future improvement.\nTo have a better understanding of the problem,\nwe also conduct evaluations on sub-goals. The\nagent is positioned at the starting point of each sub-\ngoal by following the expert demonstration and the\nsuccess rate of accomplishing the sub-goal is mea-\nsured. HiTUT predicts Ô¨Årst a symbolic sub-goal\nrepresentation and then the action sequence to com-\nplete the sub-goal. As shown in Table 3, HiTUT\noutperforms previous models on almost all of the\nmanipulation sub-goals by a large margin. The per-\nformance gain is particularly signiÔ¨Åcant in unseen\nenvironment, which demonstrates the advantage of\nour explicit hierarchical task modeling in low-level\naction planning.\nValid Seen Valid Unseen\n#BT Success Goal-Cond Success Goal-Cond\nNo 10.5 (6.0) 18.4 (13.8) 5.2 (3.0) 13.5 (11.1)\n2 18.9 (9.9) 27.6 (18.0) 10.2 (5.9) 20.2 (13.6)\n4 23.1 (11.3) 32.9 (18.6) 12.9 (7.0) 22.7 (12.9)\n6 25.6 (12.0) 35.1 (18.5) 14.5 (7.4) 24.3 (12.3)\n8 27.2 (12.5) 37.0 (18.5) 16.2 (7.8) 25.9 (12.1)\nTable 4: Success rates w.r.t. the allowed maximum backtrack-\ning number (#BT).\nSeq2Seq MOCAOur model with different #backtracks\nno 1 2 4 6 8\nSeen 51 54 35 48 56 64 68 70\nUnseen 22 32 31 45 53 60 63 65\nTable 5: Success rate of the navigation sub-goal Goto with\nbacktracking .\n4.2.2 The Role of Backtracking\nWe conduct experiments to better understand the\nrole of self-monitoring and backtracking. We re-\npeat the task-solving evaluation with different lim-\nits on the allowed maximum number of backtrack-\ning. The agent only stops when the model pre-\ndicts to stop (i.e., predicts End) or it reaches the\nbacktracking limit. As shown in Table 4, as the\nlimit increases, the task/goal-condition success rate\nincreases accordingly. One thing notable is that\nthe gap between success rates (weighted and un-\nweighted) become larger when more backtrack at-\ntempts are allowed. This is within our expecta-\ntion because backtracking deviates from instruc-\ntion following navigation to goal-oriented explo-\nration, which usually takes more steps than the\nexpert demonstration.\nSince backtracking is particularly targeted to nav-\nigation sub-goals Goto (see Section 3.3), we further\nexamine the role of number of re-tries (i.e. back-\ntracks) in completing the sub-goal. As shown in\nTable 5, HiTUT reaches more targets when given\nmore opportunities to backtrack. The backtracking\nis most beneÔ¨Åcial in unseen environment.\n4.2.3 Complexity of Tasks\nTask decomposition provides a tool to enable better\nunderstanding of task complexity and agent‚Äôs abil-\nity. To do that, we replace different part of model\npredictions by the corresponding oracle sub-goals,\nactions, or masks, as shown in Table 6.\nUsing oracle sub-goals improves the success\nrate for 2%-6% (line SG), showing sub-goal plan-\nning is a relatively easy problem and the agent can\nperform reasonably well. After using the oracle\n4209\n1% 5% 15% 25%\n91\n92\n93\n94\n95\n96\n97\n98\n99Sub-Goal Type\nall (100% data)\nall\nno vision\nno goal directive\nsub-goal history only\n(a) Sub-Goal Type\n1% 5% 15% 25%\n40\n50\n60\n70\n80\n90Sub-Goal Arg\nall (100% data)\nall\nno vision\nno goal directive\nsub-goal history only (b) Sub-Goal Argument\n1% 5% 15% 25%\n72\n74\n76\n78\n80\n82Navi. Action Type\nall (100% data)\nall\nno vision\nno instruction (c) Navigation Action Type\n1% 5% 15% 25%\n95\n96\n97\n98\n99\n100Mani. Action Type\nall (100% data)\nall\nno vision\nno instruction\naction history only\n(d) Manipulation Action Type\n1% 5% 15% 25%\n93\n94\n95\n96\n97\n98\n99Mani. Action Arg\nall (100% data)\nall\nno vision\nno instruction\naction history only (e) Manipulation Action Argument\n1% 5% 15% 25%\n80\n82\n84\n86\n88\n90Mani. Mask Selection\nall (100% data)\nall\nno instruction (f) Manipulation Mask Selection\nFigure 5: Step-by-step prediction accuracies given the golden sub-goal/action history w.r.t. the proportion of training data on\nthe unseen validation set. Each solid line corresponds to a speciÔ¨Åc input conÔ¨Åguration. Dashed lines are the scores obtained\nusing 100% of training data.\nMethod Valid Seen Valid Unseen\nSuccess Goal-Cond Success Goal-Cond\nHiTUT 25.2 (12.2) 34.8 (18.5) 12.4 (6.8) 23.7 (12.0)\n+ Oracle\nSG 29.0 (15.6) 39.1 (21.3) 14.0 (7.6) 25.6 (12.7)\nN 75.0 (72.7) 78.0 (77.4) 57.9 (60.0) 67.7 (65.2)\nSG+N 79.2 (77.8) 84.0 (81.3) 64.2 (64.2) 72.0 (68.1)\nSG+N+M 89.0 (100) 90.0 (100) 80.5 (100) 83.7 (100)\nSG+N+GR 99.3 (99.0) 99.4 (99.1) 99.4 (99.3) 99.6 (99.6)\nTable 6: Success rates of HiTUT with different parts of pre-\ndictions replaced by oracle operations with expert demonstra-\ntions. N, M, SG and GR denote oracle navigation actions,\nmanipulation actions, sub-goals and object grounding (i.e.,\nmask generation) respectively.\nnavigation actions, the seen and unseen success\nrates are boosted by an absolute gain of 50% and\n46% respectively (line N), indicating that navigat-\ning to reach target objects is a particularly hard\nproblem and the agent performs poorly. When ora-\ncle sub-goals, navigation actions, and manipulation\nactions (only symbolic representations) are given\n(line SG+N+M), the task success is bounded by the\nperformance of the pre-trained object mask gener-\nator (i.e., visual grounding of the object). When\noracle object masks are given together with oracle\nsub-goals and navigation actions (line SG+N+GR)\nand the agent only needs to predict symbolic repre-\nsentation of manipulation actions, the performance\nis near perfect. These last two lines indicate that\npredicting the type and the argument of a manip-\nulation action is a rather simple problem in the\nALFRED benchmark while grounding action ar-\nguments to the visual environment remains a chal-\nlenging task.\nWe further examine the complexity of learning\nto solve sub-problems by evaluating the next-step\nprediction accuracy given the golden history under\ndifferent conditions as shown in Figure 5. The mod-\nels are trained and evaluated with different com-\nbinations of input and different amount of train-\ning data. We observe that excluding the visual\ninput does not hurt performance for sub-goal pre-\ndiction and manipulation action prediction (shown\nby a,b,d,e). This indicates that in ALFRED, pure\nsymbolic planning is often independent from visual\nunderstanding, which is consistent with the Ô¨Ånd-\nings in (Shridhar et al., 2020). However, this could\nbe an oversimpliÔ¨Åcation brought by the bias in the\ndataset rather than a true reÔ¨Çection of the physical\nworld. For example, next action prediction can be\nmade by remembering the correlation of predicates\ninstead of reasoning over vision and language, due\nto the lack of diversity of the task environments.\nRemoving language instructions causes a minimal\nperformance drop of 1%-2% on action prediction\ntasks, which brings up the question about the use-\nfulness of language instructions in this benchmark.\nFurthermore, the prediction accuracy is above 90%\nand 98% with only 5% training data for sub-goal\n4210\nand manipulation planning respectively, while the\nnavigation accuracy is only 82% given all the data.\nThis again supports the Ô¨Ånding that planning and\nperforming navigation actions is a much harder\nproblem than sub-goal planning and manipulation\nactions in ALFRED.\n5 Discussion and Conclusion\nThis paper presents a hierarchical task learning\napproach that achieves the new state-of-the-art per-\nformance on the ALFRED benchmark. The task\ndecomposition and explicit representation of sub-\ngoals enable a better understanding of the problem\nspace as well as the current strengths and limi-\ntations. Our empirical results and analysis have\nshown several directions to pursue in the future.\nFirst, we need to develop more advanced compo-\nnent technologies integral to task learning, e.g.,\nmore advanced navigation modules through either\nmore effective structures (Hong et al., 2020) or\nricher perceptions (Shen et al., 2019) to solve navi-\ngation bottleneck. We need to develop better repre-\nsentations and more robust and adaptive learning\nalgorithms to support self-monitoring and back-\ntracking. We also need to seek ways to improve\nvisual grounding, which is crucial to both naviga-\ntion and manipulation.\nSecond, we should also take a closer look at\nthe construction and objective of existing bench-\nmarks. How a benchmark is created and how truth-\nfully it reÔ¨Çects the complexity of the physical world\nwould impact the scalability and reliability of the\napproach in the real world. As for the objective,\nthere is a distinction between learning to perform\ntasks and learning to follow language instructions.\nIf the objective is the former, the agent should be\nmeasured by the ability to learn to accomplish high-\nlevel goal directives without being given speciÔ¨Åc\nlanguage instructions at the inference time. If the\nobjective is the latter, then the agent should be mea-\nsured by how faithful it follows human instructions\naside from achieving the goals, similar to (Jain\net al., 2019). We need to be clear about the objec-\ntives and develop evaluation metrics accordingly.\nFinally, when humans perform poorly in a com-\nplex task, we have the ability to diagnose the prob-\nlem and put more energy on learning the difÔ¨Åcult\npart. Physical agents should also have similar abil-\nities. In task learning, on the one hand, the agent\nshould be able to master simple sub-tasks from a\nfew data instances, e.g., through a few turns of inter-\nactions with humans (Karamcheti et al., 2020). On\nthe other hand, it should be aware of the bottleneck\nof its learning progress and proactively request for\nhelp when problems are encountered either dur-\ning learning or during deployment (She and Chai,\n2017). How to effectively design interactive and\nactive learning algorithms for the agent to learn\ncomplex and compositional tasks remains an im-\nportant open research question.\nAcknowledgments\nThis work is supported by the National Science\nFoundation (IIS-1949634). The authors would like\nto thank the anonymous reviewers for their valuable\ncomments and suggestions.\nReferences\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko S ¬®underhauf, Ian D. Reid,\nStephen Gould, and Anton van den Hengel.\n2018. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real en-\nvironments. In 2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt\nLake City, UT, USA, June 18-22, 2018, pages 3674‚Äì\n3683. IEEE Computer Society.\nB. D. Argall, S. Chernova, M. Veloso, and B. Browning.\n2009. A survey of robot learning from demonstra-\ntion. Robotics and autonomous systems, 57(5):469‚Äì\n483.\nValts Blukis, Yannick Terme, Eyvind Niklasson,\nRoss A. Knepper, and Yoav Artzi. 2019. Learning to\nmap natural language instructions to physical quad-\ncopter control using simulated Ô¨Çight. In CoRL.\nJoyce Y . Chai, Qiaozi Gao, Lanbo She, Shaohua Yang,\nSari Saba-Sadiya, and Guangyue Xu. 2018. Lan-\nguage to action: Towards interactive task learning\nwith physical agents. In IJCAI, pages 2‚Äì9.\nHoward Chen, Alane Suhr, Dipendra Misra, Noah\nSnavely, and Yoav Artzi. 2019. TOUCHDOWN:\nnatural language navigation and spatial reasoning\nin visual street environments. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019 ,\npages 12538‚Äì12547. Computer Vision Foundation /\nIEEE.\nAbhishek Das, Samyak Datta, Georgia Gkioxari, Ste-\nfan Lee, Devi Parikh, and Dhruv Batra. 2018a. Em-\nbodied question answering. In 2018 IEEE Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pages 1‚Äì10. IEEE Computer Society.\nAbhishek Das, Georgia Gkioxari, Stefan Lee, Devi\nParikh, and Dhruv Batra. 2018b. Neural modular\ncontrol for embodied question answering. In Con-\nference on Robot Learning, pages 53‚Äì62. PMLR.\n4211\nKutluhan Erol, James Hendler, and Dana S Nau. 1994.\nHtn planning: Complexity and expressivity. In\nAAAI, volume 94, pages 1123‚Äì1128.\nDaniel Fried, Ronghang Hu, V olkan Cirik, Anna\nRohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,\nand Trevor Darrell. 2018. Speaker-follower mod-\nels for vision-and-language navigation. In Advances\nin Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018,\nMontr¬¥eal, Canada, pages 3318‚Äì3329.\nKevin A Gluck and John E Laird. 2018. Interactive\ntask learning: Humans, robots, and agents acquir-\ning new tasks through natural interactions.The MIT\nPress.\nDaniel Gordon, Aniruddha Kembhavi, Mohammad\nRastegari, Joseph Redmon, Dieter Fox, and Ali\nFarhadi. 2018. IQA: visual question answering in\ninteractive environments. In 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018 ,\npages 4089‚Äì4098. IEEE Computer Society.\nKaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and\nRoss B. Girshick. 2017. Mask R-CNN. In IEEE In-\nternational Conference on Computer Vision, ICCV\n2017, Venice, Italy, October 22-29, 2017 , pages\n2980‚Äì2988. IEEE Computer Society.\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez\nOpazo, and Stephen Gould. 2020. A recurrent\nvision-and-language bert for navigation. arXiv:\nComputer Vision and Pattern Recognition.\nVihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish\nVaswani, Eugene Ie, and Jason Baldridge. 2019.\nStay on the path: Instruction Ô¨Ådelity in vision-and-\nlanguage navigation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1862‚Äì1872, Florence, Italy. Asso-\nciation for Computational Linguistics.\nPeter Jansen. 2020. Visually-grounded planning with-\nout vision: Language models infer detailed plans\nfrom high-level instructions. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2020, pages 4412‚Äì4417, Online. Association for\nComputational Linguistics.\nSiddharth Karamcheti, Dorsa Sadigh, and Percy Liang.\n2020. Learning adaptive language interfaces\nthrough decomposition. In Proceedings of the First\nWorkshop on Interactive and Executable Semantic\nParsing, pages 23‚Äì33, Online. Association for Com-\nputational Linguistics.\nLiyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtz-\nman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin\nChoi, and Siddhartha Srinivasa. 2019. Tactical\nrewind: Self-correction via backtracking in vision-\nand-language navigation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6741‚Äì6749.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-\nderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gor-\ndon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi.\n2017. Ai2-thor: An interactive 3d environment for\nvisual ai. arXiv preprint arXiv:1712.05474.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740‚Äì755. Springer.\nChangsong Liu, Shaohua Yang, Sari Saba-Sadiya,\nNishant Shukla, Yunzhong He, Song-Chun Zhu,\nand Joyce Chai. 2016. Jointly learning grounded\ntask structures from language instruction and visual\ndemonstration. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1482‚Äì1492, Austin, Texas. Asso-\nciation for Computational Linguistics.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4487‚Äì4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-\nRegib, Zsolt Kira, Richard Socher, and Caiming\nXiong. 2019a. Self-monitoring navigation agent\nvia auxiliary progress estimation. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nChih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caim-\ning Xiong, and Zsolt Kira. 2019b. The regretful\nagent: Heuristic-aided navigation through progress\nestimation. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 6732‚Äì6740. Com-\nputer Vision Foundation / IEEE.\nDipendra Misra, John Langford, and Yoav Artzi. 2017.\nMapping instructions and visual observations to ac-\ntions with reinforcement learning. In Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing , pages 1004‚Äì1015,\n4212\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nDipendra Kumar Misra, Andrew Bennett, Valts Blukis,\nEyvind Niklasson, Max Shatkhin, and Yoav Artzi.\n2018. Mapping instructions to actions in 3d environ-\nments with visual goal prediction. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2667‚Äì2678.\nShiwali Mohan and John E. Laird. 2014. Learning\ngoal-oriented hierarchical tasks from situated inter-\nactive instruction. In Proceedings of the Twenty-\nEighth AAAI Conference on ArtiÔ¨Åcial Intelligence,\nJuly 27 -31, 2014, Qu ¬¥ebec City, Qu ¬¥ebec, Canada ,\npages 387‚Äì394. AAAI Press.\nA. Mohseni-Kabir, C. Li, V . Wu, D. Miller, B. Hylak,\nS. Chernova, D. Berenson, C. Sidner, and C. Rich.\n2018. Simultaneous learning of hierarchy and prim-\nitives (slhap) for complex robot tasks. Autonomous\nRobotics.\nVan-Quang Nguyen and Takayuki Okatani. 2020. A\nhierarchical attention model for action learning from\nrealistic environments and directives. ECCV EVAL\nWorkshop.\nP. E. Rybski, K. Yoon, J. Stolarz, and M. M. Veloso.\n2007. Interactive robot task training through dia-\nlog and demonstration. In The 2nd ACM/IEEE In-\nternational Conference onHuman-Robot Interaction\n(HRI), pages 49‚Äì56.\nM. Scheutz, E. Krause, B. Oosterveld, T. Frasca, and\nR. Platt. 2017. Spoken instruction-based one-shot\nobject and action learning in a cognitive robotic ar-\nchitecture. In Proceedings of the 16th International\nConference on Autonomous Agents and Multiagent\nSystems (AAMAS 2017), pages 1378‚Äì1386.\nLanbo She and Joyce Chai. 2017. Interactive learning\nof grounded verb semantics towards human-robot\ncommunication. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1634‚Äì1644,\nVancouver, Canada. Association for Computational\nLinguistics.\nWilliam B. Shen, Danfei Xu, Yuke Zhu, Fei-Fei Li,\nLeonidas J. Guibas, and Silvio Savarese. 2019. Situ-\national fusion of visual representation for visual nav-\nigation. In 2019 IEEE/CVF International Confer-\nence on Computer Vision, ICCV 2019, Seoul, Ko-\nrea (South), October 27 - November 2, 2019 , pages\n2881‚Äì2890. IEEE.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. 2020. ALFRED:\nA benchmark for interpreting grounded instructions\nfor everyday tasks. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020 , pages\n10737‚Äì10746. IEEE.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C ÀÜot¬¥e,\nYonatan Bisk, Adam Trischler, and Matthew J.\nHausknecht. 2020. Alfworld: Aligning text and em-\nbodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768.\nKunal Pratap Singh, Suvaansh Bhambri, Byeonghwi\nKim, Roozbeh Mottaghi, and Jonghyun Choi. 2020.\nMOCA: A modular object-centric approach for\ninteractive instruction following. arXiv preprint\narXiv:2012.03208.\nShane Storks, Qiaozi Gao, Govind Thattai, and Gokhan\nTur. 2021. Are we there yet? learning to localize\nin embodied instruction following. arXiv preprint\narXiv:2101.03431.\nHao Tan, Licheng Yu, and Mohit Bansal. 2019. Learn-\ning to navigate unseen environments: Back transla-\ntion with environmental dropout. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2610‚Äì2621, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nA. L. Thomaz and M. Cakmak. 2009. Learning about\nobjects with human teachers. In Proceedings of the\n4th ACM/IEEE International Conference on Human\nRobot Interaction, HRI ‚Äô09, pages 15‚Äì22, New York,\nNY , USA. ACM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Fun-\ntowicz, et al. 2019. Huggingface‚Äôs transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nYubo Zhang, Hao Tan, and Mohit Bansal. 2020. Diag-\nnosing the environment bias in vision-and-language\nnavigation. In Proceedings of the Twenty-Ninth\nInternational Joint Conference on ArtiÔ¨Åcial Intelli-\ngence, IJCAI 2020, pages 890‚Äì897. ijcai.org.\nYuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox,\nLi Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, and\nAli Farhadi. 2017. Visual semantic planning using\ndeep successor representations. In IEEE Interna-\ntional Conference on Computer Vision, ICCV 2017,\nVenice, Italy, October 22-29, 2017 , pages 483‚Äì492.\nIEEE Computer Society.\n4213\nAppendix\nA Additional Training Details\nWe use the RoBERTa (Liu et al., 2019b) implemen-\ntation from Huggingface (Wolf et al., 2019). The\nmodel is Ô¨Åne-tuned for 10 epochs with the Adam\n(Kingma and Ba, 2015) optimizer on the ALFRED\ntraining set. The learning rate warms up over the\nÔ¨Årst half of the Ô¨Årst epoch to a peak value of 1e-5,\nand then linearly decayed. The model achieving\nthe highest navigation action prediction accuracy\non the validation seen set is selected for evaluation.\nAll the models are trained on one NVIDIA V100\n16GB GPU.\nB Additional Evaluation Details\nWe follow the evaluation setting in the ALFRED\nbenchmark2. For each episode, the agent is given\na task, which is composed of a goal directive G\nand several sub-goal instructions. The agent needs\nto sequentially perform actions to achieve the goal\nbased on the visual observations of RGB image\nonly. This progress ends if the agent predicts an\nEnd action (an End sub-goal for HiTUT), which is\nmade after up to 10 failed interaction attempts or\nreaching the maximum step limitation. For HiTUT,\nthere is also a maximum number of backtracking\nattempts, and the model will be forced to stop if\nthe budget runs out. The maximum number of\nbacktracking is 8 in all of our experiments with-\nout explicitly mentioning the backtracking number.\nWe also leverage two techniques to reduce the in-\nteraction attempt failures. We use the obstruction\ndetection trick proposed in Singh et al. (2020) to\navoid failures caused by repeated tries of moving\ntoward obstructions. We propose a self-monitoring\napproach to check the validity of manipulation ac-\ntions. If no mask is selected or a predicted action\nargument is not consistent with the class prediction\nfrom Mask R-CNN for the selected object, the ma-\nnipulation action is decided as failed and the agent\nperforms a backtrack without trying to execute the\naction. Note that in Table 4, we remove the in-\nteraction attempt constraint when comparing the\neffect of different allowed maximum backtracking\nnumbers, thus the results of #BT = 8is slightly\nhigher than those shown in Table 2.\n2https://leaderboard.allenai.org/\nalfred/submissions/get-started\nTask-Type MOCA HiTUT\nSeen Unseen Seen Unseen\nPick & Place 29.5 5.0 35.9 26.0\nCool & Place 26.1 0.7 19.0 4.6\nStack & Place 5.2 1.8 12.2 7.3\nHeat & Place 15.8 2.7 14.0 11.9\nClean & Place 22.3 2.4 50.0 21.2\nExamine & Place 20.2 13.2 26.6 8.1\nPick Two & Place 11.2 1.1 17.7 12.4\nAverage 18.6 3.8 25.2 12.4\nTable 7: Success rates across 7 task types on the valida-\ntion sets. Highest values per fold are bold.\nModel #Backtracking Seen SR Unseen SR\nRoBERTa no 10.5 5.2\nScratch no 7.9 2.8\nRoBERTa 4 23.1 12.9\nScratch 4 18.1 10.2\nRoBERTa 8 27.2 16.2\nScratch 8 26.8 14.0\nMOCA - 19.15 3.78\nTable 8: The validation success rates for models pre-\ntrained and trained from scratch with different allowed\nmaximum number of backtrackings.\nC Additional Results\nA detailed per-task performance comparison of Hi-\nTUT and MOCA is shown in Table 7. As the\ncomparison might be unfair since HiTUT bene-\nÔ¨Åts from model pre-training, we also conduct an\nablation study to show the effectiveness of pre-\ntraining. In Table 8, we compare the Ô¨Åne-tuned\nRoBERTa model to a Transformer with the same\nsize trained from scratch to show the role of the\nRoBERTa pretraining.We can see that RoBERTa\nconsistently improves the performance over train-\ning from scratch both w/o and w/ backtracking\nwith an absolute gain between 0.4% and 5% on\ntask success rate. Notably, Scratch with 4 or 8\nbacktrackings still outperform MOCA by a large\nmargin in terms of the unseen success rate.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7871929407119751
    },
    {
      "name": "Transformer",
      "score": 0.577939510345459
    },
    {
      "name": "Task (project management)",
      "score": 0.4573570489883423
    },
    {
      "name": "Natural language processing",
      "score": 0.3440036475658417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32411840558052063
    },
    {
      "name": "Engineering",
      "score": 0.11456650495529175
    },
    {
      "name": "Systems engineering",
      "score": 0.10944759845733643
    },
    {
      "name": "Electrical engineering",
      "score": 0.06539809703826904
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}