{
  "title": "AI-powered Automatic Item Generation for Psychological Tests: A Conceptual Framework for an LLM-based Multi-Agent AIG System",
  "url": "https://openalex.org/W4413725215",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2135461458",
      "name": "Philseok Lee",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A2117028037",
      "name": "Mina Son",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A2810133368",
      "name": "Zihao Jia",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A2135461458",
      "name": "Philseok Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117028037",
      "name": "Mina Son",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2810133368",
      "name": "Zihao Jia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2033648667",
    "https://openalex.org/W2801702920",
    "https://openalex.org/W2157575844",
    "https://openalex.org/W4383913712",
    "https://openalex.org/W4405276448",
    "https://openalex.org/W2168459328",
    "https://openalex.org/W3160492784",
    "https://openalex.org/W4387024647",
    "https://openalex.org/W4214897088",
    "https://openalex.org/W4385849309",
    "https://openalex.org/W4401415431",
    "https://openalex.org/W4390730779",
    "https://openalex.org/W4381573643",
    "https://openalex.org/W2923413722",
    "https://openalex.org/W2926871074",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W2077821296",
    "https://openalex.org/W4378509427",
    "https://openalex.org/W4319293546",
    "https://openalex.org/W4323315336",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4390742613",
    "https://openalex.org/W958651444",
    "https://openalex.org/W2745464303",
    "https://openalex.org/W2136219431",
    "https://openalex.org/W4321004017",
    "https://openalex.org/W4391591599",
    "https://openalex.org/W376379408",
    "https://openalex.org/W1598821627",
    "https://openalex.org/W4393952013",
    "https://openalex.org/W2155377391",
    "https://openalex.org/W2033715270",
    "https://openalex.org/W4295942986",
    "https://openalex.org/W4319225554",
    "https://openalex.org/W2159013703",
    "https://openalex.org/W2166416080",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4226025268",
    "https://openalex.org/W4381549348",
    "https://openalex.org/W4226163610",
    "https://openalex.org/W4385963839",
    "https://openalex.org/W4389217265",
    "https://openalex.org/W3157897384",
    "https://openalex.org/W4408557525",
    "https://openalex.org/W2772730860",
    "https://openalex.org/W4384342518",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W4291824852",
    "https://openalex.org/W4213278005",
    "https://openalex.org/W4404061654",
    "https://openalex.org/W4310993626",
    "https://openalex.org/W4387725253",
    "https://openalex.org/W4390832893",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4401416363",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4392882830",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2808233647",
    "https://openalex.org/W4284881574",
    "https://openalex.org/W4384812233",
    "https://openalex.org/W4239510280",
    "https://openalex.org/W2037668503",
    "https://openalex.org/W2040130400",
    "https://openalex.org/W2141632279",
    "https://openalex.org/W2330055266",
    "https://openalex.org/W2793062952",
    "https://openalex.org/W4234815647",
    "https://openalex.org/W4401949180",
    "https://openalex.org/W4394580931",
    "https://openalex.org/W4360985815",
    "https://openalex.org/W2791246286",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W2500994499",
    "https://openalex.org/W4384111914",
    "https://openalex.org/W2082032767",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4388685197",
    "https://openalex.org/W4386644471",
    "https://openalex.org/W4383473935",
    "https://openalex.org/W4387994499",
    "https://openalex.org/W4388560379",
    "https://openalex.org/W2981386833",
    "https://openalex.org/W4238837262",
    "https://openalex.org/W4404783595",
    "https://openalex.org/W1598960279",
    "https://openalex.org/W198315133",
    "https://openalex.org/W1479925024"
  ],
  "abstract": "Abstract Large Language Models (LLMs) are transforming industrial-organizational psychology and human resource management, with one of their most promising applications being automatic item generation (AIG) for psychological test development. Although recent advances in LLM-based AIG—particularly for non-cognitive assessments such as personality— show significant potential, ensuring rigorous quality control remains a persistent challenge. This study introduces a novel AIG framework, the LLM-based Multi-agent AIG system (LM-AIG), where each agent is responsible for different stages of item development, including item generation, content review, linguistic evaluation, bias assessment, and item revision. The LM-AIG also incorporates human feedback to enhance item quality. We implemented the LM-AIG framework using the open-source tool AutoGen to generate items assessing attitudes toward the use of AI in the workplace. To evaluate the quality of the generated items, we conducted an empirical study based on structured ratings from human raters, assessing construct relevance, linguistic clarity, appropriate language level, contextual specificity, and potential bias. This paper further discusses the role of human-in-the-loop mechanisms within the LM-AIG system and outlines future research directions.",
  "full_text": "Vol.:(0123456789)\nJournal of Business and Psychology \nhttps://doi.org/10.1007/s10869-025-10067-y\nORIGINAL PAPER\nAI‑powered Automatic Item Generation for Psychological Tests: \nA Conceptual Framework for an LLM‑based Multi‑Agent AIG System\nPhilseok Lee1  · Mina Son1  · Zihao Jia1 \nAccepted: 12 August 2025 \n© The Author(s) 2025\nAbstract\nLarge Language Models (LLMs) are transforming industrial-organizational psychology and human resource management, \nwith one of their most promising applications being automatic item generation (AIG) for psychological test development. \nAlthough recent advances in LLM-based AIG—particularly for non-cognitive assessments such as personality— show \nsignificant potential, ensuring rigorous quality control remains a persistent challenge. This study introduces a novel AIG \nframework, the LLM-based Multi-agent AIG system (LM-AIG), where each agent is responsible for different stages of item \ndevelopment, including item generation, content review, linguistic evaluation, bias assessment, and item revision. The LM-\nAIG also incorporates human feedback to enhance item quality. We implemented the LM-AIG framework using the open-\nsource tool AutoGen to generate items assessing attitudes toward the use of AI in the workplace. To evaluate the quality of \nthe generated items, we conducted an empirical study based on structured ratings from human raters, assessing construct \nrelevance, linguistic clarity, appropriate language level, contextual specificity, and potential bias. This paper further discusses \nthe role of human-in-the-loop mechanisms within the LM-AIG system and outlines future research directions.\nKeywords Automatic item generation · Large language model · Multi-agent AI system · Test development · Personnel \nassessment · Psychometrics\nThe application of artificial intelligence (AI) and machine \nlearning (ML) is significantly reshaping the fields of indus-\ntrial and organizational psychology and human resource \nmanagement (Budhwar et al., 2023; Campion & Campion, \n2023). These technological advancements have opened new \npossibilities for more innovative and efficient personnel \nassessments across various domains, including job analysis \n(e.g., Koenig et al., 2023; Putka et al., 2023), performance \nevaluations (e.g., Speer, 2018, 2021), employment selection \n(e.g., Campion & Campion, 2023; Fan et al., 2023; Hickman \net al., 2023; Holtrop et al., 2022; Thompson et al., 2023), \nand psychological test development (e.g., Fyffe et al., 2024; \nGötz et al., 2024; Hernandez & Nie, 2023; Hommel, 2023; \nHommel et al., 2022; Lee et al., 2023).\nAmong recent developments, large language models \n(LLMs) have emerged as a particularly transformative devel-\nopment in personnel assessment. Trained on extensive and \ndiverse textual corpora using the transformer1 architectures \n(Vaswani et al., 2017), LLMs have demonstrated remark -\nable capabilities across a wide range of tasks, including text \ngeneration, classification, question-answering, logical rea-\nsoning, and information extraction (e.g., Brown et al., 2020; \nLiu et al., 2023; Radford et al., 2019; Wei et al., 2022). A \nkey advantage of LLMs is their ability to perform these tasks \nthrough natural language prompting. This prompt-based \napproach, which relies solely on language instructions, sig-\nnificantly reduces the time and resources typically required \nAdditional supplementary materials may be found here by \nsearching on article title https:// osf. io/ colle ctions/ jbp/ disco ver\n * Philseok Lee \n plee27@gmu.edu\n1 Department of Psychology, George Mason University, 4400 \nUniversity Drive, David King Hall Room 3056, Fairfax, \nVA 22030, USA\n1 The transformer architecture is a neural network design that pro-\ncesses all parts of a sequence simultaneously, unlike older models \nthat go step by step. It uses self-attention to focus on important parts \nof the sequence and understand relationships between words, even \nwhen they’re far apart. This approach makes transformers highly \neffective for tasks like language translation, text generation, and sum-\nmarization.\n Journal of Business and Psychology\nfor machine learning or fine-tuning processes (Demszky \net al., 2023; Hao et al., 2024; Jia & Lee, 2025).\nOne of the most promising applications of LLMs is \nautomated item generation (AIG) for test development in \npsychological and educational research (Bulut et al., 2024; \nDemszky et al., 2023; Tan et al., 2024). Traditionally, test \ndevelopment has been a labor-intensive and time-consuming \nprocess; however, AIG techniques provide the potential to \nsignificantly reduce the required effort while maintaining \npsychometric soundness. To date, most AIG research has \nfocused on educational contexts, particularly in the devel-\nopment of academic aptitude, licensure, and knowledge-\nbased assessments (see Gierl & Haladyna, 2012; Gierl & \nLai, 2018; Kurdi et al., 2020). In contrast, the application of \nAIG to non-cognitive assessments, such as personality tests, \nhas only recently gained traction in the fields of psychology \nwith the advent of LLMs (e.g., Fyffe et al., 2024; Götz et al., \n2024; Hernandez & Nie, 2023 ; Hommel et al., 2022 ; Lee \net al., 2023; von Davier, 2018).\nDespite recent advances in LLM-based AIG for non-\ncognitive assessments, significant opportunities for \nimprovement remain, particularly in the domain of qual-\nity control. Although human oversight remains essential \nto ensure the accuracy, reliability, and fairness of AI-\nbased assessments (Hao et al., 2024), current AIG studies \ntypically use human reviewers in a post-hoc, retrospective \nmanner, evaluating items only after generation rather than \nembedding quality control within the AIG process itself \n(e.g., Götz et al., 2024; Hernandez & Nie, 2023; Hom-\nmel et al., 2022; Lee et al., 2023). For example, Götz \net al. (2024) used a culturally diverse, double-blind expert \ncommittee to assess item quality, such as conceptual rel-\nevance, linguistic clarity, and balanced keying. While rig-\norous, this post hoc approach may constrain the overall \nefficiency of item generation, highlighting the need for a \nmore integrated and interactive quality control framework \nwithin the AIG pipeline.\nBuilding on this line of inquiry, the present study intro-\nduces a multi-agent AI system—referred to as LLM-based \nMulti-agent AIG (LM-AIG). Importantly, this study does \nnot aim to automate item generation merely to reduce time, \neffort, or cost. Instead, we conceptualize the AI multi-agents \nsystem as collaborative research partners in the pursuit of \ntrustworthy automated item generation. The remainder of \nthis paper is structured as follows. First, we review recent \nresearch on AIG for non-cognitive assessments. Next, we \nintroduce the concept of multi-agent systems and their \npotential benefits for psychological test development. Build-\ning on this foundation, we propose an LM-AIG framework \nand demonstrate its application through an empirical exam-\nple. We then evaluate content validity and other qualities \nof generated items based on structured ratings from human \nraters. Finally, we discuss the implications of the LM-AIG \nframework, the role of human experts within the system, and \ndirections for future research.\nEarly Efforts in LLM‑Based AIG \nfor Non‑Cognitive Assessments\nAs previously noted, while AIG has been applied to edu-\ncational and knowledge-based cognitive testing for several \ndecades, its application to non-cognitive assessments is a \nrelatively recent development. One of the earliest explo-\nrations in this area was conducted by von Davier (2018), \nwho employed a long short-term memory (LSTM) 2 model \n(Hochreiter & Schmidhuber, 1997) to generate 24 items \nmeasuring the Big Five personality traits. This study dem-\nonstrated the potential of language models to automatically \ngenerate valid personality test items, highlighting a promis-\ning direction for psychological test development.\nHowever, LSTM models present several limitations \nwhen applied to psychological test development. They \ntend to closely resemble existing items rather than generate \nnovel ones that capture nuanced personality constructs and \nsemantic meanings (Hommel et al., 2022). Moreover, as a \nsupervised learning model, LSTMs require large volumes \nof human-annotated training data, which can be particularly \nchallenging requirement for emerging or underexplored con-\nstructs. In von Davier’s (2018) study, the model was trained \non 3,320 personality items from the International Personality \nItem Pool (IPIP; Goldberg et al., 2006). While LSTM-based \napproaches have laid important groundwork for AIG in non-\ncognitive assessments, their inherent limitations suggest they \nmay not represent the most effective solution for advancing \nresearch or practice in this field.\nApplications of Transformer Architectures \nin AIG for Non‑Cognitive Testing\nRecently, researchers have increasingly adopted advanced \ntransformer models for AIG in non-cognitive assessments \n(e.g., Fyffe et al., 2024; Götz et al., 2024; Hernandez & \n2 The LSTM model uses special “gates” (i.e., forget, input, and out-\nput gates) to control how information flows through the system. The \nforget gate decides what information from previous steps should be \nkept or discarded. The input gate determines how important new \ninformation is. The output gate controls how much information is \npassed on to the next stage of processing. These gates help LSTM \nmaintain relevant information over long sequences, addressing a com-\nmon problem in neural networks called the “vanishing gradient prob-\nlem”.\nJournal of Business and Psychology \nNie, 2023; Hommel et al., 2022 , 2023; Lee et al., 2023). \nTransformer models offer two major advantages over their \npredecessors. First, the self-attention mechanism enables \neach word in a sentence to interact dynamically with all \nother words, allowing the model to capture more nuanced, \ncontext-specific meanings. Second, unlike earlier neural \nnetwork models such as RNNs and LSTMs, which process \ninputs sequentially, transformers operate in parallel. This \nparallelization substantially reduces computational time and \nenhances efficiency when handling large-scale text datasets \n(Tunstall et al., 2022). Among these models, generative pre-\ntrained transformers (GPT) have demonstrated particular \neffectiveness in text generation tasks and are now widely \nadopted in AIG research.\nHommel et al. ( 2022) employed GPT-2 (Radford et al., \n2019) to generate items for the Big Five personality traits. \nThey fine-tuned GPT-2 on large datasets of validated per -\nsonality items, training the model to encode both construct \nlabels and their corresponding items. In an empirical demon-\nstration, the researchers generated 25 personality items and \ncompared them to human-authored items. Results from a \nconfirmatory factor analysis (CFA) indicated that the overall \nmodel fit was comparable between the machine-generated \nand human-authored scales. Notably, approximately two-\nthirds of the GPT-2-generated items exhibited acceptable \npsychometric properties, with factor loadings exceeding .40.\nExpanding on this line of research, Hernandez and Nie \n(2023) adopted a novel approach by fine-tuning GPT-2 on \na large corpus of existing personality items to enhance the \nsemantic quality of the generated content. After fine-tuning, \nthe model produced one million new items. Their analysis \nrevealed that the AI-generated item pool (AI-IP) contained \na significant number of unique items, demonstrating greater \nlexical diversity and novel statements compared to existing \nitem banks. Their linguistic acceptability analysis confirmed \nthat most items were grammatically correct, though slightly \nless accurate than those created by humans. Notably, they \nemployed zero-shot classification models for content valid-\nity, allowing categorization of items into content domains \nwithout relying on labeled training data.\nGötz et al. (2024) developed the Psychometric Item Gen-\nerator (PIG), an open-source NLP algorithm powered by \nGPT-2, to automate the generation of personality items. \nThey demonstrated PIG’s effectiveness in producing items \nfor both novel and established psychological constructs. For \ninstance, they used PIG to develop items for a new construct, \nwanderlust, defined as a strong desire to travel and explore, \nsubstantially reducing the time and effort required for \nmanual item development. Additionally, PIG was employed \nto generate short-form scales for the Big Five traits, yielding \nreliable and valid instruments comparable to those devel-\noped by human experts.\nIn contrast to prior studies employing GPT-2, Lee et al. \n(2023) utilized a more advanced GPT-3 3 model (Brown \net al., 2020) to generate personality items measuring Big \nFive traits. Leveraging prompt engineering techniques, they \ngenerated 25 personality items and compared their psycho-\nmetric properties to those of human-authored personality \nIPIP items. They also examined the measurement invari-\nance of machine-generated personality items across gender \ngroups. Overall, the results indicated that the GPT-3-gen-\nerated items demonstrated strong psychometric properties, \nincluding reliability, discriminant and convergent validity, \nfactor structure, and criterion-related validity. Moreover, \nthe items demonstrated minimal evidence of gender-based \nmeasurement bias.\nBuilding on previous research that primarily focused on \ngenerating items for broad Big Five personality constructs, \nHo (2024) shifted attention to more specific facets of con-\nscientiousness, including organization, productiveness, and \nresponsibility. Using GPT-4 (OpenAI, 2024), the author \ninvestigated the influence of generation hyperparameters—\nparticularly temperature 4—on item quality. The findings \nindicated that a temperature setting of 1 was most effective \nfor producing personality items with strong content valid-\nity. While the GPT-4-generated scales demonstrated psycho-\nmetric properties comparable to those of human-authored \nscales, the latter still outperformed the machine-generated \nitems on several psychometric indicators. Ho concluded that \nwhile LLMs show considerable promise for AIG, significant \n3 GPT-3 substantially enhances model capacity compared to GPT-2, \nhaving approximately 100 times more parameters, with 175 billion \nparameters across 96 layers. It was trained on a vast corpus of 300 \nbillion tokens of internet text data. This increased scalability enables \nGPT-3 to generate text with more complex semantic and syntactic \nstructures, demonstrating greater robustness to grammatical errors. In \naddition, GPT-3 provides prompt-based few-shot learning (Liu et al., \n2021). This feature allows the model to perform NLP tasks effectively \nwith minimal examples, eliminating the need for extensive human-\nannotated training data or complex fine-tuning processes.\n4 In LLMs, the temperature setting regulates the randomness or creativ-\nity of the model’s responses. This parameter typically ranges from 0 to \n2. Lower temperatures (e.g., 0 to 0.5) produce more predictable outputs, \nwhile higher temperatures yield more creative and varied responses. \nA temperature setting of 1 allows for a balanced level of creativity and \nflexibility, which can be particularly useful for generating diverse con-\ntent, such as writing test items or simulating human reasoning.\n Journal of Business and Psychology\nTable 1  Summary of LLM-based AIG for non-cognitive assessments\nLSTM Long short-term memory, GPT Generative Pre-trained Transformer. * We presented studies based on the order of GPT models\na  No existing scale was used\nReference Model* Construct (base scale) Generated initial item pool Findings on the psychometric properties of machine-generated items\nVon Davier (2018) LSTM Big Five personality (IPIP, Goldberg et al., 2006) Not specified • Most machine-authored items load highly on the corresponding \ntrait factor when exploratory factor analysis was conducted on the \ncombined set of existing personality items and automatically gener-\nated items\nHommel et al. (2022) GPT-2 Big Five personality (IPIP, Goldberg et al., 2006) 1,360 • Machine-authored scales generally show acceptable internal consist-\nency and structural validity (assessed via confirmatory and explora-\ntory factor analyses). A significant number of items showed accept-\nable factor loadings. Machine-authored scales have lower reliability \ncoefficients than human-authored scales for all Big Five dimensions \nexcept for openness to experience\nHernandez and Nie (2023) GPT-2 Study 1: personality (IPIP, Goldberg et al., 2006),\nStudy 4: Big Five personality (BFI-2, Soto & John, 2017), self-\nefficacy (Sherer et al., 1982), curiosity (Kashdan et al., 2018)\nStudy 1: 1,000,000\nStudy 4: 5,000\n• Study 1: Machine-authored items show greater diversity of words \nthan and great similarity in structure (i.e., time length, readability, \npercentage of parts of speech) with that of human generated items\n• Study 4: Machine-authored scales show good convergence with cor-\nresponding human-generated scales overall, although the magnitudes \nsomewhat vary. They also exhibit internal consistency and structural \nvalidity (assessed via confirmatory factor analysis) that are similar to \nthose of human-authored scales\nGötz et al. (2024) GPT-2 Demonstration 1:  Wanderlusta\nDemonstration 2: Big Five personality (BFI-2, Soto & John, 2017)\nDemonstration 1: 65\nDemonstration 2: 100,000\n• Machine-authored items selected based on expert evaluations show \nrobust psychometric properties by exhibiting evidence of internal \nconsistency, test–retest reliability, structural validity (assessed via \nexploratory structural equation model and confirmatory factor analy-\nsis), and predictive validity for life outcomes\nLee et al. (2023) GPT-3 Big Five personality (IPIP, Goldberg et al., 2006) 100 • Machine-authored items show acceptable internal consistency, \nstructural validity (assessed via exploratory and confirmatory factor \nanalyses), and criterion-related validity. Most machine-authored \nitems were free of gender-based different item functioning. Machine-\nauthored scales exhibit scalar measurement invariance between male \nand female groups\nHo (2024) GPT-4 Conscientiousness (BFI-2 conscientiousness scale, Soto & John, \n2017)\n12 items for each of nine \nhyperparameter conditions\n• The best combination of hyperparameters to generate items with good \ncontent validity is temperature of 1 and presence penalty of 0\n• Machine-authored scales generally show internal consistency, struc-\ntural validity (assessed via confirmatory factor analysis), criterion-\nrelated validity, and item and test information comparable to those of \nhuman-authored scales, however human-authored scales still outper-\nformed machine-authored scales consistently across these indicators\nJournal of Business and Psychology \nchallenges remain in developing high-quality items for \nnuanced facets of complex psychological constructs. Table 1 \nprovides a summary of recent studies on LLM-based AIG \nfor non-cognitive assessments.\nAddressing Gaps in LLM‑Based AIG \nfor Non‑Cognitive Assessments\nMuch of the existing research on AIG has largely relied \non post hoc evaluations by human raters to assess content \nvalidity (e.g., Götz et al., 2024; Hommel et al., 2022; Lee \net al., 2023; von Davier, 2018). While effective, this retro-\nspective manual review process could be time consuming \nand labor intensive, particularly when applied to extensive \nitem pools, thereby limiting the scalability and overall effi-\nciency of AIG systems (Circi et al., 2023). For instance, \nevaluating 1,000 newly generated items would require \na substantial commitment of expert time and effort. To \naddress this bottleneck, Hernandez and Nie (2023) applied \nzero-shot classification models to automate content valid-\nity evaluation. In their approach, the model estimated the \nlikelihood of an item aligning with each of the five per -\nsonality dimensions and assigned the item to the dimen-\nsion with the highest probability. This method achieved \nan average classification accuracy of 64.31%, suggesting \nthat further improvement is needed. Similarly, Fyffe et al. \n(2024) applied transformer-based models, such as BERT, \nDeBERTa, and RoBERTa, using few-shot learning tech-\nniques to classify items into content domains.\nHowever, classification models typically produce \nnumerical metrics, such as accuracy rates, which are insuf-\nficient for providing actionable insights into item qual-\nity. In the AIG process, meaningful feedback, evaluative \njudgments, and specific recommendations are more valu-\nable than aggregate performance scores. By incorporat-\ning established psychometric practices for content validity \nevaluation (e.g., Hinkin & Tracey, 1999), AIG could better \nassess how well an item aligns with its intended construct \n(i.e., correspondence) and how effectively it differentiates \nthe target construct from related constructs (i.e., distinc-\ntiveness). This content-validity-based evaluation would \noffer more targeted, constructive feedback at various \nstages of the AIG process. Aligning AIG with established \nguidelines for content validity (e.g., Colquitt et al., 2019) \nwould improve the rigor, interpretability, and quality of \ngenerated items.\nIn addition, LLMs are susceptible to inheriting and per -\npetuating societal biases embedded in their training data, \nincluding gender stereotypes, racial biases, and cultural \nassumptions (Gallegos et al., 2024). These biases may \nmanifest in various forms, such as language patterns, con-\ntextual framing, and implicit associations, often skewing \ncontent toward the perspectives of majority groups (Cal-\niskan & Lewis, 2020; Charlesworth & Banaji, 2021; Gal-\nlegos et al., 2024). As a result, LLM-generated items may \nlack cultural sensitivity and inclusivity, leading to differen-\ntial item functioning (DIF) that systematically advantages \nor disadvantages specific subgroups (Lee et al., 2023). \nSuch bias not only undermines the fairness of assess-\nments but also threatens their validity, potentially leading \nto inequitable decisions in organizational and educational \ncontexts (Vandenberg, 2002). Despite growing awareness \nof this issue, few AIG studies have incorporated system-\natic bias auditing or mitigation strategies into the AIG \npipeline. Hommel et al. (2022) underscore the urgent need \nfor more rigorous methods to identify and address item-\nlevel bias, as well as to ensure measurement invariance in \nfuture LLM-based AIG research. Accordingly, developing \nand implementing effective auditing strategies is critical to \nenhancing the fairness and psychometric quality of LLM-\nbased AIG systems.\nAnother key area for advancement is the integration of \nreal-time grammatical error correction capabilities. Hernan-\ndez and Nie (2023) reported that approximately 8% of the \nitems generated by GPT-2 contained grammatical errors. \nWhile newer LLMs such as GPT-4 may produce more lin-\nguistically accurate items, grammar evaluation still remains \na critical and labor-intensive component of the AIG process. \nAs Hernandez and Nie (2023) noted, “rather than use these \ngrammar-checking models retrospectively, future versions \nof this system might want to apply them prospectively when \ndeveloping the initial item pool (p. 1030)”. Moreover, inte-\ngrating real-time grammatical validation could improve mul-\ntilingual item generation, supporting the broader application \nof LLM-based AIG across diverse linguistic and cultural \ncontexts.\nFinally, most LLM-based AIG research lack the integra-\ntion of direct human input during the item generation pro-\ncess. Incorporating a human-in-the-loop mechanism could \nsubstantially improve item quality by introducing critical \noversight and transforming AIG from a static, unidirectional \nsystem into an interactive, collaborative process. Real-time \nhuman feedback allows for the immediate identification and \ncorrection of problematic items, reducing the accumulation \nof errors and the need for extensive post hoc revisions. This \nsynergy between AI and human judgment promotes greater \nalignment with psychometric principles and enhances both \nthe efficiency and validity of the AIG process.\nIntroduction of a Multi‑Agent Framework\nTo address the challenges identified in prior research, we \npropose an LLM-based Multi-agent AIG system. A multi-\nagent system consists of autonomous artificial entities \n Journal of Business and Psychology\ncapable of independently perceiving their environment, \nprocessing information, making decisions, and execut-\ning actions to achieve designated objectives (Cañas, 2022; \nCheng et al., 2024). While the evolution of artificial agents \nhas undergone several stages (see Ginsberg, 2012; Wilkins, \n2014), recent advancements in LLMs have markedly \nenhanced agent capabilities, enabling them to operate more \nintelligently and efficiently in complex environments (Cheng \net al., 2024; Guo et al., 2024; Li et al., 2024).\nDeveloping an effective LM-AIG system requires careful \nconsideration of several core components. The first is the \nagent-environment interface, which defines the operational \ncontext and guides agents’ perception and decision-mak -\ning. Such interfaces have been successfully applied across \na range of domains, including software development (Qian \net al., 2023), games (Park et al., 2023), robotics (Zhang et al., \n2023), economics (Li et al., 2023), political science (Xiao \net al., 2023), and public health (Williams et al., 2023). In the \ncontext of psychological assessment, the environment must \nbe structured to reflect relevant psychological constructs, test \nspecifications, and item development guidelines. Aligning \nthe interface with these domain-specific elements enables \nagents to interpret requirements accurately and generate \nitems that are psychometrically appropriate and contextu-\nally valid.\nThe second component is the definition of agent profiles, \nwhich includes specifying each agent’s traits, behaviors, \nknowledge bases, and roles. In the context of AIG, agents \ntake on roles similar to a team of human experts, such as \nitem writers, content reviewers, linguistic reviewers, and \nbias reviewers. Guo et al. (2024) categorize agent profil-\ning methods into three types: i) model-generated, ii) data-\nderived, and iii) pre-defined. Model-generated profiles  \nare created automatically by LLMs, which assign roles and \nexpertise based on specific needs. While this approach may \nbe efficient when managing many agents, it may lack control \nand precision over the outcomes. In contrast, data-derived \nprofiles are constructed using real-world datasets incor -\nporating demographic information such as race, ethnicity, \ngender, age, and location to enhance realism. However, this \napproach may be resource-intensive and requires extensive \ndata, especially when generating diverse agent pools. For the \npurposes of this study, we adopt a pre-defined profiling strat-\negy, wherein agent roles and attributes are manually speci-\nfied according to established psychometric frameworks (e.g., \nClark & Watson, 2019; Hinkin, 2005; Lambert & Newman, \n2023; Zickar et al., 2020). This approach ensures alignment \nwith best practices in test development and provides greater \ncontrol over agent behavior and task execution.\nThe third consideration is inter-agent communication \nstyle. Guo et al. (2024) identify three primary communica-\ntion styles: i) cooperative, ii) debate, and iii) competitive. \nIn cooperative communication, agents collaborate towards \nshared goals, exchanging information to improve collec-\ntive outputs. This communication style has been effectively \napplied in domains such as software development (e.g., \nHong et al., 2023; Qian et al., 2023) and robotic collabora-\ntion (e.g., Mandi et al., 2024). The debate model, by con-\ntrast, involves argumentative interactions in which agents \npresent, defend, and critique differing viewpoints. This \napproach is particularly effective for reaching consensus or \nrefining solutions, as demonstrated in scientific debate simu-\nlations (e.g., Du et al., 2023; Chan et al., 2023). In competi-\ntive communication, agents pursue individual goals that may \nconflict with one another. This approach has been used to \nsimulate economic theories (e.g., Zhao et al., 2023) and war \nscenarios (e.g., Hua et al., 2023). For this study, we adopt a \ncooperative communication style, reflecting the collabora-\ntive nature of typical test development. Agents will work \ntogether to refine and improve test items, sharing insights \nand building on each other’s contributions to produce high-\nquality, psychometrically sound results.\nA fourth consideration involves the communication \nstructure that governs interactions among agents. Guo et al. \n(2024) identify three primary structures: layered, central-\nized, and decentralized. In a layered structure, agents are \norganized hierarchically, with each level assigned specific \nroles, typically interacting within their own layer or with \nadjacent ones. This structure has proven effective in domains \nsuch as software development (e.g., Hong et al., 2023; Qian \net al., 2023). Decentralized communication, by contrast, \noperates on a peer-to-peer network, where agents interact \ndirectly with each other. It is commonly used in modeling \nhuman behavior during epidemics (e.g., Ghaffarzadegan \net al., 2024), simulating war scenarios (e.g., Hua et al., \n2023), and designing communication games (e.g., Xu et al., \n2023). Centralized communication, in turn, relies on a cen-\ntral agent or group of agents to coordinate interactions, with \nother agents communicating primarily through this hub. This \nstructure has been effectively implemented in multi-robot \nplanning (e.g., Chen et al., 2024) and chemistry laboratory \nsimulations (e.g., Zheng et al., 2023), offering a more con-\ntrolled and directed flow of communication. In this study, \nwe adopt a hybrid communication structure that integrates \nthe strengths of these approaches. Agents are organized into \nspecialized task groups—such as item generation, content \nreview, bias review, and linguistic review—following a lay-\nered hierarchy, while a central meta-editor agent manages \ncoordination, synthesizes feedback, and ensures alignment \nwith psychometric standards and test specifications. This \nhybrid approach facilitates both operational efficiency and \nadaptive collaboration in the development of test items.\nFinally, feedback integration is essential for improving \nadaptability and quality control within LM-AIG systems. Wang \net al. (2024) categorize feedback mechanisms into three types: \ni) environmental feedback, ii) agent interaction feedback, and \nJournal of Business and Psychology \niii) human feedback. Environmental feedback comes from the \nreal or virtual environments in which the agents operate as \nseen in programming environments (e.g., Hong et al., 2023; \nQian et al., 2023) or robotic systems (e.g., Chan et al., 2023; \nMandi et al., 2024). Agent interaction feedback occurs through \ninter-agent communication and evaluation. Agents refine their \noutcomes by engaging in dialogue and critical evaluation with \none another, often used in problem-solving scenarios like sim-\nulations or debates (e.g., Aher et al., 2023; Park et al., 2022). \nBy contrast, human feedback incorporates expert input and \ndomain-specific knowledge directly into the system. In this \nstudy, we adopt the human feedback approach, ensuring that \nthe quality of generated items is monitored and validated by \nhuman subject matter experts. This ensures that items meet \nhigh standards of relevance, accuracy, and psychometric valid-\nity throughout the AIG process.\nConceptual Framework of the LLM‑Based \nMulti‑Agent AIG System\nBuilding on the foundational concepts outlined above, we \nintroduce the LM-AIG system specifically designed for psy-\nchological assessment contexts. As illustrated in Fig.  1, the \nproposed framework consists of autonomous, specialized \nagents, each responsible for a distinct function within the \nitem generation and review process. This system reflects best \npractices in test development by ensuring that evaluative \nagents operate independently of item generation processes, \nthereby enhancing objectivity and reducing potential bias \n(Boateng et al., 2018).\nThe LM-AIG process begins with the Item Writer Agent, \nwhich is responsible for the creation of item pools. Its pri-\nmary task is to generate items that accurately reflect the \noperational definition of the target construct, while adher -\ning to established best practices in item writing. To achieve \nthis, the agent draws on detailed item specifications—includ-\ning construct definitions, item development guidelines (see \nTable 2), and carefully crafted prompts—to ensure the psy-\nchometric quality of the items.\nFollowing item generation, a multi-stage review process \nis initiated, coordinated by the Critic Agent. This process \nis structured through a nested communication structure \ninvolving three specialized reviewer agents—each dedi-\ncated to a distinct aspect of item quality: content, language, \nand bias (e.g., Wendler & Burrus, 2013; Zieky, 2013). \nWithin this framework, the Content Reviewer Agent, Lin-\nguistic Reviewer Agent, and Bias Reviewer Agent interact \nFig. 1  A Simplified Interactions of Agents in the LM-AIG System\n Journal of Business and Psychology\ndynamically with the Critic Agent, ensuring a comprehen -\nsive and collaborative evaluation.\nThe Content Reviewer Agent evaluates the extent to \nwhich each item reflects its intended construct and main-\ntains content validity. We incorporate Hinkin and Tracey’s \n(1999) content validity method into this process. In line with \ntheir recommendation, the Content Reviewer is designed to \nrepresent naïve judges, such as typical employees as a repre-\nsentative sample, rather than human raters formally trained \nin psychology or psychometrics. The Content Reviewer \nAgent uses a 7-point scale to evaluate alignment with the \nconstruct definition and computes both correspondence and \nTable 2  Guideline for the Item Generation\nAgent Guideline\nItem writer • Use short and simple language (Clark et al., 2019; Haladyna & Rodriguez, 2013; Hinkin, 2005; Jebb et al., 2021)\n• Avoid jargon, slang, difficult vocabulary, unfamiliar technical terms, and vague or ambiguous terms (Lambert & New-\nman, 2023; Peter et al., 2018)\n• Items should be appropriate for the reading level of the target population (Clark et al., 2019; Hinkin, 2005)\n• Write significantly more items than needed (e.g., three times: Lambert & Newman, 2023; Zickar, 2020)\n• The use of negative and reverse-coded items is controversial. Many studies show the detrimental effects of negative or \nreverse-coded items (DiStefano & Motl, 2006; Schriesheim & Eisenbach, 1995; Sliter & Zickar, 2014; Sonderen et al., \n2013; Woods, 2006). For example, there is evidence that reverse-keyed items may be confusing to participants, that the \nopposite of a reverse-keyed construct may be fundamentally different from the construct, and that reverse-keyed items \ntend to negatively affect the factor structure of scales. Item writers are often advised to avoid negative items\n• Avoid double-barreled items (Clark et al., 2019; Lambert & Newman, 2023; Zickar, 2020). For example, “My manager is \nintelligent and enthusiastic” should be not be used\n• Avoid items that virtually everyone (e.g., “Sometimes I am happier than at other times”) or no one (e.g., “I am always \nfurious”) will endorse (Clark et al., 2019; Hinkin, 2005)\n• Avoid colloquialisms that may be not be familiar across age, ethnicity, region, gender, and so forth (Clark et al., 2019)\n• Avoid vague words such as many, most, often, or sometimes. These have no formal quantity and so represent an open \ninvitation to miscomprehension. For example, contains an item ‘‘Overall, how satisfied are you with your life nowa-\ndays?’’ Nowadays is a vague term. A better item would be ‘‘Overall, how happy have you been with your life over the last \nthree months.’’ (Hardy & Ford, 2014)\n• It is important to keep all items consistent in terms of perspective, being sure not to mix items that assess behaviors with \nitems that assess affective responses (Harrison & McLaughlin, 1993). An example of this would be including items such \nas “My boss is hardworking” and “I respect my boss” in the same measure\nContent reviewer • Use expert ratings to assess whether items adequately represent the domain of interest (Anderson & Gerbing, 1991; \nColquitt et al., 2019; Hinkin & Tracey, 1999)\nFor example: Using the 7-point scale, please rate the following items on how well each matches the concept. 1: Item does \nan EXTREMELY BAD job of measuring the concept; 2: Item does a VERY BAD job of measuring the concept; 3: Item \ndoes a SOMEWHAT BAD job of measuring the concept; 4: Item does an ADEQUATE job of measuring the concept; \n5: Item does a SOMEWHAT GOOD job of measuring the concept; 6: Item does a VERY GOOD job of measuring the \nconcept; 7: Item does an EXTREMELY GOOD job of measuring the concept (Hinkin & Tracey, 1999)\nLinguistic reviewer • Check for grammatical accuracy and stylistic consistency\n• Check the level of language used in the item. It is likely difficult for the average respondent in the United States to under-\nstand and respond to surveys that contain items that require more than a seventh- to eighth-grade reading level\n• Check for items that contain unnecessary negative language,\n• Check for whether the item is confusing, unnecessarily difficult, or appears to be tricky or double-barreled? (Calderón et \net al., 2006; Wendler & Burrus, 2013)\nBias reviewer • Check for potential bias in items that may disadvantage certain demographic groups, such as gender, religion, race, age, \nand culture (Zieky, 2013)\nMeta editor • Synthesize feedback from all previous agents\n• Edit items as needed and discard items that cannot be fixed\n• Integrate human expert insights and suggestions\n• Identify any remaining issues\nJournal of Business and Psychology \ndistinctiveness indices, as outlined by Colquitt et al. (2019), \nto assess the item’s content validity. Next, the Linguistic \nReviewer Agent examines the grammatical correctness and \nreadability of each item. This agent also identifies issues \nsuch as confusing or overly complex wording, double-bar -\nreled items, or unnecessary negative language (e.g., Wendler \n& Burrus, 2013). Finally, the Bias Reviewer Agent assesses \neach item for potential sources of demographic bias, includ-\ning but not limited to gender, race, age, religion, and cultural \nbackground (e.g., Zieky, 2013 ). This agent uses a 5-point \nscale to assess the level of bias in each item, with items rated \n5 considered bias-free.\nThe review process culminates with the Meta Reviewer \nAgent, which synthesizes feedback from all preceding \nagents. This agent integrates the feedback and recommen-\ndations, resolves conflicting evaluations, and ensures that \neach item meets the required psychometric standards. Cru-\ncially, the Meta Reviewer Agent also incorporates human \nfeedback, allowing for expert oversight and further ensuring \nthe quality, relevance, and fairness of the final item pool. By \nintegrating human feedback, the system maintains a balance \nbetween automated efficiency and expert oversight, address-\ning potential limitations of purely AI-based evaluations.\nDemonstration of Multi‑Agent AIG \nin Practice\nIn this section, we demonstrate the application of proposed \nLM-AIG system using the construct Attitudes Toward the Use \nof AI in the Workplace (AAAW), developed by Park et al. \n(2024). The AAAW framework comprises six dimensions: i) \nhuman-likeness of AI, ii) adaptability of AI, iii) quality of AI, \nvi) AI use anxiety, v) job insecurity, and v) personal utility.\nIn contrast to prior AIG research on non-cognitive assess-\nments, which has predominantly focused on the Big Five per-\nsonality traits (e.g., Götz et al., 2024; Hernandez & Nie, 2023; \nHommel et al., 2022; Lee et al., 2023; von Davier, 2018), we \nselected the newly developed and validated AAAW constructs \nfor three key reasons. First, the widespread availability of Big \nFig. 2  Workflow of LM-AIG System via AutoGen\n Journal of Business and Psychology\nFive personality items in public databases poses a risk of infor-\nmation leakage, whereby LLMs may inadvertently replicate or \nclosely mimic existing items rather than generate novel content. \nSecond, focusing on a newer construct allows us to demonstrate \nthe flexibility and adaptability of the LM-AIG system in gen-\nerating items for emerging psychological concepts. Third, by \nusing a validated existing scale, we were able to benchmark the \nperformance of our LM-AIG framework against established, \npsychometrically sound content.\nTo implement the LM-AIG system, we utilized AutoGen, an \nopen-source tool designed to orchestrate, optimize, and automate \nmulti-agent workflows. AutoGen supports various agent types, \nincluding tool-using agents and human-in-the-loop configura-\ntions, providing flexibility in task execution across diverse con-\ntexts. More information about AutoGen is available at https:// \nmicro soft. github. io/ autog en/. The demonstration was conducted \nusing Google Colab, a cloud-based platform for executing and \nsharing Python code. This section presents descriptions of each \nagent alongside their corresponding outputs. The example code \nused in this demonstration is accessible via the provided Google \nColab link.5 Figure 2 illustrates the overall workflow of the LM-\nAIG system as implemented using AutoGen.\nWe began by installing the required packages and \nimporting the necessary libraries. Subsequently, we con-\nfigured the execution environment by setting up API 6 \nkeys for OpenAI and Perplexity Web Search. These keys \nare essential for enabling text generation (via OpenAI) \nand web search capabilities (via Perplexity). Readers \ncan obtain the required API keys by registering with the \nrespective OpenAI and Perplexity platforms.\nAfter configuring the environment, we specified the \nlanguage model settings, including model type and tem-\nperature. We used the GPT-4o model, recognized for its \nadvanced generative capabilities. Recent studies (e.g., Jia \n& Lee, 2025; Lee et al., 2024; Speer et al., 2024) have \nshown that generative LLMs, such as GPT-4 and GPT-\n4o, can effectively rate or score responses, exhibiting \npsychometric properties comparable to those of supervised \nNLP models. Following the best practices recommended \nby Ho (2024) , we set the temperature parameter to 1.0 \nfor AIG. In contrast, for the summarization task, we set \nthe temperature to 0 to minimize stochasticity, thereby \nensuring more precise, fact-based outputs from web search \nresults.\nTo generate items, we defined the six AAAW con -\nstructs in a single Python dictionary for clarity and code \nefficiency. The dictionary includes definitions and sample \nitems for each of the six dimensions as described in Park \net al. (2024). Additionally, we implemented structured \nitem-writing guidelines, summarized in Table  1, to ensure \nthat all generated items align with established psychomet-\nric standards. Although items were generated for all six \ndimensions, this demonstration section focuses specifically \non the AI use anxiety dimension.\nNext, we initialized the WebSurferAgent to collect con-\ntent related to employee attitudes toward AI use anxiety. \nThe WebSurferAgent leverages Perplexity’s web search \ncapabilities to retrieve relevant content from diverse \nonline sources, including academic articles, news, indus-\ntry reports, and blogs. To prevent information leakage, we \nexplicitly instructed the WebSurferAgent to exclude the \nstudy by Park et al. (2024) from its search queries.\nTo streamline interaction with the web search function-\nality, we defined the UserProxyAgent, which facilitates \ncommunication with the WebSurferAgent and automates the \nprocess of gathering web-based information. After specify-\ning the task parameters, we initiated the interaction between \nweb_surfer and user_proxy using user_proxy.initiate_chat. \nBelow is an excerpt of the output generated by the WebSurf-\nerAgent. The complete set of references retrieved via the \nPerplexity API can be accessed by printing the structured \nresponse citations; however, for demonstration purposes, \na condensed version is presented here. Then, the system \ncompiles and saves the results as a summary. \n5 https:// drive. google. com/ file/d/ 1fq1G HQ_ JR9dy WXxYf 6CY34 \n0zMIX hY7AQ/ view? usp= shari ng\n6 An API (Application Programming Interface) is a tool that allows \ndifferent software systems to communicate and share data with each \nother. It provides a standardized way for users to access specific fea-\ntures or services from another application without needing to know \nits internal details.\nJournal of Business and Psychology \n\n Journal of Business and Psychology\nNext, the Critic Agent, which coordinates the entire \nLM-AIG workflow, initiates the item generation process. \nThe Item Writer Agent then generates an initial pool of \neight items. 7 In the system prompt, the agent is provided \nwith a role definition emphasizing its advanced expertise \nin psychological scale development, psychometric the-\nory, and the target construct. It is instructed to generate \neight items specifically targeting the AI Use Anxiety  \ndimension, drawing on both psychological constructs \nand the insights retrieved by the WebSurferAgent. To \nensure adherence to psychometric standards, the agent is \nalso given detailed item-writing guidelines (see Table  2). \nThe initial set of eight items generated by the Item Writer \nAgent is as follows:\nNext, within the nested structure of the Critic Agent, the \nContent Reviewer Agent evaluates the content validity of \nthe generated items based on the correspondence and dis-\ntinctiveness indices proposed by Hinkin and Tracey (1999). \nTo facilitate this evaluation, the Content Reviewer Agent \nis provided with definitions for three constructs: the target \nconstruct and two orbiting constructs (i.e., conceptually \nrelated but distinct constructs). Using this information, the \nagent rates each item on a 7-point Likert scale and calculates \nthe two key indices following the procedures outlined by \nColquitt et al. (2019):\n• Correspondence (c-value) = average deﬁnitional correspondence rating\na\n• Distinctiveness (d-value)  \n= average of all (intended correspondence rating − orbiting correspondence rating )\na−1\n• where a represents the number of anchors (e.g., a −1 = 6 \nfor a 7-point scale)\nAccording to Colquitt et al. (2019), items with a c-value \ngreater than 0.88 and a d-value greater than 0.35 are con-\nsidered to have strong content validity. We classified them \nas ‘meeting’ the criterion. Items below these thresholds are \ncategorized as ‘not meeting’ the criterion. For items that do \nnot meet the specified criterion, the Content Reviewer Agent \noffers feedback to guide further refinement. An excerpt of \nthe output is presented below.\n7 For demonstration purposes, we doubled the original number of \nitems from four to eight; however, researchers need to adjust this \nquantity based on the specific objectives of their study.\nJournal of Business and Psychology \nNext, the Linguistic Reviewer Agent evaluated the AI \nUse Anxiety  items to ensure they meet linguistic stand-\nards concerning grammatical accuracy, clarity, simplicity, \nand appropriateness. The agent employed a 5-point rating \nscale, where:\n• A score of 1 (very poor) indicates major linguistic issues, \nsuch as grammatical errors, confusing sentence struc-\nture, or language exceeding a naïve rater’ reading level.\n• A score of 5 (excellent) indicates that the item is gram-\nmatically correct, stylistically consistent, and expressed \nin clear, accessible language suitable for a naïve rater.\nItems that receive a score of 5 are considered linguis-\ntically sound. For items scoring 4 or below, the agent \nprovides a detailed explanation of the issues and suggests \npossible improvements. An excerpt of the output provided \nby the Linguistic Reviewer Agent is presented below.\n Journal of Business and Psychology\nFollowing the linguistic review, the Bias Reviewer \nAgent evaluates each item for potential biases related to \ngender, ethnicity, race, age, sexual orientation, and culture. \nThe agent uses a 5-point rating scale, where:\n• A rating of 1 (highly biased) is assigned to items that contain \nsignificant or explicit bias, making it inappropriate for use.\n• A rating of 5 (completely unbiased) indicates that \nthe item is free from any identifiable bias and is fully \nappropriate for diverse populations.\nJournal of Business and Psychology \nItems receiving a score of 5 are considered unbiased \nand suitable for inclusion in psychological assessments. \nFor items rated 4 or below, the Bias Reviewer Agent \nprovides an explanation identifying the source of bias and \noffers specific suggestions for improvement. An excerpt \nof the Bias Reviewer Agent’s output is provided below:\n Journal of Business and Psychology\nNext, the Meta Editor Agent was tasked with synthe-\nsizing feedback from all preceding review stages and \nidentifying any remaining issues for revision. The follow -\ning excerpt presents the results, including an overall evalu-\nation, review summaries, and recommendations.\nJournal of Business and Psychology \nIn the final stage of the review process, human feedback \nis integrated into the LM-AIG system. To evaluate the \nLM-AIG system’s capacity for autonomous refinement, we \nintentionally limited human feedback to a minimal prompt: \n“Please refine the items following the suggestions and rec -\nommendations, then recommend the best 4 items that are \nnot too similar in content. Please tell me how you refine \nthe items .” Although more detailed feedback could have \nbeen provided for each item, this minimal input enabled \nus to examine the essential role of human intervention in \nenhancing item quality. Upon receiving the feedback, the \nCritic Agent forwards it to the Item Writer Agent, who \nrevises the items accordingly. The revised items were then \nreturned to the Critic Agent, who evaluated the revisions \nand finalized the selection of the top four items. The fol-\nlowing excerpt presents the results:\n Journal of Business and Psychology\nThe LM-AIG system recommended the following four \nitems to assess AI use anxiety: (i) I feel uneasy about using \nAI tools during my work tasks, (ii) The possibility of making \nmistakes when using AI makes me feel anxious at work, (iii) \nI am worried that I don’t have the necessary skills to use AI \nat work effectively, and (iv) I feel apprehensive about how \nAI could change my role at work. While this demonstration \nfocuses specifically on the AI use anxiety dimension, Table 3 \npresents a comparison of items generated by the multi-agent \nLM-AIG system across all six AAAW dimensions, alongside \nthose developed by Park et al. (2024).8\nTable 3  Item Comparison Between Park et al. (2023) and the LM-AIG\nDimension Park et al. (2023) LM-AIG\nAI Use Anxiety • Using AI for work is somewhat intimidating to me • I feel uneasy about using AI tools during my work \ntasks\n• I would feel nervous operating AI in front of other \npeople at work\n• The possibility of making mistakes when using AI \nmakes me feel anxious at work\n• I would feel uneasy if I was given a job where I had \nto use AI\n• I am worried that I don’t have the necessary skills to \nuse AI at work effectively\n• I would feel paranoid talking with AI at work • I feel apprehensive about how AI could change my \nrole at work\nPersonal Utility • Using AI would allow me to have increased confi-\ndence in my skills at work\n• AI tools enhance my efficiency in completing work \ntasks\n• Using AI would provide me with personal feelings \nof worthwhile accomplishment at work\n• Using AI increases my confidence in handling work \nchallenges\n• Using AI would provide me with feelings of enjoy-\nment at work from using the technology\n• AI streamlines routine tasks, allowing me to focus on \nimportant work\n• Using AI would give me greater control over my \nwork\n• The learning resources provided by AI aid my career \ndevelopment\nPerceived Humanlikeness of AI • AI has desires • AI systems seem capable of expressing emotions\n• AI has beliefs • AI systems act in ways that resemble human behavior\n• AI has ability to experience emotion • Interacting with AI feels natural and human-like\n• AI has free will • AI displays empathy similar to humans\nPerceived Adaptability of AI • AI learns from experience at work • AI in the workplace can learn and improve its func-\ntions\n• AI improves itself at work • AI systems at work can adjust to new tasks\n• AI can learn at work • AI can develop new skills when facing work chal-\nlenges\n• AI adapts itself over time at work • AI's learning capacity enhances efficiency at work\nPerceived Quality of AI • AI provides workers with a complete set of infor-\nmation\n• AI consistently provides accurate information at work\n• AI produces correct information • AI provides reliable data for my tasks\n• The information provided by AI is well formatted • AI presents information in a clear format\n• AI operates reliably • AI applications function consistently without errors\nJob Insecurity • I am worried that what I can do now with my work \nskills will be replaced by AI\n• I think AI could replace my role at work\n• I am worried about my career due to AI replacing \nemployees\n• I believe my job could be replaced by AI in the near \nfuture\n• I think my job could be replaced by AI • I feel my job security is at risk because of AI\n• I am worried about AI replacing what humans can \ndo at work\n• I worry that AI could perform my tasks better than \nI can\n8 Park et al. (2024) originally developed a scale comprising 25 items \nacross six dimensions; however, one item was intentionally omitted \nin the present study to align with the total of 24 AI-generated items.\nJournal of Business and Psychology \nEmpirical Evaluation Based on Human \nRatings\nParticipants and Procedures\nTo evaluate the quality of the AI-generated items, we con-\nducted an empirical study using structured ratings provided \nby human raters. Twenty-one graduate students enrolled in \nan industrial-organizational psychology program partici-\npated as human raters. Training sessions were conducted in \nsmall groups of one to three participants, during which we \nreviewed the study procedures and evaluation criteria.\nFollowing the training, participants completed rating \ntasks through a Qualtrics survey. Content validity was \nassessed using a sorting task adapted from Anderson and \nGerbing (1991). After practicing with sample items, partici-\npants reviewed operational definitions of six dimensions of \nattitudes toward AI at work. They then sorted both AI-gen-\nerated items and the original items from Park et al. (2024)—\nhereafter referred to as human-generated items—into the \ndimension they believed best reflected each item's content.\nItem quality was assessed according to four key crite-\nria: i) linguistic clarity (“To what extent is the item clearly \nworded?”), ii) appropriate language level (“What extent is \nthe item written at a reading level that is easily understand-\nable by the target population (e.g., employees)”), iii) contex-\ntual specificity (“To what extent does the item reflect practi-\ncal, observable workplace experiences that are relevant to \nthe construct being measured”), and iv) freedom of bias (“To \nwhat extent is the item free from language or content that \ncould disadvantage any demographic group (e.g., gender, \nreligion, race, age, culture)?”). These criteria were drawn \nfrom established item development guidelines (see Table 2). \nEach was assessed using a single item rated on a 5-point \nLikert scale.\nThe mean age of participants was 29.00 years (SD = 6.41), \nand 81.0% identified as female. Regarding racial back -\nground, 57.1% identified as White, 4.8% as Black, 33.3% as \nAsian, and 4.8% as Other. Participants were informed dur -\ning the training process that they would be evaluating both \nhuman- and AI-generated items. However, they were blinded \nto item source throughout the survey, unaware of whether \nitems were AI- or human-generated. To mitigate potential \norder effects, both the sequence of the rating tasks and the \npresentation order of items were randomized.\nAnalytical Strategy\nWe assessed content validity using two indices from Colquitt \net al. (2019): the proportion of substantive agreement  (Psa), \nwhich reflects how frequently participants assigned an item to \nits intended construct, and the substantive validity coefficient \n (Csv), which indicates the item’s distinctiveness from related \nconstructs. Both indices were computed at the item level. For \nitem quality, we evaluated descriptive statistics (means and \nstandard deviations) and graphical representations for each of \nthe four criteria across all items and dimensions.\nResult\nTable 4 presents the mean and standard deviation of two \ncontent validity indices,  Psa and  Csv), for both human- and \nAI-generated items across the six AAAW dimensions. In \nthis comparison, items from Park et al. (2024) served as \nthe baseline and consistently received higher ratings from \nhuman evaluators than the AI-generated items.\nAI-generated items demonstrated strong content validity \nin four dimensions: human-likeness of AI, job insecurity, \nquality of AI, and personal utility. In these dimensions, both \n Psa and  Csv values exceeded 0.90. Notably, for the quality of \nAI dimension, AI-generated items received slightly higher \nratings  (Psa = 1.00 and  Csv = 1.00) than human-authored \nitems  (Psa = 0.98,  Csv = 0.96). A similar trend was observed \nin the personal utility dimension, where AI-generated items \nwere rated slightly higher  (Psa = 0.95 and  Csv = 0.91) com-\npared to human-authored items  (Psa = 0.93 and  Csv = 0.88). \nIn contrast, AI-generated items received comparatively \nlower ratings in the adaptability of AI  (Psa = 0.85,  Csv = 0.75) \nand AI use anxiety  (Psa = 0.76,  Csv = 0.73) dimensions, where \nhuman- authored items achieved perfect scores  (Psa = 1.00, \n Csv = 1.00).\nTo further investigate the validity of AI-generated items, \nwe examined ratings at the item level. Figure  3 presents \nbox plots of SME ratings for four items per dimension. It \nrevealed that three AI-generated items received substantially \nlower ratings than their human-generated counterparts. In \nthe adaptability of AI dimension, item 4 (“AI’s learning \ncapacity enhances efficiency at work”) received particu-\nlarly low scores  (Psa = 0.43,  Csv = 0.10). Similarly, in the AI \nuse anxiety dimension, item 4 (“I feel apprehensive about \nhow AI could change my role at work”) was rated poorly \n (Psa = 0.33,  Csv = 0.29). In the job insecurity dimension, \nitem 4 (“I worry that AI could perform my tasks better than \nI can”) showed a moderate decline in ratings  (Psa = 0.76, \n Csv = 0.52). These underperforming items notably contrib-\nuted to the lower average validity scores observed within \ntheir respective dimensions.\nTo better understand the reasons underlying the low \ncontent validity ratings, we conducted follow-up group \ninterviews with the human raters. During these sessions, \nparticipants were asked to elaborate on their evaluations, \nparticularly for items that had received comparatively lower \nratings. For item 4 in the adaptability of AI dimension (“AI’s \n Journal of Business and Psychology\nlearning capacity enhances efficiency at work”), human \nraters noted that the item was misaligned with the core con-\nstruct. Specifically, while adaptability is defined by behav -\nioral responsiveness and flexibility, the item emphasized \nperformance outcomes—namely, increased efficiency—\nrather than adaptability per se. As such, human raters con-\ncluded that the item introduces construct-irrelevant content. \nRegarding item 4 in the AI use anxiety dimension (“I feel \napprehensive about how AI could change my role at work”), \nhuman raters pointed out that the item blurred the construct \nbetween anxiety and job insecurity. Although it expressed \nanxiety, the phrasing—particularly “how AI could change \nmy role”—was seen as emphasizing anticipatory uncertainty \nand potential role disruption, aligning more closely with job \ninsecurity than with the immediate emotional distress typi-\ncally associated with anxiety. For item 4 in the job insecurity \ndimension (“I worry that AI could perform my tasks bet-\nter than I can”), human raters acknowledged its conceptual \nrelevance but attributed its moderate ratings to speculative \nframing. The modal verb “could” conveys a hypothetical \nthreat, reducing the item’s immediacy. Additionally, the \ncomparative phrase “better than I can” was viewed as poten-\ntially reflecting performance anxiety or concerns about self-\nefficacy, rather than signaling job insecurity in a direct and \nunambiguous manner.\nNext, we assessed item quality across six dimensions \nusing four evaluative criteria: linguistic clarity, appropri-\nate language level, contextual specificity, and freedom from \nbias. Figure  4 presents the mean ratings for human- and \nAI-generated items on each criterion. AI-generated items \nconsistently outperformed human-authored items in linguis-\ntic clarity, particularly in the AI use anxiety (AI = 4.81 vs. \nHuman = 4.53), job insecurity (AI = 4.82 vs. Human = 4.61), \nand personal utility (AI = 4.57 vs. Human = 4.33) dimen-\nsions. For language appropriateness, both item types \nreceived highly similar ratings (range: 4.74–4.93), indicat-\ning comparable language appropriateness for the intended \npopulation. In terms of contextual specificity, although some \nvariation was observed across dimensions, AI-generated \nitems were generally rated as comparable to or slightly \nsuperior to human-authored items. For example, ratings for \nAI-generated items exceeded those of human-authored items \nin adaptability of AI (AI = 4.86 vs. Human = 4.79), human-\nlikeness of AI (AI = 3.75 vs. Human = 3.46), and quality of \nAI (AI = 4.26 vs. Human = 3.85). Finally, both item types \nreceived consistently high and nearly identical ratings on \nfreedom from bias (range: 4.74–4.98), suggesting minimal \nbias and no meaningful differences between the two formats.\nAs previously noted, minimal human feedback was pro-\nvided, and the system relied primarily on the autonomous deci-\nsion-making of AI agents. Overall, the results were promising. \nOf the 24 AI-generated items, 21 were deemed appropriate \nin terms of content validity by human raters, while 3 items \nwere identified as problematic. Moreover, the AI-generated \nitems consistently demonstrated high quality across all four \nevaluative criteria: linguistic clarity, appropriateness language \nlevel, contextual specificity, and absence of bias. Nevertheless, \nparticular attention is warranted for the three items that the AI \nagent system failed to generate appropriately. Taken together, \nthese findings suggest that while the multi-agent system is \neffective in producing items with strong psychometric proper-\nties, expert human feedback remains essential—particularly for \nensuring content validity. This point will be further elaborated \nin the subsequent discussion section.\nDiscussion\nThis research introduces a conceptual framework for \nan AIG system powered by LLM-based multi-agents. \nThe proposed LM-AIG system comprises specialized \nagents, each assigned to a distinct role in the item devel-\nopment process: the Critic Agent orchestrates the AIG \nworkflow, the Item Writing Agent  generates the initial \nitem pool, the Content Review Agent ensures content \nrelevance and accuracy, the Linguistic Review Agent  \nevaluates clarity and readability, the Bias Review Agent  \nassesses potential biases, and the Meta Editor Agent  \nTable 4  Human evaluation of \ncontent validity indices between \nhuman- and AI-generated items\n* Note:  Psa = proportion of substantive agreement;  Csv = substantive validity coefficient\nDimension Human-Generated AI-Generated\nPsa Csv Psa Csv\nMean SD Mean SD Mean SD Mean SD\nAdaptability of AI 1.00 0.00 1.00 0.00 0.85 0.28 0.75 0.44\nAI Use Anxiety 1.00 0.00 1.00 0.00 0.76 0.37 0.73 0.39\nHuman-likeness of 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00\nJob Insecurity 0.98 0.05 0.95 0.10 0.93 0.11 0.90 0.13\nQuality of AI 0.98 0.03 0.96 0.05 1.00 0.00 1.00 0.00\nPersonal Utility 0.93 0.02 0.88 0.05 0.95 0.00 0.91 0.00\nJournal of Business and Psychology \nsynthesizes feedback from all agents and incorporates \nhuman input. We demonstrated the effectiveness of the \nLM-AIG framework by generating items measuring atti -\ntudes toward the use of AI in the workplace. Empirical \nevaluations based on human ratings revealed that 21 out \nof 24 AI-generated items were comparable in quality to \nhuman-authored items. While overall performance was \npromising, our findings reveal important implications \nand design considerations, which we discuss below.\nConsiderations in the Use of LM‑AIG Systems\nThe first key consideration concerns the role of human feed-\nback within the LM-AIG system. Despite minimal human \nfeedback9 in our LM-AIG demonstration, the majority of \nAI-generated items received good evaluations across key \ncriteria, including linguistic clarity, appropriate language \nlevel, contextual specificity, and freedom from bias. These \nresults suggest that the LM-AIG system is capable of auton-\nomously producing linguistically high-quality items, even \nwith limited human intervention. In contrast, human raters \nraised concerns regarding the content validity for three \nitems. These results may reflect a fundamental challenge for \nLLMs. While LLMs excel at generating fluent and structur-\nally coherent text, such surface-level fluency does not guar-\nantee accurate representation of psychological constructs. \nThis challenge is especially relevant in the context of item \ndevelopment, where conceptual precision is critical. As one \nreviewer noted, delegating item generation entirely to AI \nmay inadvertently distance researchers from the constructs \nthey aim to assess. Human involvement in item writing \nFig. 3  Content validity indices (Psa and Csv) for human-generated and ai-generated items\n9 This refers to human feedback within the LM-AIG system \ndescribed on page 27. In the empirical evaluation, however, human \nraters focused solely on evaluating the quality of the generated items, \nrather than providing feedback to the AI.\n Journal of Business and Psychology\noften fosters deeper theoretical engagement and enhances \nconstruct alignment. Reduced human input, therefore, may \ncompromise the conceptual integrity of the items.\nTo mitigate this risk, strategically positioned human feed-\nback within the LM-AIG system is essential. In the present \nstudy, which serves as a conceptual introduction to the LM-\nAIG framework, human input was incorporated only at the \nfinal stage of the multi-agent pipeline. However, human-in-\nthe-loop mechanisms can be implemented in various ways, \nincluding the integration of human oversight at multiple \nstages throughout the system. While such an approach may \nenhance item quality by providing continuous feedback, it \nmay also introduce inefficiencies or operational bottlenecks. \nOur findings suggest that the LM-AIG system is capable of \nindependently generating items that meet high standards in \nterms of linguistic clarity, appropriate language level, con-\ntextual specificity, and bias mitigation. However, concerns \nregarding construct alignment remain. Accordingly, we rec-\nommend that researchers incorporate targeted human feed-\nback after the Content Review Agent stage. This selective \nintervention allows for the early detection and resolution of \nconstruct-related issues, thereby improving content valid-\nity without significantly diminishing the efficiency of the \noverall system.\nAdditionally, as noted by an anonymous reviewer, involv-\ning human experts earlier in the item generation process may \nyield further benefits. Introducing human input during the \nideation and initial generation phases could enhance content \nquality from the outset, without anchoring human judgment \nto AI-generated outputs or limiting their role to late-stage \nreview. This approach may help preserve the originality and \ndiversity of ideas, while still leveraging the strengths of AI. \nThis line of inquiry highlights the need for future research \nfocused on optimizing human–AI collaboration throughout \nthe item development process, with particular emphasis on \nbalancing creativity, efficiency, and content validity.\nAnother important consideration concerns the role of \nprompt engineering in shaping the stylistic and structural \nfeatures of AI-generated items. In our analysis, AI-generated \nitems were generally longer, ranging from 10 to 16 words, \ncompared to human-authored items, which typically fell \nwithin the 8 to 12-word range. Moreover, AI-generated \nitems more frequently included modal verbs (e.g., “could,” \n“can”) and adopted a hypothetical tone. This pattern aligns \nwith prior research indicating that LLMs exhibit a natural \ntendency toward verbosity and syntactic complexity (Bria-\nkou et al., 2024; Saito et al., 2023). Such stylistic differences \nbetween human-authored and AI-generated items highlight \nthe importance of carefully designed prompting strategies. \nFor example, prompts that explicitly constrain item length, \ndiscourage hypothetical tone, or prohibit certain modal \nphrases can help guide LM-AIG to produce items that reflect \nthe concise and declarative style typical of human-authored \ncontent. Through careful prompt refinement, researchers can \nenhance stylistic alignment of AI-generated items.\nTest security is another critical consideration in the imple-\nmentation of the LM-AIG sytstem. The use of closed-source \nLLMs controlled by major technology companies—such as \nOpenAI, Google, and Microsoft—poses potential risks to \ntest security. When the agent system relies on APIs from \nthese providers, there remains a possibility that test items or \ndetails of the item generation process for high-stakes assess-\nments could be exposed, despite their assurances that data \nwill not be disclosed or misused. For contexts where test \nsecurity is paramount, researchers are advised to consider \nintegrating open-source LLMs, such as Llama or Mistral, \ninto the LM-AIG pipeline, rather than relying on closed-\nsource models like OpenAI’s GPT, Anthropic’s Claude, or \nGoogle’s Gemini. Open-source alternatives offer greater \ntransparency and control over data handling and system \narchitecture, thereby reducing the likelihood of data leakage \nand enhancing the security of assessment content throughout \nthe item generation process.\nAdditionally, researchers should be aware that the system \nmay encounter so-called “dead ends,” where further itera-\ntions fail to yield meaningful improvements in item quality. \nSuch stagnation can arise for various reasons, but it is par -\nticularly likely when the data underpinning the item genera-\ntion framework is not well-aligned with the task at hand. \nThese concerns highlight the need for critical engagement: \nresearchers must carefully scrutinize the system’s assess-\nments, reasoning, and recommendations, applying their own \nexpertise and judgment to ensure the quality, validity, and \nrelevance of the generated content.\nIt is also important to acknowledge the probabilistic \nnature of LLMs. Their outputs are non-deterministic due \nto stochastic sampling (e.g., temperature, top-k, 10 top-p11 \nsampling), floating-point arithmetic,12 and parallel computa-\ntion on GPUs/TPUs. As a result, identical prompts may yield \ndifferent outputs across runs. Researchers must document \n10 Top-k sampling looks at the “k” most likely next words (for \nexample, the top 10 words), and then randomly picks one of them.\n11 Instead of choosing from a fixed number of top words, the Top-p \nsampling looks at the top words that together make up a certain per -\ncentage (like 90%) of the likelihood.\n12 Floating-point arithmetic is a way computers handle very large or \nvery small numbers. Instead of storing every number exactly (which \nwould take up too much space), they store an approximation of the \nnumber using a fixed amount of memory.\nJournal of Business and Psychology \nthe system’s parameters—including model version, sam-\npling strategy, and agent decisions—to ensure transparency, \nreproducibility, and fair evaluation.\nFurthermore, researchers should be cautious about any \nunexpected information leakage. To minimize this risk, \nwe explicitly instructed the WebSurferAgent to exclude \nthe study by Park et al. (2024) from its search queries. \nAlthough we confirmed that our generated items did not \ndirectly replicate Park et al.’s original items, simply instruct-\ning the model to avoid content from a specific source does \nnot fully guarantee the prevention of information leakage. \nSimilar or identical items may appear in other publications, \nand given the vast and often opaque nature of LLM train-\ning data, ensuring that generated content is entirely novel \nremains a challenge. The risk of inadvertently reproducing \ncopyrighted material—particularly when generated items \nclosely resemble or replicate existing scale items—is there-\nfore a legitimate concern. To address this, subject matter \nexperts should develop carefully crafted prompt templates \nthat account for a wide range of relevant source materials \nand conduct thorough reviews of AI-generated content—not \nonly for conceptual relevance and clarity, but also for poten-\ntial overlap with existing measures. This additional layer \nof scrutiny can help mitigate ethical and legal risks while \nsupporting the development of original and valid assess-\nment items.\nFinally, researchers employing LM-AIG systems must \nremain informed about evolving regulatory frameworks. \nFor example, in July 2023, New York City 13 implemented \na regulation requiring employers and employment agencies \nutilizing automated employment decision tools to conduct \nindependent annual bias audits and to provide advance \nnotice to candidates. Similarly, in the European Union, the \nEU AI Act, 14 effective as of February 2025, establishes \nspecific requirements for “high-risk” AI systems, includ-\ning those used in employment testing and decision-making. \nDevelopers and users of LM-AIG systems must therefore \nremain vigilant and responsive to emerging legal and ethi-\ncal standards.\nFig. 4  Human ratings of item quality indices for human- and AI-generated items\n13 https:// www. nyc. gov/ site/ dca/ about/ autom ated- emplo yment- decis \nion- tools. page\n14 https:// eur- lex. europa. eu/ legal- conte nt/ EN/ TXT/? uri= CELEX: \n52021 PC0206\n Journal of Business and Psychology\nTaken together, these considerations highlight the impor-\ntance of thoughtful agent design and the strategic integra -\ntion of human feedback throughout the LM-AIG process. \nIn particular, bias review is a complex and highly context-\ndependent task that lacks a universally accepted “ground \ntruth” for what constitutes bias, intersecting with longstand-\ning challenges in algorithmic fairness (e.g., Langer et al., \n2025; Mehrabi et al., 2021). The outcome of such evalua-\ntions can vary depending on how bias is defined and which \nprompts are used. Therefore, it is essential that each agent—\nespecially those tasked with evaluating bias—is carefully \ncrafted with clearly articulated criteria.\nAdvantages of LM‑AIG Systems for Personnel \nAssessments\nWhen carefully designed to incorporate these consid-\nerations, a multi-agent AIG framework offers several key \nadvantages over conventional AIG approaches that rely on \na single LLM. While a single LLM can be prompted to per-\nform multiple tasks sequentially, such as item generation, \nbias detection, and language optimization, this approach \noften leads to reduced clarity in role execution, overlapping \ncontextual information, and diminished control over quality \nat each stage (Ashery et al., 2024; Chandna et al., 2025; \nZong et al., 2024). In contrast, the multi-agent framework \naddresses these limitations by assigning distinct responsibili-\nties to specialized agents and incorporating targeted human \nfeedback at critical junctures (Tran et al., 2025). This clear \ndivision of roles enhances transparency, task specificity, and \ncontrol over the output.\nThe second advantage of the LM-AIG is its flexibility. By \nutilizing heterogeneous LLMs, each agent can leverage the \nspecialized strengths of different models. For example, an \nagent tasked with advanced logical reasoning may use GPT-\no1, known for its strong reasoning and analytical capabili-\nties, while a content generation agent could employ Claude \n4 for its sophisticated and coherent text generation. Mean-\nwhile, a multimedia analysis agent might employ Gemini \n2.0 for its advanced multimodal processing, integrating \ntext, image, audio, and video generation. This targeted use \nof models ensures that each agent operates with the LLM \nbest suited to its role and task, thereby enhancing the overall \nquality of the system.\nThe third advantage is its support for dynamic collabora-\ntion structures among agents. In this study, we implemented \na hybrid architecture that combined hierarchical layer -\ning with centralized coordination. However, the system’s \ndesign allows for flexible adaptation to various collaboration \nmodels tailored to different project goals. For example, a \ndynamic group discussion structure may be used, wherein \nmultiple agents engage concurrently in a roundtable-style \ndialogue to share insights and concerns. The AutoGen \nframework further expands these possibilities, enabling \nresearchers to implement alternative communication struc-\ntures,15 such as sequential conversation, hierarchical review, \nand iterative refinement loops. Such flexibility allows the \nLM-AIG system to optimize test development workflows for \ndifferent psychological domains and operational contexts.\nFinally, the LM-AIG system can integrate a wide range of \nAI tools, such as web search, web scraping, and Retrieval-\nAugmented Generation (RAG), 16 within the multi-agent \nframework. These tools support a more comprehensive \napproach to item development by facilitating both deduc-\ntive17 and inductive18 strategies, a practice widely advocated \nin psychometric literature (e.g., Boateng et al., 2018; Hinkin, \n1995). Specifically, the system can leverage AI capabili-\nties to extract construct-relevant behaviors, language, and \ncontextual nuances from diverse online sources, including \nforums, blogs, reports, and social media. This allows for the \ndevelopment of items that reflect the lived experiences and \nbehaviors of the intended population, thus complementing \ntraditional theory-driven approaches and expert judgments.\nFuture Research Directions\nA promising avenue for future research involves evaluating \nwhether agent-based systems can effectively augment—or \neven replace—the role of human experts in defining and \nselecting psychological constructs. In the present study, \nhuman experts defined focal construct, provided sample \nitems, and also identified related orbiting constructs based \non established theoretical frameworks. We regard this \nmanual approach as a necessary safeguard to ensure con-\nstruct relevance and theoretical coherence. However, future \ninvestigations should explore the feasibility of agent sys-\ntems autonomously identifying psychological constructs \nand generating corresponding definitions by synthesizing \n15 https:// micro soft. github. io/ autog en/0. 2/ docs/ Use- Cases/ agent_ \nchat/\n16 Retrieval Augmented Generation (RAG) is a technique that \nimproves AI by combining information retrieval with response gener-\nation. First, it searches a database for relevant facts or data (retrieval). \nThen, it uses that information to create a well-informed, detailed \nanswer (generation). This approach makes AI responses more accu-\nrate and context-aware, especially for complex or specific questions.\n17 Deductive methods involve generating items grounded in well-\nestablished constructs identified through literature reviews and theo-\nretical frameworks, thus ensuring conceptual clarity and construct \nvalidity.\n18 Inductive methods involve generating items from data collected \ndirectly from the target population, such as focus groups interviews or \nopen-ended responses.\nJournal of Business and Psychology \nopen-source materials. If successful, such capabilities could \nsubstantially enhance the flexibility of AI-driven assessment \ndevelopment.\nNext, the level of human input required in the LM-AIG \npipeline warrants systematic investigation. For example, \nthree configurations can be empirically compared: (1) fully \nautomated generation with final-stage human review, (2) \nsemi-automated cycles with expert feedback after each \nround, and (3) collaborative co-creation, where experts \nset templates and intervene selectively. Human input may \nbe treated as a tunable parameter, and future work should \naim to empirically identify the most efficient and rigorous \nconfiguration.\nThird, the LM-AIG systems hold significant potential for \nthe development of situational judgment tests (SJTs), par -\nticularly in contexts where access to incumbents or subject \nmatter experts is limited. By leveraging tools such as Web-\nSearch or retrieval-augmented generation (RAG), agents \ncan extract relevant information and examples from diverse \nonline sources—including news articles, case studies, \nindustry reports, academic publications, and professional \nforums—to generate realistic, job-relevant scenarios. More-\nover, agents can be assigned different levels of knowledge, \nskills, abilities, and other characteristics (KSAOs), ena-\nbling the generation of response options that reflect varying \ndegrees of effectiveness and expertise.\nFourth, we did not ask the human raters to assess whether \nthey could distinguish between AI-generated and human-\nauthored items. Nonetheless, exploring whether such distinc-\ntions can be reliably made represents an interesting direc-\ntion for future research. This line of inquiry raises important \nquestions about the perceived authenticity and psychological \nrealism of AI-generated content—factors that may influence \nhow individuals interpret and respond to such items, with \npotential implications for construct validity.\nFifth, we did not employ a prompt specifically designed to \nstylistically align the AI-generated items with those written \nby humans. Comparing stylistically aligned and non-aligned \nitems could yield valuable insights into the role of prompt \ndesign in shaping item quality and perceived naturalness. \nWe encourage future research to investigate these questions \nin a more systematic manner.\nSixth, future research should explore the extension of \nthe LM-AIG system beyond text-based assessments to mul-\ntimodal formats that incorporate text, audio, images, and \nvideo. Advanced LMMs can enable agents to create interac-\ntive, game-like scenarios designed to measure complex traits \nsuch as creativity, emotional intelligence, problem-solving, \nstrategic thinking, and personality. In addition, the LM-AIG \nframework could be adapted for use with alternative item \nformats beyond Likert-type scales, including forced-choice \nand other nontraditional assessment formats. Exploring \nthese possibilities may significantly broaden the scope and \nutility of AI-assisted test development across diverse psy -\nchological domains.\nFinally, an interesting direction for future research would \nbe to examine how the use of the LM-AIG framework com-\npares to traditional human-developed test construction in \nterms of time, cost, and effort efficiency. Although it is often \nassumed that AI-assisted approaches are more efficient than \nthose developed solely by human experts (Lee et al., 2023), \nthere is currently limited empirical evidence to support this \nclaim. Therefore, investigating this question would provide \nmeaningful insights into the practical value of AI-assisted \ntest development.\nConclusion\nAs industries and organizations increasingly adopt \nAI-based assessments, the demand for more scalable, \nadaptive, and valid assessment tools continues to grow \n(Campion & Campion, 2023; SIOP, 2023). This study \nintroduces a novel approach to test development through \na large language model-based multi-agent item genera -\ntion system. We hope this research lays a foundation for \ncontinued innovation in AI-assisted test development and \ninspires future work exploring how collaborative AI sys-\ntems can enhance the validity, equity, and effectiveness \nof assessments in both psychological and organizational \ndomains.\nData Availability The data that support the findings of this study are \navailable from the first author upon reasonable request.\nDeclarations \nConflict of interest The authors declare that they have no conflict of \ninterest.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAher, G. V., Arriaga, R. I., & Kalai, A. T. (2023). Using large lan -\nguage models to simulate multiple humans and replicate human \n Journal of Business and Psychology\nsubject studies. Proceedings of Machine Learning Research, 202, \n337–371.\nAnderson, J. C., & Gerbing, D. W. (1991). Predicting the performance \nof measures in a confirmatory factor analysis with a pretest \nassessment of their substantive validities. Journal of Applied \nPsychology, 76, 732–740. https:// doi. org/ 10. 1037/ 0021- 9010. \n76.5. 732\nAshery, A. F., Aiello, L. M., & Baronchelli, A. (2024). The dynamics \nof social conventions in LLM populations: Spontaneous emer -\ngence, collective biases and tipping points. arXiv. https:// arxiv. \norg/ abs/ 2410. 08948\nBoateng, G. O., Neilands, T. B., Frongillo, E. A., Melgar-Quiñonez, \nH. R., & Young, S. L. (2018). Best practices for developing and \nvalidating scales for health, social, and behavioral research: A \nprimer. Frontiers in Public Health, 6, 149. https:// doi. org/ 10. \n3389/ fpubh. 2018. 00149\nBriakou, E., Liu, Z., Cherry, C., & Freitag, M. (2024). On the implica-\ntions of verbose LLM outputs: A case study in translation evalu-\nation. arXiv. https:// arxiv. org/ abs/ 2410. 00863\nBrislin, R. W. (1970). Back-translation for cross-cultural research. \nJournal of Cross-Cultural Psychology, 1(3), 185–216. https:// \ndoi. org/ 10. 1177/ 13591 04570 00100 301\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhari-\nwal, P., ... & Amodei, D. (2020). Language models are few-shot \nlearners. Advances in Neural Information Processing Systems, \n33, 1877–1901.\nBudhwar, P., Chowdhury, S., Wood, G., Aguinis, H., Bamber, G. J., \nBeltran, J. R., ... & Varma, A. (2023). Human resource manage-\nment in the age of generative artificial intelligence: Perspectives \nand research directions on ChatGPT. Human Resource Manage-\nment Journal, 33(3), 606–659. https:// doi. org/ 10. 1111/ 1748- \n8583. 12524\nBulut, O., Beiting-Parrish, M., Casabianca, J. M., Slater, S. C., Jiao, H., \nSong, D., ... & Morilova, P. (2024). The Rise of Artificial Intel-\nligence in Educational Measurement: Opportunities and Ethical \nChallenges. arXiv preprint arXiv:2406.18900.\nCalderón, J. L., Morales, L. S., Liu, H., & Hays, R. D. (2006). Varia-\ntion in the readability of items within surveys. American Journal \nof Medical Quality, 21(1), 49–56. https:// doi. org/ 10. 1177/ 10628 \n60605 283572\nCaliskan, A., & Lewis, M. (2020). Social biases in word embeddings \nand their relation to human cognition. PsyArXiv. https:// doi. org/ \n10. 31234/ osf. io/ d84kg\nCampion, M. A., & Campion, E. D. (2023). Machine learning appli-\ncations to personnel selection: Current illustrations, lessons \nlearned, and future research. Personnel Psychology, 76(4), 993–\n1009. https:// doi. org/ 10. 1111/ peps. 12621\nCañas, J. J. (2022). AI and ethics when human beings collaborate with \nAI agents. Frontiers in Psychology, 13, 836650. https:// doi. org/ \n10. 3389/ fpsyg. 2022. 836650\nChan, C. M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., ... & Liu, Z. \n(2023). Chateval: Towards better LLM-based evaluators through \nmulti-agent debate. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2308. \n07201\nChandna, B., Bashir, Z., & Sen, P. (2025). Dissecting bias in LLMs: \nA mechanistic interpretability perspective. arXiv. https:// arxiv. \norg/ abs/ 2506. 05166\nCharlesworth, T. E. S., & Banaji, M. R. (2021). Word embeddings \nreveal social group attitudes and stereotypes in large language \ncorpora. In M. Dehghani & R. L. Boyd (Eds.), Atlas of language \nanalysis in psychology. Guilford Press.\nChen, Y., Arkin, J., Zhang, Y., Roy, N., & Fan, C. (2024). Scalable \nmulti-robot collaboration with large language models: Central-\nized or decentralized systems? IEEE International Conference \non Robotics and Automation (ICRA), 2024, 4311–4317. https:// \ndoi. org/ 10. 1109/ ICRA5 7147. 2024. 10610 676\nCheng, Y., Zhang, C., Zhang, Z., Meng, X., Hong, S., Li, W., ... & \nHe, X. (2024). Exploring large language model based intelligent \nagents: Definitions, methods, and prospects. arXiv. https:// doi. \norg/ 10. 48550/ arXiv. 2401. 03428\nCirci, R., Hicks, J., & Sikali, E. (2023). Automatic item generation: \nFoundations and machine learning-based approaches for assess-\nments. Frontiers in Education, 8, 858273.\nClark, L. A., & Watson, D. (2019). Constructing validity: New developments \nin creating objective measuring instruments. Psychological Assess-\nment, 31(12), 1412–1427. https:// doi. org/ 10. 1037/ pas00 00626\nColquitt, J. A., Sabey, T. B., Rodell, J. B., & Hill, E. T. (2019). Content \nvalidation guidelines: Evaluation criteria for definitional corre-\nspondence and definitional distinctiveness. Journal of Applied \nPsychology, 104(10), 1243–1265. https:// doi. org/ 10. 1037/ apl00 \n00406\nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chan-\ndhok, S., ... & Pennebaker, J. W. (2023). Using large language \nmodels in psychology. Nature Reviews Psychology, 2(11), 688–\n701. https:// doi. org/ 10. 1038/ s44159- 023- 00241-5\nDiStefano, C., & Motl, R. W. (2006). Further investigating method \neffects associated with negatively worded items on self-report \nsurveys. Structural Equation Modeling, 13(3), 440–464. https:// \ndoi. org/ 10. 1207/ s1532 8007s em1303_6\nDu, Y., Li, S., Torralba, A., Tenenbaum, J. B., & Mordatch, I. (2023). \nImproving factuality and reasoning in language models through \nmultiagent debate. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2305. 14325\nFan, J., Sun, T., Liu, J., Zhao, T., Zhang, B., Chen, Z., ... & Hack, E. \n(2023). How well can an AI chatbot infer personality? Examin-\ning psychometric properties of machine-inferred personality \nscores.  Journal of Applied Psychology,  108(8), 1277–1299. \nhttps:// doi. org/ 10. 1037/ apl00 01082\nFyffe, S., Lee, P., & Kaplan, S. (2024). “Transforming” personality \nscale development: Illustrating the potential of state-of-the-art \nnatural language Processing. Organizational Research Methods, \n27(2), 265–300. https:// doi. org/ 10. 1177/ 10944 28123 11557 71\nGallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Der-\nnoncourt, F., ... & Ahmed, N. K. (2024). Bias and fairness in \nlarge language models: A survey.  Computational Linguistics, \n50(3), 1097–1179. https:// doi. org/ 10. 1162/ coli_a_ 00524\nGhaffarzadegan, N., Majumdar, A., Williams, R., & Hosseinichimeh, \nN. (2024). Generative agent-based modeling: An introduction \nand tutorial. System Dynamics Review, 40(1), e1761. https:// doi. \norg/ 10. 1002/ sdr. 1761\nGierl, M. J., & Haladyna, T. M. (2012). Automatic item generation: \nTheory and practice. Routledge. https:// doi. org/ 10. 4324/ 97802 \n03803 912\nGierl, M. J., & Lai, H. (2018). Using automatic item generation to cre-\nate solutions and rationales for computerized formative testing. \nApplied Psychological Measurement, 42(1), 42–57. https:// doi. \norg/ 10. 1177/ 01466 21617 726788\nGinsberg, M. (2012). Essentials of artificial intelligence. Morgan Kauf-\nman Publishers, Inc.\nGoldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R., Ashton, M. \nC., Cloninger, C. R., & Gough, H. G. (2006). The international \npersonality item pool and the future of public-domain personal-\nity measures. Journal of Research in Personality, 40(1), 84–96. \nhttps:// doi. org/ 10. 1016/j. jrp. 2005. 08. 007\nGötz, F. M., Maertens, R., Loomba, S., & van der Linden, S. (2024). \nLet the algorithm speak: How to use neural networks for auto-\nmatic item generation in psychological scale development. Psy-\nchological Methods, 29(3), 494–518. https:// doi. org/ 10. 1037/ \nmet00 00540\nGuo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N. V., ... & \nZhang, X. (2024). Large language model based multi-agents: \nA survey of progress and challenges. arXiv. https:// doi. org/ 10. \n48550/ arXiv. 2402. 01680\nJournal of Business and Psychology \nHaladyna, T. M., & Rodriguez, M. C. (2013). Developing and validat-\ning test items. Routledge.\nHambleton, R. K., Merenda, P. F., & Spielberger, C. D. (Eds.). \n(2004). Adapting educational and psychological tests for cross-\ncultural assessment. Psychology Press.\nHao, J., von Davier, A. A., Yaneva, V., Lottridge, S., von Davier, M., \n& Harris, D. J. (2024). Transforming assessment: The impacts \nand implications of large language models and generative ai. \nEducational Measurement: Issues and Practice, 43(2), 16–29. \nhttps:// doi. org/ 10. 1111/ emip. 12602\nHardy, B., & Ford, L. R. (2014). It’s not me, it’s you: Miscompre-\nhension in surveys. Organizational Research Methods,  17(2), \n138–162. https:// doi. org/ 10. 1177/ 10944 28113 520185\nHarrison, D. A., & McLaughlin, M. E. (1993). Cognitive processes in \nself-report responses: Tests of item context effects in work atti-\ntude measures. Journal of Applied Psychology, 78(1), 129–140.\nHernandez, I., & Nie, W. (2023). The AI-IP: Minimizing the guesswork \nof personality scale item development through artificial intel-\nligence. Personnel Psychology, 76(4), 1011–1035. https:// doi.  \norg/ 10. 1111/ peps. 12543\nHickman, L., Herde, C. N., Lievens, F., & Tay, L. (2023). Automatic \nscoring of speeded interpersonal assessment center exercises via \nmachine learning: Initial psychometric evidence and practical \nguidelines. International Journal of Selection and Assessment,  \n31(2), 225–239. https:// doi. org/ 10. 1111/ ijsa. 12418\nHinkin, T. R. (1995). A review of scale development practices in the \nstudy of organizations. Journal of Management, 21(5), 967–988. \nhttps:// doi. org/ 10. 1177/ 01492 06395 02100 509\nHinkin, T. R. (2005). Scale development principles and practices. In \nR. A. Swanson & E. F. Holton (Eds.), Research in organizations: \nFoundations and methods of inquiry (pp. 161–179). Berrett-Koe-\nhler Publishers.\nHinkin, T. R., & Tracey, J. B. (1999). An analysis of variance approach \nto content validation. Organizational Research Methods, 2, 175–\n186. https:// doi. org/ 10. 1177/ 10944 28199 22004\nHo, B. G. (2024) The power of language: GPT-4's role in personality \nitem generation and its psychometric properties. [Unpublished \nmaster thesis]. Bowling Green State University.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. \nNeural Computation, 9(8), 1735–1780. https:// doi. org/ 10. 1162/ \nneco. 1997.9. 8. 1735\nHoltrop, D., Oostrom, J. K., van Breda, W. R. J., Koutsoumpis, A., & \nde Vries, R. E. (2022). Exploring the application of a text-to-\npersonality technique in job interviews. European Journal of \nWork and Organizational Psychology, 31(6), 799–816. https://  \ndoi. org/ 10. 1080/ 13594 32X. 2022. 20514 84\nHommel, B. E. (2023). Expanding the methodological toolbox: \nMachine-based item desirability ratings as an alternative to \nhuman-based ratings. Personality and Individual Differences,  \n213, 112307. https:// doi. org/ 10. 1016/j. paid. 2023. 112307\nHommel, B. E., Wollang, F. J. M., Kotova, V., Zacher, H., & Schmukle, \nS. C. (2022). Transformer-based deep neural language modeling \nfor construct-specific automatic item generation. Psychometrika, \n87(2), 749–772. https:// doi. org/ 10. 1007/ s11336- 021- 09823-9\nHong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., ... & \nWu, C. (2023). MetaGPT: Meta programming for multi-agent \ncollaborative framework. arXiv. https:// doi. org/ 10. 48550/ arXiv. \n2308. 00352\nHua, W., Fan, L., Li, L., Mei, K., Ji, J., Ge, Y., ... & Zhang, Y. (2023). \nWar and peace (WarAgent): Large language model-based multi-\nagent simulation of world wars. arXiv. https:// doi. org/ 10. 48550/ \narXiv. 2311. 17227\nJebb, A. T., Ng, V., & Tay, L. (2021). A review of key Likert scale \ndevelopment advances: 1995–2019. Frontiers in Psychology, 12, \n637547. https:// doi. org/ 10. 3389/ fpsyg. 2021. 637547\nJia, Z., & Lee, P. (2025). Efficient processing of long sequence \ntext data in Transformer: An examination of five dif-\nferent approaches.  Organizational Research Methods, \n10944281251326062.\nKashdan, T. B., Stiksma, M. C., Disabato, D. J., McKnight, P. E., Bek-\nier, J., Kaji, J., & Lazarus, R. (2018). The five-dimensional curi-\nosity scale: Capturing the bandwidth of curiosity and identifying \nfour unique subgroups of curious people. Journal of Research \nin Personality, 73, 130–149. https:// doi. org/ 10. 1016/j. jrp. 2017. \n11. 011\nKoenig, N., Tonidandel, S., Thompson, I., Albritton, B., Koohifar, F., \nYankov, G., ... & Newton, C. (2023). Improving measurement \nand prediction in personnel selection through the application \nof machine learning. Personnel Psychology, 76(4), 1061–1123. \nhttps:// doi. org/ 10. 1111/ peps. 12608\nKurdi, G., Leo, J., Parsia, B., Sattler, U., & Al-Emari, S. (2020). A \nsystematic review of automatic question generation for educa-\ntional purposes. International Journal of Artificial Intelligence \nin Education, 30, 121–204.\nLambert, L. S., & Newman, D. A. (2023). Construct development and \nvalidation in three practical steps: Recommendations for review-\ners, editors, and authors. Organizational Research Methods,  \n26(4), 574–607. https:// doi. org/ 10. 1177/ 10944 28122 11153 74\nLanders, R. N., & Behrend, T. S. (2023). Auditing the AI auditors: \nA framework for evaluating fairness and bias in high stakes AI \npredictive models. American Psychologist, 78(1), 36–49. https:// \ndoi. org/ 10. 1037/ amp00 00972\nLanger, M., Baum, K., & Schlicker, N. (2025). Effective human over-\nsight of AI-based systems: A signal detection perspective on the \ndetection of inaccurate and unfair outputs. Minds & Machines,  \n35, 1. https:// doi. org/ 10. 1007/ s11023- 024- 09701-0\nLee, P., Fyffe, S., Son, M., Jia, Z., & Yao, Z. (2023). A paradigm shift \nfrom “human writing” to “machine generation” in personality \ntest development: An application of state-of-the-art natural lan-\nguage processing. Journal of Business and Psychology, 38(1), \n163–190. https:// doi. org/ 10. 1007/ s10869- 022- 09864-6\nLee, G. G., Latif, E., Wu, X., Liu, N., & Zhai, X. (2024). Applying \nlarge language models and chain-of-thought for automatic scor-\ning. Computers and Education: Artificial Intelligence, 6, 100213.\nLi, N., Gao, C., Li, Y., & Liao, Q. (2023). Large language model-\nempowered agents for simulating macroeconomic activities. \narXiv. https:// doi. org/ 10. 48550/ arXiv. 2310. 10436\nLi, Y., Wen, H., Wang, W., Li, X., Yuan, Y., Liu, G., ... & Liu, Y. \n(2024). Personal LLM agents: Insights and survey about the \ncapability, efficiency and security. arXiv. https:// doi. org/ 10.  \n48550/ arXiv. 2401. 05459\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). \nPre-train, prompt, and predict: A systematic survey of prompt-\ning methods in natural language processing. ACM Computing \nSurveys, 55(9), 1–35.\nLiu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2021). \nWhat makes good in-context examples for GPT-3? arXiv. https:// \ndoi. org/ 10. 48550/ arXiv. 2101. 06804\nMandi, Z., Jain, S., & Song, S. (2024). Roco: Dialectic multi-robot \ncollaboration with large language models. In IEEE International \nConference on Robotics and Automation (ICRA), 2024, (pp. \n286–299). https:// doi. org/ 10. 1109/ ICRA5 7147. 2024. 10610 855\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. \n(2021). A survey on bias and fairness in machine learning. ACM \nComputing Surveys (CSUR), 54(6), 1–35.\nMiller, T. (2019). Explanation in artificial intelligence: Insights from \nthe social sciences. Artificial Intelligence, 267, 1–38. https:// doi. \norg/ 10. 1016/j. artint. 2018. 07. 007\nOpenAI. (2024). GPT-4 technical report. ArXiv. https:// doi. org/ 10.  \n48550/ arXiv. 2303. 08774\n Journal of Business and Psychology\nPark, J., Woo, S. E., & Kim, J. (2024). Attitudes towards artificial intel-\nligence at work: Scale development and validation. Journal of \nOccupational and Organizational Psychology, 97(3), 920–951. \nhttps:// doi. org/ 10. 1111/ joop. 12502\nPark, J. S., Popowski, L., Cai, C., Morris, M. R., Liang, P., & Bernstein, \nM. S. (2022). Social simulacra: Creating populated prototypes \nfor social computing systems. In Proceedings of the 35th Annual \nACM Symposium on User Interface Software and Technology  \n(pp. 1–18). Association for Computing Machinery. https:// doi.  \norg/ 10. 1145/ 35261 13. 35456 16\nPark, J. S., O'Brien, J., Cai, C. J., Morris, M. R., Liang, P., & Bernstein, \nM. S. (2023). Generative agents: Interactive simulacra of human \nbehavior. In Proceedings of the 36th Annual ACM Symposium \non User Interface Software and Technology (pp. 1–22). Associa-\ntion for Computing Machinery. https:// doi. org/ 10. 1145/ 35861 83. \n36067 63\nPeter, S. C., Whelan, J. P., Pfund, R. A., & Meyers, A. W. (2018). \nA text comprehension approach to questionnaire readability: \nAn example using gambling disorder measures. Psychological \nAssessment, 30, 1567–1580.\nPutka, D. J., Oswald, F. L., Landers, R. N., Beatty, A. S., McCloy, R. \nA., & Yu, M. C. (2023). Evaluating a natural language process-\ning approach to estimating KSA and interest job analysis ratings. \nJournal of Business and Psychology, 38(2), 385–410. https:// doi. \norg/ 10. 1007/ s10869- 022- 09824-0\nQian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., ... & Sun, M. \n(2023). ChatDev: Communicative agents for software develop-\nment. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2307. 07924\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, \nI. (2019). Language models are unsupervised multitask learn-\ners. OpenAI Blog. https:// openai. com/ resea rch/ better- langu  \nage- models\nSaito, K., Wachi, A., Wataoka, K., & Akimoto, Y. (2023). Verbos -\nity bias in preference labeling by large language models. arXiv. \nhttps:// arxiv. org/ abs/ 2310. 10076\nSchriesheim, C. A., & Eisenbach, R. J. (1995). An exploratory and con-\nfirmatory factor-analytic investigation of item wording effects on \nthe obtained factor structures of survey questionnaire measures. \nJournal of Management, 21(6), 1177–1193. https:// doi. org/ 10. \n1016/ 0149- 2063(95) 90028-4\nSherer, M., Maddux, J. E., Mercandante, B., Prentice-Dunn, S., Jacobs, \nB., & Rogers, R. W. (1982). The self-efficacy scale: Construction \nand validation. Psychological Reports, 51(2), 663–671. https://  \ndoi. org/ 10. 2466/ pr0. 1982. 51.2. 663\nSliter, K. A., & Zickar, M. J. (2014). An IRT examination of the psy -\nchometric functioning of negatively worded personality items. \nEducational and Psychological Measurement, 74(2), 214–226. \nhttps:// doi. org/ 10. 1177/ 00131 64413 504584\nSociety for Industrial and Organizational Psychology. (January \n2023). Considerations and recommendations for the validation \nand use of AI-based assessments for employee selection. https:// \nwww. siop. org/ Porta ls/ 84/ SIOP% 20Con sider ations% 20and% \n20Rec ommen datio ns% 20for% 20the% 20Val idati on% 20and% \n20Use% 20of% 20AI- Based% 20Ass essme nts% 20for% 20Emp \nloyee% 20Sel ection% 20010 323. pdf? ver= 5w576 kFXzx LZNDM \noJqdI Mw% 3d% 3d\nSonderen, E. V., Sanderman, R., & Coyne, J. C. (2013). Ineffective-\nness of reverse wording of questionnaire items: Let’s learn from \ncows in the rain. PLOS ONE, 8(9). https:// doi. org/ 10. 1371/ \njourn al. pone. 00689 67\nSoto, C. J., & John, O. P. (2017). The next big five inventory (BFI-2): \nDeveloping and assessing a hierarchical model with 15 facets to \nenhance bandwidth, fidelity, and predictive power. Journal of \nPersonality and Social Psychology, 113(1), 117–143. https:// doi. \norg/ 10. 1037/ pspp0 000096\nSpeer, A. B. (2018). Quantifying with words: An investigation of the \nvalidity of narrative-derived performance scores. Personnel Psy-\nchology, 71(3), 299–333. https:// doi. org/ 10. 1111/ peps. 12263\nSpeer, A. B. (2021). Scoring dimension-level job performance from \nnarrative comments: Validity and generalizability when using \nnatural language processing. Organizational Research Methods, \n24(3), 572–594. https:// doi. org/ 10. 1177/ 10944 28120 930815\nSpeer, A. B., Perrotta, J., & Kordsmeyer, T. L. (2024). Taking It Easy: \nOff-the-Shelf Versus Fine-Tuned Supervised Modeling of Per -\nformance Appraisal Text.  Organizational Research Methods, \n10944281241271249.\nTan, B., Armoush, N., Mazzullo, E., Bulut, O., & Gierl, M. (2024). A review \nof automatic item generation techniques leveraging large language \nmodels. EdArXiv Preprints. https:// doi. org/ 10. 35542/ osf. io/ 6d8tj\nThompson, I., Koenig, N., Mracek, D. L., & Tonidandel, S. (2023). \nDeep learning in employee selection: Evaluation of algorithms \nto automate the scoring of open-ended assessments. Journal of \nBusiness and Psychology, 38, 509–527. https:// doi. org/ 10. 1007/ \ns10869- 023- 09874-y\nTran, K. T., Dao, D., Nguyen, M. D., Pham, Q. V., O'Sullivan, B., & \nNguyen, H. D. (2025). Multi-agent collaboration mechanisms: A \nsurvey of LLMs. arXiv. https:// arxiv. org/ abs/ 2501. 06322\nTunstall, L., Von Werra, L., & Wolf, T. (2022). Natural language pro-\ncessing with transformers. O’Reilly Media Inc.\nVandenberg, R. J. (2002). Toward a further understanding of and \nimprovement in measurement invariance methods and proce-\ndures. Organizational Research Methods, 5(2), 139–158. https:// \ndoi. org/ 10. 1177/ 10944 28102 00500 200\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, \nA. N., ... & Polosukhin, I. (2017). Attention is all you need. \nAdvances in Neural Information Processing Systems, 30.\nvon Davier, M. (2018). Automated item generation with recurrent neu-\nral networks. Psychometrika, 83(4), 847–857. https:// doi. org/ 10. \n1007/ s11336- 018- 9608-y\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., ... & Wen, \nJ. (2024). A survey on large language model based autonomous \nagents. Frontiers of Computer Science, 18(6), 186345. https://  \ndoi. org/ 10. 1007/ s11704- 024- 40231-1\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & \nZhou, D. (2022). Chain-of-thought prompting elicits reasoning in \nlarge language models. Advances in neural information process-\ning systems, 35, 24824–24837.\nWendler, C., & Burrus, J. (2013). The importance of editorial reviews \nin ensuring item quality. In K. F. Geisinger, B. A. Bracken, J. F. \nCarlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, & M. C. Rod-\nriguez (Eds.), APA handbook of testing and assessment in psy -\nchology, Vol. 1. Test theory and testing and assessment in indus-\ntrial and organizational psychology (pp. 283–291). American \nPsychological Association. https:// doi. org/ 10. 1037/ 14047- 016\nWilkins, D. E. (2014). Practical planning: Extending the classical AI \nplanning paradigm. Morgan Kaufmann Publishers Inc.\nWilliams, R., Hosseinichimeh, N., Majumdar, A., & Ghaffarzadegan, \nN. (2023). Epidemic modeling with generative agents. arXiv. \nhttps:// doi. org/ 10. 48550/ arXiv. 2307. 04986\nWoods, C. M. (2006). Careless responding to reverse-worded items: \nImplications for confirmatory factor analysis. Journal of Psycho-\npathology and Behavioral Assessment, 28, 186–191. https:// doi. \norg/ 10. 1007/ s10862- 005- 9004-7\nXi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., ... & Gui, T. \n(2023). The rise and potential of large language model based \nJournal of Business and Psychology \nagents: A survey. arXiv. https://  doi. org/ 10. 48550/ arXiv. 2309. \n07864\nXiao, B., Yin, Z., & Shan, Z. (2023). Simulating public administration \ncrisis: A novel generative agent-based simulation system to lower \ntechnology barriers in social science research. arXiv. https:// doi. \norg/ 10. 48550/ arXiv. 2311. 06957\nXu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., & Liu, Y. (2023). \nExploring large language models for communication games: An \nempirical study on Werewolf. arXiv. https:// doi. org/ 10. 48550/ \narXiv. 2309. 04658\nZhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J. B., ... & \nGan, C. (2023). Building cooperative embodied agents modu-\nlarly with large language models. arXiv. https:// doi. org/ 10. 48550/ \narXiv. 2307. 02485\nZhao, Q., Wang, J., Zhang, Y., Jin, Y., Zhu, K., Chen, H., & Xie, X. \n(2023). CompeteAI: Understanding the competition behaviors \nin large language model-based agents. arXiv. https:// doi. org/ 10. \n48550/ arXiv. 2310. 17512\nZheng, Z., Zhang, O., Nguyen, H. L., Rampal, N., Alawadhi, A. H., \nRong, Z., ... & Yaghi, O. M. (2023). ChatGPT research group \nfor optimizing the crystallinity of MOFs and COFs.  ACS Cen-\ntral Science, 9(11), 2161–2170. https:// pubs. acs. org/ doi/ 10. 1021/ \nacsce ntsci. 3c010 87\nZickar, M. J. (2020). Measurement development and evaluation. \nAnnual Review of Organizational Psychology and Organiza-\ntional Behavior, 7(1), 213–232. https:// doi. org/ 10. 1146/ annur \nev- orgps ych- 012119- 044957\nZieky, M. (2013). Fairness Review in Assessment. In K. F. Geisinger, B. \nA. Bracken, J. F. Carlson, J.-I. C. Hansen, N. R. Kuncel, S. P. Reise, \n& M. C. Rodriguez (Eds.), APA handbook of testing and assess-\nment in psychology, Vol. 1. Test theory and testing and assessment in \nindustrial and organizational psychology (pp. 293–302). American \nPsychological Association. https:// doi. org/ 10. 1037/ 14047- 017\nZong, C., Yan, Y., Lu, W., Huang, E., Shao, J., & Zhuang, Y. \n(2024). Triad: A framework leveraging a multi-role LLM-\nbased agent to solve knowledge base question answering. \narXiv preprint arXiv:2\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Industrial and organizational psychology",
  "concepts": [
    {
      "name": "Industrial and organizational psychology",
      "score": 0.875799298286438
    },
    {
      "name": "Psychology",
      "score": 0.6312227249145508
    },
    {
      "name": "Applied psychology",
      "score": 0.481803834438324
    },
    {
      "name": "Conceptual model",
      "score": 0.41011637449264526
    },
    {
      "name": "Social psychology",
      "score": 0.330300509929657
    },
    {
      "name": "Computer science",
      "score": 0.29781901836395264
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}