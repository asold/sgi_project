{
  "title": "A Novel Scenarios Engineering Methodology for Foundation Models in Metaverse",
  "url": "https://openalex.org/W4312238169",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2032229597",
      "name": "Xuan Li",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2775974243",
      "name": "Yonglin Tian",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2098270589",
      "name": "Peijun Ye",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2184708023",
      "name": "Hai-Bin Duan",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2297432335",
      "name": "Fei-Yue Wang",
      "affiliations": [
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6790978476",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4296425724",
    "https://openalex.org/W4224242420",
    "https://openalex.org/W2398173124",
    "https://openalex.org/W3118198986",
    "https://openalex.org/W4206805025",
    "https://openalex.org/W2947938940",
    "https://openalex.org/W2528059250",
    "https://openalex.org/W3150923322",
    "https://openalex.org/W2404399993",
    "https://openalex.org/W2124386111",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W1677409904",
    "https://openalex.org/W2117228865",
    "https://openalex.org/W2883730939",
    "https://openalex.org/W6747920752",
    "https://openalex.org/W4239025696",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2969446427",
    "https://openalex.org/W2889054948",
    "https://openalex.org/W4285167337",
    "https://openalex.org/W4200028901",
    "https://openalex.org/W2998586455",
    "https://openalex.org/W6632100814",
    "https://openalex.org/W1971844566",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W6794212170",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W6842542540",
    "https://openalex.org/W2769912520",
    "https://openalex.org/W2971512416",
    "https://openalex.org/W2987677324",
    "https://openalex.org/W2983539658",
    "https://openalex.org/W3086579950",
    "https://openalex.org/W3120795858",
    "https://openalex.org/W3133873791",
    "https://openalex.org/W3114952123",
    "https://openalex.org/W2154703456",
    "https://openalex.org/W4296425796",
    "https://openalex.org/W2980281588",
    "https://openalex.org/W3010904226",
    "https://openalex.org/W2082290569",
    "https://openalex.org/W2963794516",
    "https://openalex.org/W2397830550",
    "https://openalex.org/W3035172746",
    "https://openalex.org/W2914735843",
    "https://openalex.org/W2888535480",
    "https://openalex.org/W4304162916",
    "https://openalex.org/W3126598676",
    "https://openalex.org/W4200426957",
    "https://openalex.org/W2161138645",
    "https://openalex.org/W3147922109",
    "https://openalex.org/W3203267027",
    "https://openalex.org/W6803340592",
    "https://openalex.org/W3097764965",
    "https://openalex.org/W6846425778",
    "https://openalex.org/W2749684264",
    "https://openalex.org/W3034269714",
    "https://openalex.org/W3135789488",
    "https://openalex.org/W4224231583",
    "https://openalex.org/W4285012713",
    "https://openalex.org/W4292945941",
    "https://openalex.org/W3100519380",
    "https://openalex.org/W4386075639",
    "https://openalex.org/W1536929369",
    "https://openalex.org/W3207563209",
    "https://openalex.org/W3195577433"
  ],
  "abstract": "Foundation models are used to train a broad system of general data to build adaptations to new bottlenecks. Typically, they contain hundreds of billions of hyperparameters that have been trained with hundreds of gigabytes of data. However, this type of black-box vulnerability places foundation models at risk of data poisoning attacks that are designed to pass on misinformation or purposely introduce machine bias. Moreover, ordinary researchers have not been able to completely participate due to the rise in deployment standards. This study introduces the theoretical framework of scenarios engineering (SE) for building accessible and reliable foundation models in metaverse, namely, \"SE-enabled foundation models in metaverse.\" Particularly, the research framework comprises a six-layer architecture (infrastructure layer, operation layer, knowledge layer, intelligence layer, management layer, and interaction layer), which can provide controllability, trustworthiness, and interactivity for the foundation models in metaverse. This creates closed-loop, virtual–real, and human–machine environments that provides the best indices and goals for the foundation models, which allows us to fully validate and calibrate the corresponding models. Then, examples of use cases from the automotive industry are listed to provide transparency on the possible use and benefits of our approach. Finally, the open research topics of related frameworks are discussed.",
  "full_text": "2148 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 53, NO. 4, APRIL 2023\nA Novel Scenarios Engineering Methodology for\nFoundation Models in Metaverse\nXuan Li , Yonglin Tian , Peijun Ye, Haibin Duan , Senior Member, IEEE, and Fei-Yue Wang , Fellow, IEEE\nAbstract—Foundation models are used to train a broad system\nof general data to build adaptations to new bottlenecks. Typically,\nthey contain hundreds of billions of hyperparameters that have\nbeen trained with hundreds of gigabytes of data. However, this\ntype of black-box vulnerability places foundation models at risk\nof data poisoning attacks that are designed to pass on misinfor-\nmation or purposely introduce machine bias. Moreover, ordinary\nresearchers have not been able to completely participate due to\nthe rise in deployment standards. This study introduces the the-\noretical framework of scenarios engineering (SE) for building\naccessible and reliable foundation models in metaverse, namely,\n“SE-enabled foundation models in metaverse.” Particularly, the\nresearch framework comprises a six-layer architecture (infras-\ntructure layer, operation layer, knowledge layer, intelligence layer,\nmanagement layer, and interaction layer), which can provide con-\ntrollability, trustworthiness, and interactivity for the foundation\nmodels in metaverse. This creates closed-loop, virtual–real, and\nhuman–machine environments that provides the best indices and\ngoals for the foundation models, which allows us to fully validate\nand calibrate the corresponding models. Then, examples of use\ncases from the automotive industry are listed to provide trans-\nparency on the possible use and beneﬁts of our approach. Finally,\nthe open research topics of related frameworks are discussed.\nIndex Terms —Foundation models, knowledge automation,\nmanagement, metaverse, parallel intelligence, scenarios engineer-\ning (SE).\nManuscript received 18 November 2022; accepted 5 December 2022. Date\nof publication 26 December 2022; date of current version 17 March 2023.\nThis work was supported in part by the Science and Technology Innovation\n2030-Key Project of “New Generation Artiﬁcial Intelligence” under Grant\n2018AAA0102303; in part by the Science and Technology Development\nFund, Macau SAR under Grant 0050/2020/A1; in part by the International\nPartner-ship Program of The Chinese Academy of Sciences under Grant\nGJHZ202112; in part by the National Natural Science Foundation of\nChina under Grant 62203250, Grant U20B2071, Grant U1913602, Grant\n91948204, Grant T2121003, Grant 62076237, and Grant T2192933; in part\nby the Young Elite Scientists Sponsorship Program of China Association of\nScience and Technology under Grant YESS20210289; and in part by the\nChina Postdoctoral Science Foundation under Grant 2020TQ1057 and Grant\n2020M682823. This article was recommended by Associate Editor Y . Tang.\n(Corresponding author: Haibin Duan.)\nXuan Li is with the Department of Mathematics and Theories, Peng Cheng\nLaboratory, Shenzhen 518000, China (e-mail: lix05@pcl.ac.cn).\nYonglin Tian, Peijun Ye, and Fei-Yue Wang are with the State Key\nLaboratory for Management and Control of Complex Systems, Institute\nof Automation, Chinese Academy of Sciences, Beijing 100190, China\n(e-mail: yonglin.tian@ia.ac.cn; peijun.ye@ia.ac.cn; feiyue@ieee.org).\nHaibin Duan is with the Key Laboratory of Virtual Reality Technology\nand Systems, School of Automation Science and Electrical Engineering,\nBeihang University, Beijing 100083, China, and also with the Department of\nMathematics and Theories, Peng Cheng Laboratory, Shenzhen 518000, China\n(e-mail: hbduan@buaa.edu.cn).\nColor versions of one or more ﬁgures in this article are available at\nhttps://doi.org/10.1109/TSMC.2022.3228594.\nDigital Object Identiﬁer 10.1109/TSMC.2022.3228594\nI. I NTRODUCTION\nC\nURRENTLY, with the rise of foundation models includ-\ning BERT [ 1], GPT-3 [ 2], as well as DALL-E [ 3],\nartiﬁcial intelligence (AI) is undergoing a paradigm shift.\nFoundation models [4] in AI are algorithms that usually trained\non large-scale datasets, which have great capacity and can be\ntransferred into various downstream tasks. With the creation\nof new technologies, the foundation models and their values\nand drawbacks need to be understood to effectively implement\ndeployment in real-world scenarios.\nAs seen in Fig. 1, foundation models provide the poten-\ntial beneﬁts of combining all the pertinent data in a domain\nand customizing tasks across numerous modes. A variety\nof downstream tasks can subsequently be accommodated\nusing this single model. Because of this property, the foun-\ndation models can be readily included into a variety of\nactual applications of intelligent systems that have signiﬁ-\ncant human impact. The utilization of huge datasets and more\nexpensive computations by foundation models to enhance\nperformance across several sectors is widely known. But\neach coin has two sides. Although foundation models have\nadvanced greatly in recent years, there are still numerous\nobstacles to overcome. First, it is possible that all AI systems\nwill share the negative biases of a few foundational mod-\nels. Second, we do not have a clear framework for assessing\nhow well they function, when they breakdown, and what\nthey can even do. Third, foundation models could have\nother negative impacts, particularly from a human, social\nand environmental perspective. In summary, resource-driven\nfoundation models lead to most people not being able to\nparticipate in the fullest way. Additionally, deep models are\nwidely regraded as black-box and the working mechanisms\ncan hardly be interpreted. Hence, we present the frame-\nwork of “scenario engineering-enabled foundation models in\nmetaverse (SEEFMM).” Scenarios engineering (SE) [ 5], [6]\nrepresents the visibility, interpretability, and reliability of a\nhuman toward the working of intelligent systems, and it serves\nto realize trustworthy AIs. This framework guarantees that\nthe internal parameters of the system being tested are at\nreasonable levels and transforms the AI from feature-based\nitems, operations, and technologies to scenario-based intelli-\ngent ecosystem. To improve the controllability, interactivity,\naccessibility, and reliability of the foundation models, we com-\nbine SE with the metaverse, which greatly helps foundation\nmodels to depict promising application prospects in real-world\nscenarios. Furthermore, we present examples of use cases from\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLI et al.: NOVEL SE METHODOLOGY FOR FOUNDATION MODELS IN META VERSE 2149\nFig. 1. Foundation models have prospective advantages of fusing all the relevant information from various modalities, and adapting tasks across mul tiple\nmodes. From left to right: data is collected and labeled by experts, the corresponding multimodal data, foundation models, and the examples of downst ream\ntasks.\nFig. 2. Industrial revolution is changing how we live, work, and communicate.\nthe automotive industry to show how our approach can be used\nand what beneﬁts it can provide.\nIndustries are fundamental and essential to the economy\nof any region. They use possible operations to process and\ntransform natural products (raw materials) into other ﬁnished\nand semi-ﬁnished products. The history of industries can be\ndivided into four great moments or revolutions, which mainly\nstemmed from major breakthroughs in advanced technology\nor energy (see Fig. 2). For instance, the discovery of coal,\nelectricity and oil made industry go through Industry 1.0\nand 2.0. The Industry 3.0 and 4.0 use data, information,\nand communications technologies in the production chain,\nthrough concepts, such as machine learning [ 7], virtual real-\nity (VR) [ 8], digital twin (DT), big data, Internet of Things\n(IoT) [9], robotics, etc. The main objective is to achieve high\nlevels of automation and information integration across differ-\nent sectors. However, the previous technologies can only be\napplied to limited and simple industrial scenarios. For instance,\nmachine-learning-based methods are mostly used in computer\nvision (CV) or natural language processing (NLP), where each\nalgorithm can only handle one industrial task, such as anomaly\ndetection [ 10] and question & answer [ 11]. Currently, these\nalgorithms excessively pursue state-of-the-art performance for\nacademic purposes at the expense of practicality. Moreover,\n2150 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 53, NO. 4, APRIL 2023\nthe digital world is based on a prerecorded environment that\nlimits opportunities to gain experience in the real world.\nHence, a natural question arises: are there any novel ways\nto use these advanced technologies more effectively in indus-\ntrial scenarios? The SEEFMM methodology is expected to\nsolve industrial problems by integrating cyberspace, physical\nspace, AI, human intelligence, and social intelligence. In other\nwords, this method can constantly promote the development\nof industries toward intelligence [ 12], information [ 13], and\nsocialization [14].\nThis study aims to provide a thorough review of SEEFMM\nresearch, covering related work, the fundamental framework,\nexamples of industrial use, unresolved research questions, etc.\nThree main contributions are as follows.\nFirst, we present a novel framework of SEEFMM, the\nresearch framework comprises six layers of architecture that\nassist and support the data, design, validation, and interaction\nfor the foundation models. For instance, the intelligence layer\nprovides the best indices and goals for foundation models, and\nthe management layer provides a closed-loop environment to\nvalidate and calibrate the foundation models in metaverse.\nSecond, using staff training, product appearance design and\ndevelopment, and quality control as examples, the possible\nuse and beneﬁts of our proposed framework in the automotive\nindustry are described.\nThird, we provide some open research topics for the\nproposed framework (e.g., domain diversity, source accessibil-\nity, ethical challenges, and protection against attacks) to further\ndevelop the related community.\nThe remainder of this essay is structured as follows.\nSection II reviews works that are connected to learning-\nbased models, industrial cyber–physical systems (ICPSs) and\ncyber–physical–social systems. In Section III, the SEEFMM\narchitecture is described in detail, and in Section IV,m a n y\ncommon application cases for the automobile sector are pro-\nvided. Section V lists a number of active framework-related\nresearch questions. In Section VI, a conclusion is reached.\nII. R\nELATED WORKS\nA. Learning-Based Models and Foundation Models\nMachine learning, which builds predictive models on sample\ndata (known as training) and uses those models to make future\npredictions or judgments, powers the majority of AI systems in\nuse today. Typically, the data is acquired through feature engi-\nneering and determines the upper limit of machine learning.\nIt is worth noting that the “handcrafted features” were com-\nmonly used with “traditional” machine learning approaches.\nFor instance, CV often uses SIFT [ 15], HOG [16], SURF [17]\nand ORB [ 18] methods to extract useful features from data.\nN L Pu s e sT F - I D F[19], One-Hot [ 20], and Word2Vec [ 21]\nmethods to learn semantic knowledge, so that the models can\nunderstand text information.\nDeep learning [ 22] is one of the most popular approaches\nto construct representations for various inputs in machine\nlearning methods, which ignores handcraft features and uses\nneural networks to automatically learn the embedding. The\n“feature engineering” approach was the dominant approach\ntill recently when deep learning techniques started [ 23], [24],\n[25], [ 26] demonstrating recognition performance better than\nthe carefully handcrafted feature detectors. Larger datasets,\nmore computation and deep neural networks enable deep\nlearning to achieve better performance on standard bench-\nmarks [27]. In deep learning, the convolutional neural network\n(CNN) [ 28] and recurrent neural network (RNN) [ 29]a r e\nmore efﬁcient feature extraction schemes, which are most\ncommonly used to analyze visual imagery and time series\nprediction. After that, different neural network architectures\n(AlexNet [ 30] and ResNet [ 31], etc) achieved the best state-\nof-the-art performance in series for ImageNet competition.\nThe term “foundation model” [ 4] coined by The Stanford\nInstitute for human-centered AI’s (HAI) refers to “any model\nthat is trained on broad data (often using self-supervision at\nscale) that can be adapted (e.g., ﬁne-tuned) to a wide range\nof downstream tasks.” Since foundation models are usually\ntrained and saved on big data, sufﬁcient computing source\nand deep neural networks, they have some unique character-\nistics. For instance, the cost of building foundation models\nwill keep increasing due to their resource-intensive nature. As\na result, research into building foundation models is led by\nbig tech companies. Huawei presented [ 32] PanGu, a class\nof massively scalable autoregressive language models with up\nto 100 billion parameters. Google researchers have scaled up\ntheir newly proposed Switch Transformer language model to a\nwhopping 1.6 trillion parameters while keeping computational\ncosts under control. The Facebook AI Research team’s FLA V A\nmodel [33], which targets language, vision, and their combina-\ntion, performed impressively on the 35th task across the vision,\nlanguage, and multimodal domains. Microsoft research team\nproposed [34] BEiT-3 which develops an advanced multimodal\nfoundation model for vision-language tasks.\nB. Industrial Cyber–Physical Systems and\nCyber–Physical–Social Systems\nFor several decades, ICPSs and cyber–physical–social\nsystems (CPSS) have been a highly researched topic.\nResearchers have proposed numerous methods for ICPS and\nCPSS. The related works are comprehensively reviewed in this\nsection. Initially, ICPS [ 35] were designed to study industrial\nactivities as simulation experiments. The idea of (Cyber–\nPhysical modeling and simulation) CPMS and a reference\narchitecture built on it were discussed by Oks et al. [ 36]f o r\ncreating industrial CPS demos. Kravets et al. [ 37] provided\npractical examples of creating virtual ICPS objects and showed\nboth vertical links between modeling levels and their horizon-\ntal linkages with related technologies and real-world objects.\nTao et al. [ 38] introduced TrustData, which is a scheme for\nhigh quality data collection for event detection in ICPS and\nis referred to as the “trustworthy and secured data collection”\nscheme.\nAs the ICPS authenticity is increasing, it is being increas-\ningly used to solve practical industrial intelligence problems.\nLi et al. [ 39] proposed DeepFed to develop deep learning\nmodels under federated scheme, which can detect the threats\nfrom cyber space in industrial CPSs. A few-shot learning\nLI et al.: NOVEL SE METHODOLOGY FOR FOUNDATION MODELS IN META VERSE 2151\nFig. 3. Examples of the operation process of SE. Left: Apollo simulators at mission control in Houston. The Lunar module simulator is in the foreground\nin green, the command module simulator is at the rear of the photograph in brown. Middle: The rendering engine within Waymo SimulationCIty enables the\nWaymo rriver engineering team to test routes. Right: The Virtual KITTI dataset.\nmodel with a Siamese CNN (FSL-SCNN) was suggested\nby Zhou et al. [ 40] to address the over-ﬁtting problem and\nimprove the precision for intelligent anomaly detection in\nindustrial CPS. In order to achieve dynamic synchronization\nbetween a physical manufacturing system and its virtual rep-\nresentation, Zhou et al. [ 41] concentrated on a small item\ndetection model for DTs, aiming to realize the dynamic syn-\nchronization between a physical manufacturing system and its\nvirtual representation.\nAs mentioned above, the term CPS was used to describe\nintelligent systems [ 42] characterized by a tight integration\namong computation, communication, and control in their oper-\nations and environments. In the next industrial revolution, the\nconcept of CPSS involving human and social factors was\nintroduced by Wang et al. [ 43], [ 44] for the effective and\nefﬁcient operations of those systems. In CPSS, the cooper-\nation among humans, devices, and society is advocated, such\nas human–device cooperation [45], device–device cooperation,\nand society–human–device relationships. The SE, foundation\nmodels, big data, AI, VR, and augmented reality (AR) are the\nsupporting technologies of industrial metaverses.\nIII. D\nETAILED IMPLEMENTATION OF\nSEEFMM F RAMEWORK\nA. Brief Introduction to SE\nOver the past few years, machine learning or deep learn-\ning has slowly taken the world by storm. Whether it is the\nindustry or the corporate sector, you can ﬁnd all kinds of\ndata-driven models. In general, we usually collect raw data\nfrom different scenarios, and then the algorithms can use the\nlabeled data to effectively predict future results, which is called\nfeature engineering. The foundation of feature engineering\nincludes ideating, selecting, and creating useful features in\nyour datasets, helping identify connections, correlations, and\npatterns to explore using a machine learning model. However,\nfeature engineering is essential for getting the most value out\nof your precious data, but all manual operations (e.g., col-\nlection, annotation, and analysis) are time consuming, labor\nintensive, and error prone. In addition, even if expert knowl-\nedge is essential to interpret data relevant to a speciﬁc context,\nit can also have domain bias effects. Therefore, it is neces-\nsary for us to ﬁnd more advanced theories to make up for the\ndefects brought by feature engineering.\nScenarios can be understood in different ways, either as\na sequence of activities or as a branching structure of those\nactivities. Furthermore, a scenario can be concrete or abstract,\nwhich means it can be real [ 46], virtual, parallel [ 47], or\nvarious intermediaries. SE is deﬁned as the integrated reﬂec-\ntion of the scenarios and activities within a certain temporal\nand spatial range, where all actionable complex systems are\nencouraged to complete the design, certiﬁcation and veriﬁca-\ntion. It aims at shaping complex systems in forms that are more\nrelevant to the underlying scenario that needs to be learned\nand tested. Speciﬁcally, SE can be used throughout the life-\ncycle of complex systems to clarify the operation processes;\nto set goals (or index) for both experts and complex systems;\nto determine suitable model parameters after system testing; to\nprovide a certiﬁcation that issued by a third party; to vali-\ndate user requirements before system speciﬁcation begins; and\nto evaluate system design, performance, and function. Next,\nwe take Apollo 13 and autonomous driving simulation test-\ning as examples to introduce the operation process of SE, as\nillustrated in Fig. 3.\n1) Scenarios Engineering in Physical Space: After the\nlaunch of Apollo 13 [48] in 1970, no one could have predicted\nthe ﬁght for survival as the oxygen tanks exploded early into\nthe mission. In the history of our species, we have never faced\nthis kind of trouble so far from home (200 000 miles away).\nNASA have many problems that were needed to be solved to\nbring the crew safely home; several of them were solved via\nSE in physical space.\nNASA used 15 simulators to train astronauts and mission\ncontrollers in every aspect of the mission, including multiple\nfailure scenarios, such as diagnosing and solving problems\nwith physical assets that do not require direct human inter-\nvention. Fig. 3 displays the Apollo simulators at mission\ncontrol in Houston, USA. After the explosion, mission control\nimmediately dispatched the backup astronauts to practice the\nmaneuvers on the simulators, which were modiﬁed to repli-\ncate the latest physical conﬁguration of the spacecraft. Since\nmission control did not have an oxygen tank explosion plan,\nthey needed to carefully evaluate and repeatedly certify the\n2152 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 53, NO. 4, APRIL 2023\nfeasibility of the proposed plan on the simulators. Finally,\nthe astronauts followed the steps of the rescue plan, and they\nsuccessfully completed the free return. Although the Apollo\n13 rescue mission took place more than 40 years before the\nterm SE was coined, we think that it remains one of the best\nexamples of SE in physical space. SE greatly increased the\nprobability of the safe return of the astronauts to Earth in the\nevent of spacecraft malfunction.\n2) SE-Enabled AI in Cyberspace: Autonomous driving\nsystems rely on massive amounts of data (perception data or\nscenario data) as foundation. From a practical perspective, the\ninclusion of CV technology can make autonomous vehicles\nsafe for passengers. How does CV make autonomous vehicles\nreliable and intelligent? For instance, to avoid accidents or col-\nlisions while driving, vehicles need to identify various objects,\ne.g., pedestrians, other vehicles, and trafﬁc lights. Moreover,\nto keep the self-driving car in a speciﬁed lane, CV with deep\nlearning uses segmentation techniques for lane line detec-\ntion. Generally, autonomous driving acquire data on location,\nroad and trafﬁc conditions, terrain, and the number of objects\n(pedestrians, cars, trucks, buses, etc.) in the area to ensure safe\ndriving. These datasets (especially labeled data [ 49]) are often\nused for recognition, detection, segmentation, and other tasks\nwhile driving.\nHowever, publicly available autonomous driving datasets\nlack the challenge of critical scenarios, and road testing\nis time-consuming and unsafe. Nowadays, techniques asso-\nciated with the construction of virtual scenarios are being\nrapidly developed and are playing an important role in\nthe research of visual intelligence of autonomous driving.\nTherefore, to overcome the shortage of real datasets, many\nresearchers have attempted to solve these problems using\nvirtual scenarios. For example, Virtual KITTI [ 50]s h o w e d\nthat virtual datasets can effectively promote visual perception\nperformance. Additionally, these virtual datasets can be used\nto quantitatively test CV algorithms under different critical\nscenarios.\nIn 2017, Waymo [51] proposed the ﬁrst simulation program\n(CarCraft) to train autonomous driving, which has now trav-\neled more than 5 billion miles. Furthermore, they presented\nthe latest virtual world “Simulation City,” where self-driving\ncars can train, test, and validate their software and hardware\nsystems to ensure that they can handle the challenges of the\nopen road. To ensure that the simulated environment is rep-\nresentative of the real world, Waymo used sensors to obtain\nabundant high-quality data from dozens of cities. Moreover,\nthey are constantly updating the simulated environment based\non this data, regularly incorporating minor subtleties into the\nsimulation.\nApollo simulators and Simulation City are different in many\nways. First, Apollo simulators are a combination of software\nand hardware platforms, while Simulation City presents sim-\nulation scenarios deﬁned entirely by software. In other words,\nthe Apollo simulators need more researchers, money, and\nresources to focus on revolutionary industries than Simulation\nCity. However, Simulation City may not be completely com-\npatible with the physical world, which poses potential prob-\nlems for the application of autonomous driving tasks in real\nscenarios. Second, the Apollo simulators need to consider\nhuman involvement and intervention. That is, the scheme is\nsimilar to human–machine hybrid intelligence, which is an\norganizational form stemming from the interaction of human,\nmachine, and environmental systems. Third, for the Apollo\nsimulators, upgrading the existing equipment is often difﬁ-\ncult. In contrast, since sensors sustainably collect information,\nSimulation City can be regularly estimated and updated.\nB. Description of SE-Enabled Foundation Models in\nMetaverse\nIt is worth noting that the above two simulators illustrate the\nimportance of the combination of hardware devices and soft-\nware platforms. The developers of Simulation City highlighted\nthat VR is another technological concept that has an ability to\ntake AI processes to new heights. Several landmark technolog-\nical trends, such as foundation models, 6G, DTs, Internet of\nEverythings (IoE), CPSS, cobots, edge computing (EC), and\nblockchain, have been integrated with SE to improve the intel-\nligence and personalization of the corresponding applications.\nThe advantages of previous research and landmark technolog-\nical trends inspired the design for the theoretically plausible\nSEEFMM framework. This framework is based on several\nprevious studies. Schuldt [ 52] modeled a three-dimensional\n(3-D) model, dividing the industry scenario into business,\nfunctional, information, communication, integration, and asset\nlayers with information interaction among layers. In their\nmodel, complex projects are split into clusters of manage-\nable parts. Wang et al. [ 53] proposed the framework of\nsmart contract scenarios, employing a multilayer architecture,\ncomprising infrastructures, contracts, operations, intelligence,\nmanifestations, and applications layers. Qin et al. [ 54]d i s -\ncussed key technologies of Industry 5.0, including EC, DT,\ncollaborative robots, IoE, big data analytics, blockchain, and\nfuture 6G systems and beyond. Subsequently, we divide the\nSEEFMM framework into six layers: including, infrastruc-\nture, operation, knowledge, intelligence, management, and\ninteraction layers, as shown in Fig. 4. The layer details are\nas follows.\n1) Infrastructure Layer: The infrastructure layer encapsu-\nlates all the infrastructures that support SE and additional\nconditions, including the hardware environment, software\ndevelopment environment (SDE), and execution environment.\nSpeciﬁcally, these infrastructures are not independent, and an\nideal organization will inﬂuence the patterns and attributes\nof SE.\nThe hardware environments refer to all scenarios-related\nhardware, including but not limited to instruments, gauges,\nsensors, computers, buildings, machines, devices, stereoscopic\ndisplay, VR-Platform, and networks. Hardware denotes any\nobject or part of an object that can be analyzed with the\nlaws of physics. For instance, an excavator or a machine in a\nfactory can both be considered as hardware. Everything out-\nside the system is called the environment, which is ignored\nin the analysis, except for its effects on the system. SDE\ndenotes the collection of hardware and software tools a system\ndeveloper uses to build software systems. In our cases, SDE\nLI et al.: NOVEL SE METHODOLOGY FOR FOUNDATION MODELS IN META VERSE 2153\nFig. 4. Six levels of hierarchical representation of the SEEFMM framework. Best viewed with zooming.\noffers a logical environment wherein software and potentially\nhardware components interact to simulate an entire system or\nsubsystem, in contrast to simulations [55] that allow only indi-\nvidual processes. For instance, the DT technology uses digital\nmodels for the manufacturing process, production facilities,\nand customer experience. The infrastructure layer essentially\naims to rapidly increase the performance through collaboration\nbetween humans and machines. The execution environments\ncan enhance the human–machine collaboration by assigning\nrepetitive and monotonous tasks to the robots/machines and\nthe tasks that need critical thinking to the humans. The exe-\ncution is driven by humans, and its primary characteristic is\nto construct intelligent scenarios in a loop. Special focus is\nplaced on identifying deﬁnitions and characteristics of CPSS\nand how the society and human aspects are integrated in the\ncurrent research trends.\n2) Operation Layer: The operation layer contains the man-\nagement software that manages hardware environments, SDEs,\nand execution environments and provides common services\nfor SE programs, including the organizational technology,\ncoordinating technology, and execution technology, namely,\nOCE technology. Usually, the operation layer is the closest to\nthe infrastructure layer. It mainly completes the acquisition,\nscheduling, and allocation of resources, such as collecting,\nstoring, and protecting information, as well as coordinating\nand controlling concurrent activities and many other tasks.\nThis layer mainly aims to improve the SE reliability through\nnew technologies [ 56], such as federated ecology, federated\ndata, blockchain intelligence, smart contracts, decentralized\nautonomous organizations (DAOs) [ 57], and decentralized\nautonomous societies (DASs), which greatly facilitate the\ndeployment of foundation models tasks.\n3) Knowledge Layer: The knowledge layer mainly\nincludes four aspects: 1) connecting and processing unit;\n2) knowledge bases; 3) expert experience; and 4) knowledge\nmodeling. Therefore, this layer can be viewed as the knowl-\nedge automation of SE. In the new intelligent era, the cyber\nspace, physical space, and social space will be integrated\nseamlessly, and the knowledge automation is the technical\nfoundation. The basic elements of the knowledge layer\nare used to collect real knowledge “small data” from real\nscenarios. In the physical world, researchers usually combine\n2154 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 53, NO. 4, APRIL 2023\nmathematical models [ 58] based on “big laws” (Newton’s\nlaws) with “small data (physical data)” to describe and\nanalyze complex systems. The traditional analysis methods\nwill yield the objective phenomenon of “modeling gap.” For\ninstance, “small data” are input to software-deﬁned scenarios\n(or models), and abundant of “big data” are automatically\ngenerated by analyzing and absorbing the main physical\ncharacteristics of the scenarios. However, complex systems,\nespecially the ones with societies and humans in the loop,\nare usually driven by Merton’s law. To solve this problem,\nknowledge automation is used to generate large cyber\ndata from small physical data (considering the human and\nsocial factors) and transform the large cyber data into deep\nintelligence for scenario-speciﬁc problems. The objective of\nsoftware-deﬁned model\nζ(E\n1,E2,G1,G2,D1,D2)\n= λv(ζvae(E1,G1) + ζvae(E2,G2))\n+ λa(ζadv(E2,G1,D1) + ζadv(E1,G2,D2))\n+ λc\n(\nζcyc(E1,G1,E2,G2) + ζcyc(E2,G2,E1,G1)\n)\n(1)\nwhere ζvae, ζadv, and ζcyc are variational autoencoder loss,\nadversarial loss and cycle-consistency loss. E1,E2, G1,G2,\nand D1,D2 are encoders, decoders, and discriminators. λv, λa,\nand λc are weights that control the importance of variational\ntraining, adversarial training and cycle-reconstruction terms,\nrespectively.\n4) Intelligence Layer: The intelligence layer encapsulates\nthe infrastructures of “6I” [ 59], including cognitive intelli-\ngence, parallel intelligence, cryto intelligence, federated intel-\nligence [ 60], social intelligence, and ecological intelligence.\nThese elements provide intelligent standards for foundation\nmodels in different scenarios. Subsequently, the proposed sce-\nnarios will have a high degree of intelligence. We evaluate all\nfoundation models using the “6I” indices (safety index, secu-\nrity index, sustainability index, sensitivity index, service index,\nand smartness index) for “6S” goals (safety, security, sustain-\nability, sensitivity, service, and smartness). The intelligence\nlayer enables these scenarios to have social capabilities, such\nas task collaborative selection, communication, and mutual\nnegotiation. Traditional feature engineering can only provide\nlimited intelligence to foundation models; thus, they are lim-\nited to performing speciﬁc tasks in a limited scope. In contrast,\nSE can transform foundation models from feature-based ele-\nments to scenario-based intelligent ecology and can achieve\nthe “6S” goal to realize the safety and sustainability of intel-\nligent systems. For instance, parallel intelligence can simulate\ndriving behaviors, allowing foundation models to be safely and\nsustainability trained and tested in critical scenarios. Note that\neach intelligence is affected by other indices, but one index\nor goal is considered dominant. Therefore, the corresponding\nindices must be designed according to the speciﬁc applications.\n5) Management Layer: As stated above, the intelligence\nlayer provides a comprehensive evaluation intelligence, index,\nand goal for foundation models in SE. The management\nlayer comprises various management, evaluation, and testing\nmechanisms, including parallel management, calibration and\ncertiﬁcation (C&C), and veriﬁcation and validation (V&V).\nThese multidimensional evaluation indices can be used to\nguide and develop intelligent and trustworthy foundation mod-\nels in metaverse. The scenario-based approach is considered\na promising way for achieving intelligent management. Thus,\nwe ﬁrst divide the scenarios into different cases. More impor-\ntantly, these different cases utilize the parallel management\nmethod to capture critical scenarios (with key scenario param-\neters) for system design, safety analysis, C&C, and V&V , with\na potential risk of harm. Here, calibration refers to the iden-\ntiﬁcation of suitable values for model parameters so that the\ninternal dynamics best ﬁt reality. If good calibration results\nare obtained, the internal parameters of the system are proven\nto be at a reasonable level. Next, the management layer uses\nblockchains and nonfungible token [ 61] (NFT) to issue certi-\nﬁcations for third-party checks. In SE, C&C is the process of\nchecking the overall performance of the managed methods and\ndevices. In the post-development phase, V&V checks whether\nthe management system has correctly achieved performance\nand functionality. Particularly, veriﬁcation is a process wherein\nthe product, service, and management system continuously\nmeet practical requirements or speciﬁcations, and validation\nprocedures involve regular critical tests to ensure that the prod-\nuct or management functions meet customer requirements. The\ncalibration of SE can be represented as\nθ\n∗ = argmin\nθ\nK∑\nt=1\n[\ny(t,θ ) −ˆy(t)\n]T · V ·\n[\ny(t,θ ) −ˆy(t)\n]\n(2)\nwhere y(t,θ) = [ y1(t,θ) ··· yn(t,θ) ]T and ˆy(t) =\n[ ˆy1(t) ··· ˆ yn(t)]T are the intermediate outputs of modules\nor components for test systems and elaborate scenarios in time\nstep t. The positive symmetric matrix V represents the impor-\ntance of each output. Clearly, the objective here is quadratic\ndistance but it can be easily extended to other metrics as well.\n6) Interaction Layer: The interaction layer deﬁnes the way\nin which humans interact with the system as well as the nature\nof the inputs and outputs that the system accepts and produces.\nThe interaction layer includes interdisciplinary ﬁelds, such\nas VR, human–computer interaction, cognitive psychology,\nhuman factors, and cognitive science. Here, human–computer\ninterface and AR provide researchers with various functions,\nincluding temperature, light intensity, object contour empha-\nsis, thermal, and 3-D map, etc. These functions help humans,\nfoundation models, and devices function together in concrete\nscenarios. In this layer, the human–machine interface is a very\nimportant part, and the improper design may lead to seri-\nous problems. For instance, a classic example is the Three\nMile Island nuclear meltdown accident, where a misdesigned\nhuman–machine interface was partly to blame. Scientiﬁcally,\nhuman–computer interactions and AR are attempts to imple-\nment CPSS, with the goal of enabling the closed-loop virtual–\nreal or human–machine interactions and feedback. With the\nadvances in computer graphics, VR, programming languages,\ncognitive psychology, industrial design discipline, human fac-\ntors, and cognitive science, a deep and solid theoretical and\nsocietal foundation is provided for human–computer interac-\ntions in metaverse.\nLI et al.: NOVEL SE METHODOLOGY FOR FOUNDATION MODELS IN META VERSE 2155\nThis section proposes a theoretical framework SEEFMM\nfor foundation models research in metaverse. The strengths\nof SEEFMM include: 1) we can construct simulators and\nautomatically generate controllable simulated data; 2) the\nframework provides the best indices and goals for foundation\nmodels; and 3) the foundation model can be validated and\ncalibrated in closed-loop, virtual–real, and human–machine\nenvironments.\nIV . I\nNDUSTRIAL APPLICATION CASES OF SEEFMM\nThe research of foundation models has advanced in the\nlast few years. On the human side, decision makers imme-\ndiately want to see the business value after a new technology\nhas been implemented, like foundation models because of the\nmass changes that are caused by it. Based on the SEEFMM\nframework, a combined hardware and software solution called\nindustrial metaverse is created to add to our industry-leading\nfoundation model design tools, and some speciﬁc examples\nare provided. Next, the speciﬁc method and technical details\nfor the industrial application cases of SEEFMM are provided.\nA. Technical Details for SEEFMM in Industrial Metaverse\n1) Generate Critical Scenarios: Advanced high-quality\nindustrial data enables state-of-the-art foundation models and\nsoftware to well train, reason, and produce. Statistical realism\nin simulated industrial scenarios can be ensured by creat-\ning realistic conditions for industrial production technology.\nTherefore, to construct realistic simulation of industrial sce-\nnarios, the best infrastructure needs to be built that acquires\ninformation from sensors (monitoring sensors, image sen-\nsors, biochemical sensors, etc.), 5G, stereoscopic display,\nVR-Platform, and IoT. Additionally, the implementation of\nnew technologies in the operation layer, such as OCE, DAO,\nand DAS, enables our framework to automatically generate\ncritical scenarios [ 62] and collect large amounts of local data.\nTherefore, industrial scenarios can provide real or simulated\nlocal data, greatly facilitating the deployment of foundation\nmodels tasks. Therefore, if a worker ﬁnds a crack in a boiler\nin a New York at night, we can recreate it in Beijing or any-\nwhere on simulation industrial scenarios, and we can even\nsimulate other minute details, such as the dimming light and\ntemperature variation. We can then train the algorithm with\nlocal data samples (held in multiple scattered edge servers).\n2) Indices and Goals: Every major task of our foun-\ndation models, whether it is perception, semantic under-\nstanding, sentiment analysis, information extraction, behavior\nprediction, instruction following, or planning, leverages pow-\nerful learning-based models that beneﬁt from our trustable\nknowledge and reliable intelligence layer. Our knowledge layer\nis based on knowledge automation research, and we are con-\ntributing to the industrial research community through our\nlarge cyber data initiative, which we are constantly expand-\ning to include new data and new intelligences in several key\nareas of research ranging from perception to prediction, safety,\nsecurity, sustainability, and society.\nAs foundation models are pervasively applied in high-risk\nindustry ﬁelds, some intelligence ﬂaws could be magniﬁed\nand could cause harm. If English speakers with strong accents\ncannot reliably use speech recognition technology, machines\nwill not be able to get correct voice commands. We are con-\ntinuously innovating and pushing the index and goals of the\nbest foundation models and incorporating those advances into\nour production stack to handle the complexities of industrial\nscenarios.\n3) Compounding Human Experience With AI Tech Stack:\nSince we operate in multiple environments, from coal to\nsteel, machinery, and chemicals, we collate human experi-\nences to create a robust generalizable AI tech. With the\nhelp of the management and interaction layers, foundation\nmodels can provide real-time machine information to people\nwith interactive devices. This means that our framework can\nenhance the quality of the entire industrial life cycle by apply-\ning different task assignments. Particularly, Bellalouna [ 63]\npresented a dynamic section view function that creates cross\nsections through the gearbox to display the state of the interior\nassembly and parts in 3-D space. If an engineer is equipped\nwith a human–computer platform and temperature, humidity,\nand pressure sensors, which transmit data via an IoT device to\nthe foundation models in real time, the engineer will receive\nall the information from certiﬁed and validated foundation\nmodels. Moreover, they will be able to follow the life-cycle\nproduction process from different perspectives. This feature\nprovides valuable support to maintenance engineers during\ntransmission reconﬁguration.\nB. Industrial Application Cases of SEEFMM\nTo elucidate the possible use and advantages of our frame-\nwork that are decisive for strategic planning, examples of\nautomotive industry metaverse use cases are provided (see in\nFig. 5). Note that the foundation models are pretrained, cali-\nbrated, and validated on multimodal data generated by various\nsources (real or virtual scenarios) in the automotive ecosystem.\n1) Staff Training: Metaverse in the automotive industry can\nbe used to create virtual–real environments that allow workers\nto practice their job skills. Foundation models enable various\nstaff training tasks in the automotive staff training indus-\ntry when ﬁne-tuned on multimodal data in the downstream\ntasks (e.g., question and answering and information extrac-\ntion). Through metaverse-related advanced technology, staff\n(wearing AR glasses) can assemble a vehicle, break down its\nessential parts for study, and master prototyping. Using foun-\ndation models (applied for maintenance, repair, and assembly\nin downstream tasks), multisensual learning contents (visual,\nauditory, and kinesthetic models) enable high immersion into\nthe learning scenery. For instance, instructions for performing\nmaintenance activities are displayed to the user, and the user\ncan also directly communicate with the foundation models to\nobtain correct assembly techniques. Importantly, the founda-\ntion models correct employee mistakes in a timely manner.\nThus, the staff training process is reliant rely on both the\nemployee and the foundation models to do all the work, num-\nber of trainers is reduced, waiting times are minimized, and\nmachine availability is increased.\n2156 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 53, NO. 4, APRIL 2023\nFig. 5. Examples of using our framework in the automotive industry. Left: Staff training (maintenance, repair and assembly downstream tasks). Middl e:\nProduct appearance design and development (realistic images generated by the ERNIE-ViLG model [ 64] in cyberspace from a text description). Right: Quality\ncontrol.\n2) Product Appearance Design and Development: In the\nproduct appearance development process, foundation models\nand interaction layer are used for the interactive visualiza-\ntion of the design of technical systems. Visualization or visual\nthinking is a type of perception that designers can learn from.\nParticularly, visualization is the mental imagery that is created\nusing our imagination. With the advancement of the education\nsystem, designers usually receive formal design education in\nuniversities, which helps improve the quality and speed of\nproduct design. However, the rules of design education limit\nthe imagination of people, leading to a lack of personalization\nin products. Foundation models and interaction layer are a\nhuman-centric design solution where the art robot and human\nlanguage collaborate with human decisions to enable personal-\nizable autonomous design and development through metaverse\nsocial networks. For instance, foundation models can create\noriginal and realistic images and art in cyberspace from a text\ndescription, as shown in Fig. 5. These realistic images are gen-\nerated by ERNIE-ViLG model [ 64], which combine different\nconcepts, attributes, and styles. The foundation models aim to\nallow machines to replace people so that the machines can\nprovide enough space for the human imagination to accom-\nplish the design tasks. Additionally, AR design allows us to\ninput our thoughts onto a screen and fully display them in the\nreal world. Unsurprisingly, foundation-model-powered graphic\ngenerators combined with human intelligence that can create\noriginal works of product have been fabricated.\n3) Quality Control: Foundation models assisting quality\ncontrol [65] have the potential to become a standard in meta-\nverse for performing various quality control tasks. Foundation\nmodels and metaverse-related advanced technologies provide\ncontactless assistance in use-cases where workers need to\nvisually inspect products. In the traditional AI development\nprocess, a series of steps, such as model selection, data\nprocessing, model optimization, and model iteration, are inde-\npendently completed for each scenario. This process tends to\noften be inefﬁcient due to the differences in the debugging\nmethods for each task. Foundation models are usually pre-\ntrained with very-large-scale defect data at various stages;\nthus, they can well judge the subtle differences of products,\nwhich improves their quality control ability. For instance,\nwhen vehicle molds come out of the steel furnace with the\nunbearable heat for human beings, visual models can be used\nto detect more than 1000 types of cracks on vehicle surfaces in\nbatches. Therefore, employees can focus on multiple speciﬁc\ndecision-making quality control tasks in a more comfortable\nenvironment and step-by-step complete the tasks without refer-\nring to paper-based instructions. In other words, foundation\nmodels perform repetitive and monotonous tasks of quality\ncontrol in harsh environments and humans perform the tasks\nthat need critical.\nV. O\nPEN RESEARCH TOPICS\nAs an emerging technology, our proposed framework shows\nraw potential, but is still in its early stages. Based on the\nproposed framework, this section presents open research topics\nin this area.\nA. Domain Diversity\nOur framework builds on decades of research and advanced\nin AI, intelligent science, instruments, optimization, network\nengineering, software engineering, computer graphics, VR,\nand other ﬁelds. Most of these contributions stem from the best\nscience and engineering research laboratories [ 66]. In fact, the\nInternet, globalization, and digital platforms have changed our\nlives and crossed the borders of countries and customs. Owing\nto the Internet and social media, connecting with anybody\nin the world, living in a different culture, living in a differ-\nent social context, living with different social norms, living\nwith different habits, and speaking a different language have\nbecome easy. We believe that subject domain diversity could\nplay a vital role in the development of our framework to facil-\nitate the development of the industrial scenario. Therefore, we\nLI et al.: NOVEL SE METHODOLOGY FOR FOUNDATION MODELS IN META VERSE 2157\nneed to assemble scientists, workers, engineers, ethicists, legal\nscholars, and others to obtain useful suggestions for improving\nour frameworks.\nB. Source Accessibility\nWe are currently in an era where the data, computations,\nand algorithms that are absolutely necessary to make small\nmodels a reality are abundant. However, similar to the early\ndays of computer technology, the use of our framework is lim-\nited to a small number of industrial developers [ 67]. This is\nbecause the most popular foundation models and correspond-\ning datasets or other metaverse-related advanced technologies\nhave not been released or are not easily accessible to more\nacademia researchers. We can expect that the development of\nSE and metaverse for foundation models will lead to incredible\ngrowth in almost every industry and it offers new hopes for the\nfuture. However, the research will rapidly grow only if more\npeople participate in it. Therefore, governments, businesses,\nand universities are working closely together to promote pub-\nlic infrastructure, personnel training, and industry services to\naddress current resource access issues.\nC. Ethical Challenges\nFoundation models, SE, and metaverse are digital tech-\nnologies that will signiﬁcantly impact on the development of\nhumanity in the near future. They have raised some basic ques-\ntions: what should we do with these systems? what should\nthe systems themselves do [ 68]? and what ethical challenges\ndo they involve? Here, our framework faces a comprehen-\nsive ethical challenge, including robot ethics, machine ethics,\nand AI ethics. Broadly speaking, ethics can be deﬁned as the\ndiscipline dealing with right versus wrong and the moral obli-\ngations and duties of entities. However, the new issues that\nmight arise from the conﬂuence of so many ethical challenges\nare unclear. One thing is clear, we need to be cautious, and\nnew ethics need to be established.\nD. Protection Against Attacks\nThe methods underpinning the best intelligent systems are\nsystematically vulnerable to cybersecurity attacks. Using these\nattacks, adversaries can manipulate the systems to alter their\nbehavior to serve a malicious end goal. As our framework\nis further integrated into critical components of the industry,\nthese cybersecurity attacks represent an emerging and sys-\ntematic vulnerability with the potential of signiﬁcant effects\non safe production [ 69]. These attacks can be divided into\nmultiple parts and require the full attention of the developer.\nOn the one hand, traditional cyberattacks caused by “bugs”\nor human errors in the code can cripple the hardware and\nsoftware associated with the metaverse. On the other hand,\nas the defects of the foundation models are inherited by all\nthe adapted models downstream, these defects are vulnerable\nand cannot be ﬁxed. Moreover, data can be weaponized in\ndifferent ways using these attacks. This requires SE to better\ndesign, calibrate, and validate the foundation models as well\nas to change how data is collected, where it is stored, and\nwhen it is used.\nVI. C\nONCLUSION\nThis study presented a novel framework of SEEFMM. The\nframework represented the visibility, observability, and inter-\nactivity of the collaborative work of human and intelligent\nsystems and aimed to realize trustworthy foundation models.\nWith the continuing research of AI and metaverse, foundation\nmodels have become a research hotspot in the SE com-\nmunity. The virtual–real, controllability, manageability, and\nveriﬁability characteristics of SE enabled foundation models to\ncomplete design, training, calibration, veriﬁcation, and appli-\ncation tasks in metaverse. Specially, the research framework\ncomprised a six-layer architecture: 1) infrastructure; 2) oper-\nation; 3) knowledge; 4) intelligence; 5) management; and\n6) interaction layers. First, the advanced infrastructure and\noperation layers enabled the foundation models and software\nto better train, reason, and produce. Second, the trustable\nknowledge and reliable intelligence layers helped new data and\nnew intelligence conduct research in several key areas. Third,\nthe management and interaction layers improved the quality of\nthe entire life cycle by using a new task assignment method.\nFinally, we provided recent examples of the automotive indus-\ntry metaverse use cases and discussed the open research topics,\nproviding a path toward further research requirements.\nR\nEFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” in Proc.\nAssoc. Comput. Linguist. (ACL) , 2019, pp. 4171–4186.\n[2] T. B. Brown et al., “Language models are few-shot learners,” in Proc.\nAdv. Neural Inf. Process. Syst. , 2020, pp. 1877–1901.\n[3] A. Ramesh et al., “Zero-shot text-to-image generation,” in Proc. Int.\nConf. Mach. Learn. , 2021, pp. 8821–8831.\n[4] R. Bommasani et al., “On the opportunities and risks of foundation\nmodels,” 2021, arXiv:2108.07258.\n[5] F.-Y . Wang, “The engineering of intelligence: DAO to I&I, C&C, and\nV&V for intelligent systems,” Int. J. Intell. Control Syst. , vol. 1, no. 3,\npp. 1–5, 2021.\n[6] X. Li et al., “From features engineering to scenarios engineering for\ntrustworthy AI: I&I, C&C, and V&V ,”IEEE Intell. Syst., vol. 37, no. 4,\npp. 18–26, Jul./Aug. 2022.\n[7] F.-Y . Wang, “Metavehicles in the metaverse: Moving to a new phase for\nintelligent vehicles and smart mobility,” IEEE Trans. Intell. Veh.,v o l .7 ,\nno. 1, pp. 1–5, Mar. 2022.\n[8] F. Hu et al., “Cyberphysical system with virtual reality for intelligent\nmotion recognition and training,”IEEE Trans. Syst., Man, Cybern., Syst.,\nvol. 47, no. 2, pp. 347–363, Feb. 2017.\n[9] G. Fortino, C. Savaglio, G. Spezzano, and M. Zhou, “Internet of Things\nas system of systems: A review of methodologies, frameworks, plat-\nforms, and tools,” IEEE Trans. Syst., Man, Cybern., Syst., vol. 51, no. 1,\npp. 223–236, Jan. 2021.\n[10] M. R. G. Raman and A. P. Mathur, “A hybrid physics-based data-\ndriven framework for anomaly detection in industrial control systems,”\nIEEE Trans. Syst., Man, Cybern., Syst. , vol. 52, no. 9, pp. 6003–6014,\nSep. 2022.\n[11] B. Jin et al., “Promotion of answer value measurement with domain\neffects in community question answering systems,” IEEE Trans. Syst.,\nMan, Cybern., Syst. , vol. 51, no. 5, pp. 3068–3079, May 2021.\n[12] F.-Y . Wang, X. Wang, L. Li, and L. Li, “Steps toward parallel intel-\nligence,” IEEE/CAA J. Automatica Sinica , vol. 3, no. 4, pp. 345–348,\nOct. 2016.\n[13] S. Wang et al., “Robotic intra-operative ultrasound: Virtual environments\nand parallel systems,” IEEE/CAA J. Automatica Sinica , vol. 8, no. 5,\npp. 1095–1106, May 2021.\n[14] F.-Y . Wang et al., “Where does AlphaGo go: From church-turing thesis\nto AlphaGo thesis and beyond,” IEEE/CAA J. Automatica Sinica,v o l .3 ,\nno. 2, pp. 113–120, Apr. 2016.\n[15] D. G. Lowe, “Object recognition from local scale-invariant features,” in\nProc. 7th IEEE Int. Conf. Comput. Vis. , 1999, pp. 1150–1157.\n2158 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS, VOL. 53, NO. 4, APRIL 2023\n[16] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\ndetection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2005,\npp. 886–893.\n[17] H. Bay, T. Tuytelaars, and L. Van Gool, “SURF: Speeded up robust\nfeatures,” in Proc. Eur. Conf. Comput. Vis. , 2006, pp. 404–417.\n[18] E. Rublee, V . Rabaud, K. Konolige, and G. Bradski, “ORB: An efﬁcient\nalternative to SIFT or SURF,” in Proc. Int. Conf. Comput. Vis. , 2011,\npp. 2564–2571.\n[19] S. Qaiser and R. Ali, “Text mining: Use of TF-IDF to examine the\nrelevance of words to documents,” Int. J. Comput. Appl., vol. 181, no. 1,\npp. 25–29, 2018.\n[20] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow, “Thermometer encod-\ning: One hot way to resist adversarial examples,” in Proc. Int. Conf.\nLearn. Represent., 2018, pp. 75–89.\n[21] K. W. Church, “Word2Vec,” Nat. Language Eng. , vol. 23, no. 1,\npp. 155–162, 2017.\n[22] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[23] X. Li, Y . Wang, L. Yan, K. Wang, F. Deng, and F.-Y . Wang, “ParallelEye-\nCS: A new dataset of synthetic images for testing the visual intelligence\nof intelligent vehicles,” IEEE Trans. Veh. Technol. , vol. 68, no. 10,\npp. 9619–9631, Oct. 2019.\n[24] X. Li, K. Wang, Y . Tian, L. Yan, F. Deng, and F.-Y . Wang, “The\nParallelEye dataset: A large collection of virtual images for traf-\nﬁc vision research,” IEEE Trans. Intell. Transp. Syst. , vol. 20, no. 6,\npp. 2072–2084, Jun. 2019.\n[25] Z. Zhang, J. Liu, G. Liu, J. Wang, and J. Zhang, “Robustness ver-\niﬁcation of swish neural networks embedded in autonomous driving\nsystems,” IEEE Trans. Comput. Social Syst. , early access, Jun. 9, 2022,\ndoi: 10.1109/TCSS.2022.3179659.\n[26] Y . Liang, M. Li, C. Jiang, and G. Liu, “CEModule: A computa-\ntion efﬁcient module for lightweight convolutional neural networks,”\nIEEE Trans. Neural Netw. Learn. Syst. , early access, Dec. 15, 2021,\ndoi: 10.1109/TNNLS.2021.3133127.\n[27] Y . Qin, C. Yan, G. Liu, Z. Li, and C. Jiang, “Pairwise Gaussian loss\nfor convolutional neural networks,” IEEE Trans. Ind. Informat., vol. 16,\nno. 10, pp. 6324–6333, Oct. 2020.\n[28] Y . LeCun and Y . Bengio, “Convolutional networks for images, speech,\nand time series,” in The Handbook of Brain Theory and Neural\nNetworks. Cambridge, MA, USA: MIT Press, 1995, pp. 157–168.\n[29] J. B. Pollack, “Recursive distributed representations,” Artif. Intell. ,\nvol. 46, no. 1, pp. 77–105, 1990.\n[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁca-\ntion with deep convolutional neural networks,” Commun. ACM, vol. 60,\nno. 6, pp. 84–90, 2017.\n[31] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual\nnetworks,” in Proc. Eur. Conf. Comput. Vis. , 2016, pp. 630–645.\n[32] W. Zeng, “PanGu- α: Large-scale autoregressive pretrained chinese lan-\nguage models with auto-parallel computation,” 2021, arXiv:2104.12369.\n[33] P. Lewis et al., “Retrieval-augmented generation for knowledge-intensive\nNLP tasks,” in Proc. Int. Conf. Adv. Neural Inf. Process. Syst. , 2020,\npp. 9459–9474.\n[34] W. Wang et al., “Image as a foreign language: BEiT pretraining for all\nvision and vision-language tasks,” 2022, arXiv:2208.10442.\n[35] C. Lv et al., “Levenberg–Marquardt backpropagation training of\nmultilayer neural networks for state estimation of a safety-critical\ncyber-physical system,” IEEE Trans. Ind. Informat. , vol. 14, no. 8,\npp. 3436–3446, Aug. 2018.\n[36] S. J. Oks, M. Jalowski, A. Fritzsche, and K. M. Möslein, “Cyber-physical\nmodeling and simulation: A reference architecture for designing demon-\nstrators for industrial cyber-physical systems,” Procedia CIRP, vol. 84,\npp. 257–264, Jan. 2019.\n[37] A. G. Kravets, N. Salnikova, K. Dmitrenko, and M. Lempert, “Industrial\ncyber-physical systems: Risks assessment and attacks modeling,” in\nCyber-Physical Systems: Industry 4.0 Challenges . Cham, Switzerland:\nSpringer Int., 2020.\n[38] H. Tao et al., “TrustData: Trustworthy and secured data collection for\nevent detection in industrial cyber-physical system,” IEEE Trans. Ind.\nInformat., vol. 16, no. 5, pp. 3311–3321, May 2020.\n[39] B. Li, Y . Wu, J. Song, R. Lu, T. Li, and L. Zhao, “DeepFed: Federated\ndeep learning for intrusion detection in industrial cyber–physical\nsystems,” IEEE Trans. Ind. Informat. , vol. 17, no. 8, pp. 5615–5624,\nAug. 2021.\n[40] X. Zhou, W. Liang, S. Shimizu, J. Ma, and Q. Jin, “Siamese neural\nnetwork based few-shot learning for anomaly detection in industrial\ncyber-physical systems,” IEEE Trans. Ind. Informat. , vol. 17, no. 8,\npp. 5790–5798, Aug. 2021.\n[41] X. Zhou et al., “Intelligent small object detection for digital twin in smart\nmanufacturing with industrial cyber-physical systems,” IEEE Trans. Ind.\nInformat., vol. 18, no. 2, pp. 1377–1386, Feb. 2022.\n[42] S. Kumar, C. Savur, and F. Sahin, “Survey of human–robot collaboration\nin industrial settings: Awareness, intelligence, and compliance,” IEEE\nTrans. Syst., Man, Cybern., Syst., vol. 51, no. 1, pp. 280–297, Jan. 2021.\n[43] F.-Y . Wang, “The emergence of intelligent enterprises: From CPS to\nCPSS,” IEEE Intell. Syst. , vol. 25, no. 4, pp. 85–88, Jul.–Aug. 2010.\n[44] X. Wang, J. Yang, J. Han, W. Wang, and F.-Y . Wang, “Metaverses and\nDeMetaverses: From digital twins in CPS to parallel intelligence in\nCPSS,” IEEE Intell. Syst. , vol. 37, no. 4, pp. 97–102, Jul.–Aug. 2022.\n[45] X. Sun, Y . Gao, R. Sutcliffe, S.-X. Guo, X. Wang, and J. Feng, “Word\nrepresentation learning based on bidirectional GRUs with drop loss for\nsentiment classiﬁcation,” IEEE Trans. Syst., Man, Cybern., Syst., vol. 51,\nno. 7, pp. 4532–4542, Jul. 2021.\n[46] C. Sun et al., “Proximity based automatic data annotation for\nautonomous driving,” IEEE/CAA J. Automatica Sinica , vol. 7, no. 2,\npp. 395–404, Mar. 2020.\n[47] S. Wang et al., “Robotic intra-operative ultrasound: Virtual environments\nand parallel systems,” IEEE/CAA J. Automatica Sinica , vol. 8, no. 5,\npp. 1095–1106, May 2021.\n[48] J. Kauffman, “A successful failure: NASA’s crisis communications\nregarding Apollo 13,”Public Relations Rev., vol. 27, no. 4, pp. 437–448,\n2001.\n[49] Y . Tian, X. Li, K. Wang, and F.-Y . Wang, “Training and testing object\ndetectors with virtual images,” IEEE/CAA J. Automatica Sinica ,v o l .5 ,\nno. 2, pp. 539–546, Mar. 2018.\n[50] A. Gaidon, Q. Wang, Y . Cabon, and E. Vig, “VirtualWorlds as proxy\nfor multi-object tracking analysis,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2016, pp. 4340–4349.\n[51] P. Sun et al., “Scalability in perception for autonomous driving: Waymo\nopen dataset,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\n2020, pp. 2446–2454.\n[52] F. Schuldt, “Ein beitrag für den methodischen test von automa-tisierten\nfahrfunktionen mit hilfe von virtuellen umgebungen,” Ph.D. disserta-\ntion, Dept. Eng. Sci., Tech. Univ. Carolo-Wilhelmina Braunschweig,\nBraunschweig, Germany, 2017.\n[53] S. Wang, L. Ouyang, Y . Yuan, X. Ni, X. Han, and F.-Y . Wang,\n“Blockchain-enabled smart contracts: architecture, applications, and\nfuture trends,” IEEE Trans. Syst., Man, Cybern., Syst. , vol. 49, no. 11,\npp. 2266–2277, Nov. 2019.\n[54] R. Qin, Y . Yuan, and F.-Y . Wang, “Research on the selection strategies\nof blockchain mining pools,” IEEE Trans. Syst., Man, Cybern., Syst. ,\nvol. 5, no. 3, pp. 748–757, Sep. 2018.\n[55] F.-Y . Wang, “The DAO to metacontrol for metasystems in metaverse:\nThe system of parallel control systems for knowledge automation and\ncontrol intelligence in CPSS,” IEEE/CAA J. Automatica Sinica ,v o l .9 ,\nno. 11, pp. 1899–1908, Nov. 2022.\n[56] F.-Y . Wang and Y . Wang, “Parallel ecology for intelligent and smart\ncyber–physical–social systems,” IEEE Trans. Computat. Social Syst. ,\nvol. 7, no. 6, pp. 1318–1323, Dec. 2020.\n[57] S. Hamburg, “Call to join the decentralized science movement,” Nature,\nvol. 600, no. 7888, p. 221, 2021.\n[58] F.-Y . Wang, X. Wang, L. Li, and L. Li, “Steps toward parallel intel-\nligence,” IEEE/CAA J. Automatica Sinica , vol. 3, no. 4, pp. 345–348,\nOct. 2016.\n[59] F.-Y . Wang, K. M. Carley, D. Zeng, and W. Mao, “Social comput-\ning: From social informatics to social intelligence,” IEEE Intell. Syst. ,\nvol. 22, no. 2, pp. 79–83, Mar./Apr. 2007.\n[60] F.-Y . Wang, R. Qin, Y . Chen, Y . Tian, X. Wang, and B. Hu,\n“Federated ecology: Steps toward confederated intelligence,” IEEE\nTrans. Computat. Social Syst. , vol. 8, no. 2, pp. 271–278, Apr. 2021.\n[61] F.-Y . Wang, R. Qin, Y . Yuan, and B. Hu, “Nonfungible tokens:\nConstructing value systems in parallel societies,”IEEE Trans. Computat.\nSocial Syst., vol. 8, no. 5, pp. 1062–1067, Oct. 2021.\n[62] X. Zhang et al., “Finding critical scenarios for automated driving\nsystems: A systematic literature review,” IEEE Trans. Softw. Eng., early\naccess, Apr. 26, 2022. [Online]. Available: https://ieeexplore.ieee.org/\nabstract/document/9763411\n[63] F. Bellalouna, “Industrial use cases for augmented reality application,”\nin Proc. 11th IEEE Int. Conf. Cogn. Infocommun. , 2020, pp. 11–18.\n[64] Z. Feng et al., “ERNIE-ViLG 2.0: Improving text-to-image diffusion\nmodel with knowledge-enhanced mixture-of-denoising-experts,” 2022,\narXiv:2210.15257.\nLI et al.: NOVEL SE METHODOLOGY FOR FOUNDATION MODELS IN META VERSE 2159\n[65] T. Wang, Y . Chen, M. Qiao, and H. Snoussi, “A fast and robust convo-\nlutional neural network-based defect detection model in product quality\ncontrol,” Int. J. Adv. Manuf. Technol. , vol. 94, no. 9, pp. 3465–3471,\n2018.\n[66] A. O. Ly and M. Akhlouﬁ, “Learning to drive by imitation: An overview\nof deep behavior cloning methods,” IEEE Trans. Intell. Veh. ,v o l . 6 ,\nno. 2, pp. 195–209, Jun. 2021.\n[67] M. E. Kabir, I. Sorkhoh, B. Moussa, and C. Assi, “Joint routing and\nscheduling of mobile charging infrastructure for V2V energy transfer,”\nIEEE Trans. Intell. Veh., vol. 6, no. 4, pp. 736–746, Dec. 2021.\n[68] D. Cao et al., “Future directions of intelligent vehicles: Potentials,\npossibilities, and perspectives,” IEEE Trans. Intell. Veh. , vol. 7, no. 1,\npp. 7–10, Mar. 2022.\n[69] F.-Y . Wang et al., “Veriﬁcation and validation of intelligent vehicles:\nObjectives and efforts from China,” IEEE Trans. Intell. Veh. ,v o l . 7 ,\nno. 2, pp. 164–169, Jun. 2022.\nXuan Li received the Ph.D. degree in control sci-\nence and engineering from the Beijing Institute of\nTechnology, Beijing, China, in 2020.\nAfter that, he joined Peng Cheng Laboratory,\nShenzhen, China, and became an Assistant Professor\nwith the Virtual Reality Studio. He was a Visiting\nScholar with the Department of Computer Science,\nStony Brook University, Stony Brook, NY , USA,\nfrom October 2018 to October 2019. His research\ninterests include scenarios engineering, computer\nvision, bionic vision computing, and machine\nlearning.\nYonglin Tian received the Ph.D. degree in con-\ntrol science and engineering from the University of\nScience and Technology of China, Hefei, China, in\n2022.\nHe is currently a Postdoctoral Researcher with\nthe Institute of Automation, Chinese Academy of\nSciences, Beijing, China. His research interests\ninclude computer vision and intelligent transporta-\ntion systems.\nPeijun Ye received the Ph.D. degree from the\nUniversity of Chinese Academy of Sciences,\nBeijing, China, in 2013.\nHe is currently an Associate Professor with the\nState Key Laboratory for Management and Control\nof Complex Systems, Institute of Automation,\nChinese Academy of Sciences, and the Research\nDirector of Parallel Data Research Center, Qingdao\nAcademy of Intelligent Industries, Qingdao, China.\nHe has authored/coauthored more than 30 papers,\nﬁve patents, and one publication. His current\nresearch interests are cognitive computing, digital human, and intelligent\ntransportation systems.\nHaibin Duan (Senior Member, IEEE) received the\nPh.D. degree in control theory and engineering from\nNanjing University of Aeronautics and Astronautics,\nNanjing, China, in 2005.\nHe is a Full Professor with the School of\nAutomation Science and Electrical Engineering,\nBeihang University, Beijing, China, where he is the\nVice Director of the State Key Laboratory of Virtual\nReality Technology and Systems, and the Head\nof the Bio-Inspired Autonomous Flight Systems\nResearch Group. He has authored or coauthored\nmore than 70 publications. His current research interests are bio-inspired\nintelligence, biological computer vision, and multi-UA V swarm autonomous\ncontrol.\nProf. Duan received the National Science Fund for Distinguished Young\nScholars of China in 2014. He is also enrolled in the Chang Jiang Scholars\nProgram of China, Scientiﬁc and Technological Innovation Leading Talent\nof “Ten Thousand Plan”-National High Level Talents Special Support Plan,\nand Top-Notch Young Talents Program of China, Program for New Century\nExcellent Talents in University of China, and Beijing NOV A Program. He is\nthe Editor-in-Chief of Guidance, Navigation and Control and an Associate\nEditor of the IEEE Transactions on Cybernetics and IEEE Transactions on\nCircuits and Systems—II: Express Briefs .\nFei-Yue Wang (Fellow, IEEE) received the Ph.D.\ndegree in computer and systems engineering from\nRensselaer Polytechnic Institute, Troy, NY , USA, in\n1990.\nHe joined the University of Arizona, Tucson, AZ,\nUSA, in 1990 and became a Professor and the\nDirector of the Robotics and Automation Laboratory\nand Program in Advanced Research for Complex\nSystems. In 1999, he founded the Intelligent\nControl and Systems Engineering Center, Institute of\nAutomation, Chinese Academy of Sciences (CAS),\nBeijing, China, under the support of the Outstanding Oversea Chinese Talents\nProgram from the State Planning Council and “100 Talent Program” from\nCAS. In 2011, he became the State Specially Appointed Expert and the\nDirector of the State Key Laboratory for Management and Control of Complex\nSystems. His current research focuses on methods and applications for paral-\nlel intelligence, social computing, and knowledge automation.\nProf. Wang was the Founding Editor-in-Chief (EiC) of the International\nJournal of Intelligent Control and Systems from 1995 to 2000, the\nIEEE Intelligent Transportation Systems Magazine from 2006 to 2007, the\nIEEE/CAA J\nOURNAL OF AUTOMATICA SINICA from 2014 to 2017, and\nthe Chinese Journal of Command and Control from 2015 to 2020. He\nwas the EiC of the IEEE I NTELLIGENT SYSTEMS from 2009 to 2012, and\nthe IEEE T RANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\nfrom 2009 to 2016, and has been the EiC of the IEEE T RANSACTIONS ON\nCOMPUTATIONAL SOCIAL SYSTEMS since 2017, and the Founding EiC of\nthe Chinese Journal of Intelligent Science and Technology since 2019. He\nis currently the President of CAA’s Supervision Council and IEEE Council\non RFID, and the Vice President of IEEE Systems, Man, and Cybernetics\nSociety.",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.6950737237930298
    },
    {
      "name": "Metaverse",
      "score": 0.5505016446113586
    },
    {
      "name": "Computer science",
      "score": 0.49976682662963867
    },
    {
      "name": "Human–computer interaction",
      "score": 0.21863910555839539
    },
    {
      "name": "Geography",
      "score": 0.12188616394996643
    },
    {
      "name": "Archaeology",
      "score": 0.10721427202224731
    },
    {
      "name": "Virtual reality",
      "score": 0.10420003533363342
    }
  ]
}