{
  "title": "Language models and linguistic theories beyond words",
  "url": "https://openalex.org/W4385065380",
  "year": 2023,
  "authors": [],
  "references": [
    "https://openalex.org/W2060418758",
    "https://openalex.org/W4362664122",
    "https://openalex.org/W4283266503",
    "https://openalex.org/W4366823268",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W2916562859",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W2905623858"
  ],
  "abstract": null,
  "full_text": "nature machine intelligence\n Volume 5 | July 2023 | 677–678 | 677\nhttps://doi.org/10.1038/s42256-023-00703-8\nEditorial\nLanguage models and linguistic theories  \nbeyond words\nThe development of large \nlanguage models is mainly a feat \nof engineering and so far has \nbeen largely disconnected from \nthe field of linguistics. Exploring \nlinks between the two directions is \nreopening longstanding debates in \nthe study of language.\nF\nrederick Jelinek, a renowned \nCzech-American researcher in natu-\nral language processing and speech \nrecognition, famously said in 1985, \n\"Every time I fire a linguist, the per -\nformance of the speech recognizer goes up”1, \nsuggesting that there may be no efficient \nway to include linguistic knowledge in such \nsystems2. Does this sentiment also hold true \nfor state-of-the-art large language models \n(LLMs), which seem to be mostly artefacts of \ncomputer science and engineering? Both LLMs \nand linguistics deal with human languages, but \nwhether or how they can benefit each other is \nnot clear.\nT o start discussing connections between \nthe two fields, a distinction needs to be made \nbetween computational linguistics and other \nkinds of linguistics — theoretical, cognitive, \ndevelopmental and so on. Computational \nlinguistics traditionally uses computational \nmodels to address questions in linguistics and \nborders the field of natural language process-\ning, which in turn builds models of language \nfor practical applications such as machine \ntranslation. The Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), \nthe largest conference in the field, has seen an \nincrease of 44% in the number of submissions \nover the past year, from 3,378 in 2022 to 4,864 \nin 2023. These numbers are hardly surprising \ngiven the rise of natural language processing \nin the past few years and, more recently, of \nLLMs. There is also increasing interest from \nresearchers in other disciplines who recog -\nnize the potential of computational models of \nlanguage in their own work. An article in our \nMay 2023 issue proposes drawing inspiration \nfrom computational linguistics and natural \nlanguage processing for building protein \nlanguage models3. Another recent article in \nNature uses a classical computational linguis-\ntic approach for designing mRNA vaccines4.\nBut other linguistic disciplines, such as \ncognitive and developmental linguistics, \nwhich focus on child language acquisition \nand human cognition, are becoming more \nvisible as well. For instance, in the search for \ncomputational models inspired by infant-like \nlearning, researchers are considering the kind \nof input that babies learn from 5. An exciting \nstep in this direction is the BabyLM challenge, \nwhich gives machine learning researchers \nthe task of training language models from \nscratch on amounts of linguistic data similar to \nthose available to a 13-year-old child: around  \n100 million words, rather than the estimated \n300 billion words ingested by ChatGPT.\nIt is generally agreed that LLMs do not imple-\nment a particular linguistic theory. Noam \nChomsky, the pioneer of modern linguistics, \nlikened LLMs to a bulldozer, saying that they \nare a useful tool to have but “not a contribution \nto science. ” Other scientists, however, hold a \ndiametrically opposite view: Steven Pianta -\ndosi, a professor of psychology and neurosci-\nence at the University of California, Berkeley, \nrecently stated that LLMs are “precise and \nformal accounts” of language learning, and \nthat their success brings Chomsky’s influential \nlinguistic theory of universal grammar, which \npostulates the existence of innate biological \nconstraints that enable humans to learn lan -\nguages, to “a remarkable downfall”6. Although \nthis specific debate recently attracted media \nattention, it is reminiscent of other ongoing \ndiscussions in linguistics and cognitive sci -\nence. One of them, which we brought up in our \nApril 2023 editorial 7, is a debate on whether \nLLMs are truly capable of understanding lan-\nguage or merely mimic it\n8. Another dispute is \nbetween those who consider statistical pat -\ntern discovery to be a useful tool in linguistics \nand language acquisition, and those who, like \nChomsky, think this sort of empirical analysis \nof surface language forms is fruitless and the \nonly viable approach is to look at the underly-\ning syntactic structures. Although there are \nnuances to such debates, all of them share a \ndisagreement about how useful — for science, \nhumanity and linguistics — the state-of-the-art \nLLMs are, and whether their cost is justified.\nThe positions taken by each side in these \ndebates are often extreme, but there have also \nbeen more balanced views on what linguis -\ntics and state-of-the-art computer models can \noffer each other. Connections between theo-\nretical linguistics and deep learning were dis-\ncussed several years ago in Language, wherein \nTal Linzen, a professor of linguistics and data \nscience at New York University, highlighted \npossible pathways for interaction between \ndeep neural networks and research on lan\n-\nguage. He argued that linguists could benefit \nin various ways from the platform for con -\nstructing models of language acquisition and \nprocessing that neural networks provide9. This \nrecommendation may apply equally well, if \nnot even better, to the recent LLMs.\nFrom the cognitive perspective, a balanced \nview on the relationship between LLMs and \nhuman cognition was outlined in a recent pre-\nprint article inspired by research in neurosci-\nence10. Although LLMs excel at language, they \nare not models of thought — or, in linguistic \nterminology, they succeed at formal compe-\ntence, being able to generate meaningful and \ncoherent texts and replicate some complex \nhuman-like linguistic behaviours, but fail at \nfunctional competence, which has to do with \nworld knowledge and pragmatics. The bal -\nance, therefore, may lie in using LLMs in the \ncapacity they actually possess: as language \ntools that can, for example, assist us in writ -\ning texts, translating them into a different \nlanguage, generating code in programming \nlanguages, etc.\nLLMs currently have little to do with lin -\nguistics and human cognition, and there is \na chance that in the future they will diverge \neven more11. However, the field of linguistics \nis clearly affected by the development of tools \nso powerful that their output can easily be con-\nfused with human-generated texts. LLMs are \nagain reopening some of the debates in lin -\nguistics that have been ongoing for decades12, \nand there is hope that they will be put to good \nuse in future linguistic research efforts.\nPublished online: 21 July 2023\n Check for updates\nnature machine intelligence Volume 5 | July 2023 | 677–678 | 678\nEditorial\nReferences\n1. Moore, R. K. ISCA https://www.isca-speech.org/archive/\npdfs/interspeech_2005/moore05_interspeech.pdf \n(2005).\n2. Jelinek, F. Lang. Resour. Eval. 39, 25–34 (2005).\n3. Vu, M. H. et al. Nat. Mach. Intell. 5, 485–496 (2023).\n4. Zhang, H. et al. Nature https://www.nature.com/articles/\ns41586-023-06127-z (2023).\n5. Zaadnoordijk, L., Besold, T. R. & Cusack, R. Nat. Mach. \nIntell. 4, 510–520 (2022).\n6. Piantadosi, S. LingBuzz https://lingbuzz.net/lingbuzz/ \n007180 (2023).\n7. Nat. Mach. Intell. 5, 331–332 (2023).\n8. Mitchell, M. & Krakauer, D. C. Proc. Natl Acad. Sci. 120, \ne2215907120 (2023).\n9. Linzen, T. Language 95, e99–e108 (2019).\n10. Mahowald, K. et al. Preprint at https://doi.org/10.48550/\narXiv.2301.06627 (2023).\n11. Pater, J. Language 95, e41–e74 (2019).\n12. Piattelli-Palmarini, M. (ed.) Language and Learning: The \nDebate Between Jean Piaget and Noam Chomsky (Harvard \nUniv. Press, 1980).",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.730377733707428
    },
    {
      "name": "Computer science",
      "score": 0.3774723410606384
    },
    {
      "name": "Psychology",
      "score": 0.33672821521759033
    },
    {
      "name": "Philosophy",
      "score": 0.17324092984199524
    }
  ]
}