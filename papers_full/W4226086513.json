{
  "title": "Temporal Attention for Language Models",
  "url": "https://openalex.org/W4226086513",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2739021270",
      "name": "Guy D. Rosin",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1780829609",
      "name": "Kira Radinsky",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3153269634",
    "https://openalex.org/W3171460770",
    "https://openalex.org/W1570098300",
    "https://openalex.org/W4212964822",
    "https://openalex.org/W3115228514",
    "https://openalex.org/W3173107309",
    "https://openalex.org/W3035345764",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950883770",
    "https://openalex.org/W2102343050",
    "https://openalex.org/W2921267133",
    "https://openalex.org/W3154525977",
    "https://openalex.org/W3217756080",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W2952276524",
    "https://openalex.org/W4246334968",
    "https://openalex.org/W2931922640",
    "https://openalex.org/W2804830075",
    "https://openalex.org/W2949573100",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W2948599618",
    "https://openalex.org/W2788036042",
    "https://openalex.org/W3167421168",
    "https://openalex.org/W3101767658",
    "https://openalex.org/W4297792210",
    "https://openalex.org/W3175236579",
    "https://openalex.org/W3002481786",
    "https://openalex.org/W2473408052",
    "https://openalex.org/W2738734060",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963760040",
    "https://openalex.org/W4285190530",
    "https://openalex.org/W2992121790"
  ],
  "abstract": "Pretrained language models based on the transformer architecture have shown great success in NLP.Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information.They are trained on the textual data alone, limiting their ability to generalize temporally.In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware self-attention mechanism.Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points. This mechanism allows the transformer to capture this temporal information and create time-specific contextualized word representations.We leverage these representations for the task of semantic change detection; we apply our proposed mechanism to BERT and experiment on three datasets in different languages (English, German, and Latin) that also vary in time, size, and genre.Our proposed model achieves state-of-the-art results on all the datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1498 - 1508\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nTemporal Attention for Language Models\nGuy D. Rosinand Kira Radinsky\nTechnion – Israel Institute of Technology, Haifa, Israel\n{guyrosin,kirar}@cs.technion.ac.il\nAbstract\nPretrained language models based on the trans-\nformer architecture have shown great success\nin NLP. Textual training data often comes from\nthe web and is thus tagged with time-speciﬁc\ninformation, but most language models ignore\nthis information. They are trained on the tex-\ntual data alone, limiting their ability to gener-\nalize temporally. In this work, we extend the\nkey component of the transformer architecture,\ni.e., the self-attention mechanism, and pro-\npose temporal attention—a time-aware self-\nattention mechanism. Temporal attention can\nbe applied to any transformer model and re-\nquires the input texts to be accompanied with\ntheir relevant time points. It allows the trans-\nformer to capture this temporal information\nand create time-speciﬁc contextualized word\nrepresentations. We leverage these representa-\ntions for the task of semantic change detection;\nwe apply our proposed mechanism to BERT\nand experiment on three datasets in different\nlanguages (English, German, and Latin) that\nalso vary in time, size, and genre. Our pro-\nposed model achieves state-of-the-art results\non all the datasets.\n1 Introduction\nLanguage models (LMs) are usually pretrained on\ncorpora derived from a snapshot of the web crawled\nat a speciﬁc moment in time (Devlin et al., 2019;\nLiu et al., 2019). But our language is constantly\nevolving; new words are created, meanings and\nword usages change. For instance, the COVID-19\npandemic has caused signiﬁcant changes to our\nlanguage; consider the new video-related sense of\n“Zoom” and the new senses recently associated with\nthe word “vaccine”.\nThe “static” nature of existing LMs makes them\nunaware of time, and in particular unware of lan-\nguage changes that occur over time. This prevents\nsuch models from adapting to time and generaliz-\ning temporally (Röttger and Pierrehumbert, 2021;\nLazaridou et al., 2021; Hombaiah et al., 2021;\nDhingra et al., 2022; Agarwal and Nenkova, 2021;\nLoureiro et al., 2022), abilities that were shown to\nbe important for many tasks in NLP and Informa-\ntion Retrieval (Kanhabua and Anand, 2016; Rosin\net al., 2017; Huang and Paul, 2019; Röttger and\nPierrehumbert, 2021; Savov et al., 2021). Recently,\nto create time-aware models, the NLP community\nhas started to use time as a feature in training and\nﬁne-tuning language models (Dhingra et al., 2022;\nRosin et al., 2022). These two studies achieve this\nby concatenating a time token to the text sequence\nbefore training the models. The former was con-\ncerned with temporal question answering, whereas\nthe latter—with semantic change detection and sen-\ntence time prediction. In this work, we introduce\na new methodology to create time-aware language\nmodels and experiment on the task of semantic\nchange detection.\nAt the heart of the transformer architecture is the\nself-attention mechanism (Vaswani et al., 2017).\nThis mechanism allows the transformer to capture\nthe complex relationships between words by re-\nlating them to each other multiple times. An at-\ntention weight has a clear meaning: how much a\nparticular word will be weighted when computing\nthe next representation for the current word (Clark\net al., 2019). This mechanism also enables the\nabove-mentioned temporal models (Dhingra et al.,\n2022; Rosin et al., 2022) to work; by concatenat-\ning time-speciﬁc tokens to the text sequences, the\nself-attention mechanism would compute the rela-\ntionships between them and the original tokens in\nthe texts, effectively making the output embeddings\ntime-aware (as the output embeddings will depend\non the concatenated time tokens).\nIn this work, instead of changing the text se-\nquences as in prior work, we modify the model\nitself and speciﬁcally the attention mechanism to\nmake it time-aware. We propose a time-aware self-\nattention mechanism that is an extension of the\n1498\nself-attention mechanism of the transformer. It con-\nsiders the time the text sequences (or documents)\nwere written when computing attention scores. As\ndescribed above, self-attention captures relation-\nships between words. We want to condition these\nrelationships on time. By adding a time matrix\nas an additional input to the self-attention (along\nwith the standard query, key, and value matrices),\nwe condition the attention weights on the time. In\nother words, the adapted mechanism also consid-\ners the time when calculating the weights of each\nword. We refer to this adapted attention as Tem-\nporal Attention(Section 3.2). See Figure 1 for an\nillustration of our proposed mechanism.\nWe experiment on the task of semantic change\ndetection — the task of identifying which words\nundergo semantic changes and to what extent. Se-\nmantic change detection methods are used in histor-\nical linguistics and digital humanities to study the\nevolution of word meaning over time and in differ-\nent domains (Kutuzov et al., 2018). Most existing\ncontextual methods detect changes by ﬁrst embed-\nding the target words in each time point and then\neither aggregating them to create a time-speciﬁc\nembedding (Martinc et al., 2020a), or computing a\ncluster of the embeddings for each time (Giulianelli\net al., 2020; Martinc et al., 2020b; Montariol et al.,\n2021; Laicher et al., 2021). The embeddings or\nclusters are compared to estimate the degree of\nchange between different times. We experiment\nwith several diverse datasets in terms of time, lan-\nguage, size, and genre. Our empirical results show\nthat our model outperforms state-of-the-art meth-\nods (Schlechtweg et al., 2019; Martinc et al., 2020a;\nMontariol et al., 2021; Rosin et al., 2022).\nOur contributions are threefold: (1) We intro-\nduce a time-aware self-attention mechanism as an\nextension of the original mechanism of the trans-\nformer. The proposed mechanism considers the\ntime the text sequences were written. The time\nis considered during the computation of attention\nscores, thus allowing to create time-speciﬁc con-\ntextualized word representations; (2) We conduct\nevaluations on the task of semantic change detec-\ntion and reach state-of-the-art performance on three\ndiverse datasets in terms of time, language, size,\nand genre; (3) We contribute our code and trained\nmodels to the community for further research.1\n1https://github.com/guyrosin/temporal_\nattention\nFigure 1: High-level illustration of our proposed tem-\nporal attention mechanism.\n2 Related Work\n2.1 Temporal Language Models\nSeveral recent studies have explored and evaluated\nthe generalization ability of language models to\ntime (Röttger and Pierrehumbert, 2021; Lazaridou\net al., 2021; Agarwal and Nenkova, 2021; Hofmann\net al., 2021; Loureiro et al., 2022). To better handle\ncontinuously evolving web content, Hombaiah et al.\n(2021) performed incremental training. Dhingra\net al. (2022) experimented with temporal language\nmodels for question answering. They focused on\ntemporally-scoped facts and showed that condition-\ning temporal language models on the temporal con-\ntext of textual data improves memorization of facts.\nRosin et al. (2022) similarly concatenated time to-\nkens to text sequences and introduced the concept\nof time masking (speciﬁc masking for the added\ntime tokens). They focused on two temporal tasks:\nsemantic change detection and sentence time pre-\ndiction. Others focused on document classiﬁcation\nby using word-level temporal embeddings (Huang\nand Paul, 2019) and adapting pretrained BERT\nmodels to domain and time (Röttger and Pierre-\nhumbert, 2021). Recently, Hofmann et al. (2021)\njointly modeled temporal and social information\nby changing the architecture of BERT and connect-\ning embeddings of adjacent time points via a latent\nGaussian process.\nIn this work, we create a temporal LM by adapt-\ning the transformer’s self-attention mechanism to\ntime. The model receives each text sequence along\nwith its writing time and uses both as input to the\ntemporal attention mechanism. As a result, the\nmodel creates time-speciﬁc contextualized word\nembeddings.\n2.2 Semantic Change Detection\nSemantic change detection is the task of identify-\ning words that change meaning over time (Kutuzov\net al., 2018; Tahmasebi et al., 2018). This task is\noften addressed using time-aware word representa-\ntions that are learned from time-annotated corpora\n1499\nand then compared between different time points\n(Jatowt and Duh, 2014; Kim et al., 2014; Kulkarni\net al., 2015; Hamilton et al., 2016; Dubossarsky\net al., 2019; Del Tredici et al., 2019). Gonen et al.\n(2020) used a simple nearest-neighbors-based ap-\nproach to detect semantically-changed words. Oth-\ners learned time-aware embeddings simultaneously\nover all time points to resolve the alignment prob-\nlem, by regularization (Yao et al., 2018), mod-\neling word usage as a function of time (Rosen-\nfeld and Erk, 2018), Bayesian skip-gram (Bamler\nand Mandt, 2017), or exponential family embed-\ndings (Rudolph and Blei, 2018).\nAll aforementioned methods limit the representa-\ntion of each word to a single meaning, ignoring the\nambiguity in language and limiting their sensitivity.\nRecent contextualized models (e.g., BERT (Devlin\net al., 2019)) overcome this limitation by taking\nsentential context into account when inferring word\ntoken representations. Such models were applied to\ndiachronic semantic change detection, where most\ndetect changes by creating time-speciﬁc embed-\ndings or computing a cluster of the embeddings for\neach time, and then comparing these embeddings or\nclusters to estimate the degree of change between\ndifferent times (Hu et al., 2019; Martinc et al.,\n2020b,a; Giulianelli et al., 2020; Laicher et al.,\n2021; Montariol et al., 2021). Recently, Rosin et al.\n(2022) suggested another approach of detecting se-\nmantic change through predicting the writing time\nof sentences. In our work, we use language models\nto create time-speciﬁc word representations and\ncompare them to detect semantic change. While\nthe above studies used language models as is, we\nmodify their inner workings to make them time-\naware by adapting the self-attention mechanism to\ntime.\n3 Model\nOur model adopts a multi-layer bidirectional trans-\nformer (Vaswani et al., 2017). It treats words\nin the document as input tokens and computes\na representation for each token. Formally, given\na sequence of nwords w1,w2,...,w n, the trans-\nformer computes D-dimensional word representa-\ntions x1,x2,...,x n ∈RD.\n3.1 Self-Attention\nThe self-attention mechanism is the foundation of\nthe transformer (Vaswani et al., 2017). It relates\ntokens to each other based on the attention score\nFigure 2: Illustration of our proposed temporal atten-\ntion mechanism.\nbetween each pair of tokens. In practice, the at-\ntention function is computed on a set of tokens\nsimultaneously; our input sequence is packed to-\ngether into a matrix X ∈Rn×D, in which each row\nicorresponds to a word representation xi in the in-\nput sentence. We denote three trainable weight\nmatrices by WQ,WK,WV ∈RD×dk . We then cre-\nate three distinct representations, i.e., query, key,\nand value: Q= XWQ, K = XWK, V = XWV ,\nrespectively, where Q,K,V ∈Rn×dk .\nAn attention function can be described as map-\nping a query and a set of key-value pairs to an out-\nput, where the query, keys, values, and outputs are\nall vectors. The output is computed as a weighted\nsum of the values, where the weight assigned to\neach value is determined by the dot product of the\nquery with all the keys:\nAttention(Q,K,V ) = softmax\n(QK⊺\n√dk\n)\nV (1)\n3.2 Temporal Attention\nWe now describe the temporal attention mechanism.\nIn the temporal setting, similarly to the vocabulary\nof the model, our model has a vocabulary of time\npoints. Theoretically, each token in an input se-\nquence could have its own time point, but we sim-\nplify and assume the most common case where text\nsequences always refer to a single time point t.2\nGiven a sequence of nwords w1,w2,...,w n and\nits corresponding time point t, our model computes\nD-dimensional time-speciﬁc word representations\nxt\n1,xt\n2,...,x t\nn, where xt\ni ∈RD. As a by-product,\n2Our mechanism also supports the setting where different\ntokens in a sequence are associated with different time points.\n1500\nwe also compute D-dimensional time representa-\ntions for the time points. Now, similarly to the\ninput matrix X (Section 3.1), we deﬁne an embed-\nding matrix Xt ∈Rn×D where each rowicontains\nthe embedding vector of xi’s time point.3\nTo incorporate time in the attention mechanism,\nwe use an additional trainable weight matrix WT ∈\nRD×dk and create its corresponding representation\nmatrix T = XtWT . Note T ∈Rn×dk , i.e., its\ndimensions are the same as the key, query, and\nvalue matrices.\nTo calculate the attention scores, we multiply the\nquery matrix by the time matrix and then multiply\nby its transposed matrix, to keep the dimensions\nintact. We then divide by the time matrix’s norm, to\navoid getting too large values. Formally, we deﬁne\ntemporal attention by:\nTemporalAttention(Q,K,V,T ) =\nsoftmax\n\nQT⊺T\n∥T∥K⊺\n√dk\n\nV\n(2)\nIntuitively, by multiplying the query by the time,\nthe attention weights are now conditioned on the\ntime, i.e., they are time-dependent.\nTemporal attention can be used together with\nother, existing temporal language models, such as\n(Rosin et al., 2022; Dhingra et al., 2022). In these\ntwo models, a time-speciﬁc token is prepended to\neach sentence. In comparison to those methods, our\napproach does not require changing the input text,\nas it only modiﬁes the attention mechanism of the\nlanguage model. We further discuss and compare\nthe two methods in Section 3.3.\nThe temporal attention mechanism requires each\ninput text to be accompanied with a time point.\nThere are no constraints on these time points, i.e.,\nthe mechanism is agnostic to the time granularity\nand the number of time points in the model.\n3.3 Theoretical Analysis\nWe now theoretically analyze the temporal atten-\ntion mechanism more deeply and compare it to ex-\nisting time concatenation methods (Dhingra et al.,\n2022; Rosin et al., 2022). We omit the scaling\nfactor √dk for readability.\nWe denote the row vectors of the matrices Q,\nK, V, and T by qi, ki, vi, and ti, respectively.\n3Most tokens share the same time point, as noted above,\nexcept for special tokens such as padding and masking tokens,\nto which we associate unique time points.\nThe attention head computes attention weights α\nbetween all pairs of words as softmax-normalized\ndot products between the query and key vectors:\nαij = softmax\n(\nqik⊺\nj\n)\n(3)\nwhere i,j ∈{1,...,n }.\nThe output yi of the attention head is a weighted\nsum of the value vectors:\nyi =\nn∑\nj=1\nαijvj =\nn∑\nj=1\nsoftmax\n(\nqik⊺\nj\n)\nvj (4)\nBaseline models, such as Rosin et al. (2022) and\nDhingra et al. (2022), prepend the text sequence\nwith a time token at index 0, resulting in:\nyi =\nn∑\nj=0\nαijvj =\nn∑\nj=1\nαijvj + softmax\n(\nq0k⊺\n0\n)\nv0\n(5)\nAs we can see, by concatenating the time token, we\nadd query, key, and value vectors for that token, i.e.,\na time component is added to the weighted sum.\nIn contrast, by using temporal attention, the at-\ntention weights become:\nαij = softmax\n(\nqi\ntit⊺\nj\n∥T∥k⊺\nj\n)\n(6)\nThe i-th output vector yi is computed as:\nyi =\nn∑\nj=1\nαijvj =\nn∑\nj=1\nsoftmax\n(\nqi\ntit⊺\nj\n∥T∥k⊺\nj\n)\nvj\n(7)\nIntuitively, we multiply by the vectors of time to\nscale the attention weight αij by time. We observe\ntwo main differences between our proposed mech-\nanism and prior work:\n1. The time component is more tightly integrated\nin temporal attention: instead of just adding\na time component to the weighted sum, in\ntemporal attention the time component is mul-\ntiplied by every component in the sum.\n2. Temporal attention requires learning an addi-\ntional weight matrix WT ∈RD×dk . In prior\nwork, each input sequence is prepended with a\ntime token, i.e., its length nis increased by 1.\nAs a result, the temporal attention mechanism\nconsumes more memory (as it has additional\nD·dk trainable parameters4), whereas prior\n4When using the standard BERT-base architecture: D·\ndk = 768· 64 = 49,152\n1501\nwork requires more time to train (as its se-\nquences are longer). From our experiments,\nthe overhead of both methods is negligible\ncompared to the memory consumption and\ntraining time of BERT (see analysis in Sec-\ntion 6.3).\n4 Semantic Change Detection\nIn this section, we employ our proposed temporal\nattention mechanism (Section 3.2) for the task of\nsemantic change detection (Kutuzov et al., 2018;\nTahmasebi et al., 2018). The ability to detect and\nquantify semantic changes is important to lexicog-\nraphy, linguistics, and is a basic component in many\nNLP tasks. For example, search in temporal cor-\npora, historical sentiment analysis, and understand-\ning historical documents. The objective of this task\nis to rank a set of target words according to their\ndegree of semantic change between two time points\nt1 and t2. In this work, we follow the practice of\n(Martinc et al., 2020a; Rosin et al., 2022) to esti-\nmate the semantic change a word underwent and\nrank the target words based on these estimates.\nGiven a target wordw, we generate time-speciﬁc\nrepresentations of it and compare them to detect\nsemantic changes. Algorithm 1 formally describes\nthe method. We begin by samplingnsentences con-\ntaining wfrom each time point t ∈{t1,t2}(line\n3). For each sentence sent, we create a sequence\nembedding by running it through the temporal at-\ntention model (note the model receives as input\nboth sentand t) and extracting the model’s hidden\nlayers that correspond to w(lines 5–6). We then\nchoose the last hhidden layers and average them\nto get a single vector (line 7). This is the contextual\nword embedding of w, denoted by v. Following,\nthe resulting embeddings are aggregated at the to-\nken level and averaged (line 10), in order to create\na non-contextual time-speciﬁc representation for w\nfor each time t, denoted by xt. Finally, we estimate\nthe semantic change of w by measuring the co-\nsine distance (cos_dist) between two time-speciﬁc\nrepresentations of the same token (line 12).\n5 Experimental Setup\n5.1 Data\nTo train and evaluate our models, we use data from\nthe SemEval-2020 Task 1 on Unsupervised De-\ntection of Lexical Semantic Change (Schlechtweg\net al., 2020). We use corpora provided by this task\nAlgorithm 1:Semantic change estimation\nInput: w(target word)\nInput: t1 (ﬁrst time point)\nInput: t2 (last time point)\nInput: C(diachronic corpus)\nInput: n(# of sentences to sample)\nInput: h(# of last hidden layers to extract)\n1 for t∈{t1,t2}do\n2 Lt ←{}\n3 Sw ←nsentences sampled from\nC(t,w)\n4 for sent∈Sw do\n5 H ←TempAttModel(sent,t)\n6 Hw ←H[w]\n7 v←AvgHiddenLayers(Hw,h)\n8 Lt.insert(v)\n9 end\n10 xt ←avg(Lt)\n11 end\n12 score= cos_dist(xt1 ,xt2 )\n13 return score\nfor English, German, and Latin, covering a vari-\nety of genres, times, languages, and sizes. They\nare all long-term: the English and German corpora\nspan two centuries each, and the Latin corpus spans\nmore than 2000 years. The German corpus is much\nlarger than the other two (x7 – x10). Each corpus\nis genre-balanced, and split into two time points;\nsee Table 1 for their statistics.\nEach corpus is accompanied with labeled data\nfor semantic change evaluation. We use the data\nfrom Subtask 2 of this task, where the objective\nis to rank a set of target words according to their\ndegree of semantic change between t1 and t2. The\nprovided data is a set of target words that are ei-\nther words that changed their meaning(s) (lost or\ngained a sense) between the two time points, or\nstable words that did not change their meaning dur-\ning that time. The target words are balanced for\npart of speech (POS) and frequency. Each target\nword was assigned a graded label (between 0 and\n1) according to their degree of semantic change (0\nmeans no change, 1 means total change). For the\nEnglish dataset, we follow (Montariol et al., 2021)\nand remove POS tags from both the corpus and the\nevaluation set.\n5.2 Baseline Methods\nWe use the following baseline methods:\n1502\nCorpus C1 Source C1 Time C1 Tokens C2 Source C2 Time C2 Tokens Target Words\nSemEval-English CCOHA 1810–1860 6.5M CCOHA 1960–2010 6.7M 37\nSemEval-Latin LatinISE -200–0 1.7M LatinISE 0–2000 9.4M 40\nSemEval-German DTA 1800–1899 70.2M BZ, ND 1946–1990 72.4M 48\nTable 1: Corpora for semantic change detection. Each corpus is split into two time points, denoted by C1 and C2.\n1. Schlechtweg et al. (2019) train Skip-gram\nwith Negative Sampling (SGNS) on two time\npoints independently and align the result-\ning embeddings using Orthogonal Procrustes.\nThey compute the semantic change scores us-\ning cosine distance.\n2. Gonen et al. (2020) use SGNS embeddings as\nwell. They represent a word in a time point by\nits top nearest neighbors according to cosine\ndistance. Then, they measure semantic change\nas the size of intersection between the nearest\nneighbors lists in the two time points.\n3. Martinc et al. (2020a) were one of the ﬁrst\nto use BERT for semantic change detec-\ntion. They create time-speciﬁc embeddings\nof words by averaging token embeddings over\nsentences in each time point, and then com-\npare them by calculating cosine distance.\n4. Montariol et al. (2021) use BERT to create a\nset of contextual embeddings for each word.\nThey cluster these embeddings and then com-\npare the cluster distributions across time slices\nusing various distance measures. We use their\nbest-performing method for each dataset as re-\nported in the paper, which uses afﬁnity propa-\ngation for clustering word embeddings and ei-\nther Wasserstein or Jensen-Shannon distance\nas a distance measure between clusters.\n5. Rosin et al. (2022) create a time-aware BERT\nmodel by preprocessing input texts to concate-\nnate time-speciﬁc tokens to them, and then\nmasking these tokens while training. They\nintroduce two methods to measure semantic\nchange, namely temporal-cosine and time-diff.\nWe use their best-performing method as re-\nported in the paper, which is temporal-cosine.\n6. “Scaled attention”: We present several base-\nlines which are simpliﬁed versions of our tem-\nporal attention mechanism. Intuitively, our\nmechanism differentiates between different\ntime points by learning a scaling factor per\neach pair of time points (based on the mul-\ntiplication of learned time vectors; see Sec-\ntion 3.2). In these baselines, we use a constant\nscaling factor per time point and calculate at-\ntention weights using the following formula:\nαij = softmax\n(\nqisijk⊺\nj\n)\nwhere sij is the scaling factor. This scaling\nmethod can be seen as a combination of (Mar-\ntinc et al., 2020a) and our temporal attention\nmethod. We present three options for sij:\n(1) Linear scaling. We hypothesize that re-\ncent texts should be given more weight, and\ndeﬁne sij = index(ti), where index(ti) is\nthe index of the time point ti out of all time\npoints t1,...,t nt . (2) Exponential scaling:\nsimilarly to linear scaling, but using an ex-\nponent: sij = 2index(ti). (3) Proportional to\nthe number of documents: here we hypothe-\nsize that larger corpora should be given more\nweight, and deﬁne sij = doc_count(ti)∑nt\nk=1 doc_count(tk) ,\nwhere doc_count(ti) is the number of docu-\nments in ti.\n5.3 Our Method\nTo train our models, for each language we use a\npretrained BERT (Devlin et al., 2019) model (bert-\nbase-uncased, with 12 layers, 768 hidden dimen-\nsions, and 110M parameters) and post-pretrain it\non the temporal corpus using our proposed tem-\nporal attention, as described in Section 3.2. For\nsemantic change detection, we use the method de-\nscribed in Section 4. We use the Hugging Face’s\nTransformers library5 for our implementation.\nBefore training, we add any missing target words\nto the model’s vocabulary. Since a pretrained\nmodel’s vocabulary may not contain all the target\nwords in our evaluation dataset, this is necessary\nto avoid the tokenizer splitting any occurrences of\nthe target words into subwords (which we found\nout to reduce performance). The added words are\nrandomly initialized.\n5https://github.com/huggingface/\ntransformers\n1503\n5.4 Metrics\nWe measure semantic change detection perfor-\nmance by the correlation between the semantic\nshift index (i.e., the ground truth) and the model’s\nsemantic shift assessment for each word in the eval-\nuation set. We follow prior work (Rosin et al.,\n2022) and use both Pearson’s correlation coefﬁ-\ncient rand Spearman’s rank correlation coefﬁcient\nρ. The difference between them is that Spearman’s\nρconsiders only the ranking order, while Pearson’s\nrconsiders the actual predicted values. In our eval-\nuation, we make an effort to evaluate our meth-\nods and the baselines using both correlation coefﬁ-\ncients, to make the evaluation as comprehensive as\npossible. There were some cases where we could\nnot reproduce the original authors’ results; in such\ncases, we opted to report only the original result.\n5.5 Implementation Details\nDue to limited computational resources, we follow\nRosin et al. (2022) and train our models with a\nmaximum input sequence length of 128 tokens.\nWe perform all experiments on a single NVIDIA\nQuadro RTX 6000 GPU. We tune the following\nhyperparameters for each language: for training:\nlearning rate in {1e-8,1e-7,1e-6,1e-5,1e-4}and\nnumber of epochs in {1,2,3,4}. For inference:\nnumber of last hidden layers to use for embedding\nextraction h∈{1,2,4,12}.\nThe chosen pretrained model and hyperparame-\nters, along with the steps number and training time\nper language are as follows:\n• For English: bert-base-uncased, 6 with 1e-9\nlearning rate for 2 epochs (6.3K steps, took 70\nminutes); all (12) hidden layers for inference.\n• For Latin: latin-bert,7 with 1e-5 learning rate\nfor 1 epoch (3.5K steps, took 25 minutes); last\nhidden layer for inference.\n• For German: bert-base-german-cased,8 with\n1e-6 learning rate for 1 epoch (38.1K steps,\ntook 10 hours); last hidden layer for inference.\n6https://huggingface.co/\nbert-base-uncased\n7https://github.com/dbamman/latin-bert\n8https://huggingface.co/\nbert-base-german-cased\n6 Results\nIn this section, we outline the results of our em-\npirical evaluation. In all tables throughout the sec-\ntion, the best result in each column is boldfaced;\nperformance is measured using Pearson’s r and\nSpearman’sρcorrelation coefﬁcients.\n6.1 Main Result\nTable 2 shows the results for semantic change de-\ntection on the SemEval datasets. Our temporal at-\ntention model outperforms all the baselines for all\ndatasets and metrics with signiﬁcant correlations\n(p <0.0005) and large margins (7%–36%). We\nobserve moderate to strong correlations (around\n0.52–0.76) for all datasets. Even for the German\ndataset, on which recent BERT-based methods got\nrelatively lower results (and were outperformed by\nword2vec-based methods such as Schlechtweg et al.\n(2019)), our model achieves strong correlations and\nstate-of-the-art performance. In Section 6.2 and\nSection 6.3, we experiment with variations of our\nmethod and achieve even stronger performance on\nthe English dataset.\nFinally, looking at the three scaled attention base-\nlines, they all perform similarly and are positioned\nbetween Martinc et al. (2020a) and our temporal\nattention model, as expected.\n6.2 Temporal Attention with Temporal\nPrepend\nUntil now, we used temporal attention on\nBERT (Devlin et al., 2019) to create our model.\nIn this section, in addition to using temporal at-\ntention, we also prepend a time token to the input\nsequences, as done in Rosin et al. (2022). That is,\nwe experiment with applying temporal attention on\ntop of their model.\nTable 3 shows the results of this combined model\ncompared to each of its components. First, prepend-\ning time tokens is inferior to the other models.\nWhen comparing our proposed temporal attention\nand the combined model, we observe mixed results:\ntemporal attention alone works better for the Latin\nand German datasets, but for the English dataset the\ncombination of temporal attention and prepending\ntime tokens performs better.\n6.3 Impact of BERT Model Size on Temporal\nAttention\nOur model is based on the most commonly used\npretrained BERT model, called BERT-base, which\n1504\nMethod\nSemEval-Eng SemEval-Lat SemEval-Ger\nr ρ r ρ r ρ\nSchlechtweg et al. (2019) 0.512 0.321 0.458 0.372 – 0.712\nGonen et al. (2020) 0.504 0.277 0.417 0.273 – 0.627\nMartinc et al. (2020a) – 0.315 – 0.496 – 0.565\nMontariol et al. (2021) 0.566 0.456 – 0.488 0.618 0.583\nRosin et al. (2022) 0.538 0.467 0.485 0.512 0.592 0.582\nScaled Linear Attention 0.517 0.506 0.524 0.478 0.580 0.550\nScaled Exp. Attention 0.491 0.487 0.633 0.528 0.569 0.526\nScaled by Doc Attention 0.532 0.478 0.657 0.505 0.595 0.567\nTemporal Attention 0.620 0.520 0.661 0.565 0.767 0.763\nTable 2: Semantic change detection results on SemEval-English, SemEval-Latin, and SemEval-German, measured\nusing Pearson’srand Spearman’sρcorrelation coefﬁcients.\nMethod\nSE-Eng SE-Lat SE-Ger\nr ρ r ρ r ρ\nTemp. Prep. 0.538 0.467 0.485 0.512 0.592 0.582\nTemp. Att. 0.620 0.520 0.556 0.556 0.767 0.763\nBoth 0.655 0.548 0.541 0.508 0.645 0.682\nTable 3: Semantic change detection results on the En-\nglish, Latin, and German datasets, comparing time to-\nken prepending (Rosin et al., 2022) with our proposed\ntemporal attention, and a combination of both.\ncontains 12 transformer layers and a hidden di-\nmension size of 768. In this section, we train and\nevaluate models of different sizes, namely ‘small’\nand ‘tiny’, that are based on much smaller pre-\ntrained variants of BERT: BERT-small9 has 26%\nof the parameters of BERT-base, containing only\n4 transformer layers while its hidden dimension\nis 512; BERT-tiny10 has just 4% of the parame-\nters of BERT-base, with 2 transformer layers and\na hidden dimension of 128. We perform this eval-\nuation only for the SemEval-English dataset, as\nsmaller pretrained BERT models are currently pub-\nlicly available only for the English language.\nTable 4 shows the comparison results, where we\ncompare the three variants of our temporal attention\nmodel, along with the two variants of Rosin et al.\n(2022). We also denote the number of trainable\nparameters for each model (see the theoretical anal-\nysis in Section 3.3). We observe a clear negative\ncorrelation between model size and performance\n(measured by both Pearson’srand Spearman’sρ);\n9https://huggingface.co/prajjwal1/\nbert-small\n10https://huggingface.co/prajjwal1/\nbert-tiny\nMethod Params r ρ\nRosin et al. (2022) base 109.52M 0.538 0.467\nRosin et al. (2022) tiny 4.42M 0.534 0.427\nTemp. Att. base 116.61M 0.620 0.520\nTemp. Att. small 29.85M 0.660 0.584\nTemp. Att. tiny 4.45M 0.703 0.627\nTable 4: Results for semantic change detection for mod-\nels of different sizes on SemEval-English.\nthe smaller the model, the better the performance.\nWhile this ﬁnding may sound counterintuitive, it\nis in line with Rosin et al. (2022), who hypothe-\nsized that to understand time there is no need to use\nextremely large models, and reported higher-than-\nexpected performance for the tiny model. In their\nstudy, that model achieved a slightly lower perfor-\nmance compared to their standard (base) model, but\nstill outperformed most baselines. Overall, this is\nan encouraging ﬁnding; smaller models mean faster\ntraining and inference times, as well as smaller\nmemory footprints. This lowers the bar to enter the\nﬁeld.\n6.4 Qualitative Analysis\nFigure 3 shows the Spearman correlation between\nthe ground truth ranks and our model’s ranks for\nthe SemEval-English dataset. The correlation is\nmoderate (0.520), and we observe a similar num-\nber of false-positive words (top-left corner) and\nfalse negatives (bottom-right corner). Interestingly,\nwe can see that the model performs better on the\nmore changed words (right half, rank above 19, e.g.,\n“plane”, “tip”, and “head”), while there are more\nerrors on the static words (left half, e.g., “chair-\n1505\nchairman\nrisk\nfiction\nrelationship\ncontemplation\ntree\nsavage\nbag\nmultitude\nlane\nquilt\nface\nthump\nattack\ngas\ndonkey\npart\ncircle\nstroke\nword\npin\nlassland\nedge\nplayer\nrag\nounce\nhead\nbit\ntwist\nstab\nball\nrecord\ngraft\nprop\ntip\nplane\n0 5 10 15 20 25 30 35\n0\n5\n10\n15\n20\n25\n30\n35\nGround truth rank\nPredicted rank\nFigure 3: Semantic change detection on the SemEval-\nEnglish dataset: ground truth ranks vs. our model’s\nranks (Spearman’sρ= 0.520).\nman”, “risk”, and “quilt”). Most of the false posi-\ntives seem to be either slang words or concerning\nword usages that are less likely to appear in our\ncorpora which is mainly composed of newsletters\nand books (Section 5.1). For example, the verb\n“stab”, which traditionally means to push a knife\ninto someone, has a newer meaning of attempting\nto do something. The noun “word” can be used to\nexpress agreement.\n7 Conclusion\nIn this paper, we presented a time-aware self-\nattention mechanism as an extension of the original\nmechanism of the transformer. The proposed mech-\nanism considers the time the text sequences were\nwritten when computing attention scores, thus al-\nlowing creating time-speciﬁc contextualized word\nrepresentations. We conducted evaluations on the\ntask of semantic change detection and reached state-\nof-the-art performance on three diverse datasets in\nterms of time, language, size, and genre. In addi-\ntion, we experimented with small-sized pretrained\nmodels and found they outperform larger models\non this task. We conduct an experiment evaluat-\ning the marginal addition of time token prepending\nalong with temporal attention and conclude that\non all but the English dataset it hurts performance.\nWe wish to study how to best combine the two\napproaches in future work. Additionally, for fu-\nture work, we plan to extend this work by apply-\ning temporal attention to other tasks, such as web\nsearch and sentence time prediction, as well as ex-\nperimenting with more time points and different\ngranularities.\nReferences\nOshin Agarwal and Ani Nenkova. 2021. Temporal ef-\nfects on pre-trained models for language processing\ntasks. arXiv preprint arXiv:2111.12790.\nRobert Bamler and Stephan Mandt. 2017. Dynamic\nword embeddings. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 380–389. PMLR.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nMarco Del Tredici, Raquel Fernández, and Gemma\nBoleda. 2019. Short-term meaning shift: A distri-\nbutional exploration. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2069–2075, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. 2022. Time-aware language\nmodels as temporal knowledge bases. Transactions\nof the Association for Computational Linguistics,\n10:257–273.\nHaim Dubossarsky, Simon Hengchen, Nina Tahmasebi,\nand Dominik Schlechtweg. 2019. Time-out: Tem-\nporal referencing for robust modeling of lexical se-\nmantic change. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 457–470, Florence, Italy. Association\nfor Computational Linguistics.\nMario Giulianelli, Marco Del Tredici, and Raquel Fer-\nnández. 2020. Analysing lexical semantic change\nwith contextualised word representations. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3960–\n3973, Online. Association for Computational Lin-\nguistics.\n1506\nHila Gonen, Ganesh Jawahar, Djamé Seddah, and\nYoav Goldberg. 2020. Simple, interpretable and sta-\nble method for detecting words with usage change\nacross corpora. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 538–555, Online. Association for\nComputational Linguistics.\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016. Diachronic word embeddings reveal statisti-\ncal laws of semantic change. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1489–1501, Berlin, Germany. Association for Com-\nputational Linguistics.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2021. Dynamic contextualized word em-\nbeddings. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 6970–6984, Online. Association for Computa-\ntional Linguistics.\nSpurthi Amba Hombaiah, Tao Chen, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Dy-\nnamic language models for continuously evolving\ncontent. In KDD 2021.\nRenfen Hu, Shen Li, and Shichen Liang. 2019. Di-\nachronic sense modeling with deep contextualized\nword embeddings: An ecological view. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3899–3908,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nXiaolei Huang and Michael J. Paul. 2019. Neural tem-\nporality adaptation for document classiﬁcation: Di-\nachronic word embeddings and domain adaptation\nmodels. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4113–4123, Florence, Italy. Association\nfor Computational Linguistics.\nAdam Jatowt and Kevin Duh. 2014. A framework for\nanalyzing semantic change of words across time. In\nIEEE/ACM Joint Conference on Digital Libraries,\npages 229–238. IEEE.\nNattiya Kanhabua and Avishek Anand. 2016. Tempo-\nral information retrieval. In Proceedings of the 39th\nInternational ACM SIGIR conference on Research\nand Development in Information Retrieval, SIGIR\n2016, Pisa, Italy, July 17-21, 2016, pages 1235–\n1238. ACM.\nYoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde,\nand Slav Petrov. 2014. Temporal analysis of lan-\nguage through neural language models. In Proceed-\nings of the ACL 2014 Workshop on Language Tech-\nnologies and Computational Social Science, pages\n61–65, Baltimore, MD, USA. Association for Com-\nputational Linguistics.\nVivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and\nSteven Skiena. 2015. Statistically signiﬁcant detec-\ntion of linguistic change. In Proceedings of the 24th\nInternational Conference on World Wide Web, WWW\n2015, Florence, Italy, May 18-22, 2015, pages 625–\n635. ACM.\nAndrey Kutuzov, Lilja Øvrelid, Terrence Szymanski,\nand Erik Velldal. 2018. Diachronic word embed-\ndings and semantic shifts: a survey. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics, pages 1384–1397, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.\nSeverin Laicher, Sinan Kurtyigit, Dominik\nSchlechtweg, Jonas Kuhn, and Sabine Schulte im\nWalde. 2021. Explaining and improving BERT\nperformance on lexical semantic change detection.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Student Research Workshop , pages\n192–202, Online. Association for Computational\nLinguistics.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomas Kocisky, Sebastian Ruder, et al. 2021. Mind\nthe gap: Assessing temporal generalization in neural\nlanguage models. Advances in Neural Information\nProcessing Systems, 34.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDaniel Loureiro, Francesco Barbieri, Leonardo Neves,\nLuis Espinosa Anke, and Jose Camacho-Collados.\n2022. Timelms: Diachronic language models from\ntwitter. arXiv preprint arXiv:2202.03829.\nMatej Martinc, Petra Kralj Novak, and Senja Pollak.\n2020a. Leveraging contextual embeddings for de-\ntecting diachronic semantic shift. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 4811–4819, Marseille, France. Euro-\npean Language Resources Association.\nMatej Martinc, Syrielle Montariol, Elaine Zosa, and\nLidia Pivovarova. 2020b. Capturing evolution in\nword usage: Just add more clusters? In Compan-\nion Proceedings of the Web Conference 2020, pages\n343–349.\nSyrielle Montariol, Matej Martinc, and Lidia Pivo-\nvarova. 2021. Scalable and interpretable semantic\nchange detection. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4642–4652, Online. As-\nsociation for Computational Linguistics.\n1507\nAlex Rosenfeld and Katrin Erk. 2018. Deep neural\nmodels of semantic shift. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 474–484, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nGuy D. Rosin, Eytan Adar, and Kira Radinsky. 2017.\nLearning word relatedness over time. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 1168–1178,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nGuy D. Rosin, Ido Guy, and Kira Radinsky. 2022.\nTime masking for temporal language models. In\nProceedings of the Fifteenth ACM International\nConference on Web Search and Data Mining, pages\n833–841.\nPaul Röttger and Janet Pierrehumbert. 2021. Tempo-\nral adaptation of BERT and performance on down-\nstream document classiﬁcation: Insights from social\nmedia. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2400–2412,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMaja R. Rudolph and David M. Blei. 2018. Dynamic\nembeddings for language evolution. In Proceedings\nof the 2018 World Wide Web Conference on World\nWide Web, WWW 2018, Lyon, France, April 23-27,\n2018, pages 1003–1011. ACM.\nPavel Savov, Adam Jatowt, and Radoslaw Nielek. 2021.\nPredicting the age of scientiﬁc papers. In Interna-\ntional Conference on Computational Science, pages\n728–735. Springer.\nDominik Schlechtweg, Anna Hätty, Marco Del Tredici,\nand Sabine Schulte im Walde. 2019. A wind of\nchange: Detecting and evaluating lexical seman-\ntic change across times and domains. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 732–746, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nDominik Schlechtweg, Barbara McGillivray, Simon\nHengchen, Haim Dubossarsky, and Nina Tahmasebi.\n2020. SemEval-2020 task 1: Unsupervised lexical\nsemantic change detection. In Proceedings of the\nFourteenth Workshop on Semantic Evaluation, pages\n1–23, Barcelona (online). International Committee\nfor Computational Linguistics.\nNina Tahmasebi, Lars Borin, and Adam Jatowt. 2018.\nSurvey of computational approaches to lexical se-\nmantic change. arXiv preprint arXiv:1811.06278.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nZijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, and\nHui Xiong. 2018. Dynamic word embeddings for\nevolving semantic discovery. In Proceedings of\nthe Eleventh ACM International Conference on Web\nSearch and Data Mining, WSDM 2018, Marina Del\nRey, CA, USA, February 5-9, 2018, pages 673–681.\nACM.\n1508",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8462149500846863
    },
    {
      "name": "Transformer",
      "score": 0.7239809632301331
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6524085998535156
    },
    {
      "name": "Language model",
      "score": 0.6367413997650146
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6249319314956665
    },
    {
      "name": "Natural language processing",
      "score": 0.5302947163581848
    },
    {
      "name": "German",
      "score": 0.4628336727619171
    },
    {
      "name": "Architecture",
      "score": 0.45533743500709534
    },
    {
      "name": "Linguistics",
      "score": 0.10934525728225708
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ]
}