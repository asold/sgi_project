{
  "title": "A Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images for UAV Geo-Localization",
  "url": "https://openalex.org/W4226255358",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3207734340",
      "name": "Jiedong Zhuang",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A3206046082",
      "name": "Xuruoyan Chen",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A1559636448",
      "name": "Ming Dai",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A2324383128",
      "name": "Wenbo Lan",
      "affiliations": [
        "China Aerodynamics Research and Development Center"
      ]
    },
    {
      "id": "https://openalex.org/A4226460353",
      "name": "Yongheng Cai",
      "affiliations": [
        "China Aerodynamics Research and Development Center"
      ]
    },
    {
      "id": "https://openalex.org/A2307981714",
      "name": "Enhui Zheng",
      "affiliations": [
        "China Jiliang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3004492228",
    "https://openalex.org/W2992240579",
    "https://openalex.org/W3032837604",
    "https://openalex.org/W3106808132",
    "https://openalex.org/W3111241683",
    "https://openalex.org/W2963898168",
    "https://openalex.org/W3009942016",
    "https://openalex.org/W3083065220",
    "https://openalex.org/W3088162569",
    "https://openalex.org/W3022140654",
    "https://openalex.org/W3095867871",
    "https://openalex.org/W3094032464",
    "https://openalex.org/W1910817791",
    "https://openalex.org/W2081418428",
    "https://openalex.org/W1981154748",
    "https://openalex.org/W2061153075",
    "https://openalex.org/W2572697301",
    "https://openalex.org/W2963474852",
    "https://openalex.org/W2799087793",
    "https://openalex.org/W2951019013",
    "https://openalex.org/W2984478347",
    "https://openalex.org/W2479919622",
    "https://openalex.org/W6766729115",
    "https://openalex.org/W3035158519",
    "https://openalex.org/W3092933908",
    "https://openalex.org/W2997604048",
    "https://openalex.org/W3189451737",
    "https://openalex.org/W2883311563",
    "https://openalex.org/W3098711604",
    "https://openalex.org/W3115854999",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2963637710",
    "https://openalex.org/W2891175865",
    "https://openalex.org/W2963383990",
    "https://openalex.org/W3081227581",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W6784923365",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W6795892075",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W2096306138",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963588253",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3170874841"
  ],
  "abstract": "It is a challenging task for unmanned aerial vehicles (UAVs) without a positioning system to locate targets by using images. Matching drone and satellite images is one of the key steps in this task. Due to the large angle and scale gap between drone and satellite views, it is very important to extract fine-grained features with strong characterization ability. Most of the published methods are based on the CNN structure, but a lot of information will be lost when using such methods. This is caused by the limitations of the convolution operation (e.g. limited receptive field and downsampling operation). To make up for this shortcoming, a transformer-based network is proposed to extract more contextual information. The network promotes feature alignment through semantic guidance module (SGM). SGM aligns the same semantic parts in the two images by classifying each pixel in the images based on the attention of pixels. In addition, this method can be easily combined with existing methods. The proposed method has been implemented with the newest UAV-based geo-localization dataset. Compared with the existing state-of-the-art (SOTA) method, the proposed method achieves almost 8&#x0025; improvement in accuracy.",
  "full_text": "Received March 5, 2022, accepted March 18, 2022, date of publication March 28, 2022, date of current version April 1, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3162693\nA Semantic Guidance and Transformer-Based\nMatching Method for UAVs and Satellite Images\nfor UAV Geo-Localization\nJIEDONG ZHUANG\n1, XURUOYAN CHEN1, MING DAI1, WENBO LAN2,\nYONGHENG CAI2, AND ENHUI ZHENG\n 1\n1Unmanned System Application Technology Research Institute, China Jiliang University, Hangzhou 310018, China\n2China Academy of Aerospace Aerodynamics (CAAA), Beijing 100074, China\nCorresponding author: Enhui Zheng (ehzheng@cjlu.edu.cn)\nABSTRACT It is a challenging task for unmanned aerial vehicles (UA Vs) without a positioning system\nto locate targets by using images. Matching drone and satellite images is one of the key steps in this task.\nDue to the large angle and scale gap between drone and satellite views, it is very important to extract ﬁne-\ngrained features with strong characterization ability. Most of the published methods are based on the CNN\nstructure, but a lot of information will be lost when using such methods. This is caused by the limitations\nof the convolution operation (e.g. limited receptive ﬁeld and downsampling operation). To make up for this\nshortcoming, a transformer-based network is proposed to extract more contextual information. The network\npromotes feature alignment through semantic guidance module (SGM). SGM aligns the same semantic parts\nin the two images by classifying each pixel in the images based on the attention of pixels. In addition, this\nmethod can be easily combined with existing methods. The proposed method has been implemented with\nthe newest UA V-based geo-localization dataset. Compared with the existing state-of-the-art (SOTA) method,\nthe proposed method achieves almost 8% improvement in accuracy.\nINDEX TERMS Cross-view image matching, geo-localization, UA V image localization, deep neural\nnetwork.\nI. INTRODUCTION\nThe researches on remote sensing images have been a hot\ntopic for a long time. There is a part of research devoted\nto detecting targets from remote sensing images [1]–[4].\nSome other works were dedicated to semantic segmentation\nof remote sensing images [5]–[8]. Another line of works\nfocused on the large scene images classiﬁcation [9]–[12].\nIn recent years, the booming development of Unmanned\naerial vehicles (UA Vs) has promoted the application of drones\nin all walks of life. UA V is easy to operate and shoot, which\nmakes it popular and gradually become the main tool for\nacquiring remote-sensing images. So far, most drones on the\nmarket rely on positioning systems (e.g. GPS or GNSS) for\npositioning and navigation. Cross-view geo-localization is\nmatching the drone images with the satellite images marked\nwith geographic location, which makes drone to obtain the\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Maurizio Magarini\n.\ncurrent position and realize autonomous positioning without\nthe assistance of the positioning system.\nCross-view geo-localization is mainly to achieve image\npositioning by matching images from different perspectives,\nwhich is challenging because the appearance and view-points\nare signiﬁcantly different in various views. Some previous\nwork used some hand-designed features such as semanti-\ncally labeled regions and feature translation for similarity\ncalculations [13]–[16]. With the rapid development of deep\nlearning and CNN, the method of manual features has been\nreplaced by neural networks autonomously extracting fea-\ntures. One line of works focused on matching ground and\nsatellite images, and implemented the method on the two\ndatasets CVUSA [17] and CV ACT [18]. In these two datasets,\nthere is an image pair, containing a panoramic ground image\nand a satellite image for a location. Hu et al.[19] proposed a\nSiamese architecture to do metric learning for the matching\ntask, which used NetVLAD [20] to encode local feature\ninto global image descriptors. Liu and Li [18] designed a\nSiamese network which explicitly encodes the orientation\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 34277\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nof each pixel in the images, signiﬁcantly boosting the dis-\ncriminative power of the learned deep features. Furthermore,\nLiu et al. [21] proposed a new Stochastic Attraction and\nRepulsion Embedding (SARE) loss function to minimize\nthe gap between the learned and the actual probability dis-\ntributions. Vo and Hays [22] proposed a new loss func-\ntion which signiﬁcantly improves the accuracy of Siamese\nand Triplet embedding networks and proved the effective-\nness of orientation supervision. Shi et al.[23] applied polar\ntransform to warp aerial images to align aerial and ground\nviews. They also designed a DSM method [24] by adopting\na dynamic similarity-matching network to estimate cross-\nview orientation alignment during localization. Another line\nof works focused on matching drone and satellite images,\nand implemented the method on the UA V-based datasets\nUniversity-1652 [25]. Zheng et al. [25] looked at image-\nretrieval tasks from a classiﬁcation perspective and optimized\nmodel by cross-entropy loss and instance loss [26]–[29].\nDing et al.[30] proposed a data augmentation method to\nsolve the problem of unbalanced drone and satellite image\nsamples in the dataset. In-spired by success of partition strate-\ngies [31]–[34] in other ﬁelds, Wang et al. [35] proposed\na rotation-invariant square-ring feature partition strategy to\nenable the network to fully mining contextual information.\nSince transformer was proposed by Vaswani et al.[36] in\nthe ﬁeld of NLP, it has maintained a high degree of pop-\nularity in deep learning. In recent years, excellent research\nbased on transformer in the Computer Vision (CV) ﬁeld\nhas emerged one after another. With its unique self-attention\nmechanism and high performance, it may even replace CNN’s\nlong-standing dominance in CV. At present, transformer\nhas penetrated into many subﬁelds of CV, such as Image\nClassiﬁcation, Object Detection, Semantic Segmentation and\nGAN, etc. [37]–[46].\nIn the ﬁeld of Image Classiﬁcation, Dosovitskiy et al.[37]\nproposed a pure transformer network, called Vision Trans-\nformer (ViT), which sequences image patches and performs\nvery well on image classiﬁcation tasks. Based on ViT,\nTouvron et al. [38] introduced a teacher-student strategy\nspeciﬁc to transformers, called Data-efﬁcient image Trans-\nformers (DeiT). DeiT used a distillation token to ensure the\nstudent learns from the teacher through attention, thereby\nspeeding up the speed of network training and reducing the\ndependence on the amount of data. In the ﬁeld of Object\nDetection, Carion et al.[39] removed the need for many hand-\ndesigned components and designed a transformer encoder-\ndecoder architecture named Detection Transformer (DETR),\nwhich reasons about the relations of the objects and the global\nimage context. However, DETR has some shortcomings such\nas slow convergence speed and limited feature spatial res-\nolution. To solve these problems, Zhu et al. [40] proposed\nDeformable DETR, whose attention modules only at-tend\nto a small set of key sampling points around a reference.\nInspired by transformer, Chi et al.[41] presented an attention-\nbased decoder module to bridge various representations into\na typical object detector. In the other ﬁelds of CV, there\nare also many excellent jobs. Zheng et al. [42] deployed a\npure transformer called Segmentation Transformer (SETR) to\nencode an image as a sequence of patches and modeled global\ncontext in every layer of the transformer. Chen et al. [43]\ndeveloped a new pre-trained model, named image processing\ntransformer (IPT). IPT could be efﬁciently employed on low-\nlevel computer vision task (e.g. denoising, super-resolution\nand deraining). Jiang et al.[44] used two pure transformers\nto build a Generative Adversarial Network (GAN). To restore\nthe texture information of the image super-resolution result,\nYang et al.[45] proposed a novel Texture Transformer Net-\nwork for Image Super-Resolution (TTSR) consisting of a\nlearnable texture extractor by DNN, a relevance embedding\nmodule, a hard-attention module for texture transfer, and a\nsoft-attention module for texture synthesis. He et al. [46]\nproposed a ViT-based pure transformer structure for pedes-\ntrian re-identiﬁcation with embedding side information and\njigsaw patch modules which improve discrimination ability\nof feature.\nHowever, there is few transformer-based method that can\nbe used for cross-view matching at present and the existing\nmethods for extracting contextual information from drone\nand satellite images are only at the block level instead of\nthe pixel level, which are not robust enough to offset and\nscale. To ﬁll the above gaps, we mainly made the following\ncontributions:\n1. Different from other existing CNN-based methods, a\nSwin-transformer-based structure is proposed to match UA V\nand satellite images (see Sections II.B).\n2. A semantic guidance module was proposed and used\nto realize the feature alignment of contextual information\nmining and inference stage improving the accuracy of the\nmodel under offset and scale (see Sections II.C).\n3. The method achieved outstanding performance. On var-\nious accuracy indicators of the benchmark dataset, the meth-\nod greatly exceeded the existing methods (see Section III).\nThe rest of this paper is organized as follows. In Section II,\nthe methodology and materials are brieﬂy introduced.\nSection III presents the experiments and results of our\nmethod. Discussion and conclusion are illustrated in\nSection IV and Section V respectively.\nII. METHODOLOGY AND MATERIALS\nA. DATASETS AND EVALUATION INDICATORS\nThe research was implemented with University-1652,\nreleased by Zheng et al. [25]. There are 1652 geographic\ntargets in 72 universities from all over the world. The dataset\nof each target consists of images from three different perspec-\ntives, including satellite, drone, and street views. Each target\nhas only one satellite-view image, about ﬁfty drone-view\nimages from different ﬁlming angles and heights, and some\nstreet-view images. This research focuses on the matching\nof satellite and drone views. The performance of method\nis mainly reﬂected in two tasks, Drone →Satellite and\nSatellite →Drone. Speciﬁcally, the purpose of Drone →\nSatellite is giving a drone image and ﬁnding the satellite\n34278 VOLUME 10, 2022\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nimage of the same place; the purpose of Satellite →Drone\nis giving a satellite image and ﬁnding all drone images of\nthe corresponding place. The details of data distribution in the\ndatasets are shown in Table 1. In the testing dataset of the\nDrone →Satellite task, there was only one true-matched\nsatellite-view image for each drone-view image.\nAll geotags satellite-view images were captured from\nGoogle Maps, which have a similar scale to that of drone-\nview images and high spatial resolutions (from level 18 to\n20, the spatial resolution ranges from 1.07 to 0.27 m).\nDue to airspace control and high cost, it is very difﬁcult\nto collect a large number of real drone-view images, so the\ndrone-view images were simulated by the 3D model provided\nby Google Earth. The view in the 3D model spirally descends,\nand the height of view from 256 to 121.5 m, while images\nwere recorded at regular intervals, so as to obtain a large\nnumber of drone images close to the real world. As shown\nin Fig. 1, the blue curve represents the ﬂight trajectory of the\ndrone, and the blue cylinder represents the shooting target.\nTABLE 1. Statistics experimental data.\nFIGURE 1. Data collection diagram.\nTo evaluate the performance of the proposed method,\nRecall@K (R@K) [47] and average precision (AP) [48]\nare selected as evaluation indicators which are two widely\nused measurements. Recall@K (R@K) represents the prob-\nability that a correct match appears in the top-k ranked\nretrieved results. And another metric, average precision (AP)\nmeasures the average retrieval performance with multiple\nground truths, it is originally widely used in image retrieval.\nB. OVERVIEW OF NETWORK\nDifference from the other existing three branches method\n[25], [30], [35] is that the proposed network is com-\nposed of drone and satellite branches without street branch.\nIn addition, the backbone is not traditional CNN structure\n(e.g. Resnet [49], VGG [50]), but is a new transformer struc-\nture Swin-Tiny [51], which has achieved good performance\nin many other computer vision ﬁelds. Swin-Tiny consists of\n4 layers. Each layer contains 2, 2, 6 and 2 self-attention mod-\nules respectively. The structure of the self-attention module\nis shown in Fig. 2. In order to compare with other meth-\nods, schematically taking an image with the same size of\n256 ×256 as that of the input of the network, the overview\nof network and the forward propagation process are shown in\nFig. 2. The whole structure was divided into two branches, the\ndrone and satellite view branches, and they share the weights\nof the backbone. The image is divided into 4 patches and\nsent to back-bone. A feature map with a size of 64 ×768 is\nobtained through Layer1 to Layer4. After being processed\nby Semantic Guidance Module (details in Section II.C), the\nfeature map is split into several parts. Each part represents\ndifferent semantics. Average pooling operation are then per-\nformed on each part, since the size of part become 1 ×768.\nAll pooled features are sent to the classiﬁer module, includ-\ning fully connected, batch normalization, dropout, and clas-\nsiﬁcation layers. The network is optimized by minimizing\ncross-entropy loss in training phase. The classiﬁcation layer\nin classiﬁer module will be removed in inference (details\nin Section II.D).\nC. SEMANTIC GUIDANCE MODULE (SGM)\nContextual information is critical to the accuracy of image\nretrieval. The existing hard partition strategy is not robust\nenough to offset and scale. Since a pixel-based partition strat-\negy is proposed, which shows good performance in ablation\nexperiments on offset and scale (details in Section III.C).\nThe input feature map of SGM can be expressed as Mj\ni , and\nthe size of M is 64 ×768. SGM sums Mj\ni along the channel\ndirection. The operation can be formulated as:\nMi =\n∑768\nj=0 Mj\ni i ∈[0, 63] (1)\nAfter the above operations, the size of Mi is 64 ×1.\nNormalization operation is performed on Mi, and the result\nis shown in Fig. 3a. The normalization operation can be\nformulated as:\nMi = Mi −Minimum(Mi)\nMaximum (Mi) −Minimum(Mi) (2)\nwhere Maximum and Minimum respectively stand for getting\nthe maximum and minimum in Mi.\nVOLUME 10, 2022 34279\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nFIGURE 2. Overview of the network and the structure of the self-attention module. In the self-attention module: ‘‘LN’’ means layer normalization;\n‘‘W-MSA’’ and ‘‘SW-MSA’’ mean multi-head self-attention modules with regular and shifted windowing configurations, respectively. ‘‘MLP’’ means\nmultilayer perceptron.\nNext SGM calculates the gradient between adjacent posi-\ntions, and divides the feature map into different regions based\non the calculation results. Take dividing the feature map into\ntwo parts as an example, the red arrow in Fig. 3a indicates\na position with a large gradient. SGM regards this position\nas the line to divide the feature map into yellow and green\nparts (shown in Fig. 3b), which represent two categories in\nthe image. The whole operation process can be formulated as:\niposition =argmax\n(Mi+1 −Mi\nMi\n)\n(3)\nThe overview and the result of SGM is shown in Fig. 4.\nAfter SGM, the feature map is obviously divided into\ntwo parts: architecture (foreground) and environment (back-\nground). This result is a good foundation for extracting con-\ntextual information. And we believe this partition method is\nrobust to offset and scale.\nFIGURE 3. Values in the heatmap and the principle of SGM.\nD. LEARNING STRATEGY AND LOSS FUNCTION\nAverage pooling is performed on each divided feature map,\nand their size is reduced to 1 ×768. The operation can be\nformulated as:\nYi =Avgpool (Xi) i ∈[0, 1] (4)\nwhere Xi stands for divided feature map, Avgpool stands\naverage pooling operation and Yi stands pooled vector.\nFIGURE 4. Overview of SGM.\nAll pooled vector is sent to classiﬁer module, which\nincludes fully connected, batch normalization, dropout, and\nclassiﬁcation layers. Each vector output by module is per-\nformed softmax to normalize the result to a feature space with\nthe value range of 0 to 1. The optimization goal is that, in this\nfeature space, the feature vectors of the same geographic\ntarget have a closer distance, on the contrary, the feature\nvectors of different geographic targets have a longer distance.\nThe network is optimized by cross-entropy loss function and\nit can be formulated as:\nLossCE (p, y) =\n{\n−log (p), y =1\n−log (1 −p), y =0 (5)\nwhere p stands for forecast result, and y stands ground-truth\nlabel.\nThe total loss in the training phase can be formulated as:\nLoss =\n∑n\ni=0 (Li\nDrone +Li\nSatellite) (6)\nwhere n stands for number of divided feature maps.\n34280 VOLUME 10, 2022\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nTABLE 2. Comparison with the state-of-the-art results reported on university-1652.\nIn the testing phase, the classiﬁcation layer in classiﬁer\nmodule will be removed. Since the output size of module\nbecomes 1 ×512. As shown in Fig. 5, two vectors are\nconcatenated to calculate the Euclidean distance of the image\nin the feature space. The distance between two vectors can be\nformulated as:\nD =∥VDrone−VSatellite∥2 (7)\nwhere V stands for concatenated vector.\nFIGURE 5. Training and testing stage.\nE. IMPLEMENTATION\nTraining images were changed from 512 × 512 to\n256 ×256 through the resize operation and augmented\nwith random ﬂipping and random cropping. The batchsize\nwas set to 16. The backbone was initialized by loading the\nweights pretrained on ImageNet-1K by Swin-Tiny. SGD was\nchosen as the optimizer with momentum of 0.9 and weight\ndecay of 5e-4 to train the model. The model was trained for\n140 epochs; the initial learning rate was 9e−4 for the back-\nbone layers, and 9e−3 for the other layers. The learning rate\ndropped to one-tenth of the original after 80 and 120 epochs.\nIn the inference phase, the similarity between images was\nevaluated by calculating the Euclidean distance between\nL2-normalizing feature vectors of images. All experiments\nwere performed with an Nvidia 3090 GPU using the PyTorch\ndeep-learning framework with FP16 training.\nIII. EXPERIMENT\nA. COMPARISON WITH THE STATE OF THE ART\nIn Table 2, the proposed method is compared with other meth-\nods on University-1652. The method has achieved 82.14 %\nR@1 accuracy and 84.72% AP on the task of Drone →\nSatellite, 88.16% R@1 accuracy and 81.81% AP on the task\nof Satellite →Drone with the standard input (image size of\n256 ×256). The performance of the method greatly surpasses\nthe existing competitive models. When choosing the model\nSwin-Large with a larger amount of parameters and calcula-\ntions as backbone, which is pretrained on a larger datasets\nImagenet-22K, the accuracy achieved higher. Experiments\nwith different input sizes shows that input images with large\nresolution can obtain better matching accuracy in these two\ntasks.\nB. ABLATION OF SGM\nIn order to verify the effectiveness of SGM, the distribution\nof embedding vectors after dimensionality reduction is visu-\nalized. As shown in Fig. 6, the SGM splits the vectors into\nVOLUME 10, 2022 34281\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\ntwo parts as expected. To explore the impact of the number of\nparts divided by SGM, an ablation experiments are performed\non SGM. As shown in Table 3 and Fig. 7, the method only\nachieved 79.19 % R@1 accuracy and 82.19% AP on the task\nof Drone →Satellite, 86.31% R@1 accuracy and 77.69%\nAP on the task of Satellite →Drone without SGM. When\nSGM was adopted and divided feature map into 2 parts, the\nmethod achieved 80.14% R@1 accuracy and 83.35% AP on\nthe task of Drone →Satellite, 87.59% R@1 accuracy and\n80.37% AP on the task of Satellite →Drone. When the\nnumber of parts increased to 3, model achieved the highest\nR@1 and AP. As the number of parts reached 4, the accuracy\nof the model dropped to 81.76 R@1 accuracy and 84.46%\nAP on the task of Drone →Satellite, 87.45% R@1 accuracy\nand 81.29% AP on the task of Satellite→ Drone. From the\nabove phenomenon, we can judge that 3 is the best number\nof parts. Meanwhile, SGM brings 2.95%, 2.53%, 1.85% and\n4.12% improvements on the 4 items in the table. Among\nthem, the greatest improvement is the AP of the task of\nSatellite →Drone, which proves that SGM is an effective\nmethod. It makes satellite image retrieve more relevant drone\nimages. Furthermore, in order to better integrate each channel\nof features, SE module [53] was inserted after each feature\nmap output from SGM, which brought an objective improve-\nment to the performance of the model.\nTABLE 3. Results of ablation experiments of semantic guidance module.\n‘‘2P-4P’’ means number of parts.\nFIGURE 6. Examples of an embedding vector being divided into two parts\nby SGM.\nC. ABLATION OF OFFSET AND SCALE\nIn order to conﬁrm whether the model is robust to offset\nand scale or not a set of ablation experiments were designed.\nFirstly, the anti-offset of images was tested on the model. The\nquery image was added by mirrored pixels at the edge of\nFIGURE 7. (a) The effect of the number of categories on R@1. (b) The\neffect of the number of categories on AP.\nthe image to achieve the effect of shifting the center target\nbuilding. The operation process is shown in Fig. 8. The\ngeographic target was shifted from 0 to 20 pixels from the\ncenter.\nThe experiment result is displayed in Table 4. The results\nshowed that when the offset increased from 0 to 10, the model\ndropped to 81.20% R@1 accuracy and 84.07% AP on the task\nof Drone →Satellite, 87.87% R@1 accuracy and 80.51% AP\non the task of Satellite →Drone. The model dropped less than\n1% in various indicators. Even when the offset increased to\n20, the model remained 79.26% R@1 accuracy and 82.22%\nAP on the task of Drone →Satellite, 85.88% R@1 accuracy\nand 79.23% AP on the task of Satellite →Drone, the decline\nof accuracy was less than 3%, which proves that the model\nwas robust to the offset. Secondly, the robustness to scale\nof the model was tested on the task of Drone →Satellite.\nThe drone-query images were split into three groups: short,\nmedium and long, which respectively represented the dif-\nferent distances of the drone between the geographic target.\nAs shown in Table 5, the model performs slightly worse\non long distance, only achieved 79.92% R@1 accuracy and\n83.05% AP. But on short and middle distance, the accuracy\nof the model exceeded the average level. We believe that it\nmay be caused by closer scales between the middle distance\nimages and satellite images. Nevertheless, there is no signif-\nicant difference in the accuracy of the model on the three\ndifferent distances, which indicates that the proposed model\nis robust to scale. The above two experiments show that the\nproposed method may adapt to complex situations in actual\napplication scenarios.\nD. INFERENCE WITH DIFFERENT PART\nEach feature vector representing each part from SGM was\nuse separately in inference phase to explore the effectiveness\nof contextual information extraction. Taking 3 parts in SGM\nas an example, the results of the ablation experiment are\nshown in Table 6. When each part was tested individually,\nthey showed strong performance individually. Part3 (P3, the\npart with highest value in the feature map) even reached R@1\n34282 VOLUME 10, 2022\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nFIGURE 8. The way of shifting drone images.\nTABLE 4. Results of ablation experiments of shifting query images during\ninference.\nTABLE 5. Results of ablation experiments of using drone images with\ndifferent distance to the geo-graphic target to conduct retrieval. ‘‘All’’\nstands for using all drone-views query images.\n80.35%, AP 83.31% on the Drone →Satellite task, and R@1\n87.96%, AP 80.44% on the Satellite →Drone task. The\nresults of the separation experiment prove that each part of the\nnetwork had extracted effective context information. When\nall parts were combined, the model achieved the highest\naccuracy, R@1 82.14%, AP 84.72% on the Drone →Satellite\ntask, and R@1 88.16%, AP 81.81% on the Satellite →Drone\ntask. Compared with the individual inference of each part,\nthe joint inference brought about 1% improvement in each\nindicator. The results of the joint inference experiment shows\nthat the method of combining contextual information can\nimprove the retrieval accuracy of the model. However, too\nmuch parts may cause an increase in inference time. P3 is a\ngood choice in a scene with high real-time requirements.\nE. MATCHING ACCURACY OF MULTIPLE QUERIES\nIn the above experiment of Drone →Satellite task, only\na single drone view image was used to retrieve satellite\nview image, which was called ‘‘single mode’’. It is believed\nthat a single drone view image can not provide complete\ninformation about geographic targets. To solve this problem,\nUniversity-1652 provides a lot of drone images with different\nheights and angles for each geographic target, which provides\nconvenience for us to retrieve satellite images based on the\nTABLE 6. Results of ablation experiments of using drone images with\ndifferent distance to the geo-graphic target to conduct retrieval.\nTABLE 7. Results of multiple queries.\ninformation of multiple drone images (called ‘‘multi mode’’).\nIn order to verify that multiple queries can improve retrieval\naccuracy, two sets of ablation experiments were designed.\nIn the experiments the feature of multiple queries was set\nas the mean value of the single image features of a geographic\ntarget. Table 7 shows the accuracy of our proposed method in\nthe two modes and comparison with other existing methods.\nWhen the ‘‘multi mode’’ was used on proposed method for\nretrieval, the model achieved 89.23% R@1 accuracy and\n90.95% AP, which brought +7.09% R@1 and +6.13% AP to\nthe model. Compared with other methods that use the ‘‘multi\nmode’’, the proposed method is also far ahead.\nTo further explore the impact of multiple angles and multi-\nple heights on accuracy, an ablation experiment on angle and\nheight were designed and the results were shown in Table 8.\n‘‘Low’’, ‘‘Middle’’ and ‘‘High’’ are respectively represent\ndrone images in different height ranges (shows in Fig. 9).\nWhen the height was kept in a certain range, using multi-\nangle drone images combination for retrieval will improve\naccuracy. The method that is called as ‘‘ALL’’ means to\ncombine images of multiple heights on the basis of multi-\nple angles. Compared with using a single height only, the\naccuracy of multiple heights had also been improved. Since\nwe believe that multi-angle and multi-height queries both can\nimprove the model accuracy.\nF. EVALUATION ON REAL DATA\nTo verify that the method generalizes well, the model was\nevaluated on the dataset captured from our school. The model\nwas trained on University-1652 and was not ﬁne-tuned on\nthis dataset. The dataset contains more than 50 locations and\nmainly focuses the accuracy of Drone →Satellite task. The\nevaluation results of different methods are shown in Table 9.\nVOLUME 10, 2022 34283\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nFIGURE 9. Flight curve of drone.\nTABLE 8. Results of multiple queries about different flight height.\nTABLE 9. Results of inference on real datasets.\nG. VISUALIZATION OF RESULTS\nThe retrieval results of Satellite →Drone and Drone →Satel-\nlite tasks are displayed in Fig. 10 to prove the reliability of\nproposed method. Fig. 10a shows the method proposed had a\nhigh top-5 hit rate on Satellite →Drone, and Fig. 10b shows\nthe model had a high R@1 accuracy.\nTo investigate the effectiveness of multiple queries, the\nmatching results of multiple queries and a single query is\nvisualized in Fig. 11. For the geographic target, the true-\nmatched satellite image is in the position of R@5 when a\nsingle query is used. When multiple queries are used, the true-\nmatched satellite image all appeared in the position of R@1.\nThe result proves the effectiveness of the multiple queries.\nFig. 12. shows the feature vectors of 11 classes images dis-\ntributed in the Euclidean space. The same color represents the\nFIGURE 10. Visualized images retrieval results. (a) Top-5 retrieval results\nof Satellite→ Drone. (b)Top-5 retrieval results of Drone→Satellite. The\ntrue matches are in green boxes, while the false matches are displayed in\nred boxes.\nFIGURE 11. Visualized single query and multiple queries retrieval results.\nsame class of images, it can be seen that the model has strong\nintra-class aggregation.\nIV. DISCUSSIONS\nAccording to the experiments results on two tasks (Satel-\nlite →Drone and Drone →Satellite), we deeply explored\nthe proposed model’s retrieval performance and compared it\nwith existing models. The baseline proposed using a trans-\nformer network as backbone showed a good performance\nwithout extra modules and tricks. As show in Fig. 13, the\ntransformer can focus on more discriminative features in the\n34284 VOLUME 10, 2022\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\nFIGURE 12. Visualization of partial data classification results using t-SNE.\nFIGURE 13. Heatmaps generated by Vgg16, Resnet50 and Swin-T.\nimages than CNN. Because self-attention can explicitly mine\nthe potential connections among patches in the whole images,\nbut the convolution operation of CNN is more inclined to\nmine local features in the image. Since, compared with other\ncomplex networks with CNN as the backbone, transformer\nappears to be more competitive in image retrieval, which\nis conducive to the transformer being able to extract more\ncharacteristic features. Based on the strong baseline, SGM is\nproposed to extract richer contextual information and achieve\nfeature alignment. When the feature map was divided into\ntoo many parts by SGM, the performance of the model\ndeclined. We conjecture that there are two reasons for this\nphenomenon: (1) Too many parts may cause redundancy\nof context information. (2) Too many branches will cause\nnetwork overﬁtting. Therefore, it is necessary to choose an\nappropriate number of parts (e.g. 3 parts). In inference stage,\nif there are resource constraints, it is a good choice a branch\nwith higher accuracy in SGM to reduce time expenditure.\nIn order to achieve higher precision in real scenes, a method\nof multiple queries was used in the experiment. The results\nprove that multiple queries can signiﬁcantly improve the\naccuracy of retrieval, which also guides us to allow drones\nto take multi-angle and multi-height shooting of the same\ngeographic location to obtain more diverse features in the\nreal scene. Nevertheless, it is believed that the model still has\ncertain ﬂaws. For example, the accuracy of SGM’s semantic\nguidance in some complex scenes needs to be improved,\nwhich is also directly related to the ﬁnal matching accuracy of\nthe model. How to make better semantic guidance is meaning-\nful and promising work, and we will conduct further research\non this problem.\nV. CONCLUSION\nIn the paper, we proposed a transformer-based network to\nmatch drones with satellite images, which can be used for\ndrone autonomous positioning without a positioning system.\nA semantic guidance method is proposed to extract the con-\ntextual information in the image and improve the model’s\nrobustness to offset and scale. In the two tasks (Satellite →\nDrone and Drone →Satellite) on the dataset University-\n1652, the model achieved high accuracy.\nThe conclusion of the experiment are mainly as follows:\n1. The transformer-based network is more competitive than\nthe CNN-base network in this task. 2. The Semantic guidance\nmodule (SGM) can effectively mine the contextual infor-\nmation in the image, and achieve feature alignment in the\ninference stage, further improving the accuracy of the model.\n3. Multiple queries is more accurate than single query, and it\nbrings a huge improvement. The proposed method therefore\nachieved a precision that greatly surpasses existing methods.\nACKNOWLEDGMENT\nThe authors would like to thank Zheng Zhedong from the\nReler Laboratory, University of Technology Sydney, for pro-\nviding the Universty-1652 dataset, open source code, and his\ncontribution in this ﬁeld.\nREFERENCES\n[1] K. Fu, Z. Chang, Y. Zhang, G. Xu, K. Zhang, and X. Sun, ‘‘Rotation-\naware and multi-scale convolutional neural network for object detection in\nremote sensing images,’’ ISPRS J. Photogramm. Remote Sens., vol. 161,\npp. 294–308, Mar. 2020.\n[2] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, ‘‘Object detection in\noptical remote sensing images: A survey and a new benchmark,’’ ISPRS\nJ. Photogramm. Remote Sens., vol. 159, pp. 296–307, Jan. 2020.\nVOLUME 10, 2022 34285\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\n[3] Y. Zhao, L. Zhao, B. Xiong, and G. Kuang, ‘‘Attention receptive pyra-\nmid network for ship detection in SAR images,’’ IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 13, pp. 2738–2756, 2020, doi:\n10.1109/JSTARS.2020.2997081.\n[4] M. Sharma, M. Dhanaraj, S. Karnam, D. G. Chachlakis, R. Ptucha,\nP. P. Markopoulos, and E. Saber, ‘‘YOLOrs: Object detection in\nmultimodal remote sensing imagery,’’ IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens. , vol. 14, pp. 1497–1508, 2021, doi:\n10.1109/JSTARS.2020.3041316.\n[5] Q. Wu, F. Luo, P. Wu, B. Wang, H. Yang, and Y. Wu, ‘‘Automatic road\nextraction from high-resolution remote sensing images using a method\nbased on densely connected spatial feature-enhanced pyramid,’’ IEEE J.\nSel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 3–17, 2021, doi:\n10.1109/JSTARS.2020.3042816.\n[6] T. Mao, H. Tang, and W. Huang, ‘‘Unsupervised classiﬁcation of multi-\nspectral images embedded with a segmentation of panchromatic images\nusing localized clusters,’’ IEEE Trans. Geosci. Remote Sens., vol. 57,\nno. 11, pp. 8732–8744, Nov. 2019.\n[7] J. Chen, Z. Yuan, J. Peng, L. Chen, H. Huang, J. Zhu, Y. Liu, and H.\nLi, ‘‘DASNet: Dual attentive fully convolutional Siamese networks for\nchange detection in high-resolution satellite images,’’ IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 14, pp. 1194–1206, 2020, doi:\n10.1109/JSTARS.2020.3037893.\n[8] X. Sun, A. Shi, H. Huang, and H. Mayer, ‘‘BAS 4Net: Boundary-aware\nsemi-supervised semantic segmentation network for very high resolution\nremote sensing images,’’ IEEE J. Sel. Topics Appl. Earth Observ. Remote\nSens., vol. 13, pp. 5398–5413, 2020, doi: 10.1109/JSTARS.2020.3021098.\n[9] M. Sheykhmousa, M. Mahdianpari, H. Ghanbari, F. Mohammadimanesh,\nP. Ghamisi, and S. Homayouni, ‘‘Support vector machine versus random\nforest for remote sensing image classiﬁcation: A meta-analysis and sys-\ntematic review,’’ IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 13, pp. 6308–6325, 2020, doi: 10.1109/JSTARS.2020.3026724.\n[10] G. Cheng, X. Xie, J. Han, L. Guo, and G.-S. Xia, ‘‘Remote sens-\ning image scene classiﬁcation meets deep learning: Challenges, meth-\nods, benchmarks, and opportunities,’’ IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 13, no. 99, pp. 3735–3756, Jun. 2020, doi:\n10.1109/JSTARS.2020.3005403.\n[11] Y. Yuan and L. Lin, ‘‘Self-supervised pretraining of transformers\nfor satellite image time series classiﬁcation,’’ IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 14, pp. 474–487, 2021, doi:\n10.1109/JSTARS.2020.3036602.\n[12] Q. Sang, Y. Zhuang, S. Dong, G. Wang, H. Chen, and L. Li,\n‘‘Improved land cover classiﬁcation of VHR optical remote sensing\nimagery based upon detail injection procedure,’’ IEEE J. Sel. Top-\nics Appl. Earth Observ. Remote Sens., vol. 14, pp. 18–31, 2021, doi:\n10.1109/JSTARS.2020.3032423.\n[13] F. Castaldo, A. Zamir, R. Angst, F. Palmieri, and S. Savarese, ‘‘Semantic\ncross-view matching,’’ in Proc. IEEE Int. Conf. Comput. Vis. Workshop\n(ICCVW), Dec. 2015, pp. 1044–1052.\n[14] T.-Y. Lin, S. Belongie, and J. Hays, ‘‘Cross-view image geolocaliza-\ntion,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2013,\npp. 891–898.\n[15] T. Senlet and A. Elgammal, ‘‘A framework for global vehicle localization\nusing stereo images and satellite and road maps,’’ in Proc. IEEE Int. Conf.\nComput. Vis. Workshops (ICCV Workshops), Nov. 2011, pp. 2034–2041.\n[16] M. Bansal, H. S. Sawhney, H. Cheng, and K. Daniilidis, ‘‘Geo-localization\nof street views with aerial image databases,’’ in Proc. 19th ACM Int. Conf.\nMultimedia (MM), 2011, pp. 1125–1128.\n[17] M. Zhai, Z. Bessinger, S. Workman, and N. Jacobs, ‘‘Predicting ground-\nlevel scene layout from aerial imagery,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jul. 2017, pp. 867–875.\n[18] L. Liu and H. Li, ‘‘Lending orientation to neural networks for cross-\nview geo-localization,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2019, pp. 5624–5633.\n[19] S. Hu, M. Feng, R. M. H. Nguyen, and G. H. Lee, ‘‘CVM-Net: Cross-view\nmatching network for image-based ground-to-aerial geo-localization,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\npp. 7258–7267.\n[20] R. Arandjelović, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, ‘‘NetVLAD:\nCNN architecture for weakly supervised place recognition,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., Jul. 2016, pp. 5297–5307.\n[21] L. Liu, H. Li, and Y. Dai, ‘‘Stochastic attraction-repulsion embedding for\nlarge scale image localization,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV), Oct. 2019, pp. 2570–2579.\n[22] N. N. Vo and J. Hays, ‘‘Localizing and orienting street views using\noverhead imagery,’’ in Proc. Eur. Conf. Comput. Vis., Amsterdam, The\nNetherlands, Oct. 2016, pp. 494–509.\n[23] Y. Shi, L. Liu, X. Yu, and H. Li, ‘‘Spatial-aware feature aggregation for\nimage based cross-view geo-localization,’’ in Proc. Neural Inf. Process.\nSyst., Vancouver, BC, Canada, Dec. 2019, pp. 8–14.\n[24] Y. Shi, X. Yu, D. Campbell, and H. Li, ‘‘Where am I looking at? Joint\nlocation and orientation estimation by cross-view matching,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,\npp. 4063–4071.\n[25] Z. Zheng, Y. Wei, and Y. Yang, ‘‘University-1652: A multi-view multi-\nsource benchmark for drone-based geo-localization,’’ in Proc. 28th ACM\nInt. Conf. Multimedia, Oct. 2020, pp. 1395–1403.\n[26] Y. Ho and S. Wookey, ‘‘The real-world-weight cross-entropy loss function:\nModeling the costs of mislabeling,’’ IEEE Access, vol. 8, pp. 4806–4813,\n2020, doi: 10.1109/ACCESS.2019.2962617.\n[27] X. Li, M. He, H. Li, and H. Shen, ‘‘A combined loss-based multiscale fully\nconvolutional network for high-resolution remote sensing image change\ndetection,’’ IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1–5, 2022, doi:\n10.1109/LGRS.2021.3098774.\n[28] Z. Zheng, L. Zheng, M. Garrett, Y. Yang, M. Xu, and Y.-D. Shen, ‘‘Dual-\npath convolutional image-text embeddings with instance loss,’’ ACM\nTrans. Multimedia Comput., Commun., Appl., vol. 16, no. 2, pp. 1–23,\nMay 2020.\n[29] Z. Zheng, L. Zheng, and Y. Yang, ‘‘A discriminatively learned CNN\nembedding for person reidentiﬁcation,’’ ACM Trans. Multimedia Comput.,\nCommun., Appl., vol. 14, no. 1, pp. 1–20, 2017.\n[30] L. Ding, J. Zhou, L. Meng, and Z. Long, ‘‘A practical cross-view\nimage matching method between UA V and satellite for UA V-based geo-\nlocalization,’’Remote Sens., vol. 13, no. 1, p. 47, Dec. 2020.\n[31] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, ‘‘Beyond part models:\nPerson retrieval with reﬁned part pooling (and a strong convolutional\nbaseline),’’ inProc. Eur. Conf. Comput. Vis., Munich, Germany, Sep. 2018,\npp. 501–518.\n[32] L. Zheng, Y. Huang, H. Lu, and Y. Yang, ‘‘Pose-invariant embedding for\ndeep person re-identiﬁcation,’’ IEEE Trans. Image Process., vol. 28, no. 9,\npp. 4500–4509, Sep. 2019.\n[33] L. Wei, S. Zhang, H. Yao, W. Gao, and Q. Tian, ‘‘GLAD: Global–local-\nalignment descriptor for scalable person re-identiﬁcation,’’ IEEE Trans.\nMultimedia, vol. 21, no. 4, pp. 986–999, Apr. 2019.\n[34] Z. Zheng, L. Zheng, and Y. Yang, ‘‘Pedestrian alignment network for large-\nscale person re-identiﬁcation,’’ IEEE Trans. Circuits Syst. Video Technol.,\nvol. 29, no. 10, pp. 3037–3045, Oct. 2018.\n[35] T. Wang, Z. Zheng, C. Yan, J. Zhang, Y. Sun, B. Zheng, and Y. Yang, ‘‘Each\npart matters: Local patterns facilitate cross-view geo-localization,’’ IEEE\nTrans. Circuits Syst. Video Technol., vol. 32, no. 2, pp. 867–879, Feb. 2022,\ndoi: 10.1109/TCSVT.2021.3061265.\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaise, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Neural\nInf. Process. Syst., Long Beach, CA, USA, Dec. 2017, pp. 6000–6010.\n[37] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is 11 worth 16×16 words:\nTransformers for image recognition at scale,’’ in Proc. Int. Conf. Learn.\nRepresent., May 2021.\n[38] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efﬁcient image transformers & distillation through atten-\ntion,’’ 2020, arXiv:2012.12877.\n[39] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to end object detection with transformers,’’ in Proc.\nEur. Conf. Comput. Vis., Aug. 2020, pp. 213–229.\n[40] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‘‘Deformable DETR:\nDeformable transformers for end-to-end object detection,’’ in Proc. Int.\nConf. Learn. Represent., May 2021.\n[41] C. Chi, F. Wei, and H. Hu, ‘‘RelationNet++: Bridging visual representa-\ntions for object detection via transformer decoder,’’ in Proc. Neural Inf.\nProcess. Syst., Dec. 2020, pp. 6–11.\n[42] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,\nT. Xiang, P. H. S. Torr, and L. Zhang, ‘‘Rethinking semantic segmenta-\ntion from a sequence-to-sequence perspective with transformers,’’ 2021,\narXiv:2012.15840.\n[43] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, ‘‘Pre-trained image processing transformer,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 12299–12310.\n34286 VOLUME 10, 2022\nJ. Zhuanget al.: Semantic Guidance and Transformer-Based Matching Method for UAVs and Satellite Images\n[44] Y. Jiang, S. Chang, and Z. Wang, ‘‘TransGAN: Two pure transformers can\nmake one strong GAN, and that can scale up,’’ 2021, arXiv:2102.07074.\n[45] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, ‘‘Learning texture transformer\nnetwork for image super-resolution,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2020, pp. 5790–5799.\n[46] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, ‘‘TransReID:\nTransformer-based object re-identiﬁcation,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis. (ICCV), Oct. 2021, pp. 11–17.\n[47] X. Wang, G. Doretto, T. Sebastian, J. Rittscher, and P. Tu, ‘‘Shape and\nappearance context modeling,’’ in Proc. IEEE 11th Int. Conf. Comput. Vis.,\nOct. 2007, pp. 1–8.\n[48] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, ‘‘Scalable\nperson re-identiﬁcation: A benchmark,’’ in Proc. IEEE Int. Conf. Comput.\nVis. (ICCV), Dec. 2015, pp. 1116–1124.\n[49] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[50] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ in Proc. Int. Conf. Learn. Represent.\n(ICLR), San Diego, CA, USA, May 2015, pp. 7–9.\n[51] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, ‘‘Swin\ntransformer: Hierarchical vision transformer using shifted Windows,’’ in\nProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 11–17.\n[52] F. Radenović, G. Tolias, and O. Chum, ‘‘Fine-tuning CNN image retrieval\nwith, no., human annotation,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 41, no. 7, pp. 1655–1668, Jun. 2018.\n[53] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, ‘‘Squeeze-and-excitation\nnetworks,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 8,\npp. 2011–2023, Aug. 2020, doi: 10.1109/TPAMI.2019.2913372.\nJIEDONG ZHUANG received the B.S. degree\nin automation from China Jiliang University,\nHangzhou, China, in 2019, where he is currently\npursuing the M.S. degree with the Department\nof Control Engineering. His research interests\ninclude deep learning and image retrieval.\nXURUOYAN CHEN received the B.S. degree\nin automation from China Jiliang University,\nHangzhou, China, in 2019, where she is currently\npursuing the M.S. degree with the Department\nof Control Engineering. Her research interests\ninclude image processing and simultaneous local-\nization and mapping.\nMING DAI received the B.S. degree in automa-\ntion from China Jiliang University, Hangzhou,\nChina, in 2020, where he is currently pursuing\nthe M.S. degree in mechanical engineering. His\nresearch interests include deep learning and image\nprocessing.\nWENBO LANreceived the M.S. degree in control\nscience and engineering from the Harbin Insti-\ntute of Technology, China, in 2009. He is cur-\nrently a Senior Engineer with China Academy of\nAerospace Aerodynamics (CAAA). His research\ninterests include UA V application and general\ndesign of UA V\nYONGHENG CAI received the M.S. degree\nin control science and engineering from the\nHarbin Institute of Technology, China, in 2012.\nHe is currently a Senior Engineer with China\nAcademy of Aerospace Aerodynamics (CAAA).\nHis research interests include UA V application,\nnavigation ﬂight control system design, and sim-\nulation of UA V.\nENHUI ZHENG received the Ph.D. degree in\ncontrol science and engineering from Zhejiang\nUniversity, Hangzhou, China, in 2006. He is cur-\nrently an Associate Professor with the Depart-\nment of Automation, China Jiliang University,\nHangzhou. He is the Deputy Secretary-General\nof the Zhejiang Model Radio Sports Association.\nHis research interests include UA V application,\nimage processing, deep learning, and simultaneous\nlocalization and mapping.\nVOLUME 10, 2022 34287",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8368364572525024
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7200490236282349
    },
    {
      "name": "Drone",
      "score": 0.6611440777778625
    },
    {
      "name": "Upsampling",
      "score": 0.6214964985847473
    },
    {
      "name": "Computer vision",
      "score": 0.6144405603408813
    },
    {
      "name": "Pixel",
      "score": 0.5004451274871826
    },
    {
      "name": "Transformer",
      "score": 0.4701017737388611
    },
    {
      "name": "Feature extraction",
      "score": 0.44455134868621826
    },
    {
      "name": "Image (mathematics)",
      "score": 0.20392683148384094
    },
    {
      "name": "Voltage",
      "score": 0.09538871049880981
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}