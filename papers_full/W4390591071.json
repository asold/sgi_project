{
  "title": "An Unsupervised Transformer-Based Multivariate Alteration Detection Approach for Change Detection in VHR Remote Sensing Images",
  "url": "https://openalex.org/W4390591071",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100599539",
      "name": "Yizhang Lin",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5100684827",
      "name": "Sicong Liu",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5037069703",
      "name": "Yongjie Zheng",
      "affiliations": [
        "Tongji University",
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5039348214",
      "name": "Xiaohua Tong",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5008163978",
      "name": "Huan Xie",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5030414508",
      "name": "Hongming Zhu",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5046318445",
      "name": "Kecheng Du",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5065210450",
      "name": "Hui Zhao",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5115590908",
      "name": "Jie Zhang",
      "affiliations": [
        "Tongji University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2953308875",
    "https://openalex.org/W2969881582",
    "https://openalex.org/W4205666860",
    "https://openalex.org/W2165577558",
    "https://openalex.org/W6629944422",
    "https://openalex.org/W2104374858",
    "https://openalex.org/W2085283969",
    "https://openalex.org/W2006383776",
    "https://openalex.org/W1998595580",
    "https://openalex.org/W2144552105",
    "https://openalex.org/W2134969826",
    "https://openalex.org/W2726981152",
    "https://openalex.org/W2334023959",
    "https://openalex.org/W2755992512",
    "https://openalex.org/W3206197202",
    "https://openalex.org/W3048631361",
    "https://openalex.org/W3209540366",
    "https://openalex.org/W3213272555",
    "https://openalex.org/W4285220262",
    "https://openalex.org/W4312807131",
    "https://openalex.org/W4378194596",
    "https://openalex.org/W4388157208",
    "https://openalex.org/W2891248708",
    "https://openalex.org/W2951991161",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W2766049824",
    "https://openalex.org/W2910587630",
    "https://openalex.org/W2979572910",
    "https://openalex.org/W2902788350",
    "https://openalex.org/W3179303890",
    "https://openalex.org/W4224067506",
    "https://openalex.org/W4226361741",
    "https://openalex.org/W3048064159",
    "https://openalex.org/W3105553032",
    "https://openalex.org/W3102692100"
  ],
  "abstract": "Multitemporal change detection (CD) plays a crucial role in the remote sensing application field. In recent years, supervised deep learning methods have shown excellent performance in detecting changes in very-high-resolution (VHR) images. However, these methods require a large number of labeled samples for training, making the process time-consuming and labor-intensive. Unsupervised approaches are more attractive in practical applications since they can produce a CD map without relying on any ground reference or prior knowledge. In this article, we propose a novel unsupervised CD approach, named transformer-based multivariate alteration detection (trans-MAD). It utilizes a pre-detection strategy that combines the compressed change vector analysis and the iteratively reweighted multivariate alteration detection (IR-MAD) to generate reliable pseudotraining samples. More accurate and robust CD results can be achieved by leveraging the IR-MAD to detect insignificant changes and by incorporating the transformer-based attention mechanism to model the difference or similarity between two distant pixels in an image. The proposed trans-MAD approach was validated on two VHR bitemporal satellite remote sensing datasets, and the obtained experimental results demonstrated its superiority comparing with the state-of-the-art unsupervised CD methods.",
  "full_text": "1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \nAn Unsupervised Transformer-based Multivariate \nAlteration Detection Approach for Change Detection \nin VHR Remote Sensing Images \n \nYizhang Lin, Sicong Liu, Senior Member, IEEE, Yongjie Zheng, Student Member, IEEE, Xiaohua Tong, Senior \nMember, IEEE, Huan Xie, Senior Member, IEEE, Hongming Zhu, Kecheng Du, Hui Zhao, Jie Zhang \n \nAbstract—Multi-temporal change detection (CD) plays a \ncrucial role in the remote sensing application field. In recent years, \nsupervised deep learning methods have shown excellent \nperformance in detecting changes in very-high-resolution (VHR) \nimages. However, these methods require a large number of labeled \nsamples for training, making the process time-consuming and \nlabor-intensive. Unsupervised approaches are more attractive in \npractical applications since they can produce a CD map without \nrelying on any ground reference or prior knowledge. In this paper, \nwe propose a novel unsupervised CD approach, named \nTransformer-based Multivariate Alteration Detection (Trans-\nMAD). It utilizes a pre-detection strategy that combines the \nCompressed Change Vector Analysis (C2VA) and the Iteratively \nReweighted Multivariate Alteration Detection (IR-MAD) to \ngenerate reliable pseudo-training samples. More accurate and \nrobust CD results can be achieved by leveraging the IR-MAD to \ndetect insignificant changes and by incorporating the \nTransformer-based attention mechanism to model the difference \nor similarity between two distant pixels in an image. The proposed \nTrans-MAD approach was validated on two VHR bi-temporal \nsatellite remote sensing datasets, and the obtained experimental \nresults demonstrated its superiority comparing with the state-of-\nthe-art unsupervised CD methods. \nIndex Terms—Change detection (CD), unsupervised, deep \nlearning, iteratively reweighted multivariate alteration detection \n(IR-MAD), transformer, very-high-resolution (VHR) remote \nsensing images. \n \nI. INTRODUCTION \nHANGE detection (CD) is a crucial task in various \nremote sensing application fields. It involves \nidentifying the differences of an object or phenomenon \nover a certain region by analyzing two or more images captured \nat different times [1]. CD technique enables successful \napplications like land-cover mapping, disaster assessment, \nurban development monitoring, and ecological environment \nmonitoring [2]. It provides a great opportunity to discover and \nanalyze land-cover changes caused by human activities or \n \nThis work was supported in part by the National Natural Science Foundation \nof China under Grants 42071324, 42241130, in part by the Shanghai Rising-\nStar Program (21QA1409100), Shanghai Municipal Science and Technology \nMajor Project (2021SHZDZX0100) and the Fundamental Research Funds for \nthe Central Universities, and in part by the Research Project of Tongji \nArchitectural Design (Group) Co., Ltd (2023J-JB11). (Corresponding author: \nSicong Liu) \nYizhang Lin, Sicong Liu, Xiaohua Tong, Huan Xie, Kecheng Du, Hui Zhao \nand Jie Zhang are with the College of Surveying and Geoinformatics, Tongji \nUniversity, Shanghai 200092, China (e-mail: evan@tongji.edu.cn; \nnatural phenomenon, which helps to make prompt and sound \ndecisions [3]. With the rapid development of observation \nplatforms and optical sensors, the very-high-resolution (VHR) \nremote sensing images have become the primary data source in \nthe data archive. VHR images contain fine spatial information \nand thus are able to depict land objects at a detailed scale, \nwhereas their spectral information is relatively coarse when \ncomparing with the dense-sampling hyperspectral images. \nIn the past decades, numerous CD methods have been \nproposed, ranging from traditional techniques to advanced deep \nlearning-based approaches. The category of traditional CD \nmethods includes image algebra, image transformation, post-\nclassification comparison, and others [4]. Within this context, \npopular CD methods are mainly developed based on spectral \nanalysis of the original bands, such as Change Vector Analysis \n(CVA) [5], Compressed Change Vector Analysis (C2VA) [6] \nand its adaptively sequential version (S2CVA) [7], Multi-\nvariate Alteration Detection (MAD) [8] and its iteratively \nreweighted version (IR-MAD) [9], Principal Component \nAnalysis (PCA)-based [10], and Slow Feature Analysis (SFA)-\nbased [11]. In order to explore more robust change \nrepresentation, some works also focused on features that \nderived from the original spectral bands and incorporated into \nC2VA or IR-MAD [12], [13]. However, such features are \nmainly shallow and artificially designed, whose effectiveness is \nnot sufficient for representing different types of changes at \ndifferent significance levels. Furthermore, these traditional CD \nmethods may face challenges concerning reduced accuracy and \nrobustness when dealing with the VHR images over complex \nscenes, since they are mainly designed based on the utilization \nof original spectral bands or extraction of simple handcrafted \nfeatures. In addition, data quality issues such as the seasonable \nvariations, illumination conditions, and spectral variability may \nlead to the degradation of the CD performance [14]–[16].  \nIn recent years, deep learning techniques have demonstrated \nremarkable success in various remote sensing application tasks \nsicong.liu@tongji.edu.cn; xhtong@tongji.edu.cn; huanxie@tongji.edu.cn; \nkecheng_du@tongji.edu.cn; zhaohui@tongji.edu.cn; \nzhangjie22@tongji.edu.cn). \nYongjie Zheng was with the College of Surveying and Geoinformatics, \nTongji University, Shanghai 200092, China. Now she is with the Department \nof Information Engineering and Computer Science, University of Trento, 38123 \nTrento, Italy (e-mail: yongjie.zheng@unitn.it). \nHongming Zhu is with the School of Software Engineering, Tongji \nUniversity, Shanghai 201804, China (zhu_hongming@tongji.edu.cn). \nC\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n[17]–[23]. They have also emerged as promising alternatives to \naddress the limitations of traditional CD methods. According to \nthe utilization of reference data, they can be divided into two \nmain categories: supervised and unsupervised methods. The \nformer relies on the available ground reference samples to train \nthe model, such as Fully Convolutional Network (FCN) [24], \nUNet++ [25], and Bitemporal Image Transformer (BIT) [26]. \nThe latter does not require ground reference samples thus is \ndata-driven with a higher degree of automation. Gong et al. [27] \nproposed a method based on a generative adversarial network \n(GAN) to generate better differential images, and built the \nmapping relationship between training data and corresponding \npatches, and finally obtained binary change maps. Saha et al. \n[28] developed a deep change vector analysis (DCVA) \nframework, and made full use of the multi-layer deep features \nextracted by convolutional neural network (CNN) to determine \nthe changed pixels. Chen et al. [29] proposed a method named \nDSMS-CN that used the deep siamese CNN to extract the \nmulti-scale spectral-spatial features for CD. Du et al. [30] \nproposed a Deep Slow Feature Analysis (DSFA) method based \non the original SFA, which used two symmetric fully connected \nnetworks to project input data into a new feature space, and then \nextracted the most invariant components to highlight the \nchanged components. Wu et al. [31] built a deep siamese kernel \nprincipal component analysis convolution mapping network \n(KPCA-MNet) to extract high-level spectral-spatial feature \nmaps and generated the final CD map. \nDespite the effectiveness of the aforementioned \nunsupervised approaches, several issues still require to be \nanalyzed and addressed. \nFirst, the quality of pseudo-training samples largely depends \non the pre-detection step. If the pre-detection algorithm does \nnot work properly, it will introduce inaccurate or even wrong \nsamples during the training process, eventually affecting CD \naccuracies. For example, in the existing KPCA-MNet algorithm, \nimage patches are randomly selected as samples for training the \nKPCA convolutional layer, whose uncertainty will inevitably \nlead to unstable CD performance with the occurrence of \nomission and commission errors. The pre-detection step in \nDSFA is achieved by using CVA to obtain pseudo-training \nsamples. To follow the convention of SFA, it only selects the \npixels with the lowest change intensity as samples. However, \nthis cannot be successfully extended to other methods relying \non different CD strategies. Note that the insufficient \nrepresentativeness of the samples will result in poor \nperformance of CD especially in those complex scenarios. \nSecond, existing methods exhibit limitations in feature \nextraction. DCVA relies on pre-trained deep CNN to extract \ndeep features, and its generalization ability is unstable when \ndealing with different complex scenarios. The DSMS-CN \nmethod utilizes deep siamese convolutional neural networks to \nextract multi-scale spectral-spatial features. However, due to its \ninherent characteristics and limited receptive field of \nconvolutional kernels, CNN can only capture spatial contextual \ninformation at the local scale. It is challenging to capture long-\nrange dependencies and contextual information, which has a \nsignificant impact on CD accuracy, leading to the occurrence of \ncommissions and omissions. There are existing methods (e.g., \nBIT, hybrid-TransCD [32], and TransUNetCD [33]) using \nTransformer to model global contextual features, but they are \nall trained in a supervised manner and require a large number \nof training samples. \nTo overcome these limitations, in this paper, we propose a \nnovel deep learning-based CD framework named Transformer-\nbased Multivariate Alteration Detection (Trans-MAD). The \nmain contributions and novelties are summarized as follows. \n1. Improved Pre-detection and Pseudo-training Sample \nGeneration: The challenging issue of pseudo-training \nsamples generation in the unsupervised CD is addressed \nby taking advantages of the joint change representation \nfrom two independent pre-detection algorithms C2VA \nand IR-MAD, without relying on the ground reference \ndata or prior knowledge. This facilitates the generation \nof highly reliable and representative pseudo-training \nsamples in an automated and unsupervised fashion, even \nin complex scenarios. \n2. Innovative Shallow-to-Deep Feature Extraction: \nConsidering that the traditional IR-MAD method \ndirectly performs linear transformations on the original \nspectral bands to extract change information, it is easily \naffected by noise. This paper develops an innovative \nunsupervised framework that uses CNN, transformer, \nand IR-MAD to harmoniously extract and fuse shallow-\nto-deep features, effectively describing local and global \nchange information, thereby reducing omission errors \nand generating more accurate CD maps. \n3. Robust Handling of Sampling Randomness: Trans-MAD \nincorporates a dedicated decision fusion-based CD \nmodule to mitigate the challenges posed by random \nsampling uncertainty when generating pseudo-training \nsamples. This strategy reduces commissions in CD \noutput and ensures the stability of the final result. \nThe proposed Trans-MAD approach is validated on two real \nVHR image datasets, and obtained experimental results confirm \nits effectiveness when comparing with the several state-of-the-\nart unsupervised CD methods. \nThe remainder of this paper is organized as follows. Section \nII introduces the related works. Section III presents in detail the \nproposed approach. Section IV describes the data sets and \ndetailed experimental design. Experimental results and \ndiscussions are provided in Section V. Finally, the conclusion \nis drawn in Section VI. \nII. RELATED WORKS \nA. CNN \nThe CNN, a specialized neural network architecture, is \nparticularly designed for data with grid-like structures. It has \nemerged as a powerful tool for detecting changes in VHR \nremote sensing images. Many research endeavors involve \ntraining CNN models to learn feature difference between bi-\ntemporal images, thereby facilitating the CD process [10]. \nCNN is characterized by hierarchical structure, typically \ncomprising convolutional layers, activation functions, and \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \npooling layers. They operate on input data using learnable \nconvolution kernels, and apply activation functions to introduce \nnonlinearity to the network. Subsequently, the pooling layer \nsubsamples each feature map to reduce redundancy. Over a \nseries of alternating convolutional and pooling layers, CNN \nautonomously generates advanced features from the input data. \nCommonly, the network parameters are optimized using \nStochastic Gradient Descent (SGD) and the backpropagation \n(BP) algorithm. Through several rounds of training with \nannotated data, CNN performs supervised learning and \ngenerates increasingly representative features. \nB. BIT \nThe BIT serves as the main feature-extracting module with \nthree key components: 1) a siamese semantic tokenizer, which \ngroups pixels into concepts to generate a compact set of \nsemantic tokens for each temporal input, 2) a transformer \nencoder, which models context of semantic concepts in token-\nbased space-time, and 3) a siamese transformer decoder, which \nprojects the corresponding semantic tokens back to pixel-space \nto obtain the refined feature map for each temporal image [26]. \nLet I1, I2∈RH×W×B be the bi-temporal images, where H, W, B \nis height, width, and number of bands of the image. BIT-based \nmodel first extracts high-level features X1 and X2 by a small \nCNN. Two token sets are computed by a semantic tokenizer \nbased on the extracted features. Then, BIT models the global \nrelationships within these token sets using a transformer \nencoder, resulting in context-rich semantic tokens T1 and T2. As \nthe core of the transformer encoder, self-attention is calculated \nas follows: \n Attention , , soft max\nk\nQKQ K V V d\n     \n (1) \nwhere Q, K, and V are query, key, and value vectors from \nsemantic tokens, respectively, and dk represents the dimension \nof the key vector. It calculates the correlation weight matrix \ncoefficients of Q and K and normalizes the weight matrix \nthrough softmax operation. The weight coefficients are \nsuperimposed on V to achieve modeling of global contextual \ninformation [26]. These context-enriched tokens contain high-\nlevel semantic details that effectively highlight the changes. To \nbridge the gap between these representations and pixel-level \nfeatures, BIT employs a modified siamese transformer decoder \nto refine the image features for each image. \nThe final deep features obtained are f1 and f2, respectively. In \nsummary, BIT interprets a feature map as a sequence of patches \nvia its semantic tokenizer, facilitating the learning and \ncorrelation of global context related to high-level semantic \nconcepts. The transformer's self-attention mechanism plays a \ncrucial role in capturing long-range dependencies among pixels, \nenabling the modeling of comprehensive contextual \ninformation within images. As a result, BIT excels in \ncomprehending the spatial relationships and overall \ncharacteristics of complex change targets in VHR images, \nsubstantially enhancing its capacity to represent change-related \ninformation. \nC. IR-MAD \nIR-MAD is an optimized iterative version of MAD, which \nessentially uses multivariate random variables to represent \nmultispectral bi-temporal images, and detects changes through \nmultivariate statistical analysis [9]. The core step of IR-MAD \nis canonical correlation analysis (CCA), which involves \nderiving linear combinations from two sets of original variables \nand using correlation coefficients to analyze the correlation \nbetween two sets of variables, thereby reflecting the overall \ncorrelation between the original bi-temporal images. The result \nobtained by calculating canonical variables and performing \ndifference operations reflects the maximum change information \nof all spectral bands. \nAccording to (2), a linear transformation is firstly performed \non the input f1 and f2 using projection vectors a and b to obtain \nthe U and V. Using U-V to represent the change information \nbetween images, the algorithm aims to find suitable a and b for \nmaximizing the variance (3) of difference between U and V, that \nis, minimize the correlation between U and V, in order to \nconcentrate as much change information as possible. \n1\n2\nT\nT\nU a f\nV b f\n  \n (2) \n2 1 Corr ,D U V U V    (3) \nThe variance-covariance matrix of f1 and f2 is (4). Using the \nLagrangian multiplier method and denoting Corr (U, V) as r, (5) \ncan be derived. The problem eventually turns into seeking \neigenvalues r2 and sorting them. \n\n\n\n\n\n2221\n1211\n \n(4) \n1 1 2\n11 12 22 21 a r a       (5) \nAfter finding the eigenvectors a and b, the corresponding \ncanonical correlation variables can be calculated. The MAD \nvariates (6) are linear combinations of input variables f1 and f2. \nThe square sum of the MAD variates divided by the standard \ndeviation approximately satisfies a chi-square distribution with \np (number of bands) degrees of freedom. In addition, if there is \nno change at pixel j, then the ith MAD value, MADij, has a \nmean 0. On this basis, calculate the chi-square distance chij \naccording to (7) and update the weight wj of pixel according to \n(8). After the convergence of the canonical correlation \ncoefficient, a thresholding algorithm is used to generate a \nbinary CD map according to a pre-defined threshold value t. \nFollowing the above principle, IR-MAD iteratively \nhighlights change targets. It can effectively process multi-\ndimensional high-level features from the feature extraction \nmodule without excessive manual intervention. \n1 2\nT TMAD a f b f   (6) \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n Fig. 1. Architecture of the proposed Trans-MAD approach. \n \n \n2\n2\n1 i\np ij\nj\ni MAD\nMADchi p \n     \n \n(7) \n2\nj jw P chi t P p t      (8) \nIII. METHODOLOGY \nThe proposed Trans-MAD approach mainly consists of three \nmodules: 1) the improved pre-detection module, 2) the shallow-\nto-deep feature extraction module, and 3) the decision fusion-\nbased CD module. The overall structure of the proposed \napproach is shown in Fig. 1. Details of each module are \nprovided as follows. \nA. Improved Pre-detection Module \nThis step aims to generate reliable pseudo-training samples \nfor CD from the original image pair. To this end, C2VA and IR-\nMAD methods are jointly considered to be used for pre-\ndetection. Let I1, I2∈RH×W×B be the bi-temporal images with B \nbands. Two change intensity maps ρC2VA and ρMAD are \ncomputed independently according to the following equations: \n 2\n2\n1 2C VA 1\nB k k\nk\nI I\n\n   (9) \n2\nMAD\n1 k\nB k\nk MAD\nMAD \n       (10) \nFig. 2 shows the scheme of generating pseudo-training \nsample patches. Let Ntotal be the total number of image pixels. \nThe number of no-change sample pixels (ωnc) and change \nsample pixels (ωc) are Nnc and Nc, respectively. Based on the \nintensity histogram, 10% Ntotal pixels with the lowest change \nintensity are selected as no-change pseudo-training samples, \nand 10% Ntotal pixels with the highest intensity are considered \nas change pseudo-training samples, while the remaining pixels \nare the background. The corresponding threshold values are tnc \nand tc for the no-change and change classes, respectively. \n \nFig. 2. Scheme of generating pseudo-training sample patches based on \nhistogram of an intensity map. \n \nThe sample pixel sets that generated by the C2VA and IR-\nMAD pre-detection step are defined as SC2VA and SMAD, \nrespectively. Then a conflict elimination operation is performed. \nFor a given pixel q in the image, its assigned categories in SC2VA \nand SMAD are CC2VA and CMAD, respectively. If q is selected as a \nsample pixel in both SC2VA and SMAD, but has category conflicts \n(CC2VA≠CMAD), then it will be removed from SC2VA and SMAD \nsimultaneously. This operation guarantees the unique and valid \ncategory of final obtained sample pixels. \nAfter the above process, the sample pixel sets are reshaped \ninto the shape of original image. Starting from the top left \ncorner of the image, an m×m pixels window is used to slide and \ncrop the image at a stride of s pixels. Finally, a series of sample \npatches containing pre-change, post-change images and binary \nlabels were cropped out from two sets of pre-detection results, \nwhich are denoted as ΩC2VA and ΩMAD, respectively. \nIn order to jointly utilize the information of two pre-detection \nalgorithms to enhance the discriminative ability of the samples, \nwe randomly select labels from two sample patch sets. In the \nnth sampling, corresponding to the previous cropping position \n(x, y), the final label Lxy will be selected from ΩC2VA or ΩMAD \nwith equal probability and added into final pseudo-training \nsample patch set Ωn. This sampling step is independently \nrepeated Ns (Ns usually is the odd value) times to obtain Ns \npseudo-training sample patch sets {Ω1, Ω2, …, ΩNs} for training \ndeep learning models. The whole process of this step is \nsummarized in Algorithm 1. \nB. Shallow-to-Deep (SD) Feature Extraction Module \nThe purpose of this module is to extract advanced features \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n \nbased on the pseudo-training samples generated in the previous \nstage. This is realized by utilizing both CNN and BIT, which \nare complementary in feature extraction, thereby enriching the \nchange representation information. In particular, CNN can \ncapture the hidden change information within a local range of \nimage through convolution operations, which is very helpful for \nidentifying tiny differences in specific regions. Unlike this, BIT \nfocuses more on global context and uses Transformer's self-\nattention mechanism to establish dependencies between distant \npixels, thus enabling a better understanding of the overall \nstructure and interrelationships of changed targets in the whole \nscene. \nAs shown in Fig. 2, a CNN backbone is firstly employed to \nprocess pseudo-training sample images from the pre-detection \nmodule and obtain the image features X1 and X2. Then they are \nfed into BIT to generate refined global features f1 and f2. These \ndeep features provide higher-level representation of original \nimages and filter out some irrelevant information, enhancing \nthe ability to capture complex changes that may not be easily \ndistinguished in the original images. By taking advantages of \nthe shallow-to-deep feature extraction process, both the local \nand global information of the bitemporal images is enhanced, \nwhich largely increases the change representability in the IR-\nMAD in the next step. By inputting the extracted SD features \ninto the IR-MAD algorithm, it can effectively reduce the \nomission errors caused by directly extracting change \ninformation from the original bands. \nNote that this feature extraction is also unsupervised and \nautomatic relying on the generated pseudo-training samples, \nand can well model the local and global image information thus \nis more robust for CD, especially in dealing with the complex \nhigh-resolution images with limited spectral bands. \nC. Decision Fusion-based CD Module \nThis step is designed to generate the final CD results. \nSpecifically, in the previous steps, a feature extraction \nframework combining CNN and Transformer extracts SD \nfeatures from the original pseudo-training sample images. \nThese enhanced features are input into the CD process and \nutilized by IR-MAD algorithm to capture better change \ninformation. Unlike directly extracting change information \nfrom original spectral bands, this CD method reveals previously \nhidden information in the data, greatly improving the ability to \ndetect and understand changes. The OTSU algorithm is used to \ngenerate the binary CD map. In addition, a majority voting \nfusion strategy is applied to the CD results obtained on different \nbatches to improve the stability and reliability of the final \noutput. Corresponding to the Ns sample sets {Ω1, Ω2, …, ΩNs} \nobtained in the pre-detection process, a total of Ns CD maps \n{CM1, CM2, …, CMNs} were obtained. For a given pixel on the \nimage, its final category C can be assigned according to the \nfollowing rule: \ns\nc\nnc\n, 2\n, otherwise.\nNcountC \n\n \n (11) \nwhere count represents the number of CD results that consider \nthe pixel to be a change. Accordingly, the final binary CD map \nCMfinal can be obtained. \nBy fusing several CD results through decision voting, \ncommission errors can be effectively reduced to a certain extent, \nand the quality of the final output is better than that of a single \ndetection CD map, with higher reliability. \nIV. DATA SET AND EXPERIMENTAL DESIGN \nA. Data Set Description \nTwo satellite remote sensing VHR images CD datasets are \nconsidered to evaluate the proposed approach in the experiment, \nwhich are introduced as follows. \nGuangzhou (GZ) Dataset [34]: This is a large-scale VHR \nmultispectral satellite image data set that acquired by Google \nEarth service, covering the suburb of Guangzhou, China. RGB \ncolor images were considered with a spatial resolution of 0.55 \nm. One pair of images (1836×1836 pixels) was selected in our \nexperiment, where the main changes are related to the buildings, \nfarmland, bare land, etc. in the image scenario (denoted as GZ \ndataset). Note that the initial annotation only focuses on the \nspecific change of buildings, thus we carefully modified the \noriginal change map and carried out further labeling to add \nmore comprehensive change classes including farmland, bare \nland, waters, and roads. The considered VHR images and the \ncorresponding change reference map are illustrated in Fig. 3. \nNanjing (NJ) Dataset: This dataset is made up of two pan-\nsharpened multispectral images acquired by BJ-3 satellite in \n2022 and 2023, covering the urban area of Nanjing city, which \nis denoted as the NJ data set. The pair of bi-temporal images \nconsist of three spectral bands (red, green and blue) with a \nspatial resolution of 0.5 m and 512×512 pixels. A change \nreference map (see Fig. 4c) was made by careful image \ninterpretation, and changes in this scene were mainly buildings, \nvegetation, road, and bare land. \nB. Experimental Design  \nIn order to evaluate the effectiveness of the proposed Trans- \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n (a) (b) (c) \nFig. 3. Bi-temporal images of the GZ dataset obtained in (a) 2015, (b) 2017, \nand (c) change reference map. \n (a) (b) (c) \nFig. 4. Bi-temporal images of the NJ dataset obtained in (a) 2022, (b) 2023, and \n(c) change reference map. \nMAD approach, several SOTA unsupervised CD methods are \nconsidered for a comparison purpose, including four traditional \nalgorithms, i.e., C2VA [6], SFA [11], MAD [8], IR-MAD [9], \nand three deep learning-based CD methods, i.e., DCVA [28], \nDSFA [30], and KPCA-MNet [31]. \nThe proposed algorithm was implemented using the PyTorch \nframework on an NVIDIA RTX2050 GPU with 4GB memory. \nThe basic learning rate was set to 0.01 and decreased linearly. \nThe SGD optimizer with a momentum of 0.9 and a weight \ndecay of 5×10-4 was applied to update the learning rate based \non the training data and improve convergence speed. The batch \nsize was set as 4 according to the GPU memory. The size of the \ncropping window m was 256 when generating pseudo-training \nsample patches. The sliding stride s was set to 64 and 16 for the \nGZ and NJ datasets, respectively. \nParameter settings for the reference methods were as follows. \nLayers in DCVA was set as {2, 5, 8}. The regularization \nparameter in DSFA was 10-4, and the network had 2 hidden \nlayers with 128 nodes per layer. The network depth of KPCA-\nMNet and the number of KPCA convolution kernels were 4 and \n8, respectively. The Radial Basis Function (RBF) kernel was \nchosen as the kernel function, with kernel parameter equal to \n5×10-4. The convolution kernel’s size was set to 3 according to \nthe experimental performance. \nC. Evaluation Metrics \nIn binary change maps, the changed (positive) areas are \nrepresented by white pixels, while the unchanged (negative) \nregions are represented by black pixels. True Positive (TP) \nindicates the number of changed pixels which are correctly \ndetected. False Positive (FP) is the number of unchanged pixels \nthat are falsely detected as changed ones. True Negative (TN) \nmeans the number of pixels predicted correctly as unchanged, \nand False Negative (FN) denotes the number of pixels predicted \nincorrectly as unchanged. \nQuantitative evaluation was carried out based on the obtained \nbinary CD maps and the corresponding pixel-level change \nreference map. To this end, overall accuracy (OA), Kappa \ncoefficient (KC), Precision, Recall, and F1-score (F1) were \ncalculated by the following formulas [32]. \nTP TNOA TP FP TN FN\n     (12) \n1\nOA PEKC PE\n   (13) \n \n2\nTP FP TP FN FN TN FP TNPE TP FP TN FN\n         (14) \nTPPrecisionTP FP   (15) \nTPRecall TP FN   (16) \n21 Precision RecallF Precision Recall\n    (17) \nIn particular, OA represents the proportion of the number of \ncorrectly classified samples to the total number of samples. KC \ndescribes the similarity between the change detection result and \nthe ground truth. Precision can represent the proportion of TP \npixels in the number of pixels detected as positive (changed). \nRecall represents the proportion of TP pixels in the number of \npixels that are actually positive (changed) on the ground. F1 is \nthe harmonic average calculated by the comprehensive \nPrecision rate and Recall rate, which represents the level of \nPrecision and Recall at the same time. \nV. RESULTS AND DISCUSSION \nA. Performance Comparison \nThe proposed approach and other seven reference methods \nwere tested on two considered VHR-CD data sets. Obtained \nresults are shown in Fig. 5 and Fig. 6, and TABLE I and \nTABLE II. The results in the second row of Fig. 5 and Fig. 6 \nare locally enlarged images, with enlarged areas marked in red \nboxes in Fig. 3 and Fig. 4, respectively. \nIn particular, for the GZ data, as shown in Fig. 5, one can see \nthat compared with the reference methods, the proposed Trans-\nMAD resulted in the best binary CD map with lower omission \nerrors, fewer noises, and a more regular and well-recognized \nchanged region. This is benefit from the following aspects: it \neffectively generates samples representing various types of \nchanges by the designed improved pre-detection strategy. Then, \nan advanced SD feature extraction module is utilized to better \ncapture features in VHR images, and local and global \ninformation about changes is modelled to effectively reduce \nomission errors. The decision fusion output step also plays an \nimportant role in reducing the uncertainty of pseudo-training \nsample generation, reducing false alarms and making the final \nCD result more accurate. From Fig. 5 (h), it is worth noting that \nTrans-MAD effectively identified changes with varying \nsignificance linked to different land-cover transitions (e.g., \nbuildings, vegetation, and bare land changes). However, the \nperformance of KPCA-MNet was relatively poor due to a large\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n (a) (b) (c) (d) (e) (f) (g) (h) \nFig. 5. Binary change maps obtained by different methods on the GZ dataset. (a) C2VA. (b) SFA. (c) MAD. (d) IR-MAD. (e) DCVA. (f) DSFA. (g) KPCA-MNet. \n(h) Trans-MAD. First row: whole CD maps; second row: enlarged subsets of the whole CD maps as highlighted in red box. \n \nTABLE I \nACCURACY INDICES ON THE GZ DATASET \nMethods OA KC Precision  Recall F1 \nC2VA 0.7046 0.3629 0.8583 0.3923 0.5385 \nSFA 0.6324 0.2337 0.6078 0.4602 0.5238 \nMAD 0.6539 0.2820 0.6322 0.5071 0.5628 \nIR-MAD 0.6669 0.3008 0.6741 0.4683 0.5526 \nDCVA 0.6252 0.1928 0.6562 0.3084 0.4196 \nDSFA 0.5897 0.1137 0.5741 0.2552 0.3534 \nKPCA-MNet 0.4876 -0.0079 0.4358 0.5644 0.4918 \nTrans-MAD 0.8116 0.6088 0.8592 0.6830 0.7610 \nnumber of commissions. This may be caused by the limited \ntraining samples and insufficient representativeness of features \nextracted by KPCA convolution, and by its incompatibility to \nthe specific scenario, finally leading to false alarms occurred in \nthe detected soil and water changes. The CD map that produced \nby DCVA exhibited fewer salt-and-pepper noises due to the \nmorphological processing. However, it produced many \nomissions associated to the vegetation changes. This could be \nattributed to the limitation of the feature extraction ability of the \npre-trained model used in the algorithm. As for the DSFA \nmethod, its detection result was fragmented with many \nomission errors, which may be affected by sample selection \nlimitations and the poor feature extraction in fully connected \nnetworks. However, as we can see from the results, for the \ncompared deep learning-based CD reference methods, they \nonly focus on the most significant land-cover changes in the \nscene (i.e., mainly are building changes). For the reference four \ntraditional unsupervised CD methods, many commission errors \nwere presented in the obtained CD maps. The main reason is \nthat they only utilize the shallow spectral features (i.e., original \nbands) of VHR images without considering more robust deep \nfeatures. \nTABLE I presents the quantitative evaluation indices \nobtained by different CD methods on GZ data. The best values \nof each evaluation criteria are highlighted in bold. As one can \nsee, the proposed Trans-MAD achieved significantly higher \naccuracies than other methods on all criteria. In particular, the \nOA value (i.e., 0.8116) was 18.64% higher than the best in deep \nlearning methods and 10.7% higher than the best in traditional \nmethods. KC value (i.e., 0.6088) was 41.6% and 24.59% higher \nthan the best performance of the deep learning method and \ntraditional method, respectively. In addition, the Precision, \nRecall, and F1 of the proposed approach were 0.8592, 0.6830, \nand 0.7610, respectively, also ranking first among the \ncomparison methods. A higher Precision rate demonstrates that \nthe proposed Trans-MAD approach presents fewer commission \nerrors in CD. In the meantime, excellent performance on Recall \nvalues indicates that fewer change areas are falsely detected as \nunchanged by Trans-MAD. In summary, among all traditional \nCD methods, C2VA performs best on OA, KC, and Precision \nindicators, while MAD-based methods perform better on Recall \nand F1. In deep learning methods, the proposed Trans-MAD \nachieves the best performance. \nFor the NJ dataset, the binary CD maps obtained by the \nproposed Trans-MAD approach and the reference methods are \nillustrated in TABLE II. Compared with other reference \nmethods, the proposed Trans-MAD obtained the most accurate \nCD map, and comprehensive qualitative evaluation showed its \nsuperior performance. Specifically, the result of the Trans-\nMAD method (Fig. 6 h) showed significant noise suppression \ncompared to the CD results of the traditional SFA method (Fig. \n6 b) and MAD method (Fig. 6 c), effectively avoiding the \ncreation of many unrelated error detection regions. Compared \nwith the CD method based on deep learning (Fig. 6 e-g), the \nTrans-MAD approach had the smallest number of omission \nerrors and significant advantage in generating more regular and \nmeaningful change maps. Experimental results confirm that \nTrans-MAD has the ability to capture both significant and \ninsignificant changes in VHR images with remarkable clarity. \nAccording to the statistical results reported in TABLE II, the \nproposed Trans-MAD outperformed the reference algorithms \non OA values (i.e., 0.7753), which indicated that more change \nand unchanged areas were accurately detected. The KC value \n(i.e., 0.5010) was 4.15% higher than the second highest value \nfrom IR-MAD, indicating a higher consistency between the CD \noutput of Trans-MAD and the reference change map. The \nRecall value (i.e., 0.6427) and F1 value (i.e., 0.6711) of Trans-\nMAD exceeded the second highest scoring method by 4.82% \nand 5.15%, respectively. The advantage of the Trans-MAD \napproach in Recall indicates that it has fewer omission errors \nthan other reference algorithms. The leading performance of \nthe Trans-MAD method in F1-score demonstrates that the \nalgorithm strikes a good balance between omission and \ncommission errors, thus can achieve high Precision and Recall \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \n (a) (b) (c) (d) (e) (f) (g) (h) \nFig. 6. Binary change maps obtained by different methods on the NJ dataset. (a) C2VA. (b) SFA. (c) MAD. (d) IR-MAD. (e) DCVA. (f) DSFA. (g) KPCA-MNet. \n(h) Trans-MAD. First row: whole CD maps; second row: enlarged subsets of the whole CD maps as highlighted in red box. \n \nTABLE II \nACCURACY INDICES ON THE NJ DATASET \nMethods OA KC Precision  Recall F1 \nC2VA 0.7549 0.4253 0.7290 0.4979 0.5917 \nSFA 0.7082 0.3651 0.5903 0.5945 0.5924 \nMAD 0.7152 0.3623 0.6144 0.5409 0.5753 \nIR-MAD 0.7680 0.4595 0.7459 0.5299 0.6196 \nDCVA 0.7571 0.3945 0.8672 0.3768 0.5253 \nDSFA 0.6685 0.1432 0.6193 0.1831 0.2827 \nKPCA-MNet 0.5344 -0.1255 0.2261 0.1260 0.1619 \nTrans-MAD 0.7753 0.5010 0.7022 0.6427 0.6711 \nTABLE III \nABLATION ANALYSIS ON DIFFERENT MODULES ON THE GZ DATASET \nMethods OA KC Precision  Recall F1 \nTrans-MAD 0.8116 0.6088 0.8592 0.6830 0.7610 \nw/o IPD (C2VA) 0.6741 0.2847 0.9199 0.2826 0.4324 \nw/o IPD (IR-MAD) 0.8034 0.5913 0.8516 0.6689 0.7493 \nw/o DF 0.7887  0.5634 0.8096 0.6786 0.7383 \nTABLE IV \nABLATION ANALYSIS ON DIFFERENT MODULES ON THE NJ DATASET \nMethods OA KC Precision  Recall F1 \nTrans-MAD 0.7753 0.5010 0.7022 0.6427 0.6711 \nw/o IPD (C2VA) 0.7286 0.3919 0.6365 0.5575 0.5944 \nw/o IPD (IR-MAD) 0.7601 0.4266 0.7714 0.4653 0.5805 \nw/o DF 0.6721  0.2551 0.5493 0.4491 0.4942 \nvalues simultaneously. The quantitative results confirm that by \nconsidering the trade-off between accuracies and CD errors (i.e., \nomissions and commissions), the proposed Trans-MAD \nachieves overall excellent CD performance with the best OA, \nKC, Recall, and F1 values. \nB. Ablation Analysis \nIn this section, ablation experiments were carried out on GZ \nand NJ dataset to evaluate the individual performance and \ncontribution of newly-added modules in the proposed Trans-\nMAD model. We evaluated the significance of improved pre-\ndetection (IPD) module and decision fusion (DF) module in \nTrans-MAD model. Unless otherwise indicated, all \nexperimental settings remain consistent and comparable. The \nfindings are depicted in TABLE III and TABLE IV. \na) Improved Pre-detection Module: This module is a core \ncomponent in Trans-MAD responsible for generating pseudo-\ntraining samples. It utilizes C2VA and IR-MAD algorithm \nsimultaneously to recognize the basic change areas with high \nconfidence from bi-temporal images. To determine the \neffectiveness of IPD module in generating pseudo-training \nsamples, we replaced it with a single pre-detection algorithm \n(i.e., C2VA or IR-MAD). As one can see from the obtained \nexperiment results, compared with the Trans-MAD, the pre-\ndetection module with only C2VA led to a significant decrease \nof Recall values from 0.6830 to 0.2826 on GZ dataset, and from \n0.6427 to 0.5575 on NJ dataset. F1-scores also decreased over \n32% and 7%, respectively, on two datasets. This highlights the \nimportance of the IPD module. Additional experiments using \nIR-MAD-only pre-detection showed the same trend as well. In \nsummary, the superiority of change information capturing and \npseudo-training samples generation ability using an improved \npre-detection module is validated. \nb) Decision Fusion-based CD Module: We also performed \nan ablation analysis to evaluate the DF Module in the Trans-\nMAD. As shown in TABLE III and TABLE IV, methods \nwithout the DF component (w/o DF) exhibited poor \nperformance. On the GZ dataset, OA values decreased from \n0.8116 to 0.7887, Precision values dropped from 0.8592 to \n0.8096, and F1-scores declined from 0.7610 to 0.7383. On the \nNJ dataset, the OA, Precision and F1 values reduced from \n0.7753, 0.7022, and 0.6711 to 0.6721, 0.5493, and 0.4942, \nrespectively. This suggests the DF module can effectively \nreduce CD errors to improve the final CD accuracy, especially \nthe commission errors. \nVI. CONCLUSIONS \nIn this paper, a new unsupervised deep learning change \ndetection method named Trans-MAD was developed. In order \nto achieve unsupervised network training, it performs an \nimproved pre-detection by taking advantages of C2VA and IR-\nMAD, enhancing the quality and diversity of generated pseudo-\ntraining samples. A deep learning-based shallow-to-deep \nfeature extraction scheme is utilized relying on CNN and \nTransformer and integrated with the IR-MAD algorithm. It \nachieves more effective feature expression, enhances change \nrepresentation, as well as reduces CD omissions. In addition, a \ndecision fusion-based CD step is implemented to reduce \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \nTABLE V \nNOTATIONS USED IN THIS PAPER \nSymbol Description \nI Input image \nH Height of image \nW Width of image \nB Number of bands of image \nX Feature extracted by CNN \nT Semantic Token \nQ Query vector \nK Key vector \nV Value vector \ndk Dimension of key \nf Feature extracted by Transformer \na&b Projection vectors used in IR-MAD algorithm \nU&V Linear transformations of original image \nD (U, V) Variance of (U, V) \nr Correlation coefficient \nΣ Variance-covariance matrix \np Degree of freedom of Chi-square distribution \nMADij The ith MAD value of pixel j \nσ Standard deviation \nchi Chi-square distance \nwj Weight of pixel j \nP Probability \nρ Intensity of change \nNtotal Number of pixels of an input image \nNnc Number of no-change sample pixels \nNc Number of change sample pixels \ntnc Threshold for determining no-change pixels \ntc Threshold for determining change pixels \nωnc No-change sample pixels \nωc Change sample pixels \nS Pseudo-training sample pixels set \nC Category of pixel \nm Size of the cropping window \ns Stride of the cropping window \nΩ Pseudo-training sample patches set \nLxy Label patch at position (x, y) \nNs Number of sample sets \nCM Change detection map \ncount Number of CD maps that consider the pixel to be a change \n \ncommission errors, thus improving the overall accuracy of CD. \nThe proposed approach exhibits excellent performance in \ncomplex urban scenarios, which is superior to other advanced \nunsupervised methods. Note that it can better identify diverse \ntypes of changes with different significance levels in the scene, \nrather than only focusing on the most significant changes (e.g., \nbuildings). \nExperiment results obtained on two VHR remote sensing CD \ndatasets confirm the effectiveness of the proposed approach in \nidentifying accurately complex changes over different scenes. \nComprehensive qualitative and quantitative evaluations \ndemonstrated that the proposed Trans-MAD method \noutperforms other compared methods, showing the potential in \nland-cover change detection. Future research will be devoted to \nimproving the adaptability and automation of the pre-detection \nalgorithm. In addition, the possibility of extending the ability of \nTrans-MAD on multi-class change detection is worth further \nstudy. \nREFERENCES \n[1] S. Liu, D. Marinelli, L. Bruzzone, and F. Bovolo, “A Review of Change \nDetection in Multitemporal Hyperspectral Images: Current Techniques, \nApplications, and Challenges,” IEEE Geoscience and Remote Sensing \nMagazine, vol. 7, no. 2, pp. 140–158, Jun. 2019, doi: \n10.1109/MGRS.2019.2898520. \n[2] S. Liu, Q. Du, X. Tong, A. Samat, and L. Bruzzone, “Unsupervised \nChange Detection in Multispectral Remote Sensing Images via Spectral-\nSpatial Band Expansion,” IEEE Journal of Selected Topics in Applied \nEarth Observations and Remote Sensing, vol. 12, no. 9, pp. 3578–3587, \nSep. 2019, doi: 10.1109/JSTARS.2019.2929514. \n[3] S. Liu, F. Bovolo, L. Bruzzone, X. Tong, and Q. Du, “Editorial Foreword \nto the Special Issue on Recent Advances in Multitemporal Remote-\nSensing Data Processing,” IEEE Journal of Selected Topics in Applied \nEarth Observations and Remote Sensing, vol. 15, pp. 776–778, 2022, doi: \n10.1109/JSTARS.2022.3140594. \n[4] D. Lu, P. Mausel, E. Brondízio, and E. Moran, “Change detection \ntechniques,” International Journal of Remote Sensing, vol. 25, no. 12, pp. \n2365–2401, Jun. 2004, doi: 10.1080/0143116031000139863. \n[5] W. A. Malila, “Change vector analysis: An approach for detecting forest \nchanges with Landsat,” in LARS symposia, 1980, p. 385. \n[6] F. Bovolo, S. Marchesi, and L. Bruzzone, “A Framework for Automatic \nand Unsupervised Detection of Multiple Changes in Multitemporal \nImages,” IEEE Transactions on Geoscience and Remote Sensing, vol. 50, \nno. 6, pp. 2196–2212, Jun. 2012, doi: 10.1109/TGRS.2011.2171493. \n[7] S. Liu, L. Bruzzone, F. Bovolo, M. Zanetti, and P. Du, “Sequential \nSpectral Change Vector Analysis for Iteratively Discovering and \nDetecting Multiple Changes in Hyperspectral Images,” IEEE Transactions \non Geoscience and Remote Sensing, vol. 53, no. 8, pp. 4363–4378, Aug. \n2015, doi: 10.1109/TGRS.2015.2396686. \n[8] A. A. Nielsen, K. Conradsen, and J. J. Simpson, “Multivariate Alteration \nDetection (MAD) and MAF Postprocessing in Multispectral, Bitemporal \nImage Data: New Approaches to Change Detection Studies,” Remote \nSensing of Environment, vol. 64, no. 1, pp. 1–19, Apr. 1998, doi: \n10.1016/S0034-4257(97)00162-4. \n[9] A. A. Nielsen, “The Regularized Iteratively Reweighted MAD Method for \nChange Detection in Multi- and Hyperspectral Data,” IEEE Transactions \non Image Processing, vol. 16, no. 2, pp. 463–478, Feb. 2007, doi: \n10.1109/TIP.2006.888195. \n[10] Turgay Celik, “Unsupervised Change Detection in Satellite Images Using \nPrincipal Component Analysis and k-Means Clustering,” IEEE \nGeoscience and Remote Sensing Letters, vol. 6, no. 4, pp. 772–776, Oct. \n2009, doi: 10.1109/LGRS.2009.2025059. \n[11] C. Wu, B. Du, and L. Zhang, “Slow Feature Analysis for Change Detection \nin Multispectral Imagery,” IEEE Transactions on Geoscience and Remote \nSensing, vol. 52, no. 5, pp. 2858–2874, May 2014, doi: \n10.1109/TGRS.2013.2266673. \n[12] S. Liu, Q. Du, X. Tong, A. Samat, L. Bruzzone, and F. Bovolo, “Multiscale \nMorphological Compressed Change Vector Analysis for Unsupervised \nMultiple Change Detection,” IEEE Journal of Selected Topics in Applied \nEarth Observations and Remote Sensing, vol. 10, no. 9, pp. 4124–4137, \nSep. 2017, doi: 10.1109/JSTARS.2017.2712119. \n[13] Y. Feng, S. Liu, and L. Tang, “Automatic extraction and change \nmonitoring of fire disaster event based on high-resolution nighttime light \nremote sensing images,” in Image and Signal Processing for Remote \nSensing XXVI, SPIE, Sep. 2020, pp. 57–65. doi: 10.1117/12.2575804. \n[14] S. Liu, L. Bruzzone, F. Bovolo, and P. Du, “Unsupervised Multitemporal \nSpectral Unmixing for Detecting Multiple Changes in Hyperspectral \nImages,” IEEE Transactions on Geoscience and Remote Sensing, vol. 54, \nno. 5, pp. 2733–2748, May 2016, doi: 10.1109/TGRS.2015.2505183. \n[15] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “An Augmented Linear \nMixing Model to Address Spectral Variability for Hyperspectral \nUnmixing,” IEEE Transactions on Image Processing, vol. 28, no. 4, pp. \n1923–1938, Apr. 2019, doi: 10.1109/TIP.2018.2878958. \n[16] Y. Zheng, S. Liu, Q. Du, H. Zhao, X. Tong, and M. Dalponte, “A Novel \nMultitemporal Deep Fusion Network (MDFN) for Short-Term \nMultitemporal HR Images Classification,” IEEE Journal of Selected \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) < \nTopics in Applied Earth Observations and Remote Sensing, vol. 14, pp. \n10691–10704, 2021, doi: 10.1109/JSTARS.2021.3119942. \n[17] D. Hong et al., “More Diverse Means Better: Multimodal Deep Learning \nMeets Remote-Sensing Imagery Classification,” IEEE Transactions on \nGeoscience and Remote Sensing, vol. 59, no. 5, pp. 4340–4354, May 2021, \ndoi: 10.1109/TGRS.2020.3016820. \n[18] X. Wu, D. Hong, and J. Chanussot, “Convolutional Neural Networks for \nMultimodal Remote Sensing Data Classification,” IEEE Transactions on \nGeoscience and Remote Sensing, vol. 60, pp. 1–10, 2022, doi: \n10.1109/TGRS.2021.3124913. \n[19] S. Liu, H. Zhao, Q. Du, L. Bruzzone, A. Samat, and X. Tong, “Novel \nCross-Resolution Feature-Level Fusion for Joint Classification of \nMultispectral and Panchromatic Remote Sensing Images,” IEEE \nTransactions on Geoscience and Remote Sensing, vol. 60, pp. 1–14, 2022, \ndoi: 10.1109/TGRS.2021.3127710. \n[20] S. Liu et al., “A Shallow-to-Deep Feature Fusion Network for VHR \nRemote Sensing Image Classification,” IEEE Transactions on Geoscience \nand Remote Sensing, vol. 60, pp. 1–13, 2022, doi: \n10.1109/TGRS.2022.3179288. \n[21] H. Zhao et al., “GCFnet: Global Collaborative Fusion Network for \nMultispectral and Panchromatic Image Classification,” IEEE Transactions \non Geoscience and Remote Sensing, vol. 60, pp. 1–14, 2022, doi: \n10.1109/TGRS.2022.3215020. \n[22] C. Li, B. Zhang, D. Hong, J. Yao, and J. Chanussot, “LRR-Net: An \nInterpretable Deep Unfolding Network for Hyperspectral Anomaly \nDetection,” IEEE Transactions on Geoscience and Remote Sensing, vol. \n61, pp. 1–12, 2023, doi: 10.1109/TGRS.2023.3279834. \n[23] D. Hong et al., “Cross-city matters: A multimodal remote sensing \nbenchmark dataset for cross-city semantic segmentation using high-\nresolution domain adaptation networks,” Remote Sensing of Environment, \nvol. 299, p. 113856, Dec. 2023, doi: 10.1016/j.rse.2023.113856. \n[24] R. Caye Daudt, B. Le Saux, and A. Boulch, “Fully Convolutional Siamese \nNetworks for Change Detection,” in 2018 25th IEEE International \nConference on Image Processing (ICIP), Athens: IEEE, Oct. 2018, pp. \n4063–4067. doi: 10.1109/ICIP.2018.8451652. \n[25] D. Peng, Y. Zhang, and H. Guan, “End-to-End Change Detection for High \nResolution Satellite Images Using Improved UNet++,” Remote Sensing, \nvol. 11, no. 11, p. 1382, Jun. 2019, doi: 10.3390/rs11111382. \n[26] H. Chen, Z. Qi, and Z. Shi, “Remote Sensing Image Change Detection \nwith Transformers,” IEEE Transactions on Geoscience and Remote \nSensing, vol. 60, pp. 1–14, 2022, doi: 10.1109/TGRS.2021.3095166. \n[27] M. Gong, X. Niu, P. Zhang, and Z. Li, “Generative Adversarial Networks \nfor Change Detection in Multispectral Imagery,” IEEE Geoscience and \nRemote Sensing Letters, vol. 14, no. 12, pp. 2310–2314, Dec. 2017, doi: \n10.1109/LGRS.2017.2762694. \n[28] S. Saha, F. Bovolo, and L. Bruzzone, “Unsupervised Deep Change Vector \nAnalysis for Multiple-Change Detection in VHR Images,” IEEE \nTransactions on Geoscience and Remote Sensing, vol. 57, no. 6, pp. 3677–\n3693, Jun. 2019, doi: 10.1109/TGRS.2018.2886643. \n[29] H. Chen, C. Wu, B. Du, and L. Zhang, “Deep Siamese Multi-scale \nConvolutional Network for Change Detection in Multi-temporal VHR \nImages,” in 2019 10th International Workshop on the Analysis of \nMultitemporal Remote Sensing Images (MultiTemp), Shanghai, China: \nIEEE, Aug. 2019, pp. 1–4. doi: 10.1109/Multi-Temp.2019.8866947. \n[30] B. Du, L. Ru, C. Wu, and L. Zhang, “Unsupervised Deep Slow Feature \nAnalysis for Change Detection in Multi-Temporal Remote Sensing \nImages,” IEEE Transactions on Geoscience and Remote Sensing, vol. 57, \nno. 12, pp. 9976–9992, Dec. 2019, doi: 10.1109/TGRS.2019.2930682. \n[31] C. Wu, H. Chen, B. Du, and L. Zhang, “Unsupervised Change Detection \nin Multitemporal VHR Images Based on Deep Kernel PCA Convolutional \nMapping Network,” IEEE Transactions on Cybernetics, vol. 52, no. 11, \npp. 12084–12098, Nov. 2022, doi: 10.1109/TCYB.2021.3086884. \n[32] Q. Ke and P. Zhang, “Hybrid-TransCD: A Hybrid Transformer Remote \nSensing Image Change Detection Network via Token Aggregation,” \nISPRS International Journal of Geo-Information, vol. 11, no. 4, p. 263, \nApr. 2022, doi: 10.3390/ijgi11040263. \n[33] Q. Li, R. Zhong, X. Du, and Y. Du, “TransUNetCD: A Hybrid \nTransformer Network for Change Detection in Optical Remote-Sensing \nImages,” IEEE Transactions on Geoscience and Remote Sensing, vol. 60, \npp. 1–19, 2022, doi: 10.1109/TGRS.2022.3169479. \n[34] D. Peng, L. Bruzzone, Y. Zhang, H. Guan, H. Ding, and X. Huang, \n“SemiCDNet: A Semisupervised Convolutional Neural Network for \nChange Detection in High Resolution Remote-Sensing Images,” IEEE \nTransactions on Geoscience and Remote Sensing, vol. 59, no. 7, pp. 5891–\n5906, Jul. 2021, doi: 10.1109/TGRS.2020.3011913. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3349775\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Change detection",
  "concepts": [
    {
      "name": "Change detection",
      "score": 0.7431932687759399
    },
    {
      "name": "Computer science",
      "score": 0.7170604467391968
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5410327911376953
    },
    {
      "name": "Remote sensing",
      "score": 0.4719492495059967
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44712644815444946
    },
    {
      "name": "Multivariate statistics",
      "score": 0.4334867596626282
    },
    {
      "name": "Computer vision",
      "score": 0.4122251272201538
    },
    {
      "name": "Machine learning",
      "score": 0.1564350724220276
    },
    {
      "name": "Geology",
      "score": 0.1047087013721466
    }
  ]
}