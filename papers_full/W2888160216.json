{
  "title": "Learning to Exploit Invariances in Clinical Time-Series Data using Sequence Transformer Networks",
  "url": "https://openalex.org/W2888160216",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287773302",
      "name": "Oh, Jeeheh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2632318657",
      "name": "Wang Jiaxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287133524",
      "name": "Wiens, Jenna",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3101973032",
    "https://openalex.org/W3025292057",
    "https://openalex.org/W2306394264",
    "https://openalex.org/W2964010366",
    "https://openalex.org/W2587019100",
    "https://openalex.org/W2790216347",
    "https://openalex.org/W2177178155",
    "https://openalex.org/W2103106495",
    "https://openalex.org/W2773662615",
    "https://openalex.org/W2962699674",
    "https://openalex.org/W2592495725",
    "https://openalex.org/W2400311702",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2114527042",
    "https://openalex.org/W3101667008",
    "https://openalex.org/W2783824366",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2964312993",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2963078493",
    "https://openalex.org/W2963866024",
    "https://openalex.org/W2106717007"
  ],
  "abstract": "Recently, researchers have started applying convolutional neural networks (CNNs) with one-dimensional convolutions to clinical tasks involving time-series data. This is due, in part, to their computational efficiency, relative to recurrent neural networks and their ability to efficiently exploit certain temporal invariances, (e.g., phase invariance). However, it is well-established that clinical data may exhibit many other types of invariances (e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping) may successfully transform and align inputs, their use often requires one to identify the types of invariances in advance. In contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable architecture that learns to identify and account for invariances in clinical time-series data. Applied to the task of predicting in-hospital mortality, our proposed approach achieves an improvement in the area under the receiver operating characteristic curve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our results suggest that a variety of valuable invariances can be learned directly from the data.",
  "full_text": "Proceedings of Machine Learning Research 85:1–15, 2018 Machine Learning for Healthcare\nLearning to Exploit Invariances in Clinical Time-Series Data\nusing Sequence Transformer Networks\nJeeheh Oh jeeheh@umich.edu Department of Electrical Engineering and Computer Science\nUniversity of Michigan\nAnn Arbor, MI\nJiaxuan Wang jiaxuan@umich.edu Department of Electrical Engineering and Computer Science\nUniversity of Michigan\nAnn Arbor, MI\nJenna Wiens wiensj@umich.edu Department of Electrical Engineering and Computer Science\nUniversity of Michigan\nAnn Arbor, MI\nAbstract\nRecently, researchers have started applying convolutional neural networks (CNNs) with one-\ndimensional convolutions to clinical tasks involving time-series data. This is due, in part,\nto their computational eﬃciency, relative to recurrent neural networks and their ability to\neﬃciently exploit certain temporal invariances, (e.g., phase invariance). However, it is well-\nestablished that clinical data may exhibit many other types of invariances ( e.g., scaling).\nWhile preprocessing techniques, ( e.g., dynamic time warping) may successfully transform\nand align inputs, their use often requires one to identify the types of invariances in advance.\nIn contrast, we propose the use of Sequence Transformer Networks, an end-to-end trainable\narchitecture that learns to identify and account for invariances in clinical time-series data.\nApplied to the task of predicting in-hospital mortality, our proposed approach achieves\nan improvement in the area under the receiver operating characteristic curve (AUROC)\nrelative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our results suggest that\na variety of valuable invariances can be learned directly from the data.\n1. Introduction\nClinical time-series data consist of a wide variety of repeated measurements/observations,\nfrom vitals (e.g., heart rate) and laboratory results to locations within a hospital (Che et al.,\n2018; Lipton et al., 2016; Oh et al., 2018). These data vary not only in the information they\nencode, but also in sampling rate and number of measurements. Analogous to how certain\ntasks in computer vision exhibit spatial invariances, invariances frequently arise in clinical\ntasks involving time-series data. These invariances describe a set of transformations that,\nwhen applied to the data, magnify task-relevant similarities between examples. For example,\nphase invariance relates to a transformation that shifts a signal, resulting in an alignment\nin phase. Such transformations can be particularly useful when processing periodic signals\ne.g., electrocardiogram waveforms (Wiens and Guttag, 2010).\nPreprocessing techniques like dynamic time warping are commonly used to exploit warp-\ning invariances and align time-series data, facilitating relevant comparisons (Liu et al., 2014;\nc⃝2018 J. Oh, J. Wang & J. Wiens.\narXiv:1808.06725v1  [cs.LG]  21 Aug 2018\nSequence Transformer Networks\nOrtiz et al., 2016). However, their computational complexity ( e.g., DTW involves solving\nan optimization problem for each new example) may be a factor leading to their limited\nuse within more general settings. In addition, such approaches require a priori knowledge\nof the types of invariances that are present in one’s data. Due to the varied nature of\nclinical time-series data and their associated prediction tasks, we expect that many such\ntasks involve multiple invariances that may not be known beforehand. This and the fact\nthat these invariances are likely task speciﬁc, are some of the main roadblocks in eﬃciently\nexploiting these invariances.\nTo addresses these challenges, we propose Sequence Transformer Networks, an approach\nfor learning task-speciﬁc invariances related to amplitude, oﬀset, and scale invariances di-\nrectly from the data. Our approach consists of an end-to-end trainable framework designed\nto capture temporal and magnitude invariances. Applied to clinical time-series data, Se-\nquence Transformer Networks learn input- and task-dependent transformations. In contrast\nto data augmentation approaches, our proposed approach makes limited assumptions about\nthe presence of invariances in the data. Learned transformations can be eﬃciently applied\nto new input data, leading to an improvement in overall predictive performance. We demon-\nstrate the utility of the proposed approach in the context of predicting in-hospital mortality\ngiven 48 hours of data collected in the intensive care unit (ICU). Relative to a baseline\nthat does not incorporate any transformations, Sequence Transformer Networks result in\nsigniﬁcant improvements in predictive performance.\nTechnical Signiﬁcance Our technical contributions are as follows: 1) we propose the\nuse of Sequence Transformer Networks, an end-to-end trainable framework designed to\ncapture temporal and magnitude invariances, 2) on a real data task, we evaluate the relative\ncontribution of each individual component of Sequence Transformer Networks towards the\noverall performance of the network and 3) present visualizations of the types of learned\ninvariances and investigate the eﬀects of Sequence Transformer Networks on intra-class\nsignal similarity. This work represents a step toward understanding and learning to exploit\ninvariances in clinical-time series data.\nClinical Relevance To investigate the capability of the proposed approach, we consider\nthe task of predicting in-hospital mortality given clinical time-series data from the ﬁrst 48\nhours of an ICU admission. We chose to focus on this task since it is widely investigated in\nthe machine learning for healthcare literature, facilitating comparisons with state-of-the-art.\nDespite its widespread use as a benchmark task (Harutyunyan et al., 2017) and potential\nclinical use as an estimate of severity of illness, we recognize that a model for predicting in-\nhospital mortality may be of limited clinical utility. Though we consider the improvements\nour proposed approach oﬀers in the context of this benchmark task, we hypothesize that\nour approach applies more broadly to other tasks involving clinical time-series data.\n2. Background & Related Work\nTasks involving time-series data may exhibit a number of diﬀerent invariances. We refer\nthe reader to the following paper for an in-depth discussion of types of invariances present\nin time-series data (Batista et al., 2011), but for completeness include a summary of com-\nmon invariances in Table 1. To exploit these invariances, researchers often turn to neural\n2\nSequence Transformer Networks\nnetworks. In particular, one-dimensional (1D) convolutional neural networks (CNNs), by\ndesign, eﬃciently exploit phase invariance. This property, in addition to their computa-\ntional eﬃciency achieved by weight sharing, has led to their successful application to a\nvariety of tasks involving sequential data (Cui et al., 2016; Wang and Oates, 2015; Gehring\net al., 2017; Dauphin et al., 2017; Yin et al., 2017), and more speciﬁcally clinical time-series\ndata (Razavian and Sontag, 2015; Razavian et al., 2016; Suresh et al., 2017; Bashivan et al.,\n2016). Recognizing that clinical time-series data exhibit other types of invariance, beyond\nphase invariance, we propose augmenting CNNs to explicitly account for task-irrelevant\nvariation.\nIn other domains, to exploit invariances researchers either i) augment their training\ndata by applying a variety of transformations or ii) modify the neural network architec-\nture. The ﬁrst approach is most popular in domains where it is straightforward to generate\nrealistic training examples ( e.g., natural images). Common image invariances include ro-\ntation, scale, translation and warping. Such transformations are easily applied to existing\nimages to create additional, realistic training examples. While less common in the health-\ncare domain, there have been successful examples of data augmentation for health data.\nFor example, Um et al. (2017) augmented multivariate time-series data collected from a\nwearable sensor placed on a person’s wrist in order to improve monitoring of patients with\nParkinson’s disease. The authors applied transformations such as noise and rotations, se-\nlected based on the task. However, in general it is not straightforward to apply such data\naugmentation schemes to clinical data because of the large number of potential invariances.\nMoreover, clinical time-series data extracted from electronic health records often consist\nof high-dimensional data measuring many diﬀerent aspects of a patient’s health. This in-\ncreases the complexity of identifying reasonable transformations and makes a brute-force\nsearch over possible transformations computationally intractable.\nOur work is more in-line with the second approach that does not rely on additional data.\nInstead, the architectures are modiﬁed to exploit a particular invariance (Wang et al., 2012;\nRazavian and Sontag, 2015; Razavian et al., 2016; Forestier et al., 2017; Wang and Oates,\n2015; Cui et al., 2016). For example, in (Razavian and Sontag, 2015) and (Razavian et al.,\n2016), the authors tackle warping by using multiple ﬁlter sizes. More speciﬁcally, three\ndiﬀerent sized ﬁlters were used to capture a range of long- and short-term temporal patterns.\nThese diﬀerent resolutions corresponded to separate convolutional layers, combined at the\nﬁnal fully connected layer. Cui et al. (2016) propose an additional preprocessing step, in\nwhich they resample and smooth their input in order to capture multiscale patterns and\nremove noise. Transformed versions of the inputs were treated as additional channels to\nthe original image. Similar to (Razavian and Sontag, 2015; Razavian et al., 2016), this\nmethod incorporates a local convolution stage that looks at each type of transformation\n(none, smoothing, down-sampling) independently before combining. Both of these works\nare geared toward speciﬁc invariances, in this case scale invariance, and require the user to\ndetermine the diﬀerent ﬁlter sizes or sampling rates.\nRecognizing the diﬃculty in identifying potential invariances or transformation a pri-\nori, we focus on learning the invariances directly from the data. Our proposed approach\nextends work by Jaderberg et al. (2015), in which a spatial transformer network is used to\nlearn spatial invariances directly from the data. In (Jaderberg et al., 2015), the parameters\nof a spatial transformer network are learned jointly with the parameters of a CNN. The\n3\nSequence Transformer Networks\ntransformer network applies a learned set of transformations including aﬃne transforma-\ntions tailored to each input before passing it through a CNN. Since we focus on clinical\ntime-series, and not images, we adapt the set of possible transformations. Speciﬁcally, our\nproposed method tackles amplitude and oﬀset invariances (which we will refer to as magni-\ntude invariance), phase invariance, and uniform scale invariance, and learns input-speciﬁc\ntransformation parameters directly from the data. We describe the details of our approach\nin the next section.\nTable 1: A list of possible invariances summarized from Batista et al. (2011). Any number\nor combination of invariances may arise in clinical time-series data, or time-series data in\ngeneral.\nInvariance Description\nAmplitude\nA transformation of the amplitude of the time series.\nThis can occur when the scale or unit of measurement\nof two time series diﬀers (e.g., temperature in Celsius\nvs. Fahrenheit).\nOﬀset\nA transformation that uniformly increases/decreases\nthe value of a time series. For example, two patients\nmay have diﬀerent resting heart rates.\nLocal Scaling (Warping)\nA transformation that locally stretches or warps the\nduration of the time series. Local warping is often\nreferenced in conjunction with Dynamic Time Warp-\ning (DTW), a good, established measure of similarity\nbetween time series with local scaling invariance.\nUniform Scaling\nA transformation that globally stretches the duration\nof the time series. For example, when resting heart\nrates diﬀer between patients, the progression of the\nsame temporal pattern may be consistently slower in\none patient versus another.\nPhase\nA transformation that shifts the start time of a time\nseries. This occurs in periodic signals such as heart-\nbeat and blood pressure waveforms.\nOcclusion\nA transformation that randomly removes data. This\ncan arise when measurements are irregularly sampled\nor missing.\nNoise\nA transformation that adds or removes noise. For ex-\nample, many single point sensors are susceptible to\nnoise that might not be indicative of the whole body’s\ncondition but indicative of that sensor’s particular lo-\ncation.\n2.1. Problem Setup & Notation\nWe consider the application of 1D CNNs to clinical time-series data for predicting a speciﬁc\noutcome. Formally, given a set of nlabeled examples consisting of dfeatures measured at T\ntime steps (X ∈Rn×d×T) and the outcome labels y∈{0,1}n, our goal is to learn a mapping\n4\nSequence Transformer Networks\nFigure 1: Examples of the types of transformations/invariances that can be learned by a\nSequence Transformer Network applied to a sine wave. θ1: scaling invariance, θ0: phase\ninvariance, φ1: amplitude invariance, φ0: oﬀset invariance. The dashed line represents the\noriginal signal and the blue lines represent potential transformations of the signal. While\nCNNs can eﬃciently exploit phase invariance, Sequence Transformers can augment other\ntypes of architectures facilitating the capture of other types of invariances.\nfrom {x(i)\nt }T\nt=1 to y(i), where x(i)\nt ∈Rd and i ∈{1 ···n}is an index into the ith sample.\nThe d features may consist of both time-varying and time-invariant data. We represent\neach feature as a set of T measurements. For time-varying data for which we do not have a\nmeasurement at time t, we carry forward the most recent value. For time-invariant data, we\ncopy the measurement across all T time-steps as in (Fiterau et al., 2017). Additional details\npertaining to the speciﬁc dataset used through our experiments can be found in Section 4.\n3. Sequence Transformer\nApplied to time-series data, 1D convolutions inherently capture some invariance in the data.\nIn particular, CNNs are capable of eﬃciently handling phase invariance ( i.e., the use of a\nﬁlter slid along the temporal dimension allows for variability in the starting point of temporal\npatterns.) CNNs also handle noise invariance, to a degree. Max pooling coupled with\nmultiple layers allows the model to smooth the inputs and learn higher-level abstractions.\nHowever, there are other types of invariances that we would like to consider, in particular\ntemporal invariance such as scaling, in addition to magnitude invariance related to the\namplitude and oﬀset of the signal. Figure 1 shows examples of these types of invariances\non a sine wave. Due to the inherent diﬀerences between these types of invariances, we\naddress them separately in the two subsections that follow. For simplicity, in this section,\nmethods are presented in terms of a univariate signal, but later our experiments focus on a\nmultivariate application.\n5\nSequence Transformer Networks\n3.1. Temporal Transformations\nTo capture invariance related to warping and scaling, we begin by learning to transform\ndata along the temporal dimension. As in (Jaderberg et al., 2015), this stage consists of\ntwo separate pieces i) learning the transformation parameters and ii) mapping those trans-\nformations in terms of discrete data points. We discuss each, in turn, below.\nTransformation Network. We begin by learning a transformation that takes points\nfrom the original input ( i.e., the source) and maps them to a new temporal location in the\ntarget. Since we only consider linear transformations along the temporal axis, we respect\nthe ordering of values, but can stretch, compress, ﬂip and/or shift the signal (across the\ntemporal axis).\nt= θ(i)\n(t′\n1\n)\n=\n(\nθ(i)\n1 θ(i)\n0\n)(t′\n1\n)\n(1)\nEquation (3.1) gives a mapping between the transformed time point t′and original time\npoint t. Given a univariate time-series {x(i)\nt }T\nt=1, t represents the tth position along the\ntemporal axis of the time-series. We learn a linear temporal transformation θ(i) ∈Rn×2\nthat applies to these indices. Speciﬁcally, we generate t′ for t′ = 1 ,...,T ′. T′ represents\nthe length of the transformed sequence and can be set to any positive integer. Here, for\nconvenience, we set T = T′. The transformation parameters θ(i) are learned via a two-layer\nCNN that is fed inputs {x(i)\nt }T\nt=1. Network architecture details are outlined in Figure 2.\nGiven a particular position, t′, in the target time series, we compute the corresponding\nposition in the original time series and set xt′ to refer to xt=θ1t′+θ0 .\nDiscrete Mapping. Since θ1t′+ θ0 for t′= 1,...,T ′is not guaranteed to map to a positive\ninteger (i.e., an index), we require an additional step to apply the learned transformation.\nWe complete the mapping using linear sampling, in which we take an average over the two\nnearest neighbors (one from left, one from the right) 1 weighted by the distance from the\noriginal transformed point.\n3.2. Magnitude Transformations\nIn order to adapt to amplitude and oﬀset invariance, we propose an additional learned\ntransformation, one that is applied to the values instead of the coordinates. Given the\ntemporally transformed inputs {x(i)\nt′ }\nT\n′\nt′ =1, we apply the following linear transformation:\nx\n′(i)\nt′ = φ(i) ·x(i)\nt′ =\n(\nφ(i)\n1 φ(i)\n0\n)\n·\n(\nx(i)\nt′\n1\n)\n(2)\nThis allows us to shift, ﬂip, stretch, and compress the signal along its magnitude. Since\nthis transformation applies directly to the values of the signal, we do not require a discrete\n1. Signals are padded by the last known value so there is no edge case where a point has only one neighbor.\n6\nSequence Transformer Networks\nConv1d\n8, 7, 1\nPool\n2, 2\nFC\n4\nConv1d\n10, 5, 1\nFC\n32\nPool\n2, 2 ✓\n\u0000\nMagnitude Transformation\nTransformation Network\nFigure 2: The architecture of a Sequence Transformer. Inputs {xt}T\nt=1 (shown as univariate\nfor illustration purpose) are fed into a Transformation Network that outputs transformation\nparameters θ and φ. Convolutional and maxpool layers are annotated with the number\nof outputted channels (omitted for maxpool), ﬁlter size and stride. Fully connected layers\n(FC) are annotated with the number of neurons. The temporal transformation is applied\nvia discrete mapping and the magnitude transformation is applied via linear transformation.\nThe output {x\n′\nt′ }T\n′\nt′ =1 represents the transformed inputs.\nmapping component. It should be noted that the transformation, φ(i) ∈Rn×2 is a function\nof x, thus it can vary from example to example.\n3.3. Sequence Transformer\nWe refer to the temporal transformation combined with the magnitude transformation as\na Sequence Transformer (Figure 2). The Sequence Transformer computes both the θ and\nφtransformation parameters based on the input and applies them to the signal.\nWhile we presented this approach in the context of a univariate signal, the technique\ngeneralizes to multivariate signals. In a multivariate setting, the Transformation Network\noutlined in Figure 2 takes as input {xt}T\nt=1, where xt ∈Rd. The Transformation Network\nthen estimates [θ,φ], based these data and the underlying model parameters. Although the\nmodel parameters are consistent across all examples, the resulting transformation param-\neters (i.e., θ ∈Rn×2 and φ∈Rn×2) are speciﬁc to each example. This transformation is\nthen applied to all signals in the input (note that temporal transformations have no eﬀect\non time-invariant data, but these signals can still be transformed in a meaningful way).\n4. Experimental Setup\nIn this section, we describe our dataset and prediction task, the baseline CNN architecture\nand implementation details.\n4.1. Dataset & Prediction Task\nTo measure the utility of the proposed approach on a real dataset, we consider a standard\nsequence-level classiﬁcation task: predicting in-hospital mortality based on the ﬁrst 48 hours\nof data collected during an intensive care unit visit. We use data from MIMIC III (Johnson\n7\nSequence Transformer Networks\net al., 2016). As in (Harutyunyan et al., 2017), we consider adult admissions with a single,\nunique ICU visit. This excludes patients with transfers between diﬀerent ICUs. Patients\nwithout labels or observations in the ICU were excluded, as were patients who died or were\ndischarged before 48 hours. After applying exclusion criteria, our ﬁnal dataset included\n21,139 patient admissions and 2,797 deaths.\nWe used the same feature extraction procedure as detailed in (Harutyunyan et al., 2017).\nCode to generate these data are publicly available 2. For completeness, we brieﬂy describe\nthe feature extraction process here. For each admission, we extracted 17 features (e.g., heart\nrate, respiratory rate, Glasgow coma scale) from the ﬁrst 48 hours of their ICU visit. We\napplied mean normalization and discretization, resulting in 59 features. Sampling rates were\nset uniformly to once per hour using carry-forward imputation. Mask features, indicating\nif a value had been imputed resulted in an additional 17 features. After preprocessing,\neach example was represented by d = 76 time-series of length T = 48 and a binary label\nindicating whether or not the patient died during the remainder of the hospital visit.\nGiven these data, the goal is to learn a mapping from the features to the probability of\nin-hospital mortality, resulting in a single prediction per patient admission. We measured\nperformance by calculating the area under the receiver operating characteristic curve (AU-\nROC) and the area under the precision recall curve (AUPR). We randomly split the data\ninto training (70%), validation (15%), and testing (15%): 14,681 (1,987 deaths) in training,\n3,222 (436 deaths) in validation and 3,236 (374 deaths) in test. We learned model parame-\nters and selected hyperparameters using training and validation data and evaluated model\nperformance using held-out test data. Speciﬁcs on hyperparameter search are presented in\nSection 4.3. We generated empirical 95% conﬁdence intervals by bootstrapping the test set.\n4.2. Baseline CNN Architecture\nAs a baseline with which to compare, we considered a CNN without any additional Sequence\nTransformer. We compared the discriminative performance of a CNN with original inputs\nto a CNN with inputs transformed via the Sequence Transformer. We referred to the ﬁrst\nmethod as our Baseline CNN. The second is our proposed method: Sequence Transformer\nNetworks. The only diﬀerence between this baseline and our proposed approach is the\nSequence Transformer (Figure 3). Both models feed either the original or transformed\nexample into a standard 1D CNN. For this CNN, we used the two layer CNN described\nin Figure 3. The CNN consists of two 1D convolutional and pooling layers followed by a\nsingle, hidden, fully connected layer.\nIn addition to considering a baseline consisting of no transformations, we also considered\nnetworks that used either i) temporal transformations only or ii) magnitude transformations\nonly. This allowed us to measure the marginal contribution of each transformation in the\nSequence Transformer.\n4.3. Implementation Details\nWe optimized the following hyperparameters: network depth, number of neurons in the\nﬁnal fully connected hidden layer, batch size, and dropout rate. We trained twenty models\n2. https://github.com/YerevaNN/mimic3-benchmarks\n8\nSequence Transformer Networks\nConv1d\n10, 5, 1\nPool\n2, 2\nConv1d\n20, 5, 1\nFC\n50\nPool\n2, 2\nSequence\nTransformer\nCNN\nˆy\nˆy\nConvolutional Neural Network (CNN)\nBaselineProposed\nFigure 3: The architecture of the CNN. Baseline CNN inputs ( {xt}T\nt=1) or Sequence Trans-\nformer inputs ({x\n′\n}T\n′\nt′ =1) are fed into a standard CNN that outputs our in-hospital mortality\nprediction. Here, the admission indexing iis omitted for simplicity. Convolutional and max-\npool layers are annotated with the number of outputted channels (omitted for maxpool),\n1D ﬁlter size and stride. Fully connected layers (FC) are annotated with the number of\nneurons.\nwith randomly selected hyperparameters, for at most 10 epochs. Hyperparameters were\nrandomly chosen from predeﬁned sets of values. Batch size was randomly selected from: 8,\n15, 30. The rate of dropout was randomly selected from: 0, .1, .2, , .9. We tested CNN\narchitectures of depth 2, 3 and 4. Finally, the number of neurons in the ﬁnal fully connected\nhidden layer was randomly chosen from: 50, 100, 250 and 500. The settings that led to the\nbest performance on the validation data are shown in Figure 3.\nSince these hyperparameters were tuned for our Baseline CNN using the original input,\nwe also considered a model tuned to the transformed signal. The resulting optimal hyper-\nparameters were largely unchanged, except that we found that a dropout rate of 0.2 (vs.\n0.3) worked better for Sequence Transformer Networks. The optimal batch size for both\nmodels was 15.\nDuring model training, we included gradient clipping. This consisted of a reduced slope\nfrom 1 to .01 outside of a reasonable range of transformation parameter values. In practice,\nwe set this range to [−2,2]. We found this implementation detail to be important. Without\nit, we witnessed quick increases in the value of the transformation parameters that led to\nunrecoverable model states.\n5. Results\nWe present the performance of the Baseline CNN, which takes as input untransformed sig-\nnals as described in Section 4.2, vs. Sequence Transformer Networks. We further break down\nthe Sequence Transformer into its two parts: temporal and magnitude transformations and\nevaluate their individual contributions. Finally, we investigate the learned transformations\nthrough a series of visualizations and analyze the eﬀect of Sequence Transformer Networks\non intra-class signal similarity.\n9\nSequence Transformer Networks\nTable 2: Test performance for the task of predicting in-hospital mortality. Relative to\nthe baseline performance, transforming the input before feeding it into the CNN results in\nconsistent improvements in both the area under the receiver operating characteristics curve\n(AUROC) and the area under the precision recall curve (AUPR).\nMethod AUROC (95% CI) AUPR (95% CI)\nBaseline CNN 0.838 (0.820, 0.859) 0.445 (0.393, 0.495)\nSequence Transformer Networks 0.851 (0.833, 0.871) 0.476 (0.424, 0.527)\nTemporal Transformations Only 0.846 (0.827, 0.867) 0.452 (0.393, 0.500)\nMagnitude Transformations Only 0.846 (0.826, 0.867) 0.463 (0.408, 0.516)\n(a)\n (b)\nFigure 4: Sequence Transformer Networks: Temporal Transformations Only. (a) Visual-\nization of temporal transformation parameters applied to the test set. Note that θ1 ≥0\nindicates signal compression, while θ0 ≤0 indicates shifting the signal forward in time. (b)\nA random test patient’s normalized diastolic blood pressure before and after θtransforma-\ntion (θ1 = 1.19, θ0 = −0.03). In addition to signal compression and shifting, the network\nsmooths the signal.\n5.1. CNN Baseline vs Sequence Transformer Networks\nOur proposed method, Sequence Transformer Networks, outperforms the Baseline CNN, in\nterms of both AUROC and AUPR, on the task of predicting in-hospital mortality using\ndata from the ﬁrst 48 hours (Table 2).\nCompared to the Baseline CNN, Sequence Transformer Networks incorporates a sec-\nondary, transformation network. However, the improvement in performance is not due to\nthe additional complexity of the model. For both models, we tuned the depth of the CNN\narchitecture. In both cases, the best CNN, determined by validation performance and pre-\nsented in the results, had a network depth of 2. Therefore a deeper network alone is not\nsuﬃcient for increasing performance.\nSince the Sequence Transformer consists of two transformations, we further break down\nthe performance increase into: temporal transformations and magnitude transformations.\nIn Table 2, we see that both types of transformations lead to marginal improvements over the\n10\nSequence Transformer Networks\n(a)\n (b)\nFigure 5: Sequence Transformer Networks: Magnitude Transformations Only. (a) Visual-\nization of magnitude transformation parameters applied to the test set. Note that φ1 ≤1\nindicates signal value compression, while φ0 ≤0 indicates a downward shift. (b) A ran-\ndom test patient’s normalized diastolic blood pressure before and after φ transformation\n(φ1 = 0.78, φ0 = −0.04).\nbaseline. Moreover, their combination appears to be complementary, though the diﬀerence\nis small.\n5.2. Learned Temporal and Magnitude Transformations\nIn this section, we qualitatively explore what the Sequence Transformer has learned. Fig-\nure 4 summarizes the transformation learned using a network that employs only temporal\ntransformations. Recall that the transformation depends on the input. Figure 4a shows the\nempirical distribution of the two temporal transformation parameters ( θ1, θ0). Each point\nrepresents a temporal transformation learned for a speciﬁc patient admission in the test\nset. In this case, most of the data occur around θ1 = 1.19 and θ0 = −0.03. Essentially, the\nnetwork learns to compress the original signal ( θ1 ≥1) and shift the signal forward in time\n(θ0 ≤0) by various degrees. In doing so, the network learns how to align the time-series\ndata from diﬀerent patient admissions. Figure 4b shows the original and the temporally\ntransformed normalized diastolic blood pressure for a randomly selected patient in the test\nset. In line with the results shown in the previous ﬁgure, the signal is compressed along\nthe x-axis and shifted forward in time. In Figure 4b, though the signal is moved forward\nin time, it is not clipped, but rather compressed. This suggests that θ0 is helping to center\nthe signals. The sudden drop oﬀ at θ1 = 2 is most likely due to the gradient clipping, since\nthat is where it begins to take eﬀect. In addition, we observe a smoothing eﬀect that is\ndue, in part, to the the linear interpolation.\nFigure 5, shows the same type of plots as Figure 4 but for a network that includes only\nmagnitude transformations. We observe that the signal is, on average, shifted down and\ncompressed. Similar to the temporal transformations, the magnitude transformations help\nalign signals. Amplitude and oﬀset invariances have a clinical signiﬁcance for many features\n11\nSequence Transformer Networks\n(a)\n (b)\n (c)\nFigure 6: Sequence Transformer Networks (a) Visualizations of temporal transformation\nparameters applied to the test set. On average, network compresses and shifts signals\nbackwards in time. (b) Visualizations of magnitude transformation parameters applied to\nthe test set. On average signal values are compressed and shifted down. (c) The learned\nnetwork smooths and shifts the normalized diastolic blood pressure to the left bottom\ndirection of the frame for a randomly selected patient using the transformations: θ1 = 1.33,\nθ0 = 0.14, φ1 = 0.86 and φ0 = −0.05.\nin this dataset including blood pressure, heart rate, respiratory rate and temperature. We\nhypothesize that these transformations help account for diﬀerent physiological baselines.\nFinally, we visualize the output of the Sequence Transformer, which learns temporal,\namplitude and oﬀset invariances together (Figure 6). In Figures 6a and 6b, each point\nrepresents a transformation learned for a speciﬁc patient in the test set. We see that\nthe network, on average, compresses the signal and shifts it slightly back in time. In\nthe temporal transformation only network (Figure 4), the network shifted signals forward\nin time. This suggests that the direction of the shift is less important than the overall\nalignment of the diﬀerent patients. For magnitude transformations, the network on average\ncompresses the signal and shifts it down. These learned transformation trends align with the\nmagnitude transformation trends learned separately (Figure 5). In Figure 6c we illustrate\nthe transformations applied to a random test patient’s normalized diastolic blood pressure.\n5.3. Increasing Intra-Class Similarity\nSequence Transformer Networks have the ability to learn transformations that reduce label\nindependent variations in the signal. By reducing irrelevant variance, transformed signals\nfrom patients with similar outcomes then appear more similar. We investigate this property\nby analyzing the intra-class Euclidean pairwise distance. On each dataset (original vs.\ntransformed), we calculated the Euclidean pairwise distance between admissions labeled\npositive and the Euclidean pairwise distance between those labeled negative.\nThe transformed dataset had on average lower pairwise intra-class distances compared to\nthe original (untransformed) data (positive: 28.2 vs. 34.9 and negative: 26.3 vs. 31.8). We\nhypothesize that this increase in intra-class similarity contributes to the overall improved\ndiscriminative performance of the Sequential Transformer Network over the Baseline CNN.\n12\nSequence Transformer Networks\n6. Conclusion\nIn this paper, we proposed the use of an end-to-end trainable method for exploiting in-\nvariances in clinical time-series data. Building oﬀ of ideas ﬁrst presented in the context of\ntransforming images, we extended the capabilities of CNNs to capture temporal, amplitude,\nand shift invariances. In general, such invariances may be task dependent (i.e., may depend\non the outcome of interest or the population studied). Given the large number of possible\nclinical tasks, techniques that automatically learn to exploit invariances based on the data\nhave a clear advantage over preprocessing techniques.\nWe demonstrated that this method leads to improved discriminative performance over\nthe Baseline CNN, on the task of predicting in-hospital-morality from multivariate clinical\ntime-series data collected during the ﬁrst 48 hours of an ICU admission. Though the\ndiﬀerence in performance is small, the improvement is evident across both AUROC and\nAUPR.\nThe proposed approach is not without limitation. More speciﬁcally, in its current form\nthe Sequence Transformer applies the same transformation across all features within an\nexample, instead of learning feature-speciﬁc transformations. Despite this limitation, the\nlearned transformations still lead to an increase in intra-class similarity. In conclusion,\nwe are encouraged by these preliminary results. Overall, this work represents a starting\npoint on which others can build. In particular, we hypothesize that the ability to capture\nlocal invariances and feature-speciﬁc invariances could lead to further improvements in\nperformance.\nAcknowledgments\nThis work was supported by the National Science Foundation (NSF award no. IIS-1553146)\nand the National Institute of Allergy and Infectious Diseases of the National Institutes of\nHealth (grant no. U01AI124255). The views and conclusions in this document are those of\nthe authors and should not be interpreted as necessarily representing the oﬃcial policies,\neither expressed or implied, of the National Science Foundation nor the National Institute\nof Allergy and Infectious Diseases of the National Institutes of Health.\nReferences\nPouya Bashivan, Irina Rish, Mohammed Yeasin, and Noel Codella. Learning representations\nfrom EEG with deep recurrent-convolutional neural networks. International Conference\non Learning Representations (ICLR), 2016.\nGustavo EAPA Batista, Xiaoyue Wang, and Eamonn J Keogh. A complexity-invariant dis-\ntance measure for time series. In Proceedings of the 2011 SIAM International Conference\non Data Mining (SDM) , pages 699–710. SIAM, 2011.\nZhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recur-\nrent neural networks for multivariate time series with missing values. Scientiﬁc Reports,\n8(1):6085, 2018.\n13\nSequence Transformer Networks\nZhicheng Cui, Wenlin Chen, and Yixin Chen. Multi-scale convolutional neural networks for\ntime series classiﬁcation. arXiv preprint arXiv:1603.06995 , 2016.\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\ngated convolutional networks. International Conference on Machine Learning (ICML) ,\n2017.\nMadalina Fiterau, Suvrat Bhooshan, Jason Fries, Charles Bournhonesque, Jennifer Hicks,\nEni Halilaj, Christopher R´ e, and Scott Delp. Shortfuse: biomedical time series repre-\nsentations in the presence of structured information. Machine Learning for Healthcare\nConference (MLHC), 2017.\nGermain Forestier, Fran¸ cois Petitjean, Hoang Anh Dau, Geoﬀrey I Webb, and Eamonn\nKeogh. Generating synthetic time series to augment sparse datasets. In IEEE Interna-\ntional Conference on Data Mining (ICDM) , pages 865–870. IEEE, 2017.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Con-\nvolutional sequence to sequence learning. International Conference on Machine Learning\n(ICML), 2017.\nHrayr Harutyunyan, Hrant Khachatrian, David C Kale, and Aram Galstyan. Mul-\ntitask learning and benchmarking with clinical time series data. arXiv preprint\narXiv:1703.07771, 2017.\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks.\nIn Advances in Neural Information Processing Systems (NIPS) , pages 2017–2025, 2015.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.\nMIMIC-III, a freely accessible critical care database. Scientiﬁc Data, 3:160035, 2016.\nZachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzel. Learning to diagnose\nwith lstm recurrent neural networks. International Conference on Learning Representa-\ntions (ICLR), 2016.\nYun Liu, Zeeshan Syed, Benjamin M Scirica, David A Morrow, John V Guttag, and Collin M\nStultz. ECG morphological variability in beat space for risk stratiﬁcation after acute\ncoronary syndrome. Journal of the American Heart Association (JAHA) , 3(3):e000981,\n2014.\nJeeheh Oh, Maggie Makar, Christopher Fusco, Robert McCaﬀrey, Krishna Rao, Erin E\nRyan, Laraine Washer, Lauren R West, Vincent B Young, John Guttag, et al. A gen-\neralizable, data-driven approach to predict daily risk of clostridium diﬃcile infection at\ntwo large academic health centers. Infection Control & Hospital Epidemiology (ICHE) ,\n39(4):425–433, 2018.\nJos´ e Javier Gonz´ alez Ortiz, Cheng Perng Phoo, and Jenna Wiens. Heart sound classiﬁcation\nbased on temporal alignment techniques. InComputing in Cardiology Conference (CinC),\n2016, pages 589–592. IEEE, 2016.\n14\nSequence Transformer Networks\nNarges Razavian and David Sontag. Temporal convolutional neural networks for diagnosis\nfrom lab tests. arXiv preprint arXiv:1511.07938 , 2015.\nNarges Razavian, Jake Marcus, and David Sontag. Multi-task prediction of disease on-\nsets from longitudinal laboratory tests. In Machine Learning for Healthcare Conference\n(MLHC), pages 73–100, 2016.\nHarini Suresh, Nathan Hunt, Alistair Johnson, Leo Anthony Celi, Peter Szolovits, and\nMarzyeh Ghassemi. Clinical intervention prediction and understanding with deep neural\nnetworks. In Machine Learning for Healthcare Conference (MLHC), pages 322–337, 2017.\nTerry T Um, Franz MJ Pﬁster, Daniel Pichler, Satoshi Endo, Muriel Lang, Sandra Hirche,\nUrban Fietzek, and Dana Kuli´ c. Data augmentation of wearable sensor data for parkin-\nsons disease monitoring using convolutional neural networks. In Proceedings of the 19th\nACM International Conference on Multimodal Interaction (ICMI), pages 216–220. ACM,\n2017.\nFei Wang, Noah Lee, Jianying Hu, Jimeng Sun, and Shahram Ebadollahi. Towards heteroge-\nneous temporal clinical event pattern discovery: a convolutional approach. In Proceedings\nof the 18th International Conference on Knowledge Discovery and Data Mining (KDD) ,\npages 453–461. ACM, 2012.\nZhiguang Wang and Tim Oates. Imaging time-series to improve classiﬁcation and impu-\ntation. Proceedings of the 24th International Join Conference on Artiﬁcial Intelligence\n(IJCAI), 2015.\nJenna Wiens and John V Guttag. Active learning applied to patient-adaptive heartbeat\nclassiﬁcation. In Advances in Neural Information Processing Systems (NIPS), pages 2442–\n2450, 2010.\nWenpeng Yin, Katharina Kann, Mo Yu, and Hinrich Sch¨ utze. Comparative study of CNN\nand RNN for natural language processing. arXiv preprint arXiv:1702.01923 , 2017.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7388709783554077
    },
    {
      "name": "Exploit",
      "score": 0.6722785234451294
    },
    {
      "name": "Dynamic time warping",
      "score": 0.6534113883972168
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5750192403793335
    },
    {
      "name": "Preprocessor",
      "score": 0.5230796933174133
    },
    {
      "name": "Transformer",
      "score": 0.4843917489051819
    },
    {
      "name": "Recurrent neural network",
      "score": 0.47782161831855774
    },
    {
      "name": "Deep learning",
      "score": 0.4416678547859192
    },
    {
      "name": "Artificial neural network",
      "score": 0.4378047287464142
    },
    {
      "name": "Time series",
      "score": 0.4348732829093933
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4186370372772217
    },
    {
      "name": "Machine learning",
      "score": 0.40980032086372375
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40262123942375183
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}