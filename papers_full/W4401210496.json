{
    "title": "Large Language Models and Rule-Based Approaches in Domain-Specific Communication",
    "url": "https://openalex.org/W4401210496",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2611005778",
            "name": "Dominik Halvon√≠k",
            "affiliations": [
                "Constantine the Philosopher University in Nitra"
            ]
        },
        {
            "id": "https://openalex.org/A2021409264",
            "name": "Jozef Kapusta",
            "affiliations": [
                "Constantine the Philosopher University in Nitra"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963270767",
        "https://openalex.org/W2908561682",
        "https://openalex.org/W4210652344",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3205869033",
        "https://openalex.org/W4327917888",
        "https://openalex.org/W6793032698",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3118866989",
        "https://openalex.org/W3045755937",
        "https://openalex.org/W3173752806",
        "https://openalex.org/W2977128309",
        "https://openalex.org/W2843010082",
        "https://openalex.org/W3171922886",
        "https://openalex.org/W3198958630",
        "https://openalex.org/W3039973605",
        "https://openalex.org/W47495561",
        "https://openalex.org/W3011267247",
        "https://openalex.org/W4390751769",
        "https://openalex.org/W4389989131",
        "https://openalex.org/W4400397477",
        "https://openalex.org/W2906130987",
        "https://openalex.org/W2966087730",
        "https://openalex.org/W2786860129",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W2947017203",
        "https://openalex.org/W4377291971",
        "https://openalex.org/W2257111739",
        "https://openalex.org/W2550389024",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W3138895808"
    ],
    "abstract": "Currently, we are once again experiencing a frenzy related to artificial intelligence. Generative Pre-trained Transformers (GPT) models are highly effective at various natural language processing tasks. Different varieties of GPT models are widely used these days to improve productivity. Graphic departments generate art designs, developers engineer intricate software solutions, leveraging services predicated on the GPT framework, and many other industries are also following the lead and implementing these new sets of tools in their workflow. However, there are areas in natural language processing where a simple solution is often more suitable and effective than current Large Language Models. In this article, we decided to analyze and compare the practical use of one of the more popular GPT solutions, J-Large, and the simple rule-based model we implemented. We integrated these two models into the internal information system of a private company focused on communication with customers in the gaming industry. Both models were trained on the same dataset provided as a log of conversational interactions for the last two years in the given system. We observed that GPT models exhibited superior performance in terms of comprehensibility and adequacy. The rule-based models showed noticeable proficiency in handling domain-specific tasks, mainly when fed with datasets extracted from the historical communication between users and a specialized domain system, such as a customer care department. As a result, with a sufficiently tailored and specific dataset at their disposal, rule-based models can effectively outpace GPT models in performing domain-specific tasks.",
    "full_text": null
}