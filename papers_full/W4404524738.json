{
  "title": "Evaluation of Entry-Level Open-Source Large Language Models for Information Extraction from Digitized Documents",
  "url": "https://openalex.org/W4404524738",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Francisco Clerton Almeida",
      "affiliations": [
        "Unifor"
      ]
    },
    {
      "id": "https://openalex.org/A2128670623",
      "name": "Carlos Caminha",
      "affiliations": [
        "Unifor"
      ]
    },
    {
      "id": null,
      "name": "Francisco Clerton Almeida",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128670623",
      "name": "Carlos Caminha",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116508771",
    "https://openalex.org/W3098281022",
    "https://openalex.org/W4387338911",
    "https://openalex.org/W6826116265",
    "https://openalex.org/W3212252817",
    "https://openalex.org/W3049596049",
    "https://openalex.org/W4403251837",
    "https://openalex.org/W2964864162",
    "https://openalex.org/W4283459184",
    "https://openalex.org/W6600195515"
  ],
  "abstract": "The rise of Large Language Models (LLMs) has transformed the field of natural language processing (NLP), offering a wide range of proprietary and open-source models varying significantly in size and complexity, often measured by billions of parameters. While larger models excel in complex tasks like summarization and creative text generation, smaller models are suited for simpler tasks such as document classification and information extraction from unstructured data. This study evaluates open-source LLMs, specifically those with 7 to 14 billion parameters, in the task of extracting information from OCR texts of digitized documents. The effectiveness of OCR can be influenced by factors such as skewed images and blurred photos, resulting in unstructured text with various issues. The utility of these models is highlighted in Intelligent Process Automation (IPA), where software robots partially replace humans in validating and extracting information, enhancing efficiency and accuracy. The documents used in this research, provided by a state treasury department in Brazil, comprise personal verification documents. Results show that open-source entry-level models perform 18% lower than a cutting-edge proprietary model with trillions of parameters, making them viable free alternatives.",
  "full_text": "Evaluation of Entry-Level Open-Source Large Language\nModels for Information Extraction from Digitized Documents\nFrancisco Clerton Almeida1, Carlos Caminha1,2\n1 Programa de Pós Graduação em Informática Aplicada - PPGIA, Unifor, Brasil\nclertonjradv@edu.unifor.br\n2 Universidade Federal do Ceará, UFC, Brasil\ncaminha@ufc.br\nAbstract. The rise of Large Language Models (LLMs) has transformed the field of natural language processing (NLP),\noffering a wide range of proprietary andopen-source models varying significantly in size and complexity, often measured\nby billions of parameters. While larger models excel in complex tasks like summarization and creative text generation,\nsmaller models are suited for simpler tasks such as document classification and information extraction from unstructured\ndata. This study evaluatesopen-source LLMs, specifically those with 7 to 14 billion parameters, in the task of extracting\ninformation from OCR texts of digitized documents. The effectiveness of OCR can be influenced by factors such as\nskewed images and blurred photos, resulting in unstructured text with various issues. The utility of these models is\nhighlighted in Intelligent Process Automation (IPA), where software robots partially replace humans in validating and\nextracting information, enhancing efficiency and accuracy. The documents used in this research, provided by a state\ntreasury department in Brazil, comprise personal verification documents. Results show thatopen-source entry-level\nmodels perform 18% lower than a cutting-edge proprietary model with trillions of parameters, making them viable free\nalternatives.\nCCS Concepts: • Computing methodologies→ Machine learning algorithms.\nKeywords: Natural Language Processing, Large Language Models, Information Extraction.\n1. INTRODUÇÃO\nAascensãodosModelosdeLinguagemdeLargaEscala, eminglês Large Language Models(LLMs), tem\nrevolucionado a área de processamento de linguagem natural (PLN), oferecendo uma vasta gama de\nmodelosproprietáriosedecódigoaberto[Minaeeet al.2024]. Essesmodelosvariamsignificativamente\nem termos de tamanho e complexidade, comumente medidos pela quantidade de parâmetros, que\nfrequentemente chegam à casa dos bilhões. Modelos maiores e de código fechado, como GPT, Claude,\ne Gemini, têm se destacado em tarefas complexas, como sumarização, tradução automática, e geração\nde texto criativo. Por outro lado, modelos menores são geralmente indicados para tarefas mais simples,\ncomo geração de conjuntos de dados sintéticos [Silva et al. 2024], classificação de documentos [Martins\nand Silva 2021] e extração de informações [Cardoso and Pereira 2020] em dados não estruturados.\nNacategoriademodeloscomaproximadamente7bilhõesdeparâmetros, encontramosváriasfamílias\nnotáveis, incluindo Mistral, LLama, Starling, e Zephyr. Alguns desses modelos têm demonstrado\ndesempenho comparável a modelos de código fechado muito maiores em diversas tarefas, oferecendo\numa alternativa eficiente e economicamente viável.\nAtarefade extraçãodeinformaçãodedocumentos digitalizadosenvolveumpassopreliminar crucial:\na realização do OCR (Optical Character Recognition). A eficácia do OCR pode variar dependendo\nda qualidade do algoritmo utilizado e da digitalização do documento. Exemplos de desafios comuns\nincluem imagens inclinadas, fotos distantes ou borradas. Esses fatores podem resultar em texto não\nestruturado com problemas, como palavras omitidas, palavras adicionais, ou palavras agrupadas (por\nexemplo, “palavraa mais” ao invés de “palavra a mais”).\nAutilidadedemodelosdeextraçãodeinformaçãoemdocumentosdigitalizadoséevidenteemtarefas\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n2 ·\nde Automação Inteligente de Processos [Chakraborti et al. 2020]. Nessas tarefas, os robôs desoftware\nassumem a responsabilidade de validar e extrair informações (sendo supervisionados por humanos),\nsubstituindo a necessidade de intervenção completamente manual das pessoas. Isso não só aumenta\na eficiência e a precisão dos processos, mas também libera os recursos humanos para se concentrarem\nem atividades mais estratégicas e menos repetitivas, promovendo uma maior produtividade e redução\nde erros humanos. Muitas organizações, tanto privadas quanto públicas, precisam validar documentos\nsubmetidos a seus processos internos, e a qualidade de digitalização desses documentos muitas vezes\ndificulta a análise rápida e eficiente.\nEste artigo foca na avaliação de LLMs de código aberto, especificamente aqueles com entre 7 e\n14 bilhões de parâmetros, na tarefa de extração de informações a partir de textos OCR de imagens\nde baixa qualidade de documentos. O objetivo desta tarefa é, dado um documento OCR, extrair\num JSON (JavaScript Object Notation) de informações relevantes e estruturadas. Os documentos\nutilizados nesta pesquisa foram fornecidos pela Secretaria da Fazenda do Estado do Ceará e incluem\ndocumentos pessoais comprobatórios. Ao todo, foram processados 326 documentos, de seis tipos\ndiferentes, totalizando 678.294tokens.\nOs resultados mostram que os modelos de código aberto de entrada têm desempenho geral 19%\ninferior (para alguns tipos de documentos houve um empate técnico, com menos de 5% de diferença)\na um modelo proprietário de ponta, com trilhões de parâmetros. Isso os posiciona como alternativas\ngratuitasrelevantesparaaexecuçãodestatarefa. Essesresultadossugeremapossibilidadeinteressante\nde refinar um modelo de código aberto de entrada com previsões de um modelo maior, um processo\nconhecido como destilação de modelos. Tal abordagem poderia transformar o modelo de entrada em\num especialista nessa tarefa, alcançando desempenho similar ao modelo de maior porte.\nEste artigo está organizado da seguinte forma: na Seção 2, apresentamos uma revisão do estado\nda arte, discutindo as técnicas iniciais e os avanços recentes em extração de informações, com ênfase\nno uso de LLMs para essa finalidade. A Seção 3 descreve a metodologia adotada, incluindo detalhes\nsobre o conjunto de dados utilizado, os modelos de linguagem avaliados, o processo de criação dos\nprompts utilizandofew-shot learning, oambientedeexecuçãodosexperimentoseamétricadeavaliação\nempregada. Na Seção 4, apresentamos os resultados obtidos, com uma análise detalhada da acurácia\ndosmodelosparadiferentestiposdedocumentos. ASeção5discuteosachadosprincipais, comparando\no desempenho dos modelos de código aberto com o modelo proprietário GPT-4o, e explorando as\nimplicações práticas e as possibilidades de refinamento dos modelos. Por fim, a Seção 6 conclui o\nartigo destacando as principais contribuições, limitações do estudo e sugestões para trabalhos futuros.\n2. ESTADO DA ARTE\nA extração de informação tem sido um campo essencial dentro da ciência de dados e Processamento de\nLinguagem Natural (PLN). Inicialmente, técnicas como OCR eram empregadas para converter textos\nem imagens digitalizadas para texto editável. Simultaneamente, expressões regulares (regex) eram\namplamenteutilizadasparaidentificarpadrõesemtextos, permitindoaextraçãodedadosestruturados\na partir de documentos não estruturados. Um exemplo dessa aplicação é demonstrado no trabalho\nde Papadopoulos et al., onde eles utilizamregex para identificar e extrair informações específicas de\ngrandes corpos científicos [Papadopoulos et al. 2020]. Avanços subsequentes trouxeram o uso de\nReconhecimento de Entidades Nomeadas, que permitiu a identificação de nomes de pessoas, lugares e\norganizações em textos, melhorando a precisão da extração de informação [Weston et al. 2019].\nA ascensão dos Modelos de Linguagem de Larga Escala revolucionou o campo da extração de infor-\nmação. Na prática, LLM de diferentes tamanhos e arquiteturas têm sido aplicados com sucesso em\ntarefas variadas, como a tradução automática, resposta a perguntas e sumarização de textos. Especi-\nficamente para extração de informação, estudos têm demonstrado a eficácia dos LLMs. Por exemplo,\nZhang et al. utilizam modelos de linguagem para extrair e localizar informações em documentos\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n· 3\nFigura 1:Exemplo de documento digitalizado e as informações a serem extraídas.Em a) é possível observar\na imagem do documento, rotacionada e borrada o que dificulta a realização do OCR. Em b) é possível observar o\ntexto detectado pelo algoritmo de OCR, observam-se falhas, como: detecção de caracteres errados; palavras faltantes;\ne palavras juntas. Em c) é possível observar o JSON que deseja-se extrair com o LLM. Informações pessoais foram\nomitidas das imagens para preservar a privacidade desses indivíduos.\ncomplexos, enquanto outra pesquisa aplicou um LLM para a síntese de evidências a partir de grandes\nvolumes de dados textuais, mostrando uma significativa melhoria na precisão e eficiência da extração\nde informações [Zhang et al. 2023; Gartlehner et al. 2023].\nPara a extração de informação em documentos digitalizados, o uso de LLMs também tem mostrado\navanços promissores. Estudos destacaram como o ajuste fino de LLMs pode melhorar a extração de\ninformações estruturadas de textos científicos complexos, evidenciando o impacto dessas tecnologias\nna pesquisa e análise de dados [Townsend et al. 2023].\n3. METODOLOGIA\n3.1 Conjunto de Dados\nOs dados utilizados neste estudo consistem em documentos digitalizados enviados pelo contribuinte\nà Secretaria da Fazenda do Estado do Ceará (SEFAZ-CE) para solicitações de isenção fiscal. Esses\ndocumentos passaram por um processo interno de extração de texto utilizando a tecnologia de OCR\nTesseract 5.3.1.2. O resultado inclui textos OCR diretamente associados às imagens escaneadas cor-\nrespondentes. A Figura 1 ilustra a imagem de um dos documentos utilizados nesta pesquisa (Figura\n1 a) e o texto detectado automaticamente pela biblioteca de OCR (Figura 1 b).\nSeis tipos de documentos foram utilizados, cada um com suas próprias informações e características\nespecíficas. A Tabela I ilustra os tipos de documentos, as informações a serem extraídas, quantidade de\ndocumentos (N), média detokens por documento (µtokens) e somatório detokens (Σtokens). Ao todo,\nforam processados 326 documentos, de seis tipos diferentes, totalizando 678.294tokens. Os tipos de\ndocumentos utilizados nesta pesquisa foram: Comprovante de endereço; Laudo Médico; Requerimento\nde isenção de ICMS; Requerimento de isenção de IPI; Procuração; e Declaração de Concessionária. Os\natributos a serem extraídos foram definidos por auditores da SEFAZ, com base na sua experiência na\nanálise desses tipos de documentos, esses profissionais elencaram os atributos que são mais importantes\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n4 ·\npara validar os pedidos de isenção fiscal solicitados pelo contribuinte.\nDocumento Informações N µtokens Σtokens\nComp. Endereço Nome e Endereço 40 2334,45 93.378\nLaudo Médico Nome, CPF e CID 47 2374,19 111.537\nReq. ICMS Nome, Fundamento, Assunto e CPF 85 1750,35 150.530\nReq. IPI Nome, CPF, Data de Transmissão e Protocolo 80 2249,97 179.998\nProcuração Nome e CPF do Outorgante, Nome e CPF do Outorgado 46 1443,25 40.431\nConcessionária Modelo do automóvel, Placa, Valor e Valor com isenção 28 2226,52 102.420\n326 - 678.294\nTabela I: Informações a serem extraídas por tipo de documento e contagem detokens dos documentos por tipo.\nFoi construída uma coleção de referência a partir da rotulagem de todos os documentos utilizados\nnesta pesquisa. Auditores da SEFAZ participaram do processo, criando para cada documento um\narquivo JSON com as informações que deveriam ser detectadas. Um exemplo dessa rotulagem pode\nser observado na Figura 1 c, onde a partir do documento ilustrado na Figura 1 a, foi construído o JSON\ncorrespondente. Cada documento foi revisado manualmente para garantir a precisão das informações,\nresultando em uma coleção de referência para comparação com os resultados dos modelos.\n3.2 LLMs utilizados\nA seguir, são descritos os LLMs utilizados neste estudo:\n—CapybaraHermes-2.5-Mistral-7B: É conhecido por seu desempenho equilibrado em tarefas de\ncompreensão e geração de texto. A arquitetura Hermes foca na eficiência computacional, enquanto\no Mistral é otimizado para tarefas de NLP complexas.\n—Firefly-Llama2-13B-v1.2: Firefly é uma versão ajustada do Llama2 com 13 bilhões de parâ-\nmetros, projetada para melhor desempenho em tarefas de extração de informações específicas. A\nversão 1.2 inclui melhorias na precisão e velocidade de processamento, tornando-o adequado para\naplicações que exigem respostas rápidas e precisas.\n—Laser-Dolphin-Mixtral-2x7B-DPO: Uma combinação inovadora de dois modelos de 7 bilhões\nde parâmetros (Mixtral), utilizando a técnica Dolphin para aprimorar a precisão em tarefas de\nprocessamento de linguagem.\n—Llama-2-13B-Chat: Uma versão específica do Llama-2 projetada para interação conversacional.\nCom 13 bilhões de parâmetros, é adaptada para gerar respostas mais naturais e contextualizadas,\nsendo particularmente eficaz em tarefas que requerem entendimento detalhado do contexto.\n—LLama-Pro-8B-Instruct: Este modelo de 8 bilhões de parâmetros é ajustado para tarefas de\ninstrução e extração de dados, proporcionando um equilíbrio entre desempenho e eficiência. É\notimizado para seguir instruções específicas, o que o torna ideal para aplicações estruturadas de\nextração de informações.\n—Meta-Llama-3-8B-Instruct-hf: Meta-Llama-3 é a terceira geração da família de LLMs Llama,\ncom 8 bilhões de parâmetros e ajustes para tarefas de instrução.\n—Mistral-7B-Instruct-v0.2: Uma versão otimizada do modelo Mistral de 7 bilhões de parâmetros,\nprojetada para tarefas de instrução com a versão 0.2 incluindo refinamentos na precisão da extração.\nEste modelo é eficaz em tarefas que exigem compreensão detalhada e instruções claras.\n—Starling-LM-7B-beta: O modelo Starling de 7 bilhões de parâmetros é uma versão beta, focada\nem experimentações para melhorar a capacidade de compreensão e geração de texto em tarefas\nespecíficas. Esta versão beta está em constante evolução, incorporandofeedback de usuários para\naprimorar sua eficácia.\nPor uma questão de performance, todos os modelos foram utilizados em versões quantizadas em 4\nbits utilizando o método AWQ (Adaptive Weight Quantization) [Zhang et al. 2022]. A quantização\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n· 5\nModelo Link Hugging Face Acesso\nCapybara Hermes-2.5-Mistral https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-AWQ 20/01/2024\nFirefly-Llama2 https://huggingface.co/TheBloke/Firefly-Llama2-13B-v1.2-AWQ 20/01/2024\nLaser-Dolphin-Mixtral https://huggingface.co/TheBloke/laser-dolphin-mixtral-2x7b-dpo-AWQ 20/01/2024\nLlama-2-13B-Chat https://huggingface.co/TheBloke/Llama-2-13B-chat-AWQ 20/01/2024\nLLama-Pro-8B-Instruct https://huggingface.co/TheBloke/LLaMA-Pro-8B-Instruct-AWQ 20/01/2024\nMeta-Llama-3-8B-Instruct-hf https://huggingface.co/solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ 20/01/2024\nMistral-7B-Instruct https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-AWQ 20/01/2024\nStarling-LM https://huggingface.co/solidrust/Starling-LM-7B-beta-AWQ 20/01/2024\nTabela II: Modelos utilizados e link de acesso noHugging Face.\nAWQ permite uma execução mais eficiente e rápida dos modelos, mantendo a precisão ao mesmo\ntempo em que reduz significativamente os requisitos de armazenamento e processamento. A Tabela\nII apresenta os links, dentro da plataformaHugging Face, para acessar os modelos utilizados. Para\nefeito de comparação com um modelo topo de linha, foi considerada a utilização do GPT-4o. Deste\nmodo, os resultados da extração com o GPT-4o serão comparados com os resultados dos LLMs de\ncódigo aberto descritos nesta seção.\n3.3 Prompt Elaborado utilizando Few-Shot Learning\nO prompt desempenha um papel crucial na performance de LLMs em tarefas diversas. A qualidade,\nclareza e especificidade dosprompts influenciam diretamente a precisão e a eficiência dos modelos em\nrealizar a tarefa especificada . A técnica defew-shot learning permite que os modelos de linguagem\nrealizem novas tarefas com uma quantidade limitada de exemplos de treinamento. Este método é\nparticularmente útil quando os dados rotulados são escassos, caros de serem obtidos ou tem quantidade\nde tokens superior ao tamanho contexto do LLM, nesse caso não sendo possível incluí-los noprompt\n[Hu et al. 2021].\nUtilizamos few-shot learningpara extrair informações específicas de documentos digitalizados utili-\nzados neste estudo, fornecendo ao modelo dois exemplos de entrada e saída desejados. Esses exemplos\nforam removidos do conjunto de teste para garantir a validade dos resultados. Destacamos ainda que\nos exemplos passados foram específicos de cada tipo de documento.\nNeste estudo, desenvolvemos umprompt detalhado para a extração de informações específicas de\ntextos. A definição completa doprompt pode ser encontrada na Figura 2.\nOs prompts foram projetados para garantir a precisão e a eficiência na extração de informações. A\nespecificação clara de que o nome do requerente deve ser um nome de pessoa (e não de empresa) e que o\nendereço deve seguir imediatamente o nome ajuda o modelo a focar nas informações mais relevantes e\nevitar dados irrelevantes. Instruir o modelo a ignorar informações que não se encaixam nas categorias\nespecificadas garante que a saída seja limpa e relevante, evitando ruídos. Exigir a apresentação dos\ndados em formato JSON com campos específicos (’Nome do Requerente’ e ’Endereço do Requerente’)\nassegura que a saída seja estruturada e consistente, facilitando seu uso posterior. A inclusão de\numa etapa de verificação assegura que as informações extraídas estejam corretas, permitindo ajustes\nconforme necessário e aumentando a confiabilidade dos resultados.\nEste processo foi aplicado para outros tipos de documentos, ajustando apenas os campos específicos\na serem extraídos conforme a necessidade.\n3.4 Ambiente de Execução dos Experimentos e Métrica de Avaliação\nOs experimentos foram realizados noGoogle Colab, utilizando as bibliotecasTransformers (4.41.2),\nAccelerate (0.32.1) eAutoAWQ (0.2.5), em uma GPU A100 de 40 GB.\nA performance dos modelos foi avaliada pela acurácia, considerando três níveis: documento indivi-\ndual, tipo de documento e total. A acurácia de um documento é a razão entre o número de atributos\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n6 ·\nObjetivo\nExtrair duas informações específicas de um texto: nome do requerente e endereço do requerente. É crucial coletar apenas dados relevantes\ne precisos.\nInstruções Detalhadas\n(1) Extração de Informações:\n—Nome do Requerente: Extraia nomes que claramente identifiquem pessoas, ignorando nomes de empresas.\n—Endereço do Requerente: Identifique o endereço que aparece imediatamente após o nome do requerente.\n(2) Ignorar informações fora do escopo:\n—Ignore informações que não se encaixam nas categorias especificadas.\n(3) Formatação de Saída:\n—Apresente os dados em formato JSON, com campos para “Nome do Requerente” e “Endereço do Requerente”. Use “Não Encontrado”\npara informações que você não encontrar.\n(4) Verificação e Conformidade:\n—Verifique se as informações extraídas estão corretas e ajustadas conforme necessário.\nExemplos de Entrada e Saída\nExemplo de entrada 1:\n‘‘{OCR_exemplo1}’’\nExemplo de Saída 1:\n{JSON_exemplo1}\nExemplo de entrada 2:\n‘‘{OCR_exemplo2}’’\nExemplo de Saída 2:\n{JSON_exemplo2}\nFigura 2: Definição do prompt utilizado para extração das informações.A informações que aparecem entre\nchaves representam variáveis python que são substituídas por textos detectados por pacotes de OCR e os seu respectivos\nJSON de saída da coleção de referência.\ncorretamente extraídos (Ncorretos) e o número total de atributos esperados (Nesperados):\nAcuráciadoc = Ncorretos\nNesperados\n(1)\nA acurácia para um tipo de documento é a média das acurácias dos documentos desse tipo. A\nacurácia total é a média das acurácias dos tipos de documentos:\nAcuráciatotal = 1\nD\nDX\ni=1\nAcuráciai (2)\nOnde D é o número de tipos de documentos.\nAtributos adicionais retornados pelo LLM não foram considerados na avaliação. Se o LLM não\nretornou um JSON válido, a acurácia do documento foi considerada zero.\n4. RESULTADOS\nOs resultados da avaliação são apresentados na Tabela III, que detalha a acurácia por tipo de do-\ncumento e a média geral para cada modelo. Os resultados apresentados evidenciam o desempenho\nde diferentes modelos na tarefa de extração de informações de documentos digitalizados. Conforme\nesperado, o modelo GPT-4o apresentou resultados superiores a todos os outros modelos, alcançando\numa acurácia total de0, 92. Este desempenho excepcional pode ser atribuído ao fato de o GPT-4o ser\num modelo de ponta, com trilhões de parâmetros, o que lhe confere uma capacidade de processamento\ne entendimento de texto significativamente maior.\nApesar da superioridade do GPT-4o, os modelos de código aberto demonstraram um desempe-\nnho notável, reduzindo a diferença para menos de 5% em alguns tipos de documentos. O modelo\nLaser-Dolphin-Mixtral-2x7B-DPO destacou-se como o melhor modelo de código aberto, obtendo uma\nacurácia total de0, 74. Esse resultado representa uma diferença de apenas 18% em comparação ao\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n· 7\nModelo Endereço Laudo ICMS IPI Proc. Conc. Acur´ aciatotal\nCapybara Hermes 0,53 (-0,20) 0,57 (-0,28) 0,94 (-0,06) 0,99 (-0,01) 0,58 (-0,38) 0,73 (-0,10) 0,72 (-0,20)\nFirefly-Llama2 0,46 (-0,27) 0,18 (-0,67) 0,97 (-0,03) 0,99 (-0,01) 0,45 (-0,51) 0,66 (-0,17) 0,61 (-0,31)\nLaser-Dolphin 0,57 (-0,16) 0,62 (-0,23) 0,97 (-0,03) 0,97 (-0,03) 0,60 (-0,36) 0,73 (-0,10) 0,74 (-0,18)\nLlama-2-13B 0,34 (-0,39) 0,15 (-0,70) 0,94 (-0,06) 0,96 (-0,04) 0,39 (-0,57) 0,68 (-0,15) 0,57 (-0,35)\nLLaMA-Pro 0,42 (-0,31) 0,31 (-0,54) 0,68 (-0,32) 0,91 (-0,09) 0,43 (-0,53) 0,59 (-0,24) 0,55 (-0,37)\nMeta-Llama-3 0,56 (-0,17) 0,46 (-0,39) 0,81 (-0,19) 0,98 (-0,02) 0,70 (-0,26) 0,80 (-0,03) 0,71 (-0,21)\nMistral-7B 0,50 (-0,23) 0,47 (-0,38) 0,95 (-0,05) 0,94 (-0,06) 0,51 (-0,45) 0,77 (-0,06) 0,69 (-0,23)\nStarling-LM 0,57 (-0,16) 0,49 (-0,36) 0,97 (-0,03) 0,97 (-0,03) 0,50 (-0,46) 0,75 (-0,08) 0,70 (-0,22)\nGPT-4o 0,73 0,85 1,00 1,00 0,96 0,83 0,92\nTabela III: Acurácia por tipo de documento e total para cada modelo. Os valores entre parênteses\nrepresentam a diferença em relação ao modelo de referência GPT-4o.\nGPT-4o, mesmo possuindo apenas 14 bilhões de parâmetros, uma fração do tamanho do modelo da\nOpen AI.\n5. DISCUSSÃO\nOs resultados desta pesquisa demonstram que, para a tarefa de extração de informações em docu-\nmentos digitalizados, os modelosopen-source de entrada apresentaram um desempenho comparável\nao modelo GPT-4o em diversos tipos de documentos. Embora o GPT-4o tenha obtido a maior acu-\nrácia geral, os modelos de código aberto, como o Laser-Dolphin-Mixtral-2x7B-DPO, mostraram uma\ndiferença de desempenho relativamente pequena. Este achado ressalta a viabilidade de utilizar LLMs\nde código aberto para tarefas de extração de informações em contextos onde os recursos financeiros\nou de infraestrutura são limitados, proporcionando uma alternativa eficaz e economicamente viável.\nVale destacar também que os resultados de todos os LLMs poderiam ser aprimorados caso se\nutilizasse uma métrica de similaridade para comparar os atributos extraídos dos documentos com\nmaior flexibilidade. Devido a possíveis erros introduzidos pelo OCR, como caracteres incorretos, é\ncomum que o erro não seja originado do LLM. A aplicação de uma métrica de similaridade permitiria\numa comparação mais robusta e realista dos valores extraídos, mitigando o impacto de pequenos\nerros e refletindo melhor as necessidades práticas em cenários de automação inteligente de processos.\nEm muitas aplicações reais, é necessário comparar valores extraídos para realizar correspondências,\ncomo verificar se um indivíduo é cônjuge de outro comparando o nome na identidade e na certidão de\ncasamento, onde um caractere incorreto pode ser tolerado.\nPor fim, os resultados apresentados neste artigo revelam que há um potencial para ajuste fino e\ndestilação [Gu et al. 2024] de modelos que é significativo. Ajustar um modelo de código aberto\nutilizando as respostas do GPT-4o poderia especializar o modelo, fazendo com que ele “imitasse” o\nmodelo maior na tarefa específica de extração de informação em documentos digitalizados.\n6. CONCLUSÃO\nEste estudo avaliou a eficácia de Modelos de Linguagem de Larga Escala de código aberto, com\nparâmetros variando entre 7 e 14 bilhões, na tarefa de extração de informações de textos OCR oriundos\nde documentos digitalizados de baixa qualidade. Os resultados indicam que, embora o modelo GPT-\n4o tenha obtido a melhor performance geral com uma acurácia de0, 92, modelos de código aberto,\ncomo o Laser-Dolphin-Mixtral-2x7B-DPO, mostraram-se competitivos, alcançando uma acurácia de\n0, 74. Este desempenho posiciona os modelosopen-source como alternativas viáveis e econômicas para\naplicações em que recursos financeiros ou de infraestrutura são limitados.\nÉ importante destacar a questão da propriedade dos dados utilizados nos experimentos. Os docu-\nmentos empregados neste estudo contêm informações sensíveis e confidenciais, fornecidas pela Secre-\ntaria da Fazenda do Estado do Ceará. Para mitigar riscos de vazamento de dados (data leakage) ao\nutilizar modelos proprietários como o GPT-4o, realizamos todos os experimentos utilizando a API da\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.\n8 ·\nOpenAI. Conforme os termos de uso da OpenAI, os dados enviados via API não são utilizados para\ntreinamento dos modelos, diferentemente dos dados submetidos através da interface do ChatGPT.\nAssim, asseguramos que as informações sensíveis não seriam armazenadas ou utilizadas para outros\nfins, mantendo a confidencialidade e a integridade dos dados.\nComotrabalhosfuturos, propomosadestilaçãodeummodelodecódigoabertodeentradautilizando\nas respostas de um modelo proprietário de alta performance, como o GPT-4o. A destilação pode ser\nrealizada em larga escala, fazendo uso de milhares de inferências (exemplos), diferente dos dados\nrotulados manualmente nesta pesquisa, que por motivos óbvios têm poucos exemplos. Esse processo,\nconhecido como aprendizado por destilação [Gu et al. 2024], tem o potencial de transformar um\nmodelo menor em um especialista na tarefa específica de extração de informações de documentos\ndigitalizados.\nPor fim, reconhecemos algumas ameaças à validade deste estudo. Primeiramente, o conjunto de\ndados utilizado, embora realista, é limitado em tamanho e variedade, o que pode afetar a generalização\ndos resultados para outros contextos ou tipos de documentos. Além disso, a qualidade variável dos\ntextos OCR, decorrente de documentos digitalizados de baixa qualidade, pode introduzir erros que\ninfluenciam o desempenho dos modelos, não necessariamente refletindo suas capacidades reais. Por\nfim, apesar das medidas tomadas para proteger os dados ao utilizar modelos proprietários, dependemos\ndas garantias fornecidas pelos provedores de serviços, o que pode representar um risco residual.\nREFERÊNCIAS\nCardoso, B. and Pereira, D.Evaluating an aspect extraction method for opinion mining in the portuguese language.\nIn Anais do VIII Symposium on Knowledge Discovery, Mining and Learning. SBC, Porto Alegre, RS, Brasil, pp.\n137–144, 2020.\nChakraborti, T., Isahagian, V., Khalaf, R., Khazaeni, Y., Muthusamy, V., Rizk, Y., and Unuvar, M.From\nrobotic process automation to intelligent process automation. In International Conference on Business Process\nManagement. Springer, pp. 215–228, 2020.\nGartlehner, G., Kahwati, L., Hilscher, R., Thomas, I., Kugley, S., Crotty, K., Viswanathan, M.,\nNussbaumer-Streit, B., Booth, G., Erskine, N., Konet, A., and Chew, R. Data Extraction for Evidence\nSynthesis Using a Large Language Model: A Proof-of-Concept Study.medRxiv, October, 2023. [Online]. Disponível\nem: https://doi.org/10.1101/2023.10.02.23296415.\nGu, Y., Dong, L., Wei, F., and Huang, M. Minillm: Knowledge distillation of large language models. In The\nTwelfth International Conference on Learning Representations, 2024.\nHu, E. J., Shi, L., Squadrato, R., Tay, Y., Ruder, S., and Raffel, C.Low-Rank Adaptation of Large Language\nModels. arXiv preprint arXiv:2106.09685, 2021. [Online]. Disponível em:https://arxiv.org/abs/2106.09685.\nMartins, V. and Silva, C.Text classification in law area: a systematic review. InAnais do IX Symposium on\nKnowledge Discovery, Mining and Learning. SBC, Porto Alegre, RS, Brasil, pp. 33–40, 2021.\nMinaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., and Gao, J.Large language\nmodels: A survey.arXiv preprint arXiv:2402.06196, 2024.\nPapadopoulos, D., Papadakis, N., and Litke, A.A Methodology for Open Information Extraction and Represen-\ntation from Large Scientific Corpora: The CORD-19 Data Exploration Use Case.Applied Sciences 10 (16): 5630,\n2020.\nSilva, M. d. L. M., Mendonça, A. L. C., Neto, E. R. D., Chaves, I. C., Caminha, C., Brito, F. T., Farias,\nV. A. E., and Machado, J. C.Facto dataset: A dataset of user reports for faulty computer components. InAnais\ndo VI Dataset Showcase Workshop. SBC, pp. 1–12, 2024.\nTownsend, V., Xie, D., Huang, P., and Cole, L.Structured Information Extraction from Complex Scientific Text\nwith Fine-Tuned Large Language Models.arXiv preprint arXiv:2212.05238, December, 2023. [Online]. Disponível\nem: https://arxiv.org/abs/2212.05238.\nWeston, L., Tshitoyan, V., Dagdelen, J., Kononova, O., Trewartha, A., Persson, K. A., Ceder, G., and\nJain, A.Named entity recognition and normalization applied to large-scale information extraction from the materials\nscience literature. Journal of Chemical Information and Modeling59 (9): 3692–3702, 2019.\nZhang, X., Wang, Y., Xu, Y., and Zhang, J.Adaptive Weight Quantization for Efficient Neural Network Inference.\nIn Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS), 2022.\nZhang, Y., Kumar, S., Singh, D., and Jain, A.LMDX: Language Model-based Document Information Extraction\nand Localization. arXiv preprint arXiv:2303.01234, 2023.\nSymposium on Knowledge Discovery, Mining and Learning, KDMiLe 2024.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7770479917526245
    },
    {
      "name": "Open source",
      "score": 0.6997640132904053
    },
    {
      "name": "Information extraction",
      "score": 0.6120782494544983
    },
    {
      "name": "Information retrieval",
      "score": 0.4916461110115051
    },
    {
      "name": "Language model",
      "score": 0.4643082916736603
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4558708369731903
    },
    {
      "name": "Natural language processing",
      "score": 0.41279181838035583
    },
    {
      "name": "World Wide Web",
      "score": 0.32208752632141113
    },
    {
      "name": "Programming language",
      "score": 0.22202354669570923
    },
    {
      "name": "Software",
      "score": 0.11075866222381592
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I121374431",
      "name": "Unifor",
      "country": "NO"
    }
  ]
}