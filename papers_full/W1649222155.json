{
    "title": "Exploiting Syntactic Structure for Natural Language Modeling",
    "url": "https://openalex.org/W1649222155",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4293488235",
            "name": "Chelba, Ciprian",
            "affiliations": [
                "Johns Hopkins University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3193262203",
        "https://openalex.org/W1543082528",
        "https://openalex.org/W2140842551",
        "https://openalex.org/W2107771181",
        "https://openalex.org/W1483677181",
        "https://openalex.org/W1525582138",
        "https://openalex.org/W180232814",
        "https://openalex.org/W2160645305",
        "https://openalex.org/W1575431606",
        "https://openalex.org/W2049633694",
        "https://openalex.org/W2110882317",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W3021713638",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W1966812932",
        "https://openalex.org/W2135258175",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2166637769",
        "https://openalex.org/W1516146643",
        "https://openalex.org/W2949237929",
        "https://openalex.org/W1590952807",
        "https://openalex.org/W2963847008",
        "https://openalex.org/W1607229519",
        "https://openalex.org/W2024490156",
        "https://openalex.org/W2096175520",
        "https://openalex.org/W2099111195"
    ],
    "abstract": "The thesis presents an attempt at using the syntactic structure in natural language for improved language models for speech recognition. The structured language model merges techniques in automatic parsing and language modeling using an original probabilistic parameterization of a shift-reduce parser. A maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms is employed for training the model. Experiments on the Wall Street Journal, Switchboard and Broadcast News corpora show improvement in both perplexity and word error rate - word lattice rescoring - over the standard 3-gram language model. The significance of the thesis lies in presenting an original approach to language modeling that uses the hierarchical - syntactic - structure in natural language to improve on current 3-gram modeling techniques for large vocabulary speech recognition.",
    "full_text": "arXiv:cs/0001020v1  [cs.CL]  24 Jan 2000\nExploiting Syntactic Structure for Natural\nLanguage Modeling\nCiprian Chelba\nA dissertation submitted to the Johns Hopkins University in conform ity with the\nrequirements for the degree of Doctor of Philosophy.\nBaltimore, Maryland\n2018\nCopyright c⃝ 2018 by Ciprian Chelba,\nAll rights reserved.\nAbstract\nThe thesis presents an attempt at using the syntactic structure in natural language\nfor improved language models for speech recognition. The structu red language model\nmerges techniques in automatic parsing and language modeling using a n original\nprobabilistic parameterization of a shift-reduce parser. A maximum likelihood rees-\ntimation procedure belonging to the class of expectation-maximizat ion algorithms is\nemployed for training the model. Experiments on the Wall Street Jou rnal, Switch-\nboard and Broadcast News corpora show improvement in both perp lexity and word\nerror rate — word lattice rescoring — over the standard 3-gram lan guage model.\nThe signiﬁcance of the thesis lies in presenting an original approach t o language\nmodeling that uses the hierarchical — syntactic — structure in natu ral language to\nimprove on current 3-gram modeling techniques for large vocabular y speech recogni-\ntion.\nAdvisor: Prof. Frederick Jelinek\nReaders: Prof. Frederick Jelinek and Prof. Michael Miller\nii\nAcknowledgements\nThe years I have spent at Hopkins taught me many valuable lessons, usually through\npeople I have interacted with and to whom I am grateful.\nI am thankful to my advisor Frederick Jelinek.\nBill Byrne and Sanjeev Khudanpur for their insightful comments an d assistance\non technical issues and not only.\nThe members of the Dependency Modeling during the summer ’96 DoD W ork-\nshop, especially: Harry Printz, Eric Ristad and Andreas Stolcke for their support on\ntechnical and programming matters. This thesis would have been on a diﬀerent topic\nwithout the creative environment during that workshop.\nThe people on the STIMULATE grant: Eric Brill, Fred Jelinek, Sanjeev Khudan-\npur, David Yarowski.\nPonani Gopalakrishnan who patiently guided my ﬁrst steps in the pra ctical aspects\nof speech recognition.\nMy former academic advisor in Bucharest, Vasile Buzuloiu, for encou raging me to\nfurther my education.\nMy colleagues and friends at the CLSP, for bearing with me all these y ears, helping\nme in my work, engaging in often useless conversations and making th is a fun time\nof my life: Radu Florian, Asela Gunawardana, Vaibhava Goel, John Hen derson,\nXiaoxiang Luo, Lidia Mangu, John McDonough, Makis Potamianos, Gra ce Ngai,\nMurat Saraclar, Eric Wheeler, Jun Wu, Dimitra Vergyri.\nAmy Berdann, Janet Lamberti and Kimberly Shiring Petropoulos at t he CLSP\nfor help on all sort of things, literally.\nJacob Laderman for keeping the CLSP machines up and running.\nMy friends who were there when thesis work was the last thing I want ed to discuss\nabout: Lynn Anderson, Delphine Dahan, Wolfgang Himmelbauer, Der ek Houston,\nMihai Pop, Victor and Delia Velculescu.\nMy host family, Ed and Sue Dickey, for oﬀering advice and help in a new c ulture,\nmaking me so welcome at the beginning of my stay in Baltimore and there after.\nMy parents, whose support and encouragement was always there when I needed\niii\nit.\niv\nTo my parents\nv\nContents\nList of Tables viii\nList of Figures x\n1 Language Modeling for Speech Recognition 4\n1.1 Basic Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.1.1 Language Model Quality . . . . . . . . . . . . . . . . . . . . . 6\n1.1.2 Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.2 Current Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2 A Structured Language Model 10\n2.1 Syntactic Structure in Natural Language . . . . . . . . . . . . . . . . 10\n2.1.1 Headword Percolation and Binarization . . . . . . . . . . . . . 12\n2.2 Exploiting Syntactic Structure for Language Modeling . . . . . . . . 18\n2.3 Probabilistic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.4 Modeling Tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.5 Pruning Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.6 Word Level Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3 Structured Language Model Parameter Estimation 36\n3.1 Maximum Likelihood Estimation from Incomplete Data . . . . . . . . 38\n3.1.1 N-best Training Procedure . . . . . . . . . . . . . . . . . . . . 40\n3.1.2 N-best Training . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.2 First Stage of Model Estimation . . . . . . . . . . . . . . . . . . . . . 43\n3.2.1 First Stage Initial Parameters . . . . . . . . . . . . . . . . . . 46\n3.3 Second Stage Parameter Reestimation . . . . . . . . . . . . . . . . . 47\n4 Experiments using the Structured Language Model 48\n4.1 Perplexity Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.1.1 Comments and Experiments on Model Parameters Reestimation 50\n4.2 Miscellaneous Other Experiments . . . . . . . . . . . . . . . . . . . . 54\n4.2.1 Choosing the Model Components Parameterization . . . . . . 54\nvi\n4.2.2 Fudged TAGGER and PARSER Scores . . . . . . . . . . . . . 57\n4.2.3 Maximum Depth Factorization of the Model . . . . . . . . . . 58\n5 A∗ Decoder for Lattices 60\n5.1 Two Pass Decoding Techniques . . . . . . . . . . . . . . . . . . . . . 60\n5.2 A∗ Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n5.2.1 A∗ for Lattice Decoding . . . . . . . . . . . . . . . . . . . . . 65\n5.2.2 Some Practical Considerations . . . . . . . . . . . . . . . . . 68\n6 Speech Recognition Experiments 70\n6.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n6.2 Perplexity Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n6.2.1 Wall Street Journal Perplexity Results . . . . . . . . . . . . . 75\n6.2.2 Switchboard Perplexity Results . . . . . . . . . . . . . . . . . 76\n6.2.3 Broadcast News Perplexity Results . . . . . . . . . . . . . . . 76\n6.3 Lattice Decoding Results . . . . . . . . . . . . . . . . . . . . . . . . . 77\n6.3.1 Wall Street Journal Lattice Decoding Results . . . . . . . . . 77\n6.3.2 Switchboard Lattice Decoding Results . . . . . . . . . . . . . 81\n6.3.3 Broadcast News Lattice Decoding Results . . . . . . . . . . . 83\n6.3.4 Taking Advantage of Lattice Structure . . . . . . . . . . . . . 87\n7 Conclusions and Future Directions 91\n7.1 Comments on Using the SLM as a Parser . . . . . . . . . . . . . . . . 91\n7.2 Comparison with other Approaches . . . . . . . . . . . . . . . . . . . 92\n7.2.1 Underlying P (W, T ) Probability Model . . . . . . . . . . . . . 92\n7.2.2 Language Model . . . . . . . . . . . . . . . . . . . . . . . . . 93\n7.3 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nA Minimizing KL Distance is Equivalent to Maximum Likelihoo d 97\nB Expectation Maximization as Alternating Minimization 99\nC N-best EM convergence 102\nD Structured Language Model Parameter Reestimation 105\nBibliography 108\nvii\nList of Tables\n2.1 Headword Percolation Rules . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2 Binarization Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3 Sample descriptor ﬁle for the deleted interpolation module . . . . . . 28\n4.1 Parameter reestimation results . . . . . . . . . . . . . . . . . . . . . . 49\n4.2 Interpolation with trigram results . . . . . . . . . . . . . . . . . . . . 49\n4.3 Evolution of diﬀerent ”perplexity” values during training . . . . . . . 52\n4.4 Dynamics of WORD-PREDICTOR distribution on types during rees-\ntimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.5 WORD-PREDICTOR conditional perplexities . . . . . . . . . . . . . 55\n4.6 TAGGER conditional perplexities . . . . . . . . . . . . . . . . . . . . 56\n4.7 PARSER conditional perplexities . . . . . . . . . . . . . . . . . . . . 56\n4.8 Perplexity Values: Fudged TAGGER and PARSER . . . . . . . . . . 58\n4.9 Maximum Depth Evolution During Training . . . . . . . . . . . . . . 59\n6.1 Treebank — CSR tokenization mismatch . . . . . . . . . . . . . . . . 72\n6.2 WSJ-CSR-Treebank perplexity results . . . . . . . . . . . . . . . . . 75\n6.3 SWB-CSR-Treebank perplexity results . . . . . . . . . . . . . . . . . 76\n6.4 SWB-CSR-Treebank perplexity results . . . . . . . . . . . . . . . . . 77\n6.5 3-gram Language Model; Viterbi Decoding Results . . . . . . . . . . 7 9\n6.6 LAT-3gram + Structured Language Model; A∗ Decoding Results . . . 79\n6.7 TRBNK-3gram + Structured Language Model; A∗ Decoding Results 80\n6.8 3-gram Language Model; Viterbi Decoding Results . . . . . . . . . . 8 1\n6.9 LAT-3gram + Structured Language Model; A∗ Decoding Results . . . 82\n6.10 TRBNK-3gram + Structured Language Model; A∗ Decoding Results 83\n6.11 Broadcast News Focus conditions . . . . . . . . . . . . . . . . . . . . 84\n6.12 3-gram Language Model; Viterbi Decoding Results . . . . . . . . . . 84\n6.13 LAT-3gram + Structured Language Model; A∗ Decoding Results . . . 85\n6.14 LAT-3gram + Structured Language Model; A∗ Decoding Results; break-\ndown on diﬀerent focus conditions . . . . . . . . . . . . . . . . . . . . 85\n6.15 TRBNK-3gram + Structured Language Model; A∗ Decoding Results 86\nviii\n6.16 TRBNK-3gram + Structured Language Model; A∗ Decoding Results;\nbreakdown on diﬀerent focus conditions . . . . . . . . . . . . . . . . . 87\n6.17 Switchboard;TRBNK-3gram + Peeking SLM; . . . . . . . . . . . . . 88\n6.18 Switchboard; TRBNK-3gram + Normalized Peeking SLM; . . . . . . 9 0\nix\nList of Figures\n2.1 UPenn Treebank Parse Tree Representation . . . . . . . . . . . . . . 11\n2.2 Parse Tree Representation after Headword Percolation and Bin arization 13\n2.3 Binarization schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.4 Partial parse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5 A word-parse k-preﬁx . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.6 Complete parse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.7 Before an adjoin operation . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.8 Result of adjoin-left under NTtag . . . . . . . . . . . . . . . . . . . . 21\n2.9 Result of adjoin-right under NTtag . . . . . . . . . . . . . . . . . . . 21\n2.10 Language Model Operation as a Finite State Machine . . . . . . . . . 22\n2.11 Recursive Linear Interpolation . . . . . . . . . . . . . . . . . . . . . . 26\n2.12 One search extension cycle . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.1 Alternating minimization between convex sets . . . . . . . . . . . . . 40\n4.1 Structured Language Model Maximum Depth Distribution . . . . . . 59\n5.1 Preﬁx Tree Organization of a Set of Hypotheses L . . . . . . . . . . . 63\n6.1 Lattice CSR to CSR-Treebank Processing . . . . . . . . . . . . . . . 74\n7.1 CFG dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n7.2 Tag reduced WORD-PREDICTOR dependencies . . . . . . . . . . . 93\n7.3 TAGGER dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n7.4 Tag reduced CONSTRUCTOR dependencies . . . . . . . . . . . . . . 94\nB.1 Alternating minimization between PT and Q(Θ) . . . . . . . . . . . . 100\nx\n1\nIntroduction\nIn the accepted statistical formulation of the speech recognition problem [17] the\nrecognizer seeks to ﬁnd the word string\nˆW .= arg max\nW\nP (A|W ) P (W )\nwhere A denotes the observable speech signal, P (A|W ) is the probability that when\nthe word string W is spoken, the signal A results, and P (W ) is the a priori probability\nthat the speaker will utter W .\nThe language model estimates the values P (W ). With W = w1, w2, . . . , w n we\nget by Bayes’ theorem,\nP (W ) =\nn∏\ni=1\nP (wi|w1, w2, . . . , w i−1) (0.1)\nSince the parameter space of P (wk|w1, w2, . . . , w k−1) is too large 1, the language\nmodel is forced to put the history Wk−1 = w1, w2, . . . , w k−1 into an equivalence\nclass determined by a function Φ( Wk−1). As a result,\nP (W ) ∼=\nn∏\nk=1\nP (wk|Φ( Wk−1)) (0.2)\nResearch in language modeling consists of ﬁnding appropriate equiva lence classiﬁers\nΦ and methods to estimate P (wk|Φ( Wk−1)).\nThe language model of state-of-the-art speech recognizers us es ( n−1)-gram equiv-\nalence classiﬁcation, that is, deﬁnes\nΦ( Wk−1) .= wk−n+1, wk−n+2, . . . , w k−1\n1The words wj belong to a vocabulary V whose size is in the tens of thousands.\n2\nOnce the form Φ( Wk−1) is speciﬁed, only the problem of estimating P (wk|Φ( Wk−1))\nfrom training data remains.\nIn most cases, n = 3 which leads to a trigram language model. The latter has\nbeen shown to be surprisingly powerful and, essentially, all attemp ts to improve on\nit in the last 20 years have failed. The one interesting enhancement, facilitated by\nmaximum entropy estimation methodology, has been the use of triggers [27] or of\nsingular value decomposition [4] (either of which dynamically identify the topic of\ndiscourse) in combination with n−gram models .\nMeasures of Language Model Quality\nWord Error Rate One possibility to measure the quality of a language model is\nto evaluate it as part of a speech recognizer. The measure of succ ess is the word error\nrate; to calculate it we need to ﬁrst ﬁnd the most favorable word alig nment between\nthe hypothesis put out by the recognizer ˆW and the true sequence of words uttered\nby the speaker W — assumed to be known a priori for evaluation purposes only —\nand then count the number of incorrect words in ˆW per total number of words in W .\nTRANSCRIPTION: UP UPSTATE NEW YORK SOMEWHERE UH OVER OVER HU GE AREAS\nHYPOTHESIS: UPSTATE NEW YORK SOMEWHERE UH ALL ALL THE HUGE AR EAS\n1 0 0 0 0 0 1 1 1 0 0\n:4 errors per 10 words in transcription; WER = 40%\nPerplexity As an alternative to the computationally expensive word error rate\n(WER), a statistical language model is evaluated by how well it predic ts a string of\nsymbols Wt — commonly referred to as test data — generated by the source to be\nmodeled.\nAssume we compare two models M1 and M2; they assign probability PM1 (Wt)\nand PM2 (Wt), respectively, to the sample test string Wt. The test string has neither\nbeen used nor seen at the estimation step of either model and it was generated by\nthe same source that we are trying to model. “Naturally”, we consid er M1 to be a\nbetter model than M2 if PM1 (Wt) > P M2 (Wt).\n3\nA commonly used quality measure for a given model M is related to the entropy\nof the underlying source and was introduced under the name of per plexity (PPL) [17]:\nP P L(M) = exp(−1/N\nN∑\nk=1\nln [PM (wk|Wk−1)]) (0.3)\nThesis Layout\nThe thesis is organized as follows:\nAfter a brief introduction to language modeling for speech recognit ion, Chap-\nter 2 gives a basic description of the structured language model (S LM) followed by\nChapters 3.1 and 3 explaining the model parameters reestimation alg orithm we used.\nChapter 4 presents a series of experiments we have carried out on the UPenn Treebank\ncorpus ([21]).\nChapters 5 and 6 describe the setup and speech recognition exper iments using\nthe structured language model on diﬀerent corpora: Wall Street Journal (WSJ, [24]),\nSwitchboard (SWB, [15]) and Broadcast News (BN).\nWe conclude with Chapter 7, outlining the relationship between our ap proach to\nlanguage modeling — and parsing — and others in the literature and poin ting out\nwhat we believe to be worthwhile future directions of research.\nA few appendices detail mathematical aspects of the reestimation technique we\nhave used.\n4\nChapter 1\nLanguage Modeling for Speech\nRecognition\nThe task of a speech recognizer is to automatically transcribe spee ch into text.\nGiven a string of acoustic features A extracted by its signal processing front-end from\nthe raw acoustic waveform, the speech recognizer tries to identif y the word sequence\nW that produced A — typically one sentence at a time. Let ˆW be the word string —\nhypothesis — output by the speech recognizer. The measure of su ccess is the word\nerror rate; to calculate it we need to ﬁrst ﬁnd the most favorable w ord alignment\nbetween ˆW and W — assumed to be known a priori for evaluation purposes only —\nand then count the number of incorrect words in the hypothesized sequence ˆW per\ntotal number of words in W .\nTRANSCRIPTION: UP UPSTATE NEW YORK SOMEWHERE UH OVER OVER HU GE AREAS\nHYPOTHESIS: UPSTATE NEW YORK SOMEWHERE UH ALL ALL THE HUGE AR EAS\n1 0 0 0 0 0 1 1 1 0 0\n:4 errors per 10 words in transcription; WER = 40%\nThe most successful approach to speech recognition so far is a st atistical one\npioneered by Jelinek and his colleagues [2]; speech recognition is viewed as a Bayes\ndecision problem: given the observed string of acoustic features A, ﬁnd the most\nlikely word string ˆW among those that could have generated A:\nˆW = argmaxW P (W |A) = argmaxW P (A|W ) · P (W ) (1.1)\n5\nThere are three broad subproblems to be solved:\n• decide on a feature extraction algorithm and model the channel pr obability\nP (A|W ) — commonly referred to as acoustic modeling ;\n• model the source probability P (W ) — commonly referred to as language mod-\neling;\n• search over all possible word strings W that could have given rise to A and ﬁnd\nout the most likely one ˆW ; due to the large vocabulary size — tens of thousands\nof words — an exhaustive search is intractable.\nThe remaining part of the chapter is organized as follows: we will ﬁrst describe\nlanguage modeling in more detail by taking a source modeling view; then we will\ndescribe current approaches to the problem, outlining their advan tages and short-\ncomings.\n1.1 Basic Language Modeling\nAs explained in the introductory section, the language modeling prob lem is to\nestimate the source probability P (W ) where W = w1, w2, . . . , w n is a sequence of\nwords.\nThis probability is estimated from a training corpus — thousands of wo rds of text\n— according to a modeling assumption on the source that generated the text. Usually\nthe source model is parameterized according to a set of paramete rs Pθ(W ), θ ∈ Θ\nwhere Θ is referred to as the parameter space.\nOne ﬁrst choice faced by the modeler is the alphabet V — also called vocabulary\n— in which the wi symbols take value. For practical purposes one has to limit the\nsize of the vocabulary. A common choice is to use a ﬁnite set of words V and map\nany word not in this set to the distinguished type <unknown>.\nA second, and much more important choice is the source model to be used. A\ndesirable way of making this choice takes into account:\n• a priori knowledge of how the source might work, if available;\n6\n• possibility to reliably estimate source model parameters; reliability of estimates\nlimits the number and type of parameters one can estimate given a ce rtain\namount of training data;\n• preferably, due to the sequential nature of an eﬃcient search alg orithm, the\nmodel should operate left-to-right, allowing the computation of\nP (w1, w2, . . . , w n) = P (w1) · ∏ n\ni=2 P (wi|w1 . . . w i−1).\nWe thus seek to develop parametric conditional models:\nPθ(wi|w1 . . . w i−1), θ ∈ Θ , wi ∈ V (1.2)\nThe currently most successful model assumes a Markov source o f a given order n\nleading to the n-gram language model :\nPθ(wi|w1 . . . w i−1) = Pθ(wi|wi−n+1 . . . w i−1) (1.3)\n1.1.1 Language Model Quality\nAny parameter estimation algorithm needs an objective function wit h respect\nto which the parameters are optimized. As stated in the introducto ry section, the\nultimate goal of a speech recognizer is low word error rate (WER). H owever, all\nattempts to derive an algorithm that would directly estimate the mod el parameters\nso as to minimize WER have failed. As an alternative, a statistical mode l is evaluated\nby how well it predicts a string of symbols Wt — commonly referred to as test data\n— generated by the source to be modeled.\n1.1.2 Perplexity\nAssume we compare two models M1 and M2; they assign probability PM1 (Wt)\nand PM2 (Wt), respectively, to the sample test string Wt. The test string has neither\nbeen used nor seen at the estimation step of either model and it was generated by\nthe same source that we are trying to model. “Naturally”, we consid er M1 to be a\nbetter model than M2 if PM1 (Wt) > P M2 (Wt). It is worth mentioning that this is\n7\ndiﬀerent than maximum likelihood estimation: the test data is not seen during the\nmodel estimation process and thus we cannot directly estimate the parameters of the\nmodel such that it assigns maximum probability to the test string.\nA commonly used quality measure for a given model M is related to the entropy\nof the underlying source and was introduced under the name of per plexity (PPL) [17]:\nP P L(M) = exp(−1/N\nN∑\ni=1\nln [PM (wi|w1 . . . w i−1)]) (1.4)\nIt is easily seen that if our model estimates the source probability ex actly:\nPM (wi|w1 . . . w i−1) = Psource(wi|w1 . . . w i−1), i = 1 . . . N\nthen (1.4) is a consistent estimate of the exponentiated source en tropy exp(Hsource).\nTo get an intuitive understanding for PPL (1.4) we can state that it m easures the\naverage surprise of model M when it predicts the next word wi in the current context\nw1 . . . w i−1.\nSmoothing\nOne important remark is worthwhile at this point: assume that our mo del M is\nfaced with the prediction wi|w1 . . . w i−1 and that wi has not been seen in the training\ncorpus in context w1 . . . w i−1 which itself possibly has not been encountered in the\ntraining corpus. If PM (wi|w1 . . . w i−1) = 0 then PM (w1 . . . w N ) = 0 thus forcing a\nrecognition error; good models M are smooth, in the sense that\n∃ǫ(M) > 0 s.t. PM (wi|w1 . . . w i−1) > ǫ, ∀wi ∈ V , ( w1 . . . w i−1) ∈ V i−1.\n1.2 Current Approaches\nIn the previous section we introduced the class of n-gram models. T hey assume\na Markov source of order n, thus making the following equivalence classiﬁcation of a\ngiven context:\n[w1 . . . w i−1] = wi−n+1 . . . w i−1 = hn (1.5)\nAn equivalence classiﬁcation of some similar sort is needed because of the impos-\nsibility to get reliable relative frequency estimates for the full conte xt prediction\n8\nwi|w1 . . . w i−1. Indeed, as shown in [27], for a 3-gram model the coverage for the\n(wi|wi−2, wi−1) events is far from suﬃcient: the rate of new (unseen) trigrams in test\ndata relative to those observed in a training corpus of size 38 million wo rds is 21% for\na 5,000-words vocabulary and 32% for a 20,000-words vocabulary. Moreover, approx.\n70% of the trigrams in the training data have been seen once, thus m aking a relative\nfrequency estimate unusable because of its unreliability.\nOne standard approach that also ensures smoothing is the deleted interpolation\nmethod [18]. It interpolates linearly among contexts of diﬀerent ord er hn:\nPθ(wi|wi−n+1 . . . w i−1) =\nk=n∑\nk=0\nλk · f(wi|hk) (1.6)\nwhere:\n• hk = wi−k+1 . . . w i−1 is the context of order k when predicting wi;\n• f(wi|hk) is the relative frequency estimate for the conditional probability P (wi|hk);\nf(wi|hk) = C(wi, hk)/C(hk),\nC(hk) =\n∑\nwi∈V\nC(wi, hk), k = 1 . . . n,\nf(wi|h1) = C(wi)/\n∑\nwi∈V\nC(wi),\nf(wi|h0) = 1 /|V|, ∀wi ∈ V , uniform ;\n• λk, k = 0 . . . n are the interpolation coeﬃcients satisfying λk > 0, k = 0 . . . n\nand ∑ k=n\nk=0 λk = 1.\nThe model parameters θ are:\n• the counts C(hn, wi); lower order counts are inferred recursively by:\nC(hk, wi) =\n∑\nwi− k∈V C(wi−k, hk, wi);\n• the interpolation coeﬃcients λk, k = 0 . . . n .\nA simple way to estimate the model parameters involves a two stage p rocess:\n1. gather counts from development data — about 90% of training data;\n9\n2. estimate interpolation coeﬃcients to minimize the perplexity of cross-validation\ndata — the remaining 10% of the training data — using the expectation-\nmaximization (EM) algorithm [14].\nOther approaches use diﬀerent smoothing techniques — maximum en tropy [5],\nback-oﬀ [20] — but they all share the same Markov assumption on th e underlying\nsource.\nAn attempt to overcome this limitation is developed in [27]. Words in the c on-\ntext outside the range of the 3-gram model are identiﬁed as “trigg ers” and retained\ntogether with the “target” word in the predicted position. The (tr igger, target) pairs\nare treated as complementary sources of information and integra ted with the n-gram\npredictors using the maximum entropy method. The method has pro ven successful,\nhowever computationally burdensome.\nOur attempt will make use of the hierarchical structuring of word s trings in natural\nlanguage for expanding the memory length of the source.\n10\nChapter 2\nA Structured Language Model\nIt has been long argued in the linguistics community that the simple mind ed\nMarkov assumption is far from accurate for modeling the natural la nguage source.\nHowever so far very few approaches managed to outperform the n-gram model in\nperplexity or word error rate, none of them exploiting syntactic st ructure for better\nmodeling of the natural language source.\nThe model we present is closely related to the one investigated in [7], h owever\ndiﬀerent in a few important aspects:\n• our model operates in a left-to-right manner, thus allowing its use d irectly in\nthe hypothesis search for ˆW in (1.1);\n• our model is a factored version of the one in [7], thus enabling the calc ulation\nof the joint probability of words and parse structure; this was not possible in\nthe previous case due to the huge computational complexity of tha t model;\n• our model assigns probability at the word level, being a proper langua ge model.\n2.1 Syntactic Structure in Natural Language\nAlthough not complete, there is a certain agreement in the linguistics community\nas to what constitutes syntactic structure in natural language. In an eﬀort to provide\nthe computational linguistics community with a database that reﬂec ts the current\n11\n( (S\n(NP-SBJ\n(NP (NNP Pierre) (NNP Vinken) )\n(, ,)\n(ADJP\n(NP (CD 61) (NNS years) )\n(JJ old) )\n(, ,) )\n(VP (MD will)\n(VP (VB join)\n(NP (DT the) (NN board) )\n(PP-CLR (IN as)\n(NP (DT a) (JJ nonexecutive) (NN director) ))\n(NP-TMP (NNP Nov.) (CD 29) )))\n(. .) ))\nFigure 2.1: UPenn Treebank Parse Tree Representation\nbasic level of agreement, a treebank was developed at the Univers ity of Pennsylvania,\nknown as the UPenn Treebank [21]. The treebank contains sentenc es which were\nmanually annotated with syntactic structure. A sample parse tree from the tree-\nbank is shown in Figure 2.1. Each word bears a part of speech tag (POS tag): e.g.\nPierre is annotated as being a proper noun (NNP). Round brackets are used to mark\nconstituents, each constituent being tagged with a non-terminal label (NT label):\ne.g. (NP (NNP Pierre) (NNP Vinken) ) is marked as noun phrase (NP). Some non-\nterminal labels are enriched with additional information which is usually discarded\nas a ﬁrst approximation: e.g. NP-TMP becomes NP. The task of recovering the parsing\nstructure with POS/NT annotation for a given word sequence (sen tence) is referred\nto as automatic parsing of natural language (or simply parsing). A sub-task whose\naim is to recover the part of speech tags for a given word sequence is referred to as\nPOS-tagging.\nThis eﬀort fostered research in automatic part-of-speech tagg ing and parsing of\nnatural language, providing a base for developing and testing algor ithms that try to\ndescribe computationally the constraints in natural language.\n12\nState of the art parsing and POS-tagging technology developed in t he computa-\ntional linguistics community operates at the sentence level. Statist ical approaches\nemploy conditional probabilistic models P (T/W ) where W denotes the sentence to\nbe parsed and T is the hidden parse structure or POS tag sequence. Due to the\nleft-to-right constraint imposed by the speech recognizer on the language model op-\neration, we will be forced to develop syntactic structure for sent ence preﬁxes. This\nis just one of the limitations imposed by the fact that we aim at incorpo rating the\nlanguage model in a speech recognizer. Information that is presen t in written text\nbut silent in speech — such as case information (Pierre vs. pierre ) an d punctuation\n— will not be used by our model either.\nThe use of headwords has become standard in the computational lin guistics com-\nmunity: the headword of a phrase is the word that best represents the phrase, all\nthe other words in the phrase being modiﬁers of the headword. For example we refer\nto years as the headword of the phrase (NP (CD 61) (NNS years) ). The lexical-\nization — headword percolation — of the treebank has proven extre mely useful in\nincreasing the accuracy of automatic parsers.\nThere are ongoing arguments about the adequacy of the tree rep resentation for\nsyntactic dependencies in natural language. One argument debat es the usage of\nbinary branching — in which one word modiﬁes exactly one other word in the same\nsentence — versus trees with unconstrained branching. Learnab ility issues favor the\nformer, as argued in [16]. It is not surprising that the binary struct ure also lends itself\nto a simpler algorithmic description and is the choice for our modeling ap proach.\nAs an example, the output of the headword percolation and binariza tion procedure\nfor the parse tree in Figure 2.1 is presented in Figure 2.2. The headwo rds are now\npercolated at each intermediate node in the tree; the additional bit — value 0 or 1\n— indicates the origin of the headword in each constituent.\n2.1.1 Headword Percolation and Binarization\nIn order to obtain training data for our model we need to binarize th e UPenn Tree-\nbank [21] parse trees and percolate headwords. The procedure w e used was to ﬁrst\n13\nse~TOP~1\nSB\nsb\nse~TOP'~1\nwill~S~0\nwill~S'~1\nvinken~NP~0\nvinken~NP'~0\nvinken~NP'~0\nvinken~NP~1\nNNP\npierre\nNNP\nvinken\n,\n,\nold~ADJP~1\nyears~NP~1\nCD\nN\nNNS\nyears\nJJ\nold\n,\n,\nwill~VP~0\nMD\nwill\njoin~VP~0\njoin~VP'~0\njoin~VP'~0\nVB\njoin\nboard~NP~1\nDT\nthe\nNN\nboard\nas~PP~0\nIN\nas\ndirector~NP~1\nDT\na\ndirector~NP'~1\nJJ\nnonexecutive\nNN\ndirector\nN~NP~1\nNNP\nnov.\nCD\nN\n.\n.\nSE\nse\nsb pierre vinken , N years old , will join the board as a nonexecutive director nov. N . se\nFigure 2.2: Parse Tree Representation after Headword Percolatio n and Binarization\n14\npercolate headwords using a context-free (CF) rule-based appr oach and then binarize\nthe parses by again using a rule-based approach.\nHeadword Percolation\nInherently a heuristic process, we were satisﬁed with the output o f an enhanced\nversion of the procedure described in [11] — also known under the na me “Magerman\n& Black Headword Percolation Rules”.\nThe procedure ﬁrst decomposes a parse tree from the treebank into its context-\nfree constituents, identiﬁed solely by the non-terminal/POS labels. Within each con-\nstituent we then identify the headword position and then, in a recur sive third step,\nwe ﬁll in the headword position with the actual word percolated up fr om the leaves\nof the tree.\nThe headword percolation procedure is based on rules for identifyin g the headword\nposition within each constituent. They are presented in table 2.1.\nLet Z → Y1 . . . Y n be one of the context-free (CF) rules that make up a given\nparse. We identify the headword position as follows:\n• identify in the ﬁrst column of the table the entry that corresponds to the Z\nnon-terminal label;\n• search Y1 . . . Y n from either left or right, as indicated in the second column of the\nentry, for the Yi label that matches the regular expressions listed in the entry;\nthe ﬁrst matching Yi is going to be the headword of the ( Z (Y1 . . .) . . . (Yn . . .))\nconstituent; the regular expressions listed in one entry are ranke d in left to right\norder: ﬁrst we try to match the ﬁrst one, if unsuccessful we try the second one\nand so on.\nA regular expression of the type <_CD|~QP> matches any of the constituents listed\nbetween angular parentheses. For example, the <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nregular expression will match any constituent that is not — list begins with <^ —\namong any of the elements in the list between <^ and >, in this case any constituent\nwhich is not a punctuation mark. The terminal labels have _ prepended to them —\n15\nTOP right _SE _SB\nADJP right <~QP|_JJ|_VBN|~ADJP|_$|_JJR>\n<^~PP|~S|~SBAR|_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nADVP right <_RBR|_RB|_TO|~ADVP>\n<^~PP|~S|~SBAR|_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nCONJP left _RB <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nFRAG left <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nINTJ left <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nLST left _LS <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nNAC right <_NNP|_NNPS|~NP|_NN|_NNS|~NX|_CD|~QP|_VBG>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nNP right <_NNP|_NNPS|~NP|_NN|_NNS|~NX|_CD|~QP|_PRP|_V BG>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nNX right <_NNP|_NNPS|~NP|_NN|_NNS|~NX|_CD|~QP|_VBG>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nPP left _IN _TO _VBG _VBN ~PP\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nPRN left ~NP ~PP ~SBAR ~ADVP ~SINV ~S ~VP\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nPRT left _RP <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nQP left <_CD|~QP> <_NNP|_NNPS|~NP|_NN|_NNS|~NX> <_DT|_P DT>\n<_JJR|_JJ> <^_CC|_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nRRC left ~ADJP ~PP ~VP <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RR B>\nS right ~VP <~SBAR|~SBARQ|~S|~SQ|~SINV>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nSBAR right <~S|~SBAR|~SBARQ|~SQ|~SINV>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nSBARQ right ~SQ ~S ~SINV ~SBAR <^_.|_,|_’’|_‘‘|_‘|_’|_:|_ LRB|_RRB>\nSINV right <~VP|_VBD|_VBN|_MD|_VBZ|_VB|_VBG|_VBP> ~S ~S INV\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nSQ left <_VBD|_VBN|_MD|_VBZ|_VB|~VP|_VBG|_VBP>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nUCP left <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nVP left <_VBD|_VBN|_MD|_VBZ|_VB|~VP|_VBG|_VBP>\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nWHADJP right <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nWHADVP right _WRB <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nWHNP right _WP _WDT _JJ _WP$ ~WHNP\n<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nWHPP left _IN <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nX right <^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB>\nTable 2.1: Headword Percolation Rules\n16\nZ\nZ'\nZ'\nZ'\nB\nZ\nZ'\nZ'\nZ'\nA\nY_1             Y_k                 Y_nY_1               Y_k                 Y_n\nFigure 2.3: Binarization schemes\nas in _CD — the non-terminal labels have the ~ preﬁx — as in ~QP; | is merely a\nseparator in the list.\nBinarization\nOnce the position of the headword within a constituent — equivalent w ith a CF\nproduction of the type Z → Y1 . . . Y n , where Z, Y1, . . . Y n are non-terminal labels\nor POStags (only for Yi) — is identiﬁed to be k, we binarize the constituent as\nfollows: depending on the Z identity, a ﬁxed rule is used to decide which of the two\nbinarization schemes in Figure 2.3 to apply. The intermediate nodes cr eated by the\nabove binarization schemes receive the non-terminal label Z′.\nThe choice among the two schemes is made according to the list of rule s presented\nin table 2.2, based on the identity of the label on the left-hand-side o f a CF rewrite\nrule.\nNotice that whenever k = 1 or k = n — a case which is very frequent — the two\nschemes presented above yield the same binary structure.\nAnother problem when binarizing the parse trees is the presence of unary produc-\ntions. Our model allows only unary productions of the type Z → Y where Z is a\nnon-terminal label and Y is a POS tag. The unary productions Z → Y where both Z\nand Y are non-terminal labels were deleted from the treebank, only the Z constituent\nbeing retained: (Z (Y (.) (.))) becomes (Z (.) (.)).\n17\n## first column : constituent label\n## second column: binarization type : A or B\n## A means right modifiers go first, left branching, then left\n## modifiers are attached via right branching\n## B means left modifiers go first, right branching, then right\n## modifiers are attached via left branching\nTOP A\nADJP B\nADVP B\nCONJP A\nFRAG A\nINTJ A\nLST A\nNAC B\nNP B\nNX B\nPP A\nPRN A\nPRT A\nQP A\nRRC A\nS B\nSBAR B\nSBARQ B\nSINV B\nSQ A\nUCP A\nVP A\nWHADJP B\nWHADVP B\nWHNP B\nWHPP A\nX B\nTable 2.2: Binarization Rules\n18\n2.2 Exploiting Syntactic Structure for Language\nModeling\nConsider predicting the word after in the sentence:\nthe contract ended with a loss of 7 cents\nafter trading as low as 89 cents.\nA 3-gram approach would predict after from (7, cents) whereas it is intuitively\nclear that the strongest predictor would be contract ended which is outside the\nreach of even 7-grams. What would enable us to identify the predict ors in the sentence\npreﬁx?\nThe linguistically correct partial parse of the sentence preﬁx when predicting\nafter is shown in Figure 2.4. The word ended is called the headword of the con-\nstituent (ended (with (...))) and ended is an exposed headword when predicting\nafter — topmost headword in the largest constituent that contains it. Ou r working\nthe_DT   contract_NN  ended_VBD cents_NNS after\ncents_NP\nof_PP\nloss_NP\nloss_NP\nended_VP'\nwith_PP\ncontract_NP\nwith_IN a_DT   loss_NN   of_IN   7_CD\nFigure 2.4: Partial parse\nhypothesis is that the syntactic structure ﬁlters out irrelevant w ords and points to\nthe important ones, thus enabling the use of information in the more distant past\nwhen predicting the next word. We will attempt to model this using th e concept of\nexposed headwords introduced before.\nWe will give two heuristic arguments that justify the use of exposed headwords:\n• the 3-gram context for predicting after — (7, cents) — is intuitively less sat-\nisfying than using the two most recent exposed headwords (contract, ended)\n19\n— identiﬁed by the parse tree;\n• the headword context does not change if we remove the (of (7 cents)) con-\nstituent — the resulting sentence is still a valid one — whereas the 3-g ram\ncontext becomes (a, loss).\nThe preliminary experiments reported in [8] — although the perplexity results\nare conditioned on parse structure developed by human annotators by having the\nentire sentence at their disposal — showed the usefulness of head words accompanied\nby non-terminal labels for making a better prediction of the word fo llowing a given\nsentence preﬁx.\nOur model will attempt to build the syntactic structure incrementa lly while travers-\ning the sentence left-to-right. The word string W can be observed whereas the parse\nstructure with headword and POS/NT label annotation — denoted b y T — remains\nhidden. The model will assign a probability P (W, T ) to every sentence W with every\npossible POStag assignment, binary branching parse, non-termina l label and head-\nword annotation for every constituent of T .\nLet W be a sentence of length n words to which we have prepended <s> and\nappended </s> so that w0 =<s> and wn+1 =</s>. Let Wk be the word k-preﬁx\nw0 . . . w k of the sentence and WkTk the word-parse k-preﬁx . To stress this point, a\nword-parse k-preﬁx contains — for a given parse — those and only t hose binary sub-\ntrees whose span is completely included in the word k-preﬁx, excludin g w0 =<s>.\nSingle words along with their POStag can be regarded as root-only tr ees. Figure 2.5\nshows a word-parse k-preﬁx; h_0 .. h_{-m} are the exposed heads , each head be-\ning a pair(headword, non-terminal label), or (word, POStag) in the case of a root-\nonly tree. A complete parse — Figure 2.6 — is deﬁned as a binary parse of the\n(w1, t1) . . . (wn, tn) (</s>,SE) 1 sequence with the restriction that (</s>, TOP’) is\nthe only allowed head. Note that (( w1, t1) . . . (wn, tn)) needn’tbe a constituent, but\nfor the parses where it is, there is no a priori restriction on which of its words is the\nheadword or what is the non-terminal label that accompanies the h eadword. This is\n1SB is a distinguished POStag for the sentence beginning symbol ¡s¿; S E is a distinguished\nPOStag for the sentence end symbol ¡/s¿;\n20\n(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>\nh_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)\nFigure 2.5: A word-parse k-preﬁx\n(<s>, SB)  (w_1, t_1)  ..................... (w_n, t_n) (</s>, SE)\n(</s>, TOP')\n(</s>, TOP)\nFigure 2.6: Complete parse\none other notable diﬀerence between our model and the traditiona l ones developed in\nthe computational linguistics community imposed by the bottom-up o peration of the\nmodel. The manually annotated trees in the treebank (see Figure 2.2 ) have all the\nwords in a sentence as one single constituent bearing a restricted s et of non-terminal\nlabels: the sentence ( S(w1, t1) . . . (wn, tn)) is a constituent labeled with S.\nAs it can be observed the UPenn treebank -style trees are a subse t of the family\nof trees allowed by our parameterization, making a direct compariso n between our\nmodel and state of the art parsing techniques — which insist on gene rating UPenn\ntreebank -style parses — less meaningful.\nThe model will operate by means of three modules:\n• WORD-PREDICTOR predicts the next word wk+1 given the word-parse k-\npreﬁx WkTk and then passes control to the TAGGER;\n• TAGGER predicts the POStag tk+1 of the next word given the word-parse\nk-preﬁx and the newly predicted word wk+1 and then passes control to the\nPARSER;\n• PARSER grows the already existing binary branching structure by r epeatedly\n21\ngenerating transitions from the following set:\n(unary, NTlabel), (adjoin-left, NTlabel) or (adjoin-right, NTlabel)\nuntil it passes control to the PREDICTOR by taking a null transition. NTlabel\nis the non-terminal label assigned to the newly built constituent and {left,right}\nspeciﬁes where the new headword is percolated from.\nThe operations performed by the PARSER are illustrated in Figures 2 .7-2.9 and\nthey ensure that all possible binary branching parses with all possib le headword and\nnon-terminal label assignments for the w1 . . . w k word sequence can be generated.\nAlgorithm 1 at the end of this chapter formalizes the above descript ion of the sequen-\n<s> <s>\nT_{-m}\nh_{-1} h_0h_{-2}\n......... \nh_{-1} h_0h_{-2}\nT_{-2} T_{-1} T_0......... \nFigure 2.7: Before an adjoin operation\n...............\nT'_0\nT_{-1} T_0<s> T'_{-1}<-T_{-2}\nh_{-1} h_0\nh'_{-1} = h_{-2}\nT'_{-m+1}<-<s>\nh'_0 = (h_{-1}.word, NTtag)\nFigure 2.8: Result of adjoin-left under NTtag\n............... T'_{-1}<-T_{-2} T_0\nh_0h_{-1}\n<s>\nT'_{-m+1}<-<s>\nh'_{-1}=h_{-2}\nT_{-1}\nh'_0 = (h_0.word, NTtag)\nFigure 2.9: Result of adjoin-right under NTtag\ntial generation of a sentence with a complete parse. The unary tra nsition is allowed\nonly when the most recent exposed head is a leaf of the tree — a regu lar word along\nwith its POStag — hence it can be taken at most once at a given position in the\ninput word string. The second subtree in Figure 2.5 provides an exam ple of a unary\ntransition followed by a null transition.\n22\nIt is easy to see that any given word sequence with a possible parse a nd headword\nannotation is generated by a unique sequence of model actions. Th is will prove very\nuseful in initializing our model parameters from a treebank.\n2.3 Probabilistic Model\nThe language model operation provides an encoding of a given word s equence\nalong with a parse tree W, T into a sequence of elementary model actions and it can\nbe formalized as a ﬁnite state machine (FSM) — see Figure 2.10. In ord er to obtain\na correct probability assignment P (W, T ) one has to simply assign proper conditional\nprobabilities on each transition in the FSM that describes the model.\nPREDICTOR TAGGER\nPARSER\npredict  word\ntag word\nadjoin_{left,right}\nnull\nFigure 2.10: Language Model Operation as a Finite State Machine\nThe probability P (W, T ) of a word sequence W and a complete parse T can be\nbroken into:\nP (W, T ) =\nn+1∏\nk=1\n[P (wk|Wk−1Tk−1) · P (tk|Wk−1Tk−1, wk) · P (T k\nk−1|Wk−1Tk−1, wk, tk)] (2.1)\nP (T k\nk−1|Wk−1Tk−1, wk, tk) =\nNk∏\ni=1\nP (pk\ni |Wk−1Tk−1, wk, tk, pk\n1 . . . p k\ni−1) (2.2)\n23\nwhere:\n• Wk−1Tk−1 is the word-parse ( k − 1)-preﬁx\n• wk is the word predicted by WORD-PREDICTOR\n• tk is the tag assigned to wk by the TAGGER\n• T k\nk−1 is the parse structure attached to Tk−1 in order to generate\nTk = Tk−1 ∥ T k\nk−1\n• Nk − 1 is the number of operations the PARSER executes at position k of\nthe input string before passing control to the WORD-PREDICTOR ( the Nk-th\noperation at position k is the null transition); Nk is a function of T\n• pk\ni denotes the i-th PARSER operation carried out at position k in the wo rd\nstring:\npk\ni ∈ { (adjoin-left, NTtag), (adjoin-right, NTtag)}, 1 ≤ i < N k ,\npk\ni =null, i = Nk\nEach ( Wk−1Tk−1, wk, tk, pk\n1 . . . p k\ni−1) is a valid word-parse k-preﬁx WkTk at position\nk in the sentence, i =\n1, Nk.\nTo ensure a proper probabilistic model over the set of complete parses for any\nsentence W , certain PARSER and WORD-PREDICTOR probabilities must be given\nspeciﬁc values 2:\n• P (null|WkTk) = 1, if h_{-1}.word = <s> and h_{0} ̸= (</s>, TOP’) — that\nis, before predicting </s> — ensures that (<s>, SB) is adjoined in the last step\nof the parsing process;\n• – P ((adjoin-right, TOP)|WkTk) = 1,\nif h_0 = (</s>, TOP’) and h_{-1}.word = <s>\n2Not all the paths through the FSM that describes the language mod el will result in a correct\nbinary tree as deﬁned by the complete parse, Figure 2.6. In order t o prohibit such paths, we\nimpose a set of constraints on the probability values of diﬀerent mod el components, consistent with\nAlgorithm 1\n24\n– P ((adjoin-right, TOP’)|WkTk) = 1,\nif h_0 = (</s>, TOP’) and h_{-1}.word ̸= <s>\nensure that the parse generated by our model is consistent with t he deﬁnition\nof a complete parse;\n• ∃ ǫ > 0, ∀Wk−1Tk−1, P (wk=</s>|Wk−1Tk−1) ≥ ǫ ensures that the model halts\nwith probability one.\nA few comments on Eq. (2.1) are in order at this point. Eq. (2.1) assig ns probabil-\nity to a directed acyclic graph ( W, T ). Many other possible probability assignments\nare possible, and probably the most obvious choice would have been t he factorization\nused in context free grammars. Our choice is dictated by its simplicity and left-to-\nright bottom-up operation. This also leads to a proper and very simp le word level\nprobability estimate — see Section 2.6 — even when pruning the set of p arses T .\nOur factorization Eq. (2.1) assumes certain dependencies betwee n the nodes in the\ngraph ( W, T ). Also, in order to be able to reliably estimate the model components\nwe need to make appropriate equivalence classiﬁcations of the cond itioning part for\neach component, respectively. This is equivalent to making certain c onditional inde-\npendence assumptions which may not be — and probably are not — cor rect and thus\nhave a damaging eﬀect on the modeling power of our model.\nThe equivalence classiﬁcation should identify the strong predictors in the context\nand allow reliable estimates from a treebank. Our choice is inspired by [1 1] and\nintuitively explained in Section 2.2:\nP (wk|Wk−1Tk−1) = P (wk|[Wk−1Tk−1]) = P (wk|h0, h−1) (2.3)\nP (tk|wk, Wk−1Tk−1) = P (tk|wk, [Wk−1Tk−1]) = P (tk|wk, h0.tag, h−1.tag) (2.4)\nP (pk\ni |WkTk) = P (pk\ni |[WkTk]) = P (pk\ni |h0, h−1) (2.5)\nThe above equivalence classiﬁcations are limited by the severe data s parseness prob-\nlem faced by the 3-gram model and by no means do we believe that the y are adequate,\nespecially that used in PARSER model (2.5). Richer equivalence classiﬁ cations should\nuse a probability estimation method that deals better with sparse da ta than the\n25\none presented in section 2.4. The limit in complexity on the WORD-PREDI CTOR\n(Eq.2.3) also makes our model directly comparable with a 3-gram mode l. A few\ndiﬀerent equivalence classiﬁcations have been tried as described in s ection 4.2.1.\nIt is worth noting that if the binary branching structure developed by the parser\nwere always right-branching and we mapped the POStag and non-te rminal tag vo-\ncabularies to a single type, then our model would be equivalent to a tr igram language\nmodel.\n2.4 Modeling Tool\nAll model components — WORD-PREDICTOR, TAGGER, PARSER — are c on-\nditional probabilistic models of the type P (u|z1, z2, . . . , z n) where u, z1, z2, . . . , z n be-\nlong to a mixed set of words, POStags, non-terminal tags and pars er operations ( u\nonly). Let U be the vocabulary in which the predicted random variable u takes values.\nFor simplicity, the probability estimation method we chose was recurs ive linear\ninterpolation among relative frequency estimates of diﬀerent orde rs fk(·), k = 0 . . . n\nusing a recursive mixing scheme (see Figure 2.11):\nPn(u|z1, . . . , z n) =\nλ(z1, . . . , z n) · Pn−1(u|z1, . . . , z n−1) + (1 − λ(z1, . . . , z n)) · fn(u|z1, . . . , z n), (2.6)\nP−1(u) = uniform (U) (2.7)\nwhere:\n• z1, . . . , z n is the context of order n when predicting u;\n• fk(u|z1, . . . , z k) is the order-k relative frequency estimate for the conditional\nprobability P (u|z1, . . . , z k):\nfk(u|z1, . . . , z k) = C(u, z1, . . . , z k)/C(z1, . . . , z k), k = 0 . . . n,\nC(u, z1, . . . , z k) =\n∑\nzk+1∈Zk+1\n. . .\n∑\nzn∈Zn\nC(u, z1, . . . , z k, zk+1 . . . zn),\nC(z1, . . . , z k) =\n∑\nu∈U\nC(u, z1, . . . , z k),\n26\n• λ(z1, . . . , z k) are the interpolation coeﬃcients satisfying\nλ(z1, . . . , z k) ∈ [0, 1], k = 0 . . . n .\nP  (u|z  ...  z   )1 nn\nf  (u)0\n-1\nn\nn-1\nP  (u|z  ...  z    )1 n-1n-1\n0P  (u)\nf  (u|z  ...  z  )\nf  (u|z  ...  z  )\n1 n\n1 n-1\nP  (u)= 1/ |U| \nFigure 2.11: Recursive Linear Interpolation\nThe λ(z1, . . . , z k) coeﬃcients are grouped into equivalence classes — “tied” —\nbased on the range into which the count C(z1, . . . , z k) falls; the count ranges for each\nequivalence class — also called “buckets” — are set such that a statis tically suﬃcient\nnumber of events ( u|z1, . . . , z k) fall in that range. The approach is a standard one [18].\nIn order to determine the interpolation weights, we apply the delete d interpolation\ntechnique:\n• we split the training data in two sets — “development” and\n“cross-validation”, respectively;\n• we get the relative frequency — maximum likelihood — estimates\nfk(u|z1, . . . , z k), k = 0 . . . n from “development” data\n• we employ the expectation-maximization (EM) algorithm [14] for dete rmining\nthe maximum likelihood estimate from “cross-validation” data of the “ tied”\ninterpolation weights λ(C(z1, . . . , z k))3;\nWe have written a general deleted interpolation tool which takes as input:\n3The “cross-validation” data cannot be the same as the developmen t data; if this were the\ncase, the maximum likelihood estimate for the interpolation weights wo uld be λ(C(z1, . . . , z k)) =\n0, disallowing the mixing of diﬀerent order relative frequency estimat es and thus performing no\nsmoothing at all\n27\n• joint counts z1, z2, . . . , z n, u gathered from the “development” and ”cross-validation\ndata”, respectively\n• initial interpolation values and bucket descriptors for all levels in the deleted\ninterpolation scheme\nThe program runs a pre-speciﬁed number of EM iterations at each le vel in the deleted\ninterpolation scheme — from bottom up, k = 0 . . . n — and returns a descriptor ﬁle\ncontaining the estimated coeﬃcients.\nThe descriptor ﬁle can then be used for initializing the module and thus rendering\nit usable for the calculation of conditional probabilities P (u/z1, z2, . . . , z n). A sample\ndescriptor ﬁle for the deleted interpolation statistics module is show n in Table 2.3.\nThe deleted interpolation method is not optimal for our problem. Our models\nwould require a method able to optimally combine the predictors of diﬀe rent nature\nin the conditioning part of the model and this is far from being met by t he ﬁxed\nhierarchical scheme used for context mixing in deleted interpolation estimation. The\nbest method would be maximum entropy [5] but due to its computation al burden we\nhave not used it.\n2.5 Pruning Strategy\nSince the number of parses for a given word preﬁx Wk grows faster than expo-\nnential4 with k, Ω(2 k), the state space of our model is huge even for relatively short\nsentences. We thus have to prune most parses without discarding the most likely\nones for a given preﬁx Wk. Our pruning strategy is a synchronous multi-stack search\nalgorithm.\nEach stack contains hypotheses — partial parses — that have bee n constructed\nby the same number of predictor and the same number of parser ope rations. The\nhypotheses in each stack are ranked according to the ln( P (Wk, Tk)) score, highest on\ntop. The amount of search is controlled by two parameters:\n4Thanks to Bob Carpenter, Lucent Technologies Bell Labs, for poin ting out this inaccuracy in\nour [9] paper\n28\n## Stats_Del_Int descriptor file\n## $Id: del_int_descriptor.tex,v 1.3 1999/03/16 17:54:16 chelba Exp $\nStats_Del_Int::_main_counts_file = counts.devel.HH_w.E0.gz ;\nStats_Del_Int::_held_counts_file = counts.check.HH_w.E0.gz ;\nStats_Del_Int::_max_order = 4 ;\nStats_Del_Int::_no_iterations = 0 ;\nStats_Del_Int::_no_iterations_at_read_in = 100 ;\nStats_Del_Int::_predicted_vocabulary_chunk = 0 ;\nStats_Del_Int::_prob_Epsilon = 1e-07 ;\nStats_Del_Int::lambdas_level.0 = 2:__1__0.019 ;\nStats_Del_Int::buckets_level.0 = 2:__0__10000000 ;\nStats_Del_Int::lambdas_level.1 = 13:__1__0.5__0.5__0.5__0.5__0.5__1__1\n__0.449__1__0.260__0.138__0.073 ;\nStats_Del_Int::buckets_level.1 = 13:__0__1__2__4__8__16__32__64\n__128__256__512__1024__10000000 ;\nStats_Del_Int::lambdas_level.2 = 13:__1__0.853__0.787__0.745__0.692\n__0.637__0.579__0.489__0.427__0.358\n__0.296__0.258__0.213 ;\nStats_Del_Int::buckets_level.2 = 13:__0__1__2__4__8\n__16__32__64__128__256\n__512__1024__10000000 ;\nStats_Del_Int::lambdas_level.3 = 13:__1__0.935__0.905__0.878__0.855\n__0.812__0.743__0.686__0.633__0.595\n__0.548__0.515__0.517 ;\nStats_Del_Int::buckets_level.3 = 13:__0__1__2__4__8\n__16__32__64__128__256\n__512__1024__10000000 ;\nStats_Del_Int::lambdas_level.4 = 13:__1__0.887__0.859__0.838__0.801\n__0.761__0.710__0.627__0.586__0.532\n__0.523__0.485__0.532 ;\nStats_Del_Int::buckets_level.4 = 13:__0__1__2__4__8\n__16__32__64__128__256\n__512__1024__10000000 ;\nTable 2.3: Sample descriptor ﬁle for the deleted interpolation module\n29\n(k) (k') (k+1)\n0 parser 0 parser 0 parser\np parser op\n op\np parser op p parser op\np+1 parser p+1 parser p+1 parser \nP_k parser P_k parser P_k parser\nk+1 predict. k+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nP_k+1parserP_k+1parser\nword predictor\nand tagger\nparser adjoin/unary  transitions\nnull parser transitions\n op\nk predict.\nk predict.\nk predict.\nk predict.\n op\nFigure 2.12: One search extension cycle\n• the maximum stack depth — the maximum number of hypotheses the s tack can\ncontain at any given time;\n• log-probability threshold — the diﬀerence between the log-probabilit y score of\nthe top-most hypothesis and the bottom-most hypothesis at any given state of\nthe stack cannot be larger than a given threshold.\nFigure 2.12 shows schematically the operations associated with the s canning of a\nnew word wk+15.\nFirst, all hypotheses in a given stack-vector are expanded with th e following word.\nThen, for each possible POS tag the following word can take, we expa nd the hypothe-\nses further. Due to the ﬁnite stack size, some are discarded. We t hen proceed with\n5Pk is the maximum number of adjoin operations for a k-length word preﬁ x; since the tree is\nbinary we have Pk = k − 1\n30\nthe PARSER expansion cycle, which takes place in two steps:\n1. ﬁrst all hypotheses in a given stack are expanded with all possible PARSER\nactions excepting the null transition. The resulting hypotheses are sent to the\nimmediately lower stack of the same stack-vector — same number of WORD-\nPREDICTOR operations and exactly one more PARSER move. Some ar e dis-\ncarded due to ﬁnite stack size.\n2. after completing the previous step, all resulting hypotheses ar e expanded with\nthe null transition and sent into the next stack-vector. Pruning can still o ccur\ndue to the log-probability threshold on each stack.\nThe pseudo-code for parsing a given input sentence is given in Algorit hms 2- 4.\nSecond Pruning Step\nThe pruning strategy described so far proved to be insuﬃcient 6 so in order to\napproximately linearize the search eﬀort with respect to sentence length, we chose to\ndiscard also the hypotheses whose score is more than a ﬁxed log-pr obability relative\nthreshold below the score of the topmost hypothesis in the curren t stack vector.\nThis additional pruning step is performed after all hypotheses in st age k′ have been\nextended with the null parser transition.\nCashed TAGGER and PARSER Lists\nAnother opportunity for speeding up the search is to have a cache d list of possible\nPOStags/parser-operations in a given TAGGER/PARSER context. A good cache-ing\nscheme should use an equivalence classiﬁcation of the context that is speciﬁc enough\nto actually reduce the list of possible options and general enough to apply in almost\nall the situations. For the TAGGER model we cache the list of POStag s for a given\nword seen in the training data and scan only those in the TAGGER exte nsion cycle —\nsee Algorithm 3. For the PARSER model we cache the list of parser op erations seen\n6Assuming that all stacks contain the maximum number of entries — eq ual to the stack-depth\n— the search eﬀort grows squared with the sentence length\n31\nin a given ( h0.tag, h−1.tag) context in the training data; parses that expose heads\nwhose pair of NTtags has not been seen in the training data are disca rded— see\nAlgorithm 4.\n2.6 Word Level Perplexity\nAttempting to calculate the conditional perplexity by assigning to a w hole sentence\nthe probability:\nP (W |T ∗) =\nn∏\nk=0\nP (wk+1|WkT ∗\nk ), (2.8)\nwhere T ∗ = argmaxT P (W, T ) — the search for T ∗ being carried out according to\nour pruning strategy — is not valid because it is not causal: when pred icting wk+1\nwe would be using T ∗ which was determined by looking at the entire sentence. To be\nable to compare the perplexity of our model with that resulting from the standard\ntrigram approach, we would need to factor in the entropy of guess ing the preﬁx of\nthe ﬁnal best parse T ∗\nk before predicting wk+1, based solely on the word preﬁx Wk.\nTo maintain a left-to-right operation of the language model, the pro bability as-\nsignment for the word at position k + 1 in the input sentence was made using:\nP (wk+1|Wk) =\n∑\nTk∈Sk\nP (wk+1|WkTk) · ρ(Wk, Tk), (2.9)\nρ(Wk, Tk) = P (WkTk)/\n∑\nTk∈Sk\nP (WkTk)\nwhere Sk is the set of all parses present in our stacks at the current stage k. This\nleads to the following formula for evaluating the perplexity:\nP P L(SLM ) = exp(−1/N\nN∑\ni=1\nln [P (wi||Wi−1)]) (2.10)\nNote that if we set ρ(Wk, Tk) = δ(Tk, T ∗\nk |Wk) — 0-entropy guess for the preﬁx of\nthe parse Tk to equal that of the ﬁnal best parse T ∗\nk — the two probability assignments\n(2.8) and (2.9) would be the same, yielding a lower bound on the perplex ity achievable\nby our model when using a given pruning strategy.\n32\nAnother possibility for evaluating the word level perplexity of our mo del is to\napproximate the probability of a whole sentence:\nP (W ) =\nN∑\nk=1\nP (W, T (k)) (2.11)\nwhere T (k) is one of the “N-best” — in the sense deﬁned by our search — parses for\nW . This is a deﬁcient probability assignment, however useful for just ifying the model\nparameter re-estimation to be presented in Chapter 3.\nThe two estimates (2.9) and (2.11) are both consistent in the sense that if the\nsums are carried over all possible parses we get the correct value f or the word level\nperplexity of our model.\nAnother important observation is that the next-word predictor p robability\nP (wk+1|WkTk) in (2.9) need not be the same as the WORD-PREDICTOR proba-\nbility (2.3) used to extract the structure Tk, thus leaving open the possibility of\nestimating it separately. To be more speciﬁc, we can in principle have a WORD-\nPREDICTOR model component that operates within the parser mod el whose role\nstrictly to extract syntactic structure and a second model that is used only for the\nleft to right probability assignment:\nP2(wk+1|Wk) =\n∑\nTk∈Sk\nPW P (wk+1|WkTk) · ρ(Wk, Tk), (2.12)\nρ(Wk, Tk) = P (WkTk)/\n∑\nTk∈Sk\nP (WkTk) (2.13)\nIn this case the interpolation coeﬃcient given by 2.13 uses the regula r WORD-\nPREDICTOR model whereas the prediction of the next word for the purpose of\nword level probability assignment is made using a separate model PW P (wk+1|WkTk).\n33\nTransition t; // a PARSER transition\npredict (<s>, SB);\ndo{\n//WORD-PREDICTOR and TAGGER\npredict (next_word, POStag);\n//PARSER\ndo{\nif(h_{-1}.word != <s>){\nif(h_0.word == </s>)\nt = (adjoin-right, TOP’);\nelse{\nif(h_0.tag == NTlabel)\nt = [(adjoin-{left,right}, NTlabel),\nnull];\nelse\nt = [(unary, NTlabel),\n(adjoin-{left,right}, NTlabel),\nnull];\n}\n}\nelse{\nif(h_0.tag == NTlabel)\nt = null;\nelse\nt = [(unary, NTlabel), null];\n}\n}while(t != null) //done PARSER\n}while(!(h_0.word==</s> && h_{-1}.word==<s>))\nt = (adjoin-right, TOP); //adjoin <s>_SB; DONE;\nAlgorithm 1: Language Model Operation\n34\ncurrent_stack_vector // set of stacks at current input position\nfuture_stack_vector // set of stacks at future input positi on\nhypothesis // initial hypothesis\nstack // initial empty stack\n// initialize algorithm\ninsert hypothesis in stack;\npush stack at end of current_stack_vector;\n// traverse input sentence\nfor each position in input sentence{\nPREDICTOR and TAGGER extension cycle;\ncurrent_stack_vector = future_stack_vector;\nerase future_stack_vector;\nPARSER extension cycle;\ncurrent_stack_vector = future_stack_vector;\nerase future_stack_vector;\n}\n// output the hypothesis with the highest score;\noutput max scoring hypothesis in current_stack_vector;\nAlgorithm 2: Pruning Algorithm\ncurrent_stack_vector // set of stacks at current input position\nfuture_stack_vector // set of stacks at future input positi on\nword // word at current input position\nfor each stack in current_stack_vector{\n// based on number of predictor and parser operations\nidentify corresponding future_stack in future_stack_vector;\nfor each hypothesis in stack{\nfor all possible POStag assignments for word{ //CACHE-ING\nexpand hypothesis with word, POStag;\ninsert hypothesis in future_stack;\n}\n}\n}\nAlgorithm 3: PREDICTOR and TAGGER Extension Algorithm\n35\ncurrent_stack_vector // set of stacks at current input position\nfuture_stack_vector // set of stacks at future input positi on\n// all possible parser transitions but the null-transition\nfor each stack in current_stack_vector, from bottom up{\n// based on number of parser operations\nidentify corresponding future_stack in current_stack_vector;\nfor each hypothesis in current_stack{ // HARD PRUNING\nfor each parser_transition except the null-transition{//CACHE-ING\nexpand hypothesis with parser_transition;\ninsert hypothesis in future_stack;\n}\n}\n}\n// null-transition moves us to the next position in the input\nfor each stack in current_stack_vector{\n// based on number of predictor and parser operations\nidentify corresponding future_stack in future_stack_vector;\nfor each hypothesis in current_stack{\nexpand hypothesis with null-transition;\ninsert hypothesis in future_stack;\n}\n}\nprune future_stack_vector //SECOND PRUNING STEP\nAlgorithm 4: Parser Extension Algorithm\n36\nChapter 3\nStructured Language Model\nParameter Estimation\nAs outlined in section 2.6, the word level probability assigned to a train ing/test\nset by our model is calculated using the proper word-level probabilit y assignment\nin equation (2.9). An alternative which leads to a deﬁcient probability m odel is to\nsum over all the complete parses that survived the pruning strate gy, formalized in\nequation (2.11). Let the likelihood assigned to a corpus C by our model Pθ be denoted\nby:\n• L L2R(C, Pθ), where Pθ is calculated using (2.9), repeated here for clarity:\nP (wk+1|Wk) =\n∑\nTk∈Sk\nP (wk+1|WkTk) · ρ(Wk, Tk),\nρ(Wk, Tk) = P (WkTk)/\n∑\nTk∈Sk\nP (WkTk)\nNote that this is a proper probability model.\n• L N (C, Pθ), where Pθ is calculated using (2.11):\nP (W ) =\nN∑\nk=1\nP (W, T (k))\nThis is a deﬁcient probability model: due to the fact that we are not su mming\nover all possible parses for a given word sequence W — we discard most of them\n37\nthrough our pruning strategy — we underestimate the probability P (W ) and\nthus\n∑\nW P (W ) < 1.\nOne seeks to devise an algorithm that ﬁnds the model parameter va lues which\nmaximize the likelihood of a test corpus. This is an unsolved problem; th e standard\napproach is to resort to maximum likelihood estimation techniques on a training\ncorpus and make provisions that will ensure that the increase in likelih ood on training\ndata carries over to unseen test data.\nIn our case we would like to estimate the model component probabilitie s (2.3 –\n2.5). The smoothing scheme outlined in Section 2.4 is intended to preve nt overtraining\nand tries to ensure that maximum likelihood estimates on the training c orpus will\ncarry over to test data. Since our problem is one of maximum likelihood estimation\nfrom incomplete data — the parse structure along with POS/NT tags and headword\nannotation for a given observed sentence is hidden — our approach will make heavy\nuse of the EM algorithm variant presented in chapter 3.1.\nThe estimation procedure proceeds in two stages: ﬁrst the “N-be st training”\nalgorithm (see Section 3.2) is employed to increase the training data “ likelihood”\nLN (C, Pθ); we rely on the consistency property outlined at the end of Sectio n 2.6 to\ncorrelate the increase in LN (C, Pθ) with the desired increase of LL2R(C, Pθ). The initial\nparameters for this ﬁrst estimation stage are gathered from a tr eebank as described\nin Section 3.2.1.\nThe second stage estimates the model parameters such that LL2R(C, Pθ) is in-\ncreased. The basic idea is to realize that the WORD-PREDICTOR in the structured\nlanguage model (as described in chapter 2) and that used for word prediction in the\nLL2R(C, Pθ) calculation can be estimated as two separate components: one th at is\nused for structure generation and a second one which is used stric tly for predicting\nthe next word as described in equation (2.9). The initial parameters for the second\ncomponent are obtained by copying the WORD-PREDICTOR estimate d at stage\none.\nAs a ﬁnal step in reﬁning the model we have linearly interpolated the s tructured\nlanguage model (2.9) with a trigram model. Results and comments on t hem are\n38\npresented in the last section of the chapter.\n3.1 Maximum Likelihood Estimation from Incom-\nplete Data\nIn many practical situations we are confronted with the following sit uation: we are\ngiven a collection of data points T = {y1, . . . , y n}, yi ∈ Y — training data — which\nwe model as independent samples drawn from the Y marginal of the parametric\ndistribution:\nqθ(x, y), θ ∈ Θ , x ∈ X , y ∈ Y\nwhere X is referred to as the hidden variable and X as the hidden event space,\nrespectively. The set\nQ(Θ) .= {qθ(X, Y ) : θ ∈ Θ }\nis referred to as the model set . Let fT (Y ) be the relative frequency probability\ndistribution induced on Y by the collection T .\nWe wish to ﬁnd the maximum-likelihood estimate of θ:\nL(T ; qθ) .=\n∑\ny∈Y\nfT (y) log(\n∑\nx∈X\nqθ(x, y)) (3.1)\nθ∗ = arg max\nθ∈Θ\nL(T ; qθ) (3.2)\nStarting with an initial parameter value θi, it is shown that a suﬃcient condition\nfor increasing the likelihood of the training data T (see Eq. 3.1) is to ﬁnd a new\nparameter value θi+1 that maximizes the so called EM auxiliary function deﬁned as:\nEMT ,θ i (θ) .=\n∑\ny∈Y\nfT (y)Eqθi (X|Y )[log(qθ(X, Y )|y)], θ ∈ Θ (3.3)\nThe EM theorem proves that choosing:\nθi+1 = arg max\nθ∈Θ\nEMT ,θ i (θ) (3.4)\nensures that the likelihood of the training data under the new param eter value is not\nlower than that under the old one, formally:\nL(T ; qθi+1 ) ≥ L (T ; qθi ) (3.5)\n39\nUnder more restrictive conditions on the model family Q(Θ) it can be shown that\nthe ﬁxed points of the EM procedure — θi = θi+1 — are in fact local maxima of the\nlikelihood function L(T ; qθ), θ ∈ Θ. The study of convergence properties under dif-\nferent assumptions on the model class as well as diﬀerent ﬂavors o f the EM algorithm\nis an open area of research.\nThe fact that the algorithm is naturally formulated to operate with p robability\ndistributions — although this constraint can be relaxed — makes it att ractive from a\ncomputational point of view: an alternative to maximizing the training data likelihood\nwould be to apply gradient maximization techniques; this may be partic ularly diﬃcult\nif not impossible when the analytic description of the likelihood as a func tion of the\nparameter θ is complicated.\nTo further the understanding of the computational aspects of u sing the EM algo-\nrithm we notice that the EM update (3.4) involves two steps:\n• E-step: for each sample y in the training data T , accumulate the expectation\nof log( qθ(X, Y )|y) under the distribution qθi (x|y); no matter what the actual\nanalytic form of log( qθ(X, Y )) is, this requires to traverse all possible derivations\n(x, y) of the seen event y that have non-zero conditional probability qθi (X =\nx|Y = y) > 0;\n• M-step: ﬁnd maximizer of the auxiliary function (3.3).\nTypically the M-step is simple and the computational bottleneck is the E-step.\nThe latter becomes intractable with large training data set size and r ich hidden event\nspace, as usually required by practical problems.\nIn order to overcome this limitation, the model space Q(Θ) is usually structured\nsuch that dynamic programming techniques can be used for carryin g out the E-\nstep — see for example the hidden Markov model(HMM) parameter re estimation\nprocedure [3]. However this advantage does not come for free: in o rder to be able to\nstructure the model space we need to make independence assump tions that weaken\nthe modeling power of our parameterization. Fortunately we are no t in a hopeless\nsituation: a simple modiﬁcation of the EM algorithm allows the traversa l of only a\n40\nsubset of all possible ( x, y), x ∈ X | y for each training sample y — the procedure is\ndubbed “N-best training“ — thus rendering it applicable to a much bro ader and more\npowerful class of models.\n3.1.1 N-best Training Procedure\nBefore proceeding with the presentation of the N-best training pr ocedure, we\nwould like to introduce a view of the EM algorithm based on information g eometry.\nHaving gained this insight we can then easily justify the N-best trainin g procedure.\nThis is an interesting area of research to which we were introduced b y the presentation\nin [6].\nInformation Geometry and EM\nThe problem of maximum likelihood estimation from incomplete data can b e\nviewed in an interesting geometric framework. Before proceeding, let us introduce\nsome concepts and the associated notation.\nAlternating Minimization Consider the problem of ﬁnding the minimum Eu-\nclidean distance between two convex sets A and B:\nd∗ .= d(a∗, b∗) = min\na∈A,b ∈B\nd(a, b) (3.6)\nThe following iterative procedure(see ﬁgure 3.1) should lead to the s olution: start\n.\n.\n.\na\nb\na\na b\n1\n1\n2\n* *\nA B\nFigure 3.1: Alternating minimization between convex sets\n41\nwith a random point a1 ∈ A; ﬁnd the point b1 ∈ B closest to a1; then ﬁx b1 and\nﬁnd the point a2 ∈ A closest to b1 and so on. It is intuitively clear that the distance\nbetween the two points considered at each iteration cannot increa se and that the ﬁxed\npoint of the above procedure — the choice for the ( a, b) points does not change from\none iteration to the next — is the minimum distance d∗ between the sets A and B.\nFormalizing this intuition proves to be less simple for a more general se tup — the\nspeciﬁcation of sets A and B and the distance used. Csiszar and Tusnady have de-\nrived suﬃcient conditions under which the above alternating minimizat ion procedure\nconverges to the minimum distance between the two sets [13]. As out lined in [12],\nthis algorithm is applicable to problems in information theory — channel capacity\nand rate distortion calculation — as well as in statistics — the EM algorit hm.\nEM as alternating minimization Let Q(Θ) be the family of probability distribu-\ntions from which we want to choose the one maximizing the likelihood of t he training\ndata (3.1). Let us also deﬁne a family of desired distributions on X × Y whose Y\nmarginal induced by the training data is the same as the relative freq uency estimate\nfT (Y ):\nPT = {p(X, Y ) : p(Y ) = fT (Y )}\nFor any pair ( p, q) ∈ PT × Q(Θ), the Kullback-Leibler distance (KL-distance)\nbetween p and q is deﬁned as:\nD(p ∥ q) .=\n∑\nx∈X ,y ∈Y\np(x, y) log p(x, y)\nq(x, y) (3.7)\nAs shown in [13], under certain conditions on the families PT and Q(Θ) and using\nthe KL-distance, the alternating minimization procedure described in the previous\nsection converges to the minimum distance between the two sets:\nD(p∗ ∥ q∗) = min\np∈PT ,q ∈Q(Θ)\nD(p ∥ q) (3.8)\nIt can be easily shown (see appendix A) that the model distribution q∗ that\nsatisﬁes (3.8) is also the one maximizing the likelihood of the training dat a,\nq∗ = arg max\ngθ ∈Q(Θ)\nL(T ; qθ)\n42\nMoreover, the alternating minimization procedure leads exactly to t he EM update\nequation(3.3, 3.4), as shown in [13] and sketched in appendix B.\nThe PT and Q(Θ) families one encounters in practical situations may not satisfy\nthe conditions speciﬁed in [13]. However, one can easily note that dec rease in D(p ∥ q)\nat each step and correct I-projection from q ∈ Q(Θ) to PT — ﬁnding p ∈ PT such\nthat we minimize D(p ∥ q) — are suﬃcient conditions for ensuring that the likelihood\nof the training data does not decrease with each iteration. Since in p ractice we are\nbound by computational limitations and we typically run just a few iter ations, the\nguaranteed non-decrease in training data likelihood is suﬃcient.\n3.1.2 N-best Training\nIn the “N-best” training paradigm we use only a subset of the condit ional hidden\nevent space X |y, for any given seen y. Associated with the model space Q(Θ) we now\nhave a family of strategies to sample from X |y a set of “N-best” hidden events x, for\nany y ∈ Y . The family is parameterized by θ ∈ Θ:\nSθ\n.= {sθ : Y → 2X , ∀θ ∈ Θ } (3.9)\nWith the following deﬁnitions:\nqs\nθ(X, Y ) .= qθ(X, Y ) · 1sθ (Y )(X) (3.10)\nqs\nθ(X|Y ) .= qs\nθ(X, Y )\n∑\nX∈∫θ (Y ) qθ(X, Y ) · 1sθ (Y )(X) (3.11)\nQ(S, Θ) .= {qs\nθ(X, Y ) : θ ∈ Θ } (3.12)\nthe alternating minimization procedure between PT and Q(S, Θ) using the KL-\ndistance will ﬁnd a sequence of parameter values θ1, . . . , θ n for which the “likelihood”:\nLs(T ; qs\nθ) =\n∑\ny∈Y\nfT (y) log(\n∑\nx∈X\nqs\nθ(x, y)) (3.13)\nis monotonically increasing: Ls(T ; qs\nθ1 ) ≤ L s(T ; qs\nθ2 ) ≤ . . . ≤ L s(T ; qs\nθn ). Note that\ndue to the truncation of qθ(X, Y ) we are dealing with a deﬁcient probability model.\nThe parameter update at each iteration is very similar to that speciﬁ ed by the EM\nalgorithm under some suﬃcient conditions, as speciﬁed in Proposition 1 and proved\nin Appendix C:\n43\nProposition 1 Assuming that ∀θ ∈ Θ , Sup(qθ(x, y)) = X × Y (“smooth” qθ(x, y))\nholds, one alternating minimization step between PT and Q(S, Θ) — θi → θi+1 — is\nequivalent to:\nθi+1 = arg max\nθ∈Θ\n∑\ny∈Y\nfT (y)Eqs\nθi\n(X|Y )[log(qθ(X, Y )|y)] (3.14)\nif θi+1 satisﬁes:\nsθi (y) ⊆ sθi+1 (y), ∀y ∈ T (3.15)\nOnly θ ∈ Θ s.t. s θi (y) ⊆ sθ(y), ∀y ∈ T are candidates in the M-step.\nThe fact that we are working with a deﬁcient probability model for wh ich the\nsupport of the distributions qs\nθi (X|Y = y), ∀y ∈ T cannot decrease from one iteration\nto the next makes the above statement less interesting: even if we didn’t substantially\nchange the model parameters from one iteration to the next— θi+1 ≈ θi — but we\nchose the sampling function such that sθi (y) ⊂ sθi+1 (y), ∀y ∈ T the “likelihood”\nLs(T ; qs\nθ) would still be increasing due to the support expansion, although th e quality\nof the model has not actually increased.\nIn practice the family of sampling functions Sθ (3.9) is chosen such that support\nof qs\nθi (X|Y = y), ∀y ∈ T has constant size — cardinality, for discrete hidden spaces.\nTypically one retains the “N-best” after ranking the hidden sequen ces x ∈ X | y in\ndecreasing order according to qθi (X|Y = y), ∀y ∈ T . Proposition 1 implies that the\nset of “N-best” should not change from one iteration to the next, being an invariant\nduring model parameter reestimation. In practice however we rec alculate the “N-\nbest” after each iteration, allowing the possibility that new hidden se quences x are\nbeing included in the “N-best” list at each iteration and others discar ded. We do\nnot have a formal proof that this procedure will ensure monotonic increase of the\n“likelihood” Ls(T ; qs\nθ).\n3.2 First Stage of Model Estimation\nLet (W, T ) denote the joint sequence of W with parse structure T — headword and\nPOS/NT tag annotation included. As described in section 2.2, W, T was produced by\n44\na unique sequence of model actions: word-predictor, tagger, an d parser moves. The\nordered collection of these moves will be called a derivation:\nd(W, T ) .= ( e1, . . . , e l)\nwhere each elementary event\nei\n.= ( u(m)|z(m))\nidentiﬁes a model component action:\n• m denotes the model component that took the action,\nm ∈ { WORD-PREDICTOR, TAGGER, PARSER };\n• u is the action taken:\n– u is a word for m = WORD-PREDICTOR;\n– u is a POS tag for m = TAGGER;\n– u ∈ { (adjoin-left, NTtag), (adjoin-right, NTtag), null}\nfor m = PARSER;\n• h is the context in which the action is taken (see equations (2.3 – 2.5)):\n– z = h0.tag, h0.word, h−1.tag, h−1.word for m = WORD-PREDICTOR;\n– z = w, h0.tag, h−1.tag for m = TAGGER;\n– z = h−1.word, h−1.tag, h0.word, h0.tag for m = PARSER;\nFor each given ( W, T ) which satisﬁes the requirements in section 2.2 there is a unique\nderivation d(W, T ). The converse is not true, namely not every derivation corre-\nsponds to a correct ( W, T ); however, the constraints in section 2.3 ensure that these\nderivations receive 0 probability.\nThe probability of a ( W, T ) sequence is obtained by chaining the probabilities of\nthe elementary events in its derivation, as described in section 2.3:\nP (W, T ) = P (d(W, T )) =\nlength(d(W,T ))∏\ni=1\np(ei) (3.16)\n45\nThe probability of an elementary event is calculated using the smooth ing technique\npresented in section 2.4 and repeated here for clarity of explanatio n:\nPn(u|z1, . . . , z n) =\nλ(z1, . . . , z n) · Pn−1(u|z1, . . . , z n−1) + (1 − λ(z1, . . . , z n)) · fn(u|z1, . . . , z n), (3.17)\nP−1(u) = uniform (U) (3.18)\n• z1, . . . , z n is the context of order n when predicting u; U is the vocabulary in\nwhich u takes values;\n• fk(u|z1, . . . , z k) is the order-k relative frequency estimate for the conditional\nprobability P (u|z1, . . . , z k):\nfk(u|z1, . . . , z k) = C(u, z1, . . . , z k)/C(z1, . . . , z k), k = 0 . . . n,\nC(u, z1, . . . , z k) =\n∑\nzk+1∈Z\n. . .\n∑\nzn∈Z\nC(u, z1, . . . , z k, zk+1 . . . zn),\nC(z1, . . . , z k) =\n∑\nu∈U\nC(u, z1, . . . , z k),\n• λk are the interpolation coeﬃcients satisfying 0 < λ k < 1, k = 0 . . . n .\nThe λ(z1, . . . , z k) coeﬃcients are grouped into equivalence classes — “tied” —\nbased on the range into which the count C(z1, . . . , z k) falls; the count ranges for\neach equivalence class are set such that a statistically suﬃcient num ber of events\n(u|z1, . . . , z k) fall in that range.\nThe parameters of a given model component m are:\n• the maximal order counts C(m)(u, z1, . . . , z n);\n• the count ranges for grouping the interpolation values into equivale nce classes\n— “tying”;\n• the interpolation value for each equivalence class;\n46\nAssuming that the count ranges and the corresponding interpolat ion values for each\norder are kept ﬁxed to their initial values — see section 3.2.1 — the only parameters to\nbe reestimated using the EM algorithm are the maximal order counts C(m)(u, z1, . . . , z n)\nfor each model component.\nIn order to avoid traversing the entire hidden space for a given obs erved word\nsequence1 we use the “N-best” training approach presented in section 3.1.1 fo r which\nthe sampling strategy is the same as the pruning strategy present ed in section 2.5.\nThe derivation of the reestimation formulas is presented in appendix D. The E-\nstep is the one presented in section 3.1.2; the M-step takes into acc ount the smoothing\ntechnique presented above (equation (3.17)).\nNote that due to both the smoothing involved in the M-step and the f act that the\nset of sampled “N-best” hidden events — parses — are reevaluated at each iteration we\nallow new maximal order events to appear in each model component w hile discarding\nothers. Not only are we estimating the counts of maximal order n-gram events in\neach model component — WORD-PREDICTOR, TAGGER, PARSER — but we also\nallow the distribution on types to change from one iteration to the other . This is\nbecause the set of hidden events allowed for a given observed word sequence is not\ninvariant — as it is the case in regular EM. For example, the count set t hat describes\nthe WORD-PREDICTOR component of the model to be used at the ne xt iteration is\ngoing to have a diﬀerent n-gram composition than that used at the c urrent iteration.\nThis change is presented in the experiments section, see Table 4.4.\n3.2.1 First Stage Initial Parameters\nEach model component — WORD-PREDICTOR, TAGGER, PARSER — is ini-\ntialized from a set of hand-parsed sentences — in this case are going to use the\nUPenn Treebank manually annotated sentences — after undergoin g headword perco-\nlation and binarization, as explained in section 2.1.1. This is a subset — ap prox. 90%\n— of the training data. Each parse tree ( W, T ) is then decomposed into its derivation\nd(W, T ). Separately for each m model component, we:\n1normally required in the E-step\n47\n• gather joint counts C(m)(u(m), z(m)) from the derivations that make up the “de-\nvelopment data” using ρ(W, T ) = 1 (see appendix D);\n• estimate the interpolation coeﬃcients on joint counts gathered fr om “check\ndata” — the remaining 10% of the training data — using the EM algorithm [14].\nThese are the initial parameters used with the reestimation proced ure described\nin the previous section.\n3.3 Second Stage Parameter Reestimation\nIn order to improve performance, we develop a model to be used st rictly for word\nprediction in (2.9), diﬀerent from the WORD-PREDICTOR model (2.3) . We will\ncall this new component the L2R-WORD-PREDICTOR.\nThe key step is to recognize in (2.9) a hidden Markov model (HMM) with ﬁxed\ntransition probabilities — although dependent on the position in the inp ut sentence\nk — speciﬁed by the ρ(Wk, Tk) values.\nThe E-step of the EM algorithm [14] for gathering joint counts C(m)(y(m), x(m)),\nm = L2R-WORD-PREDICTOR-MODEL, is the standard one whereas the M-step\nuses the same count smoothing technique as that described in sect ion 3.2.\nThe second reestimation pass is seeded with the m = WORD-PREDICTOR model\njoint counts C(m)(y(m), x(m)) resulting from the ﬁrst parameter reestimation pass (see\nsection 3.2).\n48\nChapter 4\nExperiments using the Structured\nLanguage Model\nFor convenience, we chose to work on the UPenn Treebank corpus [21] — a subset\nof the WSJ (Wall Street Journal) corpus. The vocabulary sizes wer e:\nword vocabulary: 10k, open — all words outside the vocabulary are mapped to the\n<unk> token; POS tag vocabulary: 40, closed; non-terminal tag vocabu lary: 52,\nclosed; parser operation vocabulary: 107, closed. The training da ta was split into\ndevelopment set (929,564wds (sections 00-20)), check set (73,7 60wds (sections 21-\n22)) and the test data consisted of 82,430wds (sections 23-24). The “check” set was\nused strictly for initializing the model parameters as described in sec tion 3.2.1; the\n“development” set was used with the reestimation techniques desc ribed in chapter 3.\n4.1 Perplexity Results\nTable 4.1 shows the results of the reestimation techniques; E0-3 and L2R0-5 de-\nnote iterations of the reestimation procedure described in section s 3.2 and 3.3, respec-\ntively. A deleted interpolation trigram model derived from the same t raining data\nhad perplexity 167.14 on the same test data.\n49\niteration DEV set TEST set\nnumber L2R-PPL L2R-PPL\nE0 24.70 167.47\nE1 22.34 160.76\nE2 21.69 158.97\nE3 = L2R0 21.26 158.28\nL2R5 17.44 153.76\nTable 4.1: Parameter reestimation results\nSimple linear interpolation between our model and the trigram model:\nQ(wk+1/Wk) = λ · P (wk+1/wk−1, wk) + (1 − λ) · P (wk+1/Wk)\nyielded a further improvement in PPL, as shown in Table 4.2. The interp olation\nweight was estimated on check data to be λ = 0 .36. An overall relative reduction of\n11% over the trigram model has been achieved.\niteration TEST set TEST set\nnumber L2R-PPL 3-gram interpolated PPL\nE0 167.47 152.25\nE3 158.28 148.90\nL2R5 153.76 147.70\nTable 4.2: Interpolation with trigram results\nAs outlined in section 2.6, the perplexity value calculated using (2.8):\nP (W |T ∗) =\nn∏\nk=0\nP (wk+1|WkT ∗\nk ), T ∗ = argmaxT P (W, T )\nis a lower bound for the achievable perplexity of our model; for the ab ove search\nparameters and E3 model statistics this bound was 99.60, corresp onding to a relative\nreduction of 41% over the trigram model. This suggests that a bett er parameterization\nin the PARSER model — one that reduces the entropy H(ρ(Tk|Wk)) of guessing the\n“good” parse given the word preﬁx — can lead to a better model. Ind eed, as we\nalready pointed out, the trigram model is a particular case of our mo del for which the\n50\nparse is always right branching and we have no POS/NT tag informatio n, leading to\nH(ρ(Tk|Wk)) = 0 and a standard 3-gram WORD-PREDICTOR. The 3-gram model\nis thus an extreme case of the structured language model: one for which the “hidden”\nstructure is a function of the word preﬁx . Our result shows that better models can\nbe obtained by allowing richer “hidden” structure — parses — and tha t a promising\ndirection of research may be to ﬁnd the best compromise between t he predictive\npower of the WORD-PREDICTOR — measured by H(wk+1|Tk, Wk))— and the ease\nof guessing the hidden structure Tk|Wk — measured by H(ρ(Tk|Wk)) — on which the\nWORD-PREDICTOR operation is based. A better solution would be a ma ximum\nentropy PARSER model which incorporates a richer set of predicto rs in a better\nway than the deleted interpolation scheme we are using. Due to the c omputational\nproblems faced by such a model we have not pursued this path altho ugh we consider\nit a very promising one.\n4.1.1 Comments and Experiments on Model Parameters Rees-\ntimation\nThe word level probability assigned to a training/test set by our mod el is cal-\nculated using the proper word-level probability assignment in equat ion (2.9). An\nalternative which leads to a deﬁcient probability model is to sum over a ll the com-\nplete parses that survived the pruning strategy, formalized in equ ation (2.11). Let\nthe likelihood assigned to a corpus C by our model Pθ be denoted by:\n• L L2R(C, Pθ), where Pθ is calculated using (2.9), repeated here for clarity:\nP (wk+1|Wk) =\n∑\nTk∈Sk\nP (wk+1|WkTk) · ρ(Wk, Tk),\nρ(Wk, Tk) = P (WkTk)/\n∑\nTk∈Sk\nP (WkTk)\nNote that this is a proper probability model.\n• L N (C, Pθ), where Pθ is calculated using (2.11):\nP (W ) =\nN∑\nk=1\nP (W, T (k))\n51\nThis is a deﬁcient probability model.\nOne seeks to devise an algorithm that ﬁnds the model parameter va lues which\nmaximize the likelihood of a test corpus. This is an unsolved problem; th e standard\napproach is to resort to maximum likelihood estimation techniques on t he training\ncorpus and make provisions that will ensure that the increase in likelih ood on training\ndata carries over to unseen test data.\nAs outlined previously, the estimation procedure of the SLM parame ters takes\nplace in two stages:\n1. the “N-best training” algorithm (see Section 3.2) is employed to inc rease the\ntraining data “likelihood” LN (C, Pθ). The initial parameters for this ﬁrst esti-\nmation stage are gathered from a treebank. The perplexity is still e valuated\nusing the formula in Eq. (2.9).\n2. estimate a separate L2R-WORD-PREDICTOR model such that LL2R(C, Pθ)\nis increased — see Eq. (2.12). The initial parameters for the L2R-WO RD-\nPREDICTOR component are obtained by copying the WORD-PREDICT OR\nestimated at stage one.\nAs explained in Section 4.1.1, the “N-best training” algorithm is employe d to\nincrease the training data “likelihood” LN (C, Pθ); we rely on the consistency of the\nprobability estimates underlying the calculation of the two diﬀerent lik elihoods to\ncorrelate the increase in LN (C, Pθ) with the desired increase of LL2R(C, Pθ).\nTo be more speciﬁc, LN (C, Pθ) and LL2R(C, Pθ) are calculated using the probability\nassignments in Eq. (2.11) — deﬁcient — and Eq. (2.9), respectively. B oth probability\nestimates are consistent in the sense that if we summed over all the parses T for a\ngiven word sequence W they would yield the correct probability P (W ) according to\nour model. Although there is no formal proof, there are reasons t o believe that the\nN-best reestimation procedure should not decrease the LN (C, Pθ) likelihood 1 but no\nclaim can be made about the increase in the LL2R(C, Pθ) likelihood — which is the one\n1It is very similar to a rigorous EM approach\n52\nwe are interested in. Our experiments show that the increase in LN (C, Pθ) is corre-\nlated with an increase in LL2R(C, Pθ), a key factor in this being a good heuristic search\nstrategy — see Section 2.5. Table 4.3 shows the evolution of diﬀerent “perplexity” val-\nues during N-best reestimation. L2R-PPL is calculated using the pro per probability\nassignment in Eq.(2.9). TOP-PPL and BOT-PPL are calculated using th e probability\nassignment in Eq.(2.8), where T ∗ = argmaxT P (W, T ) and T ∗ = argminT P (W, T ),\nrespectively — the search for T ∗ being carried out according to our pruning strategy;\nwe condition the word predictions on the topmost and bottom-most parses present\nin the stacks after parsing the entire sentence. SUM-PPL is calcula ted using the\ndeﬁcient probability assignment in Eq.(2.11). It can be noticed that T OP-PPL and\nBOT-PPL stay almost constant during the reestimation process; T he value of TOP-\nPPL is slightly increasing and that of BOT-PPL is slightly decreasing. As expected,\nthe value of the SUM-PPL decreases and its decrease is correlated with that of the\nL2R-PPL.\n“Perplexity” Iteration Relative Change\nE0 E3\nTOP-PPL 97.5 99.3 +1.85%\nBOT-PPL 107.9 106.2 -1.58%\nSUM-PPL 195.1 175.5 -10.05%\nL2R-PPL 167.5 158.3 -5.49%\nTable 4.3: Evolution of diﬀerent ”perplexity” values during training\nIt is very important to note that due to both the smoothing involved in the M-step\n— imposed by the smooth parameterization of the model 2 — and the fact that the set\nof sampled “N-best” hidden events — parses — are reevaluated at e ach iteration, we\nallow new maximal order events to appear in each model component w hile discarding\nothers. Not only are we estimating the counts of maximal order n-gram events in each\nmodel component — WORD-PREDICTOR, TAGGER, PARSER — but we also allow\n2Unlike standard parameterizations, we do not reestimate the relat ive frequencies from which\neach component probabilistic model is derived; that would lead to a sh rinking or, at best, ﬁxed set\nof events\n53\nthe distribution on types to change from one iteration to the other. This is because\nthe set of hidden events allowed for a given observed word sequenc e is not invariant.\nFor example, the count set that describes the WORD-PREDICTOR c omponent of\nthe model to be used at the next iteration may have a diﬀerent n-gr am composition\nthan that used at the current iteration.\nWe evaluated the change in the distribution on types 3 of the maximal order events\n(y(m), x(m)) from one iteration to the next. Table 4.4 shows the dynamics of the set\nof types of the diﬀerent order events during the reestimation pro cess for the WORD-\nPREDICTOR model component. Similar dynamics were observed for t he other two\ncomponents of the model. The equivalence classiﬁcations correspo nding to each order\nis:\n• z = h0.tag, h0.word, h−1.tag, h−1.word for order 4;\n• z = h0.tag, h0.word, h−1.tag for order 3;\n• z = h0.tag, h0.word for order 2;\n• z = h0.tag for order 1;\nAn event of order 0 consists of the predicted word only.\niteration no. tokens no. types for order\n0 1 2 3 4\nE0 929,564 9,976 77,225 286,329 418,843 591,505\nE1 929,564 9,976 77,115 305,266 479,107 708,135\nE2 929,564 9,976 76,911 305,305 482,503 717,033\nE3 929,564 9,976 76,797 307,100 490,687 731,527\nL2R0 (=E3) 929,564 9,976 76,797 307,100 490,687 731,527\nL2R1-5 929,564 9,976 257,137 2,075,103 3,772,058 5,577,709\nTable 4.4: Dynamics of WORD-PREDICTOR distribution on types during\nreestimation\n3A type is a particular value, regarded as one entry in the alphabet sp anned by a given random\nvariable\n54\nThe higher order events — closer to the root of the linear interpolat ion scheme in\nFigure 2.11 — become more and more diverse during the ﬁrst estimatio n stage, as\nopposed to the lower order events. This shows that the “N-best” parses for a given\nsentence change from one iteration to the next. Although the E0 c ounts were col-\nlected from “1-best” parses — binarized treebank parses — the inc rease in number\nof maximal order types from E0 to E1 — collected from “N-best”, N = 10 — is far\nfrom dramatic, yet higher than that from E1 to E2 — both collected f rom “N-best”\nparses.\nThe big increase in number of types from E3 (=L2R0) to L2R1 is due to the fact\nthat at each position in the input sentence, WORD-PREDICTOR coun ts are now\ncollected for all the parses in the stacks, many of which do not belon g to the set of\nN-best parses for the complete sentence used for gathering counts during E0-3.\nAlthough the perplexity on test data still decreases during the sec ond reestimation\nstage — we are not over-training — this decrease is very small and no t worth the\ncomputational eﬀort if the model is linearly interpolated with a 3-gra m model, as\nshown in Table 4.2. Better integration of the 3-gram and the head pr edictors is\ndesirable.\n4.2 Miscellaneous Other Experiments\n4.2.1 Choosing the Model Components Parameterization\nThe experiments presented in [8] show the usefulness of the two mo st recent ex-\nposed heads for word prediction. The same criterion — conditional p erplexity —\ncan be used as a guide in selecting the parameterization of each mode l component:\nWORD-PREDICTOR, TAGGER, PARSER. For each model component w e gather\nthe counts from the UPenn Treebank as explained in Section 3.2.1. Th e relative\nfrequencies are determined from the “development” data, the int erpolation weights\nestimated on “check” data — as described in Section 3.2.1. We then te st each model\ncomponent on counts gathered from the “test” data. Note that the smoothing scheme\ndescribed in Section 2.4 discards elements of the context z\nfrom right to left.\n55\nSelecting the WORD-PREDICTOR Equivalence Classiﬁcation\nThe experiments in [8] were repeated using deleted interpolation as a modeling tool\nand the training/testing setup described above. The results for d iﬀerent equivalence\nclassiﬁcations of the word-parse k-preﬁx ( Wk, Tk) are presented in Table 4.5. The\nEquivalence Classiﬁcation Cond. PPL Voc. Size\nHH z = h0.tag, h0.word, h−1.tag, h−1.word 115 10,000\nWW z = w−1.tag, w−1.word, w−2.tag, w−2.word 156 10,000\nhh z = h0.word, h−1.word 154 10,000\nww z = w−1.word, w−2.word 167 10,000\nTable 4.5: WORD-PREDICTOR conditional perplexities\ndiﬀerent equivalence classiﬁcations of the word-parse k-preﬁx re tain the following\npredictors:\n1. ww: the two previous words — regular 3-gram model;\n2. hh: the two most recent exposed headwords — no POS/NT label in formation;\n3. WW: the two previous exposed words along with their POS tags;\n4. HH: the two most recent exposed heads — headwords along with t heir NT/POS\nlabels;\nIt can be seen that the most informative predictors for the next w ord are the exposed\nheads — HH model. Except for the ww model 4, none of the others is a valid word-\nlevel perplexity since it conditions the prediction on hidden informatio n (namely the\ntags present in the treebank parses); the entropy of guessing t he hidden information\nwould need to be factored in.\nSelecting the TAGGER Equivalence Classiﬁcation\nThe results for diﬀerent equivalence classiﬁcations of the word-pa rse k-preﬁx ( Wk, Tk)\nfor the TAGGER model are presented in Table 4.6. The diﬀerent equiv alence classi-\n4regular 3-gram model\n56\nEquivalence Classiﬁcation Cond. PPL Voc. Size\nHHw z = wk, h0.tag, h0.word, h−1.tag, h−1.word 1.23 40\nWWw z = wk, w−1.tag, w−1.word, w−2.tag, w−2.word 1.24 40\nttw z = wk, h0.tag, h−1.tag 1.24 40\nTable 4.6: TAGGER conditional perplexities\nﬁcations of the word-parse k-preﬁx retain the following predictor s:\n1. WWw: the two previous exposed words along with their POS tags an d the word\nto be tagged;\n2. HHw: the two most recent exposed heads — headwords along with their NT/POS\nlabels and the word to be tagged;\n3. ttw: the NT/POS labels of the two most recent exposed heads an d the word to\nbe tagged;\nIt can be seen that among the equivalence classiﬁcations considere d, none performs\nsigniﬁcantly better than the others, and the prediction of the POS tag for a given\nword is a relatively easy task — the conditional perplexities are very c lose to one.\nBecause of its simplicity, we chose to work with the ttw equivalence cla ssiﬁcation.\nSelecting the PARSER Equivalence Classiﬁcation\nThe results for diﬀerent equivalence classiﬁcations of the word-pa rse k-preﬁx ( Wk, Tk)\nfor the PARSER model are presented in Table 4.7. The diﬀerent equiv alence classi-\nEquivalence Classiﬁcation Cond. PPL Voc. Size\nHH z = h0.tag, h0.word, h−1.tag, h−1.word 1.68 107\nhhtt z = h0.tag, h−1.tag, h0.word, h−1.word 1.54 107\ntt z = h0.tag, h−1.tag 1.71 107\nTable 4.7: PARSER conditional perplexities\nﬁcations of the word-parse k-preﬁx retain the following predictor s:\n57\n1. HH: the two most recent exposed heads — headwords along with t heir NT/POS\nlabels and the word to be tagged;\n2. hhtt: same as HH just that the backing-oﬀ order is changed;\n3. ttw: the NT/POS labels of the two most recent exposed heads;\nIt can be seen that the presence of headwords improves the accu racy of the PARSER\ncomponent; also, the backing-oﬀ order of the predictors is import ant — hhtt vs. HH.\nWe chose to work with the hhtt equivalence classiﬁcation.\n4.2.2 Fudged TAGGER and PARSER Scores\nThe probability values for the three model components fall into diﬀe rent ranges.\nAs pointed out at the beginning of this chapter, the WORD-PREDICT OR vocabulary\nis of the order of thousands whereas the TAGGER and PARSER have vocabulary\nsizes of the order of tens. This leads to the undesirable eﬀect that the contribution of\nthe TAGGER and PARSER to the overall probability of a given partial p arse P (W, T )\nis very small compared to that of the WORD-PREDICTOR. We explore d the idea\nof bringing the probability values into the same range by fudging the TAGGER and\nPARSER probability values, namely:\nP (W, T ) =\nn+1∏\nk=1\n[P (wk|Wk−1Tk−1) ·\n{\nP (tk|Wk−1Tk−1, wk) · P (T k\nk−1|Wk−1Tk−1, wk, tk)\n} γ\n] (4.1)\nP (T k\nk−1|Wk−1Tk−1) =\nNk∏\ni=1\nP (pk\ni |Wk−1Tk−1, wk, tk, pk\n1 . . . p k\ni−1) (4.2)\nwhere γ is the fudge factor. For γ ̸= 1 .0 we do not have a valid probability assignment\nanymore, however the L2R-PPL calculated using Eq. (2.9) is still a va lid word-level\nprobability assignment due to the re-normalization of the interpolat ion coeﬃcients.\nTable 4.8 shows the PPL values calculated using Eq. (2.9) where P (W, T ) is calculated\nusing Eq. (4.1). As it can be seen the optimal fudge factor turns ou t to be 1.0,\ncorresponding to the correct calculation of the probability P (W, T ).\n58\nfudge 0.01 0.02 0.05 0.1 0.2 0.5 1.0 2.0 5.0 10.0 20.0 50.0 100.0\nPPL 341 328 296 257 210 168 167 189 241 284 337 384 408\nTable 4.8: Perplexity Values: Fudged TAGGER and PARSER\n4.2.3 Maximum Depth Factorization of the Model\nThe word level probability assignment used by the SLM — Eq. (2.9) — ca n be\nthought of as a model factored over diﬀerent maximum reach dept hs. Let D(Tk) be\nthe “depth” in the word-preﬁx Wk at which the headword h−1.word can be found.\nEq. (2.9) can be rewritten as:\nP (wk+1|Wk) =\nd=k∑\nd=0\nP (d|Wk) · P (wk+1|Wk, d), (4.3)\nwhere:\nP (d|Wk) =\n∑\nTk∈Sk\nρ(Wk, Tk) · δ(D(Tk), d)\nP (wk+1|Wk, d) =\n∑\nTk∈Sk\nP (Tk|Wk, d) · P (wk+1|Wk, Tk)\nP (Tk|Wk, d) = ρ(Wk, Tk) · δ(D(Tk), d)/P (d|Wk)\nWe can interpret Eq. (4.3) as a linear interpolation of models that rea ch back to\ndiﬀerent depths in the word preﬁx Wk. The expected value of D(Tk) shows how far\ndoes the SLM reach in the word preﬁx:\nESLM [D] = 1 /N\nk=N∑\nk=0\nd=k∑\nd=0\nd · P (d|Wk) (4.4)\nFor the 3-gram model we have E3−gram [D] = 2. We evaluated the expected depth\nof the SLM using the formula in Eq. (4.4). The results are presented in Table 4.9.\nIt can be seen that the memory of the SLM is considerably higher tha n that of the\n3-gram model — whose depth is 2.\nFigure 4.1 shows 5 the distribution P (d|Wk), averaged over all positions k in the\n5The nonzero value of P (1|W ) is due to the fact that the prediction of the ﬁrst word in a sentenc e\nis based on context of length 1 in both SLM and 3-gram models\n59\niteration expected depth\nnumber E[D]\nE0 3.35\nE1 3.46\nE2 3.45\nTable 4.9: Maximum Depth Evolution During Training\ntest string:\nP (d|W ) = 1 /N\nN∑\nk=1\nP (d|Wk)\n0 5 10 15 20 25\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ndepth\nP(depth)\nDepth distribution according to P(T/W)\nE[depth(E0)] = 3.35\nE[depth(E1)] = 3.46\nFigure 4.1: Structured Language Model Maximum Depth Distribution\nIt can be seen that the SLM makes a prediction which reaches farth er than the\n3-gram model in about 40% of cases, on the average.\n60\nChapter 5\nA∗ Decoder for Lattices\n5.1 Two Pass Decoding Techniques\nIn a two-pass recognizer, a computationally cheap decoding step is run in the\nﬁrst pass, a set of hypotheses is retained as an intermediate resu lt and then a more\nsophisticated recognizer is run over these in a second pass — usually referred to as the\nrescoring pass. The search space in the second pass is much more r estricted compared\nto the ﬁrst pass so we can aﬀord using better — usually also computa tionally more\nintensive — acoustic and/or language models.\nThe two most popular two-pass strategies diﬀer mainly in the number of interme-\ndiate hypotheses saved after the ﬁrst pass and the form in which t hey are stored.\nIn the so-called “N-best 1 rescoring” method, a list of complete hypotheses along\nwith acoustic/language model scores are retained and then resco red using more com-\nplex acoustic/language models.\nDue to the limited number of hypotheses in the N-best list, the secon d pass rec-\nognizer might be too constrained by the ﬁrst pass so a more compre hensive list of\nhypotheses is often needed. The alternative preferred to N-bes t list rescoring is “lat-\ntice rescoring”. The intermediate format in which the hypotheses a re stored is now a\ndirected acyclic graph in which the nodes are a subset of the languag e model states in\nthe composite hidden Markov model and the arcs are labeled with wor ds. Typically,\n1The value of N is typically 100–1000\n61\nthe ﬁrst pass acoustic/language model scores associated with ea ch arc — or link —\nin the lattice are saved and the nodes contain time alignment informat ion.\nFor both cases one can calculate the “oracle” word error rate: th e word error rate\nalong the hypothesis with the minimum number of errors. The oracle- WER decreases\nwith the number of hypotheses saved.\nOf course, a set of N-best hypotheses can be assembled as a lattic e, the diﬀerence\nbetween the two being just in the number of diﬀerent hypotheses — with diﬀerent\ntime-alignments — stored in the lattice. One reason which makes the N -best rescor-\ning framework attractive is the possibility to use “whole sentence” la nguage models:\nmodels that are able to assign a score only to complete sentences du e to the fact that\nthey do not operate in a left-to-right fashion. The drawbacks are that the number of\nhypotheses explored is too small and their quality reminiscent of the models used in\nthe ﬁrst pass. To clarify the latter assertion, assume that the se cond pass language\nmodel to be applied is dramatically diﬀerent from the one used in the ﬁr st pass and\nthat if we aﬀorded to extract the N-best using the better languag e model they would\nhave a diﬀerent kind of errors, speciﬁc to this language model. In th at case simple\nrescoring of the N-best list generated using the weaker language m odel may constrain\ntoo much the stronger language model used in the second pass, no t allowing it to\nshow its merits.\nIt is thus desirable to have a sample of the possible word hypotheses which is as\ncomplete as possible — not biased towards a given model — and at the s ame time of\nmanageable size. This is what makes lattice rescoring the chosen met hod in our case,\nhoping that simply by increasing the number of hypotheses retained one reduces the\nbias towards the ﬁrst pass language model.\n5.2 A∗ Algorithm\nThe A∗ algorithm [22] is a tree search strategy that could be compared to d epth-\nﬁrst tree-traversal: pursue the most promising path as deeply as possible.\n62\nLet a set of hypotheses\nL = {h : x1, . . . , x n}, x i ∈ W ∗ ∀ i\nbe organized as a preﬁx tree. We wish to obtain the maximum scoring h ypothesis\nunder the scoring function f : W∗ → ℜ :\nh∗ = arg max\nh∈L\nf(h)\nwithout scoring all the hypotheses in L, if possible with a minimal computational\neﬀort.\nThe algorithm operates with preﬁxes and suﬃxes of hypotheses in t he set L;\nwe will denote preﬁxes — anchored at the root of the tree — with x and suﬃxes\n— anchored at a leaf — with y. A complete hypothesis h can be regarded as the\nconcatenation of a x preﬁx and a y suﬃx: h = x.y. We assume that the function f(·)\ncan be evaluated at any preﬁx x, i.e. f(x) is a meaningful quantity.\nTo be able to pursue the most promising path, the algorithm needs to evaluate\nall the possible suﬃxes for a given preﬁx x = w1, . . . , w p that are allowed in L —\nsee ﬁgure 5.1. Let CL(x) be the set of suﬃxes allowed by the tree for a preﬁx x and\nassume we have an overestimate for the f(x.y) score of any complete hypothesis x.y,\ng(x.y):\ng(x.y) .= f(x) + h(y|x) ≥ f(x.y)\nImposing the condition that h(y|x) = 0 for empty y, we have\ng(x) = f(x), ∀ complete x ∈ L\nthat is, the overestimate becomes exact for complete hypothese s h ∈ L. Let the A∗\nranking function gL(x) be deﬁned as:\ngL(x) .= max\ny∈CL(x)\ng(x.y) = f(x) + hL(x), where (5.1)\nhL(x) .= max\ny∈CL(x)\nh(y|x) (5.2)\ngL(x) is an overestimate for the f(·) score of any complete hypothesis that has the\npreﬁx x; the overestimate becomes exact for complete hypotheses:\n63\nCL\nw\nw\nw\n1\n2\np\n(x)\nFigure 5.1: Preﬁx Tree Organization of a Set of Hypotheses L\ngL(x) ≥ f(x.y), ∀y ∈ CL(x) (5.3)\ngL(h) = f(h), ∀ complete h ∈ L (5.4)\nThe A∗ algorithm uses a potentially inﬁnite stack 2 in which preﬁxes x are ordered\nin decreasing order of the A∗ ranking function gL(x)3; at each extension step the\ntop-most preﬁx x = w1, . . . , w p is popped form the stack, expanded with all possible\none-symbol continuations of x in L and then all the resulting expanded preﬁxes —\namong which there may be complete hypotheses as well — are inserte d back into the\nstack. The stopping condition is: whenever the popped hypothesis is a complete one,\nretain it as the overall best hypothesis h∗ — see Algorithm 5.\nThe justiﬁcation for the correctness of the algorithm lies in the fac t that upon\ncompletion, any other preﬁx x in the stack has a lower stack-score than h∗:\ngL(x) < g L(h∗) = f(h∗)\nBut gL(x) ≥ f(x.y), ∀y ∈ CL(x) which means that no complete hypothesis x.y could\n2The stack need not be larger than |L| = n\n3In fact any overestimate satisfying both Eq. (5.3) and (5.4) will ens ure correctness of the\nalgorithm\n64\n//empty_hypothesis;\n//top_most_hypothesis;\n//a_hypothesis;\ninsert empty_hypothesis in stack;\ndo\n{ // one Astar extension step\ntop_most_hypothesis = pop top-most hypothesis from stack;\nfor all possible one symbol continuations w of top_most_hypothesis\n{\na_hypothesis = expand top_most_hypothesis with w;\ninsert a_hypothesis in stack;\n}\n}while(top_most_hypothesis is incomplete)\n//top_most_hypothesis is the highest f(.) scoring one\nAlgorithm 5: A∗ search\npossibly result in a higher f(·) score than h∗, formally:\nf(x.y) ≤ gL(x) < g L(h∗) = f(h∗), ∀x ∈ stack\nSince the stack is inﬁnite, it is guaranteed to contain preﬁxes for all hypotheses h ∈ L\n— see Algorithm 5 — which means that:\nf(x.y) ≤ gL(x) < g L(h∗) = f(h∗), ∀x.y ∈ L\nTo get a better grasp of the workings of A∗ we examine two limiting cases: perfect\nestimation of the scoring function f() value along the most promising suﬃx for any\ngiven preﬁx, and no clue at all.\nIn the ﬁrst case we have g(x.y) = f(x) + h(y|x) = f(x.y); notice that the A∗\nranking function becomes gL(x) = max y∈CL(x) f(x.y), ∀y ∈ CL(x), which means that\nwe are able to ﬁnd the best continuation of the current preﬁx. This makes the\nentire A∗ algorithm pointless: for x being the empty hypothesis, we just calculate\ngL(x) and retain the complete “continuation” y = h∗ that yielded maximal gL(x).\nThe A∗ algorithm simply builds h∗ by traversing y left to right; the topmost entry\n65\nin the stack will always have score f(h∗), diﬀerently distributed among x and y in\nx.y: f(x) + h(y|x) = f(h∗). The number of A∗ extension steps (see Algorithm 5)\nwill be equal to the length of h∗ making the search eﬀort minimal. Notice that in\nthis particular case a truncated stack at depth 1 suﬃces, sugges ting that there is\na correlation between the search eﬀort and the goodness of the e stimate in the A∗\nranking function.\nIn the second case we can set h(y|x) = ∞ for y non-empty and, of course, h(y|x) =\n0 for empty y. This will make gL(x) = f(x), if x is complete and gL(x) = ∞, if x\nis incomplete; any incomplete hypothesis will thus have a higher score than any\ncomplete hypothesis, causing A∗ to evaluate all the complete hypotheses in L hence\ndegenerating into an exhaustive search; the search eﬀort is maxim al.\nIn practice the h(y|x) function is chosen heuristically.\n5.2.1 A∗ for Lattice Decoding\nThere are a few reasons that make A∗ appealing for our problem:\n• the lattice can be conceptually structured as a preﬁx tree of hypo theses — the\ntime alignment is taken into consideration when comparing two word pr eﬁxes;\n• the algorithm operates with whole preﬁxes x, making it ideal for incorporating\nlanguage models whose memory is the entire utterance preﬁx;\n• a reasonably good overestimate h(y|x) and an eﬃcient way to calculate hL(x)\nare readily available using the n-gram model, as we will explain later.\nBefore explaining our approach to lattice decoding using the A∗ algorithm, let us\ndeﬁne a few terms.\nThe lattices we work with retain the following information after the ﬁr st pass:\n• time-alignment of each node;\n• for each link connecting two nodes in the lattice we retain:\n– word identity w(link);\n66\n– acoustic model score — log-probability of acoustic segment covere d by the\nlink given the word, logPAM (A(link)|w, link ); to make this possible, the\nending nodes of the link must contain all contextual information nec essary\nfor assigning acoustic model scores; for example, in a crossword t riphone\nsystem, all the words labeling the links leaving the end node must have\nthe same ﬁrst phone;\n– n-gram language model score — log-probability of the word, logPNG (w|link);\nagain, to make this possible, the start node of the link must contain t he\ncontext ( n − 1)-gram — it is a state in the ﬁnite state machine describing\nthe n-gram language model used to generate the lattice; we thus r efer to\nlattices as bigram or trigram lattices depending on the order of the la n-\nguage model that was used for generating it. The size of the lattice grows\nexponentially fast with the language model order.\nThe lattice has a unique starting and ending node, respectively.\nA link in the lattice is an arc connecting two nodes of the lattice. Two links ar e\nconsidered identical if and only if their word identity is the same and th eir starting\nand ending nodes are the same, respectively.\nA path p through the lattice is an ordered set of links l0 . . . l n with the constraint\nthat any two consecutive links cover adjacent time intervals:\np = {l0 . . . l n : ∀i = 0 . . . n − 1, ending\nnode(li) = starting node(li+1)} (5.5)\nWe will refer to the starting node of l0 as the starting node of path p and to the\nending node of ln as the ending node of path p.\nA partial path is a path whose starting node is the same as the starting node of the\nentire lattice and a complete path is one whose starting/ending nodes are the same\nas those of the entire lattice, respectively.\nWith the above deﬁnitions, a lattice can be conceptually organized as a preﬁx\ntree of paths. When rescoring the lattice using a diﬀerent language model than the\none that was used in the ﬁrst pass, we seek to ﬁnd the complete pat h p = l0 . . . l n\n67\nmaximizing:\nf(p) =\nn∑\ni=0\n[logPAM (li) + LMweight · logPLM (w(li)|w(l0) . . . w (li−1)) − logPIP ] (5.6)\nwhere:\n• logPAM (li) is the acoustic model log-likelihood assigned to link li;\n• logPLM (w(li)|w(l0) . . . w (li−1)) is the language model log-probability assigned\nto link li given the previous links on the partial path l0 . . . l i;\n• LMweight > 0 is a constant weight which multiplies the language model score of\na link; its theoretical justiﬁcation is unclear but experiments show it s usefulness;\n• logPIP > 0 is the “insertion penalty”; again, its theoretical justiﬁcation is\nunclear but experiments show its usefulness.\nTo be able to apply the A∗ algorithm we need to ﬁnd an appropriate stack entry\nscoring function gL(x) where x is a partial path and L is the set of complete paths\nin the lattice. Going back to the deﬁnition (5.1) of gL(·) we need an overestimate\ng(x.y) = f(x) + h(y|x) ≥ f(x.y) for all possible y = lk . . . l n complete continuations\nof x allowed by the lattice. We propose to use the heuristic:\nh(y|x) =\nn∑\ni=k\n[logPAM (li) + LMweight · (logPNG (li) + logPCOMP ) − logPIP ]\n+LMweight · logPF INAL · δ(k < n ) (5.7)\nA simple calculation shows that if logPLM (li) satisﬁes:\nlogPNG (li) + logPCOMP ≥ logPLM (li), ∀li\nthen gL(x) = f(x) + maxy∈CL(x)h(y|x) is a an appropriate choice for the A∗ stack\nentry scoring function.\nThe justiﬁcation for the logPCOMP term is that it is supposed to compensate for\nthe per word diﬀerence in log-probability between the n-gram model NG and the\nsuperior model LM with which we rescore the lattice — hence logPCOMP > 0. Its\nexpected value can be estimated from the diﬀerence in perplexity be tween the two\n68\nmodels LM and NG. Theoretically we should use a higher value than the maximum\npointwise diﬀerence between the two models:\nlogPCOMP ≥ max\n∀li\n[logPLM (li|l0 . . . l i−1) − logPNG (li)]\nbut in practice we set it by trial and error starting with the expecte d value as an\ninitial guess.\nThe logPF INAL > 0 term is used for practical considerations as explained in the\nnext section.\nThe calculation of gL(x) (5.1) is made very eﬃcient after realizing that one can\nuse the dynamic programming technique in the Viterbi algorithm [29]. I ndeed, for\na given lattice L, the value of hL(x) is completely determined by the identity of the\nending node of x; a Viterbi backward pass over the lattice can store at each node t he\ncorresponding value of hL(x) = hL(ending node(x)) such that it is readily available\nin the A∗ search.\n5.2.2 Some Practical Considerations\nIn practice one cannot maintain a potentially inﬁnite stack. We chose to control\nthe stack depth using two thresholds: one on the maximum number o f entries in\nthe stack, called stack-depth-threshold and another one on the maximum log-\nprobability diﬀerence between the top most and the bottom most hy potheses in the\nstack, called stack-logP-threshold.\nAs glimpsed from the two limiting cases analyzed in Section (5.2), there is a\nclear interaction between the quality of the stack entry scoring fu nction (5.1) and\nthe number of hypotheses explored, which in practice has to be con trolled by the\nmaximum stack size.\nA gross overestimate used in connection with a ﬁnite stack may lure t he search to\na cluster of paths which is suboptimal — the desired cluster of paths may fall out of\nthe stack if the overestimate happens to favor a wrong cluster.\nAlso, longer preﬁxes — thus having shorter suﬃxes — beneﬁt less fr om the per\nword logPCOMP compensation which means that they may fall out of a stack already\n69\nfull with shorter hypotheses — which have high scores due to compe nsation. This is\nthe justiﬁcation for the logPF INAL term in the compensation function h(y|x): the vari-\nance var[logPLM (li|l0 . . . l i−1)−logPNG (li)] is a ﬁnite positive quantity so the compen-\nsation is likely to be closer to the expected value E[logPLM (li|l0 . . . l i−1) − logPNG (li)]\nfor longer y continuations than for shorter ones; introducing a constant logPF INAL\nterm is equivalent to an adaptive logPCOMP depending on the length of the y suﬃx\n— smaller equivalent logPCOMP for long suﬃxes y for which E[logPLM (li|l0 . . . l i−1)−\nlogPNG (li)] is a better estimate for logPCOMP than it is for shorter ones.\nBecause the structured language model is computationally expens ive, a strong\nlimitation is being placed on the width of the search — controlled by the\nstack-depth-threshold and the stack-logP-threshold. For an acceptable search\nwidth — runtime — one seeks to tune the compensation parameters t o maximize\nperformance measured in terms of WER. However, the correlation between these\nparameters and the WER is not clear and makes the diagnosis of sear ch problems\nextremely diﬃcult. Our method for choosing the search parameter s was to sample\na few complete paths p1, . . . , p N from each lattice, rescore those paths according to\nthe f(·) function (5.6) and then rank the h∗ path output by the A∗ search among\nthe sampled paths. A correct A∗ search should result in average rank 0. In practice\nthis doesn’t happen but one can trace the topmost path p∗ in the oﬀending cases —\np∗ ̸= h∗ and f(p∗) > f (h∗):\n• if a preﬁx of the p∗ hypothesis is still present in the stack when A∗ returns then\nthe search failed strictly because of insuﬃcient compensation;\n• if no preﬁx of p∗ is present in the stack then the incorrect search outcome was\ncaused by an interaction between compensation and insuﬃcient sea rch width.\nThe method we chose for sampling paths from the lattice was an N-be st search\nusing the n-gram language model scores; this is appropriate for pr agmatic reasons —\none prefers lattice rescoring to N-best list rescoring exactly beca use of the possibility\nto extract a path that is not among the candidates proposed in the N-best list — as\nwell as practical reasons — they are among the “better” paths in t erms of WER.\n70\nChapter 6\nSpeech Recognition Experiments\nThe set of experiments presented in Section 4.1 showed improvemen t in perplexity\nover the 3-gram language model. The experimental setup is howeve r fairly restrictive\nand artiﬁcial when compared to a real world speech recognition tas k:\n• although the headword percolation and binarization procedure is au tomatic,\nthe treebank used as training data was generated by human annot ators;\n• albeit statistically signiﬁcant, the amount of training data (approxim atively 1\nmillion words) is small compared to that used for developing language m odels\nused in real world speech recognition experiments;\n• the word level tokenization of treebank text is diﬀerent than that used in the\nspeech recognition community, the former being tuned to facilitate linguistic\nanalysis.\nIn the remaining part of the chapter we will describe the experiment al setup used\nfor speech recognition experiments involving the structured langu age model, results\nand conclusions. The experiments were run on three diﬀerent corp ora — Switchboard\n(SWB), Wall Street Journal (WSJ) and Broadcast News (BN) — sam pling diﬀerent\npoints of the speech recognition spectrum — conversational spee ch over telephone\nlines at one end and read grammatical text recorded in ideal acoust ic conditions at\nthe other end.\n71\nIn order to evaluate our model’s potential as part of a speech reco gnizer, we had\nto address as follows the problems outlined above:\n• manual vs. automatic parse trees There are two corpora for which there exist\ntreebanks, although of limited size: Wall Street Journal (WSJ) and Switchboard\n(SWB). The UPenn Treebank [21] contains manually parsed WSJ text . There\nalso exists a small part of Switchboard which was manually parsed at U Penn\n—- approx. 20,000 words. This allows the training of an automatic par ser — we\nhave used the Collins parser [11] for SWB and the Ratnaparkhi pars er [26] for\nWSJ and BN — which is going to be used to generate an automatic treebank,\npossibly with a slightly diﬀerent word-tokenization than that of the t wo manual\ntreebanks. We evaluated the sensitivity of the structured langua ge model to\nthis aspect and showed that the reestimation procedure present ed in Chapter 3\nis powerful enough to overcome any handicap arising from automat ic treebanks.\n• more training data The availability of an automatic parser to generate parse\ntrees for the SLM training data — used for initializing the SLM — opens t he\npossibility of training the model on much more data than that used in t he ex-\nperiments presented in Section 4.1. The only limitations are of comput ational\nnature, imposed by the speed of the parser used to generate the automatic\ntreebank and the eﬃciency and speed of the reestimation procedu re for the\nstructured language model parameters. As our experiments sho w, the reesti-\nmation procedure leads to a better structured model — under bot h measures of\nperplexity and word error rate 1. In practice the speed of the SLM is the limiting\nfactor on the amount of training data. For Switchboard we have on ly 2 million\nwords of language modeling training data so this is not an issue; for WS J we\nwere able to accommodate only 20 million words of training data, much le ss\nthan the 40 million words used by standard language models on this tas k; for\nBN the discrepancy between the baseline 3-gram and the SLM is even bigger,\nwe were able to accommodate only 14 million words of training data, muc h less\nthan the 100 million words used by standard language models on this ta sk.\n1Reestimation is also going to smooth out peculiarities in the automatica lly generated treebank\n72\n• diﬀerent tokenization We address this problem in the following section.\n6.1 Experimental Setup\nIn order to train the structured language model (SLM) as describ ed in Chapter 3\nwe use parse trees from which to initialize the parameters of the mod el2 . Fortunately\na part of the SWB/WSJ data has been manually parsed at UPenn [21],[10 ]; let us\nrefer to this corpus as a Treebank. The training data used for spe ech recognition —\nCSR — is diﬀerent from the Treebank in two aspects:\n• the Treebank is only a subset of the usual CSR training data;\n• the Treebank tokenization is diﬀerent from that of the CSR corpus ; among other\nspurious small diﬀerences, the most frequent ones are of the typ e presented in\nTable 6.1.\nTreebank CSR\ndo n’t don’t\nit ’s it’s\njones ’ jones’\ni ’m i’m\ni ’ll i’ll\ni ’d i’d\nwe ’ve we’ve\nyou ’re you’re\nTable 6.1: Treebank — CSR tokenization mismatch\nOur goal is to train the SLM on the CSR corpus.\nTraining Setup\nThe training of the SLM model proceeds as follows:\n2The use of initial statistics gathered in a diﬀerent way is an interestin g direction of research; the\nconvergence properties of the reestimation procedure become e ssential in such a situation\n73\n• Process the CSR training data to bring it closer to the Treebank for mat. We\napplied the transformations suggested by Table 6.1; the resulting c orpus will be\ncalled CSR-Treebank, although at this stage we only have words and no parse\ntrees for it;\n• Transfer the syntactic knowledge from the Treebank onto the CS R-Treebank\ntraining corpus; as a result of this stage, CSR-Treebank is truly a “ treebank”\ncontaining binarized and headword annotated trees:\n– for the SWB experiments we parsed the SWB-CSR-Treebank corpu s using\nthe SLM trained on the SWB-Treebank — thus using the SLM as a pars er;\nthe vocabulary for this step was the union between the SWB-Treeb ank\nand the SWB-CSR-Treebank closed vocabularies. The resulting tre es are\nalready binary and have headword annotation.\n– for the WSJ and BN experiments we parsed the WSJ-CSR-Treebank cor-\npus using the Ratnaparkhi maximum entropy parser [26], trained on the\nUPenn Treebank data 3. The resulting trees were binarized and annotated\nwith headwords using the procedure described in Section 2.1.1.\n• Apply the SLM parameter reestimation procedure on the CSR-Tree bank train-\ning corpus using the parse trees obtained at the previous step for gathering\ninitial statistics.\nNotice that we have avoided “transferring” the syntactic knowled ge from the Tree-\nbank tokenization directly onto the CSR tokenization; the reason is that CSR word\ntokens like “he’s” or “you’re” cross boundaries of syntactic const ituents in the Tree-\nbank corpus and the transfer of parse trees from the Treebank to the CSR corpus is\nfar from obvious and likely to violate syntactic knowledge present in t he treebank.\n3The parser is mismatched, the most important diﬀerence being the f act that in the training data\nof the parser numbers are written as “$123” whereas in the data t o be parsed they are expanded\nto “one hundred twenty three dollars”; we rely on the SLM paramet er reestimation procedure to\nsmooth out this mismatch\n74\nLattice Decoding Setup\nTo be able to run lattice decoding experiments we need to bring the lat tices — in\nCSR tokenization — to the CSR-Treebank format. The only operatio n involved in this\ntransformation is splitting certain words into two parts, as sugges ted by Table 6.1.\nEach link whose word needs to be split is cut into two parts and an inter mediate\nnode is inserted into the lattice as shown in ﬁgure 6.1. The acoustic an d language\nmodel scores of the initial link are copied onto the second new link. For all the\ns\ns_time\ne\ne_time\nw, AMlnprob, NGlnprob\ns\ns_time i\ne\ne_time\nw_1, 0, 0\nw_2, AMlnprob, NGlnprob\ne_time\nw -> w_1 w_2\nFigure 6.1: Lattice CSR to CSR-Treebank Processing\ndecoding experiments we have carried out, the WER is measure d after undoing the\ntransformations highlighted above; the reference transcr iptions for the test data were\nnot touched and the NIST SCLITE 4 package was used for measuring the WER .\nThe reﬁnement of the SLM presented in Section 2.6, Eq. (2.12—2.13) was not used\nat all during the following experiments due to its low ratio of improveme nt versus\ncomputational cost.\n6.2 Perplexity Results\nAs a ﬁrst step we evaluated the perplexity performance of the SLM relative to that\nof a deleted interpolation 3-gram model trained in the same conditions . As outlined\nin the previous section, we worked on the CSR-Treebank corpus.\n4SCLITE is a standard program supplied by NIST for scoring speech r ecognizers\n75\n6.2.1 Wall Street Journal Perplexity Results\nWe chose to work on the DARPA’93 evaluation HUB1 test setup. The s ize of the\ntest set is 213 utterances, 3446 words. The 20kwds open vocabu lary and baseline\n3-gram model are the standard ones provided by NIST and LDC.\nAs a ﬁrst step we evaluated the perplexity performance of the SLM relative to\nthat of a deleted interpolation 3-gram model trained under the sam e conditions:\ntraining data size 20Mwds (a subset of the training data used for th e baseline 3-gram\nmodel), standard HUB1 open vocabulary of size 20kwds; both the t raining data and\nthe vocabulary were re-tokenized such that they conform to the Upenn Treebank\ntokenization. We have linearly interpolated the SLM with the above 3- gram model:\nP (·) = λ · P3gram (·) + (1 − λ) · PSLM (·)\nshowing a 10% relative reduction over the perplexity of the 3-gram m odel. The results\nare presented in Table 6.2. The SLM parameter reestimation proced ure5 reduces the\nPPL by 5% ( 2% after interpolation with the 3-gram model ). The main r eduction\nin PPL comes however from the interpolation with the 3-gram model s howing that\nalthough overlapping, the two models successfully complement each other. The inter-\npolation weight was determined on a held-out set to be λ = 0 .4. In this experiment\nboth language models operate in the UPenn Treebank text tokeniza tion.\nLanguage Model L2R Perplexity\nDEV set TEST set\nno int 3-gram int\nTrigram 33.0 147.8 147.8\nSLM; Initial stats(iteration 0) 39.1 151.9 135.9\nSLM; Reestimated(iteration 1) 34.6 144.1 132.8\nTable 6.2: WSJ-CSR-Treebank perplexity results\n5Due to the fact that the parameter reestimation procedure for t he SLM is computationally\nexpensive we ran only a single iteration\n76\n6.2.2 Switchboard Perplexity Results\nFor the Switchboard experiments the size of the training data was 2 .29 Mwds; the\nsize of the test data set aside for perplexity measurements was 28 Kwds — WS97\nDevTest [10]. We used a closed vocabulary of size 22Kwds. Again, we h ave also\nlinearly interpolated the SLM with the deleted interpolation 3-gram ba seline showing\na modest reduction in perplexity:\nP (wi|Wi−1) = λ · P3−gram (wi|wi−1, wi−2) + (1 − λ) · PSLM (wi|Wi−1)\nThe interpolation weight was determined on a held-out set to be λ = 0 .4. The results\nare presented in Table 6.3.\nLanguage Model L2R Perplexity\nDEV set TEST set\nno int 3-gram int\nTrigram 22.53 68.56 68.56\nSLM; Seeded with Auto-Treebank 23.94 72.09 65.80\nSLM; Reestimated(iteration 4) 22.70 71.04 65.35\nTable 6.3: SWB-CSR-Treebank perplexity results\n6.2.3 Broadcast News Perplexity Results\nFor the Broadcast News experiments the size of the training data w as 14 Mwds;\nthe size of the test data set aside for perplexity measurements wa s 23150 wds —\nDARPA’96 HUB4 dev-test. We used an open vocabulary of size 61Kwd s. Again, we\nhave also linearly interpolated the SLM with the deleted interpolation 3 -gram baseline\nbuilt on exactly the same training data showing an overall 7% relative r eduction in\nperplexity:\nP (wi|Wi−1) = λ · P3−gram (wi|wi−1, wi−2) + (1 − λ) · PSLM (wi|Wi−1)\nThe interpolation weight was determined on a held-out set to be λ = 0 .4. The results\nare presented in Table 6.4.\n77\nLanguage Model L2R Perplexity\nDEV set TEST set\nno int 3-gram int\nTrigram 35.4 217.8 217.8\nSLM; Seeded with Auto-Treebank 57.7 231.6 205.5\nSLM; Reestimated(iteration 2) 40.1 221.7 202.4\nTable 6.4: SWB-CSR-Treebank perplexity results\n6.3 Lattice Decoding Results\nWe proceeded to evaluate the WER performance of the SLM using th e A∗ lattice\ndecoder described in Chapter 5. Before describing the experiment s we need to make\nclear one point; there are two language model scores associated w ith each link in the\nlattice:\n• the language model score assigned by the model that generated t he lattice,\nreferred to as the LAT3-gram; this model operates on text in the CSR tokeniza-\ntion;\n• the language model score assigned by rescoring each link in the lattic e with the\ndeleted interpolation 3-gram built on the data in the CSR-Treebank t okeniza-\ntion, referred to as the TRBNK3-gram;\n6.3.1 Wall Street Journal Lattice Decoding Results\nThe lattices on which we ran rescoring experiments were obtained us ing the stan-\ndard 20k (open) vocabulary language model (LAT3-gram) trained on more training\ndata than the SLM — about 40Mwds. The deleted interpolation 3-gra m model\n(TRBNK3-gram) built on much less training data — 20Mwds, same as SL M — and\nusing the same standard open vocabulary — after re-tokenizing it s uch that it matches\nthe UPenn Treebank text tokenization — is weaker than the one use d for generating\nthe lattices, as conﬁrmed by our experiments. Consequently, we r an lattice rescoring\nexperiments in two setups:\n78\n• using the language model that generated the lattice — LAT3-gram — as the\nbaseline model; language model scores are available in the lattice.\n• using the TRBNK3-gram language model — same training conditions as the\nSLM; we had to assign new language model scores to each link in the lat tice.\nThe 3-gram lattices we used have an “oracle” WER 6 of 3.4%; the baseline WER\nis 13.7%, obtained using the standard 3-gram model provided by DAR PA (dubbed\nLAT3-gram) — trained on 40Mwds and using a 20k open vocabulary.\nComparison between LAT3-gram and TRBNK3-gram\nA ﬁrst batch of experiments evaluated the power of the two 3-gra m models at\nour disposal. The LAT3-gram scores are available in the lattice from t he ﬁrst pass\nand we can rescore each link in the lattice using the TRBNK3-gram mod el. The\nViterbi algorithm can be used to ﬁnd the best path through the latt ice according\nto the scoring function (5.6) where logPLM (·) can be either of the above or a linear\ncombination of the two. Notice that the linear interpolation of link lang uage model\nscores:\nP (l) = λ · PLAT 3−gram (l) + (1 − λ) · PT RBNK 3−gram (l)\ndoesn’t lead to a proper probabilistic model due to the tokenization m ismatch. In\norder to correct this problem we adjust the workings of the TRBNK 3-gram to take\ntwo steps whenever a split link is encountered and interpolate with th e correct LAT3-\ngram probability for the two links. For example:\nP (don′t|x, y) = λ · PLAT 3−gram (don′t|x, y) +\n(1 − λ) · PT RBNK 3−gram (do|x, y) · PT RBNK 3−gram (n′t|y, do) (6.1)\nThe results are shown in Table 6.5. The parameters in (5.6) were set t o: LMweight = 16,\nlogP_{IP} = 0, usual values for WSJ.\n6The “oracle” WER is calculated by ﬁnding the path with the least numbe r of errors in each\nlattice\n79\nλ 0.0 0.2 0.4 0.6 0.8 1.0\nWER(%) 14.7 14.2 13.8 13.7 13.5 13.7\nTable 6.5: 3-gram Language Model; Viterbi Decoding Results\nLAT3-gram driven search using the SLM\nA second batch of experiments evaluated the performance of the SLM. The per-\nplexity results show that interpolation with the 3-gram model is bene ﬁcial for our\nmodel. The previous experiments show that the LAT3-gram model is more powerful\nthan the TRBNK3-gram model. The interpolated language model sco re:\nP (l) = λ · PLAT 3−gram (l) + (1 − λ) · PSLM (l)\nis calculated as explained in the previous section — see Eq. 6.1.\nThe results for diﬀerent interpolation coeﬃcient values are shown in Table 6.6.\nThe parameters controlling the SLM were the same as in Chapter 3.\nAs explained previously, due to the fact that the SLM’s memory exte nds over\nthe entire preﬁx we need to apply the A∗ algorithm to ﬁnd the overall best path\nin the lattice. The parameters controlling the A∗ search were set to: logPCOMP\n= 0.5, logPF INAL = 0, LMweight = 16, logPIP = 0, stack-depth-threshold=30,\nstack-depth-logP-threshold=100 (see 5.6 and 5.7).\nThe logPCOMP , logPF INAL and stack-depth-threshold,\nstack-depth-logP-threshold were optimized directly on test data for the best in-\nterpolation value found in the perplexity experiments. The LMweight , logPIP pa-\nrameters are the ones typically used with the 3-gram model for the WSJ task; we did\nnot adjust them to try to ﬁt the SLM better.\nλ 0.0 0.4 1.0\nWER(%) (iteration 0 SLM ) 14.4 13.0 13.7\nWER(%) (iteration 1 SLM ) 14.3 13.2 13.7\nTable 6.6: LAT-3gram + Structured Language Model; A∗ Decoding Results\n80\nThe structured language model achieved an absolute improvement in WER of\n0.7% (5% relative) over the baseline.\nTRBNK3-gram driven search using the SLM\nWe rescored each link in the lattice using the TRBNK3-gram language m odel and\nused this as a baseline for further experiments. As showed in Table 6 .5, the baseline\nWER becomes 14.7%. The relevance of the experiments using the TRB NK3-gram\nrescored lattices is somewhat questionable since the lattice was gen erated using a\nmuch stronger language model — the LAT3-gram. Our point of view is the following:\nassume that we have a set of hypotheses which were produced in so me way; we then\nrescore them using two language models, M1 and M2; if model M2 is tru ly superior\nto M1 7, then the WER obtained by rescoring the set of hypotheses using m odel M2\nshould be lower than that obtained using model M1.\nWe repeated the experiment in which we linearly interpolate the SLM wit h the\n3-gram language model:\nP (l) = λ · PT RBNK 3−gram (l) + (1 − λ) · PSLM (l)\nfor diﬀerent interpolation coeﬃcients. The A∗ search parameters were the same as\nbefore. The results are presented in Table 6.7. The structured lan guage model inter-\nλ 0.0 0.4 1.0\nWER(%) (iteration 0 SLM ) 14.6 14.3 14.7\nWER(%) (iteration 3 SLM ) 13.8 14.3 14.7\nTable 6.7: TRBNK-3gram + Structured Language Model; A∗ Decoding Results\npolated with the trigram model achieves 0.9% absolute (6% relative) r eduction over\nthe trigram baseline; the parameters controlling the A∗ search have not been tuned\nfor this set of experiments.\n7From a speech recognition perspective\n81\n6.3.2 Switchboard Lattice Decoding Results\nOn the Switchboard corpus, the lattices for which we ran decoding e xperiments\nwere obtained using a language model (LAT3-gram) trained in very s imilar conditions\n— roughly same training data size and vocabulary, closed over test d ata — to the ones\nunder which the SLM and the baseline deleted interpolation 3-gram mo del (TRBNK3-\ngram) were trained. The only diﬀerence is the tokenization — CSR vs. CSR-Treebank,\nsee Section 6.1 — which makes the LAT3-gram act as phrase based lan guage model\nwhen compared to TRBNK3-gram. The experiments conﬁrmed that LAT3-gram is\nstronger than TRBNK-3gram.\nAgain, we ran lattice rescoring experiments in two setups:\n• using the language model that generated the lattice — LAT3-gram — as the\nbaseline model; language model scores are available in the lattice.\n• using the TRBNK3-gram language model — same training conditions as the\nSLM; we had to assign new language model scores to each link in the lat tice.\nComparison between LAT3-gram and TRBNK3-gram\nThe results are shown in Table 6.8, for diﬀerent interpolation values:\nP (l) = λ · PLAT 3−gram (l) + (1 − λ) · PT RBNK 3−gram (l)\nThe parameters in (5.6) were set to: LMweight = 12, logP_{IP} = 10.\nλ 0.0 0.2 0.4 0.6 0.8 1.0\nWER(%) 42.3 41.8 41.2 41.0 41.0 41.2\nTable 6.8: 3-gram Language Model; Viterbi Decoding Results\nLAT3-gram driven search using the SLM\nThe previous experiments show that the LAT3-gram model is more p owerful than\nthe TRBNK3-gram model. We thus wish to interpolate the SLM with the LAT3-gram\n82\nmodel:\nP (l) = λ · PLAT 3−gram (l) + (1 − λ) · PSLM (l)\nWe correct the interpolation the same way as described in the WSJ ex periments —\nsee Section 6.3.1, Eq. 6.1.\nThe parameters controlling the SLM were the same as in chapter 3. T he parame-\nters controlling the A∗ search were set to: logPCOMP = 0.5, logPF INAL = 0, LMweight\n= 12, logPIP = 10, stack-depth-threshold=40, stack-depth-logP-threshold=100\n(see 5.6 and 5.7). The logPCOMP , logPF INAL and stack-depth-threshold,\nstack-depth-logP-threshold were optimized directly on test data for the best in-\nterpolation value found in the perplexity experiments. In all other e xperiments they\nwere kept ﬁxed to these values. The LMweight , logPIP parameters are the ones\ntypically used with the 3-gram model for the Switchboard task; we d id not adjust\nthem to try to ﬁt the SLM better.\nThe results for diﬀerent interpolation coeﬃcient values are shown in Table 6.9.\nλ 0.0 0.4 1.0\nWER(%) (SLM iteration 0) 41.8 40.7 41.2\nWER(%) (SLM iteration 3) 41.6 40.5 41.2\nTable 6.9: LAT-3gram + Structured Language Model; A∗ Decoding Results\nThe structured language model achieved an absolute improvement of 0.7% WER\nover the baseline; the improvement is statistically signiﬁcant at the 0 .001 level ac-\ncording to a sign test at the sentence level.\nFor tuning the search parameters we have applied the N-best lattic e sampling\ntechnique described in section 5.2.2. As a by-product, the WER perf ormance of the\nstructured language model on N-best list rescoring — N = 25 — was 40 .4%. The\naverage rank of the hypothesis found by the A∗ search among the N-best ones — after\nrescoring them using the structured language model interpolated with the trigram —\nwas 0.3. There were 329 oﬀending sentences — out of a total of 242 7 sentences — in\nwhich the A∗ search lead to a hypothesis whose score was lower than that of the top\nhypothesis among the N-best(0-best). In 296 cases the preﬁx o f the rescored 0-best\n83\nwas still in the stack when A∗ returned — inadequate compensation — and in the\nother 33 cases, the 0-best hypothesis was lost during the search due to the ﬁnite stack\nsize.\nTRBNK3-gram driven search using the SLM\nWe rescored each link in the lattice using the TRBNK3-gram language m odel and\nused this as a baseline for further experiments. As showed in Table 6 .8, the baseline\nWER is 42.3%.\nWe then repeated the experiment in which we linearly interpolate the S LM with\nthe 3-gram language model:\nP (l) = λ · PT RBNK 3−gram (l) + (1 − λ) · PSLM (l)\nfor diﬀerent interpolation coeﬃcients. The parameters controlling the A∗ search\nwere set to: logPCOMP = 0.5, logPF INAL = 0, LMweight = 12, logPIP = 10,\nstack-depth-threshold=40, stack-depth-logP-threshold=100 (see 5.6 and 5.7).\nThe results are presented in Table 6.10. The structured language m odel interpolated\nλ 0.0 0.4 1.0\nWER(%) (iteration 0 SLM ) 42.0 41.6 42.3\nWER(%) (iteration 3 SLM ) 42.0 41.6 42.3\nTable 6.10: TRBNK-3gram + Structured Language Model; A∗ Decoding Results\nwith the trigram model achieves 0.7% absolute reduction over the tr igram baseline.\n6.3.3 Broadcast News Lattice Decoding Results\nThe Broadcast News (BN) lattices for which we ran decoding experim ents were\nobtained using a language model (LAT3-gram) trained on much more training data\nthan the SLM; a typical ﬁgure for BN is 100Mwds. We could accommod ate 14Mwds\nof training data for the SLM and the baseline deleted interpolation 3- gram model\n(TRBNK3-gram). The experiments conﬁrmed that LAT3-gram is st ronger than\nTRBNK-3gram.\n84\nThe set set on which we ran the experiments was the DARPA’96 HUB4 d ev-test.\nWe used an open vocabulary of 61kwds. Again, we ran lattice rescor ing experiments\nin two setups:\n• using the language model that generated the lattice — LAT3-gram — as the\nbaseline model; language model scores are available in the lattice.\n• using the TRBNK3-gram language model — same training conditions as the\nSLM; we had to assign new language model scores to each link in the lat tice.\nThe test set is segmented in diﬀerent focus conditions summarized in Table 6.11.\nFocus Description\nF0 baseline broadcast speech (clean, planned)\nF1 spontaneous broadcast speech (clean)\nF2 low ﬁdelity speech (typically narrowband)\nF3 speech in the presence of background music\nF4 speech under degraded acoustical conditions\nF5 non-native speakers (clean, planned)\nFX all other speech (e.g. spontanous non-native)\nTable 6.11: Broadcast News Focus conditions\nComparison between LAT3-gram and TRBNK3-gram\nThe results are shown in Table 6.12, for diﬀerent interpolation values :\nP (l) = λ · PLAT 3−gram (l) + (1 − λ) · PT RBNK 3−gram (l)\nThe parameters in (5.6) were set to: LMweight = 13, logP_{IP} = 10.\nλ 0.0 0.2 0.4 0.6 0.8 1.0\nWER(%) 35.2 34.0 33.2 33.0 32.9 33.1\nTable 6.12: 3-gram Language Model; Viterbi Decoding Results\n85\nLAT3-gram driven search using the SLM\nThe previous experiments show that the LAT3-gram model is more p owerful than\nthe TRBNK3-gram model. We thus wish to interpolate the SLM with the LAT3-gram\nmodel:\nP (l) = λ · PLAT 3−gram (l) + (1 − λ) · PSLM (l)\nWe correct the interpolation the same way as described in the WSJ ex periments —\nsee Section 6.3.1, Eq. 6.1.\nThe parameters controlling the SLM were the same as in chapter 3. T he parame-\nters controlling the A∗ search were set to: logPCOMP = 0.5, logPF INAL = 0, LMweight\n= 13, logPIP = 10, stack-depth-threshold=25, stack-depth-logP-threshold=100\n(see 5.6 and 5.7).\nThe results for diﬀerent interpolation coeﬃcient values are shown in Table 6.13.\nThe breakdown on diﬀerent focus conditions is shown in Table 6.14. Th e SLM achieves\nλ 0.0 0.4 1.0\nWER(%) (SLM iteration 0) 34.4 33.0 33.1\nWER(%) (SLM iteration 2) 35.1 33.0 33.1\nTable 6.13: LAT-3gram + Structured Language Model; A∗ Decoding Results\nλ Decoder SLM iteration F0 F1 F2 F3 F4 F5 FX overall\n1.0 Viterbi 13.0 30.8 42.1 31.0 22.8 52.3 53.9 33.1\n0.0 A∗ 0 13.3 31.7 44.5 32.0 25.1 54.4 54.8 34.4\n0.4 A∗ 0 12.5 30.5 42.2 31.0 23.0 52.9 53.9 33.0\n1.0 A∗ 0 12.9 30.7 42.1 31.0 22.8 52.3 53.9 33.1\n0.0 A∗ 2 14.8 31.7 46.3 31.6 27.5 54.3 54.8 35.1\n0.4 A∗ 2 12.2 30.7 42.0 31.1 22.5 53.1 54.4 33.0\n1.0 A∗ 2 12.9 30.7 42.1 31.0 22.8 52.3 53.9 33.1\nTable 6.14: LAT-3gram + Structured Language Model; A∗ Decoding Results; break-\ndown on diﬀerent focus conditions\n0.8% absolute (6% relative) reduction in WER on the F0 focus condition despite the\n86\nfact that the overall WER reduction is negligible. We also note the ben eﬁcial eﬀect\ntraining has on the SLM performance on the F0 focus condition.\nTRBNK3-gram driven search using the SLM\nWe rescored each link in the lattice using the TRBNK3-gram language m odel and\nused this as a baseline for further experiments. As showed in Table 6 .12, the baseline\nWER is 35.2%.\nWe then repeated the experiment in which we linearly interpolate the S LM with\nthe 3-gram language model:\nP (l) = λ · PT RBNK 3−gram (l) + (1 − λ) · PSLM (l)\nfor diﬀerent interpolation coeﬃcients. The parameters controlling the A∗ search\nwere set to: logPCOMP = 0.5, logPF INAL = 0, LMweight = 13, logPIP = 10,\nstack-depth-threshold=25, stack-depth-logP-threshold=100 (see 5.6 and 5.7).\nThe results are presented in Table 6.15. The breakdown on diﬀerent focus conditions\nλ 0.0 0.4 1.0\nWER(%) (SLM iteration 0) 35.4 34.9 35.2\nWER(%) (SLM iteration 2) 35.0 34.7 35.2\nTable 6.15: TRBNK-3gram + Structured Language Model; A∗ Decoding Results\nis shown in Table 6.16. The SLM achieves 1.1% absolute (8% relative) red uction in\nWER on the F0 focus condition and an overall WER reduction of 0.5% ab solute. We\nalso note the beneﬁcial eﬀect training has on the SLM performance .\nConclusions to Lattice Decoding Experiments\nWe note that the parameter reestimation doesn’t improve the WER p erformance\nof the model in all cases. The SLM achieves an improvement over the 3-gram baseline\non all three corpora: Wall Street Journal, Switchboard and Broad cast News.\n87\nλ Decoder SLM iteration F0 F1 F2 F3 F4 F5 FX overall\n1.0 Viterbi 14.5 32.5 44.9 33.3 25.7 54.9 56.1 35.2\n0.0 A∗ 0 14.6 32.9 44.6 33.1 26.3 54.4 56.9 35.4\n0.4 A∗ 0 14.1 32.2 44.4 33.0 25.0 54.2 56.1 34.9\n1.0 A∗ 0 14.5 32.4 44.9 33.3 25.7 54.9 56.1 35.2\n0.0 A∗ 2 13.7 32.4 44.7 32.9 26.1 54.3 56.3 35.0\n0.4 A∗ 2 13.4 32.2 44.1 31.9 25.3 54.2 56.2 34.7\n1.0 A∗ 2 14.5 32.4 44.9 33.3 25.7 54.9 56.1 35.2\nTable 6.16: TRBNK-3gram + Structured Language Model; A∗ Decoding Results;\nbreakdown on diﬀerent focus conditions\n6.3.4 Taking Advantage of Lattice Structure\nAs we shall see, in order to carry out experiments in which we try to t ake further\nadvantage of the lattice, we need to have proper language model s cores on each lattice\nlink. For all the experiments in this section we used the TRBNK3-gram rescored\nlattices.\nPeeking Interpolation\nAs described in Section 2.6, the probability assignment for the word a t position\nk + 1 in the input sentence is made using:\nP (wk+1/Wk) =\n∑\nTk∈Sk\nP (wk+1/WkTk) · ρ(Wk, Tk) (6.2)\nwhere\nρ(Wk, Tk) = P (WkTk)/\n∑\nTk∈Sk\nP (WkTk) (6.3)\nwhich ensures a proper probability over strings W ∗, where Sk is the set of all parses\npresent in the SLM stacks at the current stage k.\nOne way to take advantage of the lattice is to determine the set of p arses Sk over\nwhich we are going to interpolate by knowing what the possible future words are —\nthe links leaving the end node of a given path in the lattice bear only a sm all set of\nwords — for our lattices, less than 10 on the average. The idea is tha t by knowing\nthe future word it is much easier to determine the most favorable pa rse for predicting\n88\nit. Let WL(p) denote the set of words that label the links leaving the end node of pa th\np in lattice L. We can then restrict the set of parses Sk used for interpolation to:\nSpruned\nk = {T i\nk : T i\nk = arg max\nTk∈Sk\nP (wi/WkTk) · ρ(Wk, Tk), ∀ wi ∈ W L(p)}\nWe obviously have Spruned\nk ⊆ Sk. Notice that this does not lead to a correct proba-\nbility assignment anymore since it violates the causality implied by the lef t-to-right\noperation of the language model. In the extreme case of |WL(p)| = 1 we have a\nmodel which, at each next word prediction step, picks from among t he parses in Sk\nonly the most favorable one for predicting the next word. This leads to the undesir-\nable eﬀect that at a subsequent prediction during the same senten ce the parse picked\nmay change, always trying to make the best possible current predic tion. In order to\ncompensate for this unwanted eﬀect we decided to run a second ex periment in which\nonly the parses in Spruned\nk are kept in the stacks of the structured language model at\nposition k in the input sentence — the other ones are discarded and thus unav ailable\nfor later predictions in the sentence. This speeds up considerably t he decoder —\napproximately 4 times faster than the previous experiment — and slig htly improves\non the results in the previous experiment but still does not increase the performance\nover the standard structured language model, as shown in Table 6.1 7. The results for\nthe standard SLM do not match those in Table 6.10 due to the fact th at in this case\nwe have not applied the tokenization correction speciﬁed in Eq. (6.1) , Section 6.3.1.\nλ 0.0 0.2 0.4 0.6 0.8 1.0\nWER(%) (standard SLM) 42.0 41.8 41.9 41.5 42.1 42.5\nWER(%) (peeking SLM) 42.3 42.0\nWER(%) (pruned peeking SLM) 42.1 41.9\nTable 6.17: Switchboard;TRBNK-3gram + Peeking SLM;\n89\nNormalized Peeking\nAnother proper probability assignment for the next word wk+1 could be made\naccording to:\nP (wk+1/Wk) = norm(α(w, Wk)), (6.4)\nwhere\nα(w, Wk) .= max\nTk∈Sk\nP (w/WkTk) · ρ(Wk, Tk) (6.5)\nand\nnorm(α(w, Wk)) .= α(wk+1, Wk)/\n∑\nw∈V\nα(w, Wk) (6.6)\nThe sum over all words in the vocabulary V — |V| ≈ 20, 000 — prohibits the\nuse of the above equation in perplexity evaluations for computation al reasons. In\nthe lattice however we have a much smaller list of future words so the summation\nneeds to be carried only over WL(p) (see previous section) for a given path p. To take\ncare of the fact that due to the truncation of V to WL(p) the probability assignment\nnow violates the left-to-right operation of the language model we c an redistribute the\n3-gram mass assigned to WL(p) according to the formula proposed in Eq. (6.4):\nPSLMnorm (wk+1/Wk(p)) = norm(α(w, Wk)) · PT RBNK 3−gram (WL(p)) (6.7)\nα(w, Wk) .= max\nTk∈Sk\nP (w/WkTk) · ρ(Wk, Tk) (6.8)\nnorm(α(w, Wk)) .= α(wk+1, Wk)/\n∑\nw∈WL(p)\nα(w, Wk) (6.9)\nPT RBNK 3−gram (WL(p)) .=\n∑\nw∈WL(p)\nPT RBNK 3−gram (w/Wk(p)) (6.10)\nNotice that if we let WL(p) = V we get back Eq. (6.4). Again, one could discard\nfrom the SLM stacks the parses which do not belong to Spruned\nk , as explained in the\nprevious section. Table 6.18 presents the results obtained when line arly interpolating\nthe above models with the 3-gram model:\nP (l/Wk(p)) = λ · PT RBNK 3−gram (l/Wk(p)) + (1 − λ) · PSLMnorm (l/Wk(p))\nThe results for the standard SLM do not match those in Table 6.10 du e to the\nfact that in this case we have not applied the tokenization correctio n speciﬁed in\n90\nλ 0.0 0.2 0.4 0.6 0.8 1.0\nWER(%) (standard SLM) 42.0 41.8 41.9 41.5 42.1 42.5\nWER(%) (normalized SLM) 42.7 42.1 42.0 42.1\nWER(%)(pruned normalized SLM) 42.2\nTable 6.18: Switchboard; TRBNK-3gram + Normalized Peeking SLM;\nEq. (6.1), Section 6.3.1. Although some of the experiments showed im provement over\nthe WER baseline achieved by the 3-gram language model, none of the m performed\nbetter than the standard structured language model linearly inte rpolated with the\ntrigram model.\n91\nChapter 7\nConclusions and Future Directions\n7.1 Comments on Using the SLM as a Parser\nThe structured language model could be used as a parser, namely s elect the most\nlikely parse according to our pruning strategy: T ∗ = argmaxT P (W, T ). Due to the\nfact that the SLM allows parses in which the words in a sentence are n ot joined under\na single root node — see the deﬁnition of a complete parse and Figure 2 .6 — a direct\nevaluation of the parse quality against the UPenn Treebank parses is unfair. However,\na simple modiﬁcation will constrain the parses generated by the SLM t o join all words\nin a sentence under a single root node.\nImposing the additional constraint that:\n• P (wk=</s>|Wk−1Tk−1) = 0 if h −1.tag ̸= SB ensures that the end of sentence\nsymbol </s> is generated only from a parse in which all the words have been\njoined in a single constituent.\nOne important observation is that in this case one has to eliminate the second\npruning step in the model and the hard pruning in the cache-ing of th e CONSTRUC-\nTOR model actions; it is suﬃcient if this is done only when operating on t he last\nstack vector before predicting the end of sentence </s>. Otherwise, the parses that\nhave all the words joined under a single root node may not be presen t in stacks before\nthe prediction of the </s> symbol, resulting in a failure to parse a given sentence.\n92\n7.2 Comparison with other Approaches\n7.2.1 Underlying P (W, T ) Probability Model\nThe actions taken by the model are very similar to a LR parser. Howe ver the\nencoding of the word sequence along with a parse tree ( W, T ) is diﬀerent, proceeding\nbottom-up and interleaving the word predictions. This leads to a diﬀe rent probability\nassignment than that in a PCFG grammar — which is based on a diﬀerent encoding\nof ( W, T ).\nA thorough comparison between the two classes of probabilistic lang uages —\nPCFGs and shift-reduce probabilistic push-down automata, to whic h the SLM per-\ntains — has been presented in [1].\nRegarding ( W, T ) as a graph, Figure 7.1 shows the dependencies in a regular CFG;\nin contrast, Figures (7.2–7.4) show the probabilistic dependencies f or each model com-\nponent in the SLM; a complete dependency structure is obtained by super-imposing\nthe three ﬁgures. To make the SLM directly comparable with a CFG we discard the\nlexical information at intermediate nodes in the tree — headword ann otation — thus\nassuming the following equivalence classiﬁcations in the model compon ents — see\nEq.(2.3–2.5):\nP (wk|Wk−1Tk−1) = P (wk|[Wk−1Tk−1]) = P (wk|h0.tag, h−1.tag) (7.1)\nP (tk|wk, Wk−1Tk−1) = P (tk|wk, [Wk−1Tk−1]) = P (tk|wk, h0.tag, h−1.tag) (7.2)\nP (pk\ni |WkTk) = P (pk\ni |[WkTk]) = P (pk\ni |h0.tag, h−1.tag) (7.3)\nIt can be seen that the probabilistic dependency structure is more complex than\nthat in a CFG even in this simpliﬁed SLM.\nAlong the same lines, the approach in [19] regards the word sequenc e W with the\nparse structure T as a Markov graph ( W, T ) modeled using the CFG dependencies\nsuperimposed on the regular word-level 2-gram dependencies, sh owing improvement\nin perplexity over both 2-gram and 3-gram modeling techniques.\n93\n<s>    the     contract   ended     with   a    loss    </s>\nSB DT NN VBD IN DT NN SE\nNP NP\nPP\nVP\nS\nTOP\nTOP'\nFigure 7.1: CFG dependencies\n<s>    the     contract   ended     with   a    loss    </s>\nSB DT NN VBD IN DT NN SE\nNP NP\nPP\nVP\nS\nTOP\nTOP'\nFigure 7.2: Tag reduced WORD-PREDICTOR dependencies\n7.2.2 Language Model\nA structured approach to language modeling has been taken in [25]: t he underly-\ning probability model P (W, T ) is a simple lexical link grammar, which is automatically\ninduced and reestimated using EM from a training corpus containing w ord sequences\n(sentences). The model doesn’t make use of POS/NT labels — which w e found ex-\ntremely useful for word prediction and parsing. Another constra int is placed on the\ncontext used by the word predictor: the two words in the context used for word\nprediction are always adjacent; our models’ hierarchical scheme a llows the exposed\nheadwords to originate at any two diﬀerent positions in the word pre ﬁx. Both ap-\nproaches share the desirable property that the 3-gram model be longs to the parameter\nspace of the model.\n94\n<s>    the     contract   ended     with   a    loss    </s>\nSB DT NN VBD IN DT NN SE\nNP NP\nPP\nVP\nS\nTOP\nTOP'\nFigure 7.3: TAGGER dependencies\n<s>    the     contract   ended     with   a    loss    </s>\nSB DT NN VBD IN DT NN SE\nNP NP\nPP\nVP\nS\nTOP\nTOP'\nFigure 7.4: Tag reduced CONSTRUCTOR dependencies\nThe language model we present is closely related to the one investiga ted in [7] 1,\nhowever diﬀerent in a few important aspects:\n• our model operates in a left-to-right manner, thus allowing its use d irectly in\nthe hypothesis search for ˆW in (1.1);\n• our model is a factored version of the one in [7], thus enabling the calc ulation\nof the joint probability of words and parse structure; this was not possible in\nthe previous case due to the huge computational complexity of tha t model;\n• our model assigns probability at the word level, being a proper langua ge model.\n1The SLM might not have happened at all, weren’t it for the work and cr eative environment in\nthe WS96 Dependency Modeling Group and the authors’ desire to wr ite a PhD thesis on structured\nlanguage modeling\n95\nThe SLM shares many features with both class based language mode ls [23] and\nskip n-gram language models [27]; an interesting approach combining c lass based\nlanguage models and diﬀerent order skip-bigram models is presented in [28]. It seems\nworthwhile to make two comments relating the SLM to these approac hes:\n• the smoothing involving NT/POS tags in the WORD-PREDICTOR is similar\nto a class based language model using NT/POS labels for classes. We d epart\nhowever from the usual approach by not making the conditional independence\nassumption P (wk+1|wk, class(wk)) = P (wk+1|class(wk)). Also, in our model the\n“class” assignment — through the heads exposed by a given parse Tk for the\nword preﬁx Wk and its “weight” ρ(Wk, Tk), see Eq. (2.9) — is highly context-\nsensitive — it depends on the entire word-preﬁx Wk — and is syntactically\nmotivated through the operations of the CONSTRUCTOR. A compar ison be-\ntween the hh and HH equivalence classiﬁcations in the WORD-PREDICT OR\n— see Table 4.5 — shows the usefulness of POS/NT labels for word pred iction.\n• recalling the depth factorization of the model in Eq. (4.3), our mode l can be\nviewed as a skip n-gram where the probability of a skip P (d0, d1|Wk) — d0, d1\nare the depths at which the two most recent exposed headwords h0, h1 can\nbe found, similar to P (d|Wk) — is highly context sensitive. Notice that the\nhierarchical scheme for organizing the word preﬁx allows for conte xts that do\nnot necessarily consist of adjacent words, as in regular skip n-gra m models.\n7.3 Future Directions\nWe have presented an original approach to language modeling that m akes use of\nsyntactic structure. The experiments we have carried out show im provement in both\nperplexity and word error rate over current state-of-the-art techniques. Preliminary\nexperiments reported in [30] show complementarity between the SL M and a topic\nlanguage model yielding almost additive results — word error rate impr ovement —\non the Switchboard task. Among the directions which we consider wo rth exploring\nin the future, are:\n96\n• automatic induction of the SLM initial parameter values;\n• better integration of the 3-gram model and the SLM;\n• better parameterization of the model components;\n• study interaction between SLM and other language modeling techniq ues such\nas cache and trigger or topic language models.\n97\nAppendix A\nMinimizing KL Distance is\nEquivalent to Maximum Likelihood\nLet fT (Y ) be the relative frequency probability distribution induced on Y by\nthe collection of training samples T ; this determines the set of desired distributions\nPT\n.= {p(X, Y ) : p(Y ) = fT (Y )}. Let Q(Θ) .= {qθ(X, Y ) : θ ∈ Θ } be the model\nspace.\nProposition 2 Finding the maximum likelihood estimate g ∈ Q(Θ) is equivalent to\nﬁnding the pair (p, q) ∈ PT × Q(Θ) which minimizes the KL-distance D(p ∥ q).\nFor a given pair ( p, q) ∈ PT × Q(Θ) we have:\nD(p ∥ q) =\n∑\nx∈X ,y ∈Y\np(x, y) log p(x, y)\nq(x, y)\n=\n∑\nx∈X ,y ∈Y\nf(y) · r(x|y) log f(y) · r(x|y)\nq(y) · q(x|y)\n=\n∑\ny∈Y\nf(y) log f(y) − L(T , q) +\n∑\ny∈Y\nf(y) · D(r(x|y) ∥ q(x|y))\n≥\n∑\ny∈Y\nf(y) log f(y) − max\nq∈Q(Θ)\nL(T , q) + 0\nThe minimum value of D(p ∥ q) is independent of p and q and is achieved if and\nonly if both:\nq(x, y) = arg max\ngθ ∈Q(Θ)\nL(T , gθ)\n98\nr(x|y) = q(x|y)\nare satisﬁed. The second condition is equivalent to p being the I-projection of a given\nq onto PT :\np = arg min\nt∈PT\nD(t ∥ q)\n= arg min\nr(x|y)\nD(f(y) · r(x|y) ∥ q)\nSo knowing the pair ( p, q) ∈ PT × Q(Θ) that minimizes D(p ∥ q) implies that the\nmaximum likelihood distribution q ∈ Q(Θ) has been found and reciprocally, once the\nmaximum likelihood distribution q ∈ Q(Θ) is given we can ﬁnd the p distribution in\nPT that will minimize D(p ∥ q), p ∈ PT , q ∈ Q(Θ).\n✷\n99\nAppendix B\nExpectation Maximization as\nAlternating Minimization\nLet fT (Y ) be the relative frequency probability distribution induced on Y by\nthe collection of training samples T ; this determines the set of desired distributions\nPT\n.= {p(X, Y ) : p(Y ) = fT (Y )}. Let Q(Θ) .= {qθ(X, Y ) : θ ∈ Θ } be the model\nspace.\nProposition 3 One alternating minimization step between PT and Q(Θ) is equiva-\nlent to an EM update step:\nEMT ,θ i (θ) .=\n∑\ny∈Y\nfT (y)Eqθi (X/Y )[log(qθ(X, Y )|y)], θ ∈ Θ (B.1)\nθi+1 = arg max\nθ∈Θ\nEMT ,θ i (θ) (B.2)\nOne alternating minimization step starts from a given distribution qn ∈ Q(Θ),\nﬁnds the I-projection pn of qn onto PT ; ﬁxing pn we then ﬁnd the I-projection qn+1 of\npn onto Q(Θ). We will show that this leads to the EM update equations B.2.\nGiven qn ∈ Q(Θ) , ∀p ∈ PT , we have:\nD(p ∥ qn) =\n∑\nx∈X ,y ∈Y\np(x, y) log p(x, y)\nqn(x, y)\n=\n∑\nx∈X ,y ∈Y\nf(y) · r(x|y) log f(y) · r(x|y)\nqn(x, y)\n100\n(x,y)p\nn\nq\nq\nP Q\n n\nn+1\n(x,y)\n(x,y)\n=\nf(y) r(x|y)\nFigure B.1: Alternating minimization between PT and Q(Θ)\n=\n∑\ny∈Y\nf(y) log f(y)\nqn(y) +\n∑\ny∈Y\nf(y)(\n∑\nx∈X\nr(x/y) log r(x/y)\nqn(x/y))\n=\n∑\ny∈Y\nf(y) log f(y)\nqn(y)\n  \nindependent of r (x|y)\n+\n∑\ny∈Y\nf(y) · D(r(x/y), qn(x/y))  \n≥0\nwhich implies that:\nmin\np∈PT\nD(p ∥ qn) =\n∑\ny∈Y\nf(y) log f(y)\nqn(y)\nis achieved by pn = f(y) · qn(x|y).\nNow ﬁxing pn we seek the q ∈ Q(Θ) which minimizes D(pn ∥ q):\nD(pn ∥ q) =\n∑\nx∈X ,y ∈Y\npn(x, y) log pn(x, y)\nq(x, y)\n=\n∑\nx∈X ,y ∈Y\nf(y) · qn(x|y) log f(y) · qn(x|y)\nq(x, y)\n=\n∑\ny∈Y\nf(y) log f(y)\nqn(y) +\n∑\ny∈Y\nf(y) · [\n∑\nx∈X\nqn(x|y) log qn(x|y)]\n  \nindependent of q (x,y )\n−\n∑\nx∈X ,y ∈Y\nf(y)qn(x|y) log q(x, y)\n101\nBut the last term can rewritten as:\n∑\nx∈X ,y ∈Y\nf(y)qn(x|y) log q(x, y) =\n∑\ny∈Y\nf(y)\n∑\nx∈X\nqn(x|y) log q(x, y)\n=\n∑\ny∈Y\nf(y)Eqn(X|Y )[log q(x, y)|y]\n  \nEMT ,θ i (θ)\nThus ﬁnding\nmin\nq∈Q(Θ)\nD(pn ∥ q)\nis equivalent to ﬁnding\nmax\nq∈Q(Θ)\nEMT ,θ i (θ)\nwhich is exactly the EM-update step (B.2).\n✷\n102\nAppendix C\nN-best EM convergence\nIn the “N-best” training paradigm we use only a subset of the condit ional hidden\nevent space X |y, for any given seen y. Associated with the model space Q(Θ) we\nnow have a family of strategies to sample from X |y a set of “N-best” hidden events\nx, for any y ∈ Y . Each sampling strategy is a function that associates a set of hidde n\nsequences to a given observed sequence: s : Y → 2X . The family is parameterized by\nθ ∈ Θ:\nS(Θ) .= {sθ : Y → 2X , ∀θ ∈ Θ } (C.1)\nEach θ value identiﬁes a particular sampling function.\nLet:\nqs\nθ(X, Y ) .= qθ(X, Y ) · 1sθ (Y )(X) (C.2)\nqs\nθ(X|Y ) .= qθ(X, Y )\n∑\nX∈∫θ (Y ) qθ(X, Y ) · 1sθ (Y )(X) (C.3)\nQ(S, Θ) .= {qs\nθ(X, Y ) : θ ∈ Θ } (C.4)\nProposition 4 Assuming that ∀θ ∈ Θ , Sup(qθ) = X × Y (“smooth” qθ(x, y)) holds,\none alternating minimization step between PT and Q(S, Θ) — θi → θi+1 — is equiva-\nlent to:\nθi+1 = arg max\nθ∈Θ\n∑\ny∈Y\nfT (y)Eqs\nθi\n(X|Y )[log(qθ(X, Y )|y)] (C.5)\n103\nif θi+1 satisﬁes:\nsθi (y) ⊆ sθi+1 (y), ∀y ∈ T (C.6)\nOnly θ ∈ Θ s.t. s θi (y) ⊆ sθ(y), ∀y ∈ T are candidates in the M-step.\nProof:\nE-step:\nGiven qs\nθi (x, y) ∈ Q(S, Θ), ﬁnd pn(x, y) = f(y)·rn(x|y) ∈ P (T ) s.t. D (f(y) · rn(x|y) ∥\nqs\nθi (x, y)) is minimized. As shown in appendix B:\nrn(x|y) = qs\nθi (x|y), ∀y ∈ (T ) (C.7)\nNotice that for smooth qθi (x|y) we have:\nSup(rn(x|y)) = Sup(qs\nθi (x|y)) = sθi (y), ∀y ∈ T (C.8)\nM-step:\ngiven pn(x, y) = f(y) · qs\nθi (x|y), find θ i+1 ∈ Θ s.t. D (pn ∥ qs\nθi+1 ) is minimized .\nLemma 1 For the M-step we only need to consider candidates θ ∈ Θ for which we\nhave\nsθi (y) ⊆ sθ(y), ∀y ∈ T (C.9)\nIndeed, assuming that ∃ (x0, y0) s.t. y 0 ∈ T and x0 ∈ sθi (y) but x0 /∈ sθ(y), we\nhave: ( x0, y0) ∈ Sup(f(y) · rn(x|y)) (see (C.8)) and ( x0, y0) /∈ Sup(qs\nθ(x, y)) (see (C.2))\nwhich means that f(y0) · rn(x0|y0) > 0 and qs\nθ(x0, y0) = 0, rendering\nD(f(y) · rn(x|y) ∥ qs\nθ(x, y)) = ∞.\n✷\nFollowing the proof in appendix B, it is easy to show that:\nθ∗ = arg max\nθ∈Θ\n∑\ny∈Y\nfT (y)Eqs\nθi\n(X|Y )[log(qs\nθ(X, Y )|y)] (C.10)\nminimizes D(pn ∥ qs\nθ), ∀θ ∈ Θ.\n104\nUsing the result in Lemma 1, only θ ∈ Θ satisfying (C.9) are candidates for the\nM-step, so:\nθ∗ = arg max\nθ∈Θ |sθi (y)⊆sθ (y), ∀y∈T\n∑\ny∈Y\nfT (y)Eqs\nθi\n(X|Y )[log(qθ(X, Y ) · 1sθ (Y )(X)|y)] (C.11)\nBut notice that Sup(qs\nθi (x|y)) = sθi (y), ∀y ∈ T (see (C.8)) and these are the only\nx values contributing to the conditional expectation on a given y ; for these however\nwe have 1 sθ (y)(x) = 1 because of (C.9). This implies that (C.11) can be rewritten as:\nθ∗ = arg max\nθ∈Θ |sθi (y)⊆sθ (y), ∀y∈T\n∑\ny∈Y\nfT (y)Eqs\nθi\n(X|Y )[log(qθ(X, Y )|y)] (C.12)\nBecause the set over which the maximization is carried over depends on θi the\nM-step is not simple. However we notice that if the maximum on the ent ire space Θ:\nθi+1 = arg max\nθ∈Θ\n∑\ny∈Y\nfT (y)Eqs\nθi\n(X|Y )[log(qθ(X, Y )|y)] (C.13)\nsatisﬁes: sθi (y) ⊆ sθi+1 (y), ∀y ∈ T , then θi+1 is the correct update θ∗.\n✷\n105\nAppendix D\nStructured Language Model\nParameter Reestimation\nThe probability of a ( W, T ) sequence is obtained by chaining the probabilities of\nthe elementary events in its derivation, as described in section 2.3:\nP (W, T ) = P (d(W, T )) =\nlength(d(W,T ))∏\ni=1\np(ei) (D.1)\nThe E-step is carried by sampling the space of hidden events for a giv en seen\nsequence W according to the pruning strategy outlined in section 2.5:\nP s\nθ (W, T ) .= Pθ(W, T ) · 1sθ (W )(T )\nP s\nθ (T |W ) .= Pθ(T, W )\n∑\nT ∈∫θ (W ) Pθ(W, T ) · 1sθ (W )(T )\nThe logarithm of the probability of a given derivation can be calculated as follows:\nlog Pθ(W, T )\n=\nlength(d(W,T ))∑\ni=1\nlog Pθ(ei)\n=\n∑\nm\n∑\n(u(m),z (m))\nlength(d(W,T ))∑\ni=1\nlog Pθ(u(m), z(m)) · δ(ei, (u(m), z(m)))\n106\n=\n∑\nm\n∑\n(u(m),z (m))\n[\nlength(d(W,T ))∑\ni=1\nδ(ei, (u(m), z(m)))] · log Pθ(u(m), z(m))\n=\n∑\nm\n∑\n(u(m),z (m))\n#[(u(m), z(m)) ∈ d(W, T )] · log Pθ(u(m), z(m))\nwhere the random variable\n#[(u(m), z(m)) ∈ d(W, T )]\ndenotes the number of occurrences of the ( u(m), z(m)) event in the derivation of W, T .\nLet\nEP s\nθi\n(T |W )[#[(u(m), z(m)) ∈ d(W, T )]] .= aθi ((u(m), z(m)), W )\n∑\nW ∈T\nf(W ) · aθi ((u(m), z(m)), W ) .= aθi (u(m), z(m))\nWe then have:\nEP s\nθi\n(T |W )[log Pθ(W, T )]\n=\n∑\nm\n∑\n(u(m),z (m))\naθi ((u(m), z(m)), W ) · log Pθ(u(m), z(m))\nand\n∑\nW ∈T\nf(W ) · EP s\nθi\n(T |W )[log Pθ(W, T )] (D.2)\n=\n∑\nm\n∑\n(u(m),z (m))\naθi (u(m), z(m)) · log Pθ(u(m), z(m)) (D.3)\nThe E-step thus consists of the calculation of the expected values aθi ((u(m), z(m))),\nfor every model component and every event ( u(m), z(m)) in the derivations that sur-\nvived the pruning process.\nIn the M-step we need to ﬁnd a new parameter value θi+1 such that me maximize\nthe EM auxiliary function (D.2):\nθi+1 = arg max\nθ∈θ\n∑\nW ∈T\nf(W ) · EP s\nθi\n(T |W )[log Pθ(W, T )] (D.4)\n= arg max\nθ∈θ\n∑\nm\n∑\n(u(m),z (m))\naθi ((u(m), z(m))) · log Pθ(u(m), z(m)) (D.5)\n107\nThe parameters θ are the maximal order joint counts C(m)(u(m), z(m)) for each\nmodel component m ∈ { WORD-PREDICTOR, TAGGER, PARSER }.\nOne can easily notice that the M-step is in fact a problem of maximum like lihood\nestimation for each model component m from joint counts aθi ((u(m), z(m))). Taking\ninto account the parameterization of Pθ(u(m), z(m)) (see Section 2.4) the problem can\nbe seen as an HMM reestimation problem. The EM algorithm can be emplo yed to\nsolve it. Convergence takes place in exactly one EM iteration to:\nC(m)\ni+1 (u(m), z(m)) = aθi ((u(m), z(m)))\n.\n108\nBibliography\n[1] Steven Abney, David McAllester, and Fernando Pereira. Relating probabilistic\ngrammars and automata. In Proceedings of ACL , volume 1, pages 541–549.\nCollege Park, Maryland, USA, 1999.\n[2] L. R. Bahl, F. Jelinek, and R. L. Mercer. A maximum likelihood approa ch to\ncontinuous speech recognition. In IEEE Transactions on Pattern Analysis and\nMachine Intelligence , volume PAMI-5, pages 179–90, March 1983.\n[3] L. Baum. An inequality and associated maximization technique in sta tistical es-\ntimation of probabilistic functions of a markov process. In Inequalities, volume 3,\npages 1–8. 1972.\n[4] J.R. Bellegarda. A latent semantic analysis framework for large–s pan language\nmodeling. In Proceedings of Eurospeech 97 , pages 1451–1454, Rhodes, Greece,\n1997.\n[5] A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum en tropy\napproach to natural language processing. Computational Linguistics , 22(1):39–\n72, March 1996.\n[6] W. Byrne, A. Gunawardana, and S. Khudanpur. Information ge ometry and EM\nvariants. Technical Report CLSP Research Note 17, Department of Electical and\nComputer Engineering, The Johns Hopkins University, Baltimore, MD , 1998.\n[7] C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudanpur, L. Man gu, H. Printz,\nE. S. Ristad, R. Rosenfeld, A. Stolcke, and D. Wu. Structure and p erformance\n109\nof a dependency language model. In Proceedings of Eurospeech, volume 5, pages\n2775–2778. Rhodes, Greece, 1997.\n[8] Ciprian Chelba. A structured language model. In Proceedings of ACL-EACL ,\npages 498–500,student section. Madrid, Spain, 1997.\n[9] Ciprian Chelba and Frederick Jelinek. Exploiting syntactic structu re for language\nmodeling. In Proceedings of COLING-ACL , volume 1, pages 225–231. Montreal,\nCanada, 1998.\n[10] CLSP. WS97. In Proceedings of the 1997 CLSP/JHU Workshop on Innova-\ntive Techniques for Large Vocabulary Continuous Speech Rec ognition. Baltimore,\nJuly-August 1997.\n[11] Michael John Collins. A new statistical parser based on bigram lex ical depen-\ndencies. In Proceedings of the 34th Annual Meeting of the Association fo r Com-\nputational Linguistics , pages 184–191. Santa Cruz, CA, 1996.\n[12] T. M. Cover and J. A. Thomas. Elements of Information Theory , pages 364–367.\nJohn Wiley & Sons, New York, 1991.\n[13] I. Csizar and G. Tusnady. Information geometry and alternat ing minimization\nprocedures. In Statistics and Decisions , volume Suplementary Issue Number 1,\npages 205–237. 1984.\n[14] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood fr om\nincomplete data via the EM algorithm. In Journal of the Royal Statistical Society ,\nvolume 39 of B, pages 1–38. 1977.\n[15] J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCHBOARD tele phone\nspeech corpus for research and development. In Proceedings of IEEE Confer-\nence on Acoustics, Speech and Signal Processing , volume 1, pages 517–520. San\nFrancisco, March 1992.\n[16] Liliane Haegeman. Introduction to Government and Binding Theory , pages 138–\n141. Blackwell, 1994.\n110\n[17] Frederick Jelinek. Information Extraction From Speech And Text . MIT Press,\n1997.\n[18] Frederick Jelinek and Robert Mercer. Interpolated estimation of markov source\nparameters from sparse data. In E. Gelsema and L. Kanal, editors , Pattern\nRecognition in Practice , pages 381–397. 1980.\n[19] U. Grenander K. E. Mark, M. I. Miller. Constrained stochastic la nguage models.\nIn S. E. Levinson and L. Shepp, editors, Image Models (and their Speech Model\nCousins). Springer, 1996.\n[20] S. Katz. Estimation of probabilities from sparse data for the lan guage model\ncomponent of a speech recognizer. In IEEE Transactions on Acoustics, Speech\nand Signal Processing , volume 35, pages 400–01, March 1987.\n[21] M. Marcus, B. Santorini, and M. Marcinkiewicz. Building a large ann otated\ncorpus of English: the Penn Treebank. Computational Linguistics , 19(2):313–\n330, 1995.\n[22] N. Nilsson. Problem Solving Methods in Artiﬁcial Intelligence , pages 266–278.\nMcGraw-Hill, New York, 1971.\n[23] P. deSouza J. Lai P. Brown, V. Della Pietra and R. Mercer. Class -based n-gram\nmodels of natural language. In Computational Linguistics , volume 18, pages\n467–479. 1997.\n[24] Doug B. Paul and Janet M. Baker. The design for the Wall Stree t journal-based\nCSR corpus. In Proceedings of the DARPA SLS Workshop . February 1992.\n[25] S. Della Pietra, V. Della Pietra, J. Gillet, J. Laﬀerty, H. Printz, an d L. Ures.\nInference and estimation of a long-range trigram model. Technical Report CMU-\nCS-94-188, School of Computer Science, Carnegie Mellon Universit y, Pittsburg,\nPA, 1994.\n111\n[26] Adwait Ratnaparkhi. A linear observed time statistical parser b ased on maxi-\nmum entropy models. In Second Conference on Empirical Methods in Natural\nLanguage Processing, pages 1–10, Providence, R.I., 1997.\n[27] Ronald Rosenfeld. Adaptive Statistical Language Modeling: A Maximum Entropy\nApproach. PhD thesis, School of Computer Science, Carnegie Mellon Universit y,\nPittsburgh, April 1994.\n[28] Lawrence Saul and Fernando Pereira. Aggregate and mixed-o rder markov models\nfor statistical language processing. In Proceedings of the Second Conference on\nEmpirical Methods in Natural Language Processing , pages 81–89. San Francisco,\nCA, 1997.\n[29] A. J. Viterbi. Error bounds for convolutional codes and an asy mmetrically opti-\nmum decoding algorithm. In IEEE Transactions on Information Theory , volume\nIT-13, pages 260–267, 1967.\n[30] Jun Wu and Sanjeev Khudanpur. Combining nonlocal, syntactic a nd n-gram\ndependencies in language modeling. In Proceedings of Eurospeech’99, page to\nappear. 1999.\n112\nVita\nCiprian Chelba received a Diploma Engineer title from ”Politehnica” Unive rsity,\nBucharest, Romania, the Faculty of Electronics and Telecommunica tions, in 1993.\nThe Diploma Thesis “Neural Network Controller for Buck Circuit” has been devel-\noped at Politecnico di Torino, Italy, under the joint advising of Prof . Vasile Buzuloiu\nand Prof. Franco Maddaleno, on a Tempus grant awarded by the EU . He received an\nMS degree from The Johns Hopkins University in 1996.\nHe is member of the IEEE and the Association for Computational Ling uistics."
}