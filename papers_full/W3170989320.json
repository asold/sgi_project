{
  "title": "A Package for Learning on Tabular and Text Data with Transformers",
  "url": "https://openalex.org/W3170989320",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2365012518",
      "name": "Ken Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2885689471",
      "name": "Akshay Budhkar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972119347",
    "https://openalex.org/W4288104408",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2990070951",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963691697"
  ],
  "abstract": "Recent progress in natural language processing has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the Transformer does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face's existing API such as tokenization and the model hub which allows easy download of different pre-trained models.",
  "full_text": "Proceedings of the Third Workshop on Multimodal Artiﬁcial Intelligence, pages 69–73\nJune 6, 2021. ©2021 Association for Computational Linguistics\n69\nMultimodal-Toolkit: A Package for Learning on Tabular and\nText Data with Transformers\nKen Gu\nGeorgian\nken.gu@georgian.io\nAkshay Budhkar\nGeorgian\nakshay@georgian.io\nAbstract\nRecent progress in natural language process-\ning has led to Transformer architectures be-\ncoming the predominant model used for nat-\nural language tasks. However, in many real-\nworld datasets, additional modalities are in-\ncluded which the Transformer does not di-\nrectly leverage. We present Multimodal-\nToolkit,1 an open-source Python package to\nincorporate text and tabular (categorical and\nnumerical) data with Transformers for down-\nstream applications. Our toolkit integrates\nwell with Hugging Face’s existing API such as\ntokenization and the model hub2 which allows\neasy download of different pre-trained models.\n1 Introduction\nIn recent years, Transformers (Vaswani et al.,\n2017) have become popular for model pre-training\n(Howard and Ruder, 2018; Peters et al., 2018; De-\nvlin et al., 2019) and have yielded state-of-the-art\nresults on many natural language processing (NLP)\ntasks. In addition, well-documented Transformer\nlibraries such as Hugging Face Transformers (Wolf\net al., 2020), and AllenNLP (Gardner et al., 2018)\nhave democratized NLP, making it easier to pro-\nductionize and experiment on Transformers.\nHowever, there are not a lot of comprehensive\ntools for Transformers to work with tabular data.\nOften in real-world datasets, there are tabular data\nas well as unstructured text data which can pro-\nvide meaningful signals for the task at hand. For\ninstance, in the small example in Figure 1, each\nrow is a data point. Columns Title and Review\nText contain text features, columns Division\nName, Class Name, and Department Name\ncontain categorical features, and the Age column\nis a numerical feature. To the best of our knowl-\nedge, no tool exists that makes it simple for Trans-\nformers to handle this extra modality. Therefore,\n1Github: https://git.io/JO5a6\n2https://huggingface.co/docs\nFigure 1: An example of a clothing review classiﬁca-\ntion dataset. Each row is a data point consisting of text,\ncategorical features, and numerical features.\ngiven the advances of Transformers for natural\nlanguage tasks and the maturity of existing Trans-\nformer libraries, we introduce Multimodal-Toolkit,\na lightweight Python package built on top of Hug-\nging Face Transformers. Our package extends ex-\nisting Transformers in the Hugging Face’s Trans-\nformers library to seamlessly handle structured tab-\nular data while keeping the existing tokenization\n(including subword segmentation), experimental\npipeline, and pre-trained model hub functionalities\nof Hugging Face Transformers. We show the effec-\ntiveness of our toolkit on three real-world datasets.\n2 Related Work\nThere have been several proposed Transformer\nmodels that aim to handle text features and addi-\ntional features of another modality. For pre-trained\nTransformers on images and text, models such as\nViLBERT (Lu et al., 2019) and VLBERT (Su et al.,\n2020) are mainly the same as the original BERT\nmodel but treat the extra image modality as addi-\ntional tokens to the input. These models require\npre-training on multimodal image and text data.\nOn the other hand, while treating image features\n70\nas additional input tokens, MMBT (Kiela et al.,\n2019) proposes to use pre-trained BERT directly\nand ﬁne-tune on image and text data. This is simi-\nlar to Multimodal-Toolkit in which no pre-training\non text and tabular data is needed.\nLikewise, Transformers have been adapted to\nalign, audio, visual, and text modalities in which\nthere is a natural ground truth alignment. MulT\n(Tsai et al., 2019) is similar to ViLBert in which\nco-attention is used between pairs of modalities but\nalso includes temporal convolutions so that input to-\nkens are aware of their temporal neighbors. Mean-\nwhile, Rahman et al. (2020) injects cross modality\nattention at certain Transformer layers via a gating\nmechanism.\nFinally, knowledge graph embeddings have also\nbeen effectively combined with input text tokens\nin Transformers. Ostendorff et al. (2019) com-\nbines knowledge graph embeddings on authors\nwith book titles and other metadata features via\nsimple concatenation for book genre classiﬁcation.\nOn the other hand, for more general language tasks,\nERNIE (Zhang et al., 2019) ﬁrst matches the to-\nkens in the input text with entities in the knowledge\ngraph. With this matching, the model fuses these\nembeddings to produce entity-aware text embed-\ndings and text-aware entity embeddings.\nHowever, these models do not capture categor-\nical and numerical data explicitly. Hugging Face\ndoes include LXMERT (Tan and Bansal, 2019) to\nhandle language and vision modality but this can\nnot be easily adapted for categorical and numeri-\ncal data. Nevertheless, existing multimodal Trans-\nformer models do give good insights into how to\ncombine categorical and numerical features. ViL-\nBERT and VLBERT for example include image\nmodality as input tokens which lead to one of our\nsimple baseline of categorical and numerical fea-\ntures as additional token inputs to the model. Like-\nwise, the gating mechanism Rahman et al. (2020),\nattention, and different weighting schemes have\nall been shown to be useful in combining different\nmodalities.\n3 Design\nThe goal of Multimodal-Toolkit is to allow users\nto quickly adapt state-of-the-art Transformer mod-\nels for situations involving text and tabular data\nwhich occur often in real-world datasets. More-\nover, we want to bring the beneﬁts of Transformers\nto more use cases while making it simple for users\nFigure 2: The framework of Multimodal-Toolkit.\nThere is a data processing module that outputs pro-\ncessed text, numerical, and categorical features that are\nthen fed as input to our Transformer With Tabular mod-\nule consisting of a Hugging Face Transformer and our\ncombining module.\nof Hugging Face Transformers to adopt. Therefore,\nwe maintain the existing interface of the popular\nHugging Face Transformers library.\nThis design enables us to easily include more\nTransformer models, leverage strengths of spe-\nciﬁc models, use a feature-rich training pipeline,\nand integrate the thousands of community trained\nmodels on Hugging Face’s model hub. We sup-\nport a variety of Transformers (e.g. BERT, AL-\nBERT, RoBERTa, XLNET) for both classiﬁcation\nand regression tasks. All together, this becomes a\nreusable Transformer With Tabular component. We\nalso provide a data preprocessing module for cate-\ngorical and numerical features. An overview of the\nsystem is shown in Figure 2. Currently, the library\nsupports PyTorch Transformers implementations.\n3.1 Combining Module\nWe implement a combining module that is model\nagnostic that takes as input, x, the text features\noutputted from a Transformer model and prepro-\ncessed categorical (c) and numerical (n) features,\nand outputs a combined multimodal representation\nm. Although existing multimodal Transformers\nincorporate cross-modal attention inside middle\nTransformer layers, we choose the design in which\nthe modality combination comes after the Trans-\nformer because this module can be easily included\nwithout much adaptation of the existing Hugging\nFace Transformer interface and can be easily ex-\ntended to new Transformers included in the future.\nInside the combining module, we implement var-\n71\nCombine Feature Method Equation\nText only m = x\nConcat m = x||c||n\nIndividual MLPs on categorical and-\nnumerical features then concat (MLP + Concat) m = x||MLP(c)||MLP(n)\nMLP on concatenated categorical and\nnumerical features then concat (Concat + MLP) m = x||MLP(c||n)\nAttention on categorical and numerical\nfeatures (Attention)\nm = αx,xWxx + αx,cWcc + αx,nWnn\nαi,j = exp(LeakyReLU(aT [Wixi||Wjxj]))∑\nk∈{x,c,n} exp(LeakyReLU(aT [Wixi||Wkxk]))\nGating on categorical and numerical\nfeatures and then sum (Rahman et al., 2020)\n(Gating)\nm = x + αh\nh = gc ⊙(Wcc) +gn ⊙(Wnn) +bh\nα= min(||x||2\n||h||2\n) ∗β,1)\ngi = R(Wgi [i||x + bi)\nwhere βis a hyperparameter and R is an activation function\nWeighted feature sum on text, categorical,\nand numerical features (Weighted Sum) m = x + wc ⊙Wcc + wn ⊙Wnn\nTable 1: The included combining methods in the combining module. Uppercase bold letters represent 2D matri-\nces, lowercase bold letters represent 1D vectors. b is a scalar bias, W represents a weight matrix, and ||is the\nconcatenation operator. Please see Rahman et al. (2020) for details on the gating mechanism.\nDataset Task Size T C N\nAirbnb Regression 64k 3 74 15\nClothing Classiﬁcation 15k 2 3 3\nPetFinder Classiﬁcation 28k 2 14 5\nTable 2: Statistics of the datasets involved in experi-\nments. T is the number of text columns. C is the num-\nber of categorical features, and N is the number of nu-\nmerical features.\nious methods of combining the different represen-\ntations in their respective feature spaces into one\nuniﬁed representation. These methods are inspired\nby the related work in multimodal Transformers as\nwell as straightforward reasonable baselines such\nas concatenation and multi-layer perceptron (MLP)\nconcatenation. Given a pre-trained Transformer,\nthe parameters of the combining module and Trans-\nformer are trained based on the supervised task. In\nother words, the Transformer is further ﬁne-tuned.\nThe included methods are shown in Table 1.\n4 Experiments\nIn this section, we study the effectiveness of lever-\naging tabular features on data with text and tabular\ndata. We evaluate Multimodal-Toolkit on three\nreal-world datasets from Kaggle.\n4.1 Datasets\nRegression: For regression, we use the Mel-\nbourne Airbnb Open Data (Airbnb) dataset (Xie,\n2019) for the task of listing price prediction. Each\ndata example is an Airbnb listing. Text features\ninclude the name of the listing, the summary of the\nlisting, and a host description.\nBinary Classiﬁcation: For binary regression,\nwe use Women’s E-Commerce Clothing Reviews\n(Clothing) (Brooks, 2018). The source of the re-\nviews is anonymous. Data examples consist of a\nreview, a rating, the clothing category of the prod-\nuct etc. The goal is to predict if the review is rec-\nommending the product.\nMulticlass Classiﬁcation: Finally, we also in-\nclude the PetFinder.my Adoption Prediction\n(PetFinder) dataset (PetFinder.my, 2018). Given\nthe listing information of a pet set for adoption, the\ngoal is to predict the speed at which a pet will be\nadopted, represented as 5 classes. Text features\ninclude the listing description and the pet name.\n4.2 Experimental Setting\nFor experiments, we test each combining feature\nmethod described in Table 1. In addition, as men-\ntioned in Section 2 we test a baseline in which the\ncategorical and numerical features are also treated\n72\nAirbnb Clothing PetFinder\nMethod RMSE MAE F1 AUPRC F1 macro F1micro\nText Only 254.0 82.74 0.957 0.992 0.088 0.281\nUnimodal 245.2 79.34 0.968 0.995 0.089 0.283\nConcat 239.3 65.68 0.958 0.992 0.199 0.362\nMLP + Concat 237.3 66.73 0.959 0.992 0.244 0.352\nConcat + MLP 238.0 65.66 0.959 0.992 0.176 0.344\nAttention 246.3 74.72 0.959 0.992 0.254 0.375\nGating (Rahman et al., 2020) 237.8 66.64 0.961 0.994 0.275 0.375\nWeighted Sum 245.2 71.19 0.962 0.994 0.266 0.380\nTable 3: Comparison of combining methods with results on regression and classiﬁcation tasks. For each metric,\nthe best performing model is in bold. For regression we use Root-mean-squared Error (RMSE) and MAE (Mean\nAbsolute Error). In both cases, lower is better. For binary classiﬁcation, we report F1 score and area under the\nprecision-recall curve (AUPRC). Meanwhile, for multiclass classiﬁcation, we use F1 macro and F1micro. In all\nclassiﬁcation metrics, higher is better.\nas text columns. For example, for the situation\nin Figure 1, the text representing categorical fea-\ntures in Division Name, Class Name, and\nDepartment Name as well the numerical value\nin Age would all be tokenized and be treated as\nadditional inputs to the Transformer. We denote\nthis baseline as Unimodal.\nFor the Clothing Review dataset, we use\nbert-base-uncased as our Transformer and\ntokenizer. For the Airbnb dataset and Pet\nAdoption datasets, because there are some data\npoints containing non-English text, we use\nbert-base-multilingual. We keep the\ntraining settings consistent for a given dataset.\nWe train for 5 epochs and perform 4-fold-cross-\nvalidation, reporting the mean performance. For\nregression, we use a learning rate of 3e-3 while for\nclassiﬁcation tasks we use a learning rate of 5e-5.\nWe report the results in Table 3.\n4.3 Results\nFrom Table 3, we observe the effectiveness of in-\ncorporating tabular features across different tasks\nand datasets. For each real-world dataset, the text-\nonly baseline is the worst performing model. This\nshows using only text data with Transformers may\nbe insufﬁcient when extra tabular data is available.\nHowever, how much the performance improves\nby leveraging Tabular features depends on the\ndataset. In the case of the Clothing Review dataset,\nthe text of the review was already a very strong sig-\nnal to the prediction, extra tabular features did not\nimprove the performance much. We hypothesize\nthe strong performance of the text only baseline\nmay be due to the task of classifying review recom-\nmendation simplifying to sentiment classiﬁcation,\nwhich the text modality provides the strongest sig-\nnals. On the other hand, for the PetFinder dataset,\nthe text description of the animal may not be sufﬁ-\ncient to predict adoption speed. Rather, it is tabular\nfeatures such as the age or the breed of the pet.\nFurthermore, the relative low raw performance of\nPetFinder dataset could be attributed to the difﬁ-\nculty of the task as a forecasting problem.\nAdditionally, although the Unimodal baseline\nis the best for the clothing dataset, this method\ndoes not appear to scale well when the number\nof categorical and numerical features increases or\nwhen the extra features’ text representation does\nnot reveal obvious semantic meaning.\n5 Conclusion\nThis paper presents Multimodal-Toolkit, an open-\nsource Python library powered by Hugging Face\nTransformers to learn on data that contains both\ntext and tabular data. We show the effectiveness\nof incorporating tabular data and treating it as a\nseparate modality with the already powerful Trans-\nformers. The modular design and shared API with\nHugging Face allow users quick access to Hugging\nFace’s community uploaded Transformer models.\nFor future work, we aim to include support for\nmore Transformers and integrate the combining\nmodule at earlier layers in the Transformer. We\nhope the toolkit brings more research attention to\nthis data scenario and we welcome open-source\ncontributions to the project.\n73\nReferences\nNick Brooks. 2018. Women’s e-commerce clothing re-\nviews.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1–\n6, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nDouwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan\nPerez, and Davide Testuggine. 2019. Supervised\nMultimodal Bitransformers for Classifying Images\nand Text. arXiv e-prints, page arXiv:1909.02950.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, volume 32, pages 13–23. Curran Asso-\nciates, Inc.\nMalte Ostendorff, Peter Bourgonje, Maria Berger, Ju-\nlian Moreno-Schneider, Georg Rehm, and Bela\nGipp. 2019. Enriching BERT with Knowledge\nGraph Embeddings for Document Classiﬁcation.\narXiv e-prints, page arXiv:1909.08402.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nPetFinder.my. 2018. Petﬁnder.my adoption prediction.\nWasifur Rahman, Md Kamrul Hasan, Sangwu Lee,\nAmirAli Bagher Zadeh, Chengfeng Mao, Louis-\nPhilippe Morency, and Ehsan Hoque. 2020. Inte-\ngrating multimodal information in large pretrained\ntransformers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2359–2369, Online. Association for\nComputational Linguistics.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\nIn International Conference on Learning Represen-\ntations.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Rus-\nlan Salakhutdinov. 2019. Multimodal transformer\nfor unaligned multimodal language sequences. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n6558–6569, Florence, Italy. Association for Compu-\ntational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nTyler Xie. 2019. Melbourne airbnb open data.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7775223255157471
    },
    {
      "name": "Python (programming language)",
      "score": 0.6887516379356384
    },
    {
      "name": "Transformer",
      "score": 0.663416862487793
    },
    {
      "name": "Lexical analysis",
      "score": 0.44459304213523865
    },
    {
      "name": "Programming language",
      "score": 0.4378495514392853
    },
    {
      "name": "Categorical variable",
      "score": 0.43232598900794983
    },
    {
      "name": "XML",
      "score": 0.4180991053581238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36579379439353943
    },
    {
      "name": "Machine learning",
      "score": 0.2492978572845459
    },
    {
      "name": "World Wide Web",
      "score": 0.24192076921463013
    },
    {
      "name": "Engineering",
      "score": 0.11364778876304626
    },
    {
      "name": "Electrical engineering",
      "score": 0.08728879690170288
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}