{
    "title": "Fast Transformers with Clustered Attention",
    "url": "https://openalex.org/W3041753721",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2747763071",
            "name": "Vyas, Apoorv",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287479182",
            "name": "Katharopoulos, Angelos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743120667",
            "name": "Fleuret, François",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2024490156",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W1779452081",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W1577877742",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2949382160",
        "https://openalex.org/W2606101940",
        "https://openalex.org/W2773781902",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W2724346673",
        "https://openalex.org/W2963042606",
        "https://openalex.org/W2995428172",
        "https://openalex.org/W2166637769",
        "https://openalex.org/W2514741789",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2963056065"
    ],
    "abstract": "Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance.",
    "full_text": "Fast Transformers with Clustered Attention\nApoorv Vyas12 Angelos Katharopoulos12 Franc ¸ois Fleuret23 ∗\n1Idiap Research Institute, Switzerland\n2Ecole Polytechnique F´ed´erale de Lausanne, Switzerland\n3University of Geneva, Switzerland\nfirstname.lastname@idiap.ch\nAbstract\nTransformers have been proven a successful model for a variety of tasks in sequence\nmodeling. However, computing the attention matrix, which is their key component,\nhas quadratic complexity with respect to the sequence length, thus making them\nprohibitively expensive for large sequences. To address this, we propose clustered\nattention, which instead of computing the attention for every query, groups queries\ninto clusters and computes attention just for the centroids. To further improve this\napproximation, we use the computed clusters to identify the keys with the highest\nattention per query and compute the exact key/query dot products. This results\nin a model with linear complexity with respect to the sequence length for a ﬁxed\nnumber of clusters. We evaluate our approach on two automatic speech recognition\ndatasets and show that our model consistently outperforms vanilla transformers\nfor a given computational budget. Finally, we demonstrate that our model can\napproximate arbitrarily complex attention distributions with a minimal number\nof clusters by approximating a pretrained BERT model on GLUE and SQuAD\nbenchmarks with only 25 clusters and no loss in performance.\n1 Introduction\nSequence modelling is a fundamental task of machine learning, integral in a variety of applica-\ntions such as neural machine translation [2], image captioning [31], summarization [18], automatic\nspeech recognition [ 8] and synthesis [ 19] etc. Transformers [ 29] have been proven a powerful\ntool signiﬁcantly advancing the state-of-the-art for the majority of the aforementioned tasks. In\nparticular, transformers employ self-attention that allows them to handle long sequences without the\nvanishing-gradient problem inherent in RNNs [12, 1].\nNonetheless, despite their impressive performance, the use of self-attention comes with computational\nand memory requirements that scale quadratic to the sequence length, limiting their applicability to\nlong sequences. The quadratic complexity becomes apparent if we consider the core mechanism of\nself-attention, namely splitting the input sequence into queries and keys and then each query attending\nto all keys. To this end, recently, there has been an increasing interest for developing methods that\naddress this limitation [7, 28, 5, 13].\nThese methods can be broadly categorized into two distinct lines of work, those that focus on\nimproving the asymptotic complexity of the self-attention computation [ 5, 14, 13, 26] and those\nthat aim at developing techniques that make transformers applicable to longer sequences without\naddressing the quadratic complexity of self-attention [7, 28]. The former limits the amount of keys\nthat each query attends to, thus reducing the asymptotic complexity. The latter increases the length\nof the sequence that a transformer can attend to without altering the underlying complexity of the\nself-attention mechanism.\n∗Work done at Idiap\nPreprint. Under review.\narXiv:2007.04825v2  [cs.LG]  29 Sep 2020\nIn this work, we propose clustered attentionwhich is a fast approximation of self-attention. Clus-\ntered attention makes use of similarities between queries and groups them in order to reduce the\ncomputational cost. In particular, we perform fast clustering using locality-sensitive hashing and\nK-Means and only compute the attention once per cluster. This results in linear complexity for a\nﬁxed number of clusters (§3.2). In addition, we showcase that we can further improve the quality of\nour approximation by separately considering the keys with the highest attention per cluster (§3.3).\nFinally, we provide theoretical bounds of our approximation quality with respect to the full attention\n(§3.2.1, §3.3.1) and show that our model can be applied for inference of pre-trained transformers\nwith minimal loss in performance.\nWe evaluate our model on two automatic speech recognition datasets and showcase that clustered\nattention consistently achieves better performance than vanilla attention when the computational\nbudget is equalized. Moreover, we demonstrate that our proposed attention can approximate a\npretrained BERT model on the popular GLUE and SQuAD benchmarks with only 25 clusters and\nwithout loss in performance.\n2 Related Work\nIn this section, we discuss the most relevant works on scaling transformers to larger sequences. We\nstart by presenting approaches that aim to speed up the attention computation in general. Subsequently,\nwe discuss approaches that speed up transformers without changing the complexity of the attention\nlayer and ﬁnally, we summarize the most related works on improving the asymptotic complexity of\nthe attention layer in transformer models.\n2.1 Attention Improvements Before Transformers\nAttention has been an integral component of neural networks for sequence modelling for several\nyears [2, 31, 4]. However, its quadratic complexity with respect to the sequence length hinders its\napplicability on large sequences.\nAmong the ﬁrst attempts to address this was the work of Britz et al. [3] that propose to aggregate\nthe information of the input sequence into fewer vectors and perform attention with these fewer\nvectors, thus speeding up the attention computation and reducing the memory requirements. However,\nthe input aggregation is performed using a learned but ﬁxed matrix that remains constant for all\nsequences, hence signiﬁcantly limiting the expressivity of the model. Similarly, Chiu & Raffel [6]\nlimit the amount of accessible elements to the attention, by attending monotonically from the past to\nthe future. Namely, if timestep iattends to position jthen timestep i+ 1cannot attend to any of the\nearlier positions. Note that in order to speed up the attention computation, the above methods are\nlimiting the number of elements that each layer attends to. Recently, some of these approaches have\nalso been applied in the context of transformers [17].\n2.2 Non-asymptotic Improvements\nIn this section, we summarize techniques that seek to apply transformers to long sequences without\nfocusing on improving the quadratic complexity of self-attention. The most important are Adaptive\nAttention Span Transformers [28] and Transformer-XL [7].\nSukhbaatar et al. [28] propose to limit the self-attention context to the closest samples (attention span),\nin terms of relative distance with respect to the time step, thus reducing both the time and memory\nrequirements of self-attention computation. This is achieved using a masking function with learnable\nparameters that allows the network to increase the attention span if necessary. Transformer-XL [7], on\nthe other hand, seeks to increase the effective sequence length by introducing segment-level recurrent\ntraining, namely splitting the input into segments and attending jointly to the previous and the current\nsegment. The above, combined with a new relative positional encoding results in models that attend\nto more distant positions than the length of the segment used during training.\nAlthough both approaches have been proven effective, the underlying limitations of self-attention\nstill remains. Attending to an element that is N timesteps away requires O\n(\nN2)\nmemory and\ncomputation. In contrast, our model trades-off a small error in the computation of the full attention\nfor an improved linear asymptotic complexity. This makes processing long sequences possible.\n2\n2.3 Improvements in Asymptotic Complexity\nChild et al. [5] factorize the self-attention mechanism in local and strided attention. The local attention\nis computed between the Cnearest positions and the strided attention is computed between positions\nthat are Csteps away from each other. WhenCis set to\n√\nNthe total asymptotic complexity becomes\nO\n(\nN\n√\nN\n)\nboth in terms of memory and computation time. With the aforementioned factorization,\ntwo self-attention layers are required in order for any position to attend to any other position. In\naddition, the factorization is ﬁxed and data independent. This makes it intuitive for certain signals\n(e.g. images), however in most cases it is arbitrary. In contrast, our method automatically groups the\ninput queries that are similar without the need for a manually designed factorization. Moreover, in\nour model, information ﬂows always from every position to every other position.\nSet Transformers [14] compute attention between the input sequence X, of length N and a set of\ntrainable parameters, I, called inducing points to get a new sequence H, of length M <<N. The\nnew sequence H is then used to compute the attention with X to get the output representation. For a\nﬁxed M, the asympotic complexity becomes linear with respect to the sequence length. Inducing\npoints are expected to encode some global structure that is task speciﬁc. However, this introduces\nadditional model parameters for each attention layer. In contrast to this, we use clustering to project\nthe input to a ﬁxed sequence of smaller length without any increase in the number of parameters.\nMoreover, we show that not only our method has the same asymptotic complexity, it can also be used\nto speed up inference of pretrained models without additional training.\nRecently, Kitaev et al. [13] introduced Reformer. A method that groups positions based on their\nsimilarity using locality-sensitive hashing (LSH) and only computes the attention within groups.\nFor groups of ﬁxed size, the asymptotic complexity of Reformer becomes linear with respect to the\nsequence length. Note that Reformer constrains the queries and keys of self-attention to be equal. As\na result, it cannot be applied to neural machine translation, image captioning or memory networks, or\ngenerally any application with heterogenous queries and keys. In addition, as it uses hash collisions\nto form groups it can only handle a small number of bits, thus signiﬁcantly reducing the quality of the\ngrouping. Instead, our method uses clustering to group the queries, resulting in signiﬁcantly better\ngroups compared to hash collisions.\n3 Scaling Attention with Fast Clustering\nIn this section, we formalize the proposed method for approximate softmax attention. In§3.1, we ﬁrst\ndiscuss the attention mechanism in vanilla transformersand present its computational complexity.\nWe then introduce clustered attentionin §3.2 and show that for queries close in the Euclidean space,\nthe attention difference can be bounded by the distance between the queries. This property allows us\nto reduce the computational complexity by clustering the queries. Subsequently, in§3.3 we show that\nwe can further improve the approximation by ﬁrst extracting the top-kkeys with the highest attention\nper cluster and then computing the attention on these keys separately for each query that belongs to\nthe cluster. A graphical illustration of our method is provided in the supplementary material.\n3.1 Vanilla Attention\nFor any sequnce of length N, the standard attention mechanism that is used in transformers is the\ndot product attention introduced by Vaswani et al.[29]. Following standard notation, we deﬁne the\nattention matrix A∈RN×N as,\nA= softmax\n(QKT\n√Dk\n)\n, (1)\nwhere Q∈RN×Dk denotes the queries and K ∈RN×Dk denotes the keys. Note that softmax (·) is\napplied row-wise. Using the attention weights Aand the values V ∈RN×Dv , we compute the new\nvalues ˆV as follows,\nˆV = AV. (2)\nAn intuitive understanding of the attention, as described above, is that given Q,K,V we create new\nvalues ˆV as the weighted average of the old ones, where the weights are deﬁned by the attention matrix\nA. Computing equation 1 requires O\n(\nN2Dk\n)\noperations and the weighted average of equation 2\nrequires O\n(\nN2Dv\n)\n. This results in an asymptotic complexity of O\n(\nN2Dk + N2Dv\n)\n.\n3\n3.2 Clustered Attention\nInstead of computing the attention matrix for all queries, we group them into Cclusters and compute\nthe attention only for these clusters. Then, we use the same attention weights for queries that belong\nto the same cluster. As a result, the attention computation now becomes O(NCDk), where C ≪N.\nMore formally, let us deﬁne S ∈{0,1}N×C, a partitioning of the queries Qinto Cnon-overlapping\nclusters, such that, Sij = 1, if the i-th query Qi belongs to the j-th cluster and 0 otherwise. Using\nthis partitioning, we can now compute the clustered attention. First, we compute the cluster centroids\nas follows,\nQc\nj =\n∑N\ni=1 SijQi\n∑N\ni=1 Sij\n, (3)\nwhere Qc\nj is the centroid of the j-th cluster. Let us denote Qc ∈RC×Dk as the centroid matrix. Now,\nwe can compute the clustered attention as if Qc were the queries. Namely, we compute the clustered\nattention matrix Ac ∈RC×N\nAc = softmax\n(QcKT\n√Dk\n)\n(4)\nand the new values ˆVc ∈RC×Dv\nˆVc = AcV. (5)\nFinally, the value of the i-th query becomes the value of its closest centroid, namely,\nˆVi =\nC∑\nj=1\nSij ˆVc\nj . (6)\nFrom the above analysis, it is evident that we only need to compute the attention weights and the\nweighted average of the values once per cluster. Then, we can broadcast the same value to all queries\nbelonging to the same cluster. This allows us to reduce the number of dot products from N for each\nquery to Cfor each cluster, which results in an asymptotic complexity of O(NCDk) +O(CNDv).\nNote that in practice, we use multi-head attention, this means that two queries belonging to the same\ncluster can be clustered differently in another attention head. Moreover, the output of the attention\nlayer involves residual connections. This can cause two queries belonging to the same cluster to\nhave different output representations. The combined effect of residual connections and multi-head\nattention allows new clustering patterns to emerge in subsequent layers.\n3.2.1 Quality of the approximation\nFrom the above, we show that grouping queries into clusters can speed-up the self-attention computa-\ntion. However, in the previous analysis, we do not consider the effects of clustering on the attention\nweights A. To address this, we derive a bound for the approximation error. In particular, we show\nthat the difference in attention can be bounded as a function of the Euclidean distance between the\nqueries.\nProposition 1. Given two queriesQi and Qj such that∥Qi −Qj∥2 ≤ϵ,\nsoftmax\n(\nQiKT )\n−softmax\n(\nQjKT )\n2 ≤ϵ∥K∥2 , (7)\nwhere ∥K∥2 denotes the spectral norm ofK.\nProof. Given that softmax (·) has Lipschitz constant less than 1 [9],\nsoftmax\n(\nQiKT )\n−softmax\n(\nQjKT )\n2\n≤\nQiKT −QjKT \n2\n≤ϵ∥K∥2\n(8)\nProposition 1 shows that queries that are close in Euclidean space have similar attention distributions.\nAs a result, the error in the attention approximation for the i-th query assigned to the j-th cluster can\nbe bounded by its distance from the cluster centroid Qc\nj.\n4\n3.2.2 Grouping the Queries\nFrom the discussion, we have shown that given a representative set of queries, we can approximate\nthe attention with fewer computations. Thus, now the problem becomes ﬁnding this representative set\nof queries. K-Means clustering minimizes the sum of squared distances between the cluster members,\nwhich would be optimal given our analysis from §3.2.1. However, for a sequence of length N one\niteration of Lloyd’s algorithm for the K-Means optimization problem has an asymptotic complexity\nO(NCDk). To speed up the distance computations, we propose to use Locality-Sensitive Hashing\n(LSH) on the queries and then K-Means in Hamming space. In particular, we use the sign of random\nprojections [27] to hash the queries followed by K-Means clustering with hamming distance as the\nmetric. This results in an asymptotic complexity of O(NCL + CBL + NDkB), where Lis the\nnumber of Lloyd iterations and Bis the number of bits used for hashing.\n3.3 Improving clustered attention\nIn the previous section, we show that clustered attention provides a fast approximation for softmax\nattention. In this section, we discuss how this approximation can be further improved by considering\nseparately the keys with the highest attention. To intuitively understand the importance of the above,\nit sufﬁces to consider a scenario where a key with low attention for some query gets a high attention\nas approximated with the cluster centroid. This can happen when the number of clusters are too low\nor due to the convergence failure of K-Means. For the clustered attention, described in §3.2, this\nintroduces signiﬁcant error in the computed value. The variation discussed below addresses such\nlimitations.\nAfter having computed the clustered attention Ac from equation 4, we ﬁnd the kkeys with the highest\nattention for each cluster. The main idea then is to improve the attention approximation on these\ntop-k keys for each query that belongs to the cluster. To do so, we ﬁrst compute the dot product\nattention as deﬁned in equation 1 on these top-kkeys for all queries belonging to this cluster. For any\nquery, the computed attention on these top-kkeys will sum up to one. This means that it cannot be\ndirectly used to substitute the clustered-attention on these keys. To address this, before substition, we\nscale the computed attention by the total probability mass assigned by the clustered attention to these\ntop-kkeys.\nMore formally, we start by introducing T ∈{0,1}C×N , where Tji = 1if the i-th key is among the\ntop-kkeys for the j-th cluster and 0 otherwise. We can then compute the probability mass, let it be\nˆmj, of the top-kkeys for the j-th cluster, as follows\nˆmj =\nN∑\ni=1\nTjiAc\nji. (9)\nNow we formulate an improved attention matrix approximation At ∈RN×N as follows\nAt\nil =\n{ ˆmj exp(QiKT\nl )∑N\nr=1 Tjr exp(QiKTr ) if Tjl = 1\nAc\njl otherwise\n. (10)\nNote that in the above, idenotes the i-th query belonging to the j-th cluster and √Dk is ommited for\nclarity. In particular, equation 10 selects the clustered attention of equation 4 for keys that are not\namong the top-kkeys for a given cluster. For the rest, it redistributes the mass ˆmj according to the\ndot product attention of the queries with the top-kkeys. The corresponding new values, ˆV ∈RN×Dv ,\nare a simple matrix product of At with the values,\nˆV = AtV. (11)\nEquation 11 can be decomposed into clustered attention computation and two sparse dot products, one\nfor every query with the top-kkeys and one for the top-kattention weights with the corresponding\nvalues. This adds O(Nkmax (Dk,Dv)) to the asymptotic complexity of the attention approximation\nof equation 4.\n3.3.1 Quality of the approximation\nIn the following, we provide proof that improved clustered attention (eq. 10) is a direct improvement\nover the clustered attention (eq. 4), in terms of the L1 distance from the attention matrix A.\n5\nfull lsh clustered (ours) i-clustered (ours)\n10 15 20 25 30 35 40\nForward Pass Time (s)\n4\n6\n8\n10PER (%)\n9\n6\n49-100\n9-200\n9-300\n6-100\n6-200\n9-100\n9-200\n9-1\n9-4\n(a) Wall Street Journal\n30 40 50 60 70 80 90 100\nForward Pass Time (s)\n14\n16\n18WER (%)\n12\n8\n6\n12-100\n12-200\n12-300\n8-100 8-200\n12-100 12-200 (b) Switchboard\nFigure 1: We compare the achieved performance of various transformer models under an equalized\ncomputational budget. The numbers near the datapoints denote the number of layers and number of\nclusters or hashing rounds where applicable. i-clustered is consistently better than all baselines for a\ngiven computational budget both in WSJ and Switchboard datasets. The details can be found in §4.1\nand §4.2 respectively.\nProposition 2. For thei-th query belonging to thej-th cluster, the improved clustered attentionAt\ni\nand clustered attentionAc\nj relate to the full attentionAi as follows,\nAt\ni −Ai\n\n1 ≤\nAc\nj −Ai\n\n1 (12)\nDue to lack of space, the proof of the above proposition is presented in the supplementary material.\nFrom equation 12 it becomes evident that improved clustered attention will always approximate the\nfull attention better compared to clustered attention.\n4 Experiments\nIn this section, we analyze experimentally the performance of our proposed method. Initially, we\nshow that our model outperforms our baselines for a given computational budget on a real-world\nsequence to sequence task, namely automatic speech recognition on two datasets, the Wall Street\nJournal dataset (§4.1) and the Switchboard dataset (§4.2). Subsequently, in §4.3, we demonstrate\nthat our model can approximate a pretrained BERT model [16] on the GLUE [30] and SQuAD [25]\nbenchmarks with minimal loss in performance even when the number of clusters is less than one\ntenth of the sequence length. Due to lack of space, we also provide, in the supplementary material,\na thorough benchmark that showcases the linear complexity of clustered attentionand an ablation\nstudy regarding how the number of clusters scales with respect to the sequence length.\nWe compare our model with the vanilla transformers [29], which we refer to as full and the Reformer\n[13], which we refer to as lsh-X, where X denotes the rounds of hashing. We refer to clustered\nattention, introduced in §3.2, as clustered-X and to improved clustered attention, introduced in\n§3.3, as i-clustered-X, where X denotes the number of clusters. Unless mentioned otherwise we\nuse k= 32for the top-kkeys with improved clustered.\nAll experiments are conducted using NVidia GTX 1080 Ti with 11GB of memory and all models\nare implemented in PyTorch [20]. For Reformer we use a PyTorch port of the published code. Note\nthat we do not use reversible layers since it is a technique that could be applied to all methods. Our\nPyTorch code can be found at https://clustered-transformers.github.io.\n4.1 Evaluation on Wall Street Journal (WSJ)\nIn our ﬁrst experiment, we employ the Wall-Street Journal dataset [21]. The input to all transformers\nis 40-dimensional ﬁlter-bank features with ﬁxed positional embeddings. We train using Connectionist\nTemporal Classiﬁcation (CTC) [11] loss with phonemes as ground-truth labels. The approximate\naverage and maximum sequence lengths for the training inputs are 780 and 2500 respectively.\n6\nSpeed Accuracy Trade-off:We start by comparing the performance of our proposed model with\nvarious transformer variants under an equalized computational budget. To this end, we train full with\n4, 6 and 9 layers to get a range of the required computation time and achievedphone error rate(PER).\nSimilarly, we train i-clustered with 6 and 9 layers. Both models are trained with 100 and 200 clusters.\nWe also train clustered with 9 layers, and 100, 200 and 300 clusters. Finally, we train Reformer with\n9 layers, and 1 and 4 hashing rounds. We refer the reader to our supplementary for the speciﬁcs of all\ntransformer architectures as well as their training details. In ﬁgure 1a, we plot the achieved PER on\nthe validation set with respect to the required time to perform a full forward pass. Our i-clustered\nachieves lower PER than all other baselines for a given computational budget.\nApproximation Quality:To assess the approximation capabilities of our method, we train differ-\nent transformer variants on the aforementioned task and evaluate them using other self-attention\nimplementations during inference. As the Reformer requires the queries to be identical to the keys\nto evaluate its approximation ability we also train a full attention model with shared queries and\nkeys, which we refer to as shared-full. Note that both clustered attention and improved clustered\nattention can be used for approximating shared-full, simply by setting keys to be equal to queries.\nTable 1 summarizes the results. We observe that improved clustered attention (7-8 rows) achieves\nthe lowest phone error rate in every comparison. This implies that it is the best choice for approxi-\nmating pre-trained models. In addition, we also note that as the number of clusters increases, the\napproximation improves as well. Furthermore, to show that the top keys alone are not sufﬁcient for\nTrain with\nfull shared-full lsh-1 lsh-4 clustered-100 i-clustered-100\nEvaluate with\nfull 5.14 - - - 7.10 5.56\nshared-full - 6.57 25.16 41.61 - -\nlsh-1 - 71.40 10.43 13.76 - -\nlsh-4 - 64.29 9.35 9.33 - -\nclustered-100 44.88 40.86 68.06 66.43 7.06 18.83\nclustered-200 21.76 25.86 57.75 57.24 6.34 8.95\ni-clustered-100 9.29 13.22 41.65 48.20 8.80 5.95\ni-clustered-200 6.38 8.43 30.09 42.43 7.71 5.60\noracle-top 17.16 77.18 43.35 59.38 24.32 6.96\nTable 1: We report validation phone error rate (PER) on the WSJ dataset (§4.1). We train with one\nmodel and evaluate with another to assess the approximation abilities of different models. Underline\ndenotes training and testing with the same model. Improved cluster (rows 7-8) approximates the full\nand the shared-full signiﬁcantly better than all the other fast attention methods.\napproximating full, we also compare with an attention variant, that for each query only keeps the\n32 keys with the highest attention. We refer to the latter as oracle-top. We observe that oracle-top\nachieves signiﬁcantly larger phone error rate than improved clustered in all cases. This implies that\nimproved clustered attention also captures the signiﬁcant long tail of the attention distribution.\nConvergence Behaviour: In Table 2, we report the required time per epoch as well as the total\ntraining time for all transformer variants with 9 layers. For completeness, we also provide the\ncorresponding phone error rates on the test set. We observe that clustered attention is more than two\ntimes faster than full (per epoch) and achieves signiﬁcantly lower PER than both Reformer variants\n(lsh-1 and lsh-4). Improved clustered is the only method that is not only faster per epoch but also in\ntotal wall-clock time required to converge.\n4.2 Evaluation on Switchboard\nWe also evaluate our model on the Switchboard dataset [10], which is a collection of 2,400 telephone\nconversations on common topics among 543 strangers. All transformers are trained with lattice-\nfree MMI loss [24] and as inputs we use 80-dimensional ﬁlter-bank features with ﬁxed positional\nembeddings. The average input sequence length is roughly 534 and the maximum sequence length is\napproximately 3850. Details regarding the transformer architectures as well as their training details\nare provided in the supplementary.\n7\nfull lsh-1 lsh-4 clustered-100 i-clustered-100\nPER (%) 5.03 9.43 8.59 7.50 5.61\nTime/Epoch (s) 2514 1004 2320 803 1325\nConvergence Time (h) 87.99 189.64 210.09 102.15 72.14\nTable 2: We report the test PER, the time per training epoch (in seconds) and the wall-clock time\nrequired for the convergence of each model (in hours).\nSpeed Accuracy Trade-off:Similar to §4.1, we compare the performance of various transformer\nmodels given a speciﬁc computational budget. To this end, we train full with 6,8 and 12 layers.\nSimilarly, we train i-clustered with 8 and 12 layers; both with 100 and 200 clusters. Finally, we also\ntrain clustered with 12 layers, and 100,200 and 300 clusters. In ﬁgure 1b, we plot the achieved word\nerror rate (WER) in the validation set of Switchboard with respect to the required time to perform a\nfull forward pass. Our i-clustered is consistently better than full for a given computational budget.\nIn particular, for a budget of approximately 50 seconds, improved clustered achieves more than\n2 percentage points lower WER. Furthermore, we note that it is consistently better than clustered\nattention for all computational budgets.\nConvergence Behaviour:Table 3 summarizes the computational cost of training the transformer\nmodels with 12 layers in the Switchboard dataset as well as the WER in the test set. We observe that\ndue to the larger sequences in this dataset both clustered and i-clustered are faster to train per epoch\nand with respect to total required wall-clock time.\nfull clustered-100 i-clustered-100\nWER (%) 15.0 18.5 15.5\nTime/Epoch (h) 3.84 1.91 2.57\nConvergence Time (h) 228.05 132.13 127.44\nTable 3: We report the test set WER, the time per training epoch (in hours) and the wall-clock time\nrequired for the convergence of each model (in hours).\n4.3 RoBERTa Approximation\nTo highlight the ability of our model to approximate arbitrarily complicated attention distributions,\nwe evaluate our proposed method on the approximation of a ﬁne-tuned RoBERTa model [16] on the\nGLUE [30] and SQuAD [25] benchmarks. In particular, we evaluate on 10 different tasks, among\nwhich there are tasks such as question answering (SQuAD) and textual entailment (RTE), which\nexhibit arbitrary and sparse attention patterns. We refer the reader to Wang et al. [30], Rajpurkar et al.\n[25] for a detailed analysis of all tasks.\nFor the GLUE tasks, the maximum sequence length is 128 while for SQuAD, it is 384. For each task,\nwe use 25 clusters for approximation which is less than 20% and 10% of the input sequence length\nfor GLUE and SQuAD tasks respectively. In Table 4, we summarize the performance per task. We\nobserve that improved clustered performs as well as the full transformer in all tasks but SQuAD, in\nwhich it is only marginally worse. Moreover, we note that clustered performs signiﬁcantly worse in\ntasks that require more complicated attention patterns such as SQuAD and RTE. For inference time,\nfull was faster than the clustered attention variants due to short sequence lengths.\n5 Conclusions\nWe have presentedclustered attentiona method that approximates vanilla transformers with signif-\nicantly lower computational requirements. In particular, we have shown that our model can be up\nto 2×faster during training and inference with minimal loss in performance. In contrast to recent\nfast variations of transformers, we have also shown that our method can efﬁciently approximate\npre-trained models with full attention while retaining the linear asymptotic complexity.\n8\nCoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI SQuAD\nfull 0.601 0.880 0.868 0.929 0.915 0.682 0.947 0.900 0.437 0.904\nclustered-25 0.598 0.794 0.436 0.746 0.894 0.498 0.944 0.789 0.437 0.006\ni-clustered-25 0.601 0.880 0.873 0.930 0.915 0.704 0.947 0.900 0.437 0.876\nTable 4: We report the performance on GLUE and SQuAD benchmarks. Following common practice,\nwe report accuracy for all tasks except STS-B and SQuAD, where we report Pearson correlation and\nF1-score respectively. For all metrics higher is better.\nThe proposed method opens several research directions towards applying transformers on long\nsequence tasks such as music generation, scene ﬂow estimation etc. We consider masked language\nmodeling for long texts to be of particular importance, as it will allow ﬁnetuning for downstream\ntasks that need a context longer than the commonly used 512 tokens.\nBroader Impact\nThis work contributes towards the wider adoption of transformers by reducing their computational\nrequirements; thus enabling their use on embedded or otherwise resource constrained devices. In\naddition, we have shown that for long sequencesclustered attentioncan result to almost50% reduction\nin GPU training time which translates to equal reduction in CO2 emmisions and energy consumption.\nAcknowledgements\nApoorv Vyas was supported by the Swiss National Science Foundation under grant number FNS-\n30213 ”SHISSM”. Angelos Katharopoulos was supported by the Swiss National Science Foundation\nunder grant numbers FNS-30209 ”ISUL” and FNS-30224 ”CORTI”.\nReferences\n[1] Arjovsky, M., Shah, A., and Bengio, Y . Unitary evolution recurrent neural networks. In\nInternational Conference on Machine Learning, pp. 1120–1128, 2016.\n[2] Bahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\n[3] Britz, D., Guan, M. Y ., and Luong, M.-T. Efﬁcient attention using a ﬁxed-size memory\nrepresentation. arXiv preprint arXiv:1707.00110, 2017.\n[4] Chan, W., Jaitly, N., Le, Q., and Vinyals, O. Listen, attend and spell: A neural network for\nlarge vocabulary conversational speech recognition. In 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 4960–4964. IEEE, 2016.\n[5] Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[6] Chiu, C.-C. and Raffel, C. Monotonic chunkwise attention. arXiv preprint arXiv:1712.05382,\n2017.\n[7] Dai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le, Q. V ., and Salakhutdinov, R.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[8] Dong, L., Xu, S., and Xu, B. Speech-transformer: a no-recurrence sequence-to-sequence model\nfor speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 5884–5888. IEEE, 2018.\n[9] Gao, B. and Pavel, L. On the properties of the softmax function with application in game theory\nand reinforcement learning, 2017.\n9\n[10] Godfrey, J. J., Holliman, E. C., and McDaniel, J. Switchboard: Telephone speech corpus for\nresearch and development. In [Proceedings] ICASSP-92: 1992 IEEE International Conference\non Acoustics, Speech, and Signal Processing, volume 1, pp. 517–520. IEEE, 1992.\n[11] Graves, A., Fern´andez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classiﬁcation:\nLabelling unsegmented sequence data with recurrent neural networks. In Proceedings of the\n23rd International Conference on Machine Learning, 2006.\n[12] Hochreiter, S., Bengio, Y ., Frasconi, P., and Schmidhuber, J. Gradient ﬂow in recurrent nets:\nthe difﬁculty of learning long-term dependencies, 2001.\n[13] Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efﬁcient transformer. In Interna-\ntional Conference on Learning Representations, 2020. URL https://openreview.net/\nforum?id=rkgNKkHtvB.\n[14] Lee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and Teh, Y . W. Set transformer: A framework\nfor attention-based permutation-invariant neural networks. In International Conference on\nMachine Learning, 2019.\n[15] Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive\nlearning rate and beyond. In International Conference on Learning Representations, 2020. URL\nhttps://openreview.net/forum?id=rkgz2aEKDr.\n[16] Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L.,\nand Stoyanov, V . Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[17] Ma, X., Pino, J. M., Cross, J., Puzon, L., and Gu, J. Monotonic multihead attention. In\nInternational Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=Hyg96gBKPS.\n[18] Maybury, M. Advances in automatic text summarization. MIT press, 1999.\n[19] Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N.,\nSenior, A., and Kavukcuoglu, K. Wavenet: A generative model for raw audio. arXiv preprint\narXiv:1609.03499, 2016.\n[20] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,\nGimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019.\n[21] Paul, D. B. and Baker, J. M. The design for the wall street journal-based csr corpus. In\nProceedings of the Workshop on Speech and Natural Language, HLT 91, 1992.\n[22] Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M.,\nMotlicek, P., Qian, Y ., Schwarz, P., Silovsky, J., Stemmer, G., and Vesely, K. The kaldi speech\nrecognition toolkit. In IEEE Workshop on Automatic Speech Recognition and Understanding.\nIEEE Signal Processing Society, 2011.\n[23] Povey, D., Zhang, X., and Khudanpur, S. Parallel training of dnns with natural gradient and\nparameter averaging. In In International Conference on Learning Representations: Workshop\ntrack, 2015.\n[24] Povey, D., Peddinti, V ., Galvez, D., Ghahremani, P., Manohar, V ., Na, X., Wang, Y ., and\nKhudanpur, S. Purely sequence-trained neural networks for asr based on lattice-free mmi. In\nInterspeech, pp. 2751–2755, 2016.\n[25] Rajpurkar, P., Jia, R., and Liang, P. Know what you dont know: Unanswerable questions\nfor squad. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 784–789, 2018.\n[26] Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with\nrouting transformers. arXiv preprint arXiv:1908.03265, 2020.\n10\n[27] Shrivastava, A. and Li, P. Asymmetric lsh (alsh) for sublinear time maximum inner product\nsearch (mips). In Advances in Neural Information Processing Systems, pp. 2321–2329, 2014.\n[28] Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A. Adaptive attention span in transformers.\narXiv preprint arXiv:1905.07799, 2019.\n[29] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,Ł., and\nPolosukhin, I. Attention is all you need. In Advances in neural information processing systems,\npp. 5998–6008, 2017.\n[30] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,\n2019.\n[31] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., and Bengio, Y .\nShow, attend and tell: Neural image caption generation with visual attention. In International\nconference on machine learning, pp. 2048–2057, 2015.\n11\nSupplementary Material for\nFast Transformers with Clustered Attention\nA Scaling Attention with Fast Clustering\nIn this section we present graphical illustrations for the proposed clustered and i-clustered attention\nmodels in §A.1 and §A.2 respectively.\nA.1 Clustered attention\nIn ﬁgure 2, we present the steps involved in clustered attention computation for an example sequence\nwith 8 queries and the number of clusters set to 3. We ﬁrst cluster the queries Qusing the K-means\nclustering to output S which indicates the membership of queries to different clusters. We use\ndifferent colors to represent different clusters. After clustering, the centroids Qc are used to compute\nthe attention weights Ac and the new values Vc for the centroids. Finally, the values are broadcasted\nto get the new values ˆV corresponding to each query.\n\u0000\u0000\n\u0000\u0000\n\u0000\n\u0000̂ \u0000\n1\n1\n1\n1\n1\n1\n1\n1\n\u0000\n\u0000\n\u0000\u0000\n\u0000̂ \n\u0000\n=\u0000\u0000\u0000\n∑\u0000\u0000=1\u0000\u0000\u0000\u0000\u0000\n∑\u0000\u0000=1\u0000\u0000\u0000\nsoftmax( )\u0000\u0000\u0000\u0000 = \u0000\u0000̂ \u0000 \u0000\u0000\nK-Means\nClustering\n=\u0000̂ \u0000 ∑\u0000=1\n\u0000\n\u0000\u0000\u0000\u0000̂ \u0000\n\u0000\nFigure 2: Flow-chart demonstrating the compuation for clustered attention. We use different colors\nto represent the query groups and the computed centroids. The same colors are then used to show the\nattention weights Ac, new values for the centroids ˆVc, and the resulting values ˆV after broadcasting.\nFor more details refer to §A.1 or §3.2 in the main paper.\nA.2 Improved clustered attention\nIn this section, we ﬁrst describe how we can efﬁciently compute the i-clustered attention using sparse\ndot products with the top-kkeys and values. We then present the ﬂow chart demonstrating the same.\nAs discussed in the §3.3 of the main paper, the improved attention matrix approximation At\ni for the\nquery, Qi belonging to the cluster jis computed as follows:\nAt\nil =\n{ ˆmj exp(QiKT\nl )∑N\nr=1 Tjr exp(QiKTr ) if Tjl = 1\nAc\nil otherwise\n, (13)\nwhere, T ∈{0,1}C×N , stores the top-kkeys for each cluster. Tji = 1if the i-th key is among the\ntop-kkeys for the j-th cluster and 0 otherwise.\n12\nAs described in the main paper, ˆmj is the total probability mass on the top-kkeys for the j-th cluster\ngiven by:\nˆmj =\nN∑\nr=1\nTjrAc\njr. (14)\nNote that we can compute the attention weightsAt\ni on the top-kkeys by ﬁrst taking sparse dot-product\nof Qi with the top-kkeys followed by the softmax activation and rescaling with total probablity mass\nmj. For the rest of the keys, the attention weight is the clustered-attention weight Ac\ni .\nSimilarly, the new values ˆVi can be decomposed into the following two terms,\nˆVi = ˆVt\ni + ˆVb\ni , (15)\nwhere ˆVt\ni is weighted average of the values corresponding to the top- k keys with weights being\nthe improved attention on the top-kkeys. ˆVb\ni is the weighted average of the rest of the values with\nweights being the clustered attention Ac\ni . The following equations show how we compute ˆVt\ni and ˆVb\ni ,\nˆVt\ni =\nN∑\nl=1\nTjlAt\nilVl, (16)\nˆVb\ni =\nN∑\nl=1\n(1 −Tjl)Ac\nilVl, (17)\nNote that ˆVt\ni is weighted average of kvalues for each query and thus requires O(NkDv) operations.\nˆVb\ni only needs to be computed once per-cluster centroid and thus requires O(NCDv) operations.\nIn ﬁgure 3 we present the i-clustered attention computation for the same example sequence with 8\nqueries and the number of clusters and top-kkeys set to 3. The lower half of the ﬁgure shows the new\nvalue ˆVt computed by ﬁrst taking sparse dot-products with the top 3 keys to get the attention weights.\nThis is followed by taking the weighted average of the 3 correponding values. The top half of the\nﬁgure shows the ˆVb computation. This is same as clustered attention computation but with attention\nweights corresponding to top 3 keys set to 0 for Ac. The resulting values ˆV is the sum of ˆVb and ˆVt.\nB Quality of the approximation\nProposition 3. For thei-th query belonging to thej-th cluster, the improved clustered attentionAt\ni\nand clustered attentionAc\nj relate to the full attentionAi as follows,\nAt\ni −Ai\n\n1 ≤\nAc\nj −Ai\n\n1 (18)\nProof. As discussed before, the improved attention matrix approximation At\ni for the query, Qi is\ncomputed as follows:\nAt\nil =\n{ ˆmj exp(QiKT\nl )∑N\nr=1 Tjr exp(QiKTr ) if Tjl = 1\nAc\nil otherwise\n, (19)\nwhere, T ∈{0,1}C×N , stores the top-kkeys for each cluster, Tji = 1if the i-th key is among the\ntop-kkeys for the j-th cluster and 0 otherwise. ˆmj is the total probability mass on the top-kkeys for\nthe j-th cluster, computed as follows:\nˆmj =\nN∑\nr=1\nTjrAc\njr. (20)\nGiven the full attention Ai, equation 19 can be simpliﬁed to\nAt\nil =\n{\nˆmj\nmi\nAil if Tjl = 1\nAc\nil otherwise , (21)\n13\nFigure 3: Flow-chart demonstrating the compuation for i-clustered attention. The lower half of\nthe ﬁgure shows the new value ˆVt computed by sparse dot-products with the keys K and values\nV corresponding to the the top-kkeys in T. The top half of the ﬁgure shows the computation for\nˆVb which is the weighted average of the rest of the values with weights coming from the clustered\nattention Ac. The resulting values ˆV is the sum of ˆVb and ˆVt. For more details refer §A.2 or to the\n§3.3 in the main paper.\nwhere, mi is the total probability mass on the same top-kkeys for the i-th query, computed using the\ntrue attention Ai, as follows:\nmi =\n∑N\nr=1 Tjr exp\n(\nQiKT\nr\n)\n∑N\nr=1 exp (QiKTr )\n(22)\n=\nN∑\nr=1\nTjrAir. (23)\nWithout loss of generality, let us assume, Tjl = 1 ∀ l ∈ {1,...,k }and Tjl = 0 ∀ l ∈\n{k+ 1,...,N }.\nIn this case, equation 21 can be written as:\nAt\nil =\n{\nˆmj\nmi\nAil if l≤k\nAc\nil if l≥k+ 1. (24)\nThe total probability masses on the top-kkeys, mi and ˆmj can now be expressed as:\nmi =\nk∑\nr=1\nAir. (25)\nˆmj =\nk∑\nr=1\nAc\njr. (26)\nFrom equation 24 it is clear that the clustered attention, Ac\ni , and the improved clustered attention, At\ni,\nonly differ on the keys {1,...,k }. Thus, it sufﬁces to show that At\ni has lower approximation error on\nthese keys. The approximation error on the top-kkeys {1,...,k }, let it be et, between the i-clustered\nattention and the full attention is as follows:\n14\net =\nk∑\nl=1\n⏐⏐Ail −At\nil\n⏐⏐ (27)\n=\nk∑\nl=1\n⏐⏐⏐⏐Ail −Ail\nˆmj\nmi\n⏐⏐⏐⏐ (28)\n=\nk∑\nl=1\nAil\n⏐⏐⏐⏐1 − ˆmj\nmi\n⏐⏐⏐⏐ (29)\n=\n⏐⏐⏐⏐1 − ˆmj\nmi\n⏐⏐⏐⏐\nk∑\nl=1\nAil (30)\n= mi\n⏐⏐⏐⏐1 − ˆmj\nmi\n⏐⏐⏐⏐ (31)\n= |mi −ˆmj| (32)\n=\n⏐⏐⏐⏐⏐\nk∑\nl=1\nAil −Ac\njl\n⏐⏐⏐⏐⏐ (33)\n≤\nk∑\nl=1\n⏐⏐Ail −Ac\njl\n⏐⏐ (34)\nTherefore,\nAi −At\ni\n\n1 =\nk∑\nl=1\n⏐⏐Ail −At\nil\n⏐⏐+\nN∑\nl=k+1\n⏐⏐Ail −At\nil\n⏐⏐ (35)\n=\nk∑\nl=1\n⏐⏐Ail −At\nil\n⏐⏐+\nN∑\nl=k+1\n⏐⏐Ail −Ac\njl\n⏐⏐ (36)\n≤\nk∑\nl=1\n⏐⏐Ail −Ac\njl\n⏐⏐+\nN∑\nl=k+1\n⏐⏐Ail −Ac\njl\n⏐⏐ (37)\n≤∥Ai −Ac\ni ∥1 (38)\nC Experiments\nC.1 Time and Memory Benchmark\nTo measure the computational cost, we compare the memory consumption and computation time on\nartiﬁcially generated sequences of various lengths. For clustered attention we use 100 clusters, 63\nbits for the LSH, and 10 Lloyd iterations for the K-Means. For the improved clustered attention, we\nuse the same conﬁguration with k= 32. For Reformer, we evaluate on two variants using 1 and 4\nrounds of hashing. All models consist of 1 layer with 6 attention heads, embedding dimension of 64\nfor each head, and a feed-forward dimension of 1536.\nIn this experiment, we measure the required memory and GPU time per single sequence elementto\nperform a forward/backward pass for the various self-attention models. Figure 4 illustrates how these\nmetrics evolve as the sequence length increases from N = 29 to N = 215. For a fair comparison,\nwe use the maximum possible batch size for each method and we divide the computational cost and\nmemory with the number of samples in each batch and the sequence length.\nWe note that, in contrast to all other methods, vanilla transformer scales quadratically with respect to\nthe sequence length and does not ﬁt in GPU memory for sequences longer than 213 elements. All\n15\nother methods scale linearly. Clustered attention becomes faster than the vanilla transformer for\nsequences with 1000 elements or more, while improved clustered attention surpasses it for sequences\nwith 2000 elements. Note that with respect to per sample memory, both clustered and improved\nclustered attention perform better than all other methods. This can be explained by the fact that\nour method does not require storing intermediate results to compute the gradients from multiple\nhashing rounds as Reformer does. It can be seen, that lsh- 1 is faster than the improved clustered\nclustered attention, however, as also mentioned by [13] Reformer requires multiple hashing rounds to\ngeneralize.\nfull lsh-1 lsh-4 clustered-100 (ours) i-clustered-100 (ours)\n29 210 211 212 213 214 215\nSequence Length\n10−2\nPer Element Computation Time\n(a) Per Element Time\n29 210 211 212 213 214 215\nSequence Length\n10−1\n100\nPer Element Memory\n (b) Per Element Memory\nFigure 4: Per element GPU time and memory consumption for a forward/backward pass. All models,\nexcept full, scale linearly with respect to the sequence length since they have constant time and\nmemory per element. Detailed analysis can be found in §C.1.\nC.2 Ablation on clusters and sequence length\nFollowing [13], we introduce a synthetic task to analyze the relationship between the number of\nclusters and sequence length. In our task, the transformer models need to copy some symbols that are\nmasked out from either the ﬁrst or second half of the sequence. In particular, we generate a random\nsequence of tokens and we prepend a unique separator token, let it be 0. The sequence is then copied\nto get a target of the form 0w0w, where w ∈{1,...,C }L, C is the number of possible symbols\nand Lis the sequence length. To generate the input, we replace some symbols from the ﬁrst half of\nAccuracy with respect to clusters and hashing rounds\n64 128 256 512\nSequence length\n100\n60\n30\n15\n# clusters\n1.0 1.0 1.0 1.0\n1.0 1.0 1.0 1.0\n1.0 1.0 1.0 1.0\n1.0 1.0 1.0 1.0\n(a) Improved clustered\n64 128 256 512\nSequence length\n100\n60\n30\n15\n# clusters\n1.0 1.0 1.0 0.96\n1.0 1.0 1.0 0.74\n1.0 1.0 0.44 0.19\n1.0 0.7 0.27 0.19 (b) Clustered\n64 128 256 512\nSequence length\n16\n8\n4\n1\n# hashing rounds\n1.0 1.0 1.0 1.0\n1.0 1.0 1.0 0.4\n1.0 1.0 1.0 0.4\n1.0 1.0 1.0 0.3 (c) Reformer\nFigure 5: The heatmaps depict the achieved accuracy on an artiﬁcial copy task (§C.2) as the sequence\nlength, the number of clusters and the number of hashing rounds varies. Improved clustered (5a) is\nthe only fast transformer variant that can solve the task perfectly for any sequence length and number\nof clusters combination.\n16\nthe sequence and some different symbols from the second half, such that the target sequence can be\nreconstructed from the input. An example of an input output pair with L= 4can be seen in ﬁgure 6.\nNote that to solve this task, transformers simply need to learn to attend to the corresponding tokens in\nthe two identical halves of the sequence.\nInput 0 4 M 2 2 0 4 5 M 2\nOutput 0 4 5 2 2 0 4 5 2 2\nFigure 6: Example of an input and output pair for the masked copy task. M denotes the masked out\ntokens.\nWe set the sequence length L to one of {31,63,127,255}which means the input length varies\nbetween N = 26 and N = 29. For each sequence, we sample tokens uniformly from {1,..., 10}and\nrandomly mask out 20% of the tokens. To analyze the impact of number of clusters on performance,\nwe train full transformer as well as clustered variants with different number of clusters and Reformer\nwith different number of hashing rounds.\nAll transformer variants consist of 4 layers, 6 attention heads, embedding dimension of 32 for each\nhead, and feed-forward dimension of 768. For both clustered and improved clustered attention, we\nset the number of bits for LSH to 63 and the number of Lloyd iterations for the K-Means to 10. Both\nclustered and improved clustered attention are trained with 15, 30, 60 and 100 clusters. We also train\nReformer with 1, 4, 8 and 16 hashing rounds. Finally, all models are trained using R-Adam optimizer\n[15] with a learning rate of 0.0002, batch size of 32 for 5000 iterations.\nIn ﬁgure 5, we illustrate the results of this experiment as heatmaps depicting the achieved accuracy\nfor a given combination of number of clusters and sequence length for clustered transformers and\nnumber of hashing rounds and sequence length for Reformer. Note that the vanilla transformer solves\nthe task perfectly for all sequence lengths. We observe that both clustered (Fig. 5b) and Reformer\n(Fig. 5c) require more clusters or more rounds as the sequence length increases. However, improved\nclustered achieves the same performance as vanilla transformers, namely perfect accuracy, for every\nnumber of clusters and sequence length combination. This result increases our conﬁdence that the\nrequired number of clusters for our method is not a function of the sequence length but of the task at\nhand.\nC.3 Automatic Speech Recognition\nIn this section, we present the details for the ASR experiments such as transformer architecture,\noptimizer and learning rate schedule. As mentioned in the main paper, fori-clustered, unless speciﬁed,\nkis set to 32. Furthermore, all transformers have 6 heads with an embedding dimension of 32 on\neach head and feed-forward dimension of 768. Other architectural details speciﬁc to experiments are\ndescribed later.\nC.3.1 Wall Street Journal\nConvergence Behaviour:\nFor this experiment, we train transformer with full, clustered and Reformer attention variants. All\nmodels consist of 9 layers. For Reformer, we train two variants with 1 and 4 rounds of hashing with\nchunk size ﬁxed to 32 as suggested. For clustered and improved clustered attention we set the number\nof clusters to 100. We also set the number of Lloyd iterations for K-Means to 10 and the bits for LSH\nto 63. All models are trained to convergence using the R-Adam optimizer [15] with a learning rate of\n0.0001, max gradient norm set to 10.0 and and weight decay of 0.01. The learning rate is dropped\nwhen the validation loss plateaus. For each model we select the largest batch size that ﬁts the GPU.\nThe full attention model was trained with a batch size of 2 while the clustered variants: clustered and\ni-clustered could ﬁt batch sizes of 14 and 10 respectively. For Reformer variants: lsh-1 and lsh-4,\nbatch sizes of 8 and 6 were used.\nIn ﬁgure 7a, we show the training loss convergence for different transformer variants. It can be\nseen that i-clustered has a much faster convergence than the clustered attention. This shows that the\nimproved clustered attention indeed approximates the full attention better. More importantly, only\n17\n0 50 100 150 200\nWall-Clock Time (in hours)\n101\n102\nTraining Loss\nfull\nclustered\nimproved-clustered\nlsh-1\nlsh-4\n(a) Wall Street Journal\n0 50 100 150 200\nWall-Clock Time (in hours)\n2 × 10−1\n3 × 10−1\n4 × 10−1\nValidation Loss\nfull\nclustered\nimproved-clustered (b) Switchboard\nFigure 7: We show training/validation loss convergence for different transformer variants. Only\ni-clustered has a faster or comparable wall-clock convergence to full attention. Both the clustered\nvariants are have a signiﬁcantly better convergence than both lsh-1 and lsh-4. Note that due to a\nsmaller batch size full makes many more updates than all other transformer variants. More details\ncan be found in §C.3.1 and §C.3.2.\nthe i-clustered attention has a comparable wall-clock convergence. Given that full has a much smaller\nbatch size, it make many more updates per-epoch. We think that a slightly smaller batchsize with\nmore updates would have been a better choice for the clustered transformers w.r.t. the wall-clock\nconvergence. This is reﬂected in the Switchboard experiments where the batchsizes for clustered\nvariants were smaller due to more layers. Finally, as can be seen from the wall-clock convergence,\nthe clustered transformers signiﬁcantly outperform the Reformer variants.\nSpeed-Accuracy Tradeoff:\nAs described in the main paper, for this task we additionally train full with 4 and 6 layers. Similary,\nwe train clustered with 9 layers, and 200 and 300 clusters. We also train an i-clustered model with 9\nlayer and 200 clusters, and smaller models with 6 layers, and 100 and 200 clusters.\nFor clustered and i-clustered variants with 9 layers, we ﬁnetuned the previously described models\ntrained with 100 clusters. We ﬁnetuned for 15 epochs with a learning rate of 0.00001. We train full\nwith 4 and 6 layers to convergence in a similar fashion to the full with 9 layers described previously.\nFinally, for i-clustered, we ﬁrst trained model with 6 layers and 100 clusters using the training strategy\nused for 9 layers and 100 clusters. We then ﬁnetuned this model for 15 epochs using 200 clusters and\na learning rate of 0.00001.\nC.3.2 Switchboard\nConvergence Behaviour:\nFor this experiment, we train transformer with full and clustered attention variants. All models consist\nof 12 layers. For clustered and improved clustered attention we set the number of clusters to 100. We\nalso set the number of Lloyd iterations for K-Means to 10 and the bits for LSH to 63.\nFollowing common practice for ﬂat-start lattice-free MMI training, we train over multiple gpus with\nweight averaging for synchronization as described in [23]. Specﬁcally, we modify the e2e training\nrecipe for the Wall Street Journalin Kaldi [22] with the following two key differences: ﬁrst, the\nacoustic model training is done in PyTorch and second, we use R-Adam optimizer instead on natural\nstochastic gradient descent.\nAll models are trained using the R-Adam optimizer with a learning rate of 0.0002, max gradient\nnorm set to 10.0 and and weight decay of 0.01. The learning rate is dropped when the validation\nloss plateaus. We use the word error rate (WER) on the validation set for early stopping and model\nselection. The full attention model is trained with a batch size of 2 while the clustered variants:\nclustered and i-clustered are trained with a batch size of 6.\nIn ﬁgure 7b, we show the training loss convergence for different transformer variants. It can be seen\nthat i-clustered has the fastest convergence for this setup. Note that the overall training time for\nclustered attention is still less than that of full as it starts to overﬁt early on the validation set WER.\n18\nSpeed-Accuracy Tradeoff:\nFor this task we additionally train full with 6 and 8 layers. Similary, we train clustered with 12 layers,\nand 200 and 300 clusters. We also train i-clustered with 12 layer and 200 clusters, and smaller models\nwith 8 layers, and 100 and 200 clusters.\nFor clustered and i-clustered variants with 12 layers, we ﬁnetuned the previously described models\ntrained with 100 clusters. We ﬁnetuned for 5 epochs with a learning rate of 0.00001. Once again, full\nwith 6 and 8 layers were trained to convergence similar to full with 12 layers described previously.\nFinally, for i-clustered with 8 layers, we ﬁrst train a model with 100 clusters using the training\nstrategy used for 12 layers and 100 clusters. We then ﬁnetuned this model for 5 epochs using 200\nclusters and a learning rate of 0.00001.\nC.4 RoBERTa Approximation\nIn this section we provide a qualitative comparison between the full attention, and the clustered\nattention variants clustered and i-clustered used for approximation. As described in main paper,\nwe use 25 clusters for both attention variants. In Figure 8 we show the attention distribution for\nthe question tokens for a randomly selected question-context tuple from the SQuAD dataset. For\neach token in the question we show the attention distribution over the input sequence formed by\nconcatenating question and context tokens with CLS and SEP tokens appended. It can be seen\nthat with only few clusters, improved clustered approximates the full attention very closely even\nwhen the attention distribution has complicated and sparse patterns. In contrast, clustered attention\nfails to capture such attention distribution during approximation. Moreover, it can further be seen\nthat for almost all question tokens, both full and improved clustered have the same tokens with the\nhighest attention weights. This further strengthens our believe that improved clustered attention can\napproximate a wide range of complicated attention patterns.\n19\nManning ﬁnished the year with a career-low 67.9 passer rating, throwing for 2,249 yards and nine\ntouchdowns, with 17 interceptions. In contrast, Osweiler threw for 1,967 yards, 10 touchdowns and six\ninterceptions for a rating of 86.4. Veteran receiver Demaryius Thomas led the team with 105 receptions\nfor 1,304 yards and six touchdowns, while Emmanuel Sanders caught 76 passes for 1,135 yards and six\nscores, while adding another 106 yards returning punts. Tight end Owen Daniels was also a big element\nof the passing game with 46 receptions for 517 yards. Running back C. J. Anderson was the team’s\nleading rusher 863 yards and seven touchdowns, while also catching 25 passes for 183 yards. Running\nback Ronnie Hillman also made a big impact with 720 yards, ﬁve touchdowns, 24 receptions, and a 4.7\nyards per carry average. Overall, the offense ranked 19th in scoring with 355 points and did not have any\nPro Bowl selections.\n(a) context\nInput tokens\nWho\nregistered\nthe\nmost\nreceptions\non\nthe\nBroncos\n?\nQuestion tokens\nCLS\nCLS registered\nCLS\nCLS most\nCLS receptions\nCLS on\nCLS the\nCLS Broncos els\nCLS ?\n(b) full\nInput tokens\nWho\nregistered\nthe\nmost\nreceptions\non\nthe\nBroncos\n?\nQuestion tokens\nCLS\nCLS registered\nCLS\nCLS most\nCLS receptions\nCLS on\nCLS the the\nCLS els\nCLS\n(c) improved-clustered\nInput tokens\nWho\nregistered\nthe\nmost\nreceptions\non\nthe\nBroncos\n?\nQuestion tokens\nCLS\nCLS\nCLS\nCLS\nCLS yards yards\nCLS\nCLS\nCLS\nCLS\n(d) clustered\nFigure 8: Attention matrices for question-context tuples forfull attention, and clustered and i-clustered\nattention used for approximation. 8a shows the the context for the question with answer higlighted in\nred. 8b shows the attention distribtution for full, 8c and 8d show the approximation using i-clustered\nand clustered respectively. Note that i-clustered has attention patterns very similar to full while\nclustered shows qualitatively different attention patterns. For each question token, we also present the\ntokens with highest attention above a threshold on the right axis. For more information refer to §C.4.\n20"
}