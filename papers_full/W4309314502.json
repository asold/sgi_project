{
  "title": "Indonesian Hoax News Classification with Multilingual Transformer Model and BERTopic",
  "url": "https://openalex.org/W4309314502",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4309315175",
      "name": "Leonnardo Benjamin Hutama",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A1877050847",
      "name": "Derwin Suhartono",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A4309315175",
      "name": "Leonnardo Benjamin Hutama",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A1877050847",
      "name": "Derwin Suhartono",
      "affiliations": [
        "Binus University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964149366",
    "https://openalex.org/W2620789444",
    "https://openalex.org/W3131684216",
    "https://openalex.org/W2894422389",
    "https://openalex.org/W3080295236",
    "https://openalex.org/W2899755469",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4323063355",
    "https://openalex.org/W2977526300",
    "https://openalex.org/W3082277326",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W3158012322",
    "https://openalex.org/W3179463712",
    "https://openalex.org/W3118868930",
    "https://openalex.org/W3208249853",
    "https://openalex.org/W3179199540",
    "https://openalex.org/W3204704911",
    "https://openalex.org/W3184872637",
    "https://openalex.org/W3181842539",
    "https://openalex.org/W3202034272",
    "https://openalex.org/W2810796800",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4229011615",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W3018807723",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Technology and information growth make all internet users can play a role in disseminating information, including hoax news. One way that can be done to avoid hoax news is to look for sources of information, but valid news is not always perceived as 'true' by individuals because human judgments can lead to bias. Several studies on automatic hoax news classification have been carried out using various deep learning approaches such as the pre-trained multilingual transformer model. This study focuses on classifying Indonesian hoax news using the pre-trained transformer multilingual model (XLM-R and mBERT) combined with a BERTopic model as a topic distribution model. The result shows that the proposed method outperforms the baseline model in classifying fake news in the low-resource language (Indonesian) with accuracy, precision, recall, and F1 results of 0.9051, 0.9515, 0.8233, and 0.8828 respectively.",
  "full_text": "https://doi.org/10.31449/inf.v46i8.4336 Informatica 46 (2022) 81–90 81 \nIndonesian Hoax News Classification with Multilingual Transformer \nModel and BERTopic \nLeonnardo B. Hutama1*, Derwin Suhartono2 \nE-mail: 1leonnardo.hutama@binus.ac.id, 2dsuhartono@binus.edu \n* Corresponding author \n1Computer Science Department, BINUS Graduate Program, Master of Computer Science, Bina Nusantara University, \nJakarta, Indonesia \n2Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia  \nKeywords: Hoax News Classification, Topic Distribution, XLM-R, mBERT, BERTopic, Indonesian Language \nReceived: August 10, 2022 \nTechnology and information growth enable internet users to play a role in disseminating information,  \nincluding hoax news. One way that to avoid hoax news is to look for sources of information, \nbut valid news is not always perceived as 'true' by individuals because human judgments can lead to bias. \nSeveral studies on automatic hoax news classification have been carried out using various deep learning \napproaches such as the pre -trained multilingual transformer model. This study focuses on classifying \nIndonesian hoax news using the pre -trained transformer multilingual model (XLM -R and mBERT) \ncombined with a BERTopic model as a topic distribution model. The result shows that the propo sed \nmethod outperforms the baseline model in classifying fake news in the low -resource language \n(Indonesian) with accuracy, precision, recall, and F1 results of 0.9051, 0.9515, 0.8233, and 0.8828 \nrespectively.  \n \nPovzetek: Raziskava se ukvarja z identifikacijo lažnih novic v Indoneziji s pomočjo modelov XLM -R and \nmBERT.\n1 Introduction\nTechnology and information growth have make it easier \nfor users to convey and consume information in recent \nyears. Today, all internet users can play a role in \ndisseminating information. However, the widely spread \ninformation is not all true and reliable, including h oax \nnews. According to the expert  [1], fake news is \ninformation that is intentionally intended to mislead \npeople with a specific purpose. Meanwhile, according to \n[2] hoax news is misleading that imitates original content \nbut has a different purpose. Th ere are still many people \nwho are victims of hoax news, including Indonesians. The \nMastel Survey [3] about the Indonesia hoax outbreak, \nconducted on 941 respondents stated that as many as \n34.60% of respondents received hoax news every day, and \nas many as 70.7% of hoaxes were received in text form \nand the largest hoax news distribution channel was \nthrough social media and websites. The survey also stated \nthat 63.30% of respondents believed that hoax news was \nnot a hoax because they got the news from trusted  people \nand 24.5% of respondents believed hoax news because of \nconvincing sentences. \nHoax news also has a negative impact, such as \ncausing anxiety, triggering public panic, and can lead to \nmanipulation and fraud that can bring down humans [4]. \nOne way th at can be done to avoid hoax news is to look \nfor credibility or sources of information. The credibility of \ninformation is very important to avoid the risk of \nconsuming hoax news [5]. However, valid news is not \nalways perceived as 'true' by individuals be cause human \njudgments on the credibility of information can be \ninfluenced by the opinion of the individual and lead to \nbias. This bias can be reduced by implementing automated \nfake news detectors [6]. \nSeveral studies on automatic hoax detection have \nbeen carried out using various deep learning approaches. \nFor example, by using Logistic Regression and Support \nVector Machine (SVM) in research  [7] and using a \ncombination of CNN and LSTM in research (e.g. [6], [8], \n[9]) who managed to get an accuracy value of around 44 -\n98%. These approaches generally use traditional word \nembedding methods such as word2vec which have \nlimitations in overcoming the polysemy that occurs when \nthe same word has different meanings. To overcome this, \nthe researcher [10] created a model, named transformer, \nwhich uses a self -attention mechanism so that it can \ncalculate a better representation of a word in a sentence. \nThe pre-trained transformer models were provided by [11] \nin the library namely  Huggingface. Transformer models \nsuch as BERT, ALBERT, and XLNet have been widely \nused in making automatic hoax detectors and have been \nshown to have better performance than traditional \napproaches (e.g. [12], [13], [14]). The transformer also has \npre-trained multilingual models, such as mBERT, XLM, \nand XLM -R, which can be used for many languages. \nCompared to other multilingual transformer models, the \nXLM-R model proved to have the best performance \nbecause it was trained with a 2.5 TB dataset size whic h is \nlarger than other multilingual models [15]. However, the \nmodel performs worse when implemented in a low -\nresource language, such as Indonesian, than in a high -\n82 Informatica 46 (2022) 81–90 L. B. Hutama  et al. \nresource language. This happens because it is difficult to \nfind data in low-resource languages. \nIn addition to using a pre -trained transformer model, \nthe performance of automatic hoax news detectors can \nalso be improved by using a topic modeling approach such \nas Latent Dirichlet Allocation (LDA). This is done by \nadding the results of the topic d istribution from the LDA \nas input for the classification model [16]. The LDA, which \nuses a probability approach, is one of the most popular \nmodels used for topic modeling. However, other models \nwith a transformer approach, such as BERTopic, have \nbeen sho wn to have better performance than LDA [17]. \nBesides having better performance, BERTopic also \nsupports multilingual text. However, few still use topic \nmodeling to maximize the performance of automatic hoax \nnews detectors with low-resource languages. \nBased on previous research, research using a \nmultilingual model is proven to be able to overcome the \nproblem of low -resource language. In addition, topic \nmodeling can also improve performance on news \nclassification. However, previous studies used this method \nonly for single -language models, and its performance, \nwhen applied to low-resource languages, is still unknown. \nAddressing the aforementioned issues, this paper proposes \na predictive model architecture using a multilingual \ntransformer pre-trained model and a topic modeling model \nto detect hoax news automatically in a low -resource \nlanguage (Indonesian). The model consists of a \ncombination of a pre -trained multilingual transformer \nmodel to obtain contextual representation results and \nBERTopic in conducting top ic distribution. In summary, \nthe contribution of this work can be written as follows: \n- We proposed a deep learning architecture with a pre -\ntrained transformer multilingual model with \nadditional topic word representation from topic \nmodeling as a feature extraction method for the hoax \nnews classification system in the Indonesian \nlanguage. \n- We evaluate its performance by comparing it with the \nprevious research algorithm which gave the best \nperformance in the classification of hoax news in the \nIndonesian language. \n- We evaluate the performance of the topic distribution \nmethod by comparing the model using and without \nusing the topic distribution method. \n- We show that our method produces better \nperformance than previous studies on the \nclassification of hoax news in the  Indonesian \nlanguage. \n2 Related work \nResearch on automatic fake news detection using the \nTransformer model has been carried out by several \nresearchers. For example, research conducted by [12] and \n[13] used the BERT transformer model to detect hoaxes \non English news datasets, namely LIAR and the FNC -1 \ndatasets sequentially. The research shows that the pre -\ntrained transformer model gets 15 -20% better accuracy \nthan the traditional CNN and LSTM models in classifying \nhoax news. Researchers [18], [19]  also re searched the \nclassification of hoax news using the ConstraintAI'21 \nEnglish news dataset with an ensemble model consisting \nof three transformer models (BERT, ALBERT, and \nXLNet). The ensemble method managed to get 98% \naccuracy in classifying hoax news. Although the accuracy \nvalue is good, this method cannot necessarily be applied \nto low -resource language news datasets, such as \nIndonesian, because the model used is only trained with \nan English corpus. This is proven by research conducted \nby [19] to detect ho ax news in English and several low -\nresource languages (Hindi, Swahili, Vietnamese, and \nIndonesian). This study proves that the performance of the \nEnglish news dataset produces an accuracy value of 84%, \nwhile for news with low-resource languages the accuracy \nvalue is 5% lower at 79%. This shows that the transformer \nmodel applied to low -resource language news produces \nless optimal performance due to the limited corpus \navailable in low-resource languages. \nThe performance of the hoax news classification \nsystem is determined by the size and language of the \ndataset used when the model is trained. However, for low-\nresource languages, such datasets are still lacking. This \ncan be overcome by using multilingual transformer \nmodels, such as mBERT because these models are trained \nin various languages. For example, research was \nconducted by [20] in detecting fake news on Arabic tweets \nusing mBERT. The study compared the mBERT model \nwith the Arabic-based BERT model (AraBERT) with and \nwithout fine -tuning. In this study, fi ne-tuned mBERT \nmanaged to get an F1 score above 0.92, which was better \nthan the other baseline models (AraBERT, Distilbert) on \nall tasks. In addition to mBERT, research by [15] has \nsucceeded in proposing a transformer -based multilingual \nmodel named XLM-RoBERTa (XLM-R) which has been \npre-trained in 100 languages and with a 2.5TB dataset \nsize. This model outperformed other multilingual models \n(mBERT and XLM) in performing classification, \nsequence labeling, and question answering. This research \nalso applied the XLM-R model to a low-resource language \nand managed to improve performance by 15.7% in \nSwahili and 11.4% in Urdu. \nResearch using the XLM -R model for low -resource \nlanguages has also been carried out by several other \nresearchers, such as using the XLM -R model for order \nclassification of low -resource languages (Polish) which \nwas conducted by [21]. This research has suc ceeded in \nproving that the XLM -R model has higher precision, \nrecall, and F1 values compared to other multilingual \nmodels (mBERT and HerBERT) in performing sequence \nlabeling. Similar studies were also conducted by [22] \nusing the XLM-R model to conduct sentiment analysis on \nlow-resource language (Korean) movie reviews. The \nstudy compared the XLM -R model with the mBERT and \nseveral pre-trained Korean models (KoBERT, KorBERT, \nand KR -BERT). In this study, the XLM -R model, \ncombined with prune and Bi-GRU, managed to get a value \nof precision, accuracy, and recall that was 3.63% better \nthan other models. \nThe XLM-R model is also used by several researchers \nsuch as [23] and [24] in building a classification system \nfor hoax news. The study was conducted by [23] using the \nIndonesian Hoax News Classification with Multilingual… Informatica 46 (2022) 81–90 83 \nXLM-R model to classify hoax news on English and \nChinese tweets. In this study, the XLM-R model was fine-\ntuned and managed to get the best average accuracy \ncompared to other traditional algorithms (Naïve Bayes, \nSVM, C4.5, Random Forest, CNN, BiLST M, C-LSTM) \nwith a value of 99% with only use raw text without being \ntranslated. In addition, despite having a large model size, \nthe training time required by the XLM -R model is also \ncomparable to other algorithms. In the research [24], the \nXLM-R model is  used to classify hoax news in Spanish. \nThis study compared XLM -R combined with CNN as a \nfeature extractor with other pre -trained transformer \nmodels, namely BETO and XLM -R without CNN. The \nbest results were obtained by the XLM-R model combined \nwith CNN with an accuracy score of 0.96 followed by the \nXLM-R model without CNN with an accuracy score of \n0.95 and BETO with an accuracy score of 0.93. These two \nstudies prove that the XLM-R model can be used in a hoax \nnews classification system in languages other tha n \nEnglish, namely Chinese and Spanish. However, there has \nbeen no research using the XLM-R model for Indonesian, \nso its performance, when applied to Indonesian, is still \nunknown due to differences in the corpus. \nThe language model plays an important role i n \ndetermining the performance of the hoax news \nclassification system. However, the performance of the \nhoax news classification system can also be improved by \nadding a feature extraction method such as Latent \nDirichlet Allocation (LDA). For example, the use of LDA \nas a feature extractor is carried out by [25] in classifying \nfake reviews using Logistic Regression and Multi -Layer \nPerceptron approaches. The study managed to get an \naccuracy score of 81% better than without using LDA. \nResearch using LDA was als o carried out by [16] who \ncombined the pre -trained transformer model XLNet and \nLDA as a feature extraction method in classifying hoax \nnews from English social media articles. The research was \nconducted by combining the results of the topic \ndistribution f rom LDA and the results of contextual \nrepresentation from XLNet. The LDA method has \nsucceeded in increasing the performance of the XLNet \nmodel with an accuracy value of 96% better than the \naccuracy value without using the LDA of 94%. Besides \nLDA, there are  several other topic distribution models \nsuch as BERTopic, a topic modeling technique based on \nBERT and TF -IDF. Research conducted by [17] in \nconducting topic modeling of Arabic news with \nBERTopic succeeded in proving that BERTopic has better \nperformance than the Latent Dirichlet Allocation (LDA) \ntopic modeling technique which uses a probability \napproach, and Non-Negative Matrix Factorization (NMF) \nwhich uses the matrix factorization method. These results \nare based on the Normalized Pointwise Mutual \nInformation (NPMI) value where BERTopic gets a \npositive NPMI value while LDA and NMF get a negative \nNPMI value. Despite the success in improving \nperformance, only a few researchers use feature extraction \ntechniques with topic distribution to improve the \nperformance of hoax news classification systems with \nlow-resource languages. \nThe summary of the related works can be seen in table \n1. \nTable 1: Related Works Summary \nNo Topic Result \n1 Hoax News Detection on \nEnglish News Dataset \nusing BERT [12][13] \nBERT model gets \n15-20% better \naccuracy than CNN \nand LSTM models. \n2 Hoax News Detection on \nEnglish News Dataset \nusing the Ensemble \nmodel (BERT, \nALBERT, XL-Net) [18] \nThe ensemble model \ngets 98% accuracy \nin classifying hoax \nnews.  \n3 Hoax News Detection on \nLow-Resource Language \nDataset (Hindi, Swahili, \nVietnamese, Indonesian) \nusing Transformer \nMultilingual model [19] \nThe multilingual \ntransformer accuracy \nis 5% lower in low-\nresource languages \n(79%) than in \nEnglish language \n(84%). \n4 Detecting Hoax News on \nArabic Tweet using \nmBERT and AraBERT \n[20] \nFine-tuned mBERT \n(multilingual model) \ngets 0.92 F1-score \nand it’s better than \nAraBERT (baseline \nmodel). \n5 Sequence Labeling on \nLow-Resource Language \n(Polish) Order using \nXLM-R [21] \nXLM-R model gets \nhigher precision, \nrecall, and F1 \ncompared to \nmBERT and \nHerBERT. \n6 Sentiment Analysis on \nLow-Resource Language \n(Korean) Movie Review \nusing XLM-R [22] \nXLM-R model with \nprune and Bi-GRU \nmanaged to get \n3.63% better \nprecision, accuracy, \nand recall than other \nmodels. \n7 Hoax News \nClassification on English \nand Chinese Tweets \nusing XLM-R [23] \nXLM-R gets the best \naccuracy value \n(99%) compared to \nNaïve Bayes, SVM, \nC4.5, Random \nForest, CNN, \nBiLSTM, C-LSTM. \n8 Hoax News Detection on \nSpanish Language using \nXLM-R combined with \nCNN [24] \nXLM-R combined \nwith CNN get \naccuracy score of \n96% and it’s better \nthan BETO model \nwith an accuracy of \n93%. \n9 Hoax News Detection \nusing Logistic \nRegression (LR) \ncombined with Latent \nDirichlet Allocation \n(LDA) [25] \nLogistic Regression \ncombined with LDA \ngets better \nperformance than \nwithout using LDA. \n84 Informatica 46 (2022) 81–90 L. B. Hutama  et al. \n10 Hoax News Detection on \nEnglish Social Media \nArticles using XLNet \nand LDA [26] \nXLNet combined \nwith LDA gets better \naccuracy (94%) than \nwithout using LDA \n(94%). \n11 Topic Modeling on \nArabic News using \nBERTopic, Latent \nDirichlet Allocation \n(LDA), and Non-\nNegative Matrix \nFactorization (NMF) \n[17] \nBERTopic gets a \npositive Normalized \nPointwise Mutual \nInformation (NPMI) \nvalue. LDA and \nNMF gets a negative \nNPMI value. \n \nBased on previous research, the multilingual model \nhas proven to be applicable to low-resource languages and \nthe use of topic distribution models such as LDA can \nimprove the performance of the classification model. \nHowever, there are still few studies that  use the \nmultilingual model for Indonesian news and the use of \ntopic distribution models can be further improved by using \na better model, such as BERTopic, compared to traditional \ntopic distribution models, such as LDA. Therefore, this \nstudy focuses on the  classification of Indonesian hoax \nnews using the multilingual transformer model (mBERT \nand XLM -R) combined with the BERTopic topic \ndistribution model to improve the performance of the \nclassification model in the low-resource language used.  \n3 Fundamental theories \n3.1 Multilingual transformer model \nThe transformer is a model architecture that focuses on \nself-attention mechanisms, without using convoluted \nconvolutions like RNN [10]. Transformers allow for more \nsignificant parallelization and the use of self-attention can \nresult in more interpretable models. The transformer \nconsists of 2 main parts, namely an encoder, and a \ndecoder. The encoder is a collection of several layers, each \nof which has 2 sub -layers, namely a multi -head self -\nattention mechanism, and a position-wise fully connected \nfeed-forward network, where there is a residual \nconnection concept to maintain information based on the \nposition of each. part to serve as input for the entire \nnetwork. The decoder has a layer similar to the encoder \nwith the add ition of one section to process multi -head \nattention to the output of the encoder. \nTransformer models are generally trained using \nEnglish datasets so that they have good performance when \nused in several NLP tasks with English data. However, the \nTransformer model can also be used in NLP tasks other \nthan English dataset by using the monolingual transformer \nmodel, which has been trained in a specific language, or \ncan use the multilingual transformer model, which has \nbeen trained in several languages. This study will focus on \ntwo multilingual transformers models in classifying \nIndonesian hoax news, namely XLM-R and mBERT. \n3.1.1 XLM-R \nXLM-R (XLM -RoBERTa) is a multi -language model \ndeveloped from the transformer architecture which has an \narchitecture similar to the XLM m odel. The difference \nbetween the XLM-R model and the XLM model lies in the \npurpose of the training where XLM focuses on the \nTranslation Language Model (TLM) while XLM -R has a \ntraining focus similar to the RoBERTa model, which \nfocuses on the Masked Language  Model (MLM). The  \nXLM-R model uses Byte Pair Encoding (BPE), where \nmost common XLM-R model uses Byte Pair Encoding \n(BPE), where the most common consecutive pair of data \nbytes is replaced with bytes that do not appear in the data, \nthereby improving vocabula ry relationships between \ndifferent languages. In addition to BPE, the XLM -R \nmodel is also trained to use the same words in different \nlanguages, thus enabling the XLM-R model to understand \nthe context of one language from another. This causes the  \n \nXLM-R mo del to be used to increase the use of low -\nresource languages by utilizing other languages that have \nhigh resources. The most prominent development of the \nXLM-R model lies in the size of the dataset used where \nXLM-R is trained using 2.5TB of Common Crawl data in \n100 languages, making the XLM -R model superior to \nprevious multi-language models such as BERT and XLM-\n100 which have Weaknesses in low resource languages. \nThe comparison of the size of the dataset used by XLM-R \n(blue bar) with multi -language mode l BERT and XLM -\n100 (orange bar) against 88 languages can be seen in  \nfigure 1.  \n3.1.2 mBERT \nMultilingual-BERT (mBERT) is a version of BERT, \nwhich has a transformer -like architecture [10] and is pre-\ntrained with unlabeled raw text. In contrast to BER T that \nonly trained in a single language, mBERT was trained in \n \nFigure 1: XLM-R Train Dataset Size (source: [15]) \n \n\nIndonesian Hoax News Classification with Multilingual… Informatica 46 (2022) 81–90 85 \n104 languages , including Indonesian,  with the largest \nWikipedia dataset using a masked language modeling \n(MLM) objective [26]. In dealing with unbalanced data, \noversampling is carried out on small langu ages and \nundersampling for large languages. This allows the \nmBERT model to be used for low -resource languages \ndataset such as Indonesian. \n3.2 BERTopic \nBERTopic is a topic modeling technique based on BERT \nand TF -IDF in creating clusters that produce easy -to-\ninterpret topics and important words that describe the \ntopic. The BERTopic model uses BERT in word \nprocessing which produces extraction results that match \nthe context of the word. Besides BERT, BERTopic also \nsupports several other word extraction models such as \nXLM-R which supports more than 50 languages for text \nextraction other than English. In addition, several \nadvantages of BERTopic that can support this research are \nthat BERTopic supports a variety of topic modeling, \nhierarchical topic reduction, and can find the number of \ntopics automatically [27]. \nFor document grouping, BERTopic uses two \nalgorithms, namely the UMAP algorithm to reduce the \ndimensions of the word insertion results and the \nHDBSCAN algorithm for document grouping [17]. The \ngrouping of documents in the BERTopic  model is based \non the value of the class -based variant of TF -IDF (c-TF-\nIDF) in determining the uniqueness of a document \ncompared to other documents.  \n3.3 Hyperparameter tuning using Optuna \nThe transformer models used in this study, XLM -R and \nmBERT, require hy perparameters to obtain optimal \ntraining results. The hyperparameters used in the model \ninclude learning rate, weight decay, and training epochs. \nThe value of these hyperparameters can be determined \nmanually, but it is very inefficient  and time-consuming. \nTherefore, in this study, hyperparameter tuning was \ncarried out in determining the hyperparameter values \nautomatically using a framework called  the Optuna. \nUnlike other optimization frameworks, Optuna provides a \nDefine-by-run API that dynamically constru cts \nhyperparameter search [28].  \n4 Research methodology \nThis research will be divided into three stages, namely the \ninitiation stage, implementation stage, and evaluation \nstage. These stages can be seen in  figure 2.  The dataset \nthat has been collected will  go through a preprocessing \nprocess before being input into the topic distribution \nmodel. The cleaned data will be processed using a topic \ndistribution model to get a contextual words \nrepresentation of the news which will then be combined \nwith news articles as input. The combined news and topic \ndistribution data will then be input to the transformer \nmodel for hyperparameter tuning to produce \nhyperparameters that will be used for mode l evaluation. \nThe evaluation results will also be compared with other \nmodels to determine the performance of the proposed \nmethod.  \n4.1 Proposed method \nThe proposed deep learning method will focus on 2 main \nparts, which are the feature extraction stage using the pre-\ntrained multilingual model (XLM -R, mBERT)  and the \ntopic distribution stage using the BERTopic Model. \nIn the first stage , news data that has passed the \npreprocessing stage will be extracted for its features \nbefore being used as classification input. The Feature \nExtraction stage at th is stage uses the pre -trained \nmultilingual model (XLM-R, mBERT) from the \n \nFigure 2: Research Methodology Stages \n \n\n86 Informatica 46 (2022) 81–90 L. B. Hutama  et al. \n \n \nFigure 3: Proposed Deep Learning Method Flowchart \n \nHunggingface library because the model supports text \ninput in multi -languages, making it suitable for u se in \nIndonesian news datasets.  The second stage aims to \nproduce topic  word representations  that will be used as \ninput, along with the results of feature extraction, in the \nclassification model.  BERTopic also supports  \nmultilingual language text as input.  \nThe proposed method will be implemented using the \npython programming language and the Huggingface \ntransformers library which can be done using the PyTorch \nlibrary. If illustrated, the proposed deep learning  \narchitecture can be seen in figure 3. Figure 3 briefly \ndescribes the flow of the proposed method architecture. \nThe dataset will first go through a preprocessing process \nwhich consists of removing URLs, converting words to \nlowercase, removing stop words, removing excess spaces,  \n \nand stemming. The results of the preprocessing will go \nthrough the topic modeling process using the BERTopic  \nmodel to get the topic words representation , and the \nfeature extraction process using the pre -trained \ntransformer multilingual model (XLM -R, mBERT ). The \nresults of topic distribution and feature extraction will then \nbe combined and used as input to the classification model \nto produce a SoftMax score which will be used as \npredictive output. \nTo get maximum performance results, a parameter \ntuning proce ss will be carried out using the Optuna \nframework to find the optimal parameters. In addition, the \nsize of the number of topics used in the topic modeling \nstage will also be divided into 5 and 10 words . After \ngetting a good performance, a performance evalu ation is \ncarried out. \n \n \n \nFigure 4: Preprocessing Progress Flow \n \n \n  \n\nIndonesian Hoax News Classification with Multilingual… Informatica 46 (2022) 81–90 87 \n4.2   Data \nThe dataset that will be used in the study is a dataset that \ncontains news articles in Indonesian that have been \nlabeled as valid or hoaxes originating from two sources. \nThe first dataset comes from research (Rahutomo et al., \n2019) with a total of 600 news data consisting of 228 \nhoaxes and 372 valid news data downloaded from \nhttps://data.mendeley.com/datasets/p3hfgr5j3m. The \ndataset has been labeled by three experts with a voting \nsystem as the result which is used as the dataset label. The \nsecond dataset comes from a GitHub repository called \nPierobeat, which was downloaded from \nhttps://github.com/pierobeat/Hoax-News-Classification, \nwith a total of 500 data consisting of 250 valid news data \nand 250 labeled hoax news data collected from official \nnews sites (Turnbackhoax, liputan6, Kompas, Detik and \nCNN Indonesia ). These datasets have two attributes, \nwhich are  the news attribute and the label attribute . In \ndetail, the number of data used can be seen in table 2. \nTable 2: News Dataset Distribution \nDataset Label Total Hoax Valid \nRahutomo 228 372 600 \nPierobeat 250 250 500 \nCombined (Rahutomo \n+ Pierobeat) 478 622 1100 \n \n4.3   Preprocessing \nThe preprocessing stage aims to make the feature \nextraction process more contextual. The steps taken \nduring preprocessing can be seen in figure 4. \nPreprocessing begins by removing URLs, which are \nnews references and symbols that have no meaning in a \nsentence. The punctuation symbol is not removed because \nit can provide context information to the Transformer \nmodel. The next stage is normalization by c hanging the \nword to lowercase. After that, redundant spaces and stop \nwords are removed which did not contain meaningful \ninformation. The last stage is stemming, which is \nremoving the affix of a word so that it becomes a basic \nword that contains the essence of a word. The stop words \nand stemming removal phase will use the NLTK Library \nwhich provides methods for stemming and stop words \ndictionary.  \n4.4   Experiment \nTo compare the proposed methodology, comparisons will \nbe made with several architectures with different types of \ntopic distribution methods using several models in \nclassifying Indonesian fake news. This research will be \ndivided into several scenarios where each scenario will \nuse different algorithms and topic distribution methods. \nThis study will u se two types of multilingual pre -trained \ntransformer models to be compared (mBERT, XLM -R). \nEach model will be trained without using the word \ndistribution of topics. Then proceed by using 5 and 10 \ntopics words representation from the BERTopic model. \nHyperparameter tuning using the Optuna framework will \nbe used to get the optimal performance.  A total of 25 \nexperiments will be carried out using the Optuna \nframework where each experiment will use different \nparameter values. At the end of the iteration will disp lay \nthe experimental results and the most optimal \nhyperparameter values. The range values specified in the \nOptuna framework can be seen in table 3. \n \nTable 3: Optuna Hyperparameter Range \nHyperparameter Range Value \nLearning Rate 4e-5 - 0.01 \nWeight Decay 4e-5 - 0.01 \nTraining Epochs 2-5 \n \n5 Result and discussion \nIn our research, the BERTopic model is used to distribute \ntopics whose results are used as input along with news \narticles. The BERTopic model automatically determines \nthe number of topics from the news dataset used, which is \n24 topics, and each topic has a different word \nrepresentation. The results of BERTopic can be seen in \ntable 4 below. \nTable 4: BERTopic Topic Words Representation Result \nTopic Top 5 Words Top 10 Words \n0 permen, dot, \nnarkoba, \nsurabaya, \nmengandung \npermen, dot, narkoba, \nsurabaya, mengandung, \nsekolah, anak, diduga, \nrazia, makanan \n1 tahanan, brimob, \npetugas, rutan, \nkerusuhan \ntahanan, brimob, \npetugas, rutan, \nkerusuhan, mako, \nrikwanto, blok, sel, \nkeributan \n2 pokemon, bahasa, \nyahudi, go, game \npokemon, bahasa, \nyahudi, go, game, \nmonster, arti, anak, \npikachu, permainan \n3 traveloka, ananda, \nanies, kanisius, \nderianto \ntraveloka, ananda, anies, \nkanisius, derianto, out, \nwalk, aksi, acara, nilai \n4 facebook, data, \nrudiantara, \nindonesia, \npengguna \nfacebook, data, \nrudiantara, indonesia, \npengguna, kominfo, \npemerintah, memblokir, \nkonten, akun \n5 lele, ikan, kanker, \nmengandung, \nkotor \nlele, ikan, kanker, \nmengandung, kotor, \ntubuh, sel, limbah, \nkolam, manusia \n6 stroke, darah, \njarum, penderita, \npertolongan \nstroke, darah, jarum, \npenderita, pertolongan, \npasien, jari, pembuluh, \nsakit, otak \n88 Informatica 46 (2022) 81–90 L. B. Hutama  et al. \n7 reog, davao, kjri, \nponorogo, \npembakaran \nreog, davao, kjri, \nponorogo, pembakaran, \ncity, filipina, reyog, \nkesenian, budaya \n8 masjid, istiqlal, \naksi, 212, abu \nmasjid, istiqlal, aksi, \n212, abu, 21, subuh, \npeserta, masuk, bppmi \n9 iphone, plus, \napple, \nmelengkung, \npengguna \niphone, plus, apple, \nmelengkung, pengguna, \nmudah, bengkok, \nsmartphone, saku, layar \n10 bulu, sikat, babi, \ngigi, bristle \nbulu, sikat, babi, gigi, \nbristle, kuas, bahan, \nrambut, produk, terbuat \n11 ahok, tni, dik, \nkaroseri, arahan \nahok, tni, dik, karoseri, \narahan, vs, lokal, \nkonflik, monas, polri \n12 palestina, israel, \nmuslim, arab, gap \npalestina, israel, \nmuslim, arab, gap, natal, \ncanada, haji, snack, jco \n13 presiden, gaji, \nwakil, kenaikan, \nrp \npresiden, gaji, wakil, \nkenaikan, rp, dokumen, \nberedar, mulyani, sri, \ndiusulkan \n14 maluku, guru, \njokowi, tpg, partai \nmaluku, guru, jokowi, \ntpg, partai, tim, 2019, \npengawas, esports, \npemilu \n15 isis, irak, bom, \nserangan, as \nisis, irak, bom, \nserangan, as, suriah, \norang, pasukan, luka, \nkelompok \n16 luhut, china, \nindonesia, kau, \nfreeport \nluhut, china, indonesia, \nkau, freeport, kapal, \nminggu, jokowi, ahad, \nnegaraku \n17 gaji, presiden, rp, \npenghasilan, juta \ngaji, presiden, rp, \npenghasilan, juta, \npejabat, rpp, pns, \nindeks, negara \n18 korban, mirna, \nmeninggal, \npelaku, \npembunuhan \nkorban, mirna, \nmeninggal, pelaku, \npembunuhan, jessica, \npolisi, ditemukan, sakit, \nbom \n19 pesawat, \npenumpang, \npenerbangan, lion, \nbandara \npesawat, penumpang, \npenerbangan, lion, \nbandara, maskapai, \npilot, air, lambertus, \nkopilot \n20 gempa, banjir, \nlongsor, bencana, \ngunung \ngempa, banjir, longsor, \nbencana, gunung, \nagung, erupsi, desa, \nkabupaten, gorontalo \n21 iklan, hago, guru, \nunj, ika \niklan, hago, guru, unj, \nika, siswa, paslon, \nsosok, profesi, peserta \n22 gula, yg, formalin, \nkristen, masuk \ngula, yg, formalin, \nkristen, masuk, mohon, \nnanas, jawa, pake, nabi \n23 presiden, yg, \nkereta, orang, ini \npresiden, yg, kereta, \norang, ini, 2019, korban, \nwakil, papua, \nmahasiswa \n \nAfter that, h yperparameter tuning was carried out \nusing the Optuna framework for the pre -trained \nmultilingual model (XLM -R and mBERT), with and \nwithout the topic word  representation, to get maximum \nperformance. The best hyperparameter results can be seen \nin table 5.  \nTable 5: Optuna Hyperparameter Tuning Best \nHyperparameter \nModel Learning \nRate \nWeight \nDecay \nEpochs \nmBERT 3.6642e-05 0.00021 4 \nXLM-R 2.067e-05 3.116e-05 4 \nmBERT + 5 \ntopic words 3.6524e-05 0.009 5 \nXLM-R + 5 \ntopic words 2.5452e-05 0.00041 5 \nmBERT + 10 \ntopic words 1.996e-05 0.0041 4 \nXLM-R + 10 \ntopic words 1.7388e-05 1.0221e-05 4 \n \nBy using these hyperparameters, an evaluation of the \ntraining results is carried out using the accuracy, precision, \nrecall, and f1 metrics. The evaluation was carried out by \ncomparing the proposed model with the BERT model \n[12][13] and the ensemble model, consisting of BERT, \nALBERT, an d XLNet [18].  Each model will be trained \nusing the news dataset with a ratio of 7:3 between the \ntraining and testing data. The training results of each \nmodel can be seen in table 6. \nTable 6: Training Result \nModel Accuracy Precision Recall F1 \nBERT \n[12][13] \n0.7454 0.72 0.6067 0.6585 \nEnsemble \nMethod \n[18] \n0.7545 0.4606 0.8723 0.6029 \nmBERT 0.7863 0.8181 0.6067 0.6967 \nXLM-R 0.8 0.64 0.826 0.72 \nmBERT \n+ 5 \ntopic \nwords \n0.8242 0.8392 0.7014 0.7642 \nXLM-R \n+ 5 \ntopic \nwords \n0.8121 0.7812 0.7462 0.7633 \nIndonesian Hoax News Classification with Multilingual… Informatica 46 (2022) 81–90 89 \nmBERT \n+ 10 \ntopic \nwords \n0.9051 0.9515 0.8233 0.8828 \nXLM-R \n+ 10 \ntopic \nwords \n0.8935 0.8818 0.8712 0.8765 \n \nTable 6 shows the results of the evaluation of this \nstudy. From these results, the proposed method, which \nuses a multilingual model (XLM -R, mBERT) combined \nwith a topic distribution model using BERTopic  \noutperforms the BERT model [12][13] and the ensemble \nmodel [18]. This happen s because those models (BERT, \nALBERT, XLNet) are only trained in English and are not \nsuitable for low -resource languages such as Indonesian. \nThe proposed method also proves that the BERTopic \nmodel successfully improves the performance of t he \nindividual pre -trained multilingual model.  The mBERT \nmodel with 10 topic words got the best accuracy, \nprecision, recall, and F1 values of 0.9051, 0.9515, 0.8233, \nand 0.8828 respectively. While the XLM-R model with 10 \ntopic words managed to get accuracy,  precision, recall , \nand F1 values which were not much different from \nmBERT, which are 0.8935, 0.8818, 0.8712, and 0.8765 \nrespectively. These results are much better than using only \nindividual pre -trained models which only produce \naccuracy, precision, recal l, and F1 values of 0.7863, \n0.8181, 0.6067, and 0.6967 for the mBERT model and 0.8, \n0.64, 0.826, and 0.71 for the XLM-R model. \nThis study also proves that the more words from the \ntopic distribution are used, the higher the performance of \nthe multilingual p re-trained model. It can be seen in the \ntable above, that the results of multilingual model training \nusing 10 topic words succeeded in increasing the \nperformance of accuracy, precision, recall, and F1 by 0.08, \n0.11, 0.12, and 0.11 respectively compared to using only \n5 topic words. \n6 Conclusion and future work \nThis study shows a comparison of pre-trained multilingual \nmodels in building a classification system for hoax news \nagainst low -resource language news (Indonesian \nLanguage). The deep learning model propos ed in this \nstudy uses a pre -trained multilingual model, namely \nmBERT and XLM -R, which are added topic-\nrepresentative words from the distribution of topics using \nthe BERTopic model. The proposed model is proven to be \nsuccessful in improving the performance of the individual \npre-trained multilingual model with the results of \naccuracy, precision, recall , and F1 of 0.9051, 0.9515, \n0.8233, and 0.8828 respectively for the mBERT model \nwith 10 topic words and 0.8935, 0.8818, 0.8712, and \n0.8765 for the XLM-R model with 10 topic words. These \nresults are proven to increase accuracy significantly when \ncompared to using only individual pre -trained \nmultilingual models in classifying hoax news on news \nwith low -resource language (Indonesian Language). In \naddition, this stud y also proves that the more topic-\nrepresentative words used, the higher the performance of \nthe model. This can be seen from the performance of using \n10 topic representative words which resulted in better \naccuracy, precision, recall , and F1 values of 0.08, 0.11, \n0.12, and 0.11 respectively compared to using only 5 topic \nrepresentative words. \nIn the future, development can be done by exploring \ndatasets related to fake news using other low -resource \nlanguages in large numbers, because this study has not \nused large amounts of data. In addition, development can \nbe done by trying different learning methods and models, \nsuch as using the ensemble method combined with \ndifferent topic distribution models. \nReferences \n[1] Werme, “Pemberdayaan Masyarakat Mengenai \nIsu Hoax  Vaksinasi Terhadap Kesehatan \nMasyarakat di Masa Pandemi Covid -19,” 2016. \nhttps://www.kompasiana.com/nanda82966/622f9\n4e3bb448645a54718b2/pemberdayaan-\nmasyarakat-mengenai-isu-hoax-vaksinasi-\nterhadap-kesehatan-masyarakat-dimasa-\npandemic-covid-19 (accessed Mar. 21, 2021). \n[2] D. M. J. Lazer et al., “The science of fake news,” \nScience (1979) , vol. 359, no. 6380, pp. 1094 –\n1096, 2018. \n[3] Mastel, “Hasil Survey Wabah Hoax Nasional \n2019,” 2019. https://mastel.id/hasil -survey-\nwabah-hoax-nasional-2019/ (accessed Jun.  21, \n2021). \n[4] Kemenkeu, “Jangan Mudah Termakan Hoax, \nSaring Sebelum Sharing,” Jun. 22, 2020. \nhttps://www.djkn.kemenkeu.go.id/artikel/baca/13\n206/Jangan-Mudah-Termakan-Hoax-Saring-\nSebelum-Sharing.html (accessed Jun. 21, 2021) . \nhttps://doi.org/10.15548/amj-kpi.v2i1.486. \n[5] M. Viviani and G. Pasi, “Credibility in social \nmedia: opinions, news, and health information—a \nsurvey,” Wiley interdisciplinary reviews: Data \nmining and knowledge discovery, vol. 7, no. 5, p. \ne1209, 2017. https://doi.org/10.1002/widm.1209. \n[6] B. P. Nayoga, R. Adipradana, R. Suryadi, and D. \nSuhartono, “Hoax Analyzer for Indonesian News \nUsing Deep Learning  Models,” Procedia \nComputer Science , vol. 179, pp. 704 –712, 2021. \nhttps://doi.org/10.1016/j.procs.2021.01.059. \n[7] A. Stöckl, “Detecting Satire in the News with \nMachine Learning,” arXiv preprint \narXiv:1810.00593, 2018. \n[8] M. Umer, Z. Imtiaz, S. Ullah, A. Mehmood, G. S. \nChoi, and B.-W. On, “Fake news stance detection \nusing deep learning architecture (CNN -LSTM),” \nIEEE Access , vol. 8, pp. 156695 –156706, 2020.  \nhttps://doi.org/10.1109/access.2020.3019735.  \n[9] A. Roy, K. Basak, A. Ekbal, and P. Bhattacharyya, \n“A deep ensemble framework for fake news \ndetection and classification,” arXiv preprint  \narXiv:1811.04670, 2018. \n90 Informatica 46 (2022) 81–90 L. B. Hutama  et al. \n[10] A. Vaswani et al., “Attention is all you need,” in \nAdvances in neural information processing \nsystems, 2017, pp. 5998–6008. \n[11] T. Wolf et al. , “Transformers: State -of-the-art \nnatural language processing,” in Proceedings of \nthe 2020 Conference on Empirical Methods in \nNatural Language Processing: System \nDemonstrations, 2020, pp. 38 –45. \nhttps://doi.org/10.18653/v1/2020.emnlp-demos.  \n[12] H. Jwa, D. Oh, K. Pa rk, J. M. Kang, and H. Lim, \n“exbake: Automatic fake news detection model \nbased on bidirectional encoder representations \nfrom transformers (bert),” Applied Sciences, vol. \n9, no. 19, p. 4062, 2019.  \nhttps://doi.org/10.3390/app9194062. \n[13] M. Qazi, M. U. S. K han, and M. Ali, “Detection \nof fake news using transformer model,” in 2020 \n3rd International Conference on Computing, \nMathematics and Engineering Technologies \n(iCoMET), 2020, pp. 1 –6. \nhttps://doi.org/10.1109/icomet48670.2020.90740\n71. \n[14] D. J. M. Pasaribu, K. Kusrini, and S. Sudarmawan, \n“Peningkatan Akurasi Klasifikasi Sentimen \nUlasan Makanan Amazon dengan Bidirectional \nLSTM dan Bert Embedding,” Inspiration: Jurnal \nTeknologi Informasi dan Komunikasi, vol. 10, no. \n1, pp. 9 –20, 2020.  \nhttps://doi.org/10.35585/inspir.v10i1.2568.  \n[15] A. Conneau et al. , “Unsupervised cross -lingual \nrepresentation learning at scale,” arXiv preprint \narXiv:1911.02116, 2019. \n[16] A. Gautam, V. Venktesh, and S. Masud, “Fake \nnews detection system using xlnet model with \ntopic distributions: Constraint@ aaai2021 shared \ntask,” in International Workshop onCombating \nOnline Hostile Posts in  Regional Languages \nduring Emergency Situation , 2021, pp. 189 –200. \nhttps://doi.org/10.1007/978-3-030-73696-5_18.  \n[17] A. Abuzayed and H. Al -Khalifa, “BERT for \nArabic Topic Modeling: A n Experimental Study \non BERTopic Technique,” Procedia Computer \nScience, vol. 189, pp. 191 –194, 2021.  \nhttps://doi.org/10.1016/j.procs.2021.05.096. \n[18] S. Gundapu and R. Mamidi, “Transf ormer based \nAutomatic COVID -19 Fake News Detection \nSystem,” arXiv preprint arXiv:2101.00180, 2021. \n[19] A. De, D. Bandyopadhyay, B. Gain, and A. Ekbal, \n“A Transformer-Based Approach to Multilingual \nFake News Detection in Low -Resource \nLanguages,” Transactions on Asian and Low -\nResource Language Information Processing , vol. \n21, no. 1, pp. 1 –20, 2021.  \nhttps://doi.org/10.1145/3472619.  \n[20] M. S. H. Ameur and H. Aliane , “AraCOVID19 -\nMFH: Arabic COVID -19 Multi-label Fake News \n& Hate Speech Detection Dataset,” Procedia \nComputer Science , vol. 189, pp. 232 –241, 2021. \nhttps://doi.org/10.1016/j.procs.2021.05.086.  \n[21] J. Radom and J. Kocoń, “Multi -task Sequence \nClassification for Disjoint Tasks in Low -resource \nLanguages,” Procedia Computer Science , vol. \n192, pp. 1132 –1140, 2021.  \nhttps://doi.org/10.1016/j.procs.2021.08.116.  \n[22] N. R. Shin, T. Kim, D. Y. Yun, S.-J. Moon, and C. \nHwang, “Sentiment analysis of Korean movie \nreviews using XLM -R,” International Journal of \nAdvanced Culture Technology , vol. 9, no. 2, pp. \n86–90, 2021. \n[23] A. Ze rvopoulos, A. G. Alvanou, K. Bezas, A. \nPapamichail, M. Maragoudakis, and K. \nKermanidis, “Deep learning for fake news \ndetection on Twitter regarding the 2019 Hong \nKong protests,” Neural Computing and \nApplications, vol. 34, no. 2, pp. 969 –982, 2022. \nhttps://doi.org/10.1007/s00521-021-06230-0.  \n[24] Z. Guan, “TSIA team at FakeDeS 2021: Fake \nnews detection in spanish using multi -model \nensemble learning,” 2021. \n[25] S. Jia, X. Zhang, X. Wang,  and Y. Liu, “Fake \nreviews detection based on LDA,” in 2018 4th \nInternational Conference on Information \nManagement (ICIM) , 2018, pp. 280 –283. \nhttps://doi.org/10.1109/infoman.2018.8392850.  \n[26] T. Pires, E. Schlinger, and D. Garrette, “How \nmultilingual is multilingual BERT?” arXiv \npreprint arXiv:1906.01502 , 2019.  \nhttps://doi.org/10.18653/v1/p19-1493.  \n[27] R. Egger and J. Yu, “A Topic Modeling \nComparison Between LDA, NMF, Top2Vec, and \nBERTopic to Demystify Twitter Posts,” Frontiers \nin Sociology , vol. 7, 2022.  \nhttps://doi.org/10.3389/fsoc.2022.886498.  \n[28] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. \nKoyama, “Optuna: A next -generation \nhyperparameter optimization framework,” in \nProceedings of the 25th ACM SIGKDD \ninternational conference on knowledge discovery \n& data mining , 2019, pp. 2623 –2631. \nhttps://doi.org/10.1145/3292500.3330701.  \n  \n \n ",
  "topic": "Hoax",
  "concepts": [
    {
      "name": "Hoax",
      "score": 0.9585322141647339
    },
    {
      "name": "Computer science",
      "score": 0.6779434680938721
    },
    {
      "name": "Indonesian",
      "score": 0.6632975935935974
    },
    {
      "name": "Transformer",
      "score": 0.6303791999816895
    },
    {
      "name": "Language model",
      "score": 0.5353755354881287
    },
    {
      "name": "The Internet",
      "score": 0.5035702586174011
    },
    {
      "name": "Dissemination",
      "score": 0.48195120692253113
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45761317014694214
    },
    {
      "name": "World Wide Web",
      "score": 0.38926875591278076
    },
    {
      "name": "Information retrieval",
      "score": 0.38714149594306946
    },
    {
      "name": "Natural language processing",
      "score": 0.37385308742523193
    },
    {
      "name": "Telecommunications",
      "score": 0.11290034651756287
    },
    {
      "name": "Engineering",
      "score": 0.10427588224411011
    },
    {
      "name": "Linguistics",
      "score": 0.10313287377357483
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166073570",
      "name": "Binus University",
      "country": "ID"
    }
  ],
  "cited_by": 26
}