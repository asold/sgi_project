{
  "title": "SODFormer: Streaming Object Detection With Transformer Using Events and Frames",
  "url": "https://openalex.org/W4385285879",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2752618189",
      "name": "Li Dianze",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2074661938",
      "name": "Li, Jianing",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1989693854",
      "name": "Tian Yonghong",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2988916019",
    "https://openalex.org/W3082397598",
    "https://openalex.org/W3011722050",
    "https://openalex.org/W2983367574",
    "https://openalex.org/W3176165731",
    "https://openalex.org/W3095334530",
    "https://openalex.org/W206948248",
    "https://openalex.org/W2766230893",
    "https://openalex.org/W2016574277",
    "https://openalex.org/W2969508737",
    "https://openalex.org/W3129601843",
    "https://openalex.org/W3174138177",
    "https://openalex.org/W2921116036",
    "https://openalex.org/W2964981172",
    "https://openalex.org/W3202344666",
    "https://openalex.org/W3194536588",
    "https://openalex.org/W2796402180",
    "https://openalex.org/W1632573554",
    "https://openalex.org/W2011589116",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2895361760",
    "https://openalex.org/W3206836360",
    "https://openalex.org/W4214516465",
    "https://openalex.org/W4281749192",
    "https://openalex.org/W3007891240",
    "https://openalex.org/W3040838455",
    "https://openalex.org/W2982433817",
    "https://openalex.org/W2990793844",
    "https://openalex.org/W3039515597",
    "https://openalex.org/W3000845761",
    "https://openalex.org/W6783508657",
    "https://openalex.org/W6755494875",
    "https://openalex.org/W3188021906",
    "https://openalex.org/W3205252664",
    "https://openalex.org/W2909642632",
    "https://openalex.org/W2769320958",
    "https://openalex.org/W2971854498",
    "https://openalex.org/W2925837493",
    "https://openalex.org/W3150422330",
    "https://openalex.org/W4225688144",
    "https://openalex.org/W4293518748",
    "https://openalex.org/W4382467391",
    "https://openalex.org/W4386075690",
    "https://openalex.org/W3010268791",
    "https://openalex.org/W6750227808",
    "https://openalex.org/W2512653618",
    "https://openalex.org/W4288450785",
    "https://openalex.org/W3196480172",
    "https://openalex.org/W2612624696",
    "https://openalex.org/W2105482032",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3203020760",
    "https://openalex.org/W3203854346",
    "https://openalex.org/W3204512785",
    "https://openalex.org/W3199425519",
    "https://openalex.org/W3202534481",
    "https://openalex.org/W4387587572",
    "https://openalex.org/W2783185291",
    "https://openalex.org/W4221145923",
    "https://openalex.org/W4283823181",
    "https://openalex.org/W2938463073",
    "https://openalex.org/W3096563782",
    "https://openalex.org/W2076661751",
    "https://openalex.org/W2129066464",
    "https://openalex.org/W3133660631",
    "https://openalex.org/W3114286648",
    "https://openalex.org/W6803301428",
    "https://openalex.org/W4312500310",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2904275768",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6767278793",
    "https://openalex.org/W3175999922",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963212638",
    "https://openalex.org/W4213412898",
    "https://openalex.org/W2988698702",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2964286567",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W3102178346",
    "https://openalex.org/W3090117981",
    "https://openalex.org/W4226288682",
    "https://openalex.org/W4286750747",
    "https://openalex.org/W3211382623",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3093024756",
    "https://openalex.org/W2803347562",
    "https://openalex.org/W3098361319",
    "https://openalex.org/W3099039688",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4226051885",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W4287901537",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2899000831"
  ],
  "abstract": "DAVIS camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been used to address major object detection challenges (e.g., fast motion blur and low-light). However, how to effectively leverage rich temporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. To address this challenge, we propose a novel streaming object detector with Transformer, namely SODFormer, which first integrates events and frames to continuously detect objects in an asynchronous manner. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., PKU-DAVIS-SOD) over 1080.1 k manual labels. Then, we design a spatiotemporal Transformer architecture to detect objects via an end-to-end sequence prediction problem, where the novel temporal Transformer module leverages rich temporal cues from two visual streams to improve the detection performance. Finally, an asynchronous attention-based fusion module is proposed to integrate two heterogeneous sensing modalities and take complementary advantages from each end, which can be queried at any time to locate objects and break through the limited output frequency from synchronized frame-based fusion strategies. The results show that the proposed SODFormer outperforms four state-of-the-art methods and our eight baselines by a significant margin. We also show that our unifying framework works well even in cases where the conventional frame-based camera fails, e.g., high-speed motion and low-light conditions. Our dataset and code can be available at https://github.com/dianzl/SODFormer.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 1\nSODFormer: Streaming Object Detection with\nTransformer Using Events and Frames\nDianze Li, Jianing Li, Member, IEEE and Y onghong Tian,Fellow, IEEE\nAbstractâ€”DAVIS camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been\nused to address major object detection challenges (e.g., fast motion blur and low-light). However, how to effectively leverage rich\ntemporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. To address this challenge, we propose a\nnovel streaming object detector with Transformer, namely SODFormer, which first integrates events and frames to continuously detect\nobjects in an asynchronous manner. Technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e.,\nPKU-DAVIS-SOD) over 1080.1k manual labels. Then, we design a spatiotemporal Transformer architecture to detect objects via an\nend-to-end sequence prediction problem, where the novel temporal Transformer module leverages rich temporal cues from two visual\nstreams to improve the detection performance. Finally, an asynchronous attention-based fusion module is proposed to integrate two\nheterogeneous sensing modalities and take complementary advantages from each end, which can be queried at any time to locate\nobjects and break through the limited output frequency from synchronized frame-based fusion strategies. The results show that the\nproposed SODFormer outperforms four state-of-the-art methods and our eight baselines by a significant margin. We also show that our\nunifying framework works well even in cases where the conventional frame-based camera fails, e.g., high-speed motion and low-light\nconditions. Our dataset and code can be available at https://github.com/dianzl/SODFormer.\nIndex Termsâ€”Neuromorphic Vision, Event Cameras, Object Detection, Transformer, Multimodal Fusion.\nâœ¦\n1 I NTRODUCTION\nO\nBJECT detection [1], [2], [3], one of the most fun-\ndamental and challenging topics, supports a wide\nrange of computer vision tasks, such as autonomous driv-\ning, intelligent surveillance, robot vision, etc. In fact, with\nconventional frame-based cameras, object detection perfor-\nmance [4], [5], [6] has suffered from a significant drop in\nsome challenging conditions (e.g., high-speed motion blur,\nlow-light, and overexposure). A key question still remains:\nHow to utilize a novel sensing paradigm that makes up for the\nlimitations of conventional cameras?\nMore recently, Dynamic and Active-Pixel Vision Sen-\nsor [7], [8] (i.e., DAVIS), namely a multimodal vision sensor,\nis designed in that spirit, combining a bio-inspired event\ncamera and a conventional frame-based camera in the same\npixel array. Its core is the novel event camera (i.e., DVS [9]),\nwhich works differently from frame-based cameras and\nreacts to light changes by triggering asynchronous events.\nSince the event camera offers high temporal resolution ( Âµs)\nand high dynamic range (HDR, up to 120 dB), it has brought\na new perspective to address some limitations of conven-\ntional cameras in fast motion and challenging light sce-\nnarios. However, just as conventional frame-based cameras\nâ€¢ Dianze Li and Jianing Li are with the National Engineering Research\nCenter for Visual Technology, School of Computer Science, Peking Uni-\nversity, Beijing 100871, China. E-mail: dianzeli@stu.pku.edu.cn, lijian-\ning@pku.edu.cn.\nâ€¢ Yonghong Tian are with the National Engineering Research Center for\nVisual Technology, School of Computer Science, Peking University, Bei-\njing 100871, China, and also with the Peng Cheng Laboratory, Shenzhen\n518000, China. E-mail: yhtian@pku.edu.cn.\nManuscript received September 23, 2022.\n(Corresponding author: Yonghong Tian and Jianing Li).\nfail in extreme-light or high-speed motion blur scenarios,\nevent cameras perform poorly in static or extremely slow-\nmotion scenes. We refer to this phenomenon as unimodal\ndegradation because it is mainly caused by the limitations\nof unimodal cameras (see Fig. 1). While event cameras\nare insensitive in static or extremely slow-motion scenes,\nframe-based cameras directly provide static fine textures\n(i.e., absolute brightness) conversely. Indeed, event cam-\neras and frame-based cameras are complementary, which\nmotivates the development of novel computer vision tasks\n(e.g., feature tracking [10], depth estimation [11], and video\nreconstruction [12]) by taking advantage of each end. There-\nfore, we aim at making complementary use of asynchronous\nevents and frames to maximize object detection accuracy.\nOne problem is that most existing event-based object\ndetectors [13], [14], [15], [16] run feed-forward models inde-\npendently, leading to not utilizing rich temporal cues in con-\ntinuous visual streams. When isolated event image [17] may\nhave issues involving small objects, occlusion, and out of\nfocus, it is natural for the human visual system [18], [19] to\nidentify objects in the temporal dimension and then assign\nthem together. In other words, the detection performance\ncan be further improved via leveraging rich temporal cues\nfrom event streams and adjacent frames. Although the field\nof computer vision has witnessed significant achievements\nof CNNs and RNNs, they show poor ability in model-\ning long-term temporal dependencies. Consequently, the\nemerging Transformer is gaining more and more attention in\nobject detection tasks [20], [21]. Thanks to the self-attention\nmechanism, Transformer [22], [23] is particularly suited to\nmodeling long-term temporal dependencies for video se-\nquence tasks [24], [25], [26]. But, thereâ€™s still no Transformer-\nbased object detector to exploit temporal cues from events\narXiv:2308.04047v1  [cs.CV]  8 Aug 2023\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 2\nğ‘¥\ny\nğ‘¡\n(a) Events and frames\n (b) Motion blur\n(c) Low-light condition\n (d) Static scenes\nFig. 1: This work makes complementary use of events and\nframes to detect objects. (a) DAVIS [7] outputs events and\nframes. (b)-(d) The left refers to frames, and the right de-\nnotes events. Note that, the event camera has the ability of\nhigh-speed and high dynamic range sensing, but it fails to\ncapture static textures as a conventional camera does.\nand frames. Meanwhile, the rare multimodal neuromorphic\nobject detection datasets (e.g., PKU-DDD17-CAR [14] and\nDAD [16]) only provide isolated images and synchronized\nevents, which means there is a lack of temporally long-term\ncontinuous and large-scale multimodal datasets including\nevents, frames, and manual labels.\nAnother problem is that current fusion strategies (e.g.,\nposting-processing [13], [14], concatenation [27] and av-\neraging [16], [28]) for events and frames are inadequate\nin exploiting complementary information between two streams\nwhile only able to synchronously predict results with the frame\nrate, incurring the bottlenecks of performance improvements and\ninference frequency. Since asynchronous events are sparse\npoints in the spatiotemporal domain with higher tempo-\nral resolution compared to structured frames, the existing\nframe-based multimodal detectors cannot directly deal with\ntwo heterogeneous visual streams. As a result, some fu-\nsion strategies attempt to first split the continuous event\nstreams into discrete image-like representations with the\nsame sampling frequency of frames and then integrate two\nsynchronized streams via the post-processing or feature\naggregation operation. In short, one drawback is that these\nfusion strategies canâ€™t distinguish the degree of degradation\nof modalities and regions, therefore fail to eliminate the\nunimodal degradation thoroughly. The other drawback is\nthat the inference frequency of these synchronized fusion\nstrategies is limited by the sampling rate of conventional\nframes. Yet, there is no work to design an effective asyn-\nchronous multimodal fusion module for events and frames\nin the object detection task.\nTo address the aforementioned problems, this paper pro-\nposes a novel streaming object detector with Transformer,\nnamely SODFormer, which continuously detects objects in\nan asynchronous way by integrating events and frames.\nActually, this work aims not to optimize Transformer-based\nobject detectors (e.g., DETR [20]) on each isolated image. In\ncontrast, our goal is to overcome the following challenges:\n(i) Lack of dataset- How do we create a large-scale multimodal\nneuromorphic dataset for object detection? (ii) Temporal correla-\ntion - How do we design a Transformer-based object detector that\nleverages rich temporal cues? (iii) Asynchronous fusion- What\nis the unifying mechanism that takes advantages from two streams\nand achieves high inference frequency ? To this end, we first\nbuild a large-scale multimodal object detection dataset (i.e.,\nPKU-DAVIS-SOD), which provides manual bounding boxes\nat a frequency of 25 Hz for 3 classes, yielding more than\n1080.1k labels. Then, we propose a spatiotemporal Trans-\nformer that aggregates spatial information from continuous\nstreams, which outputs the detection results via an end-to-\nend sequence prediction problem. Finally, an asynchronous\nattention-based fusion module is designed to effectively in-\ntegrate two sensing modalities while eliminating unimodal\ndegradation and breaking the limitation of frame rate. The\nresults show that our SODFormer outperforms the state-of-\nthe-art methods and our two single-modality baselines by a\nlarge margin. We also verify the efficacy of our SODFormer\nin fast motion and low-light scenarios.\nTo sum up, the main contributions of this work are\nsummarized as follows:\nâ€¢ We propose a novel Transformer-based framework\nfor streaming object detection (SODFormer), which first\nintegrates events and frames via Transformer to con-\ntinuously detect objects in an asynchronous manner.\nâ€¢ We design an effective temporal Transformer module ,\nwhich leverages rich temporal cues from two visual\nstreams to improve the object detection performance.\nâ€¢ We develop an asynchronous attention-based fusion\nmodule that takes complementary advantages from\neach end, eliminates unimodal degradation and\novercomes the limited inference frequency from\nframe rate.\nâ€¢ We establish alarge-scale and temporally long-term mul-\ntimodal neuromorphic dataset (i.e., PKU-DAVIS-SOD),\nwhich will open an opportunity for the research of\nthis challenging problem.\nThe rest of the paper is organized as follows. Section 2 re-\nviews prior works. We describe how to build a competitive\ndataset in Section 3. Section 4 introduces the camera work-\ning principle and formulates the novel problem. Section 5\npresents our solution of streaming object detection with\nTransformer. Finally, Section 6 analyzes the performance of\nthe proposed method, and some discussions are present in\nSection 7, while some conclusions are drawn in Section 8.\n2 R ELATED WORK\nThis section first reviews neuromorphic object detection\ndatasets (Section 2.1) and the corresponding object detectors\n(Section 2.2), followed by an overview of object detectors\nwith Transformer (Section 2.3) and a survey of fusion ap-\nproaches for events and frames (Section 2.4).\n2.1 Neuromorphic Object Detection Datasets\nThe publicly available object detection datasets utilizing\nevent cameras have a limited amount [29], [30], [31], [32].\nGen1 Detection dataset [33] and 1Mpx Detection dataset [34]\nprovide large-scale annotations for object detection using\nevent cameras. However, it may be difficult to obtain fine\ntextures and achieve high-precision object detection by only\nusing DVS events. Although event-based simulators (e.g.,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 3\nESIM [35], V2E [36], and RS [37]) can directly convert video\nobject datasets to the neuromorphic domain, those trans-\nformed strategies fail to reflect realistic high-speed motion\nor extreme light scenarios, which are exactly what event\ncameras are good at. Besides, PKU-DDD17-CAR dataset [14]\nand DAD dataset [16] provide 3155 and 6247 isolated hybrid\nsequences, respectively. Nevertheless, these two small-scale\ndatasets are not continuous streams modeling temporal\ndependencies. Therefore, this work aims to establish a large-\nscale and temporally long-term multimodal object detection\ndataset involving challenging scenarios.\n2.2 Neuromorphic Object Detection Approaches\nThe existing object detectors using event cameras can be\nroughly divided into two categories. The first category [38],\n[39], [40], [41], [42], [43], [44], [45], [46] is the single-modality\nthat only processes asynchronous events. These methods\nusually first convert asynchronous events into 2D image-\nlike representations [47], and then input them into frame-\nbased detectors (e.g., YOLOs [48]). Although only utilizing\ndynamic events can achieve satisfactory performance in\nsome specific scenes, it becomes clear that high-precision\ndetection requires static fine textures.\nThe second category [6], [13], [14], [15], [16], [27], [49],\n[50], [51] refers to the multi-modality that combines multiple\nvisual streams. For example, some works [13], [14], [49]\nfirst detect objects on each isolated frame or event temporal\nbin, and then merge the detection results of two modalities\nby post-processing (e.g., NMS [52]). Besides, a grafting\nalgorithm [6] is proposed to integrate events and thermal\nimages. Some attention-based aggregation operations [15],\n[27] are designed to fuse each isolated frame and event\ntemporal bin from the PKU-DDD17-CAR [14] dataset, while\nothers [16], [28] average the features from each modality.\nHowever, these joint frameworks have not explored rich\ntemporal cues from continuous visual streams. Moreover,\nthese post-processing or aggregation fusion operations are\nhard to capture global context across two sensing modal-\nities, while averaging strategy cannot eliminate unimodal\ndegradation thoroughly. Thus, we design a streaming object\ndetector that aims at leveraging rich temporal cues and\nmaking fully complementary use of two visual streams.\n2.3 Object Detection with Transformer\nTransformer, an attention-based architecture, is first intro-\nduced by [22] for machine translation. Its core attention\nmechanism [53] scans through each element of a sequence\nand updates it by aggregating information from the whole\nsequence with different attention weights. The global com-\nputation makes Transformers perform better than RNNs\non long sequences. Until now, Transformers have been\nmigrated to computer vision and replaced RNNs in many\nproblems. In video object detection, while RNNs [54],\n[55], [56] have achieved great success, they require metic-\nulously hand-designed components (e.g., anchor genera-\ntion). To simplify these pipelines, DETR [20], an end-to-\nend Transformer-based object detector, is proposed. It is\nworth mentioning that DETR [20] is the first Transformer-\nbased object detector via a sequence predicting model.\nWhile DETR largely simplifies the classical CNN-based\n(a) Recording platform\n (b) DAVIS346 camera\nFig. 2: Experimental setup. A DAVIS346 camera is installed\non the front windshield of the driving car.\ndetection paradigm, it suffers from very slow convergence\nand relatively low performance at detecting small objects.\nMore recently, a few approaches attempt to design opti-\nmized architectures to help detect small objects and speed\nup training. For instance, Deformable DETR [21] adopts\nthe multi-scale deformable attention module that achieves\nbetter performance than DETR with 10 Ã— fewer training\nepochs. PnP-DETR [57] significantly reduces spatial redun-\ndancy and achieves more efficient computation via a two-\nstep poll-and-pool sampling module. However, most works\noperate on each isolated image and do not exploit tempo-\nral cues from continuous visual streams. Inspired by the\nability of Transformer to model long-term dependencies in\nvideo sequence tasks [24], [25], [26], we propose a temporal\nTransformer model to leverage rich temporal cues from\ncontinuous events and adjacent frames.\n2.4 Multimodal Fusion for Events and Frames\nSome computer vision tasks (e.g., video reconstruction [12],\n[58], [59], [60], object detection [13], [14], object tracking [61],\n[62], depth estimation [11], and SLAM [63], [64], [65]) have\nsought to integrate two complementary modalities of events\nand frames. For example, JDF [14] adopts the Dempster-\nShafer theory to fuse events and frames for object detection.\nA recurrent asynchronous multimodal network [11] is in-\ntroduced to fuse events and frames for depth estimation.\nIn fact, most fusion strategies (e.g., post-processing, con-\ncatenation, and averaging) canâ€™t distinguish the degradation\ndegree of different modalities and regions, resulting in the\nfailure to eliminate unimodal degradation completely (see\nSection 2.2). More importantly, most fusion strategies are\nsynchronized with event and frame streams, which causes\nthe joint output frequency to be limited by the sampling\nrate of conventional frames. Obviously, the limited inference\nfrequency canâ€™t meet the needs of fast object detection in\nreal-time high-speed scenarios, so we seek to utilize the high\ntemporal resolution advantage of event cameras and make\nuse of events and frames. In RAMNet [11], RNN is utilized\nto introduce an asynchronous fusion method that allows\ndecoding the task variable at any time. Nevertheless, to the\nbest of our knowledge, there are no prior attention-based\nworks involving asynchronous fusion strategy for events\nand frames in the object detection task. Thus, we design\na novel asynchronous attention-based fusion module that\neliminates unimodal degradation in two modalities with\nhigh inference frequency.\n3 PKU-DAVIS-SOD D ATASET\nThis section first presents the details of how to build our\ndataset (Section 3.1). Then, we give detailed statistics to bet-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 4\nTABLE 1: The number of labeled frames and bounding\nboxes in each set of our PKU-DAVIS-SOD dataset.\nName Normal Motion blur Low-light\nframes boxes frames boxes frames boxes\nTraining 115104 528944 31587 87056 22826 55343\nValidation 31786 155778 9763 24828 8892 14143\nTest 35459 150492 11859 44757 8729 18807\nTotal 182349 835214 53209 156641 40447 88293\nter understand the newly built dataset (Section 3.2). Finally,\nwe make a comparison of related datasets (Section 3.3).\n3.1 Data Collection and Annotation\nThe goal of this dataset is to offer a dedicated platform for\nthe training and evaluation of streaming object detection\nalgorithms. Thus, a novel multimodal vision sensor (i.e.,\nDAVIS346, resolution 346 Ã—260) is used to record multiple\nhybrid sequences in a driving car (see Fig. 2). The event\ncamera simultaneously outputs events with high temporal\nresolution and conventional frames with 25 FPS. Mostly,\nwe fix the DAVIS346 in a driving car (see Fig. 2(a)), and\nrecord the sequences while the car is driving on city roads.\nNevertheless, it is difficult to capture high-speed motion\nblur owing to the relative speed between vehicles on city\nroads. For the convenience of acquiring high-speed objects,\nwe additionally provide some sequences in which the cam-\nera is set at the flank of the road. The raw recordings\nconsider velocity distribution, light condition, category di-\nversity, object scale, etc. To provide manual bounding boxes\nin challenging scenarios (e.g., high-speed motion blur and\nlow-light), grayscale images are reconstructed from asyn-\nchronous events using E2VID [66] at 25 FPS when RGB\nframes are of low quality. When the objects are not visible\nin all three modalities (i.e., RGB frames, event images, and\nreconstructed frames), they can still be labeled because\nour PKU-DAVIS-SOD dataset provides continuous visual\nstreams, and we can obtain information about the unclear\nobjects from nearby images. Furthermore, some unclear\nobjects in a single image may be visible in a piece of video.\nAfter the temporal calibration, we first select three common\nand important object classes (i.e., car, pedestrian, and two-\nwheeler) in our daily life. Then, all bounding boxes are\nannotated from RGB frames or synchronized reconstructed\nimages by a well-trained professional team.\n3.2 Dataset Statistics\nPKU-DAVIS-SOD dataset provides 220 hybrid driving se-\nquences and labels at a frequency of 25 Hz. As a result, this\ndataset contains 276k timestamps (i.e., labeled frames) and\n1080.1k bounding boxes in total. Afterward, we split them\ninto 671.3k for training, 194.7k for validation, and 214.1k for\ntesting. Table 1 shows the number of bounding boxes in each\nset. Besides, we further analyze the attributes of the newly\nbuilt dataset from the four following perspectives.\nCategory Diversity . Fig. 3(a) displays the number distri-\nbutions of three types of labeled objects (i.e., car, pedestrian,\nand two-wheeler) in our PKU-DAVIS-SOD dataset. We can\nfind that the numbers of cars, pedestrians, and two-wheelers\nin each set (i.e., training, validation, and testing) are (580340,\n0\n100000\n200000\n300000\n400000\n500000\n600000\nCar Pedestrian Two-wheeler\nTrain Validation Test\n25%\n13%62%\nMedium\nLarge\nSmall\n13%\n87%\nNormal-speed\nHigh-speed\n 8%\n92%\nNormal-light\nLow-light\n(a) Category diversity\n0\n100000\n200000\n300000\n400000\n500000\n600000\nCar Pedestrian Two-wheeler\nTrain Validation Test\n25%\n13%62%\nMedium\nLarge\nSmall\n13%\n87%\nNormal-speed\nHigh-speed\n 8%\n92%\nNormal-light\nLow-light (b) Object scale\n0\n100000\n200000\n300000\n400000\n500000\n600000\nCar Pedestrian Two-wheeler\nTrain Validation Test\n25%\n13%62%\nMedium\nLarge\nSmall\n13%\n87%\nNormal-speed\nHigh-speed\n 8%\n92%\nNormal-light\nLow-light\n(c) Velocity distribution\n0\n100000\n200000\n300000\n400000\n500000\n600000\nCar Pedestrian Two-wheeler\nTrain Validation Test\n25%\n13%62%\nMedium\nLarge\nSmall\n13%\n87%\nNormal-speed\nHigh-speed\n 8%\n92%\nNormal-light\nLow-light (d) Light intensity\nFig. 3: Data statics in our PKU-DAVIS-SOD dataset. (a) The\ndistributions of three types of objects (i.e., car, pedestrian,\nand two-wheeler). (b)-(d) The proportions of object scales,\nmoving speeds, and light intensities.\n162894, 193118), (34744, 5800, 5680), and (67599, 28400,\n19144), respectively. For intuition, the ratios of the above\nnumbers are approximately 3.5 : 1 : 1.2, 6 : 1 : 1, and 3.5 : 1.5\n: 1, respectively.\nObject Scale . Fig. 3(b) shows the proportions or object\nscales in our PKU-DAVIS-SOD dataset. The object height is\nused to reflect its scale since the height and the scale are\nclosely related in driving scenes [67]. We broadly classify all\nobjects into three scales (i.e., large, medium, and small). The\nmedium scale varies from 20 pixels to 80 pixels, the small\nrefers to the object less than 20 pixels in height, and the large\nscale means the object height is greater than 80 pixels.\nVelocity Distribution . As illustrated in Fig. 3(c), we di-\nvide the motion speed level into normal-speed and high-\nspeed. Event camera, offering high temporal resolution, can\ncapture high-speed moving objects, such as a rushing car.\nOn the contrary, RGB frames may result in motion blur\ndue to the limited frame rate. In the process of dividing\nthe dataset, we use video as the basic unit. If a sequence\ncontains lots of motion blur scenarios, it is classified as high-\nspeed, otherwise, it would be classified as normal-speed\ninstead. Fig. 3(c) shows that the proportion of high-speed\nscenarios is 13%, and the remaining involving normal-speed\nobjects is 87%.\nLight Intensity. Since the raw visual streams are recorded\nfrom daytime to night, it covers the diversity in illumination\nvariance. The sequences belonging to the low-light scenario\nare generally collected in low-light conditions (night, rainy\ndays, tunnels, etc.) and their RGB frames consist mainly\nof low-light scenes. Thus, we can easily judge the light\ncondition by viewing RGB frames with our eyes. Fig. 3(d)\nillustrates that the proportions of normal light and low-light\nare 92% and 8%, respectively.\nFor better visualization, we show representative samples\n(see Fig. 4) including RGB frames, event images, and DVS\nreconstructed images, which cover the category diversity,\nobject scale, velocity, and light change.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 5\n(a) Category diversity (a) Object scale (c) Velocity distribution (d) Light change\n(a) Category diversity\n(a) Category diversity (b) Object scale (c) Velocity distribution (d) Light change (b) Object scale\n(a) Category diversity (a) Object scale (c) Velocity distribution (d) Light change (c) Velocity distribution\n(a) Category diversity (a) Object scale (c) Velocity distribution (d) Light change (d) Light change\nFig. 4: Representative samples of our PKU-DAVIS-SOD dataset. The three rows from top to bottom refer to RGB frame,\nevent images, and DVS reconstructed images from asynchronous events using E2VID [66].\nTABLE 2: Comparison with related object detection datasets using event cameras. Note that, our PKU-DAVIS-SOD dataset\nis the first large-scale multimodal neuromorphic object detection dataset that provides temporally long-term asynchronous\nevents, conventional frames, and manual labels at 25 Hz.\nDataset Year Venue Resolution Modality Classes Boxes Label Frequency High-speed Low-light\nPKU-DDD17-CAR [14] 2019 ICME 346 Ã—260 Events, Frames 1 3155 Manual 1 Hz âœ— âœ“\nTUM-Pedestrian [13] 2019 ICRA 240 Ã—180 Events, Frames 1 9203 Manual 1 Hz âœ— âœ—\nPedestrian Detection [41] 2019 FNR 240 Ã—180 Events 1 28109 Manual 1 Hz âœ— âœ—\nGen1 Detection [33] 2020 arXiv 304 Ã—240 Events 2 255k Pseudo 1, 4 Hz âœ— âœ“\n1 Mpx Detection [34] 2020 NIPS 1280 Ã—720 Events 3 25M Pseudo 60 Hz âœ— âœ“\nDAD [16] 2021 ICIP 346 Ã—260 Events, Frames 1 6427 Manual 1 Hz âœ— âœ“\nPKU-DAVIS-SOD 2022 Ours 346 Ã—260 Events, Frames 3 1080.1k Manual 25 Hz âœ“ âœ“\n3.3 Comparison with Other Datasets\nTo clarify the advancements of the newly built dataset,\nwe compare it with some related object detection datasets\nusing event cameras in Table 2. Apparently, our PKU-\nDAVIS-SOD dataset is the first large-scale and open-source\nmultimodal neuromorphic object detection dataset 1. In\ncontrast, two publicly large-scale datasets (i.e., Gen1 Detec-\ntion [33] and 1 Mpx Detection [34]) only provide temporally\nlong-term event streams, thus it is difficult for them to\nserve high-precision object detection, especially in static\nor extremely slow motion scenarios. Whatâ€™s more, TUM-\nPedestrian dataset [13] and Pedestrian Detection dataset [41]\nprovide event streams for pedestrian detection, but they\nhave not yet been publicly available. DAD dataset [16],\nfollowed by PKU-DDD17-CAR dataset [14], offers isolated\nframes and event temporal bins rather than temporally\ncontinuous visual streams.\nAll in all, such a novel bio-inspired multimodal camera\nand professional design with high labor intensity enable our\nPKU-DAVIS-SOD to be a competitive dataset with multiple\ncharacteristics: (i) High temporal sampling resolution with 12\nMeps from event streams ; (ii) High dynamic range property\nwith 120 dB from event streams , (iii) Two temporally long-term\nvisual streams with labels at 25 Hz , (iv) Real-world scenarios\nwith abundant diversities in object category, object scale, moving\nvelocity, and light change.\n4 P RELIMINARY AND PROBLEM DEFINITION\nThis section first presents the details of DAVIS camera work-\ning principle (Section 4.1). Then, we formulate the definition\n1. https://www.pkuml.org/research/pku-davis-sod-dataset.html\nof streaming object detection (Section 4.2).\n4.1 DAVIS Sampling Principle\nDAVIS is a multimodal vision sensor that simultaneously\noutputs two complementary modalities of asynchronous\nevents and frames under the same pixel array. Due to\nthe complementarity of events and frames, the output of\nthe DAVIS camera contains more information, making the\nDAVIS camera naturally surpass unimodal sensors like\nconventional RGB cameras and event-based cameras. In\nparticular, the core event camera [68], namely DVS, has\nindependent pixels that react to changes in light intensity\nR(u, t) with a stream of events. More specifically, an event\nen is a four-attribute tuple (xn, yn, tn, pn) using addressing\nevent representation (AER) [69], triggered for the pixel\nu = ( xn, yn) at the timestamp tn when the log-intensity\nchanges over the pre-defined threshold Î¸th. This process can\nbe depicted as:\nlnR(un, tn) âˆ’ lnR(un, tn âˆ’ âˆ†tn) = pnÎ¸th, (1)\nwhere âˆ†tn is the temporal sampling interval at a pixel, the\npolarity pn âˆˆ {âˆ’1, 1} refers to whether the brightness is\ndecreasing or increasing.\nIntuitively, asynchronous events appear as sparse and\ndiscrete points [70] in the spatiotemporal domain, which can\nbe described as follows:\nS (x, y, t) =\nNeX\nn=1\npnÎ´ (x âˆ’ xn, yâˆ’ yn, tâˆ’ tn) , (2)\nwhere Ne is the event number during the spatiotempo-\nral window. Î´ (Â·) refers to the Dirac delta function, withR\nÎ´ (t) dt = 1 and Î´ (t) = 0, âˆ€t Ì¸= 0.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 6\n4.2 Streaming Object Detection\nLet S(x, y, t) and I={I1, ..., IN } are two complementary\nmodalities of asynchronous events and frame sequence from\na DAVIS camera. In general, to make the asynchronous\nevents compatible with deep learning methods, a con-\ntinuous event stream needs to be divided into K event\ntemporal bins S = {S1, ..., SK}. For the current times-\ntamp ti, the spatiotemporal location information of objects\n(i.e., bounding boxes) can be calculated using adjacent\nframes {Iiâˆ’n, ..., Ii}, nâˆˆ [0, N] and multiple temporal bins\n{Siâˆ’k, ..., Si}, kâˆˆ [0, K], it can be formulated by:\nBi = D ({Iiâˆ’n, ..., Ii}, {Siâˆ’k, ..., Si}) , (3)\nwhere Bi = {(xi,j, yi,j, wi,j, hi,j, ci,j, pi,j, ti)}jâˆˆ[1,J] is a list\nof J bounding boxes in the timestamp ti. More specifically,\n(wi,j, hi,j) are the width and the height of the j bounding\nbox, (xi,j, yi,j) are the corresponding upper-left coordinates,\nci,j and pi,j are the predicted class and the confidence score\nof the j bounding box, respectively. The function D is the\nproposed streaming object detector, which can leverage rich\ntemporal cues from n+1 adjacent frames and k+1 temporal\nbins. The parameters n and k determine the length of\nmining temporal information and affect the fusion strategy\nbetween two heterogeneous visual streams.\nGiven the ground-truth Â¯B =\n\b Â¯B1, ...,Â¯BN\n\t\n, we aim at\nmaking the output B = {B1, ..., BN } of the optimized\ndetector Ë†D to fit Â¯B as much as possible, and it can formulate\nthe following minimization problem as:\nË†D = arg min\nD\n1\nN\nNX\nn=1\nLD(Bi, Â¯Bi), (4)\nwhere LD is the designed loss function of the proposed\nstreaming object detector.\nNote that, this novel streaming object detector us-\ning events and frames works differently from the feed-\nforward object detection framework (e.g., YOLOs [48] and\nDETR [20]). It has two unique properties, which exactly\naccount for the nomenclature of â€streamingâ€: (i) Objects can\nbe accurately detected via leveraging rich temporal cues; (ii)\nEvent streams offering high temporal resolution can meet\nthe need of detecting objects at any time among continuous\nvisual steams and overcome the limited inference frequency\nfrom conventional frames. Consequently, following similar\nworks using point clouds [71], [72] and video streams [73],\nwe call our proposed detector as streaming object detector.\n5 M ETHODOLOGY\nIn this section, we first give an overview of our approach\n(Section 5.1). Besides, we briefly revisit the event represen-\ntation (Section 5.2) and the spatial Transformer (Section 5.3).\nThen, we present the details of how to exploit temporal\ncues from continuous visual streams (Section 5.4). Finally,\nwe design an asynchronous attention-based fusion strategy\nfor events and frames (Section 5.5).\n5.1 Network Overview\nThis work aims at designing a novel streaming object detec-\ntor with Transformer, termed SODFormer, which continu-\nously detects objects in an asynchronous way by integrating\nevents and frames. As illustrated in Fig. 5, our framework\nconsists of four modules: event representation, spatial Trans-\nformer, temporal Transformer, and asynchronous attention-\nbased fusion module. More precisely, to make asynchronous\nevents compatible with deep learning methods, the contin-\nuous event stream is first divided into event temporal bins\nS = {S1, ..., SK}, and each binSi can be converted into a 2D\nimage-like representation Ei (i.e., event embeddings [47]).\nThen, the spatial Transformer adopts the main structures\nof the Deformable DETR [21] to extract feature representa-\ntions from each sensing modality, which involves a feature\nextraction backbone (e.g., ResNet50 [74]) and the spatial\nTransformer encoder (STE). Besides, the proposed temporal\nTransformer contains the temporal Deformable Transformer\nencoder (TDTE) and temporal Deformable Transformer de-\ncoder (TDTD). The TDTE assigns the outputs of STE in the\ntemporal dimension, which can improve the accuracy of\nobject detection by leveraging rich temporal information.\nMeanwhile, the proposed asynchronous attention-based fu-\nsion module exploits the attention mechanism to generate\na fused feature, which can eliminate unimodal degradation\nin two modalities with a high inference frequency. Finally,\nThe TDTD integrates object queries and the fused feature to\npredict the bounding boxes Bi.\n5.2 Event Representation\nWhen using deep learning methods to process asyn-\nchronous events, spatiotemporal point sets are usually con-\nverted into successive measurements. In general, the kernel\nfunction [47] is usually used to map the event temporal bin\nSi into an event embedding Ei, which should ideally exploit\nthe spatiotemporal information from asynchronous events.\nAs a result, we can formulate this mapping as follows:\nEi =\nX\nenâˆˆâˆ†T\nK(x âˆ’ xn, yâˆ’ yn, tâˆ’ tn), (5)\nwhere the designed kernel function K(x, y, t) can be deep\nneural networks or handcrafted operations.\nIn this work, we attempt to encode the event temporal\nbin into three existing event representations (i.e., event im-\nages [17], voxel grids [75] and sigmoid representation [39])\nowning to an accuracy-speed trade-off. Actually, any event\nrepresentation can be an alternative because our SOD-\nFormer provides a generic interface accepting various input\ntypes of the event-based object detector.\n5.3 Revisiting Spatial Transformer\nIn DETR [20], the multi-head self-attention (MSA) combines\nmultiple attention heads in parallel to increase the feature\ndiversity. To be concrete, the feature maps are first extracted\nvia CNN-based backbone. Then, Xq and Xk are derived\nfrom these feature maps as query and key-value in attention\nmechanism with both sizes of (HW, d). Let q refer to a query\nelement with the feature map Xq, and k indexes a key-\nvalue element with the feature map Xk. The MSA operation\nintegrates the outputs of M attention heads as:\nMSA (Xq, Xk) =\nMX\nm=1\nWm\nhX\nAmqk Â· Wâ€²\nmXk\ni\n, (6)\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 7\nEvent \ntensors\nğ‘¦ğ‘¦\nğ‘¥ğ‘¥\nğ‘¡ğ‘¡\nBackbone\nğ‘†ğ‘†ğ‘–ğ‘–\nğ¼ğ¼ğ‘–ğ‘–\nFrame flow\nEvent flow\nMSA\nTDMSA\nFFN\nï¿½ï¿½ï¿½\nMSA\nTDMSA\nFFN\nMSA\nDMSA\nFFNï¿½ï¿½ï¿½\nMSA\nFFN\nDMSA\nTemporal Transformer decoder\nï¿½ï¿½ï¿½\nObject queries\nBoxes and classes\nğ‘©ğ‘©ğ‘–ğ‘–\nFrame flow\nEvent flow\nQuery flow\nFused flow\nObject queries\nğ¼ğ¼ğ‘–ğ‘–âˆ’ğŸğŸ\nğ¼ğ¼ğ‘–ğ‘–âˆ’ğ’ğ’\nğ‘†ğ‘†ğ‘–ğ‘–âˆ’1\nğ‘†ğ‘†ğ‘–ğ‘–âˆ’ğ‘˜ğ‘˜\nğ¼ğ¼ğ’Šğ’Šâˆ’ğ’ğ’, â€¦ , ğ¼ğ¼ğ’Šğ’Š\nMSA\nTDMSA\nFFN\nï¿½ï¿½ï¿½ MSA\nTDMSA\nFFN\nğ¸ğ¸ğ’Šğ’Šâˆ’ğ‘˜ğ‘˜, â€¦ , ğ¸ğ¸ğ’Šğ’Š Temporal Transformer encoder\nğ¹ğ¹ğ‘–ğ‘–âˆ’ğ‘˜ğ‘˜\nğ‘†ğ‘† , â€¦ , ğ¹ğ¹ğ‘–ğ‘–âˆ’1\nğ‘†ğ‘†\nğ¹ğ¹ğ‘–ğ‘–\nğ‘†ğ‘†\nğ¹ğ¹ğ‘–ğ‘–âˆ’ğ’ğ’\nğ¼ğ¼ , â€¦ , ğ¹ğ¹ğ‘–ğ‘–âˆ’ğŸğŸ\nğ¼ğ¼\nğ‘¡ğ‘¡\nAttention \nweight mask\nAsynchronous attention-based \nfusion module\nğ‘¡ğ‘¡\nğ‘¡ğ‘¡\nBackbone\nSTE\nSTE\nğ¹ğ¹ğ‘–ğ‘–\nğ¼ğ¼\nğ‘­ğ‘­ğ’‡ğ’‡\nğ‘¿ğ‘¿ğ‘°ğ‘°\nğ‘¿ğ‘¿ğ‘ºğ‘º\nğ‘ğ‘ğ‘ğ‘\nFig. 5: The pipeline of the proposedstreaming object detection with Transformer (SODFormer). The event stream is first split into\nevent temporal bins and then encoded into event embeddings [47]. Then, we use a modal-specific shared backbone (e.g.,\nResNet50 [74]) to extract features from frames and event embeddings, respectively. Meanwhile, our temporal Transformer\nencoder links the outputs of the spatial Transformer encoder (STE) for each stream. Our asynchronous attention-based\nfusion module exploits the attention mechanism to generate a fused representation. Finally, the designed temporal\nTransformer decoder integrates object queries and the fused flow to predict the final bounding boxes.\nwhere m indexes the single attention head, Wm and Wâ€²\nm\nare learnable weights. Amqk = exp(\nXT\nq WT\nmqWmkXkâˆšdk\n) is the\nattention weight, in which Wmq and Wmk are also learnable\nweights, dk is a scaling factor.\nTo achieve fast convergence and high computation effi-\nciency, Deformable DETR [21] designs a deformable multi-\nhead self-attention (DMSA) operation to attend the local\nL sampling points instead of all pixels in feature map\nX. Specifically, DMSA first determines the corresponding\nlocations of each query in other feature maps, which we\nrefer to as reference points, and then adds the learnable\nsampling offsets to reference points to obtain the locations\nof sampling points. It can be described as:\nDMSA (Xq, X, Pq) =\nMX\nm=1\nWm\n\" LX\nl=1\nAmlqÂ·\nWâ€²\nmX(Pq + âˆ†Pmlq)] ,\n(7)\nwhere Pq is a 2D reference point, âˆ†Pmlq denotes the sam-\npling offset relative toPq, and Amlq is the learnable attention\nweight of the lth point in mth self-attention head.\nIn this study, we adopt the encoder of the Deformable\nDETR as our spatial Transformer encoder (STE) to extract\nfeatures from each visual stream. Meanwhile, we further\nextend the spatial DMSA strategy to the temporal domain\n(Section 5.4), which leverages rich temporal cues from con-\ntinuous event stream and adjacent frames.\n5.4 Temporal Transformer\nThe temporal Transformer aggregates multiple spatial fea-\nture maps from the spatial Transformer and generates the\npredicted bounding boxes. It includes two main modules:\ntemporal deformable Transformer encoder (TDTE) (Sec-\ntion 5.4.1) and temporal deformable Transformer decoder\n(TDTD) (Section 5.4.2).\n5.4.1 Temporal Deformable Transformer Encoder\nOur TDTE aims at aggregating multiple spatial feature maps\nfrom the spatial Transformer encoder (STE) using the tem-\nporal deformable multi-head self-attention (TDMSA) oper-\nation and then generating a spatiotemporal representation\nvia the designed multiple stacked blocks.\nThe core idea of the proposed TDMSA operation is\nthat the temporal deformable attention only attends to the\nlocal sampling points in the spatial domain and aggregates\nall sampling points in the temporal domain. Obviously,\nTDMSA directly extends from single feature map Xi (i.e.,\nDMSA [21]) to multiple feature maps X={Xiâˆ’k, ..., Xi} in\nthe temporal domain (see Fig. 6). Meanwhile, TDMSA exists\na total M attention heads for each temporal deformable\nattention operation, and it can be expressed as:\nTDMSA (Xq, X, Pq) =\nMX\nm=1\nWm\nï£®\nï£°\nLX\nl=1\niâˆ’1X\nj=iâˆ’k\nAmljqÂ·\nWâ€²\nmXj(Pjq + âˆ†Pmljq)] ,\n(8)\nwhere Pjq is a 2D reference point in jth feature map Xj.\nAmljq and âˆ†Pmljq are the learnable attention weight and\nthe sampling offset of the lth sampling point from the jth\nfeature map and the mth attention head.\nTDTE includes multiple consecutive stacked blocks (see\nFig. 5). Each block performs MSA, TDMSA, and feed-\nforward network (FFN) operations. Besides, each operation\nis followed by the dropout [76] and the layer normaliza-\ntion [77]. Specifically, TDTE first takes Xi as both the query\nand the key-value via MSA in the timestamp ti. Then,\nTDMSA integrates the current feature map Xi and adjacent\nfeature maps {Xiâˆ’k, ..., Xiâˆ’1} to leverage rich temporal\ncues. Finally, FFN is used to output a spatiotemporal rep-\nresentation, shown as XI or XS in Fig. 5. Thus, our TDTE\ncan be formulated as:\nË†Zi = MSA(Xi, Xi),\nËœZi = TDMSA( Ë†Zi, {Xiâˆ’k, ..., Xiâˆ’1}, Pq),\nXS = FFN( ËœZi),\n(9)\nwhere Ë†Zi and ËœZi are the outputs of MSA and TDMSA\noperations in our TDTE, respectively. Note that, though\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 8\nSampling offsets âˆ†ğ‘ƒğ‘šğ‘™ğ‘˜ğ‘\nReference \npoint ğ‘ƒğ‘˜ğ‘\nQuery feature \nğ‘‹ğ‘\nÃ—ğ‘€\nğ‘‹ğ‘˜âˆ’1ğ‘‹ğ‘˜âˆ’2\nğ‘¡\nAttention weight\nâŠ—\nâŠ—\nâŠ—\nâŠ•\nğ´ğ‘šğ‘™ğ‘˜ğ‘\nLinear Softmax Multi-head\nğ‘‹ğ‘˜\nFig. 6: The architecture of the proposed temporal de-\nformable multi-head self-attention (TDMSA).\nframe flow and event flow are processed the same way in\nthis step, there are two TDTEs that do not share weights\nprocessing frame flow and event flow separately.\n5.4.2 Temporal Deformable Transformer Decoder\nThe goal of our TDTD is to output the final result via a\nsequence prediction problem. The inputs of TDTD contain\nboth fused feature maps from our fusion module (Sec-\ntion 5.5) and object queries. The object queries are a small\nfixed number (denoted as Nq) of learnable positional em-\nbeddings. They serve as the â€queryâ€ part of the input to the\nTDTD. Each stacked block in the decoder consists of three\noperations (i.e., MSA, DMSA, and FFN). In the MSA, Nq\nobject queries interact with each other in parallel via multi-\nhead self-attention at each decoder layer, where the query\nelements and key-value elements are both the object queries.\nAs illustrated in Fig. 5, the DMSA [21] transforms the query\nflows and the fused features from two streams into an\noutput embedding. Ideally, each object query learns to focus\non a specific region in images and extracts region features\nfor final detection in this process, similar to the anchors in\nFaster R-CNN [56] but is simpler and more flexible. Finally,\nthe FFN independently processes each output embedding to\npredict Nq bounding boxes B=\n\b\nB1, B2, Â·Â·Â· , BNq\n\t\n. Notably,\nNq is typically greater than the actual number of objects\nin the current scene, so the predicted bounding boxes can\neither be a detection or a â€no objectâ€ (labeled as Ï•) as\nDETR [20] does. In particular, our TDTD achieves more\nefficient and faster converging by replacing MSA [20] with\nDMSA.\n5.5 Asynchronous Attention-Based Fusion Module\nTo integrate two heterogeneous visual streams, the most\nexisting fusion strategies [13], [14], [15], [16], [49] usually\nfirst split asynchronous events into discrete temporal bins\nsynchronized with frames, and then fuse two streams via\nthe post-processing or feature aggregation (e.g., averaging\nand concatenation). However, these synchronized fusion\nstrategies may have two limitations: (i) The post-processing\nor feature aggregation operations cannot distinguish the\ndegradation degree of different modalities and regions,\nwhich means they are difficult to eliminate the unimodal\ndegradation thoroughly and thus incur the bottleneck of\nperformance improvement; (ii) The joint output frequency\nmay be limited by the sampling rate of conventional frames\nğ‘¡\nğ‘¡\nFrame flow\nEvent flow\nğ’•ğ’‹\nğ’•ğ‘–\nğ»ğ‘Š Ã—â„ğ‘‘\nğ»ğ‘Š Ã—â„ğ‘‘\nâˆ†ğ‘¡ = ğ‘¡ğ‘– âˆ’ğ‘¡ğ‘—\nğ»ğ‘Š Ã—ğ»ğ‘Š ğ»ğ‘Š Ã—1\nğ»ğ‘Š Ã—ğ»ğ‘Š\nğ»ğ‘Š Ã—1Softmax\nğ‘€ğ‘„ğ¼\nğ‘€ğ¾ğ¼\nğ‘€ğ‘„ğ‘ \nğ‘€ğ¾ğ‘ \nğ‘´ğ‘º\nğ‘´ğ‘°\nğ‘­ğ’‡\nğ‘¿ğ‘°\nğ‘¿ğ‘º\nâ€¦\nâ€¦\nFig. 7: Overview of the proposed asynchronous attention-\nbased fusion module. It can fuse two heterogeneous data at\ndifferent sampling timestamps and exploit the high tempo-\nral resolution of event streams.\n(e.g., 25 Hz), which fails to meet the needs of fast object\ndetection in real-time high-speed scenarios. Therefore, we\ndesign an asynchronous attention-based fusion module to\novercome the two limitations mentioned above.\nOne key question of two heterogeneous visual streams is\nhow to achieve the high joint output frequency on demand\nfor a given task. In general, the high temporal resolution\nevent stream can be split into discrete event temporal bins,\nthe frequency of which can be flexibly adjusted to be much\nlarger than the conventional frame rate in real-time high-\nspeed scenarios. As shown in Fig. 7, the event flow has\nmore temporal sampling timestamps than the frame flow\nin an asynchronous manner. For the timestamp ti, XS is\nthe output of the FFN in the TDTE. The proposed fusion\nstrategy first searches the most recent sampling timestamp\ntj in the frame flow, which can be described as:\nj = arg min\nk\n|ti âˆ’ tk|, âˆ†t â‰¥ 0, (10)\nwhere âˆ†t = ti âˆ’ tk is the temporal difference between\nthe event sampling timestamp and the frame sampling\ntimestamp.\nAnother key question of two complementary data is\nhow to tackle the unimodal degradation that the feature\naggregation operations (e.g., averaging) fail to overcome\nand implement pixel-wise adaptive fusion for better perfor-\nmance. Unlike the previous weight mask by a multi-layer\nperceptron (MLP) [78], we compute the pixel-wise weight\nmask using the attention mechanism, which simplifies our\nmodel and endows it with stronger interpretability. Our\nfusion strategy starts with two intuitive assumptions: (i) the\nstronger the association between two pixels is, the greater\nthe attention weight between them will be, and (ii) the\npixels in degraded regions are supposed to have a weaker\nassociation with other pixels. Combining the two points\nabove, we conclude that the sum of attention weights be-\ntween a pixel and all other pixels represents the degradation\ndegree of that pixel to some extent, and thus can serve\nas the weight for that pixel during fusion. Next, we take\nevent flow as an example to illustrate how our attention\nweight mask module (see Fig. 5) obtains fusion weights. To\nimplement this, we first remove the softmax operation in\nthe standard self-attention to calculate the attention weights\nWS\nXY âˆˆ RHW Ã—HW between each pixel, and accumulate\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 9\nattention weights for each pixel x as WS\nx = P\ny WS\nxy, which\ncan be summarized as follows:\nWS\nX =\nX\ny\n(XS Ã— MQS ) Ã— (XS Ã— MKS )T\nâˆšdk\n, (11)\nwhere WS\nX âˆˆ RWH Ã—1 is the weight mask with the width\nW and the height H for event flow. MQS and MKS are\nlearnable matrix parameters. dk is a scaling factor in the\nself-attention [22]. As shown in Fig. 7, the aforementioned\nsteps are exactly the same for the process of frame flow.\nThen, we concatenate the weight masks along the pixel\ndimension and adopt the softmax to that dimension (i.e.,\nsoftmax of [WI\nj , WS\nj ], j = 1, 2, Â·Â·Â· , HW) to normalize the\npixel-wise attention weights at each pixel. Meanwhile, we\nfurther split the weight masks along the pixel dimension to\nobtain the frame weight masks MI and the event weight\nmasks MS as follows:\nMI, MS = split(softmax(\nï£®\nï£¯ï£°\nWI\n1 WS\n1\n...\n...\nWI\nHW WS\nHW\nï£¹\nï£ºï£»)). (12)\nFinally, we can obtain the fused feature map Ff by sum-\nming up the unimodal feature maps with the corresponding\npixel-wise weight masks by:\nFf = (MI âŠ™ XI) âŠ• (MS âŠ™ XS), (13)\nwhere âŠ• denotes the sum operation, and âŠ™ refers to the\nelement-wise multiply operation.\n6 E XPERIMENTS\nThis section first describes the experimental setting and\nimplementation details (Section 6.1). Then, the effective test\n(Section 6.2) is conducted to verify the validity of our SOD-\nFormer, which contains a performance evaluation in various\nscenarios and a comparison to related detectors. Meanwhile,\nwe further implement the ablation study (Section 6.3) to see\nwhy and how our approach works, where we investigate\nthe influences of each designed module and parameter\nsettings. Finally, the scalability test (Section 6.4) provides\ninterpretable explorations of our SODFormer.\n6.1 Experimental Settings\nIn this part, we will present the dataset, implementation\ndetails, and evaluation metrics as follows.\nDataset and Setup . Our PKU-DAVIS-SOD dataset is de-\nsigned for streaming object detection in driving scenes,\nwhich provides two heterogeneous and temporally long-\nterm visual streams with manual labels at 25 Hz (see\nSection 3). This newly built dataset contains three subsets\nincluding 671.3k labels for training, 194.7k for validation,\nand 214.1k for testing. Each subset is further split into three\ntypical scenarios (i.e., normal, low-light, and motion blur).\nImplementation Details. We select event images [17] as the\nevent representation and the ResNet50 [74] as the backbone\nto achieve a trade-off between accuracy and speed. All\nparameters of the backbone and the spatial Transformer\nencoder (STE) are shared among different temporal bins of\nthe same modality. We set the attention headM to 8 and the\nsampling point L to 4 for the deformable multi-head self-\nattention (DMSA) in Eq.(7) and the temporal deformable\nmulti-head self-attention (TDMSA) in Eq.(8). The temporal\naggregation size in TDMSA is set to 9 owning to the balance\nof the accuracy and the speed. During training, the height\nof an image is randomly resized to a number from [256: 576:\n32], and the width is obtained accordingly while maintain-\ning the aspect ratio. Similarly, for evaluation, all images are\nresized to 352 in height and 468 in width to maintain the\naspect ratio. Other data augmentation methods such as crop\nand horizontal flip are not utilized because our temporal\nTransformer requires accurate reference points. Following\nthe Deformable DETR [21], we adopt the matching cost and\nthe Hungarian loss for training with loss weights of 2, 5, 2\nfor classification, L1 and GIoU, respectively. All networks\nare trained for 25 epochs using the Adam optimizer [79],\nwhich is set with the initial learning rate of 2 Ã— 10âˆ’4, the\ndecayed factor of 0.1 after the 20th epoch, and the weight\ndecay of 10âˆ’4. All experiments are conducted on NVIDIA\nTesla V100 GPUs.\nEvaluation Metrics. To compare different approaches, the\nmean average precision (e.g., COCO mAP [80]) and running\ntime (ms) are selected as two evaluation metrics, which are\nthe most broadly utilized in the object detection task. In\nthe effective test, we give a comprehensive evaluation using\naverage precision with various IoUs (e.g., AP , AP 0.5, and\nAP0.75) and AP across different scales (i.e., AP S, APM, and\nAPL). In other tests, we report the detection performance\nusing the AP 0.5. Following the video object detection, we\ncompute the AP for each class and the mAP for three classes\nin all labeled timestamps. In other words, we evaluate the\nobject detection performance with the output frequency of\n25 Hz in every labeled timestamp.\n6.2 Effective Test\nThe objective of this part is to assess the validity of our SOD-\nFormer, so we implement two main experiments including\nperformance evaluation in various scenarios (Section 6.2.1)\nand comparison with state-of-the-art methods (Section 6.2.2)\nas follows.\n6.2.1 Performance Evaluation in Various Scenarios\nTo give a comprehensive evaluation of our PKU-DAVIS-\nSOD dataset, we report the quantization results (see Table 3)\nand the representative visualization results (see Fig. 8) from\nthree following perspectives.\nNormal Scenarios. Performance evaluations of all modali-\nties (i.e., frames, events, and two visual streams) in normal\nscenarios can be found in Table 3. Our single-modality base-\nline only processes frames or events without using the asyn-\nchronous attention-based fusion module. We can see that\nour baseline using RGB frames achieves better performance\nthan using events in normal scenarios. This is because the\nevent camera has a flaw with weak texture in the spatial\ndomain, thus only dynamic events (i.e., brightness changes)\nare hard to achieve high-precision recognition, especially in\nstatic or extremely slow motion scenes (see Fig. 8(a)). On the\ncontrary, RGB frames can provide static fine textures (i.e.,\nabsolute brightness) under ideal conditions. In particular,\nour SODFormer obtains the performance enhancement from\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 10\nTABLE 3: Performance evaluation of our PKU-DAVIS-SOD dataset in various scenarios. Our SODFormer integrates frames\nand events to detect objects in a continuous manner, and our baseline processes the single-modality (i.e., frames or events)\nwithout utilizing the asynchronous attention-based fusion module.\nScenario Modality AP50 mAP mAP 50 mAP75 mAPS mAPM mAPLCar Pedestrian Two-wheeler\nNormal\nEvents 0.440 0.220 0.451 0.147 0.371 0.090 0.072 0.268 0.526\nFrames 0.747 0.365 0.558 0.228 0.557 0.138 0.166 0.336 0.539\nFrames + Events 0.752 0.359 0.596 0.241 0.569 0.163 0.166 0.363 0.609\nMotion blur\nEvents 0.327 0.159 0.380 0.113 0.289 0.064 0.051 0.181 0.255\nFrames 0.561 0.303 0.394 0.163 0.419 0.096 0.100 0.201 0.365\nFrames + Events 0.570 0.285 0.441 0.183 0.432 0.125 0.109 0.230 0.387\nLow-light\nEvents 0.524 0.0002 0.294 0.093 0.273 0.039 0.075 0.183 0.286\nFrames 0.570 0.128 0.357 0.114 0.351 0.048 0.082 0.198 0.344\nFrames + Events 0.595 0.130 0.399 0.122 0.374 0.048 0.091 0.206 0.376\nAll\nEvents 0.424 0.188 0.390 0.128 0.334 0.071 0.065 0.210 0.348\nFrames 0.700 0.316 0.452 0.195 0.489 0.116 0.142 0.264 0.417\nFrames + Events 0.705 0.313 0.493 0.207 0.504 0.133 0.144 0.285 0.454\n(a) Normal\n (b) Motion blur\n (c) Low-light\nFig. 8: Representative visualization results on various scenarios of our PKU-DAVIS-SOD dataset. The four rows from top\nto bottom refer to our baseline using RGB frames, our baseline using event images, our SODFormer using RGB frames and\nDVS events, and ground truth labeled in event reconstructed images [66].\nthe single stream to two visual streams via introducing the\nasynchronous attention-based fusion module.\nMotion Blur Scenarios . By comparing normal scenarios\nand motion blur scenarios in Table 3, we can find that\nthe detection performance using events degrades less than\nthat utilizing frames. This can be explained by that RGB\nframes appear as motion blur in high-speed motion scenes\n(see Fig. 8(b)), resulting in a remarkable decrease in object\ndetection performance. Note that, by introducing the event\nstream, the performance of our SODFormer can appreciably\nenhance over our baseline using RGB frames.\nLow-Light Scenarios. As illustrated in Table 3, the perfor-\nmance of our baseline using RGB frames drops sharply by\n0.206 in low-light scenarios. Meanwhile, although the mAP\nof DVS events also drops by 0.098 in absolute term, it is sig-\nnificantly less compared to that of RGB frames (i.e., 0.098 vs.\n0.206). On a relative basis, there is also an evident difference\nas the mAP of RGB frames and DVS events drop by 0.370\nand 0.264, respectively. This may be caused by the fact that\nthe event camera has the advantage of HDR to sense moving\nobjects in extreme light conditions (see Fig. 8(c)). Notably,\nonly the AP 50 of â€Carâ€ objects increases while the others\ndecrease. We attribute this to the different image quality of\ndifferent classes of objects in the event modality. Intuitively,\nâ€Carâ€ objects tend to have more distinct outlines and higher\nimage quality. As a result, â€Carâ€ objects are less distributed\nby the increase of noise in low-light scenes [82] and thus\nachieve comparable performance to that in normal scenes.\nMore specifically, our SODFormer outperforms the baseline\nusing RGB frames by a large margin (0.374 versus 0.351) in\nlow-light scenarios.\nIn summary, our SODFormer consistently obtains better\nperformance than the single-modality methods in three\nscenarios meanwhile keeping relatively comparable com-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 11\nTABLE 4: Comparison with state-of-the-art methods and our baselines on our PKU-DAVIS-SOD dataset. Our SODFormer,\nmaking complementary use of RGB frames and DVS events, outperforms seven state-of-the-art methods involving event\ncameras, four using RGB frames, and two joint object detectors. * denotes that a method leverages temporal cues.\nModality Method Input representation Backbone Temporal mAP 50 Runtime (ms)\nEvents\nSSD-events [38] Event image SSD No 0.221 7.2\nNGA-events [6] Voxel grid YOLOv3 No 0.232 8.0\nYOLOv3-RGB [48] Reconstructed image YOLOv3 No 0.244 178.51\nFaster R-CNN [56] Event image R-CNN No 0.251 74.5\nDeformable DETR [21] Event image DETR No 0.307 21.6\nLSTM-SSD* [81] Event image SSD Yes 0.273 22.7\nASTMNet* [43] Event embedding Rec-Conv-SSD Yes 0.291 21.3\nOur baseline* Event image Our spatiotemporal Transformer Yes 0.334 25.0\nFrames\nFaster R-CNN [56] RGB frame R-CNN No 0.443 75.2\nYOLOv3-RGB [48] RGB frame YOLOv3 No 0.426 7.9\nDeformable DETR [21] RGB frame DETR No 0.461 21.5\nLSTM-SSD* [81] RGB frame SSD Yes 0.456 22.4\nOur baseline* RGB frame Our spatiotemporal Transformer Yes 0.489 24.9\nEvents + Frames\nMFEPD [13] Event image + RGB frame YOLOv3 No 0.438 8.2\nJDF [14] Channel image + RGB frame YOLOv3 No 0.442 8.3\nOur SODFormer Event image + RGB frame Deformable DETR Yes 0.504 39.7\nputational speed. By introducing DVS events, the mAP\non our PKU-DAVIS-SOD dataset improves an average of\n1.5% over using RGB frames. Whatâ€™s more, representative\nvisualization results in Fig. 8 show that RGB frames fail to\ndetect objects in high-speed motion blur and low-light sce-\nnarios. However, the event camera, offering high temporal\nresolution and HDR, provides new insight into addressing\nthe shortages of conventional cameras.\n6.2.2 Comparison with State-of-the-Art Methods\nWe will instigate why and how our SODFormer works from\nthe following three perspectives.\nEvaluation on DVS Modality . To evaluate our temporal\nTransformer for DVS events, we compare our baseline*\n(i.e., our SODFormer without the asynchronous attention-\nbased fusion module) with five feed-forward event-based\nobject detectors (i.e., event image for SSD [38], voxel grid\nfor YOLOv3 [6], reconstructed image for YOLOv3 [48],\nevent image for Faster R-CNN [56], and event image for\nDeformable DETR [21]) and two recurrent object detectors\n(event image for LSTM-SSD [81] and event embedding for\nASTMNet [43]) that leverages temporal cues. As shown in\nTable 4, our baseline*, utilizing event images for our tem-\nporal Transformer, achieves significant improvement over\nall these object detectors. Besides, our other baseline, using\nE2VID [66] to reconstruct frames, performs better than other\ninput representations (e.g., event image and voxel grid), but\na long time is spent on the first stage of image reconstruction\nand then on the second stage of object detection.\nEvaluation on RGB Modality . We present four state-of-\nthe-art object detectors for RGB frames including (i) Single\nRGB frame for Faster R-CNN, (ii) Single RGB frame for\nYOLOv3, (iii) Single RGB frame for Deformable DETR, (iv)\nSequential RGB frames for LSTM-SSD to compare them\nwith our baseline* that utilizes sequential RGB frames for\nour spatiotemporal Transformer. Compared to three object\ndetectors utilizing single RGB frames, our baseline* obtains\nthe best performance via introducing the temporal Trans-\nformer for RGB frames (see Table 4). Furthermore, it obtains\nbetter performance than LSTM-SSD while maintaining com-\nparable computational speed. This is because the designed\nspatiotemporal Transformer is more effective than the stan-\ndard convolutional-recurrent operation. In other words, the\nresults agree with our motivation that leveraging rich tem-\nporal cues for streaming object detection is beneficial.\nBenefit From Multimodal Fusion . From Table 4, we make\na comparison of our SODFormer between two existing\njoint object detectors (e.g., MFEPD [13] and JDF [14]). Ap-\nparently, our SODFormer outperforms two competitors by\na large margin. We can find that our SODFormer, incor-\nporating the temporal Transformer and the asynchronous\nattention-based fusion module, outperforms four state-of-\nthe-art detectors and our eight baselines. More precisely,\nby introducing DVS events, our SODFormer has a 1.5%\nimprovement over the best baseline using sequential RGB\nframes. Compared with the best competitor using DVS\nevents, our SODFormer truly shines (0.504 versus 0.334),\nwhich indicates that RGB frames with fine textures may be\nvery significant for high-precision object detection.\nWe further present some visualization results on our\nPKU-DAVIS-SOD dataset in Fig. 9. Note that, our SOD-\nFormer outperforms the single-modality methods and the\nbest multi-modality competitor (i.e., JDF [14]). Unfortu-\nnately, RGB frames fail to detect objects in high-speed or\nlow-light scenes. Much to our surprise, our SODFormer can\nperform robust detect objects in challenging situations, due\nto its high temporal resolution and HDR properties that are\ninherited from DVS events, as well as its fine texture that\ncomes from RGB frames.\n6.3 Ablation Test\nIn this section, we implement an ablation study to in-\nvestigate how parameter setting and each design choice\ninfluence our SODFormer.\n6.3.1 Contribution of Each Component\nTo explore the impact of each component on the final per-\nformance, we choose the feed-forward detector (i.e., using\nRGB frame for Deformable DETR [21]) as the baseline. As\nillustrated in Table 5, three methods, namely (a), (b), and our\nSODFormer, utilize temporal Transformer, averaging fusion\nmodule and asynchronous attention-based fusion module\nrespectively, consistently achieve higher performance on our\nPKU-DAVIS-SOD dataset than the baseline. More specif-\nically, method (a), adopting the temporal Transformer to\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 12\nRGB frame NGA-events JDF SODFormer (Ours) Ground truth\n(a)\n(b)\n(d)\n(c)\nReconstructed image\nFig. 9: Representative examples of different object detection results on our PKU-DAVIS-SOD dataset. (a) Moving cars in\nnormal scenario. (b) Rushing cars in overexposure scenario. (c) High-speed running two-wheelers with motion blur. (d)\nTraveling two-wheelers in low-light scenario.\nTABLE 5: The contribution of each component to our SOD-\nFormer on our PKU-DAVIS-SOD dataset. All results are ob-\ntained with the baseline using RGB frames for Deformable\nDETR [21].\nMethod Baseline (a) (b) Ours\nEvents âœ“ âœ“\nTemporal transformer âœ“ âœ“ âœ“\nFusion Module âœ“\nmAP50 0.461 0.489 0.494 0.504\nRuntime (ms) 21.5 24.9 39.4 39.7\nleverage rich temporal cues, obtains a 2.8% mAP improve-\nment over the baseline. Comparing method (b) with (a),\nthe absolute promotion is merely 0.5%, which indicates that\nit is insufficient to combine RGB frames and DVS events\nusing the averaging operation. Moreover, our SODFormer,\nadopting the asynchronous attention-based fusion strategy\nto replace the averaging fusion, achieves a 1.0% mAP im-\nprovement over method (b) while maintaining comparable\ncomputational complexity. Intuitively, our SODFormer em-\nploys these effective components to process two heteroge-\nneous visual streams and achieves robust object detection\non our PKU-DAVIS-SOD dataset.\n6.3.2 Influence of Temporal Aggregation Size\nTo analyze the temporal aggregation strategy in our SOD-\nFormer, we set the temporal Transformer with different sizes\n(e.g., 1, 3, 5, 9, and 13) of temporal aggregation. We refer\nto this as temporal aggregation size, which represents the\nnumber of event temporal bins S and adjacent frames I to\nprocess at a given timestamp ti. It fits in Eq. 8 as k + 1.\nAs depicted in Table 6, we find that our strategy improves\nmAPs by 1.2%, 1.9%, 2.8%, and 2.7% when compared to a\nTABLE 6: The influence of temporal aggregation size on our\nPKU-DAVIS-SOD dataset. The single-modality baseline is\nconducted using RGB frames for Deformable DETR [21].\nTemporal aggregation size 1 3 5 9 13\nmAP50 0.461 0.473 0.480 0.489 0.488\nRuntime (ms) 21.5 24.1 24.2 24.9 25.5\nsingle temporal aggregation size. Note that, by aggregating\nvisual streams in a larger temporal size, richer temporal\ncues can be utilized in the temporal Transformer, resulting\nin better object detection performance. Nevertheless, as the\ntemporal aggregation size becomes larger, the computa-\ntional time also gradually increases. Additionally, we find\nthat the modelâ€™s capacity for modeling temporally long-\nterm dependency is finite with the increase in temporal\naggregation size. In this study, our temporal aggregation\nsize is set to 9 for a trade-off between accuracy and speed.\nTo improve the interpretability of streaming object de-\ntection, we further present some comparative visualization\nresults about whether they utilize rich temporal cues or not\n(see Fig. 10). The feed-forward baseline using RGB frames\nsuffers from some failure cases involving small objects and\noccluded objects, as shown in the first and third rows\nin Fig. 10. Fortunately, our SODFormer overcomes these\nlimitations via leveraging rich temporal cues from adjacent\nframes or event streams. Therefore, a trade-off between\naccuracy and speed may be beneficial and necessary for\ncertain scenarios requiring a higher level of accuracy in\nobject detection.\n6.3.3 Influence of Fusion Strategy\nIn order to evaluate the effectiveness of our fusion module,\nwe compare it with some typical fusion methods in Table 7.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 13\nğ‘¡ğ‘–ğ‘¡ğ‘–âˆ’7ğ‘¡ğ‘–âˆ’14ğ‘¡ğ‘–âˆ’21ğ‘¡ğ‘–âˆ’28ğ‘¡ğ‘–âˆ’35\nBaseline\nOur\nSODFomer\nBaseline\nOur\nSODFomer\nFig. 10: Representative visualization results in continuous sequences of our PKU-DAVIS-SOD dataset. Note that, our\nSODFormer achieves better object detection performance than the feed-forward baseline without using temporal cues,\nespecially when involving small objects in the distance or partially occluded objects.\nTABLE 7: Comparison of the proposed asynchronous\nattention-based fusion module with typical fusion strategies\non our PKU-DAVIS-SOD dataset.\nMethod mAP 50 Runtime (ms)\nNMS [52] 0.438 8.2\nConcatenation [83] 0.478 39.6\nAveraging [28] 0.494 39.4\nOurs 0.504 39.7\n(a)\n(b)\n(c)\nFrame Event Averaging fusion Ours\nFig. 11: Representative instances of fusion results on our\nPKU-DAVIS-SOD dataset. The four columns from left to\nright are RGB frames, event images, fused images using av-\neraging fusion, and fused images using our fusion strategy.\nSpecifically, the concatenation and averaging method are\nboth applied to the feature maps produced by the tempo-\nral Transformer, referred to as XS and XI in Fig. 5. The\nXI and XS are concatenated along the last dimension for\nconcatenation and are averaged when averaging. The fused\nfeature map is input to the decoder in both methods. Note\nthat, our approach obtains the best performance against\nTABLE 8: Comparison of our SODFormer with various\nevent representations on our PKU-DAVIS-SOD dataset.\nMethod mAP 50 Runtime (ms)\nSigmoid representation [39] 0.309 39.4\nVoxel grid [75] 0.491 41.5\nEvent images [17] 0.504 39.7\nthe post-processing operations (e.g., NMS [52]) and end-\nto-end feature aggregation techniques (e.g., averaging [28]\nand concatenation [83]). More precisely, our strategy gets\naround 6.6%, 2.6% and 1.0% improvements on our PKU-\nDAVIS-SOD dataset with three corresponding methods.\nCompared with the averaging operation, our approach im-\nproves the mAP from 0.494 to 0.504 while maintaining\ncomparable computation time. Additionally, we present\nsome visualization comparison results in Fig. 11. Three\nrepresentative instances show that our approach performs\nbetter than the averaging operation. This is because our\nasynchronous attention-based fusion module can adaptively\ngenerate pixel-wise weight masks and eliminate unimodal\ndegradation thoroughly.\n6.3.4 Influence of Event Representation\nTo verify the generality of our SODFormer for various\nevent representations, we compare three typical event rep-\nresentations (i.e., event images [17], voxel grids [75] and\nsigmoid representation [39]) owning to an accuracy-speed\ntrade-off. As illustrated in Table 8, the performance of our\nSODFormer varies with different event representations, but\ntheir running speed is almost identical. It indicates that our\nSODFormer can provide a generic interface in combination\nwith various input representations, such as, image-like rep-\nresentations (e.g., event images [17] and sigmoid represen-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 14\nFig. 12: Visualization of all predicted bounding boxes from the test subset of our PKU-DAVIS-SOD dataset. We present 10\nrepresentative examples from 300 predicted slots. Boxes are regarded as points with centers obtained from their coordinates.\nBlue points refer to large vertical boxes, red points to horizontal boxes, and green points to small boxes. Notably, each slot\ncan be trained to focus on certain areas and object sizes with a variety of operating modes.\nTABLE 9: The influence of the number of TDTEâ€™s layers on\nour PKU-DAVIS-SOD dataset.\nThe number of encoder layers 2 4 6 8 12\nmAP50 0.481 0.480 0.489 0.471 0.450\nRuntime (ms) 23.5 24.2 24.9 25.2 26.1\nTABLE 10: The influence of the number of TDTDâ€™s layers on\nour PKU-DAVIS-SOD dataset.\nThe number of decoder layers 2 4 6 8 12\nmAP50 0.476 0.480 0.489 0.491 0.489\nRuntime (ms) 21.1 22.3 24.9 29.5 34.8\ntation [39]) and spatiotemporal representations (i.e., voxel\ngrids [75]). Meanwhile, it is obvious that the performance\nis highly dependent on how well the representation is and\ndrops heavily with worse methods (e.g., sigmoid represen-\ntation). Indeed, we believe that a good event representation\nmakes asynchronous events directly compatible with our\nSODFormer while maximizing the detection performance.\n6.3.5 Influence of the Number of Transformerâ€™s Layer\nWe will analyze the effect of the number of layers in the\ndesigned temporal Transformer on the final performance\nfrom the following two perspectives.\nThe Number of TDTEâ€™s Layer . As shown in Table 9, we\nfirst explore the influence of the number of encoder layers in\nour TDTE on our PKU-DAVIS-SOD dataset. To our surprise,\nas the number of encoder layers increases, our SODFormer\nshows only slight improvements up to 6 layers and a rapid\ndecline due to overfitting after that, yet the inference time\nis gradually getting longer. Considering that increasing the\nnumber of encoder layers in our TDTE doesnâ€™t improve the\ndetection performance but increases the computational time,\nthe number of encoder layers in our TDTE is set to 6.\nThe Number of TDTDâ€™s Layer . Table 10 further illustrates\nthe influence of decoder layers in our TDTD on our PKU-\nDAVIS-SOD dataset. We can find that the best performance\noccurs when we set the number of decoder layers to 8. How-\never, the mAP is almost unchanged and the computational\ntime increases rapidly when the number of decoder layers\nexceeds 6. In order to balance detection performance and\ncomputational complexity, it is reasonable to set the number\nof TDTDâ€™s layers to 6.\n6.4 Scalability Test\nThis subsection will present the predicted output slot analy-\nsis (Section 6.4.1) and the visualization of the designed tem-\nporal deformable attention (Section 6.4.2). Then, we further\npresent how to process two heterogeneous visual streams in\nan asynchronous manner (Section 6.4.3). Finally, we analyze\nsome failure cases of our SODFormer (Section 6.4.4).\n6.4.1 The Predicted Output Slot Analysis\nTo increase the interpretability of our SODFormer, we visu-\nalize the predicted bounding boxes of different slots for all\nlabeled timestamps in the test subset of our PKU-DAVIS-\nSOD dataset. Inspired by DETR [20], each predicted box\nis represented as a point whose coordinates are the center\nnormalized by image size. Fig. 12 shows 10 representa-\ntive examples out of 300 predicted slots in our temporal\ndeformable Transformer decoder (TDTD). Apparently, we\nfind that each slot can learn to focus on certain areas and\nbounding box sizes. Actually, our SODFormer can learn a\ndifferent specialization for each object query slot, and all\nslots can cover the distribution of all objects in our PKU-\nDAVIS-SOD dataset.\n6.4.2 Visualization of Temporal Deformable Attention\nTo better understand the proposed temporal deformable at-\ntention, we visualize sampling points and attention weights\nof the last layer in our temporal deformable Transformer\nencoder (TDTE). As shown in Fig. 13(a), our feed-forward\nbaseline fails to detect the middle car owning to the oc-\nclusion, while our unimodal SODFormer succeeded by uti-\nlizing rich temporal cues in adjacent frames. In addition,\nwe present the distribution of sampling points in the last\nlayer of our TDTE (see Fig. 13(b)). All sampling points are\nmapped to the corresponding location in RGB frames, and\neach sampling point is marked as a color-filled circle whose\ncolor represents the size of the weight. The reference point\nis labeled as a green cross marker. We can find that the\ndistribution of sampling points is focused on the foreground\narea surrounding the reference point radially instead of the\nwhole image. As for the attention weights, it is obvious\nthat the closer the sampling point to the reference point\nis, the greater the attention weight will be. Specifically, the\nsampling points in adjacent frames are uniformly clustered\nin the foreground area, providing rich temporal cues of\nthe objects in the temporal domain. This indicates that\nthe proposed temporal deformable attention can adapt its\nsampling points to concentrate on the foreground object in\nadjacent frames.\n6.4.3 Asynchronous Inference Analysis\nIn general, the global shutter of conventional frame-based\ncameras limits their sampling rate, resulting in a relatively\nlong time interval between two adjacent RGB frames. Actu-\nally, it is difficult to accurately locate moving objects with\nhigh output frequency in real-time high-speed scenarios.\nTowards this end, the proposed asynchronous attention-\nbased fusion module aims at breaking through the limited\noutput frequency from synchronized frame-based fusion\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 15\nLow\nHigh\n(a) Detection results (b) Distribution of sampling points\nFig. 13: Visualization of TDMSA in the last layer of our TDTE. (a) Comparison of object detection results between the\nfeed-forward baseline (top) and our unimodal SODFormer without using events (bottom). (b) Distribution of sampling\npoints in reference frames. The reference point is denoted by a green cross marker, and each sampling point is marked as\na color-filled circle whose color represents the size of its weight.\nğ¼ğ‘¡âˆ’1\nğ¼ğ‘¡\nğ‘†\nğ‘¡\nFig. 14: Visualization of our asynchronous fusion strategy.\nThe top figure depicts two adjacent frames and an event\nstream. The bottom figures present the detection perfor-\nmance of the two frames and the four sampling timestamps\nfrom the continuous event stream.\nmethods. As illustrated in Table 11 and Fig. 14, we de-\nsign two specific experiments to verify the effectiveness of\nour asynchronous fusion strategy. First, we compare the\nperformance of single-modality baseline* without utilizing\nthe asynchronous attention-based fusion module and our\nSODFormer in different frame rates. Note that we only\nchange the frame rate, while the temporal event bins are\nfixed to 25 Hz, so as the output frequency. This ensures\nthat every prediction has a corresponding annotation. We\ncan see from Table 11 that both methods suffer a reduction\nin performance as the frame rate decreases, but the per-\nformance of our SODFormer is significantly less reduced\nthan that of baseline*. Meanwhile, from the last row, we see\nthat our SODFormer is still acceptable when the output fre-\nquency is four times the frame rate. This indicates that our\nasynchronous attention-based fusion module has a strong\nability to conduct object detection of two asynchronous\nmodalities. To further illustrate how our SODFormer per-\nforms in asynchronous detection when the input frequency\nis high, we conduct another experiment in which we use\nthe form of a sliding window with 0.04s as its length from\nthe continuous event stream and shift it forward by 0.01s\nat a time. As a result, the frequency of the divided event\ntemporal bins can be as high as 100 Hz. Then, we input the\nRGB frames of 25 Hz and event temporal bins of 100 Hz into\nTABLE 11: Comparison of the performance of SODFormer\nand Baseline* in different frame rates. Our Baseline* denotes\nSODFormer that processes only frame modality without\nutilizing the asynchronous attention-based fusion module.\nMethod Frame rate (FPS)\n25 12.5 8.33 6.25\nBaseline* 0.489 0.458 0.412 0.372\nSODFormer 0.504 0.495 0.472 0.448\nour SODFormer, as shown in Fig. 14. We find that the four\nfigures in the middle show the detection performance of the\nevent stream between the two RGB frames. Therefore, we\ncan confirm that our asynchronous fusion strategy is able to\nfill the gap between two adjacent RGB frames and helps our\nSODFormer to detect objects in an asynchronous manner.\nFig. 15: Representative failure cases of our SODFormer in\nPKU-DAVIS-SOD dataset. The four columns from left to\nright refer to RGB frames, event images, our SODFormer\nusing RGB frames and DVS events, and ground truth la-\nbeled in reconstructed images.\n6.4.4 Failure Case Analysis\nAlthough our SODFormer achieves satisfactory results even\nin challenging scenarios, the problem involving some failure\ncases is still far from being solved. As depicted in Fig. 15,\nthe first row shows that static or slow-moving cars in low-\nlight conditions are hard to perform robust detection. This\nis because the relatively low dynamic range of conventional\ncameras results in poor image quality in low-light scenes.\nMeanwhile, the event camera evidently senses dynamic\nchanges, but they generate almost no events in static or\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 16\nslow-moving scenarios. The second row illustrates that\nsmall moving objects at high-speed fail to be detected. This\nmay be caused by the fact that the rushing object which\nis of high relative speed in RGB frames is almost invisible\nowing to the influence of high-speed motion blur. At the\nsame time, event cameras capture moving objects at high-\nspeed but they display weak textures. In fact, it indicates\nthat our PKU-DAVIS-SOD dataset is a challenging and\ncompetitive dataset. Whatâ€™s more, these failure cases beyond\nour SODFormer need to be addressed in future work.\n7 D ISCUSSION\nAn effective and robust streaming object detector will fur-\nther highlight the potential of the unifying framework using\nevents and frames. Here, we will discuss the generality and\nthe limitation of our method as follows.\nGenerality. One might think that this work does not\nexplore one core issue of how to design a novel event\nrepresentation. Actually, any event representation can be\nregarded as the input of our SODFormer. The ablation\nstudy verifies that our SODFormer can improve detection\nperformance by introducing DVS with different event rep-\nresentations (see Section 6.3.4). The highly realistic synthetic\ndataset should be worth exploring in the future, as they\ncan provide large-scale data for model training and testing\nin simulation scenarios. However, synthetic datasets from\nexisting simulators are unrealistic and are not suited for\nverifying the effectiveness of our SODFormer in extreme\nscenarios (e.g., low-light or high-speed motion blur). Some\nmay argue that the backbone of our SODFormer is a CNN-\nbased feature extractor. Indeed, this study does not design\na pre-trained vision Transformer backbone (e.g., ViT [84]\nand Swin Transformer [85]) for event streams. Indeed, in-\nvestigating an event-based vision Transformer to learn an\neffective event representation is interesting and of wide\napplicability in computer vision tasks (e.g., video recon-\nstruction, object detection, and depth estimation).\nLimitation. Currently, the distribution of the sampling\npoints in TDMSA is almost identical in different reference\nframes, which are radially around the reference point (see\nSection 6.4.2). However, this sampling strategy doesnâ€™t take\nthe motion trajectory of the object into account and limits\nthe modelâ€™s capacity for modeling temporally long-term\ndependency (see Section 6.3.2). We consider following the\nwork [86] of using optical flow to track the motion trajectory\nin optimizing the selection of sampling points, but this is\nnot included in our SODFormer due to the additional com-\nplexity. In fact, how to track objects efficiently with a low\ncomputational complexity remains a topic worth exploring.\n8 C ONCLUSION\nThis paper presents a novel streaming object detector with\nTransformer (i.e., SODFormer) using events and frames,\nwhich highlights how events and frames can be utilized\nto deal with major object detection challenges (e.g., fast\nmotion blur and low-light). To the best of our knowledge,\nthis is the first trial exploring a Transformer-based architec-\nture to continuously detect objects from two heterogeneous\nvisual streams. To achieve this, we first build a large-scale\nmultimodal object detection dataset (i.e., PKU-DAVIS-SOD\ndataset) including two visual streams and manual labels.\nThen, a spatiotemporal Transformer is designed to detect\nobjects via an end-to-end sequence prediction problem, its\ntwo core innovative modules are the temporal Transformer\nand the asynchronous attention-based fusion module. The\nresults demonstrate that our SODFormer outperforms four\nstate-of-the-art methods and our eight baselines, inheriting\nhigh temporal resolution and wide HDR range properties\nfrom DVS events (i.e., brightness changes) and fine textures\nfrom RGB frames (i.e., absolute brightness). We believe that\nthis study makes a major step towards solving the problem\nof fusing events and frames for streaming object detection.\nACKNOWLEDGMENTS\nThis work is partially supported by the National Natural\nScience Foundation of China under Grant 62027804, Grant\n61825101 and Grant 62088102.\nREFERENCES\n[1] L. Liu, W. Ouyang, X. Wang, P . Fieguth, J. Chen, X. Liu, and\nM. Pietik Â¨ainen, â€œDeep learning for generic object detection: A\nsurvey,â€ Int. J. Comput. Vis., vol. 128, no. 2, pp. 261â€“318, 2020. 1\n[2] S. Long, X. He, and C. Yao, â€œScene text detection and recognition:\nThe deep learning era,â€ Int. J. Comput. Vis., pp. 1â€“24, 2020. 1\n[3] K. Oksuz, B. C. Cam, S. Kalkan, and E. Akbas, â€œImbalance prob-\nlems in object detection: A review,â€IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 43, no. 10, pp. 3388â€“3415, 2021. 1\n[4] K. Sun, W. Wu, T. Liu, S. Yang, Q. Wang, Q. Zhou, Z. Ye, and\nC. Qian, â€œFab: A robust facial landmark detection framework for\nmotion-blurred videos,â€ in Proc. IEEE Int. Conf. Comput. Vis., 2019,\npp. 5462â€“5471. 1\n[5] M. Sayed and G. Brostow, â€œImproved handling of motion blur in\nonline object detection,â€ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2021, pp. 1706â€“1716. 1\n[6] Y. Hu, T. Delbruck, and S.-C. Liu, â€œLearning to exploit multiple\nvision modalities by using grafted networks,â€ in Proc. Eur. Conf.\nComput. Vis., 2020, pp. 85â€“101. 1, 3, 11\n[7] C. Brandli, R. Berner, M. Yang, S.-C. Liu, and T. Delbruck, â€œA\n240Ã— 180 130 db 3 Âµs latency global shutter spatiotemporal vision\nsensor,â€ IEEE J. Solid-State Circuits , vol. 49, no. 10, pp. 2333â€“2341,\n2014. 1, 2\n[8] D. P . Moeys, F. Corradi, C. Li, S. A. Bamford, L. Longinotti, F. F.\nVoigt, S. Berry, G. Taverni, F. Helmchen, and T. Delbruck, â€œA\nsensitive dynamic and active pixel vision sensor for color or neural\nimaging applications,â€ IEEE Trans. Biomed. Circuits Syst. , vol. 12,\nno. 1, pp. 123â€“136, 2017. 1\n[9] P . Lichtsteiner, C. Posch, and T. Delbruck, â€œA 128 Ã—128 120 db\n15Âµs latency asynchronous temporal contrast vision sensor,â€ IEEE\nJ. Solid-State Circuits, vol. 43, no. 2, pp. 566â€“576, 2008. 1\n[10] D. Gehrig, H. Rebecq, G. Gallego, and D. Scaramuzza, â€œEklt: Asyn-\nchronous photometric feature tracking using events and frames,â€\nInt. J. Comput. Vis., vol. 128, no. 3, pp. 601â€“618, 2020. 1\n[11] D. Gehrig, M. R Â¨uegg, M. Gehrig, J. Hidalgo-Carri Â´o, and D. Scara-\nmuzza, â€œCombining events and frames using recurrent asyn-\nchronous multimodal networks for monocular depth prediction,â€\nIEEE Robot. Autom. Lett., vol. 6, no. 2, pp. 2822â€“2829, 2021. 1, 3\n[12] S. Tulyakov, D. Gehrig, S. Georgoulis, J. Erbach, M. Gehrig, Y. Li,\nand D. Scaramuzza, â€œTime lens: Event-based video frame interpo-\nlation,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp.\n16 155â€“16 164. 1, 3\n[13] Z. Jiang, P . Xia, K. Huang, W. Stechele, G. Chen, Z. Bing, and\nA. Knoll, â€œMixed frame-/event-driven fast pedestrian detection,â€\nin Proc. IEEE Conf. Robot. Autom., 2019, pp. 8332â€“8338. 1, 2, 3, 5, 8,\n11\n[14] J. Li, S. Dong, Z. Yu, Y. Tian, and T. Huang, â€œEvent-based vision\nenhanced: A joint detection framework in autonomous driving,â€\nin Proc. IEEE Int. Conf. Multimedia Expo., 2019, pp. 1396â€“1401. 1, 2,\n3, 5, 8, 11\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 17\n[15] H. Cao, G. Chen, J. Xia, G. Zhuang, and A. Knoll, â€œFusion-based\nfeature attention gate component for vehicle detection based on\nevent camera,â€ IEEE Sensors J., pp. 1â€“9, 2021. 1, 3, 8\n[16] M. Liu, N. Qi, Y. Shi, and B. Yin, â€œAn attention fusion network for\nevent-based vehicle object detection,â€ in Proc. IEEE Int. Conf. Image\nProcess., 2021, pp. 3363â€“3367. 1, 2, 3, 5, 8\n[17] A. I. Maqueda, A. Loquercio, G. Gallego, N. Garc Â´Ä±a, and D. Scara-\nmuzza, â€œEvent-based vision meets deep learning on steering\nprediction for self-driving cars,â€ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2018, pp. 5419â€“5427. 1, 6, 9, 13\n[18] R. J. Dolan, G. Fink, E. Rolls, M. Booth, A. Holmes, R. Frackowiak,\nand K. Friston, â€œHow the brain learns to see objects and faces in\nan impoverished context,â€ Nature, vol. 389, no. 6651, pp. 596â€“599,\n1997. 1\n[19] A. G. Huth, S. Nishimoto, A. T. Vu, and J. L. Gallant, â€œA con-\ntinuous semantic space describes the representation of thousands\nof object and action categories across the human brain,â€ Neuron,\nvol. 76, no. 6, pp. 1210â€“1224, 2012. 1\n[20] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, â€œEnd-to-end object detection with transformers,â€ in\nProc. Eur. Conf. Comput. Vis., 2020, pp. 213â€“229. 1, 2, 3, 6, 8, 14\n[21] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, â€œDeformable detr:\nDeformable transformers for end-to-end object detection,â€ in Proc.\nInt. Conf. Learn. Represent., 2020, pp. 1â€“16. 1, 3, 6, 7, 8, 9, 11, 12\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Å. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€\nin Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998â€“6008. 1, 3, 9\n[23] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah, â€œTransformers in vision: A survey,â€arXiv, pp. 1â€“28, 2021.\n1\n[24] T. H. Kim, M. S. Sajjadi, M. Hirsch, and B. Scholkopf, â€œSpatio-\ntemporal transformer network for video restoration,â€ in Proc. Eur.\nConf. Comput. Vis., 2018, pp. 106â€“122. 1, 3\n[25] L. He, Q. Zhou, X. Li, L. Niu, G. Cheng, X. Li, W. Liu, Y. Tong,\nL. Ma, and L. Zhang, â€œEnd-to-end video object detection with\nspatial-temporal transformers,â€ in Proc. ACM Int. Conf. Multime-\ndia., 2021. 1, 3\n[26] Y. Zhang, X. Li, C. Liu, B. Shuai, Y. Zhu, B. Brattoli, H. Chen, I. Mar-\nsic, and J. Tighe, â€œVidtr: Video transformer without convolutions,â€\nin Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 13 577â€“13 587. 1, 3\n[27] A. Tomy, A. Paigwar, K. S. Mann, A. Renzaglia, and C. Laugier,\nâ€œFusing event-based and RGB camera for robust object detection\nin adverse conditions,â€ in Proc. IEEE Conf. Robot. Autom., 2022, pp.\n933â€“939. 2, 3\n[28] H. Li, X.-J. Wu, and J. Kittler, â€œMdlatlrr: A novel decomposition\nmethod for infrared and visible image fusion,â€ IEEE Trans. Image\nProcess., vol. 29, no. 1, pp. 4733â€“4746, 2020. 2, 3, 13\n[29] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba,\nA. Censi, S. Leutenegger, A. Davison, J. Conradt, K. Daniilidis\net al., â€œEvent-based vision: A survey,â€ in IEEE Trans. Pattern Anal.\nMach. Intell., 2020, pp. 1â€“25. 2\n[30] S.-C. Liu, B. Rueckauer, E. Ceolini, A. Huber, and T. Delbruck,\nâ€œEvent-driven sensing for efficient perception: Vision and audition\nalgorithms,â€ IEEE Signal Process. Mag. , vol. 36, no. 6, pp. 29â€“37,\n2019. 2\n[31] K. Roy, A. Jaiswal, and P . Panda, â€œTowards spike-based machine\nintelligence with neuromorphic computing,â€ Nature, vol. 575, no.\n7784, pp. 607â€“617, 2019. 2\n[32] G. Chen, H. Cao, J. Conradt, H. Tang, F. Rohrbein, and A. Knoll,\nâ€œEvent-based neuromorphic vision for autonomous driving: A\nparadigm shift for bio-inspired visual sensing and perception,â€\nIEEE Signal Process. Mag., vol. 37, no. 4, pp. 34â€“49, 2020. 2\n[33] P . de Tournemire, D. Nitti, E. Perot, D. Migliore, and A. Sironi,\nâ€œA large scale event-based detection dataset for automotive,â€ in\narXiv, 2020, pp. 1â€“8. 2, 5\n[34] E. Perot, P . de Tournemire, D. Nitti, J. Masci, and A. Sironi,\nâ€œLearning to detect objects with a 1 megapixel event camera,â€ in\nProc. Adv. Neural Inf. Process. Syst., 2020, pp. 1â€“14. 2, 5\n[35] H. Rebecq, D. Gehrig, and D. Scaramuzza, â€œEsim: An open event\ncamera simulator,â€ in Proc. Conf. Robot Learn., 2018, pp. 969â€“982. 3\n[36] Y. Hu, S.-C. Liu, and T. Delbruck, â€œV2e: From video frames to\nrealistic dvs events,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recog.\nWorksh., 2021, pp. 1312â€“1321. 3\n[37] Z. Kang, J. Li, L. Zhu, and Y. Tian, â€œRetinomorphic sensing: A\nnovel paradigm for future multimedia computing,â€ in Proc. ACM\nInt. Conf. Multimedia., 2021, pp. 144â€“152. 3\n[38] M. Iacono, S. Weber, A. Glover, and C. Bartolozzi, â€œTowards event-\ndriven object detection with off-the-shelf deep learning,â€ in Proc.\nIEEE Conf. Intell. Robot. Syst., 2018, pp. 1â€“9. 3, 11\n[39] N. F. Chen, â€œPseudo-labels for supervised learning on dynamic\nvision sensor data, applied to object detection under ego-motion,â€\nin Proc. IEEE Conf. Comput. Vis. Pattern Recog. Worksh. , 2018, pp.\n644â€“653. 3, 6, 13, 14\n[40] M. Cannici, M. Ciccone, A. Romanoni, and M. Matteucci, â€œEvent-\nbased convolutional networks for object detection in neuromor-\nphic cameras,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recog.\nWorksh., 2019, pp. 1656â€“1665. 3\n[41] G. Chen, H. Cao, C. Ye, Z. Zhang, X. Liu, X. Mo, Z. Qu, J. Conradt,\nF. RÂ¨ohrbein, and A. Knoll, â€œMulti-cue event information fusion for\npedestrian detection with neuromorphic vision sensors,â€ Frontiers\nin Neurorobotics, vol. 13, p. 10, 2019. 3, 5\n[42] C. Ryan, B. Oâ€™Sullivan, A. Elrasad, A. Cahill, J. Lemley, P . Kielty,\nC. Posch, and E. Perot, â€œReal-time face & eye tracking and blink\ndetection using event cameras,â€ Neural Netw., vol. 141, pp. 87â€“97,\n2021. 3\n[43] J. Li, J. Li, L. Zhu, X. Xiang, T. Huang, and Y. Tian, â€œAsynchronous\nspatio-temporal memory network for continuous event-based ob-\nject detection,â€ IEEE Trans. Image Process. , vol. 31, pp. 2975â€“2987,\n2022. 3, 11\n[44] X. Xiang, L. Zhu, J. Li, Y. Tian, and T. Huang, â€œTemporal up-\nsampling for asynchronous events,â€ in Proc. IEEE Int. Conf. Multi-\nmedia Expo., 2022, pp. 1â€“6. 3\n[45] D. Wang, X. Jia, Y. Zhang, X. Zhang, Y. Wang, Z. Zhang, D. Wang,\nand H. Lu, â€œDual memory aggregation network for event-based\nobject detection with learnable representation,â€ inProc. AAAI Conf.\non Artificial Intell., 2023, pp. 2492â€“2500. 3\n[46] M. Gehrig and D. Scaramuzza, â€œRecurrent vision transformers for\nobject detection with event cameras,â€ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2023. 3\n[47] D. Gehrig, A. Loquercio, K. G. Derpanis, and D. Scaramuzza,\nâ€œEnd-to-end learning of representations for asynchronous event-\nbased data,â€ in Proc. IEEE Int. Conf. Comput. Vis. , 2019, pp. 5633â€“\n5643. 3, 6, 7\n[48] J. Redmon and A. Farhadi, â€œYolov3: An incremental improve-\nment,â€ in arXiv, 2018, pp. 1â€“6. 3, 6, 11\n[49] H. Liu, D. P . Moeys, G. Das, D. Neil, S.-C. Liu, and T. Delbr Â¨uck,\nâ€œCombined frame-and event-based detection and tracking,â€ in\nProc. IEEE Int. Symposium Circuits Syst. , 2016, pp. 2511â€“2514. 3,\n8\n[50] Z. El Shair and S. A. Rawashdeh, â€œHigh-temporal-resolution object\ndetection and tracking using images and events,â€ J. Imag., vol. 8,\nno. 8, pp. 1â€“21, 2022. 3\n[51] N. Messikommer, D. Gehrig, M. Gehrig, and D. Scaramuzza,\nâ€œBridging the gap between events and frames through unsuper-\nvised domain adaptation,â€ IEEE Robot. Autom. Lett. , vol. 7, no. 2,\npp. 3515â€“3522, 2022. 3\n[52] J. Hosang, R. Benenson, and B. Schiele, â€œLearning non-maximum\nsuppression,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\n2017, pp. 4507â€“4515. 3, 13\n[53] D. Bahdanau, K. H. Cho, and Y. Bengio, â€œNeural machine transla-\ntion by jointly learning to align and translate,â€ in Proc. Int. Conf.\nLearn. Represent., 2015, pp. 1â€“15. 3\n[54] R. Girshick, J. Donahue, T. Darrell, and J. Malik, â€œRich feature hier-\narchies for accurate object detection and semantic segmentation,â€\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 580â€“587.\n3\n[55] R. Girshick, â€œFast R-CNN,â€ in Proc. IEEE Int. Conf. Comput. Vis. ,\n2015, pp. 1440â€“1448. 3\n[56] S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster r-cnn: Towards real-\ntime object detection with region proposal networks,â€ IEEE Trans.\nPattern Anal. Mach. Intell. , vol. 39, no. 6, pp. 1137â€“1149, 2017. 3, 8,\n11\n[57] T. Wang, L. Yuan, Y. Chen, J. Feng, and S. Yan, â€œPnp-detr: Towards\nefficient visual analysis with transformers,â€ in Proc. IEEE Int. Conf.\nComput. Vis., 2021, pp. 4661â€“4670. 3\n[58] W. Shang, D. Ren, D. Zou, J. S. Ren, P . Luo, and W. Zuo, â€œBring-\ning events into video deblurring with non-consecutively blurry\nframes,â€ in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 4531â€“4540.\n3\n[59] L. Zhu, J. Li, X. Wang, T. Huang, and Y. Tian, â€œNeuspike-net:\nHigh speed video reconstruction via bio-inspired neuromorphic\ncameras,â€ in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 2400â€“2409.\n3\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2022 18\n[60] P . Duan, Z. Wang, B. Shi, O. Cossairt, T. Huang, and A. Katsagge-\nlos, â€œGuided event filtering: Synergy between intensity images\nand neuromorphic events for high performance imaging,â€ IEEE\nTrans. Pattern Anal. Mach. Intell., 2021. 3\n[61] J. Zhang, X. Yang, Y. Fu, X. Wei, B. Yin, and B. Dong, â€œObject\ntracking by jointly exploiting frame and event domain,â€ in Proc.\nIEEE Int. Conf. Comput. Vis., 2021, pp. 13 043â€“13 052. 3\n[62] X. Wang, J. Li, L. Zhu, Z. Zhang, Z. Chen, X. Li, Y. Wang, Y. Tian,\nand F. Wu, â€œVisevent: Reliable object tracking via collaboration of\nframe and event flows,â€ in arXiv, 2021. 3\n[63] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza,\nâ€œUltimate slam? combining events, images, and imu for robust\nvisual slam in hdr and high-speed scenarios,â€ IEEE Robot. Autom.\nLett., vol. 3, no. 2, pp. 994â€“1001, 2018. 3\n[64] Y.-F. Zuo, J. Yang, J. Chen, X. Wang, Y. Wang, and L. Kneip, â€œDevo:\nDepth-event camera visual odometry in challenging conditions,â€\nin Proc. IEEE Conf. Robot. Autom., 2022, pp. 2179â€“2185. 3\n[65] L. Gao, Y. Liang, J. Yang, S. Wu, C. Wang, J. Chen, and L. Kneip,\nâ€œVector: A versatile event-centric benchmark for multi-sensor\nslam,â€ IEEE Robot. Autom. Lett., 2022. 3\n[66] H. Rebecq, R. Ranftl, V . Koltun, and D. Scaramuzza, â€œEvents-to-\nvideo: Bringing modern computer vision to event cameras,â€ in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3857â€“3866.\n4, 5, 10, 11\n[67] Y. Pang, J. Cao, Y. Li, J. Xie, H. Sun, and J. Gong, â€œTju-dhd: A\ndiverse high-resolution dataset for object detection,â€ IEEE Trans.\nImage Process., vol. 30, pp. 207â€“219, 2020. 4\n[68] C. Posch, T. Serrano-Gotarredona, B. Linares-Barranco, and T. Del-\nbruck, â€œRetinomorphic event-based vision sensors: bioinspired\ncameras with spiking output,â€ Proc. IEEE. , vol. 102, no. 10, pp.\n1470â€“1484, 2014. 5\n[69] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gille-\nspie, â€œSilicon auditory processors as computer peripherals,â€ IEEE\nTrans. Neural Netw., vol. 4, no. 3, pp. 523â€“528, 1993. 5\n[70] J. Li, Y. Fu, S. Dong, Z. Yu, T. Huang, and Y. Tian, â€œAsynchronous\nspatiotemporal spike metric for event cameras,â€IEEE Trans. Neural\nNetw. Learn. Syst., 2021. 5\n[71] W. Han, Z. Zhang, B. Caine, B. Yang, C. Sprunk, O. Alsharif,\nJ. Ngiam, V . Vasudevan, J. Shlens, and Z. Chen, â€œStreaming object\ndetection for 3-d point clouds,â€ in Proc. Eur. Conf. Comput. Vis. ,\n2020, pp. 423â€“441. 6\n[72] Q. Chen, S. Vora, and O. Beijbom, â€œPolarstream: Streaming object\ndetection and segmentation with polar pillars,â€ in Proc. Adv.\nNeural Inf. Process. Syst., 2021, pp. 26 871â€“26 883. 6\n[73] J. Yang, S. Liu, Z. Li, X. Li, and J. Sun, â€œStreamyolo: Real-time\nobject detection for streaming perception,â€ arXiv, 2022. 6\n[74] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning\nfor image recognition,â€ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2016, pp. 770â€“778. 6, 7, 9\n[75] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, â€œUnsupervised\nevent-based learning of optical flow, depth, and egomotion,â€ in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 989â€“997.\n6, 13, 14\n[76] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, â€œDropout: A simple way to prevent neural\nnetworks from overfitting,â€ J. Mach. Learn. Research , vol. 15, no. 1,\npp. 1929â€“1958, 2014. 7\n[77] J. Xu, X. Sun, Z. Zhang, G. Zhao, and J. Lin, â€œUnderstanding\nand improving layer normalization,â€ Proc. Adv. Neural Inf. Process.\nSyst., vol. 32, pp. 4381â€“4391, 2019. 7\n[78] O. Mazhar, R. Babu Ë‡ska, and J. Kober, â€œGem: Glare or gloom, i\ncan still see youâ€“end-to-end multi-modal object detection,â€ IEEE\nRobot. Autom. Lett., vol. 6, no. 4, pp. 6321â€“6328, 2021. 8\n[79] D. P . Kingma and J. Ba, â€œAdam: A method for stochastic optimiza-\ntion,â€ in Proc. Int. Conf. Learn. Represent., 2014, pp. 1â€“15. 9\n[80] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . DollÂ´ar, and C. L. Zitnick, â€œMicrosoft coco: Common objects in\ncontext,â€ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2014,\npp. 740â€“755. 9\n[81] M. Zhu and M. Liu, â€œMobile video object detection with\ntemporally-aware feature maps,â€ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2018, pp. 5686â€“5695. 11\n[82] S. Guo and T. Delbruck, â€œLow cost and latency event camera\nbackground activity denoising,â€ IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 45, no. 1, pp. 785â€“795, 2023. 10\n[83] J. Xu, W. Wang, H. Wang, and J. Guo, â€œMulti-model ensemble with\nrich spatial information for object detection,â€ Pattern Recognit. ,\nvol. 99, p. 107098, 2020. 13\n[84] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al. , â€œAn image is worth 16x16 words: Transformers for image\nrecognition at scale,â€ in Proc. Int. Conf. Learn. Represent., 2020. 16\n[85] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\nâ€œSwin transformer: Hierarchical vision transformer using shifted\nwindows,â€ in Proc. IEEE Int. Conf. Comput. Vis., 2021. 16\n[86] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, â€œFlow-guided feature\naggregation for video object detection,â€ inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2017, pp. 408â€“417. 16\nDianze Li received the B.S degree from the\nSchool of Electronics Engineering and Com-\nputer Science, Peking University, Beijing, China,\nin 2022. He is currently pursuing the Ph.D. de-\ngree with the National Engineering Research\nCenter for Visual Technology, School of Com-\nputer Science, Peking University, Beijing, China.\nHis current research interests include event-\nbased vision, spiking neural networks, and neu-\nromorphic engineering.\nJianing Li (Member, IEEE) received the B.S.\ndegree from the College of Computer and In-\nformation Technology, China Three Gorges Uni-\nversity, China, in 2014, and the M.S. degree\nfrom the School of Microelectronics and Com-\nmunication Engineering, Chongqing University,\nChina, in 2017, and the Ph.D. degree from the\nNational Engineering Research Center for Vi-\nsual Technology, School of Computer Science,\nPeking University, Beijing, China, in 2022.\nHe is the author or coauthor of over 20 tech-\nnical papers in refereed journals and conferences, such as the IEEE\nTRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLI-\nGENCE, the IEEE TRANSACTIONS ON IMAGE PROCESSING, the\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING\nSYSTEMS, CVPR, ICCV, AAAI, and ACM MM. He received the Lixin\nTang Scholarship from Chongqing University, Chongqing, China, in\n2016. His research interests include event-based vision, neuromorphic\nengineering, and robotics.\nYonghong Tian(Sâ€™00-Mâ€™06-SMâ€™10-Fâ€™22) is cur-\nrently the Dean of School of Electronics and\nComputer Engineering, Peking University, Shen-\nzhen Graduate School, 518055, China, a Boya\nDistinguished Professor with the School of Com-\nputer Science, Peking University, China, and is\nalso the deputy director of Artificial Intelligence\nResearch, PengCheng Laboratory, Shenzhen,\nChina. His research interests include neuromor-\nphic vision, distributed machine learning and\nmultimedia big data. He is the author or coauthor\nof over 300 technical articles in refereed journals and conferences.\nProf. Tian was/is an Associate Editor of IEEE TCSVT (2018.1-2021.12),\nIEEE TMM (2014.8-2018.8), IEEE Multimedia Mag. (2018.1-2022.8),\nand IEEE Access (2017.1-2021.12). He co-initiated IEEE Intâ€™l Conf.\non Multimedia Big Data (BigMM) and served as the TPC Co-chair of\nBigMM 2015, and aslo served as the Technical Program Co-chair of\nIEEE ICME 2015, IEEE ISM 2015 and IEEE MIPR 2018/2019, and\nGeneral Co-chair of IEEE MIPR 2020 and ICME2021. He is a TPC\nMember of more than ten conferences such as CVPR, ICCV, ACM\nKDD, AAAI, ACM MM and ECCV. He was the recipient of the Chinese\nNational Science Foundation for Distinguished Y oung Scholars in 2018,\ntwo National Science and Technology Awards and three ministerial-level\nawards in China, and obtained the 2015 EURASIP Best Paper Award for\nJournal on Image and Video Processing, and the best paper award of\nIEEE BigMM 2018, and the 2022 IEEE SA Standards Medallion and SA\nEmerging Technology Award. He is a Fellow of IEEE, a senior member\nof CIE and CCF , a member of ACM.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7031049132347107
    },
    {
      "name": "Computer vision",
      "score": 0.6288717985153198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6236172914505005
    },
    {
      "name": "Object detection",
      "score": 0.587830126285553
    },
    {
      "name": "Transformer",
      "score": 0.4646349251270294
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32186663150787354
    },
    {
      "name": "Engineering",
      "score": 0.12959882616996765
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ]
}