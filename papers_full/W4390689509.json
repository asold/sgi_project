{
  "title": "Forecasting solar flares with a transformer network",
  "url": "https://openalex.org/W4390689509",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5012085247",
      "name": "Keahi Pelkum Donahue",
      "affiliations": [
        "Crest Middle School"
      ]
    },
    {
      "id": "https://openalex.org/A5029627235",
      "name": "Fadil Inceoglu",
      "affiliations": [
        "Cooperative Institute for Research in Environmental Sciences",
        "NOAA National Centers for Environmental Information"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386051999",
    "https://openalex.org/W4297734170",
    "https://openalex.org/W3188442532",
    "https://openalex.org/W2033706575",
    "https://openalex.org/W2093586470",
    "https://openalex.org/W2122755404",
    "https://openalex.org/W6683965311",
    "https://openalex.org/W6675720598",
    "https://openalex.org/W116732727",
    "https://openalex.org/W2034895394",
    "https://openalex.org/W2783270368",
    "https://openalex.org/W1987308214",
    "https://openalex.org/W6757010476",
    "https://openalex.org/W2058897163",
    "https://openalex.org/W2118978333",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W4205189720",
    "https://openalex.org/W2767943071",
    "https://openalex.org/W1999016464",
    "https://openalex.org/W2808889620",
    "https://openalex.org/W4310702739",
    "https://openalex.org/W4229453907",
    "https://openalex.org/W3159718431",
    "https://openalex.org/W2743247456",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3008201674",
    "https://openalex.org/W2946010671",
    "https://openalex.org/W3144183308",
    "https://openalex.org/W1513732820",
    "https://openalex.org/W3120604493",
    "https://openalex.org/W4290802519",
    "https://openalex.org/W3165416686",
    "https://openalex.org/W2800761015",
    "https://openalex.org/W2161302203",
    "https://openalex.org/W2050032978",
    "https://openalex.org/W4385699414",
    "https://openalex.org/W3155068540",
    "https://openalex.org/W1966973332",
    "https://openalex.org/W2082719813",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W4225539031",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2990852342",
    "https://openalex.org/W6685268241",
    "https://openalex.org/W2967663220",
    "https://openalex.org/W3098949033",
    "https://openalex.org/W3165030534",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2101807845",
    "https://openalex.org/W3100360708",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3123786616",
    "https://openalex.org/W3102780218",
    "https://openalex.org/W3099506237",
    "https://openalex.org/W3103602890",
    "https://openalex.org/W3103216244",
    "https://openalex.org/W2162931300"
  ],
  "abstract": "Space weather phenomena, including solar flares and coronal mass ejections, have significant influence on Earth. These events can cause satellite orbital decay due to heat-induced atmospheric expansion, disruption of GPS navigation and telecommunications systems, damage to satellites, and widespread power blackouts. The potential of flares and associated events to damage technology and disrupt human activities motivates prediction development. We use Transformer networks to predict whether an active region (AR) will release a flare of a specific class within the next 24 h. Two cases are considered: ≥C-class and ≥M-class. For each prediction case, separate models are developed. We train the Transformer to use time-series data to classify 24- or 48-h sequences of data. The sequences consist of 18 physical parameters that characterize an AR from the Space-weather HMI Active Region Patches data product. Flare event information is obtained from the Geostationary Operational Environmental Satellite flare catalog. Our model outperforms a prior study that similarly used only 24 h of data for the ≥C-class case and performs slightly worse for the ≥M-class case. When compared to studies that used a larger time window or additional data such as flare history, results are comparable. Using less data is conducive to platforms with limited storage, on which we plan to eventually deploy this algorithm.",
  "full_text": "TYPE Original Research\nPUBLISHED 08 January 2024\nDOI 10.3389/fspas.2023.1298609\nOPEN ACCESS\nEDITED BY\nRichard James Morton,\nNorthumbria University, United Kingdom\nREVIEWED BY\nReinaldo Roberto Rosa,\nNational Institute of Space Research\n(INPE), Brazil\nFrancesco Marchetti,\nUniversity of Padua, Italy\nSpiridon Kasapis,\nNational Aeronautics and Space\nAdministration, United States\n*CORRESPONDENCE\nFadil Inceoglu,\nfadil.inceoglu@colorado.edu\nRECEIVED 21 September 2023\nACCEPTED 13 December 2023\nPUBLISHED 08 January 2024\nCITATION\nPelkum Donahue K and Inceoglu F\n(2024), Forecasting solar flares with a\ntransformer network.\nFront. Astron. Space Sci. 10:1298609.\ndoi: 10.3389/fspas.2023.1298609\nCOPYRIGHT\n© 2024 Pelkum Donahue and Inceoglu.\nThis is an open-access article distributed\nunder the terms of the Creative\nCommons Attribution License (CC BY) .\nThe use, distribution or reproduction in\nother forums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which does\nnot comply with these terms.\nForecasting solar flares with a\ntransformer network\nKeahi Pelkum Donahue 1 and Fadil Inceoglu 2,3*\n1Nederland Middle-Senior High School, Nederland, CO, United States, 2Cooperative Institute for\nResearch in Environmental Sciences, University of Colorado Boulder, Boulder, CO, United States,\n3National Centers for Environmental Information, National Oceanic and Atmospheric Administration,\nBoulder, CO, United States\nSpace weather phenomena, including solar flares and coronal mass ejections,\nhave significant influence on Earth. These events can cause satellite orbital\ndecay due to heat-induced atmospheric expansion, disruption of GPS navigation\nand telecommunications systems, damage to satellites, and widespread power\nblackouts. The potential of flares and associated events to damage technology\nand disrupt human activities motivates prediction development. We use\nT ransformer networks to predict whether an active region (AR) will release a\nflare of a specific class within the next 24 h. Two cases are considered: ≥C-\nclass and ≥M-class. For each prediction case, separate models are developed. We\ntrain the T ransformer to use time-series data to classify 24- or 48-h sequences\nof data. The sequences consist of 18 physical parameters that characterize an\nAR from the Space-weather HMI Active Region Patches data product. Flare\nevent information is obtained from the Geostationary Operational Environmental\nSatellite flare catalog. Our model outperforms a prior study that similarly used\nonly 24 h of data for the ≥C-class case and performs slightly worse for the\n≥M-class case. When compared to studies that used a larger time window or\nadditional data such as flare history, results are comparable. Using less data is\nconducive to platforms with limited storage, on which we plan to eventually\ndeploy this algorithm.\nKEYWORDS\nsolar flare, solar activity, machine learning, transformer, forecast\n1 Introduction\nSolar flares are often defined as intense, localized bursts of electromagnetic radiation\nfrom the Sun occurring on the timescale of minutes to hours ( Benz, 2017). Powerful flares\ncan cause satellite orbital decay due to heat-induced atmospheric expansion ( Schwenn,\n2006). Moreover, such flares are often associated with solar energetic particle events (SEPs)\nand coronal mass ejections (CMEs) ( Schwenn, 2006), which have the potential to induce\nfurther severe consequences. For example, SEPs, which may reach the Earth soon after\nthe flare does, can harm astronauts, disrupt GPS navigation, and damage satellite systems\n(Pulkkinen, 2007). CMEs, which can take from hours to days to reach the Earth, may cause\ngeomagnetically induced currents (Pulkkinen et al., 2003). These can corrode pipes, disrupt\ntelecommunications devices, and permanently damage transformers, potentially leading to\nwidespread and long-term power blackouts (Pulkkinen, 2007).\nIn light of such severe consequences, it has become the subject of much research to\npredict these events. However, the physical processes underlying solar flares are not well\nunderstood. It is suggested that the energy release is the result of magnetic reconnection,\nwhere accumulated energy in the magnetic field is impulsively released when the field falls\nFrontiers in Astronomy and Space Sciences 01 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nto a lower-energy state (Benz, 2017). Flares tend to occur in locations\nat which the magnetic field is strongly sheared ( Hagyard et al.,\n1984), where magnetic flux emerging from below the sheared\nfield can destabilize the magnetic structures, causing impulsive\nenergy release ( Choudhary et al., 1998 ). Photospheric magnetic\nfield shear and flux parameters are among those in the Space-\nweather HMI Active Region Patches (SHARPs) data product\n(Bobra et al., 2014 ), which is recorded by the Helioseismic and\nMagnetic Imager (HMI) aboard the Solar Dynamics Observatory\n(SDO) and consists of data collected since 2010. The SHARPs\ndata series contains multiple physical parameters characterizing the\nphotospheric vector magnetic field for automatically detected and\ntracked active regions (ARs) over the duration of their lifetimes at a\ncadence of 12 min (Bobra et al., 2014).\nV arious AI methods are used in heliophysics to tackle problems\nranging from transportation of the charged particles throughout the\nheliosphere ( Inceoglu et al., 2022a ) to detecting magnetic activity\nstructures on the Sun ( Jarolim et al., 2021 ; Inceoglu et al., 2022b )\nand to predicting CMEs (Inceoglu et al., 2018; Raju and Das, 2023),\nas well as predicting the occurrences of solar flares with a forecast\nwindow of 24–48 h. In 2015, Bobra and Couvidat (2015) trained\na support vector machine (SVM) algorithm on SHARPs data to\nclassify whether an AR would emit a flare. In 2018, Nishizuka et al.\n(2018) used vector magnetograms, along with 1,600 and 131 Å filter\nimages and soft X-ray emission light curves, to train a deep neural\nnetwork to predict whether an AR would release a flare of a specific\nmagnitude within 24 h. The authors later applied this algorithm to\ndevelop an operational solar flare prediction model (Nishizuka et al.,\n2021). Jonas et al. (2018) attempted a similar prediction task, but\nusing image data recorded by the Atmospheric Imaging Assembly\naboard the SDO satellite along with SHARPs data. The authors\nused 1,600, 171, and 193 Å images describing the photosphere,\nchromosphere, transition region, and corona. Florios et al. (2018)\nalso used image data, but instead calculated predictors from SHARPs\nline-of-sight (LoS) magnetograms to train multilayer perceptron,\nSVM, and random forest algorithms. Liu et al. (2019) conducted\none of the first studies fully utilizing the time dependence of the\nSHARPs datatset. The authors trained a long short-term memory\n(LSTM) network on various SHARPs photospheric magnetic field\nparameters and flare history parameters. A similar study was\nconducted byW ang et al. (2020), who only used photospheric vector\nmagnetic field parameters from the SHARPs data series to train\nan LSTM. Similar to Florios et al. (2018) , Li et al. (2020) used LoS\nSHARPs magnetograms, but instead trained a deep convolutional\nneural network (CNN).Ribeiro and Gradvohl (2021)used ensemble\nlearning methods to combine the predictions of SVM, Random\nForest, and Light Gradient Boosting Machine algorithms that had\nbeen trained on multiple datasets, including SHARPs. This study\ndid not use the input data as a time series, but retained some\ntemporal information by utilizing features calculated from the\ntime leading up to the flare. Transformers are another recently\ndeveloped type of neural network designed around self-attention\nmechanisms that enable the extraction of relevant data from a\nsequence ( Bahdanau et al., 2014 ) without the need for recurrence\n(V aswani et al., 2017). Thus far, only Abduallah et al. (2023) has\napplied this network to flare forecasting, developing an operational\nmodel using a Transformer + CNN + LSTM network.\nIn this study, we use Transformer networks to predict whether\nan AR will release a flare of a specific magnitude within 24 h after\nmeasurement. The rest of this paper is organized as follows.Section 2\ndescribes the data features, preprocessing steps, Transformer model\narchitecture, training process, and performance metrics. Section 3\ndescribes threshold selection and analyzes experimental results.\nSection 4 compares our results with those from prior studies and\ndiscusses next steps and further applications.\n2 Methods\n2.1 Data and preprocessing\nOur study utilizes the SHARPs data product, which consists\nof data recorded by the Helioseismic and Magnetic Imager\naboard the Solar Dynamics Observatory ( Bobra et al., 2014 ).\nThe HMI instrument measures the photospheric magnetic field\nevery 12 min for ARs that are tracked for their entire lifetimes\n(Bobra et al., 2014). From these measurements, various parameters\nare calculated that characterize the behavior of the magnetic field\nin each AR at each time step. The 18 parameters used in this\nstudy are described in Table 1, and are downloaded for ARs from\nMay 2010 through December 2022. T o ensure data quality, data\nare only downloaded if they meet the following conditions: 1) the\nabsolute value of the orbital velocity of the SDO satellite is less\nthan 3,500 m s−1 and 2) data are of high quality (the signal-to-noise\nratio is low and there are no problems with the observation, i.e., an\neclipse).\nFlare event data are obtained from X-ray flux measurements\nmade by the Geostationary Operational Environmental Satellite\n(GOES) operated by the National Oceanic and Atmospheric\nAdministration (NOAA). The GOES satellite detects solar flares\nthrough observation of X-rays, yielding a flare catalog providing\ninformation including event start/peak/end times, flare class, and\nNOAA active region number (Garcia, 1994). Flares are classified by\nthe peak soft X-ray (SXR) flux observed by the GOES satellite, as\nseen in Table 2. The GOES data are then used to classify the SHARPs\ndata by GOES flare class and to extract the measurements occurring\nbefore the peak time. SHARPs data for which no corresponding\nNOAA number can be found in the GOES catalog are ignored.\nThe result is a time series of SHARPs parameter values leading\nup to each flare and separated by flare class. The number of ARs\ncorresponding to each class are displayed in Table 2. It should be\nnoted that there are very few A-class events only because their\nemission may not be intense enough to be detected over the X-ray\nbackground (W ang et al., 2020).\nFollowing the generation of SHARPs data leading up to each\nflare, which are sorted by flare class, several preprocessing steps are\napplied. Missing timesteps are filled through linear interpolation,\nand NaN values at the beginning or end of the time series are filled\nusing backward/forward fill. This process avoids errors caused by\none or more parameters not being present at any time step. Any\nsequences that contain less than 48 h of data are removed. The\nresulting sequences (time-series) are then converted to first-degree\ndifference sequences, which consist of the changes from each data\npoint to the next. This process is highly computationally efficient and\ndoes not compromise performance.\nFrontiers in Astronomy and Space Sciences 02 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nTABLE 1 Summary of the 18 physical SHARPs parameters used in this study.\nKeyword Description Calculation\nUSFLUX[1] T otal unsigned flux ϕ = ∑|Bz |dA\nMEANGAM[1] Mean angle of field\nfrom radial\nγ = 1\nN ∑ arctan( Bh\nBz\n)\nMEANGBT[1] Horizontal gradient\nof total field\n|∇Btot| = 1\nN ∑ √( ∂B\n∂x )\n2\n+ ( ∂B\n∂y )\n2\nMEANGBZ[1] Horizontal gradient\nof vertical field\n|∇Bz| = 1\nN ∑ √( ∂Bz\n∂x )\n2\n+ ( ∂Bz\n∂y )\n2\nMEANGBH[1] Horizontal gradient\nof horizontal field\n|∇Bh| = 1\nN ∑ √( ∂Bh\n∂x )\n2\n+ ( ∂Bh\n∂y )\n2\nMEANJZD[1] V ertical current\ndensity\nJz ∝ 1\nN ∑ (\n∂By\n∂x − ∂Bx\n∂y )\nTOTUSJZ[1] T otal unsigned\nvertical current\nJztotal\n= ∑ |Jz|dA\nMEANALP[1] Characteristic twist\nparameter, α\nαtotal ∝ ∑ JzBz\n∑ B2\nz\nMEANJZH[1] Current helicity (Bz\ncontribution)\nHc ∝ 1\nN ∑ BzJz\nTOTUSJH[1] T otal unsigned\ncurrent helicity\nHctotal\n∝ ∑ |BzJz|\nABSNJZH[1] Absolute value of the\nnet current helicity\nHcabs\n∝ | ∑ BzJz|\nSA VNCPP[1] Sum of the modulus\nof the net current per\npolarity\nJzsum\n∝ |∑B+\nz JzdA| + |∑B−\nz JzdA|\nMEANPOT[1] Proxy for mean\nphotospheric excess\nmagnetic energy\ndensity\nρ ∝ 1\nN ∑ (BObs − BPot)2\nTOTPOT[1] Proxy for total\nphotospheric\nmagnetic free energy\ndensity\nρtot ∝ ∑ (BObs − BPot)2dA\nMEANSHR[1] Shear angle Γ = 1\nN ∑ arccos( BObs⋅BPot\n|BObs||BPot| )\nSHRGT45[1] Fractional area with\nshear >45°\narea with shear >45°/HARP\narea\nR_V ALUE[2] Flux contribution\nsurrounding\npolarity-inversion\nlines[1]\nW eighted unsigned flux density\nsummed over all instrument\npixels in a 160 × 160 pixel box\ncentered on the region[2]\nAREA_ACR[1] De-projected area of\nactive pixels on\nsphere in\nmicro-hemisphere\nArea of the strong active pixels\ndetermined from the\nline-of-sight field\nEach row displays the name/keyword of the parameter, its physical meaning, and how it is\ncalculated from magnetic field measurements.\n[1] Bobra et al. (2014), with updates at http://jsoc.stanford.edu/\n[2] Schrijver (2007).\nAfter the data preparation process, input data are generated. The\nfollowing process generates the 24-h data window, but the general\nprocess remains the same for the 48-h case. First, for each sequence\nin each flare class, data recorded until at least 48 h before the flare\npeak are defined as pre-window and data recorded within 48 h of\nthe flare peak are defined as intra-window. The resulting sequences\nare then divided into positive and negative groups, depending on\nthe prediction case (≥C vs. ≥M). For instance, for the ≥C-class case,\nthe negative set consists of all A-class, B-class, pre-window C-, M-,\nand X-class sequences while the positive set consists of intra-window\nC-, M-, and X-class sequences. This split is depicted inFigure 1. Note\nthat this figure only depicts the ≥C-class prediction case, as the flare\nmagnitude cutoff (negative vs. positive) would change for the ≥M-\nclass case. Next, for each sequence (positive or negative), different\n24-h long sequences starting at random times are generated. These\nare the sequences that are fed into the model. Generating random\ndata sequences at different times enables within-24-h prediction,\ninstead of selecting a fixed window. This data generation process\n(random 24/48-h sequence extraction) is done separately for each\nprediction case (flare class and data duration), so no two models have\nthe same input data set.\nAfter generating model input data, we shuffle the data and\nrandomly split it into training, validation, and testing data for the\nmachine learning model. In this study, 20% of the data are used\nas testing data, while 80% are used as training data. Of the 80%\nthat is allotted to training, 20% are used for validation. This is\nalso done separately for each prediction case, yielding different data\nsets for each model. After train/test splitting, input data sets (train,\ntest, validation) are normalized by scaling relative to the maximum\nvalues for the corresponding parameter found in the training data.\nIn addition, we acknowledge that although methods such as k-\nfold cross-validation would provide more statistical robustness to\nthe results, they are not feasible computationally. Even with 2 x\nRTX4090 GPUs with a total of 128 GB of memory, computation\nis limited because each of the training data sets is over 35 GB,\nand the hyperparameter search space is unconventionally large (see\nSection 2.2). Therefore, we do not include such methods.\nBecause 24-h sequences are generated at random times within\neach data window, some sequences may overlap. When sequences\nare then split randomly into training and testing data, some parts\nof sequences may appear in both the training and testing data. W e\nacknowledge that this may cause slight artificial gain in performance\nof the model on the testing set. This could be addressed by separation\nof training/testing data by year. However, it has been shown that\nseparating training/testing data by year yields significantly different\nresults depending on the years selected ( W ang et al., 2020), which\nmay be due to the solar cycle. For instance, W ang et al. (2020)\ndemonstrated that the True Skill Statistic (TSS) may change by more\nthan 0.20 TSS units for the ≥M-class case, depending on which year\nis selected for testing.\nDue to the rarity of severe flare events, the training data set is\nhighly imbalanced, meaning that there are vastly fewer severe flares\n(≥M or ≥C, depending on the prediction case) than non-severe\nevents. This is unsuitable for model training for flare prediction\npurposes. If a model is trained on a data set containing a vast\nmajority of negative examples, it will simply learn to predict that\nFrontiers in Astronomy and Space Sciences 03 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nTABLE 2 Flare classification system and number of corresponding active regions used in this study.\nClass A B C M X\nSXR Flux (W m−1) 10 –8 − 10–7 10–7 − 10–6 10–6 − 10–5 10–5 − 10–4 >10−4\nNumber of ARs 7 4,873 4,742 521 41\nThe second row shows the soft X-ray flux (SXR) levels corresponding to each flare class, as measured by GOES (See Fletcher et al., 2011). In this study, two prediction cases are considered:\n≥C-class and ≥M-class. The bottom row displays the number of active regions in each class for which data was retrieved from the SHARPs data product and matched with a specific flare event\nfrom the GOES flare catalog using a NOAA identification number.\nFIGURE 1\nData splitting methodology for the ≥C-class prediction case using 24 h of data. Each grey block represents a pre-flare sequence of SHARPs time-series\ndata. If the corresponding flare is an A- or B-class flare (the top grey block), all generated sequences are classified as negative (as represented by red\narrows, 24 h in length). For C-, M-, or X-class flares (the bottom row), sequences ending within 24 h of the flare peak time (t_flare) are labeled as positive\n(depicted as green arrows in the figure). Since the sequences are 24 h long, any sequences starting within 48 h of the flare peak (after t_flare −48 h) are\nlabeled as positive. This results in any 24-h sequences in which a ≥C-class flare occurs within 24 h after measurement being classified as positive.\nall examples are negative. While this may achieve high accuracy,\nit does not have any predictive value. T o overcome this issue,\nwe undersample the negative class by randomly removing enough\nnegative examples to yield a balanced training data set. The\nvalidation set remains imbalanced, enabling model parameters to\nstill be optimized according to the imbalanced nature of flare\nprediction. The undersampling process is also done separately for\neach prediction case. Nishizuka et al. (2018), Liu et al. (2019) , and\nW ang et al. (2020) all use various cost functions to address the\nimbalance (usually a form of cross-entropy) and do not use data\nbalancing techniques. While we do use a similar cost function,\nwe also use undersampling techniques, leading to more efficient\ncomputation and data storage.\n2.2 Network architecture and training\nThe Transformer network utilizes self-attention mechanisms\nto process time series data without the need for recurrence\n(V aswani et al., 2017). It was originally designed for sequence\nmodeling and transduction, specifically natural language processing\ntasks such as translation and English constituency parsing. Since\n2017, it has been applied successfully to video action detection\n(Girdhar et al., 2019), skin lesion analysis (He et al., 2022), anomaly\ndetection ( T uli et al., 2022 ), protein prediction ( Nambiar et al.,\n2023), earthquake location ( Münchmeyer et al., 2021), and further\ndiverse applications. The multivariate time series data of the\nphotospheric magnetic field parameters may be suitable to the\napplication of the Transformer. Self-attention mechanisms enable\na model to extract the most important parts of a sequence\n(Bahdanau et al., 2014 ). They can learn interdependencies among\nvariables to process sequences such as time series data for tasks\nsuch as classification or translation. By eliminating recurrence\nthrough the use of these mechanisms, Transformer models are\nmore parallelizable, achieving state-of-the-art performance with\nsignificantly reduced processing times (V aswani et al., 2017).\nThe architecture of our Transformer model is shown inFigure 2.\nThe input sequence, which spans 24 h, is input into a Transformer\nencoder (orange block in Figure 2). The first residual connection\nsplits off at this point. The input is then normalized and sent to\nthe multi-head attention (MHA) block. This primarily consists of\nseveral scaled dot-product attention layers running simultaneously,\nenabling the model to ‘attend to’ different parts of the sequence\n(V aswani et al., 2017) and to extract the parts that are more relevant\nto classification. The resulting vector is then sent to a dropout layer,\nwhich is used to prevent the neural network from overfitting by\ndropping a specific proportion of units randomly ( Srivastava et al.,\n2014). The output of this layer is then added to the original input\nthat previously split off as a residual connection. The purpose of\nthis residual connection is to enable a channel of information\nflow that is unaffected by the attention mechanism, resulting\nFrontiers in Astronomy and Space Sciences 04 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nFIGURE 2\nOur model architecture. The input sequence is first passed into N\ntransformer encoder blocks (depicted in orange), which each include\na multi-head attention block and a feed-forward block. Then, after a\npooling layer (blue), there is a multi-layer perceptron (MLP) with\ndropout, depicted in green. The last yellow layer represents the final\noutput neuron, which is activated by the sigmoid function.\nin easier model optimization ( He et al., 2016 ). Another residual\nconnection splits off at this point, while the main information flow is\nnormalized and sent to a feed-forward network. W e use three layers\nof convolution kernels that are convolved with the normalized data\nin one dimension. Dropout layers are also added between Conv1D\nlayers. The output of the feed forward part is then added to the\nresidual connection that previously split off. The result is the output\nof the Transformer encoder. This output vector is then fed back into\nanother Transformer encoder. This is repeated for N transformer\nencoder blocks (bottom right of Figure 2). The output vector of\nthe Nth encoder block is sent to an average pooling layer that\nreduces its size and makes the model more robust to noise and small\nvariations (Boureau et al., 2010). The output is then sent to a multi-\nlayer perceptron (MLP). The purpose of the MLP is to process the\noutput vector of the Transformer encoder for binary classification.\nOur MLP consists of 3 densely connected layers followed by a\ndropout layer. Finally, there is an output layer of size one with the\nsigmoid activation function, which converts the single probability\nvalue returned by the final neuron to a probability of belonging\nto the positive class with a range of (0,1). The final class label is\nlater obtained by comparison to a chosen threshold; if the output\nlies above that threshold, it is classified as positive (otherwise it is\nclassified as negative).\nThe Transformer model is trained using the Adam optimizer\n(Kingma and Ba, 2015 ), with the learning rate as one of the\nhyperparameters to be optimized and all other parameters set to\ndefault. W e use the Binary Cross-Entropy loss function (shown in\nEq. 1) as the cost to be optimized. In this equation, N is the number\nof sequences and K is the number of classes.wk represents the weight\nof class k (which gives more importance to positive examples). ynk\nis the true class label, while ̂ynk is the predicted probability of the\nsequence belonging to the positive class.\nJ =\nN\n∑\nn=1\nK\n∑\nk=1\nwkynk log ( ̂ynk) (1)\nDuring training, the model is evaluated on three scores:\naccuracy, area under the receiver operating characteristic curve\n(ROC_AUC), and area under the precision-recall curve (PR_AUC).\nAccuracy indicates the proportion of all samples that are classified\ncorrectly. The ROC curve plots the true positive rate (recall) vs.\nfalse positive rate for thresholds ranging from 0 to 1 ( Marzban,\n2004). When the area under the curve (AUC) is evaluated, it\nprovides an overall measure of model performance across various\nclassification thresholds, with larger ROC_AUC values indicating\nbetter performance. W e use this metric because it does not require\nchoosing a specific threshold ( Marzban, 2004), enabling us to later\nselect a threshold to optimize performance. The precision-recall\ncurve plots precision and recall for the same threshold range (0–1) to\nshow the trade-off between precision and recall, and the area under\nthis curve indicates overall performance across thresholds. See\nSection 3.1 for further discussion of precision and recall. Another\nway of tackling this issue is to use a cost function, the so-called\nScore-Oriented Loss (SOL) function ( Marchetti et al., 2022), which\nprioritizes a selected evaluation metric to optimize the DL (deep\nlearning) model to maximize it.\nModel hyperparameter optimization is achieved using Bayesian\noptimization, a method which enables selection of hyperparameters\nbased on results of prior combinations ( Wu et al., 2019 ). The\nhyperparameters, search spaces, and optimized values are shown\nin Table 3. The objective of the Bayesian optimization method is\nto maximize the ROC_AUC achieved on the validation set. The\nmaximum number of trials, or hyperparameter combinations tested,\nis set to 10. The number of executions per trial is set to 1. These\nparameters are limited due to the lack of computational power\navailable, as described previously. Limitations are exacerbated by\nour extended optimization process, which includes hyperparameters\nbeyond those conventionally examined (See Table 3). The various\nmodel configurations are tested with 100 epochs, but are set\nto stop training when the ROC_AUC does not improve over 5\nepochs. Once the best hyperparameters have been found through\nFrontiers in Astronomy and Space Sciences 05 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nTABLE 3 Hyperparameter search space and optimal values.\nOptimal values\nParameter Search space C-24 C-48 M-24 M-48\nNumber of Transformer Blocks [1, 2, 3, 4] 4 4 2 3\nOptimizer Learning Rate [ .0001, .001, 0.01] 0.001 0.0001 0.0001 0.001\nMHA Head Size [32, 64, 128, 256] 32 128 32 64\nNumber of Heads in MHA [2, 4, 8] 8 8 4 4\nDimensionality of Conv1D Output [2, 4, 8] 8 2 4 4\nTransformer Dropout Rate [0.0, 0.1, 0.2, 0.3, 0.4, 0.5] 0.1 0.0 0.1 0.3\nUnits in 1st MLP Layer [256, 128, 64, 32] 128 128 32 64\nMLP Dropout Rate [0.0, 0.1, 0.2, 0.3, 0.4, 0.5] 0.1 0.4 0.3 0.1\nEach row lists the hyperparameter, the potential values that are sought to be optimized, and the values that produced the best results for each prediction task and data window duration.\nBayesian optimization, the model is re-trained using the selected\nvalues and a batch size of 32, then saved. The methods described\nin this section are implemented in Python with T ensorflow and\nKeras.\n2.3 Performance metrics\nOnce the model has been trained on the training/validation data\nsets, it is evaluated on the test set, which consists of previously\nunseen sequences. First, a threshold is chosen to obtain predicted\nclass labels. Then, a confusion matrix is generated by calculating\nthe number of correctly and incorrectly predicted examples in each\nclass. Figure 3 illustrates a confusion matrix and lists values for\neach of the four prediction cases. When an example sequence is\nclassified by the model, there are four possible outcomes relative to\nthe correct label: 1) True Positive (TP): the model correctly classifies\nit as positive, 2) False Negative (FN): the model incorrectly classifies\nit as negative, 3) False Positive (FP): the model incorrectly classifies\nit as positive, and 4) True Negative (TN): the model correctly\nclassifies it as negative. The confusion matrix shows the number\nof examples resulting in each of these outcomes. Any metrics not\nreported in this paper can be calculated from the values given\nin Figure 3.\nOnce the four values are calculated (TP , FN, FP , TN), model\nperformance can be assessed through the calculation of performance\nmetrics. These metrics are described in Table 4. Accuracy (ACC)\ndescribes the total proportion of examples that the model correctly\nclassifies. This metric should not be used in imbalanced classification\ntasks such as flare prediction because a model may reach high\naccuracy by learning to classify all examples as belonging to\nthe majority class ( He and Garcia, 2009 ). Despite achieving high\naccuracy, such a model would have no predictive value. Instead\nof accuracy, balanced accuracy (BACC) is used. This metric is\nequivalent to accuracy calculated with weighted values determined\nby the class imbalance, and is calculated by averaging the true\npositive rate (recall) and the true negative rate ( Brodersen et al.,\n2010). The true positive rate (also known as recall) is the proportion\nof positive examples that the model correctly classifies. Likewise, the\ntrue negative rate is the proportion of negative examples correctly\nclassified. In addition to ACC and BACC, we use precision (PRE)\nand recall (REC) to provide information concerning the ability of\nthe model to correctly identify positive (severe) flares. Precision\nindicates the accuracy of positive predictions. This metric is useful\nin describing the reliability of severe flare predictions. Precision and\nrecall are especially relevant in cases like flare prediction where it\nis important for any model deployed in an operational setting to\nnot ‘miss’ any severe events but also to not have too many false\nalarms. T o incorporate both of these ideas, Hanssen and Kuipers\n(1965) developed the True Skill Statistic (TSS). This is calculated\nby subtracting the false alarm rate from the recall value, balancing\nPRE and REC. While a model that is trained to frequently predict\nthe positive case will have a high recall value, it will also often\nhave many false alarms, thereby reducing precision. Such a model\nwould produce a low TSS score, providing more useful information\nregarding the predictive value of the model. This metric is especially\nsuitable to imbalanced prediction cases such as flare prediction\nbecause it is unaffected by the class imbalance ( W oodcock, 1976).\nLastly, the Heidke Skill Score (HSS) is used, a metric which describes\nhow the model performs relative to a random chance prediction\n(Heidke, 1926). This provides useful information on the predictive\nvalue of the model and has the benefit of using all four confusion\nmatrix values (Bloomfield et al., 2012), although it should be noted\nthat TSS also uses all four values.\n3 Results\n3.1 Performance metric scores\nThe output of the model for a single example is the probability\nof belonging to the positive class, which ranges from 0 to 1. In\norder to obtain a class label, it must be compared to a threshold\n(above which it is classified as positive). W e select this threshold such\nFrontiers in Astronomy and Space Sciences 06 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nFIGURE 3\nConfusion matrices. The matrix on the left depicts a generic confusion matrix, where each cell shows the number of examples corresponding to that\noutcome. TP is T rue Positive, FN is False Negative, FP is False Positive, and TN is T rue Negative. The results for each of the 4 models are shown on the\nright. These values are then used to calculate model evaluation metrics (see Section 3.1)\nTABLE 4 Definitions of various model evaluation metrics.\nCalculation Meaning\nACC[1] TP+TN\nTP+FP+TN+FN Proportion of examples\ncorrectly classified\nBACC[2] 1\n2 ( TP\nTP+FN + TN\nTN+FP ) Averaged true positive\nrate and true negative\nrate\nPRE[1] TP\nTP+FP Accuracy of positive\n(severe) predictions\nREC[1] TP\nTP+FN Fraction of positive\nexamples correctly\nidentified\nTSS[3] TP\nTP+FN − FP\nTN+FP Recall minus false alarm\nrate\nHSS[4] 2(TP×TN−FP×FN)\n(TP+FN)(FN+TN)+(TP+FP)(FP+TN) Fractional performance\nrelative to random\nchance\nThe metrics considered are accuracy (ACC), balanced accuracy (BACC), precision (PRE),\nrecall (REC), True Skill Statistic (TSS), and Heidke Skill Score (HSS). Each row displays the\nmetric abbreviation, how it is calculated from confusion matrix values, and what it means in\nterms of model performance.\n[1] He and Garcia (2009).\n[2] Brodersen et al. (2010).\n[3] Bloomfield et al. (2012).\n[4] Heidke (1926).\nthat overall performance is maximized. W e calculate each of the six\nmetrics for all thresholds in the range 0.05–0.95. Figure 4 displays\neach metric as a function of threshold for all four models. The\ngeneral shapes of the performance curves do not vary dramatically\nwith different data window lengths (24 vs. 48 h). However, there\nis a significant difference between the curve shapes for the two\nclass cases ( ≥M vs. ≥C). While the ≥C-class curves have an area\nwhere performance is clearly optimized, the ≥M-class curves are\nrelatively flat on the left side of the graphs; the overall model\nperformance for all thresholds less than ∼0.5 is very similar. This\ncan be explained by the distribution of model output probabilities.\nFor the 24-h data window case for ≥ M-class case, 82% of the model\noutput probabilities lie either under 0.1 or over 0.6 (for the ≥C-class\nvariation, this figure drops to 64%). This means that relatively few\nflares produce output probabilities between 0.1 and 0.6, resulting in\nflat skill score curves in the range.\n3.1.1 Accuracy and balanced accuracy\nThe maximum scores of each metric for each prediction case\n(flare class and data window) are displayed in Table 5. The table also\ndisplays the thresholds at which these maxima occur. Note that these\nscores are not our final model scores. For the≥C-class case with 24 h\nof data, a maximum accuracy of 0.811 is reached with a threshold of\n0.76. For the 48-h case, a score of 0.816 is reached with a threshold of\n0.72. When considering the≥M-class case, the 24-h and 48-h models\nreach accuracy maxima of 0.968 and 0.965, respectively. Both of\nthe ≥M-class accuracy scores are achieved with a threshold of 0.95.\nThese accuracy values, while high, are not necessarily indicative of\nmodel predictive value due to the imbalanced nature of the test set.\nEven though the training set is balanced to avoid disproportionately\nweighting negative examples in model training, the test set is kept\nimbalanced as it must accurately reflect operational forecasting in\nwhich there is no knowledge of final class labels. For instance, if a\nmodel were to classify all flares as negative, it would achieve 70%\naccuracy in the 24-h ≥ C-class case and 97% accuracy in the 24-h\n≥ M-class case. Such a model would have no predictive value. This\nis seen in Figure 4, where the ≥M-class accuracy scores are both\noptimized when the threshold is 0.95. This is because with such a\nhigh threshold, the model predicts nearly all flares to be negative,\nthereby increasing the accuracy. W e do not use accuracy to select\nthe optimal threshold due to this influence of the class imbalance.\nBalanced accuracy shows a much more useful curve with a peak\nthat is not defined by the class imbalance. For the ≥C-class case, the\n24- and 48-h models achieve maximum BACC values of 0.795 and\n0.808 at thresholds 0.6 and 0.47, respectively. For the ≥M-class case,\nthe 24-h model reaches BACC = 0.834 with a threshold of 0.47, while\nthe 48-h model reaches BACC = 0.846 with a threshold of 0.49. These\nFrontiers in Astronomy and Space Sciences 07 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nFIGURE 4\nEvaluation metrics across thresholds for all four prediction cases ( ≥C-class with a 24-h data window, ≥C-class with a 48-h data window, ≥M-class with\na 24-h data window, and ≥M-class with a 48-h data window). Six metrics were calculated for test set predictions for all thresholds between 0.05 and\n0.95, inclusive. The threshold chosen, which is shown with a vertical red line, is where the maximum TSS score occurs. Threshold values yielding\nmaximum performance are shown in Table 6.\nthresholds are eventually chosen to optimize overall performance, as\ndepicted by the vertical red lines in Figure 4.\n3.1.2 Precision and recall\nAcross all models, maximum precision values are attained with\nthresholds near 0.95. The PRE scores for the ≥C-class case are 0.901\nfor the 24-h case and 0.936 for the 48-h case. These high thresholds\nand scores are the result of the inherent nature of precision. A high\nthreshold will result in the model only classifying an event as positive\nwhen its probability of being severe is extremely high, resulting in\nextremely reliable positive predictions. This trend can be seen in\nFigure 4, where precision scores increase almost monotonically. The\nFrontiers in Astronomy and Space Sciences 08 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nTABLE 5 Maximum Skill Scores by Metric. For each metric and prediction case (flare class and data window), the maximum score achieved by each metric is\ndisplayed, along with the threshold yielding that score. This is done separately for each metric and prediction case, and is not our final result.\n(hr) ACC BACC PRE REC TSS HSS\n≥C, 24\nV alue 0.811 0.795 0.901 0.995 0.591 0.548\nThreshold 0.76 0.60 0.93 0.05 0.60 0.72\n≥C, 48\nV alue 0.816 0.808 0.936 0.989 0.615 0.586\nThreshold 0.72 0.47 0.95 0.05 0.47 0.63\n≥M, 24\nV alue 0.968 0.834 0.262 0.992 0.667 0.243\nThreshold 0.95 0.47 0.95 0.05 0.47 0.85\n≥M, 48\nV alue 0.965 0.846 0.359 0.975 0.692 0.250\nThreshold 0.95 0.49 0.94 0.05 0.49 0.90\nhighest precision scores for the ≥M-class case are 0.262 for the 24-h\nvariation, and 0.359 for the 48-h variation. While these scores occur\nwhen the threshold is near 0.95 for the same reason as described in\nthe ≥C-class case, they are significantly lower. This is because there\nare many more false positives compared to true positives, causing\nsevere predictions to become less reliable.\nAll four models ( ≥C with 24 h of data, ≥C with 48 h, ≥M with\n24 h, and ≥M with 48 h) attain maximum recall scores when the\nthreshold is 0.05. For the≥C-class case, scores of 0.995 and 0.989 are\nreached for the 24-h and 48-h cases, respectively. For the ≥M-class\ncase, the scores are 0.992 and 0.975. While this pattern of extremely\nhigh scores at the extremes of the threshold range is similar to\nthat observed in precision, it shows the opposite trend. While\nprecision increases with threshold, recall decreases monotonically.\nThe reason for this is that a low threshold will result in many positive\npredictions, thereby reducing the number of severe events that the\nmodel does not identify, as expressed in a high recall value. Due\nto the uniform optimization of precision and recall through near-\n1 and near-0 thresholds, we do not use these metrics to select the\noptimal threshold.\n3.1.3 Heidke skill score and true skill statistic\nThe ≥ C-class models achieve maximum HSS scores of 0.548 for\nthe 24-h variation (threshold = 0.72) and 0.586 for the 48-h variation\n(threshold = 0.63). When considering the ≥M-class case, the 24-h\nmodel reaches HSS = 0.243 (threshold = 0.85) and the 48-h model\nreaches HSS = 0.250 (threshold = 0.90). It is observed in Figure 4\nthat, like the precision curves, the HSS curves are far lower for the\n≥M-class case than for the ≥C-class case. A large number of false\npositives relative to true positives, as seen in Figure 3, may result in\nthis trend. HSS, which is affected by the class imbalance (W oodcock,\n1976), is not used in threshold selection.\nFor the ≥C-class case, the 24- and 48-h models achieve\nmaximum TSS values of 0.591 and 0.615 at thresholds 0.6 and 0.47,\nrespectively. For the ≥M-class case, the 24-h model reaches TSS =\n0.667 with a threshold of 0.47, while the 48-h model reaches TSS =\n0.692 with a threshold of 0.49.\nThese optimal thresholds are identical to those of balanced\naccuracy. TSS is unaffected by the class imbalance (W oodcock, 1976)\nand is thus used to select the final classification thresholds. TSS\nhas been shown to provide the best metric for flare forecasting\nperformance comparison ( Bloomfield et al., 2012 ). As TSS and\nBACC are optimized at identical thresholds, the final values\nchosen reflect the metrics that are unaffected by class imbalance\n(BACC and TSS).\n3.2 Common threshold selection\nThe selected thresholds are 0.6 for the 24-h ≥ C-class case, 0.47\nfor the 48-h ≥ C-class case and 24-h ≥ M-class case, and 0.49 for the\n48-h ≥ M-class case. This yields the scores shown in Table 6. BACC,\nREC, and TSS scores are higher for the ≥M-class case than for the\n≥C-class case. These metrics all increase with a higher true positive\nrate, indicating that the models generally misses more≥C-class flares\nthan ≥ M-class flares. This means that, if a severe flare occurs, the\nmodels are more likely to correctly distinguish it from weaker flares\nif they are trained to recognize ≥M-class flares. For PRE and HSS,\nthe pattern is reversed: performance is significantly higher in the\n≥C-class case. This may be explained by the large number of false\npositives when predicting ≥M-class flares. If the ≥M-class models\nare more prone to predicting too many examples as positive, it would\nresult in more false positives. This would decrease precision and HSS\nscores while boosting BACC, REC, and TSS scores (which is also\nobserved). This also explains why precision and HSS scores for the\n≥M-class cases are significantly lower than all other scores.\nThe skill scores for both the ≥C-class and ≥M-class models\nshow an increase in nearly all metrics when using 48 h of data\ninstead of 24 h. This suggests that there are certain indicators\noccurring more than 48 h before severe flares that help the model\ndistinguish such regions. The performance increase is slightly larger\nfor the ≥C-class case, suggesting that these indicators are more\nimportant in this prediction task when compared to identifying\nonly more powerful flares ( ≥M). The performance increase is\nalso significantly greater in precision and HSS scores (7%–26%\nFrontiers in Astronomy and Space Sciences 09 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nTABLE 6 Experimental Results: Skill Scores.\nClass Data window (hrs) Threshold ACC BACC PRE REC TSS HSS\n≥C\n24 0.60 0.779 0.795 0.591 0.835 0.591 0.528\n48 0.47 0.794 0.808 0.641 0.849 0.615 0.569\n≥M\n24 0.47 0.759 0.834 0.106 0.913 0.667 0.143\n48 0.49 0.788 0.846 0.132 0.908 0.692 0.180\nThe ‘Class’ column shows which flare class the models are predicting. ‘Data window’ indicates the duration of the data window used to achieve within 24-h prediction. The ‘Threshold’ column\nindicates the chosen threshold to convert decimal model output to binary class labels. The rest of the columns report experimental results for each of the four models, calculated from confusion\nmatrices (see Section 3.1).\nincrease instead of <5%). The reason for this can be seen in the\nconfusion matrices ( Figure 3): increasing the data duration to 48 h\ndramatically decreases the number of false positives. This means\nthat severe flare predictions become more reliable with larger time\nwindows.\n4 Discussion\n4.1 Performance comparison\nTable 7 displays skill scores for various models developed in\nprior studies. The most direct comparison can be made with the\nLSTM model developed by W ang et al. (2020), as they did not use\nany additional data such as flare history or satellite images. The\nauthors also use 24-h sequences of SHARPs time-series data, but\ninclude 20 different SHARPs parameters, as opposed to 18 used in\nthis study. While we randomly select test data, they split the data\nby year. The authors developed two models with different testing\nyears. Here we compare our results with their highest-performing\nmodel. Metrics displayed here are calculated from confusion matrix\nvalues. When using the same duration of data (24 h), our model\noutperforms their LSTM in terms of BACC, REC, and TSS for\nthe ≥C-class case. In terms of accuracy, precision, and HSS, the\nLSTM outperforms our models. This is consistent with the nature\nof the metrics: our model has a higher true positive rate, which is\nnaturally accompanied by more false positives. When using TSS to\nindicate overall performance ( Bloomfield et al., 2012 ), our model\nachieves better performance overall for the ≥C-class case when\nusing the same data duration. When using 48 h of data instead,\nour Transformer achieves higher BACC, REC, TSS, and HSS scores.\nFor the ≥M-class case, our 24-h model achieves higher scores\nonly in recall. Using 48 h of data instead, our model compares\nmore favorably, scoring higher in BACC, REC, and TSS. For both\nthe ≥C- and ≥M-class prediction cases, the score increases again\ndemonstrate the positive impact of longer data windows on model\nperformance.\nLiu et al. (2019) used 10-h sequences of 25 physical SHARPs\nparameters in addition to flare history data to predict flares within\n24 h with an LSTM with attention architecture. Flare history\nparameters enable the model to access information concerning past\nflares within that active region. This additional data proved to be an\nimportant discriminating feature for models that included them, as\ndemonstrated in the feature assessment conducted by the authors.\nThey also used the full data set instead of undersampling the negative\nclass. For the≥C-class case, our 24-h model outperforms their LSTM\nonly in terms of precision and recall. In this case, their model is better\noverall when TSS is used as an indicator of overall quality. However,\nwhen 48 h of data are used instead, our model demonstrates better\nperformance despite not using flare history data, scoring higher in\nBACC, PRE, REC, TSS, and HSS. For the≥M-class case, their model\noutperforms our Transformers across all metrics, regardless of data\nwindow duration.\nThe random forest (RF) model developed byFlorios et al. (2018)\nalso used SHARPs data, but the authors calculated predictors from\nnear-real-time (NRT) LoS magnetograms instead of using definitive\nSHARPs parameters as model input. NRT data, which are available\nearlier, can have small differences from definitive data due to lack\nof certain data correction and calibration steps ( Hoeksema et al.,\n2014). Also unlike the previously compared studies, they do not\nuse data in a time-series format. Because the authors did not\nprovide confusion matrix values, we are only able to compare\naccuracy, TSS, and HSS. In the ≥C-class case, their RF achieves\nhigher scores than our 24-h model, while our 48-h model scores\nslightly higher in terms of TSS. For the ≥M-class case, their model\noutperforms our Transformers for both data durations, with a 12%\nTSS increase over the 24-h model and an 8% increase over the\n48-h model.\nLi et al. (2020) attempted the same prediction task with a\nsignificantly different approach. The authors used a deep CNN\nto classify SHARPS LoS magnetograms by whether the ARs\nobserved release a flare of a specific magnitude within 24 h. Like\nFlorios et al. (2018), they do not use a time-series format. The only\narea where our Transformer models achieve higher performance\nthan the CNN is in terms of recall for both the 24-h and\n48-h ≥M-class cases. For the ≥C-class case, their CNN achieves a\n15% higher TSS score than our 24-h model and an 11% higher\nscore than our 48-h model. In the ≥M-class case, the CNN\nscores 13% and 9% higher than our 24- and 48-h Transformers,\nrespectively.\n4.2 Conclusion\nW e develop Transformer networks to predict whether an\nAR will release a flare of a specific magnitude within 24 h. The\nFrontiers in Astronomy and Space Sciences 10 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nTABLE 7 Comparison with Skill Scores Obtained in Prior Studies.\nModel ACC BACC PRE REC TSS HSS\n≥C\nThis study (24 h) Transformer 0.788 0.795 0.608 0.810 0.589 0.537\nThis study (48 h) Transformer 0.804 0.806 0.667 0.811 0.612 0.580\nW ang et al. (2020) LSTM 0.858 0.779 0.672 0.643 0.559 0.568\nLiu et al. (2019) LSTM 0.829 0.803 0.544 0.762 0.607 0.539\nFlorios et al. (2018) RF 0.84 — — — 0.60 0.59\nLi et al. (2020) CNN 0.861 0.840 0.906 0.889 0.679 0.671\n≥M\nThis study (24 h) Transformer 0.788 0.830 0.115 0.876 0.661 0.158\nThis study (48 h) Transformer 0.811 0.843 0.142 0.878 0.687 0.196\nW ang et al. (2020) LSTM 0.945 0.840 0.276 0.730 0.681 0.378\nLiu et al. (2019) LSTM 0.909 0.895 0.222 0.881 0.790 0.347\nFlorios et al. (2018) RF 0.93 — — — 0.74 0.49\nLi et al. (2020) CNN 0.891 0.875 0.889 0.816 0.749 0.759\nLSTM indicates the use of a long short-term memory network, RF indicates a random forest, and CNN indicates a convolutional neural network. W ang et al. (2020)developed two models, one\nusing ARs from 2015 for testing and one using ARs from 2015 to 2018. For this comparison, the 2015-testing model was used, which yielded higher overall performance ( W ang et al., 2020).\nMetrics are calculated from confusion matrix values provided. Florios et al. (2018) did not provide the necessary confusion matrices, so only the metrics reported are shown.\nmodel is trained on time-series data consisting of 18 physical\nparameters that characterize an AR from the Space-weather HMI\nActive Region Patches (SHARPs) data product, obtained from\nhttp://jsoc.stanford.edu/ for ARs from May 2010 through December\n2022. After the data are sorted by flare class using the GOES flare\ncatalog and preprocessed, example sequences are generated for\neach of the four prediction cases: ≥C-class using 24 h of data,\n≥C-class using 48 h of data, ≥M-class using 24 h of data, and ≥M-\nclass using 48 h of data. The data are then split randomly, with\n80% being used for model training and 20% for testing. After\nnormalization and undersampling of the negative class to yield\na balanced data set, the data are prepared for model input. The\nmodels used primarily consist of an attention-based Transformer\nencoder followed by a multi-layer perceptron. Ideal model\nhyperparameters are chosen through Bayesian optimization, and\nthe classification probability threshold is selected to yield maximum\noverall performance. The primary conclusions from this study are\nas follows:\n1. The model architecture, which was designed for sequence\ntransduction tasks, is applied to developing a network for\ntime series classification for flare prediction. Model input\nconsists of only 24 or 48 h of time-series SHARPs data, in\ncontrast to the larger time windows, additional parameters,\nor image data used in prior studies. Data size is further\nreduced in the undersampling process used to balance the\ndata set.\n2. An analysis of model evaluation metrics concludes that of the\nmetrics assessed, the True Skill Statistic (TSS) provides the\nmost accurate information on a model’ s predictive value. This is\nused for probability classification threshold selection and model\ncomparison.\n3. Model performance is optimized across all four Transformer\nmodels when the threshold (the probability value above which\ndata sequences are classified as positive) is close to 0.5,\nwith slight variations depending on data window length and\nflare class.\n4. For the ≥C-class case, both of our Transformer models\noutperform a prior study that similarly used comparatively\nlimited data. For the ≥M-class case, our 48-h model performs\nslightly better while the 24-h model performs slightly worse.\nWhen our models are compared to studies that used additional\ndata such as flare history or different data (e.g., satellite\nimages), performance is comparable. For the≥C-class case, using\n48 h of input data for our Transformer network yields higher\nperformance than all prior studies compared that used physical\nSHARPs parameters.\nOur results suggest that Transformer networks are a very\npromising method of flare forecasting, especially with limited\ndata. Model performance may be improved by using k-fold cross-\nvalidation techniques, which were not included in this study due to\nlimited computational resources. In addition, we acknowledge that\nthe reliance on data measurement and calculation accuracy is a weak\npoint across all such machine-learning space weather forecasting\nmethods. Future data including severe flare events may improve\nmodel performance due to the rarity of such events in present\ndata sets. Our limited-data approach is conducive to platforms with\nlimited storage such as nanosatellites. In the future, we plan to deploy\nTransformer networks on a CubeSat platform, where the model will\nFrontiers in Astronomy and Space Sciences 11 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nutilize on-board satellite measurements of solar processes to\nforecast solar flares, thereby providing early warning of severe\nevents.\nData availability statement\nThe magnetic field data used in this study can be found\nin the Joint Science Operations Center (JSOC) database: http://\njsoc.stanford.edu/. The flare event data used in this study can\nbe found in the GOES X-ray flare catalog provided by the\nNational Centers for Environmental Information: https://www.\nngdc.noaa.gov/stp/satellite/goes-r.html. Code files will be made\navailable at https://github.com/keahipelkumdonahue/transformer_\nflare_forecasting.\nAuthor contributions\nKP: Data curation, Formal Analysis, Investigation,\nMethodology, Software, Supervision, V alidation, Visualization,\nWriting–original draft. FI: Conceptualization, Data curation,\nFormal Analysis, Investigation, Methodology, Project\nadministration, Resources, Software, Supervision, V alidation,\nWriting–review and editing.\nFunding\nThe author(s) declare financial support was received for\nthe research, authorship, and/or publication of this article. This\nresearch was supported by the NOAA cooperative agreements\nNA17OAR4320101 and NA22OAR4320151.\nAcknowledgments\nW e would like to thank Paul T . M. Loto’ aniu for very helpful\nsuggestions and the SDO/HMI team for their production of the\nSHARPs data product. KPD would also like to thank Alberto Real\nfor useful discussions and Kira Badyrka for support.\nConflict of interest\nThe authors declare that the research was conducted in\nthe absence of any commercial or financial relationships\nthat could be construed as a potential conflict of\ninterest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their affiliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or claim\nthat may be made by its manufacturer, is not guaranteed or endorsed\nby the publisher.\nAuthor disclaimer\nThe views, opinions, and findings contained in this report are\nthose of the authors and should not be construed as an official\nNational Oceanic and Atmospheric Administration, National\nAeronautics and Space Administration, or other U.S. Government\nposition, policy, or decision.\nReferences\nAbduallah, Y ., W ang, J. T . L., W ang, H., and Xu, Y . (2023). Operational\nprediction of solar flares using a transformer-based framework. Sci. Rep. 13, 13665.\ndoi:10.1038/s41598-023-40884-1\nBahdanau, D., Cho, K., and Bengio, Y . (2014). Neural machine translation\nby jointly learning to align and translate. arXiv e-prints , arXiv:1409 .0473.\ndoi:10.48550/arXiv.1409.0473\nBenz, A. O. (2017). Flare observations. Living Rev. Sol. Phys. 14, 2.\ndoi:10.1007/s41116-016-0004-3\nBloomfield, D. S., Higgins, P . A., McAteer, R. T . J., and Gallagher, P . T . (2012).\nT oward reliable benchmarking of solar flare forecasting methods. ApJ 747, L41.\ndoi:10.1088/2041-8205/747/2/L41\nBobra, M. G., and Couvidat, S. (2015). Solar flare prediction using SDO/HMI vector\nmagnetic field data with a machine-learning algorithm.ApJ 798, 135. doi:10.1088/0004-\n637X/798/2/135\nBobra, M. G., Sun, X., Hoeksema, J. T ., T urmon, M., Liu, Y ., Hayashi, K., et al.\n(2014). The helioseismic and magnetic imager (HMI) vector magnetic field pipeline:\nSHARPs - space-weather HMI active region Patches. Sol. Phys. 289, 3549–3578.\ndoi:10.1007/s11207-014-0529-3\nBoureau, Y .-L., Ponce, J., and Lecun, Y . (2010). “ A theoretical analysis of\nfeature pooling in visual recognition, ” in ICML 2010 - Proceedings, 27th\nInternational Conference on Machine Learning, Haifa, Israel, June 21-24, 2010,\n111–118.\nBrodersen, K. H., Ong, C. S., Stephan, K. E., and Buhman, J. M. (2010).\n“The balanced accuracy and its posterior distribution, ” in 2010 20th International\nConference on Pattern Recognition, Istanbul, T urkey, 23-26 August 2010, 3121–3124.\ndoi:10.1109/ICPR.2010.764\nChoudhary, D. P ., Ambastha, A., and Ai, G. (1998). Emerging flux and X-class flares\nin NOAA 6555. Sol. Phys. 179, 133–140. doi:10.1023/A:1005063609450\nFletcher, L., Dennis, B. R., Hudson, H. S., Krucker, S., Phillips, K., V eronig, A.,\net al. (2011). An observational overview of solar flares. Space Sci. Rev. 159, 19–106.\ndoi:10.1007/s11214-010-9701-8\nFlorios, K., Kontogiannis, I., Park, S.-H., Guerra, J. A., Benvenuto, F ., Bloomfield,\nD. S., et al. (2018). Forecasting solar flares using magnetogram-based predictors and\nmachine learning. Sol. Phys. 293, 28. doi:10.1007/s11207-018-1250-4\nGarcia, H. A. (1994). T emperature and emission measure from goes soft x-ray\nmeasurements. Sol. Phys. 154, 275–308. doi:10.1007/BF00681100\nGirdhar, R., Carreira, J. J., Doersch, C., and Zisserman, A. (2019). “Video action\ntransformer network, ” in 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), Long Beach, CA, USA, 15-20 June 2019, 244–253.\ndoi:10.1109/CVPR.2019.00033\nHagyard, M. J., Smith, J., T euber, D., and W est, E. A. (1984). A quantitative study\nrelating observed shear in photospheric magnetic fields to repeated flaring. Sol. Phys.\n91, 115–126. doi:10.1007/BF00213618\nHanssen, A. J., and Kuipers, W . J. (1965). On the relationship between the frequency\nof rain and various meteorological parameters . Koninklijk Nederlands Meteorologisch\nInstituut 81, 2–25.\nHe, H., and Garcia, E. A. (2009). Learning from imbalanced data.IEEE Trans. Knowl.\nData Eng. 21, 1263–1284. doi:10.1109/TKDE.2008.239\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” in 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), Las V egas, NV , USA, 27-30 June 2016, 770–778. doi: 10.1109/CVPR.\n2016.90\nFrontiers in Astronomy and Space Sciences 12 frontiersin.org\nPelkum Donahue and Inceoglu 10.3389/fspas.2023.1298609\nHe, X., Tan, E. L., Bi, H., Zhang, X., Zhao, S., and Lei, B. (2022). Fully\ntransformer network for skin lesion analysis. Med. Image Anal. 77, 102357.\ndoi:10.1016/j.media.2022.102357\nHeidke, P . (1926). Berechnung des erfolges und der güte der\nwindstärkevorhersagen im sturmwarnungsdienst. Geogr. Ann. 8, 301–349.\ndoi:10.1080/20014422.1926.11881138\nHoeksema, J. T ., Liu, Y ., Hayashi, K., Sun, X., Schou, J., Couvidat, S., et al.\n(2014). The helioseismic and magnetic imager (HMI) vector magnetic field pipeline:\noverview and performance. Sol. Phys. 289, 3483–3530. doi: 10.1007/s11207-014-\n0516-8\nInceoglu, F ., Jeppesen, J. H., Kongstad, P ., Hernández Marcano, N. J., Jacobsen, R. H.,\nand Karoff, C. (2018). Using machine learning methods to forecast if solar flares will Be\nassociated with CMEs and SEPs. ApJ 861, 128. doi:10.3847/1538-4357/aac81e\nInceoglu, F ., Pacini, A. A., and Loto’aniu, P . T . M. (2022a). Utilizing AI to unveil\nthe nonlinear interplay of convection, drift, and diffusion on galactic cosmic ray\nmodulation in the inner heliosphere. Sci. Rep. 12, 20712. doi: 10.1038/s41598-022-\n25277-0\nInceoglu, F ., Shprits, Y . Y ., Heinemann, S. G., and Bianco, S. (2022b). Identification\nof coronal holes on AIA/SDO images using unsupervised machine learning. ApJ 930,\n118. doi:10.3847/1538-4357/ac5f43\nJarolim, R., V eronig, A. M., Hofmeister, S., Heinemann, S. G., T emmer, M.,\nPodladchikova, T ., et al. (2021). Multi-channel coronal hole detection with\nconvolutional neural networks. A&A 652, A13. doi:10.1051/0004-6361/202140640\nJonas, E., Bobra, M., Shankar, V ., T odd Hoeksema, J., and Recht, B. (2018).\nFlare prediction using photospheric and coronal image data. Sol. Phys. 293, 48.\ndoi:10.1007/s11207-018-1258-9\nKingma, D. P ., and Ba, J. (2015). “ Adam: a method for stochastic optimization, ” in3rd\ninternational conference on learning representations, ICLR 2015. Editors Y Bengio, and\nY LeCun.\nLi, X., Zheng, Y ., W ang, X., and W ang, L. (2020). Predicting solar flares using\na novel deep convolutional neural network. ApJ 891, 10. doi: 10.3847/1538-4357/\nab6d04\nLiu, H., Liu, C., W ang, J. T . L., and W ang, H. (2019). Predicting solar flares\nusing a long short-term memory network. ApJ 877, 121. doi: 10.3847/1538-4357/\nab1b3c\nMarchetti, F ., Guastavino, S., Piana, M., and Campi, C. (2022). Score-oriented\nloss (SOL) functions. Pattern Recognit. 132, 108913. doi: 10.1016/j.patcog.2022.\n108913\nMarzban, C. (2004). The ROC curve and the area under it as performance measures.\nWeather Forecast.19, 1106–1114. doi:10.1175/825.1\nMünchmeyer, J., Bindi, D., Leser, U., and Tilmann, F . (2021). Earthquake magnitude\nand location estimation from real time seismic waveforms with a transformer network.\nGeophys. J. Int. 226, 1086–1104. doi:10.1093/gji/ggab139\nNambiar, A., Heflin, M., Liu, S., Maslov, S., Hopkins, M., Ritz, A., et al. (2023).\nTransformer neural networks for protein family and interaction prediction tasks. J.\nComput. Biol. 30, 95–111. doi:10.1089/cmb.2022.0132\nNishizuka, N., Kubo, Y ., Sugiura, K., Den, M., and Ishii, M. (2021). Operational\nsolar flare prediction model using Deep Flare Net. Earth, Planets Space 73, 64.\ndoi:10.1186/s40623-021-01381-9\nNishizuka, N., Sugiura, K., Kubo, Y ., Den, M., and Ishii, M. (2018). Deep flare net\n(DeFN) model for solar flare prediction. ApJ 858, 113. doi:10.3847/1538-4357/aab9a7\nPulkkinen, A., Thomson, A., Clarke, E., and McKay, A. (2003). April 2000\ngeomagnetic storm: ionospheric drivers of large geomagnetically induced currents.\nAnn. Geophys. 21, 709–717. doi:10.5194/angeo-21-709-2003\nPulkkinen, T . (2007). Space weather: terrestrial perspective. Living Rev. Sol. Phys. 4,\n1. doi:10.12942/lrsp-2007-1\nRaju, H., and Das, S. (2023). Interpretable ML-based forecasting of CMEs associated\nwith flares. Sol. Phys. 298, 96. doi:10.1007/s11207-023-02187-6\nRibeiro, F ., and Gradvohl, A. L. S. (2021). Machine learning techniques\napplied to solar flares forecasting. Astronomy Comput. 35, 100468.\ndoi:10.1016/j.ascom.2021.100468\nSchrijver, C. J. (2007). A characteristic magnetic field pattern associated with all\nmajor solar flares and its use in flare forecasting. ApJ 655, L117–L120. doi: 10.1086/\n511857\nSchwenn, R. (2006). Space weather: the solar perspective. Living Rev. Sol. Phys. 3, 2.\ndoi:10.12942/lrsp-2006-2\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).\nDropout: a simple way to prevent neural networks from overfitting.J. Mach. Learn. Res.\n15, 1929–1958. doi:10.3389/fnins.2013.12345\nT uli, S., Casale, G., and Jennings, N. R. (2022). TranAD: deep transformer networks\nfor anomaly detection in multivariate time series data.arXiv e-prints, arXiv:2201.07284.\ndoi:10.48550/arXiv.2201.07284\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.\nN., et al. (2017). Attention is all you need. arXiv e-prints , arXiv:1706.03762.\ndoi:10.48550/arXiv.1706.03762\nW ang, X., Chen, Y ., T oth, G., Manchester, W . B., Gombosi, T . I., Hero, A. O.,\net al. (2020). Predicting solar flares with machine learning: investigating solar cycle\ndependence. ApJ 895, 3. doi:10.3847/1538-4357/ab89ac\nW oodcock, F . (1976). The evaluation of yes/No forecasts for scientific and\nadministrative purposes. Mon. Weather Rev. 104, 1209–1214. doi: 10.1175/1520-\n0493(1976)104⟨1209:TEOYFF⟩2.0.CO;2\nWu, J., Chen, X.-Y ., Zhang, H., Xiong, L.-D., Lei, H., and Deng, S.-\nH. (2019). Hyperparameter optimization for machine learning models\nbased on bayesian optimization. J. Electron. Sci. T echnol. 17, 26–40.\ndoi:10.11989/JEST .1674-862X.80904120\nFrontiers in Astronomy and Space Sciences 13 frontiersin.org",
  "topic": "Solar flare",
  "concepts": [
    {
      "name": "Solar flare",
      "score": 0.8056238293647766
    },
    {
      "name": "Geostationary Operational Environmental Satellite",
      "score": 0.7956628799438477
    },
    {
      "name": "Space weather",
      "score": 0.7502031922340393
    },
    {
      "name": "Flare",
      "score": 0.7462901473045349
    },
    {
      "name": "Geostationary orbit",
      "score": 0.6724640130996704
    },
    {
      "name": "Physics",
      "score": 0.6393260359764099
    },
    {
      "name": "Coronal mass ejection",
      "score": 0.6032596230506897
    },
    {
      "name": "Transformer",
      "score": 0.529306173324585
    },
    {
      "name": "Satellite",
      "score": 0.40496698021888733
    },
    {
      "name": "Meteorology",
      "score": 0.3807191550731659
    },
    {
      "name": "Remote sensing",
      "score": 0.3692871034145355
    },
    {
      "name": "Real-time computing",
      "score": 0.335781455039978
    },
    {
      "name": "Astrophysics",
      "score": 0.2936707139015198
    },
    {
      "name": "Astronomy",
      "score": 0.2814486026763916
    },
    {
      "name": "Computer science",
      "score": 0.24017438292503357
    },
    {
      "name": "Solar wind",
      "score": 0.18148958683013916
    },
    {
      "name": "Plasma",
      "score": 0.12610024213790894
    },
    {
      "name": "Voltage",
      "score": 0.085569828748703
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}