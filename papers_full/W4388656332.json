{
  "title": "Protein language models can capture protein quaternary state",
  "url": "https://openalex.org/W4388656332",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2324504538",
      "name": "Orly Avraham",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A3154004352",
      "name": "Tomer Tsaban",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A4225072945",
      "name": "Ziv Ben-Aharon",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A4306910066",
      "name": "Linoy Tsaban",
      "affiliations": [
        "Hebrew University of Jerusalem",
        "Hadassah Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3010630782",
      "name": "Ora Schueler‐Furman",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2324504538",
      "name": "Orly Avraham",
      "affiliations": [
        "Laboratory of Molecular Genetics",
        "Hebrew University of Jerusalem",
        "Jerusalem Institute for Israel Studies"
      ]
    },
    {
      "id": "https://openalex.org/A3154004352",
      "name": "Tomer Tsaban",
      "affiliations": [
        "Laboratory of Molecular Genetics",
        "Jerusalem Institute for Israel Studies",
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A4225072945",
      "name": "Ziv Ben-Aharon",
      "affiliations": [
        "Laboratory of Molecular Genetics",
        "Hebrew University of Jerusalem",
        "Jerusalem Institute for Israel Studies"
      ]
    },
    {
      "id": "https://openalex.org/A4306910066",
      "name": "Linoy Tsaban",
      "affiliations": [
        "Hebrew University of Jerusalem",
        "Hadassah Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3010630782",
      "name": "Ora Schueler‐Furman",
      "affiliations": [
        "Laboratory of Molecular Genetics",
        "Hebrew University of Jerusalem",
        "Jerusalem Institute for Israel Studies"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2152764495",
    "https://openalex.org/W4296695943",
    "https://openalex.org/W2106357210",
    "https://openalex.org/W2949693295",
    "https://openalex.org/W2103017472",
    "https://openalex.org/W3112600195",
    "https://openalex.org/W4307425304",
    "https://openalex.org/W1965827393",
    "https://openalex.org/W3095583226",
    "https://openalex.org/W3106745904",
    "https://openalex.org/W3005560673",
    "https://openalex.org/W2035503835",
    "https://openalex.org/W2081598613",
    "https://openalex.org/W2605158478",
    "https://openalex.org/W3004762425",
    "https://openalex.org/W2768330140",
    "https://openalex.org/W4308463927",
    "https://openalex.org/W4380272022",
    "https://openalex.org/W4323538612",
    "https://openalex.org/W2169478909",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W4312197262",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4225438928",
    "https://openalex.org/W4322617258",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W3118936575",
    "https://openalex.org/W4309485183",
    "https://openalex.org/W3144701084",
    "https://openalex.org/W4213112325",
    "https://openalex.org/W4211208250",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W2521200999",
    "https://openalex.org/W2342249984",
    "https://openalex.org/W2011301426",
    "https://openalex.org/W3150635270",
    "https://openalex.org/W4391563878",
    "https://openalex.org/W4385798891"
  ],
  "abstract": "Abstract Background Determining a protein’s quaternary state, i.e. the number of monomers in a functional unit, is a critical step in protein characterization. Many proteins form multimers for their activity, and over 50% are estimated to naturally form homomultimers. Experimental quaternary state determination can be challenging and require extensive work. To complement these efforts, a number of computational tools have been developed for quaternary state prediction, often utilizing experimentally validated structural information. Recently, dramatic advances have been made in the field of deep learning for predicting protein structure and other characteristics. Protein language models, such as ESM-2, that apply computational natural-language models to proteins successfully capture secondary structure, protein cell localization and other characteristics, from a single sequence. Here we hypothesize that information about the protein quaternary state may be contained within protein sequences as well, allowing us to benefit from these novel approaches in the context of quaternary state prediction. Results We generated ESM-2 embeddings for a large dataset of proteins with quaternary state labels from the curated QSbio dataset. We trained a model for quaternary state classification and assessed it on a non-overlapping set of distinct folds (ECOD family level). Our model, named QUEEN (QUaternary state prediction using dEEp learNing), performs worse than approaches that include information from solved crystal structures. However, it successfully learned to distinguish multimers from monomers, and predicts the specific quaternary state with moderate success, better than simple sequence similarity-based annotation transfer. Our results demonstrate that complex, quaternary state related information is included in such embeddings. Conclusions QUEEN is the first to investigate the power of embeddings for the prediction of the quaternary state of proteins. As such, it lays out strengths as well as limitations of a sequence-based protein language model approach, compared to structure-based approaches. Since it does not require any structural information and is fast, we anticipate that it will be of wide use both for in-depth investigation of specific systems, as well as for studies of large sets of protein sequences. A simple colab implementation is available at: https://colab.research.google.com/github/Furman-Lab/QUEEN/blob/main/QUEEN_prediction_notebook.ipynb .",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate‑\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdo‑\nmain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nAvraham et al. BMC Bioinformatics          (2023) 24:433  \nhttps://doi.org/10.1186/s12859-023-05549-w\nBMC Bioinformatics\nProtein language models can capture \nprotein quaternary state\nOrly Avraham1, Tomer Tsaban1, Ziv Ben‑Aharon1, Linoy Tsaban2,3 and Ora Schueler‑Furman1* \nAbstract \nBackground: Determining a protein’s quaternary state, i.e. the number of monomers \nin a functional unit, is a critical step in protein characterization. Many proteins form \nmultimers for their activity, and over 50% are estimated to naturally form homomul‑\ntimers. Experimental quaternary state determination can be challenging and require \nextensive work. To complement these efforts, a number of computational tools have \nbeen developed for quaternary state prediction, often utilizing experimentally vali‑\ndated structural information. Recently, dramatic advances have been made in the field \nof deep learning for predicting protein structure and other characteristics. Protein \nlanguage models, such as ESM‑2, that apply computational natural‑language models \nto proteins successfully capture secondary structure, protein cell localization and other \ncharacteristics, from a single sequence. Here we hypothesize that information \nabout the protein quaternary state may be contained within protein sequences as well, \nallowing us to benefit from these novel approaches in the context of quaternary state \nprediction.\nResults: We generated ESM‑2 embeddings for a large dataset of proteins with qua‑\nternary state labels from the curated QSbio dataset. We trained a model for quaternary \nstate classification and assessed it on a non‑overlapping set of distinct folds (ECOD \nfamily level). Our model, named QUEEN (QUaternary state prediction using dEEp learN‑\ning), performs worse than approaches that include information from solved crystal \nstructures. However, it successfully learned to distinguish multimers from monomers, \nand predicts the specific quaternary state with moderate success, better than simple \nsequence similarity‑based annotation transfer. Our results demonstrate that complex, \nquaternary state related information is included in such embeddings.\nConclusions: QUEEN is the first to investigate the power of embeddings for the pre‑\ndiction of the quaternary state of proteins. As such, it lays out strengths as well as limi‑\ntations of a sequence‑based protein language model approach, compared to struc‑\nture‑based approaches. Since it does not require any structural information and is fast, \nwe anticipate that it will be of wide use both for in‑depth investigation of specific \nsystems, as well as for studies of large sets of protein sequences. A simple colab \nimplementation is available at: https:// colab. resea rch. google. com/ github/ Furman‑ Lab/ \nQUEEN/ blob/ main/ QUEEN_ predi ction_ noteb ook. ipynb.\nKeywords: Protein language models, Natural language processing, Protein quaternary \nstate, Oligomeric state prediction, Multilayer perceptron, Deep learning\n*Correspondence:   \nora.furman‑schueler@mail.huji.\nac.il\n1 Department of Microbiology \nand Molecular Genetics, \nFaculty of Medicine, Institute \nfor Biomedical Research Israel‑\nCanada, The Hebrew University \nof Jerusalem, Jerusalem, Israel\n2 Gaffin Center \nfor Neuro‑Oncology, Sharett \nInstitute for Oncology, Hadassah \nMedical Center and Faculty \nof Medicine, Hebrew University \nof Jerusalem, Jerusalem, Israel\n3 The Wohl Institute \nfor Translational Medicine, \nHadassah Medical Center \nand Faculty of Medicine, \nHebrew University of Jerusalem, \nJerusalem, Israel\nPage 2 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \nBackground\nProteins are the “working class” of living cells, performing many of the functions critical \nfor life. Learning molecular details about the structure and function of a protein is often \na difficult task, as this entails low-throughput and rigorous work. The quaternary state \n(qs), i.e., the number of units assembling together to form a functional unit is an impor -\ntant characteristic of a protein. Many proteins form oligomers to carry out their molecu-\nlar assignments [1]. These oligomers can be of homo- or hetero-oligomeric nature, i.e., \nidentical subunits or different subunits, respectively. The oligomeric formation can be \nobligatory or dynamic [2], and is often crucial for the proper activity of the complex. \nExamples for oligomeric proteins include the homotetrameric β-galactosidase [3], and \nthe homotrimeric HTRA1 protease [4], where for both the homo-multimer formation is \nessential for their activity.\nProteins can be classified into families, whether functional (such as GO [5, 6] or \nKEGG [7]), structural (such as ECOD [8], PFAM [9], CATH [10]) or other classifications \n(e.g. sequence similarity). These classifications are important when comparing proteins \nto similar members of the relevant cluster. In this context, families of proteins may all \nadopt the same qs, or alternatively, may contain members that form various qs (Fig.  1). \nThis adds a tier of complexity to qs determination, as simply learning or annotation \ntransfer within families will not always yield the correct assignment.\nUsually, experimental approaches are used to determine the qs, such as SEC-MALS, \nIEX-MALS, ultracentrifugation, to name a few. To alleviate these often expensive \nand laborious approaches, computational protocols have also been developed for this \ntask, mostly starting from a solved crystal structure [11]. As an example, the widely \nused PISA protocol determines the most likely biologically relevant assembly based \non the calculated strength of all different contacts among monomers in a solved \nstructure [12]. However, PISA has several drawbacks, the main one being its depend -\nency on information from a solved multimeric structure, where it chooses the correct \nassembly from within the oligomers in the crystal lattice. Another experiment-based \nFig. 1 Proteins from the same fold family can assume different quaternary states (qs) All three proteins \nbelong to the “Cytidine and deoxycytidylate deaminase zinc‑binding region” family (ECOD dCMP_cyt_\ndeam_1, f_id: 2492.1.1.5), but form either a dimer A, tetramer B or hexamer C. This demonstrates the \ncomplexity of determining qs, as members of the same structural family may form different states.\nPage 3 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \nqs predictor, EPPIC takes into account evolutionary information to ascertain the cor -\nrect qs from a crystal lattice or other solved structures [13]. Another tool, GalaxyHo -\nmomer, also utilizes solved structures (or generates a model, if no solved structure is \navailable), by selecting the best option among complexes generated by template based \ndocking and ab-initio modeling [14]. The PROTCID database contains annotations \nabout the quaternary structure of a protein monomer that is based on recurring mon -\nomer interactions in distinct solved structures of a protein [15]. Finally, QSbio takes \ninto account information about qs of homologs retrieved from several data reposito -\nries [16]. This is done by superposing complete complexes and assessing the correct \nqs by exploiting the available evolutionary information. As for many other structural \ntasks, AlphaFold has been used in this context as well, where the confidence measures \npTM, pLDDT and PAE are used to assess the most probable multimeric state [2 , 17, \n18], or by generating and scoring a variety of complex structures [19].\nWhile it is quite natural to use structural information to elucidate the qs of a protein, \nthere are also disadvantages to the above approaches, in particular, the computationally \nheavy structure prediction when no experimentally solved structure is available. How \nfar can we then get using only sequence information? A straight-forward way would be \nto infer qs based on the qs reported for the protein family to which the protein belongs, \nor from the closest homolog, when available. However, a given protein family may host \nstructurally similar proteins with distinct qs (as, e.g., in Fig.  1). How then should the qs \nbe determined from sequence only? More sophisticated sequence representations could \nhelp address this challenge.\nLearning information about a protein from its sequence is a well-established approach \nin protein annotation, as for example the use of multiple sequence alignments to extract \nevolutionary information [20]. Nonetheless, the construction of multiple sequence \nalignments is costly in time and computing power, and dependent on the availabil -\nity of many homologous sequences, and is thus inherently biased towards evolution -\nary conservation. To address these problems and others, Natural Language Processing \nconcepts have been applied to protein sequences, generating protein language models \n(pLMs) [21–23]. Briefly, a Neural Network is trained on a tremendous amount of protein \nsequences, learning the connection between the residues, and more specifically, their \ncontextual meaning. Once learned, information can be extracted as embeddings (i.e. a \nnumeric vector representation of the protein). These embeddings are subsequently used \nin a transfer learning step as input for supervised learning. The resulting embeddings \nare implemented for various tasks, ranging from predicting the protein secondary struc -\nture, localization and characteristics, to predicting three dimensional models [23–25]. \nThe embeddings are very powerful, which stems in part from the fact that training was \ncarried out when generating the embeddings, therefore the transfer learning step can be \nperformed on a (relatively) small dataset (e.g. [26]).\nTraining a method for qs classification is challenging, for many reasons, in par -\nticular due to the inherently unbalanced data, with monomers outnumbering all \nother classes (in, e.g., the QSbio dataset; see, Fig.  2 below). The small classes have \na dozen or even less entries, many of them with high sequence similarity, thus clus -\ntering together and providing less additional information. Moreover, the changes \nPage 4 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \nneeded to shift a sequence from one qs to another may be very small, involving as \nfew as 5 residues and in cases even a single point mutation [4 , 27].\nIn this study we examine the power of pLM embeddings, derived from the pre-\ntrained pLM ESM-2 [25], to capture and consequently classify the qs of proteins. \nWe assemble our training set using curated data from QSbio, including only homo -\nmeric proteins, and using only high confidence entries. Our goal in this research is \ntwofold: First, we wish to examine to what extent the embeddings hold the capacity \nto capture qs, if at all. For this purpose, we generate training and test sets, which \nare available for future research. Second, we built a useful classifier termed QUEEN \n(QUaternary state prediction using dEEp learNing), which could be employed both \nfor low-throughput manual examination of protein sequences, and especially for \nhigh-throughput large scale classifications. One of the major advantages of using \npLM is the speed of inference, once the embeddings have been generated. We show \nthat ESM-2 embeddings can indeed be used for this task, with performance beyond \nthat of simple sequence homology-based annotation transfer. We build a MultiLayer \nPerceptron (MLP) model using the embeddings as input, and explore the resulting \nclassifier, its successful predictions and its failures.\nCNumber of sequences (pdbs) in each qs labelA\n104\n103\n102\n101\n100\n1  2  3  4  5   6  7  8  9  10 11 12 13 14 15 16 18 24 60\nNumber of different qs\n524 31\nNumber of families\n103\n102\n101\n100\nNumber of different qs in each ECOD familyB\n2631\n526\n83\n16\n3\nFig. 2 Overall data statistics and distribution of the quaternary state dataset used in this study A. Distribution \nof qs: For each qs (x‑axis) the number of entries is shown (y‑axis, in log‑scale). Left and right bars show the \ntraining set (in color, after down‑sampling; dark gray: samples removed for training) and hold‑out set (light \ngray), respectively. Down‑sampling was necessary due to the uneven distribution of the data, which is \nsignificantly skewed towards monomers, followed by dimers, trimers, tetramers and hexamers. B. Distribution \nof count of families (y‑axis) with different qs within a given ECOD family (x‑axis). While most families show \nthe same qs for all their entries, a significant fraction contains a diverse set of qs (exact numbers are included \nin the boxes). C. Details of the composition of qs in different ECOD families, shown as a network, where the \nnodes represent different qs (colored as in A.), sized according to their amount (and percentage indicated). \nThe edges represent families containing two different qs, with width proportional to the amount (and \nnumbers indicated). Note that B and C show numbers in the training set, after down‑sampling and removal \nof small groups.\nPage 5 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \nResults\nOrganization of the dataset: The QSbio database contains carefully curated infor -\nmation about the multimeric state of different proteins for which the structure has \nbeen determined [16]. The starting point of our study is a dataset extracted from \nQSbio, including only homomeric proteins of highest confidence, and only a single \nannotation per protein data bank (pdb [28]) entry (a total of 31,994 unique protein \nsequences, see Methods for full details). In this redundant dataset, each unique pro -\ntein sequence is included as a separate entry, where very similar sequences will most \noften, but not always, have the same qs annotation. We separated this dataset into a \ntraining and a validation (hold-out) set, so that sequences with over 30% sequence \nidentity would be in the same set. We then used the structural domain-based ECOD \ndatabase [ 8] to cluster the sequences into domain families (at the family structural \nsimilarity level, “f_id”) for their further investigation. Overall, this dataset covers 19 \ndifferent qs, ranging from monomers to 60mer homopolymers (Fig.  2A), and many \nECOD families contain representatives of various qs (Fig.  2B and C,  see also Addi -\ntional file  1: Table SI, Additional file  2: Table SII, for detailed information about the \ndataset and results, and Additional file 3 : Figure S1).\nDistinction of different qs by the language model: First, we generated embeddings \nfor each entry in the database. Each protein is represented by one embedding vector, \nwhich is obtained as the average of the vectors of the different residues in the protein \nsequence (see Methods for more details). In order to assess the capacity of pLMs to \ncapture qs, we used supervised dimensionality reduction to visually demonstrate that \nthe data indeed clusters by qs (Fig.  3). The large groups of labels (namely monomers, \ndimers, trimers, tetramers and hexamers), as well as some other qs are well separated \nFig. 3 Capture of different qs by the pLM embeddings Supervised dimensionality reduction by UMAP is able \nto separate proteins of different qs, in particular for the qs represented by many entries in the dataset, with \nnice separation for octamers as well (light green). qs are colored as in Fig. 1A. Note that this map includes all \nthe data, i.e., both the training and hold out sets.\nPage 6 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \non this map, demonstrating the model’s ability to characterize distinct features of \neach group. This suggests that the model could be used to predict the qs.\nInferring qs by annotation transfer from similar proteins: To assess the ability of \nQUEEN to correctly predict the qs of a protein, we used annotation transfer to assign \na qs to each sequence in the test set (Fig.  4A, B and Additional file  1: Figure S2). In this \napproach, the qs is inferred from the most similar protein with available annotation, i.e. \nthe nearest neighbor in the embedding space (calculated as cosine similarity, see Meth -\nods), similarly to previous studies [29]. This can be compared to a corresponding anno -\ntation transfer based on sequence similarity. Using the embedding space has advantages: \na distance can be calculated between any two vectors even if they are very different, \nsince they represent a feature of a protein sequence. This is in contrast to sequence rep -\nresentation that is residue-based, and therefore necessitates sequence alignment, which \ncan be challenging when comparing distant sequences.\nAnnotation transfer using embedding distance outperformed corresponding anno -\ntation transfer based on sequence identity in predicting qs (Table  1). This applies \nto prediction with available prior knowledge (i.e., on a redundant set that contains \nsequence-similar proteins, Additional file 1: Figure S2). Importantly, this holds also when \nprior knowledge is not available (i.e., for the test set that does not contain any entry with \nsignificant sequence identity to the training set): The balanced accuracy increases from \n0.15 to 0.23 (Table 1, compare Figs. 4A, B).\nWhen the qs of a new sequence is predicted, it is often homologous to previously \nannotated sequences, which improves prediction (compare Additional file  1: Figure \nS2 and Fig.  4, and see Table  1). In such a setting, (i.e., when including qs information \nof homologs), we observe a significant separation between cosine similarities used for \ntransfer of correct and incorrect predictions (Fig.  5A; with the exception of qs = 7; Indi-\nvidual p-values are summarized in Additional file  1: Table S4). Thus, in these cases the \nFig. 4 Accuracy of quaternary state prediction by different approaches The prediction is based on transfer \nof the qs annotation to each sequence in the test set based on A. the closest sequence in the train set (as \ndetermined by blast). The 0‑predicted column indicates the fraction without any significant blast hit, and \nconsequently no prediction; B. the highest similarity in embedding space in the train set (i.e., cosine similarity \nbetween embedded vectors); and C. QUEEN—a deep learning model trained on the embeddings (see \nText). The confusion matrix includes the frequency of cells representing predicted vs. actual labels (on x and \ny‑axes, respectively), where a matrix occupying only the diagonal represents full success, while off‑diagonal \nvalues represent wrong predictions. The balanced accuracy increases from left to right as indicated by \nthe darker diagonal, highlighting improved prediction when moving from sequence, to language model \nrepresentation, to QUEEN, the MLP model. Results are shown for the test set, based on information learned \nfrom an independent training set. For corresponding confusion matrices on a redundant set containing also \ninformation of sequence similar proteins, see Additional file 1: Figure S2.\nPage 7 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \ncosine similarity can be used to assess whether simple annotation transfer may be suf -\nficient to determine the qs. This is however not applicable for qs assignment without \ninformation from homolog proteins, as apparent in Fig. 5B.\nTraining a model for qs prediction based on embeddings: The above initial analysis sug-\ngests that the embeddings include information about qs. To optimally leverage their use, \nwe trained an embedding-based MLP model to predict qs. We trained and performed \nhyperparameter tuning on 5 sets of cross-validation within our training set (each time \ntraining on 80% to predict a different 20% set, see Methods and Additional file  1: Figure \nS4) to define the final model and its parameters (Fig.  4C; Importantly, the hold-out set \nwas not used at any step of training and validation of QUEEN). To achieve improved \nperformance we experimented with several strategies, which highlighted the importance \nof down-sampling of monomers and dimers to obtain a more equilibrated dataset for \ntraining (resulting in similar size of monomer, dimer and tetramer qs, see Figs.  2A, C). \nEven after down-sampling, the resulting confusion matrix (Fig.  4C) still highlights the \nrelatively high success rate of the monomer qs predictions. As for multimers, for many \nof the wrong predictions, dimers and tetramers are chosen, more so than monomers \n(see columns of dimer and tetramer predictions in Fig.  4C). This hints that QUEEN may \nhave learned to detect multimerization rather than the exact qs, as shown in the striking \nexample of the predominant classification of heptamers as dimers (see below).\nCompared to nearest neighbor annotation transfer, QUEEN shows significant \nimprovement (compare Fig. 4C to B; balanced accuracy 0.36 vs. 0.23, see Table 1). While \nsuccess is not uniform across all labels, correct prediction (> 40% in the diagonal of \nFig. 4C) is achieved in 7 out of the 12 trained and predicted qs classes. When examin -\ning the results with one representative per each ECOD family (i.e., no redundant infor -\nmation) the balanced accuracy decreases to 0.26 ± 0.03. The difference in performance \nmight be a result of the smaller size of the evaluated set.\nConfidence of prediction as indication of success of QUEEN: While QUEEN provides \nprediction of a specific qs, it is also interesting to examine the underlying probabili -\nties that drive the label prediction and to assess whether they are indicative of suc -\ncessful vs. wrong predictions. Reassuringly, comparison of the distributions of the \ncorresponding probabilities (i.e., true-positives and false-positives) reveals a signifi -\ncant difference between the two (Fig.  6), with p -values ranging from 0.04 to 6*10 –88 \nTable 1 Performance of different models for the prediction of quaternary states (qs). Balanced \nAccuracy and F1 scores\n1  no sequence with > 30% sequence available to transfer from; 2BA: balanced accuracy; 3 F1: F1 score; Precision and Recall \nvalues are provided in Additional file 3: Table S3, and Precision-Recall and ROC curves are provided in Additional file 3: \nFigure S3. See Methods for definitions\nAnnotation transfer, based on pLM MLP model, based on\nSequence pLM ESM-2 \nembeddings \n(QUEEN)\nProtbert \nembeddings\nNo information about \nsequence homologs avail‑\nable 1\nBA2 0.15 0.23 0.36 0.19\nF13 0.43 0.54 0.52 0.41\nFull homology information \navailable\nBA 0.6 0.67 – –\nF1 0.79 0.85\nPage 8 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \n(Additional file  1: Table S5). Of note, in contrast to the pLM-based annotation trans -\nfer for which we can only provide a confidence estimate when information about the \nqs of homologous sequences is available (based on cosine similarity, Fig.  5), using the \nQUEEN MLP model, this is now also possible for qs predictions that are not based on \nhomolog information (based on the predicted probability).\nThe distributions of predicted qs shown in Fig.  7 reveal a clear preference for mono -\nmers for proteins that form monomers, as expected from the successful prediction \nfor the qs of monomeric proteins (See Additional file  1: Figure S5 for full plots). In \ncontrast, the distribution of qs predictions for heptamers (that QUEEN did not learn \n12 3 4567 89 10 11 12 13 14 15 16 18 24 60\nqs\nCosine similarity\n123 4567 81 01 21 42 4\n1.0\n0.95\n0.90\n0.85\n1.0\n0.95\n0.90\n0.85\nA Full homology information available\nB No information about qs of sequence homologs is considered\nFig. 5 Cosine similarity provides an estimate of the reliability of qs predictions based on annotation transfer \nfrom homolog proteins A. Cosine similarity using information that includes qs of homologous sequences. \nFor each qs (x‑axis) a box plot depicts the distribution of the cosine similarities for correct (green) and \nincorrect (purple) qs predictions (y‑axis). The plot is capped at 0.85, with 3 dots removed for clarity. The two \ndistributions differ significantly (one sided Wilcoxon test, p‑values < 0.05, except for qs = 7, p‑value = 0.051; \nsee Additional file 2: Table S1). This separation demonstrates the power of embeddings to capture qs, and can \nbe used to assess the confidence of annotation transfers. B. Corresponding plot of cosine similarities when \ninformation of qs of homologous sequences is NOT included (i.e. no entry with > 30% sequence identity is \nconsidered for annotation transfer). In this case, the two distributions show no significant difference.\nPage 9 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \n   1        2       3        4       5       6        7        8      10      12     14     24\nQS\nProbability\n1.0\n0.9\n0.8\n0.7\n0.5\n0.4\n0.6\n0.3\nProbabilities for correct and wrong predictions\nFig. 6 QUEEN predictions show consistently higher confidence for correct assignments For each qs (x‑axis) \nbox plots and dots show the distribution of the probabilities of correct (green) and incorrect (purple) \npredictions. Correct predictions are consistently predicted with higher probability than incorrect predictions, \nfor all qs (one sided Wilcoxon test, p‑value < 0.05). This separation can be used to assess correct predictions, \nalso in the absence of homologs.\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n  1     2    3     4    5    6     7    8    10  12   14  24\nqs=1\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n  1     2    3    4    5     6    7    8    10  12  14  24\nqs=2\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n  1     2    3    4     5    6    7     8    10   12  14  24\nqs=7\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n  1     2    3    4    5     6    7    8    10   12  14  24 \nqs=24\nFig. 7 Distribution of predicted probability (‘confidence’) for representative qs categories The plots show \nviolin plots of the prediction probabilities (y‑axis) for different qs labels (x‑axis), for actual qs = 1,2,7 and 24 \n(indicated in the title and highlighted in cyan on the x‑axis).\nPage 10 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \nwell, see Fig.  4C), is much more varied. However, as also apparent in the confusion \nmatrix (Fig.  4C), the false negative heptamers are predicted to form different multim -\nerization states, despite monomers being the largest class. This reinforces the notion \nthat protein sequences hold information about multimerization, even though not \nalways about the correct state. 24-mer is another interesting example, where besides \nhigh probability of 24-mers, the model also predicts the divisors of 24, with enrich -\nment of the rare 8- and 12-mer classes.\nExamination of diverse families: Incorrect predictions shown in Fig.  7 could be a con -\nsequence of inaccurate learning for ECOD families that adopt more than one qs. We \ntherefore compared performance for ECOD families for which all members adopt the \nsame qs, to those that adopt diverse qs (families with single members were treated sepa -\nrately, as from one annotated sequence it cannot be concluded whether proteins in such \na family can adopt different qs). Overall, the variation of qs within the family (i.e., how \nmany different qs it includes) is well captured by QUEEN (Fig.  8A). For homogenous \nfamilies QUEEN predominantly predicts a single qs, while for families with more qs, \nthe number of predicted different qs predominantly corresponds to the actual number \nof different qs. In this context, performance tends to be slightly improved for single qs \nfamilies (Fig. 8B, C). Moreover, for families with diverse qs, predictions for proteins that \nadopt the dominant qs of that family are more accurate than corresponding predictions \nfor proteins adopting outlier qs.\nComparison of QUEEN performance to other related approaches: How well does \nthis approach perform compared to other related methods? Comparison may pro -\nvide new insight into the features that determine qs. Of note, to our knowledge, \nthe present model is the first to use sequence information only, in contrast to other \nmethods that all use a solved or predicted structure as input for qs determination or \n   12 34 56 781 01 21 42 4\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1 Accuracy\n2000\n1500\n1000\n500\nCounts\n% of predicted different QS in families\nNumber of predicted different qs \nvs actual number of qs\n# of different qs in families\n1 2345\n20\n40\n60\n80\n100\n1\n1\n1\n2\n22\n2\n3\n3\n3\n3\n4 4\n4\n0\n56 6\n   12 34 56 7810 12 14 24\n0\nC\nBA\nFig. 8 Prediction success and diversity of families with multiple qs A. QUEEN has an accurate representation \nof the degree of diversity of different families: Clusters with multiple qs are predicted as such. Shown are \nthe percentage of families predicted to have different qs (y‑axis) for a given number of distinct qs in a family \n(x‑axis). Numbers of qs are indicated in the boxes and color‑coded. B. Accuracy (y‑axis) for different qs (x‑axis). \nThe different bars show four distinct groups within each qs: Overall accuracy (grey), families adopting a \nsingle qs (yellow), families adopting multiple qs—predominant qs (light green) and rare qs (dark green). C: \nCorresponding counts of the groups presented in B.\nPage 11 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \nprediction. Not surprisingly therefore, PISA outperforms QUEEN for every class, and \nEPPIC shows a very similar trend (Note however the better performance of QUEEN \nfor 24mers; Fig.  9A). Nevertheless, examination of the degree of complementarity of \nthese approaches suggests that a significant fraction of proteins is correctly predicted \nonly by QUEEN (5–20% for different qs; Fig.  9B and C). Therefore, a combination of \nthe different approaches may be beneficial, pending our ability to identify the correct \npredictions.\nComparison to other language models—Protbert: We show here classification and qs \nassignment based on embeddings of the protein sequences. These embeddings were \ngenerated using the language model developed by Meta AI—ESM-2. We examined \nwhether the source of the embeddings has an impact on the final results, and indeed \nit does. Repeating this protocol using embeddings from protbert_BFD resulted in \nworse performance, showing less improvement compared to simple sequence based \nnearest neighbor annotation transfer (see Table 1 ).\nRobustness of QUEEN performance demonstrated on the independent holdout set:  \nThe results presented until now were calculated on the test sets of the 5 cross-valida -\ntion runs. Based on these results, we used the optimized hyperparameter set to define \nour final model. To evaluate its performance, we now opened the hold out set that we \nhad set aside at the beginning of our study. The performance using sequence or pLM-\nbased annotation transfer on this (smaller) hold out set is slightly better (see Fig.  10 \nfor confusion matrices and Table  2; compare to Fig. 3 and Table 1 above). Reassuringly, \nQUEEN performs similarly to what we report for the cross-validation performance, \nwith balanced accuracy slightly lower at 0.3, and for most qs, correct predictions can \nbe identified based on the estimated prediction probability, as observed during train -\ning (compare Additional file  1: Figure S6 to Fig.  6). A similar balanced accuracy is also \nobtained when examining one representative only (0.28 ± 0.05).\nSuccess rate for each method by qs\nQS\n1    2    3 456     7    8   10 12 14 24\n0.1\n0.2\n0.4\n0.6\n0.8\n1.0Percentage of success \nA\n  12 346  \n 5     7     8   10   12   14   24 \n300\n250\n200\n150\n100\n50\n0\n4000\n3000\n2000\n1000\n0\nCounts of success \nC\nQS\nPISA\nEPPIC\nQUEEN\nBoth correct\nPISA correct\nQUEEN correct\nBoth wrong\nComplementarity between PISA and QUEEN\nB\nFig. 9 Comparison of success of qs predictions of different methods: Structure‑based PISA, EPPIC, and \nsequence‑based QUEEN. A. Success (y‑axis) for different qs (x‑axis) for the different approaches. PISA and \nEPPIC predominantly outperform QUEEN. Note that PISA and EPPIC use not only the structure but also the \nfull crystal information, and they only choose from the available options within the crystal lattice. B and \nC. Success (y‑axis) colored according to performance by PISA and QUEEN for different qs, highlighting the \nsignificant overlap, but also the additional potential contribution of QUEEN. The division between B and C is \nfor clarity of the y‑axis scale, B showing the larger classes 1–4, 6, and C showing the smaller classes 5, 7–24\nPage 12 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \nDiscussion\nProtein language models have proven to be most useful to the study of proteins, in par -\nticular due to the rich information that their embeddings can provide [30, 31]. In this \nwork we explore the ability of protein embeddings to capture and classify quaternary \nstates, qs. Our motivation is twofold: First, this allows us to understand how well infor -\nmation about the qs is represented in a sophisticated, yet sequence-based model that \ndoes not explicitly consider the protein structure as input. Second, we wished to provide \na simple and fast tool to the public for the prediction of the qs of a protein based on its \nsequence only, to allow for its wide application and hopefully the acceleration of protein \nresearch.\nOur analysis shows that QUEEN performs better than a sequence-based annota -\ntion transfer approach, as well as the use of pLM embeddings only, demonstrating that \nimproved representation of sequence features, and additional learning can improve \nthe representation of qs. Our analysis also demonstrates the difference in performance \nusing different pLMs, as it is better than a corresponding Protbert-based pLM. Over -\nall, QUEEN shows a good ability to distinguish between monomers and multimers. For \nseven qs success rate tops 40%, a performance which can likely be further improved. \nFuture attempts to predict qs using pLM can profit from including structural infor -\nmation, such as features from a solved or predicted structure, or the use of structural \nembeddings. As an example, a recent study aimed at prediction of protein function has \nFig. 10 Accuracy of qs prediction by different approaches for the holdout set: The prediction is based on \ntransfer of the qs annotation to each sequence in the holdout set based on A. the closest sequence in the \ntraining set (as determined by blast); B. The highest similarity in embedding space in the Training set (i.e., \ncosine similarity between embedded vectors); and C. QUEEN trained on the embeddings (see Text). The \nconfusion matrix includes the frequency of cells representing predicted vs. actual label (on x and y‑axes, \nrespectively), where a matrix occupying only the diagonal represents full success, while off‑diagonal values \nrepresent wrong predictions. The balanced accuracy increases from left to right as indicated by the darker \ndiagonal, highlighting improved prediction when moving from sequence, to language model representation, \nto the deep learning model\nTable 2 Performance on the holdout set for the prediction of quaternary states (qs)\nAnnotation transfer, based on QUEEN\nSequence pLM\nBA 0.18 0.26 0.30\nF1 0.34 0.55 0.58\nPage 13 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \ngenerated a model that learns PFAM domains, and uses these to infer functional annota-\ntion [32]. Given information on qs of a larger set of proteins, a similar approach could be \nimplemented for qs prediction.\nQUEEN has a “sense” of diversity, i.e., sequences from the same family, despite hav -\ning a similar sequence and similar fold, are not always classified with the same qs. In \nparticular, the degree of variation of qs in different families is overall nicely captured \nby the model. Therefore, QUEEN could be useful to estimate the overall homogeneity \nof the qs within a new family. Nevertheless, as is also true for many other deep learn -\ning applications, QUEEN is not able to accurately predict qs changes that occur from \npoint mutation (data not shown). Of note, QUEEN was trained on sequences extracted \nfrom solved structure rather than the full protein sequence as reported in uniprot [33] \n(for which the qs is not necessarily the same and known), and therefore it remains to be \ninvestigated how well it will perform for full sequences. In its current implementation, \nwe suggest using it for the prediction of selected regions that mainly include defined \ndomains, as QUEEN did not have the opportunity to train on unstructured regions that \nare not resolved in solved structures.\nA number of qs prediction and assessment tools are already available. The widely used \nPISA strictly relies on a solved structure—not only the monomeric fold, but the entire \ncomplex. In this context, PISA calculates and determines the qs from within the avail -\nable options in the complex. When a crystal structure is examined by PISA, only the \ncombinations comprising the crystal lattice and the consequent complex are consid -\nered. This reduces, often dramatically, the qs that may be considered, improving predic -\ntions. Nonetheless, QUEEN succeeds in certain cases where PISA fails (Fig.  9 B and C). \nPending our ability to reliably identify these cases, this could pave the way for further \nimprovement by a pLM-based model such as QUEEN. Optimal extended leverage of \nstructure prediction can be obtained by using state of the art structure prediction meth -\nods such as AlphaFold and ESMfold to provide structural information (e.g. [2]), in par -\nticular by modeling different qs to identify the most promising one [17].\nAdditional avenues remain to be explored, such as the investigation of more LMs \nfor this task, besides ESM and ProtBert. Furthermore, a more complex model can be \ndesigned, by relating to the entire matrix rather than averaging the values. This work is \nmerely the first step in using PLMs for qs prediction, demonstrating it is indeed possible.\nConclusions\nProtein sequences hold information regarding the protein’s quaternary structure. Lever -\naging this information can be directed towards predicting the qs from sequence alone, as \nshown here in detail. This work offers a useful tool for qs prediction, and opens the door \nto further investigation and improvement of utilizing pLMs to predict qs.\nMethods\nAim, design and setting of study\nThe aim of this study is to develop a pLM sequence based MLP model for the predic -\ntion of the quaternary state of proteins (i.e., the number of monomers that assemble to \ngenerate a functional unit). We designed the study based on a filtered set of annotated \nproteins from the curated QSbio database: this set was divided into non-overlapping \nPage 14 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \nprotein families to allow for the stringent evaluation of performance on an independent \nset of proteins. We compare this model to several other approaches on the same dataset, \nincluding annotation transfer based on (1) the most similar sequence with known qs, (2) \nthe most similar pLM embeddings, and (3) a different pLM, Protbert_BFD; as well as \npredictions by established, structure-based protocols (4) PISA and (5) EPPIC.\nDataset\nThe data used in this study is a subset of the data annotated in QSbio V6_2020, cour -\ntesy of Emmanuel Levy [16]. The data was filtered so that the remaining set met the \nfollowing criteria: homomers (i.e., comprised of copies of the same monomer), with \nqs annotation (that was not changed in an internal validation process, i.e., “corrected_\nnsub” =  = “nsub”), and highest confidence level (best biological unit “Best_BU” =  = 1 \nand QSBIO_err_prob < 15). Duplicate fasta sequences were removed, retaining only the \nhighest confidence annotation. This process resulted in a final set of 31,994 entries, span-\nning 19 qs labels. The sequences were taken from the solved PDB structure, including all \nresidues present in the experimental construct. PISA and EPPIC performance were also \nextracted from this database.\nGenerating independent train and hold-out sets\nThis filtered set above was split into a train and a hold-out set (using the sklearn  func-\ntion model_selection.StratifiedGroupKFold [34]. To prevent leakage between the data \nused for training and the used for final validation of the model (the hold-out set), we \ngrouped the sequences by similarity (using MMseqs [35]), and clustered the data by \n30% sequence identity with at least 30% coverage: mmseqs easy-cluster < input_fasta.\nfasta > session < session_dir_location > –min-seq-id 0.3 -c 0.3 -s 8 –max-seqs 1000 –clus -\nter-mode 1 –cluster-reassign.\nThis relatively low coverage cut-off was selected to make sure the groups are indeed \nas separated as possible. Grouping of the data was done with the cluster representatives, \ndefining the sets so that an entire cluster is included in the same set. We stratified the \nsets to ensure similar distributions of qs states in both sets, as much as possible.\nAbout 10% of the data was put aside as a hold-out set. Within the remaining set we \nperformed several cross-validation experiments for hyper-parameter tuning, model \nselection etc. For this, we divided this set into 5 sets of approximately similar size, and \n5 cross-validation experiments were performed using each time a 80%-20% separa -\ntion of training and test set (the approximation is due to the group and stratification \nconstraints).\nGeneration of embeddings and pooling\nEmbeddings from the ESM-2 model were generated locally, with no need for a GPU. \nThe ESM-2 model generates a vector of length 1280 per each amino acid in each protein \nsequence, resulting in a matrix of size L × 1280 (L = sequence length). We performed \nmean pooling, averaging the matrix over the L dimension, to obtain a vector of 1280 for \neach sequence. This results in uniform embedding length for each entry.\nPage 15 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \nProtbert_BFD embeddings were generated similarly, where the representation for each \nresidue is a vector of length 1024, thus resulting in a vector representation of length \n1024 per entry.\nDimensionality reduction\nWe performed supervised dimensionality reduction with UMAP [36]. The parameters \nused are: n_components = 3, n_neighbors = 350, min_dist = 0.5.\nAfter obtaining the reduced vectors we plotted all three target dimensions for \nvisualization.\nNearest neighbor annotation\nComparison of the embeddings to the sequence was carried out by using the nearest \nneighbor algorithm to transfer the qs annotations. Annotation transfer was conducted \nin two parallel implementations: (1) without inclusion of homolog sequences for com -\nparison and annotation transfer (i.e., sequences in the test set were assigned a qs based \non the closest entry in the train set). (2) including all close homolog sequences as well \n(i.e., including sequences from the test set). The nearest neighbor was identified based \non sequence similarity distance calculated with a local alignment as incorporated in \nMMseqs [35], with default parameters (sequence-based annotation transfer), or based \non cosine similarity of the embedding vectors (for pLM-based annotation transfer).\nHyperparameter tuning and MLP model\nExploring hyperparameters was done using RandomizedSearchCV from sklearn  [34]. \nAmong the hyperparameters used was the downsampling of the monomer and the \ndimer classes, which improved performance and was thus included in the pipeline. For \ndownsampling we used the package imblearn and the function RandomUnderSampler \n[37]. The final downsampling factor chosen was 3, thus reducing to 33% the number of \nmonomers and dimers in the training set.\nThe hyperparameter search was performed on the fivefold cross-validation described \nabove, and the final parameters were selected based on the best results for adjusted bal -\nanced accuracy (see Additional file 1: Figure S4).\n• Model parameters:\nMLPClassifier\nhidden_layer_sizes=(120,), learning_rate=’adaptive’ , solver=’adam’ ,\nlearning_rate_init=0.01, max_iter=1000, n_iter_no_change=20,\nrandom_state=22, tol=0.001)\n• Sampled space (in bold: chosen parameter value):\nactivation = [’identity’ , ’logistic’ , ’tanh’ , ’relu’]\nlearning_rate = [’constant’ , ’invscaling’ , ’adaptive’]\nlearning_rate_init = [0.001,0.005, 0.01, 0.05, 0.1]\nsolver = [’sgd’ , ’adam’]\nPage 16 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \nMax_iter = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\nn_iter_no_change = [5, 10, 15, 20, 30, 40]\ntol = [0.0001, 0.0005, 0.001, 0.01, 0.1, 1]\nhidden_layer_sizes = [(10,), (20,), (40,), (60,), (80,), (100,), (120,), (140,), (160,), (180,), \n(200,), alpha = [0.0001, 0.0005, 0.001, 0.01, 0.1, 1]\nbatch_size = [10, 50, 100, 150, 200, 250, 400, 600, 1000, 1500, 2000, 5000]\nStatistical analysis\nThe statistical analysis was carried out using a one-sided (“greater”) Wilcoxon test, \nimplemented through scipy.stats.ranksums, with alternative = ’greater’, the correct pre -\ndictions passed as x and the wrong predictions passed as y.\nPackages and versions\nAll the scripts were written using Python3.7.\nEmbedding the sequences was carried out using Torch. The ESM-2 model used is Esm-\nForSequenceClassification, with the specific version of esm2_t33_650M_UR50D() [25]. \nWe initialized the parameter called \"problem_type\" to be \"multi label” .\nEmbedding with Protbert was done using BertModel and the specific version prot_\nbert_bfd [23].\nMMseqs version ad5837b3444728411e6c90f8c6ba9370f665c443 was used, installed \nlocally.\nUMAP version 0.5 was used.\nFor analyses and data handling we used pandas version 1.3.5 [38].\nGraphs and plots were generated with matplotlib and seaborn, versions 3.5.3 and \n0.12.0 [39, 40].\nThe network plot was generated using networkx version 3.0.\nPymol\nProtein structure visualization was done using PyMol version 2.2.0 Open-Source.\nAbbreviations\nQs  Quaternary state\npLM  Protein language model\nQUEEN  QUaternary state prediction using dEEp learNing\nMLP  MultiLayer Perceptron\nBA  Balanced accuracy\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859‑ 023‑ 05549‑w.\nAdditional file 1: Supplementary Table SI:  Embeddings.\nAdditional file 2: Supplementary Table SII:  Model Results. \nAdditional file 3: Supplementary Material. Contains Supplementary Figures S1‑S6 and Supplementary Tables SIII‑SV.\nAcknowledgements\nThe authors thank Prof. Emmanuel Levy for the updated QSbio data and for insightful discussions. The authors, and \nspecifically OA, would like to thank Matan Avraham for fruitful discussions and thoughts, and for his help with Figures 2C \nand 7.\nPage 17 of 18\nAvraham et al. BMC Bioinformatics          (2023) 24:433 \n \nAuthor contributions\nOA and TT conceived the idea for the research. TT performed proof of concepts experiments, and OA performed the \nexperiments for the model presented here. OA, TT and OSF refined the concept and realized the final project and analy‑\nsis. ZBA and LT assisted with analyses and ideas. OA and OSF wrote the manuscript. OSF supervised the project, OA and \nOSF acquired funding.\nFunding\nFunding for this research to OA was partially provided by: Teva Pharmaceutical Industries Ltd as part of the Israeli \nNational Forum for BioInnovators (NFBI) and a Raymond and Janine Bollag Post‑Doctoral Fellowship Fund (Lady Davis). \nThis work was supported, in whole or in part, by the Israel Science Foundation, founded by the Israel Academy of Sci‑\nence and Humanities Grant 301/2021 (to O.S.‑F.)\nAvailability of data and materials\nAll data is provided as supplementary tables in csv format. The data is also available in the github repository: https:// \ngithub. com/ Furman‑ Lab/ QUEEN.\nDeclarations\nEthics approval and consent to participate.\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 31 March 2023   Accepted: 27 October 2023\nReferences\n 1. Goodsell DS, Olson AJ. Structural symmetry and protein function. Annu Rev Biophys Biomol Struct. 2000;29:105–53.\n 2. Marciano S, Dey D, Listov D, Fleishman SJ, Sonn‑Segev A, Mertens H, et al. Protein quaternary structures in solution \nare a mixture of multiple forms. Chem Sci. 2022;13(39):11680–95.\n 3. Jacobson RH, Zhang XJ, DuBose RF, Matthews BW. Three‑dimensional structure of beta‑galactosidase from E. coli. \nNature. 1994;369(6483):761–6.\n 4. Uemura M, Nozaki H, Koyama A, Sakai N, Ando S, Kanazawa M, et al. HTRA1 mutations identified in symptomatic \ncarriers have the property of interfering the trimer‑dependent activation cascade. Front Neurol. 2019;28(10):693.\n 5. Ashburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, et al. Gene Ontology: tool for the unification of biol‑\nogy. Nat Genet. 2000;25(1):25–9.\n 6. The Gene Ontology Consortium. The gene ontology resource: enriching a GOld mine. Nucleic Acids Res. \n2021;49(D1):D325–34.\n 7. Kanehisa M, Furumichi M, Sato Y, Kawashima M, Ishiguro‑Watanabe M. KEGG for taxonomy‑based analysis of path‑\nways and genomes. Nucleic Acids Res. 2023;51(D1):D587–92.\n 8. Cheng H, Schaeffer RD, Liao Y, Kinch LN, Pei J, Shi S, et al. ECOD: an evolutionary classification of protein domains. \nPLoS Comput Biol. 2014;10(12): e1003926.\n 9. Mistry J, Chuguransky S, Williams L, Qureshi M, Salazar GA, Sonnhammer ELL, et al. Pfam: the protein families data‑\nbase in 2021. Nucleic Acids Res. 2021;49(D1):D412–9.\n 10. Sillitoe I, Bordin N, Dawson N, Waman VP , Ashford P , Scholes HM, et al. CATH: increased structural coverage of func‑\ntional space. Nucleic Acids Res. 2021;49(D1):D266–73.\n 11. Elez K, Bonvin AMJJ, Vangone A. Biological vs. crystallographic protein interfaces: an overview of computational \napproaches for their classification. Crystals. 2020;10(2):114.\n 12. Krissinel E, Henrick K. Inference of macromolecular assemblies from crystalline state. J Mol Biol. 2007;372(3):774–97.\n 13. Duarte JM, Srebniak A, Schärer MA, Capitani G. Protein interface classification by evolutionary analysis. BMC Bioinfor‑\nmatics. 2012;22(13):334.\n 14. Baek M, Park T, Heo L, Park C, Seok C. GalaxyHomomer: a web server for protein homo‑oligomer structure prediction \nfrom a monomer sequence or structure. Nucleic Acids Res. 2017;45(W1):W320–4.\n 15. Xu Q, Dunbrack RL. ProtCID: a data resource for structural information on protein interactions. Nat Commun. \n2020;11(1):711.\n 16. Dey S, Ritchie DW, Levy ED. PDB‑wide identification of biological assemblies from conserved quaternary structure \ngeometry. Nat Methods. 2018;15(1):67–72.\n 17. Akdel M, Pires DEV, Pardo EP , Jänes J, Zalevsky AO, Mészáros B, et al. A structural biology community assessment of \nAlphaFold2 applications. Nat Struct Mol Biol. 2022;29(11):1056–67.\n 18. Schweke H, Levin T, Pacesa M, Goverde CA, Kumar P , Duhoo Y, et al. An atlas of protein homo‑oligomerization across \ndomains of life. BioRxiv. 2023 Jun 11;\n 19. Olechnovič K, Valančauskas L, Dapkūnas J, Venclovas Č. Prediction of protein assemblies by structure sampling fol‑\nlowed by interface‑focused scoring. Proteins. 2023 Aug 14;\n 20. Balakrishnan S, Kamisetty H, Carbonell JG, Lee S‑I, Langmead CJ. Learning generative models for protein fold fami‑\nlies. Proteins. 2011;79(4):1061–78.\nPage 18 of 18Avraham et al. BMC Bioinformatics          (2023) 24:433 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 21. Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM. Unified rational protein engineering with sequence‑based \ndeep representation learning. Nat Methods. 2019;16(12):1315–22.\n 22. Bepler T, Berger B. Learning protein sequence embeddings using information from structure. arXiv. 2019;\n 23. Elnaggar A, Heinzinger M, Dallago C, Rihawi G, Wang Y, Jones L, et al. ProtTrans: Towards Cracking the Language of \nLife’s Code Through Self‑Supervised Deep Learning and High Performance Computing. arXiv. 2020;\n 24. Verkuil R, Kabeli O, Du Y, Wicky BI, Milles LF, Dauparas J, et al. Language models generalize beyond natural proteins. \nBioRxiv. 2022 Dec 22;\n 25. Lin Z et al. Evolutionary‑scale prediction of atomic‑level protein structure with a language model. Science. \n2023;379(6637):1123–1130.\n 26. Littmann M, Heinzinger M, Dallago C, Weissenow K, Rost B. Protein embeddings and deep learning predict binding \nresidues for various ligand classes. Sci Rep. 2021;11(1):23916.\n 27. Bana J, Warwar J, Bayer EA, Livnah O. Self‑assembly of a dimeric avidin into unique higher‑order oligomers. FEBS J. \n2023 \n 28. Berman HM, Westbrook J, Feng Z, Gilliland G, Bhat TN, Weissig H, et al. The protein data bank. Nucleic Acids Res. \n2000;28(1):235–42.\n 29. Littmann M, Heinzinger M, Dallago C, Olenyi T, Rost B. Embeddings from deep learning transfer GO annotations \nbeyond homology. Sci Rep. 2021;11(1):1160.\n 30. Ferruz N, Heinzinger M, Akdel M, Goncearenco A, Naef L, Dallago C. From sequence to function through structure: \nDeep learning for protein design. Comput Struct Biotechnol J. 2023;21:238–50.\n 31. Ofer D, Brandes N, Linial M. The language of proteins: NLP , machine learning & protein sequences. Comput Struct \nBiotechnol J. 2021;25(19):1750–8.\n 32. Bileschi ML, Belanger D, Bryant DH, Sanderson T, Carter B, Sculley D, et al. Using deep learning to annotate the \nprotein universe. Nat Biotechnol. 2022;40(6):932–7.\n 33. UniProt Consortium. The universal protein resource (uniprot) in 2010. Nucleic Acids Res. 2010 (Database \nissue):D142–8.\n 34. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit‑learn: Machine Learning in Python. \narXiv. 2012;\n 35. Steinegger M, Söding J. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. \nNat Biotechnol. 2017;35(11):1026–8.\n 36. McInnes L, Healy J, Melville J. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. \narXiv. 2018;\n 37. Lemaˆıtre G, Nogueira F, Aridas CK. Imbalanced‑learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets \nin Machine Learning. Journal of Machine Learning Research. 2017 \n 38. McKinney W. Data structures for statistical computing in python. Proceedings of the 9th Python in Science Confer‑\nence. SciPy; 2010. p. 56–61.\n 39. Hunter JD. Matplotlib: a 2D graphics environment. Comput Sci Eng. 2007;9(3):90–5.\n 40. Waskom M. Seaborn: statistical data visualization. JOSS. 2021;6(60):3021.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Protein quaternary structure",
  "concepts": [
    {
      "name": "Protein quaternary structure",
      "score": 0.8524582386016846
    },
    {
      "name": "Computer science",
      "score": 0.6131182312965393
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5634199380874634
    },
    {
      "name": "Quaternary",
      "score": 0.5522025227546692
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5379285216331482
    },
    {
      "name": "Quaternary science",
      "score": 0.4393075704574585
    },
    {
      "name": "Complement (music)",
      "score": 0.4197912812232971
    },
    {
      "name": "Natural language processing",
      "score": 0.37276315689086914
    },
    {
      "name": "Machine learning",
      "score": 0.34603679180145264
    },
    {
      "name": "Biology",
      "score": 0.19368955492973328
    },
    {
      "name": "Genetics",
      "score": 0.09802988171577454
    },
    {
      "name": "Complementation",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Protein subunit",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}