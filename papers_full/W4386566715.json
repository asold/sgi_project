{
  "title": "Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models",
  "url": "https://openalex.org/W4386566715",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5000188096",
      "name": "Shrimai Prabhumoye",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031170568",
      "name": "Mostofa Patwary",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5072436307",
      "name": "Mohammad Shoeybi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066242985",
      "name": "Bryan Catanzaro",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385894687",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4225959163",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W4287028455",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2963912046",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W4221148519",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4206637810",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W3167841296",
    "https://openalex.org/W3086249591",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2973727699"
  ],
  "abstract": "Pretrained large language models have become indispensable for solving various natural language processing (NLP) tasks. However, safely deploying them in real world applications is challenging because they generate toxic content. To address this challenge, we propose two novel pretraining data augmentation strategies that significantly reduce model toxicity without compromising its utility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data to the pretraining samples, and (2) INST: adds instructions to those samples indicating their toxicity. Our results indicate that our best performing strategy (INST) substantially reduces the toxicity probability up to 61% while preserving the accuracy on five benchmark NLP tasks as well as improving AUC scores on four bias detection tasks by 1.3%. We also demonstrate the generalizability of our techniques by scaling the number of training samples and the number of model parameters.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2636–2651\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nAdding Instructions during Pretraining:\nEffective Way of Controlling Toxicity in Language Models\nShrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro\nNVIDIA\n{sprabhumoye, mpatwary, mshoeybi, bcatanzaro}@nvidia.com\nAbstract\nPretrained large language models have become\nindispensable for solving various natural lan-\nguage processing (NLP) tasks. However, safely\ndeploying them in real world applications is\nchallenging because they generate toxic con-\ntent. To address this challenge, we propose two\nnovel pretraining data augmentation strategies\nthat significantly reduce model toxicity with-\nout compromising its utility . Our two strate-\ngies are: (1) MEDA: adds raw toxicity score as\nmeta-data to the pretraining samples, and (2)\nINST: adds instructions to those samples indi-\ncating their toxicity. Our results indicate that\nour best performing strategy (INST) substan-\ntially reduces the toxicity probability up to61%\nwhile preserving the accuracy on five bench-\nmark NLP tasks as well as improving AUC\nscores on four bias detection tasks by 1.3%.\nWe also demonstrate the generalizability of our\ntechniques by scaling the number of training\nsamples and the number of model parameters.\n1 Introduction\nPretrained large language models (LMs) have\nbecome indispensable for solving various NLP\ntasks (Brown et al., 2020; Smith et al., 2022;\nChowdhery et al., 2022), yet it has been challeng-\ning to safely deploy them for real world applica-\ntions (McGuffie and Newhouse, 2020; Prabhumoye\net al., 2021a). They have been known to generate\nharmful language encompassing hate speech, abu-\nsive language, social biases, and threats (Gehman\net al., 2020; Welbl et al., 2021; Bender et al., 2021;\nHovy and Prabhumoye, 2021). These harmful gen-\nerations are broadly referred to as “toxicity”.1 This\nwork focuses on reducing the toxicity in large LMs\nby augmenting their pretraining data.\nPrior work has primarily focused on reducing\ntoxicity either by finetuning LMs on non-toxic\n1In this work we use toxicity as defined by Perspec-\ntiveAPI (PerspectiveAPI, 2022) but our techniques can be\napplied to other broader definitions of bias or toxicity.\nFigure 1: Overview of the proposed approaches and the\nbaseline (BASE). We propose two new data augmen-\ntation strategies, MEDA and INST. The text in purple\nare control variables indicating the desired toxicity level\nof the text. The text in black is the input to the model\nand the text in green is the generated output using each\nstrategy with 1.3b parameter model.\ndata (Gururangan et al., 2020; Ouyang et al., 2022;\nWang et al., 2022) or using decoding time al-\ngorithms to re-weight the probabilities of toxic\nwords (Krause et al., 2021; Schick et al., 2021; Liu\net al., 2021). These methods typically incur further\ncosts of finetuning additional LMs (Krause et al.,\n2021; Liu et al., 2021), generating large amount\nof non-toxic data (Wang et al., 2022), or procur-\ning human feedback (Ouyang et al., 2022). These\ntechniques are known to reduce toxicity but also\ncompromise perplexity and downstream task per-\nformance (Wang et al., 2022; Xu et al., 2021). Fur-\nthermore, these methods are only useful after the\nLMs are pretrained.\nOur approach aims to reduce toxicity during pre-\ntraining itself, thus incurring no additional cost\nonce the LM is trained. We augment the pretrain-\ning data with information pertaining to its toxicity.\n2636\nWe believe that instead of filtering toxic data (Welbl\net al., 2021; Ngo et al., 2021), the toxicity informa-\ntion can guide the LM to detect toxic content and\nhence generate non-toxic text. Hence, we develop\ntwo novel data augmentation strategies: (1) MEDA:\nadds raw toxicity score of a sample as meta-data,\nand (2) INST: augments an instruction to the sam-\nple indicating its toxicity. We use a classifier to\nobtain sample-level toxicity score of the pretrain-\ning data. These scores are used by MEDA and\nINST to educate the LM about toxicity.\nFig. 1 shows an example of how MEDA and\nINST are applied. We add the raw toxicity score of\nthe sample along with a tag “toxicity: 0.1” for the\nMEDA strategy. Similarly, we add an instruction\nsuch as “This is a non-toxic post. Post:” to the\ntokens of a non-toxic sample for INST strategy. No\ndata augmentation is applied for BASE.\nThe goal of our strategies is to reduce tox-\nicity in text generation while preserving util-\nity on benchmark NLP tasks and bias detection\ntasks. Prior work only evaluates the success of\ntoxicity reduction techniques on REALTOXICI -\nTYPROMPTS (Gehman et al., 2020). Few tech-\nniques are evaluated for their utility in performing\nsome benchmark NLP tasks (Wang et al., 2022;\nXu et al., 2021). But toxicity reduction techniques\nlike data filtering can reduce the bias and toxicity\ndetection capabilities of the LMs (Xu et al., 2021).\nSome techniques like finetuning (Gururangan et al.,\n2020; Wang et al., 2022) can also reduce the capa-\nbility of the LM to effectively perform downstream\nend-to-end text generation tasks.\nHence, we expand the evaluation to include -\n(1) Bias Detection Tasks: we evaluate the capabil-\nity of our strategies to detect social biases (Sap\net al., 2020), and (2) Text Generation Task: we\nmeasure the performance of our strategies on the\nE2E task (Novikova et al., 2017).\nIn summary, our primary contribution is: we de-\nvelop MEDA and INST - two new strategies to re-\nduce toxicity by augmenting pretraining data (§2.2).\nTo our knowledge, these are the first toxicity reduc-\ntion techniques which augment the pretraining data\nwith toxicity information without filtering any data.\nAdditionally, we broaden the current evaluation to\ninclude two new metrics on bias detection and text\ngeneration task (§3.1). Furthermore, we perform\nexperiments with scaling the number of training\nsamples and the number of parameters of the LM.\nOur results demonstrate that our best performing\nstrategy (INST) considerably reduces the toxicity\nprobability of the generations by as much as∼61%\nwhile preserving the utility of the LM on five bench-\nmark NLP tasks as well as improving on the four\nbias detection tasks by 1.3% (§4). Moreover, we\ndemonstrate that our strategies applied at sample-\nlevel perform better than document-level (Welbl\net al., 2021; Ngo et al., 2021) by 11% in toxicity\nprobability reduction (§6).\n2 Methodology\nOur approach guides the LM about the toxicity of\nthe data it sees during training and directs it to gen-\nerate non-toxic content. We educate the LM by\naugmenting the pretraining dataset Dwith toxic-\nity information. We first use a classifier to obtain\ntoxicity scores for samples (S) in D. We add the\ndesired toxicity scores to S in two forms - as raw\nscores in the form of meta-data and as instructions\nin natural language form.\n2.1 Toxicity Scoring\nWe use the widely accepted Commercial Perspec-\ntiveAPI (PerspectiveAPI, 2022) to get toxicity\nscores for each sample. Note that our strategies\ncan be applied using any other classifier. Perspec-\ntiveAPI scores text of at most size20KB characters\nor ∼4000 tokens. This is larger than the maximum\nsequence length permitted by LMs (typically 1024\nor 2048 tokens). Hence, first obtaining Perspec-\ntiveAPI score and then splitting the documents into\nsamples of maximum permitted sequence length\nwould yield inaccurate toxicity scores for the sam-\nples. Moreover, some documents are larger than\n4000 tokens. Note that simply splitting the larger\ndocuments into chunks of 2000 tokens and then av-\neraging the PerspectiveAPI scores for each chunk\ndoes not yield accurate results. 2 Hence, we first\nprocess the documents in our dataset into samples\nof 2000 tokens and then get PerspectiveAPI scores\nfor all the samples.\nDataset We use the corpus and the sampling\nweights for each dataset described in Smith et al.\n(2022). In total we used 15 datasets - the Com-\nmon Crawl (CC-2020-50 and CC-2021-04) (Com-\nmonCrawl, 2022) accounting for majority of the\nsamples. From The Pile (Gao et al., 2020), we\nuse Books3, OpenWebText2, Stack Exchange,\nPubMed Abstracts, Wikipedia, Gutenberg (PG-19),\n2We show this analysis in detail in Appendix A.\n2637\nFigure 2: Histogram of toxicity scores for the entire pre-\ntraining corpus and the percentage of samples belonging\nto each category. We observe that 73.6% samples have\ntoxicity scores below 0.2 and only 4.14% samples have\ntoxicity scores above 0.5.\nBookCorpus2, NIH ExPorter, ArXiv, GitHub, and\nPile-CC datasets. In addition, we also use Real-\nnews (Zellers et al., 2019b) and CC-Stories (Trinh\nand Le, 2018). In total, this corpus consists of 339\nbillion tokens and we only select a subset of the\ncorpus to train models that use our strategy.\nAnalysis of Toxicity ScoresWe divide the doc-\numents in our corpus into samples of sequence\nlength 2000 tokens. Fig. 2 shows a histogram of\nthe toxicity scores of the samples for the entire cor-\npus. We observe that a majority of 73.6% samples\nhave toxicity scores below 0.2 and only 4.14% of\nthe samples have toxicity scores above 0.5. This is\nin agreement with the document-level data analysis\nshown in Gehman et al. (2020).\nFig. 3 shows the percentage of samples that have\ntoxicity scores ≥0.5 for each dataset in our corpus.\nWe identify that BookCorpus2 and Stories have the\nhighest percentages of toxic samples - 18% and\n11%. The datasets with less than 1% toxic sam-\nples are Github, Wikipedia, ArXiv, StackExchange,\nPubMedAbstracts, and NIH-ExPorter.\nFilter Approach (FILT) This strategy filters data\nwith toxicity scores above a certain threshold. Prior\nwork removes the toxic documents and trains the\nLM on less data (Welbl et al., 2021; Ngo et al.,\n2021). To avoid inconsistency in obtaining toxi-\ncity score at document-level and then applying it\nto samples, we employ this strategy at the sample-\nlevel. We use a toxicity threshold of 0.5. From\nFig. 2, we see that this method filters out 4.14%\nof the samples. In contrast, for fair comparison\nFigure 3: Percentage of samples in each dataset of our\ncorpus with toxicity score ≥0.5. We observe that Book-\nCorpus2 has the highest percentage of toxic samples\n(18%) and NIH-ExPorter has the lowest percentage of\ntoxic samples (0.1%).\nwith baseline, we maintain the same number of\nsamples across strategies. Hence, we replenish the\npretraining dataset with 4.14% of non-toxic sam-\nples. Note that this is a stronger strategy compared\nto completely removing documents.\n2.2 Proposed Strategies\nWe suppose that filtering data can potentially com-\npromise the capability of the LM in performing\nbenchmark NLP tasks and especially hinder its bias\ndetection capabilities. Our approach is designed\nto guide the LM by providing it information about\ntoxicity of the samples it sees during training.\nOur Algorithm for Data AugmentationAlgo-\nrithm 1 illustrates the logic of augmenting the\ndata for the two strategies - MEDA and INST.\nThese strategies modify samples S in the pre-\ntraining dataset Dbased on their toxicity scores\n(tox_score) received through PerspectiveAPI. We\nconsider two threshold: HIGH THRESH above\nwhich the samples are augmented with the control\nvariable Ctox and LOWTHRESH below which sam-\nples are appended with the control variable Cnont.\nEach strategy described below has its own value of\nCtox indicating that S is toxic and Cnont marking\nthat S is non-toxic.\nNote that we don’t augment Ctox or Cnont to\nevery sample that lies within a threshold. We use\nadditional variables - PRMTOX controls the permis-\nsible toxic samples for which Ctox can be added,\nand PRMNONT regulates the percentage of non-\ntoxic samples for which Cnont can be augmented.\nThis is done to encourage generalizability of the\nLM where we show the model toxic samples with\nCtox and without it (just the way the sample is seen\n2638\nAlgorithm 1Data Augmentation Method\nfor S in Ddo\ntox_score ←PerspectiveAPI(S)\nif tox_score ≥HIGH THRESH then\nr ←get_random()\nif r <PRMTOX then\nS ←Ctox+S\nelse iftox_score < LOWTHRESH then\nr ←get_random()\nif r <PRMNONT then\nS ←Cnont+S\nwithout any modification by the baseline model).\nAdditionally, all the samples with LOWTHRESH\n≤tox_score < HIGH THRESH are not modified.\nMeta-Data Augmentation (MEDA) This strat-\negy guides the LM by adding toxicity score in the\nform of raw meta-data at the beginning of some\nof the samples in the pretraining set. This makes\nCnont = “toxicity: 0.1” for all the samples with\ntoxicity scores below the LOWTHRESH and Ctox\n= “toxicity: 0.5” for all the samples with toxicity\nscores above the HIGH THRESH . Note that all the\nsamples within a threshold get the same value, ei-\nther 0.1 or 0.5. For example, even if a sample has\na toxicity score of 0.8, Ctox will still be equal to\n“toxicity: 0.5” if the HIGH THRESH = 0.6.\nInstruction Augmentation (INST) This strategy\nguides the LM by adding an instruction in the nat-\nural language form at the beginning of samples\nindicating if they are toxic or not. This instruc-\ntion is based on the toxicity score of the sample.\nFor samples with toxicity score above the HIGH -\nTHRESH , Ctox= {“Instruction: Complete the fol-\nlowing text in a toxic manner. Text:” , “This is\na toxic post. Post:” , “Instruction: Complete the\nfollowing text in a toxic manner. Text:” }. Simi-\nlarly, Cnont= {“Instruction: Complete the follow-\ning prompt in a non-toxic manner. Prompt:”, “This\nis a non-toxic post. Post:”, “Instruction: Complete\nthe following text in a respectable manner. Text:”}.\nWe randomly select one of the three possible in-\nstructions to be added to the samples.\n3 Experimental Setup\nModeling Details We train all our models from\nscratch using the decoder-only architecture in\nMegatron-LM (Shoeybi et al., 2019). We train\nbaseline models (BASE) without any data augmen-\ntation strategies. Subsequently, we train models\nusing each data augmentation strategy under four\ndifferent configurations which scale the number of\npretraining samples as well as the number of model\nparameters. We train 357 million parameter mod-\nels with 96 million (357m-96m) samples and 150\nmillion (357m-150m) samples. Similarly, we train\n1.3 billion parameter models with 96 million (1.3b-\n96m) and 150 million (1.3b-150m) samples.3\nFor all models, we use HIGH THRESH = 0.5,\nLOWTHRESH = 0.1 and PRMTOX= 0.9. This im-\nplies 90% of toxic samples in the dataset are ap-\npended with Ctox. Note that only 4.14% of the en-\ntire train set has tox_score ≥0.5 (Fig. 2) which\nmeans that 3.73% (90% of 4.14) of the entire train\nset receive Ctox. For MEDA PRMNONT= 0.5\nand for INST PRMNONT= 0.9. 34.59% of the en-\ntire dataset has tox_score < 0.1 (from Fig.2).\nThis means that for MEDA 17.3% of samples\nand for INST 31.13% of samples are augmented\nwith Cnont. From these values, we derive that for\nMEDA ∼79% of samples and for INST 65.14%\nof samples don’t undergo any modification. Unless\nmentioned otherwise, we use greedy decoding for\nall the evaluation tasks.\n3.1 Evaluation\nWe evaluate the success of our strategies along four\ndifferent dimensions. We would like our strategies\n(1) to reduce toxicity in text generation, (2) to per-\nform as good as the baseline on benchmark NLP\ntasks as well as (3) bias detection tasks, and (4) to\nnot affect the quality of downstream text generation\ntasks. We do not finetune the LMs to evaluate on\nany of the below mentioned tasks.\nToxicity Evaluation We follow the setup in\nGehman et al. (2020) and use the full set of prompts\n(∼100k) to evaluate toxicity of LM generations\nvia PerspectiveAPI. Gehman et al. (2020) propose\ntwo metrics: (1) Expected Maximum Toxicity cal-\nculates the maximum toxicity scores over 25 gener-\nations for the same prompt with different random\nseeds, and then averages the maximum toxicity\nscores over all prompts, and (2) Toxicity Probabil-\nity evaluates the probability of generating a toxic\ncontinuation at least once over 25 generations for\nall prompts. We follow Gehman et al. (2020) and\nrestrict the generations up to 20 tokens or below.\nWang et al. (2022) show that toxicity scores from\nPerspectiveAPI are strongly correlated with human\n3Additional hyper-parameter details in Appendix C.\n2639\nFigure 4: We average the percentage relative gains or losses achieved by each of the strategies over BASE across\nthe eleven tasks described in §3.1. We show that MEDA and INST perform better than FILT and BASE on all four\nmodel configurations. Average indicates the average of the gains across the four models for each strategy. These\nresults demonstrate that INST performs the best across the board.\nevaluation (Pearson correlation coefficient = 0.97).\nHence, we only report PerspectiveAPI evaluation.\nUnder this setup, we perform two types of experi-\nments: (1) with no control variable which is exactly\nsame as (Gehman et al., 2020), and (2) generating\ncontinuations by adding the respective control vari-\nable Cnont for MEDA and INST. Note that we only\nadd Cnont at beginning of all the prompts and not\nCtox. This is because we want to encourage the LM\nto generate a non-toxic continuation to the given\nprompt without sacrificing their quality.\nBenchmark NLP Tasks To ensure that our strate-\ngies do not affect the utility of the LMs, we\nevaluate them on five benchmark NLP tasks cov-\nering - completion prediction (LAMBADA (Pa-\nperno et al., 2016)), natural language understand-\ning (ANLI (Nie et al., 2020)), and commonsense\nreasoning (Winogrande (Sakaguchi et al., 2020),\nHellaswag (Zellers et al., 2019a), PiQA (Bisk et al.,\n2020)). We evaluate them in the fewshot set-\nting without any finetuning following the setup in\nBrown et al. (2020); Smith et al. (2022). We report\naverage accuracy across the five tasks.\nNote that these are prediction tasks where the\nLM has to choose an answer given a set of choices.\nThe model does not have to perform free-form gen-\nerations for these tasks. Hence, we do not evaluate\nby adding the control variables.\nBias Detection Tasks To ensure that our strate-\ngies do not affect the bias detection capabilities\nof the LMs, we evaluate them on four social bias\ndetection tasks - detect if the text is offensive, inten-\ntional insult, contains lewd language, and predict\nif the text is offensive to a group or an individual.\nThe bias detection tasks are described in Sap et al.\n(2020). We follow the setup in Prabhumoye et al.\n(2021b) to perform 32-shot classification where 32\nsamples are selected from the train set to be pro-\nvided as in-context samples to the LM. We report\naverage Area Under Cover (AUC) scores (Scikit-\nlearn, 2022) across the four tasks. In these tasks as\nwell, we don’t use the control variables.\nText Generation Task To evaluate if our tech-\nniques affect the downstream text generation, we\nassess them on the E2E dataset (Novikova et al.,\n2017). We perform the task in a few-shot manner\nby providing the LM with 10-shots as context. We\nmeasure the success on Rouge-L metric by compar-\ning the generation with ground truth. The primary\ngoal of this task is to check if adding control vari-\nables affects the performance of E2E task.\n4 Results and Discussions\nThrough this section we present the aggregated re-\nsults and analyze them. To do this, we calculate the\nrelative percentage difference compared to BASE\nfor all the twelve metrics across the eleven tasks\n- expected maximum toxicity, toxicity probability,\naccuracy of five NLP tasks, AUC scores of four\nbias detection tasks, and Rouge-L for E2E task.\nWe then compute an average across all the met-\nrics (we also include the experiments with control\nvariable Cnont). In Fig. 4 we show the average per-\ncentage gains achieved by each strategy across the\neleven tasks. The detailed results for all the tasks\nare shown in Tables 2 and 3 in Appendix B. We\nwill go in detail about each evaluation metric in\nsubsequent sections.\nFig. 4 demonstrates that all the strategies are\nbetter than BASE. Fig. 4 illustrates that MEDA\nand INST are generalizable because they deliver\nconsistent gains when scaled from 96m samples\nto 150m samples and from 357m to 1.3b model\nparameters. If we average the gains across the\nfour models, then we observe that INST strategy\nattains the most gains (14%) and hence is the best\n2640\nFigure 5: We show the average percentage gain in toxic-\nity reduction by MEDA and INST across the four model\nconfigurations. We observe that we get higher gains\n(43% higher for MEDA and 54% higher INST in abso-\nlute terms) when using Cnont.\nstrategy. Moreover, both the strategies developed in\nthis work - the meta-data-based MEDA is 6% and\ninstruction-based INST is 6.3% better than FILT.\nSince the performance of the strategies is con-\nsistent across the four models, we only show the\naverage behavior of the approaches across the four\nmodels for each of the metrics.\nToxicity Evaluation Fig.5 presents the relative\npercentage gains in expected maximum toxicity\nand toxicity probability compared to BASE. It also\nshows the results for MEDA and INST by using\ntheir respective control variable Cnont vs not.\nWe observe that MEDA and INST are successful\nin reducing the toxicity of generations as evaluated\nby REALTOXICITY PROMPTS setup (Gehman et al.,\n2020). Specifically, we see huge gains in toxicity\nreduction when we use control variable Cnont asso-\nciated with MEDA and INST (compare striped vs\nnon-striped bars for each color). This is because we\nare directing the LM to generate non-toxic content\nby prefacing the prompt with Cnont. FILT gives\n8% improvement over BASE on expected maxi-\nmum toxicity and 17% gain on toxicity probability.\nBut this cannot be improved further because there\nare no control variables associated with this strat-\negy. INST on the other hand establishes 29.3%\nimprovement in expected maximum toxicity and\na 61.3% improvement in toxicity probability i.e\nINST reduces the probability of generating toxic\ncontent by ∼61% and the probability is as low\nas 0.14. This suggests that guiding the LM with\ntoxicity information in the form of instructions is\nmore successful in reducing toxicity compared to\nfiltering data.\nBenchmark NLP tasks Fig. 6 shows the average\naccuracy of five benchmark NLP tasks across the\nfour model configurations for the three strategies\nalong with BASE. We observe that MEDA and\nFigure 6: We report average accuracy across five bench-\nmark NLP tasks and average AUC across four bias de-\ntection tasks for each strategy (including BASE) across\nfour models. We see that all the strategies perform as\ngood as the BASE proving that our data augmentation\nstrategies don’t compromise the utility of the LM.\nINST are as competent as BASE on the NLP tasks.\nIn fact, we observe a marginal gain of < 1% for\nFILT, MEDA and INST. This shows that our data\naugmentation strategies don’t harm the utility of\nthe LMs trained on it.\nBias Detection Tasks Additionally, Fig. 6 also\nshows the average AUC scores of the models across\nthe four configurations for the four bias detection\ntasks. Similar to NLP tasks, the results illustrate\nthat all the strategies perform better than baseline\n(we see a gain of 2.5% for FILT, 1.4% for MEDA,\nand 1.3% for INST). We believe MEDA and INST\nperform well on these tasks because they were\nshown examples of both toxic and non-toxic sam-\nples through their respective control variables.\nWe suppose that FILT performs well on both\nbenchmark NLP tasks and bias detection tasks be-\ncause it was trained on equal number of samples\nas MEDA and INST. Additionally, it saw only non-\ntoxic samples (the toxic samples were replaced by\nnon-toxic samples). Hence, the perplexity for toxic\nsentences would be higher in FILT. We discuss this\nin detail in §6.\nText Generation Task Since we see huge gains\nin Fig. 5 by adding Cnont, we want to evaluate if\nadding Cnont affects the performance of a down-\nstream text generation task. Fig. 7 shows the av-\nerage of Rouge-L scores across the four model\nconfigurations for MEDA and INST strategy us-\ning Cnont (striped bars) and without using it (non-\nstriped bars). Fig. 7 illustrates that overall there is\nno effect of adding control variables on the E2E\ntask (we see a gain of 0.5% and 2.0% for MEDA\nand INST respectively when using Cnont).\n2641\nFigure 7: We report Rouge-L scores for the E2E task\nfor MEDA and INST strategies with and without Cnont.\nWe demonstrate that augmenting Cnont does not affect\nthe performance of the LM on E2E task.\n4.1 Ablations\nAdditional details are provided in Appendix D.\nScaling the Model Size We scale our experi-\nments to a 8.3 billion parameter models with 150\nmillion samples and train using three strategies -\nBASE, FILT and our best performing INST. Table\n5 shows the results of these models on toxicity eval-\nuation, benchmark NLP tasks and bias detection\ntasks. We see similar trends to our main results i.e\nthe FILT strategy provides only8% improvement\nwhereas the INST strategy demonstrates a huge\ngain of 34% for expected maximum toxicity and\nFILT provides 19.6% and INST illustrates a signif-\nicant gain of 69.5 % for toxicity probability when\nwe use the non-toxic control variable Cnont. Sim-\nilarly, we observe a 0.1% decrease for FILT and\n0.7% increase for INST in benchmark NLP tasks;\nand we see a 11.5% increase for FILT and 12.7%\nincrease for INST for bias detection tasks. These\nexperiments illustrate that INST strategy performs\neven better on larger LMs.\nINST Variations With our best performing INST\nstrategy, we vary the PRMNONT. For INST, PRM-\nNONT is 0.9. We train INST-11 with PRMNONT\n= 0.11 and INST-50 with PRMNONT = 0.5 for\nall four model configurations. The percentage of\ntoxic samples for which the model sees Ctox re-\nmains the same (3.73%) for all the three variations.\nThe percentage of non-toxic samples for which the\nmodel sees Cnont is 3.8% for INST-11, 17.3% for\nINST-50 and 31.13% for INST.\nThe results in Fig. 8 indicate that increasing\nPRMNONT increases the overall average percent-\nage gain across eleven tasks. This implies that\nadding Cnont to more number of samples is helpful\nfor the model in understanding toxicity. We leave it\nfor future work to explore the limit of PRMNONT.\nFigure 8: Average gains achieved by INST-11, INST-50\nand INST over BASE across the eleven tasks and four\nmodel configurations. We see that the average perfor-\nmance of the model improves when higher percentage\nof samples receive the control variable Cnont.\nFILT Variations We also vary the threshold of\nfiltering toxic data. The threshold is 0.5 for FILT,\n0.4 for FILT-0.4 and0.35 for FILT-0.35. The per-\ncentage of toxic samples removed is 4.14% for\nFILT, 8.07% for FILT-0.4 and somewhere between\n8.07 and 14.94% for FILT-0.35. Note that we re-\nplenish the pretraining corpus with corresponding\npercentages of non-toxic samples. This is done to\nmaintain fairness of number of samples across the\nmodels. With this experiment we wanted to see if\niteratively replacing higher percentage of samples\nwith non-toxic samples helps in reducing toxicity.\nWe train a 357m parameter model on 96m samples\nfor FILT-0.4 and FILT-0.35 data strategies.\nResults in Fig. 9 illustrate that replacing higher\npercentage of toxic samples with non-toxic samples\nhelps but our proposed INST still performs the best.\nWe don’t experiment with lower values of threshold\nbecause it will be difficult to replenish the data with\nnon-toxic samples. Note that if samples were not\nreplaced and only filtered out then we would see a\ndrop in the utility of the LMs as more percentage\nof samples are removed (shown in detail in §6).\nMEDA Variations Similar to the INST varia-\ntions, we vary the PRMNONT in MEDA. For\nMEDA, PRMNONT= 0.5. We train MEDA-11\nwith PRMNONT= 0.11 and MEDA-90 with PRM-\nNONT= 0.9 for 357m-96m configuration.\nFig. 10 shows the resulting gain over the BASE.\nWe observe a different trend here compared to\nFig. 8. Here the best performing strategy is MEDA\nand increasing the percentage of non-toxic sam-\nples (MEDA-90) for which the model receives the\ncontrol variable Cnont does not improve the aver-\nage gain. This is because we have identified the\noptimal value of PRMNONT for MEDA and its\n2642\nFigure 9: Average gains achieved by FILT, FILT-0.4,\nFILT-0.35 and INST over BASE across the eleven tasks\non 357m-96m model configuration. We see that the\naverage performance of the model improves when we\nfilter more toxic samples and replace them with non-\ntoxic samples. We illustrate that INST strategy still\nperforms better than variations of filter strategy.\nvariations. We have not yet explored the optimal\nvalue of PRMNONT for INST.\nWe also experimented with using the raw toxi-\ncity scores from Perspective API directly without\nbinning them as in case of MEDA for 357m-96m\nconfiguration. Specifically, in case of MEDA all\nthe samples within a threshold get the same value,\neither 0.1 or 0.5. In this ablation (MEDA-R), the\nsamples within a threshold would get the raw scores\nlike 0.01 or 0.67 up to two decimal points. We ob-\nserve that MEDA-R does not reduce as much toxic-\nity compared to MEDA (toxicity probability: only\n7% reduction by MEDA-R compared to 36% by\nMEDA; xpected maximum toxicity: 5% reduction\nby MEDA-R as opposed to 22% by MEDA).\n5 Related Work\nFinetuning-based Methods Pretrained LMs can\nbe further finetuned using different training al-\ngorithms like domain-adaptive training meth-\nods (Gehman et al., 2020; Gururangan et al., 2020;\nSolaiman and Dennison, 2021; Wang et al., 2022)\nand reinforcement learning (Ouyang et al., 2022;\nPerez et al., 2022) on non-toxic data. These meth-\nods can only be employed after LMs are pretrained.\nThese methods typically incur further costs of fine-\ntuning additional LMs (Krause et al., 2021; Liu\net al., 2021), generating large amount of non-toxic\ndata (Wang et al., 2022), or procuring human feed-\nback (Ouyang et al., 2022). Our work on the other\nhand is targeted towards reducing toxicity by aug-\nmenting the pretraining corpus and hence will not\nincur additional cost after the LM is trained.\nDecoding Time Algorithms They reduce toxic-\nity of the generations at decoding time by altering\nthe probabilities of certain tokens. Gehman et al.\n(2020) show a study on using PPLM (Dathathri\net al., 2020), word-filtering, and vocabulary shift-\ning (Keskar et al., 2019). Schick et al. (2021) use\nthe internal knowledge of the LM to reduces the\nprobability of generating toxic text. The GeDi ap-\nproach (Krause et al., 2021) guides the generation\nat each step by computing classification probabili-\nties for all possible next tokens. Liu et al. (2021)\npropose DEXPERTS which controls the generation\nwith an“expert” LM trained on non-toxic data and\n“anti-expert” LM trained on toxic data. These tech-\nniques are efficient at reducing toxicity but fail to\nconsider the underlying semantic meaning of the\ngenerated text at the sequence level. They may\nalso reduce the utility of the LM at performing\ndownstream tasks (Wang et al., 2022).\nAnalysis of Toxicity in Pretraining DataLarge\nbody of work analyzes the pretraining data and ad-\nvocates for choosing it carefully (Gehman et al.,\n2020; Welbl et al., 2021; Bender et al., 2021).\nGehman et al. (2020) provide an analysis of toxicity\non a subset of pretraining data at a document-level.\nOur analysis (§2.1) of the entire pretraining corpus\nis at a sample-level and in agreement with Gehman\net al. (2020). An analysis by Sap et al. (2019) re-\nports that filtering data based on PerspectiveAPI\ncould lead to a decrease in text by African Amer-\nican authors. Our proposed approaches (MEDA\nand INST) don’t filter data. Additionally, Xu et al.\n(2021) present an analysis on different detoxifica-\ntion techniques like DAPT, PPLM, GeDi and filter-\ning (Gururangan et al., 2020; Dathathri et al., 2020;\nKrause et al., 2021). They conclude that these tech-\nniques hurt equity and decrease the utility of LMs\non language used by marginalized groups. These\nstudies necessitate tackling toxicity at the pretrain-\ning data stage without filtering.\nNgo et al. (2021) present experiments by filtering\ntoxic documents based on the loglikelihood of the\ntext. Our work augments pretraining data with\ntoxicity information.\n6 Comparison with Prior Work\nPrior work and this study uses different model con-\nfigurations in terms of model parameters, pretrain-\ning data, number of samples, and hyperparameters.\nWe show comparison with closest model configura-\ntion with our work. We only compare the relative\n2643\nchanges because the baselines are different for our\nwork and Wang et al. (2022); Liu et al. (2021);\nWelbl et al. (2021). Also, PerspectiveAPI (Perspec-\ntiveAPI, 2022) update their models regularly and\nhence scores returned by it may change over time.\nFinetuning-based Methods Wang et al. (2022)\ndevelop SGEAT which first generates large amount\nof non-toxic data using a pretrained LM (Smith\net al., 2022) and then uses domain adaptive finetun-\ning. They have the same model parameters and\npretraining data as our models. We follow the\nsame toxicity evaluation setup and similar setup\nfor benchmark NLP tasks. We compare 357m-\n150m INST model (Table 2) with the SGEAT357m\nmodel in Tables 1 and 3 of Wang et al. (2022). We\nsee that toxicity probability is relatively reduced\nby a massive 60% for INST and 38% for SGEAT;\naccuracy for benchmark NLP tasks show a relative\nimprovement of 0.9% for INST whereas SGEAT\ndecreases the NLP utility by 1.4%; perplexity is\nrelatively increased by 0.85% for INST and 9% for\nSGEAT. We would like to note that prior work (Gu-\nrurangan et al., 2020; Wang et al., 2022; Ouyang\net al., 2022) has not evaluated bias detection tasks\nand text generation task (E2E).\nDecoding Time Algorithms Due to similar\nmodel configuration with Wang et al. (2022), we\ncompare DEXPERTS (reported in Table 2) with re-\nsults on INST 1.3b-150m configuration (Table 3).\nWe observe that toxicity probability gets relatively\nreduced by 69.5% for DEXPERTS and 63.5% for\nINST; but accuracy of benchmark NLP tasks is\nsignificantly decreased by DEXPERTS (15%) and\nonly 1.3% by INST. Hence, even if decoding time\nalgorithms provide a higher decrease in toxicity,\nthey are not usable for general NLP tasks.\nFiltering Methods Prior work (Welbl et al.,\n2021; Ngo et al., 2021) removes entire documents\nwith toxicity above a threshold from the training set.\nFILT strategy replaces the toxic samples with equiv-\nalent number of non-toxic samples. To have a fair\ncomparison, we train two models: (1) a baseline\n357m parameter model (BASE-Doc), and (2) we\nfilter documents (2.5%) with toxicity score above\n0.5 and train a model (FILT-Doc) on the remain-\nder 97.5% of the documents. FILT-Doc reduces\nexpected maximum toxicity by 4.2% and toxicity\nprobability by 4.6% compared to BASE-Doc. This\ndemonstrates that FILT-Doc provides lesser rela-\ntive gains in toxicity reduction compared to FILT\nand INST (FILT gives 7.3% and INST provides\n28.1% reduction in expected maximum toxicity;\nFILT shows 16% and INST displays 59.7% reduc-\ntion in toxicity probability for 357m-150m in Ta-\nble 2). This shows that sample-level FILT is∼11%\nbetter than document-level FILT-Doc on toxicity\nprobability. We observe that FILT-Doc loses utility\non benchmark NLP tasks by 1% and loses bias de-\ntection capabilities by 8% compared to BASE-Doc.\nBased on the above comparisons, we conclude\nthat INST strategy developed in this work demon-\nstrates massive reduction in toxicity while preserv-\ning the utility of the LM on benchmark NLP tasks\nas well as bias detection tasks.\n7 Conclusion and Future Work\nWe develop two new strategies to reduce toxic-\nity using data augmentation - MEDA and INST.\nThrough extensive experiments, we demonstrate\nthat MEDA and INST reduce toxicity probability\nsubstantially (54% and 61% respectively) while\nnot compromising on the utility of the LM on five\nbenchmark NLP tasks and four bias detection tasks.\nWe also show that adding control variables does\nnot compromise performance on E2E task.\nIn this work, we show how toxicity can be re-\nduced in LMs by augmenting the pretraining data\nwith toxicity information. We believe that this idea\ncan be extended to other dimensions of social bi-\nases and hate speech. Prior work shows that adding\ninstructions during finetuning can help various NLP\ntasks and improve the LMs capabilities to gener-\nalize for instructions on unseen tasks (Wei et al.,\n2021; Ouyang et al., 2022). We postulate that these\nobservations can be applied to adding instructions\nto the pretraining data which can make INST gen-\neralizable to reduce different types of biases.\nThe key idea of adding relevant information to\nthe pretraining data via instructions can be applied\nmore broadly and opens new directions for future\nwork. Future work can focus on controlling the\ngeneration by adding general instructions to the\npretraining data. Current work has applied MEDA\nand INST on binary view of toxicity i.e something\nis toxic or non-toxic. Hence, another interesting\ndirection is to explore the degrees of toxicity and\nincorporate it with MEDA and INST strategies.\nFuture work can also evaluate the generalizability\nand applicability of INST strategy on more text\ngeneration tasks.\n2644\nLimitations\nThe current studies presented in this work rely\non PerspetiveAPI (PerspectiveAPI, 2022). Per-\nspectiveAPI scoring has been shown to be biased\nagainst marginalized communities (Gehman et al.,\n2020; Welbl et al., 2021; Xu et al., 2021). This can\nimpact the strategies developed in this work. But\nwe would like to note that MEDA and INST tech-\nniques can be used with any other classifier which\nprovides toxicity scores. Another limitation of this\nwork is that it requires a reliable classifier which\nprovides effective score of toxicity. If the classi-\nfier provides with inaccurate toxicity scores then it\nwould impact the performance of MEDA and INST.\nTo apply the strategies discussed in this work, we\nhave to label the whole pretraining dataset. This\nis true even for FILT strategy. Although not a lim-\nitation, this is an artifact of working on curating\npretraining dataset. We would also like to point out\nthat the control variables introduced in this work\ncan be used for both generating non-toxic content\nas well as toxic content. If we append sample with\nCtox control variable instead of Cnont then the LM\nwould generate toxic data. We would like to assert\nthat the intended use of this technique is to generate\ntext that is not toxic.\nReferences\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? . In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nCommonCrawl. 2022. Common crawl. https://\ncommoncrawl.org/. (Accessed on 10/18/2022).\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nInternational Conference on Learning Representa-\ntions.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nDirk Hovy and Shrimai Prabhumoye. 2021. Five\nsources of bias in natural language processing. Lan-\nguage and Linguistics Compass, 15(8):e12432.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2021. Gedi: Genera-\ntive discriminator guided sequence generation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 4929–4952.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A Smith,\nand Yejin Choi. 2021. Dexperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6691–6706.\nKris McGuffie and Alex Newhouse. 2020. The radical-\nization risks of gpt-3 and advanced neural language\nmodels. arXiv preprint arXiv:2009.06807.\nHelen Ngo, Cooper Raterink, João GM Araújo, Ivan\nZhang, Carol Chen, Adrien Morisot, and Nicholas\nFrosst. 2021. Mitigating harm in language models\nwith conditional-likelihood filtration. arXiv preprint\narXiv:2108.07790.\n2645\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nnli: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901.\nJekaterina Novikova, Ondˇrej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue ,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The lambada dataset: Word predic-\ntion requiring a broad discourse context. In Proceed-\nings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1525–1534.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai,\nRoman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. 2022. Red team-\ning language models with language models. arXiv\npreprint arXiv:2202.03286.\nPerspectiveAPI. 2022. Perspective | develop-\ners. https://developers.perspectiveapi.com/\ns/. (Accessed on 10/18/2022).\nShrimai Prabhumoye, Brendon Boldt, Ruslan Salakhut-\ndinov, and Alan W Black. 2021a. Case study: Deon-\ntological ethics in NLP. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3784–3798, Online.\nAssociation for Computational Linguistics.\nShrimai Prabhumoye, Rafal Kocielnik, Mohammad\nShoeybi, Anima Anandkumar, and Bryan Catan-\nzaro. 2021b. Few-shot instruction prompts for pre-\ntrained language models to detect social biases.arXiv\npreprint arXiv:2112.07868.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2020. Winogrande: An ad-\nversarial winograd schema challenge at scale. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 8732–8740.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1668–1678, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\nbias frames: Reasoning about social and power im-\nplications of language. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5477–5490, Online. Association\nfor Computational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in nlp. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nScikit-learn. 2022. Roc-auc-score. https://\nscikit-learn.org/stable/modules/generated/\nsklearn.metrics.roc_auc_score.html. (Ac-\ncessed on 04/13/2022).\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor adapting language models to society (palms) with\nvalues-targeted datasets. Advances in Neural Infor-\nmation Processing Systems, 34:5861–5873.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nBoxin Wang, Wei Ping, Chaowei Xiao, Peng Xu,\nMostofa Patwary, Mohammad Shoeybi, Bo Li, An-\nima Anandkumar, and Bryan Catanzaro. 2022. Ex-\nploring the limits of domain-adaptive training for\ndetoxifying large-scale language models. arXiv\npreprint arXiv:2202.04173.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato,\nSumanth Dathathri, John Mellor, Lisa Anne Hen-\ndricks, Kirsty Anderson, Pushmeet Kohli, Ben Cop-\npin, and Po-Sen Huang. 2021. Challenges in detox-\nifying language models. In Findings of the Associ-\n2646\nation for Computational Linguistics: EMNLP 2021,\npages 2447–2469.\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Guru-\nrangan, Maarten Sap, and Dan Klein. 2021. Detoxi-\nfying language models risks marginalizing minority\nvoices. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2390–2397.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019a. Hellaswag: Can\na machine really finish your sentence? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4791–4800.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019b. Defending against neural fake\nnews. Advances in neural information processing\nsystems, 32.\nA Analysis of PerspectiveAPI scores\nDoc. Id #chars Doc. Score 2k chars 5k chars\n1 3198 0.0816 0.1052 0.0816\n2 7053 0.0778 0.0996 0.0827\n3 4337 0.0806 0.0731 0.0806\n4 3575 0.1293 0.1275 0.1293\n5 2168 0.0763 0.0767 0.0763\n6 9820 0.1395 0.1801 0.2051\n7 9917 0.0851 0.2052 0.2428\n8 3971 0.2400 0.2612 0.2400\n9 9644 0.2586 0.3880 0.2843\n10 6964 0.2208 0.3644 0.3546\nTable 1: PerspectiveAPI scores of documents using\nthree different ways. #chars denotes the number of char-\nacters in a document. Doc. Score is the PerspectiveAPI\ntoxicity score when the entire document is passed. 2k\nchars displays the average PerspectiveAPI toxicity score\nwhen the document is split into chunks of 2k chars and\n5k chars displays the average PerspectiveAPI toxicity\nscore when the document is split into chunks of5k chars.\nDoc. Id denotes the id of the document.\nPerspectiveAPI accepts maximum text size per\nrequest of 20 KB. This is approximately 20k char-\nacters. We select 10 documents with less than\n10k characters for the purpose of our analysis.\nThis analysis aims to study the difference between\nPerspectiveAPI toxicity scores when we pass the\nwhole document vs chunking the document and\nthen averaging the scores for each chunk. We ob-\ntain PerspectiveAPI toxicity score in three ways:\n(1) we pass the whole document and get the Per-\nspectiveAPI toxicity score (denoted as “Doc. Score”\nin Table 1), (2) we split the document into chunks\nof 2000 characters and then take the weighted av-\nerage of PerspectiveAPI toxicty scores for all the\nchunks (denoted as “2k chars” in Table 1), and (3)\nwe split the document into chunks of 5000 charac-\nters and then take the weighted average of Perspec-\ntiveAPI toxicity scores for all the chunks (denoted\nas “5k chars” in Table 1).\nTable 1 shows the result of this analysis. We ob-\nserve that all the three types of scores are different.\nMore importantly the ranking between the docu-\nments changes if we consider each of the three\napproaches. For example if we rank the docu-\nment ids from lowest score to highest toxicity score\nthen the ranking according to the approaches are:\n(1) Doc. Score is 5, 2, 3, 1, 7, 4, 6, 10, 8, 9 (2) 2k\nchars is 3, 5, 2, 1, 4, 6, 7, 8, 10, 9 and (3) 5k chars\nis 5, 3, 1, 2, 4, 6, 8, 7, 9, 10. This study shows that\ndocument longer than 20k characters cannot be\nsplit into multiple chunks to obtain an average Per-\n2647\nModel 96m-samples 150m-samples\nEMT TP NLP BD E2E EMT TP NLP BD E2E\nBASE 0.44 0.36 47.5 50.6 27.6 0.43 0.35 48.2 50.0 30.8\nFILT 0.40 0.29 48.0 51.2 27.4 0.40 0.30 48.5 52.2 30.0\n↓8.1% ↓18.5% ↑1.1% ↑1.2% ↓0.8% ↓7.3% ↓16.0% ↑0.6% ↑4.4% ↓2.8%\nMEDA 0.41 0.31 48.1 50.1 28.5 0.41 0.31 48.2 49.5 30.7\n↓5.9% ↓13.2% ↑1.4% ↓1.0% ↑3.2% ↓4.8% ↓11.0% ↑0.0% ↓1.0% ↓0.7%\nINST 0.42 0.33 47.9 50.2 28.9 0.42 0.33 48.7 51.1 29.7\n↓2.8% ↓6.8% ↑0.8% ↓0.9% ↑4.9% ↓1.9% ↓5.4% ↑0.9% ↑2.3% ↓3.7%\nExperiment using control variableCnont\nMEDA 0.33 0.18 - - 28.3 0.33 0.17 - - 30.7\n↓24.0% ↓49.8% ↑2.6% ↓23.9% ↓51.2% ↓0.5%\nINST 0.31 0.15 - - 29.8 0.31 0.14 - - 29.7\n↓29.0% ↓59.3% ↑7.8% ↓28.1% ↓59.7% ↓3.5%\nTable 2: Results for 357m parameter models on all the metrics. EMT is Expected Maximum Toxicity; TP is Toxicity\nProbability; NLP indicates the average of accuracy on five benchmark NLP tasks; BD displays the average AUC on\nfour bias detection tasks; and E2E shows the Rouge-L scores of the LMs on the E2E task. For benchmark NLP\ntasks, bias detection tasks and E2E task we show the relative percentage improvement over BASE with a ↑% and\ndecrement with a ↓% . For the expected maximum toxicity and toxicity probability, we show the improvement with\n↓% because lower is better for these metrics. We may observe that two strategies obtain the exact same score but\nthere is a difference in their relative percentages. This is because these scores are computed up to 4 decimal digits\nbut we only report scores up to 2 decimals here.\nspectiveAPI score.\nMore importantly, even for documents which are\nless than 20k characters, it is not guaranteed that\nthe entire sequence will appear together in a sample\nduring the data preprocessing phase. Hence, first\nobtaining PerspectiveAPI score and then splitting\nthe documents into samples of sequence length\n2000 tokens would yield inaccurate toxicity scores\nfor the samples. Hence, our approach is focused on\nsample-level toxicity scoring for providing the LM\nwith precise toxicity information. This impacts our\nMEDA and INST strategies which rely on guiding\nthe LM at sample-level about toxicity information.\nB Details of Main Results\nTable 2 and 3 show the results for the eleven tasks\nwith and with the control variable Cnont for 357m\nand 1.3b parameter models respectively. EMT is\nExpected Maximum Toxicity; TP is Toxicity Prob-\nability; NLP indicates the average of accuracy on\nfive benchmark NLP tasks; BD displays the aver-\nage AUC score on four bias detection tasks; and\nE2E shows the Rouge-L scores of the LMs on the\nE2E task. For benchmark NLP tasks, bias detection\ntasks and E2E task we show the relative percentage\nimprovement over BASE with a ↑% and decrement\nwith a ↓% . For the expected maximum toxicity and\ntoxicity probability, we show the improvement with\n↓% because lower is better for these metrics. In Ta-\nbles 2 and 3, we may observe that two strategies\nobtain the exact same score but there is a difference\nin their relative percentages. This is because these\nscores are computed up to 4 decimal digits but we\nonly report scores up to 2 decimals here.\nWe calculate the relative percentage difference\ncompared to BASE for all the twelve metrics across\nthe eleven tasks - expected maximum toxicity, toxi-\ncity probability, accuracy of five NLP tasks, AUC\nscores of four bias detection tasks, and Rouge-L\nfor E2E task. We then compute an average across\nall the metrics (we also include the experiments\nwith control variable Cnont). These aggregated re-\nsults are shown in Fig. 4. Fig. 4 shows the average\npercentage gains achieved by each strategy across\nthe eleven tasks.\nC Hyper-parameter Details\nAll the LMs trained in this work are GPT-\nstyle (Brown et al., 2020) Transformer architec-\ntures (Vaswani et al., 2017) trained with Megatron\ntoolkit (Shoeybi et al., 2019). We use BPE tok-\nenization with a vocabulary of size 50256. All the\nmodels are trained on sequence length of 2048 to-\nkens. Note that our samples are of size2000 tokens.\nWe leave48 tokens for adding either raw scores for\nMEDA or instructions for INST. Note that the base-\nline is also trained on same samples of2000 tokens.\nWe pad the extra spaces with a PAD_TOKEN.\n2648\nModel 96m-samples 150m-samples\nEMT TP NLP BD E2E EMT TP NLP BD E2E\nBASE 0.44 0.37 52.6 53.0 30.7 0.44 0.37 54.4 53.8 31.1\nFILT 0.40 0.30 52.9 55.9 29.9 0.41 0.31 54.5 53.2 31.9\n↓8.5% ↓18.8% ↑0.6% ↑5.5% ↓2.5% ↓7.4% ↓16.5% ↑0.2% ↓1.1% ↑2.6%\nMEDA 0.42 0.33 53.0 57.2 31.8 0.42 0.34 53.9 53.5 33.0\n↓4.7% ↓10.7% ↑0.8% ↑7.9% ↑3.7% ↓4.2% ↓9.1% ↓0.9% ↓0.6% ↑6.1%\nINST 0.43 0.34 53.3 53.9 30.6 0.42 0.34 53.7 54.9 31.7\n↓3.6% ↓9.7% ↑1.3% ↑1.7% ↓0.2% ↓3.7% ↓9.2% ↓1.3% ↑2.0% ↑2.1%\nExperiment using control variableCnont\nMEDA 0.32 0.16 - - 31.9 0.32 0.16 - - 33.6\n↓27.5% ↓58.1% ↑4.2% ↓26.8% ↓57.4% ↑8.2%\nINST 0.31 0.15 - - 31.3 0.31 0.14 - - 32.6\n↓30.2% ↓62.7% ↑2.1% ↓30.0% ↓63.5% ↑4.9%\nTable 3: Results for 1.3b parameter models. EMT is Expected Maximum Toxicity; TP is Toxicity Probability;\nNLP indicates the average of accuracy on five benchmark NLP tasks; BD displays the average AUC on four bias\ndetection tasks; and E2E shows the Rouge-L scores of the LMs on the E2E task. For benchmark NLP tasks, bias\ndetection tasks and E2E task we show the relative percentage improvement over BASE with a ↑% and decrement\nwith a ↓% . For the expected maximum toxicity and toxicity probability, we show the improvement with ↓% because\nlower is better for these metrics. We may observe that two strategies obtain the exact same score but there is a\ndifference in their relative percentages. This is because these scores are computed up to 4 decimal digits but we\nonly report scores up to 2 decimals here.\n357m parameter models We train them with24\nlayers, with a hidden size of 1024 and 16 attention\nheads. We use max −position −embeddings\nof 2048; 162761 warmup samples; a learning rate\nof 3.0e−4 with minimum learning rate of 3.0e−5.\nWe use cosine learning decay style. Additionally,\nwe use clip-grad = 1.0, weight-decay = 0.1, adam-\nbeta1 = 0.9, and adam-beta2 = 0.95. Each of these\nmodels are trained on 64 A100 GPUs with 40GB\nmemory. The models with 96m samples are trained\nfor 54 GPU hours and models with 150m samples\nare trained for 84 GPU hours.\n1.3b parameter models We train them with 24\nlayers, with a hidden size of 2048 and 32 attention\nheads. We use max-position-embeddings = 2048\nwith 244141 warmup samples; a learning rate of\n2.0e−4 with a minumum learning rate of 2.0e−5\nand cosine decay style. We use clip-grad = 1.0,\nweight-decay = 0.1, adam-beta1 = 0.9, and adam-\nbeta2 = 0.95. Each of these models are trained on\n64 A100 GPUs with 40GB memory. The models\nwith 96m samples are trained for 113 GPU hours\nand models with 150m samples are trained for 176\nGPU hours.\nBounds for the new variablesWe describe the\nbounds for LOWTHRESH , HIGH THRESH , PRM-\nTOX and PRMNONT variables introduced in this\nwork.\n0 < LOWTHRESH < 1\n0 < HIGH THRESH < 1\n0 ≤PRMTOX ≤1\n0 ≤PRMNONT ≤1\nNote that PRMTOX = 0 means that no samples\nabove the HIGH THRESH are augmented with Ctox;\nand PRMTOX = 1 means that all the samples above\nthe HIGH THRESH are augmented with Ctox. Simi-\nlarly, PRMNONT = 0 implies that no samples below\nLOWTHRESH are modified; and PRMNONT = 1\nmeans that all the samples below LOWTHRESH are\naugmented with Cnont.\nNumber of Shots for TasksTable 4 shows the\nnumber of shots used as context for each task fol-\nlowing the setups in Brown et al. (2020); Smith\net al. (2022); Prabhumoye et al. (2021b).\nTask # of Shots\nLAMBADA (Paperno et al., 2016) 15\nANLI (Nie et al., 2020) 50\nWinogrande (Sakaguchi et al., 2020) 50\nPiQA (Bisk et al., 2020) 50\nHellaswag (Zellers et al., 2019a) 20\nBias Detection (Prabhumoye et al., 2021b) 32\nTable 4: Number of shots used as context for each task.\n2649\nModel 150m-samples\nEMT TP NLP BD\nBASE 0.43 0.35 64.9 54.7\nFILT 0.39 0.28 64.8 61.0\n↓8.0% ↓19.6% ↓0.1% ↑11.5%\nINST 0.41 0.30 65.3 61.7\n↓4.5% ↓13.4% ↑0.7% ↑12.7%\nExperiment using control variableCnont\nINST 0.28 0.11 - -\n↓34.0% ↓69.5%\nTable 5: Results for 8.3b parameter models trained with\n150 million samples.\nModel 96m-samples\nEMT TP NLP BD E2E\nBASE 0.44 0.36 47.5 50.6 27.6\nFILT 0.40 0.29 48.0 51.2 27.4\n↓8.1% ↓18.5% ↑1.1% ↑1.2% ↓0.8%\nFILT-0.4 0.38 0.25 47.4 50.0 28.8\n↓13.1% ↓30.8% ↓0.3% ↓1.1% ↑4.3%\nFILT-0.35 0.37 0.23 48.0 50.4 28.4\n↓15.1% ↓36.8% ↑1.1% ↓0.4% ↑2.9%\nINST 0.42 0.33 47.9 50.2 28.9\n↓2.8% ↓6.8% ↑0.8% ↓0.9% ↑4.9%\nExperiment using control variableCnont\nINST 0.31 0.15 - - 29.8\n↓29.0% ↓59.3% ↑7.8%\nTable 6: Results for 357m-96m configuration on all\nthe metrics for variations of FILT such as FILT-0.4 and\nFILT-0.35 in comparison with INST.\nD Ablation Experiments\nScaling the Model Size Table 5 shows results\nfor 8.3 billion parameter models which use BASE,\nFILT and INST strategies on 357m-150m model\nconfiguration.\nFILT Variations Table 6 shows the results for\nBASE, FILT, FILT-0.4, FILT-0.35 and INST for all\nthe 357m-96m model configuration on all eleven\ntasks. These results are aggregated and presented\nin Fig. 9.\nMEDA Variations Table 7 shows the results for\nBASE, MEDA-11, MEDA, MEDA-90 and INST\nfor 357m-96m model configuration on all eleven\ntasks. These results are aggregated and presented\nin Fig. 10.\nINST Variations Table 8 shows the results for\nBASE, INST-11, INST-50 and INST for all the\nfour model configuration on all eleven tasks. These\nresults are aggregated and presented in Fig. 8.\nModel 96m-samples\nEMT TP NLP BD E2E\nBASE 0.44 0.36 47.5 50.6 27.6\nMEDA-11 0.42 0.33 47.1 52.6 28.4\n↓4.6% ↓8.8% ↓0.8% ↑4.1% ↑3.0%\nMEDA 0.41 0.31 48.1 50.1 28.5\n↓5.9% ↓13.2% ↑1.4% ↓1.0% ↑3.2%\nMEDA-90 0.42 0.33 47.0 47.6 28.6\n↓2.9% ↓8.2% ↓1.1% ↓6% ↑3.8%\nINST 0.42 0.33 47.9 50.2 28.9\n↓2.8% ↓6.8% ↑0.8% ↓0.9% ↑4.9%\nExperiment using control variableCnont\nMEDA-11 0.35 0.20 - - 28.4\n↓20.7% ↓43.5% ↑3.0%\nMEDA 0.33 0.18 - - 28.3\n↓24.0% ↓49.8% ↑2.6%\nMEDA-90 0.31 0.14 - - 28.4\n↓28.6% ↓59.9% ↑3.0%\nINST 0.31 0.15 - - 29.8\n↓29.0% ↓59.3% ↑7.8%\nTable 7: Results for 357m-96m configuration on all\nthe metrics for MEDA and INST in comparison with\nMEDA-11 and MEDA-90.\nFigure 10: Average the gains achieved by MEDA-11,\nMEDA, MEDA-90, and INST over BASE across the\neleven tasks for 357m-96m model configuration.\n2650\nModel 96m-samples 150m-samples\nEMT TP NLP BD E2E EMT TP NLP BD E2E\nExperiments with357m parameter models\nBASE 0.44 0.36 47.5 50.6 27.6 0.43 0.35 48.2 50.0 30.8\nINST-11 0.41 0.32 46.6 50.9 28.1 0.42 0.34 48.7 50.7 28.7\n↓5.9% ↓11.8% ↓1.8% ↑0.5% ↑1.9% ↓2.0% ↓3.9% ↑1.0% ↑1.4% ↓6.8%\nINST-50 0.41 0.32 47.9 49.9 28.2 0.42 0.33 48.3 49.1 29.5\n↓5.6% ↓11.9% ↑0.9% ↓1.5% ↑2.3% ↓3.0% ↓6.8% ↑0.2% ↓1.8% ↓4.3%\nINST 0.42 0.33 47.9 50.2 28.9 0.42 0.33 48.7 51.1 29.7\n↓2.8% ↓6.8% ↑0.8% ↓0.9% ↑4.9% ↓1.9% ↓5.4% ↑0.9% ↑2.3% ↓3.7%\nExperiment using control variableCnont for 357m parameter models\nINST-11 0.36 0.23 - - 28.3 0.38 0.25 - - 29.0\n↓18.5% ↓36.8% ↑2.7% ↓13.0% ↓27.4% ↓6.0%\nINST-50 0.32 0.16 - - 28.0 0.34 0.18 - - 29.5\n↓26.7% ↓54.6% ↑1.5% ↓22.3% ↓48.2% ↓4.4%\nINST 0.31 0.15 - - 29.8 0.31 0.14 - - 29.7\n↓29.0% ↓59.3% ↑7.8% ↓28.1% ↓59.7% ↓3.5%\nExperiments with1.3b parameter model\nBASE 0.44 0.37 52.6 53.0 30.7 0.44 0.37 54.4 53.8 31.1\nINST-11 0.41 0.32 52.9 54.1 33.5 0.42 0.33 54.3 54.0 31.0\n↓6.3% ↓13.3% ↑0.5% ↑2.2% ↑9.1% ↓4.9% ↓11.0% ↓0.1% ↑0.3% ↓0.3%\nINST-50 0.41 0.32 53.4 54.5 30.8 0.42 0.34 53.9 54.6 32.6\n↓6.3% ↓14.3% ↑1.4% ↑2.9% ↑0.4% ↓3.7% ↓8.4% ↓0.8% ↑1.4% ↑4.9%\nINST 0.43 0.34 53.3 53.9 30.6 0.42 0.34 53.7 54.9 31.7\n↓3.6% ↓9.7% ↑1.3% ↑1.7% ↓0.2% ↓3.7% ↓9.2% ↓1.3% ↑2.0% ↑2.1%\nExperiment using control variableCnont for 1.3b parameter models\nINST-11 0.32 0.15 - - 34.2 0.33 0.18 - - 32.1\n↓28.3% ↓59.6% ↑11.7% ↓24.3% ↓51.4% ↑3.3%\nINST-50 0.31 0.14 - - 30.9 0.32 0.15 - - 32.9\n↓29.6% ↓61.4% ↑0.7% ↓27.3% ↓58.4% ↑6.0%\nINST 0.31 0.15 - - 31.3 0.31 0.14 - - 32.6\n↓30.2% ↓62.7% ↑2.1% ↓30.0% ↓63.5% ↑4.9%\nTable 8: Results for 357m and 1.3b parameter models on all the metrics for INST and its variations INST-11 and\nINST-50. For benchmark NLP tasks, bias detection tasks and E2E task we show the relative percentage improvement\nover BASE with a ↑% and decrement with a ↓% . For the expected maximum toxicity and toxicity probability, we\nshow the improvement with ↓% because lower is better for these metrics.\n2651",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.8595342636108398
    },
    {
      "name": "Computer science",
      "score": 0.7571549415588379
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6689991354942322
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6302930116653442
    },
    {
      "name": "Machine learning",
      "score": 0.5530039072036743
    },
    {
      "name": "Natural language processing",
      "score": 0.5197281837463379
    },
    {
      "name": "Language model",
      "score": 0.4103958010673523
    },
    {
      "name": "Statistics",
      "score": 0.16473442316055298
    },
    {
      "name": "Mathematics",
      "score": 0.07451751828193665
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}