{
  "title": "Learning to Discover, Ground and Use Words with Segmental Neural Language Models",
  "url": "https://openalex.org/W2952125979",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2108388156",
      "name": "Kazuya Kawakami",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2107310219",
      "name": "Chris Dyer",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A297118547",
      "name": "Phil Blunsom",
      "affiliations": [
        "University of Oxford",
        "DeepMind (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2074546930",
    "https://openalex.org/W2116211107",
    "https://openalex.org/W2404744763",
    "https://openalex.org/W2415378728",
    "https://openalex.org/W2036682528",
    "https://openalex.org/W2105738468",
    "https://openalex.org/W4297826211",
    "https://openalex.org/W2963899393",
    "https://openalex.org/W2531381952",
    "https://openalex.org/W2963357986",
    "https://openalex.org/W2126449874",
    "https://openalex.org/W2252172689",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2076618452",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2950403373",
    "https://openalex.org/W2964325845",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2251536071",
    "https://openalex.org/W2828202920",
    "https://openalex.org/W2170240579",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2592647456",
    "https://openalex.org/W1980862600",
    "https://openalex.org/W2096204319",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2586148577",
    "https://openalex.org/W2103149536",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2008225289",
    "https://openalex.org/W1993755070",
    "https://openalex.org/W2140991203",
    "https://openalex.org/W2952288254",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2609370997",
    "https://openalex.org/W2530486890",
    "https://openalex.org/W2161952424",
    "https://openalex.org/W2328091329",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2891546148",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2158266063",
    "https://openalex.org/W2126377586",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963077280",
    "https://openalex.org/W2029948425",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2094249282",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2952343510",
    "https://openalex.org/W1909733559"
  ],
  "abstract": "We propose a segmental neural language model that combines the generalization power of neural networks with the ability to discover word-like units that are latent in unsegmented character sequences. In contrast to previous segmentation models that treat word segmentation as an isolated task, our model unifies word discovery, learning how words fit together to form sentences, and, by conditioning the model on visual context, how words’ meanings ground in representations of nonlinguistic modalities. Experiments show that the unconditional model learns predictive distributions better than character LSTM models, discovers words competitively with nonparametric Bayesian word segmentation models, and that modeling language conditional on visual context improves performance on both.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6429–6441\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n6429\nLearning to Discover, Ground and Use Words\nwith Segmental Neural Language Models\nKazuya Kawakami♠♣ Chris Dyer♣ Phil Blunsom♠♣\n♠Department of Computer Science, University of Oxford, Oxford, UK\n♣DeepMind, London, UK\n{kawakamik,cdyer,pblunsom}@google.com\nAbstract\nWe propose a segmental neural language\nmodel that combines the generalization power\nof neural networks with the ability to discover\nword-like units that are latent in unsegmented\ncharacter sequences. In contrast to previous\nsegmentation models that treat word segmen-\ntation as an isolated task, our model uniﬁes\nword discovery, learning how words ﬁt to-\ngether to form sentences, and, by condition-\ning the model on visual context, how words’\nmeanings ground in representations of non-\nlinguistic modalities. Experiments show that\nthe unconditional model learns predictive dis-\ntributions better than character LSTM models,\ndiscovers words competitively with nonpara-\nmetric Bayesian word segmentation models,\nand that modeling language conditional on vi-\nsual context improves performance on both.\n1 Introduction\nHow infants discover words that make up their ﬁrst\nlanguage is a long-standing question in develop-\nmental psychology (Saffran et al., 1996). Machine\nlearning has contributed much to this discussion\nby showing that predictive models of language are\ncapable of inferring the existence of word bound-\naries solely based on statistical properties of the\ninput (Elman, 1990; Brent and Cartwright, 1996;\nGoldwater et al., 2009). However, there are two se-\nrious limitations of current models of word learning\nin the context of the broader problem of language\nacquisition. First, language acquisition involves not\nonly learning what words there are (“the lexicon”),\nbut also how they ﬁt together (“the grammar”). Un-\nfortunately, the best language models, measured\nin terms of their ability to predict language (i.e.,\nthose which seem acquire grammar best), segment\nquite poorly (Chung et al., 2017; Wang et al., 2017;\nKádár et al., 2018), while the strongest models\nin terms of word segmentation (Goldwater et al.,\n2009; Berg-Kirkpatrick et al., 2010) do not ade-\nquately account for the long-range dependencies\nthat are manifest in language and that are easily\ncaptured by recurrent neural networks (Mikolov\net al., 2010). Second, word learning involves not\nonly discovering what words exist and how they ﬁt\ntogether grammatically, but also determining their\nnon-linguistic referents, that is, their grounding.\nThe work that has looked at modeling acquisition\nof grounded language from character sequences—\nusually in the context of linking words to a visu-\nally experienced environment—has either explic-\nitly avoided modeling word units (Gelderloos and\nChrupała, 2016) or relied on high-level represen-\ntations of visual context that overly simplify the\nrichness and ambiguity of the visual signal (John-\nson et al., 2010; Räsänen and Rasilo, 2015).\nIn this paper, we introduce a single model that\ndiscovers words, learns how they ﬁt together (not\njust locally, but across a complete sentence), and\ngrounds them in learned representations of natu-\nralistic non-linguistic visual contexts. We argue\nthat such a uniﬁed model is preferable to a pipeline\nmodel of language acquisition (e.g., a model where\nwords are learned by one character-aware model,\nand then a full-sentence grammar is acquired by a\nsecond language model using the words predicted\nby the ﬁrst). Our preference for the uniﬁed model\nmay be expressed in terms of basic notions of sim-\nplicity (we require one model rather than two), and\nin terms of the Continuity Hypothesis of Pinker\n(1984), which argues that we should assume, ab-\nsent strong evidence to the contrary, that children\nhave the same cognitive systems as adults, and dif-\nferences are due to them having set their parameters\ndifferently/immaturely.\nIn §2 we introduce a neural model of sentences\nthat explicitly discovers and models word-like units\nfrom completely unsegmented sequences of char-\nacters. Since it is a model of complete sentences\n6430\n(rather than just a word discovery model), and it\ncan incorporate multimodal conditioning context\n(rather than just modeling language uncondition-\nally), it avoids the two continuity problems identi-\nﬁed above. Our model operates by generating text\nas a sequence of segments, where each segment\nis generated either character-by-character from a\nsequence model or as a single draw from a lexical\nmemory of multi-character units. The segmenta-\ntion decisions and decisions about how to generate\nwords are not observed in the training data and\nmarginalized during learning using a dynamic pro-\ngramming algorithm (§3).\nOur model depends crucially on two components.\nThe ﬁrst is, as mentioned, a lexical memory. This\nlexicon stores pairs of a vector (key) and a string\n(value) the strings in the lexicon are contiguous\nsequences of characters encountered in the training\ndata; and the vectors are randomly initialized and\nlearned during training. The second component\nis a regularizer (§4) that prevents the model from\noverﬁtting to the training data by overusing the\nlexicon to account for the training data.1\nOur evaluation (§5–§7) looks at both language\nmodeling performance and the quality of the\ninduced segmentations, in both unconditional\n(sequence-only) contexts and when conditioning\non a related image. First, we look at the seg-\nmentations induced by our model. We ﬁnd that\nthese correspond closely to human intuitions about\nword segments, competitive with the best exist-\ning models for unsupervised word discovery. Im-\nportantly, these segments are obtained in models\nwhose hyperparameters are tuned to optimize val-\nidation (held-out) likelihood, whereas tuning the\nhyperparameters of our benchmark models using\nheld-out likelihood produces poor segmentations.\nSecond, we conﬁrm ﬁndings (Kawakami et al.,\n2017; Mielke and Eisner, 2018) that show that\nword segmentation information leads to better lan-\nguage models compared to pure character models.\nHowever, in contrast to previous work, we realize\nthis performance improvement without having to\nobserve the segment boundaries. Thus, our model\nmay be applied straightforwardly to Chinese, where\nword boundaries are not part of the orthography.\n1Since the lexical memory stores strings that appear in the\ntraining data, each sentence could, in principle, be generated\nas a single lexical unit, thus the model could ﬁt the training\ndata perfectly while generalizing poorly. The regularizer pe-\nnalizes based on the expectation of the powered length of\neach segment, preventing this degenerate solution from being\noptimal.\nAblation studies demonstrate that both the lexi-\ncon and the regularizer are crucial for good per-\nformance, particularly in word segmentation—\nremoving either or both signiﬁcantly harms per-\nformance. In a ﬁnal experiment, we learn to model\nlanguage that describes images, and we ﬁnd that\nconditioning on visual context improves segmen-\ntation performance in our model (compared to the\nperformance when the model does not have access\nto the image). On the other hand, in a baseline\nmodel that predicts boundaries based on entropy\nspikes in a character-LSTM, making the image\navailable to the model has no impact on the quality\nof the induced segments, demonstrating again the\nvalue of explicitly including a word lexicon in the\nlanguage model.\n2 Model\nWe now describe the segmental neural language\nmodel (SNLM). Refer to Figure 1 for an illustration.\nThe SNLM generates a character sequence x =\nx1,...,x n, where each xi is a character in a ﬁnite\ncharacter set Σ. Each sequence x is the concatena-\ntion of a sequence of segments s = s1,..., s|s|\nwhere |s| ≤n measures the length of the se-\nquence in segments and each segment si ∈Σ+\nis a sequence of characters, si,1,...,s i,|si|. In-\ntuitively, each si corresponds to one word. Let\nπ(s1,..., si) represent the concatenation of the\ncharacters of the segments s1 to si, discarding seg-\nmentation information; thus x = π(s). For exam-\nple if x = anapple, the underlying segmentation\nmight be s = an apple (with s1 = an and\ns2 = apple), or s = a nap ple, or any of the\n2|x|−1 segmentation possibilities for x.\nThe SNLM deﬁnes the distribution over x as the\nmarginal distribution over all segmentations that\ngive rise to x, i.e.,\np(x) =\n∑\ns:π(s)=x\np(s). (1)\nTo deﬁne the probability of p(s), we use the chain\nrule, rewriting this in terms of a product of the\nseries of conditional probabilities, p(st |s<t). The\nprocess stops when a special end-sequence segment\n⟨/S⟩is generated. To ensure that the summation in\nEq. 1 is tractable, we assume the following:\np(st |s<t) ≈p(st |π(s<t)) = p(st |x<t), (2)\nwhich amounts to a conditional semi-Markov\nassumption—i.e., non-Markovian generation hap-\n6431\nnC a uy o l o ko ta\n…\no\nl\no\no\nk\no\n</w> \nk\nl\n<w> l \nl o \nl o o k …\nap app appl apple lo loo look \nl o o k \n…\nlooks looke looked \nFigure 1: Fragment of the segmental neural language model while evaluating the marginal likelihood of a sequence.\nAt the indicated time, the model has generated the sequence Canyou, and four possible continuations are shown.\npens inside each segment, but the segment genera-\ntion probability does not depend on memory of the\nprevious segmentation decisions, only upon the se-\nquence of characters π(s<t) corresponding to the\npreﬁx character sequence x<t. This assumption\nhas been employed in a number of related models\nto permit the use of LSTMs to represent rich his-\ntory while retaining the convenience of dynamic\nprogramming inference algorithms (Wang et al.,\n2017; Ling et al., 2017; Graves, 2012).\n2.1 Segment generation\nWe model p(st |x<t) as a mixture of two models,\none that generates the segment using a sequence\nmodel and the other that generates multi-character\nsequences as a single event. Both are conditional\non a common representation of the history, as is the\nmixture proportion.\nRepresenting history To represent x<t, we use\nan LSTM encoder to read the sequence of charac-\nters, where each character typeσ∈Σ has a learned\nvector embedding vσ. Thus the history represen-\ntation at time t is ht = LSTM enc(vx1 ,..., vxt ).\nThis corresponds to the standard history representa-\ntion for a character-level language model, although\nin general, we assume that our modelled data is not\ndelimited by whitespace.\nCharacter-by-character generation The ﬁrst\ncomponent model, pchar(st |ht), generates st by\nsampling a sequence of characters from a LSTM\nlanguage model over Σ and a two extra special\nsymbols, an end-of-word symbol ⟨/W⟩ /∈Σ and\nthe end-of-sequence symbol ⟨/S⟩discussed above.\nThe initial state of the LSTM is a learned trans-\nformation of ht, the initial cell is 0, and different\nparameters than the history encoding LSTM are\nused. During generation, each letter that is sam-\npled (i.e., each st,i) is fed back into the LSTM in\nthe usual way and the probability of the charac-\nter sequence decomposes according to the chain\nrule. The end-of-sequence symbol can never be\ngenerated in the initial position.\nLexical generation The second component\nmodel, plex(st |ht), samples full segments from\nlexical memory. Lexical memory is a key-value\nmemory containing M entries, where each key, ki,\na vector, is associated with a value vi ∈Σ+. The\ngeneration probability of st is deﬁned as\nh′\nt = MLP(ht)\nm = softmax(Kh′\nt + b)\nplex(st |ht) =\nM∑\ni=1\nmi[vi = st],\nwhere [vi = st] is 1 if the ith value in memory is\nst and 0 otherwise, and K is a matrix obtained by\nstacking the k⊤\ni ’s. This generation process assigns\nzero probability to most strings, but the alternate\ncharacter model can generate all of Σ+.\nIn this work, we ﬁx the vi’s to be subsequences\nof at least length 2, and up to a maximum length\nLthat are observed at least F times in the training\ndata. These values are tuned as hyperparameters\n(See Appendix C for details of the experiments).\nMixture proportion The mixture proportion, gt,\ndetermines how likely the character generator is to\n6432\nbe used at time t(the lexicon is used with probabil-\nity 1 −gt). It is deﬁned by as gt = σ(MLP(ht)).\nTotal segment probability The total generation\nprobability of st is thus\np(st |x<t) = gtpchar(st |ht)+\n(1 −gt)plex(st |ht).\n3 Inference\nWe are interested in two inference questions: ﬁrst,\ngiven a sequence x, evaluate its (log) marginal\nlikelihood; second, given x, ﬁnd the most likely\ndecomposition into segments s∗.\nMarginal likelihood To efﬁciently compute the\nmarginal likelihood, we use a variant of the forward\nalgorithm for semi-Markov models (Yu, 2010),\nwhich incrementally computes a sequence of prob-\nabilities, αi, where αi is the marginal likelihood of\ngenerating x≤i and concluding a segment at time\ni. Although there are an exponential number of\nsegmentations of x, these values can be computed\nusing O(|x|) space and O(|x|2) time as:\nα0 = 1, α t =\nt−1∑\nj=t−L\nαjp(s = xj:t |x<j).\n(3)\nBy letting xt+1 = ⟨/S⟩, then p(x) = αt+1.\nMost probable segmentation The most proba-\nble segmentation of a sequence x can be computed\nby replacing the summation with a max operator\nin Eq. 3 and maintaining backpointers.\n4 Expected length regularization\nWhen the lexical memory contains all the sub-\nstrings in the training data, the model easily over-\nﬁts by copying the longest continuation from the\nmemory. To prevent overﬁtting, we introduce a reg-\nularizer that penalizes based on the expectation of\nthe exponentiated (by a hyperparameter β) length\nof each segment:\nR(x,β) =\n∑\ns:π(s)=x\np(s |x)\n∑\ns∈s\n|s|β.\nThis can be understood as a regularizer based on\nthe double exponential prior identiﬁed to be ef-\nfective in previous work (Liang and Klein, 2009;\nBerg-Kirkpatrick et al., 2010). This expectation\nis a differentiable function of the model parame-\nters. Because of the linearity of the penalty across\nsegments, it can be computed efﬁciently using the\nabove dynamic programming algorithm under the\nexpectation semiring (Eisner, 2002). This is par-\nticularly efﬁcient since the expectation semiring\njointly computes the expectation and marginal like-\nlihood in a single forward pass. For more details\nabout computing gradients of expectations under\ndistributions over structured objects with dynamic\nprograms and semirings, see Li and Eisner (2009).\n4.1 Training Objective\nThe model parameters are trained by minimizing\nthe penalized log likelihood of a training corpus D\nof unsegmented sentences,\nL=\n∑\nx∈D\n[−log p(x) + λR(x,β)].\n5 Datasets\nWe evaluate our model on both English and Chi-\nnese segmentation. For both languages, we used\nstandard datasets for word segmentation and lan-\nguage modeling. We also use MS-COCO to evalu-\nate how the model can leverage conditioning con-\ntext information. For all datasets, we used train,\nvalidation and test splits.2 Since our model assumes\na closed character set, we removed validation and\ntest samples which contain characters that do not\nappear in the training set. In the English corpora,\nwhitespace characters are removed. In Chinese,\nthey are not present to begin with. Refer to Ap-\npendix A for dataset statistics.\n5.1 English\nBrent Corpus The Brent corpus is a standard\ncorpus used in statistical modeling of child lan-\nguage acquisition (Brent, 1999; Venkataraman,\n2001).3 The corpus contains transcriptions of utter-\nances directed at 13- to 23-month-old children. The\ncorpus has two variants: an orthographic one (BR-\ntext) and a phonemic one (BR-phono), where each\ncharacter corresponds to a single English phoneme.\nAs the Brent corpus does not have a standard train\nand test split, and we want to tune the parameters\nby measuring the ﬁt to held-out data, we used the\nﬁrst 80% of the utterances for training and the next\n10% for validation and the rest for test.\n2The data and splits used are available at\nhttps://s3.eu-west-2.amazonaws.com/\nk-kawakami/seg.zip.\n3https://childes.talkbank.org/derived\n6433\nEnglish Penn Treebank (PTB) We use the com-\nmonly used version of the PTB prepared by\nMikolov et al. (2010). However, since we removed\nspace symbols from the corpus, our cross entropy\nresults cannot be compared to those usually re-\nported on this dataset.\n5.2 Chinese\nSince Chinese orthography does not mark spaces\nbetween words, there have been a number of efforts\nto annotate word boundaries. We evaluate against\ntwo corpora that have been manually segmented\naccording different segmentation standards.\nBeijing University Corpus (PKU) The Beijing\nUniversity Corpus was one of the corpora used\nfor the International Chinese Word Segmentation\nBakeoff (Emerson, 2005).\nChinese Penn Treebank (CTB) We use the\nPenn Chinese Treebank Version 5.1 (Xue et al.,\n2005). It generally has a coarser segmentation than\nPKU (e.g., in CTB a full name, consisting of a\ngiven name and family name, is a single token),\nand it is a larger corpus.\n5.3 Image Caption Dataset\nTo assess whether jointly learning about meanings\nof words from non-linguistic context affects seg-\nmentation performance, we use image and caption\npairs from the COCO caption dataset (Lin et al.,\n2014). We use 10,000 examples for both training\nand testing and we only use one reference per im-\nage. The images are used to be conditional context\nto predict captions. Refer to Appendix B for the\ndataset construction process.\n6 Experiments\nWe compare our model to benchmark Bayesian\nmodels, which are currently the best known unsu-\npervised word discovery models, as well as to a\nsimple deterministic segmentation criterion based\non surprisal peaks (Elman, 1990) on language mod-\neling and segmentation performance. Although\nthe Bayeisan models are shown to able to discover\nplausible word-like units, we found that a set of hy-\nperparameters that provides best performance with\nsuch model on language modeling does not pro-\nduce good structures as reported in previous works.\nThis is problematic since there is no objective cri-\nteria to ﬁnd hyperparameters in fully unsupervised\nmanner when the model is applied to completely\nunknown languages or domains. Thus, our experi-\nments are designed to assess how well the models\ninfers word segmentations of unsegmented inputs\nwhen they are trained and tuned to maximize the\nlikelihood of the held-out text.\nDP/HDP Benchmarks Among the most effec-\ntive existing word segmentation models are those\nbased on hierarchical Dirichlet process (HDP) mod-\nels (Goldwater et al., 2009; Teh et al., 2006) and hi-\nerarchical Pitman–Yor processes (Mochihashi et al.,\n2009). As a representative of these, we use a simple\nbigram HDP model:\nθ·∼DP(α0,p0)\nθ·|s ∼DP(α1,θ·) ∀s ∈Σ∗\nst+1 |st ∼Categorical(θ·|st ).\nThe base distribution, p0, is deﬁned over strings in\nΣ∗∪{⟨/S⟩}by deciding with a speciﬁed probability\nto end the utterance, a geometric length model, and\na uniform probability over Σ at a each position. In-\ntuitively, it captures the preference for having short\nwords in the lexicon. In addition to the HDP model,\nwe also evaluate a simpler single Dirichlet process\n(DP) version of the model, in which the st’s are\ngenerated directly as draws from Categorical(θ·).\nWe use an empirical Bayesian approach to select\nhyperparameters based on the likelihood assigned\nby the inferred posterior to a held-out validation\nset. Refer to Appendix D for details on inference.\nDeterministic Baselines Incremental word seg-\nmentation is inherently ambiguous (e.g., the let-\nters the might be a single word, or they might be\nthe beginning of the longer word theater). Never-\ntheless, several deterministic functions of preﬁxes\nhave been proposed in the literature as strategies for\ndiscovering rudimentary word-like units hypothe-\nsized for being useful for bootstrapping the lexical\nacquisition process or for improving a model’s pre-\ndictive accuracy. These range from surprisal crite-\nria (Elman, 1990) to sophisticated language models\nthat switch between models that capture intra- and\ninter-word dynamics based on deterministic func-\ntions of preﬁxes of characters (Chung et al., 2017;\nShen et al., 2018).\nIn our experiments, we also include such deter-\nministic segmentation results using (1) the surprisal\ncriterion of Elman (1990) and (2) a two-level hi-\nerarchical multiscale LSTM (Chung et al., 2017),\nwhich has been shown to predict boundaries in\n6434\nwhitespace-containing character sequences at posi-\ntions corresponding to word boundaries. As with\nall experiments in this paper, the BR-corpora for\nthis experiment do not contain spaces.\nSNLM Model conﬁgurations and Evaluation\nLSTMs had 512 hidden units with parameters\nlearned using the Adam update rule (Kingma and\nBa, 2015). We evaluated our models with bits-per-\ncharacter (bpc) and segmentation accuracy (Brent,\n1999; Venkataraman, 2001; Goldwater et al., 2009).\nRefer to Appendices C–F for details of model con-\nﬁgurations and evaluation metrics.\nFor the image caption dataset, we extend the\nmodel with a standard attention mechanism in the\nbackbone LSTM (LSTMenc) to incorporate image\ncontext. For every character-input, the model calcu-\nlates attentions over image features and use them to\npredict the next characters. As for image represen-\ntations, we use features from the last convolution\nlayer of a pre-trained VGG19 model (Simonyan\nand Zisserman, 2014).\n7 Results\nIn this section, we ﬁrst do a careful comparison of\nsegmentation performance on the phonemic Brent\ncorpus (BR-phono) across several different segmen-\ntation baselines, and we ﬁnd that our model obtains\ncompetitive segmentation performance. Addition-\nally, ablation experiments demonstrate that both\nlexical memory and the proposed expected length\nregularization are necessary for inferring good seg-\nmentations. We then show that also on other cor-\npora, we likewise obtain segmentations better than\nbaseline models. Finally, we also show that our\nmodel has superior performance, in terms of held-\nout perplexity, compared to a character-level LSTM\nlanguage model. Thus, overall, our results show\nthat we can obtain good segmentations on a vari-\nety of tasks, while still having very good language\nmodeling performance.\nWord Segmentation (BR-phono) Table 1 sum-\nmarizes the segmentation results on the widely\nused BR-phono corpus, comparing it to a variety\nof baselines. Unigram DP, Bigram HDP, LSTM\nsuprisal and HMLSTM refer to the benchmark\nmodels explained in §6. The ablated versions of our\nmodel show that without the lexicon (−memory),\nwithout the expected length penalty (−length), and\nwithout either, our model fails to discover good seg-\nmentations. Furthermore, we draw attention to the\ndifference in the performance of the HDP and DP\nmodels when using subjective settings of the hyper-\nparameters and the empirical settings (likelihood).\nFinally, the deterministic baselines are interesting\nin two ways. First, LSTM surprisal is a remarkably\ngood heuristic for segmenting text (although we\nwill see below that its performance is much less\ngood on other datasets). Second, despite careful\ntuning, the HMLSTM of Chung et al. (2017) fails\nto discover good segments, although in their paper\nthey show that when spaces are present between,\nHMLSTMs learn to switch between their internal\nmodels in response to them.\nFurthermore, the priors used in the DP/HDP\nmodels were tuned to maximize the likelihood as-\nsigned to the validation set by the inferred poste-\nrior predictive distribution, in contrast to previous\npapers which either set them subjectively or in-\nferred them (Johnson and Goldwater, 2009). For\nexample, the DP and HDP model with subjective\npriors obtained 53.8 and 72.3 F1 scores, respec-\ntively (Goldwater et al., 2009). However, when the\nhyperparameters are set to maximize held-out like-\nlihood, this drops obtained 56.1 and 56.9. Another\nresult on this dataset is the feature unigram model\nof Berg-Kirkpatrick et al. (2010), which obtains\nan 88.0 F1 score with hand-crafted features and\nby selecting the regularization strength to optimize\nsegmentation performance. Once the features are\nremoved, the model achieved a 71.5 F1 score when\nit is tuned on segmentation performance and only\n11.5 when it is tuned on held-out likelihood.\nP R F1\nLSTM surprisal (Elman, 1990) 54.5 55.5 55.0\nHMLSTM (Chung et al., 2017) 8.1 13.3 10.1\nUnigram DP 63.3 50.4 56.1\nBigram HDP 53.0 61.4 56.9\nSNLM (−memory, −length) 54.3 34.9 42.5\nSNLM (+memory, −length) 52.4 36.8 43.3\nSNLM (−memory, +length) 57.6 43.4 49.5\nSNLM (+memory, +length) 81.3 77.5 79.3\nTable 1: Summary of segmentation performance on\nphoneme version of the Brent Corpus (BR-phono).\nWord Segmentation (other corpora) Table 2\nsummarizes results on the BR-text (orthographic\nBrent corpus) and Chinese corpora. As in the pre-\nvious section, all the models were trained to maxi-\n6435\nmize held-out likelihood. Here we observe a simi-\nlar pattern, with the SNLM outperforming the base-\nline models, despite the tasks being quite different\nfrom each other and from the BR-phono task.\nP R F1\nBR-text\nLSTM surprisal 36.4 49.0 41.7\nUnigram DP 64.9 55.7 60.0\nBigram HDP 52.5 63.1 57.3\nSNLM 68.7 78.9 73.5\nPTB\nLSTM surprisal 27.3 36.5 31.2\nUnigram DP 51.0 49.1 50.0\nBigram HDP 34.8 47.3 40.1\nSNLM 54.1 60.1 56.9\nCTB\nLSTM surprisal 41.6 25.6 31.7\nUnigram DP 61.8 49.6 55.0\nBigram HDP 67.3 67.7 67.5\nSNLM 78.1 81.5 79.8\nPKU\nLSTM surprisal 38.1 23.0 28.7\nUnigram DP 60.2 48.2 53.6\nBigram HDP 66.8 67.1 66.9\nSNLM 75.0 71.2 73.1\nTable 2: Summary of segmentation performance on\nother corpora.\nWord Segmentation Qualitative Analysis We\nshow some representative examples of segmenta-\ntions inferred by various models on the BR-text and\nPKU corpora in Table 3. As reported in Goldwater\net al. (2009), we observe that the DP models tend\nto undersegment, keep long frequent sequences to-\ngether (e.g., they failed to separate articles). HDPs\ndo successfully prevent oversegmentation; how-\never, we ﬁnd that when trained to optimize held-\nout likelihood, they often insert unnecessary bound-\naries between words, such asyo u. Our model’s per-\nformance is better, but it likewise shows a tendency\nto oversegment. Interestingly, we can observe a ten-\ndency tends to put boundaries between morphemes\nin morphologically complex lexical items such as\ndumpty ’s, and go ing. Since morphemes are the\nminimal units that carry meaning in language, this\nsegmentation, while incorrect, is at least plasuible.\nTurning to the Chinese examples, we see that both\nbaseline models fail to discover basic words such\nas 山间 (mountain) and 人们 (human).\nFinally, we observe that none of the models suc-\ncessfully segment dates or numbers containing mul-\ntiple digits (all oversegment). Since number types\ntend to be rare, they are usually not in the lexicon,\nmeaning our model (and the H/DP baselines) must\ngenerate them as character sequences.\nLanguage Modeling Performance The above\nresults show that the SNLM infers good word seg-\nmentations. We now turn to the question of how\nwell it predicts held-out data. Table 4 summa-\nrizes the results of the language modeling exper-\niments. Again, we see that SNLM outperforms\nthe Bayesian models and a character LSTM. Al-\nthough there are numerous extensions to LSTMs to\nimprove language modeling performance, LSTMs\nremain a strong baseline (Melis et al., 2018).\nOne might object that because of the lexicon,\nthe SNLM has many more parameters than the\ncharacter-level LSTM baseline model. However,\nunlike parameters in LSTM recurrence which are\nused every timestep, our memory parameters are\naccessed very sparsely. Furthermore, we observed\nthat an LSTM with twice the hidden units did not\nimprove the baseline with 512 hidden units on both\nphonemic and orthographic versions of Brent cor-\npus but the lexicon could. This result suggests more\nhidden units are useful if the model does not have\nenough capacity to ﬁt larger datasets, but that the\nmemory structure adds other dynamics which are\nnot captured by large recurrent networks.\nMultimodal Word Segmentation Finally, we\ndiscuss results on word discovery with non-\nlinguistic context (image). Although there is much\nevidence that neural networks can reliably learn\nto exploit additional relevant context to improve\nlanguage modeling performance (e.g. machine\ntranslation and image captioning), it is still unclear\nwhether the conditioning context help to discover\nstructure in the data. We turn to this question here.\nTable 5 summarizes language modeling and seg-\nmentation performance of our model and a baseline\ncharacter-LSTM language model on the COCO im-\nage caption dataset. We use the Elman Entropy\ncriterion to infer the segmentation points from the\nbaseline LM, and the MAP segmentation under\nour model. Again, we ﬁnd our model outperforms\nthe baseline model in terms of both language mod-\neling and word segmentation accuracy. Interest-\ningly, we ﬁnd while conditioning on image context\nleads to reductions in perplexity in both models,\nin our model the presence of the image further im-\nproves segmentation accuracy. This suggests that\n6436\nExamples\nBR-text\nReference are you going to make him pretty this morning\nUnigram DP areyou goingto makehim pretty this morning\nBigram HDP areyou go ingto make him p retty this mo rn ing\nSNLM are you go ing to make him pretty this morning\nReference would you like to do humpty dumpty’s button\nUnigram DP wouldyoul iketo do humpty dumpty ’s button\nBigram HDP would youlike to do humptyd umpty ’s butt on\nSNLM would you like to do humpty dumpty ’s button\nPKU\nReference 笑声 、 掌声 、 欢呼声 ， 在 山间 回荡 ， 勾 起 了 人们 对 往事 的 回忆 。\nUnigram DP 笑声 、 掌声 、 欢呼 声 ，在 山 间 回荡 ， 勾 起了 人们 对 往事 的 回忆 。\nBigram HDP 笑 声、 掌声 、 欢 呼声 ，在 山 间 回 荡， 勾 起了 人 们 对 往事 的 回忆 。\nSNLM 笑声、 掌声 、 欢呼声 ， 在 山间 回荡 ， 勾起 了 人们 对 往事 的 回忆 。\nReference 不得 在 江河 电缆 保护区 内 抛锚 、 拖锚 、 炸鱼 、 挖沙 。\nUnigram DP 不得 在 江河电缆 保护 区内抛锚、 拖锚 、炸鱼、挖沙 。\nBigram HDP 不得 在 江 河电缆 保护 区内 抛 锚、拖 锚 、 炸鱼、 挖沙 。\nSNLM 不得 在 江河 电缆 保护区 内 抛锚 、 拖锚、 炸鱼 、 挖沙 。\nTable 3: Examples of predicted segmentations on English and Chinese.\nBR-text BR-phono PTB CTB PKU\nUnigram DP 2.33 2.93 2.25 6.16 6.88\nBigram HDP 1.96 2.55 1.80 5.40 6.42\nLSTM 2.03 2.62 1.65 4.94 6.20\nSNLM 1.94 2.54 1.56 4.84 5.89\nTable 4: Test language modeling performance (bpc).\nour model and its learning mechanism interact with\nthe conditional context differently than the LSTM\ndoes.\nTo understand what kind of improvements in\nsegmentation performance the image context leads\nto, we annotated the tokens in the references with\npart-of-speech (POS) tags and compared relative\nimprovements on recall between SNLM (−image)\nand SNLM ( +image) among the ﬁve POS tags\nwhich appear more than 10,000 times. We ob-\nserved improvements on ADJ ( +4.5%), NOUN\n(+4.1%), VERB (+3.1%). The improvements on\nthe categories ADP ( +0.5%) and DET ( +0.3%)\nare were more limited. The categories where we\nsee the largest improvement in recall correspond\nto those that are likely a priori to correlate most\nreliably with observable features. Thus, this result\nis consistent with a hypothesis that the lexican is\nsuccessfully acquiring knowledge about how words\nidiosyncratically link to visual features.\nSegmentation State-of-the-Art The results re-\nported are not the best-reported numbers on the En-\nbpc↓ P ↑ R ↑ F1↑\nUnigram DP 2.23 44.0 40.0 41.9\nBigram HDP 1.68 30.9 40.8 35.1\nLSTM (−image) 1.55 31.3 38.2 34.4\nSNLM (−image) 1.52 39.8 55.3 46.3\nLSTM (+image) 1.42 31.7 39.1 35.0\nSNLM (+image) 1.38 46.4 62.0 53.1\nTable 5: Language modeling (bpc) and segmentation\naccuracy on COCO dataset. +image indicates that the\nmodel has access to image context.\nglish phoneme or Chinese segmentation tasks. As\nwe discussed in the introduction, previous work has\nfocused on segmentation in isolation from language\nmodeling performance. Models that obtain better\nsegmentations include the adaptor grammars (F1:\n87.0) of Johnson and Goldwater (2009) and the\nfeature-unigram model (88.0) of Berg-Kirkpatrick\net al. (2010). While these results are better in terms\nof segmentation, they are weak language models\n(the feature unigram model is effectively a unigram\nword model; the adaptor grammar model is effec-\ntively phrasal unigram model; both are incapable of\ngeneralizing about substantially non-local depen-\ndencies). Additionally, the features and grammars\nused in prior work reﬂect certain English-speciﬁc\ndesign considerations (e.g., syllable structure in the\ncase of adaptor grammars and phonotactic equiva-\nlence classes in the feature unigram model), which\nmake them questionable models if the goal is to ex-\n6437\nplore what models and biases enable word discov-\nery in general. For Chinese, the best nonparametric\nmodels perform better at segmentation (Zhao and\nKit, 2008; Mochihashi et al., 2009), but again they\nare weaker language models than neural models.\nThe neural model of Sun and Deng (2018) is similar\nto our model without lexical memory or length reg-\nularization; it obtains 80.2 F1 on the PKU dataset;\nhowever, it uses gold segmentation data during\ntraining and hyperparameter selection, 4 whereas\nour approach requires no gold standard segmenta-\ntion data.\n8 Related Work\nLearning to discover and represent temporally ex-\ntended structures in a sequence is a fundamental\nproblem in many ﬁelds. For example in language\nprocessing, unsupervised learning of multiple lev-\nels of linguistic structures such as morphemes (Sny-\nder and Barzilay, 2008), words (Goldwater et al.,\n2009; Mochihashi et al., 2009; Wang et al., 2014)\nand phrases (Klein and Manning, 2001) have been\ninvestigated. Recently, speech recognition has ben-\neﬁted from techniques that enable the discovery\nof subword units (Chan et al., 2017; Wang et al.,\n2017); however, in that work, the optimally dis-\ncovered character sequences look quite unlike or-\nthographic words. In fact, the model proposed by\nWang et al. (2017) is essentially our model with-\nout a lexicon or the expected length regularization,\ni.e., (−memory, −length), which we have shown\nperforms quite poorly in terms of segmentation ac-\ncuracy. Finally, some prior work has also sought to\ndiscover lexical units directly from speech based\non speech-internal statistical regularities (Kam-\nper et al., 2016), as well as jointly with ground-\ning (Chrupała et al., 2017).\n9 Conclusion\nWord discovery is a fundamental problem in lan-\nguage acquisition. While work studying the prob-\nlem in isolation has provided valuable insights\n(showing both what data is sufﬁcient for word dis-\ncovery with which models), this paper shows that\nneural models offer the ﬂexibility and performance\nto productively study the various facets of the prob-\nlem in a more uniﬁed model. While this work uni-\nﬁes several components that had previously been\n4https://github.com/\nEdward-Sun/SLM/blob/\nd37ad735a7b1d5af430b96677c2ecf37a65f59b7/\ncodes/run.py#L329\nstudied in isolation, our model assumes access to\nphonetic categories. The development of these\ncategories likely interact with the development of\nthe lexicon and acquisition of semantics (Feldman\net al., 2013; Fourtassi and Dupoux, 2014), and thus\nsubsequent work should seek to unify more aspects\nof the acquisition problem.\nAcknowledgments\nWe thank Mark Johnson, Sharon Goldwater, and\nEmmanuel Dupoux, as well as many colleagues at\nDeepMind, for their insightful comments and sug-\ngestions for improving this work and the resulting\npaper.\nReferences\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,\nJohn DeNero, and Dan Klein. 2010. Painless unsu-\npervised learning with features. In Proc. NAACL.\nMichael R Brent. 1999. An efﬁcient, probabilistically\nsound algorithm for segmentation and word discov-\nery. Machine Learning, 34(1):71–105.\nMichael R Brent and Timothy A Cartwright. 1996. Dis-\ntributional regularity and phonotactic constraints are\nuseful for segmentation. Cognition, 61(1):93–125.\nWilliam Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly.\n2017. Latent sequence decompositions. In Proc.\nICLR.\nGrzegorz Chrupała, Lieke Gelderloos, and Afra Al-\nishahi. 2017. Representations of language in a\nmodel of visually grounded speech signal.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In Proc. ICLR.\nJason Eisner. 2002. Parameter estimation for proba-\nbilistic ﬁnite-state transducers. In Proc. ACL.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nThomas Emerson. 2005. The second international Chi-\nnese word segmentation bakeoff. In Proc. SIGHAN\nWorkshop.\nNaomi H. Feldman, Thomas L. Grifﬁths, Sharon Gold-\nwater, and James L. Morgan. 2013. A role for the\ndeveloping lexicon in phonetic category acquisition.\nPsychological Review, 120(4):751–778.\nAbdellah Fourtassi and Emmanuel Dupoux. 2014. A\nrudimentary lexicon and semantics help bootstrap\nphoneme acquisition. In Proc. EMNLP.\n6438\nLieke Gelderloos and Grzegorz Chrupała. 2016. From\nphonemes to images: levels of representation in a re-\ncurrent neural model of visually-grounded language\nlearning. In Proc. COLING.\nSharon Goldwater, Thomas L Grifﬁths, and Mark John-\nson. 2009. A Bayesian framework for word segmen-\ntation: Exploring the effects of context. Cognition,\n112(1):21–54.\nAlex Graves. 2012. Sequence transduction with\nrecurrent neural networks. arXiv preprint\narXiv:1211.3711.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nMark Johnson, Katherine Demuth, Michael Frank, and\nBevan K. Jones. 2010. Synergies in learning words\nand their referents. In Proc. NIPS.\nMark Johnson and Sharon Goldwater. 2009. Improving\nnonparameteric bayesian inference: experiments on\nunsupervised word segmentation with adaptor gram-\nmars. In Proc. NAACL, pages 317–325.\nÁkos Kádár, Marc-Alexandre Côté, Grzegorz Chru-\npała, and Afra Alishahi. 2018. Revisiting the hier-\narchical multiscale LSTM. In Proc. COLING.\nHerman Kamper, Aren Jansen, and Sharon Goldwater.\n2016. Unsupervised word segmentation and lexi-\ncon induction discovery using acoustic word embed-\ndings. IEEE Transactions on Audio, Speech, and\nLanguage Processing, 24(4):669–679.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to create and reuse words in open-\nvocabulary neural language modeling. In Proc.\nACL.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proc. ICLR.\nDan Klein and Christopher D Manning. 2001. Distribu-\ntional phrase structure induction. In Workshop Proc.\nACL.\nZhifei Li and Jason Eisner. 2009. First-and second-\norder expectation semirings with applications to\nminimum-risk training on translation forests. In\nProc. EMNLP.\nPercy Liang and Dan Klein. 2009. Online EM for un-\nsupervised models. In Proc. NAACL.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In Proc. ECCV, pages\n740–755.\nWang Ling, Edward Grefenstette, Karl Moritz Her-\nmann, Tomáš Ko ˇciský, Andrew Senior, Fumin\nWang, and Phil Blunsom. 2017. Latent predictor net-\nworks for code generation. In Proc. ACL.\nGábor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In Proc. ICLR.\nSebastian J. Mielke and Jason Eisner. 2018. Spell once,\nsummon anywhere: A two-level open-vocabulary\nlanguage model. In Proc. NAACL.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proc. In-\nterspeech.\nDaichi Mochihashi, Takeshi Yamada, and Naonori\nUeda. 2009. Bayesian unsupervised word segmen-\ntation with nested Pitman–Yor language modeling.\nSteven Pinker. 1984. Language learnability and lan-\nguage development. Harvard University Press.\nOkko Räsänen and Heikki Rasilo. 2015. A joint\nmodel of word segmentation and meaning acquisi-\ntion through cross-situational learning. Psychologi-\ncal Review, 122(4):792–829.\nJenny R Saffran, Richard N Aslin, and Elissa L New-\nport. 1996. Statistical learning by 8-month-old in-\nfants. Science, 274(5294):1926–1928.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. In Proc.\nICLR.\nK. Simonyan and A. Zisserman. 2014. Very deep con-\nvolutional networks for large-scale image recogni-\ntion. CoRR, abs/1409.1556.\nBenjamin Snyder and Regina Barzilay. 2008. Unsuper-\nvised multilingual learning for morphological seg-\nmentation. In Proc. ACL.\nZhiqing Sun and Zhi-Hong Deng. 2018. Unsupervised\nneural word segmentation for Chinese via segmental\nlanguage modeling.\nYee-Whye Teh, Michael I. Jordan, Matthew J. Beal,\nand Daivd M. Blei. 2006. Hierarchical Dirichlet pro-\ncesses. Journal of the American Statistical Associa-\ntion, 101(476):1566–1581.\nAnand Venkataraman. 2001. A statistical model for\nword discovery in transcribed speech. Computa-\ntional Linguistics, 27(3):351–372.\nChong Wang, Yining Wan, Po-Sen Huang, Abdelrah-\nman Mohammad, Dengyong Zhou, and Li Deng.\n2017. Sequence modeling via segmentations. In\nProc. ICML.\nXiaolin Wang, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2014. Empirical study of unsuper-\nvised Chinese word segmentation methods for SMT\non large-scale corpora.\n6439\nNaiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta\nPalmer. 2005. The Penn Chinese treebank: Phrase\nstructure annotation of a large corpus. Natural lan-\nguage engineering, 11(2):207–238.\nShun-Zheng Yu. 2010. Hidden semi-Markov models.\nArtiﬁcial Intelligence, 174(2):215–243.\nHai Zhao and Chunyu Kit. 2008. An empirical com-\nparison of goodness measures for unsupervised Chi-\nnese word segmentation with a uniﬁed framework.\nIn Proc. IJCNLP.\n6440\nA Dataset statistics\nTable 6 summarizes dataset statistics.\nB Image Caption Dataset Construction\nWe use 8000, 2000 and 10000 images for\ntrain, development and test set in order of in-\nteger ids specifying image in cocoapi 5 and use\nﬁrst annotation provided for each image. We\nwill make pairs of image id and annotation\nid available from https://s3.eu-west-2.\namazonaws.com/k-kawakami/seg.zip.\nC SNLM Model Conﬁguration\nFor each RNN based model we used 512 dimen-\nsions for the character embeddings and the LSTMs\nhave 512 hidden units. All the parameters, includ-\ning character projection parameters, are randomly\nsampled from uniform distribution from −0.08 to\n0.08. The initial hidden and memory state of the\nLSTMs are initialized with zero. A dropout rate of\n0.5 was used for all but the recurrent connections.\nTo restrict the size of memory, we stored sub-\nstrings which appeared F-times in the training cor-\npora and tuned F with grid search. The maximum\nlength of subsequencesLwas tuned on the held-out\nlikelihood using a grid search. Tab. 7 summarizes\nthe parameters for each dataset. Note that we did\nnot tune the hyperparameters on segmentation qual-\nity to ensure that the models are trained in a purely\nunsupervised manner assuming no reference seg-\nmentations are available.\nD DP/HDP Inference\nBy integrating out the draws from the DP’s, it is\npossible to do inference using Gibbs sampling di-\nrectly in the space of segmentation decisions. We\nuse 1,000 iterations with annealing to ﬁnd an ap-\nproximation of the MAP segmentation and then\nuse the corresponding posterior predictive distribu-\ntion to estimate the held-out likelihood assigned\nby the model, marginalizing the segmentations us-\ning appropriate dynamic programs. The evaluated\nsegmentation was the most probable segmentation\naccording to the posterior predictive distribution.\nIn the original Bayesian segmentation work, the\nhyperparameters (i.e., α0, α1, and the components\nof p0) were selected subjectively. To make com-\nparison with our neural models fairer, we instead\nused an empirical approach and set them using the\n5https://github.com/cocodataset/cocoapi\nheld-out likelihood of the validation set. However,\nsince this disadvantages the DP/HDP models in\nterms of segmentation, we also report the original\nresults on the BR corpora.\nE Learning\nThe models were trained with the Adam update\nrule (Kingma and Ba, 2015) with a learning rate of\n0.01. The learning rate is divided by 4 if there is no\nimprovement on development data. The maximum\nnorm of the gradients was clipped at 1.0.\nF Evaluation Metrics\nLanguage Modeling We evaluated our models\nwith bits-per-character (bpc), a standard evalua-\ntion metric for character-level language models.\nFollowing the deﬁnition in Graves (2013), bits-per-\ncharacter is the average value of−log2 p(xt |x<t)\nover the whole test set,\nbpc = −1\n|x|log2 p(x),\nwhere |x|is the length of the corpus in characters.\nThe bpc is reported on the test set.\nSegmentation We also evaluated segmentation\nquality in terms of precision, recall, and F1 of word\ntokens (Brent, 1999; Venkataraman, 2001; Gold-\nwater et al., 2009). To get credit for a word, the\nmodels must correctly identify both the left and\nright boundaries. For example, if there is a pair of\na reference segmentation and a prediction,\nReference: do you see a boy\nPrediction: doyou see a boy\nthen 4 words are discovered in the prediction where\nthe reference has 5 words. 3 words in the prediction\nmatch with the reference. In this case, we report\nscores as precision = 75.0 (3/4), recall = 60.0 (3/5),\nand F1, the harmonic mean of precision and recall,\n66.7 (2/3). To facilitate comparison with previ-\nous work, segmentation results are reported on the\nunion of the training, validation, and test sets.\n6441\nSentence Char. Types Word Types Characters Average Word Length\nTrain Valid Test Train Valid Test Train Valid Test Train Valid Test Train Valid Test\nBR-text 7832 979 979 30 30 29 1237 473 475 129k 16k 16k 3.82 4.06 3.83\nBR-phono 7832 978 978 51 51 50 1183 457 462 104k 13k 13k 2.86 2.97 2.83\nPTB 42068 3370 3761 50 50 48 10000 6022 6049 5.1M 400k 450k 4.44 4.37 4.41\nCTB 50734 349 345 160 76 76 60095 1769 1810 3.1M 18k 22k 4.84 5.07 5.14\nPKU 17149 1841 1790 90 84 87 52539 13103 11665 2.6M 247k 241k 4.93 4.94 4.85\nCOCO 8000 2000 10000 50 42 48 4390 2260 5072 417k 104k 520k 4.00 3.99 3.99\nTable 6: Summary of Dataset Statistics.\nmax len (L) min freq (F) λ\nBR-text 10 10 7.5e-4\nBR-phono 10 10 9.5e-4\nPTB 10 100 5.0e-5\nCTB 5 25 1.0e-2\nPKU 5 25 9.0e-3\nCOCO 10 100 2.0e-4\nTable 7: Hyperparameter values used.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7848311066627502
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7204122543334961
    },
    {
      "name": "Natural language processing",
      "score": 0.6561183929443359
    },
    {
      "name": "Language model",
      "score": 0.6503464579582214
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5814267992973328
    },
    {
      "name": "Word (group theory)",
      "score": 0.5637750625610352
    },
    {
      "name": "Segmentation",
      "score": 0.5437615513801575
    },
    {
      "name": "Character (mathematics)",
      "score": 0.5268743634223938
    },
    {
      "name": "Generalization",
      "score": 0.5085271596908569
    },
    {
      "name": "Artificial neural network",
      "score": 0.5002539157867432
    },
    {
      "name": "Task (project management)",
      "score": 0.49404194951057434
    },
    {
      "name": "Contrast (vision)",
      "score": 0.4799542725086212
    },
    {
      "name": "Context model",
      "score": 0.4733688533306122
    },
    {
      "name": "Bayesian probability",
      "score": 0.4237351417541504
    },
    {
      "name": "Speech recognition",
      "score": 0.3557348847389221
    },
    {
      "name": "Machine learning",
      "score": 0.33106714487075806
    },
    {
      "name": "Linguistics",
      "score": 0.16581392288208008
    },
    {
      "name": "Mathematics",
      "score": 0.08134183287620544
    },
    {
      "name": "Object (grammar)",
      "score": 0.07217633724212646
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ]
}