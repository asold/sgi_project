{
    "title": "Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks",
    "url": "https://openalex.org/W3161569725",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A1933003623",
            "name": "Ben Saunders",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A2219775466",
            "name": "Necati Cihan Camgoz",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A2145160160",
            "name": "Richard Bowden",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A1933003623",
            "name": "Ben Saunders",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2219775466",
            "name": "Necati Cihan Camgoz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2145160160",
            "name": "Richard Bowden",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3045437443",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2139359116",
        "https://openalex.org/W116902681",
        "https://openalex.org/W6634817459",
        "https://openalex.org/W2981531407",
        "https://openalex.org/W2769102608",
        "https://openalex.org/W2759302818",
        "https://openalex.org/W2799020610",
        "https://openalex.org/W3126451397",
        "https://openalex.org/W2559085405",
        "https://openalex.org/W2984529706",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W6675871991",
        "https://openalex.org/W1979013937",
        "https://openalex.org/W2746301562",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2250636693",
        "https://openalex.org/W2080378008",
        "https://openalex.org/W2250670799",
        "https://openalex.org/W2962795401",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W602977392",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W4253468347",
        "https://openalex.org/W6736562241",
        "https://openalex.org/W6754184789",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2054032682",
        "https://openalex.org/W2963489164",
        "https://openalex.org/W6755406732",
        "https://openalex.org/W2964253156",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2966391918",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W157023553",
        "https://openalex.org/W2017050967",
        "https://openalex.org/W2954798773",
        "https://openalex.org/W2941870244",
        "https://openalex.org/W2188882108",
        "https://openalex.org/W2755802490",
        "https://openalex.org/W2587277634",
        "https://openalex.org/W2906958245",
        "https://openalex.org/W2970756316",
        "https://openalex.org/W6770208262",
        "https://openalex.org/W2982627166",
        "https://openalex.org/W2616969219",
        "https://openalex.org/W1868705103",
        "https://openalex.org/W2948479456",
        "https://openalex.org/W1568066624",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W6678815747",
        "https://openalex.org/W2938759099",
        "https://openalex.org/W3127711005",
        "https://openalex.org/W2426359742",
        "https://openalex.org/W6756040250",
        "https://openalex.org/W6678095681",
        "https://openalex.org/W2802023636",
        "https://openalex.org/W2622385665",
        "https://openalex.org/W2963842958",
        "https://openalex.org/W2173520492",
        "https://openalex.org/W2995928604",
        "https://openalex.org/W2946200149",
        "https://openalex.org/W6718379498",
        "https://openalex.org/W3081751228",
        "https://openalex.org/W3107972479",
        "https://openalex.org/W2112656927",
        "https://openalex.org/W4235517609",
        "https://openalex.org/W2161919149",
        "https://openalex.org/W2997573805",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W4234518198",
        "https://openalex.org/W2066601700",
        "https://openalex.org/W2963092440",
        "https://openalex.org/W1497212834",
        "https://openalex.org/W3034464039",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2097501508",
        "https://openalex.org/W6726983635",
        "https://openalex.org/W2595110011",
        "https://openalex.org/W6736588030",
        "https://openalex.org/W3004835408",
        "https://openalex.org/W2601324753",
        "https://openalex.org/W2963091746",
        "https://openalex.org/W3024396289",
        "https://openalex.org/W2963047498",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2107603586",
        "https://openalex.org/W2121818271",
        "https://openalex.org/W3034765865",
        "https://openalex.org/W1579853615",
        "https://openalex.org/W2432004435",
        "https://openalex.org/W3102937540",
        "https://openalex.org/W2953072278",
        "https://openalex.org/W2091636093",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2949099979",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2968458594",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2903831537",
        "https://openalex.org/W2157548127",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2607987856",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3126473778",
        "https://openalex.org/W2951824008",
        "https://openalex.org/W2118244636",
        "https://openalex.org/W2896197082",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2987870723",
        "https://openalex.org/W2606712314"
    ],
    "abstract": "Abstract Sign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate. Sign language production (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation and full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works have produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-expressive production. In this work, we propose a novel Progressive Transformer architecture, the first SLP model to translate from spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer network architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking the production progress over time and predicting the end of sequence. We present extensive data augmentation techniques to reduce prediction drift, alongside an adversarial training regime and a mixture density network (MDN) formulation to produce realistic and expressive sign pose sequences. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further provide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions.",
    "full_text": "International Journal of Computer Vision (2021) 129:2113–2135\nhttps://doi.org/10.1007/s11263-021-01457-9\nContinuous 3D Multi-Channel Sign Language Production via\nProgressive Transformers and Mixture Density Networks\nBen Saunders 1 · Necati Cihan Camgoz 1 · Richard Bowden 1\nReceived: 16 October 2020 / Accepted: 7 March 2021 / Published online: 7 May 2021\n© The Author(s) 2021\nAbstract\nSign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate. Sign language\nproduction (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation\nand full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works\nhave produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-\nexpressive production. In this work, we propose a novel Progressive Transformer architecture, the ﬁrst SLP model to translate\nfrom spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer\nnetwork architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking\nthe production progress over time and predicting the end of sequence. We present extensive data augmentation techniques\nto reduce prediction drift, alongside an adversarial training regime and a mixture density network (MDN) formulation to\nproduce realistic and expressive sign pose sequences. We propose a back translation evaluation mechanism for SLP , presenting\nbenchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further\nprovide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions.\nKeywords Sign language production · 3D Multi-channel sign language · Continuous sequence generation\n1 Introduction\nSign languages are visual multi-channel languages and the\nmain medium of communication for the Deaf. Around 5%\nof the worlds population experience some form of hearing\nloss (World Health Organisation 2020). In the UK alone,\nthere are an estimated 9 million people who are Deaf or hard\nof hearing (British Deaf Association 2020). For the Deaf\nnative signer, a spoken language may be a second language,\nmeaning their spoken language skills can vary immensely\n(Holt 1993). Therefore, sign languages are the preferred form\nof communication for the Deaf communities.\nSign languages possess different grammatical structure\nand syntax to spoken languages (Stokoe 1980). As high-\nlighted in Fig. 1, the translation between spoken and sign\nlanguages requires a change in order and structure due to\nCommunicated by Manuel J. Marin-Jimenez.\nB Ben Saunders\nb.saunders@surrey.ac.uk\n1 Centre for Vision, Speech and Signal Processing, University\nof Surrey, Guildford, UK\ntheir non-monotonic relationship. Sign languages are also\n3D visual languages, with position and movement relative\nto the body playing an important part of communication. In\norder to convey complex meanings and context, sign lan-\nguages employ multiple modes of articulation. The manual\nfeatures of hand shape and motion are combined with the\nnon-manual features of facial expressions, mouthings and\nupper body posture (Sutton-Spence and Woll 1999).\nSign languages have long been researched by the vision\ncommunity (Bauer et al. 2000; Starner and Pentland 1997;\nTamura and Kawasaki 1988). Previous research has focused\non the recognition of sign languages and the subsequent\ntranslation to spoken language. Although useful, this is a\ntechnology more applicable to allowing the hearing to under-\nstand the Deaf, and often not that helpful for the Deaf\ncommunity. The opposite task of sign language production\n(SLP) is far more relevant to the Deaf. Automatically trans-\nlating spoken language into sign language could increase the\nsign language content available in the predominately hearing-\nfocused world.\nTo be useful to the Deaf community, SLP must produce\nsequences of natural, understandable sign akin to a human\n123\n2114 International Journal of Computer Vision (2021) 129:2113–2135\nFig. 1 Sign language production (SLP) example showing correspond-\ning spoken language, gloss representation and sign language sequences.\nThe Text to Gloss , Gloss to Pose and Text to Pose translation tasks are\nhighlighted, where end-to-end SLP is a direct translation from spo-\nken language to sign language, skipping the gloss intermediary. In this\nmanuscript we use text to denote spoken language sequences\ntranslator (Bragg et al. 2019). Previous deep learning-based\nSLP work has been limited to the production of concate-\nnated isolated signs (Stoll 2020; Zelinka and Kanis 2020),\nwith a focus solely on the manual features. These works\nalso approach the problem in a fragmented Text to Gloss\n1\nand Gloss to Pose production (Fig. 1, left), where impor-\ntant context can be lost in the gloss bottleneck. However,\nthe production of full sign sequences is a more challenging\ntask, as there is no direct alignment between sign sequences\nand spoken language sentences. Ignoring non-manual fea-\ntures disregards the contextual and grammatical information\nrequired to fully understand the meaning of the produced\nsigns (V alli and Lucas 2000). These works also produce only\n2D skeleton data, lacking the depth channel to truly model\nrealistic motion.\nIn this work, we present a Continuous 3D Multi-Channel\nSign Language Production model, the ﬁrst SLP network\nto translate from spoken language sentences to continuous\n3D multi-channel sign language sequences in an end-to-end\nmanner. This is shown on the right of Fig. 1 as a direct trans-\nlation from source spoken language, without the need for a\ngloss intermediary. We propose a Progressive Transformer\narchitecture that uses an alternative formulation of trans-\nformer decoding for continuous sequences, where there is\nno pre-deﬁned vocabulary. We introduce a counter decoding\ntechnique to predict continuous sequences of variable length\nby tracking the production progress over time and predicting\nthe end of sequence. Our sign pose productions contain both\nmanual and non-manual features, increasing both the realism\nand comprehension.\n1 Glosses are a written representation of sign, deﬁned as minimal lexical\nitems.\nTo reduce the prediction drift often seen in continuous\nsequence production, we present several data augmentation\nmethods. These create a more robust model and reduce the\nerroneous nature of auto-regressive prediction. Continuous\nprediction often results in a under-articulated output due to\nthe problem of regression to the mean, and thus we pro-\npose the addition of adversarial training. A discriminator\nmodel conditioned on source spoken language is introduced\nto prompt a more realistic and expressive sign production\nfrom the progressive transformer. Additionally, due to the\nmultimodal nature of sign languages, we also experiment\nwith a mixture density network (MDN) modelling, utilising\nthe progressive transformer outputs to paramatise a Gaussian\nmixture model.\nTo evaluate quantitative performance, we propose a back\ntranslation evaluation method for SLP , using a Sign Language\nTranslation (SLT) back-end to translate sign productions\nback to spoken language. We evaluate on the challenging\nRWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset,\npresenting several benchmark results of both Gloss to Pose\nand Text to Pose conﬁgurations, to underpin future research.\nWe also provide a user evaluation of our sign productions,\nto evaluate the comprehension of our SLP model. Finally,\nwe share qualitative results to give the reader further insight\ninto the models performance, producing accurate sign pose\nsequences of unseen text input.\nThe contributions of this paper can be summarised as:\n– The ﬁrst SLP model to translate from spoken language to\ncontinuous 3D sign pose sequences, enabled by a novel\ntransformer decoding technique.\n– An application of conditional adversarial training to SLP ,\nfor the production of realistic sign\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2115\n– The combination of transformers and mixture density\nnetworks to model multimodal continuous sequences.\n– Benchmark SLP results on the PHOENIX14T dataset\nand a new back translation evaluation metric, alongside\na comprehensive Deaf user evaluation.\nPreliminary versions of this work were presented in\nSaunders et al. ( 2020a; 2020b). This extended manuscript\nincludes additional formulation and the introduction of a\nMDN modelling for expressive sign production. Extensive\nnew quantitative and qualitative evaluation is provided to\nexplore the capabilities of our approach, alongside a user\nstudy with Deaf participants to measure the comprehension\nof our produced sign language sequences.\nThe rest of this paper is organised as follows: We outline\nthe previous work in SLP and surrounding areas in Sect. 2.\nOur progressive transformer network and proposed model\nconﬁgurations are presented in Sect. 3. Sect. 4 provides the\nexperimental setup, with quantitative evaluation in Sect. 5\nand qualitative evaluation in Sect. 6. Finally, we conclude\nthe paper in Sect. 7 by discussing our ﬁndings and future\nwork.\n2 Related Work\nTo understand the sign language computational research\nlandscape, we ﬁrst outline the recent literature in sign lan-\nguage recognition (SLR) and SLT and then detail previous\nwork in SLP . Sign languages reside at the intersection\nbetween vision and language, so we also review recent devel-\nopments in neural machine translation (NMT). Finally, we\nprovide background on the applications of adversarial train-\ning and mixture density networks (MDNs) to sequence tasks,\nspeciﬁcally applied to human pose generation.\n2.1 Sign Language Recognition and Translation\nThe goal of vision-based sign language research is to develop\nsystems capable of recognition, translation and production of\nsign languages (Bragg et al. 2019). There has been promi-\nnent sign language computational research for over 30 years\n(Bauer et al. 2000; Starner and Pentland 1997; Tamura and\nKawasaki 1988), with an initial focus on isolated sign recog-\nnition (Grobel and Assan 1997; Ozdemir et. al 2016) and a\nrecent expansion to Continuous Sign Language Recognition\n(CSLR) (Camgoz et al. 2017; Chai et al. 2013; Koller et al.\n2015). However, the majority of work has relied on manual\nfeature representations (Cooper et al. 2012) and statistical\ntemporal modelling (V ogler and Metaxas 1999).\nRecently, larger sign language datasets have been released,\nsuch as RWTH-PHOENIX-Weather- 2014 (PHOENIX14)\n(Forster et al. 2014), Greek sign language (GSL) (Adaloglou\n2019) and the Chinese Sign Language Recognition Dataset\n(Huang et al. 2018). These have enabled the application of\ndeep learning approaches to CSLR, such as convolutional\nneural networks (CNNs) (Koller et al. 2016, 2019) and recur-\nrent neural networks (RNNs) (Cui et al. 2017; Koller et al.\n2017).\nExpanding upon CSLR, Camgoz et al. ( 2018) introduced\nthe task of SLT, aiming to directly translate sign videos to spo-\nken language sentences. Due to the differing grammar and\nordering between sign and spoken language (Stokoe 1980),\nSLT is a more challenging task than CSLR. The majority of\nwork has utilised NMT networks for SLT (Camgoz 2018;\nKo et al. 2019; Orbay and Akarun 2020;Y i n 2020), translat-\ning directly to spoken language or via a gloss intermediary.\nTransformer based models are the current state-of-the-art in\nSLT, jointly learning the recognition and translation tasks\n(Camgoz et al. 2020b). The inclusion of multi-channel fea-\ntures have also been shown to reduce the dependence on gloss\nannotation in SLT (Camgoz et al. 2020a).\n2.2 Sign Language Production\nPrevious research into SLP has focused on avatar-based tech-\nniques that generate realistic-looking sign production, but\nrely on pre-recorded phrases that are expensive to create\n(Ebling and Huenerfauth 2015; Glauert et al. 2006; McDon-\nald et al. 2016; Zwitserlood et al. 2004). Non-manual feature\nproduction has been included in avatar generation, such as\nmouthings (Elliott et al. 2008) and head positions (Cox et al.\n2002), but have been viewed as “stiff and emotionless” with\nan “absense of mouth patterns” (Kipp et al. 2011b). MoCap\napproaches have successfully produced realistic productions,\nbut are expensive to scale (Pengfei and Huenerfauth 2010).\nStatistical Machine Translation (SMT) has also been applied\nto SLP (Kayahan and Gungor 2019; Kouremenos et al. 2018),\nrelying on rules-based processing that can be difﬁcult to\nencode.\nRecently, there has been an increase in deep learning\napproaches to automatic SLP (Stoll 2020; Xiao et al. 2020;\nZelinka and Kanis 2020). Stoll et al. ( 2020) presented a SLP\nmodel that used a combination of NMT and generative adver-\nsarial networks (GANs). The authors break the problem into\nthree independent processes trained separately, producing a\nconcatenation of isolated 2D skeleton poses mapped from\nsign glosses via a look-up table. As seen with other works,\nthis production of isolated signs of a set length and order\nwithout realistic transitions results in robotic animations that\nare poorly received by the Deaf (Bragg et al. 2019). Contrary\nto Stoll et al. our work focuses on automatic sign production\nand learning the mapping between text and skeleton pose\nsequences directly, instead of providing this a priori.\nThe closest work to this paper is that of Zelinka and Kanis\n(2020), who use a neural translator to synthesise skeletal pose\n123\n2116 International Journal of Computer Vision (2021) 129:2113–2135\nfrom text. A single 7-frame sign is produced for each input\nword, generating sequences with a ﬁxed length and order-\ning that disregards the natural syntax of sign language. In\ncontrast, our model allows a dynamic length of output sign\nsequence, learning the length and ordering of corresponding\nsigns from the data, whilst using a progress counter to deter-\nmine the end of sequence generation. Unlike Zelinka et al.\nwho work on a proprietary dataset, we produce results on the\npublicly available PHOENIX14T, providing a benchmark for\nfuture SLP research.\nPrevious deep learning-based SLP works produce solely\nmanual features, ignoring the important non-manuals that\nconvey crucial context and meaning. Mouthings, in particu-\nlar, are vital to the comprehension of most sign languages,\ndifferentiating signs that may otherwise be homophones. The\nexpansion to non-manuals is challenging due to the required\ntemporal coherence with manual features and the intricacies\nof facial movements. We expand production to non-manual\nfeatures by generating synchronised mouthings and facial\nmovements from a single model, for expressive and natural\nsign production.\n2.3 Neural Machine Translation\nNMT is the automatic translation from a source sequence to\na target sequence of a differing language, using neural net-\nworks. To tackle this sequence-to-sequence task, RNNs were\nintroduced by Cho et al. ( 2014), which iteratively apply a\nhidden state computation across each token of the sequence.\nThis was later developed into encoder-decoder architectures\n(Sutskever et al. 2014), which map both sequences to an inter-\nmediate embedding space. Encoder model have the drawback\nof a ﬁxed sized representation of the source sequence. This\nproblem was overcome by an attention mechanism that facil-\nitated a soft-search over the source sentence for the most\nuseful context (Bahdanau et al. 2015).\nTransformer networks were recently proposed by V aswani\net al. ( 2017), achieving state-of-the-art performance in many\nNMT tasks. Transformers use self-attention mechanisms to\ngenerate representations of entire sequences with global\ndependencies. Multi-headed attention (MHA) layers are used\nto model different weighted combinations of each sequence,\nimproving the representational power of the model. A map-\nping between the source and target sequence representations\nis created by an encoder-decoder attention, learning the\nsequence-to-sequence task.\nTransformers have achieved impressive results in many\nclassic natural language processing (NLP) tasks such as lan-\nguage modelling (Dai et al. 2019; Zhang et al. 2019) and\nsentence representation (Devlin et al. 2018), alongside other\ndomains including image captioning (Zhou et al. 2018) and\naction recognition (Girdhar et al. 2019). Related to this work,\ntransformer networks have been applied to many continuous\noutput tasks such as speech synthesis (Ren et la. 2019b),\nmusic production (Huang et al. 2018) and speech recogni-\ntion (Povey et al. 2018).\nApplying sequence-to-sequence methods to continuous\noutput tasks is a relatively underresearched problem. In order\nto determine sequence length of continuous outputs, previous\nworks have used a ﬁxed output size (Zelinka and Kanis 2020),\na binary end-of-sequence (EOS) ﬂag (Graves 2013) or a con-\ntinuous representation of an EOS token (Mukherjee et al.\n2019). We propose a novel counter decoding technique that\npredicts continuous sequences of variable length by tracking\nthe production progress over time and implicitly learning the\nend of sequence.\n2.4 Adversarial Training\nAdversarial training is the inclusion of a discriminator model\ndesigned to improve the realism of a generator by critiquing\nthe productions (Goodfellow et al. 2014). GANs, which\ngenerate data using adversarial techniques, have produced\nimpressive results when applied to image generation (Isola\net al. 2017; Radford et al. 2015; Zhu et al. 2017) and, more\nrecently, video generation tasks (Tulyakov et al. 2018; V on-\ndrick et al. 2016). Conditional GANs (Mirza and Osindero\n2014) extended GANs with generation conditioned upon spe-\nciﬁc data inputs.\nGANs have also been applied to natural language tasks\n(Kevin et al. 2017; Press et al. 2017; Zhang et al. 2016).\nSpeciﬁc to NMT, Wu et al. ( 2017) designed Adversarial-\nNMT, complimenting the original NMT model with a CNN\nbased adversary, and Yang et al. ( 2017) proposed a GAN\nsetup with translation conditioned on the input sequence.\nSpeciﬁc to human pose generation, adversarial discrim-\ninators have been used for the production of realistic pose\nsequences (Cai et al. 2018; Chan et al. 2019; Ren et al.\n2019a). Ginosar et al. ( 2019) show that the task of generat-\ning skeleton motion suffers from regression to the mean, and\nadding an adversarial discriminator can improve the realism\nof gesture production. Lee et al. ( 2019) use a conditioned\ndiscriminator to produce smooth and diverse human danc-\ning motion from music. In this work, we use a conditional\ndiscriminator to produce expressive sign pose outputs from\nsource spoken language.\n2.5 Mixture Density Networks\nMixture density networks (MDNs) create a multimodal pre-\ndiction to better model distributions that may not be modelled\nfully by a single density distribution. MDNs combine a\nconventional neural network with a mixture density model,\nmodelling an arbitrary conditional distribution via a direct\nparametrisation (Bishop 1994). The neural network estimates\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2117\nthe density components, predicting the weights and statistics\nof each distribution.\nMDNs are often used for continuous sequence genera-\ntion tasks due to their ability to model sequence uncertainty\n(Schuster 2000). Graves et al. ( 2013) combined an MDN with\na RNN for continuous handwriting generation, which has\nbeen expanded to sketch generation (Ha and Eck 2018; Zhang\net al. 2017) and reinforcement learning (Ha and Schmidhu-\nber 2018). MDNs have also been applied to speech synthesis\n(Wang et al. 2017), future prediction (Makansi et al. 2019)\nand driving prediction (Hu et al. 2018).\nMDNs have also been used for human pose estimation,\neither to predict multiple hypotheses (Chen and Hee 2019),\nto better model uncertainty (Prokudin et al. 2018; V aramesh\nand Tuytelaars 2020) or to deal with occlusions (Ye and Kim\n2018). To the best of our knowledge, this work is the ﬁrst to\ncombine transformers with MDNs for sequence modelling.\nWe employ MDNs to capture the natural variability in sign\nlanguages and to model production using multiple distribu-\ntions.\n3 Continuous 3D Sign Language Production\nIn this section, we introduce our SLP model, which learns\nto translate spoken language sentences to continuous sign\npose sequences. Our objective is to learn the conditional\nprobability p(Y |X) of producing a sequence of signs Y =\n(y\n1,..., yU ) with U frames, given a spoken language sen-\ntence X = (x1,..., xT ) with T words. Glosses could also be\nused as source input, replacing the spoken language sentence\nas an intermediary. In this work we represent sign language\nas a sequence of continuous skeleton poses modelling the\n3D coordinates of a signer, of both manual and non-manual\nfeatures.\nProducing a target sign language sequence from a ref-\nerence spoken language sentence poses several challenges.\nFirstly, there exists a non-monotic relationship between spo-\nken and sign language, due to the different grammar and\nsyntax in the respective domains (Stokoe 1980). Secondly,\nthe target signs inhabit a continuous vector space, requiring\na differing representation to the discrete space of text and\ndisabling the use of classic end of sequence tokens. Finally,\nthere are multiple channels encompassed within sign that\nmust be produced concurrently, such as the manual (hand\nshape and position) and non-manual features (mouthings and\nfacial expressions) (Pfau et al. 2010).\nTo address the production of continuous sign sequences,\nwe propose a Progressive Transformer model that enables\ntranslation from a symbolic to a continuous sequence domain\n(PT in Fig. 2). We introduce a counter decoding that enables\nthe model to track the progress of sequence generation and\nimplicitly learn sequence length given a source sentence.\nWe also propose several data augmentation techniques that\nreduce the impact of prediction drift.\nTo enable the production of expressive sign, we intro-\nduce an adversarial training regime for SLP , supplementing\nthe progressive transformer generator with a conditional\nadversarial discriminator, (Disc in Fig. 2). To enhance the\ncapability to model multimodal distributions, we also pro-\npose a MDN formulation of the SLP network. In the\nremainder of this section we describe each component of\nthe proposed architecture in detail.\n3.1 Progressive Transformer\nWe build upon the classic transformer (V aswani et al. 2017),\na model designed to learn the mapping between symbolic\nsource and target languages. We modify the architecture to\ndeal with continuous output representations such as sign lan-\nguage, alongside introducing a counter decoding technique\nthat enables sequence prediction of variable lengths. Our SLP\nmodel tracks the progress of continuous sequence production\nthrough time, hence the name Progressive Transformer.\nIn this work, Progressive Transformers translate from the\nsymbolic domains of gloss or spoken language to contin-\nuous 3D sign pose sequences. These sequences represent\nthe motion of a signer producing a sign language sentence.\nThe model must produce sign pose outputs that express an\naccurate translation of the given input sequence and embody\na realistic sign pose sequence. Our model consists of an\nencoder-decoder architecture, where the source sequence is\nﬁrst encoded to a latent representation before being mapped\nto a target output during decoding in an auto-regressive man-\nner.\n3.1.1 Source Embeddings\nAs per the standard NMT pipeline, we ﬁrst embed the\nsymbolic source tokens, x\nt , via a linear embedding layer\n(Mikolov et al. 2013). This represent the one-hot-vector in a\nhigher-dimensional space where tokens with similar mean-\nings are closer. This embedding, with weight, W , and bias,\nb, can be formulated as:\nw\nt = W x · xt + bx (1)\nwhere wt is the vector representation of the source tokens.\nAs with the original transformer implementation, we\napply a temporal encoding layer after the source embed-\nding, to provide temporal information to the network. For\nthe encoder, we apply positional encoding, as:\nˆw\nt = wt + PositionalEncoding(t) (2)\n123\n2118 International Journal of Computer Vision (2021) 129:2113–2135\nFig. 2 Architecture details of our Progressive Transformer and Condi-\ntional Discriminator network. The Progressive Transformer produces\na sign pose sequence, ˆy1:U , and respective counter values, ˆc1:U , from\nsource spoken language, ˆx1:T , in an auto-regressive prediction. The\nConditional Discriminator takes as input either ground-truth or pro-\nduced sign pose sequences alongside the respective source spoken\nlanguage, and predicts a single realism scalar, dp. The network is\ntrained end-to-end via a weighted combination of regression loss, Lreg ,\nand adversarial loss, LGAN .( PT progressive transformer, PE positional\nencoding, CE counter encoding, Disc discriminator)\nwhere PositionalEncoding is a predeﬁned sinusoidal function\nconditioned on the relative sequence position t (V aswani et al.\n2017).\n3.1.2 Target Embeddings\nThe target sign sequence consists of 3D joint positions of\nthe signer. Due to their continuous nature, we ﬁrst apply\na novel temporal encoding, which we refer to as counter\nencoding (CE in Fig. 2). The counter, c, holds a value between\n0 and 1, representing the frame position relative to the total\nsequence length. The target joints, yu , are concatenated with\nthe respective counter value, cu , formulated as:\nju =[ yu ,cu ] (3)\nwhere cu is the counter value for frame u, as a proportion\nof sequence length, U. At each time-step, counter values,\nˆc, are predicted alongside the skeleton pose, as shown in\nFig. 3, with sequence generation concluded once the counter\nreaches 1. We call this process Counter Decoding, determin-\ning the progress of sequence generation and providing a way\nto predict the end of sequence without the use of a tokenised\nvocabulary.\nThe counter value provides the model with informa-\ntion relating to the length and speed of each sign pose\nsequence, determining the sign duration. At inference, we\ndrive the sequence generation by replacing the predicted\ncounter value, ˆc, with the linear timing information, c\n∗,t o\nproduce a stable output sequence.\nThese counter encoded joints, ju , are next passed through\na linear embedding layer, which can be formulated as:\nˆju = W y · ju + by (4)\nwhere ˆju is the embedded 3D joint coordinates of each frame,\nyu .\n3.1.3 Encoder\nThe progressive transformer encoder, EPT , consists of a\nstack of L identical layers, each containing 2 sub-layers.\nGiven the temporally encoded source embeddings, ˆwt ,a\nMHA sub-layer ﬁrst generates a weighted contextual rep-\nresentation, performing multiple projections of scaled dot-\nproduct attention. This aims to learn the relationship between\neach token of the sequence and how relevant each time step\nis in the context of the full sequence. Formally, scaled dot-\nproduct attention outputs a vector combination of values, V ,\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2119\nFig. 3 Counter decoding example, showing the simultaneous auto-regressive prediction of continuous sign pose, ˆyu , and counter value, ˆcu ∈{ 0 : 1}.\nA counter value of 1, ˆc = 1.0, denotes end of sequence and decoding is stopped\nweighted by the relevant queries, Q,k e y s , K , and dimen-\nsionality, dk :\nAttention(Q, K , V ) = softmax\n( QK T\n√dk\n)\nV (5)\nMHA uses multiple self-attention heads, h, to generate\nparallel mappings of the same queries, keys and values,\neach with varied learnt parameters. This allows different\nrepresentations of the input sequence to be generated, learn-\ning complementary information in different sub-spaces. The\noutputs of each head are then concatenated together and pro-\njected forward via a ﬁnal linear layer, as:\nMHA(Q, K , V ) =[ head\n1, ...,head h ]· W O ,\nwhere · headi = Attention(QW Q\ni , KW K\ni , VW V\ni ) (6)\nand W O ,W Q\ni ,W K\ni and W V\ni are weights related to each input\nvariable.\nThe outputs of MHA are then fed into a second sub-layer of\na non-linear feed-forward projection. A residual connection\n(He et al. 2016) and subsequent layer norm (Ba et al. 2016)\nis employed around each of the sub-layers, to aid training.\nThe ﬁnal encoder output can be formulated as:\nh\nt = EPT ( ˆwt |ˆw1:T ) (7)\nwhere ht is the contextual representation of the source\nsequence.\n3.1.4 Decoder\nThe progressive transformer decoder ( DPT ) is an auto-\nregressive model that produces a sign pose frame at each\ntime-step, alongside the previously described counter value.\nDistinct from symbolic transformers, our decoder produces\ncontinuous sequences.\nThe counter-concatenated joint embeddings, ˆj\nu , are used\nto represent the sign pose of each frame. Firstly, an initial\nMHA sub-layer is applied to the joint embeddings, similar to\nthe encoder but with an extra masking operation. The mask-\ning of future frames prevents the model from attending to\nsubsequent time steps that are yet to be decoded.\nA further MHA mechanism is then used to map the sym-\nbolic representations from the encoder to the continuous\ndomain of the decoder. A ﬁnal feed forward sub-layer fol-\nlows, with each sub-layer followed by a residual connection\nand layer normalisation as in the encoder. The output of the\nprogressive decoder can be formulated as:\n[ˆy\nu , ˆcu ]= DPT ( ˆj1:u−1, h1:T ) (8)\nwhere ˆyu corresponds to the 3D joint positions representing\nthe produced sign pose of frame u and ˆcu is the respective\ncounter value. The decoder learns to generate one frame at a\ntime until the predicted counter value, ˆcu , reaches 1, deter-\nmining the end of sequence as seen in Fig. 3. The model is\ntrained using the mean squared error (MSE) loss between the\npredicted sequence, ˆy1:U , and the ground truth, y∗\n1:U :\nL MSE = 1\nU\nu∑\ni=1\n(y∗\n1:U −ˆy1:U )2 (9)\n123\n2120 International Journal of Computer Vision (2021) 129:2113–2135\n(a) (b) (c)\nFig. 4 Data augmentation techniques to reduce prediction drift and create a more robust SLP model. a Future prediction is the prediction of multiple\nfuture frames. b Just counter uses only the counter positions as input. c Gaussian noise applies noise to the input skeleton pose. ( PT progressive\ntransformer)\nAt inference time, the full sign pose sequence, ˆy1:U ,i s\nproduced in an auto-regressive manner, with predicted sign\nframes used as input to future time steps. Once the predicted\ncounter value reaches 1, decoding is complete and the full\nsign sequence is produced.\n3.2 Data Augmentation\nAuto-regressive sequential prediction can often suffer from\nprediction drift, with erroneous predictions accumulating\nover time. As transformer models are trained to predict the\nnext time-step using ground truth inputs, they are often not\nrobust to noise in predicted inputs. The impact of drift is\nheightened for an SLP model due to the continuous nature of\nskeleton poses. As neighbouring frames differ little in con-\ntent, a model can learn to just copy the previous ground truth\ninput and receive a small loss penalty.\nAt inference time, with predictions based off previous out-\nputs, errors are quickly propagated throughout the entire sign\nsequence production. To overcome the problem of prediction\ndrift, in this section we propose various data augmentation\napproaches, namely Future Prediction , Just Counter and\nGaussian Noise .\n3.2.1 Future Prediction\nOur ﬁrst data augmentation method is conditional future pre-\ndiction, requiring the model to predict more than just the next\nframe in the sequence. Figure 4a shows an example future\nprediction of y\nu+1,..., yu+t from the input y1:u . Due to the\nshort time step between neighbouring frames, the movement\nbetween frames is small and the model can learn to just pre-\ndict the previous frame with some noise. Predicting more\nframes into the future means the movement of sign has to\nbe learnt, rather than simply copying the previous frame. At\ninference time, only the next frame prediction is considered\nfor production.\n3.2.2 Just Counter\nInspired by the memorisation capabilities of transformer\nmodels, we next propose a pure memorisation approach to\nsign production. Contrary to the usual input of full skeleton\njoint positions, only the counter values are provided as target\ninput. Figure 4b demonstrates the input of c\n1:u as opposed to\ny1:u . The model must decode the target sign pose sequence\nsolely from the counter positions, having no knowledge of\nthe previous frame positions. This halts the reliance on the\nground truth joint embeddings it previously had access to,\nforcing a deeper understanding of the source spoken lan-\nguage and a more robust production. The network setup is\nalso now identical at both training and inference, with the\nmodel having to generalise only to new data rather than new\nprediction inputs.\n3.2.3 Gaussian Noise\nOur ﬁnal augmentation technique is the application of noise\nto the input sign pose sequences during training, increasing\nthe variety of data. This is shown in Fig. 4c, where the input\ny\n1:u is summed with noise ϵ1:u . At each epoch, distribution\nstatistics of each joint are collected, with randomly sampled\nnoise applied to the inputs of the next epoch. The addition\nof Gaussian noise causes the model to become more robust\nto prediction input error, as it must learn to correct the aug-\nmented inputs back to the target outputs. At inference time,\nthe model is more used to noisy inputs, increasing the ability\nto adapt to erroneous predictions and correct the sequence\ngeneration.\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2121\nFig. 5 An average of multiple\nvalid sign poses (blurred) results\nin an under-articulated\nproduction due to the problem\nof regression to the mean\n3.3 Adversarial Training\nSign languages contain naturally varied movements, as each\nsigner produces sign sequences with slightly different articu-\nlations and movements. Realistic sign consists of subtle and\nprecise movements of the full body, which can easily be lost\nwhen training solely to minimise joint error [e.g. Eq. ( 9)].\nSLP models trained solely for regression can lack pose artic-\nulation, suffering from the problem of regression to the mean.\nSpeciﬁcally, average hand shapes are produced with a lack\nof comprehensive motion, due to the high variability of these\njoints. Figure 5 highlights this problem, as the average of\nthe valid blurred poses results in an under-articulated mean\nproduction that does not convey the required meaning.\nTo address under-articulation, we propose an adversarial\ntraining mechanism for SLP . As shown in Fig. 2, we introduce\na conditional discriminator, D, alongside the SLP generator,\nG. We frame SLP as a min-max game between the two net-\nworks, with D evaluating the realism of G’s productions. We\nuse the previously described progressive transformer archi-\ntecture as G (Fig. 2 left) to produce sign pose sequences. We\nbuild a convolutional network for D (Fig. 6), trained to pro-\nduce a single scalar that represents realism, given a sign pose\nsequence and corresponding source input sequence. These\nmodels are co-trained in an adversarial manner, which can\nbe formalised as:\nmin\nG\nmax\nD\nLGAN (G, D)\n= E[log D(Y ∗ | X)]+ E[log(1 − D(G(X) | X))] (10)\nwhere Y ∗ is the ground truth sign pose sequence, y∗\n1:U , G(X)\nequates to the produced sign pose sequence, ˆY =ˆy1:U , and\nX is the source spoken language.\n3.3.1 Generator\nOur generator, G, learns to produce sign pose sequences\ngiven a source spoken language sequence, integrating the\nprogressive transformer into a GAN framework. Contrary\nto the standard GAN implementation, we require sequence\ngeneration to be conditioned on a speciﬁc source input.\nTherefore, we remove the traditional noise input (Goodfellow\net al. 2014), and generate a sign pose sequence conditioned\non the source sequence, taking inspiration from conditional\nGANs (Mirza and Osindero 2014).\nWe propose training G using a combination of loss func-\ntions, namely regression loss, L\nReg ,[ E q .(9)] and adversarial\nloss, LG\nGAN ,[ E q .( 10)]. The total loss function is a weighted\ncombination of these losses, as:\nLG = λReg L Reg (G) + λGAN LG\nGAN (G, D) (11)\nwhere λReg and λGAN determine the importance of each loss\nfunction during training.\n3.3.2 Discriminator\nWe present a conditional adversarial discriminator, D,u s e dt o\ndifferentiate generated sign pose sequences, ˆY , and ground-\ntruth sign pose sequences, Y ∗, conditioned on the source\nspoken language sequence, X. Figure 6 shows an overview\nof the discriminator architecture.\nFor each pair of source-target sequences, (X, Y ), of either\ngenerated or real sign pose, the aim of D is to produce a single\nscalar, dp ∈ (0,1). This represents the probability that the\nsign pose sequence originates from the data, Y ∗:\ndp = P(Y = Y ∗ | X, Y ) ∈ (0,1) (12)\nThe sequence counter value is removed before being input to\nthe discriminator, in order to critique only the sign content.\nDue to the variable frame lengths of the sign sequences, we\napply padding to transform them to a ﬁxed length, U\nmax ,t h e\nmaximum frame length of target sequences found in the data:\nYpad =[ Y1:U ,∅U:Umax ] (13)\nwhere Ypad is the sign pose sequence padded with zero vec-\ntors, ∅, enabling convolutions upon the now ﬁxed size tensor.\nIn order to condition D on the source spoken language, we\nﬁrst embed the source tokens via a linear embedding layer.\nAgain to deal with variable sequence length, these embed-\ndings are also padded to a ﬁxed length T\nmax , the maximum\nsource sequence length:\nX pad =[ W X · X1:T + bX ,∅T :Tmax ] (14)\nwhere W X and bX are the weight and bias of the source\nembedding respectively and ∅ is zero padding. As shown in\nthe centre of Fig. 6, the source representation is then con-\ncatenated with the padded sign pose sequence, to create the\nconditioned features, H:\nH =[ Y\npad , X pad ] (15)\nN 1D convolutional ﬁlters are passed over the sign pose\nsequence, analysing the local context to determine the tem-\nporal continuity of the signing motion. This is more effective\n123\n2122 International Journal of Computer Vision (2021) 129:2113–2135\nFig. 6 Architecture details of our conditional discriminator model. Sign pose, Y1:U , is concatenated with source text, X1:T , and projected to a single\nscalar, dp, that represents the realism of the sign pose sequence\nthan a frame level discriminator at determining realism,\nas a mean hand shape is a valid pose for a single frame,\nbut not consistently over a large temporal window. Leaky\nReLU activation (Maas et al. 2013) is applied after each\nlayer, promoting healthy gradients during training. A ﬁnal\nfeed-forward linear layer and sigmoid activation projects the\ncombined features down to the single scalar, d\np, representing\nthe probability that the sign pose sequence is real.\nWe train D to maximise the likelihood of producing\ndp = 1 for real sign sequences and dp = 0 for generated\nsequences. This objective can be formalised as maximising\nEq. ( 10), resulting in the loss function L D = L D\nGAN (G, D).\nAt inference time, D is discarded and G is used to pro-\nduce sign pose sequences in an auto-regressive manner as\nin Sect. 3.1.\n3.4 Mixture Density Networks\nThe previously-described model architectures generate deter-\nministic productions, with each model predicting a single\nnon-stochastic pose at each time step. A single prediction is\nunable to model any uncertainty or variation that is found in\ncontinuous sequence generation tasks like SLP . The deter-\nministic modelling of sequences can again result in a mean,\nunder-articulated production with no room for expression or\nvariability.\nTo overcome the issues of deterministic prediction, we\npropose the use of a mixture density network (MDN) to\nmodel the variation found in sign language. As shown in\nFig. 7, multiple distributions are used to parameterise the\nentire prediction subspace, with each mixture component\nmodelling a separate valid movement into the future. This\nenables prediction of all valid signing motions and their\ncorresponding uncertainty, resulting in a more expressive\nproduction.\n3.4.1 Formulation\nMDNs use a neural network to parameterise a mixture distri-\nbution (Bishop 1994). A subset of the network predicts the\nmixture weights whilst the rest generates the parameters of\neach of the individual mixture distributions. We use our pre-\nviously described progressive transformer architecture, but\namend the output to model a mixture of Gaussian distribu-\ntions. Given a source token, x\nt , we can model the conditional\nprobability of producing the sign pose frame, yu ,a s :\np(yu |xt ) =\nM∑\ni=1\nαi (xt )φi (yu |xt ) (16)\nwhere M is the number of mixture components used in the\nMDN. αi (xt ) is the mixture weight of the ith distribution,\nregarded as a prior probability of the sign pose frame being\ngenerated from this mixture component. φi (yu |xt ) is the con-\nditional density of the sign pose for the ith mixture, which\ncan be expressed as a Gaussian distribution:\nφi (yu |xt ) = 1\nσi (xt )\n√\n2π\nexp\n∥ yu −μi (xt )∥2\n2σi (xt )2 (17)\nwhere μi (xt ) and σi (xt ) denote the mean and variance of the\nith distribution, respectively. The parameters of the MDN are\npredicted directly by the progressive transformer, as shown\nin Fig. 7. The mixture coefﬁcients, α(xt ), are passed through\na softmax activation function to ensure each lies in the range\n[0,1] and sum to 1. An exponential function is applied to the\nvariances, σ(x\nt ), to ensure a positive output.\n3.4.2 Optimisation\nDuring training, we minimise the negative log likelihood of\nthe ground truth data coming from our predicted mixture\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2123\nFig. 7 An overview of our Mixture Density Network (MDN) network.\nMultiple mixture distributions, m, are parameterised by the progres-\nsive transformer (PT) outputs, taking input source spoken language and\nprevious sign pose frames. An output sign pose is sampled from the\nmixture distributions, producing an expressive and variable sign lan-\nguage sequence. The network is trained end-to-end with a negative log\nlikelihood, L\nMDN\ndistribution. This can be formulated as:\nL MDN =−\nU∑\nu=1\nlog p(yu |xt )\n=−\nU∑\nu=1\nlog\nM∑\ni=1\nαi (xt )φi (yu |xt ) (18)\nwhere U is the number of frames in the produced sign pose\nsequence and M is the number of mixture components.\n3.4.3 Sampling\nAt inference time, we sample sign pose productions from the\nmixture density computed in Eq. ( 16), as shown in Fig. 7.\nFirstly, we select the most likely distribution for this source\ntoken, xt , from the mixture weights, imax = argmax i αi (xt ).\nFrom this chosen distribution, we sample the sign pose, pre-\ndicting μ\nimax (xt ) as a valid pose. To ensure there is no jitter in\nthe sign pose predictions, we set σ(xt ) = 0. This avoids the\nlarge variation in small joint positions a large sigma would\ncreate, particularly for the hands.\nTo predict a sequence of multiple time steps, we sample\neach frame from the mixture density model in an auto-\nregressive manner as in Sect. 3.1. The sampled sign frames\nare used as input to future transformer time-steps, to produce\nthe full sign pose sequence, ˆy\n1:U .\n3.4.4 MDN + Adversarial\nThe MDN can also be combined with our adversarial training\nregime outlined in Sect. 3.3. The MDN model is formulated\nas the adversarial generator pitched against an unchanged\nconditional discriminator, where a sampled sign pose is\nused as discriminator input. Again, the ﬁnal loss function\nis a weighted combination of the negative log-posterior loss\n[Eq. ( 18)] and the adversarial generator loss [Eq. ( 10)], as:\nL\nG\nMDN = λMDN L MDN (G) + λGAN LG\nGAN (G, D) (19)\nAt inference time, the discriminator model is discarded and\na sign pose sequence is sampled from the resulting mixture\ndistribution, as previously explained.\n3.5 Sign Pose Sequence Outputs\nEach of these model conﬁgurations are trained to produce\na sign pose sequence, ˆy1:U , given a source spoken language\ninput, x1:T . Animating a video from this skeleton sequence is\na trivial task, plotting the joints and connecting the relevant\nbones, with timing information provided from the progres-\nsive transformer counter. These 3D joints can subsequently\nbe used to animate an avatar (Kipp et al. 2011a; McDonald\net al. 2016) or condition a GAN ( Chan et al. 2019).\nEven though the produced sign pose sequence is a valid\ntranslation of the given text, it may be signed at a different\nspeed than that found in the reference data. This is not incor-\nrect, as every signer signs with a varied motion and speed,\nwith our model having its own cadence. However, in order\nto ease the visual comparison with reference sequences, we\napply dynamic time warping (DTW) (Berndt and Clifford\n1994) to temporally align the produced sign pose sequences.\nThis action does not amend the content of the productions,\nonly the temporal coherence for visualisation.\n123\n2124 International Journal of Computer Vision (2021) 129:2113–2135\nFig. 8 Skeleton pose extraction, using 2D human pose estimation (Cao\net al. 2017) and 2D to 3D mapping (Zelinka and Kanis 2020)\nAlthough our focus has not been on building a real-time\nsystem, our current implementation is near real-time and a\nspoken language sentence can be translated to a sign language\nvideo within seconds. However, the nature of translation\nrequires a delay as the context of a whole sentence is needed\nbefore it can be translated. As such, the small delay intro-\nduced by the automatic system does not present a signiﬁcant\nfurther delay.\n4 Experimental Setup\nIn this section, we outline our experimental setup, detail-\ning the dataset, evaluation metrics and model conﬁguration.\nWe also introduce the back translation evaluation metric and\nevaluation protocols.\n4.1 Dataset\nIn this work, we use the publicly available PHOENIX14T\ndataset introduced by Camgoz et al. ( 2018), a continuous\nSLT extension of the original PHOENIX14 corpus (Forster\net al. 2014), becoming the benchmark for SLT research. This\ncorpus includes parallel German Sign Language—Deutsche\nGebärdensprache (DGS) videos and German translation\nsequences with redeﬁned segmentation boundaries generated\nusing the forced alignment approach of Koller et al. ( 2016).\n8257 videos of 9 different signers are provided, with a vocab-\nulary of 2887 German words and 1066 different sign glosses.\nWe use the original training, validation and testing split as\nproposed by Camgoz et al. ( 2018).\nWe train our SLP network to generate sequences of 3D\nskeleton pose representing sign language, as shown in Fig. 8.\n2D upper body joint and facial landmark positions are ﬁrst\nextracted using OpenPose (Cao et al. 2017). We then use\nthe skeletal model estimation improvements presented in\nZelinka and Kanis ( 2020) to lift the 2D upper body joint\npositions to 3D. Finally, we apply skeleton normalisation\nsimilar to Stoll et al. ( 2020), with face coordinates scaled to\na consistent size and centered around the nose joint.\nTable 1 Ground-truth back translation results for Manual, Non-Manual\nand Manual + Non-Manual skeleton pose representations\nDEV SET TEST SET\nRepresentation BLEU-4 BLEU-4\nManual 11.05 9.97\nNon-Manual 8.65 9.18\nManual + Non-Manual 11.44 11.01\n4.2 Back Translation Evaluation\nThe evaluation of a continuous sequence generation model\nis a difﬁcult task, with previous SLP evaluation metrics\nof MSE (Zelinka and Kanis 2020) falling short of a true\nmeasure of sign understanding. In this work, we propose\nback-translation as a means of SLP evaluation, translat-\ning back from the produced sign pose sequences to spoken\nlanguage. This provides an automatic measure of how under-\nstandable the productions are, and the amount of translation\ncontent that is preserved. We ﬁnd a close correspondence\nbetween back translation score and the visual production\nquality and liken it to the wide use of the inception score for\ngenerative models which uses a pre-trained classiﬁer (Sal-\nimans et al. 2016). Similarly, recent SLP work has used\nan SLR discriminator to evaluate isolated skeletons (Xiao\net al. 2020), but did not measure the translation perfor-\nmance. Back translation is a relative evaluation metric, best\nused to compare between similar model conﬁgurations. If\nthe chosen SLT model is amended, absolute model perfor-\nmances will likely also change. However, as we have seen in\nour experimentation, the relative performance comparisons\nbetween models remain consistent. This ensures that com-\nparison results between models remains valid.\nWe use the state-of-the-art SLT system (Camgoz et al.\n2020b) as our back translation model, modiﬁed to take sign\npose sequences as input. We build a sign language trans-\nformer model with 1 layer, 2 heads and an embedding size\nof 128. This is also trained on the PHOENIX14T dataset,\nensuring a robust translation from sign to text. We gener-\nate spoken language translations of the produced sign pose\nsequences and compute BLEU and ROUGE scores. We pro-\nvide BLEU n-grams from 1 to 4 for completeness.\nWe build multiple SLT models trained with various\nskeleton pose representations, namely Manual (Body), Non-\nManual (Face) and Manual + Non-Manual . We evaluate the\nback translation performance for each conﬁguration, to see\nhow understandable the representation is and the amount of\nspoken language that can be recovered. As seen in Table 1,t h e\nManual + Non-Manual conﬁguration achieves the best back\ntranslation result, with Non-Manual achieving a signiﬁcantly\nlower result. This demonstrates that manual and non-manual\nfeatures contain complementary information when translat-\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2125\nTable 2 Text to Gloss translation results of our transformer architecture, compared to that of Stoll et al. ( 2020)\nDEV SET TEST SET\nApproach BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nStoll et al. ( 2020) 16.34 22.30 32.47 50.15 48.42 15.26 21.54 32.25 50.67 48.10\nOurs 20.23 27.36 38.21 55.65 55.41 19.10 26.24 37.10 55.18 54.55\ning back to spoken language and supports our use of a\nmulti-channel sign pose representation.\nAs seen in our quantitative experiments in Sect. 5, our\nsign production sequences can achieve better back trans-\nlation performance than the original ground truth skeleton\ndata. We believe this is due to a smoothing of the training\ndata during production, as the original data contains artifacts\neither from 2D pose estimation, the 2D-to-3D mapping or\nthe quality of the data itself. As our model learns to generate\na temporally continuous production without these artifacts,\nour sign pose is signiﬁcantly smoother than the ground truth.\nThis explains the higher back translation performance from\nproduction compared to the ground truth data.\n4.3 Evaluation Protocols\nWith back translation as an evaluation metric, we now set SLP\nevaluation protocols on the PHOENIX14T dataset. These can\nbe used as measures for ablation studies and benchmarks for\nfuture work.\nT ext to Gloss (T2G): The ﬁrst evaluation protocol is the sym-\nbolic translation between spoken language and sign language\nrepresentation. This task is a measure of the translation into\nsign language grammar, an initial task before a pose produc-\ntion. This can be measured with a direct BLEU and ROUGE\ncomparison, without the need for back translation.\nGloss to Pose (G2P) : The second evaluation protocol evalu-\nates the SLPs models capability to produce a continuous sign\npose sequence from a symbolic gloss representation. This\ntask is a measure of the production capabilities of a network,\nwithout requiring translation from spoken language.\nT ext to Pose (T2P) : The ﬁnal evaluation protocol is full end-\nto-end translation from a spoken language input to a sign pose\nsequence. This is the true measure of the performance of an\nSLP system, consisting of jointly performing translation to\nsign and a production of the sign sequence. Success on this\ntask enables SLP applications in domains where expensive\ngloss annotation is not available.\n4.4 Model Configuration\nIn the following experiments, our progressive transformer\nmodel is built with 2 layers, 4 heads and an embedding size\nof 512, unless stated otherwise. All parts of our network\nare trained with Xavier initialisation from scratch (Glorot\nand Bengio 2010), Adam optimization with default param-\neters (Kingma and Ba 2014) and a learning rate of 10\n−3.\nWe use a plateau learning rate scheduler with a patience of\n7 epochs, a decay rate of 0.7 and a minimum learning rate\nof 2 × 10\n−4. Our code is based on Kreutzer et al. ’s NMT\ntoolkit, JoeyNMT ( 2019), and implemented using PyTorch\n(Paszke et al. 2017).\n5 Quantitative Evaluation\nIn this section, we present a thorough quantitative evaluation\nof our SLP model, providing results and subsequent discus-\nsion. We ﬁrst conduct experiments using the Text to Gloss\nsetup. We then evaluate the Gloss to Pose and the end-to-end\nText to Pose setups. Finally, we provide results of our user\nstudy with Deaf participants.\n5.1 Text to Gloss Translation\nTo provide a baseline, our ﬁrst experiment evaluates the per-\nformance of a classic transformer architecture (V aswani et al.\n2017) for the translation of spoken language to sign glosses\nsequences. We train a vanilla transformer model to predict\nsign gloss intermediary, with 2 layers, 8 heads and an embed-\nding size of 256. We compare our performance against Stoll\net al. (2020), who use an encoder-decoder network with 4 lay-\ners of 1000 Gated Recurrent Units (GRUs) as a translation\narchitecture.\nTable 2 shows that a transformer model achieves state-of-\nthe-art results, signiﬁcantly outperforming that of Stoll et al.\n(2020). This supports our use of the proposed transformer\narchitecture for sign language understanding.\n5.2 Gloss to Pose Production\nIn our next set of experiments, we evaluate our progressive\ntransformer on the Gloss to Pose task outlined in Sect. 4.3.\nAs a baseline, we train a progressive transformer model to\ntranslate from gloss to sign pose without augmentation.\n5.2.1 Data Augmentation\nOur base model suffers from prediction drift, with erroneous\npredictions accumulating over time. As transformer models\n123\n2126 International Journal of Computer Vision (2021) 129:2113–2135\nTable 3 Future prediction results on the Gloss to Pose task\nDEV SET TEST SET\nF f Ft BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\n0 (Base) 1 (Base) 7.38 9.62 13.81 25.03 26.55 7.13 9.30 13.63 24.86 26.03\n0 2 9.52 12.13 16.91 27.98 30.68 9.34 11.99 16.78 28.03 30.29\n05 11.30 14.17 19.19 30.45 33.18 10.69 13.49 18.68 30.69 31.78\n0 10 10.99 13.83 19.02 30.57 32.34 9.93 12.50 17.49 28.94 30.86\n0 20 10.08 12.84 17.79 29.30 31.27 9.23 12.02 17.27 29.53 30.11\n2 5 10.93 13.85 19.23 31.55 32.80 10.23 13.13 18.60 30.87 32.38\n5 10 10.32 13.07 18.44 30.95 31.81 9.37 12.12 17.53 30.39 30.52\nBold is used to signify the best performing model. Evaluation upon modifying the prediction frames, F f to Ft\nTable 4 Just counter results on the Gloss to Pose task\nDEV SET TEST SET\nConﬁguration BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nBase 7.38 9.62 13.81 25.03 26.55 7.13 9.30 13.63 24.86 26.03\nJust counter 12.34 15.04 21.17 32.43 35.59 12.16 15.50 21.45 33.53 34.80\nBold is used to signify the best performing model. Evaluation against a base architecture that uses full skeleton pose as input\nare trained to predict the next time-step, they are often not\nrobust to noise in the target input. Therefore, we experiment\nwith multiple data augmentation techniques introduced in\nSect. 3.2; namely Future Prediction, Just Counter and Gaus-\nsian Noise .\nFuture Prediction Our ﬁrst data augmentation method is\nconditional future prediction, requiring the model to predict\nmore than just the next frame in the sequence. The model\nis trained to produce future frames between F\nf and Ft .A s\ncan be seen in Table 3, prediction of multiple future frames\ncauses an increase in model performance, from a base level\nof 7.38 BLEU-4 to 11.30 BLEU-4. We believe this is because\nthe model cannot rely on just copying the previous frame to\nminimise the loss, but is instead required to predict the true\nmotion with future pose predictions.\nThere exists a trade-off between beneﬁt and complexity\nfrom increasing the number of predicted frames. We ﬁnd\nthe best performance comes from a prediction of 5 frames\nfrom the current time step. This is sufﬁcient to encourage\nforward planning and motion understanding, but without a\nlarge averse effect on model complexity.\nJust Counter Inspired by the memorisation capabilities of\ntransformer models, we next evaluate a pure memorisation\napproach. Only the counter values are provided as target\ninput to the model, as opposed to the usual full 3D skeleton\njoint positions. We show a further performance increase with\nthis approach, considerably increasing the BLEU-4 score as\nshown in Table 4.\nWe believe the just counter model helps to allay the effect\nof drift, as the model must learn to decode the target sign\npose solely from the counter position. It cannot rely on the\nground truth joint embeddings it previously had access to.\nThis halts the effect of erroneous sign pose prediction, as\nthey are no longer fed back into the model. The setup at\ntraining and inference is now identical, requiring the model\nto only generalise to new data.\nGaussianN o i s eOur ﬁnal augmentation evaluation examines\nthe effect of applying noise to the skeleton pose sequences\nduring training. For each joint, randomly sampled noise is\napplied to the input multiplied by a noise factor, r\nn, repre-\nsenting the degree of noise augmentation.\nTable5 shows that Gaussian Noise augmentation achieves\nstrong performance, with rn = 5 giving the best results so far\nof 12.80 BLEU-4. A small amount of input noise causes the\nmodel to become more robust to auto-regressive prediction\nerrors, as it must learn to correct the augmented inputs back\nto the target outputs. However, an increase of r\nn above 5\ncauses a large degradation, affecting the model training and\nsubsequent testing performance.\nOverall, the proposed data augmentation techniques have\nbeen shown to signiﬁcantly improve model performance and\nare fundamental to the production of understandable sign\npose sequences. In the rest of our experiments, we use Gaus-\nsian Noise augmentation with r\nn = 5.\n5.2.2 Adversarial Training\nWe next evaluate our adversarial training regime outlined in\nSect. 3.3. During training, a generator, G, and discriminator,\nD compete in a min-max game where G must create realistic\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2127\nTable 5 Gaussian noise results on the Gloss to Pose task\nDEV SET TEST SET\nrn BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\n0 (Base) 7.38 9.62 13.81 25.03 26.55 7.13 9.30 13.63 24.86 26.03\n1 9.77 12.41 17.15 28.47 31.09 9.41 12.14 17.36 29.32 31.16\n2 10.62 13.13 18.19 29.42 32.54 10.50 13.39 18.76 30.57 32.09\n5 12.80 16.03 21.60 33.56 35.86 11.85 15.16 21.56 34.56 35.31\n10 12.14 15.26 20.78 32.21 34.77 11.75 15.01 21.26 33.90 34.33\n20 12.19 15.54 21.50 33.69 35.42 11.56 14.89 20.91 33.44 34.81\nBold is used to signify the best performing model. Evaluation upon modifying the noise rate, rn\nsign pose productions to fool D. During testing, we drop D\nand use the trained G to produce sign pose sequences given an\ninput source text. For the adversarial experiments, we build\nour progressive transformer generator with 2 layers, 2 heads\nand an embedding size of 256. Best performance is achieved\nwhen the regression, λ\nReg , and adversarial, λGAN , losses are\nweighted as λReg = 100 and λGAN = 0.001 respectively.\nThis reﬂects the larger relative scale of the adversarial loss.\nWe ﬁrst conduct an experiment with a non-conditional\nadversarial training regime. Only the sign pose sequence\nis critiqued, without conditioning upon source input. As\nshown on the top row of Table 6, this discriminator archi-\ntecture produces a weak performing generator, of only 12.65\nBLEU-4. This is less than the previous augmentation results,\nshowing how an adversary applied solely to produced sign\nsequences negatively affects performance. The discriminator\nis prompting realistic production with no regards to source\ntext, affecting the quality of the central translation task.\nWe next evaluate the conditional adversarial training\nregime, re-introducing a critique conditioned on source input.\nWe evaluate different discriminator architectures by varying\nthe number of CNN layers, N . This changes the strength of\nthe adversary, which is required to be ﬁnely balanced against\nthe generator in the min-max setup. Results are shown in\nTable 6, where an increase of N from 3 to 6 increases perfor-\nmance to a peak of 13.13 BLEU-4. This shows how a stronger\ndiscriminator can enforce a more realistic and expressive\nproduction from the generator. However, once N increases\nfurther and the discriminator becomes too strong, generator\nperformance is negatively affected.\nOverall, our conditional adversarial training regime has\ndemonstrated improved performance over a model trained\nsolely with a regression loss. Even for the test set, the result\nof 12.76 BLEU-4 is considerably higher than previous per-\nformance. This shows that the inclusion of a discriminator\nmodel increases the comprehension of sign production when\nconditioned on source sequence input. We believe this is due\nto the discriminator pushing the generator towards both a\nmore expressive production and an accurate translation, in\norder to deceive the adversary. This, in turn, increases the\nsign content contained in the generated sequence, leading to\na more understandable output and higher performance.\n5.2.3 Mixture Density Networks\nOur ﬁnal Gloss to Pose evaluation is of the mixture density\nnetwork (MDN) model conﬁguration outlined in Sect. 3.4.\nDuring training, a multimodal distribution is created that best\nmodels the data, which is then used to sample from during\ninference. In this experiment, our progressive transformer\nmodel is built with 2 layers, 2 heads and an embedding size\nof 512.\nWe evaluate different numbers of mixture components, M,\nwith results shown in Table 7. As shown, initially increasing\nM allows a multimodal prediction over a larger subspace,\nbetter modelling the sequence variation. This is supported\nby the results, with M = 4 achieving the highest validation\nperformance of 13.14 BLEU-4. We ﬁnd the regression to the\nmean of a deterministic prediction to be reduced, leading to a\nmore expressive production. The subtleties of sign poses are\nrestored, particularly for the small and variable ﬁnger joints.\nAs M increases further, the added model complexity out-\nweighs these beneﬁts, leading to a performance degradation.\nOur proposed MDN formulation achieves a higher per-\nformance than the previous deterministic approach of the\nprogressive transformer. Comparison against the adversarial\nconﬁguration shows a slight increase in performance (13.14\nand 13.13 BLEU-4 respectively). However, given the back\ntranslation evaluation is not perfect, one might consider the\nperformance of the MDN and adversarial models’ to be simi-\nlar, within the error margin of the SLT system. Both methods\nhave a similar result of reducing the regression to the mean\nfound in the original architecture and increasing sign pose\narticulation.\nWe additionally evaluate the combination of the MDN loss\nwith the previously described adversarial loss, as explained\nin Sect. 3.4.4. This creates a network that uses a mixture\ndistribution generator and a conditional discriminator. As in\nSect. 5.2.2, we weight the MDN, λ\nMDN = 100, and adver-\nsarial, λGAN = 0.001, losses respectively. As shown at the\n123\n2128 International Journal of Computer Vision (2021) 129:2113–2135\nTable 6 Adversarial training results on the Gloss to Pose task\nDEV SET TEST SET\nN Con. BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\n6 12.65 16.09 22.04 35.95 36.29 12.05 15.34 21.25 33.37 34.90\n3 ✓ 12.76 15.91 21.54 32.97 36.06 12.16 15.70 22.34 35.43 35.71\n4 ✓ 12.70 15.96 21.76 33.69 36.40 12.06 15.46 21.56 33.49 35.55\n5 ✓ 12.42 15.74 21.55 32.94 35.89 12.43 15.83 21.85 33.81 35.66\n6 ✓ 13.13 16.53 22.36 34.13 36.45 12.60 16.05 22.37 34.67 36.29\n7 ✓ 12.54 15.96 21.90 33.62 36.11 12.76 16.15 22.24 34.36 35.29\n8 ✓ 12.41 15.89 22.02 34.99 35.95 12.38 15.80 22.09 34.60 35.85\nBold is used to signify the best performing model. Evaluation upon inclusion of conditioning on the source input (Con.) and the amount of\ndiscriminator layers, N\nTable 7 Mixture density network results on the Gloss to Pose task\nDEV SET TEST SET\nM Adv. BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\n1 12.22 15.47 21.15 32.91 35.39 10.88 14.04 19.87 32.75 32.95\n2 12.89 16.16 21.80 33.23 36.16 11.60 14.71 20.40 32.18 34.31\n4 13.14 16.77 22.59 33.84 39.06 11.94 15.22 21.19 33.66 35.19\n5 12.75 15.91 21.40 32.67 36.04 11.57 14.77 20.66 32.69 34.48\n10 11.48 14.52 19.92 31.62 33.67 10.90 14.02 19.77 32.15 33.39\n20 12.59 16.02 22.17 35.07 36.28 12.15 15.35 21.34 33.62 35.47\n30 12.61 15.93 21.72 33.72 36.28 12.11 15.54 21.69 33.30 35.26\n50 11.15 14.18 19.66 30.95 33.58 10.56 13.67 19.60 32.62 33.30\n4 ✓ 12.88 16.17 21.83 33.50 35.60 12.32 15.62 21.82 34.35 35.36\nBold is used to signify the best performing model. Evaluation upon the mixture components, M and the addition of adversarial loss (Adv)\nbottom of Table 7, a combination of the MDN and adversarial\ntraining actually results in a lower performance than either\nindividually on the dev set, of 12.88 BLEU-4. However, for\nthe test set, this combination results in a slightly better per-\nformance than the MDN alone. Both of these conﬁgurations\naim to alleviate the effect of regression to the mean, but may\nadversely affect the performance of the other due to their\nsimilar goals.\n5.3 Text to Pose Production\nWe next evaluate our models on the Text to Pose task outlined\nin Sect. 4.3. This is the true end-to-end translation task, direct\nfrom a source spoken language sequence without the need\nfor a gloss intermediary.\n5.3.1 Model Conﬁgurations\nWe start by evaluating the various model conﬁgurations pro-\nposed in Sect. 3; namely base architecture, Gaussian noise\naugmentation, adversarial training and the MDN. The results\nof different conﬁgurations are shown in Table 8.\nAs with the Gloss to Pose task, Gaussian Noise augmenta-\ntion increases performance from the base architecture, from\n7.30 BLEU-4 to 10.75. We believe this is due to the reduction\nof the prediction drift as previously explained. The addition\nof adversarial training again increases performance, to 11.41\nBLEU-4. The conditioning of the discriminator is even more\nimportant for this task, as the input is spoken language and\nprovides more context for production.\nThe best Text to Pose performance of 11.54 BLEU-4\ncomes from the MDN model. As mentioned earlier, the per-\nformance of the adversarial and MDN setups’ can be seen\nas equivalent considering the utilized SLT system is not per-\nfect. Due to the increased context given by the source spoken\nlanguage, there is a larger natural variety in sign production.\nTherefore, the multimodal modelling of the MDN is further\nenhanced, as highlighted by the performance gains. The addi-\ntion of adversarial training on top of an MDN model does\nnot increase performance further, as was seen in the previous\nevaluations.\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2129\nTable 8 Results of the Text to Pose task for different model conﬁgurations\nDEV SET TEST SET\nConﬁguration BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nBase 7.30 9.21 12.87 23.15 26.11 6.79 8.74 12.57 23.46 25.02\nGaussian Noise 10.75 13.47 18.41 29.43 32.02 10.08 12.91 18.17 29.96 31.66\nAdversarial 11.41 14.26 19.45 31.02 33.59 10.16 12.98 18.33 29.61 32.03\nMDN 11.54 14.48 19.63 30.94 33.40 11.68 14.55 19.70 31.56 33.19\nMDN + Adv. 11.49 14.36 19.38 30.04 33.92 11.18 14.08 19.35 30.66 33.43\nBold is used to signify the best performing model.\nTable 9 Results of the Text to Pose and Text to Gloss to Pose network conﬁgurations for the Text to Pose task\nDEV SET TEST SET\nConﬁguration BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nText to Pose 11.54 14.48 19.63 30.94 33.40 11.68 14.55 19.70 31.56 33.19\nText to Gloss to Pose 11.21 14.22 19.46 30.37 32.95 13.64 17.05 23.09 34.94 36.90\nBold is used to signify the best performing model.\n5.3.2 Text to Pose v Text to Gloss to Pose\nOur ﬁnal experiment evaluates two end-to-end network con-\nﬁgurations; sign production either direct from text [Text to\nPose (T2P)] or via a gloss intermediary [Text to Gloss to Pose\n(T2G2P)]. These two tasks are outlined in Fig. 1, T2G2P on\nthe left, T2P on the right.\nAs can be seen from Table 9, the T2P model outperforms\nthe T2G2P for the development set. We believe this is because\nthere is more information available within spoken language\ncompared to a gloss representation, with more tokens per\nsequence to predict from. Predicting gloss sequences as an\nintermediary can act as an information bottleneck, as all the\ninformation required for production needs to be present in the\ngloss. Therefore, any contextual information present in the\nsource text can be lost. However, in the test set, we achieve\nbetter performance using gloss intermediaries. We believe\nthis is due to the effects of the limited number of training\nsamples and the smaller vocabulary size of glosses on the\ngeneralisation capabilities of our networks.\nThe success of the T2P network shows that our progres-\nsive transformer model is powerful enough to complete two\nsub-tasks; ﬁrstly mapping spoken language sequences to a\nsign representation, then producing an accurate sign pose\nrecreation. This is important for future scaling of the SLP\nmodel architecture, as many sign language domains do not\nhave gloss availability.\nFurthermore, our ﬁnal BLEU-4 scores outperform similar\nend-to-end Sign to Text methods which do not utilise gloss\ninformation (Camgoz 2018) (9.94 BLEU-4). Note that this is\nan unfair direct comparison, but it does provide an indication\nof model performance and the quality of the produced sign\npose sequences.\n5.4 User Evaluation\nThe only true way to evaluate the sign production is in dis-\ncussion with the Deaf communities, the end users. As our\noutputs are sign language sequences, we wish to understand\nhow understandable they are to a native Deaf signer. We per-\nform this evaluation with the skeletal output of the model,\nas we do not wish to confuse the translation ability of the\nsystem with the visual aesthetics of an avatar. However, by\nassessing the skeleton directly, we lose a lot of information\nthat is conveyed in images such as shadow and occlusion. We\ntherefore do a relative comparison between ground-truth and\nproduced sequences, allowing us to assess the productions\nfairly. Although this work is in its infancy, we understand it\nis important to get early feedback from the Deaf communi-\nties. We believe the Deaf communities should be empowered\nand be involved in all steps of the development of any tech-\nnology that is targeting their native languages.\nWe conducted a user evaluation with native DGS speak-\ners to estimate the comprehension of our produced sign pose\nsequences. We designed a survey consisting of a compari-\nson of the productions against ground truth data, the Visual\nTask, and a Translation Task that evaluates the sign compre-\nhension. We animated our sign pose sequences as explained\nin Sect. 3.5 and placed the videos in an online survey. The\nuser evaluation was conducted in collaboration with HFC\nHuman-Factors-Consult GmbH.\nWe evaluated with two different model conﬁgurations;\nadversarial training and MDNs, providing users with differ-\nent sequences from each and randomising the order of the\nvideos. We received 20 Deaf participants who completed the\nevaluation, both comparing the production quality and test-\ning the sign comprehension.\n123\n2130 International Journal of Computer Vision (2021) 129:2113–2135\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 9 Qualitative evaluation of an example sign pose sequence. The source input is at the top, with the ground truth video frames and poses at the\nbottom. Middle rows contain produced sign pose sequences of different model conﬁgurations\nTable 10 User evaluation results of the Visual task, showing the per-\ncentage of users who rated the ground truth (GT) or produced sequences\n(Prod) of a higher visual quality or equal\nConﬁguration GT (%) Prod (%) Equal (%)\nAdversarial 14.58 8.33 77.08\nMDN 0.00 15.38 84.62\n5.4.1 Visual Task\nOur ﬁrst evaluation is a visual task, where a video of a sign\nproduction is shown alongside the corresponding ground\ntruth sign sequence. The user is asked to rate both videos,\nwith an implicit comparison between them. The comparison\nresults are shown in Table 10, for both the adversarial and\nMDN model conﬁgurations.\nOverall, the user feedback was mainly equal between the\nproduced and ground-truth videos, with slightly more partic-\nipants preferring the productions. This highlights the quality\nof the produced sign language videos, often as they are\nsmoothly generated without any visual jitters. On the con-\ntrary, the original sequences often suffer from visual jitter,\ndue to the motion blur in the original videos and the artifacts\nintroduced in the 3D pose estimation.\nThe MDN conﬁguration received higher ratings from\nthe participants than the adversarial setup. 15.38% of\nusers preferred the MDN productions over the ground-truth\nsequences, compared to 8.33% for the adversarial model.\nThis demonstrates that the participants preferred the visuals\nof the MDN model. The quantitative back translation results\nfor these models were similar (Sect. 5.2), but the users feed-\nback suggests the MDNs production was of higher quality.\n5.4.2 Translation Task\nOur second evaluation is a translation task, designed to mea-\nsure the translation accuracy of the sign productions. An\nautomatic production was shown alongside 4 possible spo-\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2131\nTable 11 User evaluation\nresults for the Translation task,\nshowing the percentage of\nparticipants who chose the\ncorrect spoken language\ntranslation out of a choice of 4\nConﬁguration Correct (%)\nAdversarial 34.72\nMDN 78.57\nken language translations of the sign sequence, where one\nis the correct sentence. The user is asked to select the most\nlikely translation.\nTable11 shows that, for the adversarial examples, 34.72%\nof users chose the correct translation, compared to 78.57%\nfor the MDN conﬁguration. This is a drastic difference in the\nunderstanding of each of the model conﬁgurations, further\ndemonstrating the success of the MDN productions. With\nthe results of both visual and translation tasks, alongside the\nsimilar quantitative performance, we can conclude that the\nproposed MDN conﬁguration generates the most realistic and\nexpressive sign pose production.\n6 Qualitative Evaluation\nIn this section, we report qualitative results for our SLP\nmodel. We share snapshot examples of sign pose sequences\nin Figs. 9 and 11, visually comparing the outputs of the pro-\nposed model conﬁgurations for the gloss to pose task. The\ncorresponding unseen spoken language sequence is shown as\ninput at the top, alongside example frames from the ground\ntruth video and the produced sign language sequence.\nAs can be seen from the provided examples, our SLP\nmodel produces visually pleasing and realistic looking sign\nwith a close correspondence to the ground truth video. Body\nmotion is smooth and accurate, whilst hand shapes are mean-\ningful if a little under-expressed. Speciﬁc to non-manual\nfeatures, we ﬁnd a close correspondence to the ground truth\nvideo alongside accurate head movement, with a slight under-\narticulation of mouthings.\nFor comparisons between model conﬁgurations, the Gaus-\nsian Noise productions can be seen to be under-expressed,\nspeciﬁcally the hand shape and motions of Fig. 9b. The adver-\nsarial training improves this, resulting in a signiﬁcantly more\nexpressive production, with larger hand shapes seen in the 6th\nframe of Fig. 11c. This is due to the discriminator pushing\nthe productions towards a more realistic output. Inclusion of\na MDN representation can be seen to provide more accuracy\nin production, with the sign poses of Fig. 9d visually closer\nto the ground truth. This is due to the mixture distribution\nmodelling the uncertainty of the continuous sign sequences,\nremoving the mean productions that can be seen in the Gaus-\nsian Noise productions.\nVisual comparisons between the adversarial and MDN\nproductions reﬂect the equal quantitative performance of\nFig. 10 Example failure sign pose productions. Due to either complex\nhandshape (left), hand occlusion (middle) or proper noun (right)\nthe two (Sect. 5.2), demonstrating two contrasting ways of\nincreasing the sign comprehension. Overall, the problem of\nregression to the mean is diminished and a more realistic\nproduction is achieved, highlighting the importance of the\nproposed model conﬁgurations.\nThese examples show that regressing continuous 3D\nhuman pose sequences can be successfully achieved using a\nself-attention based approach. The predicted joint locations\nfor neighbouring frames are closely positioned, showing that\nthe model has learnt the subtle signer movements. Smooth\ntransitions between signs are produced, highlighting a dif-\nference from the discrete generation of spoken language.\nFigure 10 shows some failure cases of the approach. Com-\nplex hand classiﬁers can be difﬁcult to replicate (left) and\nhand occlusion affects the quality of training data (middle).\nWe ﬁnd that the most difﬁcult production occurs with proper\nnouns and speciﬁc entities, due to the lack of grammatical\ncontext and examples in the training data (right).\n7 Conclusions\nIn this work, we presented a Continuous 3D Multi-Channel\nSign Language Production model, the ﬁrst SLP model to\ntranslate from text to continuous 3D sign pose sequences\nin an end-to-end manner. To enable this, we proposed a\nProgressive Transformer architecture with an alternative\nformulation of transformer decoding for variable length\ncontinuous sequences. We introduced a counter decoding\ntechnique to predict continuous sequences of variable lengths\nby tracking the production progress over time and predicting\nthe end of sequence.\nTo reduce the prediction drift that is often seen in con-\ntinuous sequence production, we presented several data\naugmentation methods that signiﬁcantly improve model\n123\n2132 International Journal of Computer Vision (2021) 129:2113–2135\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 11 Qualitative evaluation of an example sign pose sequence. The source input is at the top, with the ground truth video frames and poses at\nthe bottom. Middle rows contain produced sign pose sequences of different model conﬁgurations\nperformance. Predicting continuous values often results in\nunder-articulated output, and thus we proposed the addition\nof adversarial training to the network, introducing a condi-\ntional discriminator model to prompt a more realistic and\nexpressive production. We also proposed a mixture density\nnetwork (MDN) modelling, utilising the progressive trans-\nformer outputs to paramatise a mixture Gaussian distribution.\nWe evaluated our approach on the challenging PHOENIX14T\ndataset, proposing a back translation evaluation metric for\nSLP . Our experiments showed the importance of data aug-\nmentation techniques to reduce model drift. We improved our\nmodel performance with the addition of both an adversarial\ntraining regime and a MDN output representation. Further-\nmore, we have shown that a direct text to pose translation\nconﬁguration can outperform a gloss intermediary model,\nmeaning SLP models are not limited to domains where\nexpensive gloss annotation is available.\nFinally, we conducted a user study of the Deaf’s response\nto our sign productions, understanding the sign compre-\nhension of the proposed model conﬁgurations. The results\nshow that our productions, while not perfect, can be further\nimproved by reducing and smoothing noise inherent to the\ndata and approaches. However, they also highlight that the\ncurrent sign productions still need improvement to be fully\nunderstandable by the Deaf. The ﬁeld of SLP is in its infancy,\nwith a potential for large growth and improvement in the\nfuture.\nWe believe the current 3D skeleton representation affects\nthe comprehension of sign pose sequences. As future work,\nwe would like to increase the realism of sign production\nby generating photo-realistic signers, using GAN image-\nto-image translation models ( Chan et al. 2019; Zhu et al.\n2017; Isola et al. 2017) to expand from the current skeleton\nrepresentation. Drawing on feedback from the user evalu-\nation, we plan to improve the hand articulation via a hand\nshape classiﬁer to increase comprehension. An automatic\nviseme generator could also be included to the pipeline to\nimprove mouthing patterns, producing features in a deter-\nministic manner direct from dictionary data.\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2133\nAcknowledgements This work received funding from the SNSF\nSinergia project ‘SMILE’ (CRSII2 160811), the European Union’s\nHorizon2020 research and innovation programme under grant agree-\nment no. 762021 ‘Content4All’ and the EPSRC project ‘ExTOL’\n(EP/R03298X/1). This work reﬂects only the authors view and the\nCommission is not responsible for any use that may be made of the\ninformation it contains.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nAdaloglou, N., Chatzis, T., Papastratis, I., Stergioulas, A., Papadopou-\nlos, G. T., Zacharopoulou, V ., Xydopoulos, G. J., Atzakas, K.,\nPapazachariou, D., & Daras, P . (2019). A comprehensive study\non sign language recognition methods. In IEEE transactions on\nmultimedia.\nBa, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization.\nArXiv preprint arXiv:1607.06450.\nBahdanau, D., Cho, K., & Bengio, Y . (2015). Neural machine translation\nby jointly learning to align and translate. In Proceedings of the\ninternational conference on learning representations (ICLR) .\nBauer, B., Hienz, H., & Kraiss, K.-F. (2000). Video-based continuous\nsign language recognition using statistical methods. In Proceed-\nings of 15th international conference on pattern recognition\n(ICPR).\nBerndt, D. J., & Clifford, J. (1994). Using dynamic time warping to ﬁnd\npatterns in time series. In AAA1 workshop on knowledge discovery\nin databases (KDD) .\nBishop, C. M. (1994). Mixture density networks. Technical Report, Cite-\nseer.\nBragg, D., Koller, O., Bellard, M., Berke, L., Boudreault, P ., Braffort, A.,\nCaselli, N., Huenerfauth, M., Kacorri, H., V erhoef, T., & V ogler,\nC. (2019). Sign language recognition, generation, and translation:\nAn interdisciplinary perspective. In The 21st international ACM\nSIGACCESS conference on computers and accessibility .\nBritish Deaf Association (BDA). (2020). UK deaf community. https://\nbda.org.uk/fast-facts-about-the-deafcommunity/ .\nCai, H., Bai, C., Tai, Y . W., & Tang, C. K. (2018). Deep video gen-\neration, prediction and completion of human action sequences.\nIn Proceedings of the European conference on computer vision\n(ECCV).\nCamgoz, N. C., Hadﬁeld, S., Koller, O., & Bowden, R. (2017). Sub-\nUNets: End-to-end hand shape and continuous sign language\nrecognition. In Proceedings of the IEEE international conference\non computer vision (ICCV) .\nCamgoz, N. C., Hadﬁeld, S., Koller, O., Ney, H., & Bowden, R. (2018).\nNeural sign language translation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition (CVPR) .\nCamgoz, N. C., Koller, O., Hadﬁeld, S., & Bowden, R. (2020a).\nMulti-channel transformers for multi-articulatory sign language\ntranslation. In Assistive computer vision and robotics workshop\n(ACVR).\nCamgoz, N. C., Koller, O., Hadﬁeld, S., & Bowden, R. (2020b). Sign\nlanguage transformers: joint end-toend sign language recognition\nand translation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition (CVPR) .\nCao, Z., Hidalgo, G., Simon, T., Wei, S. E., & Sheikh, Y . (2017). Open-\nPose: Realtime multi-person 2D pose estimation using part afﬁnity\nﬁelds. In Proceedings of the IEEE conference on computer vision\nand pattern recognition (CVPR) .\nChai, X., Li, G., Lin, Y ., Xu, Z., Tang, Y ., Chen, X., & Zhou, M. (2013).\nSign language recognition and translation with kinect. In IEEE\ninternational conference on automatic face and gesture recogni-\ntion (AFGR) .\nChan, C., Ginosar, S., Zhou, T., & Efros, A. A. (2019). Everybody\ndance now. In Proceedings of the IEEE international conference\non computer vision (CVPR) .\nCho, K., V an Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares,\nF., Schwenk, H., & Bengio, Y . (2014). Learning phrase repre-\nsentations using RNN encoder-decoder for statistical machine\ntranslation. In Conference on empirical methods in natural lan-\nguage processing (EMNLP) .\nCooper, H. M., Ong, E. J., Pugeault, N., & Bowden, R. (2012). Sign lan-\nguage recognition using sub-units. Journal of Machine Learning\nResearch (JMLR).\nCox, S., Lincoln, M., Tryggvason, J., Nakisa, M., Wells, M., Tutt, M., &\nAbbott, S. (2002). TESSA: A system to aid communication with\ndeaf people. In Proceedings of the ACM international conference\non assistive technologies .\nCui, R., Liu, H., & Zhang, C. (2017). Recurrent Convolutional neu-\nral networks for continuous sign language recognition by staged\noptimization. In Proceedings of the IEEE conference on computer\nvision and pattern recognition (CVPR) .\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., & Salakhutdinov,\nR. (2019). Transformer-XL: Attentive language models beyond\na ﬁxed-length context. In: International conference on learning\nrepresentations (ICLR).\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT:\nPre-training of deep bidirectional transformers for language under-\nstanding. In Proceedings of the annual meeting of the association\nfor computational linguistics (ACL) .\nEbling, S., & Huenerfauth, M. (2015). Bridging the Gap between Sign\nLanguage Machine Translation and Sign Language Animation\nusing Sequence Classiﬁcation. In Proceedings of SLPAT 2015:\n6th workshop on speech and language processing for assistive\ntechnologies.\nElliott, R., Glauert, J. R., Kennaway, J. R., Marshall, I., & Safar, E.\n(2008). Linguistic modelling and language-processing technolo-\ngies for avatar-based sign language presentation. In Universal\naccess in the information society.\nForster, J., Schmidt, C., Koller, O., Bellgardt, M., & Ney, H. (2014).\nExtensions of the sign language recognition and translation corpus\nRWTH-PHOENIX-Weather. In Proceedings of the international\nconference on language resources and evaluation (LREC).\nGinosar, S., Bar, A., Kohavi, G., Chan, C., Owens, A., & Malik, J.\n(2019). Learning individual styles of conversational gesture. In\nProceedings ofthe IEEE conference on computer vision and pat-\ntern recognition (CVPR).\nGirdhar, R., Carreira, J., Doersch, C., & Zisserman, A. (2019). Video\naction transformer network. In Proceedings of the IEEE conference\non computer vision and pattern recognition (CVPR) .\nGlauert, J. R. W., Elliott, R., Cox, S. J., Tryggvason, J., & Sheard, M.\n(2006). V ANESSA: A system for communication between deaf\nand hearing people. In Technology and disability.\nGlorot, X., & Bengio, Y . (2010). Understanding the difﬁculty of train-\ning deep feedforward neural networks. In Proceedings of the\n123\n2134 International Journal of Computer Vision (2021) 129:2113–2135\ninternational conference on artiﬁcial intelligence and statistics\n(AISTATS).\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,\nD., Ozair, S., Courville, A., & Bengio, Y . et al. (2014). Gener-\native adversarial nets. In Proceedings of the advances in neural\ninformation processing systems (NIPS) .\nGraves, A. (2013). Generating sequences with recurrent neural net-\nworks. ArXiv preprint arXiv:1308.0850.\nGrobel, K., & Assan, M. (1997). Isolated sign language recognition\nusing hidden Markov models. In IEEE international conference\non systems, man, and cybernetics .\nHa, D., & Eck, D. (2018). A neural representation of sketch drawings.\nIn International conference on learning representations (ICLR) .\nHa, D., & Schmidhuber, J. (2018). Recurrent world models facilitate\npolicy evolution. In Advances in neural information processing\nsystems (NIPS) .\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning\nfor image recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition (CVPR) .\nHolt, J. A. (1993). Stanford achievement test—8th edition: Reading\ncomprehension subgroup results. In American annals of the deaf .\nHu, Y ., Zhan, W., & Tomizuka, M. (2018). Probabilistic prediction of\nvehicle semantic intention and motion. In IEEE intelligent vehicles\nsymposium (IV) .\nHuang, C. Z. A., V aswani, A., Uszkoreit, J., Shazeer, N., Simon, I.,\nHawthorne, C., Dai, A. M., Hoffman, M. D., Dinculescu, M., &\nEck, D. (2018). Music transformer. In International conference on\nlearning representations (ICLR) .\nHuang, J., Zhou, W., Zhang, Q., Li, H., & Li, W. (2018). Video-based\nsign language recognition without temporal segmentation. In AAAI\nconference on artiﬁcial intelligence (AAAI) .\nIsola, P ., Zhu, J. Y ., Zhou, T., & Efros, A. A. (2017). Image-to-image\ntranslation with conditional adversarial networks. In Proceedings\nof the IEEE conference on computer vision and pattern recognition\n(CVPR).\nKayahan, D., & Gungor, T. (2019). A hybrid translation system from\nTurkish spoken language to Turkish sign language. In IEEE inter-\nnational symposium on innovations in intelligent systems and\napplications (INISTA).\nKingma, D. P ., & Ba, J. (2014). ADAM: A method for stochastic\noptimization. In Proceedings of the international conference on\nlearning representations (ICLR) .\nKipp, M., Heloir, A., & Nguyen, Q. (2011a). Sign language avatars:\nAnimation and comprehensibility. In International workshop on\nintelligent virtual agents (IVA) .\nKipp, M., Nguyen, Q., Heloir, A., & Matthes, S. (2011b). Assessing\nthe DeafUserPer-spective on sign language avatars. In The pro-\nceedings of the 13th international ACM SIGACCESS conference\non computers and accessibility (ASSETS).\nKo, S. K., Kim, C. J., Jung, H., & Cho, C. (2019). Neural sign lan-\nguage translation based on human keypoint estimation. In Applied\nsciences.\nKoller, O., Camgoz, N. C., Ney, H., & Bowden, R. (2019). Weakly super-\nvised learning with multi-stream CNN-LSTM-HMMs to discover\nsequential parallelism in sign language videos. In IEEE transac-\ntions on pattern analysis and machine intelligence (PAMI) .\nKoller, O., Forster, J., & Ney, H. (2015). Continuous sign language\nrecognition: Towards large vocabulary statistical recognition sys-\ntems handling multiple signers. In Computer vision and image\nunderstanding (CVIU) .\nKoller, O., Zargaran, S., & Ney, H. (2017). Re-sign: Re-aligned end-\nto-end sequence modelling with deep recurrent CNN-HMMs. In\nProceedings of the IEEE conference on computer vision and pat-\ntern recognition (CVPR) .\nKoller, O., Zargaran, O., Ney, H., & Bowden, R. (2016). Deep sign:\nHybrid CNN-HMM for continuous sign language recognition. In\nProceedings of the British machine vision conference (BMVC) .\nKouremenos, D., Ntalianis, K. S., Siolas, G., & Stafylopatis, A. (2018).\nStatistical machine translation for Greek to Greek sign language\nusing parallel corpora produced via rule-based machine transla-\ntion. In IEEE 31st international conference on tools with artiﬁcial\nintelligence (ICTAI).\nKreutzer, J., Bastings, J., & Riezler S. (2019). Joey NMT: A minimalist\nNMT toolkit for novices. In Proceedings of the 2019 conference\non empirical methods in natural language processing and the\n9th international joint conference on natural language process-\ning (EMNLP-IJCNLP): System demonstrations.\nLee, H.-Y ., Yang, X., Liu, M. Y ., Wang, T. C., Lu, Y . D., Yang, M.\nH., & Kautz, J. (2019). Dancing to music. In Advances in neural\ninformation processing systems (NIPS).\nLi, C., & Lee, G. H. (2019). Generating multiple hypotheses for 3D\nhuman pose estimation with mixture density network. In Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition (CVPR).\nKevin, L., Li, D., He, X., Zhang, Z., & Sun, M. T. (2017). Adversarial\nranking for language generation. In Advances in neural informa-\ntion processing systems (NIPS).\nPengfei, L., & Huenerfauth, M. (2010). Collecting a motion-capture cor-\npus of American sign language for data-driven generation research.\nIn Proceedings of the NAACL HLT 2010 workshop on speech and\nlanguage processing for assistive technologies.\nMaas, A. L., Hannun, A. Y ., & Ng, A. Y . (2013). Rectiﬁer nonlinearities\nimprove neural network acoustic models. In Proceedings of the\ninternational conference on machine learning (ICML).\nMakansi, O., Ilg, E., Cicek, O., & Brox, T. (2019). Overcoming\nlimitations of mixture density networks: A sampling and ﬁtting\nframework for multimodal future prediction. In Proceedings ofthe\nIEEE conference on computer vision and pattern recognition\n(CVPR).\nMcDonald, J. et al. (2016). Automated technique for real-time produc-\ntion of lifelike animations of American sign language. In Universal\naccess in the information society (UAIS).\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013).\nDistributed representations of words and phrases and their compo-\nsitionality. In Advances in neural information processing systems\n(NIPS).\nMirza, M., & Osindero, S. (2014). Conditional generative adversarial\nnets. ArXiv preprint arXiv:1411.1784.\nMukherjee, S., Ghosh, S., Ghosh, S., Kumar, P ., & Roy, P . P . (2019).\nPredicting video-frames using encoder-convlstm combination. In\nIEEE international conference on acoustics, speech and signal\nprocessing (ICASSP).\nOrbay, A., & Akarun, L. (2020). Neural sign language translation by\nlearning tokenization. In IEEE international conference on auto-\nmatic face and gesture recognition (FG).\nOzdemir, O., Necati, C. C., & Lale, A. (2016). Isolated sign language\nrecognition using improved dense trajectories. In Proceedings of\nthe signal processing and communication application conference\n(SIU).\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z.\n(2017). Automatic differentiation in PyTorch. In NIPS Autodiff\nWorkshop.\nPfau, R., & Quer, J. (2010). Nonmanuals: their grammatical and\nprosodic roles.\nPovey, D., Hadian, H., Ghahremani, P ., Li, K., & Khudanpur, S. (2018).\nA time-restricted self-attention layer for ASR. In IEEE inter-\nnational conference on acoustics, speech and signal processing\n(ICASSP).\n123\nInternational Journal of Computer Vision (2021) 129:2113–2135 2135\nPress, O., Bar, A., Bogin, B., Berant, J., & Wolf, L. (2017). Language\ngeneration with recurrent generative adversarial networks without\npre-training. ArXiv preprint arXiv:1706.01399.\nProkudin, S., Gehler, P ., & Nowozin, S. (2018). Deep directional\nstatistics: Pose estimation with uncertainty quantiﬁcation. In Pro-\nceedings of the European conference on computer vision (ECCV) .\nRadford, A., Metz, L., & Chintala, S. (2015). Unsupervised repre-\nsentation learning with deep convolutional generative adversarial\nnetworks. ArXiv preprint arXiv:1511.06434.\nRen, X., Li, H., Huang, Z., & Chen, Q. (2019). Music-oriented\ndance video synthesis with pose perceptual loss. ArXiv preprint\narXiv:1912.06606.\nRen, Y ., Ruan, Y ., Tan, X., Qin, T., Zhao, S., Zhao, Z., & Liu, T. Y .\n(2019). Fastspeech: Fast, robust and controllable text to speech. In\nAdvances in neural information processing systems (NIPS) .\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V ., Radford, A.,\n& Chen, X. (2016). Improved techniques for training GANs. In\nAdvances in neural information processing systems (NIPS) .\nSaunders, B., Camgoz, N. C., & Bowden, R. (2020a). Adversarial train-\ning for multi-channel sign language production. In Proceedings of\nthe British machine vision conference (BMVC) .\nSaunders, B., Camgoz, N. C., & Bowden, R., (2020b). Progressive trans-\nformers for end-to-end sign language production. In Proceedings\nof the European conference on computer vision (ECCV) .\nSchuster, M. (2000). Better generative models for sequential data\nproblems: bidirectional recurrent mixture density networks. In\nAdvances in neural information processing systems (NIPS) .\nStarner, T., & Pentland, A., (1997). Real-time American sign language\nrecognition from video using hidden Markov models. In Motion-\nbased recognition.\nStokoe, W. C. (1980). Sign language structure. In: Annual review of\nanthropology.\nStoll, S., Camgoz, N. C., Hadﬁeld, S., & Bowden, R. (2020). Text2Sign:\nTowards sign language production using neural machine transla-\ntion and generative adversarial networks. In International journal\nof computer vision (IJCV) .\nSutskever, I., Vinyals, O., & Le, Q. V ., (2014). Sequence to sequence\nlearning with neural networks. In Proceedings of the advances in\nneural information processing systems (NIPS) .\nSutton-Spence, R., & Woll, B. (1999). The linguistics of British sign\nlanguage: An introduction . Cambridge University Press.\nTamura, S. & Kawasaki, S., (1988). Recognition of sign language\nmotion images. In Pattern recognition.\nTulyakov, S., Liu, M. Y ., Yang, X., & Kautz, J. (2018). MoCoGAN:\nDecomposing motion and content for video generation. In Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition (CVPR).\nV alli, C. & Lucas, C., (2000). Linguistics of American sign language:\nAn introduction. Gallaudet University Press.\nV aramesh, A., & Tuytelaars, T., (2020). Mixture dense regression for\nobject detection and human pose estimation. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition\n(CVPR).\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need.\nIn Advances in neural information processing systems (NIPS) .\nV ogler, C., & Metaxas, D., (1999). Parallel midden Markov models for\nAmerican sign language recognition. In Proceedings of the IEEE\ninternational conference on computer vision (ICCV).\nV ondrick, C., Pirsiavash, H., & Torralba, A. (2016). Generating videos\nwith scene dynamics. In Advances in neural information process-\ning systems (NIPS) .\nWang, X., Takaki, S., & Yamagishi, J., (2017). An autoregressive recur-\nrent mixture density network for parametric speech synthesis. In\nIEEE international conference on acoustics, speech and signal\nprocessing (ICASSP).\nWorld Health Organisation (WHO) (2020). Deafness and hearing loss.\nhttps://www.who.int/news-room/fact-sheets/detail/deafness-\nand-hearing-loss .\nW u ,L . ,X i a ,Y . ,T i a n ,F . ,Z h a o ,L . ,Q i n ,T . ,L a i ,J . ,&L i u ,T .Y .\n(2017). Adversarial neural machine translation. In Proceedings\nof the Asian conference on machine learning (ACML) .\nXiao, Q., Qin, M., & Yin, Y ., (2020). Skeleton-based Chinese sign lan-\nguage recognition and generation for bidirectional communication\nbetween deaf and hearing people. In Neural networks .\nYang, Z., Chen, W., Wang, F., & Xu, B. (2017). Improving neural\nmachine translation with conditional sequence generative adver-\nsarial nets. In Proceedings of the conference of the North American\nchapter of the association for computational linguistics (ACL) .\nYe, Q., & Kim, T-K. (2018). Occlusion-aware hand pose estimation\nusing hierarchical mixture density network. In Proceedings of the\nEuropean conference on computer vision (ECCV) .\nYin, K. (2020). Attention is all you sign: Sign language translation with\ntransformers. In ECCV sign language recognition, translation and\nproduction workshop.\nZelinka, J., & Kanis, J. (2020). Neural sign language synthesis: Words\nare our glosses. In The IEEE winter conference on applications of\ncomputer vision (WACV) .\nZhang, X.-Y ., Yin, F., Zhang, Y . M., Liu, C. L., & Bengio, Y . (2017).\nDrawing and recognizing Chinese characters with recurrent neural\nnetwork. In IEEE transactions on pattern analysis and machine\nintelligence (PAMI).\nZhang, Y ., Gan, Z., & Carin, L. (2016). Generating text via adversarial\ntraining. In Neural information processing systems (NIPS) work-\nshop on adversarial training .\nZhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., & Liu, Q. (2019).\nERNIE: Enhanced language representation with informative enti-\nties. In 57th annual meeting of the association for computational\nlinguistics (ACL).\nZhou, L., Zhou, Y ., Corso, J. J., Socher, R., & Xiong, C. (2018).\nEnd-to-end dense video captioning with masked transformer. In\nProceedings of the IEEE conference on computer vision and pat-\ntern recognition (CVPR) .\nZhu, J.-Y ., Park, T., Isola, P ., & Efros, A. A. (2017). Unpaired image-\nto-image translation using cycle-consistent adversarial networks.\nIn Proceedings of the IEEE conference on computer vision and\npattern recognition (CVPR) .\nZwitserlood, I., V erlinden, M., Ros, J., & V an Der Schoot, S. (2004).\nSynthetic signing for the deaf: Esign. In Proceedings of the confer-\nence and workshop on assistive technologies for vision and hearing\nimpairment (CVHI) .\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}