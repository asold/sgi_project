{
    "title": "PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models",
    "url": "https://openalex.org/W4385573001",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2096381008",
            "name": "Yupeng Zhang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2099256167",
            "name": "Hongzhi Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105896935",
            "name": "Sirui Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1993208100",
            "name": "Wei Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2133880114",
            "name": "Zhoujun Li",
            "affiliations": [
                "Beihang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2130158090",
        "https://openalex.org/W4223512566",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3035204084",
        "https://openalex.org/W2924690340",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W3104215796",
        "https://openalex.org/W4226070387",
        "https://openalex.org/W4253067820",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3007685714",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2971275988",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3034199299",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2975185270",
        "https://openalex.org/W3035315091",
        "https://openalex.org/W2707890836",
        "https://openalex.org/W4287777801",
        "https://openalex.org/W2970860468",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W4225691633",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3104136798",
        "https://openalex.org/W1984685202",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2984582583",
        "https://openalex.org/W3173195958",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2965862774",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W3106061119",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W2963748441"
    ],
    "abstract": "A wide range of NLP tasks benefit from the fine-tuning of pretrained language models (PLMs). However, a number of redundant parameters which contribute less to the downstream task are observed in a directly fine-tuned model. We consider the gap between pretraining and downstream tasks hinders the training of these redundant parameters, and results in a suboptimal performance of the overall model. In this paper, we present PATS (Perturbation According To Sensitivity), a noisy training mechanism which considers each parameter’s importance in the downstream task to help fine-tune PLMs. The main idea of PATS is to add bigger noise to parameters with lower sensitivity and vice versa, in order to activate more parameters’ contributions to downstream tasks without affecting the sensitive ones much. Extensive experiments conducted on different tasks of the GLUE benchmark show PATS can consistently empower the fine-tuning of different sizes of PLMs, and the parameters in the well-performing models always have more concentrated distributions of sensitivities, which experimentally proves the effectiveness of our method.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3680–3687\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nPATS: Sensitivity-aware Noisy Learning for Pretrained Language Models\nYupeng Zhang1, Hongzhi Zhang2, Sirui Wang2, Wei Wu2 and Zhoujun Li1∗\n1Beihang University, Beijing, China 2Meituan Inc., Beijing, China\n{G0vi_qyx, lizj}@buaa.edu.cn\n{zhanghongzhi03, wangsirui, wuwei30}@meituan.com\nAbstract\nA wide range of NLP tasks benefit from\nthe fine-tuning of pretrained language mod-\nels (PLMs). However, a number of redundant\nparameters which contribute less to the down-\nstream task are observed in a directly fine-tuned\nmodel. We think the gap between pretraining\nand downstream tasks hinders the training of\nthese redundant parameters, and results in a\nsuboptimal performance of the overall model.\nIn this paper, we present PATS (Perturbation\nAccording To Sensitivity), a noisy training\nmechanism which considers each parameter’s\nimportance in the downstream task to help\nfine-tune PLMs. The main idea of PATS is\nto add bigger noise to parameters with lower\nsensitivity and vice versa, in order to activate\nmore parameters’ contributions to downstream\ntasks without affecting the sensitive ones much.\nExtensive experiments conducted on different\ntasks of the GLUE benchmark show PATS can\nconsistently empower the fine-tuning of differ-\nent sizes of PLMs, and the parameters in the\nwell-performing models always have more con-\ncentrated distributions of sensitivities, which\nexperimentally proves the effectiveness of our\nmethod.\n1 Introduction\nWith a huge number of model parameters and well\ndesigned training objectives, pretrained language\nmodels (PLMs) have brought a new era to NLP\n(Guu et al., 2020; Liu, 2019; Zhu et al., 2020b;\nQiu et al., 2020). Fine-tuning PLMs such as BERT\n(Devlin et al., 2019) has become a basic and effec-\ntive way in many downstream tasks (Wadden et al.,\n2019; Sun et al., 2019; Howard and Ruder, 2018).\nHowever, recent study has shown that aggres-\nsive fine-tuning can induce an unstable and subop-\ntimal performance of the models especially with\ninsufficient data (Dodge et al., 2020; Raffel et al.,\n2019), which attracts some researchers to figure\n∗Corresponding author.\nout the culprits and explore effective methods to\nsolve them (Peters et al., 2019; Houlsby et al., 2019;\nMosbach et al., 2020). For example, there are some\nregularization methods like RecAdam (Chen et al.,\n2020) and Mixout (Lee et al., 2020), and adversar-\nial training techniques like SMART (Jiang et al.,\n2020) and FreeLB (Zhu et al., 2020a) to alleviate\nthe overfitting of data in downstream tasks; Be-\nyond that, Wu et al. (2022) proposed NoisyTune\nwith the argument that in addition to the overfitting\nof the limited downstream data, there could also\nexist overfitting in pretraining tasks, which could\nresult in enormous gaps between pretraining and\ndownstream task data. In order to overcome the\ngaps, NoisyTune simply adds some noise to pa-\nrameters in the PLM before fine-tuning. Besides,\nit has also been demonstrated that the existence\nof a large number of redundant parameters could\nalso be a factor in the suboptimal performances\nof aggressively fine-tuned PLMs (Fan et al., 2019;\nSanh et al., 2020; Dalvi et al., 2020). Consider-\ning the redundant parameters in a model are not\ninsufficiently trained, Liang et al. (2022) proposed\na learning rate scheduler named SAGE in which\nlarger learning rates are assigned to these param-\neters of low sensitivity (a measure of parameter’s\nimportance to downstream tasks).\nThere could be some connection between the\ngaps caused by overfitting of pretraining tasks and\nthe redundancy of parameters. We consider it could\nbe the gaps between pretraining and downstream\ntasks that hinder the training of these redundant\nparameters. SAGE enlarges the learning rates of\ninsensitive parameters to help their training. How-\never, with the sensitivity measurement considered,\nthe insensitive parameters usually have smaller gra-\ndients, so enlarged learning rates may help them\nlittle to escape the sub-optimal areas compared\nto involving additional noise. One noisy training\nmethod to alleviate the gaps is NoisyTune, in which\nparameters of a matrix in a PLM are added with\n3680\nnoise according to the standard deviation of the\nmatrix before fine-tuning. Nevertheless, there are\nfew explanations about why or whether the param-\neters in the same matrix should be perturbed with\nthe same intensity. Considering different parame-\nters have different contributions to the model, noise\nfrom a unified distribution may disturb knowledge\nof some sensitive parameters, resulting in a loss\nof performance. Besides, since each task needs to\ncapture an appropriate textual pattern and the data\nof it usually comes from a special domain, differ-\nent downstream tasks could have different kinds\nof gaps with those of the pretraining. So the noise\nadded to overcome the gaps should also be related\nto the downstream task data.\nIn this paper, we propose a novel parameter-\nwise noisy fine-tuning method called PATS\n(Perturbation According To Sensitivity) to make\nfull use of perturbation on parameters to handle\nthe problems above. We focus on balancing the\ncontributions of all parameters in the model by ac-\ntivating the insensitive ones to play better roles in\ndownstream tasks. So the main idea of our method\nis adding different intensities of noise to parame-\nters according to their sensitivity when fine-tuning\nPLMs, different from NoisyTune (Fig. 1 (b)) in\nwhich noise added to a matrix of parameters is\nfrom a unified distribution and unrelated to down-\nstream task data. Specifically, during fine-tuning\nin PATS (Fig. 1 (c)), larger noise will be added to\nthe parameters with lower sensitivity (such as the\nparameter shown in red), while sensitive parame-\nters (such as the parameter shown in purple) will\nbe barely perturbed.\nOur contributions can be summarized as follows:\n1) We propose a simple but effective method to\nhelp all parameters be trained sufficiently when\nfine-tuning PLMs in downstream tasks. 2) Among\nall the training methods with noise, PATS is the\nfirst sensitivity-aware one which perturbs models\nwith noise of different distributions according to pa-\nrameters’ sensitivity, to the best of our knowledge.\n3) Extensive experiments on the GLUE benchmark\nshow PATS makes a difference in boosting the per-\nformance of PLMs in downstream NLP tasks.\n2 Approach\nIn this section, we present our PATS for PLMs\nfine-tuning. Previous matrix-wise noisy methods\nperturb a PLM by adding noise from a uniform\ndistribution to a matrix of parameters. Different\nPLM\nFine-\ntuning\n(a) Commonly \nFine-tuning \nPLM\n(b) NoisyTune: \nFine-tuning \nPerturbed PLM\n(c) PATS: Fine-\ntuning PLM with \nSensitivity-based \nNoise\nTask \nData\nTask \nData\nTask \nData\nSensitivity from low to high\nNeuron Parameter\nPerturbation Perturbed Parameter\nFigure 1: Different schemata of fine-tuning PLMs.\nIn NoisyTune, a matrix of parameters in a PLM are\nperturbed with the same intensity before fine-tuning;\nIn PATS, parameters with lower sensitivity (the \"red\"\nparameter) to downstream data are added with larger\nnoise, and vice versa (like the \"purple\" parameter).\nfrom them, in PATS, each parameter even from\nthe same matrix will be paid to different attention\naccording to its sensitivity. It is also worth noting\nthat in PATS, a PLM is not perturbed in advance\nlike NoisyTune, instead the perturbation happens\nduring training as the task data comes in. In the fol-\nlowing sections, we will introduce the calculation\nof parameter sensitivity first and then present the\nnoisy learning mechanism in detail.\n2.1 Sensitivity Measurement\nThe sensitivity of a parameter is used to measure\nthe change of the output or loss after setting it to\nzero (Molchanov et al., 2017, 2019; Ding et al.,\n2019; Xiao et al., 2019; Lee et al., 2019). To be\nspecific, given a BERT-like pre-trained language\nmodel M with parameters Θ = {θ1,θ2,··· ,θn}∈\nRn, the sensitivity of the j-th parameter θj is writ-\nten as sj, which can be defined as:\nsj = |L(Θ) −L(θ1,··· ,θj−1,0,θj+1,··· ,θn)|\n≈|θj∇θj L(Θ)|, (1)\nwhere Lis a loss function and we use the first-\ndegree Taylor polynomial to approximate sj ignor-\ning the higher order remainder to accelerate the\ncalculation of it.\nIn order to avoid huge oscillation of sj caused\nby an abnormal batch of data, we adopt the expo-\n3681\nnential moving average of sj used in many other\nmodels and optimizers (Liang et al., 2022; Klinker,\n2010; Zhuang et al., 2022; Shahidi et al., 2020)\nas the real sensitivity indicator, which can be ex-\npressed by the following equation:\n¯sj = β¯sj∗ + (1 −β)sj,β ∈(0,1), (2)\nwhere ¯sj and ¯sj∗ are the exponential moving aver-\nage of sj in the current and previous iteration. βis\na hyper-parameter used to adjust the importance of\nsj calculated by the current batch of data.\n2.2 Training with Noise\nAlgorithm 1 PATS for Adamax(max(·) returns a\nmatrix with the maximum values of each element of\nthe input matrices or vectors; sum(·) returns a scalar\nequal to the sum of all values of a matrix or vector;\nI denotes an all-ones matrix; ⊙denotes Hadamard\nproduct and ⊘denotes Hadamard division)\nInput:Step size α; Model parameters Θ ∈Rn;\nNumber of training iterations T; Number of\nparameters in the current matrix N; Exponen-\ntial decay rates β,β1,β2 ∈ [0,1); Basic noise\nλ; Minimum effective sensitivity indicator γ; A\nsmall number that prevents an error of divid-\ning by zero ϵ ∈ (0,1); Data D; Loss function\nL(·).\n1: Initialize ˜S(0) ←0 ∈RN.\n2: Initialize M(0) ←0 ∈RN.\n3: Initialize U(0) ←0 ∈RN.\n4: for t←1 to T do\n5: d(t) sample\n←− D.\n6: G(t) ←∇Θ(t) L(d(t),Θ(t)).\n7: S(t) ←Θ(t) ⊙G(t).\n8: M(t) ←β1M(t−1) + (1 −β1)G(t).\n9: U(t) ←max(β2U(t−1),|G(t)|).\n10: ˜S(t) ←β˜S(t−1) + (1 −β)S(t).\n11: R ←λmax(sum(˜S(t))I ⊘(N˜S(t) + ϵI) −\nγI,0).\n12: Q ∼N(0,R).\n13: Z ∼B(N,p).\n14: Θ(t+1) ←Θ(t)−(α/(1−βt\n1))M(t)⊘U(t)+\nQ ⊙Z.\n15: t←t+ 1\n16: end for\nOur goal is to mainly activate the contributions\nof less sensitive parameters by perturbing them\nwith bigger noise and leave parameters with larger\nsensitivity less affected at the same time. In our\nframework, we use a hyper-parameter λas initial\nnoise and the degree of perturbation to different\nparameters will be scaled up and down based on\nit according to their sensitivity. The intensity of\nperturbation can be formulated by the following\nequations:\n¯s= 1\nN\nN∑\ni=1\n¯si (3)\nrj = λ·max( ¯s\n¯sj + ϵ −γ,0),0 <ϵ ≪1 (4)\nIn Eq. 3, ¯sis the average sensitivity of the ma-\ntrix containing θj with N parameters. rj in Eq. 4\nmeans the intensity of the noise to be added on a\nparameter θj, which is scaled on λby the division\nof ¯sand ¯sj. ϵis a small number used to prevent\nzero denominator. Since rj and ¯sj are inversely\ncorrelated, the intensity of noise added to every\nparameter with a lower sensitivity than the average\nwill be larger than λ, and vice versa as we expect.\nAs for the reason why ¯s is restricted to the cur-\nrent matrix, as we found, the value distributions\nof different matrix parameters are sometimes very\ndifferent. For example, values of parameters in ma-\ntrix A are significant higher than those in matrixB.\nAnd with the sensitivity measurement considered,\nsensitive parameters are usually themselves large\non value. So if ¯sis calculated based on all parame-\nters of the model, some matrices of parameters with\nlow values and sensitivity may be perturbed fiercely\nto some values very far from their original ones,\nwhich has unstable performances on experiments.\nTo further reduce the perturbation on sensitive pa-\nrameters and let them keep regular gradient-driven\nupdate, we use a margin constant γ in Eq. 4 to\nzero-out the noise added on the parameters that are\nhighly sensitive.\nFor each parameter θj, the noise qj that may\nfinally be added to it is independently randomly\nsampled from a Gaussian distribution with the\nmean of zero and the standard deviation of σj as\nqj ∼N(0,σ2\nj), where σj = √rj. So in an itera-\ntion, we update each parameter by:\n˜θj = θj −η·∇θj L(Θ) + qj ·z, (5)\nwhere η is learning rate and z ∼ B(1,p) is a\nrandom value sampled from Bernoulli distribution\nwhich outputs 1 with probability pand 0 with prob-\nability 1 −p. Algorithm 1 shows the PATS algo-\nrithm for Adamax (Kingma and Ba, 2014) opti-\nmizer.\n3682\nModel CoLA\nMcc\nMRPC\nF1\nRTE\nAcc\nSTS-B\nPcc\nQQP\nF1\nQNLI\nAcc\nMNLI\nAcc\nSST\nAcc\nAvg\nScore\nBERTbase 58.94 90.19 68.03 89.28 88.53 91.96 84.63 92.77 83.18\nBERTbase+SAGE 59.45 90.53 71.78 89.81 88.61 91.87 84.59 93.06 83.65\nBERTbase+NoisyTune 60.01 90.34 69.71 89.81 88.58 91.82 84.64 92.85 83.47\nBERTbase+PATS 60.67 91.05 72.08 89.86 88.64 92.02 84.80 92.89 84.00\nRoBERTalarge 66.67 91.89 85.44 91.98 89.26 94.45 90.25 96.10 88.25\nRoBERTalarge+SAGE 67.36 93.27 85.56 92.05 89.27 94.54 90.25 96.25 88.57\nRoBERTalarge+NoisyTune 67.47 93.26 85.52 92.00 89.36 94.53 90.04 96.12 88.56\nRoBERTalarge+PATS 68.62 93.52 86.29 92.23 89.40 94.64 90.44 96.30 88.90\nTable 1: Results of models on GLUE dev set.\n3 Experiments\n3.1 Datasets and Baselines\nWe conduct extensive experiments on the eight\ntasks of the GLUE benchmark (Wang et al., 2018)\nand adopt the publicly available BERT-base (De-\nvlin et al., 2019) and RoBERTa-large (Liu et al.,\n2019) models on every task individually. The fol-\nlowing three baselines are selected for comparison:\n(1) Standard PLM fine-tuning, which fine-tunes\nPLMs directly; (2) NoisyTune (Wu et al., 2022),\nwhich is a noisy training method that adds matrix-\nwise noise before fine-tuning; (3) SAGE (Liang\net al., 2022), which is an optimized learning rate\nschedule which adjusts the learning rate of every\nparameter according to its sensitivity.\n3.2 Performance Evaluation\nOn each task, we repeat our experiments 5 times\nwith different random seeds and report the aver-\nage scores of every model, which are shown in\nTable 1.1 According to the results, PATS opti-\nmized models consistently outperforms directly\nfine-tuned ones on different downstream tasks, es-\npecially on those with small datasets (CoLA &\nMRPC & RTE). Specifically, PATS improves by\naround 2 points on CoLA and RTE, and around 1\npoint on MRPC. In addition, as a parameter-wise\nmethod based on sensitivity, PATS experimentally\noutperforms the matrix-wise noisy method Noisy-\nTune and the sensitivity-based learning rate sched-\nuler SAGE on 7 out of the 8 tasks. The experimen-\ntal results demonstrate the effectiveness of PATS.\n1The results of the MNLI task are obtained by averag-\ning the output accuracies of the models on the mnli-matched\ndataset and the mnli-mismatched dataset.\n0.0 0.5 1.0 1.5 2.0\nSensitivity 1e 6\n0\n1\n2\n3Density\n1e6 CoLA\nPATS-CoLA\nstandard-CoLA\n0.0 0.5 1.0 1.5\nSensitivity 1e 6\n0.0\n0.5\n1.0\n1.5\n2.0Density\n1e6 MRPC\nPATS-MRPC\nstandard-MRPC\n0.0 0.5 1.0 1.5\nSensitivity 1e 6\n0\n1\n2Density\n1e6 RTE\nPATS-RTE\nstandard-RTE\n0 1 2\nSensitivity 1e 6\n0.0\n0.5\n1.0\n1.5Density\n1e6 STS-b\nPATS-STS-b\nstandard-STS-b\nFigure 2: The sensitivity distributions of the parameters\nin different optimized models.\n3.3 Empirical Analysis\nIn this section, we conduct additional analyses on\nsensitivity of parameters in the fine-tuned models.\nFig. 2 shows the sensitivity distribution of the\nmodel parameters fine-tuned in different ways. It\nis found that the sensitivity of the parameters in the\nPATS optimized models is more tightly clustered\nthan that in the models fine-tuned in the common\nway. Besides, there remain fewer insensitive pa-\nrameters in PATS optimized models than those in\nbaseline models. And it is no longer a few high-\nsensitive parameters that dominate the models as\nwhat happens in normal fine-tuning, which indi-\ncates that perturbation helps parameters with low\nsensitivity gain more attention during training and\nlets the contribution of each parameter in the opti-\nmized models more balanced.\nTo further investigate the effect of PATS on small\ndatasets, we also post the accuracies of the models\nfine-tuned on different proportions of training data\nsampled from the CoLA 2 dataset with and with-\n2The phenomena observed on other tasks are similar.\n3683\n1.0\n1.5\n2.0\n2.5std of sensitivity\n1e 6\nPATS\nstandard\n20 30 40 50 60 70 80 90 100\npercentage of training data\n40\n45\n50\n55\n60accuracy\nPATS\nstandard\nFigure 3: Performances of PATS on data of different\nsizes.\nout PATS. Fig. 3 shows PATS optimized models\nconsistently outperform directly fine-tuned ones on\ndifferent sizes of datasets, demonstrating the gener-\nalizability of the approach. Moreover, we can also\nobserve that as the size of training data increases,\nthe performances of the models improve along with\nconcomitant decreases in the standard deviations\nof sensitivity. This phenomenon further indicates\nthat training with limited data will lose some of\nthe performance capabilities of PLMs by leaving\nmore undertrained or insensitive parameters, be-\ncause small datasets is insufficient for PLMs to\novercome the gap between pretraining and down-\nstream tasks. The inverse correlation between ac-\ncuracy and sensitivity concentration justifies our\noriginal intention of balancing the sensitivity of\nparameters. And the displayed performances ex-\nperimentally demonstrate its availability.\n4 Conclusion\nWe propose a novel noisy training method called\nPATS to optimize fine-tuning of PLMs. Since ag-\ngressive fine-tuning PLMs will leave a large num-\nber of insensitive parameters which contribute little\nto the overall model, PATS activates them and bal-\nance the contributions of all parameters in down-\nstream tasks by adding noise to each parameter\naccording to its sensitivity in the process of train-\ning. PATS is a simple mechanism without much\ncomputational and memory overhead compared\nto adversarial training which requires additional\nbackwards passes. Extensive experiments on eight\ntasks of the GLUE benchmark show that PATS can\nconsistently improve the performance of PLMs on\ndownstream tasks with the sensitivity of the pa-\nrameters more concentrated, which is especially\npronounced on small datasets.\nLimitations\nPATS introduces four additional hyperparameters,\nwhich increases some work of users on hyperparam-\neter tuning. For example, a too small λcould make\nfew differences while an overlarge λmay result in\nunstable performances of models. Though we have\nsummarized effective parameter configurations on\nthe NLU tasks of the GLUE benchmark, it cannot\nguarantee that these settings are still applicable on\nother tasks such as neural machine translation. We\nwill explore the connections between the hyperpa-\nrameters in theory and narrow the search ranges of\nthe hyperparameter group in future work.\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-masked language models for uni-\nfied language model pre-training. In Proceedings of\nthe 37th International Conference on Machine Learn-\ning, volume 119 ofProceedings of Machine Learning\nResearch, pages 642–652. PMLR.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second pascal recognising tex-\ntual entailment challenge. In Proceedings of the sec-\nond PASCAL challenges workshop on recognising\ntextual entailment, volume 6, pages 6–4. Venice.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\n3684\nKevin Clark, Minh-Thang Luong, Quoc Le, and Christo-\npher D. Manning. 2020. Pre-training transformers\nas energy-based cloze models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 285–294,\nOnline. Association for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and\nYonatan Belinkov. 2020. Analyzing redundancy in\npretrained transformer models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4908–4926,\nOnline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXiaohan Ding, guiguang ding, Xiangxin Zhou, Yuchen\nGuo, Jungong Han, and Ji Liu. 2019. Global sparse\nmomentum sgd for pruning very deep neural net-\nworks. In Advances in Neural Information Process-\ning Systems, volume 32. Curran Associates, Inc.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. arXiv\npreprint arXiv:2022.06305.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9. Association for Computa-\ntional Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. InAdvances in Neural\nInformation Processing Systems, volume 33, pages\n9782–9793. Curran Associates, Inc.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nZhiqi Huang, Lu Hou, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2021. GhostBERT: Generate\nmore features with cheap operations for BERT. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 6512–\n6523, Online. Association for Computational Lin-\nguistics.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efficient fine-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2177–2190, Online. Association\nfor Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nFrank Klinker. 2010. Exponential moving average ver-\nsus moving exponential average. Mathematische\nSemesterberichte, 58(1):97–107.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to finetune\nlarge-scale pretrained language models. In Interna-\ntional Conference on Learning Representations.\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip\nTorr. 2019. SNIP: Single-Shot Network Pruning\nBased on Connectiono Sensitivity. In International\nConference on Learning Representations.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth International Conference on the Principles of\nKnowledge Representation and Reasoning.\nChen Liang, Haoming Jiang, Simiao Zuo, Pengcheng\nHe, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and\nTuo Zhao. 2022. No parameters left behind: Sensi-\ntivity guided adaptive learning rate for training large\ntransformer models. In International Conference on\nLearning Representations.\nYang Liu. 2019. Fine-tune BERT for extractive summa-\nrization. CoRR, abs/1903.10318.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\n3685\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri\nFrosio, and Jan Kautz. 2019. Importance estimation\nfor neural network pruning. In 2019 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 11256–11264.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo\nAila, and Jan Kautz. 2017. Pruning convolutional\nneural networks for resource efficient inference. In\n5th International Conference on Learning Repre-\nsentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings. OpenRe-\nview.net.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. arXiv preprint arXiv:2006.04884.\nMatthew E. Peters, Sebastian Ruder, and Noah A. Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. In Proceedings of\nthe 4th Workshop on Representation Learning for\nNLP (RepL4NLP-2019), pages 7–14, Florence, Italy.\nAssociation for Computational Linguistics.\nXiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan Shao,\nNing Dai, and XuanJing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nVictor Sanh, Thomas Wolf, and Alexander Rush. 2020.\nMovement pruning: Adaptive sparsity by fine-tuning.\nIn Advances in Neural Information Processing Sys-\ntems, volume 33, pages 20378–20389. Curran Asso-\nciates, Inc.\nHamidreza Shahidi, Ming Li, and Jimmy Lin. 2020.\nTwo birds, one stone: A simple, unified model for\ntext generation from structured and unstructured data.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 3864–\n3870, Online. Association for Computational Lin-\nguistics.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification?\nIn Chinese Computational Linguistics, pages 194–\n206, Cham. Springer International Publishing.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5784–\n5789, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2022. NoisyTune: A little noise can help\nyou finetune pretrained language models better. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 680–685, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nXia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran.\n2019. Autoprune: Automatic network pruning by\nregularizing auxiliary parameters. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020a. Freelb: Enhanced ad-\nversarial training for natural language understanding.\nIn International Conference on Learning Representa-\ntions.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2020b. Incorporating bert into neural machine trans-\nlation. arXiv preprint arXiv:2002.06823.\nWeiming Zhuang, Yonggang Wen, and Shuai Zhang.\n2022. Divergence-aware federated self-supervised\nlearning. In International Conference on Learning\nRepresentations.\n3686\nModel COLA MRPC RTE STS-B QQP QNLI MNLI SST\nBERTbase 1e-4 1e-4 1e-4 2e-4 1e-4 2e-4 8e-5 8e-5\nRoBERTalarge 3e-5 5e-5 5e-5 5e-5 1e-4 1e-5 3e-5 3e-5\nBERTbase+PATS 1e-4 3e-4 3e-4 3e-4 2e-4 2e-4 1e-4 3e-4\nRoBERTalarge+PATS 8e-5 8e-5 8e-5 5e-5 1e-4 3e-5 1e-5 1e-5\nTable 2: Learning rate settings for PATS on the tasks of the GLUE benchmark.\nHyperparameters Range\nλ {5e-7, 8e-7, 1e-6, 2e-6, 3e-6}\nγ {1e-3, 2e-3, 3e-3, 5e-3, 8e-3,2e-2}\nβ {0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85}\nlearning rate {1e-5, 3e-5, 5e-5, 7e-5, 8e-5, 1e-4, 2e-4, 3e-4, 5e-4}\nTable 3: Searching ranges of hyperparameters in our experiments.\nA Appendix\nA.1 Datasets\nThe experiments are conducted on the GLUE\nbenchmark, which contains several types of Natural\nLanguage Understanding (NLU) tasks such as lin-\nguistic acceptability (CoLA, Warstadt et al. 2019),\ntext similarity (STS-B, Cer et al. 2017) and natural\nlanguage inference (RTE & MNLI & QNLI, Da-\ngan et al. 2005; Bar-Haim et al. 2006; Giampiccolo\net al. 2007; Bentivogli et al. 2009; Williams et al.\n2018; Bowman et al. 2015; Rajpurkar et al. 2016)\ntasks. Among the nine tasks, WNLI (Levesque\net al., 2012) task is excluded in our experiments, on\nwhich BERT-like models have no obvious advan-\ntage over other mainstream baselines (Hou et al.,\n2020; Clark et al., 2020; Huang et al., 2021). Con-\nsistent with previous works (Bao et al., 2020; Wu\net al., 2022), we evaluate results on the dev set of\nGLUE.\nA.2 Training Details\nFor all the baseline models and our proposed PATS,\nwe adopt a linear-decay learning rate schedule and\nchoose Adamax (Kingma and Ba, 2014) which is\nthe best-performing optimizer for baseline models\non the GLUE benchmark to optimize the training.\nIn PATS, we perturb the parameters of all the en-\ncoder layers except the Layer Normalization layers.\nIn our training process, we set λ = 2 ×10−6,\nγ = 0 .002, β = 0 .75, p = 0 .2 for all tasks. In\naddition, we adopt a linear warm-up learning rate\nschedule with 0.1 of total training iterations. The\nbatch size of models is uniformly set to 32. We\npost the best performance models on each task after\n10 epochs of training. The learning rates that yields\nthe best generalization performance of models op-\ntimized by PATS and Standard PLM fine-tuning\non each task are listed in Table 2. We present the\nsearching range of hyperparameters in Table 3.\nOur implementation is based on the MT-DNN\ncode-base.3. And we use Nvidia V100 GPUs for\nall experiments.\nA.3 Other Implemention Details\nAll datasets of the GLUE benchmark\nare downloaded from https:// gluebench-\nmark.com/tasks. For the baseline model SAGE,\nwe use the code from the Github respository\nhttps://github.com/cliang1453/SAGE. The other\nbaseline models are implemented by ourselves.\nFor the distribution of sensitivity shown in Fig.\n2, we discard some outliers and only choose the\nparameters with sensitivity in the range of [5e-8,\n1e-5] for visualization.\n3https://github.com/namisan/mt-dnn\n3687"
}