{
  "title": "Handwriting Transformers",
  "url": "https://openalex.org/W3139980562",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2757192054",
      "name": "Ankan Kumar Bhunia",
      "affiliations": [
        "Zayed University"
      ]
    },
    {
      "id": "https://openalex.org/A2096017297",
      "name": "Salman Khan",
      "affiliations": [
        "Zayed University"
      ]
    },
    {
      "id": "https://openalex.org/A2180688527",
      "name": "Hisham Cholakkal",
      "affiliations": [
        "Zayed University"
      ]
    },
    {
      "id": "https://openalex.org/A1998328463",
      "name": "Rao Muhammad Anwer",
      "affiliations": [
        "Zayed University"
      ]
    },
    {
      "id": "https://openalex.org/A2110150181",
      "name": "Fahad Shahbaz Khan",
      "affiliations": [
        "Zayed University"
      ]
    },
    {
      "id": "https://openalex.org/A2171354718",
      "name": "Mubarak A. Shah",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2394605686",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6765779288",
    "https://openalex.org/W3109645351",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W6788556936",
    "https://openalex.org/W3092947864",
    "https://openalex.org/W6779841522",
    "https://openalex.org/W6682579049",
    "https://openalex.org/W2784936144",
    "https://openalex.org/W3003967978",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6783175930",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W3034904792",
    "https://openalex.org/W2784512264",
    "https://openalex.org/W6757193177",
    "https://openalex.org/W6638273328",
    "https://openalex.org/W3013377786",
    "https://openalex.org/W6691459498",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W6736210646",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3148140980",
    "https://openalex.org/W2152928267",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2251939518"
  ],
  "abstract": "We propose a novel transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local writing style patterns. The proposed HWT captures the long and short range relationships within the style examples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder attention that enables style-content entanglement by gathering the style representation of each query character. To the best of our knowledge, we are the first to introduce a transformer-based generative network for styled handwritten text generation. Our proposed HWT generates realistic styled handwritten text images and significantly outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT generalizes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images.",
  "full_text": "Handwriting Transformers\nAnkan Kumar Bhunia1 Salman Khan1,2 Hisham Cholakkal1 Rao Muhammad Anwer1\nFahad Shahbaz Khan1,3 Mubarak Shah4\n1Mohamed bin Zayed University of AI, UAE 2Australian National University, Australia\n3Link¨oping University, Sweden 4University of Central Florida, USA\nAbstract\nWe propose a novel transformer-based styled handwrit-\nten text image generation approach, HWT, that strives to\nlearn both style-content entanglement as well as global\nand local writing style patterns. The proposed HWT cap-\ntures the long and short range relationships within the style\nexamples through a self-attention mechanism, thereby en-\ncoding both global and local style patterns. Further, the\nproposed transformer-based HWT comprises an encoder-\ndecoder attention that enables style-content entanglement\nby gathering the style representation of each query char-\nacter. To the best of our knowledge, we are the ﬁrst to in-\ntroduce a transformer-based generative network for styled\nhandwritten text generation.\nOur proposed HWT generates realistic styled handwrit-\nten text images and signiﬁcantly outperforms the state-of-\nthe-art demonstrated through extensive qualitative, quanti-\ntative and human-based evaluations. The proposed HWT\ncan handle arbitrary length of text and any desired writ-\ning style in a few-shot setting. Further, our HWT general-\nizes well to the challenging scenario where both words and\nwriting style are unseen during training, generating realis-\ntic styled handwritten text images.\n1. Introduction\nGenerating realistic synthetic handwritten text images,\nfrom typed text, that is versatile in terms of both writ-\ning style and lexicon is a challenging problem. Automatic\nhandwritten text generation can be beneﬁcial for people\nhaving disabilities or injuries that prevent them from writ-\ning, translating a note or a memo from one language to an-\nother by adapting an author’s writing style or gathering ad-\nditional data for training deep learning-based handwritten\ntext recognition models. Here, we investigate the problem\nof realistic handwritten text generation of unconstrained\ntext sequences with arbitrary length and diverse calligraphic\nattributes representing writing styles of a writer.\nGenerative Adversarial Networks (GANs) [8] have been\nFigure 1: Comparison of HWT (c) with GANwriting [14]\n(d) and Davis et al. [5] (e) in imitating the desired unseen\nwriting style (a) for given query text (b). While [14, 5]\ncapture global writing styles ( e.g., slant), they struggle to\nimitate local style patterns ( e.g., character style, ligatures).\nHWT (c) imitates both global and local styles, leading to\na more realistic styled handwritten text image generation.\nFor instance, style of ‘n’ (red line) appearing in (a) is mim-\nicked by HWT, for a different word including same char-\nacter ‘n’. Similarly, a group of characters in ‘ thought’\nand ‘personalities’ (blue and magenta lines) are styled\nin a way that matches with words (‘ throughout’ and\n‘qualities’) sharing some common characters in (a).\nFurthermore, HWT preserves cursive patterns and connec-\ntivity of all characters in word ‘also’ (green line).\ninvestigated for ofﬂine handwritten text image generation\n[4, 3, 14, 7, 5]. These methods strive to directly synthe-\narXiv:2104.03964v1  [cs.CV]  8 Apr 2021\nsize text images by using ofﬂine handwriting images during\ntraining, thereby extracting useful features, such as writing\nappearance (e.g., ink width, writing slant) and line thickness\nchanges. Alonso et al. [3] propose a generative architec-\nture that is conditioned on input content strings, thereby not\nrestricted to a particular pre-deﬁned vocabulary. However,\ntheir approach is trained on isolated ﬁxed-sized word im-\nages and struggles to produce high quality arbitrarily long\ntext along with suffering from style collapse. Fogelet al. [7]\nintroduce a ScrabbleGAN approach, where the generated\nimage width is made proportional to the input text length.\nScrabbleGAN is shown to achieve impressive results with\nrespect to the content. However, both [3, 7] do not adapt to\na speciﬁc author’s writing style.\nRecently, GAN-based approaches [5, 14] have been in-\ntroduced for the problem of styled handwritten text image\ngeneration. These methods take into account both content\nand style, when generating ofﬂine handwritten text images.\nDavis et al. [5] propose an approach based on StyleGAN\n[15] and learn generated handwriting image width based on\nstyle and input text. The GANwriting framework [14] con-\nditions handwritten text generation process to both textual\ncontent and style features in a few-shot setup.\nIn this work, we distinguish two key issues that impede\nthe quality of styled handwritten text image generation in\nthe existing GAN-based methods [5, 14]. First, both style\nand content are loosely connected as their representative\nfeatures are processed separately and later concatenated.\nWhile such a scheme enables entanglement between style\nand content at the word/line-level, it does not explicitly en-\nforce style-content entanglement at the character-level. Sec-\nond, although these approaches capture global writing style\n(e.g., ink width, slant), they do not explicitly encode local\nstyle patterns ( e.g., character style, ligatures). As a result\nof these issues, they struggle to accurately imitate local cal-\nligraphic style patterns from reference style examples (see\nFig. 1). Here, we look into an alternative approach that ad-\ndresses both these issues in a single generative architecture.\n1.1. Contributions\nWe introduce a new styled handwritten text genera-\ntion approach built upon transformers, termed Handwriting\nTransformers (HWT), that comprises an encoder-decoder\nnetwork. The encoder network utilizes a multi-headed self-\nattention mechanism to generate a self-attentive style fea-\nture sequence of a writer. This feature sequence is then\ninput to the decoder network that consists of multi-headed\nself- and encoder-decoder attention to generate character-\nspeciﬁc style attributes, given a set of query word strings.\nConsequently, the resulting output is fed to a convolutional\ndecoder to generate ﬁnal styled handwritten text image.\nMoreover, we improve the style consistency of the gen-\nerated text by constraining the decoder output through a\nloss term whose objective is to re-generate style feature se-\nquence of a writer at the encoder.\nOur HWT imitates the style of a writer for a given query\ncontent through self- and encoder-decoder attention that\nemphasizes relevant self-attentive style features with re-\nspect to each character in that query. This enables us to cap-\nture style-content entanglement at the character-level. Fur-\nthermore, the self-attentive style feature sequence generated\nby our encoder captures both the global ( e.g., ink width,\nslant ) and local styles (e.g., character style, ligatures) of a\nwriter within the feature sequence.\nWe validate our proposed HWT by conducting extensive\nqualitative, quantitative and human-based evaluations. In\nthe human-based evaluation, our proposed HWT was pre-\nferred 81% of the time over recent styled handwritten text\ngeneration methods [5, 14], achieving human plausibility in\nterms of the writing style mimicry. Following GANwrit-\ning [14], we evaluate our HWT on all the four settings on\nthe IAM handwriting dataset. On the extreme setting of\nout-of-vocabulary and unseen styles (OOV-U), where both\nquery words and writing styles are never seen during train-\ning, the proposed HWT outperforms GANwriting [14] with\nan absolute gain of 16.5 in terms of Fr `echet Inception Dis-\ntance (FID) thereby demonstrating our generalization capa-\nbilities. Further, our qualitative analysis suggest that HWT\nperforms favorably against existing works, generating real-\nistic styled handwritten text images (see Fig. 1).\n2. Related Work\nDeep learning-based handwritten text generation ap-\nproaches can be roughly divided into stroke-based online\nand image-based ofﬂine methods. Online handwritten text\ngeneration methods [9, 2] typically require temporal data\nacquired from stroke-by-stroke recording of real handwrit-\nten examples (vector form) using a digital stylus pen. On\nthe other hand, recent generative ofﬂine handwritten text\ngeneration methods [4, 3, 14, 7] aim to directly generate\ntext by performing training on ofﬂine handwriting images.\nGraves [9] proposes an approach based on Recurrent\nNeural Network (RNN) with Long-Term Memory (LSTM)\ncells, which enables predicting future stroke points from\nprevious pen positions and an input text. Aksan et al. [4]\npropose a method based on conditional Variational RNN\n(VRNN), where the input is split into two separate latent\nvariables to represent content and style. However, their ap-\nproach tends to average out particular styles across writers,\nthereby reducing details [17]. In a subsequent work [1], the\nVRNN module is substituted by Stochastic Temporal CNNs\nwhich is shown to provide more consistent generation of\nhandwriting. Kotani et al . [17] propose an online hand-\nwriting stroke representation approach to represent latent\nstyle information by encoding writer-, character- and writer-\ncharacter-speciﬁc style changes within an RNN model.\nOther than sequential methods, several recent works\nhave investigated ofﬂine handwritten text image generation\nusing GANs. Haines et al. [11] introduce an approach to\ngenerate new text in a distinct style inferred from source im-\nages. Their model requires a certain degree of human inter-\nvention during character segmentation and is limited to gen-\nerating characters that are in the source images. The work of\n[4] utilize CycleGAN [24] to synthesize images of isolated\nhandwritten characters of Chinese language. Alonso et al.\n[3] propose an approach, where handwritten text generation\nis conditioned by character sequences. However, their ap-\nproach suffers from style collapse hindering the diversity of\nsynthesized images. Fogel et al. [7] propose an approach,\ncalled ScrabbleGAN, that synthesizes handwritten word us-\ning a fully convolutional architecture. Here, the characters\ngenerated have similar receptive ﬁeld width. A conversion\nmodel is introduced by [20] that approximates online hand-\nwriting from ofﬂine samples followed by using style trans-\nfer technique to the online data. This approach relies on\nconversion model’s performance.\nFew recent GAN-based works [5, 14] investigate the\nproblem of ofﬂine styled handwritten text image generation.\nDavis et al . [5] propose an approach, where handwritten\ntext generation is conditioned on both text and style, cap-\nturing global handwriting style variations. Kang et al. [14]\npropose a method, called GANwriting, that conditions text\ngeneration on extracting style features in a few-shot setup\nand textual content of a predeﬁned ﬁxed length.\nOur Approach: Similar to GANwriting [14], we also in-\nvestigate the problem of styled handwritten text generation\nin a few-shot setting, where a limited number of style exam-\nples are available for each writer. Different from GANwrit-\ning, our approach possesses the ﬂexibility to generate styled\ntext of arbitrary length. In addition, existing works [5, 14]\nonly capture style-content entanglement at the word/line-\nlevel. In contrast, our transformer-based approach enables\nstyle-content entanglement both at the word and character-\nlevel. While [5, 14] focuses on capturing the writing style\nat the global level, the proposed method strives to imitate\nboth global and local writing style.\n3. Proposed Approach\nMotivation: To motivate our proposed HWT method,\nwe ﬁrst distinguish two desirable characteristics to be con-\nsidered when designing an approach for styled handwritten\ntext generation with varying length and any desired style in\na few-shot setting, without using character-level annotation.\nStyle-Content Entanglement: As discussed earlier, both\nstyle and content are loosely connected in recently intro-\nduced GAN-based works [14, 5] with separate processing\nof style and content features, which are later concatenated.\nSuch a scheme does not explicitly encode style-content en-\ntanglement at the character-level. Moreover, there are sep-\narate components for style, content modeling followed by\na generator for decoding stylized outputs. In addition to\nstyle-content entanglement at word/line level, an entangle-\nment between style and content at the character-level is ex-\npected to aid in imitating the character-speciﬁc writing style\nalong with generalizing to out-of-vocabulary content. Fur-\nther, such a tight integration between style and content leads\nto a cohesive architecture design.\nGlobal and Local Style Imitation:While the previous req-\nuisite focuses on connecting style and content, the second\ndesirable characteristic aims at modeling both the global as\nwell as local style features for a given calligraphic style. Re-\ncent generative methods for styled handwritten text genera-\ntion [14, 5] typically capture the writing style at the global\nlevel (e.g., ink width, slant). However, the local style pat-\nterns (e.g., character style, ligatures) are not explicitly taken\ninto account while imitating the style of a given writer. We\nargue that both global and local style patterns are desired to\nbe imitated for accurate styled text image generation.\n3.1. Approach Overview\nProblem Formulation: We aim to learn the complex\nhandwriting style characteristics of a particular writer i ∈\nW, where Wincludes a total of M writers. We are given\na set of P handwritten word images, Xs\ni = {xij}P\nj=1, as\nfew-shot calligraphic style examples of each writer. The\nsuperscript ‘s’ in Xs\ni denotes use of the set as a source of\nhandwriting style which is transferred to the target images\n˜Xt\ni with new textual content but consistent style properties.\nThe textual content is represented as a set of input query\nword strings A = {aj}Q\nj=1, where each word string aj\ncomprises an arbitrary number of characters from permitted\ncharacters set C. The set Cincludes alphabets, numerical\ndigits and punctuation marks etc. Given a query text string\naj ∈A from an unconstrained set of vocabulary and Xs\ni,\nour model strives to generate new images ˜Xt\ni with the same\ntext aj in the writing style of a desired writer i.\nOverall Architecture: Fig. 2 presents an overview of\nour proposed HWT approach, where a conditional genera-\ntor Gθ synthesizes handwritten text images, a discrimina-\ntor Dψ ensures realistic generation of handwriting styles,\na recognizer Rφ aids in textual content preservation, and a\nstyle classiﬁer Sη ensures satisfactory transfer of the calli-\ngraphic styles. The focus of our design is the introduction of\na transformer-based generative network for unconstrained\nstyled handwritten text image generation. Our generatorGθ\nis designed in consideration to the desirable characteristics\nlisted earlier leveraging the impressive learning capabilities\nof transformer models. To meticulously imitate a handwrit-\ning style, a model is desired to learn style-content entangle-\nment as well as global and local style patterns.\nTo this end, we introduce a transformer-based handwrit-\ning generation model, which enables us to capture the long\nFigure 2: Overall architecture of our Handwriting Transformers (HWT) to generate styled handwritten text images˜Xt\ni. HWT\ncomprises a conditional generator having an encoder TEand a decoder network TD. Both the encoder and decoder networks\nconstitute a hybrid convolution and multi-head self-attention design, which combines the strengths of CNN and transformer-\nbased models i.e., highly expressive relationship modeling while working with limited handwriting style example images.\nResultantly, our design seamlessly achieves style-content entanglement that encodes relationships between textual content\nand writer’s style along with learning both global and local style patterns for given inputs (Xs\ni and A).\nand short range contextual relationships within the style ex-\namples Xs\ni by utilizing a self-attention mechanism. In this\nway, both the global and local style patterns are encoded.\nAdditionally, our transformer-based model comprises an\nencoder-decoder attention that allows style-content entan-\nglement by inferring the style representation for each query\ncharacter. A direct applicability of transformer-based de-\nsign is infeasible in our few-shot setting due to its large data\nrequirements and quadratic complexity. To circumvent this\nissue, our proposed architecture design utilizes the expres-\nsivity of a transformer within the CNN feature space.\nThe main idea of the proposed HWT method is simple\nbut effective. A transformer-based encoder TEis ﬁrst used\nto model self-attentive style context that is later used by\na decoder TD to generate query text in a speciﬁc writer’s\nstyle. We deﬁne learnable embedding vector qc ∈R512 for\neach character c of the permissible character set C. For ex-\nample, we represent the query word ‘deep’ as a sequence of\nits respective character embeddings Qdeep = {qd ... qp}.\nWe refer them as query embeddings. Such a character-wise\nrepresentation of the query words and the transformer-based\nsequence processing helps our model to generate handwrit-\nten words of variable length, and also qualiﬁes it to pro-\nduce out-of-vocabulary words more efﬁciently. Moreover,\nit avoids averaging out individual character-speciﬁc styles\nin order to maintain the overall (global and local) writing\nstyle. The character-wise style interpolation and transfer is\nensured by the self- and encoder-decoder attention in the\ntransformer module that infers the style representation of\neach query character based on a set of handwritten samples\nprovided as input. We describe the proposed generative ar-\nchitecture in Sec. 3.2 and the loss objectives in Sec. 3.3.\n3.2. Generative Network\nThe generator Gθ includes two main components: an\nencoder network TE : Xs\ni → Z and a decoder network\nTD : (Z,A) → ˜Xt\ni. The encoder produces a sequence\nof feature embeddings Z ∈ RN×d (termed as style fea-\nture sequence) from a given set of style examples Xs\ni. The\ndecoder takes Z as an input and converts the input word\nstrings aj ∈ Ato realistic handwritten images ˜Xt\ni with\nsame style as the given examples Xs\ni of a writer i. Both\nthe encoder and decoder networks constitute a hybrid de-\nsign based on convolution and multi-head self-attention net-\nworks. This design choice combines the strengths of CNNs\nand transformer models i.e., highly expressive relationship\nmodeling while working with limited handwriting images.\nIts worth mentioning that a CNN-only design would strug-\ngle to model long-term relations within sequences while an\narchitecture based solely on transformer networks would\ndemand large amount of data and longer training times [16].\nEncoder TE. The encoder aims at modelling both global\nand local calligraphic style attributes (i.e., slant, skew, char-\nacter shapes, ligatures, ink widths etc.) from the style ex-\namples Xs\ni. Before feeding style images to the highly ex-\npressive transformer architecture, we need to represent the\nstyle examples as a sequence. A straightforward way would\nbe to ﬂatten the image pixels into a 1D vector [6]. How-\never, given the quadratic complexity of transformer models\nand their large data requirements, we ﬁnd this to be infea-\nsible. Instead, we use a CNN backbone network to obtain\nsequences of convolutional features from the style images.\nFirst, we use a ResNet18 [12] model to generate lower-\nresolution activation maps hij ∈ Rh×w×d for each style\nimage xij. Then, we ﬂatten the spatial dimension of hij\nto obtain a sequence of feature maps of size n×d, where\nn= h×w. Each vector in the feature sequence represents\na region in the original image and can be considered as the\nimage descriptor for that particular region. After that, we\nconcatenate the feature sequence vectors extracted from all\nstyle images together to obtain a single tensorHi ∈RN×d,\nwhere N = n×P.\nThe next step includes modeling the global and local\ncompositions between all entities of the obtained feature se-\nquence Z. A transformer-based encoder is employed for\nthat purpose. The encoder has Llayers, where each layer\nhas a standard architecture that consists of a multi-headed\nself-attention module and a Multi-layer Perceptron (MLP)\nblock. At each layer l, the multi-headed self-attention maps\nthe input sequence from the previous layer Hl−1 into a\ntriplet (key K, query Q, value V ) of intermediate repre-\nsentations given by,\nQ = Hl−1WQ,K = Hl−1WK,V = Hl−1WV,\nwhere WQ ∈RN×dq , WK ∈RN×dk and WV ∈RN×dv\nare the learnable wight matrix for query, key and value re-\nspectively. For each head, the process is represented as,\nOj = softmax\n(QKT\n√dk\n)\nV ∈RN×dv , j ∈{1,..,J }.\n(1)\nThe concatenation of all Jhead outputs O = [O1,..., OJ]\nis then fed through an MLP layer to obtain the output fea-\nture sequence Hl for the layer l. This update procedure is\nrepeated for a total of Llayers, resulting in the ﬁnal feature\nsequence Z ∈RN×d. To retain information regarding the\norder of input sequences being supplied, we add ﬁxed posi-\ntional encodings [23] to the input of each attention layer.\nDecoder TD. The initial stage in the decoder uses the\nstandard architecture of the transformer that consists of\nmulti-headed self- and encoder-decoder attention mecha-\nnisms. Unlike the self-attention, the encoder-decoder at-\ntention derives the key and value vectors from the output of\nthe encoder, whereas the query vectors come from the de-\ncoder layer itself. For an mj character word aj ∈A(length\nFigure 3: Visualization of encoder-decoder attention maps\nat the last layer of the transformer decoder. The attention\nmaps are computed for each character in the query word\n(‘statistical’) which are then mapped to spatial regions\n(heat maps) in the example style images. Here, heat maps\ncorresponding to the four different query characters ‘t’, ‘i’,\n‘c’ and ‘l’ are shown. For instance, the top-left attention\nmap corresponding to the character ‘t’, highlights multiple\nimage regions containing the character ‘t’.\nmj being variable depending on the word), the query em-\nbedding Qaj = {qck }mj\nk=1 is used as a learnt positional en-\ncoding to each attention layer of the decoder. Intuitively,\neach query embedding learns to look up regions of interest\nin the style images to infer the style attributes of all query\ncharacters (see Fig. 3). Over multiple consecutive decoding\nlayers, these output embeddings accumulate style informa-\ntion, producing a ﬁnal output Faj = {fck }mj\nk=1 ∈Rmj×d.\nWe process the entire query embedding in parallel at each\ndecoder layer. We add a randomly sampled noise vector\nN(0,1) to the outputFaj in order to model the natural vari-\nation of individual handwriting. For an m-character word,\nwe concatenate these mj embedding vectors and pass them\nthrough a linear layer, resulting in an mj ×8192 matrix.\nAfter reshaping it to a dimension of 512 ×4 ×4mj, we\npass it through a CNN decoder having four residual blocks\nfollowed by a tanh activation layer to obtain ﬁnal output\nimages (styled hand written text images).\n3.3. Training and Loss Objectives\nOur training algorithm follows the traditional GAN\nparadigm, where a discriminator network Dψ is employed\nto tell apart the samples generated from generator Gθ from\nthe real ones. As the generated word images are of varying\nwidth, the proposed discriminator Dψ is also designed to\nbe convolutional in nature. We use the hinge version of the\nadversarial loss [18] deﬁned as,\nLadv =E[max (1−Dψ(Xs\ni,0))] +\nE[max (1 +Dψ(Gθ(Xs\ni,A)),0)] . (2)\nWhile Dψ promotes real-looking images, it does not pre-\nserve the content or the calligraphic styles. To preserve the\ntextual content in the generated samples we use a handwrit-\nten recognizer network Rφ that examines whether the gen-\nerated samples are actually real text. The recognizer Rφ\nis inspired by CRNN [21]. The CTC loss [10] is used to\ncompare the recognizer output to the query words that were\ngiven as input toGθ. Recognizer Rφ is only optimized with\nreal, labelled, handwritten samples, but it is used to encour-\nage Gθ to produce readable text with accurate content. The\nloss is deﬁned as,\nLR = Ex∼{Xs\ni , ˜Xt\ni }\n[\n−\n∑\nlog (p(yr|Rφ(x)))\n]\n. (3)\nHere, yr is the transcription string of x ∼\n{\nXs\ni, ˜Xt\ni\n}\n.\nA style classiﬁer network Sη is employed to guide the\nnetwork Gθ in producing samples conditioned to a partic-\nular writing style. The network Sη attempts to predict the\nwriter of a given handwritten image. The cross-entropy ob-\njective is applied as a loss function. Sη is trained only on\nthe real samples using the loss given below,\nLS = Ex∼{Xs\ni , ˜Xt\ni }\n[\n−\n∑\nyilog(Sη(x))\n]\n. (4)\nAn important feature of our design is to utilize a cycle\nloss that ensures the encoded style features have cycle con-\nsistency. This loss function enforces the decoder to preserve\nthe style information in the decoding process, such that the\noriginal style feature sequence can be reconstructed from\nthe generated image. Given the generated word images ˜Xt\ni,\nwe use the encoder TE to reconstruct the style feature se-\nquence ˜Z. The cycle loss Lc minimizes the error between\nthe style feature sequence Z and its reconstruction ˜Z by\nmeans of a L1 distance metric,\nLc = E\n[TE(Xs\ni) −TE( ˜Xt\ni)\n\n1\n]\n. (5)\nThe cycle loss imposes a regularization to the decoder for\nconsistently imitating the writing style in the generated\nstyled text images. Overall, we train our HWT model in\nan end-to-end manner with the following loss objective,\nLtotal = Ladv + LS + LR + Lc. (6)\nWe observe balancing the gradients of the network Sη and\nRφ is helpful in the training with our loss formulation. Fol-\nlowing [3], we normalize the ∇Sη and ∇Rφ to have the\nsame standard deviation (σ) as adversarial loss gradients,\n∇Sη ←α\n(σD\nσS\n.∇Sη\n)\n,∇Rφ ←α\n(σD\nσR\n.∇Rφ\n)\n. (7)\nHere, α is a hyper-parameter that is ﬁxed to 1 during the\ntraining of our model.\n4. Experiments\nWe perform extensive experiments on IAM handwrit-\ning dataset [19]. It consists of 9862 text lines with around\nTable 1: Comparison of the HWT with GANwriting [14]\nand Daviset al. [5]in terms of FID scores computed be-\ntween the generated text images and real text images of the\nIAM dataset. Our HWT performs favorably against [14, 5]\nin all four settings: In-V ocabulary words and seen style\n(IV-S), In-V ocabulary words and unseen style (IV-U), Out-\nof-vocabulary content and seen style (OOV-S) and Out-of-\nvocabulary content and unseen style (OOV-U). On the chal-\nlenging setting of OOV-U, HWT achieves an absolute gain\nof 16.5 in FID score, compared to GANwriting [14]. Best\nresults are in bold.\nIV-S↓ IV-U↓ OOV-S↓ OOV-U↓\nGANwriting [14] 120.07 124.30 125.87 130.68\nDaviset al. [5] 118.56 128.75 127.11 136.67\nHWT (Ours) 106.97 108.84 109.45 114.10\n62,857 English words, written by 500 different writers. For\nthorough evaluation, we reserve an exclusive subset of 160\nwriters for testing, while images from the remaining 340\nwriters are used for our model training. In all our exper-\niments, we resize images to a ﬁxed height of 64 pixels,\nwhile maintaining the aspect ratio of original image. For\ntraining, we use P = 15style example images, as in [14].\nBoth the transformer encoder and transformer decoder net-\nworks employ three attention layers (L= 3) and each atten-\ntion layer applies multi-headed attention having 8 attention\nheads (J = 8). We set the embedding size d to 512. In\nall experiments, we train our model for 4k epochs with a\nbatch size of 8 on a single V100 GPU. Adam optimizer is\nemployed during training with a learning rate of 0.0002.\n4.1. Styled Handwritten Text Generation\nWe ﬁrst evaluate (Tab. 1) our approach for styled hand-\nwritten text image generation, where both style and content\nare desired to be imitated in the generated text image. Fol-\nlowing [14], we use Fr `echet Inception Distance (FID) [13]\nevaluation metric for comparison. The FID metric is mea-\nsured by computing the distance between the Inception-v3\nfeatures extracted from generated and real samples for each\nwriter and then averaging across all writers. We evaluate\nour HWT with GANwriting [14] and Davis et al . [5] in\nfour different settings: In-V ocabulary words and seen styles\n(IV-S), In-V ocabulary words and unseen styles (IV-U), Out-\nof-V ocabulary words and seen styles (OOV-S), and Out-\nof-V ocabulary words and unseen styles (OOV-U). Among\nthese settings, most challenging one is the OOV-U, where\nboth words and writing styles are never seen during training.\nFor OOV-S and OOV-U settings, we use a set of 400 words\nthat are distinct from IAM dataset transcription, as in [14].\nIn all four settings, the transcriptions of real samples and\ngenerated samples are different. Tab. 1 shows that HWT\nperforms favorably against both existing methods [14, 5].\nFig 4 presents the qualitative comparison of HWT with\nFigure 4: Qualitative comparison of our HWT (second column) with GANwriting [14] (third column) and Davis et al. [5]\n(fourth column). We use the same textual content ’No two people can write precisely the same way just like no two people\ncan have the same ﬁngerprints’ for all three methods. The ﬁrst column shows the style examples from different writers.\nDavis et al. [5] captures the global style, e.g. slant, but struggles to mimic the character-speciﬁc style details. On the other\nhand, since GANwriting [14] is limited to a ﬁxed length query words, it is unable to complete the provided textual content.\nOur HWT better mimics global and local style patterns, generating more realistic handwritten text images.\n[14, 5] for styled handwritten text generation. We present\nresults for different writers, whose example style images are\nshown in the ﬁrst column. For all the three methods, we use\nthe same textual content. While Davis et al. [5] follows the\nleftward slant of the last style example from the top, their\napproach struggles to capture character-level styles and cur-\nsive patterns (e.g. see the word ‘the’). On the other hand,\nGANwriting [14] struggles to follow leftward slant of the\nlast style example from the top and character-level styles.\nOur HWT better imitates both the global and local style pat-\nterns in these generated example text images.\n4.2. Handwritten Text Generation\nHere, we evaluate the quality of the handwritten text im-\nage generated by our HWT. For a fair comparison with the\nrecently introduced ScrabbleGAN [7] and Davis et al. [5],\nwe report our results in the same evaluation settings as used\nby [7, 5]. Tab. 2 presents the comparison with [7, 5] in\nterms of FID and geometric-score (GS). Our HWT achieves\nfavourable performance, compared to both approaches in\nterms of both FID and GS scores. Different from Tab. 1, the\nresults reported here in Tab. 2 indicates the quality of the\ngenerated images, compared with the real examples in the\nIAM dataset, while ignoring style imitation capabilities.\n4.3. Ablation study\nWe perform multiple ablation studies on the IAM dataset\nto validate the impact of different components in our frame-\nTable 2: Handwritten text image generation quality com-\nparison of our proposed HWT with ScrabbleGAN [7]\nand Davis et al. [5] on the IAM dataset. Results are re-\nported in terms of FID and GS by following the same eval-\nuation settings, as in [7, 5]. Our HWT performs favorably\nagainst these methods in terms of both FID and GS. Best\nresults are in bold.\nFID ↓ GS ↓\nScrabbleGAN [7] 20.72 2.56 ×10−2\nDavis et al. [5] 20.65 4.88 ×10−2\nHWT (Ours) 19.40 1.01 × 10−2\nwork. Tab. 3 shows the impact of integrating transformer\nencoder (Enc), transformer decoder (Dec) and cycle loss\n(CL) to the baseline (Base). Our baseline neither uses trans-\nformer modules nor utilizes cycle loss. It only employs\na CNN encoder to obtain style features, whereas the con-\ntent features are extracted from the one-hot representation\nof query words. Both content and style features are passed\nthrough a CNN decoder to generate styled handwritten text\nimages. While the baseline is able to generate realistic text\nimages, it has a limited ability to mimic the given writer’s\nstyle leading to inferior FID score (row 1). The introduc-\ntion of the transformer encoder into the baseline (row 2)\nleads to an absolute gain of 5.6 in terms of FID score,\nhighlighting the importance of our transformer-based self-\nattentive feature sequence in the generator encoder. We ob-\nTable 3: Impact of integrating transformer encoder\n(Enc), transformer decoder (Dec) and cycle loss (CL) to\nthe baseline (Base)on the OOV-U settings of IAM dataset.\nResults are reported in terms of FID score. Best results are\nreported in bold. On right, we show the effect of each com-\nponent when generating two example words ‘freedom’ and\n‘precise’ mimicking two given writing styles.\nFID ↓ Style Example\nBase 134.45\nBase + Enc 128.80\nBase + Dec 124.81\nBase + Enc + Dec 116.50\nBase + Enc + Dec + CL 114.10\nserve here that the generated sample still lacks details in\nterms of character-speciﬁc style patterns. When integrat-\ning the transformer decoder into the baseline (row 3), we\nobserve a signiﬁcant gain of 9.6 in terms of FID score. No-\ntably, we observe a signiﬁcant improvement (17.9 in FID)\nwhen integrating both transformer encoder and decoder to\nthe baseline (row 4). This indicates the importance of self-\nand encoder-decoder attention for achieving realistic styled\nhandwritten text image generation. The performance is fur-\nther improved by the introduction of cycle loss to our ﬁnal\nHWT architecture (row 4).\nAs described earlier (Sec. 3.2), HWT strives for style-\ncontent entanglement at character-level by feeding query\ncharacter embeddings to the transformer decoder network.\nHere, we evaluate the effect of character-level content en-\ncoding (conditioning) by replacing it with word-level con-\nditioning. We obtain the word-level embeddings, by using\nan MLP that aims to obtain string representation of each\nquery word. These embeddings are used as conditional in-\nput to the transformer decoder. Table 4 suggests that HWT\nbeneﬁts from character-level conditioning that ensures ﬁner\ncontrol of text style. The performance of word-level condi-\ntioning is limited to mimicking the global style, whereas our\ncharacter-level approach ensures locally realistic as well as\nglobally consistent style patterns.\n4.4. Human Evaluation\nHere, we present results of our two user studies on 100\nhuman participantsto evaluate whether the proposed HWT\nachieves human plausibility in terms of the style mimicry.\nFirst, a User preference study compares styled text im-\nages generated by our method with GANwriting [14] and\nDavis et al. [5]. Second, a User plausibility study that eval-\nuates the proximity of the synthesized samples generated by\nour method to the real samples. In both studies, synthesized\nsamples are generated using unseen writing styles of test set\nTable 4: Comparison between word and character-level\nconditioning on IAM dataset. Results are reported in terms\nof FID score. Our character-level conditioning performs fa-\nvorably, compared to its word-level counterpart. Best re-\nsults are reported in bold. On the right, we show the effect\nof word and character-level conditioning, when generating\ntwo example words ‘symbols’ and ‘same’ mimicking two\ngiven writing styles.\nFID↓ Style Example\nWord-level 126.87\nCharacter-level 114.10\nwriters of IAM dataset, and for textual content we use sen-\ntences from Stanford Sentiment Treebank [22] dataset.\nFor User preference study , each participant is shown\nthe real handwritten paragraph of a person and synthesized\nhandwriting samples of that person using HWT, Davis et\nal. [5] and GANwriting [14], randomly organized. The par-\nticipants were asked to mark the best method for mimicking\nthe real handwriting style. In total, we have collected 1000\nresponses. The results of this study shows that our proposed\nHWT was preferred 81% of the time over the other two\nmethods.\nFor User plausibility study , each participant is shown\na person’s actual handwriting, followed by six samples,\nwhere each of these samples is either genuine or synthesized\nhandwriting of the same person. Participants are asked\nto identify whether a given handwritten sample is genuine\nor not (forged/synthesized) by looking at the examples of\nthe person’s real handwriting. Thus, each participant pro-\nvides 60 responses, thereby we collect 6000 responses for\n100 participants. For this study, only 48.1% of the images\nhave been correctly classiﬁed, thereby showing a compara-\nble performance to a random choice in a two-class problem.\n5. Conclusion\nWe introduced a transformer-based styled handwritten\ntext image generation approach, HWT, that comprises a\nconditional generator having an encoder-decoder network.\nOur HWT captures the long and short range contextual re-\nlationships within the writing style example through a self-\nattention mechanism, thereby encoding both global and lo-\ncal writing style patterns. In addition, HWT utilizes an\nencoder-decoder attention that enables style-content entan-\nglement at the character-level by inferring the style repre-\nsentation for each query character. Qualitative, quantitative\nand human-based evaluations show that our HWT produces\nrealistic styled handwritten text images with varying length\nand any desired writing style.\nReferences\n[1] Emre Aksan and Otmar Hilliges. Stcn: Stochastic temporal\nconvolutional networks. arXiv preprint arXiv:1902.06568 ,\n2019. 2\n[2] Emre Aksan, Fabrizio Pece, and Otmar Hilliges. Deepwrit-\ning: Making digital ink editable via deep generative model-\ning. In CHI, pages 1–14, 2018. 2\n[3] Eloi Alonso, Bastien Moysset, and Ronaldo Messina. Ad-\nversarial generation of handwritten text images conditioned\non sequences. In ICDAR, pages 481–486. IEEE, 2019. 1, 2,\n3, 6\n[4] Bo Chang, Qiong Zhang, Shenyi Pan, and Lili Meng. Gen-\nerating handwritten chinese characters using cyclegan. In\nWACV, pages 199–207. IEEE, 2018. 1, 2, 3\n[5] Brian Davis, Chris Tensmeyer, Brian Price, Curtis Wiging-\nton, Bryan Morse, and Rajiv Jain. Text and style conditioned\ngan for generation of ofﬂine handwriting lines.BMVC, 2020.\n1, 2, 3, 6, 7, 8, 5, 9, 10, 11, 12, 13\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 5\n[7] Sharon Fogel, Hadar Averbuch-Elor, Sarel Cohen, Shai Ma-\nzor, and Roee Litman. Scrabblegan: semi-supervised vary-\ning length handwritten text generation. In CVPR, pages\n4324–4333, 2020. 1, 2, 3, 7\n[8] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. arXiv\npreprint arXiv:1406.2661, 2014. 1\n[9] Alex Graves. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013. 2\n[10] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and\nJ¨urgen Schmidhuber. Connectionist temporal classiﬁcation:\nlabelling unsegmented sequence data with recurrent neural\nnetworks. In ICML, pages 369–376, 2006. 6\n[11] Tom SF Haines, Oisin Mac Aodha, and Gabriel J Brostow.\nMy text in your handwriting. TOG, 35(3):1–18, 2016. 3\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770–778, 2016. 5\n[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. arXiv preprint arXiv:1706.08500, 2017. 6\n[14] Lei Kang, Pau Riba, Yaxing Wang, Marc ¸al Rusi ˜nol, Ali-\ncia Forn ´es, and Mauricio Villegas. Ganwriting: Content-\nconditioned generation of styled handwritten word images.\nIn ECCV, pages 273–289. Springer, 2020. 1, 2, 3, 6, 7, 8, 5,\n9, 10, 11, 12\n[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, pages 4401–4410, 2019. 2\n[16] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 4\n[17] Atsunobu Kotani, Stefanie Tellex, and James Tompkin. Gen-\nerating handwriting via decoupled style descriptors. In\nECCV, pages 764–780. Springer, 2020. 2\n[18] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv\npreprint arXiv:1705.02894, 2017. 5\n[19] U-V Marti and Horst Bunke. The iam-database: an english\nsentence database for ofﬂine handwriting recognition. IJ-\nDAR, 5(1):39–46, 2002. 6\n[20] Martin Mayr, Martin Stumpf, Anguelos Nikolaou, Math-\nias Seuret, Andreas Maier, and Vincent Christlein.\nSpatio-temporal handwriting imitation. arXiv preprint\narXiv:2003.10593, 2020. 3\n[21] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end\ntrainable neural network for image-based sequence recog-\nnition and its application to scene text recognition. PAMI,\n39(11):2298–2304, 2016. 6\n[22] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,\nChristopher D Manning, Andrew Y Ng, and Christopher\nPotts. Recursive deep models for semantic compositional-\nity over a sentiment treebank. In EMNLP, pages 1631–1642,\n2013. 8, 1\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, undeﬁnedukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. InNIPS, page\n6000–6010, Red Hook, NY , USA, 2017. Curran Associates\nInc. 5\n[24] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV, pages 2223–2232,\n2017. 3\nHandwriting Transformers\nSupplementary Material\nIn this supplementary material, we present additional hu-\nman study details, additional qualitative results, and addi-\ntional ablation study results. In Sec. 1, we provide details of\nhuman study experiments. Sec. 2 presents the additional vi-\nsualisation results of transformer encoder-decoder attention\nmaps. Sec. 3 shows qualitative comparison of our proposed\nHWT. Sec. 4 shows the interpolations between two differ-\nent calligraphic styles on the IAM dataset. Finally, Sec. 5\npresents additional ablation results.\n1. Human Study Additional Details\nHere, we present results of our two user studies on 100\nhuman participants to evaluate the human plausibility in\nterms of style mimicry of our proposed HWT. In both these\nuser studies, the forged samples are generated using unseen\nwriting styles of test set writers of IAM dataset, and for\ntextual content we use sentences from Stanford Sentiment\nTreebank [22] dataset.\nUser Preference Study:Fig. 1 shows the interface for the\nUser preference study experiment, which compares styled\ntext images. In this study, each participant is shown a\nreal handwritten text image of a person and the synthe-\nsized handwriting text images of that person using our pro-\nposed HWT, Davis et al . [5] and GANwriting [14]. We\nrandomly present generated results of these methods to the\nuser. Then, the user can compare the real image and the\ngenerated images side by side on the same screen and with-\nout any time restriction to give the answer. Each participant\nis required to provide response for a total of ten questions.\nOverall, we have collected 1000 responses from 100 partic-\nipants. Table 1 shows the results of User preference study.\nDavis et al. [5] and GANwriting [14] were preferred 9% (90\nresponses out of the total 1000) and 10% (100 responses\nout of the total 1000), respectively. Our proposed HWT\nwas preferred 81% (810 responses out of the total 1000 re-\nsponses) over the other two existing methods.\nUser Plausibility Study:Fig. 2 shows the interface for the\nUser plausibility study , which evaluates the proximity of\nthe synthesized samples generated by our proposed HWT\nto the real samples. Here, each participant is shown a per-\nson’s actual handwriting, followed by six samples, where\neach of these samples is either genuine or synthesized hand-\nwriting of the same person. Participants are asked to iden-\ntify whether a given handwritten sample is genuine or not\n(forged/synthesized) with no time limit restriction to an-\nswer the question. In total, we collect 6000 responses for\n100 human participants as each one provides 60 responses.\nTable 1: User preference study in comparison to GANwrit-\ning [14] and Davis et al. [5]. The result shows that our pro-\nposed HWT was preferred 81% of the time over the other\ntwo compared methods.\nTotal\nResponses\nUser\nPreferences\nGANwriting [14]\n1000\n100\nDavis et al. [5] 90\nHWT (Ours) 810\nTable 2: Confusion matrix (%) obtained from User plausi-\nbility study. Only 48.1% of the images were correctly clas-\nsiﬁed, indicating an output comparable to a random choice\nin a two-class problem.\nActual Predicted\nReal Fake\nReal 24.9 25.1\nFake 26.8 23.2\nThe study revealed that the generated images produced by\nour proposed HWT were deemed plausible. Table 2 shows\nthe confusion matrix of the human assessments. For this\nparticular study, only 48.1% of the images have been cor-\nrectly classiﬁed, which indicates a comparable performance\nto random choice in a two-class problem.\n2. Additional Visualizations of Transformer\nEncoder-Decoder Attention\nFig. 3 shows the visualization of attention maps obtained\nusing encoder-decoder of our approach (HWT) at the last\nlayer of the transformer decoder. We compute the attention\nmatrices for four different words: ‘ laughs’, ‘because’,\n‘inside’, and ‘fashion’. Note that the attention maps\ngenerated by our model focus on the relevant regions of in-\nterest in the style examples for each query character. For in-\nstance, to infer character-speciﬁc style attributes of a given\ncharacter ‘h’ in the query word ‘laughs’, the model gives\npriority to multiple image regions containing the charac-\nter ‘h’. Note that if the query character isn’t found in the\nstyle examples, the model attempts to ﬁnd similar charac-\nters. For example, to obtain character representation of ‘ u’\nin the query word ‘laughs’, the attention algorithm high-\nlights image regions containing similar characters (e.g. ‘n’).\nFigure 1: Screenshot of the Interface used inUser preference studyexperiment. Each participant is shown the real handwritten\ntext image (on the left side) of a person and synthesized handwriting text images (on the right side) of that person generated\nusing three different methods. Participants have to mark the best method for mimicking the real handwriting style.\n3. Additional Qualitative Comparison\nFigs. 4-21 show qualitative comparison between our pro-\nposed HWT with [14, 5] for styled handwritten text gener-\nation. Note that we use the same textual content for all the\nexamples ﬁgures for all the three methods to ensure a fair\ncomparison. The ﬁrst row in each ﬁgure presents the dif-\nferent writers example style images. The rest of the rows\ncorrespond to our HWT and [14, 5] respectively. The qual-\nitative results suggest that our method is promising at im-\nitating character-level patterns, while the other two meth-\nods struggle to retain character-speciﬁc details. The success\nof the other two methods is limited to capturing only the\nglobal patterns (e.g., slant, ink widths). In some cases, these\nmethods even struggle to capture global styles. In Fig. 6,\nFig. 16 and Fig. 18, Davis et al. [5] suffer to capture the\nslant. Whereas, in Fig. 16 and Fig. 20, the ink width of the\nimages generated by this method is not consistent with the\nstyle examples. On the other hand, since GANwriting [14]\nis limited to a ﬁxed length query words, it is unable to com-\nplete few words that exceed the limit.\nFigs. 22-23 show qualitative results using the same text\nas in the style examples to compare our proposed HWT with\n[14, 5]. Figs. 24-26 show examples, where we aim to gener-\nate arbitrarily long words. The results show that our model\nis capable of consistently imitating the styles of the given\nstyle example, even for arbitrarily long words. Note that\nGANwriting [14] struggles to generate long words.\n4. Latent Space Interpolations\nFig. 27 shows interpolations between two different cal-\nligraphic styles on the IAM dataset. To interpolate by λ\nbetween two writers A and B, we compute the weighted\naverage ZAB = λZA + (1−λ)ZB, while keeping the tex-\ntual contents ﬁxed. Here, ZA and ZB are the style feature\nsequences obtained from encoder TE. It is worth mention-\ning that our models produce images seamlessly by adjust-\ning from one style to other, which indicates that our model\ngeneralizes in the latent space rather than memorizing any\ntrivial writing patterns.\n5. Additional Ablation Results\nFig. 28 presents additional qualitative results that show\nthe impact of integrating transformer encoder (Enc), trans-\nformer decoder (Dec) and cycle loss (CL) to the baseline\n(Base). Fig. 29 shows additional qualitative comparisons\nbetween word-level and character-level conditioning.\nFigure 2: Screenshot of the Interface used in User plausibility study experiment. Each participant is shown a person’s actual\nhandwriting (on the left side), followed by six samples (on the right side), where three out of these samples are genuine and\nthe rest are synthesized. Participants have to classify each sample as genuine or forgery by looking at the real image.\n\nFigure 3: Additional visualization results of encoder-decoder attention maps at the last layer of the transformer decoder. The\nattention maps are computed for four different query words: ‘ laughs’, ‘because’, ‘inside’, and ‘fashion’. Heat maps\ncorresponding to all characters (including repetitions, as the letter ‘ i’ appears twice in ‘inside’) of these query words are\nshown in the ﬁgure.\n\nFigure 4: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al. [5], when gener-\nating the same text ‘With more character development this might have been an eerie thriller with better payoffs it could have\nbeen a thinking’.\nFigure 5: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al. [5], when gen-\nerating the same text ‘Its not helpful to listen to extremist namecalling regardless of whether you think Kissinger was a\ncalculating’.\nFigure 6: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al. [5], when gen-\nerating the same text ‘Shaky closeups of turkeyonrolls stubbly chins liver spots red noses and the ﬁlmmakers new bobbed do\ndraw easy chuckles but’.\n\nFigure 7: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al. [5], when gen-\nerating the same text ‘This ﬁlm was made by and for those folks who collect the serial killer cards and are fascinated by the\nmere suggestion’.\nFigure 8: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al. [5], when gener-\nating the same text ‘Its a drawling slobbering lovable runon sentence of a ﬁlm a Southern Gothic with the emotional arc of\nits raw blues’.\nFigure 9: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Daviset al. [5], when generat-\ning the same text‘LRB W RRB hile long on amiable monkeys and worthy environmentalism Jane Goodalls Wild Chimpanzees\nis short’.\n\nFigure 10: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al. [5], when gen-\nerating the same text ‘For close to two hours the audience is forced to endure three terminally depressed mostly inarticulate\nhyper dysfunctional’\nFigure 11: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Claude Chabrols camera has a way of gently swaying back and forth as it cradles its characters\nveiling tension beneath’.\nFigure 12: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Though the plot is predictable the movie never feels formulaic because the attention is on the\nnuances of the’.\n\nFigure 13: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘A comingofage tale from New Zealand whose boozy languid air is balanced by a rich visual clarity\nand deeply felt’.\nFigure 14: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Unfortunately Kapur modernizes AEW. Masons story to suit the sensibilities of a young American\na decision that plucks The’.\nFigure 15: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Unless Bob Crane is someone of particular interest to you this ﬁlms impressive performances and\nadept direction are’.\n\nFigure 16: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Afﬁrms the gifts of all involved starting with Spielberg and going right through the ranks of the\nplayers oncamera and off’.\nFigure 17: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Though this rude and crude ﬁlm does deliver a few gut-busting laughs its digs at modern society\nare all things we ve seen’.\nFigure 18: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘You ll laugh at either the obviousness of it all or its stupidity or maybe even its inventiveness but\nthe point is’.\n\nFigure 19: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘Writerdirector s Mehta s effort has tons of charm and the whimsy is in the mixture the intoxicating\nmasala of cultures’.\nFigure 20: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘While easier to sit through than most of Jaglom s selfconscious and gratingly irritating ﬁlms it s’.\nFigure 21: Additional qualitative comparisons of our proposed HWT with GANwriting [14] and Davis et al . [5], when\ngenerating the same text ‘The connected stories of Breitbart and Hanussen are actually fascinating but the ﬁlmmaking in\nInvincible is such that the’.\n\nFigure 22: Reconstruction results using the proposed HWT in comparison to GANwriting [14] and Davis et al. [5]. We use\nthe same text as in the style examples to generate handwritten images.\n\nFigure 23: Reconstruction results using the proposed HWT in comparison to GANwriting [14] and Davis et al. [5].\n\nFigure 24: Handwritten text image generation of arbitrarily long words. We generate the 21-letter word\n‘Incomprehensibilities’ in three different styles and compare the results with Daviset al. [5].\nFigure 25: Handwritten text image generation of arbitrarily long words. We generate the 30-letter word\n‘Pseudopseudohypoparathyroidism’ in three different styles and compare the results with Daviset al. [5].\nFigure 26: Handwritten text image generation of arbitrarily long words. We generate the 28-letter word\n‘Antidisestablishmentarianism’ in three different styles and compare the results with Daviset al. [5].\n\nFigure 27: Latent space interpolations between calligraphic styles on the IAM dataset. The ﬁrst and last image in each column\ncorrespond to writing styles of two different writers. Total we have shown ﬁve sets of interpolation results. We observe how\nthe generated images seamlessly adjust from one style to another. This result shows that our model can generalize in the\nlatent space rather than memorizing any trivial writing patterns.\nFigure 28: Additional qualitative ablation of integrating transformer encoder (Enc), transformer decoder (Dec) and cycle loss\n(CL) to the baseline (Base) on the IAM dataset. We show the effect of each component when generating six different words\n‘especially’, ‘ethereal’, ‘emotional’, ‘standard’,‘resorts’, and ‘under’.\nFigure 29: Additional qualitative comparisons between word and character-level conditioning on IAM dataset. We show the\ncomparison between word and character-level conditioning when generating six different words ‘engaging’, ‘actually’,\n‘movie’, ‘rhythms’,‘what’, and ‘evocative’.\n",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7862882614135742
    },
    {
      "name": "Handwriting",
      "score": 0.738383412361145
    },
    {
      "name": "Computer science",
      "score": 0.7207580804824829
    },
    {
      "name": "Encoder",
      "score": 0.6045461893081665
    },
    {
      "name": "Generative grammar",
      "score": 0.5404269695281982
    },
    {
      "name": "Natural language processing",
      "score": 0.5034322142601013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4959586560726166
    },
    {
      "name": "Writing style",
      "score": 0.4169837236404419
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32976871728897095
    },
    {
      "name": "Speech recognition",
      "score": 0.32469338178634644
    },
    {
      "name": "Linguistics",
      "score": 0.26763617992401123
    },
    {
      "name": "Engineering",
      "score": 0.07316499948501587
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": []
}