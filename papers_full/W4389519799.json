{
  "title": "Zero-Shot Cross-Lingual Summarization via Large Language Models",
  "url": "https://openalex.org/W4389519799",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2631512973",
      "name": "Jia’an Wang",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2136055769",
      "name": "Yunlong Liang",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2133392087",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5083845893",
      "name": "Beiqi Zou",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2143202255",
      "name": "Zhixu Li",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2123523293",
      "name": "Jianfeng Qu",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6864014924",
    "https://openalex.org/W3156665996",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W3034378323",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3035691816",
    "https://openalex.org/W4321485482",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W2104766406",
    "https://openalex.org/W3174724858",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4360584537",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4205336123",
    "https://openalex.org/W4327487298",
    "https://openalex.org/W4225930230",
    "https://openalex.org/W4312450628",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W4221141894",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2251834758",
    "https://openalex.org/W4285126962",
    "https://openalex.org/W4321276803",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4385573683",
    "https://openalex.org/W3101729512",
    "https://openalex.org/W4211081986",
    "https://openalex.org/W4361019484",
    "https://openalex.org/W4385572291",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2971139875",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4327525855",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385573707",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W126360095",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4221143437",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4283802945",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385574138",
    "https://openalex.org/W2129683077",
    "https://openalex.org/W3207649915",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4304194220"
  ],
  "abstract": "Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability. Due to the composite nature of CLS, which requires models to perform summarization and translation simultaneously, accomplishing this task in a zero-shot manner is even a challenge for LLMs. Therefore, we sincerely hope and recommend future LLM research could use CLS as a testbed.",
  "full_text": "Proceedings of The 4th New Frontiers in Summarization Workshop, pages 12–23\nDecember 6, 2023 ©2023 Association for Computational Linguistics\nZero-Shot Cross-Lingual Summarization via Large Language Models\nJiaan Wang1∗†, Yunlong Liang2∗, Fandong Meng3, Beiqi Zou4\nZhixu Li5, Jianfeng Qu1 and Jie Zhou3\n1Soochow University, Suzhou, China 2Beijing Jiaotong University, Beijing, China\n3Pattern Recognition Center, WeChat AI, Tencent Inc, China 4Princeton University, NJ, USA\n5Fudan Unversity, Shanghai, China\njawang.nlp@gmail.com yunlongliang@bjtu.edu.cn\nfandongmeng@tencent.com bzou@cs.princeton.edu\nAbstract\nGiven a document in a source language, cross-\nlingual summarization (CLS) aims to gener-\nate a summary in a different target language.\nRecently, the emergence of Large Language\nModels (LLMs), such as GPT-3.5, ChatGPT\nand GPT-4, has attracted wide attention from\nthe computational linguistics community. How-\never, it is not yet known the performance of\nLLMs on CLS. In this report, we empirically\nuse various prompts to guide LLMs to perform\nzero-shot CLS from different paradigms (i.e.,\nend-to-end and pipeline), and provide a prelim-\ninary evaluation on the generated summaries.\nWe find that ChatGPT and GPT-4 originally pre-\nfer to produce lengthy summaries with detailed\ninformation. These two LLMs can further bal-\nance informativeness and conciseness with the\nhelp of an interactive prompt, significantly im-\nproving their CLS performance. Experimental\nresults on three widely-used CLS datasets show\nthat GPT-4 achieves state-of-the-art zero-shot\nCLS performance, and performs competitively\ncompared with the fine-tuned mBART-50.\nMoreover, we also find some multi-lingual and\nbilingual LLMs ( i.e., BLOOMZ, ChatGLM-\n6B, Vicuna-13B and ChatYuan) have limited\nzero-shot CLS ability. Due to the composite na-\nture of CLS, which requires models to perform\nsummarization and translation simultaneously,\naccomplishing this task in a zero-shot manner\nis even a challenge for LLMs. Therefore, we\nsincerely hope and recommend future LLM re-\nsearch could use CLS as a testbed.\n1 Introduction\nCross-Lingual Summarization (CLS) aims to pro-\nvide a target-language (e.g., Chinese) summary for\na lengthy document in a different source language\n(e.g., English) (Leuski et al., 2003; Wan et al., 2010;\n∗Equal Contribution. Work was done when Wang and\nLiang was interning at Pattern Recognition Center, WeChat\nAI, Tencent Inc, China.\n†Corresponding author.\nFigure 1: An example of zero-shot CLS via ChatGPT.\nYao et al., 2015; Zhu et al., 2019, 2020; Ladhak\net al., 2020; Perez-Beltrachini and Lapata, 2021;\nBai et al., 2021a; Liang et al., 2022b; Feng et al.,\n2022; Hasan et al., 2021; Wang et al., 2022a,b;\nLiang et al., 2022a; Liu et al., 2022; Zheng et al.,\n2022; Aumiller et al., 2022). This task could help\npeople quickly capture their interests from foreign\ndocuments.\nIn recent years, a number of powerful multi-\nlingual pre-trained generative models have been\nproposed one after another, such as mBART (Liu\net al., 2020), mBART-50 (Tang et al., 2021),\nmT5 (Xue et al., 2021) and BLOOM (Scao et al.,\n2022). The parameters in these models have grad-\nually increased from million levels (e.g., 580M in\nmT5-base and 610M in mBART-Large) to billion\nlevels (e.g., 3.7B in mT5-XL, 13B in mT5-XXL\nand 176B in BLOOM), facilitating various research\ntopics (e.g., machine translation and CLS) in the\nmulti-lingual world. Besides, large language mod-\n12\nels (LLMs) have been key to strong performance\nwhen transferring to new tasks by simply condi-\ntioning on a few input-label pairs (in-context learn-\ning) (Dong et al., 2022; Min et al., 2022) or short\nsentences describing crucial reasoning steps (chain-\nof-thoughts) (Fu et al., 2022; Zhang et al., 2022).\nMore recently, ChatGPT and GPT-4 (OpenAI,\n2023) have attracted great attention from both the\nresearch communities and industries. Similar to\nInstructGPT (Ouyang et al., 2022), ChatGPT is\ncreated by fine-tuning a GPT-3.5 series model\nvia reinforcement learning from human feedback\n(RLHF) (Christiano et al., 2017). GPT-4, as a multi-\nmodal LLM that can accept image and text inputs\nand produce text outputs, exhibits human-level per-\nformance on various benchmark datasets (OpenAI,\n2023). With the emergence of ChatGPT and GPT-\n4, there is growing interest in leveraging LLMs\nfor various NLP tasks (Qin et al., 2023; Jiao et al.,\n2023; Bang et al., 2023; Yang et al., 2023; Zhong\net al., 2023; Wang et al., 2023; Bubeck et al., 2023;\nTan et al., 2023; Peng et al., 2023; Liu et al., 2023;\nYong et al., 2023). Nevertheless, the exploration of\nLLMs on CLS is still lacking.\nIn this report, we present a preliminary evalu-\nation of LLMs’ zero-shot CLS performance, in-\ncluding GPT-3.5, ChatGPT, GPT-4, BLOOMZ,\nChatGLM-6B, Vicuna-13B and ChatYuan. In de-\ntail, we design various prompts to guide LLMs to\nperform CLS in an end-to-end manner with or with-\nout chain-of-thoughts (CoT). Figure 1 gives an ex-\nample of prompting ChatGPT to perform zero-shot\nCLS. To further exploit the interaction capability\nof conversational LLMs (e.g., ChatGPT and GPT-\n4), we leverage an interactive prompt to let them\nproduce more concise summaries. Moreover, to\nprovide a deeper analysis of LLMs’ zero-shot CLS\nperformance, we compare them with fine-tuned\nmBART-50 (Tang et al., 2021) which has shown\nits superiority in many previous CLS works (Wang\net al., 2022a; Feng et al., 2022; Perez-Beltrachini\nand Lapata, 2021). Experimental results on three\nCLS datasets, covering three domains (news, how-\nto guide and dialogue) and two cross-lingual di-\nrections (En⇒Zh and En⇒De)1, show that GPT-\n4 achieves the best zero-shot performance but is\nstill worse than the fine-tuned mBART-50 model\nin terms of ROUGE scores and BERTScore. We\n1Since a CLS dataset might contain multiple source and tar-\nget languages, we use “X⇒Y” to indicate the source language\nand target language are X and Y , respectively. En: English;\nZh: Chinese; De: German.\nalso conduct case studies to show that ChatGPT\nand GPT-4 could absorb the core idea of the given\nsource-language documents and generate fluent and\nconcise target-language summaries.\nIn addition, we find that the current open-source\nLLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B\nand ChatYuan) achieve limited zero-shot CLS per-\nformance, which is significantly worse than that of\nGPT-4. This leads us to conclude that the compos-\nite end-to-end CLS prompts are difficult for them\nto follow, and there is still a challenge for LLMs\nto perform zero-shot CLS in an end-to-end manner\nwhich requires simultaneously carrying out transla-\ntion and summarization. Based on the finding, we\nsuggest that future multi-lingual or bilingual LLM\nresearch uses CLS as a testbed to evaluate LLMs’\ncapabilities to follow composite instructions as well\nas combine their different abilities.\nOur main contributions are concluded as follows:\n• To the best of our knowledge, we are the first to\nexplore the zero-shot CLS performance of LLMs.\nTo achieve that, we design various prompts to\nguide LLMs to perform CLS in an end-to-end\nmanner with or without chain-of-thoughts.\n• Experimental results on three widely-used CLS\nbenchmark datasets, covering various domains\nand languages, show several LLMs (especially\nChatGPT and GPT-4) achieve competitive results\ncompared with the strong fine-tuned baseline.\n• We also find the current open-source LLMs gener-\nally achieve limited zero-shot CLS performance,\nmaking us think CLS could be used as a testbed\nfor future LLM research due to its challenges.\n2 Methodology\n2.1 Cross-Lingual Summarization Prompts\nWe heuristically design 3 prompts to guide LLMs\nto perform zero-shot CLS in an end-to-end manner,\nwhich is shown as follows with an example from\nan English document to a Chinese summary:\n• The direct (Dir) prompt guides LLMs straightfor-\nwardly output the corresponding target-language\nsummary without chain-of-thoughts (CoT):\nPlease summarize the\nfollowing text in\nChinese: [English Doc]\nwhere [English Doc] indicates a given En-\nglish document.\n• The translate-then-summarize (TS) CoT prompt\n13\nPlease summarize the following text in Chinese: Annie: \nAre you going to be at school? Christine: Not tomorrow. I \nam not well. Annie: Oh noes! What happened? Christine: \nGot the flu, I think. Annie: what's your temperature? \nChristine: Not high, I'm not running a fever or anything \nAnnie: Are you eating ok? Christine: Yeah. Just blocked \nnose, sore throat. Tired. Annie: Sounds like you've got a \ncold. You need anything? Christine: I could do with some \nTheraflu. Annie: OK, I think we've still got some sachets, \nshould be in date. I'll drop them through your letterbox \nlater on. Christine: Yeah. Don't call in because I'll feel bad \nif you catch this cold off me. Annie: I think I probably had \nit already, but you might be sleeping. Christine: If the light \nin my room is on, call if you want.\n安妮问克里斯汀是否明天会去学校，克里斯汀回答因为\n感到不舒服，不会去。安妮询问原因，得知克里斯汀可\n能得了流感，但体温不高，只是有些堵塞和喉咙痛，很\n累。安妮建议克里斯汀可能只是感冒了，问她是否需要\n任何帮助，克里斯汀说可以用些治疗流感的药物，安妮\n答应送过去。克里斯汀告诉安妮不要来拜访，因为她不\n想传染感冒，但若光亮还开着，可以来敲门。\nPlease make the Chinese summary shorter.\n安妮问克里斯汀是否明天去学校，克里斯汀因感冒不舒\n服，要安妮帮忙送药，但不要拜访。\nPlease first translate the following text to Chinese and then \nsummarize the translated text in Chinese: Annie: Are you \ngoing to be at school? Christine: Not tomorrow. I am not \nwell. Annie: Oh noes! What happened? Christine: Got the \nflu, I think. Annie: what's your temperature? Christine: \nNot high, I'm not running a fever or anything Annie: Are \nyou eating ok? Christine: Yeah. Just blocked nose, sore \nthroat. Tired. Annie: Sounds like you've got a cold. You \nneed anything? Christine: I could do with some Theraflu. \nAnnie: OK, I think we've still got some sachets, should be \nin date. I'll drop them through your letterbox later on. \nChristine: Yeah. Don't call in because I'll feel bad if you \ncatch this cold off me. Annie: I think I probably had it \nalready, but you might be sleeping. Christine: If the light \nin my room is on, call if you want.\n安妮：你明天会去学校吗？克里斯汀：不会，我不太\n舒服。安妮：哦，不好！出了什么事？[…]安妮：我觉\n得我可能已经得过了，但是你可能在睡觉。克里斯汀：\n如果我房间的灯还开着，你可以打电话来。\n这段对话描述了安妮和克里斯汀的健康状况。克里斯\n汀感觉不太舒服，得了流感。安妮询问了克里斯汀的\n体温和饮食情况，确认克里斯汀需要一些热力伏。由\n于担心感染给安妮，克里斯汀不希望安妮打电话或上\n门探访，但安妮表示可能已经得过这种感冒了。\nPlease make the Chinese summary shorter.\n对话描述了安妮和克里斯汀的健康状况，确认克里斯\n汀得了流感，需要热力伏。由于担心传染，克里斯汀\n不希望安妮打电话或上门探访。\nPlease first summarize the following text and then translate \nthe summary to Chinese: Annie: Are you going to be at \nschool? Christine: Not tomorrow. I am not well. Annie: Oh \nnoes! What happened? Christine: Got the flu, I think. Annie: \nwhat's your temperature? Christine: Not high, I'm not \nrunning a fever or anything Annie: Are you eating ok? \nChristine: Yeah. Just blocked nose, sore throat. Tired. Annie: \nSounds like you've got a cold. You need anything? Christine: \nI could do with some Theraflu. Annie: OK, I think we've \nstill got some sachets, should be in date. I'll drop them \nthrough your letterbox later on. Christine: Yeah. Don't call \nin because I'll feel bad if you catch this cold off me. Annie: I \nthink I probably had it already, but you might be sleeping. \nChristine: If the light in my room is on, call if you want.\nAnnie and Christine are having a conversation about \nChristine's health. […] Annie suggests that she might \nhave already had the cold.\nAnnie和Christine在谈论Christine的健康问题。\nChristine因为感觉不舒服，明天不会去上学。Annie\n问了Christine的症状，并提供Theraflu。Christine接\n受了这个提议，但要求Annie不要过来拜访，因为她\n不想传染感冒。Annie表示可能已经感染过这个感冒。\nPlease make the Chinese summary shorter.\nAnnie问Christine是否会去学校，Christine因病请假。\nAnnie问Christine的症状，提供药物，但不会拜访因为\n担心传染感冒。\n(a) Dir+Inta (b) TS+Inta (c) ST+Inta\nFigure 2: An illustration of all prompts used to guide LLMs to perform zero CLS (take ChatGPT as an example).\nmakes LLMs first translate the given document\nfrom the source language to the target language,\nand then summarize the translated document to\nperform CLS:\nPlease first translate\nthe following text\nto Chinese and then\nsummarize the translated\ntext in Chinese:\n[English Doc]\n• The summarize-then-translate (ST) CoT prompt\nlets LLMs first summarize the given document\nand then translate the output summary to the tar-\nget language:\nPlease first summarize\nthe following text\nand then translate the\nsummary to Chinese:\n[English Doc]\nNote that though the TS and ST CoT prompts\nguide LLMs to perform CLS step by step, the be-\nhaviors are end-to-end since the target-language\nsummaries are generated within a single turn.\nTo further exploit the potentiality of conversa-\ntional LLMs, inspired by Bang et al. (2023), after\nprompting with Dir, TS or ST prompt, we adopt an\ninteractive (Inta) prompt to make the preliminarily\ngenerated summary more concise:\nPlease make the Chinese\nsummary shorter.\nand the whole process is denoted as “ Dir+Inta”,\n“TS+Inta” or “ST+Inta”.\n2.2 Large Language Models\nWe explore the CLS ability of the following LLMs:\n• Davinci-003 is the most advanced GPT-3.5\nmodel with 175B parameters. We evaluate its\nperformance by requesting the official API pro-\nvided by OpenAI with default settings.2\n• ChatGPT is created by fine-tuning a GPT-3.5\nseries model via reinforcement learning from hu-\nman feedback (RLHF) (Christiano et al., 2017).\nWe conduct experiments on the ChatGPT plat-\nform3 between February 17 to February 19, 2023.\n• GPT-4, as a multi-modal LLM that can accept\nimage and text inputs and produce text outputs,\nexhibits human-level performance on various\nbenchmark datasets (OpenAI, 2023). We assess\nGPT-4 on the ChatGPT platform between March\n15 to March 19, 2023.\n• BLOOMZ (Muennighoff et al., 2022) is an open-\nsource multi-lingual LLM with 176B parameters.\nThe model supports 59 languages, and is created\nby fine-tuning BLOOM (Scao et al., 2022) on an\ninstruction corpus (i.e., XP3).\n• ChatGLM-6B4 is an open-source bilingual (i.e.,\nChinese and English) language model based on\nGeneral Language Model (GLM) framework (Du\n2https://platform.openai.com/docs/\nmodels/gpt-3-5\n3https://chat.openai.com/\n4https://github.com/THUDM/ChatGLM-6B\n14\nDataset Src Lang. Trg Lang. Domain Example Doc. Length Sum. Length\nCrossSum English Chinese News 3981 / 497 / 50 out of 497 814.2 35.6\nWikiLingua English Chinese How-to guide 13211 / 1886 / 50 out of 3775 538.6 53.2\nGerman 40839 / 5833 / 50 out of 11669 526.1 63.4\nXSAMSum English Chinese Dialogue 14732 / 818 / 50 out of 819 140.1 27.6\nGerman 14732 / 818 / 50 out of 819 140.1 31.7\nTable 1: Statistics of CLS datasets used in experiments. “Src Lang.” and “Trg Lang” denote the source and the target\nlanguages. “Doc. Length” and “Sum. Length” show the average length of source documents and target summaries\n(token level). “Example” lists the number of samples in each dataset w.r.t training, validation and test sets.\net al., 2022). The model suffers from both instruc-\ntion tuning and RLHF.\n• Vicuna-13B5 is an open-source LLM created by\nfine-tuning LLaMA (Touvron et al., 2023) on\nuser-shared conversations collected from Chat-\nGPT. We evaluate the model via its demo plat-\nform6 between March 31 to April 2, 2023.\n• ChatYuan7 is an open-source bilingual (i.e., Chi-\nnese and English) LLM with 7.7B parameters.\nThe training process of this model includes in-\nstruction tuning and RLHF.\nAmong the above LLMs, ChatGPT, GPT-4,\nChatGLM-6B, Vicuna-13B and ChatYuan are\nconversational LLMs while Davinci-003 and\nBLOOMZ are not. When evaluating their zero-shot\nCLS performance, we only equip conversational\nLLMs with the interactive prompt.\n3 Experiments\n3.1 Experimental Setup\nDatasets. We evaluate LLMs on the following\nthree CLS datasets: CrossSum (En ⇒Zh) (Hasan\net al., 2021), WikiLingua (En ⇒Zh/De) (Ladhak\net al., 2020) and XSAMSum (En⇒Zh/De) (Wang\net al., 2022a). CrossSum is collected from BBC\nnews website, it contains 3,981 English news re-\nports paired with Chinese summaries. WikiLingua\ninvolves 18,887 English how-to guides paired with\nChinese summaries, and 58,375 English how-to\nguides paired with German summaries. Note that\nboth CrossSum and WikiLingua also provide CLS\nsamples in other cross-lingual directions, and we\nonly utilize En⇒Zh or (and) En⇒De samples in\nthis work. XSAMSum contains 16,369 English\ndialogues paired with both Chinese and German\nsummaries. The detailed statistics of these datasets\n5https://vicuna.lmsys.org/\n6https://chat.lmsys.org/\n7https://github.com/clue-ai/ChatYuan\nare listed in Table 1. Since ChatGPT, GPT-4 and\nVicuna-13B can only be interacted with manually\nwhen we conduct experiments, evaluating their per-\nformance is time-consuming. Thus, we randomly\nsample 50 documents from the test set of each CLS\ndataset for evaluation.\nMetrics. We adopt ROUGE-1/2/L (R-1/2/L) (Lin,\n2004) and BERTScore (B-S) (Zhang et al., 2020) in\nour experiments. The ROUGE scores measure the\nlexical overlap between the generated summaries\nand corresponding references based on the uni-\ngram, bigram and longest common subsequence,\nwhile the BERTScore measures the semantic sim-\nilarity. For ROUGE scores, we use multi-lingual\nrouge8 toolkit. For BERTScore, we use bert-score9\ntoolkit, and the score is calculated based on bert-\nbase-multilingual-cased10 model.\nBaselines. We also compare zero-shot LLMs with\nfine-tuned mBART-50 (Tang et al., 2021) to provide\na deeper analysis. mBART-50 is a multi-lingual ver-\nsion of BART (Lewis et al., 2020) with the vanilla\ntransformer encoder-decoder architecture (Vaswani\net al., 2017). This model has been pre-trained on\nlarge-scale multi-lingual unlabeled corpora with\nBART-like denoising objectives.\n3.2 Implementation Details\nFor ChatGPT, GPT-4 and Vicuna-13B, we manu-\nally evaluate their results via the corresponding plat-\nform and demo websites. Among them, the demo\nwebsite of Vicuna-13B cannot support the long in-\nput sequences, and it will automatically truncate the\nlong sequences, thus we only evaluate Vicuna-13B\non XSAMSum (En⇒Zh/De). For Davinci-003, we\nuse the official API with default settings.\n8https://github.com/csebuetnlp/xl-sum/\ntree/master/multilingual_rouge_scoring\n9https://github.com/Tiiiger/bert_score\n10https://huggingface.co/\nbert-base-multilingual-cased\n15\nModel CrossSum (En⇒Zh) WikiLingua (En⇒Zh) WikiLingua (En⇒De) XSAMSum (En⇒Zh) XSAMSum (En⇒De)\nR-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S\nmBART-50 26.1 7.4 22.1 65.4 32.1 10.4 26.8 68.5 26.8 7.7 20.5 62.5 40.6 14.4 33.9 74.5 42.4 18.9 35.4 73.7\nChatYuan-7.7B (Dir) 0.3 0.0 0.3 51.7 4.1 1.2 2.6 54.4 - - - - 0.8 0.3 0.7 48.5 - - - -ChatYuan-7.7B (Dir+Inta) 0.2 0.0 0.2 52.0 4.7 1.6 3.4 51.6 - - - - 0.3 0.1 0.3 47.0 - - - -ChatYuan-7.7B (TS) 0.4 0.0 0.4 46.6 8.2 2.7 5.4 56.0 - - - - 11.3 4.2 8.7 49.5 - - - -ChatYuan-7.7B (TS+Inta) 2.0 0.5 1.4 46.9 6.9 2.1 4.3 53.4 - - - - 9.5 3.2 6.9 52.3 - - - -ChatYuan-7.7B (ST) 0.5 0.0 0.4 49.6 6.9 2.1 4.2 56.1 - - - - 7.5 2.5 5.5 49.6 - - - -ChatYuan-7.7B (ST+Inta) 1.2 0.4 0.9 49.7 7.3 2.3 4.6 55.5 - - - - 6.0 2.0 3.9 48.5 - - - -ChatGLM-6B (Dir) 5.7 2.3 2.4 53.9 14.5 5.3 9.9 59.5 - - - - 20.4 9.1 15.3 58.8 - - - -ChatGLM-6B (Dir+Inta) 7.9 2.4 5.3 55.6 14.6 5.1 9.5 59.1 - - - - 18.0 8.0 14.0 59.5 - - - -ChatGLM-6B (TS) 8.4 2.9 4.8 54.1 14.6 5.3 9.8 59.7 - - - - 21.5 9.6 16.6 57.9 - - - -ChatGLM-6B (TS+Inta) 9.6 3.0 6.1 55.2 14.9 5.1 9.4 59.1 - - - - 18.7 8.1 15.0 58.6 - - - -ChatGLM-6B (ST) 5.8 1.8 3.6 53.2 15.6 5.5 10.2 59.9 - - - - 19.8 8.3 14.7 58.1 - - - -ChatGLM-6B (ST+Inta) 2.2 0.6 1.7 53.8 9.8 3.3 6.1 57.0 - - - - 12.7 5.1 9.9 56.8 - - - -Vicuna-13B (Dir) - - - - - - - - - - - - 19.5 7.2 14.5 60.1 22.5 4.9 17.6 58.5Vicuna-13B (Dir+Inta) - - - - - - - - - - - - 24.1 9.7 18.9 63.0 28.7 7.8 22.0 60.5Vicuna-13B (TS) - - - - - - - - - - - - 18.3 7.1 14.6 61.6 25.0 5.9 18.2 59.4Vicuna-13B (TS+Inta) - - - - - - - - - - - - 22.0 7.9 17.4 64.3 31.7 8.9 24.2 61.2Vicuna-13B (ST) - - - - - - - - - - - - 17.5 6.1 13.6 59.6 27.3 6.8 20.4 59.3Vicuna-13B (ST+Inta) - - - - - - - - - - - - 19.8 7.4 15.4 62.2 31.6 9.4 24.1 61.9BLOOMZ-176B (Dir) 0.7 0.1 0.7 29.2 0.3 0.0 0.2 8.9 0.0 0.0 0.0 3.3 21.4 11.2 17.8 65.3 13.0 1.2 11.9 56.2BLOOMZ-176B (TS) 2.1 1.3 1.6 21.5 0.4 0.0 0.3 5.6 0.0 0.0 0.0 5.0 30.415.0 25.2 64.8 12.5 0.7 11.4 54.4BLOOMZ-176B (ST) 3.0 1.2 2.4 33.8 0.3 0.0 0.2 9.0 0.0 0.0 0.0 3.3 28.1 13.4 23.4 66.3 13.8 1.3 12.8 54.8\nDavinci-003 (Dir) 18.7 3.6 14.7 60.2 23.6 3.8 17.8 60.9 18.8 2.6 12.2 60.7 24.4 8.0 20.7 63.4 35.5 12.4 27.3 62.4Davinci-003 (TS) 22.98.9 13.5 59.6 23.7 8.2 15.1 61.0 16.9 2.0 10.9 59.2 33.317.1 26.6 64.7 34.7 11.5 26.1 62.0Davinci-003 (ST)26.29.3 16.961.3 24.2 8.4 15.9 61.2 19.8 2.8 13.1 60.4 34.118.226.4 68.1 35.7 11.7 26.9 63.0ChatGPT (Dir) 14.2 3.3 10.3 60.3 20.9 5.6 15.5 62.7 16.9 2.1 10.7 60.1 21.3 5.5 17.1 63.5 32.0 10.3 24.5 61.4ChatGPT (Dir+Inta) 22.1 3.8 15.6 61.8 28.4 6.5 22.1 64.5 22.4 2.8 14.7 61.3 27.2 6.9 22.9 67.5 39.616.031.464.3ChatGPT (TS) 15.8 3.3 11.9 60.9 24.8 5.4 19.1 62.9 19.4 2.4 12.6 60.0 26.0 7.3 21.2 66.4 33.2 9.6 25.3 61.1ChatGPT (TS+Inta) 22.6 4.116.962.7 26.1 5.3 19.7 63.7 21.6 2.4 15.1 60.8 27.4 6.7 22.4 67.1 39.4 13.5 29.4 63.3ChatGPT (ST) 16.5 3.8 12.0 60.8 27.2 7.3 20.3 64.3 21.3 3.5 14.4 60.9 26.8 7.7 21.3 66.7 31.7 8.8 23.5 60.8ChatGPT (ST+Inta) 21.6 3.5 15.5 61.7 30.1 8.122.464.9 21.4 3.1 15.4 60.6 31.4 11.5 28.1 70.1 35.9 13.2 29.0 62.8GPT-4 (Dir) 13.7 3.7 10.1 59.7 23.1 9.1 15.5 63.5 20.4 3.3 13.8 62.2 24.5 7.1 19.5 66.1 34.7 13.4 25.3 61.7GPT-4 (Dir+Inta) 20.3 4.4 14.1 61.9 30.411.7 20.965.7 24.83.9 17.063.531.3 7.3 26.5 70.740.513.4 30.8 64.2GPT-4 (TS) 19.4 3.6 14.3 60.9 28.5 11.4 18.2 64.2 23.1 3.8 16.362.734.712.528.571.0 38.9 11.9 29.0 63.3GPT-4 (TS+Inta) 22.7 4.3 16.1 62.2 29.212.620.3 64.9 23.6 3.9 17.362.9 30.8 6.5 25.6 70.9 39.1 13.532.664.1GPT-4 (ST) 19.0 4.3 14.1 61.7 30.2 12.2 19.5 64.2 23.4 3.8 16.463.0 32.1 10.7 26.4 70.7 38.6 12.3 29.5 63.2GPT-4 (ST+Inta) 22.6 4.9 16.863.1 30.511.9 21.3 65.2 23.14.2 17.462.7 29.2 8.2 25.471.439.0 11.5 31.2 63.7\nTable 2: Experimental results on CrossSum, WikiLingua and XSAMSum. Pink denotes the fine-tuned baseline.\nLight blue and blue denote the zero-shot performance of open-source and non-open-source LLMs, respectively.\nGreen indicates the zero-shot result is better than that of the fine-tuned baseline. “-” denotes the model cannot be\nevaluated in the corresponding dataset.\nFor BLOOMZ, ChatGLM-6B and ChatYuan, we\ndownload the corresponding checkpoints and eval-\nuate their performances following the officially re-\nleased codes. The 176B BLOOMZ makes use of\n5*80G GPUs to load with FP16 precision. We use\na sampling decoding strategy and set the temper-\nature to 0.7. We only evaluate ChatGLM-6B and\nChatYuan on En⇒Zh cross-lingual direction due\nto their bilingualism (i.e., Chinese and English).\nFor mBART-50 baseline, inspired by Feng\net al. (2022) and Wang et al. (2022a), we employ\nmBART-large-50-many-to-many-mmt\nmodel11 via the implementation of Huggingface\nTransformers (Wolf et al., 2020). This model is\nfine-tuned with 4 batch size, 5e-6 learning rate and\n10 epochs on each CLS dataset.\n11https://huggingface.co/facebook/\nmbart-large-50-many-to-many-mmt\n3.3 Main Results\nTable 2 lists the experimental results. As we can\nsee, Davinci-003, ChatGPT and GPT-4 achieve\ncompetitive results with the fine-tuned mBART-50.\nThe Effect of Each CLS Prompt.Among three\nend-to-end prompts (i.e., Dir, ST and TS), the CoT\nprompts lead to better performance than the di-\nrect prompt, indicating the effectiveness of CoT. It\nalso indicates that it is still challenging for a single\nmodel to directly perform CLS without giving any\ncrucial or helpful instructions.\nThe Effect of Interactive Prompt.Further, with\nthe help of the interactive prompt, the performance\nof ChatGPT and GPT-4 significantly improve and\neven outperform mBART-50 in several automatic\nmetrics. As shown in Table 3, more concise sum-\nmaries can be generated after inputting the interac-\ntive prompt, e.g., 183.7 tokens generated by Chat-\nGPT (Dir) on CrossSum, while the counterpart\nof ChatGPT (Dir+Inta) is 66.4 tokens. Figure 3\n16\nBoris: Due to the number of PTO requests this week, \nwe had to decide who will be allowed to take off \nbased on seniority.  \nSarah: I understand. \nBoris: Unfortunately, you are too low on the totem \npole and we will have to deny your request. \nSarah: Oh, that's too bad! Is there any compromise? I \nreally wanted to have some time with my family. \nBoris: I can give you first choice of dates the week \nafter, but that's the best I can do. \nSarah: What if someone cancels? \nBoris: That is a rare, but yes, I will come to you first. \nSarah: Thank you. \nBoris: Again, I'm very sorry. \nSarah: Could you amend my request for the \nfollowing week, Monday through Wednesday? \nBoris: Certainly. That should be no problem. \nSarah: That's great, at least I'll have a long weekend! \nBoris: Very true. No problems from my side. \nSarah: Do I need to initial the change? \nBoris: Yes, please come see my assistant at your \nearliest convenience. \nSarah: Certainly, no problem. \nBoris: Thank you for being so flexible. \nSarah: Sure, I understand we can't all be off. You are \njust trying to be fair. \nBoris: That's it exactly. Sarah: Thank you again for \nexplaining it so well. \nBoris: My pleasure. Have a nice holiday and enjoy \nyour time off. Sarah: Thank you!\nEnglish Dialogue Generated Summaries\nChatGPT (Dir): Boris通知Sarah由于请假请求众多，公司不得不按资历高低来决定谁可以休假。由于Sarah资历不够高，公司\n不得不拒绝她的请求。Sarah想知道是否有妥协的余地，Boris答应给她下周第一批可以选择的日期，但这是他所能做的最好安\n排。Sarah问如果有人取消假期怎么办，Boris说这是很少见的，但如果确实发生，他会优先通知Sarah。最后，Sarah改变了请\n假日期，Boris同意了，并建议她尽快与他的助手联系确认。他们互相道别，Sarah感谢Boris的解释，Boris祝她假期愉快。\nChatGPT (Dir+Inta): Sarah想请假，但因资历不够高被拒绝。Boris答应给她下周第一批可以选择的日期，如果有人取消假期，\n会优先通知她。最后，Sarah改变了请假日期，Boris同意了。\nChatGPT (TS): 对于申请休假的员工，由于请假申请数量较多，公司根据资历来决定谁可以休假，导致低资历的员工被拒绝\n休假请求。员工虽然表示遗憾，但同意公司的决定并愿意接受推迟的休假时间，直到后来有机会更改休假时间并获得了首选\n日期。公司对员工的灵活性表示感谢，并向员工祝福愉快的假期。\nChatGPT (TS+Inta): 公司根据资历决定员工休假，低资历员工被拒绝休假请求。员工同意公司决定并愿意推迟休假时间，后\n来获得了首选日期。公司感谢员工的灵活性，祝愿员工假期愉快。\nChatGPT (ST): 鲍里斯告诉萨拉，由于请假申请太多，他必须根据资历批准休假，但很遗憾，萨拉的请求不能被批准。然而，\n鲍里斯提供萨拉选择下周的日期，如果有人取消。萨拉问能否修改为下周的请求，鲍里斯同意了。他们以积极的态度结束了\n谈话，鲍里斯祝萨拉度过愉快的假期，萨拉感谢他的理解。\nChatGPT (ST+Inta): 因请假申请太多，鲍里斯按资历批准休假，未能批准萨拉的请求。但鲍里斯提供了选择下周日期的机会。\n萨拉同意改为下周休假，他们友好结束谈话。\nGPT-4 (Dir): Boris通知Sarah由于请假请求众多，公司不得不按资历高低来决定谁可以休假。由于Sarah资历不够高，公司不得\n不拒绝她的请求。Sarah想知道是否有妥协的余地，Boris答应给她下周第一批可以选择的日期，但这是他所能做的最好安排。\nSarah问如果有人取消假期怎么办，Boris说这是很少见的，但如果确实发生，他会优先通知Sarah。最后，Sarah改变了请假日\n期，Boris同意了，并建议她尽快与他的助手联系确认。他们互相道别，Sarah感谢Boris的解释，Boris祝她假期愉快\nGPT-4 (Dir+Inta): Sarah想请假，但因资历不够高被拒绝。Boris答应给她下周第一批可以选择的日期，如果有人取消假期，会\n优先通知她。最后，Sarah改变了请假日期，Boris同意了。\nGPT-4 (TS): 由于本周请假申请众多，鲍里斯根据员工资历决定谁能休假，萨拉的申请被拒。经过沟通，鲍里斯同意在下周给\n萨拉优先选择日期，若有人取消，也会优先考虑萨拉。最后，鲍里斯帮助萨拉将申请更改为下周一至周三。萨拉表示理解公\n平性并感谢鲍里斯的帮助。\nGPT-4 (TS+Inta): 鲍里斯因员工请假申请多而根据资历拒绝了萨拉的请求。经沟通，鲍里斯同意下周给萨拉优先选择日期，\n若有人取消，也会优先考虑。萨拉表示理解公平性并感谢鲍里斯。\nGPT-4 (ST): 由于本周休假申请数量众多，鲍里斯不得不根据资历来决定谁可以休假，因此拒绝了萨拉的申请。他们讨论了潜\n在的妥协方案，最终同意萨拉在下周一至周三休假。如果有人取消休假，鲍里斯将首先向萨拉提供名额。萨拉需要在鲍里斯\n的助手那里签署更改。\nGPT-4 (ST+Inta): 鲍里斯因资历问题拒绝了萨拉的休假申请。他们商量后同意萨拉下周一至三休假。若有人取消，鲍里斯会\n优先通知萨拉。萨拉需在助手处确认更改。\nFigure 3: Example dialogue document in XSAMSum and summaries generated by ChatGPT and GPT-4.\nalso shows an example English document with the\ncorresponding summaries generated by ChatGPT\nand GPT-4 via different prompts. Therefore, the\nconversational LLMs prefer to generate lengthy\nsummaries probably due to the RLHF training pro-\ncess, and the interactive prompt further helps them\nbalance informativeness and conciseness, and sig-\nnificantly improves their zero-shot CLS ability.\nBest Zero-Shot LLM vs. Fine-Tuned mBART.\nGPT-4 achieves state-of-the-art zero-shot CLS per-\nformance among all LLMs, justifying its superior-\nity. But the model is still slightly worse than the\nfine-tuned mBART-50 in terms of automatic eval-\nuation metrics. One possible reason is that zero-\nshot LLMs are not aware of the text style of the\ngolden summaries when performing zero-shot CLS\non each dataset. However, lower automatic scores\ndo not indicate worse performance. For example,\nas discussed by Goyal et al. (2022), the news sum-\nmaries generated by GPT-3 achieve lower ROUGE\nscores than fine-tuned models but higher in human\nevaluation. Thus, the comparison between LLMs\nand fine-tuned mBART-50 in CLS needs human\nevaluation, which we reserve for the future.\nLimited Performance of Open-Source LLMs.\nFor open-source LLMs, i.e., BLOOMZ, ChatGLM-\n6B, Vicuna-13B and ChatYuan-7.7B, they per-\nform poorly on CrossSum and WikiLingua datasets\nwhose documents typically contain more lengthy\ncontent than those of XSAMSum. Although they\nperform decently on XSAMSum, there is still a\nMethod CrossSum WikiLingua XSAMSum\nEn⇒Zh En⇒Zh En⇒De En⇒Zh En⇒De\nmBART-50 32.7 46.6 75.4 22.3 27.9\nDavinci-003 (Dir) 83.3 78.5 149.1 61.8 62.5Davinci-003 (TS) 82.1 76.2 148.6 53.4 65.8Davinci-003 (ST) 44.7 49.1 91.7 43.4 52.1ChatGPT (Dir) 183.7 176.6 273.5 68.6 75.3ChatGPT (Dir+Inta) 66.4 50.0 80.7 28.7 42.5ChatGPT (TS) 155.1 82.1 149.3 48.2 60.9ChatGPT (TS+Inta) 63.4 46.2 70.0 30.3 41.1ChatGPT (ST) 132.7 94.3 124.2 54.9 68.1ChatGPT (ST+Inta) 57.8 50.1 71.6 29.3 37.5GPT-4 (Dir) 227.1 170.5 193.1 70.4 74.4GPT-4 (Dir+Inta) 102.2 58.7 75.1 30.1 38.3GPT-4 (TS) 93.9 85.6 114.7 44.1 53.8GPT-4 (TS+Inta) 56.5 45.4 66.5 26.3 33.8GPT-4 (ST) 106.6 87.8 109.5 43.6 53.7GPT-4 (ST+Inta) 62.7 48.0 65.1 26.7 33.3\nGolden 36.1 50.0 66.8 23.9 29.6\nTable 3: The average length (token level) of the gen-\nerated summaries on the test set of each CLS dataset.\nLight green indicates the length of golden summaries.\nlarge gap compared to GPT-4. Thus, we conclude\nthat zero-shot CLS is challenging for LLMs to per-\nform due to its composite nature that requires mod-\nels to perform summarization and translation si-\nmultaneously. In this situation, we suggest future\nbilingual or multi-lingual LLM research adopt CLS\nas a testbed to evaluate the LLMs’ capabilities to\nfollow composite instructions as well as combine\ntheir different ability.\n3.4 LLM-based Evaluation\nIt is worth noting that conducting human evaluation\non the generated summaries of both LLMs and fine-\ntuned models is not trivial since human evaluators\n17\nXSAMSum (En⇒Zh) XSAMSum (En ⇒De)\nCoherence Relevance Consistency Fluency Coherence Relevance Consistency Fluency\nmBART-50 54.0 32.3 36.6 55.8 54.6 36.0 45.3 52.1\nChatYuan-7.7B (Dir) 44.6 17.3 40.3 53.6 - - - -\nChatYuan-7.7B (Dir+Inta) 43.6 21.0 37.3 52.0 - - - -\nChatYuan-7.7B (TS) 41.0 17.0 24.8 48.8 - - - -\nChatYuan-7.7B (TS+Inta) 39.0 15.3 17.3 41.5 - - - -\nChatYuan-7.7B (ST) 47.3 19.6 36.3 54.6 - - - -\nChatYuan-7.7B (ST+Inta) 48.6 15.3 33.0 52.0 - - - -\nChatGLM-6B (Dir) 58.8 31.0 49.0 61.0 - - - -\nChatGLM-6B (Dir+Inta) 60.6 35.3 55.1 60.8 - - - -\nChatGLM-6B (TS) 52.0 22.0 25.3 54.0 - - - -\nChatGLM-6B (TS+Inta) 55.0 31.6 46.5 58.5 - - - -\nChatGLM-6B (ST) 58.6 27.0 37.3 56.5 - - - -\nChatGLM-6B (ST+Inta) 59.3 34.0 53.1 63.8 - - - -\nVicuna-13B (Dir) 50.3 28.0 39.6 52.8 64.3 53.6 67.1 63.8\nVicuna-13B (Dir+Inta) 55.5 36.0 43.0 56.8 63.8 49.0 62.0 63.6\nVicuna-13B (TS) 57.1 44.6 57.3 56.5 68.3 55.6 69.3 66.5\nVicuna-13B (TS+Inta) 55.1 35.6 49.3 52.1 66.6 56.3 66.3 64.0\nVicuna-13B (ST) 54.6 33.6 46.3 56.6 65.0 54.0 62.8 62.1\nVicuna-13B (ST+Inta) 53.6 37.0 44.3 55.1 69.5 57.3 67.6 66.3\nBLOOMZ-176B (Dir) 53.5 38.3 44.3 54.3 63.1 51.0 61.0 63.8\nBLOOMZ-176B (TS) 52.3 37.0 37.6 53.6 59.3 48.3 61.3 58.8\nBLOOMZ-176B (ST) 54.3 37.3 44.3 55.5 59.5 48.0 60.3 60.3\nDavinci-003 (Dir) 60.0 33.0 55.0 59.3 71.3 60.6 76.6 69.5\nDavinci-003 (TS) 56.3 26.6 38.6 54.3 68.5 54.3 68.6 68.3\nDavinci-003 (ST) 62.8 46.6 54.0 61.5 68.6 61.6 77.0 70.1\nChatGPT (Dir) 63.1 45.3 70.0 65.3 74.0 64.0 82.0 71.6\nChatGPT (Dir+Inta) 58.0 45.0 58.0 60.6 68.3 65.3 76.0 69.1\nChatGPT (TS) 63.0 49.6 59.0 62.8 71.5 62.0 77.6 71.8\nChatGPT (TS+Inta) 64.5 49.3 61.3 60.5 70.0 59.0 77.0 69.8\nChatGPT (ST) 64.3 51.6 64.0 62.3 72.3 63.6 77.0 74.0\nChatGPT (ST+Inta) 64.1 51.0 60.6 65.3 69.1 60.3 73.6 67.0\nGPT4 (Dir) 64.0 48.6 67.6 67.0 75.3 68.0 83.6 74.5\nGPT4 (Dir+Inta) 62.0 50.6 57.3 63.1 70.0 68.6 77.6 70.3\nGPT4 (TS) 66.0 55.3 63.3 65.6 73.6 68.6 79.0 72.0\nGPT4 (TS+Inta) 62.3 48.6 59.0 63.8 65.1 60.3 69.3 68.3\nGPT4 (ST) 63.0 52.6 64.0 62.3 72.8 67.3 80.3 72.3\nGPT4 (ST+Inta) 60.6 46.3 56.3 63.5 70.8 62.6 79.0 70.1\nTable 4: Evaluation results (judged by ChatGPT) on XSAMSum. Pink denotes the fine-tuned baseline. Light blue\nand blue denote the zero-shot performance of open-source and non-open-source LLMs, respectively. “-” denotes\nthe model cannot be evaluated in the corresponding dataset.\ncan easily realize which summaries are generated\nby LLMs or fine-tuned models. In this manner,\nthe evaluators may have biases during scoring each\nsummary. To ensure the fairness of human judg-\nment, Stiennon et al. (2020) only retain the gener-\nated summaries whose length belongs to a certain\nrange, and then collect human judgment on these\nsummaries to minimize the potential evaluation\nbias caused by summary length. In our scene, the\ntext styles of LLMs and fine-tuned models are quite\ndifferent, which might also lead to bias. Thus, the\nhuman evaluation of comparing zero-shot LLMs\nand fine-tuned models on CLS needs more care-\nfully designed.\nAs an alternative to human evaluation, recent\nstudies (Liu et al., 2023; Kocmi and Federmann,\n2023; Wang et al., 2023) show that the natural\nlanguage generation (NLG) results evaluated by\nLLMs could achieve better correlations with hu-\nmans. Following Wang et al. (2023), we utilize\nChatGPT to score the generated summaries in a\nreference-free manner on four aspects, i.e., coher-\nence, relevance, consistency and fluency. An exam-\nple prompt is shown in Figure 4, and please refer\nto Wang et al. (2023) for prompts of all aspects.\nThe instruction of each aspect (marked in purple\nin Figure 4) is inspired by SummEval (a widely-\nused summarization meta-evaluation benchmark\ndataset) (Fabbri et al., 2021). We utilize the official\n18\nScore the following dialogue summarization given the \ncorresponding dialogue with respect to fluency on a \ncontinuous scale from 0 to 100, where a score of zero \nmeans \"disfluency\" and score of one hundred means \n\"perfect fluency\". Note that fluency measures the quality \nof individual sentences, are they well-written and \ngrammatically correct. Consider the quality of individual \nsentences.\nDialogue: [a given dialogue]\nSummary: [one generated summary]\nScores:\nFigure 4: An example prompt used to guide ChatGPT\nto score the summarization results in the aspect of flu-\nency. Purple indicates the detailed instruction of the\ncorresponding aspect.\nAPIs provided by OpenAI12 to conduct the experi-\nments with gpt-3.5-turbo model, and set the\ntemperature to 0 to eliminate the randomness of\nevaluation results.\nTable 4 shows the evaluation results on XSAM-\nSum (En⇒Zh/De). As we can see, GPT-4 achieves\nthe best performance in most aspects, showing its\nsuperiority. Besides, compared with the fine-tuned\nmBART-50 baseline, several zero-shot LLMs, in-\ncluding ChatGLM-6B, Vicuna-13B, Davinci-003,\nChatGPT and GPT-4, achieve better results in all as-\npects, demonstrating the potentiality of performing\nzero-shot CLS via LLMs. For example, ChatGLM-\n6B (Dir+Inta) achieves 60.6, 35.3, 55.1 and 60.8\nscores in aspects of coherence, relevance, con-\nsistency and fluency respectively on XSAMSum\n(En⇒Zh), while the counterparts of mBART-50\nare 54.0, 32.3, 36.6 and 55.8, respectively. For\nGPT-4 (Dir), the corresponding scores even reach\n64.0, 48.6, 67.6 and 67.0, significantly better than\nthe fine-tuned mBART-50.\nMoreover, while we show the interactive prompt\ncan improve the performance of zero-shot LLM in\nterms of ROUGE scores and BERTScore, we do\nnot find the same trend in the LLM-based evalua-\ntion results. In some cases, the interactive prompt\neven leads to worse LLM-based scores. We conjec-\nture that the interactive prompt would force zero-\nshot LLMs to reduce the length of the generated\nsummaries, and the models cannot make a good\ntrade-off between conciseness and other aspects,\nwhich is also hard for humans.\n12https://platform.openai.com/docs/\nguides/gpt/chat-completions-api\n4 Related Work\n4.1 Cross-Lingual Summarization\nGiven documents in one language, cross-lingual\nsummarization (CLS) generates summaries in an-\nother language. Early work typically focuses on\npipeline methods (Leuski et al., 2003; Or˘asan and\nChiorean, 2008; Wan et al., 2010; Wan, 2011; Yao\net al., 2015), i.e., translation and then summariza-\ntion or summarization and then translation. Re-\ncently, with the availability of large-scale CLS\ndatasets (Zhu et al., 2019; Ladhak et al., 2020;\nPerez-Beltrachini and Lapata, 2021; Wang et al.,\n2022a; Zheng et al., 2022), many researchers\nshift the research attention to end-to-end CLS\nmodels. According to a comprehensive CLS re-\nview (Wang et al., 2022b), the end-to-end mod-\nels involve multi-task learning (Cao et al., 2020;\nBai et al., 2021b; Liang et al., 2022b), knowl-\nedge distillation (Nguyen and Luu, 2022), resource-\nenhanced (Zhu et al., 2020; Jiang et al., 2022)\nand pre-training (Xu et al., 2020; Chi et al., 2021)\nframeworks. However, none of them explore LLMs\nperformance on CLS. To our knowledge, we are\nthe first to explore can LLMs perform zero-shot\nCLS and how their results are.\n4.2 Large Language Models\nRecently, there are growing interest in leveraging\nLLMs for various NLP tasks. Bang et al. (2023),\nQin et al. (2023) and Zhong et al. (2023) con-\nduct systematic investigations of ChatGPT’s per-\nformance on various downstream tasks. Jiao et al.\n(2023) and Peng et al. (2023) evaluate ChatGPT on\nmachine translation. Yong et al. (2023) show that\nChatGPT could generate high-quality code-mixed\ntext. Tan et al. (2023) explore the performance\nof ChatGPT on knowledge-based question answer-\ning (KBQA). Some works (Kocmi and Federmann,\n2023; Wang et al., 2023; Liu et al., 2023; Ji et al.,\n2023) utilize ChatGPT or GPT-4 as an evaluation\nmetric to assess task-specific model performance.\n5 Conclusion and Future Work\nIn this technical report, we evaluate the zero-shot\nperformance of mainstream bilingual and multi-\nlingual LLMs on cross-lingual summarization. We\nfind that Davinci-003, ChatGPT and GPT-4 can\ncombine the ability to summarize and translate to\nperform zero-shot CLS, and achieve competitive\nresults with the fine-tuned baseline (i.e., mBART-\n50). In addition, the current open-source LLMs\n19\n(i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and\nChatYuan) generally show their limited ability to\nperform CLS in an end-to-end manner, showing the\nchallenge of performing zero-shot CLS still exists.\nIn the future, we would like to unleash the po-\ntentiality of LLMs and leverage LLMs to perform\nCLS in few-shot learning manners.\nLimitations\nWhile we evaluate the performance of LLMs on the\ncross-lingual summarization task, there are some\nlimitations worth noting: (1) We only evaluate the\nlower threshold of these models’ CLS performance.\nPrompts are important to guide LLMs to perform\nspecific tasks, and future work could explore bet-\nter prompts to obtain better results. (2) This report\nonly uses two cross-lingual directions (En⇒Zh and\nEn⇒De) in experiments, and all the languages are\nconsidered high-resource languages in the world.\nThe performance of LLMs on low-resource lan-\nguages still needs to be explored. According to Jiao\net al. (2023), the machine translation ability of\nChatGPT is limited on low-resource languages. We\nconjecture that the same situation might exist in\nCLS. (3) Though the general trend of the evalu-\nation results should be correct, the comparisons\nbetween LLMs are not rigorous due to the decod-\ning strategies of these models are not the same.13\nThis is one of the major reasons leading to the lim-\nited soundness of this work. (4) In the future, we\nwould like to conduct human evaluation to give\nmore analyses.\nAcknowledgement\nWe thank anonymous reviewers for their construc-\ntive suggestions and comments.\nReferences\nDennis Aumiller, Ashish Chouhan, and Michael Gertz.\n2022. EUR-lex-sum: A multi- and cross-lingual\ndataset for long-form summarization in the legal do-\nmain. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7626–7639, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nYu Bai, Yang Gao, and Heyan Huang. 2021a. Cross-\nlingual abstractive summarization with limited par-\nallel resources. In Proceedings of the 59th Annual\n13Currently, we cannot set the decoding strategy of GPT-4\nwhen manually evaluating it on the ChatGPT platform. Be-\nsides, it is difficult to ensure the decoding strategies of LLMs\nare totally the same when using online platforms or demos.\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6910–6924, Online. Association for\nComputational Linguistics.\nYu Bai, Heyan Huang, Kai Fan, Yang Gao, Zewen Chi,\nand Boxing Chen. 2021b. Bridging the gap: Cross-\nlingual summarization with compression rate. ArXiv\npreprint, abs/2110.07936v1.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nYue Cao, Hui Liu, and Xiaojun Wan. 2020. Jointly\nlearning to align and summarize for neural cross-\nlingual summarization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 6220–6231, Online. Association\nfor Computational Linguistics.\nZewen Chi, Li Dong, Shuming Ma, Shaohan Huang,\nSaksham Singhal, Xian-Ling Mao, Heyan Huang,\nXia Song, and Furu Wei. 2021. mT6: Multilingual\npretrained text-to-text transformer with translation\npairs. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1671–1683, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nAlexander R. Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. SummEval: Re-evaluating Summariza-\ntion Evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\n20\nXiachong Feng, Xiaocheng Feng, and Bing Qin. 2022.\nMSAMSum: Towards benchmarking multi-lingual\ndialogue summarization. In Proceedings of the Sec-\nond DialDoc Workshop on Document-grounded Dia-\nlogue and Conversational Question Answering, pages\n1–12, Dublin, Ireland. Association for Computational\nLinguistics.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. arXiv preprint\narXiv:2210.00720.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint arXiv:2209.12356.\nTahmid Hasan, Abhik Bhattacharjee, Wasi Uddin\nAhmad, Yuan-Fang Li, Yong-Bin Kang, and Ri-\nfat Shahriyar. 2021. Crosssum: Beyond english-\ncentric cross-lingual abstractive text summariza-\ntion for 1500+ language pairs. ArXiv preprint ,\nabs/2112.08804v1.\nYunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun,\nDongyu Pan, Baochang Ma, and Xiangang Li. 2023.\nExploring chatgpt’s ability to rank content: A prelim-\ninary study on consistency with human preferences.\narXiv preprint arXiv:2303.07610.\nShuyu Jiang, Dengbiao Tu, Xingshu Chen, R. Tang,\nWenxian Wang, and Haizhou Wang. 2022. Clue-\nGraphSum: Let key clues guide the cross-\nlingual abstractive summarization. ArXiv preprint,\nabs/2203.02797v2.\nWenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? a preliminary study. arXiv preprint\narXiv:2301.08745.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summariza-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4034–4048,\nOnline. Association for Computational Linguistics.\nAnton Leuski, Chin-Yew Lin, Liang Zhou, Ulrich Ger-\nmann, Franz Josef Och, and Eduard H. Hovy. 2003.\nCross-lingual c*st*rd: English access to hindi in-\nformation. ACM Trans. Asian Lang. Inf. Process. ,\n2:245–269.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang,\nYufeng Chen, and Jie Zhou. 2022a. Summary-\noriented vision modeling for multimodal abstractive\nsummarization. arXiv preprint arXiv:2212.07672.\nYunlong Liang, Fandong Meng, Chulun Zhou, Jinan\nXu, Yufeng Chen, Jinsong Su, and Jie Zhou. 2022b.\nA variational hierarchical model for neural cross-\nlingual summarization. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2088–\n2099, Dublin, Ireland. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nNayu Liu, Kaiwen Wei, Xian Sun, Hongfeng Yu, Fang-\nlong Yao, Li Jin, Guo Zhi, and Guangluan Xu. 2022.\nAssist non-native viewers: Multimodal cross-lingual\nsummarization for how2 videos. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 6959–6969, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nThong Thanh Nguyen and Anh Tuan Luu. 2022. Im-\nproving neural cross-lingual abstractive summariza-\ntion via employing optimal transport distance for\nknowledge distillation. Proceedings of the AAAI\nConference on Artificial Intelligence, 36(10):11103–\n11111.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\n21\nConstantin Or˘asan and Oana Andreea Chiorean. 2008.\nEvaluation of a cross-lingual Romanian-English\nmulti-document summariser. In Proceedings of\nthe Sixth International Conference on Language Re-\nsources and Evaluation (LREC’08), Marrakech, Mo-\nrocco. European Language Resources Association\n(ELRA).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nKeqin Peng, Liang Ding, Qihuang Zhong, Li Shen,\nXuebo Liu, Min Zhang, Yuanxin Ouyang, and\nDacheng Tao. 2023. Towards making the most of\nchatgpt for machine translation. Available at SSRN\n4390455.\nLaura Perez-Beltrachini and Mirella Lapata. 2021. Mod-\nels and datasets for cross-lingual summarisation. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n9408–9423, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008–\n3021.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan\nHu, Yongrui Chen, and Guilin Qi. 2023. Evalu-\nation of chatgpt as a question answering system\nfor answering complex questions. arXiv preprint\narXiv:2303.07992.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2021. Multilingual translation from de-\nnoising pre-training. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 3450–3466, Online. Association for Computa-\ntional Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nXiaojun Wan. 2011. Using bilingual information for\ncross-language document summarization. In Pro-\nceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1546–1555, Portland, Ore-\ngon, USA. Association for Computational Linguis-\ntics.\nXiaojun Wan, Huiying Li, and Jianguo Xiao. 2010.\nCross-language document summarization based on\nmachine translation quality prediction. In Proceed-\nings of the 48th Annual Meeting of the Association for\nComputational Linguistics, pages 917–926, Uppsala,\nSweden. Association for Computational Linguistics.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nJiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng,\nZhixu Li, Jianfeng Qu, and Jie Zhou. 2022a. Clid-\nSum: A benchmark dataset for cross-lingual dialogue\nsummarization. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 7716–7729, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nJiaan Wang, Fandong Meng, Duo Zheng, Yunlong\nLiang, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2022b.\nA Survey on Cross-Lingual Summarization. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:1304–1323.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nRuochen Xu, Chenguang Zhu, Yu Shi, Michael Zeng,\nand Xuedong Huang. 2020. Mixed-lingual pre-\ntraining for cross-lingual summarization. In Pro-\nceedings of the 1st Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 10th International Joint Conference\n22\non Natural Language Processing , pages 536–541,\nSuzhou, China. Association for Computational Lin-\nguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nXianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and\nWei Cheng. 2023. Exploring the limits of chatgpt\nfor query or aspect-based text summarization. arXiv\npreprint arXiv:2302.08081.\nJin-ge Yao, Xiaojun Wan, and Jianguo Xiao. 2015.\nPhrase-based compressive cross-language summa-\nrization. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 118–127, Lisbon, Portugal. Association for\nComputational Linguistics.\nZheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde,\nSkyler Wang, Samuel Cahyawijaya, Holy Lovenia,\nLintang Sutawika, Jan Christian Blaise Cruz, Long\nPhan, Yin Lin Tan, et al. 2023. Prompting large\nlanguage models to generate code-mixed texts: The\ncase of south east asian languages. arXiv preprint\narXiv:2303.13592.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models. arXiv preprint\narXiv:2210.03493.\nShaohui Zheng, Zhixu Li, Jiaan Wang, Jianfeng\nQu, An Liu, Lei Zhao, and Zhigang Chen. 2022.\nLong-document cross-lingual summarization. arXiv\npreprint arXiv:2212.00586.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too?\na comparative study on chatgpt and fine-tuned bert.\narXiv preprint arXiv:2302.10198.\nJunnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Ji-\najun Zhang, Shaonan Wang, and Chengqing Zong.\n2019. NCLS: Neural cross-lingual summarization.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3054–\n3064, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJunnan Zhu, Yu Zhou, Jiajun Zhang, and Chengqing\nZong. 2020. Attend, translate and summarize: An\nefficient method for neural cross-lingual summariza-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1309–1321, Online. Association for Computational\nLinguistics.\n23",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8755882978439331
    },
    {
      "name": "CLs upper limits",
      "score": 0.8544189929962158
    },
    {
      "name": "Computer science",
      "score": 0.6889029145240784
    },
    {
      "name": "Shot (pellet)",
      "score": 0.44735702872276306
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4175841212272644
    },
    {
      "name": "Task (project management)",
      "score": 0.4175358712673187
    },
    {
      "name": "Natural language processing",
      "score": 0.3953060507774353
    },
    {
      "name": "Medicine",
      "score": 0.14788222312927246
    },
    {
      "name": "Chemistry",
      "score": 0.12834611535072327
    },
    {
      "name": "Engineering",
      "score": 0.11017027497291565
    },
    {
      "name": "Optometry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}