{
  "title": "Empowering Few-Shot Recommender Systems With Large Language Models-Enhanced Representations",
  "url": "https://openalex.org/W4391984480",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5113079407",
      "name": "Zhoumeng Wang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6601336008",
    "https://openalex.org/W3014972541",
    "https://openalex.org/W3085807072",
    "https://openalex.org/W4288083766",
    "https://openalex.org/W3047626246",
    "https://openalex.org/W3036247427",
    "https://openalex.org/W3036868398",
    "https://openalex.org/W3003365227",
    "https://openalex.org/W3176496359",
    "https://openalex.org/W2177923071",
    "https://openalex.org/W2050599922",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6850627791",
    "https://openalex.org/W6851596285",
    "https://openalex.org/W4386555196",
    "https://openalex.org/W6850988808",
    "https://openalex.org/W3157020699",
    "https://openalex.org/W1412447802",
    "https://openalex.org/W2112856797",
    "https://openalex.org/W2810479486",
    "https://openalex.org/W4284691940",
    "https://openalex.org/W2088621849",
    "https://openalex.org/W2124074612",
    "https://openalex.org/W3027413395",
    "https://openalex.org/W2783774207",
    "https://openalex.org/W2048508267",
    "https://openalex.org/W2057333662",
    "https://openalex.org/W2888192920",
    "https://openalex.org/W2031237011",
    "https://openalex.org/W2166956738",
    "https://openalex.org/W6640643859",
    "https://openalex.org/W6854506972",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2788376297",
    "https://openalex.org/W2788953034",
    "https://openalex.org/W2786995169",
    "https://openalex.org/W2515144511",
    "https://openalex.org/W2575006718",
    "https://openalex.org/W4386744926",
    "https://openalex.org/W4386729453",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3021636956",
    "https://openalex.org/W2804552794",
    "https://openalex.org/W3087818431",
    "https://openalex.org/W2512971201",
    "https://openalex.org/W2475334473",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W6680830989",
    "https://openalex.org/W2605350416",
    "https://openalex.org/W2219888463",
    "https://openalex.org/W4366733551",
    "https://openalex.org/W4322759378",
    "https://openalex.org/W3100921056",
    "https://openalex.org/W2140310134",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W4394994587",
    "https://openalex.org/W33382267",
    "https://openalex.org/W4361193179"
  ],
  "abstract": "Recommender systems utilizing explicit feedback have witnessed significant advancements and widespread applications over the past years. However, generating recommendations in few-shot scenarios remains a persistent challenge. Recently, large language models (LLMs) have emerged as a promising solution for addressing natural language processing (NLP) tasks, thereby offering novel insights into tackling the few-shot scenarios encountered by explicit feedback-based recommender systems. To bridge recommender systems and LLMs, we devise a prompting template that generates user and item representations based on explicit feedback. Subsequently, we integrate these LLM-processed representations into various recommendation models to evaluate their significance across diverse recommendation tasks. Our ablation experiments and case study analysis collectively demonstrate the effectiveness of LLMs in processing explicit feedback, highlighting that LLMs equipped with generative and logical reasoning capabilities can effectively serve as a component of recommender systems to enhance their performance in few-shot scenarios. Furthermore, the broad adaptability of LLMs augments the generalization potential of recommender models, despite certain inherent constraints. We anticipate that our study can inspire researchers to delve deeper into the multifaceted dimensions of LLMs&#x2019; involvement in recommender systems and contribute to the advancement of the explicit feedback-based recommender systems field.",
  "full_text": "Received 20 December 2023, accepted 11 February 2024, date of publication 20 February 2024, date of current version 29 February 2024.\nDigital Object Identifier 10.1 109/ACCESS.2024.3368027\nEmpowering Few-Shot Recommender Systems\nWith Large Language Models-\nEnhanced Representations\nZHOUMENG WANG\nMarketing Programme, The Chinese University of Hong Kong Business School, Hong Kong\ne-mail: johnnywang@link.cuhk.edu.hk\nABSTRACT Recommender systems utilizing explicit feedback have witnessed significant advancements\nand widespread applications over the past years. However, generating recommendations in few-shot\nscenarios remains a persistent challenge. Recently, large language models (LLMs) have emerged as\na promising solution for addressing natural language processing (NLP) tasks, thereby offering novel\ninsights into tackling the few-shot scenarios encountered by explicit feedback-based recommender systems.\nTo bridge recommender systems and LLMs, we devise a prompting template that generates user and item\nrepresentations based on explicit feedback. Subsequently, we integrate these LLM-processed representations\ninto various recommendation models to evaluate their significance across diverse recommendation tasks.\nOur ablation experiments and case study analysis collectively demonstrate the effectiveness of LLMs\nin processing explicit feedback, highlighting that LLMs equipped with generative and logical reasoning\ncapabilities can effectively serve as a component of recommender systems to enhance their performance\nin few-shot scenarios. Furthermore, the broad adaptability of LLMs augments the generalization potential\nof recommender models, despite certain inherent constraints. We anticipate that our study can inspire\nresearchers to delve deeper into the multifaceted dimensions of LLMs’ involvement in recommender systems\nand contribute to the advancement of the explicit feedback-based recommender systems field.\nINDEX TERMS Large language models, recommender systems, ChatGPT, representations.\nI. INTRODUCTION\nRecommender systems are defined as techniques that utilize\nusers’ explicitly or implicitly expressed preferences to\nprovide recommendations for items of interest, address\nthe issue of information overload, and deliver novelty and\nsurprise [1]. With the advancement of deep learning, the field\nof recommender systems has witnessed significant progress\nin recent years. Initially, collaborative filtering and ID-based\nmethods are widely adopted across diverse recommendation\nscenarios [2], [3], [4]. Subsequently, there has been a growing\nresearch focus on incorporating textual side information into\nrecommender systems to develop knowledge-based [5], [6],\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Prakasam Periasamy\n.\n[7] and content-based [8], [9] approaches that effectively\nleverage explicit feedback.\nHowever, the majority of recommendation methods con-\ntinue to grapple with multiple long-standing challenges. The\nmobile nature of cyber users and the continuous emergence\nof new items have underscored the significance of few-\nshot scenarios, where recommender systems are required to\nprovide recommendations based on limited user information.\nSimultaneously, recommender systems commonly possess\na task-specific property that constrains their generalization\ncapabilities across different data sources and application\nscenarios. Such property is currently being challenged in the\ndynamic cyberspace, where explicit feedback from users has\nbecome increasingly complex and overwhelming in volume.\nMoreover, as essential tools for consumer engagement,\nmarketing, and business analysis [10], [11], recommender\n29144\n\n 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 12, 2024\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nsystems necessitate interpretability and transparency; never-\ntheless, the integration of deep learning has hindered these\naspects.\nThe recent advancements in large language models\n(LLMs) have offered promising prospects for addressing the\naforementioned challenges. Emerging LLMs with generative\nand logical reasoning capabilities, such as ChatGPT, exhibit\nremarkable proficiency in text summarization and possess\npotential for association [12], [13], thereby endowing them\nwith a natural aptitude for engagement in textual explicit\nfeedback processing. Meanwhile, the integration of LLMs\ninto diverse recommendation tasks from various perspectives\nhas emerged as a pivotal area of investigation. Nevertheless,\nprior research [14] suggests that when employed directly and\nsolely as a recommender system in few-shot scenarios, LLMs\ndo not demonstrate superior performance across various tasks\ncompared to traditional recommendation models. In contrast,\nrecent studies highlight LLMs’ effective participation in\nrecommendations as a component of recommender systems\n[15], [16]. This motivates our novel research proposal:\ninvestigating the potential of utilizing LLMs to generate\nuser and item representations using textual explicit feedback,\nthereby enhancing the performance of existing recommender\nmodels in few-shot scenarios.\nTo investigate this subject, we conduct an in-depth study\nby referencing previous research [14], [17]. We develop\na template to process movie reviews from a deliberately\nselected public dataset using LLMs to generate user and item\nrepresentations. These representations are then incorporated\ninto selected recommendation models for evaluation on two\ntasks: interaction prediction and direct recommendation.\nTo specifically investigate the extraction and association\ncapabilities of the experimental LLMs, we manually adjusted\nthe number of training samples to simulate a few-shot\nscenario.\nComprehensive experimental results indicate that utilizing\nLLMs for representation generation significantly enhances\nthe performance of specific recommendation models in a\nfew-shot scenario, demonstrating that LLMs can effectively\nserve as an explicit feedback processing method for multiple\nrecommendation tasks. Our manual observations also suggest\nthat certain LLMs with generative and logical reasoning\ncapabilities possess a distinctive ability to generate sup-\nplementary information through association. LLMs’ broad\napplicability across diverse scenarios and proficiency in\nprocessing textual information even in the absence of\nquantitative metrics can augment the generalization potential\nof recommender systems. It is worth noting that the observed\nenhancements are more pronounced in recommendation\nmodels that integrate neural networks. This phenomenon\ncould be attributed to inherent constraints imposed by model\nstructures and characteristics of the embeddings.\nWe hope the results of this experiment can inspire\nresearchers to further explore the incorporation of LLMs\ninto the recommendation process, while offering valuable\ninsights in specific research fields, such as interpretability,\ncold-start challenges, and model enhancement within explicit\nfeedback-involved recommender systems.\nII. RELATED STUDY\nA. EXPLICIT FEEDBACK FOR RECOMMENDATION\nIn contrast to implicit feedback derived primarily from\nuser behavior observations, explicit feedback is openly\nand actively provided by users themselves to reflect their\npreferences and attitudes. The concept of explicit feedback\nmentioned in the book Recommender Systems: An Introduc-\ntion encompasses ratings and annotations [18], while Konstan\nand Riedl [19] broaden its definition to include diverse forms\nof user-contributed content such as reviews, tags, blog posts,\ntweets, Facebook updates, among others.\nIn previous studies, ratings have been regarded as a crucial\nform of explicit feedback that enhances the performance of\nrecommender systems [20], [21] and can be combined with\nimplicit feedback to cater to diverse recommendation tasks\n[22], [23]. Text, serving as another manifestation of explicit\nfeedback, can also be leveraged by recommender systems.\nTextual explicit feedback is commonly manifested as user\nreviews and comments [24] that are generated in various\nlanguages [25]. Other forms of textual explicit feedback\ninclude but are not limited to Tweets [26], web chats [27],\nmessages accompanied by geographic information [28], and\nTags [29]. Therefore, natural language processing (NLP)\nplays a crucial role in constructing recommender systems\nthat rely on textual explicit feedback. Text mining has\nlong been considered as an essential prerequisite in various\nrecommendation models [24], encompassing techniques such\nas Latent Dirichlet Allocation (LDA) [30], TF-IDF [29],\nword segmentation [25], rule-based classifiers [31], and\nmore. The processed text can be leveraged to support\nrecommender systems built through approaches such as col-\nlaborative filtering [25], [30], content-based filtering [27] or\nknowledge-based [28].\nIn recent years, the embedding process has emerged\nas a prominent focus in recommendation studies due to\nadvancements in related research. The utilization of LLMs\nin recommendation has been increasingly prevalent owing to\ntheir proficiency in comprehending and processing human\nnatural language [32]. Transformer architecture models (e.g.,\nBERT, GPT, and T5 [33]) have been extensively employed\nin aspects including Pre-training, Fine-tuning, and Prompt-\ning [32]. Attention mechanism has also been integrated in the\ndevelopment of recommender system models. For instance,\nNARRE [34], a neural attention recommendation framework\nutilizing user reviews, is introduced to simultaneously predict\nusers’ ratings towards items and generate review-level\nexplanations for the prediction. Other attention models such\nas TARMF [35] and MPCN [36] that leverage textual\nexplicit feedback also exhibit superior performance across\ndiverse recommendation tasks compared to existing deep\nlearning-based recommendation models (e.g., ConvMF [37],\nDeepCoNN [38]).\nVOLUME 12, 2024 29145\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nB. CHATGPT FOR RECOMMENDATION\nReleased by OpenAI in 2022, ChatGPT [39] is an advanced\nLLM and dialogue system that has demonstrated exceptional\nperformance across various vertical domains. It showcases\nremarkable capabilities in context-based comprehension,\nsummarization, and text generation [12]. The investiga-\ntion into the methodology of transferring and employing\nChatGPT’s extensive knowledge and paradigm acquired\nfrom large-scale corpora to recommendation scenarios has\nemerged as a cutting-edge pursuit in the academic domain.\nChatGPT can independently serve as a versatile rec-\nommendation model capable of handling various recom-\nmendation tasks. Liu et al. [14] consider ChatGPT as a\nself-contained recommender system and construct a bench-\nmark to track its performance in specific recommendation\ntasks, such as rating prediction and direct recommendation.\nChatGPT can also serve as a component of existing recom-\nmender systems. Gao et al. [16] introduce Chat-REC, which\nemploys ChatGPT as an interface for conversational recom-\nmendations, thereby enhancing the performance of existing\nrecommendation models and rendering the recommendation\nprocess more interactive and explainable. Dai et al. [13]\npropose ChatAug that utilizes ChatGPT to rephrase sentences\nfor textual data augmentation, simultaneously demonstrating\nthe effectiveness of ChatGPT as a text summarization tool\nwhen accompanied by pretrained language models (BERT).\nIn terms of natural language generation tasks, ChatGPT\ndemonstrates remarkable proficiency in generating per-\nsuasive recommendation interpretations and advertisements\nunder specific conditions [14], [40]. Related research also\nsuggests that the engagement of ChatGPT could be a inno-\nvative solution to address few-shot learning challenges [41].\nHowever, recent research [14] reveals that when employed\nindependently in few-shot scenarios as a recommender\nsystem, ChatGPT’s performance falls short compared to a\nseries of classical recommendation models across diverse\nrecommendation tasks, such as top-N direct recommenda-\ntions. The aforementioned studies inspire us to explore the\nutilization of ChatGPT as an explicit feedback processing\nmethod indirectly participating in few-shot recommendation\nscenarios.\nIII. REPRESENTATIONS GENERATION\nA. TASK FORMULATION\nChatGPT is designed to excel in user-oriented tasks, enabling\nus to adopt prompting paradigms [42] to target specific\ntasks without the need for fine-tuning. Drawing partly from\nrelevant studies [14], our experiment initially utilizes the\nwell-established ChatGPT model, gpt-3.5-turbo, to generate\ntextual user and item representations by providing ChatGPT\nwith tailored prompts. Each prompt consists of three compo-\nnents: review injection, task description, and format indicator.\nThe review injection is designed to provide ChatGPT with a\nsequence of reviews from the same subject (a specific user\nor item). The task description aims to elucidate the input\nmaterials and establish clear task requirements. The format\nindicator serves to standardize response formats and constrain\ncontent scope. Additionally, we set a limiter when generating\nprompts to prevent them from exceeding the maximum token\nlimit in ChatGPT.\nGiven that the API interface of ChatGPT necessitates its\ninvocation in a conversational format, we assume the template\nτ, which denotes the procedure for employing ChatGPT\nto generate a textual representation of a specific subject\nby utilizing its review collections. Formally, this can be\nexpressed as\nτ = [X]suffix[Y ] (1)\nwhere Y represents a slot subsequently filled by ChatGPT’s\nresponse, suffix (i.e., task description and format indicator)\nidentifies certain text specifically designed to guide ChatGPT\nin accomplishing representation generation, and input X is\na sequence of reviews r pertaining to the specific subject,\nformally:\nX = {r1, r2, r3, . . . ,rn}. (2)\nB. GENERATE TEXTUAL REPRESENTATIONS BY USING\nCHATGPT\nThe example in Fig.1 illustrates the generation of a user\nrepresentation through template τ. It is noteworthy that\nthe generation of the item representations also adheres\nto template τ, albeit with a slightly different suffix; we\nmodify the task description context for item representations\ngeneration to ‘‘(...Based on your understanding of these\nmovie reviews, summarize the movie’s tag and scenes,\nassociate and infer what type of audience and fans may be\nattracted by this movie.’’ In response to this description,\nChatGPT would provide associations and inferences such\nas ‘‘Audiences who prefer heartwarming scenes and happy\nendings’’ for item representations.\nChatGPT incorporates a certain degree of randomness\nto ensure the diversity of generated response, which may\npose challenges in terms of reproduction and evaluation.\nThe implementation of the format indicator component has\nbeen observed to effectively standardize the responses and\nmitigate irrelevant variations. During preliminary training\nwith small sample sizes, ChatGPT exhibits exceptional\nassociation and inference capabilities that surpass our ini-\ntial expectations. In certain instances, ChatGPT accurately\n‘‘guesses’’ a specific movie and subsequently retrieves\ncomprehensive information from its own database, even when\nthe movie title is not explicitly mentioned in the original\nreviews. To ensure controlled variables, we explicitly instruct\nChatGPT to exclusively focus on materials provided by us\nwhen generating representations.\nC. EMBED TEXTUAL REPRESENTATIONS BY USING\nLANGUAGE MODELS\nAfter generating textual user and item representations,\nwe employ MacBERT [43], a pre-trained LLM for Chinese,\n29146 VOLUME 12, 2024\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nFIGURE 1. Example of using ChatGPT to generate a textual user representation. Notably, the original reviews, prompts, and ChatGPT responses are\nall in Chinese; we employ ChatGPT to translate them into english for improved readability.\nto embed them to become our experimental dataset. Simul-\ntaneously, we construct a control dataset by concatenating\nreviews that belong to the same subject (item or user),\nembedding them with MacBERT, and merging the outputs.\nAdditionally, we use a pre-trained Chinese Word2vec model\n[44] that does not employ attention mechanism to generate\nembeddings as an extra reference in some cases.\nThe length of each embedding generated using MacBERT\nis 1,024, while the length of those generated using Word2vec\nis 200. Considering the superior efficiency of MacBERT\nin natural language embedding tasks, we primarily uti-\nlize MacBERT-processed embeddings as our main control\ndatasets and only refer to experimental results obtained\nfrom using Word2vec-processed embeddings under specific\nconditions. The model selection as well as the embedding\nprocess partially drew upon a relevant study [13].\nIV. EVALUATION\nTo assess the effectiveness of LLMs as a textual explicit\nfeedback processing method for recommender systems,\nwe conduct ablation studies on diverse tasks with the aim of\nanswer the following research questions:\n• RQ1: Do the LLM-processed user and item repre-\nsentations exhibit disparities compared to the original\nreviews?\n• RQ2: How effectively do these representation function\nacross different recommendation models and tasks, in a\nfew-shot scenario?\n• RQ3: Do the textual representations generated by\nChatGPT in our experiment possess additional observ-\nable attributes and features, beyond those demonstrated\nin the aforementioned experiment results?\nA. EXPERIMENTAL SETUP\n1) WORKFLOW\nBuilding upon previous studies [13], [14], we design our\nexperimental workflow as follows: Firstly, we construct\neligible datasets that include explicit user feedback and\nrelevant information (Section IV.B). Secondly, the select\nuser profiles and reviews are transformed into prompts for\nChatGPT to generate textual user and item representations\n(elaborated in Section III). Thirdly, the textual representations\ngenerated by ChatGPT undergo manual observation for case\nstudy purposes (Section IV.E) while concurrently being\nembedded by using MacBERT to construct an experimental\ndataset. Finally, the experimental dataset is incorporated into\nselected recommendation models for various recommenda-\ntion tasks(Section IV.C, 4.D), along with control datasets.\nThe complete workflow of our experimental process is\nillustrated in Fig.2.\n2) BASELINES AND METRICS\nIn Section IV.C, we examine the disparities between the\nembeddings in the experimental dataset (ChatGPT-processed\nand MacBERT-embedded) and the embeddings in the control\nVOLUME 12, 2024 29147\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nFIGURE 2. Schematic representation of the complete experimental workflow.\ndataset (non-ChatGPT-processed and MacBERT-embedded).\nWe employ three statistical methods [45], namely cosine\nsimilarity, Manhattan distance, and Euclidean distance,\nto quantify the semantic relationships between embeddings\nof each subject (user/item) across the two datasets, namely\nembX from the experimental datasets and embX′ from the\ncontrol datasets. We computed the mean cosine similarity,\nmean Manhattan distance, and mean Euclidean distance by\naveraging the results across all the subjects. The formula is\npresented below, where n represents the size of the dataset\nand d is the length of an individual embedding (1,024 for\nMacBERT embeddings):\nmean Manhattan distance\n= 1\nn\nn∑\nk=1\nd∑\ni=1\n|embXki − embX′\nki| (3)\nmean Euclidean distance\n= 1\nn\nn∑\nk=1\n√\nd∑\ni=1\n(embXki − embX′\nki)2 (4)\nmean cosine similarity\n= 1\nn\nn∑\nk=1\n∑d\ni=1 embXkiembX′\nki√∑d\ni=1 embX2\nki\n√∑d\ni=1 embX\n′2\nki\n. (5)\nIn Section IV.D, we evaluate the effectiveness of incorpo-\nrating the LLM-processed embeddings into classical recom-\nmendation models for two recommendation tasks: interaction\nprediction and direct recommendation. The former consti-\ntutes a pivotal component in some neural network-based\nrecommender systems [46], [47], while the latter represents a\nprevalent recommendation task.\nFor interaction prediction (i.e., predicting whether a user\nwill engage in interaction with a specific item), we employ\nLinear, MLP [47], and CNN [48] models as our baselines.\nWe consider user-item interactions as labels; specifically,\nground truth interactions will be labeled as 1, while negative\nsamples (labeled as 0) are generated by randomly assigning\neach user an item that they have not interacted with in\nreality. Given the binary classification nature of the task,\nwe utilize Accuracy, Precision, and F1 Score as evaluation\nmetrics to assess performance. For direct recommendation\n(i.e., recommending items that are most likely to align with a\nuser’s preferences), we employ BPR-MF [49], NCF-Linear,\nNCF-MLP, and NCF-CNN [50] as baselines.\nWe evaluate their performance using widely adopted\nmetrics in recommender system studies, namely top-k Hit\nRatio (HR@k) and top-k Mean Reciprocal Rank (MRR@k).\nConsidering the few-shot scenario, we report results on\neither HR@10,100 and either MRR@10,100. It is worth\nnoting that despite varying in structural configurations, the\naforementioned baselines integrating MLP and CNN neural\nnetworks have a comparable number of layers and are not\nfine-tuned respectively.\nB. DATASET CONSTRUCTION\nThe dataset employed in our experiment is the publicly\navailable Douban Chinese Moviedata-10M [51], which\nshares similarities with the benchmark MovieLens dataset\n[52] in terms of content and format. The Douban dataset\nencompasses a substantial amount of explicit feedback\nprovided by platform users, each sample presented as a\nuser-item interaction comprising a user ID, an item ID, a piece\nof movie review, and other pertinent information such as\na rating and a timestamp. In contrast to the MovieLens\ndataset, the Douban dataset primarily comprises Chinese\ntext, and encompasses a substantial number of colloquial\nexpressions, internet memes, emojis, and other intricate\nlinguistic corpora. We intentionally perform minimal data\ncleansing to thoroughly evaluate ChatGPT’s proficiency in\nhandling real-world explicit feedback.\nTo construct our experimental dataset, a cohort of 1,000\nusers is randomly selected. We extract the historical user-item\n29148 VOLUME 12, 2024\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nTABLE 1. User training dataset details.\ninteraction samples of these users and sort them in chronolog-\nical order. The item IDs corresponding to the two most recent\ninteractions are extracted as test and validation samples,\nrespectively, and are concatenated into their respective sets.\nThe remaining interaction samples of these users constitute\nthe training dataset for inputting into ChatGPT to generate\ntextual user representations. To simulate a few-shot scenario,\nwe artificially control the number of interaction samples per\nuser by randomly discarding excess samples while ensuring\nat least one sample per user remains. Detailed statistical\ninformation about the user training dataset is provided in\nTab.1.\nAfter extracting all the samples corresponding to the\naforementioned 1,000 users, we construct the remaining\nsamples as the item training dataset. Each item in the dataset\nis designed to have at least one corresponding sample.\nWe apply filtering rules to ensure that all items present in\nthe user training dataset, validation dataset, and test dataset\nexist simultaneously in the item training dataset. Moreover,\nin order to maintain control variables when constructing the\ncontrol datasets, we restrict the number of samples per item\nto a maximum of 10. Following filtration, approximately\n98.05% of the reviews in these samples are less than\n150 words long (with 79.84% being under 100 words).\nEventually, during the stage of constructing the training\ndataset, we obtain a user training dataset consisting of 1,000\nusers, encompassing a total of 7,270 interaction samples;\nadditionally, we create validation and test datasets with each\ncomprising 1,000 samples (one per user); finally, an item\ntraining dataset is compiled containing over 300,000 inter-\naction samples from 38,750 items. By providing ChatGPT\nwith tailored prompts derived from the two training datasets\n(elaborated in Section III), we generate a corresponding\nnumber of textual user and item representations, which are\nthen combined to form textual user and item representation\ndatasets and subsequently embedded by language models to\nform experimental datasets. The workflow for constructing\nthe datasets is illustrated in Fig.3.\nAs outlined in Section III, we employ MacBERT and\nWord2vec to embed the textual item and user representations\nfor generating embedding datasets. Additionally, we build\ncontrol datasets in accordance with the methodology detailed\nin the section.\nIn total, we acquire the following datasets:\n• A pair of textual representation datasets (user & item).\n• A pair of experimental datasets (user & item): ChatGPT-\nprocessed + MacBERT-embedded\nTABLE 2. Semantic distances between experimental and control datasets.\n• Three pairs of control datasets (user & item):\nOnly MacBERT-embedded; Only Word2vec-embedded;\nChatGPT-processed + Word2vec-embedded.\nC. DISPARITIES EVALUATION (RQ1)\nIn this section, we quantify the semantic relationships\nbetween embeddings of each subject (user/item) across\nthe experimental dataset (ChatGPT-processed + MacBERT-\nembedded) and the control dataset (MacBERT-embedded).\nThe evaluation method proposed in Section IV.A is employed\nto obtain the statistical measurement results presented in\nTab.2.\nThe cosine similarity metric primarily focuses on the angu-\nlar relationship between two vectors in a multi-dimensional\nspace. When comparing two semantically similar sentences,\nregardless of their length, the angle between their vectors\nbecomes smaller, resulting in a higher value for cosine\nsimilarity. Euclidean distance and Manhattan distance calcu-\nlations encompass both direction and magnitude, which can\nserve as a complementary measure to cosine similarity.\nWhen comparing the experimental and control datasets,\nboth in terms of items and users, we observe that the result of\nMean Cosine distance approaches 1, indicating a significant\nsemantic similarity between the representations generated by\nChatGPT and the original reviews. We also note that the Mean\nEuclidean and Manhattan distances deviate significantly from\nzero. Based on these results, we suggest that while the\nChatGPT-generated representations demonstrate comparable\nsemantics to the original reviews, they do exhibit significant\ndisparities in terms of information content and quantity.\nThis discrepancy may be attributed to their truncated\nlength and refined content. In general, the aforementioned\nfindings partially substantiate the effectiveness of ChatGPT\nin extracting a substantial portion of salient features and\ncrucial information from the original reviews, albeit with\na reconfigured textual composition and altered content.\nThe reconfiguration and alteration will be examined in\nSection IV.E through a detailed case study.\nD. PERFORMANCE COMPARISON ON\nRECOMMENDATION TASKS (RQ2)\nFig.4 depicts the workflow for conducting ablation exper-\niments on two recommendation tasks using user-item\ninteractions and the user and item embeddings from both\nexperimental and control datasets. Notably, we conduct\n10 independent repetitions to train each model in the two\nVOLUME 12, 2024 29149\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nFIGURE 3. Schematic representation of the datasets construction workflow.\nFIGURE 4. Schematic representation of the experimental workflow for two recommendation tasks.\nrecommendation tasks and report the average results, aiming\nto comprehensively investigate their overall performance.\nFor the interaction prediction task, we concatenate the user\nembedding and item embedding from the same interaction\nsamples (including randomly generated negative samples)\nand input them into binary classification models along with\ntheir labels for training. Subsequently, we assess the model’s\nAccuracy, Precision, and F1 Score on the ground truth test\ndataset.\nFor the direct recommendation task, we initialize the\nrecommendation model with user-item interaction dataset.\nUpon model initialization, the BPR and NCF models auto-\nmatically generate random embeddings for users and items,\nwhich are subsequently updated during training based on the\nmodels’ learning from the user-item interaction dataset (with\nratings). After model training, these fine-tuned embeddings\nserve as a foundation for recommending items to selected\nusers. In our study, we eliminate ratings by substituting\nthem with a uniform constant to prevent the recommendation\nmodel from relying on ratings. As compensation, we replace\nthe model-automatically-generated embeddings with the user\nand item embeddings in our experimental and control\ndatasets. We assess the performance of the models using\nHR (Hit Rate) and MRR (Mean Reciprocal Rank), while\nadditionally considering scenarios where these embeddings\ncontinue to undergo fine-tuning or remain fixed during model\ntraining.\n1) INTERACTION PREDICTION\nFor the interaction prediction task, we conduct ablation\nexperiments on experimental and control datasets using\nclassical Linear, MLP, and CNN models respectively. The\n29150 VOLUME 12, 2024\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nTABLE 3. Performance comparison on interaction prediction tasks.\nstatistical measurements obtained from these experiments are\nreported in Tab.3.\nBased on our observations, under the same MLP model,\nthe experimental dataset demonstrate superiority over the\ncontrol datasets. The results suggest that the incorporating\nChatGPT-processed representation embeddings holds the\npotential to enhance certain recommender models that\nemploy neural networks in a few-shot scenario.\nNotably, among all experimental models that integrated\nneural networks, the MLP model stands out as the only one\nto exhibit statistically significant results in both experimental\nand control datasets. In contrast, we observe that the CNN\nmodel exhibited a significantly high training loss and failed\nto successfully converge during training. We speculate that\nthis phenomenon can be attributed to the length of the\nconcatenated embedding and the limited number of the\ntraining samples, as certain neural networks may encounter\ndetrimental effects on learning and convergence with a\nfew-shot scenario characterized by an abundance of training\nfeatures. This partially elucidates the unsatisfactory model\nperformance observed in our experimental findings.\n2) DIRECT RECOMMENDATION\nFor the direct recommendation task, we conduct ablation\nexperiments using experimental and control datasets on the\nBPR and NCF recommendation models, and investigate the\nimpact of enabling or disabling automatic model updating\nduring training. The specific experimental results are pre-\nsented in Tab.4 and Tab.5, with all outputs appropriately\nrounded to ensure a reader-friendly presentation.. Due\nto significant variations in performance among different\nrecommendation models, we adopt HR and MRR @10\nfor NCF models and @100 for BPR models, respectively,\nto effectively showcase their performance. Furthermore,\nwe present the percentage improvement of experimental\nmodels in comparison to the baseline model (which employs\nrandomly generated embeddings) across diverse datasets,\nwith a primary focus on results demonstrating an increase of\n200% or more for emphasis.\nThe ablation experiments demonstrate the significance\nof utilizing ChatGPT-processed embeddings to enhance a\nseries of recommended models in few-shot scenarios. This\nenhancement is particularly evident in recommendation\nTABLE 4. Performance comparison on BPR-MF model.\nTABLE 5. Performance comparison on NCF models.\nmodels that incorporate neural networks. Specifically, NCF-\nMLP outperforms NCF-CNN in terms of both HR and MRR\nmetrics; models that fixed embeddings during training exhibit\ncomparatively superior performance compared to those\nfine-tuned.\nBased on the experimental results, we suggest that the\nintegration of neural networks enhances the recommendation\nmodels’ capacity to process LLM-generated embeddings,\nwhich implies a substantial number of training features.\nWe speculate that the limited sample size poses challenges\nfor all neural networks, thereby compromising the validity\nof LLM-generated embeddings when automatically fine-\ntuned, whereas MLP is the sole network demonstrating\nsuperior adaptability in few-shot scenarios in our experiments\n(as evidenced by the results presented in the interaction\nprediction recommendation task). Meanwhile, recommen-\ndation models that do not incorporate neural networks\nencounter significant difficulties when dealing with lengthy\nembeddings. This could partially account for the supe-\nrior experimental results obtained by utilizing Word2vec-\nembedded embeddings (which have shorter lengths compared\nVOLUME 12, 2024 29151\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\nto MacBERT-embedded embeddings) in BPR-MF models as\nopposed to other datasets.\nE. CASE STUDY (RQ3)\nIn addition to conducting ablation experiments, we perform\na comprehensive case study on the textual user and item\nrepresentations to complement our findings and uncover\npotentially overlooked information within the embedding\nprocess. Our manual observations suggest that ChatGPT\ndemonstrates exceptional proficiency in processing explicit\ntextual feedback.\nSpecifically, it consistently demonstrates precise recogni-\ntion and comprehension of contextual information with vary-\ning sentiment tendencies, even in the absence of quantitative\nmetrics such as ratings. Notably, ChatGPT effectively handles\nreviews that contain positive, neutral, and negative snippets\nsimultaneously by either disregarding the negative portion\nor considering an opposing viewpoint for recommendations.\nAdditionally, ChatGPT adeptly identifies quotations within\nthe reviews (e.g., movie lines, plots, extra materials) and\nutilizes them appropriately. The aforementioned observations\ncollectively suggest that ChatGPT holds the potential to\nenhance the generalization capability of recommendation\nmodels by providing adaptability for diverse recommenda-\ntion scenarios, such as social media platforms that exclusively\ncomprising textual content.\nMeanwhile, in contrast to conventional language models,\nChatGPT demonstrates a unique ability to generate expansion\ncontext even when provided with limited information. While\ntraditional NLP approaches primarily focus on keyword\nidentification and extraction, ChatGPT goes beyond by\nintroducing new content that may deviate from the original\ncorpus. For instance, as depicted in Fig.1, ChatGPT suggests\nthe keyword ‘‘furry lovely animals,’’ possibly due to the\nuser’s preference for documentaries featuring bears and\nanimations. Essentially, ChatGPT ‘‘refines and reinforces’’\ninitial representations by augmenting them with supple-\nmentary information through association and inference.\nThis could partially account for the observed semantic\nsimilarity yet content disparity between the experimental\nand control datasets, as evidenced by the findings in\nSection IV.C. Furthermore, the effectiveness of the refined\nand reinforced representations is demonstrated with support\nfrom the experimental results presented in Section IV.D.\nThis partially indicates that the additional information\ncontained within these representations, generated through\nChatGPT’s association and inference, carries significant\nimplications. In other words, these supplementary pieces of\ninformation reasonably reflect users’ underlying thoughts to\na certain extent. To summarize, ChatGPT demonstrates its\neffectiveness in handling few-shot recommendation scenarios\ncompared to conventional language models, owing to its\ndistinctive capabilities in associative thinking and logical\nreasoning.\nIt is noteworthy that in this experiment, ChatGPT functions\nas a symbolic representation of emerging LLMs endowed\nwith generative and logical reasoning capabilities. Consider-\ning the continuous advancements in technology, forthcoming\nLLMs equipped with enhanced proficiencies in association\nand inference may ultimately supplant ChatGPT within our\nexperimental framework. Nevertheless, the insights derived\nfrom this investigation retain significant reference value for\nfuture studies.\nV. CONCLUSION\nIn this study, we conduct ablation experiments to assess the\neffectiveness of harnessing LLMs to enhance few-shot rec-\nommender systems in various recommendation tasks. Despite\nthe limitations imposed by model structures, the inclusion\nof LLM-processed representations significantly enhances\nthe performance of specific neural network-based recom-\nmendation models in our experimental few-shot scenario.\nBased on the experimental results, we suggest that LLMs\nequipped with generative and logical reasoning capabilities\ncan serve as an effective NLP method for recommender\nsystems, proficiently handling textual explicit feedback\nthrough their distinctive capabilities and enhancing the\ngeneralization potential of recommendation models. Moving\nforward, we envision integrating additional recommendation\nmodels based on neural networks into our study. Furthermore,\nwe are intrigued by the potential business applications\n(e.g., marketing analytics, advertisement generation) of the\nChatGPT-generated textual user and item representations.\nACKNOWLEDGMENT\nThe author wishes to extend his appreciation to Prof. Jimbo,\nProf. Howard, Li Zhi, and Lei for their support throughout his\nacademic journey.\nREFERENCES\n[1] L. Mocean and C. M. Pop, ‘‘Marketing recommender systems: A new\napproach in digital economy,’’ Inf. Economica, vol. 16, no. 4, p. 142, 2012.\n[2] J. Bobadilla, S. Alonso, and A. Hernando, ‘‘Deep learning architecture for\ncollaborative filtering recommender systems,’’ Appl. Sci., vol. 10, no. 7,\np. 2441, Apr. 2020.\n[3] F. Rezaimehr and C. Dadkhah, ‘‘A survey of attack detection approaches\nin collaborative filtering recommender systems,’’ Artif. Intell. Rev., vol. 54,\nno. 3, pp. 2011–2066, Mar. 2021.\n[4] S. Zhang, L. Yao, A. Sun, and Y . Tay, ‘‘Deep learning based recommender\nsystem: A survey and new perspectives,’’ ACM Comput. Surv. , vol. 52,\nno. 1, pp. 1–38, Jan. 2020.\n[5] F. Cena, L. Console, and F. Vernero, ‘‘Logical foundations of knowledge-\nbased recommender systems: A unifying spectrum of alternatives,’’ Inf.\nSci., vol. 546, pp. 60–73, Feb. 2021.\n[6] M. Dong, X. Zeng, L. Koehl, and J. Zhang, ‘‘An interactive knowledge-\nbased recommender system for fashion product design in the big data\nenvironment,’’Inf. Sci., vol. 540, pp. 469–488, Nov. 2020.\n[7] P. M. Alamdari, N. J. Navimipour, M. Hosseinzadeh, A. A. Safaei, and\nA. Darwesh, ‘‘A systematic study on the recommender systems in the E-\ncommerce,’’IEEE Access, vol. 8, pp. 115694–115716, 2020.\n[8] D. Mittal, S. Shandilya, D. Khirwar, and A. Bhise, ‘‘Smart billing\nusing content-based recommender systems based on fingerprint,’’ in ICT\nAnalysis and Applications, vol. 2. Cham, Switzerland: Springer, 2020,\npp. 85–93.\n[9] Y . Pérez-Almaguer, R. Yera, A. A. Alzahrani, and L. Martínez, ‘‘Content-\nbased group recommender systems: A general taxonomy and further\nimprovements,’’Expert Syst. Appl., vol. 184, Dec. 2021, Art. no. 115444.\n[10] A. Ansari, S. Essegaier, and R. Kohli, ‘‘Internet recommendation systems,’’\nJ. Marketing Res., vol. 37, no. 3, pp. 363–375, Aug. 2000.\n29152 VOLUME 12, 2024\nZ. Wang: Empowering Few-Shot Recommender Systems With LLMs-Enhanced Representations\n[11] A. V . Bodapati, ‘‘Recommendation systems with purchase data,’’ J.\nMarketing Res., vol. 45, no. 1, pp. 77–93, Feb. 2008.\n[12] T. B. Brown, ‘‘Language models are few-shot learners,’’ in Proc. NIPS,\n2020, pp. 1877–1901.\n[13] H. Dai, Z. Liu, W. Liao, X. Huang, Y . Cao, Z. Wu, L. Zhao, S. Xu,\nW. Liu, N. Liu, S. Li, D. Zhu, H. Cai, L. Sun, Q. Li, D. Shen, T. Liu, and\nX. Li, ‘‘AugGPT: Leveraging ChatGPT for text data augmentation,’’ 2023,\narXiv:2302.13007.\n[14] J. Liu, C. Liu, P. Zhou, R. Lv, K. Zhou, and Y . Zhang, ‘‘Is ChatGPT a good\nrecommender? A preliminary study,’’ 2023, arXiv:2304.10149.\n[15] D. Di Palma, G. M. Biancofiore, V . W. Anelli, F. Narducci, T. Di Noia,\nand E. Di Sciascio, ‘‘Evaluating ChatGPT as a recommender system: A\nrigorous approach,’’ 2023, arXiv:2309.03613.\n[16] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, ‘‘Chat-REC:\nTowards interactive and explainable LLMs-augmented recommender\nsystem,’’ 2023, arXiv:2303.14524.\n[17] Z. Kefato, S. Girdzijauskas, N. Sheikh, and A. Montresor, ‘‘Dynamic\nembeddings for interaction prediction,’’ in Proc. Web Conf., Apr. 2021,\npp. 1609–1618.\n[18] D. Jannach, M. Zanker, A. Felfernig, and G. Friedrich, Recommender\nSystems: Introduction. Cambridge, U.K.: Cambridge Univ. Press, 2010.\n[19] J. A. Konstan and J. Riedl, ‘‘Recommender systems: From algorithms to\nuser experience,’’ User Model. User-Adapted Interact., vol. 22, nos. 1–2,\npp. 101–123, Apr. 2012.\n[20] Q. Zhao, F. M. Harper, G. Adomavicius, and J. A. Konstan, ‘‘Explicit\nor implicit feedback? Engagement or satisfaction: A field experiment on\nmachine-learning-based recommender systems,’’ in Proc. 33rd Annu. ACM\nSymp. Appl. Comput., Apr. 2018, pp. 1331–1340.\n[21] S.-Y . Liu, H. H. Chen, C.-M. Chen, M.-F. Tsai, and C.-J. Wang, ‘‘IPR:\nInteraction-level preference ranking for explicit feedback,’’ in Proc. 45th\nInt. ACM SIGIR Conf. Res. Develop. Inf. Retr., Jul. 2022, pp. 1912–1916.\n[22] N. N. Liu, E. W. Xiang, M. Zhao, and Q. Yang, ‘‘Unifying explicit and\nimplicit feedback for collaborative filtering,’’ in Proc. 19th ACM Int. Conf.\nInf. Knowl. Manage., Oct. 2010, pp. 1445–1448.\n[23] G. Jawaheer, M. Szomszor, and P. Kostkova, ‘‘Comparison of implicit\nand explicit feedback from an online music recommendation service,’’\nin Proc. 1st Int. Workshop Inf. Heterogeneity Fusion Recommender Syst.,\nSep. 2010, pp. 47–51.\n[24] Y . Betancourt and S. Ilarri, ‘‘Use of text mining techniques for recom-\nmender systems,’’ in Proc. 22nd Int. Conf. Enterprise Inf. Syst., 2020,\npp. 780–787.\n[25] D. Miao and F. Lang, ‘‘A recommendation system based on text mining,’’\nin Proc. Int. Conf. Cyber-Enabled Distrib. Comput. Knowl. Discovery\n(CyberC), Oct. 2017, pp. 318–321.\n[26] K. Chen, T. Chen, G. Zheng, O. Jin, E. Yao, and Y . Yu, ‘‘Collaborative\npersonalized tweet recommendation,’’ in Proc. 35th Int. ACM SIGIR Conf.\nRes. Develop. Inf. Retr., Aug. 2012, pp. 661–670.\n[27] S. Loh, F. Lorenzi, R. Saldaña, and D. Licthnow, ‘‘A tourism recommender\nsystem based on collaboration and text analysis,’’ Inf. Technol. Tourism,\nvol. 6, no. 3, pp. 157–165, Jan. 2003.\n[28] R. L. Rosa, G. M. Schwartz, W. V . Ruggiero, and D. Z. Rodríguez,\n‘‘A knowledge-based recommendation system that includes sentiment\nanalysis and deep learning,’’ IEEE Trans. Ind. Informat., vol. 15, no. 4,\npp. 2124–2135, Apr. 2019.\n[29] R. Krestel, P. Fankhauser, and W. Nejdl, ‘‘Latent Dirichlet allocation for tag\nrecommendation,’’ inProc. 3rd ACM Conf. Recommender Syst., Oct. 2009,\npp. 61–68.\n[30] N. Jakob, S. H. Weber, M. C. Müller, and I. Gurevych, ‘‘Beyond the\nstars: Exploiting free-text user reviews to improve the accuracy of movie\nrecommendations,’’ in Proc. 1st Int. CIKM workshop Topic-Sentiment\nAnal. Mass Opinion, Nov. 2009, pp. 57–64.\n[31] Y . Li, J. Nie, Y . Zhang, B. Wang, B. Yan, and F. Weng, ‘‘Contextual\nrecommendation based on text mining,’’ in Proc. Coling, Posters , 2010,\npp. 692–700.\n[32] W. Fan, Z. Zhao, J. Li, Y . Liu, X. Mei, Y . Wang, Z. Wen, F. Wang, X. Zhao,\nJ. Tang, and Q. Li, ‘‘Recommender systems in the era of large language\nmodels (LLMs),’’ 2023, arXiv:2307.02046.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 5998–6008.\n[34] C. Chen, M. Zhang, Y . Liu, and S. Ma, ‘‘Neural attentional rating\nregression with review-level explanations,’’ in Proc. World Wide Web Conf.\nWorld Wide Web (WWW), 2018, pp. 1583–1592.\n[35] Y . Lu, R. Dong, and B. Smyth, ‘‘Coevolutionary recommendation model:\nMutual learning between ratings and reviews,’’ in Proc. World Wide Web\nConf. World Wide Web (WWW), 2018, pp. 773–782.\n[36] Y . Tay, A. T. Luu, and S. C. Hui, ‘‘Multi-pointer co-attention networks\nfor recommendation,’’ in Proc. 24th ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining, Jul. 2018, pp. 2309–2318.\n[37] D. Kim, C. Park, J. Oh, S. Lee, and H. Yu, ‘‘Convolutional matrix\nfactorization for document context-aware recommendation,’’ in Proc. 10th\nACM Conf. Recommender Syst., Sep. 2016, pp. 233–240.\n[38] L. Zheng, V . Noroozi, and P. S. Yu, ‘‘Joint deep modeling of users and\nitems using reviews for recommendation,’’ in Proc. 10th ACM Int. Conf.\nWeb Search Data Mining, Feb. 2017, pp. 425–434.\n[39] J. Achiam et al., ‘‘GPT-4 technical report,’’ 2023, arXiv:2303.08774.\n[40] M. Remountakis, K. Kotis, B. Kourtzis, and G. E. Tsekouras, ‘‘Using\nChatGPT and persuasive technology for personalized recommendation\nmessages in hotel upselling,’’ Information, vol. 14, no. 9, p. 504, Sep. 2023.\n[41] D. Di Palma, ‘‘Retrieval-augmented recommender system: Enhancing\nrecommender systems with large language models,’’ in Proc. 17th ACM\nConf. Recommender Syst., Sep. 2023, pp. 1369–1373.\n[42] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, ‘‘Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in\nnatural language processing,’’ACM Comput. Surv., vol. 55, no. 9, pp. 1–35,\nSep. 2023.\n[43] Y . Cui, W. Che, T. Liu, B. Qin, S. Wang, and G. Hu, ‘‘Revisiting\npre-trained models for Chinese natural language processing,’’ 2020,\narXiv:2004.13922.\n[44] Y . Song, S. Shi, J. Li, and H. Zhang, ‘‘Directional skip-gram: Explicitly\ndistinguishing left and right context for word embeddings,’’ in Proc. Conf.\nNorth Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol.\n(Short Papers), vol. 2, 2018, pp. 175–180.\n[45] D. Verma and S. N. Muralikrishna, ‘‘Semantic similarity between short\nparagraphs using deep learning,’’ in Proc. IEEE Int. Conf. Electron.,\nComput. Commun. Technol. (CONECCT), Jul. 2020, pp. 1–5.\n[46] P. Covington, J. Adams, and E. Sargin, ‘‘Deep neural networks for\nYouTube recommendations,’’ in Proc. 10th ACM Conf. Recommender\nSyst., Sep. 2016, pp. 191–198.\n[47] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye,\nG. Anderson, G. Corrado, W. Chai, and M. Ispir, ‘‘Wide & deep learning for\nrecommender systems,’’ in Proc. 1st Workshop Deep Learn. Recommender\nSyst., 2016, pp. 7–10.\n[48] Y . Kim, ‘‘Convolutional neural networks for sentence classification,’’\n2014, arXiv:1408.5882.\n[49] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme,\n‘‘BPR: Bayesian personalized ranking from implicit feedback,’’ 2012,\narXiv:1205.2618.\n[50] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua, ‘‘Neural\ncollaborative filtering,’’ in Proc. 26th Int. Conf. World Wide Web, 2017,\npp. 173–182.\n[51] D. Liu, Y . Gao, and Y . Xu. (2019). Douban Moviedata. [Online]. Available:\nhttp://moviedata.csuldw.com/ and https://github.com/csuldw/AntSpider\n[52] F. M. Harper and J. A. Konstan, ‘‘The MovieLens datasets: History\nand context,’’ ACM Trans. Interact. Intell. Syst., vol. 5, no. 4, pp. 1–19,\nJan. 2016.\nZHOUMENG WANGreceived the bachelor’s and\nMaster of Science degrees in marketing from The\nChinese University of Hong Kong, Shenzhen, in\n2022 and 2023, respectively.\nSince his graduation, he has been a research\nassistant. He received the prestigious Bowen\nScholarship and consistently made the Dean’s\nList twice during his study. His research interests\ninclude large language models, recommender\nsystems, and digital marketing. In 2022, he has\nparticipated in a Kaggle Recommender System Competition and won the\nSilver Medal.\nVOLUME 12, 2024 29153",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.797044038772583
    },
    {
      "name": "Recommender system",
      "score": 0.7940590381622314
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5447347164154053
    },
    {
      "name": "Natural language processing",
      "score": 0.4530446529388428
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40481486916542053
    },
    {
      "name": "Information retrieval",
      "score": 0.39273321628570557
    },
    {
      "name": "World Wide Web",
      "score": 0.35193049907684326
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ]
}