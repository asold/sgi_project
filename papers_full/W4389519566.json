{
  "title": "Bridging Information-Theoretic and Geometric Compression in Language Models",
  "url": "https://openalex.org/W4389519566",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2266283209",
      "name": "Emily Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2883632585",
      "name": "Corentin Kervadec",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2318310288",
      "name": "Marco Baroni",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W2028569884",
    "https://openalex.org/W2962779416",
    "https://openalex.org/W3173703884",
    "https://openalex.org/W2962807446",
    "https://openalex.org/W4309534039",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2030499717",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W3163313089",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2964184826",
    "https://openalex.org/W4297749952",
    "https://openalex.org/W4299853676",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2068474925",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3104335155",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2022845879",
    "https://openalex.org/W2963477238",
    "https://openalex.org/W3034513977",
    "https://openalex.org/W3004639598",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2157169955",
    "https://openalex.org/W2040549971",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3101334901",
    "https://openalex.org/W3154949003",
    "https://openalex.org/W4234345389",
    "https://openalex.org/W4386908184",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2996320484",
    "https://openalex.org/W4385571398",
    "https://openalex.org/W2947024452",
    "https://openalex.org/W3194668998",
    "https://openalex.org/W2925135347",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W3168685366",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4320855461"
  ],
  "abstract": "For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12397–12420\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBridging Information-Theoretic and Geometric Compression\nin Language Models\nEmily Cheng and Corentin Kervadec\nUniversitat Pompeu Fabra / Barcelona\n{name.lastname}@upf.edu\nMarco Baroni\nICREA / Barcelona\nmarco.baroni@upf.edu\nAbstract\nFor a language model (LM) to faithfully model\nhuman language, it must compress vast, poten-\ntially infinite information into relatively few\ndimensions. We propose analyzing compres-\nsion in (pre-trained) LMs from two points of\nview: geometric and information-theoretic. We\ndemonstrate that the two views are highly cor-\nrelated, such that the intrinsic geometric di-\nmension of linguistic data predicts their coding\nlength under the LM. We then show that, in\nturn, high compression of a linguistic dataset\npredicts rapid adaptation to that dataset, con-\nfirming that being able to compress linguistic\ninformation is an important part of successful\nLM performance. As a practical byproduct\nof our analysis, we evaluate a battery of in-\ntrinsic dimension estimators for the first time\non linguistic data, showing that only some en-\ncapsulate the relationship between information-\ntheoretic compression, geometric compression,\nand ease-of-adaptation.\n1 Introduction\nTo speak a language is not to memorize all pos-\nsible utterances, but to instead extract the finite\nruleset and lexicon that generates them (Chomsky,\n1986). That is, language, though nominally high-\ndimensional, can be compressed to a comparatively\nsmall intrinsic dimension.\nThe recent success of (large) language models\ndemonstrates that artificial neural networks, too,\ncan acquire linguistic knowledge. Current language\nmodels (LMs) are Transformer-based architectures\nat-scale (Vaswani et al., 2017) that are trained on\nthe conditional distribution of natural language\n(OpenAI, 2023; Zhang et al., 2022; Touvron et al.,\n2023; Chowdhery et al., 2022) and that are inching\ncloser to human-like linguistic robustness (Brown\net al., 2020; Liang et al., 2022; Wang et al., 2019b).\nFor an LM to faithfully model language, it must en-\ncode linguistic training data into finitely many vari-\nables that allow generalization to infinitely many\ngrammatical utterances. That is, it must perform a\nsuccessful form of data compression.\nThus, following the line of research that aims to\nbetter understand LM behavior (Wei et al., 2022;\nZhang et al., 2021; Rogers et al., 2020), in this work\nwe provide initial insights on how they compress\nlinguistic knowledge. We demonstrate an empirical\nlink between two types of compression: geometric\nand information-theoretic. In particular, we ask:\nhow, and by how much, do LMs compress linguis-\ntic data? Furthermore, what are linguistic correlates\nto compressibility? Is compression a good predic-\ntor of rapid adaptation? We show that (1) intrinsic\ndimension (ID) of linguistic data representations\nunder an LM tracks information-theoretic coding\nlength; (2) greater data compression predicts ease-\nof-adaptation in causal language modeling tasks;\n(3) interpretable linguistic properties such as vo-\ncabulary size and syntactic structure modulate ID;\nand (4) different model sizes recover similar ranges\nof ID. Finally, as a practical contribution, (5) we\nexplore different ways to estimate ID of linguistic\ndata, and find only some to capture the relation\nbetween ID, coding length, and ease-of-adaptation.\n2 Related Work\nCausal Language Models State-of-the-art lan-\nguage models are based on the Transformer archi-\ntecture (Vaswani et al., 2017), which consists of\nalternating feed-forward and self-attention modules\n(Bahdanau et al., 2015). They are trained usingself-\nsupervised learning on sequences of tokens, where\na token is defined as the atomic unit (e.g., a word\nor a sub-word) fed into the language model. Due to\ntheir current ubiquity, we focus on autoregressive\nmodels trained on a causal language modeling ob-\njective, that is, next token prediction given a context\nof previous tokens (Brown et al., 2020; Radford and\nNarasimhan, 2018; Zhang et al., 2022).\nLanguage models are typically measured against\nhuman performance on linguistic benchmarks.\n12397\nEvaluation may be done post-finetuning or in a\nlow-shot regime, where the model completes a lin-\nguistic task given few or zero examples. A variety\nof benchmarks, such as GLUE (Wang et al., 2019b),\nSuperGLUE (Wang et al., 2019a), and BigBENCH\n(Srivastava et al., 2022) have been proposed, evalu-\nating, for instance, model performance on textual\nentailment, question-answering, semantic equiva-\nlence, or sentiment analysis. Indeed, human base-\nlines for GLUE and SuperGLUE have already been\nsurpassed by LMs that contain billions of parame-\nters (e.g., PaLM 540B (Chowdhery et al., 2022)).\n2.1 Compression in LMs\nThere is a wide body of work analyzing compres-\nsion in deep neural architectures. In statistical\nlearning theory, compression has been empirically\nand theoretically linked to generalization (Shwartz-\nZiv and Tishby, 2017; Arora et al., 2018). More-\nover, deep learning models are thought to mini-\nmize description length (Perez et al., 2021; V oita\nand Titov, 2020; Blier and Ollivier, 2018). In\nlarge LMs, implicit compression of neural network\nparameters has been linked to ease-of-finetuning\nand generalization (Aghajanyan et al., 2021). Our\nwork complements this line of research by focus-\ning on compression of data representations rather\nthan network parameters. We consider compres-\nsion from two different perspectives: information-\ntheoretic and geometric (intrinsic dimension).\nInformation-theoretic compression Compres-\nsion in neural networks can be quantified from\nan information-theoretic point of view (Shannon,\n1948). For instance, the information plane (IP), a\nwidely studied framework introduced by Shwartz-\nZiv and Tishby (2017) and Tishby and Zaslavsky\n(2015), quantifies compression per-layer as the mu-\ntual information between representations and in-\nputs. However, there is little consensus in the rele-\nvant literature on the appropriate estimator to mea-\nsure this internal compression of inputs (Saxe et al.,\n2018; Goldfeld et al., 2019; Noshad et al., 2019;\nChelombiev et al., 2019; Geiger, 2022).\nInstead, as probabilistic models, LMs are nat-\nural black-box compressors: the negative log-\nlikelihood of the next token given context is, by def-\ninition, its Shannon coding length in bits (Shannon,\n1948). Concurrent work explores the equivalence\nbetween self-supervised prediction and lossless\ncompression, demonstrating that LMs can be pow-\nerful general-purpose compressors (Delétang et al.,\n2023). We similarly focus on information-theoretic\ncoding length of inputs under a pre-trained model,\nwhich is a simple measure of compression that, to\nour knowledge, has not been shown to be analyt-\nically equivalent to geometric compression. We\ndevelop this measure further in section 3.2.\nIntrinsic Dimension (ID) Geometric compres-\nsion is commonly quantified using dimensional-\nity reduction techniques. Often underlying these\napproaches is the manifold learning hypothesis\n(Goodfellow et al., 2016), or the notion that real-\nlife, high-dimensional data often lie on a low-\ndimensional manifold. Intrinsic dimension (ID),\nor the number of degrees of freedom in the data, is\nthe dimension of this data manifold.\nPerhaps the most prototypical ID estimators\nare linear projective methods like random projec-\ntion (Li et al., 2018) or Principal Component Analy-\nsis (PCA) (Jolliffe, 1986). While these project data\nto a linear subspace, the underlying geometric ob-\nject need not be linear; therefore, e.g., PCA poorly\nestimates ID for curved manifolds (Campadelli\net al., 2015). Nonlinear ID estimators include Cor-\nrelation Dimension (Grassberger and Procaccia,\n1983); Fisher Separability (Albergante et al., 2019);\nand a host of “nearest-neighbor\" (NN)-based meth-\nods, which use the fact that manifolds look locally\nEuclidean to fit the ID based on local neighbor dis-\ntributions (Facco et al., 2017; Levina and Bickel,\n2004; Haro et al., 2008; Amsaleg et al., 2018). Such\nmethods outperform linear ones on ID estimation\nbenchmarks (Campadelli et al., 2015). In section 4,\nwe will assess these methods in the context of lin-\nguistic data, and analyze how each of them relates\nto coding length and ease-of-adaptation.\nIn deep learning, there has been recent interest\nin using ID to characterize learning complexity.\nIntrinsic dimension has been quantified for neu-\nral network parameters (Li et al., 2018), as well\nas for input data and their representations in vi-\nsual and protein-sequence domains (Cohen et al.,\n2020; Recanatesi et al., 2019; Ansuini et al., 2019;\nValeriani et al., 2023; Pope et al., 2021). These\nstudies show that deep neural architectures learn\nlow-dimensional structures, encoding parameter\nweights and training data into orders-of-magnitude\nlower ID than their ambient dimension.\nIn the linguistic domain, low ID of LM parame-\nters has been shown to underlie efficient task adap-\ntation (Aghajanyan et al., 2021), where optimiza-\ntion occurs in low-dimensional, task-specific sub-\n12398\nspaces (Zhang et al., 2023). Moreover, parameter\nredundancy in pre-trained LMs can be exploited to\ndesign parameter-efficient finetuning methods such\nas LoRA (Hu et al., 2022). We are interested in ID\nof data representations as opposed to LM parame-\nters, as (1) we want to study how different linguistic\nproperties affect their coding; (2) ID estimation of\nmodel parameters can be expensive– large LMs\ncan have billions of parameters, while input repre-\nsentations are lower-dimensional, e.g., D= 4096\nin OPT-6.7b (Zhang et al., 2022). In related work\non LM representation ID, contextual word embed-\ndings have been found to lie in low-dimensional\nlinear subspaces (Mamou et al., 2020; Hernandez\nand Andreas, 2021). Most similar to our work, Cai\net al., 2021 show that Transformer embeddings of\nthe Wikitext and Penn TreeBank datasets constitute\nnonlinear manifolds of ID ∼O(10).\n3 Methods\nOur work attempts to bridge notions of geometric\nand information-theoretic compression of linguistic\ndata under an LM, and subsequently relate these\nto ease-of-adaptation. We do so by quantifying the\nID of data representations, information-theoretic\ncoding length of linguistic inputs under an LM, and\nease-of-finetuning in order to determine whether\nthese three phenomena are correlated.\nNotation Let a linguistic dataset X = {x(i)}N\ni=1\nconsist of N sequences of tokens, where each\nsequence x(i) has length l(x(i)). Let Mbe a\n(pre-trained) causal language model described by\npM(·|x<j), the conditional probability distribution\nof the jth token given its past context of tokens.\nModels & Datasets Experiments are performed\nfor the product of models M∈ [OPT-350m, OPT-\n1.3b, OPT-6.7b] and datasets X ∈table 1. We\nfocus on OPT suite of causal language models due\nto their accessibility (Zhang et al., 2022). For the\ndatasets, we start from a list of corpora including\nthe GLUE and SuperGLUE benchmarks, then pick\nthose whose size is large enough for ID estimates to\nconverge (N ≥10000). Then, for computational\nefficiency, we randomly subsample each dataset to\nsize N′ = max(N,50000), where 50000 is cho-\nsen conservatively based on preliminary analyses\nof convergence of bootstrapped ID estimates (see\nappendix A.2).\nIn addition to external datasets, we create one\nbaseline dataset per model, which we call OPTCor-\nBenchmark Datasets\nGLUE cola, mnli, mrpc, qnli qqp, rte,\nsst2, stsb\nSuperGLUE boolq, multirc, wic\nIMDB, Penn Treebank,\nBookcorpus, Wikitext fr,\nTweets, Pile-10k, CNN\nDailymail, Openwebtext-10k,\nCONCODE, OPTCorpus,\nOPTCorpus-permuted,\nOPTCorpus-swapped,\nOPTCorpus-random, Wiki-\ntext, Wikitext-permuted,\nWikitext-swapped, Wikitext-\nrandom\nTable 1: List of datasets used in experiments. We use all\ndatasets for ID and PPL estimation and the ones in the\nlast block for finetuning (except for OPTCorpus, which\nalready reflects the distribution of the model).\npus, of ∼24 million tokens by repeatedly randomly\nsampling from Muntil [EOS] is reached. The con-\nditional next-token distribution of OPTCorpus ap-\nproximates that of Mso to serve as a reference\ndatapoint.\nIn order to determine the effects of syntax\nand lexical semantics on compression, we define\nthree transformations which we apply to a dataset:\n(1) dataset-permuted: for each sequence in dataset,\nrandomly permute its tokens. This ablates syn-\ntax to retain bag-of-tokens lexical information.\n(2) dataset-swapped: excluding special tokens, cre-\nate a random permutation σ over the vocabulary.\nFor each sequence indataset, deterministically map\neach token by σ. This ablates lexical patterns, re-\ntaining syntactic structure. (3) dataset-random:\nrandomly replace each token in dataset with an-\nother (excluding special tokens). This ablates both\nsyntactic and lexical structure.\nNotably, several shallow linguistic descriptors\nare preserved with these transformations: dataset\nsize, sequence length and vocabulary size (1-3),\nvocabulary entropy (1,2), and token frequency (1).\nWe apply the transformations to OPTCorpus and\nwikitext, producing six additional datasets.\n3.1 Intrinsic Dimension\nGiven model Mand dataset X, we estimate the\nrepresentational ID of X under Mas follows (see\nalso fig. 1):\n12399\n“Sally went to the store”\nLayer \n0\nLayer \n1\nLayer \n2\nData\n“Sally went to the store”\npos. embed\nattention \nfeedforward \nattention \nfeedforward representations\n   \n……\nEmbeddings\nIntrinsic Dimension\nrepresentations\n   \n  Jim ate pizza\nFigure 1: ID estimation. Data (bottom left) are fed\ninto an LM with M blocks (left). Activations post-\nembedding/[feedforward, attention] blocks i= 1···M\nare extracted and aggregated across the sequence length\nto produce an N ×Ddimensional matrix of represen-\ntations Ri. Then, the ID of each Ri is estimated to\nproduce an ID “profile\" (right plot).\n1. Preprocess data. Let the context window\nlength of Mbe lM. We preprocess X by\nsplitting all x(i) with length l(i) > lMinto\nsequences with maximum length lM.\n2. Gather representations. Evaluate M(X),\ngathering intermediate representations after\ncontextualized embedding and after each\nattention+feed-forward block. In particular,\nrepresentations are extracted after the residual\nconnection and LayerNorm.\n3. Aggregate representations. Because input\nsequences x(i) are variable-length, we use the\nvector associated to the last token of each layer\nto represent it. Due to auto-regressive self-\nattention, the last token in the sequence is the\nonly one to incorporate information from all\nother tokens in the sequence. Moreover, in the\ncontext of causal language modeling, the last\ntoken representation is the one used for next-\ntoken prediction in the top layer of the model,\nso it is the one where all information relevant\nto the prediction task should be concentrated.\nWe leave testing alternative aggregation strate-\ngies, such as average pooling (cf. Valeriani\net al., 2023) to future work.\nAfter the aggregation step, we have dataset\nrepresentations\nR := {Rj}M\nj=1; Rj ∈RN×D,\nwhere D, the hidden dimension of the model,\nis the ambient (extrinsic) dimension of data\nrepresentations, and Mhas M layers.\n4. Estimate ID. Per layer j, compute the ID\ndj of Rj using ID estimator g : RN×D →\nZ+; Rj ↦→dj.\nWe test 12 different ID estimators g, grouping\nthem into categories based on technique: nine NN-\nbased (Facco et al., 2017; Farahmand et al., 2007;\nCarter et al., 2010; Amsaleg et al., 2019, 2018;\nHaro et al., 2008; Johnsson et al., 2015; Rozza et al.,\n2012; Ceruti et al., 2014), one projective (PCA),\none based on fine-grained clustering (Fisher Sep-\narability, Albergante et al., 2019), and one fractal-\nbased (Correlation Dimension, Grassberger and\nProcaccia, 1983). Further details on estimators\ncan be found in appendix A.1. We implement all\nestimators using the skdim Python package (Bac,\n2020).\n3.2 Information-Theoretic Compression\nInformation-theoretic compression is directly re-\nlated to the training objective of the model, which\nminimizes the average negative log-likelihood loss\nof next-token prediction over the training set.\nLearning minimizes coding length The average\nnegative log-likelihood (NLL) training objective of\ncausal LMs is given by\nmin\nθ\n1∑N\ni=1 l(x(i))\nN∑\ni=1\nl(x(i))∑\nj=1\n−log pM(x(i)\nj |x(i)\n<j; θ),\n(1)\nthat is, to minimize the empirical negative log-\nlikelihood of the next token given its context with\nrespect to model parameters θ. This is analytically\nequivalent to minimizing the average number of\nbits to encode the jth token under pM.\nWe are interested in quantifying information-\ntheoretic compression at the sequence level. We do\nso by using perplexity, a common metric in NLP.\nPerplexity The perplexity (PPL) of a sequence\nx(i) is the exponentiated negative log-likelihood\nloss\nPPL(i) := 2\n1\nl(x(i))\n∑l(x(i))\nj=1 −log pM(x(i)\nj |x(i)\n<j;θ)\n. (2)\nWe compute the average PPL for each dataset X\nby performing forward passes through M. As PPL\nis monotonic in coding length, we use PPL as our\nmeasure of interest to proxy information-theoretic\ncompression, later relating this quantity to the rep-\nresentational ID of X.\n12400\n3.3 Ease-of-Adaptation\nLow ID of pre-trained LM parameters has been\nshown to predict ease-of-finetuning (Aghajanyan\net al., 2021). We complement this finding by cor-\nrelating the ID of data representations to an LM’s\nease-of-adaptation to that dataset.\nEase-of-adaptation to a downstream task de-\npends not only on the inputs X but also on task-\nspecific outputs. For instance, binary classification\ncan be less complex than causal language model-\ning given the same inputs X. As it is not always\nclear what is the best way to encode the outputs in\norder to measure the quantities of our interest, we\nfocus on adaptation under a causal language mod-\neling objective, which is the same as the model’s\npre-training objective. This entails little loss of gen-\nerality, as task adaptation is nowadays commonly\nframed as a language model adaptation problem.\nAdaptation procedure We perform finetuning\nfor each of OPT-350m, 1.3b, and 6.7b on the\ndatasets X in table 1 that are suited to causal lan-\nguage modeling, i.e., omitting [Super]GLUE.\nDue to resource constraints, and as we com-\npare between datasets and not models, we perform\nfull finetuning for OPT-350m and finetune using\nLoRA (Hu et al., 2022) for the larger sizes. We\nend finetuning at a maximum of 15 epochs or when\nvalidation loss converges. Loss is considered to\nhave converged as soon as it fails to decrease for\n3 evaluation steps, each 500 iterations apart. De-\ntailed hyperparameter settings may be found in\nappendix C.\nAdaptation metrics We quantify ease-of-\nadaptation with the following, where T is defined\nas the number of iterations until convergence:\n1. PPLT: final evaluation perplexity.\n2. Sample complexity S = 1\nT\n∑T\nt=1 PPLt,\nwhere PPLt is the evaluation PPL at eval-\nuation step t.\nFinally, we compute Spearman correlations be-\ntween these metrics, zero-shot perplexity (PPL0),\nand ID to assess whether the two types of compres-\nsion and ease-of-adaptation are linked.\n4 Results\nWe find that, similar to protein models and visual\nnetworks (Ansuini et al., 2019; Valeriani et al.,\n2023), the ID of linguistic data representations\nis significantly lower than their ambient dimen-\nsion: in our case, by roughly 2 orders of magni-\ntude. In particular, while the ambient dimension\nof representations in OPT-350m, OPT-1.3b, and\nOPT-6.7b are D= 1024, 2048, and 4096, respec-\ntively (Zhang et al., 2022), dataset representational\nID is d= O(10), see fig. 5.\nFor simplicity, in the main article we present re-\nsults on one representative NN-based estimator,\nthe Expected Simplex Skewness (ESS) method\nof Johnsson et al., 2015, as it is the only one\nto significantly correlate with all other estimators\n(α= 0.1) (see fig. 6).1 We present results on other\nestimators in appendix E, and we comment on other\n(non-NN-based) ID estimators in section 4.4. Also\nfor practicality, we report in the main article results\nobtained with one model, OPT-6.7b, only com-\nmenting on other models when relevant, and one\naggregated ID measure across layers: the max ID\nvalue, seen as a conservative upper bound on ID\nacross layers. This choice is supported by the ob-\nservation that, for most datasets, the ID profile over\nlayers is quite flat (appendix D). Results with other\naggregated measures are presented in appendix E.\nOur primary result is that, in the pre-trained LMs\ntested, information-theoretic compression (PPL)\npredicts geometric compression (ID), and low ID\npredicts ease-of-adaptation. We then take a closer\nlook at which linguistic attributes of a dataset pre-\ndict its ID, finding that not only several shallow lin-\nguistic descriptors but also grammatical and lexical\nstructure enable geometric compression. Finally,\nwe find that, qualitatively, ID tends to be stable\nacross model sizes and types , and comment on the\ndifferences between ID estimators.\n4.1 ID tracks information-theoretic\ncompression\nInformation-theoretic and geometric description\nlength of data under OPT are Spearman-correlated.\nAs shown for OPT-6.7b in fig. 2a, data PPL pre-\ndicts ID, ρ = 0.51 (p = 0.01). The significant\npositive correlation between PPL and ID, more-\nover, holds for all model sizes tested: ρ = 0.66,\np <0.01 for OPT-350m and ρ = 0.49, p = 0.01\nfor OPT-1.3b, the correlation being most salient\nfor the smallest model (see figs. E.3a and E.3d).\nWe hypothesize that model optimization in higher\ndimensions permits discovery of better representa-\n1We also experimented with ensembling ID metrics, but\nfound the various ensembling methods hard to justify as NN-\nbased estimators are very over-represented in our panel.\n12401\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n50\n100\n150\n200\n250\n300ESS Max ID\ncola\nmnli\nmrpc\nqnliqqprte sst2\nstsbboolq\nmultirc\nwic\nimdb wikitext\ncode\npenntreebank\nwikitext_fr\ntweets\npiledailymail\nopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.51 (p=0.01)\nopt-6.7b\n(a) Intrinsic dimension (ESS max ID)\nvs. data perplexity (log-PPL).\n50 100 150 200 250 300\nESS Max ID\n2\n4\n6\n8\n10\n12\n14\n16Finetune Sample Complexity (log)\nimdb bookcorpus\nwikitext_fr\npenntreebankwikitext\ntweets\npilecodedailymailopenwebtext\noptcorpus_permuted\noptcorpus_swapped\noptcorpus_random\ncode_swapped\ncode_random\nwikitext_permuted_tokens\nwikitext_swapped_tokens\nwikitext_random_tokens\n=0.72 (p=0.00)\nopt-6.7b\n(b) Sample complexity vs. Intrinsic dimen-\nsion (ESS max ID)\n50 100 150 200 250 300\nESS Max ID\n2\n4\n6\n8\n10\n12\n14\n16Finetune Eval. PPL (log)\nimdbbookcorpuswikitext_frpenntreebankwikitext\ntweets\npile\ncode\ndailymailopenwebtext\noptcorpus_permutedoptcorpus_swapped\noptcorpus_random\ncode_swapped\ncode_random\nwikitext_permuted_tokens\nwikitext_swapped_tokens\nwikitext_random_tokens\n=0.73 (p=0.00)\nopt-6.7b\n(c) Final log-PPL vs. Intrinsic dimension\n(ESS max ID).\nFigure 2: Intrinsic dimension is significantly positively correlated to (a) dataset PPL (p= 0.01); and also predicts\nease-of-adaptation metrics (b) sample complexity (p< 0.01) and (c) final evaluation PPL (p< 0.01). Both ID and\ndataset perplexity in (a) are measured before finetuning; in (b) and (c), ID is measured before finetuning and the\ny-axes after finetuning.\ntions in d-dimensional intrinsic space, as evidenced\nby the correlation between performance and model\nsize in LM scaling laws. Then, on a given dataset,\nlarger model sizes may encounter an ID floor effect,\nthus weakening the correlation between PPL and\nID compared to smaller sizes.\n4.2 Compression is linked to\nEase-of-Adaptation\nAs evidenced by the positive trends in figs. 2b\nand 2c, ID predicts sample complexity (ρ= 0.72,\np <0.01) as well as final PPL after convergence\n(ρ = 0.73, p <0.01). These trends, moreover,\nare robust to model size: ID predicts sample com-\nplexity at ρ = 0.61, p = 0.01 for OPT-1.3b and\nρ = 0.81, p = 0.01 for OPT-350m (figs. E.3b\nand E.3e); and ID predicts final PPL after fine-\ntuning at ρ = 0.65, p <0.01 for OPT-1.3b and\nρ = 0.81, p = 0.01 for OPT-350m (figs. E.3c\nand E.3f).\nResults indicate that data which are more com-\npressed zero-shot under the LM are easier to adapt\nto. Moreover, they corroborate findings in Agha-\njanyan et al., 2021, in which low parameter ID\nin pre-trained LMs predicts rapid adaptation. We\nhypothesize that this is because intrinsic data rank\nbottlenecks intrinsic parameter rank and vice-versa\n(see Rozza et al., 2012 for discussion).\n4.3 Linguistic Correlates to Compression\nIn pre-trained OPT, geometric compression can be\nexplained partially by data perplexity. But, taking\na closer look, ID also correlates with interpretable\nlinguistic descriptors such as syntactic structure or\ntoken entropy.\nV HV ˜L N tok\nMax ID 0.50\np=0.01\n0.44\np=0.02\n0.15 0.15\nTable 2: For OPT-6.7b, Spearman correlations ρ be-\ntween ESS max ID and dataset vocabulary size (V), vo-\ncabulary entropy (HV ), average sequence length ˜L, and\nsize in tokensNtok. Significant correlations at α= 0.05\nare displayed with the corresponding p-value.\nLinguistic Structure Permits Compression Ab-\nlating linguistic structure increases ID and per-\nplexity of data representations. This may be seen\nin fig. 2a for OPT-6.7b, where both the ID and\nperplexity of the permuted, swapped, and random\nvariants increase with respect to the baseline for the\nwikitext dataset. Moreover, this relationship holds\nfor all model sizes, see figs. E.3a and E.3d, and\ngenerally for other ID estimators, see appendix E.\nThis indicates that learned grammatical and lexical\nstructure permits compression in LMs; furthermore,\nthe small increase in ID from wikitext to wikitext-\npermuted compared to other variants suggests that\nbag-of-words lexical semantics, rather than com-\nplex syntactic structure, accounts for much of geo-\nmetric compression. Interestingly, this ties in with\ngeneral estimates of the relative information load\nof syntax and lexical semantics in human language\n(Mollica and Piantadosi, 2019).\nSome Shallow Linguistic Descriptors Predict ID\nbut Are Uncorrelated to Perplexity Beyond lin-\nguistic structure, two related shallow linguistic de-\nscriptors also predict ID: vocabulary size V (num-\nber of unique tokens in the dataset) and vocabulary\nentropy HV (Shannon entropy of token frequency\n12402\n0.0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90Intrinsic Dimension\nwikitext\n0.0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 sst2\nmodel\nopt-350m\nopt-1.3b\nopt-6.7b\n0.0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 tweets\nFigure 3: Example evolution of ID (ESS) over layers for three tasks (left to right): wikitext, sst2, and tweets. In\ngeneral, ID profiles can be dissimilar for different datasets under the same model, and ID profiles for OPT-350m,\n1.3b, and 6.7b appear correlated, with larger extrinsic dimension lending itself to slightly (not proportionately) larger\nintrinsic dimension.\nPPL\nPPL\nInput.\nLength\nInput.\nLength\nVocab\nSize\nVocab\nSize\nVocab\nEntropy\nVocab\nEntropy\nSize\nSize\n1.00 -0.46\n-0.46 1.00 0.72 0.43\n0.72 1.00 0.74\n0.43 0.74 1.00\n1.00\nSpearman Corr. (opt-6.7b, =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 4: Spearman correlations between data descrip-\ntors: information-theoretic descriptor PPL (top/left),\nand shallow linguistic descriptors (bottom/right); only\ncorrelations significant at α = 0.1 shown. Shallow\nmetrics highly correlate to each other but not to PPL.\nin the dataset), see Table 2. In contrast, average\nsequence length (in tokens) ˜Land dataset size (in\ntokens) Ntok do not predict ID.2\nThat descriptors such as vocabulary size and en-\ntropy predict ID is intuitive: with, e.g., a larger\nvocabulary, more information needs to be encoded.\nFurthermore, as expected, these descriptors are cor-\nrelated to one another. However, the descriptors are\nnot positively correlated to PPL (fig. 4), suggesting\nthat the relationship between information-theoretic\nand geometric compression is not explained by\nshallow dataset properties.\nThe relation between shallow linguistic descrip-\ntors and ID generally holds across model size and\nID estimators; see appendix E for further discus-\nsion, extending to other layer-aggregate measures\nof ID.\n2It is crucial that the dataset be big enough to prevent spu-\nrious correlations between size and ID due to scaling effects\n(see appendix A.2).\n0\n 50\n 100\n 150\n 200\n 250\n 300\nESS Max ID\n350m\n1.3b\n6.7b\nModel Size\nFigure 5: Distributions of max ID (aggregated over lay-\ners of the Transformer) across tasks, computed using the\nESS estimator. Despite the model extrinsic dimension\ndoubling (ED = 1024, 2048, 4096 from top to bottom),\nthe ID remains fairly stable. Apart from a few outliers,\nthe max ID over layers is less than 100, that is, all IDs\ncomputed over all layers are generally O(10).\n4.4 Intrinsic Dimension of Representations\nWe have presented ID as it relates to information-\ntheoretic compression and ease-of-adaptation; now\nwe address the question of geometric compression\nitself. First, we show that the various models com-\npress data into similar ranges of ID regardless of\nextrinsic hidden dimension, lending weight to the\nmanifold hypothesis for linguistic data. We fur-\nther report on how ID estimation with different\nmethods produces a complicated picture, and we\ncomment on results evaluated on the full battery of\n12 estimators.\nDifferent model size, similar ID We find that\nall tested models compress data to a similar range\nof ID, around O(10). Although the model hidden\ndimension doubles from OPT-350m to 1.3b to 6.7b,\nthe range of data representational ID does not sig-\n12403\nPCA\nFisherS\nCorrInt\nTLE\nMLE\nESS\nT woNN\nMOM\nMADA\nPCA\nFisherS\nCorrInt\nTLE\nMLE\nESS\nT woNN\nMOM\nMADA\n1.00 0.94 -0.12 -0.09 0.20 -0.28 0.23 0.05\n0.94 1.00 -0.12 -0.18 0.19 -0.27 0.12\n1.00 0.50 0.14 0.56 0.18 0.09\n-0.12 -0.12 0.50 1.00 0.69 0.72 0.59 0.49 0.42\n-0.09 -0.18 0.14 0.69 1.00 0.33 0.38 0.83 0.86\n0.20 0.19 0.56 0.72 0.33 1.00 0.45 0.27 0.12\n-0.28 -0.27 0.18 0.59 0.38 0.45 1.00\n0.23 0.12 0.09 0.49 0.83 0.27 1.00 0.93\n0.05 0.42 0.86 0.12 0.93 1.00\nLayer-wise Spearman Corr. of ID Methods (opt-6.7b, = 0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 6: Spearman correlations between ID metrics: the bottom-right block of correlated metrics correspond to\nNN-based methods, while the top-left are PCA, Fisher Separability, and Correlation Dimension, respectively a\nlinear projective, fine-grained clustering, and fractal method.\n0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth (opt-6.7b)\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05Relative Intrinsic Dimension\nTWEETS Representational ID\nESS\nT woNN\nFisherS\nPCA\nCorrInt\nMLE\nTLE\nMOM\nMADA\nFigure 7: Different ID methods’ relative ID estimates\n(= ID / ED) over layers in OPT-6.7b for the Tweets\ndataset. While some groups of methods are correlated,\nthey produce different ID profiles for the same data.\nnificantly change, see fig. 5.\nMoreover, the evolution of ID across the layers\nof different model sizes follows similar trajecto-\nries: qualitatively, for all tasks, the OPT models’\nID profiles follow similar global patterns (e.g., sim-\nilar number of peaks), with larger models having\nslightly larger intrinsic dimension, see examples\nin fig. 3. For extended discussion of ID evolution\nacross layers, see appendix D.\nSimilar ID values and evolution across different\nmodels echo results in the visual domain (Ansuini\net al., 2019) and evidence the manifold hypothe-\nsis for linguistic data. Together with past work,\nour results suggest that different-sized (but similar-\nperforming) models which are trained on the same\ndata and objective can independently recover the la-\ntent dimension of data and exhibit similar patterns\nof processing.\nID estimators aren’t equal; some are useful\nThough ID estimators based on similar analytical\nmethods are correlated (fig. 6), different ID estima-\ntors can produce different ID profiles for the same\ndataset (e.g., fig. 7). This can stem from a number\nof factors pertaining to assumptions and analytical\nmethod. For instance, we find the NN-based meth-\nods to be most predictive of data perplexity and\nease-of-adaptation, and PCA and Fisher Separabil-\nity to be least predictive. This may be because (1)\nPCA assumes that the underlying data manifold is\nlinear, which may lead to poor ID estimation (con-\nsider a 1D line embedded in 2D space; PCA will\nestimate an ID of 2 as there are two principal direc-\ntions of variance); (2) Fisher Separability system-\natically underestimates ID for non-uniformly dis-\ntributed data (Albergante et al., 2019), and Trans-\nformer representations are highly anisotropic (Cai\net al., 2021). Lastly, among the 12 estimators tested,\nwe could not produce sensible results for three of\n12404\nthem (Rozza et al., 2012; Ceruti et al., 2014; Carter\net al., 2010), see appendix A.3 for discussion.\nWhile we cannot claim that any single estimator\nproduces the “true” ID, it appears that, for purposes\nof ID estimation of linguistic datasets as encoded in\nLMs, NN-based methods are the most useful ones,\nbeing reliable predictors of information-theoretic\ncompression and ease-of-adaptation (appendix E).\nMore generally, the differing results obtained with\nvarious ID estimators reveal a need to validate them\nagainst linguistic data, which may violate under-\nlying assumptions of common estimators, such as\nthe global isotropy assumption in Fisher Separa-\nbility (Albergante et al., 2019). While there in-\ndeed exist ID estimation benchmarks for synthetic\nmanifolds and image data, and while NN-based\nestimators outperform linear ones in these bench-\nmarks (Campadelli et al., 2015), a benchmark has\nnot yet been developed for linguistic data, to our\nknowledge.\n5 Discussion\nWe have quantified geometric compression in neu-\nral language models using the ID of representations,\nwhere ID tracks Shannon information-theoretic\ncoding length. This bridges two notions of descrip-\ntion length in pre-trained neural LMs by showing\nthey are significantly positively correlated. Our re-\nsult has also practical implications, suggesting that\nID and perplexity predict how easy it is to finetune\na model to a task (similarly to what observed in the\ncontext of zero-shot prompting by Gonen et al.,\n2022). More speculatively, the relation between ID\nand task adaptation may inform future modeling\nwork that actively encourages data compression at\ntraining time, to indirectly inject fast-adaptation\ncapabilities into a model.\nID estimators are not equal: we focus on NN-\nbased methods because they explain useful proper-\nties of the data and ease-of-finetuning. Our work\nis a first attempt to evaluate a wide range of ID\nestimators on natural language representations, and\nreveals the need for a further principled study of\nID estimation of linguistic data.\nThat nonlinear ID estimators predict\ninformation-theoretic compression and ease-\nof-adaptation over linear ones highlights a need\nto go beyond PCA in analyzing compression of\nspecific linguistic phenomena. Our experiments\non wikitext and variants also demonstrate a need\nfor further experiments on, e.g., idiomaticity, or\nspecific linguistic constructions (cf. Hernandez\nand Andreas, 2021 for analysis using PCA).\nWhile our work investigates the relationship be-\ntween ease-of-adaptation and zero-shot compres-\nsion, a logical next step is to investigate how fine-\ntuning dynamically affects data compression under\nthe model. We hypothesize that the result may de-\npend on whether the dataset used for finetuning is\nmemorized by the model during training.\nFinally, while LMs are trained to reproduce the\ndistribution of human language, it is yet unclear\nwhether analyzing the linguistic representations\nof the former allows us to make statements about\nthe dimensionality of the latter. Then, an open\nquestion remains: what is the “true” dimensionality\nof natural language, and to what extent do LMs\nrecover it?\n12405\nLimitations\n• While we confirmed that modern LMs do com-\npress language data, and that this compression\nis correlated with ease-of-learning, we only\nprovided a limited characterization of the rela-\ntion between LM compression and linguistic\nproperties of the input, such as lexical infor-\nmation and syntactic structure.\n• Due to both access restriction and computa-\ntional limitations, we cannot replicate our in-\nvestigations on huge language models such as\nChatGPT.\n• A related question is to what extent the cor-\nrelation we report between compression and\nease of learning would hold (or even how it\ncould be meaningfully formulated) in the con-\ntext of prompt-based zero-shot task adaptation\nas afforded by huge LMs.\n• More generally, it remains to be explored how\na number of modeling choices, such as non-\ncausal predictive objectives or instruction tun-\ning, would affect our generalizations.\nEthics Statement\nThis paper does not introduce new models or\ndatasets, and it presents an abstract analysis of\nlanguage model data compression that, we think,\nshould not raise ethical concerns. We believe on\nthe other hand that our focus on improving the\nunderstanding of how language models process\ninformation can be generally beneficial, in an AI\nlandscape in which powerful language models are\ndeployed with little understanding of the mechan-\nics by which they work, and, consequently, little\nability to control their behavior.\nAcknowledgements\nWe thank Alessandro Laio for generous guidance\non intrinsic dimensionality estimation. Jacob An-\ndreas provided very helpful feedback on the project.\nWe also thank the members of the UPF COLT\nlab, especially Gemma Boleda, Roberto Dessì and\nLucas Weber for early feedback, the members of\nthe Barcelona Apple Machine Learning Research\ngroup and the participants in the EviL seminar for\nhelpful feedback and suggestions. Our work was\nfunded by the European Research Council (ERC)\nunder the European Union’s Horizon 2020 research\nand innovation programme (grant agreement No.\n101019291). This paper reflects the authors’ view\nonly, and the ERC is not responsible for any use\nthat may be made of the information it contains.\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettle-\nmoyer. 2021. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 7319–7328,\nOnline. Association for Computational Linguistics.\nLuca Albergante, Jonathan Bac, and Andrei Zinovyev.\n2019. Estimating the effective dimension of large\nbiological datasets using fisher separability analysis.\nIn 2019 International Joint Conference on Neural\nNetworks (IJCNN), page 1–8.\nLaurent Amsaleg, Oussama Chelly, Teddy Furon,\nStéphane Girard, Michael E. Houle, Ken-ichi\nKawarabayashi, and Michael Nett. 2018. Extreme-\nvalue-theoretic estimation of local intrinsic dimen-\nsionality. Data Mining and Knowledge Discovery ,\n32(6):1768–1805.\nLaurent Amsaleg, Oussama Chelly, Michael E. Houle,\nKen-ichi Kawarabayashi, Miloš Radovanovi ´c, and\nWeeris Treeratanajaru. 2019. Intrinsic Dimensional-\nity Estimation within Tight Localities, Proceedings,\npage 181–189. Society for Industrial and Applied\nMathematics.\nAlessio Ansuini, Alessandro Laio, Jakob H Macke,\nand Davide Zoccolan. 2019. Intrinsic dimension\nof data representations in deep neural networks. In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and\nYi Zhang. 2018. Stronger generalization bounds\nfor deep nets via a compression approach. In Pro-\nceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 254–263. PMLR.\nJonathan Bac. 2020. Intrinsic dimension estimation in\npython — scikit-dimension 0.3.2 documentation.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nFrancesco Barbieri, Jose Camacho-Collados, Luis\nEspinosa-Anke, and Leonardo Neves. 2020. TweetE-\nval:Unified Benchmark and Comparative Evaluation\nfor Tweet Classification. In Proceedings of Findings\nof EMNLP.\nStas Bekman. 2022. Openwebtext-10k dataset.\n12406\nChristopher M. Bishop. 2007. Pattern Recognition and\nMachine Learning (Information Science and Statis-\ntics), 1 edition. Springer.\nLéonard Blier and Yann Ollivier. 2018. The descrip-\ntion length of deep learning models. In Advances in\nNeural Information Processing Systems, volume 31.\nCurran Associates, Inc.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embedding\nspace: Clusters and manifolds. In International Con-\nference on Learning Representations.\nP. Campadelli, E. Casiraghi, C. Ceruti, and A. Rozza.\n2015. Intrinsic dimension estimation: Relevant tech-\nniques and a benchmark framework. Mathematical\nProblems in Engineering, 2015:e759567.\nKevin M. Carter, Raviv Raich, and Alfred O. Hero III.\n2010. On local intrinsic dimension estimation and its\napplications. IEEE Transactions on Signal Process-\ning, 58(2):650–663.\nClaudio Ceruti, Simone Bassis, Alessandro Rozza,\nGabriele Lombardi, Elena Casiraghi, and Paola Cam-\npadelli. 2014. Danco: An intrinsic dimensionality\nestimator exploiting angle and norm concentration.\nPattern Recognition, 47(8):2569–2581.\nIvan Chelombiev, Conor Houghton, and Cian\nO’Donnell. 2019. Adaptive estimators show infor-\nmation compression in deep neural networks. In\nInternational Conference on Learning Representa-\ntions.\nNoam Chomsky. 1986. Knowledge of Language: Its\nNature, Origin, and Use. Praeger, Wesport, CT.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav\nPetrov, and Noah Fiedel. 2022. Palm: Scaling lan-\nguage modeling with pathways. (arXiv:2204.02311).\nArXiv:2204.02311 [cs].\nUri Cohen, SueYeon Chung, Daniel D. Lee, and Haim\nSompolinsky. 2020. Separability and geometry of\nobject manifolds in deep neural networks. Nature\nCommunications, 11(1):746.\nGrégoire Delétang, Anian Ruoss, Paul-Ambroise\nDuquenne, Elliot Catt, Tim Genewein, Christo-\npher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,\nMatthew Aitchison, Laurent Orseau, Marcus Hut-\nter, and Joel Veness. 2023. Language modeling is\ncompression.\nFrancesco Denti, Diego Doimo, Alessandro Laio, and\nAntonietta Mira. 2022. The generalized ratios in-\ntrinsic dimension estimator. Scientific Reports ,\n12(11):20005.\nVittorio Erba, Marco Gherardi, and Pietro Rotondo.\n2019. Intrinsic dimension estimation for locally un-\ndersampled data. Scientific Reports, 9(1):17133.\nElena Facco, Maria d’Errico, Alex Rodriguez, and\nAlessandro Laio. 2017. Estimating the intrin-\nsic dimension of datasets by a minimal neighbor-\nhood information. Scientific Reports, 7(1):12140.\nArXiv:1803.06992 [cs, stat].\nAmir massoud Farahmand, Csaba Szepesvári, and Jean-\nYves Audibert. 2007. Manifold-adaptive dimension\nestimation. In Proceedings of the 24th international\nconference on Machine learning, page 265–272, Cor-\nvalis Oregon USA. ACM.\nK. Fukunaga and D.R. Olsen. 1971. An algorithm for\nfinding intrinsic dimensionality of data. IEEE Trans-\nactions on Computers, C-20(2):176–183.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe Pile: An 800GB dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nBernhard C. Geiger. 2022. On information plane anal-\nyses of neural network classifiers – a review. IEEE\nTransactions on Neural Networks and Learning Sys-\ntems, 33(12):7039–7051. ArXiv:2003.09671 [cs,\nmath, stat].\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Ste-\nfanie Tellex. 2019. Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus.\n12407\nZiv Goldfeld, Ewout Van Den Berg, Kristjan Gree-\nnewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury,\nand Yury Polyanskiy. 2019. Estimating information\nflow in deep neural networks. In Proceedings of the\n36th International Conference on Machine Learn-\ning, volume 97 of Proceedings of Machine Learning\nResearch, pages 2299–2308. PMLR.\nHila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and\nLuke Zettlemoyer. 2022. Demystifying prompts in\nlanguage models via perplexity estimation. https:\n//arxiv.org/abs/2212.04037.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning. MIT Press. http://www.\ndeeplearningbook.org.\nPeter Grassberger and Itamar Procaccia. 1983. Measur-\ning the strangeness of strange attractors.\nGloria Haro, Gregory Randall, and Guillermo Sapiro.\n2008. Translated poisson mixture model for stratifi-\ncation learning. International Journal of Computer\nVision, 80(3):358–374.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS, pages 1693–1701.\nEvan Hernandez and Jacob Andreas. 2021. The low-\ndimensional linear geometry of contextualized word\nrepresentations. In Proceedings of the 25th Confer-\nence on Computational Natural Language Learning,\npages 82–93, Online. Association for Computational\nLinguistics.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, page 1643–1652, Brussels, Bel-\ngium. Association for Computational Linguistics.\nKerstin Johnsson, Charlotte Soneson, and Magnus\nFontes. 2015. Low bias local intrinsic dimension\nestimation from expected simplex skewness. IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 37(1):196–202.\nIan Jolliffe. 1986. Principal Component Analysis .\nSpringer.\nElizaveta Levina and Peter Bickel. 2004. Maximum\nlikelihood estimation of intrinsic dimension. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 17. MIT Press.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason\nYosinski. 2018. Measuring the intrinsic dimension\nof objective landscapes. In International Conference\non Learning Representations.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nJonathan Mamou, Hang Le, Miguel Del Rio, Cory\nStephenson, Hanlin Tang, Yoon Kim, and Sueyeon\nChung. 2020. Emergence of separable manifolds in\ndeep language representations. In Proceedings of the\n37th International Conference on Machine Learning,\npage 6713–6723. PMLR.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated cor-\npus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313–330.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nFrancis Mollica and Steven Piantadosi. 2019. Humans\nstore about 1.5 megabytes of information during\nlanguage acquisition. Royal Society Open Science,\n6(3):181393.\nNeel Nanda. 2022. Pile-10k dataset.\nMorteza Noshad, Yu Zeng, and Alfred O. Hero. 2019.\nScalable mutual information estimation using depen-\ndence graphs. In ICASSP 2019 - 2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), page 2962–2966.\n12408\nOpenAI. 2023. Gpt-4 technical report.\n(arXiv:2303.08774). ArXiv:2303.08774 [cs].\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nRissanen data analysis: Examining dataset character-\nistics via description length. In Proceedings of the\n38th International Conference on Machine Learning,\npage 8500–8513. PMLR.\nPhil Pope, Chen Zhu, Ahmed Abdelkader, Micah Gold-\nblum, and Tom Goldstein. 2021. The intrinsic di-\nmension of images and its impact on learning. In\nInternational Conference on Learning Representa-\ntions.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nStefano Recanatesi, Matthew Farrell, Madhu Advani,\nTimothy Moore, Guillaume Lajoie, and Eric Shea-\nBrown. 2019. Dimensionality compression and ex-\npansion in deep neural networks. ArXiv:1906.00443\n[cs, stat].\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nA. Rozza, G. Lombardi, C. Ceruti, E. Casiraghi, and\nP. Campadelli. 2012. Novel high intrinsic dimension-\nality estimators. Machine Learning, 89(1):37–65.\nAndrew Michael Saxe, Yamini Bansal, Joel Dapello,\nMadhu Advani, Artemy Kolchinsky, Brendan Daniel\nTracey, and David Daniel Cox. 2018. On the infor-\nmation bottleneck theory of deep learning. In Inter-\nnational Conference on Learning Representations.\nC E Shannon. 1948. A mathematical theory of commu-\nnication.\nRavid Shwartz-Ziv and Naftali Tishby. 2017. Opening\nthe black box of deep neural networks via informa-\ntion.\nAntoine Simoulin and Benoit Crabbé. 2021. Un modèle\nTransformer Génératif Pré-entrainé pour le ______\nfrançais. In Traitement Automatique des Langues\nNaturelles, pages 246–255, Lille, France. ATALA.\nAhmed Soliman. 2022. Codexglue-concode dataset.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-\nmekci, Bill Yuchen Lin, Blake Howald, Cameron\nDiao, Cameron Dour, Catherine Stinson, Cedrick Ar-\ngueta, César Ferri Ramírez, Chandan Singh, Charles\nRathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu,\nChris Callison-Burch, Chris Waites, Christian V oigt,\nChristopher D. Manning, Christopher Potts, Cindy\nRamirez, Clara E. Rivera, Clemencia Siro, Colin Raf-\nfel, Courtney Ashcraft, Cristina Garbacea, Damien\nSileo, Dan Garrette, Dan Hendrycks, Dan Kilman,\nDan Roth, Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam,\nDieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dim-\nitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekate-\nrina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Martínez-Plumed, Francesca\nHappé, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nLópez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Ko-\nco´n, Jana Thompson, Jared Kaplan, Jarema Radom,\nJascha Sohl-Dickstein, Jason Phang, Jason Wei, Ja-\nson Yosinski, Jekaterina Novikova, Jelle Bosscher,\nJennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse En-\ngel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jil-\nlian Tang, Joan Waweru, John Burden, John Miller,\nJohn U. Balis, Jonathan Berant, Jörg Frohberg, Jos\nRozen, Jose Hernandez-Orallo, Joseph Boudeman,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\n12409\nHe, Luis Oliveros Colón, Luke Metz, Lütfi Kerem\n¸ Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramírez Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, Mukund Varma T, Nanyun Peng, Nathan\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas Roberts, Nick Doiron, Nikita\nNangia, Niklas Deckers, Niklas Muennighoff, Ni-\ntish Shirish Keskar, Niveditha S. Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pascale\nFung, Paul Pu Liang, Paul Vicol, Pegah Alipoormo-\nlabashi, Peiyuan Liao, Percy Liang, Peter Chang,\nPeter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel\nHabacker, Ramón Risco Delgado, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Rohan\nSikand, Roman Novak, Roman Sitelew, Ronan Le-\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Rus-\nlan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Sto-\nvall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\nMohammad, Sajant Anand, Sam Dillavou, Sam\nShleifer, Sam Wiseman, Samuel Gruetter, Samuel R.\nBowman, Samuel S. Schoenholz, Sanghyun Han,\nSanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian,\nSayan Ghosh, Sean Casey, Sebastian Bischoff, Sebas-\ntian Gehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-\nantadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Théo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Timothy Telleen-Lawton,\nTitus Tunduny, Tobias Gerstenberg, Trenton Chang,\nTrishala Neeraj, Tushar Khot, Tyler Shultz, Uri Sha-\nham, Vedant Misra, Vera Demberg, Victoria Nyamai,\nVikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fe-\ndus, William Saunders, William Zhang, Wout V ossen,\nXiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu,\nXudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz,\nYangqiu Song, Yasaman Bahri, Yejin Choi, Yichi\nYang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid,\nZhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui\nWang, and Ziyi Wu. 2022. Beyond the imita-\ntion game: Quantifying and extrapolating the ca-\npabilities of language models. (arXiv:2206.04615).\nArXiv:2206.04615 [cs, stat].\nNaftali Tishby and Noga Zaslavsky. 2015. Deep learn-\ning and the information bottleneck principle. In 2015\nIEEE Information Theory Workshop (ITW), page 1–5.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schel-\nten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\nramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Au-\nrelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. (arXiv:2307.09288).\nArXiv:2307.09288 [cs].\nLucrezia Valeriani, Diego Doimo, Francesca Cu-\nturello, Alessandro Laio, Alessio Ansuini, and Al-\nberto Cazzaniga. 2023. The geometry of hid-\nden representations of large transformer models.\n(arXiv:2302.00294). ArXiv:2302.00294 [cs, stat].\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), page\n183–196, Online. Association for Computational Lin-\nguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019a. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\n12410\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of ICLR, New Orleans, LA. Published on-\nline: https://openreview.net/group?id=ICLR.\ncc/2019/conference.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOpt: Open pre-trained transformer language mod-\nels. (arXiv:2205.01068). ArXiv:2205.01068 [cs].\nYu Zhang, Peter Ti ˇno, Aleš Leonardis, and Ke Tang.\n2021. A survey on neural network interpretability.\nIEEE Transactions on Emerging Topics in Computa-\ntional Intelligence, 5(5):726–742.\nZhong Zhang, Bang Liu, and Junming Shao. 2023. Fine-\ntuning happens in tiny subspaces: Exploring intrinsic\ntask-specific subspaces of pre-trained language mod-\nels. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1701–1713, Toronto, Canada.\nAssociation for Computational Linguistics.\nY . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urta-\nsun, A. Torralba, and S. Fidler. 2015. Aligning books\nand movies: Towards story-like visual explanations\nby watching movies and reading books. In 2015\nIEEE International Conference on Computer Vision\n(ICCV), pages 19–27, Los Alamitos, CA, USA. IEEE\nComputer Society.\n12411\nA ID Estimation\nIn this appendix, we detail the tested ID estimation\nmethods (appendix A.1) and validate their conver-\ngence on a sample of 50,000 datapoints of the\nbookcorpus dataset in appendix A.2.\nA.1 ID Estimator Details\nFor each ID estimator tested, we provide a brief\ndescription as well as the hyperparameters used\n(which we will denote k) in relation to the skdim\npackage. As ID estimation theory is not our focus,\nwe refer the reader to Campadelli et al., 2015 for a\ncomprehensive review of methods.\nNote that ID estimators can be classified as\nglobal or local. Global ID estimation assumes\nthe entire dataset lies on a single geometric object,\nand estimates its dimensionality from the entire\ndataset at once. In contrast, local ID estimation\ncomputes point-wise ID of data neighborhoods,\nthen aggregates the per-neighborhood estimates to\narrive at a global estimate. This aggregation of\nlocal estimates can correct for negative biases of\nglobal ID estimators in high d, where this bias is\nsymptomatic of concentration of measure in high\ndimensions (Carter et al., 2010). While Mamou\net al., 2020 show that linguistic representations lie\non separable manifolds, we assumed for simplicity\nthat our data lie on one geometric object and use\neither global or local methods to estimate its ID,\nleaving per-manifold ID estimation to future work.\nEstimator Method Global/Local\nPCA Projective Global\nFisherS Fine-grained\nclustering\nGlobal\nCorrInt Fractal Global\nTwoNN NN-based Global\nKNN NN-based Global\nMiND ML NN-based Global\nDANCo NN-based Global\nTLE NN-based Local\nMLE NN-based Local\nMOM NN-based Local\nMADA NN-based Local\nESS NN-based Local\nTable A.1: Taxonomy of 12 ID estimators tested (3 of\nwhich unsuccessfully: KNN, MiND ML, and DANCo),\nby method and by whether global or local. If local, point-\nwise ID estimates are computed and mean-aggregated.\nA.1.1 Global Estimators\nPCA Projective method that assumes a linear\nmanifold. The ID d is the number of principal\nvalues λ >λmax\nk , where hyperparameter k = 20\n(Fukunaga and Olsen, 1971).\nFisher Separability (FisherS) Fine-grained clus-\ntering estimator that relies on “clumping\" phenom-\nena in high dimensions. To estimate d, (1) the\ndata are transformed using PCA (hyperparameter\nk = 10) and projected onto the unit sphere; (2)\nthe proportion of linearly separable datapoints is\ncompared to that of a theoretical equidistribution\non a d-sphere (Albergante et al., 2019).\nCorrelation Dimension (CorrInt) The volume\nsof a d-dimensional radius-rhypersphere grows\nas s∼rd. For two values of r: r1 and r2, estimate\nsby counting the nearest neighbors in a radius- r\nsphere around each point. Then, fit d(Grassberger\nand Procaccia, 1983).\nWe choose r1 corresponding to hyperparameter\nk1 = 10nearest neighbors and r2 corresponding to\nhyperparameter k2 = 20nearest neighbors.\nTwoNN A nearest-neighbors (NN)-based estima-\ntor that assumes local uniform data density (up to\nthe second neighbor). Regresses log(1−F(µ))\nlog µ = d,\nwhere F is the cumulative density function and\nµi := r2\nr1\nis the 2NNs distance ratio for each\nxi ∈X. Following Facco et al., 2017, we dis-\ncard hyperparameter k = 0.1 fraction of largest\nµi from the regression, a heuristic that improves\nestimation accuracy.\nA.1.2 Local Estimators\nESS For each datapoint, take hyperparameter\nk = 10 nearest neighbors. This neighborhood\nis assumed to be approximable by a uniform dis-\ntribution of datapoints on a tangent plane to the\nmanifold. Expected Simplex Skewness, or ESS\n(called ESSa in Johnsson et al., 2015), constructs a\nsimplex with one vertex at the centroid of the local\ndataset and other vertices at the other datapoints.\nThen, the expected simplex skewness measure is\ndefined as the volume of the simplex divided by the\nvolume if all edges to the centroid were orthogonal.\nThe ESS is computed over all such local datasets\nthen compared to the theoretical expected value in\nd-dimensions in order to estimate d.\nTLE Tight Local ID Estimation consists of com-\nputing a modified Correlation Dimension on small\n12412\nneighborhoods of hyperparameter k = 20neigh-\nbors around query datapoints, then fitting dusing\nmaximum-likelihood estimation (Amsaleg et al.,\n2019).\nMLE We use the Maximum Likelihood Estima-\ntor of Haro et al., 2008, which extends Levina\nand Bickel, 2004 to handle noise. For simplic-\nity, we assume one underlying data manifold (the\nmethod extends to multiple underlying manifolds).\nThen, MLE models the number of points falling\ninto a small neighborhood around query points by\na (Translated) Poisson process. The maximum-\nlikelihood estimator for the Poisson λparameter\nis fit given data, from which one can solve for the\nlocal ID at that neighborhood. Finally, local IDs\nare averaged to arrive at the overall estimate.\nMOM The Method-of-Moments estimator from\nAmsaleg et al., 2018 takes as reference the dis-\ntribution of distances X to a query point q, over\nhyperparameter k= 100NNs of q. The procedure\nestimates empirical moments of X and compares\nthem to the theoretical value, which is a function\nof d, in order to solve for d.\nMADA Manifold Adaptive Dimension Estima-\ntion (MADA) compares the empirical probability\nthat datapoints fall into a ball around a query dat-\napoint to the theoretical probability, which is a\nfunction of d. More precisely,\nP(xi ∈B(x,r)) =η(x,r)rd\nfor some function η, where for small-enough r,\nη(x,r) is essentially constant. Then, dis fit over\nmany query points xand xi in the dataset (Farah-\nmand et al., 2007).\nA.2 ID Estimation Convergence Analysis\nIn section 3.1, we state that ID estimation is per-\nformed for datasets X of size N > 10000. For\nlarge datasets ( N >50000), we compute ID on\nrandom subsample of min(N,50000) data points\nfor efficiency. Here, we verify that ID computed\non a sample of size 10000 < N < 50000 for\nmax(D) = 4096reasonably converges, the reason\nbeing that NN-based ID estimators can be scale-\ndependent. Scale-dependence, or sensitivity to\ndata resolution, of NN-based estimators is well-\ndocumented in the literature. Due to concentration\nof measure in high dimensions, these estimators\nare sensitive to data density and underestimate ID\nfor d≳ 10 (Facco et al., 2017; Ansuini et al., 2019;\n0.0 1.0 2.0 3.0 4.0 5.0\nN Samples (1e4)\n10\n20\n30\n40\n50ID Estimate\nConvergence of ID Methods (bookcorpus)\nT woNN\nFisherS\nPCA\nMLE\nCorrInt\nMOM\nTLE\nMADA\nESS\nFigure A.1: Convergence of ID estimates for\nbookcorpus on the last layer of OPT-6.7b, from N′ ≈\n200 to N′ = 50000. Each datapoint is the mean ID\nestimate, shown with one standard deviation, on a boot-\nstrapped sample of size N′, computed over 3 random\nseeds.\nCeruti et al., 2014), see Erba et al., 2019 for discus-\nsion.\nSimilar to Ansuini et al., 2019, we justify our\nchoice of 50000 with a convergence analysis of\nID estimators on a large, complex dataset, that is,\nbookcorpus (N = 74004228), using OPT-6.7b\n(D = 4096), and a one layer representation (the\nlast). Starting at N′ = 50000, we systematically\nsubsample the representations on a log-scale and\nshow that ID estimates averaged over three ran-\ndom seeds appear to converge for all estimators by\nN′≈10000 (see fig. A.1). Convergence of these\nestimates permits us to reliably include them in our\nanalysis linking ID to PPL and ease-of-adaptation.\nFinally, our approach is limited for small\ndatasets, as N needs to grow exponentially in d\nin order to estimate d(Bishop, 2007). As attested\nin the literature (Campadelli et al., 2015; Alber-\ngante et al., 2019), several ID estimators are empir-\nically more robust to this curse of dimensionality,\nsuch as non-NN-based estimators PCA and FisherS,\nwhich converge earlier than N′ ≈10000. More-\nover, note that for the remaining ID estimators, for\nsmall N′ ⪅ 10000, they are increasing, thus sys-\ntematically underestimating ID, until starting to\nconverge at around N′≈10000. Therefore, while\nN′= 10000is not an end-all be-all cut-off for ID\nconvergence of linguistic data, we employ it as a\nheuristic for dataset selection.\nA.3 Other Estimators Tested\nWe tested a total of 12 estimators. In addition to\nthe 9 discussed in appendix A.1, we tested three\nNN-based global estimators, DANCo (Ceruti et al.,\n12413\n2014), MiND ML (Rozza et al., 2012), and kNN\n(without bootstrapping) (Carter et al., 2010), but\nencountered difficulty tuning the hyperparameters\nto produce sensible ID estimates (MiND ML pro-\nduced d= 10for almost all layers on all datasets;\nthe other estimators produced d≈D). For this rea-\nson, we omit them from discussion of the results.\nFinally, in future work, it is worth testing the\nrecent FCI estimator of Erba et al., 2019 and the\nGeneralized Ratios ID estimator of Denti et al.,\n2022, which are more robust to scaling effects by\ndesign.\nB Dataset Details\nIn this appendix, we outline how [Super]GLUE\ndata are preprocessed for PPL and ID computation,\nand describe the remaining corpora in table B.2.\nAll datasets are freely available on HuggingFace at\nthe time of writing.\nPreprocessing [Super]GLUE GLUE and Super-\nGLUE are two linguistic classification benchmarks\ndesigned to test language models on tasks such as\ntextual entailment and semantic equivalence (Wang\net al., 2019b,a). For example, in Quora Question\nPairs (qqp), two questions are given to the model\nas natural language, and the model is tasked to\npredict whether they are equivalent. If there are\nmultiple inputs, we treat them as individual data\npoints in the ID / PPL computations and feed them\nto the model separately as in Wang et al., 2019a,\nleaving other strategies, such as concatenation with\ndelimiters (Brown et al., 2020), to future work.\nC Finetuning Details\nWe implement full finetuning for OPT-350m and\nparameter-efficient finetuning using LoRA (Hu\net al., 2022) for the other LMs on a standard cross-\nentropy causal language modeling objective, op-\ntimized using AdamW (Loshchilov and Hutter,\n2019).\nTo avoid an expensive hyperparameter search,\nwe set a constant batch size for each model (the\nlargest fitting in memory), with a constant learning\nrate schedule. Then, the only hyperparameter we\nvary is the learning rate from 5e-5, 5e-6 to 5e-7.\nFor each learning rate, we average final evaluation\nPPL after training over three random seeds, and\nwe choose the best learning rate by the lowest final\nevaluation PPL. All values reported are averaged\nover 3 random seeds for the best learning rate.\nCorpus Summary\nIMDB (Maas\net al., 2011)\nMovie reviews\nPenn Treebank\n(Marcus et al.,\n1993)\nText-only version\nof Penn Treebank\n(content from 1989\nWall Street Journal)\nBookcorpus\n(Zhu et al.,\n2015)\nText from books\nWikitext (Mer-\nity et al., 2017)\nCleaned Wikipedia\nWikitext fr\n(Simoulin and\nCrabbé, 2021)\nCleaned French\nWikipedia\nTweets (Barbieri\net al., 2020)\nTwitter dataset’s\nsentiment evalua-\ntion subset\nPile-10k\n(Nanda, 2022)\nFirst 10k sequences\nof The Pile (Gao\net al., 2020)\nOpenwebtext-\n10k (Bekman,\n2022)\n10k sequences\nfrom Openwebtext\n(Gokaslan et al.,\n2019)\nCNN Dailymail\n(Hermann et al.,\n2015)\nNews articles from\nCNN\nCONCODE\n(Soliman, 2022)\nSet of Java classes\nfrom CONCODE\n(Iyer et al., 2018)\nTable B.2: Description of external non-[Super]GLUE\ncorpora\nHyperparameter Value\nBatch size 8 (opt-350m); 16 (opt-\n6.7b); 32 (opt-1.3b)\nlr 5e-5, 5e-6, 5e-7\nmax train epochs 15\nLoRA rank 8\nLoRA α 8\nAdamW β1,β2 (0.9, 0.999)\nAdamW ϵ 1e-6\nTable C.3: Hyperparameters used in finetuning, where\nOPT-350m is fully finetuned and all other models are\nfinetuned using LoRA.\nHyperparameters can be found in table C.3.\nCode is adapted from HuggingFace implementa-\ntions and can be found at https://github.com/\n12414\nchengemily1/id_bridging.\nD ID over layers\nIn Section 4, we have presented results focusing\non max ID as our aggregate measure across layers.\nHowever, the evolution of ID across layers is an\ninteresting problem in itself, and reveals how Trans-\nformers sequentially compress (or decompress) lin-\nguistic representations (cf. Valeriani et al., 2023\nand Ansuini et al., 2019 for equivalent studies on\nprotein sequence and vision models, respectively).\nIn the large majority of cases, ID is relatively\nstable across layers, justifying the choice of focus-\ning on an aggregate measure: see representative\nexamples in fig. D.2a. We do, however, also ob-\nserve small clusters of datasets where the ID profile\nacross layers exhibits different patterns. For exam-\nple, for a number of datasets (example in fig. D.2b),\nwe observe a first increase in ID in the early layers\nfollowed by a later increase at the middle layers,\nwhich might suggest kernel-function-like dimen-\nsionality expansions before new reductions, along\nthe lines of what was observed by Ansuini et al.,\n2019. Another interesting pattern pertains to the\nOPTCorpus, showing a late increase in dimension-\nality. Intriguingly, this pattern is not disrupted by\nthe ablations, as shown in fig. D.2c. Still, we want\nto emphasize that in most cases ID profiles are rel-\natively flat: even when they aren’t (as in figs. D.2b\nand D.2c), the fluctuations stay within relatively\nnarrow ranges. We conjecture that residual con-\nnections have a stabilizing effect on ID across lay-\ners. Notably, however, Valeriani et al., 2023 found\nthat Transformers applied to visual and protein se-\nquence data displayed an early peak in ID that is\nmissing in our linguistic datasets, whose profiles\nvary in shape. We leave a fuller understanding of\nthe differences to future work.\nE Extended Results\nHere, we review results on the relationship between\ngeometric compression, information-theoretic com-\npression, and ease-of-adaptation for all model sizes,\nID estimators, and aggregations of ID.\nAlternative Aggregations of ID In addition to\nthe max ID over layers, which is discussed in the\nmain article, we test the following other aggrega-\ntions: min, first (ID of positional embeddings),\nlast (ID before next-word prediction), mean, and\nmedian, which are “point-aggregates\" of ID, and\nchange (last−first) and range (max−min), which\nsummarize the spread of ID, i.e., how data is pro-\ncessed in terms of expansion or reduction over lay-\ners.\nWe find that when one aggregation of ID is cor-\nrelated to PPL, so are other aggregations. For in-\nstance, as the max ID influences the mean ID, they\nare often both correlated to PPL. For estimators\nCorrInt, TwoNN, ESS, TLE, and MLE, we notice\nvirtually all aggregations of ID correlating to per-\nplexity in OPT-350m (right column of figs. E.6\nand E.7).\nID vs. Perplexity We observe the following\nabout the Spearman correlation between aggrega-\ntions of ID and dataset perplexity:\n• Correlation strength tends to increase as\nmodel size decreases. This is seen by look-\ning at the leftmost columns of the correlation\nplots: there are more significant correlations\n(colored squares) as model size decreases in\nfigs. E.6 and E.7.\n• When significant, min, max, first, last, mean,\nand median ID are positively correlatedto\nPPL for all model sizes and ID estimators.\n• When significant, change in ID is negatively\ncorrelated with PPL for all model sizes and\nID estimators. That is, intuitively, if an input\nis harder for the model to process (larger PPL),\nit projects its features into higher-dimensional\nspace for next-token prediction, which we in-\nterpret to be a kernel-expansion-like operation\nà la Ansuini et al., 2019.\n• When significant, range of ID positively cor-\nrelates to PPL for all model sizes and ID\nestimators. This correlation can be partially\nexplained by those between the min and max\nID and PPL.\nID vs. Ease-of-Adaptation We find that sev-\neral ID aggregates are good predictors of ease-of-\nadaptation across model size and ID estimator. We\nobserve the following:\n• For Correlation Dimension and NN-based ID\nestimators, when significant, point estimates\nof ID (e.g., max, mean) positively correlate\nto final evaluation PPL and sample com-\nplexity, as evidenced by the orange-colored\nsquares in the bottom-right of plots in figs. E.6\nand E.7. This trend breaks down for PCA and\nFisherS, but appears robust to model size.\n12415\n0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth (opt-6.7b)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Relative Intrinsic Dimension (ESS)\nstsb\nimdb\nboolq\n(a) Relatively stable ID\n0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth (opt-6.7b)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Relative Intrinsic Dimension (ESS)\nmnli\nwic\nbookcorpus (b) Expansions then reductions in ID\n0 0.2 0.4 0.6 0.8 1.0\nRelative Layer Depth (opt-6.7b)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10Relative Intrinsic Dimension (ESS)\noptcorpus\noptcorpus_swapped\noptcorpus_random\noptcorpus_permuted (c) ID profiles for OPTCorpora\nFigure D.2: Representative (relative) ID profiles, computed with ESS, of dataset representations against layer depth\nfor OPT-6.7b, qualitatively grouped into three categories: (a) “flat\" (the most common case), (b) “hilly\", and (c)\ngradual increase then sudden decrease, corresponding to OPTCorpora. ID profiles are calculated on min(N,50000)\nsequences for each dataset shown, where N is the number of sequences in the dataset.\n• Results on ID spread (change and range)\nare inconclusive, giving sometimes negative\nand sometimes positive correlations depend-\ning on the estimator and model size.\nID vs. Shallow Linguistic Descriptors The re-\nlationship between ID and shallow linguistic de-\nscriptors like vocab size, vocab entropy, average\nsequence length, and number of tokens appears to\ndiverge by ID method category. We observe the\nfollowing (see figs. E.6 and E.7):\n• Trends appear to be opposite for PCA/FisherS\nand the other ID estimators.\n• For PCA/FisherS, ID generally correlates neg-\natively to linguistic descriptors like average\ninput length, vocab size, and vocab entropy.\n• When significant, vocab size and entropy gen-\nerally correlate positively to ID aggregates for\nestimators that aren’t PCA/FisherS.\nID and Linguistic Structure Beyond ESS (sec-\ntion 4.3), the increasing trend in ID/PPL for vari-\nants of wikitext, successively baseline <permuted\n<swapped ≲ random, also holds for virtually all\nID estimators and model sizes (fig. E.5)– all but\nPCA/FisherS (not pictured in the figure), who esti-\nmate ID of all variants to be baseline = permuted\n= swapped <random.\n12416\n5 10\nDataset Perplexity (log)\n40\n60\n80\n100\n120\n140ESS Max ID\ncola\nmnli\nmrpc\nqnli\nqqprte sst2\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb wikitext\ncode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymailopenwebtextoptcorpus optcorpus_permuted\noptcorpus_randomoptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.49 (p=0.01)\nopt-1.3b\n(a) ESS max ID vs. log-PPL\n50 100 150 200 250\nESS Max ID\n5\n10\n15\n20\n25Finetune Sample Complexity (log)\nimdbbookcorpuswikitext_frpenntreebankwikitext\ntweets\npilecode dailymailopenwebtext\noptcorpus_permuted\noptcorpus_swapped\noptcorpus_random\ncode_permutedcode_swapped\ncode_random\nwikitext_permuted_tokens\nwikitext_swapped_tokens\nwikitext_random_tokens\n=0.61 (p=0.01)\nopt-1.3b (b) ESS sample complexity vs. max ID\n50 100 150 200 250\nESS Max ID\n5\n10\n15\n20\n25Finetune Eval. PPL (log)\nimdbbookcorpuswikitext_frpenntreebankwikitext\ntweets\npilecode dailymailopenwebtext\noptcorpus_permuted\noptcorpus_swapped\noptcorpus_random\ncode_permutedcode_swapped\ncode_random\nwikitext_permuted_tokens\nwikitext_swapped_tokens\nwikitext_random_tokens\n=0.65 (p=0.00)\nopt-1.3b (c) ESS final PPL (log) vs. max ID\n4 6 8 10 12\nDataset Perplexity (log)\n20\n40\n60\n80\n100\n120\n140ESS Max ID\nmrpc\nqnli\nrte sst2\nstsb\nbookcorpusboolq\nwic\nimdb\nwikitext\ncode\npenntreebankwikitext_frtweets\npile\ndailymailopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.66 (p=0.00)\nopt-350m\n(d) ESS max ID vs. log-PPL\n20 40 60 80 100 120 140\nESS Max ID\n2\n4\n6\n8\n10\n12Finetune Sample Complexity (log)\nimdb\nbookcorpuswikitext_frpenntreebankwikitext\ntweetspile\ncode\ndailymailopenwebtext\noptcorpus_permuted\noptcorpus_swapped\noptcorpus_random\nwikitext_permuted_tokenswikitext_swapped_tokens\nwikitext_random_tokens\n=0.81 (p=0.00)\nopt-350m (e) ESS sample complexity vs. max ID\n20 40 60 80 100 120 140\nESS Max ID\n2\n4\n6\n8\n10\n12Finetune Eval. PPL (log)\nimdb\nbookcorpuswikitext_frpenntreebankwikitext\ntweetspile\ncode\ndailymailopenwebtext\noptcorpus_permuted\noptcorpus_swapped\noptcorpus_random\nwikitext_permuted_tokens\nwikitext_swapped_tokens\nwikitext_random_tokens=0.81 (p=0.00)\nopt-350m (f) ESS final PPL (log) vs. max ID\nFigure E.3: For OPT-1.3b (top row) and OPT-350m (bottom row), max ID estimated with ESS is positively correlated\nto (a,d) dataset PPL (significant with p≤0.01 for both models); predicts ease-of-adaptation metrics (b,e) sample\ncomplexity (significant with p≤0.01 for both models); and (c,f) final evaluation PPL (significant with p≤0.01 for\nboth models). Datapoints for wikitext and its perturbations are labeled for reference. The right two plots contain the\nsubset of datasets tested that are suitable for a causal language modeling objective.\nPerplexity\nInput LengthVocab Size\nVocab Entropy\nSize\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\n1.00\n1.00 0.68 0.43\n0.68 1.00 0.78\n0.43 0.78 1.00\n1.00\nSpearman Corr. (opt-350m, =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(a) OPT-350m data correlations\nPCA\nFisherS\nCorrInt\nTLE\nMLE\nESS\nT woNN\nMOM\nMADA\nPCA\nFisherS\nCorrInt\nTLE\nMLE\nESS\nT woNN\nMOM\nMADA\n1.00 0.89 0.10 -0.22 -0.17 -0.28 -0.24 -0.10\n0.89 1.00 0.10 -0.31 -0.33 -0.16 -0.32 -0.16 -0.28\n0.10 0.10 1.00 0.52 0.51 0.60 0.39 0.42 0.52\n-0.22 -0.31 0.52 1.00 0.95 0.89 0.73 0.82 0.82\n-0.17 -0.33 0.51 0.95 1.00 0.85 0.75 0.76 0.85\n-0.16 0.60 0.89 0.85 1.00 0.60 0.74 0.77\n-0.28 -0.32 0.39 0.73 0.75 0.60 1.00 0.42 0.50\n-0.24 -0.16 0.42 0.82 0.76 0.74 0.42 1.00 0.79\n-0.10 -0.28 0.52 0.82 0.85 0.77 0.50 0.79 1.00\nLayer-wise Spearman Corr. of ID Methods (opt-350m, = 0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (b) OPT-350m ID correlations\nPerplexity\nInput LengthVocab Size\nVocab Entropy\nSize\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\n1.00\n1.00 0.76 0.50\n0.76 1.00 0.72\n0.50 0.72 1.00\n1.00\nSpearman Corr. (opt-1.3b, =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(c) OPT-1.3b data correlations\nPCA\nFisherS\nCorrInt\nTLE\nMLE\nESS\nT woNN\nMOM\nMADA\nPCA\nFisherS\nCorrInt\nTLE\nMLE\nESS\nT woNN\nMOM\nMADA\n1.00 0.94 0.17 -0.10 -0.11 0.28 -0.53 0.47 0.22\n0.94 1.00 0.25 -0.10 0.32 -0.45 0.38 0.12\n0.17 0.25 1.00 0.44 0.27 0.60 0.11 0.25 0.13\n-0.10 0.44 1.00 0.85 0.79 0.74 0.48 0.52\n-0.11 -0.10 0.27 0.85 1.00 0.59 0.64 0.68 0.77\n0.28 0.32 0.60 0.79 0.59 1.00 0.28 0.60 0.49\n-0.53 -0.45 0.11 0.74 0.64 0.28 1.00 0.18\n0.47 0.38 0.25 0.48 0.68 0.60 1.00 0.93\n0.22 0.12 0.13 0.52 0.77 0.49 0.18 0.93 1.00\nLayer-wise Spearman Corr. of ID Methods (opt-1.3b, = 0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (d) OPT-1.3b ID correlations\nFigure E.4: Data metrics Spearman correlations for left column (a) OPT-350m and (c) OPT-1.3b; and ID metrics\nSpearman correlations for right column (b) OPT-350m and (d) OPT-1.3b. Note the inter-correlations between\nshallow linguistic descriptors input length, vocab size, and vocab entropy for all model sizes, as well as the grouping\nof ID metrics into one block corresponding to NN-based methods (bottom right of each plot).\n12417\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n10\n15\n20\n25\n30\n35CorrInt Max ID\ncolamnli\nmrpc\nqnli\nqqprte\nsst2stsbboolq multirc\nwic\nimdb wikitext\ncode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.49 (p=0.01)\nopt-6.7b\n5 10\nDataset Perplexity (log)\n10\n15\n20\n25CorrInt Max ID\ncola\nmnli\nmrpc\nqnli\nqqp\nrte\nsst2\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb\nwikitext\ncode\npenntreebankwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus_permuted\noptcorpus_randomoptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens=0.39 (p=0.04)\nopt-1.3b\n4 6 8 10 12\nDataset Perplexity (log)\n5\n10\n15\n20\n25\n30CorrInt Max ID\nmnli\nmrpc\nqqprte\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweetspile\ndailymail\nopenwebtextoptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.62 (p=0.00)\nopt-350m\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n0\n20\n40\n60\n80\n100T woNN Max ID\ncola\nmnli\nmrpc\nqnli\nqqprte\nsst2\nstsbboolq multirc\nwic\nimdb wikitext\ncode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.13 (p=0.50)\nopt-6.7b\n5 10\nDataset Perplexity (log)\n10\n20\n30\n40\n50\n60T woNN Max ID\ncola\nmnli\nmrpc\nqnli\nqqp\nrte\nsst2\nstsb\nbookcorpus\nboolq multirc\nwic\nimdb\nwikitext\ncodepenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.22 (p=0.27)\nopt-1.3b\n4 6 8 10 12\nDataset Perplexity (log)\n10\n20\n30\n40\n50T woNN Max ID\nmnli\nmrpc\nqqp\nrte\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweetspile\ndailymail\nopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.60 (p=0.00)\nopt-350m\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n20\n30\n40\n50\n60\n70TLE Max ID\ncola\nmnli\nmrpc\nqnliqqp\nrte sst2\nstsbboolq multirc\nwic\nimdb\nwikitext\ncode\npenntreebankwikitext_frtweets\npile\ndailymailopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.43 (p=0.02)\nopt-6.7b\n5 10\nDataset Perplexity (log)\n15\n20\n25\n30\n35\n40\n45TLE Max ID\ncola\nmnli\nmrpc\nqnli\nqqprte\nsst2\nstsb\nbookcorpus\nboolq multirc\nwicimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_randomoptcorpus_swappedwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.40 (p=0.03)\nopt-1.3b\n4 6 8 10 12\nDataset Perplexity (log)\n10\n15\n20\n25\n30\n35\n40TLE Max ID\nmnli\nmrpc\nqqp\nrte\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweetspile\ndailymail\nopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.58 (p=0.00)\nopt-350m\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n20\n40\n60\n80\n100\n120\n140MLE Max ID\ncola\nmnli\nmrpc\nqnliqqprte\nsst2\nstsb\nboolq\nmultirc\nwic\nimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweetspile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.19 (p=0.35)\nopt-6.7b\n5 10\nDataset Perplexity (log)\n20\n30\n40\n50MLE Max ID\ncola\nmnli\nmrpc\nqnli\nqqprte\nsst2\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus_permuted\noptcorpus_randomoptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.28 (p=0.15)\nopt-1.3b\n4 6 8 10 12\nDataset Perplexity (log)\n10\n15\n20\n25\n30\n35\n40\n45\n50MLE Max ID\nmnli\nmrpc\nqqp\nrte\nstsb\nbookcorpus\nboolq multircwic\nimdb\nwikitext\ncode\npenntreebank\nwikitext_fr\ntweetspile\ndailymail\nopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.57 (p=0.00)\nopt-350m\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n20\n40\n60\n80\n100\n120\n140MOM Max ID\ncola\nmnli\nmrpc\nqnliqqprte sst2\nstsb\nboolq\nmultirc\nwic\nimdb\nwikitextcode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_randomoptcorpus_swappedwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.25 (p=0.20)\nopt-6.7b\n5 10\nDataset Perplexity (log)\n20\n30\n40\n50\n60\n70\n80MOM Max ID\ncola\nmnli\nmrpc\nqnli\nqqprte sst2\nstsb\nbookcorpus\nboolq multirc\nwic\nimdb\nwikitextcode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_randomoptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.19 (p=0.33)\nopt-1.3b\n4 6 8 10 12\nDataset Perplexity (log)\n5\n10\n15\n20\n25\n30\n35\n40MOM Max ID\nmrpc\nqnli\nrte sst2\nstsb\nbookcorpus\nboolq\nwic\nimdb\nwikitext\ncode\npenntreebankwikitext_frtweetspiledailymail\nopenwebtextoptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.42 (p=0.04)\nopt-350m\n2 4 6 8 10 12 14\nDataset Perplexity (log)\n25\n50\n75\n100\n125\n150\n175\n200MADA Max ID\ncola\nmnli\nmrpcqnliqqprte sst2\nstsb\nboolq\nmultirc\nwic\nimdb\nwikitextcode\npenntreebank\nwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus_permuted\noptcorpus_randomoptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.24 (p=0.24)\nopt-6.7b\n5 10\nDataset Perplexity (log)\n40\n60\n80MADA Max ID\ncola\nmnli\nmrpc\nqnli\nqqprte\nsst2\nstsb\nbookcorpus\nboolq\nmultirc\nwic\nimdb\nwikitextcode\npenntreebankwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus optcorpus_permuted\noptcorpus_randomoptcorpus_swapped\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.12 (p=0.56)\nopt-1.3b\n4 6 8 10 12\nDataset Perplexity (log)\n10\n20\n30\n40\n50\n60MADA Max ID\nmrpc\nrte\nsst2\nstsb\nbookcorpus\nboolq\nwic\nimdb\nwikitext\ncode\npenntreebankwikitext_fr\ntweets\npile\ndailymail\nopenwebtext\noptcorpus\noptcorpus_permuted\noptcorpus_random\noptcorpus_swapped\nwikitext_random_tokens\nwikitext_permuted_tokens\nwikitext_swapped_tokens\n=0.42 (p=0.04)\nopt-350m\nFigure E.5: For OPT-6.7b (left), OPT-1.3b (middle) and OPT-350m (right), and for all other ID estimators considered,\nwe plot max ID vs. dataset PPL (log), labeling the points for wikitext. Across estimators and model sizes, we see a\ngeneral trend that ablating linguistic structure increases ID as well as PPL, in the order of baseline <permuted <\nswapped ≲ random.\n12418\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.41 0.33\n0.51\n-0.37 -0.49 -0.52\n-0.47 -0.44 -0.44\n-0.52 -0.40\n-0.49\n0.41 0.33\n0.87 0.46 0.51\n0.90 0.41 0.48 0.43 -0.43\nopt-6.7b PCA Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(a) OPT-6.7b, PCA\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n-0.52 -0.37\n-0.35\n0.35\n-0.33 -0.32 -0.43 -0.50\n-0.49 -0.39 -0.33\n-0.55 -0.37\n-0.55\n-0.35\n0.93 0.40 0.49 -0.57\n0.94 0.46 0.39 -0.54\nopt-1.3b PCA Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (b) OPT-1.3b, PCA\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.40 0.45 0.47\n-0.34 -0.42\n-0.38 -0.41 -0.55\n-0.37\n0.64 0.73 0.79 -0.45 0.47 -0.46 -0.61\n0.64 0.73 0.79 -0.45 0.47 -0.46 -0.61\nopt-350m PCA Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (c) OPT-350m, PCA\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n-0.32\n0.55\n-0.42 -0.48 -0.58\n-0.45 -0.34 -0.37\n-0.46 -0.33\n-0.40 -0.36 -0.44 -0.51\n0.87 0.46 0.51 0.45\n0.90 0.41 0.48 0.51\nopt-6.7b FisherS Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(d) OPT-6.7b, FisherS\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n-0.34\n-0.46 -0.46\n0.35 0.34\n-0.46 -0.49 -0.53\n-0.47 -0.35 -0.35\n-0.52 -0.35\n-0.34 -0.37 -0.35 -0.40\n-0.38 -0.44\n0.93 0.40 0.49\n0.94 0.46 0.39\nopt-1.3b FisherS Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (e) OPT-1.3b, FisherS\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.49 0.34 0.41\n-0.34 -0.38\n-0.44 -0.44 -0.55\n0.64 0.73 0.79 -0.45 -0.55 -0.67\n0.64 0.73 0.79 -0.45 -0.55 -0.67\nopt-350m FisherS Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (f) OPT-350m, FisherS\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.49 0.33\n0.42\n0.32\n0.33 0.36 0.52\n0.87 0.46 0.51 0.45 0.52 0.53 0.44\n0.90 0.41 0.48 0.41 0.51 0.50 0.41\nopt-6.7b CorrInt Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(g) OPT-6.7b, CorrInt\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.39\n0.37\n-0.47\n0.36\n0.93 0.40 0.49 0.39 0.39 0.41\n0.94 0.46\nopt-1.3b CorrInt Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (h) OPT-1.3b, CorrInt\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.34 0.36\n0.62 0.41\n0.49\n0.45\n0.50 0.50\n0.64 0.73 0.79 -0.45 0.59 0.69 0.51 0.49 0.59\n0.64 0.73 0.79 -0.45 0.59 0.69 0.51 0.49 0.59\nopt-350m CorrInt Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (i) OPT-350m, CorrInt\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.41 0.53\n0.58 0.64 0.32 0.39\n0.50\n0.65 0.60\n0.52 0.54 0.46\n0.54 0.53 0.45\n0.63 0.49 0.36\n0.64 0.64 0.46\n0.87 0.46 0.51 0.50 0.42 0.64 0.64\n0.90 0.41 0.48 0.49 0.39 0.62 0.65\nopt-6.7b TwoNN Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(j) OPT-6.7b, TwoNN\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.42 0.50 0.47\n0.67 0.72 0.39 0.33\n0.36 0.35\n0.69 0.70 0.36\n0.58 0.58 0.38\n0.58 0.58 0.40\n0.57 0.59 0.39\n0.66 0.59 0.43\n0.93 0.40 0.49 0.64 0.65 0.41 0.41 0.86 0.80\n0.94 0.46 0.58 0.59 0.84 0.79\nopt-1.3b TwoNN Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (k) OPT-1.3b, TwoNN\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.39 0.38\n0.60 0.59 0.48\n0.54 0.59 0.34\n0.37 0.36 0.42\n0.34 0.41 0.43 0.35\n0.59 0.53 0.50\n0.51 0.66 0.75\n0.64 0.73 0.79 -0.45 0.66 0.49 0.84\n0.64 0.73 0.79 -0.45 0.66 0.49 0.84\nopt-350m TwoNN Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (l) OPT-350m, TwoNN\nFigure E.6: Global ID Estimators: full panel of Spearman correlations for models (left to right columns) OPT-6.7b,\n1.3b, and 350m. Each figure is a summary of Spearman correlations given a model and global ID metric, significant\nat α= 0.1, between aggregated ID and data metrics (top left), ease-of-finetuning metrics and data metrics (bottom\nleft), and ease-of-finetuning and ID (bottom right).\n12419\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.59 -0.42\n0.51 0.50 0.44\n0.55 -0.64\n0.34 0.52 0.51 0.46\n0.63 0.33\n0.48\n0.72 0.32\n0.38 0.43 0.64 0.54\n0.87 0.46 0.51 0.58 0.73 0.79 0.78 0.69 0.63\n0.90 0.41 0.48 0.55 0.72 0.80 0.76 0.64 0.63\nopt-6.7b ESS Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(a) OPT-6.7b, ESS\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.49 -0.53\n0.49 0.47\n0.49 -0.70\n0.59 0.51 0.34\n0.51\n0.35 -0.36\n0.83 0.45 0.32\n0.34 0.55 0.60 0.45\n0.93 0.40 0.49 0.46 0.65 0.57 0.59 0.41 0.58\n0.94 0.46 0.40 0.61 0.52 0.55 0.57\nopt-1.3b ESS Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (b) OPT-1.3b, ESS\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.62 0.39\n0.66 0.38 0.49\n0.64\n0.35 0.46 0.44\n0.63 0.48 0.55\n0.61 0.50 0.54\n0.44\n0.46 0.38 0.48\n0.64 0.73 0.79 -0.45 0.69 0.81 0.78 0.49 0.84 0.84 0.74\n0.64 0.73 0.79 -0.45 0.69 0.81 0.78 0.49 0.84 0.84 0.74\nopt-350m ESS Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (c) OPT-350m, ESS\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.36 0.42\n0.43 0.36 0.63 0.42 0.34\n0.43 -0.50\n0.64 0.68 0.42\n0.39 0.52 0.33 0.38\n0.33 0.44 0.40\n0.79 0.57 0.37\n0.60 0.67 0.54\n0.87 0.46 0.51 0.62 0.60 0.49 0.46 0.62\n0.90 0.41 0.48 0.58 0.54 0.45 0.39 0.64\nopt-6.7b TLE Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(d) OPT-6.7b, TLE\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.44\n0.40 0.47 0.66 0.38\n-0.37\n0.67 0.71 0.40\n0.37 0.46 0.41\n0.41 0.36\n0.72 0.51 0.35\n0.66 0.63 0.48\n0.93 0.40 0.49 0.46 0.48\n0.94 0.46 0.41 0.46\nopt-1.3b TLE Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (e) OPT-1.3b, TLE\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.46\n0.58 0.55 0.50\n0.48\n0.47 0.47\n0.48 0.51 0.41\n0.47 0.37 0.53 0.45\n0.48 0.34\n0.60 0.34 0.45 0.53\n0.64 0.73 0.79 -0.45 0.58 0.73 0.78 0.45 0.64 0.64 0.68\n0.64 0.73 0.79 -0.45 0.58 0.73 0.78 0.45 0.64 0.64 0.68\nopt-350m TLE Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (f) OPT-350m, TLE\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.36 0.47 0.63 -0.32\n-0.45\n0.61 0.65 0.42\n0.36 0.47 0.44\n0.35 0.37\n0.76 0.54 0.33\n0.39 0.42 0.58 -0.36\n0.87 0.46 0.51 0.44 0.51\n0.90 0.41 0.48 0.39 0.47\nopt-6.7b MLE Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(g) OPT-6.7b, MLE\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.34\n0.49 0.60 0.42\n0.65 0.64\n0.39 0.56\n0.36 0.50\n0.69 0.44\n0.55 0.48 0.40\n0.93 0.40 0.49 0.39\n0.94 0.46 0.40\nopt-1.3b MLE Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (h) OPT-1.3b, MLE\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.34\n0.57 0.52 0.63\n0.44\n0.47 0.49\n0.50 0.39 0.58 0.53\n0.53 0.38 0.56 0.53\n0.51 0.37\n0.49 0.38 0.50\n0.64 0.73 0.79 -0.45 0.52 0.75 0.74 0.54 0.75 0.76 0.68\n0.64 0.73 0.79 -0.45 0.52 0.75 0.74 0.54 0.75 0.76 0.68\nopt-350m MLE Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (i) OPT-350m, MLE\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.56 -0.58\n0.52 -0.42\n0.36 -0.35\n0.47 0.49 0.36\n-0.40\n-0.34\n0.65 0.43\n0.52 -0.39\n0.87 0.46 0.51 0.51\n0.90 0.41 0.48 0.47\nopt-6.7b MOM Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(j) OPT-6.7b, MOM\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.39 -0.56\n0.37\n0.36 -0.52\n0.55 0.47\n0.77 0.43\n-0.35\n0.93 0.40 0.49\n0.94 0.46\nopt-1.3b MOM Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (k) OPT-1.3b, MOM\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.37 0.47\n0.42 0.41 0.45\n0.35 -0.38\n0.36 0.36\n0.43 0.43 0.50\n0.38 0.47 0.49\n0.57\n0.43 0.45 0.38\n0.64 0.73 0.79 -0.45 0.63 0.66 0.66 0.68 0.66 0.44\n0.64 0.73 0.79 -0.45 0.63 0.66 0.66 0.68 0.66 0.44\nopt-350m MOM Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (l) OPT-350m, MOM\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.49 -0.50\n-0.45\n0.41 0.57 0.51 0.34\n0.45 -0.42\n0.35 -0.32\n0.69\n0.48 -0.43\n0.87 0.46 0.51 0.41 0.42\n0.90 0.41 0.48 0.40\nopt-6.7b MADA Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(m) OPT-6.7b, MADA\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n-0.48\n0.38 -0.41\n-0.62\n0.65 0.52\n0.72 0.45\n0.38 -0.43\n0.93 0.40 0.49\n0.94 0.46\nopt-1.3b MADA Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (n) OPT-1.3b, MADA\nPerplexity\nInput Length\nVocab Size\nVocab Entropy\nSize\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nMin ID\nMax ID\nFirst ID\nLast ID\nMean ID\nMedian ID\nChange in ID\nRange of ID\nEval. PPL\nSample Complexity\n0.45 0.55\n0.42 0.37 0.52\n0.38 0.36 0.43\n0.48 0.45 0.63 0.69\n0.50 0.44 0.64 0.64\n0.41\n0.64 0.73 0.79 -0.45 0.66 0.47 0.51 0.82 0.81\n0.64 0.73 0.79 -0.45 0.66 0.47 0.51 0.82 0.81\nopt-350m MADA Spearman Corr. Summary ( =0.1)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n (o) OPT-350m, MADA\nFigure E.7: Local ID Estimators: full panel of Spearman correlations for models (left to right columns) OPT-6.7b,\n1.3b, and 350m. Each figure is a summary of Spearman correlations given a model and ID metric, significant at\nα= 0.1, between aggregated ID and data metrics (top left), ease-of-finetuning metrics and data metrics (bottom\nleft), and ease-of-finetuning and ID (bottom right).\n12420",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7136508822441101
    },
    {
      "name": "Data compression",
      "score": 0.5989530682563782
    },
    {
      "name": "Dimension (graph theory)",
      "score": 0.5788453221321106
    },
    {
      "name": "Bridging (networking)",
      "score": 0.5658538341522217
    },
    {
      "name": "Information theory",
      "score": 0.5272142887115479
    },
    {
      "name": "Compression (physics)",
      "score": 0.5258097052574158
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.5140265226364136
    },
    {
      "name": "Language model",
      "score": 0.49342623353004456
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4916127324104309
    },
    {
      "name": "Estimator",
      "score": 0.4489785432815552
    },
    {
      "name": "Natural language processing",
      "score": 0.42402786016464233
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4207046627998352
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3443678915500641
    },
    {
      "name": "Mathematics",
      "score": 0.21626749634742737
    },
    {
      "name": "Statistics",
      "score": 0.10293829441070557
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11932220",
      "name": "Institució Catalana de Recerca i Estudis Avançats",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210104448",
      "name": "Institut Català de Ciències del Clima",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I170486558",
      "name": "Universitat Pompeu Fabra",
      "country": "ES"
    }
  ],
  "cited_by": 5
}