{
  "title": "Large Language Models in Health Care: Charting a Path Toward Accurate, Explainable, and Secure AI",
  "url": "https://openalex.org/W4391469382",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2250618308",
      "name": "Dhruv Khullar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2074490930",
      "name": "Xingbo Wang",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2074490930",
      "name": "Xingbo Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2905483812",
    "https://openalex.org/W4284974349",
    "https://openalex.org/W3005742798",
    "https://openalex.org/W3204153914",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4321472284",
    "https://openalex.org/W4287553002"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nLarge Language Models in Health Care: Charting a Path \nToward Accurate, Explainable, and Secure AI\nDhruv Khullar, MD, MPP1,2  , Xingbo Wang, PhD1, and Fei Wang, PhD1\n1Department of Population Health Sciences, Weill Cornell Medicine, New York, NY, USA; 2Department of Medicine, Weill Cornell Medicine, \nNew York, NY, USA\nJ Gen Intern Med  \nDOI: 10.1007/s11606-024-08657-2 \n© The Author(s), under exclusive licence to Society of General Internal \nMedicine 2024\nA \nlarge language model (LLM) is a type of artificial intel-\nligence (AI) that uses self-supervised neural networks \ntrained on vast quantities of text to generate human-like \nresponses. In the future, general-purpose or health care–spe-\ncific LLMs may increasingly support clinical diagnosis; \nreduce administration burdens; partially automate electronic \ncommunication between patients and clinicians; educate stu-\ndents via didactic instruction or patient simulation; acceler -\nate scientific discovery by helping to formulate, refine, and \nanswer research questions; and enhance health literacy and \nself-care among patients.\nBut realizing the potential for LLMs in health care also \nrequires confronting the technology’s shortcomings. These \nmodels do not “think” for themselves and do not understand \nthe reality they are meant to influence; rather, they predict \nthe next “token,” or text unit, in a string. Because their out -\nput carries the grammar, syntax, and aesthetics of believable \nlanguage, users often accept it as truth. Because the models \nare highly complex, encompassing tens of billions of param-\neters, they cannot provide—and we cannot ascertain—the \nrationale for their output. And because they are trained on \nvast swaths of online data, they can be manipulated to pro -\nduce harmful effects. Although many factors may shape the \nuse of LLMs in health care,1 in this Viewpoint, we focus on \nthree key areas: accuracy, explainability, and security.\nACC URA CY\nWhen AI is used to target advertisements to online shoppers, \nerrors have limited consequences. But in high-stakes fields \nsuch as health care, high levels of accuracy are needed. For \nmodels to be accurate, their output should adhere to the best \navailable evidence and remain uncontaminated by political \nor commercial interests. Currently, however, LLMs often \ngenerate incorrect or misleading statements—sometimes \ncalled “hallucinations.” These inaccuracies can occur even \nwhen a model’s training data includes ground truth and other \ncorrective information. In medicine, the pursuit of accuracy \npresents particular challenges because the literature often \nincludes contradictory data and because, even if a model \nis technically accurate, the right decision for a patient may \ndeviate from established guidelines. In addition, recent \nresearch suggests that the reliability of AI models may dete-\nriorate with time.2\nLLMs are trained using an iterative process known as rein-\nforcement learning with human feedback (Fig.  1). In this \napproach, the model produces various outputs and humans \nrank them according to their preferences, guiding the model \ntoward a desired state. The technique has delivered highly \nfluent text, but in health care, more emphasis must be placed \non factual correctness. This could be accomplished by plac-\ning greater reward weights on accuracy, involving more \ndomain experts in the refinement process, and constructing \ndatasets with enhanced salience of ground truths. Moreover, \nto mitigate the potential for bias, training data should be rep-\nresentative of the patient population on which a model will \nbe used. Some experts have also argued for the use of hybrid \nmodels that, in addition to using stochastic deep learning \ntechniques, employ classical AI, which relies on symbolic \nrepresentations of concepts to mimic the way humans learn \nand reason. 3 This may involve encoding pathophysiologic \nprinciples, diagnostic frameworks, and illness scripts into \nLLMs.\nEXPLAINABILITY\nIn health care, the rationale by which a model arrives at \nits output may be as important as the output itself. Models \nthat can provide the motivations for their responses—that \ncan “show their work”—may be critical for their adoption, \neffectiveness, and fairness, as well as knowing when human \ndecision-makers should deviate from a model’s recommen-\ndations, and why.\nIn 2021, the National Institute of Standards and Technol-\nogy developed principles for explainable AI, including that \nmodels provide explanations for their outputs with evidence, \naccuracy, meaning (i.e., they are readily understood), and \ncaution (i.e., they identify and declare the limits of their \nknowledge).4 The size of LLMs makes it difficult to examine \nReceived October 3, 2023 \nAccepted January 24, 2024\n1239\n39(7):1239– 41\nPublished online February 2, 2024\nKhullar et al.: Large Language Models in Health Care JGIM\ntheir parameters directly, but several indirect strategies can \nbe considered to enhance explainability. Shapley Additive \nExplanations can help quantify the contribution of various \nconcepts or decision paths toward a given prediction. This \nmethod can, for instance, create summary plots of the most \nimportant factors leading a model to rank certain diagno -\nses or treatments above others. Continued efforts should \nbe made to make these plots readily interpretable by non-\ntechnical users.\nAnother approach, known as prompt engineering, manip-\nulates inputs progressively to steer models toward certain \noutputs, and can be used to urge models to generate inter -\nmediate steps that result in a final outcome. 5 Some health \nsystems have recently hired prompt engineers to adapt gen-\neral-purpose LLMs for medical applications, and to design, \ntest, and evaluate various prompts. Future research could \nsystematically examine patterns in the prompts that users \nadopt to obtain desired answers, and help develop guidelines \nfor the effective use of prompt engineering in health care.\nSECURITY\nBecause LLMs are trained on huge quantities of internet \ndata, they are susceptible to outside attacks, creating risks \nfor model security and privacy. Researchers have demon -\nstrated the ease and inexpensiveness with which attackers \ncan inject false or misleading information into a model’s \ntraining data—sometimes called “data poisoning.”6 This can \noccur when even a small fraction (e.g., 0.1%) of instances \nin a dataset is altered. Several strategies can be considered \nto guard against data poisoning, including validating and \npreprocessing training data, using outlier detection algo -\nrithms to flag suspicious information, and employing adver-\nsarial training programs that intentionally introduce spurious \nexamples to train models to distinguish between authentic \nand deceptive inputs.\nA second security concern centers on data privacy—an \nespecially pressing issue when it comes to personal health \ninformation. Even absent malicious intent, user privacy \ncan be compromised. For example, a recent data breach at \nOpenAI’s ChatGPT resulted in some users being able to see \nthe titles of other people’s conversations and contributed to \nItaly’s decision to temporarily ban the product. Research has \nalso demonstrated that adversarial attacks can recover indi -\nviduals’ information from LLMs, including names, phone \nnumbers, and email addresses, even when models are trained \non private datasets.7 To reduce such security risks, training \ndata should be appropriately curated, sourced (to avoid web-\nsites hosting sensitive content), and de-duplicated (to limit \nrepeated inclusion of personal information). Developers and \nregulators can also consider audits to determine the privacy \nrisks posed by models.\nCONCLUSION\nEnsuring the benefits of LLMs exceed the risks will require \nthat humans can trust, understand, and improve upon their \noutput, and that patients and clinicians remain the princi -\npal arbiters of consequential decisions. It will also require \nconfronting a competitive commercial landscape, in which \ncompanies face limited public oversight and have strong \nincentives to rapidly bring products to market. A prudent \npath may involve public-private research and business part-\nnerships that help align incentives. For example, the Coali -\ntion for Health AI, launched in 2022, brings together tech -\nnologists, academics, policymakers, and health systems to \nhelp harmonize best practices for AI deployment and educate \nusers about evaluating risks, benefits, and limitations. In July \n2023, several leading AI companies committed to volun -\ntary safeguards, such as conducting research to mitigate pri-\nvacy concerns, testing products for security risks, and using \nwatermarks for AI-generated content; in October, the Biden \nAdministration issued an executive order requiring compa -\nnies to notify the government and perform additional test -\ning on models that could threaten public health and safety. \nThese are important steps, but continued attention is needed \nFigure 1  Schematic of a large language model trained by reinforcement learning with human feedback. Note: The LLM outputs in this \nfigure were generated by OpenAI’s ChatGPT.\n1240\nKhullar et al.: Large Language Models in Health CareJGIM\nto ensure that these innovative new technologies meet the \ntime-tested standards of safe, high-quality medical care.\nCorresponding Author:  Dhruv  Khullar, MD, MPP; Department of \nPopulation Health Sciences, Weill Cornell Medicine, New York, NY, USA \n(e-mail: khd9010@med.cornell.edu).\nFunding This work was supported by the Physicians Foundation \nCenter for the Study of Physician Practice and Leadership at Weill \nCornell Medicine.\nDeclarations: \nConflict of Interest:  Dr. Khullar reports receiving grants from the \nAgency for Healthcare Research and Quality, Arnold Ventures, and \nthe National Institutes of Health, outside the submitted work.\nDisclaimer: The Physicians Foundation had no role in the design \nand conduct of the study; collection, management, analysis, and \ninterpretation of the data; preparation, review, or approval of the \nmanuscript; and decision to submit the manuscript for publication.\nREFERENCES\n 1. Wang F, Casalino LP, Khullar D.  Deep Learning in Medicine—Prom -\nise, Progress, and Challenges. JAMA Intern Med. 2019;179(3):293-294. \nhttps:// doi. org/ 10. 1001/ jamai ntern med. 2018. 7117\n 2. Vela D, Sharp A, Zhang R, Nguyen T, Hoang A, Pianykh OS. Temporal \nQuality Degradation in AI Models. Sci Rep. 2022;12(1):11654. https:// \ndoi. org/ 10. 1038/ s41598- 022- 15245-z\n 3. Marcus G. The Next Decade in AI: Four Steps Towards Robust Artificial \nIntelligence.; 2020. https:// doi. org/ 10. 48550/ arXiv. 2002. 06177\n 4. Phillips PJ, Hahn CA, Fontana PC, et al. Four Principles of Explainable \nArtificial Intelligence . National Institute of Standards and Technology; \n2021. https:// doi. org/ 10. 6028/ NIST. IR. 8312\n 5. Wei J, Wang X, Schuurmans D, et  al. Chain-of-Thought Prompting \nElicits Reasoning in Large Language Models. In: ; 2022. Accessed April \n19, 2023. https:// openr eview. net/ forum? id=_ VjQlM eSB_J\n 6. Carlini N, Jagielski M, Choquette-Choo CA, et  al. Poisoning web-\nscale training datasets is practical. Published online February 20, \n2023. Accessed April 17, 2023. http:// arxiv. org/ abs/ 2302. 10149\n 7. Carlini N, Tramer F, Wallace E, et  al. Extracting Training Data from \nLarge Language Models. Published online June 15, 2021. https:// doi. \norg/ 10. 48550/ arXiv. 2012. 07805\nPublisher’s Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\n1241",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.8998297452926636
    },
    {
      "name": "Path (computing)",
      "score": 0.5879303812980652
    },
    {
      "name": "Health care",
      "score": 0.4699400067329407
    },
    {
      "name": "MEDLINE",
      "score": 0.4505425691604614
    },
    {
      "name": "Programming language",
      "score": 0.10272777080535889
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Computer science",
      "score": 0.0
    }
  ]
}