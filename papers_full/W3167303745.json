{
    "title": "Progressive Generation of Long Text with Pretrained Language Models",
    "url": "https://openalex.org/W3167303745",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5101403295",
            "name": "Bowen Tan",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5101461004",
            "name": "Zichao Yang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5008835202",
            "name": "Maruan Al-Shedivat",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5009547049",
            "name": "Eric P. Xing",
            "affiliations": [
                null,
                "Mohamed bin Zayed University of Artificial Intelligence",
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5085608858",
            "name": "Zhiting Hu",
            "affiliations": [
                "Carnegie Mellon University",
                "UC San Diego Health System"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2948629866",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3098295156",
        "https://openalex.org/W2965282611",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2176263492",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3039805635",
        "https://openalex.org/W2540413092",
        "https://openalex.org/W3093059841",
        "https://openalex.org/W4287642224",
        "https://openalex.org/W2963837967",
        "https://openalex.org/W2963592583",
        "https://openalex.org/W2935206035",
        "https://openalex.org/W2594978815",
        "https://openalex.org/W4309793872",
        "https://openalex.org/W2963443335",
        "https://openalex.org/W4288375073",
        "https://openalex.org/W2963993699",
        "https://openalex.org/W2912937082",
        "https://openalex.org/W2325227998",
        "https://openalex.org/W2963878748",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3099872554",
        "https://openalex.org/W3100714086",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W3114869109",
        "https://openalex.org/W3103753836",
        "https://openalex.org/W2949644922",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2947898088",
        "https://openalex.org/W3101652466",
        "https://openalex.org/W2970383515",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2133286915",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4297779694",
        "https://openalex.org/W4394651747",
        "https://openalex.org/W1956559956",
        "https://openalex.org/W2953345635",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2920538220",
        "https://openalex.org/W2914855263",
        "https://openalex.org/W3088754460",
        "https://openalex.org/W1985610876",
        "https://openalex.org/W3021347125",
        "https://openalex.org/W2963248296",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2757121784",
        "https://openalex.org/W2983962589",
        "https://openalex.org/W2985694911",
        "https://openalex.org/W2963730239",
        "https://openalex.org/W2797585226",
        "https://openalex.org/W2962821399"
    ],
    "abstract": "Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric Xing, Zhiting Hu. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4313–4324\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4313\nProgressive Generation of Long T ext\nwith Pretrained Language Models\nBowen T an1, Zichao Y ang1, Maruan Al-Shedivat 1, Eric P . Xing1,2,3, Zhiting Hu 1,4\n1Carnegie Mellon University, 2Petuum Inc., 3MBZUAI, 4UC San Diego\n{btan2, zichaoy, alshedivat, epxing}@andrew.cmu.edu, zhh019@ucsd.edu\nAbstract\nLarge-scale language models (LMs) pre-\ntrained on massive corpora of text, such as\nGPT -2, are powerful open-domain text genera-\ntors. However, as our systematic examination\nreveals, it is still challenging for such models\nto generate coherent long passages of text (e.g.,\n1000 tokens), especially when the models are\nﬁne-tuned to the target domain on a small cor-\npus. Previous planning-then-generation meth-\nods also fall short of producing such long\ntext in various domains. To overcome the\nlimitations, we propose a simple but effec-\ntive method of generating text in a progressive\nmanner, inspired by generating images from\nlow to high resolution. Our method ﬁrst pro-\nduces domain-speciﬁc content keywords and\nthen progressively reﬁnes them into complete\npassages in multiple stages. The simple de-\nsign allows our approach to take advantage of\npretrained LMs at each stage and effectively\nadapt to any target domain given only a small\nset of examples. We conduct a comprehensive\nempirical study with a broad set of evaluation\nmetrics, and show that our approach signiﬁ-\ncantly improves upon the ﬁne-tuned large LMs\nand various planning-then-generation methods\nin terms of quality and sample efﬁciency. Hu-\nman evaluation also validates that our model\ngenerations are more coherent.\n1\n1 Introduction\nGenerating coherent long text (e.g., 1000s of to-\nkens) is useful in myriad applications of creating re-\nports, essays, and other long-form content. Y et the\nproblem is particularly challenging as it demands\nmodels to capture global context, plan content, and\nproduce local words in a consistent manner. Prior\nstudies on “long” text generation have typically\nlimited to outputs of 50-200 tokens (Shen et al.,\n2019; Bosselut et al., 2018; Zhao et al., 2020).\n1Code available at https://github.com/\ntanyuqian/progressive-generation\nFigure 1: Results of large-scale LMs (GPT -2 and BART)\nﬁne-tuned on 10K stories. Coherence of text is evaluated by\nBERT next sentence prediction (NSP) score, where x-axis is\nthe position of the evaluated sentences in the passage. There is\na signiﬁcant gap in coherence between text by human and text\nby large-scale LMs. Our proposed ProGen instead generates\nmore coherent samples close to human text.\nRecent large-scale pretrained language models\n(LMs), such as GPT -2 (Radford et al., 2019) and\nBART (Lewis et al., 2020), emerged as an impres-\nsive open-ended text generator capable of produc-\ning surprisingly ﬂuent text. The massive LMs are\ntypically pretrained on large corpora of generic\ntext once, and then ﬁne-tuned with small domain-\nspeciﬁc data. The latest work has mostly focused\non the regime of relatively short text with low hun-\ndreds of tokens. For example, Holtzman et al.\n(2020); See et al. (2019); Hua and Wang (2020)\nstudied GPT -2 and BART generations with a max-\nimum length ranging from 150 to 350 tokens. In\nthis work, we study the problem of generating co-\nherent, much longer passages of text (e.g., 1000\ntokens). GPT -3 (Brown et al., 2020) was reported\nto produce long essays, yet the results seem to need\nextensive human curations (e.g.,MarketMuse; Gar-\ndian), and the system is not publicly available to\nadapt to arbitrary desired domains.\nIn this work, we examine ﬁne-tuning of large-\nscale LMs for domain-speciﬁc generation of extra-\n4314\nlong text. We ﬁnd that samples produced by GPT -2\nﬁne-tuned on small domain-speciﬁc corpora exhibit\nvarious imperfections, including excessive repet-\nitiveness and incoherence between sentences far\napart. Figure 1 measures the coherence of text gen-\nerated by the ﬁne-tuned GPT -2 w.r.t the BERT next\nsentence prediction (Devlin et al., 2019) score. As\nthe ﬁgure shows, GPT -2 models (regardless of the\nmodel size) exhibit a signiﬁcant gap in the score\ncompared with human text, hence falling short in\ngenerating coherent text.\nWe hypothesize that the problem is mainly\ncaused by the sequential generation order of the\nLMs, which makes global content planning of the\npassage difﬁcult, especially when the generated\ntext is long and contains thousands of words. One\ncould potentially adopt the recentplanning-then-\ngeneration or non-monotonic methods (Sec 2), yet\nthose methods either require specialized neural ar-\nchitectures that need costly retraining for each do-\nmain (Gu et al., 2019; Stern et al., 2019; Chan\net al., 2019; Fan et al., 2019), or rely on dedicated\nintermediate content plans (e.g., summaries, SRL\nlabels) (F a ne ta l ., 2019; Y a oe ta l ., 2019) with lim-\nited ﬂexibility and producing sub-optimal results\nas shown in our experiments.\nTo overcome the limitations, we introduce a new\nmethod for Progressive Generation of Text (Pro-\nGen). We observe that generation of some words\n(e.g., stop words) does not require many contexts,\nwhile other words are decisive and have long-term\nimpact on the whole content of the passage. Mo-\ntivated by this observation, our approach ﬁrst pro-\nduces a sequence of most informative words, then\nprogressively reﬁnes the sequence by adding ﬁner-\ngrained details in multiple stages, until completing\na full passage. The generation at each stage is\nconditioning on the output of the preceding stage\nwhich provides anchors and steers the current gen-\neration (Figure 2). The intermediate words pro-\nduced at each stage are deﬁned based on a simple\nTF-IDF informativeness metric.\nThe approach enjoys several core advantages:\n(1) Although the progressive approach implements\na conceptually non-monotonic generation process,\ngeneration at each stage can still be performed in\na left-to-right manner and thus is directly compati-\nble with the powerful pretrained monotonic LMs.\nThe LMs at different stages are easily ﬁne-tuned to\naccommodate a target domain using only small, in-\ndependently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to\na ﬁner-resolution one, which is much simpler than\nthe overall task of mapping from conditions to full\npassages of text. In this work, we use BART (Lewis\net al., 2020) for generation at each stage, though\none can also plug in other off-the-shelf LMs. As\nseen from Figure 1, ProGen can generate more\nmuch coherent text compared with GPT -2 and\nnearly match human text in terms of the BERT -\nNSP score;\n(2) In contrast to the typical 2-stage\nplanning-then-generation in prior work, the simple\nprogressive strategy offers added ﬂexibility for an\narbitrary number of intermediate stages, yielding\nimproved results;\n(3) The training data for each\nstage is extracted from domain corpus using the\nsimple TF-IDF metric, without need of additional\nresources (e.g., pretrained summarization models)\nas in prior work, making the method broadly appli-\ncable to various domains and languages.\nWe conduct extensive empirical studies on the\nCNN News (Hermann et al., 2015) and Writing-\nPrompts (Fan et al., 2018) corpora, evaluating vari-\nous systems by a wide-range of automatic metrics\nas well as human judgement. Results show that Pro-\nGen achieves strongly improved performance by\ndecomposing the generation into more progressive\nstages. Our method produces diverse text passages\nof higher quality and coherence than a broad set of\nmodels, including ﬁne-tuned GPT -2, BART, and\nother various planning-then-generation strategies.\n2 Related Work\nContent planning in generation. The idea of\nseparate content planning and surface realization\nhas been studied in early text generation sys-\ntems (Reiter and Dale, 1997). Recent neural ap-\nproaches have also adopted similar planning-then-\ngeneration strategies for data-to-text (Moryossef\net al. , 2019; Puduppully et al. , 2019), story-\ntelling (Fan et al., 2019; Y ao et al., 2019; Xu et al.,\n2020), machine translation (Ford et al., 2018), and\nothers (Hua and Wang, 2019; Y a oe ta l ., 2017).\nThese models often involve customized architec-\ntures incompatible with the existing large LMs.\nScaling those models for long text generation thus\ncan require expensive training, which restricts sys-\ntematic studies. On the other hand, it is possible to\nadopt some of the content planning strategies (e.g.,\nsummaries or SRL sequences as the plans (Fan\net al., 2019)), and repurpose pretrained LMs for\ngeneration in each stage. However, these strategies\n4315\nwith dedicated intermediate plans and a pre-ﬁxed\nnumber (typically 2) of stages can have limited\nﬂexibility, leading to sub-optimal results as shown\nin our empirical study. Besides, creating training\ndata for planning requires additional resources (e.g.,\npretrained summarization models or SRL models)\nwhich are not always available (e.g., in certain do-\nmains or for low-resource languages). In contrast,\nwe propose a simple way for designing the interme-\ndiate stages based on word informativeness, which\ncan ﬂexibly increase the number of stages for im-\nproved results, and easily create training data for\nall stages without additional models.\nNon-monotonic generation and reﬁnement.\nAnother relevant line of research is non-monotonic\ngeneration (Welleck et al., 2019; Gu et al., 2019;\nStern et al., 2019; Chan et al., 2019; Zhang et al.,\n2020), inﬁlling (Zhu et al., 2019; Shen et al., 2020;\nQin et al., 2020), or reﬁnement (Lee et al., 2018;\nNovak et al., 2016; Mansimov et al., 2019; Kasai\net al., 2020) that differs from the restricted left-to-\nright generation in conventional LMs. Again, those\napproaches largely depend on specialized architec-\ntures and inference, making them difﬁcult to be\nintegrated with the powerful pretrained LMs. The\nprior studies have focused on generating short text.\nOur proposed coarse-to-ﬁne progressive generation\nconceptually presents a non-monotonic process\nbuilt upon the pretrained monotonic LMs, which\npermits fast adaptation to any target domain and\ngeneration of much longer text.\nLong text generation. Previous work has made\nattempts to generate text of up to two or three hun-\ndred tokens. Those methods often adopt the similar\nidea of planning-then-generation as above (Shen\net al., 2019; Zhao et al., 2020; Bosselut et al., 2018;\nSee et al., 2019; Hua and Wang, 2020; Rashkin\net al., 2020). Another line of work instead focuses\non extending the transformer architecture (V aswani\net al., 2017) to model longer text sequences (e.g.,\nDai et al., 2019; Wang et al., 2020; Choroman-\nski et al. , 2021, etc). For example, Liu et al.\n(2018) used a hybrid retrieval-generation archi-\ntecture for producing long summaries; Dai et al.\n(2019) showed long text samples qualitatively. Our\nwork systematically examines the pretrained LMs\nin generating long domain-speciﬁc text, and pro-\nposes a new approach that empowers pretrained\nLMs for producing samples of signiﬁcantly higher-\nquality.\n3 Progressive Generation of T ext\nOne of the main challenges in generating long co-\nherent passages is modeling long-range dependen-\ncies across the entire sequences (e.g., 1000 tokens).\nWe propose a progressive generation approach that\nis conceptually simple yet effective. Intuitively,\nprogressive generation divides the complex prob-\nlem of generating the full passage into a series of\nmuch easier steps of generating coarser-grained\nintermediate sequences. Contrary to generating\neverything from left to right from scratch, our pro-\ngressive generation allows the model to ﬁrst plan\nglobally and then shift attention to increasingly\nﬁner details, which results in more coherent text.\nFigure 2 illustrates the generation process.\n3.1 Generation Process\nLet y := [y1,y 2,...,y T ] be the output text, where\neach yi is a token of language (a word or a sub-\nword). The output sequences are generated either\nconditionally on any other informationx (e.g., gen-\nerations of a story given a prompt), or uncondi-\ntionally (in which case we assumex ≡ ∅ while\nkeeping the same notation).\nInstead of generating the full passagey directly,\nwe propose to add multiple intermediate stages:\nx → c1 → c2 ··· → cK → y, where for each\nstage k ∈{ 1,...,K }, ck is an intermediate se-\nquence containing information of the passage at\ncertain granularity. For instance, at the ﬁrst stage,\nc1 can be seen as a highest-level content plan con-\nsisting of the most informative tokens such as key\nentities. Then, based on the plan, we gradually\nreﬁne them into subsequentck, each of which con-\ntains ﬁner-grained information than that of the pre-\nceding stage. At the ﬁnal stage, we reﬁnecK into\nthe full passage by adding the least informative\nwords (e.g., stop words). The generation process\ncorresponds to a decomposition of the conditional\nprobability as:\nP(y, {ck}|x)= P(c1|x)\nΠK\nk=2P(ck|ck−1, x)P(y|cK , x). (1)\nAs the above intuition, ck at early stages as the\nhigh-level content plans should contain informa-\ntive or important words, to serve as skeletons for\nsubsequent enrichment.\nWe next concretely deﬁne the order of genera-\ntion, namely, which words should each stage gen-\nerates. Speciﬁcally, we propose a simple method\n4316\nCondition\njeep dog \nbarking ofﬁcer \nskinny jeep \nsandy …\nshouted  jeep  dog  \ncircles vehicle barking \nofﬁcer yellow skinny  \nanimal circling jeep \nspit vehicle tumbling \nrough sandy adjusting \ngun proceeded canine \ndog barking        …\nLM1`` Shut the dog up ,’’\nshouted my head ofﬁcer from the jeep . The dog was \nrunning circles around our vehicle , barking at the people \ninside . The ofﬁcer tapped my shoulder and pointed to the \nyellow , skinny animal circling our jeep .  “ But sir.. , ” I \nmanaged to spit out before he took both his hands and \npushed me out of the vehicle . I went tumbling out , and \nlanded on the rough sandy ground . I stood up adjusting the \ngun hanging from my shoulder and proceeded to walk \ntowards the canine . The dog stopped its barking , and \nshifted its black eyes to me .  “ Come here little pup . Hey \ncome here , I ’ m not going to hurt ya , ” I said trying to coax \nit nearer to me . Actually , I didn ’ t know if I was going to \nhurt the little mutt or not yet . Reaching my hand towards \nmy waist , I pulled off a tiny bit of my rations . I held it out \nmy hand , with the ration laying on my open palm . The dog \nperked it ’ s ears , and came a few inches closer to me. […]\n…\nGenerationProgressive Generation of Text\nx yc1 c2 …\nLM2 LMK\nFigure 2: Progressive generation of long texty given any conditionx. Each stage reﬁnes the results from the previous stage by\nadding ﬁner-grained details. Added content at each stage is highlighted in different colors.\nthat constructs a vocabularyVk for each stage k,\nbased on the importance of words in the target\ndomain. Each particular stage k only produces\ntokens belonging to its vocabularyVk. By the pro-\ngressive nature of the generation process, we have\nV1 ⊂ ··· ⊂ VK ⊂V . That is, V1 contains the\nsmallest core set of words in the domain, and the\nvocabularies gradually expand at later stages until\narriving the full vocabularyV. Note that vocabular-\nies in later stages are supersets of those in earlier\nstages. This allows the later stages to remedy and\npolish potential mistakes made in earlier stages\nwhen necessary. We discuss the construction of the\nvocabularies in the below.\nStage-wise vocabularies based on word impor-\ntance. Given a text corpusDof the target domain\nwith the full vocabulary V, we deﬁne the impor-\ntance scores of words inV based on the TF-IDF\nmetric. We then rank all the words and assign the\ntop Vk words to the intermediate vocabulary Vk.\nHere Vk is a hyper-parameter controlling the size\nof Vk.\nMore concretely, for each wordw ∈V , we ﬁrst\ncompute its standard TF-IDF score (Salton and\nMcGill, 1986) in each document d ∈D , which\nessentially measures how importantw is to d. The\nimportance of the word w in the domain is then\ndeﬁned as the average TF-IDF score across all doc-\numents containing w:\nimportance(w, D)=\n∑\nd∈D TF_IDF(w, d)\nDF(w, D) , (2)\nwhere TF_IDF(w, d)is the TF-IDF score of word\nw in document d; and DF(w, D) is the document\nAlgorithm 1 Training for Progressive Text Generation\nInputs:\nDomain corpus D\nV ocabulary sizes forK stages\nK pretrained LMs (e.g. GPT -2 or BART)\n1: Construct stage-wise vocabularies{Vk} based on word\nimportance Eq.(2)\n2: Extract intermediate sequences {c∗\nk} using {Vk}; add\ndata noises (Sec3.2)\n3: Fine-tune all LMs independently (Sec3.2)\nOutput: Fine-tuned LMs for generation at all stages in a\nprogressive manner\nfrequency, i.e., the number of documents in the\ncorpus that contain the wordw.\nPretrained language models as building blocks.\nCompared to many of the previous planning-then-\ngeneration and non-monotonic generation methods,\none of the key advantages of our progressive gen-\neration design is the direct compatibility with the\npowerful pretrained LMs that perform left-to-right\ngeneration. Speciﬁcally, although our approach im-\nplements a non-monotonic generation process that\nproduces importance words ﬁrst, we can generate\nintermediate sequences ck at each stage still in a\nleft-to-right manner. Thus, we can plug pretrained\nLM, such as GPT -2 or BART, into each stage to\ncarry out the generation. As described more in\nsection 3.2, for each stagek, we can conveniently\nconstruct stage-speciﬁc training data from the do-\nmain corpus D using the stage-wise vocabulary\nVk, and ﬁne-tune the stage-k LM in order to gen-\nerate intermediate sequences at the stage that are\npertaining to the target domain.\nOne can add masks on the pretrained LM’s to-\n4317\nken distributions to ensure the stage-k LM only\nproduces tokens belonging toVk. In practice, we\nfound it is not necessary, as the pretrained LM\ncan usually quickly learns the pattern through ﬁne-\ntuning and generate appropriate tokens during in-\nference. In our experiments we use BART for all\nstages, since BART is an encoder-decoder model\nwhich can conveniently take as inputs the resulting\nsequence from the preceding stage and generate\nnew. (For the ﬁrst stage in an unconditional genera-\ntion task, we simply set\nx = ∅.) We note that GPT -\n2, and other relevant pretraiened LMs, can indeed\nalso be used as a conditional generator (Radford\net al., 2019; Liu et al., 2018) and thus be plugged\ninto any of stages.\n3.2 Training\nOur approach permits straightforward training/ﬁne-\ntuning of the (pretrained) LMs at different stages\ngiven the domain corpusD. In particular, we can\neasily construct independent training data for each\nstage, and train all LMs in parallel. Note that no\nadditional resources such as pretrained summariza-\ntion or semantic role labeling models are requested\nas in previous work, making our approach directly\napplicable to a potentially broader set of domains\nand languages. We plan to explore the use of our\nmethod in multi-lingual setting in the future.\nMore concretely, for each stagek, we use the\nstage vocabularies Vk−1 and Vk to ﬁlter all rel-\nevant tokens in the documents as training data.\nThat is, given a document, we extract the sub-\nsequence c∗\nk−1 of all tokens from the document\nthat are belonging to Vk−1, and similarly extract\nsub-sequence c∗\nk belonging to Vk. The c∗\nk−1\nand\nc∗\nk are then used as the input and the ground-truth\noutput, respectively, for training the LM at stage\nk with maximum likelihood learning. Therefore,\ngiven the stage-wise vocabularies{Vk}, we can au-\ntomatically extract training data from the domain\ncorpus D for different stages, and train the LMs\nseparately.\nIn the multi-stage generation, the intermediate\nsequences are not natural language. Y et we found\nthat ﬁne-tuning pretrained LMs (such as BART and\nGPT -2) to generate the intermediate sequences is\nindeed very efﬁcient in terms of data and computa-\ntion. We tried training other models such as small\nsequence-to-sequence models and n-gram models\nfrom scratch, which we found is much harder, re-\nquiring more data, or yielding inferior performance.\nThis again highlights the importance of using pre-\ntrained LMs, as enabled by our simple method\ndesign.\nStage-level exposure bias and data noising.In\nthe above training process, the outputs of each\nLM are conditioning on the ground-truth input se-\nquences extracted from the real corpus. In contrast,\nat generation time, the LM takes as inputs the im-\nperfect sequences produced at the previous stage,\nwhich can result in new mistakes in the outputs\nsince the LM has never be exposed to noisy inputs\nduring training. Thus, the discrepancy between\ntraining and generation can lead to mistakes in gen-\neration accumulating through the stages. The phe-\nnomenon resembles theexposure biasissue (Ran-\nzato et al., 2016) of sequential generation models\nat token level, where the model is trained to predict\nthe next token given the previous ground-truth to-\nkens, while at generation time tokens generated by\nthe model itself are instead used to make the next\nprediction.\nTo alleviate the issue and increase the robustness\nof each intermediate LM, we draw on the rich liter-\nature of addressing token-level exposure bias (Xie\net al., 2017; T a ne ta l ., 2019). Speciﬁcally, during\ntraining, we inject noise into the ground-truth in-\nputs at each stage by randomly picking ann-gram\n(n ∈{ 1, 2, 3, 4}) and replacing it with another ran-\ndomly sampled n-gram. The data noising encour-\nages the LMs to learn to recover from the mistakes\nin inputs, leading to a more robust system during\ngeneration.\n4 Experiments\n4.1 Setup\nDomains. We evaluate on two text generation do-\nmains including: (1) CNN News(Hermann et al.,\n2015) for unconditional generation. (2) Writing-\nPrompts (Fan et al., 2018) for conditional story\ngeneration. The task is to generate a story given\na prompt. The two datasets are chosen since they\nboth contain long documents, with CNN’s average\nand maximum length being 512 and 926, and Writ-\ningPrompts’s being 437 and 942, respectively. To\ndemonstrate the data efﬁciency of our approaches\nadapting to target domains, we sample 1,000 docu-\nments in each dataset for training.\nModel conﬁgs. We use BARTs for all stages of\ngeneration. Due to computation limitations, we ex-\nperiment models with 2, 3, 4-stages generations. In\n4318\nour 2-stage model, our ﬁrst stage covers about 25%\nof all content; in the 3-stage model, the ﬁrst and\nsecond stages cover 15% and 25% of all content,\nrespectively; and in the 4-stage model, our ﬁrst\nthree stages cover 15%, 20%, 25% of all content.\nFor model training, we follow the same protocol as\n(See et al., 2019) to ﬁne-tune all pretrained mod-\nels until convergence. To combat exposure bias,\nwe add noise to the training data as described in\nSec 3.2, with the probability of replacing 1,2,3,4-\ngrams 0.1/0.05/0.025/0.0125. In the generation\nphase, we use top-p decoding ( Holtzman et al.,\n2020) with\np =0 .95 to generate 1024 tokens\nat maximum. Experiments were conducted with\nRTX6000 GPUs. It took around 4 hours for model\nﬁne-tuning and generation with a single GPU.\nComparison methods. We compare with a wide\nrange of baselines, categorized into two groups:(1)\nThe large pretrained LMs including BART (Lewis\net al., 2020) and GPT -2 in both small and large\nsizes (Radford et al., 2019). The LMs generate text\nin a standard left-to-right manner;(2) Progressive\ngeneration with various strategies adopted in the\nprior planning-then-generation work. Same as our\nproposed method, each stage adapts a pretrained\nBART for generation. Speciﬁcally,Summary ﬁrst\ngenerates a short summary text as the content plan\nand conditioning on the summary produces the full\npassage of text (F a ne ta l ., 2019). For training,\nsummaries are obtained using the state-of-the-art\npretrained CNN news summarization model based\non BART;Keyword ﬁrst generates a series of key-\nwords, based on which the full text is generated\nin the next stage. Following ( Y ao et al., 2019),\nthe keywords are extracted with the RAKE algo-\nrithm (Rose et al., 2010) for training;SRL follows\nthe recent work (F a ne ta l ., 2019) by ﬁrst generating\na sequence of predicates and arguments and then\nproducing the full text conditionally. The same\nsemantic role labeling tool as in the prior work is\nused here to create training data.\nSRL+NER and\nSRL+Coref further augment the SRL method by\nan additional stage of generating entity anonymized\ntext conditioning on the predicates sequence prior\nto the ﬁnal stage (F a ne ta l ., 2019). SRL+NER\nuses an NER model to mask all entities, while\nSRL+Coref applies coreference resolution to mask\nall clusters of mentions. We use the same NER\nand coreference tools as in (Fan et al., 2019). Fi-\nnally, as a reference, we also present the results of\nHuman-written text (i.e., the text in the dev set).\n4.2 Automatic Evaluation\n4.2.1 Evaluation Metrics\nTo evaluate the generation quality for the domain-\nspeciﬁc open-ended generation as studied here, we\nprimarily measure the “closeness” between two\nsets of text, one generated by the model and the\nother the real text from the target domain. We eval-\nuate with a broad array of automatic metrics, in-\ncluding lexical-based qualitymetrics and semantic-\nbased qualitymetrics. We also evaluate the genera-\ntion diversity.\nMS-Jaccard (MSJ) is a lexical-based metric\n(Montahaei et al., 2019), where MSJ-n measures\nthe similarity ofn-grams frequencies between two\nsets of text with Jaccard index.\nTF-IDF Distance (TID) is deﬁned as the dis-\ntance between the average TF-IDF features of two\ntext sets. We use it as an additional lexical-based\nquality measure.\nFréchet BERT Distance (FBD)\nis a semantic-\nbased metric (Montahaei et al., 2019) that measures\nthe Fréchet Distance in the BERT feature space be-\ntween the generated and real text. By using the\nBERT features from shallow (S), medium (M), and\ndeep (D) layers, we can compute FBD-S/M/D, re-\nspectively.\nBackward BLEU (B-BLEU) is a diversity met-\nric (Shi et al., 2018) measuring how well the gener-\nated text covers n-grams occurred in the test set.\nHarmonic BLEU (HA-BLEU) (Shi et al., 2018)\nis an aggregated quality and diversity metric that in-\ncorporates both the standard BLEU (i.e., precision)\nand the Backward BLEU (i.e., recall).\n4.2.2 Results\nFigures 3 and 4 show the results of the various sys-\ntems on the news and story domains, respectively,\nmeasured with different metrics against test set. We\ngive more complete results in the appendix. We\ncan see that our progressive generation approach\nconsistently outperforms the standard, single-stage\nLMs (GPT2-Small, GPT2-Large and BART)\nby a large margin on almost all metrics in both\ndomains. Further, by increasing the number of pro-\ngression stages, our method steadily achieves even\nstronger performance. This highlights the beneﬁts\nof the ﬂexible progressive generation strategy.\nThe various models using pretrained LMs with\nprevious planning-then-generation strategies show\n4319\n13.5\n14.5\n15.5\n16.5\n17.5\n18.5\nBaselines Ours\nHuman\n25\n27\n29\n31\n33\nBaselines Ours\nHuman\nLexical-based quality metrics Semantic-based quality metric\nDiversity\n25\n26\n27\n28\n29\n30\n31\n32\nBaselines Ours\nHuman\nAggregated diversity and quality\nHA-BLEU4B-BLEU4\nMSJ-4 TID FBD-D\n0\n20\n40\n60\nBaselines                      Ours\nHuman\n \n0\n4\n8\n12\n16\nBaselines                          Ours\nHuman\nFigure 3: Results on the CNN News domain measured by different metrics. For TID and FBD, the lower value the better. More\nresults (MSJ-n, B-BLEUn and HA-BLEUn with different n values, and FBD-S/M) are included in the appendix. The three\nsets of comparison methods are shown in different colors, with ourProGen in red, standard large LMs inblue, and the various\nmodels with previous planning strategies ingreen. Human results are shown as dashed lines, often indicating the best potential\nperformance (except for the diversity related metrics).\n30\n31\n32\n33\n34\n35\n36\n37\nBaselines                  Ours\nHuman\nHA-BLEU4\n15\n16\n17\n18\n19\n20\n21\n22\nBaselines      Ours\nHuman\nMSJ-4 FBD-D\nLexical-based quality metrics Semantic-based qua lity metric Aggregated diversity and quality\n0\n10\n20\n30\n40\n50\n60\nBaselines                          Ours\nHuman\nFigure 4: Results on the story domain measured by different metrics. More complete results are in appendix.\nmixed results across the different metrics. For ex-\nample, Summary achieves strong performance in\nterms of the semantic-based quality metric FBD-D\n(partially because the summaries are closer to the\nreal text in the BERT feature space), but signiﬁ-\ncantly falls behind other models in terms of diver-\nsity (B-BLEU4) and other quality metrics like MSJ\nand HA-BLEU. Similarly, theSRL-based methods\ngive only mediocre results in terms of the semantic-\nbased FBD-D. In contrast, our approach maintains\na relatively consistent performance level. In par-\nticular, our 4-stage model,ProGen-4, is steadily\namong the best across all metrics, further validating\nFluency Coherence\npassage sentence (%)\nBART 4.59 2.95 70.29\nGPT2-Small 4.42 3.41 74.69\nSummary 4.39 3.37 76.19\nProGen-4 (Ours) 4.46 3.83 86.22\nTable 1: Human evaluation results on CNN.\nthe advantage of the proposed simple yet ﬂexible\nmulti-stage generation.\nThese results also indicate the necessity of using\na large diverse set of automatic metrics for a com-\nprehensive evaluation, and motivate human studies\nfor further assessment.\n4320\nFigure 5: Sample efﬁciency on the\nstory domain with the FBD metric\n(the lower, the better).\nFBD-D ↓ MSJ-4 ↑ HA-BL4 ↑\nProGen-2 39.94 16.50 30.45\n-Noise 47.18 16.25 31.39\nProGen-3 38.30 16.68 30.64\n-Noise 39.64 16.65 30.72\nProGen-4 36.49 16.96 31.32\n-Noise 39.78 16.85 30.86\nTable 2: Effect of noise on CNN.\nFBD-D ↓ TID ↓\nProGen-2 39.94 6.2\nGoldPlan 30.16 3.5\nHuman 25.63 2.6\nTable 3: GoldPlan Results on CNN.\nFirst Stage: beckham \\n liverpool bayern chelsea\\n beckham chelsea mancini\\n liverpool \\n club [...]\nSecond Stage: beckham newcastle \\n footballer acquisition transfer liverpool bayern munich chelsea munich \\n beckham\nchelsea season italian siro gianluigimancini season italy \\n season liverpool season \\n wembley club \\n [...]\nFinal Stage:\n(CNN) – England striker Ashley Beckham has joined Premier League strugglers Newcastle United. \\n England Football\nAssociation chairman Stephen McClaren conﬁrmed theacquisition of the 20-year-old England international on Wednesday,\nmaking the move ofﬁcial following a lucrativetransfer agreement with Liverpool, Bayern Munich and Chelsea.\\n Beckham\nspent the ﬁrst half of this year on loan atChelsea and did not start a single match during theseason at the Italian capital’s San\nSiro despite being named the player of theseason of the year byItaly. \\n However, he played 17 times lastseason and 12\ntimes this season forLiverpool, making 33 league appearances and scoring seven goals.\\n He is currently third on the all-time\nEngland goalscoring list behind only England manager Alex Ferguson and newclub teammate Paul Gascoigne. [...]\nTable 4: An excerpt of a 3-stage generated example by ProGen-3 on the CNN News domain.\n4.3 Human Evaluation\nIn our human study, we asked three university stu-\ndents who are proﬁcient English speakers to eval-\nuate the coherence and ﬂuency of the generated\ntext. To better assess the coherence of the long\npassages of text, we evaluate at both the passage\nlevel and the ﬁner-grained sentence level. More\nconcretely, for\npassage-level coherence, human\nraters assign a coherence score to each full-length\ntext sample, on a 5-point Likert scale. For a more\ndetailed assessment, we further evaluatesentence-\nlevel coherence, where human raters label each\nsentence in the text passage with 0 or 1, indicating\nwhether the particular sentence is coherent with the\nproceeding context in the passage. We then calcu-\nlate the average percentage of coherent sentences\nin the generated text by each model. Human raters\nalso evaluate the language quality for a ﬂuency\nscore on a 5-point Likert scale. We compare our\nmethod with the systems that show highest gen-\neration quality in automatic evaluation, including\nBART, GPT2-Small, and Summary. We evalu-\nated 50 examples for each comparison model on\nthe CNN domain. The Pearson correlation coefﬁ-\ncient of human scores is 0.52, showing moderate\ninter-rater agreement.\nTable 1 shows the results. All systems receive\nclose ﬂuency scores. Our approach obtained signif-\nicantly higher coherence scores at both passage and\nsentence levels. In particular, over 86% sentences\nin our model generations are considered as coher-\nent with the context, improving over other models\nby at least 10 absolute percent.\n4.4 Ablation Study and Analysis\nSample efﬁciency. We study how the progres-\nsive generation could improve the sample efﬁciency\nof large LMs ﬁne-tuned to target domains. The\nintuition is that by focusing on the subsets of in-\nformative words, the early stages can more efﬁ-\nciently capture the domain-speciﬁc characteristics\nand then steer the subsequent reﬁnement stages.\nFigure 5 shows the results where we report the\nFBD score averaged over FBD-S/M/D. We can see\nour approach can make more efﬁcient use of the\ntraining data in learning to generate high quality\nsamples. For example, with only 1K training exam-\nples, our method achieves comparable results with\nlarge LMs trained on 30K examples.\nGeneration with gold plans. To investigate the\nimportance of dividing the generation process into\nstages and what the stages learn separately, we add\nanother set of text into our comparison. It is a 2-\nstages model whose ﬁrst stage is the ground truth\n(gold plan) while the second stage kept the same\n(a BART model), shown asGoldPlan in Table3.\nNote that with gold plan, our model greatly de-\ncreases the gap with human text in terms of lexical\n(TID) and semantic (FBD-D) quality metrics. The\nresults highlight the importance of plans in text\n4321\ngeneration. The intermediate plans act as an in-\nformation bottleneck, and high-quality plans could\nlead to high-quality text generation.\nEffect of data noising. We study the ablation of\ndata noising, to check whether the noising opera-\ntion really helps reduce stage-wise exposure bias\n(Sec 3.2) as we expected. Table2 shows the com-\nparison between models with and without noise in\ntraining. The added noise generally brings perfor-\nmance improvement in terms of various metrics.\nExample generations. Table 4 shows an exam-\nple of text generated via three stages. We can see\nour model ﬁrst generates the key subjectbeckham\nand the team nameliverpool in the very ﬁrst stage,\nthen adds more ﬁne-grained details likeacquisition,\ntransfer in the second stage and ﬁnally expands\nthe keywords into a full document describing Beck-\nham’s joining a new team.\n5 Conclusion\nWe have proposed a new approach for domain-\nspeciﬁc generation of long text passages in a pro-\ngressive manner. Our method is simple and efﬁ-\ncient by ﬁne-tuning large-scale off-the-shelf lan-\nguage models. We conduct extensive experiments\nusing a variety of metrics and human studies. We\ndemonstrate that our method outperforms a wide\nrange of large pretrained LMs with single-stage\ngeneration or prior planning-then-generation strate-\ngies, in terms of quality and coherence of the pro-\nduced samples. The multi-stage generation also\nopens up new opportunities to enhance controlla-\nbility of text generation, which we would love to\nexplore in the future.\nReferences\nAntoine Bosselut, Asli Celikyilmaz, Xiaodong He,\nJianfeng Gao, Po-Sen Huang, and Y ejin Choi. 2018.\nDiscourse-aware neural rewards for coherent text\ngeneration. In NAACL, pages 173–184.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners.I n NeurIPS, volume 33, pages 1877–1901.\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\native insertion-based modeling for sequences.arXiv\npreprint arXiv:1906.01604.\nKrzysztof Choromanski, V alerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2021. Rethinking attention\nwith performers. ICLR.\nZihang Dai, Zhilin Y ang, Yiming Y ang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context.I n ACL, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.I n NAACL, pages 4171–4186.\nAngela Fan, Mike Lewis, and Y ann Dauphin. 2018.Hi-\nerarchical neural story generation.I n ACL, pages\n889–898.\nAngela Fan, Mike Lewis, and Y ann Dauphin. 2019.\nStrategies for structuring story generation. InACL.\nNicolas Ford, Daniel Duckworth, Mohammad Norouzi,\nand George E Dahl. 2018. The importance of gener-\nation order in language modeling. InEMNLP.\nGardian. A robot wrote this entire article. are you\nscared yet, human?\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019.\nInsertion-based decoding with automatically in-\nferred generation order.TACL, 7:661–676.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NeurIPS, pages 1693–1701.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Y ejin\nChoi. 2020. The curious case of neural text degener-\nation. In ICLR.\nXinyu Hua and Lu Wang. 2019. Sentence-level con-\ntent planning and style speciﬁcation for neural text\ngeneration. In EMNLP.\nXinyu Hua and Lu Wang. 2020. P AIR: Planning and\niterative reﬁnement in pre-trained transformers for\nlong text generation. InEMNLP, pages 781–793.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer .I n\nICML.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural\nsequence modeling by iterative reﬁnement .I n\nEMNLP, pages 1173–1182.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, V eselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension.I n ACL, pages 7871–7880.\n4322\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. InICLR.\nElman Mansimov, Alex Wang, Sean Welleck, and\nKyunghyun Cho. 2019. A generalized framework of\nsequence generation with application to undirected\nsequence models. arXiv preprint arXiv:1905.12790.\nMarketMuse. Gpt-3 exposed: Behind the smoke and\nmirrors.\nEhsan Montahaei, Danial Alihosseini, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diver-\nsity and quality in text generation models. NAACL\nWorkshop.\nAmit Moryossef, Y oav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. InNAACL.\nRoman Novak, Michael Auli, and David Grangier.\n2016. Iterative reﬁnement for machine translation.\narXiv preprint arXiv:1610.06602.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with content selection and\nplanning. In AAAI, volume 33, pages 6908–6915.\nLianhui Qin, V ered Shwartz, Peter West, Chandra Bha-\ngavatula, Jena D. Hwang, Ronan Le Bras, Antoine\nBosselut, and Y ejin Choi. 2020.Back to the future:\nUnsupervised backprop-based decoding for counter-\nfactual and abductive commonsense reasoning.I n\nEMNLP, pages 794–805. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.OpenAI\nBlog, 1(8):9.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks.ICLR.\nHannah Rashkin, Asli Celikyilmaz, Y ejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state\ntracking.I n EMNLP, pages 4274–4295.\nEhud Reiter and Robert Dale. 1997. Building applied\nnatural language generation systems. Natural Lan-\nguage Engineering, 3(1):57–87.\nStuart Rose, Dave Engel, Nick Cramer, and Wendy\nCowley. 2010. Automatic keyword extraction from\nindividual documents. Text mining: applications\nand theory, 1:1–20.\nGerard Salton and Michael J McGill. 1986. Introduc-\ntion to modern information retrieval.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nY erukola, and Christopher D. Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? In CoNLL, pages 843–861.\nDinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun\nChen, Xin Wang, Jianfeng Gao, and Lawrence Carin.\n2019. Towards generating long and coherent text\nwith multi-level latent variable models. In ACL,\npages 2079–2089.\nTianxiao Shen, Victor Quach, Regina Barzilay, and\nTommi Jaakkola. 2020. Blank language models.I n\nEMNLP, pages 5186–5198.\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing\nHuang. 2018. Toward diverse text generation with\ninverse reinforcement learning.IJCAI.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible\nsequence generation via insertion operations .I n\nICML, volume 97, pages 5976–5985.\nBowen Tan, Zhiting Hu, Zichao Y ang, Ruslan Salakhut-\ndinov, and Eric P Xing. 2019. Connecting the dots\nbetween mle and rl for sequence generation. ICLR\nWorkshop.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998–6008.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nSean Welleck, Kianté Brantley, Hal Daumé III, and\nKyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. In ICML.\nZiang Xie, Sida I Wang, Jiwei Li, Daniel Lévy, Aiming\nNie, Dan Jurafsky, and Andrew Y Ng. 2017. Data\nnoising as smoothing in neural network language\nmodels. In ICLR.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Anima Anandkumar, and Bryan\nCatanzaro. 2020. MEGA TRON-CNTRL: Control-\nlable story generation with external knowledge us-\ning large-scale language models.I n EMNLP, pages\n2831–2845.\nLili Y ao, Nanyun Peng, Ralph Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Y an. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nAAAI, volume 33, pages 7378–7385.\nLili Y ao, Y aoyuan Zhang, Y ansong Feng, Dongyan\nZhao, and Rui Y an. 2017. Towards implicit content-\nintroducing for generative short-text conversation\nsystems. In EMNLP, pages 2190–2199.\nYizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe\nGan, Chris Brockett, and Bill Dolan. 2020.\nPOINTER: Constrained progressive text genera-\ntion via insertion-based generative pre-training.I n\nEMNLP, pages 8649–8670.\n4323\nLiang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang,\nHongxia Y ang, and Xu Sun. 2020. Graph-based\nmulti-hop reasoning for long text generation.arXiv\npreprint arXiv:2009.13282.\nWanrong Zhu, Zhiting Hu, and Eric Xing. 2019. Text\ninﬁlling. arXiv preprint arXiv:1901.00158.\n4324\nAppendix: Complete Results\nWe include complete result numbers of experiments here.\nGPT2-S GPT2-L BART Summ. RAKE SRL SRL-N SRL-C ProGen-2 ProGen-3 ProGen-4 Dev\nB-BL2 72.84 71.89 71.51 73.28 69.78 70.25 74.50 74.71 72.25 74.10 74.57 75.82\nB-BL3 48.53 47.48 47.55 49.26 45.39 46.54 51.19 51.40 48.44 50.38 51.06 52.08\nB-BL4 28.64 28.55 28.11 29.31 26.09 27.25 31.04 31.06 28.88 30.32 30.96 32.29\nB-BL5 15.87 15.62 15.57 16.35 14.01 14.88 17.58 17.41 16.08 17.09 17.53 19.35\nHA-BL2 73.61 71.97 74.56 74.59 71.63 67.47 74.51 75.11 74.64 75.17 75.86 75.72\nHA-BL3 49.26 47.83 50.27 50.32 47.34 44.51 50.87 51.18 50.64 51.07 51.88 52.01\nHA-BL4 29.21 28.26 30.03 29.88 27.51 25.84 30.45 30.49 30.45 30.64 31.32 32.28\nHA-BL5 16.22 15.77 16.77 16.52 14.84 13.91 16.94 16.87 17.09 17.18 17.63 19.40\nMSJ-2 49.24 46.94 49.85 46.97 44.19 43.85 49.39 44.37 49.46 50.16 51.00 54.51\nMSJ-3 28.79 27.29 29.43 27.99 26.01 25.90 29.58 26.92 29.54 30.04 30.56 32.54\nMSJ-4 15.73 14.85 16.24 15.48 14.12 14.15 16.33 14.99 16.50 16.68 16.96 18.60\nMSJ-5 8.38 7.91 8.72 8.25 7.36 7.43 8.68 8.02 8.90 8.95 9.10 10.87\nTID 8.7 9.2 6.8 4.5 7.8 16.1 5.2 5.2 6.2 5.4 4.0 2.6\nFBD-S 16.21 18.50 7.76 2.93 4.17 14.26 11.42 4.66 3.26 3.16 2.64 5.98\nFBD-M 24.92 29.61 22.49 15.00 25.92 37.24 22.63 20.28 19.05 18.84 17.38 12.26\nFBD-D 43.07 44.15 44.86 33.08 54.12 64.83 43.26 44.34 39.94 38.30 36.49 25.63\nTable 5: Complete results on the CNN News domain.\nGPT2-S GPT2-L BART Summ. RAKE SRL SRL-N SRL-C ProGet-2 ProGet-3 ProGet-4 Dev\nB-BL2 78.38 77.43 76.96 77.19 76.97 77.98 77.90 77.62 78.64 78.73 78.41 79.20\nB-BL3 55.51 54.18 54.45 54.45 53.86 55.67 55.49 55.09 56.44 56.50 56.25 56.02\nB-BL4 33.41 32.20 33.02 32.88 31.95 33.83 33.75 33.36 34.46 34.62 34.52 34.08\nB-BL5 17.59 16.79 17.55 17.53 16.47 17.93 17.98 17.63 18.32 18.49 18.57 18.40\nHA-BL2 78.19 76.96 79.99 79.30 77.19 79.24 77.73 77.46 80.57 80.72 80.50 79.51\nHA-BL3 55.39 54.33 57.86 56.83 54.71 57.00 55.71 55.14 58.11 58.38 58.35 56.39\nHA-BL4 33.32 32.52 35.63 34.63 32.70 34.63 33.93 33.36 35.43 35.84 35.96 34.36\nHA-BL5 17.46 16.94 19.16 18.47 16.86 18.26 18.03 17.60 18.72 19.14 19.30 18.55\nMSJ-2 55.27 54.21 55.89 52.63 51..88 47.51 45.39 43.36 55.14 56.51 56.18 60.07\nMSJ-3 34.48 33.70 35.46 33.46 32.59 30.88 29.51 28.22 34.81 35.80 35.74 37.42\nMSJ-4 19.32 18.83 20.27 19.17 18.33 17.87 17.11 16.39 19.63 20.29 20.39 21.22\nMSJ-5 10.16 9.90 10.73 10.27 9.57 9.54 9.21 8.82 10.16 10.60 10.76 11.34\nTID 4.6 8.3 5.1 4.5 5.8 5.5 5.3 7.0 5.1 5.0 4.8 3.4\nFBD-S 3.49 3.43 5.34 5.06 8.28 6.03 7.49 8.63 3.72 3.90 3.81 1.96\nFBD-M 19.30 19.41 21.75 18.11 22.97 21.85 23.15 25.01 19.36 19.04 18.62 12.23\nFBD-D 40.18 41.22 43.97 33.90 44.32 43.63 45.87 48.92 39.82 39.05 38.68 28.82\nTable 6: Complete results on the story domain."
}