{
    "title": "Neural Speech Synthesis with Transformer Network",
    "url": "https://openalex.org/W2903739847",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2891909358",
            "name": "Naihan Li",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2118325933",
            "name": "Shujie Liu",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2122251602",
            "name": "Yan-qing Liu",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2029189534",
            "name": "Sheng Zhao",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2098006703",
            "name": "Ming Liu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2891909358",
            "name": "Naihan Li",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2122251602",
            "name": "Yan-qing Liu",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2029189534",
            "name": "Sheng Zhao",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2098006703",
            "name": "Ming Liu",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "University of Electronic Science and Technology of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1599623585",
        "https://openalex.org/W2148228080",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W6623517193",
        "https://openalex.org/W2120847449",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W2150658333",
        "https://openalex.org/W2587284713",
        "https://openalex.org/W6756197946",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W2766557690",
        "https://openalex.org/W2154920538",
        "https://openalex.org/W6676641785",
        "https://openalex.org/W2168510624",
        "https://openalex.org/W1991133427",
        "https://openalex.org/W2102003408",
        "https://openalex.org/W6679146927",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2111284386",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2604184139",
        "https://openalex.org/W2962778134",
        "https://openalex.org/W854541894",
        "https://openalex.org/W2804078698",
        "https://openalex.org/W2901997113",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2584032004",
        "https://openalex.org/W2519091744",
        "https://openalex.org/W4298857617",
        "https://openalex.org/W2129142580",
        "https://openalex.org/W4294619240",
        "https://openalex.org/W2963975282",
        "https://openalex.org/W2963850025",
        "https://openalex.org/W2964243274",
        "https://openalex.org/W4301368689"
    ],
    "abstract": "Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).",
    "full_text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nNeural Speech Synthesis with Transformer Network\nNaihan Li,∗1,4 Shujie Liu,2 Yanqing Liu,3 Sheng Zhao,3 Ming Liu1,4\n1University of Electronic Science and Technology of China\n2Microsoft Research Asia\n3Microsoft STC Asia\n4CETC Big Data Research Institute Co.,Ltd, Guizhou, China\nlnhzsbls1994@163.com\nshujliu, yanqliu, szhao@microsoft.com\ncsmliu@uestc.edu.cn\nAbstract\nAlthough end-to-end neural text-to-speech (TTS) methods\n(such as Tacotron2) are proposed and achieve state-of-the-\nart performance, they still suffer from two problems: 1) low\nefﬁciency during training and inference; 2) hard to model\nlong dependency using current recurrent neural networks\n(RNNs). Inspired by the success of Transformer network in\nneural machine translation (NMT), in this paper, we intro-\nduce and adapt the multi-head attention mechanism to replace\nthe RNN structures and also the original attention mecha-\nnism in Tacotron2. With the help of multi-head self-attention,\nthe hidden states in the encoder and decoder are constructed\nin parallel, which improves training efﬁciency. Meanwhile,\nany two inputs at different times are connected directly by\na self-attention mechanism, which solves the long range de-\npendency problem effectively. Using phoneme sequences as\ninput, our Transformer TTS network generates mel spec-\ntrograms, followed by a WaveNet vocoder to output the ﬁ-\nnal audio results. Experiments are conducted to test the ef-\nﬁciency and performance of our new network. For the efﬁ-\nciency, our Transformer TTS network can speed up the train-\ning about 4.25 times faster compared with Tacotron2. For\nthe performance, rigorous human tests show that our pro-\nposed model achieves state-of-the-art performance (outper-\nforms Tacotron2 with a gap of 0.048) and is very close to\nhuman quality (4.39 vs 4.44 in MOS).\n1 Introduction\nText to speech (TTS) is a very important task for user inter-\naction, aiming to synthesize intelligible and natural audios\nwhich are indistinguishable from human recordings. Tra-\nditional TTS systems have two components: front-end and\nback-end. Front-end is responsible for text analysis and lin-\nguistic feature extraction, such as word segmentation, part\nof speech tagging, multi-word disambiguation and prosodic\nstructure prediction; back-end is built for speech synthesis\nbased on linguistic features from front-end, such as speech\nacoustic parameter modeling, prosody modeling and speech\ngeneration. In the past decades, concatenative and paramet-\nric speech synthesis systems were mainstream techniques.\nHowever, both of them have complex pipelines, and deﬁn-\ning good linguistic features is often time-consuming and lan-\n∗Work done during internship at Microsoft STC Asia.\nCopyright c⃝ 2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nguage speciﬁc, which requires a lot of resource and man-\npower. Besides, synthesized audios often have glitches or\ninstability in prosody and pronunciation compared to human\nspeech, and thus sound unnatural.\nRecently, with the rapid development of neural net-\nworks, end-to-end generative text-to-speech models, such\nas Tacotron (Wang et al. 2017) and Tacotron2 (Shen et al.\n2017), are proposed to simplify traditional speech synthe-\nsis pipeline by replacing the production of these linguistic\nand acoustic features with a single neural network. Tacotron\nand Tacotron2 ﬁrst generate mel spectrograms directly from\ntexts, then synthesize the audio results by a vocoder such as\nGrifﬁn Lim algorithm (Grifﬁn and Lim 1984) or WaveNet\n(Van Den Oord et al. 2016). With the end-to-end neural net-\nwork, quality of synthesized audios is greatly improved and\neven comparable with human recordings on some datasets.\nThe end-to-end neural TTS models contain two components,\nan encoder and a decoder. Given the input sequence (of\nwords or phonemes), the encoder tries to map them into a\nsemantic space and generates a sequence of encoder hidden\nstates, and the decoder, taking these hidden states as context\ninformation with an attention mechanism, constructs the de-\ncoder hidden states then outputs the mel frames. For both\nencoder and decoder, recurrent neural networks (RNNs) are\nusually leveraged, such as LSTM (Hochreiter and Schmid-\nhuber 1997) and GRU (Cho et al. 2014).\nHowever, RNNs can only consume the input and generate\nthe output sequentially, since the previous hidden state and\nthe current input are both required to build the current hid-\nden state. The characteristic of sequential process limits the\nparallelization capability in both the training and inference\nprocess. For the same reason, for a certain frame, informa-\ntion from many steps ahead may has been biased after mul-\ntiple recurrent processing. To deal with these two problems,\nTransformer (Vaswani et al. 2017) is proposed to replace the\nRNNs in NMT models.\nInspired by this idea, in this paper, we combine the ad-\nvantages of Tacotron2 and Transformer to propose a novel\nend-to-end TTS model, in which the multi-head attention\nmechanism is introduced to replace the RNN structures in\nthe encoder and decoder, as well as the vanilla attention\nnetwork. The self-attention mechanism unties the sequential\ndependency on the last previous hidden state to improve the\nparallelization capability and relieve the long distance de-\n6706\npendency problem. Compared with the vanilla attention be-\ntween the encoder and decoder, the multi-head attention can\nbuild the context vector from different aspects using differ-\nent attention heads. With the phoneme sequences as input,\nour novel Transformer TTS network generates mel spec-\ntrograms, and employs WaveNet as vocoder to synthesize\naudios. We conduct experiments with 25-hour professional\nspeech dataset, and the audio quality is evaluated by human\ntesters. Evaluation results show that our proposed model\noutperforms the original Tacotron2 with a gap of 0.048 in\nCMOS, and achieves a similar performance (4.39 in MOS)\nwith human recording (4.44 in MOS). Besides, our Trans-\nformer TTS model can speed up the training process about\n4.25 times compared with Tacotron2. Audio samples can be\naccessed on https://neuraltts.github.io/transformertts/\n2 Background\nIn this section, we ﬁrst introduce the sequence-to-sequence\nmodel, followed by a brief description about Tacotron2 and\nTransformer, which are two preliminaries in our work.\n2.1 Sequence to Sequence Model\nA sequence-to-sequence model (Sutskever, Vinyals, and\nLe 2014; Bahdanau, Cho, and Bengio 2014) converts an\ninput sequence (x1,x2,...,x T ) into an output sequence\n(y1,y2,...,y T′ ), and each predicted yt is conditioned on\nall previously predicted outputs y1,...,y t−1. In most cases,\nthese two sequences are of different lengths ( T ̸= T′). In\nNMT, this conversion translates the input sentence in one\nlanguage into the output sentence in another language, based\non a conditional probability p(y1,...,y ′\nT |x1,...,x T ):\nht = encoder(ht−1,xt) (1)\nst = decoder(st−1,yt−1,ct) (2)\nwhere ct is the context vector calculated by an attention\nmechanism:\nct = attention(st−1,h) (3)\nthus p(y1,...,y ′\nT |x1,...,x T ) can be computed by\np(y1,...,y ′\nT |x1,...,x T ) =\nT′\n∏\nt=1\np(yt|y<t,x) (4)\nand\np(yt|y<t,x) =softmax(f(st)) (5)\nwhere f(·) is a fully connected layer. For translation tasks,\nthis softmax function is among all dimensions of f(st) and\ncalculates the probability of each word in the vocabulary.\nHowever, in the TTS task, the softmax function is not re-\nquired and the hidden statess calculated by decoder are con-\nsumed directly by a linear projection to obtain the desired\nspectrogram frames.\n2.2 Tacotron2\nTacotron2 is a neural network architecture for speech syn-\nthesis directly from text, as shown in Fig. 1 . The embedding\nsequence of input is ﬁrstly processed with a 3-layer CNN to\nFigure 1: System architecture of Tacotron2.\nextract a longer-term context, and then fed into the encoder,\nwhich is a bi-directional LSTM. The previous mel spectro-\ngram frame (the predicted one in inference, or the golden\none in training time), is ﬁrst processed with a 2-layer fully\nconnected network (decoder pre-net), whose output is con-\ncatenated with the previous context vector, followed by a 2-\nlayer LSTM. The output is used to calculate the new context\nvector at this time step, which is concatenated with the out-\nput of the 2-layer LSTM to predict the mel spectrogram and\nstop token with two different linear projections respectively.\nFinally the predicted mel spectrogram is fed into a 5-layer\nCNN with residual connections to reﬁne the mel spectro-\ngram.\n2.3 Transformer for NMT\nTransformer (Vaswani et al. 2017), shown in Fig. 2, is a\nsequence to sequence network, based solely on attention\nmechanisms and dispensing with recurrences and convo-\nlutions entirely. In recent works, Transformer has shown\nextraordinary results, which outperforms many RNN-based\nmodels in NMT. It consists of two components: an encoder\nand a decoder, both are built by stacks of several identity\nblocks. Each encoder block contains two subnetworks: a\nmulti-head attention and a feed forward network, while each\ndecoder block contains an extra masked multi-head attention\ncomparing to the encoder block. Both encoder and decoder\nblocks have residual connections and layer normalizations.\n6707\nFigure 2: System architecture of Transformer.\n3 Neural TTS with Transformer\nCompared to RNN-based models, using Transformer in neu-\nral TTS has two advantages. First it enables parallel training\nby removing recurrent connections, as frames of an input\nsequence for decoder can be provided in parallel. The sec-\nond one is that self attention provides an opportunity for in-\njecting global context of the whole sequence into each in-\nput frame, building long range dependencies directly. Trans-\nformer shortens the length of paths forward and backward\nsignals have to traverse between any combination of posi-\ntions in the input and output sequences down to 1. This helps\na lot in a neural TTS model, such as the prosody of synthe-\nsized waves, which not only depends on several words in the\nneighborhood, but also sentence level semantics.\nIn this section we will introduce the architecture of our\nTransformer TTS model, and analyze the function of each\npart. The overall structure diagram is shown in Fig. 3.\n3.1 Text-to-Phoneme Converter\nEnglish pronunciation has certain regularities, for example,\nthere are two kinds of syllables in English: open and closed.\nThe letter ”a” is often pronounced as /eı/ when it’s in an\nopen syllable, while it is pronounced as /æ/ or /a :/ in closed\nsyllables. We can rely on the neural network to learn such a\nregularity in the training process. However, it is difﬁcult to\nlearn all the regularities when, which is often the case, the\ntraining data is not sufﬁcient enough, and some exceptions\nhave too few occurrences for neural networks to learn. So we\nmake a rule system and implement it as a text-to-phoneme\nconverter, which can cover the vast majority of cases.\n3.2 Scaled Positional Encoding\nTransformer contains no recurrence and no convolution so\nthat if we shufﬂe the input sequence of encoder or decoder,\nwe will get the same output. To take the order of the se-\nquence into consideration, information about the relative or\nabsolute position of frames is injected by triangle positional\nembeddings, shown in Eq. 7:\nPE(pos,2i) = sin( pos\n10000\n2i\ndmodel\n) (6)\nPE(pos,2i+ 1) = cos( pos\n10000\n2i\ndmodel\n) (7)\nwhere posis the time step index,2iand 2i+1 is the channel\nindex and dmodel is the vector dimension of each frame. In\nNMT, the embeddings for both source and target language\nare from language spaces, so the scales of these embeddings\nare similar. This condition doesn’t hold in the TTS scenar-\nioe, since the source domain is of texts while the target do-\nmain is of mel spectrograms, hence using ﬁxed positional\nembeddings may impose heavy constraints on both the en-\ncoder and decoder pre-nets (which will be described in Sec.\n3.3 and 3.4). We employ these triangle positional embed-\ndings with a trainable weight, so that these embedding can\nadaptively ﬁt the scales of both encoder and decoder pre-\nnets’ output, as shown in Eq. 8:\nxi = prenet(phonemei) +αPE(i) (8)\nwhere αis the trainable weight.\n3.3 Encoder Pre-net\nIn Tacotron2, a 3-layer CNN is applied to the input text em-\nbeddings, which can model the longer-term context in the in-\nput character sequence. In our Transformer TTS model, we\ninput the phoneme sequence into the same network, which\nis called ”encoder pre-net”. Each phoneme has a trainable\nembedding of 512 dims, and the output of each convolution\nlayer has 512 channels, followed by a batch normalization\nand ReLU activation, and a dropout layer as well. In addi-\ntion, we add a linear projection after the ﬁnal ReLU acti-\nvation, since the output range of ReLU is [0,+∞), while\neach dimension of these triangle positional embeddings is in\n[−1,1]. Adding 0-centered positional information onto non-\nnegative embeddings will result in a ﬂuctuation not centered\non the origin and harm model performance, which will be\ndemonstrated in our experiment. Hence we add a linear pro-\njection for center consistency.\n3.4 Decoder Pre-net\nThe mel spectrogram is ﬁrst consumed by a neural network\ncomposed of two fully connected layers(each has 256 hid-\nden units) with ReLU activation, named ”decoder pre-net”,\nand it plays an important role in the TTS system. Phonemes\nhas trainable embeddings thus their subspace is adaptive,\n6708\nFigure 3: System architecture of our model.\nwhile that of mel spectrograms is ﬁxed. We infer that de-\ncoder pre-net is responsible for projecting mel spectrograms\ninto the same subspace as phoneme embeddings, so that the\nsimilarity of a ⟨phoneme,melframe ⟩pair can be mea-\nsured, thus the attention mechanism can work. Besides, 2\nfully connected layers without non-linear activation are also\ntried but no reasonable attention matrix aligning the hidden\nstates of encoder and decoder can be generated. In our other\nexperiment, hidden size is enlarged from 256 to 512, how-\never that doesn’t generate signiﬁcant improvement but needs\nmore steps to converge. Accordingly, we conjecture that mel\nspectrograms have a compact and low dimensional subspace\nthat 256 hidden units are good enough to ﬁt. This conjecture\ncan also be evidenced in our experiment, which is shown in\nSec. 4.6, that the ﬁnal positional embedding scale of decoder\nis smaller than that of encoder. An additional linear projec-\ntion is also added like encoder pre-net not only for center\nconsistency but also obtain the same dimension as the trian-\ngle positional embeddings.\n3.5 Encoder\nIn Tacotron2, the encoder is a bi-directional RNN. We re-\nplace it with Transformer encoder which is described in Sec.\n2.3 . Comparing to original bi-directional RNN, multi-head\nattention splits one attention into several subspaces so that\nit can model the frame relationship in multiple different as-\npects, and it directly builds the long-time dependency be-\ntween any two frames thus each of them considers global\ncontext of the whole sequence. This is crucial for synthe-\nsized audio prosody especially when the sentence is long,\nas generated samples sound more smooth and natural in\nour experiments. In addition, employing multi-head atten-\ntion instead of original bi-directional RNN can enable par-\nallel computing to improve training speed.\n3.6 Decoder\nIn Tacotron2, the decoder is a 2-layer RNN with location-\nsensitive attention (Chorowski et al. 2015). We replace it\nwith Transformer decoder which is described in Sec. 2.3.\nEmploying Transformer decoder makes two main differ-\nences, adding self-attention, which can bring similar advan-\ntages described in Sec. 3.5, and using multi-head attention\ninstead of the location-sensitive attention. The multi-head\nattention can integrate the encoder hidden states in multi-\nple perspectives and generate better context vectors. Taking\nattention matrix of previous decoder time steps into consid-\neration, location-sensitive attention used in Tacotron2 can\nencourage the model to generate consistent attention results.\nWe try to modify the dot product based multi-head attention\nto be location sensitive, but that doubles the training time\nand easily run out of memory.\n3.7 Mel Linear, Stop Linear and Post-net\nSame as Tacotron2, we use two different linear projections\nto predict the mel spectrogram and the stop token respec-\ntively, and use a 5-layer CNN to produce a residual to reﬁne\nthe reconstruction of mel spectrogram. It’s worth mention-\ning that, for the stop linear, there is only one positive sample\nin the end of each sequence which means ”stop”, while hun-\ndreds of negative samples for other frames. This imbalance\nmay result in unstoppable inference. We impose a positive\nweight (5.0 ∼8.0) on the tail positive stop token when cal-\nculating binary cross entropy loss, and this problem was ef-\nﬁciently solved.\n4 Experiment\nIn this section, we conduct experiments to test our proposed\nTransformer TTS model with 25-hour professional speech\npairs, and the audio quality is evaluated by human testers in\nMOS and CMOS.\n4.1 Training Setup\nWe use 4 Nvidia Tesla P100 to train our model with an in-\nternal US English female dataset, which contains 25-hour\nprofessional speech (17584 ⟨text,wave⟩pairs, with a few\ntoo long waves removed). 50ms silence at head and 100ms\nsilence at tail are kept for each wave. Since the lengths of\ntraining samples vary greatly, ﬁxed batch size will either run\n6709\nout of memory when long samples are added into a batch\nwith a large size or waste the parallel computing power if\nthe batch is small and into which short samples are divided.\nTherefore, we use the dynamic batch size where the maxi-\nmum total number of mel spectrogram frames is ﬁxed and\none batch should contain as many samples as possible. Thus\nthere are on average 16 samples in single batch per GPU. We\ntry training on a single GPU, but the procedures are quiet\ninstable or even failed, by which synthesized audios were\nlike raving and incomprehensible. Even if training doesn’t\nfail, synthesized waves are of bad quality and weird prosody,\nor even have some severe problems like missing phonemes.\nThus we enable multi-GPU training to enlarge the batch size,\nwhich effectively solves those problems.\n4.2 Text-to-Phoneme Conversion and Pre-process\nTacotron2 uses character sequences as input, while our\nmodel is trained on pre-normalized phoneme sequences.\nWord and syllable boundaries, punctuations are also in-\ncluded as special markers. The process pipeline to get train-\ning phoneme sequences contains sentence separation, text\nnormalization, word segmentation and ﬁnally obtaining pro-\nnunciation. By text-to-phoneme conversion, mispronuncia-\ntion problems are greatly reduced especially for those pro-\nnunciations that are rarely occurred in our training set.\n4.3 WaveNet Settings\nWe train a WaveNet conditioned on mel spectrogram with\nthe same internal US English female dataset, and use it as\nthe vocoder for all models in this paper. The sample rate of\nground truth audios is 16000 and frame rate (frames per sec-\nond) of ground truth mel spectrogram is 80. Our autoregres-\nsive WaveNet contains 2 QRNN layers and 20 dilated layers,\nand the sizes of all residual channels and dilation channels\nare all 256. Each frame of QRNN’s ﬁnal output is copied\n200 times to have the same spatial resolution as audio sam-\nples and be conditions of 20 dilated layers.\n4.4 Training Time Comparison\nOur model can be trained in parallel since there is no re-\ncurrent connection between frames. In our experiment, time\nconsume in a single training step for our model is ∼0.4s,\nwhich is 4.25 times faster than that of Tacotron2 ( ∼1.7s)\nwith equal batch size (16 samples per batch). However, since\nthe parameter quantity of our model is almost twice than\nTacotron2, it still takes ∼3 days to converge comparing to\n∼4.5 days of that for Tacotron2.\n4.5 Evaluation\nWe randomly select 38 ﬁxed examples with various lengths\n(no overlap with training set) from our internal dataset as\nthe evaluation set. We evaluate mean option score (MOS) on\nthese 38 sentences generated by different models (include\nrecordings), in which case we can keep the text content con-\nsistent and exclude other interference factors hence only ex-\namine audio quality. For higher result accuracy, we split the\nwhole MOS test into several small tests, each containing one\ngroup from our best model, one group from a comparative\nSystem MOS CMOS\nTacotron2 4.39 ±0.05 0\nOur Model 4.39 ±0.05 0.048\nGround Truth 4.44 ±0.05 -\nTable 1: MOS comparison among our model, our Tacotron2\nand recordings.\nFigure 4: Mel spectrogram comparison. Our model (6-layer)\ndoes better in reconstructing details as marked in red rectan-\ngles, while Tacotron2 and our 3-layer model blur the texture\nespecially in high frequency region. Best viewed in color.\nmodel and one group of recordings. Those MOS tests are\nrigorous and reliable, aseach audio is listened to by at least\n20 testers, who are all native English speakers(compar-\ning to Tacotron2’s 8 testers in Shen et al.(2017)), and each\ntester listens less than 30 audios.\nWe train a Tacotron2 model with our internal US English\nfemale dataset as the baseline (also use phonemes as input),\nand gain equal MOS with our model. Therefore we test the\ncomparison mean option score (CMOS) between samples\ngenerated by Tacotron2 and our model for a ﬁner contrast.\nIn the comparison mean option score (CMOS) test, testers\nlisten to two audios (generated by Tacotron2 and our model\nwith the same text) each time and evaluates how the latter\nfeels comparing to the former using a score in [−3,3] with\nintervals of 1. The order of the two audios changes randomly\nso testers don’t know their sources. Our model wins by a gap\nof 0.048, and detailed results are shown in Table 1.\nWe also select mel spectrograms generated by our model\nand Tacotron2 respectively with the same text, and com-\n6710\nFigure 5: PE scale of encoder and decoder.\nRe-center MOS\nNo 4.32 ±0.05\nYes 4.36 ±0.05\nGround Truth 4.43 ±0.05\nTable 2: MOS comparison of whether re-centering pre-net’s\noutput.\npare them together with ground truth, as shown in column\n1,2 and 3 of Fig. 4. As we can see, our model does better\nin reconstructing details as marked in red rectangles, while\nTacotron2 left out the detailed texture in high frequency re-\ngion.\n4.6 Ablation Studies\nIn this section, we study the detail modiﬁcation of network\narchitecture, and conduct several experiments to show our\nimprovements.\nRe-centering Pre-net’s Output As described in Sec. 3.3\nand 3.4, we re-project both the encoder and decoder pre-\nnets’ outputs for consistent center with positional embed-\ndings. In contrast, we add no linear projection in encoder\npre-net and add a fully connected layer with ReLU activation\nin decoder pre-net. The results imply that center-consistent\npositional embedding performs slightly better, as shown in\nTable 2.\nDifferent Positional Encoding Methods We inject posi-\ntional information into both encoder’s and decoder’s input\nsequences as Eq. 8. Fig. 5 shows that the ﬁnal positional\nembedding scales of encoder and decoder are different, and\nTable 3 shows model with trainable scale performs slightly\nbetter. We think that the trainable scale relaxes the constraint\non encoder and decoder pre-nets, making positional infor-\nmation more adaptive for different embedding spaces.\nWe also try adding absolute position embeddings (each\nposition has a trainable embedding) to the sequence, which\nalso works but has some severe problems such as missing\nphonemes when the sequences became long. That’s because\nlong sample is relatively rare in the training set, so the em-\nbeddings for large indexes can hardly be trained and thus the\nPE Type MOS\nOriginal 4.37 ±0.05\nScaled 4.40 ±0.05\nGround Truth 4.41 ±0.04\nTable 3: MOS comparison of scaled and original PE.\nLayer Number MOS\n3-layer 4.33 ±0.06\n6-layer 4.41 ±0.05\nGround Truth 4.44 ±0.05\nTable 4: Ablation studies in different layer numbers.\nHead Number MOS\n4-head 4.39 ±0.05\n8-head 4.44 ±0.05\nGround Truth 4.47 ±0.05\nTable 5: Ablation studies in different head numbers.\nposition information won’t be accurate for rear frames in a\nlong sample.\nModel with Different Hyper-Parameter Both the en-\ncoder and decoder of the original Transformer is composed\nof 6 layers, and each multi-head attention has 8 heads. We\ncompare performance and training speed with different layer\nand head numbers, as shown in Table 4, 5 and 6. We ﬁnd that\nreducing layers and heads both improve the training speed,\nbut on the other hand, harm model performance in different\ndegrees.\nWe notice that in both the 3-layer and 6-layer model, only\nalignments from certain heads of the beginning 2 layers’\nare interpretable diagonal lines, which shows the approx-\nimate correspondence between input and output sequence,\nwhile those of the following layers are disorganized. Even\nso, more layers can still lower the loss, reﬁne the synthesized\nmel spectrogram and improve audio quality. The reason is\nthat with residual connection between different layers, our\nmodel ﬁts target transformation in a Taylor-expansion way:\nthe starting terms account most as low ordering ones, while\nthe subsequential ones can reﬁne the function. Hence adding\nmore layer makes the synthesized wave more natural, since\nit does better in processing spectrogram details (shown in\ncolumn 4, Fig. 4). Fewer heads can slightly reduce training\ntime cost since there are less production per layer, but also\nharm the performance.\n5 Related Work\nTraditional speech synthesis methods can be categorized\ninto two classes: concatenative systems and parametric sys-\ntems. Concatenative TTS systems (Hunt and Black 1996;\nBlack and Taylor 1997) split original waves into small units,\n6711\n3-layer 6-layer\n4-head - 0.44\n8-head 0.29 0 .50\nTable 6: Comparison of time consuming (in second) per\ntraining step of different layer and head numbers.\nand stitch them by some algorithms such as Viterbi (Viterbi\n1967) followed by signal process methods (Charpentier and\nStella 1986; Verhelst and Roelands 1993) to generate new\nwaves. Parametric TTS systems (Tokuda et al. 2000; Zen,\nTokuda, and Black 2009; Ze, Senior, and Schuster 2013;\nTokuda et al. 2013) convert speech waves into spectrograms,\nand acoustic parameters, such as fundamental frequency and\nduration, are used to synthesize new audio results.\nTraditional speech synthesis methods require extensive\ndomain expertise and may contain brittle design choices.\nChar2Wav (Sotelo et al. 2017) integrates the front-end and\nthe back-end as one seq2seq (Sutskever, Vinyals, and Le\n2014; Bahdanau, Cho, and Bengio 2014) model and learns\nthe whole process in an end-to-end way, predicting acoustic\nparameters followed by a SampleRNN (Mehri et al. 2016)\nas the vocoder. However, acoustic parameters are still inter-\nmediate for audios, thus Char2Wav is not a really end-to-\nend TTS model, and their seq2seq and SampleRNN models\nneed to be separately pre-trained, while Tacotron, proposed\nby Wang et al.(2017), is an end-to-end generative text-to-\nspeech model, which can be trained by⟨text,spectrogram⟩\npairs directly from scratch, and synthesizes speech audios\nwith generated spectrograms by Grifﬁn Lim algorithm (Grif-\nﬁn and Lim 1984). Based on Tacotron, Tacotron2 (Shen et\nal. 2017), a uniﬁed and entirely neural model, generates\nmel spectrograms by a Tacotron-style neural network and\nthen synthesizes speech audios by a modiﬁed WaveNet (Van\nDen Oord et al. 2016). WaveNet is an autoregressive gen-\nerative model for waveform synthesis, composed of stacks\nof dilated convolutional layers and processes raw audios\nof very high temporal resolution (e.g., 24,000 sample rate),\nwhile suffering from very large time cost in inference. This\nproblem is solved by Parallel WaveNet (Oord et al. 2017),\nbased on the inverse autoregressive ﬂow (IAF) (Kingma et\nal. 2016) and reaches 1000×real time. Recently, ClariNet\n(Ping, Peng, and Chen 2018), a fully convolutional text-to-\nwave neural architecture, is proposed to enable the fast end-\nto-end training from scratch. Moreover, V oiceLoop (Taig-\nman et al. 2018) is an alternative neural TTS method mim-\nicking a person’s voice based on samples captured in-the-\nwild, such as audios of public speeches, and even with an\ninaccurate automatic transcripts.\nOn the other hand, Transformer (Vaswani et al. 2017)\nis proposed for neural machine translation (NMT) and\nachieves state-of-the-art result. Previous NMT models are\ndominated by RNN-based (Bahdanau, Cho, and Bengio\n2014) or CNN-based (e.g. ConvS2S (Gehring et al. 2017),\nByteNet (Kalchbrenner et al. 2016)) neural networks. For\nRNN-based models, both training and inference are sequen-\ntial for each sample, while CNN-based models enable paral-\nlel training. Both RNN and CNN based models are difﬁcult\nto learn dependencies between distant positions since RNNs\nhave to traverse a long path and CNN has to stack many con-\nvolutional layers to get a large receptive ﬁeld, while Trans-\nformer solves this using self attention in both its encoder\nand decoder. The ability of self-attention is also proved in\nSAGAN (Zhang et al. 2018), where original GANs without\nself-attention fail to capture geometric or structural patterns\nthat occur consistently in some classes (for example, dogs\nare often drawn without clearly deﬁned separate feet). By\nadding self-attention, these failure cases are greatly reduced.\nBesides, multi-head attention is proposed to obtain differ-\nent relations in multi-subspaces. Recently, Transformer has\nbeen applied in automatic speech recognition (ASR) (Zhou\net al. 2018a; 2018b), proving its ability in acoustic modeling\nother than natural language process.\n6 Conclusion and Future Work\nWe propose a neural TTS model based on Tacotron2 and\nTransformer, and make some modiﬁcation to adapt Trans-\nformer to neural TTS task. Our model generates audio sam-\nples of which quality is very closed to human recording, and\nenables parallel training and learning long-distance depen-\ndency so that the training is sped up and the audio prosody\nis much more smooth. We ﬁnd that batch size is crucial for\ntraining stability, and more layers can reﬁne the detail of\ngenerated mel spectrograms especially for high frequency\nregions thus improve model performance.\nEven thought Transformer has enabled parallel training,\nautoregressive model still suffers from two problems, which\nare slow inference and exploration bias. Slow inference is\ndue to the dependency of previous frames when infer cur-\nrent frame, so that the inference is sequential, while explo-\nration bias comes from the autoregressive error accumula-\ntion. We may solve them both at once by building a non-\nautoregressive model, which is also our current research in\nprogress.\n7 Acknowledgments\nThis work is supported in part by National Science Founda-\ntion of China under Grant No.61572113, and the Fundamen-\ntal Research Funds for the Central Universities under Grants\nNo.ZYGX2015J155, ZYGX2016J084, ZYGX2016J195.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nBlack, A. W., and Taylor, P. 1997. Automatically cluster-\ning similar units for unit selection in speech synthesis. In\nFifth European Conference on Speech Communication and\nTechnology.\nCharpentier, F., and Stella, M. 1986. Diphone synthesis us-\ning an overlap-add technique for speech waveforms concate-\nnation. In Acoustics, Speech, and Signal Processing, IEEE\nInternational Conference on ICASSP’86., volume 11, 2015–\n2018. IEEE.\n6712\nCho, K.; Van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning\nphrase representations using rnn encoder-decoder for statis-\ntical machine translation. Computer Science.\nChorowski, J. K.; Bahdanau, D.; Serdyuk, D.; Cho, K.; and\nBengio, Y . 2015. Attention-based models for speech recog-\nnition. In Advances in neural information processing sys-\ntems, 577–585.\nGehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,\nY . N. 2017. Convolutional sequence to sequence learning.\narXiv preprint arXiv:1705.03122.\nGrifﬁn, D., and Lim, J. 1984. Signal estimation from mod-\niﬁed short-time fourier transform. IEEE Transactions on\nAcoustics, Speech, and Signal Processing32(2):236–243.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation9(8):1735–1780.\nHunt, A. J., and Black, A. W. 1996. Unit selection in a\nconcatenative speech synthesis system using a large speech\ndatabase. In Acoustics, Speech, and Signal Processing,\n1996. ICASSP-96. Conference Proceedings., 1996 IEEE In-\nternational Conference on, volume 1, 373–376. IEEE.\nKalchbrenner, N.; Espeholt, L.; Simonyan, K.; Oord, A.\nv. d.; Graves, A.; and Kavukcuoglu, K. 2016. Neu-\nral machine translation in linear time. arXiv preprint\narXiv:1610.10099.\nKingma, D. P.; Salimans, T.; Jozefowicz, R.; Chen, X.;\nSutskever, I.; and Welling, M. 2016. Improved variational\ninference with inverse autoregressive ﬂow. In Advances in\nNeural Information Processing Systems, 4743–4751.\nMehri, S.; Kumar, K.; Gulrajani, I.; Kumar, R.; Jain, S.;\nSotelo, J.; Courville, A.; and Bengio, Y . 2016. Samplernn:\nAn unconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837.\nOord, A. v. d.; Li, Y .; Babuschkin, I.; Simonyan, K.;\nVinyals, O.; Kavukcuoglu, K.; Driessche, G. v. d.; Lock-\nhart, E.; Cobo, L. C.; Stimberg, F.; et al. 2017. Parallel\nwavenet: Fast high-ﬁdelity speech synthesis. arXiv preprint\narXiv:1711.10433.\nPing, W.; Peng, K.; and Chen, J. 2018. Clarinet: Parallel\nwave generation in end-to-end text-to-speech.arXiv preprint\narXiv:1807.07281.\nShen, J.; Pang, R.; Weiss, R. J.; Schuster, M.; Jaitly, N.;\nYang, Z.; Chen, Z.; Zhang, Y .; Wang, Y .; Skerry-Ryan,\nR.; et al. 2017. Natural tts synthesis by conditioning\nwavenet on mel spectrogram predictions. arXiv preprint\narXiv:1712.05884.\nSotelo, J.; Mehri, S.; Kumar, K.; Santos, J. F.; Kastner, K.;\nCourville, A.; and Bengio, Y . 2017. Char2wav: End-to-end\nspeech synthesis. ICLR 2017 workshop.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. In Advances in\nneural information processing systems, 3104–3112.\nTaigman, Y .; Wolf, L.; Polyak, A.; and Nachmani, E. 2018.\nV oiceloop: V oice ﬁtting and synthesis via a phonological\nloop. In International Conference on Learning Represen-\ntations.\nTokuda, K.; Yoshimura, T.; Masuko, T.; Kobayashi, T.; and\nKitamura, T. 2000. Speech parameter generation algorithms\nfor hmm-based speech synthesis. In Acoustics, Speech, and\nSignal Processing, 2000. ICASSP’00. Proceedings. 2000\nIEEE International Conference on, volume 3, 1315–1318.\nIEEE.\nTokuda, K.; Nankaku, Y .; Toda, T.; Zen, H.; Yamagishi, J.;\nand Oura, K. 2013. Speech synthesis based on hidden\nmarkov models. Proceedings of the IEEE 101(5):1234–\n1252.\nVan Den Oord, A.; Dieleman, S.; Zen, H.; Simonyan, K.;\nVinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A. W.;\nand Kavukcuoglu, K. 2016. Wavenet: A generative model\nfor raw audio. In SSW, 125.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems, 5998–6008.\nVerhelst, W., and Roelands, M. 1993. An overlap-add tech-\nnique based on waveform similarity (wsola) for high quality\ntime-scale modiﬁcation of speech. InAcoustics, Speech, and\nSignal Processing, 1993. ICASSP-93., 1993 IEEE Interna-\ntional Conference on, volume 2, 554–557. IEEE.\nViterbi, A. 1967. Error bounds for convolutional codes\nand an asymptotically optimum decoding algorithm. IEEE\ntransactions on Information Theory13(2):260–269.\nWang, Y .; Skerry-Ryan, R.; Stanton, D.; Wu, Y .; Weiss, R. J.;\nJaitly, N.; Yang, Z.; Xiao, Y .; Chen, Z.; Bengio, S.; et al.\n2017. Tacotron: A fully end-to-end text-to-speech synthesis\nmodel. arXiv preprint.\nZe, H.; Senior, A.; and Schuster, M. 2013. Statistical para-\nmetric speech synthesis using deep neural networks. In\nAcoustics, Speech and Signal Processing (ICASSP), 2013\nIEEE International Conference on, 7962–7966. IEEE.\nZen, H.; Tokuda, K.; and Black, A. W. 2009. Statisti-\ncal parametric speech synthesis. Speech Communication\n51(11):1039–1064.\nZhang, H.; Goodfellow, I.; Metaxas, D.; and Odena, A.\n2018. Self-attention generative adversarial networks. arXiv\npreprint arXiv:1805.08318.\nZhou, S.; Dong, L.; Xu, S.; and Xu, B. 2018a. A com-\nparison of modeling units in sequence-to-sequence speech\nrecognition with the transformer on mandarin chinese.arXiv\npreprint arXiv:1805.06239.\nZhou, S.; Dong, L.; Xu, S.; and Xu, B. 2018b.\nSyllable-based sequence-to-sequence speech recognition\nwith the transformer in mandarin chinese. arXiv preprint\narXiv:1804.10752.\n6713"
}