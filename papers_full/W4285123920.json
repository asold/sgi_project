{
  "title": "Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences",
  "url": "https://openalex.org/W4285123920",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5034490691",
      "name": "Joshua Bensemann",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5049443230",
      "name": "Alex Yuxuan Peng",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5017390828",
      "name": "Diana Benavides‐Prado",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5100350358",
      "name": "Yang Chen",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5062975742",
      "name": "Neşet Tan",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5029292433",
      "name": "Paul M. Corballis",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5028387093",
      "name": "Patricia Riddle",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A5057995059",
      "name": "Michael Witbrock",
      "affiliations": [
        "University of Auckland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1762161369",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W2394729563",
    "https://openalex.org/W2533058588",
    "https://openalex.org/W3094540663",
    "https://openalex.org/W3171355829",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4293612949",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2905110202",
    "https://openalex.org/W2394756230",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3041467857",
    "https://openalex.org/W3169953327",
    "https://openalex.org/W1974841121",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2616278914",
    "https://openalex.org/W3034444624",
    "https://openalex.org/W3092785544",
    "https://openalex.org/W2953874899",
    "https://openalex.org/W2947012833",
    "https://openalex.org/W3105645800",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2886100558",
    "https://openalex.org/W1480151146",
    "https://openalex.org/W3210923133",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2135957164",
    "https://openalex.org/W3106388706",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963890755",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2767693128",
    "https://openalex.org/W2084777116",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2212216676",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2011826874",
    "https://openalex.org/W2518599539",
    "https://openalex.org/W3002336103",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2518578398",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3100748148"
  ],
  "abstract": "Joshua Bensemann, Alex Peng, Diana Benavides-Prado, Yang Chen, Neset Tan, Paul Michael Corballis, Patricia Riddle, Michael Witbrock. Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics. 2022.",
  "full_text": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75 - 87\nMay 26, 2022 ©2022 Association for Computational Linguistics\nEye Gaze and Self-attention: How Humans and Transformers Attend\nWords in Sentences\nJoshua Bensemann∗, Alex Yuxuan Peng, Diana Benavides-Prado, Yang Chen,\nNe¸ set Özkan Tan, Paul Michael Corballis, Patricia Riddle,and Michael Witbrock\nUniversity of Auckland\nAbstract\nAttention describes cognitive processes that\nare important to many human phenomena in-\ncluding reading. The term is also used to de-\nscribe the way in which transformer neural\nnetworks perform natural language processing.\nWhile attention appears to be very different\nunder these two contexts, this paper presents\nan analysis of the correlations between trans-\nformer attention and overt human attention\nduring reading tasks. An extensive analysis\nof human eye tracking datasets showed that\nthe dwell times of human eye movements were\nstrongly correlated with the attention patterns\noccurring in the early layers of pre-trained\ntransformers such as BERT. Additionally, the\nstrength of a correlation was not related to\nthe number of parameters within a transformer.\nThis suggests that something about the trans-\nformers’ architecture determined how closely\nthe two measures were correlated.\n1 Introduction\nAttention is a process that is associated with both\nreading in humans and with Natural Language Pro-\ncessing (NLP) by state-of-the-art Deep Neural Net-\nworks (DNN) (Bahdanau et al., 2015). In both\ncases, it is the words within a sentence that are\nattended to during processing. In DNNs, attention\nresults from mechanisms built into the network.\nSpeciﬁcally, in the current state-of-the-art method\nTransformers (Vaswani et al., 2017), this attention\nprocess is the result of the dot product of two vec-\ntors that represent individual words in the text. For\nhumans, attention processes are more complex as\nthey can be broken into overt and covert attention\n(Posner, 1980). Overt attention is characterized by\nobservable physical movements of which eye gaze\nis a well known example that is relevant to read-\ning (Rayner, 2009). Covert attention, on the other\nhand, is characterized by mental shifts in focus and,\n∗Email: josh.bensemann@auckland.ac.nz\ntherefore, not directly observable. For this study\nwe have focused on the overt attention measure of\neye gaze, with words at the center of an eye ﬁxa-\ntion being the words that we assume were being\nattended.\nWhile attention in human reading processes and\ntransformers appear to be completely different, this\npaper will present an analysis showing the rela-\ntionship between the two1. Speciﬁcally, attention\nin well-known transformers such as BERT (De-\nvlin et al., 2019), and its derivatives are closely\nrelated to humans’ eye ﬁxations during reading.\nWe observed strong to moderate strength correla-\ntions between the dwell times of eyes over words\nand the self-attention in transformers such as BERT.\nWe have explored some reasons for these different\ncorrelation levels and speculated on others.\nThis analysis is part of an ongoing research line\nwhere we attempt to overcome attention limits in\ntransformers. When using transformers, both mem-\nory and computational requirements grow quadrat-\nically as the sequence length increases because\nevery token attends to all other tokens. In previous\nwork, we have used the attention mechanisms of\npre-trained transformers as attention ﬁlters that can\nreduce a sequence length for a sentiment analysis\ntask by 99% while still maintaining 70% accuracy\n(Tan et al., 2021). Our motivation for this paper\nwas to explore the possibility of using models of\neye gaze as an alternative ﬁlter. Strong correlations\nbetween the attentions produced by transformers\nand the overt attention of humans would suggest\nthat models of eye movements could potentially be\nused in computationally inexpensive methods for\napproximating transformer attention. Alternatively\nwe could use eye movements to train transformer\nattention towards overt attention patterns2.\n1Code and Full Results available at https://github.\ncom/Strong-AI-Lab/Eye-Tracking-Analysis\n2See appendix for a preliminary attempt.\n75\n1.1 Transformers\nTransformers (Vaswani et al., 2017) have domi-\nnated the leader boards for NLP tasks since their\nintroduction to the deep learning community. Addi-\ntionally, transformers have had an impact on com-\nputer vision (Dosovitskiy et al., 2021), including\ngenerative networks (Jiang et al., 2021). The gen-\neral superior performance of transformers at these\ntasks is due to its attention mechanism:\nAttention(Q,K,V) = softmax\n(QK⊤\n√n\n)\nV\n(1)\nwhere the word vectors representations of the text\nsequence Q are compared to those from sequence\nK. This is used to determine the amount of informa-\ntion word representations from the former should\nincorporate from the latter. If the query and key se-\nquence are the same, as in a transformers encoder,\nit is called self-attention. The results of the atten-\ntion process are then multiplied by sequence V to\nget the ﬁnal outputs from the attention layer. V\ncontains different representations for the words in\nK.\nThe more relevant a word in K is to those in Q,\nthe more attention Q words allocate to that word.\nResearch has examined the Q x K part of the at-\ntention mechanism to understand how transformers\nprocess information. Vaswani et al. (2017) showed\nthat transformers could use words in Q to learn\nanaphora resolution by appropriately attending the\nword \"its\" in K.\nThe introduction of transformers was quickly\nfollowed by a proliferation of pre-trained models\nusing the transformers architecture. Arguably, the\nmost famous of these models is BERT, a.k.a. the\nBidirectional Encoder Representations from Trans-\nformers model (Devlin et al., 2019). BERT was\ndesigned to encode information from whole pas-\nsages of text into a single vector representation. Its\nbidirectional structure means that each word token\nis placed in the context of the entire sequence in-\nstead of just the tokens appearing before it. This\nstructure provided an increase in performance on\nthe GLUE benchmarks (Wang et al., 2019b) over\nmono-directional models such as the original GPT\n(Radford et al., 2018).\nTo ensure that the model learned to attend to the\nsequence as the whole, BERT was trained using\nMasked Language Modeling (MLM), a task in-\nspired by the Cloze procedure (Taylor, 1953) from\nhuman reading comprehension studies. In MLM,\nrandom words from a sequence are hidden during\ninput. The model then has to predict what word\nwas hidden based on the context of surrounding\nwords. BERT was also trained to perform Next\nSentence Prediction (NSP) during MLM, forcing\nwords from one sentence to attend to words in other\nsentences. BERT achieved state-of-the-art perfor-\nmance in multiple NLP benchmarks following this\ntraining regime, which led to its widespread adop-\ntion.\nBERT’s impact on the ﬁeld can be seen in the\nnumber of subsequent models that are its direct\ndescendants. Examples include models such as\nRoBERTa (Liu et al., 2019), which uses BERT’s\narchitecture but was trained via different methods.\nOther models, such as ALBERT (Lan et al., 2020),\nwere created to condense BERT for faster perfor-\nmance with minimal accuracy loss. Even mod-\nels such as XLNet (Yang et al., 2019) extended\nBERT’s architecture to include recurrence mecha-\nnisms introduced in other models (Dai et al., 2019).\nIn turn, some of these descendant models have\nbeen used to create other models. For example,\nBIGBIRD (Zaheer et al., 2020) was built using\nRoBERTa as its base.\n1.2 Combining Transformers and Eye Gaze\nThere is a growing ﬁeld of research that combines\npre-trained transformers with eye-tracking data.\nResearchers have used outputs from BERT as fea-\ntures for machine learning models to predict eye\nﬁxations. In some instances, these outputs are com-\nbined with other features (Choudhary et al., 2021);\nin other instances, BERT itself is ﬁne-tuned to pre-\ndict eye ﬁxations. For example Hollenstein et al.\n(2021a) have shown that BERT can be effective at\npredicting eye movements for texts written in mul-\ntiple languages, including English, Dutch, German,\nand Russian.\nGiven the strong relationship between eye gaze\nand attention, it is unsurprising that there have been\nattempts to compare eye gaze to attention generated\nin transformers. Sood et al. (2020a) compared eye\nmovements in reading comprehension task to three\ndifferent neural networks, including XLNet. After\nﬁne-tuning XLNet, they compared attention from\nthe last encoder layer to eye gaze and reported a\nnon-signiﬁcant correlation. However, their compar-\nison only reported the correlation for the ﬁnal atten-\ntion layer of the network, while other studies com-\nparing transformer attention to human metrics have\n76\nindicated that the strength of an association can\ndiffer by layer (Toneva and Wehbe, 2019). There-\nfore, the present study calculated correlations with\neye movements from all layers of the transformers.\nWith that said, our results focused on the ﬁrst layer\nas it generally produced the strongest correlations\nto eye gaze data.\nFollowing the work of Sood et al. (2020a), the\npresent study is a large-scale analysis of the rela-\ntionship between attention in pre-trained transform-\ners and human attention derived from eye gaze. We\ncompared the self-attention values of 31 variants\nfrom 11 different transformers, including BERT, its\ndescendants, and a few other state-of-the-art trans-\nformers (Table 1). No ﬁne-tuning was performed;\nmodels were the same as those reported in their\nrespective papers. Using the BERT-based models\nwith their original parameter weights allowed us to\ninvestigate the effect that training regime had on\nhow closely the attention was related to overt eye-\nbased attention. Using non-BERT models allowed\nus to examine what effect model architecture had\non this relationship. Finally, the different datasets\nenabled an exploration into how the human partic-\nipants’ task also affects this relationship. Results\nshowed signiﬁcant correlations between attention\nin the ﬁrst layer of the transformers and total dwell\ntime. These correlations were unrelated to the size\nof the model.\n2 Related Work\nThere have been attempts to combine DNNs with\neye data to perform various tasks. Some basic tasks\ninclude predicting how an eye will move across\npresented stimuli, whether text-based (Sood et al.,\n2020b) or images in general (Ghariba et al., 2020;\nLi and Yu, 2016; Harel et al., 2006; Huang et al.,\n2015; Tavakoli et al., 2017). These predictions can\nbe used to create saliency maps that show what\nareas of a visual display are attractive to the eye.\nIn turn, saliency maps can be used to either un-\nderstand biological visual processes or be incorpo-\nrated as meta-data into machine learning models.\nThe later endeavor has led to some improvements\nin task performance. In a recent example, Sood\net al. (2020b) achieved state-of-the-art results in a\ntext compression task by creating a Text Saliency\nModel (TSM) using a BiLSTM network that out-\nputs embeddings into transformer self-attention lay-\ners. The TSM was pre-trained on synthetic data\nsimulated by the E-Z reader model (Reichle et al.,\n1998) and ﬁne-tuned on human eye-tracking data.\nThe model’s output was used to neuromodulate\n(Vecoven et al., 2020) a task-speciﬁc model via\nmultiplicative attention.\nEye gaze data itself can be used to inspire new\nways for neural networks to perform NLP tasks\n(Zheng et al., 2019). For example, it is well known\nthat the human eye does not ﬁxate on every word\nduring reading (Duggan and Payne, 2011). Nev-\nertheless, humans, until recently, performed well\nabove machines in many NLP tasks (Fitzsimmons\net al., 2014; He et al., 2021). These observations\nimply that the word skipping process is not detri-\nmental to reading tasks. Some researchers have\nexploited this process by explicitly training their\nmodels to ignore words (Yu et al., 2017; Seo et al.,\n2018; Hahn and Keller, 2016). For example, Yu\net al. (2017) trained LSTM models to predict the\nnumber of words to skip while performing senti-\nment analysis and found that the model could skip\nseveral words at a time and still be as accurate, if\nnot more accurate, than the non-skipping models.\nAdditionally, Hahn and Keller (2018) showed that\nthe skipping processes could be modelled using\nactual eye movements and achieve the same result.\nThese word skipping models exploit overt attention\nonly, and it would be interesting to know what hap-\npens if skipping was modelled on covert attention\ninstead.\nOther research exploring the relationship be-\ntween DNNs and human data has examined how\nclosely the metrics used to measure eye movement\nare related to metrics used for machine language\nmodels. Studies of this type require identifying\ncomparable processes between the two different\nsystems and a suitable dataset. For example, Hao\net al. (2020) compared model perplexity to psy-\ncholinguistic features.\nThere have even been comparisons of DNN at-\ntention to what humans attend to during reading\ntasks. Sen et al. (2020) compared the attention of\nhumans during a sentiment analysis task to RNN\nmodels. Crowdsourced workers were asked to rate\nsentiments of YELP reviews and then highlight\nthe important words for their decision-making pro-\ncess. They found correlations between the RNN\noutputs and human behavior. The strength of these\ncorrelations diminished as the length of the text\nincreased.\nClosely related to our study is the work of Sood\net al. (2020a) who attempted to compare eye gaze\n77\nto the attention mechanisms of three different neu-\nral network architectures. One of the models was\nthe BERT-based transformer, XLNet (Yang et al.,\n2019). The other two networks were bespoke CNN\nand LSTM models. All models were trained on the\nMovieQA dataset (Tapaswi et al., 2016), and atten-\ntion values were taken from the later levels of the\nnetworks. Several questions for the original dataset\nwere selected for human testing, where the par-\nticipants’ eye gazes were tracked while they read\nand answered the questions. Sood et al. (2020a)\nobserved that the attention scores from both the\nCNN and LSTM networks had strong negative cor-\nrelations with the eye data. However, there was\nno signiﬁcant correlation between eye gaze and\nXLNet.\nFinally, there has been recent work using trans-\nformer representations to predict brain activity. For\nexample, Toneva and Wehbe (2019) used layer rep-\nresentations of different transformers, including\nBERT and Transformer-XL, to predict activation\nin areas of the brain. They found that the mid-\ndle layers best predict the activation as the con-\ntext (sequence length) grew. Toneva and Wehbe\n(2019) tentatively suggested that this means there\nis a relationship between the layer and the type\nof processing occurring. To their surprise, they\nalso found that modifying lower levels of BERT\nto produce uniform attention improved prediction\nperformance.\nSchrimpf et al. (2021) performed a similar anal-\nysis using many of the models included in the\npresent study. They found that the output of some\ntransformers could be used to predict their partic-\nipants brain behavior to almost perfect accuracy.\nPrediction performance differed by model size and\ntraining regime, with GPT-2 performing best (Rad-\nford et al., 2019). Surprisingly, Schrimpf et al.\n(2021) found that untrained models also produced\nabove chance prediction, leading them to suggest\nthat the architecture of transformers captures im-\nportant features of language before training occurs.\n3 Analysis of Self-Attention Against Eye\nGaze\nAll analyses used HuggingFace’s (Wolf et al., 2020)\nversion of the transformer and associated tokenizer.\nThe models’ weights were identical to those down-\nloaded from HuggingFace; no ﬁne-tuning was con-\nducted. All analyses report Spearman correlations\n(Coefﬁcient, 2008) to avoid data normality issues\nand provide a direct comparison to previously re-\nported work.\n3.1 Datasets\nSix different datasets were used in our study. In\nall cases, eye-tracking data were captured from\nparticipants performing reading tasks in English.\nThe GECO Corpus (Cop et al., 2017) contains\ndata from 19 Dutch bilingual and 14 English read-\ners who read \"The Mysterious Affair at Styles\" by\nAgatha Christie across four sessions. Comprehen-\nsion tests occurred between sessions. The bilingual\nparticipants completed two sessions in English and\ntwo in Dutch. We selected all English sessions for\nour analysis, regardless of the participant’s bilin-\ngual status.\nThe PROVO Corpus (Luke and Christianson,\n2018) contains 55 passages (average of 2.5 sen-\ntences). Passages were taken from online news\narticles, magazines, and works of ﬁction. Partici-\npants were 84 native English speakers instructed to\nread for comprehension.\nThe ZuCo Corpus (Hollenstein et al., 2018) is a\ncombined reading, eye-tracking, and EEG dataset.\nData was captured from 12 native English speakers\nwho could read at their own pace with sentences\npresented one at a time. The participants completed\nthree different tasks. Task 1 was a sentiment anal-\nysis task. Task 2 was a standard reading compre-\nhension task where questions were presented after\nreading the text. Task 3 was also a reading com-\nprehension task; however, the question appeared\nonscreen while the participant was reading.\nWe also used data from Sood et al. (2020a).\nThey collected data from 32 passages taken from\nthe MovieQA (Tapaswi et al., 2016) dataset. In\nStudy 1, 18 participants answered questions from\n16 passages under varying conditions such as multi-\nchoice, free answer with text present, and free an-\nswer from memory. In Study 2, 4 participants an-\nswered multi-choice questions from the remaining\n16 passages.\nAdditionally, we used data from Frank et al.\n(2013) where 48 participants read 205 sentences\nfrom unpublished novels for comprehension. The\ndataset contains eye movements from both native\nand non-native English speakers. Participants oc-\ncasionally answered yes/no questions following a\nsentence.\nThe ﬁnal dataset comes from Mishra et al. (2016)\nwho conducted a sarcasm detection task. The\n78\nTable 1: List of models used in this paper\nModel Pre-trained models in Huggingface repository\nALBERT (Lan et al., 2020) albert-base-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-\nv2\nBART (Lewis et al., 2020) facebook-bart-base, facebook-bart-large\nBERT (Devlin et al., 2019) bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-\nbase-multilingual-cased\nBIGBIRD (Zaheer et al., 2020) google-bigbird-roberta-base, google-bigbird-roberta-large\nDeBERTa (He et al., 2021) microsoft-deberta-base, microsoft-deberta-large, microsoft-deberta-xlarge,\nmicrosoft-deberta-v2-xlarge, microsoft-deberta-v2-xxlarge\nDistilBERT (Sanh et al., 2019) distilbert-base-uncased, distilbert-base-cased, distilbert-base-multilingual-cased\nMuppet (Aghajanyan et al., 2021) facebook-muppet-roberta-base, facebook-muppet-roberta-large\nRoBERTa (Liu et al., 2019) roberta-base, roberta-large\nSqueezeBERT (Iandola et al., 2020) squeezebert-squeezebert-uncased\nXLM (Conneau et al., 2020) xlm-roberta-base, xlm-roberta-large\nXLNet (Yang et al., 2019) xlnet-base-cased, xlnet-large-cased\ndataset was taken from a wide variety of sources, all\nshort passages containing a maximum of 40 words.\nParticipants were non-native English speakers who\nwere highly proﬁcient in English.\n3.2 Models\nTable 1 lists the 31 variants from the 11 differ-\nent bidirectional transformers models that we used.\nOur analysis method required all tokens to attend to\nall other tokens in a sequence. Therefore, unidirec-\ntional models such as GPT-2 (Radford et al., 2019)\nwere excluded as they prevent tokens early in a se-\nquence from attending tokens later in that sequence.\nWe grouped the models into three types: 1) Basic\nmodels have the same architecture as BERT. 2)\nCompact modelsare those designed to be smaller\nversions of basic models. 3) Alternative models\nare those that greatly differ from the basic models.\n3.2.1 Basic Models\nBERT (Devlin et al., 2019): On release, BERT\nwas state-of-the-art. It was trained using MLM, in\nwhich 15% of tokens were masked. Training also\nincorporated NSP by forcing the model to predict\nwhether two sentences were contiguous or not. Our\nanalysis includes a multilingual BERT and both the\ncased and uncased versions of English BERT.\nRoBERTa (Liu et al., 2019): RoBERTa has an\narchitecture identical to BERT but was trained for\nlonger, with larger batch sizes and more data. The\nMLM examples were dynamically generated dur-\ning a batch, unlike BERT which used the same\nmask patterns every time a sample was used. The\nNSP task was dropped as it did not affect perfor-\nmance.\nWe have also included the MUPPET version of\nRoBERTa (Aghajanyan et al., 2021), trained using\nmultitask learning with tasks from four domains:\nclassiﬁcation, commonsense reasoning, reading\ncomprehension, and summarization. Finally, we\nhave included XLM-RoBERTa (Conneau et al.,\n2020), a multilingual version of RoBERTa.\n3.2.2 Compact Models\nALBERT (Lan et al., 2020): A Lite BERT is a\nBERT-based model that uses two tricks to reduce\nthe number of parameters and time taken required\nto train the model. 1) Factorized embedding pa-\nrameterization - decomposing the large vocabu-\nlary embedding matrix into two small matrices; 2)\nCross-layer sharing - parameters for all layers are\nshared.\nDistilBERT (Sanh et al., 2019): This model used\na Teacher – Student method for the distillation\nof knowledge (Buciluˇa et al., 2006; Hinton et al.,\n2015). Sanh et al. (2019) started with a full model\nand kept every second layer to create the student.\nThe student was then trained on original training\ndata. This procedure resulted in an almost as pow-\nerful model but half the size.\nSqueezeBERT (Iandola et al., 2020): Squeeze-\nBERT is Bert but with grouped convolutional lay-\ners instead of feed-forward layers. The model was\ntrained using the same methods as ALBERT.\n3.2.3 Alternative Attention Mechanisms\nDeBERTa (He et al., 2021): DeBERTa differs from\nothers on this list in that it decouples attention by\nword semantics from attention by word location.\nVersion 2 of the model used a form of adversar-\nial training to improve model generalization and\nsurpassed human performance on Super GLUE\n79\nbenchmarks. We have used the RoBERTa based\nversions in this analysis.\nOne problem with transformers is the quadratic\nmemory, and computational growth as sequence\nlength increases because every token attends to all\nother tokens. Some have dealt with this problem\nby modifying the attention patterns to approximate\nthis full attention pattern without requiring all of\nthe attention comparisons. BIGBIRD (Zaheer et al.,\n2020) is an example that uses this attention approx-\nimation. The model uses a combination of global,\nsparse, and random attention. Again, we have used\nthe RoBERTa based version of the model.\n3.2.4 Alternative Architectures\nXLNet (Yang et al., 2019): This model is a BERT\nextension using random permutations of word or-\nder during training. The model also incorporates\nthe recurrence mechanism used in Transformer-XL\n(Dai et al., 2019).\nBART (Lewis et al., 2020): BART is an encoder-\ndecoder model that is to recover data from cor-\nrupted text input. BART has approximately 10%\nmore parameters than comparable BERT models\nand no ﬁnal feed-forward layer. Pre-training was\nbased on corrupting the inputs using token masking,\ntoken deletion, token inﬁlling, sentence permuta-\ntion, and document rotation.\n3.3 Analysis Method\nThe transformer data were created by converting\nthe original texts into sentences and then tokeniz-\ning those sentences to create sequences. The next\nstep was inputting tokenized sequences into the\ntransformer and extracting the attention matrices\nproduced for each attention head. In terms of Equa-\ntion 1, we took the output of the softmax function\nbefore it was multiplied by V as that provided a\nnormalized value indicating what proportion of at-\ntention each token payed to all others.\nThe attention value for each token was calcu-\nlated by averaging across attention heads and ma-\ntrix rows. This calculation produced a single vector\nrepresenting the amount of attention allocated to\neach token by all others in the sentence. Our pro-\ncedure differs from Sood et al. (2020a) who used\nthe maximum attention from each word instead of\nthe mean. Some preliminary analyses suggested\nthat the mean attention values provided more stable\nresults across datasets. The results using the maxi-\nmum values are available on our GitHub repository\nfor comparison purposes. If a word was tokenized\ninto sub-words, those sub-words were also aver-\naged to produce a single value. The special tokens\n[CLS] and [SEP] were used for the attention calcu-\nlations but dropped from the ﬁnal word-level atten-\ntion vector. Finally, attention was normalized by\nsentence by calculating the proportion of attention\nallocated to each word.\nDwell time was used for the overt human atten-\ntion data. Dwell time is a measurement of the total\ntime that a participant’s eye ﬁxated on a word. This\nchoice was necessary for consistency between anal-\nyses as it was the only measure to appear in all\ndatasets. Dwell time data was extracted for each\nword in a sentence, with one sentence being pro-\nduced for each participant in the original data. The\ndwell time data were also normalized by sentence\nby calculating the dwell time proportion for each\nword. The data from individual participants were\nthen averaged to create one normalized sentence\nfor each sentence in the text.\nData from the transformers and human partici-\npants were then matched so that each word in the\ntext had a sentence normalized attention score from\nthe transformers and the average participant. After\nmatching, all the words from a text were pooled\nand used to calculate the Spearman correlation val-\nues. One-word sentences were removed as both\nscores were always 1.0, which inﬂated the correla-\ntion scores.\n3.4 Results and Discussion\nThere were signiﬁcant positive correlations be-\ntween the total dwell time and the attention from\nall layers of the different models. This ﬁnding was\nan apparent departure from the results of Sood et al.\n(2020a) who reported a non-signiﬁcant correlation\nof -.16 between the last layer of XLNet and their\ndataset. For comparison, we obtained a .428 cor-\nrelation for their Study 1 data and .327 for their\nStudy 2 data from XLNet’s last layer. Although\nthey did not directly specify the normalization they\nused, we suspect that the difference in results is due\nto us using sentence-level normalization and Sood\net al. (2020a) using paragraph normalization. For\ncomparison, we ran the same procedure using para-\ngraph normalization and obtained non-signiﬁcant\ncorrelations just as they did. In general, many of\nthe correlations obtained using sentence normal-\nization become much weaker when using the para-\ngraph normalization. This ﬁnding corresponds well\nwith the Sen et al. (2020) ﬁnding that attention for\n80\nFigure 1: The relative position of the layer with the highest correlation. 0 is the ﬁrst layer, 1 is the last layer. There\nare multiple dots for each model because each dot represents the highest correlation from a different dataset.\nFigure 2: The correlations between the ﬁrst layer attention patterns and eye gaze data from all models. The box\nplots represent the spread of correlation values across datasets.\nnon-transformer neural networks became less cor-\nrelated with eye movements as the length of the\ntext increased. All analyses presented here refer to\nsentence-level correlations. Paragraph-level analy-\nses can be found in our GitHub repository.\nOur ﬁrst analysis investigated which attention\nlayer was most closely correlated with the eye gaze\ndata. Figure 1 shows the relative position of the\nlayer with the highest correlation by model. In\nmany cases, the highest correlation was produced\nby the earlier layers of each model, in 66.2% of\ncases this was the ﬁrst layer (position 0). Notable\nexceptions to this rule are the multilingual versions\nof BERT and RoBERTa (i.e., XLM) and many com-\npact models. Although further studies are needed,\nthe ﬁnding that multilingual variants of models do\nnot behave like monolingual variants is in line with\nsome previously reported studies (Conneau et al.,\n2020; Hollenstein et al., 2021b; Vuli´c et al., 2020),\nwhere some studies report multilingual beneﬁts and\nwhile others do not.\nFurther investigations found that when the ﬁrst\n81\nTable 2: First layer correlations By dataset. Strongest correlations have been bolded.\nModel GECO Mishra Provo Sood S1 Sood S2 ZuCo S1 ZuCo S2 ZuCo S3 Frank et al\nalbert-v1 0.744 0.754 0.497 0.450 0.326 0.501 0.580 0.325 0.652\nalbert-v2 0.748 0.739 0.492 0.460 0.329 0.503 0.585 0.326 0.637\nbart 0.729 0.758 0.526 0.451 0.323 0.511 0.550 0.313 0.638\nbert-cased 0.802 0.783 0.668 0.584 0.410 0.643 0.679 0.328 0.744\nbert-multilingual-cased 0.753 0.727 0.525 0.459 0.338 0.489 0.622 0.324 0.603\nbert-uncased 0.816 0.791 0.710 0.626 0.434 0.693 0.722 0.324 0.746\nbirdbird-roberta 0.775 0.774 0.600 0.511 0.363 0.582 0.565 0.319 0.693\ndeberta-v1 0.731 0.735 0.511 0.432 0.310 0.502 0.533 0.289 0.549\ndeberta-v2 0.824 0.770 0.708 0.601 0.423 0.688 0.712 0.306 0.660\ndistilbert-cased 0.786 0.772 0.623 0.523 0.378 0.629 0.632 0.341 0.670\ndistilbert-multilingual-cased 0.742 0.740 0.513 0.452 0.337 0.487 0.620 0.333 0.602\ndistilbert-uncased 0.796 0.780 0.649 0.576 0.396 0.649 0.678 0.319 0.725\nroberta 0.709 0.755 0.523 0.453 0.329 0.504 0.537 0.291 0.632\nroberta-muppet 0.712 0.763 0.527 0.460 0.329 0.501 0.542 0.297 0.665\nsqueezebert 0.730 0.769 0.505 0.458 0.320 0.499 0.549 0.348 0.650\nxlm 0.690 0.715 0.391 0.358 0.271 0.379 0.476 0.313 0.532\nxlnet 0.678 0.736 0.436 0.369 0.287 0.408 0.470 0.297 0.584\nlayer did not produce the highest correlation, the\nﬁrst-layer correlation value was on average, 0.055\nlower than the best correlation value. In 75% of\ncases, this difference was less than 0.082. There-\nfore, the ﬁrst layer value appears to be a good rep-\nresentation of the correlation between the model\nand the eye gaze data. An extreme example of\nthis were the ALBERT variants, which, likely due\nto weight sharing during training, have virtually\nidentical correlations from attention values from\neach of its levels (Figure 3). Due to its general best\nperformance, the ﬁrst layer results have been used\nat the best performance for all models. Analyses\nusing the actual best performance can be observed\nin our GitHub repository, although those results are\nhighly similar to those reported here.\nOur next analysis compared performance across\nmodels based on the ﬁrst layer correlations. Figure\n2 shows that, in general, the size of the model does\nnot determine the correlation between the human\neye and transformer attention. Evidence for this\ncan be seen in minor differences between various-\nsized variants of the same model. For example,\nthe cased and uncased versions of BERT-base and\nBERT-large are very similar, despite the large vari-\nant containing 340 million parameters compared\nto the base variants’ 110 million. Similar obser-\nvations can be observed across the other models,\nespecially DeBERTa, where the largest variants\nhave 1.5 billion parameters, and the smaller ones\ncontain less than 1/3 of that number. This observa-\ntion was conﬁrmed with a non-signiﬁcant sign test\n(p = .090) that compared each variant to the next\nsmallest variant in its model type. Due to this simi-\nlarity, results in Table 2 reports a single value per\nmodel type that is an average for each size variant.\nTable 3 shows the highest correlation by dataset. In\nmost cases, this model was either BERT-uncased\nor DeBERTa-V2.\nWhile the number of parameters is not what de-\ntermines the correlations, comparing across models\nin Figure 2 suggests that training is essential for\ndetermining those relationships. For example, the\nBERT models have identical architectures to vari-\nous RoBERTa models, yet Table 2 shows that the\nBERT correlations were consistently higher than\nthe RoBERTa based models. The other clear ex-\namples of training effects can be seen in the dif-\nferences between DeBERTa V1 and V2, where V2\nmodels use the Scale-invariant-Fine-Tuning (SiFT)\nalgorithm introduced in the original paper. Interest-\ningly, the addition of the SiFT algorithm allowed\nDeBERTa V2 to surpass human performance on\nthe SuperGLUE benchmarks (Wang et al., 2019a),\nand Table 3 shows that this model was often the\nsecond-highest correlated model. While it would\nbe great to ﬁnd a direct relationship between how\nhuman-like a model’s performance is and how cor-\nrelated its attention patterns are to eye movements,\nthat is not the case. Excluding the compact models,\nthe BERT descendants outperform it on many of\nthe benchmarks, yet only DeBERTA comes close\nto having stronger correlations to human eye move-\nments. In most cases, attention patterns less corre-\nlated with overt human attention produced better\noverall performance on NLP tasks.\nTables 2 and 3 show the rankings by correlation\nare similar between datasets, with BERT-uncased\n82\nTable 3: The three models with strongest correlation to eye-tracking data for each dataset. The uncased version of\nBERT produced the strongest correlation in 7 out of 9 cases.\nGECO Mishra Provo Sood S1 Sood S2 ZuCo S1 ZuCo S2 ZuCo S3 Frank-et-al\n1 deberta-v2 bert-uncased bert-uncased bert-uncased bert-uncased bert-uncased bert-uncased squeezebert bert-uncased\n2 bert-uncased bert-cased deberta-v2 deberta-v2 deberta-v2 deberta-v2 deberta-v2 distilbert-cased bert-cased\n3 bert-cased distilbert-uncased bert-cased bert-cased bert-cased distilbert-uncased bert-cased distilbert-multilingual distilbert-uncased\nproducing the highest correlation in all but two\ncases. In one of the exceptions, the GECO dataset,\nBERT-uncased, was ranked second. In the other ex-\nception, ZuC0 Task 3, the ranking was much lower.\nIn general, the correlations from ZuCo Task 3 differ\ngreatly from the other datasets. The correlations\nare lower for all models, and the model rankings\nare very different, with two of the compact models,\nSqueezeBERT and DistillBERT, ranking highest,\nand BERT-uncased, ninth. Task 3’s participants\nwere the same as Tasks 1 and 2. Those ﬁrst two\ntasks produced results closer to the other datasets,\nmeaning Task 3’s lower correlations were likely\ndue to the task itself.\nInterestingly, in Task 3, the participants were\npresented with the question on the screen, allowing\nthem to direct their eye gaze to ﬁnd the informa-\ntion they required. This contrasts with most of\nthe other datasets where the questions about the\ndata were presented after reading. The only excep-\ntions to this were some tasks by Sood et al. (2020a)\nwhere the question appeared on screen in Study 2\nand in 2/3s of the tests in Study 1. Furthermore,\nthe correlations from Sood et al. (2020a) Studies\n2 and 1 were also the second and third lowest of\nthe datasets, respectively (Table 2). While further\nstudy is needed, the lower correlations from SOOD\net al. and ZuCo Task 3 may indicate that while\ntransformer attention patterns produce strong cor-\nrelations when reading typically, the relationship\ndrops when the reader actively searches for infor-\nmation.\nOur ﬁnal analysis looked at correlations across\nlevels of BERT (Figure 3). The results of Toneva\nand Wehbe (2019) suggest that the middle layers\nof BERT provided the best features for predict-\ning brain activity in humans. They speculated that\nthese relationships could mean that the middle lay-\ners of BERT could be related to the kinds of pro-\ncessing that occurs in those brain levels. Our re-\nsults show that the attention patterns from BERT’s\nﬁrst layer were closely related to eye gaze data.\nAgain, while speculative, our results combined\nwith Toneva and Wehbe (2019) would suggest that\nfor BERT at least, the lower levels correspond best\nFigure 3: The average correlations across layers for\nbert-base-cased and albert-base-v1.\nto text information entering the eyes. In contrast,\nthe middle layers correspond to speciﬁc processing.\nWith that said, not all transformers produced the\nstrongest correlations from their ﬁrst layer. For\nexample, as mentioned above, Figure 3 shows the\ndata from ALBERT-V1 where the correlations from\nall levels were relatively the same.\n4 Conclusion\nThis paper analyzed the correlations between atten-\ntion in pre-trained transformers and human atten-\ntion derived from eye gaze. We found correlations\nbetween the two that were generally stronger in\nthe earlier layers of the model and, in most cases,\nstrongest in the ﬁrst layer. These correlations were\nunaffected by the model’s size, as different sized\nvariants of models produced similar correlations.\nThe training the models received did appear to\nmatter, although the present study cannot deter-\nmine the full extent of that relationship. We found\nthat correlations were weaker from eye-tracking\nstudies where the participants could actively guide\ntheir reading towards seeking the information they\nneeded than when presented with questions after\nreading. While we found a relationship between\novert human attention and attention in some pre-\ntrained transformers, additional research would be\nrequired before models of eye gaze could be used\nto replace attention in transformers.\n83\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava,\nXilun Chen, Luke Zettlemoyer, and Sonal Gupta.\n2021. Muppet: Massive multi-task representations\nwith pre-ﬁnetuning. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 5799–5811, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the 12th ACM SIGKDD international\nconference on Knowledge discovery and data min-\ning, pages 535–541.\nShivani Choudhary, Kushagri Tandon, Raksha Agar-\nwal, and Niladri Chatterjee. 2021. Mtl782_iitd at\ncmcl 2021 shared task: Prediction of eye-tracking\nfeatures using bert embeddings and linguistic fea-\ntures. In Proceedings of the Workshop on Cogni-\ntive Modeling and Computational Linguistics, pages\n114–119.\nSpearman Rank Correlation Coefﬁcient. 2008. The\nconcise encyclopedia of statistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nUschi Cop, Nicolas Dirix, Denis Drieghe, and Wouter\nDuyck. 2017. Presenting geco: An eyetracking cor-\npus of monolingual and bilingual sentence reading.\nBehavior research methods, 49(2):602–615.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image is\nworth 16x16 words: Transformers for image recog-\nnition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nGeoffrey B Duggan and Stephen J Payne. 2011. Skim\nreading by satisﬁcing: evidence from eye tracking.\nIn Proceedings of the SIGCHI conference on human\nfactors in computing systems, pages 1141–1150.\nGemma Fitzsimmons, Mark J. Weal, and Denis\nDrieghe. 2014. Skim reading: an adaptive strategy\nfor reading on the web. In ACM Web Science Con-\nference, WebSci ’14, Bloomington, IN, USA, June 23-\n26, 2014, pages 211–219. ACM.\nStefan L Frank, Irene Fernandez Monsalve, Robin L\nThompson, and Gabriella Vigliocco. 2013. Read-\ning time data for evaluating broad-coverage models\nof english sentence processing. Behavior research\nmethods, 45(4):1182–1190.\nBashir Muftah Ghariba, Mohamed S Shehata, and Pe-\nter McGuire. 2020. A novel fully convolutional net-\nwork for visual saliency prediction. PeerJ computer\nscience, 6:e280.\nMichael Hahn and Frank Keller. 2016. Modeling hu-\nman reading with neural attention. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, pages 85–95.\nMichael Hahn and Frank Keller. 2018. Modeling\ntask effects in human reading with neural attention.\narXiv preprint arXiv:1808.00054.\nYiding Hao, Simon Mendelsohn, Rachel Sterneck,\nRandi Martinez, and Robert Frank. 2020. Probabilis-\ntic predictions of people perusing: Evaluating met-\nrics of language model performance for psycholin-\nguistic modeling. arXiv preprint arXiv:2009.03954.\nJonathan Harel, Christof Koch, and Pietro Perona.\n2006. Graph-based visual saliency. In Advances\nin Neural Information Processing Systems 19, Pro-\nceedings of the Twentieth Annual Conference on\nNeural Information Processing Systems, Vancouver,\nBritish Columbia, Canada, December 4-7, 2006 ,\npages 545–552. MIT Press.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n84\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nNora Hollenstein, Federico Pirovano, Ce Zhang, Lena\nJäger, and Lisa Beinborn. 2021a. Multilingual lan-\nguage models predict human reading behavior. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 106–123, Online. Association for Computa-\ntional Linguistics.\nNora Hollenstein, Federico Pirovano, Ce Zhang, Lena\nJäger, and Lisa Beinborn. 2021b. Multilingual lan-\nguage models predict human reading behavior. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 106–123.\nNora Hollenstein, Jonathan Rotsztejn, Marius Troen-\ndle, Andreas Pedroni, Ce Zhang, and Nicolas Langer.\n2018. Zuco, a simultaneous eeg and eye-tracking re-\nsource for natural sentence reading. Scientiﬁc data,\n5(1):1–13.\nXun Huang, Chengyao Shen, Xavier Boix, and\nQi Zhao. 2015. Salicon: Reducing the semantic\ngap in saliency prediction by adapting deep neural\nnetworks. In Proceedings of the IEEE international\nconference on computer vision, pages 262–270.\nForrest Iandola, Albert Shaw, Ravi Krishna, and Kurt\nKeutzer. 2020. SqueezeBERT: What can computer\nvision teach NLP about efﬁcient neural networks?\nIn Proceedings of SustaiNLP: Workshop on Simple\nand Efﬁcient Natural Language Processing , pages\n124–135, Online. Association for Computational\nLinguistics.\nYifan Jiang, Shiyu Chang, and Zhangyang Wang. 2021.\nTransgan: Two pure transformers can make one\nstrong gan, and that can scale up. Advances in Neu-\nral Information Processing Systems, 34.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nGuanbin Li and Yizhou Yu. 2016. Visual saliency de-\ntection based on multiscale deep cnn features. IEEE\ntransactions on image processing , 25(11):5012–\n5024.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSteven G Luke and Kiel Christianson. 2018. The provo\ncorpus: A large eye-tracking corpus with predictabil-\nity norms. Behavior research methods, 50(2):826–\n833.\nAbhijit Mishra, Diptesh Kanojia, and Pushpak Bhat-\ntacharyya. 2016. Predicting readers’ sarcasm un-\nderstandability by modeling gaze behavior. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 30.\nMichael I Posner. 1980. Orienting of attention. Quar-\nterly journal of experimental psychology , 32(1):3–\n25.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nKeith Rayner. 2009. The 35th sir frederick bartlett lec-\nture: Eye movements and attention in reading, scene\nperception, and visual search. Quarterly journal of\nexperimental psychology, 62(8):1457–1506.\nErik D Reichle, Alexander Pollatsek, Donald L Fisher,\nand Keith Rayner. 1998. Toward a model of eye\nmovement control in reading. Psychological review,\n105(1):125.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Ca-\nrina Kauf, Eghbal A Hosseini, Nancy Kanwisher,\nJoshua B Tenenbaum, and Evelina Fedorenko. 2021.\nThe neural architecture of language: Integrative\nmodeling converges on predictive processing. Pro-\nceedings of the National Academy of Sciences ,\n118(45).\nCansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan\nKong, and Elke Rundensteiner. 2020. Human at-\ntention maps for text classiﬁcation: Do humans and\nneural networks focus on the same words? In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4596–\n4608.\n85\nMin Joon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2018. Neural speed reading via skim-rnn.\nIn 6th International Conference on Learning Rep-\nresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceed-\nings.\nEkta Sood, Simon Tannert, Diego Frassinelli, Andreas\nBulling, and Ngoc Thang Vu. 2020a. Interpreting\nattention models with human visual attention in ma-\nchine reading comprehension. In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 12–25, Online. Association\nfor Computational Linguistics.\nEkta Sood, Simon Tannert, Philipp Müller, and An-\ndreas Bulling. 2020b. Improving natural language\nprocessing tasks with human gaze-guided neural at-\ntention. Advances in Neural Information Processing\nSystems, 33:6327–6341.\nNeset Özkan Tan, Joshua Bensemann, Diana\nBenavides-Prado, Yang Chen, Mark Gahegan,\nLia Lee, Alex Yuxuan Peng, Patricia Riddle, and\nMichael Witbrock. 2021. An explainability analysis\nof a sentiment prediction task using a transformer-\nbased attention ﬁlter. In Proceedings of the Ninth\nAnnual Conference on Advances in Cognitive\nSystems.\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,\nAntonio Torralba, Raquel Urtasun, and Sanja Fidler.\n2016. Movieqa: Understanding stories in movies\nthrough question-answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 4631–4640.\nHamed R Tavakoli, Ali Borji, Jorma Laaksonen, and\nEsa Rahtu. 2017. Exploiting inter-image similarity\nand ensemble of extreme learners for ﬁxation predic-\ntion using deep features. Neurocomputing, 244:10–\n18.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism quarterly,\n30(4):415–433.\nMariya Toneva and Leila Wehbe. 2019. Interpret-\ning and improving natural-language processing (in\nmachines) with natural language-processing (in the\nbrain). Advances in Neural Information Processing\nSystems, 32:14954–14964.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nNicolas Vecoven, Damien Ernst, Antoine Wehenkel,\nand Guillaume Drion. 2020. Introducing neuromod-\nulation in deep neural networks to learn adaptive be-\nhaviours. PloS one, 15(1):e0227922.\nIvan Vuli´c, Simon Baker, Edoardo Maria Ponti, Ulla\nPetti, Ira Leviant, Kelly Wing, Olga Majewska, Eden\nBar, Matt Malone, Thierry Poibeau, et al. 2020.\nMulti-simlex: A large-scale evaluation of multi-\nlingual and crosslingual lexical semantic similarity.\nComputational Linguistics, 46(4):847–897.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019a. Superglue: a\nstickier benchmark for general-purpose language un-\nderstanding systems. In Proceedings of the 33rd In-\nternational Conference on Neural Information Pro-\ncessing Systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in neural infor-\nmation processing systems, 32.\nAdams Wei Yu, Hongrae Lee, and Quoc Le. 2017.\nLearning to skim text. InProceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1880–\n1890.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nYukun Zheng, Jiaxin Mao, Yiqun Liu, Zixin Ye, Min\nZhang, and Shaoping Ma. 2019. Human behavior in-\nspired machine reading comprehension. In Proceed-\nings of the 42nd International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 425–434.\n86\nA Investigating the Effect of Injecting\nEye Gaze Bias During Training\nAs a preliminary experiment, we investigated the\neffect of injecting human eye gaze bias during train-\ning on test accuracy. We used the BERT model (De-\nvlin et al., 2019) and the sarcasm-detection dataset\npublished in Mishra et al. (2016) as a case study.\nA.1 Method\nThe Mishra et al. (2016) dataset was originally\nproposed to predict non-native English speakers’\nunderstanding of sarcasm by using eye-tracking in-\nformation. The dataset contains information on the\nﬁxation duration of each word for each participant.\nWe injected the eye-gazing bias during training by\noptimising the following loss function:\nL= H(y,ˆy) +αH(p,ˆp) (2)\nwhere H(y,ˆy) is the cross-entropy loss of the\nbinary classiﬁcation task of sarcasm detection, and\nH(p,ˆp) computes the divergence of the ﬁrst-layer\nattention values from the distribution of the nor-\nmalised ﬁxation duration values given a sentence.\nThe hyperparameter αcontrols the weight of the\nsecond term in the loss function.\nOur experiments only used the ﬁxation dura-\ntion values from Participant 6 because they had\nthe highest overall accuracy for sarcasm detection\n(90.29%). All the hyperparameters were tuned on a\nvalidation set extracted from the training set before\nbeing applied to the entire training set.\nA.2 Results\nThe results are plotted in Figure 4. As expected, the\nmodels ﬁne-tuned from pre-trained BERT models\nhad signiﬁcantly better test accuracy on both the\nsmall and large training sets than models trained\nfrom scratch on the Mishra et al. (2016) dataset.\nA t-test conﬁrmed that when the models were\ntrained on the large training set without pre-\ntraining, an eye gaze bias injection during train-\ning hurt the performance ( p < .05 ). With pre-\ntraining, both models in Figure 4(b) performed\nbetter than the best participant in the Mishra et al.\n(2016) dataset. The bias injection still lowered\nthe mean accuracy, although the difference was\nno longer statistically signiﬁcant. When the small\ntraining set was used to train the models, we found\nno signiﬁcant difference after the bias injection.\nComparing our results to Sood et al. (2020b)\nsuggests that training a model to predict eye gaze\nNo bias With bias\n0.86\n0.88\n0.90\n0.92\n0.94Test accuracy\n(a) Large training set without pre-training\nNo bias With bias\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96Test accuracy\n(b) Large training set with pre-training\nNo bias With bias\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84Test accuracy\n(c) Small training set without pre-training\nNo bias With bias\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88Test accuracy\n(d) Small training set with pre-training\nFigure 4: Comparison of the BERT models trained\nwith eye gaze bias against the models trained without\nin terms of test accuracy. Models in plots (a) and (b)\nwere trained on 693 examples, and the results were ob-\ntained after 20 runs. Models in plots (c) and (d) were\ntrained on only 70 examples, and the experiments were\nrepeated 50 times. The same test set (300 examples)\nwas used for all the experiments.\nimproves text compression performance, whereas\nusing eye gaze data to regulate sarcasm detection\ndecreased performance. It is unknown whether the\ndifference in results is due to our task choice or to\nour method of using human data.\n87",
  "topic": "Gaze",
  "concepts": [
    {
      "name": "Gaze",
      "score": 0.774638831615448
    },
    {
      "name": "Chen",
      "score": 0.5658037066459656
    },
    {
      "name": "Cognitive science",
      "score": 0.5536938309669495
    },
    {
      "name": "Transformer",
      "score": 0.5293653011322021
    },
    {
      "name": "Computer science",
      "score": 0.47885796427726746
    },
    {
      "name": "Cognition",
      "score": 0.42374420166015625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38663363456726074
    },
    {
      "name": "Psychology",
      "score": 0.38080257177352905
    },
    {
      "name": "Linguistics",
      "score": 0.33503079414367676
    },
    {
      "name": "Philosophy",
      "score": 0.18315589427947998
    },
    {
      "name": "Engineering",
      "score": 0.1263706088066101
    },
    {
      "name": "Neuroscience",
      "score": 0.07728037238121033
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154130895",
      "name": "University of Auckland",
      "country": "NZ"
    }
  ],
  "cited_by": 14
}