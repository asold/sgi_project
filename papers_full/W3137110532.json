{
  "title": "LAMBERT: Layout-Aware Language Modeling for Information Extraction",
  "url": "https://openalex.org/W3137110532",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2258138868",
      "name": "Łukasz Garncarek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5046898079",
      "name": "Rafał Powalski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5054685387",
      "name": "Tomasz Stanisławek",
      "affiliations": [
        "Warsaw University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2616488486",
      "name": "Bartosz Topolski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3007653768",
      "name": "Piotr Halama",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298576206",
      "name": "Michał Turski",
      "affiliations": [
        "Adam Mickiewicz University in Poznań"
      ]
    },
    {
      "id": "https://openalex.org/A2077610807",
      "name": "Filip Graliński",
      "affiliations": [
        "Adam Mickiewicz University in Poznań"
      ]
    },
    {
      "id": "https://openalex.org/A2258138868",
      "name": "Łukasz Garncarek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5046898079",
      "name": "Rafał Powalski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5054685387",
      "name": "Tomasz Stanisławek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2616488486",
      "name": "Bartosz Topolski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3007653768",
      "name": "Piotr Halama",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298576206",
      "name": "Michał Turski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2077610807",
      "name": "Filip Graliński",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2110412794",
    "https://openalex.org/W2059719138",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2164737869",
    "https://openalex.org/W2901299405",
    "https://openalex.org/W2006014785",
    "https://openalex.org/W2891117443",
    "https://openalex.org/W1966382373",
    "https://openalex.org/W2922714365",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2058053171",
    "https://openalex.org/W3000758063",
    "https://openalex.org/W1965145972",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2087291337",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3201693581",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3176851559",
    "https://openalex.org/W2997154779",
    "https://openalex.org/W3163650427",
    "https://openalex.org/W3093218477",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3113463745",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2999038738",
    "https://openalex.org/W3010341014",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3104953317"
  ],
  "abstract": null,
  "full_text": "LAMBERT: Layout-Aware Language Modeling\nfor Information Extraction\nŁukasz Garncarek1⋆, Rafał Powalski1, Tomasz Stanisławek1,2,\nBartosz Topolski1, Piotr Halama1, Michał Turski1,3, and Filip Graliński1,3\n1 Applica.ai, Zajęcza 15, 00-351 Warszawa, Poland\nfirstname.lastname@applica.ai\n2 Warsaw University of Technology, Koszykowa 75, 00-662 Warszawa, Poland\nfirstname.lastname@pw.edu.pl\n3 Adam Mickiewicz University, 1 Wieniawskiego, 61-712 Poznań, Poland\nfirstname.lastname@amu.edu.pl\nAbstract. We introduce a simple new approach to the problem of un-\nderstanding documents where non-trivial layout inﬂuences the local se-\nmantics. To this end, we modify the Transformer encoder architecture\nin a way that allows it to use layout features obtained from an OCR\nsystem, without the need to re-learn language semantics from scratch.\nWe only augment the input of the model with the coordinates of to-\nken bounding boxes, avoiding, in this way, the use of raw images. This\nleads to a layout-aware language model which can then be ﬁne-tuned on\ndownstream tasks.\nThe model is evaluated on an end-to-end information extraction task\nusing four publicly available datasets: Kleister NDA, Kleister Charity,\nSROIE and CORD. We show that our model achieves superior perfor-\nmance on datasets consisting of visually rich documents, while also out-\nperforming the baseline RoBERTa on documents with ﬂat layout (NDA\nF1 increase from 78.50 to 80.42). Our solution ranked ﬁrst on the public\nleaderboardfortheKeyInformationExtractionfromtheSROIEdataset,\nimproving the SOTAF1-score from 97.81 to 98.17.\nKeywords: Language model · Layout · Key information extraction ·\nTransformer· Visually rich document· Document understanding\n1 Introduction\nThe sequential structure of text leads to it being treated as a sequence of tokens,\ncharacters, or more recently, subword units. In many problems related to Nat-\nural Language Processing (NLP), this linear perspective was enough to enable\nsigniﬁcant breakthroughs, such as the introduction of the neural Transformer ar-\nchitecture [28]. In this setting, the task of computing token embeddings is solved\nby Transformer encoders, such as BERT [6] and its derivatives, achieving top\nscores on the GLUE benchmark [29].\n⋆ corresponding author; the ﬁrst four authors have equally contributed to the paper.\narXiv:2002.08087v5  [cs.CL]  28 May 2021\n2 Ł. Garncarek et al.\nThey all deal with problems arising in texts deﬁned as sequences of words.\nHowever, in many cases there is a structure more intricate than just a linear\nordering of tokens. Take, for instance, printed or richly-formatted documents,\nwhere the relative positions of tokens contained in tables, spacing between para-\ngraphs, or diﬀerent styles of headers, all carry useful information. After all, the\ngoal of endowing texts with layout and formatting is to improve readability.\nIn this article we present one of the ﬁrst attempts to enrich the state-of-the-\nart methods of NLP with layout understanding mechanisms, contemporaneous\nwith [32], to which we compare our model. Our approach injects the layout\ninformation into a pretrained instance of RoBERTa. We ﬁne-tune the augmented\nmodel on a dataset consisting of documents with non-trivial layout.\nWe evaluate our model on the end-to-end information extraction task, where\nthe training set consists of documents and the target values of the properties to\nbe extracted, without any additional annotations specifying the locations where\nthe information on these properties can be found in the documents. We compare\nthe results with a baseline RoBERTa model, which relies on the sequential order\nof tokens obtained from the OCR alone (and does not use the layout features),\nand with the solution of [32,31]. LAMBERT achieves superior performance on\nvisually rich documents, without sacriﬁcing results on more linear texts.\n1.1 Related work\nThere are two main lines of research into understanding documents with non-\ntrivial layout. The ﬁrst one is Document Layout Analysis (DLA), the goal of\nwhich is to identify contiguous blocks of text and other non-textual objects on\nthe page and determine their function and order in the document. The obtained\nsegmentation can be combined with the textual information contained in the\ndetected blocks. This kind of method has recently been employed in [17].\nMany services employ DLA functionality for OCR (which requires document\nsegmentation), table detection or form ﬁeld detection, and their capabilities are\nstill expanding. The most notable examples are Amazon Textract [1], the Google\nCloud Document Understanding AI platform [8], and Microsoft Cognitive Ser-\nvices [20]. However, each has limitations, such as the need to create rules for\nextracting information from the tables recognized by the system, or use training\ndatasets with annotated document segments. More recent works on information\nextraction using DLA include, among others, [14,3,10,2,19,22,25]. They concen-\ntrate on speciﬁc types of documents, such as invoices or forms, where the layout\nplays a relatively greater role: more general documents may contain tables, but\nthey can also have large amounts of unstructured text.\nThe second idea is to directly combine the methods of Computer Vision\nand NLP. This could be done, for instance, by representing a text-ﬁlled page\nas a multi-channel image, with channels corresponding to the features encoding\nthe semantics of the underlying text, and, subsequently, using convolutional\nnetworks. This method was used, among others, by Chargrid and BERTgrid\nmodels [15,5]. On the other hand, LayoutLM [32] and TRIE [34] used the image\nrecognition features of the page image itself. A more complex approach was taken\nLAMBERT: Layout-Aware Language Modeling 3\nby PICK [33], which separately processes the text and images of blocks identiﬁed\nin the document. In this way it computes the vertex embeddings of the block\ngraph, which is then processed with a graph neural network.\nOur idea is also related to the one used in [24], though in a diﬀerent set-\nting. They considered texts accompanied by audio-visual signal injected into a\npretrained BERT instance, by combining it with the input embeddings.\nLAMBERT has a diﬀerent approach. It uses neither the raw document image,\nnor the block structure that has to be somehow inferred. It relies on the tokens\nand their bounding boxes alone, both of which are easily obtainable from any\nreasonable OCR system.\n1.2 Contribution\nOur main contribution is the introduction of aLayout-Aware Language Model,\na general-purpose language model that views text not simply as a sequence\nof words, but as a collection of tokens on a two-dimensional page. As such it\nis able to process plain text documents, but also tables, headers, forms and\nvarious other visual elements. The implementation of the model is available at\nhttps://github.com/applicaai/lambert.\nA key feature of this solution is that it retains the crucial trait of language\nmodels: the ability to learn in an unsupervised setting. This allows the exploita-\ntion of abundantly available unannotated public documents, and a transfer of\nthe learned representations to downstream tasks. Another advantage is the sim-\nplicity of this approach, which requires only an augmentation of the input with\ntoken bounding boxes. In particular, no images are needed. This eliminates an\nimportant performance factor in industrial systems, where large volumes of doc-\numents have to be sent over a network between distributed processing services.\nAnothercontributionofthepaperisanextensiveablationstudyoftheimpact\nof augmenting RoBERTa with various types of additional positional embeddings\nonmodelperformanceontheSROIE[12],CORD[21],KleisterNDAandKleister\nCharity datasets [27].\nFinally, we created a new dataset for the unsupervised training of layout-\naware language models. We will share a 200k document subset, amounting to\n2M visually rich pages, accompanied by a dual classiﬁcation of documents: busi-\nness/legal documents with complex structure; and others. Due to IIT-CDIP Test\nCollection dataset [16] accessibility problems4, this would constitute the largest\nwidely available dataset for training layout-aware language models. It would al-\nlow researchers to compare the performance of their solutions not only on the\nsame test sets, but also with the same training set. The dataset is published\nat https://github.com/applicaai/lambert , together with a more detailed\ndescription that is too long for this paper.\n4 the linkhttps://ir.nist.gov/cdip/ seems to be dead (access on Feb 17, 2021)\n4 Ł. Garncarek et al.\n2 Proposed method\nWe inject the layout information into the model in two ways. Firstly, we modify\nthe input embeddings of the original RoBERTa model by adding the layout term.\nWe also experiment with completely removing the sequential embedding term.\nSecondly, we apply relative attention bias, used in [26,11,23] in the context of\nsequential position. The ﬁnal architecture is depicted in Figure 1.\nSequential \nembedding\nSemantic\nembedding\n \nLayout \nembedding Relative 1D bias Relative 2D bias\nAdd AddAdapter layer\nToken IDs Sequential\npositions\nOutput embeddings\nRelative bias\nBounding box \ncoordinates\nAttention\nAdd & Norm\nAdd & Norm\nFeed Forward \nLinear Linear Linear\nAttention scores\nAdd relative bias\nSoftmax & matrix\nmultiplication\nConcatenate\nLinear\nFig.1: LAMBERT model architecture. Diﬀerences with the plain RoBERTa\nmodel are indicated by white text on dark blue background.N = 12 is the\nnumber of transformer encoder layers, andh = 12 is the number of attention\nheads in each encoder layer.Q, K, andV are, respectively, the queries, keys and\nvalues obtained by projecting the self-attention inputs.\n2.1 Background\nThe basic Transformer encoder, used in, for instance, BERT [6] and RoBERTa\n[18], is a sequence-to-sequence model transforming a sequence of input embed-\ndings xi ∈Rn into a sequence of output embeddingsyi ∈Rm of the same length,\nLAMBERT: Layout-Aware Language Modeling 5\nfor the input/output dimensionsn and m. One of the main distinctive features\nof this architecture is that it discards the order of its input vectors. This allows\nparallelization levels unattainable for recurrent neural networks.\nIn such a setting, the information about the order of tokens is preserved not\nby the structure of the input. Instead, it is explicitly passed to the model, by\ndeﬁning the input embeddings as\nxi = si + pi, (1)\nwhere si ∈ Rn is the semantic embedding of the token at position i, taken\nfrom a trainable embedding layer, while pi ∈ Rn is a positional embedding,\ndepending only on i. In order to avoid confusion, we will, henceforth, use the\nterm sequential embeddings instead of positional embeddings, as the positional\nmight be understood as relating to the 2-dimensional position on the page, which\nwe will deal with separately.\nSince in RoBERTa, on which we base our approach, the embeddingspi are\ntrainable, the number of pretrained embeddings (in this case 512) deﬁnes a\nlimit on the length of the input sequence. In general, there are many ways to\ncircumvent this limit, such as using predeﬁned [28] or relative [4] sequential\nembeddings.\n2.2 Modiﬁcation of input embeddings\nWe replace the input embeddings deﬁned in (1) with\nxi = si + pi + L(ℓi). (2)\nHere, ℓi ∈Rk stands forlayout embeddings, which are described in detail in the\nnext subsection. They carry the information about the position of thei-th token\non the page.\nThe dimensionkof the layout embeddings is allowed to diﬀer from the input\nembedding dimension n, and this diﬀerence is dealt with by a trainable linear\nlayerL: Rk →Rn. However, our main motivation to introduce the adapter layer\nL was to gently increase the strength of the signal of layout embeddings during\ntraining. In this way, we initially avoided presenting the model with inputs that\nit was not prepared to deal with. Moreover, in theory, in the case of non-trainable\nlayout embeddings, the adapter layer may be able to learn to projectℓi onto a\nsubspace of the embedding space that reduces interference with the other terms\nin (2). For instance, it is possible for the image of the adapter layer to learn\nto be approximately orthogonal to the sum of the remaining terms. This would\nminimize any information loss caused by adding multiple vectors. While this was\nour theoretical motivation, and it would be interesting to investigate in detail\nhow much of it actually holds, such detailed considerations of a single model\ncomponent exceed the scope of this paper. We included the impact of using the\nadapter layer in the ablation study.\nWe initialize the weight matrix of L according to a normal distribution\nN(0,σ2), with the standard deviationσ being a hyperparameter. We have to\n6 Ł. Garncarek et al.\nchoose σ carefully, so that in the initial phase of training, theL(ℓi) term does\nnot interfere overly with the already learned representations. We experimentally\ndetermined the valueσ= 0.02 to be near-optimal5.\n2.3 Layout embeddings\nIn our setting, a document is represented by a sequence of tokensti and their\nbounding boxesbi. To each element of this sequence, we assign its layout embed-\nding ℓi, carrying the information about the position of the token with respect\nto the whole document. This could be performed in various ways. What they all\nhave in common is that the embeddingsℓi depend only on the bounding boxes\nbi and not on the tokensti.\nWe base our layout embeddings on the method originally used in [7], and\nthen in [28] to deﬁne the sequential embeddings. We ﬁrst normalize the bounding\nboxes by translating them so that the upper left corner is at(0,0), and dividing\ntheir dimensions by the page height. This causes the page bounding box to\nbecome (0,0,w, 1), wherew is the normalized width.\nThe layout embedding of a token will be deﬁned as the concatenation of four\nembeddings of the individual coordinates of its bounding box. For an integerd\nand a vector of scaling factorsθ ∈Rd, we deﬁne the corresponding embedding\nof a single coordinatet as\nembθ(t) = (sin(tθ); cos(tθ)) ∈R2d, (3)\nwhere the sin and cos are performed element-wise, yielding two vectors inRd.\nThe resulting concatenation of single bounding box coordinate embeddings is\nthen a vector inR8d.\nIn [28, Section 3.5], and subsequently in other Transformer-based models\nwith precomputed sequential embeddings, the sequential embeddings were de-\nﬁned byembθ with θ being a geometric progression interpolating between1 and\n10−4. Unlike the sequential position, which is a potentially large integer, bound-\ning box coordinates are normalized to the interval[0,1]. Hence, for our layout\nembeddings we use larger scaling factors(θr), namely a geometric sequence of\nlength n/8 interpolating between 1 and 500, where n is the dimension of the\ninput embeddings.\n2.4 Relative bias\nLet us recall that in a typical Transformer encoder, a single attention head\ntransforms its input vectors into three sequences: queriesqi ∈Rd, keyski ∈Rd,\nand values vi ∈ Rd. The raw attention scores are then computed as αij =\nd−1/2qT\ni kj. Afterwards, they are normalized using softmax, and used as weights\nin linear combinations of value vectors.\nThe point of relative bias is to modify the computation of the raw attention\nscores by introducing a bias term:α′\nij = αij + βij. In the sequential setting,\n5 we tested the values 0.5, 0.1, 0.02, 0.004, and 0.0008\nLAMBERT: Layout-Aware Language Modeling 7\nβij = W(i−j) is a trainable weight, depending on the relative sequential position\nof tokensiand j. This form of attention bias was introduced in [23], and we will\nrefer to it assequential attention bias.\nWe introduce a simple and natural extension of this mechanism to the two-\ndimensional context. In our case, the biasβij depends on the relative positions\nof the tokens. More precisely, letC ≫1 be an integer resolution factor (the\nnumber of cells in a grid used to discretize the normalized coordinates). Ifbi =\n(x1,y1,x2,y2) is the normalized bounding box of thei-th token, we ﬁrst reduce\nit to a2-dimensional position(ξi,ηi) = (Cx1,C(y1 + y2)/2), and then deﬁne\nβij = H(⌊ξi −ξj⌋) +V(⌊ηi −ηj⌋), (4)\nwhere H(ℓ) and V(ℓ) are trainable weights deﬁned for every integerℓ∈[−C,C).\nA good value forC should allow for a distinction between consecutive lines and\ntokens, without unnecessarily aﬀecting performance. For a typical document\nC = 100is enough, and we ﬁx this in our experiments.\nThis form of attention bias will be referred to as 2D attention bias. We\nsuspect that it should help in analyzing, say, tables by allowing the learning of\nrelationships between cells.\n3 Experiments\nAll experiments were performed on 8 NVIDIA Tesla V100 32GB GPUs. As our\npretrained base model we used RoBERTa in its smaller, base variant (125M\nparameters, 12 layers, 12 attention heads, hidden dimension 768). This was\nalso employed as the baseline, after additional training on the same dataset\nwe used for LAMBERT. The implementation and pretrained weights from the\ntransformers library [30] were used.\nIn the LAMBERT model, we used the layout embeddings of dimensionk =\n128, and initialized the adapter layerL with standard deviationσ = 0.02, as\nnoted in Section 2. For comparison, in our experiments, we also included the\npublished version of the LayoutLM model [32], which is of a similar size.\nThe models were trained on a masked language modeling objective extended\nwith layout information (with the same settings as the original RoBERTa [18]);\nand subsequently, on downstream information extraction tasks. In the remainder\nof the paper, these two stages will be referred to as, respectively,training and\nﬁne-tuning.\nTraining was performed on a collection of PDFs extracted fromCommon\nCrawl made up of a variety of documents (we randomly selected up to 10 doc-\numents from any single domain). The documents were processed with an OCR\nsystem, Tesseract 4.1.1-rc1-7-gb36c, to obtain token bounding boxes. The\nﬁnal model was trained on the subset of the corpus consisting of business doc-\numents with non-trivial layout, ﬁltered by an SVM binary classiﬁer, totaling to\napproximately 315k documents (3.12M pages). The SVM model was trained on\n700 manually annotated PDF ﬁles to distinguish between business (e.g. invoices,\nforms) and non-business documents (e.g. poems, scientiﬁc texts).\n8 Ł. Garncarek et al.\nIn the training phase, we used the Adam optimizer with the weight decay\nﬁx from [30]. We employed a learning rate scheduling method similar to the one\nused in [6], increasing the learning rate linearly from0 to 1e−4 for the warm-up\nperiod of10% of the training time and then decreasing it linearly to0. The ﬁnal\nmodel was trained with batch size of128 sequences (amounting to64K tokens)\nfor approximately 1000k steps (corresponding to training on 3M pages for 25\nepochs). This took about 5 days to complete a single experiment.\nAfter training our models, we ﬁne-tuned and evaluated them independently\non multiple downstream end-to-end information extraction tasks. Each evalu-\nation dataset was split into training, validation and test subsets. The models\nwere extended with a simple classiﬁcation head on top, consisting of a single\nlinear layer, and ﬁne-tuned on the task of classifying entity types of tokens. We\nemployed early stopping based on theF1-score achieved on the validation part\nof the dataset. We used the Adam optimizer again, but this time without the\nlearning rate warm-up, as it turned out to have no impact on the results.\nThe extended model operates as a tagger on the token level, allowing for\nthe classiﬁcation of separate tokens, while the datasets contain only the values\nof properties that we are supposed to extract from the documents. Therefore,\nthe further processing of output is required. To this end, we use the pipeline\ndescribed in [27].\nEvery contiguous sequence of tokens tagged as a given entity type is treated\nas a recognized entity and assigned a score equal to the geometric mean of\nthe scores of its constituent tokens. Then, every recognized entity undergoes a\nnormalization procedure speciﬁc to its general data type (e.g. date, monetary\namount, address, etc.). This is performed using regular expressions: for instance,\nthe dateJuly, 15th 2013 is converted to2013-07-15. Afterwards, duplicates\nare aggregated by summing their scores, leading to a preference for entities\ndetected multiple times. Finally, the highest-scoring normalized entity is selected\nas the output of the information extraction system. The predictions obtained this\nway are compared with target values provided in the dataset usingF1-score as\nthe evaluation metric. See [27] for more details.\n4 Results\nWe evaluated our models on four public datasets containing visually rich doc-\numents. The Kleister NDA and Kleister Charity datasets are part of a larger\nKleister dataset, recently made public in [27] (many examples of documents,\nand detailed descriptions of extraction tasks can be found therein). The NDA\nset consists of legal agreements, whose layout variety is limited. It should prob-\nably be treated as a plain-text dataset. The Charity dataset on the other hand\ncontains reports of UK charity organizations, which include various tables, dia-\ngrams and other graphic elements, interspersed with text passages. All Kleister\ndatasets come with predeﬁned train/dev/test splits, with 254/83/203 documents\nfor NDA and 1729/440/609 for Charity.\nLAMBERT: Layout-Aware Language Modeling 9\nTable 1: Comparison of F1-scores for the considered models. Best results in\neach column are indicated in bold. In parentheses, the length of training of\nour models, expressed in non-unique pages, is presented for comparison. For\nRoBERTa, the ﬁrst row corresponds to the original pretrained model without\nany further training, while in the second row the model was trained on our\ndataset. aresult obtained from relevant publication;bresult of a single model,\nobtained from the SROIE leaderboard [13]\nModel Params Our experiments External results\nNDA Charity SROIE* CORD SROIE CORD\nRoBERTa [18] 125M 77.91 76.36 94.05 91.57 92.39 b —\nRoBERTa (16M) 125M 78.50 77.88 94.28 91.98 93.03 b —\nLayoutLM [32] 113M 77.50 77.20 94.00 93.82 94.38 a 94.72a\n343M 79.14 77.13 96.48 93.62 97.09 b 94.93a\nLayoutLMv2 [31] 200M — — — — 96.25 a 94.95a\n426M — — — — 97.81 b 96.01a\nLAMBERT (16M) 125M 80.31 79.94 96.24 93.75 — —\nLAMBERT (75M) 125M 80.42 81.34 96.93 94.41 98.17 b —\nThe SROIE [12] and CORD [21] datasets are composed of scanned and\nOCRed receipts. Documents in SROIE are annotated with four target entities\nto be extracted, while in CORD there are 30 diﬀerent entities. We use the public\n1000 samples from the CORD dataset with the train/dev/test split proposed by\nthe authors of the dataset (respectively, 800/100/100). As for SROIE, it consists\nof a public training part, and test part with unknown annotations. For the pur-\npose of ablation studies, we further subdivided the public part of SROIE into\ntraining and test subsets (546/80 documents; due to the lack of a validation set\nin this split, we ﬁne-tuned for 15 epochs instead of employing early stopping).\nWe refer to this split as SROIE*, while the name SROIE is reserved for the\noriginal SROIE dataset, where the ﬁnal evaluation on the test set is performed\nthrough the leaderboard [13].\nInTable1,wepresenttheevaluationresults achievedondownstreamtasksby\nthe trained models. With the exception of the Kleister Charity dataset, where\nonly 5 runs were made, each of the remaining experiments were repeated 20\ntimes, and the mean result was reported. We compare LAMBERT with baseline\nRoBERTa (trained on our dataset) and the original RoBERTa [18] (without ad-\nditional training); LayoutLM [32]; and LayoutLMv2 [31]. The LayoutLM model\npublished by its authors was plugged into the same pipeline that we used for\nLAMBERT and RoBERTa. In the ﬁrst four columns we present averaged results\nof our experiments, and for CORD and SROIE we additionally provide the re-\nsults reported by the authors of LayoutLM, and presented on the leaderboard\n[13].\n10 Ł. Garncarek et al.\nSincetheLayoutLMv2modelwasnotpubliclyavailableatthetimeofprepar-\ning this article, we could not perform experiments ourselves. As a result some\nof the results are missing. For CORD, we present the scores given in [31], where\nthe authors did not mention, though, whether they averaged over multiple runs,\nor used just a single model. A similar situation occurs for LayoutLM; we pre-\nsented the average results of 20 runs (best run of LAMBERT attained the score\nof 95.12), which are lower than the scores presented in [31]. The diﬀerence could\nbe attributed to using a diﬀerent end-to-end evaluation pipeline, or averaging (if\nthe results in [31,32] come from a single run).\nFor the full SROIE dataset, most of the results were retrieved from the public\nleaderboard [13], and therefore they come from a single model. For the base\nvariants of LayoutLM and LayoutLMv2, the results were unavailable, and we\npresent the scores from the corresponding papers.\nIn our experiments, the base variant of LAMBERT achieved top scores for all\ndatasets. However, in the case of CORD, the result reported in [31] for the large\nvariant of LayoutLMv2 is superior. If we consider the best scores of LAMBERT\n(95.12) instead of the average, and the scores of LayoutLM reported in [32],\nLAMBERT slightly outperforms LayoutLM, while still being inferior to Lay-\noutLMv2. Due to the lack of details on the results of LayoutLM, it is unknown\nwhich of these comparisons is valid.\nFor Kleister datasets, the base variant (and in the case of Charity, also the\nlarge variant) of LayoutLM did not outperform the baseline RoBERTa. We sus-\npect that this might be the result of LayoutLM being better attuned to the\nevaluation pipeline used by its authors, and the fact that it was based on an\nuncased language model. In the Kleister dataset, meanwhile, performance for\nentities such as names may depend on casing.\n5 Hyperparameters and ablation studies\nIn order to investigate the impact of our modiﬁcations to RoBERTa, we per-\nformed an extensive study of hyperparameters and the various components of\nthe ﬁnal model. We investigated the dimension of layout embeddings, the impact\nof the adapter layerL, the size of training dataset, and ﬁnally we performed a\ndetailed ablation study of the embeddings and attention biases we had used to\naugment the baseline model.\nIn the studies, every model was ﬁne-tuned and evaluated20 times on each\ndataset, except for Kleister Charity dataset, on which we ﬁne-tuned every model\n5 times: evaluations took much longer on Kleister Charity. For each model and\ndataset combination, the mean score was reported, together with the two-sided\n95% conﬁdence interval, computed using the correspondingt-value. We consid-\nered diﬀerences to be signiﬁcant when the corresponding intervals were disjoint.\nAllthe resultsarepresentedinTable2,whichisdividedintosectionscorrespond-\ning to diﬀerent studies. TheF1-scores are reported asincreases with respect to\nthe reported mean baseline score, to improve readability.\nLAMBERT: Layout-Aware Language Modeling 11\nTable 2: Improvements ofF1-score over the baseline for various variants of LAM-\nBERT model. The ﬁrst row (with grayed background) contains theF1-scores of\nthe baseline RoBERTa model. The other grayed row corresponds to full LAM-\nBERT. Statistically insigniﬁcant improvements over the baseline are grayed. In\neach of three studies, the best result together with all results insigniﬁcantly\nsmaller are in bold.aﬁltered datasets;bmodel with a disabled adapter layer\nTrain epochs\nand pages\nEmbeddings\ndimension\nInputs Datasets\nsequential\nseq. bias\nlayout\n2D bias\nNDA Charity SROIE* CORD\n8×2M 128\n• 78.50±1.16 77.88±0.48 94.28±0.42 91.98±0.62\n• 1.94±0.46 −0.82±0.74 0.33±0.22 −0.15±0.49\n• • 2.42±0.61 0.52±0.64 0.79±0.17 0.03±0.57\n• 1.25±0.59 2.62±0.80 1.86±0.15 0.89±0.83\n• −0.49±0.62 2.02±0.48 0.53±0.28 −0.17±0.62\n• • 0.88±0.50 3.00±0.37 1.94±0.16 0.68±0.62\n• • 1.74±0.67 0.06±0.93 1.94±0.18 1.42±0.53\n• • 1.73±0.60 2.02±0.53 2.09±0.22 1.93±0.71\n• • • 0.54±0.85 1.84±0.42 2.08±0.38 2.15±0.65\n• • • 1.66±0.76 0.32±1.35 1.75±0.35 1.06±0.54\n• • • 0.85±0.91 1.84±0.27 2.01±0.24 1.95±0.46\n• • • • 1.81±0.60 2.06±0.69 1.96±0.16 1.77±0.46\n8×2M\n128 • • 1.74±0.67 0.06±0.93 1.94±0.18 1.42±0.53\n384 • • 0.90±0.54 0.70±0.40 1.86±0.22 1.51±0.60\n768 • • 0.71±1.04 0.50±0.85 2.18±0.25 1.54±0.51\n768b • • 0.77±0.58 2.30±0.20 0.37±0.15 1.58±0.52\n8×2M\n128\n• • • • 1.81±0.60 2.06±0.26 1.96±0.18 1.77±0.46\n8×2Ma • • • • 1.86±0.66 1.92±0.19 2.60±0.18 1.59±0.61\n25×3Ma • • • • 1.92±0.50 3.46±0.21 2.65±0.13 2.43±0.19\n5.1 Baseline\nAs a baseline for the studies we use the publicly available pretrained base variant\nof the RoBERTa model with 12 layers, 12 attention heads, and hidden dimension\n768. We additionally trained this model on our training set, and ﬁne-tuned it on\nthe evaluation datasets in a manner analogous to LAMBERT.\n5.2 Embeddings and biases\nIn this study we disabled various combinations of input embeddings and at-\ntention biases. The models were trained on 2M pages for 8 epochs, with 128-\ndimensional layout embeddings (if enabled). The resulting models were divided\ninto three groups. The ﬁrst one contains sequential-only combinations which do\nnot employ the layout information at all, including the baseline. The second\ngroup consists of models using only the bounding box coordinates, with no ac-\ncess to sequential token positions. Finally, the models in the third group use\nboth sequential and layout inputs. In this group we did not disable the sequen-\n12 Ł. Garncarek et al.\ntial embeddings. It includes the full LAMBERT model, with all embeddings and\nattention biases enabled.\nGenerally, we observe that none of the modiﬁcations has led to a signiﬁcant\nperformance deterioration. Among the models considered, the only one which\nreported a signiﬁcant improvement for all four datasets—and at the same time,\nthe best improvement—was the full LAMBERT.\nFor the Kleister datasets the variance in results was relatively higher than in\nthe case of SROIE* and CORD. This led to wider conﬁdence intervals, and re-\nduced the number of signiﬁcant outcomes. This is true especially for the Kleister\nNDA dataset, which is the smallest one. In Kleister NDA, signiﬁcant improve-\nments were achieved for both sequential-only models, and for full LAMBERT.\nThe diﬀerences between these increases were insigniﬁcant. It would seem that,\nfor sequential-only models, the sequential attention bias is responsible for the\nimprovement. But after adding the layout inputs, it no longer leads to improve-\nmentswhenunaccompaniedbyothermodiﬁcations.Still,achievingbetterresults\non sequential-only inputs may be related to the plain text nature of the Kleister\nNDA dataset.\nWhile other models did not report signiﬁcant improvement over the baseline,\nthere are still some diﬀerences between them to be observed. The model using\nonly 2D attention bias is signiﬁcantly inferior to most of the others. This seems\nto agree with the intuition that relative 2D positions are the least suitable way\nto pass positional information about plain text.\nIn the case of the Kleister Charity dataset, signiﬁcant improvements were\nachieved by all layout-only models, and all models using the 2D attention bias.\nBest improvement was attained by full LAMBERT, and two layout-only models\nusing the layout embeddings; the 2D attention bias used alone improved the\nresults signiﬁcantly, but did not reach the top score. The conﬁdence intervals\nare too wide to oﬀer further conclusions, and many more experiments will be\nneeded to increase the signiﬁcance of the results.\nFor the SROIE* dataset, except for two models augmented only with a single\nattention bias, all improvements proved signiﬁcant. Moreover, the diﬀerences\nbetween all the models using layout inputs are insigniﬁcant. We may conclude\nthat passing bounding box coordinates in any way, except through 2D attention\nbias, signiﬁcantly improves the results. As to the lack of signiﬁcant improvements\nfor 2D attention bias, we hypothesize that this is due to its relative nature.\nIn all other models the absolute position of tokens is somehow known, either\nthrough the layout embeddings, or the sequential position. When a human reads\na receipt, the absolute position is one of the main features used to locate the\ntypical positions of entities.\nFor CORD, which is the more complex of the two receipt datasets, signiﬁcant\nimprovements were observed only for combined sequential and layout models. In\nthis group, the model using both sequential and layout embeddings, augmented\nwith sequential attention bias, did not yield a signiﬁcant improvement. There\nwere no signiﬁcant diﬀerences among the remaining models in the group. Con-\nLAMBERT: Layout-Aware Language Modeling 13\ntrary to the case of SROIE*, none of the layout-only models achieved signiﬁcant\nimprovement.\n5.3 Layout embedding dimension\nIn this study we evaluated four models, using both sequential and layout em-\nbeddings, varying the dimension of the latter. We considered 128-, 384-, and\n768-dimensional embeddings. Since this is the same as for the input embeddings\nof RoBERTaBASE, it was possible to remove the adapter layerL, and treat this\nas another variant, in Table 2 denoted as 768b.\nIn Kleister NDA, there were no signiﬁcant diﬀerences between any of the\nevaluated models, and no improvements over the baseline. On the other hand,\nin Kleister Charity, disabling the adapter layer and using the 768-dimensional\nlayout embeddings led to signiﬁcantly better performance. These results remain\nconsistent with earlier observations that in Kleister NDA the best results were\nachieved by sequential-only models, while in the case of Kleister Charity, by\nlayout-only models. It seems that in the case of NDA the performance is inﬂu-\nenced mostly by the sequential features, while in the case of Charity, removing\nthe adapter layer increases the strength of the signal of the layout embeddings,\ncarrying the layout features which are the main factor aﬀecting performance.\nIn SROIE* and CORD all results were comparable, with one exception,\nnamely in SROIE*, the model with the disabled adapter layer did not, unlike\nthe remaining models, perform signiﬁcantly better than the baseline.\n5.4 Training dataset size\nIn this study, following the observations from [9], we considered models trained\non 3 diﬀerent datasets. The ﬁrst model was trained for 8 epochs on 2M unﬁltered\n(see Section 3 for more details of the ﬁltering procedure) pages. In the second\nmodel, we used the same training time and dataset size, but this time only\nﬁltered pages were used. Finally, the third model was trained for 25 epochs on\n3M ﬁltered pages.\nIt is not surprising that increasing the training time and dataset size, leads to\nan improvement in results, at least up to a certain point. In the case of Kleister\nNDA dataset, there were no signiﬁcant diﬀerences in the results. For Kleister\nCharity, the best result was achieved for the largest training dataset, with 75M\nﬁltered pages. This result was also signiﬁcantly better than the outcomes for the\nsmaller dataset. In the case of SROIE* the two models trained on datasets with\nﬁltered documents achieved a signiﬁcantly higher score than the one trained on\nunﬁltered documents. There was, in fact, no signiﬁcant diﬀerence between these\ntwo models. This supports the hypothesis that, in this case, ﬁltering could be\nthe more important factor. Finally, for CORD the situation is similar to Kleister\nCharity.\n14 Ł. Garncarek et al.\n6 Conclusions and further research\nWe introduced LAMBERT, a layout-aware language model, producing contex-\ntualized token embeddings for tokens contained in formatted documents. The\nmodel can be trained in an unsupervised manner. For the end user, the only\ndiﬀerence with classical language models is the use of bounding box coordinates\nas additional input. No images are needed, which makes this solution particu-\nlarly simple, and easy to include in pipelines that are already based on OCR-ed\ndocuments.\nThe LAMBERT model outperforms the baseline RoBERTa on information\nextraction from visually rich documents, without sacriﬁcing performance on doc-\numents with a ﬂat layout. This can be clearly seen in the results for the Kleister\nNDA dataset. Its base variant with around 125M parameters is also able to com-\npete with the large variants of LayoutLM (343M parameters) and LayoutLMv2\n(426M parameters), with Kleister and SROIE datasets achieving superior re-\nsults. In particular, LAMBERTBASE achieved ﬁrst place on the Key Information\nExtraction from the SROIE dataset leaderboard [13].\nThe choice of particular LAMBERT components is supported by an ablation\nstudy including conﬁdence intervals, and is shown to be statistically signiﬁcant.\nAnother conclusion from this study is that for visually rich documents the point\nwhere no more improvement is attained by increasing the training set has not\nyet been reached. Thus, LAMBERT’s performance can still be improved upon\nby simply increasing the unsupervised training set. In the future we plan to\nexperiment with increasing the model size, and training datasets.\nFurther research is needed to ascertain the impact of the adapter layerLon\nthe model performance, as the results of the ablation study were inconclusive. It\nwould also be interesting to understand whether the mechanism through which\nit aﬀects the results is consistent with the hypotheses formulated in Section 2.\nAcknowledgments. The authors would like to acknowledge the support the\nApplica.ai project has received as being co-ﬁnanced by the European Regional\nDevelopment Fund (POIR.01.01.01–00–0605/19–00).\nReferences\n1. Amazon: Amazon Textract. https://aws.amazon.com/textract/ (accessed\nNovember 25, 2019) (2019)\n2. Bart, E., Sarkar, P.: Information extraction by ﬁnding repeated structure. In: DAS\n’10 (2010)\n3. Cesarini, F., Francesconi, E., Gori, M., Soda, G.: Analysis and understanding of\nmulti-class invoices. IJDAR6, 102–114 (2003)\n4. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.:\nTransformer-XL: Attentive language models beyond a ﬁxed-length context. In:\nACL (2019)\nLAMBERT: Layout-Aware Language Modeling 15\n5. Denk, T.I., Reisswig, C.: BERTgrid: Contextualized Embedding for 2D Document\nRepresentation and Understanding. In: Workshop on Document Intelligence at\nNeurIPS 2019 (2019)\n6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: NAACL-HLT (2019)\n7. Gehring, J., Auli, M., Grangier, D., Yarats, D., Dauphin, Y.N.: Convolutional\nsequence to sequence learning. In: ICML (2017)\n8. Google: Cloud Document Understanding AI.https://cloud.google.com/docum\nent-understanding/docs/ (accessed November 25, 2019) (2019)\n9. Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey,\nD., Smith, N.A.: Don’t stop pretraining: Adapt language models to domains and\ntasks. In: Proceedings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics. pp. 8342–8360. Association for Computational Linguistics\n(2020). https://doi.org/10.18653/v1/2020.acl-main.740\n10. Hamza, H., Belaïd, Y., Belaïd, A., Chaudhuri, B.: An end-to-end administrative\ndocument analysis system. In: 2008 The Eighth IAPR International Workshop on\nDocument Analysis Systems. pp. 175–182 (2008)\n11. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M., Chen, D., Lee, H., Ngiam,\nJ., Le, Q.V., Wu, Y., Chen, Z.: Gpipe: Eﬃcient training of giant neural networks\nusing pipeline parallelism. In: NeurIPS (2019)\n12. ICDAR: Competition on Scanned Receipts OCR and Information Extraction.ht\ntps://rrc.cvc.uab.es/?ch=13 (accessed February 21, 2021) (2019)\n13. ICDAR: Leaderboard of the Information Extraction Task, Robust Reading Com-\npetition. https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=3 (accessed\nApril 7, 2020) (2020)\n14. Ishitani, Y.: Model-based information extraction method tolerant of ocr errors for\ndocument images. Int. J. Comput. Process. Orient. Lang.15, 165–186 (2002)\n15. Katti, A.R., Reisswig, C., Guder, C., Brarda, S., Bickel, S., Höhne, J., Faddoul,\nJ.B.: Chargrid: Towards understanding 2D documents. In: EMNLP (2018)\n16. Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., Heard, J.: Building a\ntest collection for complex document information processing. In: Proceedings of the\n29th Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval (2006)\n17. Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph convolution for multimodal informa-\ntion extraction from visually rich documents. In: NAACL-HLT (2019)\n18. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: RoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv1907.11692 (2019)\n19. Medvet,E.,Bartoli,A.,Davanzo,G.:Aprobabilisticapproachtoprinteddocument\nunderstanding. IJDAR14, 335–347 (12 2011)\n20. Microsoft: Cognitive Services.https://azure.microsoft.com/en-us/services\n/cognitive-services/ (accessed November 25, 2019) (2019)\n21. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., Lee, H.: CORD: A Consoli-\ndated Receipt Dataset for Post-OCR Parsing. In: Document Intelligence Workshop\nat Neural Information Processing Systems (2019)\n22. Peanho, C., Stagni, H., Silva, F.: Semantic information extraction from images of\ncomplex documents. Applied Intelligence37, 543–557 (12 2012)\n23. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research21(140), 1–67 (2020)\n16 Ł. Garncarek et al.\n24. Rahman, W., Hasan, M., Lee, S., Zadeh, A., Mao, C., Morency, L.P., Hoque,\nE.: Integrating multimodal information in large pretrained transformers. In: ACL\n(2020)\n25. Rusinol, M., Benkhelfallah, T., Poulain d’Andecy, V.: Field extraction from ad-\nministrative documents by incremental structural templates. In: ICDAR (2013)\n26. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position represen-\ntations. In: NAACL-HLT (2018)\n27. Stanisławek, T., Graliński, F., Wróblewska, A., Lipiński, D., Kaliska, A., Rosal-\nska, P., Topolski, B., Biecek, P.: Kleister: A novel task for information extraction\ninvolving long documents with complex layout. ArXiv (accepted to ICDAR 2021)\n2105.05796 (2021)\n28. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,\nŁ., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems 30 (2017)\n29. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-\ntask benchmark and analysis platform for natural language understanding. In:\nProceedings of ICLR (2019),https://gluebenchmark.com/ (accessed November\n26, 2019)\n30. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\nRault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C.,\nJernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M., Lhoest, Q., Rush,\nA.M.: Transformers: State-of-the-art natural language processing. In: Proceedings\nofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:Sys-\ntem Demonstrations. pp. 38–45. Association for Computational Linguistics, Online\n(Oct 2020),https://www.aclweb.org/anthology/2020.emnlp-demos.6\n31. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C.,\nChe, W., Zhang, M., Zhou, L.: LayoutLMv2: Multi-modal pre-training for visually-\nrich document understanding. arXiv2012.14740 (2020)\n32. Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: LayoutLM: Pre-training of\ntext and layout for document image understanding. In: Proceedings of the 26th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining.\np. 1192–1200 (2020)\n33. Yu, W., Lu, N., Qi, X., Gong, P., Xiao, R.: PICK: Processing key information\nextraction from documents using improved graph learning-convolutional networks.\nIn: 2020 25th International Conference on Pattern Recognition (ICPR). pp. 4363–\n4370 (2021). https://doi.org/10.1109/ICPR48806.2021.9412927\n34. Zhang, P., Xu, Y., Cheng, Z., Pu, S., Lu, J., Qiao, L., Niu, Y., Wu, F.: TRIE:\nEnd-to-end text reading and information extraction for document understanding.\nProceedings of the 28th ACM International Conference on Multimedia (2020)",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8268078565597534
    },
    {
      "name": "Security token",
      "score": 0.7044943571090698
    },
    {
      "name": "Transformer",
      "score": 0.6218228340148926
    },
    {
      "name": "Language model",
      "score": 0.6021997928619385
    },
    {
      "name": "Encoder",
      "score": 0.6003986597061157
    },
    {
      "name": "Minimum bounding box",
      "score": 0.5060593485832214
    },
    {
      "name": "Natural language processing",
      "score": 0.483803927898407
    },
    {
      "name": "Task (project management)",
      "score": 0.46818840503692627
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4591076374053955
    },
    {
      "name": "Baseline (sea)",
      "score": 0.45686978101730347
    },
    {
      "name": "Information extraction",
      "score": 0.45152583718299866
    },
    {
      "name": "Scratch",
      "score": 0.4460170269012451
    },
    {
      "name": "Bounding overwatch",
      "score": 0.42982012033462524
    },
    {
      "name": "Information retrieval",
      "score": 0.39728331565856934
    },
    {
      "name": "Image (mathematics)",
      "score": 0.30864325165748596
    },
    {
      "name": "Programming language",
      "score": 0.14882799983024597
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I108403487",
      "name": "Warsaw University of Technology",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I59411706",
      "name": "Adam Mickiewicz University in Poznań",
      "country": "PL"
    }
  ]
}