{
  "title": "Not the Silver Bullet: LLM-enhanced Programming Error Messages are Ineffective in Practice",
  "url": "https://openalex.org/W4403799160",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2197792391",
      "name": "Eddie Antonio Santos",
      "affiliations": [
        "University College Dublin"
      ]
    },
    {
      "id": "https://openalex.org/A2095961539",
      "name": "Brett A. Becker",
      "affiliations": [
        "University College Dublin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4237281666",
    "https://openalex.org/W2296095100",
    "https://openalex.org/W4323033785",
    "https://openalex.org/W3000240292",
    "https://openalex.org/W2791302163",
    "https://openalex.org/W2791265653",
    "https://openalex.org/W2013935724",
    "https://openalex.org/W1986496634",
    "https://openalex.org/W4392564669",
    "https://openalex.org/W2050390685",
    "https://openalex.org/W3163358064",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W4317502110",
    "https://openalex.org/W4392858442",
    "https://openalex.org/W2915219867",
    "https://openalex.org/W4323033692",
    "https://openalex.org/W2056596215",
    "https://openalex.org/W4391417542",
    "https://openalex.org/W3020721369",
    "https://openalex.org/W2604953184",
    "https://openalex.org/W4388850815",
    "https://openalex.org/W2747427841",
    "https://openalex.org/W4386099272",
    "https://openalex.org/W4388850769",
    "https://openalex.org/W2007004218",
    "https://openalex.org/W4403430763",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4313477803",
    "https://openalex.org/W1986204096",
    "https://openalex.org/W4385500670",
    "https://openalex.org/W4404534210",
    "https://openalex.org/W4232632581"
  ],
  "abstract": "The sudden emergence of large language models (LLMs) such as ChatGPT has had\\na disruptive impact throughout the computing education community. LLMs have\\nbeen shown to excel at producing correct code to CS1 and CS2 problems, and can\\neven act as friendly assistants to students learning how to code. Recent work\\nshows that LLMs demonstrate unequivocally superior results in being able to\\nexplain and resolve compiler error messages -- for decades, one of the most\\nfrustrating parts of learning how to code. However, LLM-generated error message\\nexplanations have only been assessed by expert programmers in artificial\\nconditions. This work sought to understand how novice programmers resolve\\nprogramming error messages (PEMs) in a more realistic scenario. We ran a\\nwithin-subjects study with $n$ = 106 participants in which students were tasked\\nto fix six buggy C programs. For each program, participants were randomly\\nassigned to fix the problem using either a stock compiler error message, an\\nexpert-handwritten error message, or an error message explanation generated by\\nGPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4\\ngenerated error messages outperformed conventional compiler error messages in\\nonly 1 of the 6 tasks, measured by students' time-to-fix each problem.\\nHandwritten explanations still outperform LLM and conventional error messages,\\nboth on objective and subjective measures.\\n",
  "full_text": "Not the Silver Bullet\nLLM-enhanced Programming Error Messages are Ineffective in Practice\nEddie Antonio Santos\nSchool of Computer Science\nUniversity College Dublin\nDublin, Ireland\neddie.santos@ucdconnect.ie\nBrett A. Becker\nSchool of Computer Science\nUniversity College Dublin\nDublin, Ireland\nbrett.becker@ucd.ie\nABSTRACT\nThe sudden emergence of large language models (LLMs) such as\nChatGPT has had a disruptive impact throughout the computing\neducation community. LLMs have been shown to excel at produc-\ning correct code to CS1 and CS2 problems, and can even act as\nfriendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in\nbeing able to explain and resolve compiler error messages‚Äîfor\ndecades, one of the most frustrating parts of learning how to code.\nHowever, LLM-generated error message explanations have only\nbeen assessed by expert programmers in artificial conditions. This\nwork sought to understand how novice programmers resolve pro-\ngramming error messages (PEMs) in a more realistic scenario. We\nran a within-subjects study with ùëõ = 106 participants in which\nstudents were tasked to fix six buggy C programs. For each pro-\ngram, participants were randomly assigned to fix the problem using\neither a stock compiler error message, an expert-handwritten error\nmessage, or an error message explanation generated by GPT-4. De-\nspite promising evidence on synthetic benchmarks, we found that\nGPT-4 generated error messages outperformed conventional com-\npiler error messages in only 1 of the 6 tasks, measured by students‚Äô\ntime-to-fix each problem. Handwritten explanations still outper-\nform LLM and conventional error messages, both on objective and\nsubjective measures.\nCCS CONCEPTS\n‚Ä¢ Social and professional topics ‚Üí Computing education; ‚Ä¢\nSoftware and its engineering ‚ÜíCompilers; Error handling and\nrecovery; ‚Ä¢ Human-centered computing ‚ÜíEmpirical studies\nin HCI.\nKEYWORDS\nAI; compiler error messages; computing education; CS1; debugging;\nfeedback; GenAI; Generative AI; GPT-4; C; LLMs; large language\nmodels; novice programmers; PEM; programming error messages\nACM Reference Format:\nEddie Antonio Santos and Brett A. Becker. 2024. Not the Silver Bullet: LLM-\nenhanced Programming Error Messages are Ineffective in Practice. In The\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nUKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom\n¬© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1177-0/24/09\nhttps://doi.org/10.1145/3689535.3689554\nUnited Kingdom and Ireland Computing Education Research (UKICER 2024),\nSeptember 5‚Äì6, 2024, Manchester, United Kingdom. ACM, New York, NY, USA,\n7 pages. https://doi.org/10.1145/3689535.3689554\n1 INTRODUCTION\nFor decades, students learning how to code have struggled with\nerror messages [5]‚Äîwhether they are emitted by compilers or run-\ntime systems, programming error messages (PEMs) have had a\nreputation for being terse [2], inadequate [8], and unreadable [12].\nError messages from C and C++ compilers especially have been\nshown to be deficient debugging tools [43].\nRecent advances in generative AI have resulted in tools like\nChatGPT and GitHub Copilot. These tools, based on large language\nmodels (LLMs), have revolutionised several fields including comput-\ning education [4]. Recent work has shown that LLMs can produce\nacceptable programming error message explanations [22] which\nbecome more accurate with larger models and more source code\ncontext [39, 48]. However, it is unknown to what extent that novice\nprogrammers are able to effectively utilise these automatically gen-\nerated error message explanations to debug their programs.\nIn this study, we had 106 students from an introductory pro-\ngramming module partake in a within-subjects study that had\nparticipants fix a number of buggy programs. For each program,\nparticipants were shown either a conventional compiler error mes-\nsage, an expert-handwritten error message, or an LLM-generated\nexplanation. The LLM used to enhance error messages was GPT-4,\nwhich at the time of the study, was the LLM used in the paid version\nof ChatGPT [30]. We measured both how long it took participants\nto resolve errors with each condition, as well as asking students\ntheir opinions on their debugging experience.\n1.1 Contributions & Research Questions\nWe provide empirical evidence demonstrating that students ap-\npear not to be any faster at resolving errors when given GPT-4\nerror message explanations compared to stock compiler error mes-\nsages. Even though students are not any faster, they still prefer\nGPT-4‚Äôs explanations to conventional compiler error messages.\nHowever, expert-handwritten are superior to both. We posit that\nerror message usability is more complex than whether or not the\ntext presented to the student contains the correct solution for the\nproblem.\nThe following questions guide this research:\nRQ1 How quickly do students resolve error messages when given\nLLM-enhanced explanations in comparison to stock compiler\nerror messages and handwritten explanations?\nRQ2 Which style of error message do students prefer?\narXiv:2409.18661v1  [cs.AI]  27 Sep 2024\nUKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom Santos and Becker\n2 BACKGROUND AND RELATED WORK\n2.1 Programming Error Messages\nProgramming error messages (PEMs) are the diagnostic messages\npresented to coders when an error is detected in their program‚Äî\neither due to a mistake in syntax or spelling in the source code,\nor due to some unrecoverable runtime condition, such as a divi-\nsion by zero, or an invalid memory access [ 5]. PEMs have been\nan obstacle to learning how to code since almost the inception of\nprogramming [2, 8, 47]. Little progress has been made to improve\ncompiler and runtime error messages, despite decades of guidelines\nproposed to improve them [1, 12, 16, 19, 23, 43]. There have been\nmany attempts at having programming systems produce better\ndiagnostics [9, 20, 25, 37, 40], however, error message enhance-\nment has seen weak [3, 6, 15, 34] to insignificant [11, 32] results in\nimproving student outcomes.\n2.2 Large Language Models and CS Education\nA confluence of advances in model architecture; novel text repre-\nsentation; massive, curated datasets; and sheer computing power\nhas rapidly enabled the development of large language models\n(LLMs) [36]: models with billions or even trillions of parameters,\ncapable of capturing the structure and predictability of text to such\nan extent that they are able to exhibit ‚Äúemergent‚Äù behaviours, like\nquestion answering, analogical reasoning, and even the ability to\nexecute programs [45].\nIn computing education, LLM-powered tools have been shown\nto ace CS1 [13] and CS2 [14] exams and provide increasingly ac-\ncurate error message explanations [ 22, 39, 48]. LLMs have even\nenabled brand new pedagogical approaches [10, 28]. Educators are\ngrappling with how to integrate LLMs into their practice [ 4]‚Äîif\nat all [21]. Without guidance, complete novices struggle to write\nthe prompts that would complete their assignments [28]. Addition-\nally, novices exhibit a number of unproductive interaction patterns\nwhen using LLM-assisted code completion [35, 44]. Some program-\nmers do not complete programming tasks faster with LLM-assisted\ncode completion, and in fact, are more likely to fail programming\ntasks [44]. Having an LLM that performs better in synthetic bench-\nmarks results in ‚Äúrelatively indistinguishable differences in terms of\nhuman performance‚Äù [27]. Despite the lack of improvement, novices\nexpress a preference for using LLMs and chatbots [31, 35, 44]. How-\never, more experienced students express concern with how LLMs\nmay hamper their learning [33].\nLLMs and PEMs. LLMs have been found to be useful at explaining\nprogramming error messages on synthetic benchmarks. Leinonen\net al. [22] used OpenAI Codex to explain Python error messages.\nThey found that the best, most accurate explanations and fixes\nwere obtained when providing source code in the prompt, as well\nas using a temperature value of 0 (explained in section 3.1). Their\nprompt forms the basis of the prompt that was used in our study.\nSantos et al. [39] and Widjojo and Treude[48] have similar findings:\nproviding Java source code in the prompt produces significantly\nbetter error message explanations. Additionally, more advanced\nmodels like GPT-4 are more likely to output accurate explanations\nand fixes than GPT-3 and Codex.\n3 METHODOLOGY\nWe conducted a within-subjects study, inspired by prior work [44].\nEach participant observed all three study conditions‚Äîcontrol, hand-\nwritten, and GPT-4 (Section 3.1). Each participant was tasked to fix\nall six buggy C programs (Section 3.2). Both condition assignment\nand task assignment were randomised to counterbalance responses,\nsuch that we would obtain a roughly equal amount of responses for\neach task/condition pair. Randomising participants‚Äô assignments\nalso helped to mitigate the learning effect. Having participants fix\nbugs under all three study conditions allowed them to directly com-\npare the different error message styles to one another, and report\nwhich style they preferred. The study began with a short question-\nnaire, after which participants were given an in-person briefing,\nwhich was followed by the six debugging tasks. After participants\nhad completed all six tasks, we asked participants questions to\ncompare the three error message styles directly. The remainder of\nthis section describes the study conditions, the tasks, and study\nprotocol in greater detail.\n3.1 Study conditions\nEach participant saw error messages presented in all three study\nconditions: GCC (control), handwritten, and GPT-4. However,\nthe order of the study conditions was shuffled for each student, so\nthat participants could directly compare the three study conditions\nthemselves. Since there were six tasks, but only three conditions,\nthe order of the study conditions was simply repeated for the latter\nthree tasks‚Äîfor example, if a participant was assigned handwritten\nfor the first task, GCC for the second, and GPT-4 for the third, then\nthey would be assigned handwritten for the fourth, GCC for the\nfifth, and GPT-4 for the sixth.\nControl: GCC. For the control condition, students were presented\nwith error messages directly obtained from the GCC 13.2.0 C com-\npiler (Figure 1a). Whenever a single programming error would\ninduce multiple spurious, cascading error messages, we would only\nshow the first error message, as advised by Becker et al. [7].\nHandwritten error messages. Error message explanations (Fig-\nure 1b) were handwritten by the first author. These explanations\nwere written in response to the problems present in the source\ncode, but not necessarily in response to any error messages emitted\nby GCC. Importantly, the handwritten explanations were finalised\nbefore error message explanations were obtained from GPT-4. There-\nfore, the author of the handwritten explanations was not influenced\nby GPT-4‚Äôs output. Every message was written in a consistent struc-\nture: first was a line beginning with the word Error: which states\nwhat the detected error is, followed by a relevant excerpt from\nthe source code. Then one or more sections beginning with the\nword Help: or Note: would either suggest a possible solution, or\nhighlight relevant information to fix the problem. The structure of\nthe messages was greatly inspired by the diagnostics emitted by the\nRust compiler, with source code excerpts mimicking the structure\nof Rust‚Äôs ‚Äúdiagnostic windows‚Äù [38]. The handwritten explanations\nwere written in such a way that they can plausibly be generated by\nan actual compiler, given sufficient context.\nGPT-4 enhanced error messages. After the handwritten error ex-\nplanations were written, we obtained error message explanations\nNot the Silver Bullet UKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom\n(a) GCC (control)\n(b) Handwritten\n(c) GPT-4\nFigure 1: Examples of the three error message styles (i.e.,\nstudy conditions) as they would appear to participants.\nusing OpenAI‚Äôs GPT-4 API. All GPT-4 responses were obtained\nbefore the study started, as to not include inference time in the\ntime-to-fix measure.1 The methodology for generating error mes-\nsage explanations is derived from prior work [22, 39]. On January 25,\n2024, we used the OpenAI API to prompt GPT-4 modelgpt-4-0613.\nEach prompt used the system message of ‚ÄúYou are a helpful assis-\ntant‚Äù. We prompted with a temperature of 0, a hyperparameter used\nto affect the determinism of an LLM‚Äôs output, where 0 indicates\nthe most deterministic and reproducible output.2 The prompt used\nwas identical to that in Santos et al. [39], providing both the error\nmessage verbatim from GCC as well as the full source code of the\nbuggy program.\n1Obtaining the full output for each of these prompts would take between 16‚Äì30 seconds,\ndepending on the problem.\n2A value greater than 0 introduces entropy into the sampling distributions, in a style\nreminiscent of Ludwig Boltzmann‚Äôs work on thermodynamics.\nWhen presenting the error message to participants (Figure 1c),\nthe message would start with Error Message: followed by the\nGCC error message. This is because GPT-4‚Äôs output would often\nmake reference to the original error message in its explanations.\nAfter this, a line starting with Explanation: would be followed\nby GPT-4‚Äôs output, rendered as Markdown. GPT-4 output would\nvary depending on the problem, however, every response consis-\ntently had a line saying something similar to ‚Äúhere‚Äôs the corrected\ncode:‚Äù followed by a full reproduction of the source code with the\nproblem resolved. In all six problems, GPT-4 generated a solution\nequivalent to the one suggested in the handwritten explanations. In\naddition, GPT-4‚Äôs error message explanations were devoid of any\nmajor technical inaccuracies or ‚Äúhallucinations‚Äù [18].\n3.2 Tasks\nStudents were to fix all of the following C programming errors, in\na randomly assigned order. All programs in this study caused GCC\nto emit compiler error messages. We did not have students debug\nproblems that would result in runtime errors (e.g., no segmentation\nfaults). The following are all six of the debugging tasks:\n(1) Flipped assignment. The left- and right-hand sides of an as-\nsignment statement were swapped such that the assignment\ntarget would be on the right-hand side, e.g., a + b = c.\n(2) #const instead of #define. A program was made attempting\nto define a constant called PI using the syntax #const PI\n3.14. This programming error intentionally conflates C‚Äôs\n#define preprocessor directive with the const type qualifier,\nboth being valid ways to define a constant in C.\n(3) Using a keyword as a name . The program attempts to\ncreate new variables called union and nonUnion, however,\nunion is a reserved word, and cannot be used as an identifier.\n(4) Missing parameter. A function was defined to convert from\nFahrenheit to Celsius, however, the function definition lacks\nany formal parameters. Despite this, the body of the function\nused a identifier fahrenheit to perform the conversion, and\nthe function is called with an argument for the temperature\nin Fahrenheit.\n(5) Missing curly brace . The opening curly brace of a func-\ntion definition was omitted, causing GCC‚Äôs parser to take a\n‚Äúgarden-path‚Äù and completely misinterpret the program.\n(6) Reassigning a constant . A formal parameter was declared\nconst, then reassigned within the function body.\n3.3 Participants\nParticipants were recruited from a class at a large research-intensive\nEuropean public university that would be an R1 in the US Carnegie\nClassification. The class was the second semester of the first-year\nprogramming sequence (CS1) for CS majors, taught in the C pro-\ngramming language. In total, 113 participants were recruited across\ntwo separate lab sections. Of those, ùëõ = 106 participants (94%) com-\npleted the study. Of the participants that completed the study, 78\nidentified as men, 23 as women, and 5 chose not to disclose their\ngender. At the beginning of the study, we asked participants a few\nquestions on their experience in programming. The most commonly\nreported experience level was between 0‚Äì3 months (32 participants).\nCuriously, 8 participants reported absolutely zero experience in\nUKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom Santos and Becker\nprogramming, despite being enrolled in the second semester of the\nprogramming sequence. The remaining 66 participants reported\ngreater than three months of programming experience.\n3.4 Protocol\nFigure 2: Screenshot of the web-based IDE, showing the\n‚Äú#const instead of #define‚Äù task under the handwritten error\nmessage condition.\nThe study was conducted during a regularly scheduled lab ses-\nsion in late January 2024‚Äîthe second scheduled lab of the semester‚Äî\nheld simultaneously in two separate classrooms. Students were\nunder no obligation to participate in the lab session; they were not\ncompensated for participation, and participation did not affect the\nstudents‚Äô grade in any way.\nOnce in the classroom, the study was conducted entirely via a\ncustom web-based survey platform, which combined multiple ques-\ntionnaires with an online IDE (Figure 2). The online IDE component\nwas created using Microsoft‚Äôs Monaco Editor, the core text edit-\ning component of Visual Studio Code [24]. Upon hitting the ‚ÄúRun‚Äù\nbutton in the IDE, the full source code was securely transmitted\nto a university-managed server, where it would be stored. Code\nwas compiled and run in a sandboxed environment provided by the\nPiston code execution engine [41].\nAt the start of the study, the first author was present and gave\na quick oral briefing. After the briefing, students were directed\nto the web platform, where they signed an online consent form\nto commence the study. First, participants were asked a few de-\nmographic questions and asked about their attitudes regarding\nprogramming and error messages. After this, participants started\nthe six debugging tasks.\nFor each task, students were presented with the buggy code\nin the web IDE (Figure 2) and were instructed to fix the problem,\nhitting the ‚ÄúRun‚Äù button and only submitting their solution once\nthe problem was resolved. We started measuring the time taken to\ncomplete the task from the moment the code editor loaded. After 5\nminutes, students were given the option to skip the current task (if\nthey were stuck on a problem and wanted to continue to the next\none). Students were given up to 10 minutes per task, after which\ntheir attempt would have timed-out; however, all 106 participants\neither submitted or skipped each task before the 10 minute time-out\nwould have taken effect. During the study, postgraduate TAs were\ninstructed to ensure that students were not using AI tools like Chat-\nGPT/GitHub Copilot. However, the students were still permitted\nto search the web for error message explanations and use Q&A\nsites like Stack Overflow. After each exercise, students were given\na short questionnaire to reflect on how they used the error message\nto solve (or not solve) the programming error. This was presented as\nfour Likert-type questions (Figure 4), followed by a question asking\nabout the message‚Äôs length. In total, 106 participants completed all\nsix tasks within 50 minutes.\n4 RESULTS\n4.1 Objective measures\nThe primary quantitative measurements that we recorded were\nthe time-to-fix for participants who successfully fixed a task, and\nwhether or not a student skipped a particular task. For each task/-\ncondition pair, we obtained between between 27‚Äì44 samples, which\nis sufficient to perform within-task comparisons.\nTable 1: Tukey‚Äôs HSD test of the means of log-transformed\ntime-to-fix, comparing the error message condition. The dif-\nference in median time-to-fix between the left and right con-\ndition is given. Bold font indicates the condition with the\nfaster (statistically significant) time-to-fix.\nTask Comparison Diff ùëù\n#const instead of #define GCC Handwritten 43.95s 0.003\nGCC GPT-4 4.93s 0.893\nHandwritten GPT-4 -39.02s 0.002\nUsing a keyword as a name GCC Handwritten 63.42s <0.001\nGCC GPT-4 28.61s 0.006\nHandwritten GPT-4 -34.82s <0.001\nMissing parameter GCC Handwritten 0.54s 0.716\nGCC GPT-4 -15.08s 0.021\nHandwritten GPT-4 -15.61s 0.157\nMissing curly brace GCC Handwritten 122.3s <0.001\nGCC GPT-4 50.6s 0.373\nHandwritten GPT-4 -71.6s <0.001\nReassigning a constant GCC Handwritten 53.0s 0.031\nGCC GPT-4 6.28s 0.996\nHandwritten GPT-4 -51.6s 0.029\nTime-to-fix. Time-to-fix is consistently right-tailed, so we log-\ntransformed the data to perform statistical comparisons. We noticed\nthat each task given had an effect on time-to-fix, so we performed\nwithin-task comparisons. For each task, we performed a one-way\nANOVA to compare the effect of the study condition on students‚Äô\nlog-transformed time-to-fix (Figure 3). A one-way ANOVA revealed\nthat there was a statistically significant ( ùëù < 0.05) difference in\nthe mean log-transformed time-to-fix between the conditions in\nall tasks, except for the ‚Äúflipped assignment‚Äù task, in which no\nNot the Silver Bullet UKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom\nMissing curly brace Reassigning a constant\nUsing a keyword as name Missing parameter\nFlipped assignment #const instead of #define\n10 100 1000 10 100 1000\nHandwritten\nGPT‚àí4\nGCC\nHandwritten\nGPT‚àí4\nGCC\nHandwritten\nGPT‚àí4\nGCC\nTime to fix (seconds, log‚àítransformed)\nFigure 3: Density plots of the log-transformed time-to-fix, by\ntask. The black vertical line denotes the median.\nstatistically significant difference was found (ùëù = 0.147). We will\nomit the flipped assignment task for the remainder of this section.\nFor the five remaining tasks, we performed Tukey‚Äôs HSD test on the\nmeans of log-transformed time-to-fix (Table 1). Because the mean\nof log-transformed time-to-fix is not useful to report, we instead\nreport the difference in median time-to-fix, as median is preserved\nafter log-transformation. We found that GPT-4 outperforms the\ncontrol (GCC error messages) in only one of the five remaining\ntasks, namely, ‚Äúusing a keyword as a name‚Äù. In three of the five\ntasks, we could not find a statistically significant difference in the\nmean log-transformed time-to-fix between GPT-4 and the control.\nHowever, the handwritten error messages outperformed both the\ncontrol and the GPT-4 enhanced error messages in all tasks, except\nthe ‚Äúmissing parameter‚Äù task, where we could not find a statistically\nsignificant difference between the handwritten error messages and\nthe control; however, stock GCC error messages outperformed\nGPT-4‚Äôs explanations.\nSkip rate. We gave the option for students to skip exercises after\nspending 5 minutes on them without submitting a solution. Overall,\nstudents rarely skipped exercises, with only 13 skips out of 636\nexercise attempts (2.0%). In other words, students successfully fixed\nthe programming errors in 98% of all exercises. There were zero\nskips observed for students fixing tasks under the handwritten\ncondition. The most skips (10) were observed for the control con-\ndition, whereas GPT-4 had only 3 skips. A ùúí2-test failed to find a\nstatistically significant difference between the three conditions.\n4.2 Subjective measures\n6% 16% 72%\n6% 18% 11% 25% 40%\n17% 23% 16% 14% 31%\n16% 75%\n13% 8% 27% 48%\n18% 18% 11% 23% 30%\n18% 74%\n6% 12% 10% 18% 55%\n19% 17% 10% 27% 26%\n5% 17% 72%\n5% 12% 10% 16% 57%\n15% 17% 13% 25% 31%\nEasy to\nunderstand\nHelped me\nunderstand\nHelped me fix\nthe code\nUseful for\nfixing\n50% 0% 50%\nHandwritten\nGPT‚àí4\nGCC\nHandwritten\nGPT‚àí4\nGCC\nHandwritten\nGPT‚àí4\nGCC\nHandwritten\nGPT‚àí4\nGCC\nStrongly disagree Disagree Neutral Agree Strongly agree\nFigure 4: Proportion of participants‚Äô Likert responses. From\ntop-to-bottom, the questions were ‚ÄúThe message was easy to\nunderstand‚Äù, ‚ÄúThe message helped me understand what was\nwrong with the code‚Äù, ‚ÄúThe message helped me fix the code‚Äù,\nand ‚ÄúThe message was useful for fixing the problem‚Äù.\nOpinion. After each exercise, we asked participants four Likert-\ntype questions to gauge their general opinion on how useful the\nmessage was for fixing the problem (Figure 4). We performed a\nlinear regression to predict participants‚Äô overall opinion given the\nstudy conditions. We modelled opinion as a numerical variable\nwhere Strongly disagree = -2, Disagree = -1, Neutral = 0, Agree =\n1, and Strongly agree = 2, then took the mean of a participant‚Äôs\nresponses per each condition. Using a handwritten message results\nin a 1.27 point increase in opinion compared to traditional compiler\nerror messages ( ùëù < 0.001); whereas using a GPT-4 generated\nmessage results in a 0.69 point increase in opinion compared to the\ncontrol (ùëù < 0.001). Overall, we found that participants rated both\nthe handwritten messages and GPT-4 explanations higher than\nGCC‚Äôs error messages, with the handwritten error messages being\nthe most highly rated. Participants rated GPT-4 error messages\nhighly in terms of being useful to help them solve the error, despite\nboth conditions suggesting equivalent fixes.\nMessage length. In addition to opinion, we also asked participants\nto rate the length of the error message after each exercise, on a\nUKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom Santos and Becker\nscale from way too short , too short , just right , too long , to way too\nlong. Participants overwhelmingly found that the handwritten error\nmessages were just the right length (88.1% of responses). GCC‚Äôs\nerror messages were mostly deemed as either the right length or too\nshort‚Äînever way too long. GPT-4‚Äôs error messages were roughly\nbinomially distributed across all five categories.\n5 DISCUSSION\nDespite promising evidence in prior studies [22, 39, 48], GPT-4 error\nmessage explanations do not help novices when they are resolving\nerror messages as much as one would expect. In fact, in one of\nthe tasks (‚Äúmissing parameter‚Äù), students were slower when using\nGPT-4‚Äôs explanation. Curiously, expert-handwritten error messages\noutperform GPT-4 error message explanations even though both\nsuggested equivalent solutions for each problem.\nWhen it comes to students‚Äô preferences, they preferred GPT-4‚Äôs\nerror messages over GCC‚Äôs terse, jargon-heavy error messages.\nThis makes sense, as GPT-4 would always produced full, complete\nsentences‚Äîa factor that was previously found to be important to\nerror message readability [12]. We were surprised that participants\ndid not report GPT-4‚Äôs error messages as being too long. That said,\nstudents were unable to use these messages effectively, even though\nGPT-4‚Äôs messages would always provide the correct way to solve\nthe programming error. Prior work has found that longer error\nmessages do not seem to help students [29].\nProgramming is (still) hard. Early results in understanding LLMs‚Äô\ncapabilities at introductory programming seemed promising [13,\n14, 22, 39], inspiring researchers to declare a new era for comput-\ning education [4] and even ‚Äúthe end of programming‚Äù [46]. How-\never, it seems that LLMs are not the transformative tool that they\nonce seemed. Our findings‚Äîthat, despite excelling in synthetic\nbenchmarks, LLMs do not significantly improve programmers‚Äô\nproductivity‚Äîare corroborated by a number of studies [17, 27, 28,\n44]. Additionally, participants express preference for LLMs [35, 44],\neven though LLMs‚Äô answers do not make them more effective at\nresolving programming errors. Simkute et al. [42] argue that this\nsupposed contradiction of LLM productivity is an already well-\nknown phenomenon in human factors research: automation alters\npeoples‚Äô workflows in unproductive ways, such as turning active\nproducers into passive evaluators. This change in workflow widens\nthe gap between the programmer‚Äôs mental model, and what the pro-\ngrammer ought to attend to while solving problems. In fact, there is\nevidence that LLM-powered code suggestion alters programmers‚Äô\nworkflows in ways that hamper productivity [26, 35]. LLMs have\nnot delivered on the promise of natural language programming [28];\nrather, they provide an indirect method of manipulating an existing\nabstraction: high-level source code. Similarly, LLMs have not funda-\nmentally altered the task of debugging; they just explain the already\ndifficult problem in a more approachable manner. Thus, debugging\nremains just as difficult as it was prior to the introduction of LLMs.\n5.1 Limitations\nThe generalisability of these results is limited by the sample of\nparticipants: all from one class at a European research university,\ntaught in one programming environment. Additionally, since the\ndebugging tasks were created for the purpose of this study, the\nprogramming errors may be inauthentic to the kind of debugging\nthat would naturally occur during programming. Another limitation\nwas that the same person who prepared the debugging tasks also\nwrote the handwritten error messages. There is also the confound in\nthat the GPT-4 error messages always presented the stock compiler\nerror message verbatim, rather than produce their own explanation\nof the error, without using the original error message as context. It\nis possible that participants read the stock compiler error message,\nwithout reading further to use the GPT-4 explanation.\n6 CONCLUSION\nWe conducted a within-subjects experiment where novice program-\nmers fixed six buggy programs using three different error message\nstyles. In contrast to results on synthetic benchmarks, GPT-4 en-\nhanced error messages were only more effective than stock compiler\nerror messages in one of the six programming tasks. Handwritten\nerror messages were more effective than the control in four of six\ntasks, and more effective than GPT-4‚Äôs error message explanations\nin five of six tasks. Overall, students preferred GPT-4‚Äôs messages\nover stock compiler error messages, but preferred the handwritten\nerror messages even more. This is despite the fact that the GPT-4\nerror message explanations and the handwritten error messages\nboth made the same suggestion to fix the problem. Future work\nshould further understand what factors make an error message\nusable and how programming environments and LLMs alike can be\nmodified to satisfy novices‚Äô needs. It appears we still have a long\nway to go to reach ‚Äúthe end of programming‚Äù.\nACKNOWLEDGMENTS\nThanks to Dennis Bouvier for inspiration on creating the debug-\nging tasks; Gavin McArdle and Di Meng for graciously helping us\nrun the study in their lab sessions; Simon Caton, Kira Finan, and\nOlivia Finan for help with data analysis; and Sajjad Karimian, Fionn\nMurphy, and Abdul Wadud for beta testing the online IDE. This\nstudy received approval from our institutional ethics review board\n(LS-23-56-Santos-Becker).\nREFERENCES\n[1] Titus Barik. 2018. Error Messages as Rational Reconstructions . Ph. D. Dissertation.\nNorth Carolina State University.\n[2] D. W. Barron. 1975. A Note on APL. Comput. J. 19, 1 (1975), 93.\n[3] Brett A. Becker. 2016. An Effective Approach to Enhancing Compiler Error\nMessages. In Proceedings of the 47th ACM Technical Symposium on Computing\nScience Education (Memphis, Tennessee, USA) (SIGCSE ‚Äô16) . ACM, NY, NY, USA,\n126‚Äì131.\n[4] Brett A. Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James\nPrather, and Eddie Antonio Santos. 2023. Programming Is Hard‚ÄîOr at Least It\nUsed to Be: Educational Opportunities and Challenges of AI Code Generation. In\nProceedings of the 54th ACM Technical Symposium on Computer Science Education\nV. 1 (Toronto ON, Canada) (SIGCSE 2023) . ACM, NY, NY, USA, 500‚Äì506.\n[5] Brett A. Becker, Paul Denny, Raymond Pettit, Durell Bouchard, Dennis J. Bouvier,\nBrian Harrington, Amir Kamil, Amey Karkare, Chris McDonald, Peter-Michael\nOsera, Janice L. Pearce, and James Prather. 2019. Compiler Error Messages Con-\nsidered Unhelpful: The Landscape of Text-Based Programming Error Message\nResearch. In Proceedings of the Working Group Reports on Innovation and Technol-\nogy in Computer Science Education (Aberdeen, Scotland, UK) (ITiCSE-WGR ‚Äô19).\nACM, NY, NY, USA, 177‚Äì210. https://doi.org/10.1145/3344429.3372508\n[6] Brett A. Becker, Kyle Goslin, and Graham Glanville. 2018. The Effects of Enhanced\nCompiler Error Messages on a Syntax Error Debugging Test. In Proceedings of\nthe 49th ACM Technical Symposium on Computer Science Education (Baltimore,\nMaryland, USA) (SIGCSE ‚Äô18) . ACM, New York, NY, USA, 640‚Äì645.\n[7] Brett A. Becker, Cormac Murray, Tianyi Tao, Changheng Song, Robert McCartney,\nand Kate Sanders. 2018. Fix the First, Ignore the Rest: Dealing with Multiple\nNot the Silver Bullet UKICER 2024, September 5‚Äì6, 2024, Manchester, United Kingdom\nCompiler Error Messages. In Proceedings of the 49th ACM Technical Symposium\non Computer Science Education (Baltimore, Maryland, USA) (SIGCSE ‚Äô18) . ACM,\nNY, NY, USA, 634‚Äì639. https://doi.org/10.1145/3159450.3159453\n[8] P. J. Brown. 1983. Error Messages: The Neglected Area of the Man/Machine\nInterface. Commun. ACM 26, 4 (Apr 1983), 246‚Äì249.\n[9] CJ Burgess. 1972. Compile-time error diagnostics in syntax-directed compilers.\nComput. J. 15, 4 (1972), 302‚Äì307.\n[10] Paul Denny, Juho Leinonen, James Prather, Andrew Luxton-Reilly, Thezyrie\nAmarouche, Brett A. Becker, and Brent N. Reeves. 2024. Prompt Problems: A\nNew Programming Exercise for the Generative AI Era. In Proceedings of the 55th\nACM Technical Symposium on Computer Science Education V. 1 (Portland, OR,\nUSA) (SIGCSE 2024) . ACM, New York, NY, USA, 296‚Äì302.\n[11] Paul Denny, Andrew Luxton-Reilly, and Dave Carpenter. 2014. Enhancing Syntax\nError Messages Appears Ineffectual. In Proceedings of the 2014 Conference on\nInnovation & Technology in Computer Science Education (Uppsala, Sweden) (ITiCSE\n‚Äô14). ACM, NY, NY, USA, 273‚Äì278. https://doi.org/10.1145/2591708.2591748\n[12] Paul Denny, James Prather, Brett A. Becker, Catherine Mooney, John Homer,\nZachary C Albrecht, and Garrett B. Powell. 2021. On Designing Programming Er-\nror Messages for Novices: Readability and Its Constituent Factors. In Proceedings\nof the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama,\nJapan) (CHI ‚Äô21) . ACM, NY, NY, USA, Article 55, 15 pages.\n[13] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Implications of\nOpenAI Codex on Introductory Programming. In Proceedings of the 24th Aus-\ntralasian Computing Education Conference (Virtual Event, Australia) (ACE ‚Äô22).\nACM, New York, NY, USA, 10‚Äì19. https://doi.org/10.1145/3511861.3511863\n[14] James Finnie-Ansley, Paul Denny, Andrew Luxton-Reilly, Eddie Antonio Santos,\nJames Prather, and Brett A. Becker. 2023. My AI Wants to Know if this Will\nBe On the Exam: Testing OpenAI‚Äôs Codex on CS2 Programming Exercises. In\nAustralasian Computing Education Conference (Melbourne, VIC, Australia) (ACE\n‚Äô23). ACM, NY, NY, USA, 8 pages. https://doi.org/10.1145/3576123.3576134\n[15] Devon Harker. 2017. Examining the Effects of Enhanced Compilers on Student\nProductivity. Master‚Äôs thesis. University of Northern British Columbia.\n[16] James J. Horning. 1974. What the Compiler Should Tell the User. In Compiler\nConstruction: An Advanced Course , F. L. Bauer, F. L. De Remer, M. Griffiths, U. Hill,\nJ. J. Horning, C. H. A. Koster, W. M. McKeeman, P. C. Poole, W. M. Waite, F. L.\nBauer, and J. Eickel (Eds.). Springer, Berlin, Heidelberg, 525‚Äì548.\n[17] Brij Howard-Sarin. 2024. The Future of the Error Message: Comparing Large\nLanguage Models and Novice Programmer Effectiveness in Fixing Errors. In\nProceedings of the 55th ACM Technical Symposium on Computer Science Education\nV. 2 (Portland, OR, USA) (SIGCSE 2024) . ACM, New York, NY, USA, 1881.\n[18] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian\nWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2023. A Survey on Hallucination in Large Language Models: Principles,\nTaxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL]\n[19] Tobias Kohn. 2019. The Error Behind The Message: Finding the Cause of Error\nMessages in Python. In Proceedings of the 50th ACM Technical Symposium on\nComputer Science Education . Association for Computing Machinery, New York,\nNY, USA, 524‚Äì530. https://doi.org/10.1145/3287324.3287381\n[20] Tobias Kohn and Bill Manaris. 2020. Tell Me What‚Äôs Wrong: A Python IDE with\nError Messages. In Proceedings of the 51st ACM Technical Symposium on Computer\nScience Education . ACM, NY, NY, USA, 1054‚Äì1060.\n[21] Sam Lau and Philip Guo. 2023. From ‚ÄúBan It Till We Understand It‚Äù to ‚ÄúResis-\ntance is Futile‚Äù: How University Programming Instructors Plan to Adapt as More\nStudents Use AI Code Generation and Explanation Tools such as ChatGPT and\nGitHub Copilot. In Proceedings of the 2023 ACM Conference on International Com-\nputing Education Research - Volume 1 (Chicago, IL, USA) (ICER ‚Äô23) . Association\nfor Computing Machinery, New York, NY, USA, 106‚Äì121.\n[22] Juho Leinonen, Arto Hellas, Sami Sarsa, Brent Reeves, Paul Denny, James Prather,\nand Brett A. Becker. 2023. Using Large Language Models to Enhance Program-\nming Error Messages. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1 (Toronto ON, Canada) (SIGCSE 2023) . ACM, NY,\nNY, USA, 563‚Äì569. https://doi.org/10.1145/3545945.3569770\n[23] Caleb Meredith. 2019. Writing Good Compiler Error Messages . Code ‚ÜíSoftware.\nRetrieved August 24, 2022 from https://calebmer.com/2019/07/01/writing-good-\ncompiler-error-messages.html\n[24] Microsoft. 2024. Monaco Editor . Retrieved April 10, 2024 from https://microsoft.\ngithub.io/monaco-editor/\n[25] P. G. Moulton and M. E. Muller. 1967. DITRAN‚Äîa compiler emphasizing diagnos-\ntics. Commun. ACM 10, 1 (jan 1967), 45‚Äì52. https://doi.org/10.1145/363018.363060\n[26] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2022. Reading\nBetween the Lines: Modeling User Behavior and Costs in AI-Assisted Program-\nming. arXiv:2210.14306 [cs]\n[27] Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian\nZhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, and\nDavid Sontag. 2024. The RealHumanEval: Evaluating Large Language Models‚Äô\nAbilities to Support Programmers. arXiv:2404.02806 [cs.SE]\n[28] Sydney Nguyen, Hannah McLean Babe, Yangtian Zi, Arjun Guha, Carolyn Jane\nAnderson, and Molly Q Feldman. 2024. How Beginning Programmers and Code\nLLMs (Mis)read Each Other. arXiv:2401.15232 [cs.HC]\n[29] Marie-H√©l√®ne Nienaltowski, Michela Pedroni, and Bertrand Meyer. 2008. Com-\npiler error messages: what can help novices? SIGCSE Bull. 40, 1 (Mar 2008),\n168‚Äì172. https://doi.org/10.1145/1352322.1352192\n[30] OpenAI. 2023. GPT-4 System Card. Retrieved May 14, 2023 from https://cdn.\nopenai.com/papers/gpt-4-system-card.pdf\n[31] Bruno Pereira Cipriano and Pedro Alves. 2024. ‚ÄúChatGPT Is Here to Help, Not\nto Replace Anybody‚Äù ‚Äî An Evaluation of Students‚Äô Opinions On Integrating\nChatGPT In CS Courses. arXiv:2404.17443 [cs.ET]\n[32] Raymond S. Pettit, John Homer, and Roger Gee. 2017. Do Enhanced Compiler\nError Messages Help Students?: Results Inconclusive.. In Proceedings of the 2017\nACM SIGCSE Technical Symposium on Computer Science Education (Seattle, Wash-\nington, USA) (SIGCSE ‚Äô17) . ACM, New York, NY, USA, 465‚Äì470.\n[33] Siddhartha Prasad, Ben Greenman, Tim Nelson, and Shriram Krishnamurthi. 2023.\nGenerating Programs Trivially: Student Use of Large Language Models. In Pro-\nceedings of the ACM Conference on Global Computing Education Vol 1 (Hyderabad,\nIndia) (CompEd 2023) . ACM, New York, NY, USA, 126‚Äì132.\n[34] James Prather, Raymond Pettit, Kayla Holcomb McMurry, Alani Peters, John\nHomer, Nevan Simone, and Maxine Cohen. 2017. On Novices‚Äô Interaction with\nCompiler Error Messages: A Human Factors Approach. In Proceedings of the\n2017 ACM Conference on International Computing Education Research (Tacoma,\nWashington, USA) (ICER ‚Äô17) . ACM, New York, NY, USA, 74‚Äì82.\n[35] James Prather, Brent N. Reeves, Paul Denny, Brett A. Becker, Juho Leinonen,\nAndrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, and Eddie Antonio\nSantos. 2023. ‚ÄúIt‚Äôs Weird That it Knows What I Want‚Äù: Usability and Interactions\nwith Copilot for Novice Programmers. ACM Trans. Comput.-Hum. Interact. 31, 1,\nArticle 4 (Nov 2023), 31 pages. https://doi.org/10.1145/3617367\n[36] Marco Ramponi. 2023. The Full Story of Large Language Models and RLHF . Re-\ntrieved June 5, 2024 from https://www.assemblyai.com/blog/the-full-story-of-\nlarge-language-models-and-rlhf/\n[37] Saul Rosen, Robert A. Spurgeon, and Joel K. Donnelly. 1965. PUFFT - The Purdue\nUniversity Fast FORTRAN Translator. Commun. ACM 8, 11 (nov 1965), 661‚Äì666.\n[38] Rustc developers. 2024. Errors and Lints ‚Äî Rust Compiler Development Guide .\nRetrieved March 4, 2024 from https://rustc-dev-guide.rust-lang.org/diagnostics\n[39] Eddie Antonio Santos, Prajish Prasad, and Brett A. Becker. 2023. Always Provide\nContext: The Effects of Code Context on Programming Error Message Enhance-\nment. In Proceedings of the ACM Conference on Global Computing Education Vol 1\n(Hyderabad, India) (CompEd 2023) . ACM, New York, NY, USA, 147‚Äì153.\n[40] Tom Schorsch. 1995. CAP: an automated self-assessment tool to check Pascal pro-\ngrams for syntax, logic and style errors. InProceedings of the Twenty-sixth SIGCSE\nTechnical Symposium on Computer Science Education (Nashville, Tennessee, USA)\n(SIGCSE ‚Äô95) . ACM, New York, NY, USA, 168‚Äì172.\n[41] Brian Seymour and contributors. 2023.Piston: A high performance general purpose\ncode execution engine . Retrieved April 10, 2024 from https://github.com/engineer-\nman/piston\n[42] Auste Simkute, Lev Tankelevitch, Viktor Kewenig, Ava Elizabeth Scott, Abigail\nSellen, and Sean Rintel. 2024. Ironies of Generative AI: Understanding and\nmitigating productivity loss in human-AI interactions. arXiv:2402.11364 [cs.HC]\n[43] V. Javier Traver. 2010. On Compiler Error Messages: What They Say and What\nThey Mean. Advances in Human-Computer Interaction 2010 (2010), 26 pages.\n[44] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs.\nExperience: Evaluating the Usability of Code Generation Tools Powered by Large\nLanguage Models. In CHI Conference on Human Factors in Computing Systems\nExtended Abstracts . ACM, NY NY, USA, 1‚Äì7.\n[45] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fe-\ndus. 2022. Emergent Abilities of Large Language Models. arXiv:2206.07682 [cs.CL]\n[46] Matt Welsh. 2022. The End of Programming. Commun. ACM 66, 1 (Dec 2022),\n34‚Äì35. https://doi.org/10.1145/3570220\n[47] Richard L. Wexelblat. 1976. Maxims for Malfeasant Designers, or How to Design\nLanguages to Make Programming as Difficult as Possible. In Proceedings of the\n2nd International Conference on Software Engineering (ICSE ‚Äô76) . IEEE Computer\nSociety Press, Washington, DC, USA, 331‚Äì336.\n[48] Patricia Widjojo and Christoph Treude. 2023. Addressing Compiler Errors: Stack\nOverflow or Large Language Models? arXiv:2307.10793 [cs.SE]",
  "topic": "Compiler",
  "concepts": [
    {
      "name": "Compiler",
      "score": 0.8510153889656067
    },
    {
      "name": "Computer science",
      "score": 0.8133441209793091
    },
    {
      "name": "Silver bullet",
      "score": 0.483434796333313
    },
    {
      "name": "Code (set theory)",
      "score": 0.4440648853778839
    },
    {
      "name": "Programming language",
      "score": 0.43218111991882324
    },
    {
      "name": "Error detection and correction",
      "score": 0.43081629276275635
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4034084975719452
    },
    {
      "name": "Multimedia",
      "score": 0.3421466052532196
    },
    {
      "name": "Machine learning",
      "score": 0.32036328315734863
    },
    {
      "name": "Algorithm",
      "score": 0.12415146827697754
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.09979942440986633
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100930933",
      "name": "University College Dublin",
      "country": "IE"
    }
  ],
  "cited_by": 8
}