{
  "title": "Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation",
  "url": "https://openalex.org/W4393178550",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2123048698",
      "name": "Shilin Yan",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2690510537",
      "name": "Renrui Zhang",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2153962965",
      "name": "Ziyu Guo",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2097952271",
      "name": "Wenchao Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2100066183",
      "name": "Hongyang Li",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2106105882",
      "name": "Yu Qiao",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2014081204",
      "name": "Hao Dong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2138058444",
      "name": "Zhongjiang He",
      "affiliations": [
        "China Telecom (China)",
        "China Telecom"
      ]
    },
    {
      "id": "https://openalex.org/A1892031112",
      "name": "Peng Gao",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2123048698",
      "name": "Shilin Yan",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2690510537",
      "name": "Renrui Zhang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2097952271",
      "name": "Wenchao Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1506313323",
      "name": "Wei Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2100066183",
      "name": "Hongyang Li",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2106105882",
      "name": "Yu Qiao",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2014081204",
      "name": "Hao Dong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2138058444",
      "name": "Zhongjiang He",
      "affiliations": [
        "China Telecom (China)",
        "China Telecom"
      ]
    },
    {
      "id": "https://openalex.org/A1892031112",
      "name": "Peng Gao",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3004019157",
    "https://openalex.org/W6792812285",
    "https://openalex.org/W4288099632",
    "https://openalex.org/W3191278083",
    "https://openalex.org/W4307504011",
    "https://openalex.org/W4281747483",
    "https://openalex.org/W3122411985",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3034692043",
    "https://openalex.org/W2793407941",
    "https://openalex.org/W4283796148",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W3011491087",
    "https://openalex.org/W2876852810",
    "https://openalex.org/W2432481613",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W3104844437",
    "https://openalex.org/W2894964039",
    "https://openalex.org/W6804533212",
    "https://openalex.org/W4221152841",
    "https://openalex.org/W4200631575",
    "https://openalex.org/W2936707910",
    "https://openalex.org/W3212022073",
    "https://openalex.org/W4323323799",
    "https://openalex.org/W4286588515",
    "https://openalex.org/W6840058269",
    "https://openalex.org/W4362599073",
    "https://openalex.org/W2890447039",
    "https://openalex.org/W4287653886",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3170088426",
    "https://openalex.org/W4386555501",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4372283850",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W4226024706",
    "https://openalex.org/W3201770677",
    "https://openalex.org/W4386977985",
    "https://openalex.org/W4281839746",
    "https://openalex.org/W4313175608",
    "https://openalex.org/W4388685775",
    "https://openalex.org/W4304084115",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4322615526",
    "https://openalex.org/W4313447410",
    "https://openalex.org/W3170630188",
    "https://openalex.org/W4389502051",
    "https://openalex.org/W4387436656",
    "https://openalex.org/W4312690830",
    "https://openalex.org/W3035097537",
    "https://openalex.org/W4386436509",
    "https://openalex.org/W3171098737",
    "https://openalex.org/W4389261656",
    "https://openalex.org/W4327534052",
    "https://openalex.org/W2962942822",
    "https://openalex.org/W4367628410",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4368754857",
    "https://openalex.org/W4312294051",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4313123347",
    "https://openalex.org/W4386075985",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W4361229539"
  ],
  "abstract": "Recently, video object segmentation (VOS) referred by multi-modal signals, e.g., language and audio, has evoked increasing attention in both industry and academia. It is challenging for exploring the semantic alignment within modalities and the visual correspondence across frames. However, existing methods adopt separate network architectures for different modalities, and neglect the inter-frame temporal interaction with references. In this paper, we propose MUTR, a Multi-modal Unified Temporal transformer for Referring video object segmentation. With a unified framework for the first time, MUTR adopts a DETR-style transformer and is capable of segmenting video objects designated by either text or audio reference. Specifically, we introduce two strategies to fully explore the temporal relations between videos and multi-modal signals. Firstly, for low-level temporal aggregation before the transformer, we enable the multi-modal references to capture multi-scale visual cues from consecutive video frames. This effectively endows the text or audio signals with temporal knowledge and boosts the semantic alignment between modalities. Secondly, for high-level temporal interaction after the transformer, we conduct inter-frame feature communication for different object embeddings, contributing to better object-wise correspondence for tracking along the video. On Ref-YouTube-VOS and AVSBench datasets with respective text and audio references, MUTR achieves +4.2% and +8.7% J&amp;F improvements to state-of-the-art methods, demonstrating our significance for unified multi-modal VOS. Code is released at https://github.com/OpenGVLab/MUTR.",
  "full_text": "Referred by Multi-Modality:\nA Unified Temporal Transformer for Video Object Segmentation\nShilin Yan1,2*, Renrui Zhang2,3*, Ziyu Guo3*, Wenchao Chen1, Wei Zhang1â€ , Hongyang Li2â€ \nYu Qiao2, Hao Dong4,5, Zhongjiang He6, Peng Gao2â€ \n1School of Computer Science, Fudan University\n2Shanghai Artificial Intelligence Laboratory\n3The Chinese University of Hong Kong\n4School of CS, Peking University\n5PKU-agibot Lab\n6China Telecom Corporation Ltd. Data&AI Technology Company\nyansl21@m.fudan.edu.cn, {zhangrenrui, guoziyu, gaopeng}@pjlab.org.cn, weizh@fudan.edu.cn\nAbstract\nRecently, video object segmentation (VOS) referred by multi-\nmodal signals, e.g., language and audio, has evoked increas-\ning attention in both industry and academia. It is challenging\nfor exploring the semantic alignment within modalities and\nthe visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different\nmodalities, and neglect the inter-frame temporal interaction\nwith references. In this paper, we propose MUTR, a Multi-\nmodal Unified Temporal transformer for Referring video ob-\nject segmentation. With a unified framework for the first time,\nMUTR adopts a DETR-style transformer and is capable of\nsegmenting video objects designated by either text or audio\nreference. Specifically, we introduce two strategies to fully ex-\nplore the temporal relations between videos and multi-modal\nsignals. Firstly, for low-level temporal aggregation before the\ntransformer, we enable the multi-modal references to cap-\nture multi-scale visual cues from consecutive video frames.\nThis effectively endows the text or audio signals with tem-\nporal knowledge and boosts the semantic alignment between\nmodalities. Secondly, for high-level temporal interaction after\nthe transformer, we conduct inter-frame feature communica-\ntion for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On\nRef-YouTube-VOS and A VSBench datasets with respective\ntext and audio references, MUTR achieves +4.2% and +8.7%\nJ &F improvements to state-of-the-art methods, demonstrat-\ning our significance for unified multi-modal VOS. Code is\nreleased at https://github.com/OpenGVLab/MUTR.\nIntroduction\nMulti-modal video object segmentation (VOS) aims to track\nand segment particular object instances across the video\nsequence referred by a given multi-modal signal, includ-\ning referring video object segmentation (RVOS) with lan-\nguage reference, and audio-visual video object segmentation\n(A V-VOS) with audio reference. Different from the vanilla\n*These authors contributed equally.\nâ€ Corresponding Author.\nCopyright Â© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nVOS (Xu et al. 2018; Yan et al. 2023) with only visual in-\nformation, the multi-modal VOS is more challenging and in\nurgent demand, which requires a comprehensive understand-\ning of different modalities and their temporal correspondence\nacross frames.\nThere exist two main challenges in multi-modal VOS.\nFirstly, it requires to not only explore the rich spatial-temporal\nconsistency in a video, but also align the multi-modal seman-\ntics among image, language, and audio. Current approaches\nmainly focus on the visual-language or visual-audio modal\nfusion within independent frames, simply by cross-modal\nattention (Chen et al. 2019; Hu et al. 2020; Shi et al. 2018) or\ndynamic convolutions (Margffoy-Tuay et al. 2018) for feature\ninteraction. This, however, neglects the multi-modal temporal\ninformation across frames, which is significant for consistent\nobject segmentation and tracking along the video. Secondly,\nfor the given references of two modalities, language and au-\ndio, existing works adopt different architecture designs and\ntraining strategies to separately tackle their modal-specific\ncharacteristics. Therefore, a powerful and unified framework\nfor multi-modal VOS still remains an open question.\nTo address these challenges, we propose MUTR, a Multi-\nmodal Unified Temporal transformer for Referring video ob-\nject segmentation. Our approach, for the first time, presents a\ngeneric framework for both language and audio references,\nand enhances the interaction between temporal frames and\nmulti-modal signals. In detail, we adopt a DETR-like (Carion\net al. 2020) encoder-decoder transformer, which serves as the\nbasic architecture to process visual information within differ-\nent frames. On top of this, we introduce two attention-based\nmodules respectively for low-level multi-modal temporal\naggregation (MTA), and high-level multi-object temporal in-\nteraction (MTI). Firstly before the transformer, we utilize the\nencoded multi-modal references as queries to aggregate infor-\nmative visual and temporal features via the MTA module. We\nconcatenate the visual features of adjacent frames and adopt\nsequential attention blocks for multi-modal tokens to progres-\nsively capture temporal visual cues of different image scales.\nThis contributes to better low-level cross-modal alignment\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6449\nand temporal consistency. Then, we regard the multi-modal\ntokens after MTA as object queries and feed them into the\ntransformer for frame-wise decoding. After that, we apply\nthe MTI module to conduct inter-frame object-wise interac-\ntion, and maintain a set of video-wise query representations\nfor associating objects across frames inspired by (Heo et al.\n2022). Such a module enhances the instance-level temporal\ncommunication and benefits the visual correspondence for\nsegmenting the same object in a video. Finally, we utilize\na segmentation head following previous works (Wu et al.\n2022, 2021) to output the final object mask referred by multi-\nmodality input.\nTo evaluate our effectiveness, we conduct extensive exper-\niments on several popular benchmarks for multi-modal VOS.\nRVOS with language reference (Ref-YouTube-VOS (Seo,\nLee, and Han 2020) and Ref-DA VIS 2017 (Khoreva,\nRohrbach, and Schiele 2019)), and one benchmark for A V-\nVOS with audio reference (A VSBench (Zhou et al. 2022)).\nOn Ref-YouTube-VOS (Seo, Lee, and Han 2020) and Ref-\nDA VIS 2017 (Khoreva, Rohrbach, and Schiele 2019) with\nlanguage references, MUTR surpasses the state-of-the-art\nmethod ReferFromer (Wu et al. 2022) by +4.2% and +4.1%\nJ &F scores, respectively. On A V-VOS (Zhou et al. 2022)\nwith audio references, we also outperform Baseline (Zhou\net al. 2022) by +8.7% J &F score.\nOverall, our contributions are summarized as follows:\nâ€¢ For the first time, we present a unified transformer ar-\nchitecture, MUTR, to tackle video object segmentation\nreferred by multi-modal inputs, i.e., language and audio.\nâ€¢ To better align the temporal information with multi-modal\nsignals, we propose two attention-based modules, MTA\nand MTI, respectively for low-level multi-scale aggre-\ngation and high-level multi-object interaction, achieving\nsuperior cross-modal understanding in a video.\nâ€¢ On benchmarks of two modalities, our approach both\nachieves state-of-the-art results, e.g., +4.2 % and +4.1%\nJ &F for Ref-YouTube-VOS and Ref-DA VIS 2017,\n+8.7% J &F for A V-VOS. This fully indicates the sig-\nnificance and generalization ability of MUTR.\nRelated Work\nReferring video object segmentation (R-VOS). R-VOS\nintroduces the language expression for target object tracking\nand segmentation, following the trend of vision-language\nlearning (Zhang et al. 2022, 2023b; Zhu et al. 2023; Fang\net al. 2023). Existing R-VOS methods can be broadly clas-\nsified into three categories. One of the most straightforward\nideas is to apply referring image segmentation methods (Ding\net al. 2021; Yang et al. 2022; Wang et al. 2022) independently\nto video frames, such as RefVOS (Bellver et al. 2020). Obvi-\nously, it disregards the temporal information, which makes\nit difficult to process common video challenges like object\ndisappearance in reproduction. Another approach involves\npropagating the target mask detected from key frame and se-\nlecting the object to be segmented based on a visual ground-\ning model (Kamath et al. 2021; Luo et al. 2020). Although it\napplies the temporal information to some extent, its complex\nmulti-stage training approach is not desirable. The recent\nwork MTTR (Botach, Zheltonozhskii, and Baskin 2022) and\nReferFormer (Wu et al. 2022) have employed query-based\nmechanisms. Nevertheless, they are end-to-end frameworks,\nthey perform R-VOS task utilizing image-level segmentation.\nConstrastly, our unified framework fully explores video-level\nvisual-attended language information for low-level temporal\naggregation.\nAudio-visual video object segmentation (A V-VOS). In-\nspired by recent multi-modality efforts (Zhang et al. 2023a;\nGao et al. 2023; Lin et al. 2023; Wang et al. 2023; Guo\net al. 2023; Han et al. 2023b,a), A V-VOS is proposed for\npredicting pixel-level individual positions based on a given\nsound signal. There is little previous work on audio-visual\nvideo object segmentation. Until recently (Zhou et al. 2022)\nproposed the audio-visual video object segmentation dataset.\nDifferent from it, (Mo and Tian 2023) is based on the recent\nvisual foundation model Segment Anything Model (Kirillov\net al. 2023; Zhang et al. 2023c) to achieve audio-visual seg-\nmentation. However, all of them lack the temporal alignment\nbetween multi-modal information.\nMethod\nIn this section, we illustrate the details of our MUTR for\nmulti-modal video object segmentation. We first describe the\noverall pipeline in Section . Then, in Section and Section , we\nrespectively elaborate on the proposed designs of the multi-\nscale temporal aggregation module (MTA), and multi-object\ntemporal interaction module (MTI).\nOverall Pipeline\nThe overall pipeline of MUTR is shown in Figure 1. We adopt\na DETR-based (Carion et al. 2020) transformer as our basic\narchitecture, including a visual backbone, a visual encoder\nand a decoder, on top of which, two modules MTA and MTI\nare proposed for temporal multi-modal interaction. In this\nsection, we successively introduce the pipeline of MUTR for\nvideo object segmentation.\nFeature Backbone. Given an input video-text/audio pair,\nwe first sample T frames from the video clip, and utilize\nthe visual backbone and a pre-trained text/audio backbone\nto extract the image and multi-modal features. Specifically,\nwe utilize ResNet (He et al. 2016) or Swin Transformer (Liu\net al. 2021) as the visual backbone, and obtain the multi-\nscale visual features of the 2nd, 3rd, 4th stages. Concurrently,\nfor the text reference, we employ an off-the-shelf language\nmodel, RoBERTa (Liu et al. 2019), to encode the linguistic\nembedding tokens. For the audio reference, we first process\nit as a spectrogram transform via a short-time Fourier Trans-\nform and then feed it into a pre-trained VGGish (Hershey\net al. 2017) model. After the text/audio encoding, a linear\nprojection layer is adopted to align the multi-modal feature\ndimension with the visual features. Note that, following pre-\nvious work (Wu et al. 2022), we adopt an early fusion module\nin the visual backbone to inject preliminary text/audio knowl-\nedge into visual features.\nMTA Module. On top of feature extraction, we feed the\nvisual and text/audio features into the multi-scale temporal\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6450\nâ€œA person isskateboarding.â€\nVisualBackbone\nAudioBackboneMTAModule\nSegmentationHead\nInputVideo\nInputAudio\nMTIModuleğ‘€\nMulti-scaleFeatures\nVisualEncoder\nâ€¦TextBackbone\nRepeat Initialize\nVisualDecoder\nâ€¦\nâ€¦\nVisualDecoder\nâ€¦\nâ€¦\nVisualDecoder\nâ€¦\nâ€¦\nğ‘€\nâ€¦ â€¦InputText\nLosson:â€¢Maskâ€¢Bboxâ€¢Class\n1stframe2ndframe3rdframe\nğ‘„â€¦\nâ€¦OutputResult ğ‘„â€² ğ‘ƒ\nFigure 1: The Overall Pipeline of MUTR for referring video object segmentation. We present a unified transformer architecture\nto tackle video object segmentation referred by multi-modal inputs. We propose MTA module and MTI module for low-level\nmulti-scale aggregation and high-level multi-object interaction, respectively.\naggregation module (MTA). We concatenate the visual fea-\ntures of adjacent frames, and adopt cascaded cross-attention\nblocks to enhance the multi-scale and multi-modal feature\nfusion, which is specifically described in Section .\nVisual Encoder-decoder Transformer. The basic trans-\nformer consists of a visual encoder and a visual decoder,\nwhich processes the video in a frame-independent manner to\nfocus on the feature fusion within a single frame. In detail, the\nvisual encoder adopts vanilla self-attention blocks to encode\nthe multi-scale visual features. The visual decoder regards the\nencoded visual features as the key and value, and the output\nreferences from the MTA module as learnable object queries\nfor decoding. Unlike the randomly initialized queries in tradi-\ntional DETR (Carion et al. 2020), ours are input-conditioned\nones obtained via MTA module, which contains video-level\nmulti-modal prior knowledge. With the visual decoder, the\nobject queries gain rich instance information, which provides\neffective cues for the final segmentation process.\nMTI Module. After the visual transformer, a multi-object\ntemporal interaction (MTI) module is proposed for object-\nwise interaction, which is described in Section . In detail, we\nutilize an MTI encoder to communicate temporal features\nof the same object in different views. Then an MTI decoder\nis proposed to grasp information into a set of video-wise\nquery representations for associating objects across frames,\ninspired by (Heo et al. 2022).\nSegmentation Head and Loss Function. On top of the\ncomponents introduced above, we obtain the final mask pre-\ndictions from the extracted multi-modal features via a seg-\nmentation head. We follow previous works (Wu et al. 2022,\n2021) to design the segmentation head that contains a bound-\ning box head, a classification head, and a mask head. Then,\nwe find the best assignment from the predictions of MUTR\nby using Hungarian Matching (Carion et al. 2020). During\ntraining, we calculate three losses in MUTR, which are focal\nloss (Lin et al. 2017)Lcls on the predictions of referred object\nsequence, Lbox on the bounding box of predicted instance,\nand Lmask on the predicted object masks. In detail, Lbox is\nthe combination of L1 loss and GIoU loss (Rezatofighi et al.\n2019), and Lmask is the summation of the Dice (Milletari,\nNavab, and Ahmadi 2016) and binary focal loss. The whole\nloss function is formulated as\nL = Î»cls Lcls + Î»box Lbox + Î»mask Lmask , (1)\nwhere Î»cls, Î»box and Î»mask denote the weights forLcls, Lbox\nand Lmask.\nMulti-scale Temporal Aggregation\nTo boost both the multi-modal and multi-frame feature fusion,\nwe introduce Multi-scale Temporal Aggregation module for\nlow-level temporal aggregation. The proposed MTA module\ngenerates a set of object queries that contain multi-modal\nknowledge for subsequent transformer decoding.\nMulti-scale Temporal Transform. As shown in Figure ??,\nthe MTA module take the text/audio featuresFr, and multi-\nscale visual features as input, i.e., the extracted features of\n2nd, 3rd, 4th stages from the visual backbone. We first utilize\nlinear projection layers on the multi-scale features to trans-\nform them into the same dimension. Specifically, we sepa-\nrately utilize 1 Ã— 1 convolution layers on the 2nd, 3rd, 4th\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6451\nframe1 frame2 frame3\nSelf-Attn.Block\nğŒğ“ğˆğ„ğ§ğœğ¨ğğğ«\nğ‘\nâ€¦ â€¦ â€¦\nğ‘\nğ‘„â€² â€¦\nâ€¦\nâ€¦â€¦â€¦\nğŒğ®ğ¥ğ­ğ¢-ğ¬ğœğšğ¥ğğ“ğğ¦ğ©ğ¨ğ«ğšğ¥ğ€ğ ğ ğ«ğğ ğšğ­ğ¢ğ¨ğ§ğ¹!\n1stCross-Attn.Block\n2ndCrossâˆ’Attn.Block\nğ¹!\"\nMulti-scaleFeatures\nProject&Concat.\nquery\nFFN\nSelfAttn. CrossAttn.\nRandomInit.\nğŒğ“ğˆğƒğğœğ¨ğğğ«\n1stframe2ndframe3rdframe\nğŒğ®ğ¥ğ­ğ¢-ğ¨ğ›ğ£ğğœğ­ğ“ğğ¦ğ©ğ¨ğ«ğšğ¥ğˆğ§ğ­ğğ«ğšğœğ­ğ¢ğ¨ğ§\n3rdCrossâˆ’Attn.Block\n4thCrossâˆ’Attn.Block\nframe1 frame2 frame3\nframe1 frame2 frame3\nframe1 frame2 frame3\n1stframe2ndframe3rdframe\n1stframe\n2ndframe\n3rdframe\nğ‘„\nğ‘ƒ\nğ‘ƒâ€²\n1stscale\n2ndscale\n3rdscale\n4thscale\nFigure 2: Multi-scale Temporal Aggregation. For low-level\nmulti-modal temporal aggregation, we propose MTA mod-\nule for inter-frame interaction, which generates tokens with\nmulti-modal knowledge as the input queries for transformer\ndecoding.\nscale features, and an additional 3 Ã— 3 convolution layer on\nthe 4th stage features to obtain the 5th scale features. We de-\nnote the projected features as {Fi\nvj }, where 2 â‰¤ i â‰¤ 5, 1 â‰¤\nj â‰¤ T represent the stage number and frame number. After\nthat, we concatenate the visual features of adjacent frames\nfor each scale, formulated as\nFi\nv = Concat(Fi\nv1, Fi\nv2, ..., Fi\nvj , ..., Fi\nvT ), (2)\nwhere 2 â‰¤ i â‰¤ 5, 1 â‰¤ j â‰¤ T, Fi\nvj represents the pro-\njected jth frame features of ith scale, and {Fi\nv}5\ni=2 is the\nfinal transformed multi-scale visual feature. Then, the result-\ning multi-modal temporal features are regarded as the key\nand value in the following cross-attention blocks.\nMulti-modal Cross-attention. On top of this, we adopt se-\nquential cross-attention mechanisms for multi-modal tokens\nto progressively capture temporal visual cues of different\nimage scales. We adopt four cross-attention blocks that are\nassigned to each scale respectively for multi-scale temporal\nfeature extracting. In each attention block, the text/audio fea-\ntures serve as the query, while the multi-scale visual features\nserve as the key and value. We formulate it as\nFf = Blockiâˆ’1(Fr, Fi\nv, Fi\nv), 2 â‰¤ i â‰¤ 5, (3)\nwhere Block represents the sequential cross-attention blocks\nin MTA module, Ff is the output multi-modal tokens that\ncontain the multi-modal information.\nAfter that, we simply repeat the class token of Ff for\nT Ã— N times, where T is the frame number and N is the\nquery number. We adopt them as the initialized queries fed\ninto the visual transformer for frame-wise decoding. With the\nframe\t1 frame\t2 frame\t3\nSelf-Attn.Block\nğŒğ“ğˆ\tğ„ğ§ğœğ¨ğğğ«\nğ‘\nâ€¦ â€¦ â€¦\nğ‘\nğ‘„â€² â€¦\nâ€¦\nâ€¦â€¦â€¦\nğŒğ®ğ¥ğ­ğ¢-ğ¬ğœğšğ¥ğ\tğ“ğğ¦ğ©ğ¨ğ«ğšğ¥\tğ€ğ ğ ğ«ğğ ğšğ­ğ¢ğ¨ğ§ğ¹!\n1stCross-Attn.Block\n2ndCrossâˆ’Attn.Block\nğ¹!\"\nMulti-scaleFeatures\nProject&Concat.\nquery\nFFN\nCross\tAttn. SelfAttn.\nRandom\tInit.\nğŒğ“ğˆ\tğƒğğœğ¨ğğğ«\n1st\tframe2nd\tframe3rd\tframe\nğŒğ®ğ¥ğ­ğ¢-ğ¨ğ›ğ£ğğœğ­\tğ“ğğ¦ğ©ğ¨ğ«ğšğ¥\tğˆğ§ğ­ğğ«ğšğœğ­ğ¢ğ¨ğ§\n3rdCrossâˆ’Attn.Block\n4thCrossâˆ’Attn.Block\nframe\t1 frame\t2 frame\t3\nframe\t1 frame\t2 frame\t3\nframe\t1 frame\t2 frame\t3\n1stframe2ndframe3rdframe\n1stframe\n2ndframe\n3rdframe\nğ‘„\nğ‘ƒ\nğ‘ƒâ€²\n1stscale\n2ndscale\n3rdscale\n4thscale\nFigure 3: Multi-object Temporal Interaction. We introduce\nMTI module for inter-frame object-wise interaction, and\nmaintain a set of video-wise query representations for as-\nsociating objects across frames.\nMTA module, the pre-initialized input queries obtain prior\nmulti-scale knowledge and temporal information for better\nmulti-modal alignment during subsequent decoding.\nMulti-object Temporal Interaction\nAs the visual transformer adopts a frame-independent man-\nner and fails to interact information among multiple frames,\nwe further introduce a Multi-object Temporal Interaction\nmodule to conduct inter-frame object-wise interaction. This\nmodule enhances the high-level temporal communication of\nobjects, and benefits the visual correspondence for effective\nsegmentation. The details of MTI are shown in Figure ??,\nwhich consists of an MTI encoder and an MTI decoder.\nMTI Encoder. We obtain the object query outputs P of\neach frame from the transformer decoder, and feed them\ninto the MTI encoder, which contains a self-attention layer\nto conduct object-wise interaction across multiple frames,\nand a feed-forward network layer for feature transformation.\nTo achieve more efficient implementation, we adopt shifted\nwindow-attention (Liu et al. 2021) with linear computational\ncomplexity in the self-attention layer. The process of MTI\nencoder is formulated as\nPâ€² = MTI_Encoder(P) (4)\nwhere MTI_Encoder denotes the MTI encoder, and Pâ€² is\nthe outputs of MTI encoder.\nMTI Decoder. Based on the MTI encoder, we maintain\na set of video-wise query Q for associating objects across\nframes, which are randomly initialized. We regard the outputs\nfrom MTI encoder as the key and value, and feed them and\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6452\nMethod Backbone Ref-YouTube-VOS Ref-DA VIS 2017\nJ &F J F J &F J F\nCMSA (Ye et al. 2019)\nResNet-50\n34.9 33.3 36.5 34.7 32.2 37.2\nURVOS (Seo, Lee, and Han 2020) 47.2 45.3 49.2 51.5 47.3 56.0\nLBDT-4 (Ding et al. 2022b) 48.2 50.6 49.4 - - -\nYOFO (Li et al. 2022) 48.6 47.5 49.7 53.3 48.8 57.9\nReferFormer (Wu et al. 2022) 58.7 57.4 60.1 61.1 58.0 64.1\nMUTR 61.9 60.4 63.4 65.3 62.4 68.2\nCITD (Liang et al. 2021)\nResNet-101\n56.4 54.8 58.1 - - -\nReferFormer (Wu et al. 2022) 59.3 58.1 60.4 61.0 58.1 63.8\nMUTR 63.6 61.8 65.4 65.3 61.9 68.6\nReferFormer (Wu et al. 2022) Swin-L 64.2 62.3 66.2 63.9 60.8 67.0\nMUTR 68.4 66.4 70.4 68.0 64.8 71.3\nMTTR (Botach, Zheltonozhskii, and Baskin 2022)\nVideo-Swin-T\n55.3 54.0 56.6 - - -\nMANet (Chen et al. 2022) 55.6 54.8 56.5 - - -\nReferFormer (Wu et al. 2022) 62.6 59.9 63.3 62.8 60.8 67.0\nMUTR 64.0 62.2 65.8 66.5 63.0 70.0\nVLT (Ding et al. 2022a)\nVideo-Swin-B\n63.8 61.9 65.6 61.6 58.9 64.3\nReferFormer (Wu et al. 2022) 64.9 62.8 67.0 64.3 60.7 68.0\nMUTR 67.5 65.4 69.6 66.4 62.8 70.0\nTable 1: Performance of MUTR on Ref-YouTube-VOS and Ref-DA VIS 2017 Datasets. We report the results of MUTR and prior\nworks on multiple backbones, where our MUTR shows the state-of-the-art performance on all datasets.\nvideo-wise queries Q into MTI decoder for video-wise de-\ncoding. The MTI decoder consists of a cross-attention layer,\na self-attention layer, and a feed-forward network layer. We\nformulate them as\nQâ€² = MTI_Decoder(Q, Pâ€², Pâ€²) (5)\nwhere MTI_Decoder represents the MTI decoder, Qâ€² is the\noutputs of MTI decoder. In this way, the proposed MTI mod-\nule promotes high-level temporal fusion and enhances the\nconnection and interaction of the same objects in different\nframes, which further contributes to effective segmentation.\nJoint Training for Multi-modality\nAs a unified VOS framework for multi-modality, MUTR has\nthe potential to segment video objects referred by either text\nor audio reference. To achieve this, we conduct joint training\nby combining both text- and audio-referred datasets. Specifi-\ncally, to balance the data amount of two modalities, the joint\ntraining data is composed of partial Ref-YouTube-VOS (Seo,\nLee, and Han 2020) (text reference) and the entire A VSBench\nS4 (Zhou et al. 2022) (audio reference). We sample a subset\nof Ref-YouTube-VOS for training (10,093 clips (5 frames per\nclip) out of 72,920), for which we utilize only one description\nfor videos with multiple text descriptions, and filter out half\nof the instances based on odd-index positions for training.\nFor text or audio reference, we accordingly switch to their\nrespective encoders for feature encoding, i.e., RoBERTa for\ntext and VGGish for audio. Then, they share the same sub-\nsequent network modules, i.e., MTA, visual encoder, visual\ndecoder, MTI, and segments head. By our temporal and cross-\nmodality interaction modules, the jointly trained MUTR can\nobtain superior performance on either of the two modalities.\nExperiments\nQuantitative Results\nRef-YouTube-VOS. As shown in Table 1, MUTR outper-\nforms the previous state-of-the-art methods by a large mar-\ngin under on all datasets. On Ref-YouTube-VOS, MUTR\nwith a lightweight backbone ResNet-50 achieves the supe-\nrior performance with overall\nJ &F of 61.9%, an improve-\nment of +3.2% than the previous state-of-the-art method\nReferformer. By adopting a more powerful backbone Swin-\nTransformer (Liu et al. 2021), MUTR improves the perfor-\nmance to J &F 68.4%, which is +4.2% than the previous\nmethod ReferFormer (Wu et al. 2022). Using a more strong\nbackbone, our method has a higher percentage of improve-\nment, which better reflects the robustness of our method on\nthe scaled-up model size. To reflect the powerful temporal\nmodeling capability of MUTR, we therefore adopt the video\nSwin transformer (Liu et al. 2022) as the backbone, which\nis a spatial-temporal encoder that can effectively capture the\nspatial and temporal cues simultaneously, to compensate for\nthe temporal limitations of the ReferFormer as discussed\nin (Hu et al. 2022). It can be observed that our method sig-\nnificantly outperforms the ReferFormer, which demonstrates\nthe effectiveness of the temporal consistency in our model.\nRef-DA VIS 2017. On the Ref-DA VIS 2017, our method\nalso achieves the best results under the same backbone setting.\nSince ReferFormer (Wu et al. 2022) does not include the\nresultson Ref-DA VIS 2017, we report its results using the\nofficial pre-trained models provided by ReferFormer.\nA V-VOS. Table 2 shows the performance of our MUTR on\nthe A VSBench dataset. MUTR significantly surpasses all the\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6453\nMethod Backbone A VSBench S4 A VSBench MS3\nJ &F J F J &F J F\nLVS (Chen et al. 2021) ResNet-18 44.5 37.9 51.0 31.3 29.5 33.0\nSST (Duke et al. 2021) ResNet-50 73.2 66.3 80.1 49.9 42.6 57.2\nLGVT (Zhang et al. 2021) Swin-B 81.1 74.9 87.3 50.0 40.7 59.3\nBaseline (Zhou et al. 2022) ResNet-50 78.8 72.8 84.8 52.9 47.9 57.8\nBaseline (Zhou et al. 2022) PvT-V2 83.3 78.7 87.9 59.3 54.0 64.5\nMUTR\nResNet-50 83.0 78.6 87.3 61.6 57.0 66.1\nResNet-101 83.1 78.5 87.6 63.7 59.0 68.3\nPvT-V2 85.1 80.7 89.5 67.9 63.7 72.0\nSwin-L 85.7 81.5 89.8 69.0 65.0 73.0\nVideo-Swin-T 83.0 78.7 87.2 64.0 59.2 68.7\nVideo-Swin-S 84.1 79.8 88.3 67.3 62.7 71.8\nVideo-Swin-B 85.7 81.6 89.7 68.8 64.0 73.5\nTable 2: Performance of MUTR on A VSBench Dataset. MUTR surpasses thestate-of-the-art method.\nMethods J &F J\nF\nReferFormer\n(Wu et al. 2022) 32.5 32.6\n32.4\nMUTRâˆ— 39.9 39.4\n40.5\nMUTR 41.3 40.6 42.0\nTable 3: Performance of MUTR on Ref-YouTube-VOS by\nMulti-modality Joint Training.\nMethods J &F J\nF\nBaseline (Zhou\net al. 2022) 78.8 72.8\n84.8\nMUTRâˆ— 79.7 74.5\n84.9\nMUTR 81.4 76.8 85.9\nTable 4: Performance of MUTR on A VSBench S4 by\nMulti-modality Joint Training.\nComponents Block\nNum.\nJ &F J\nF\nMulti-scale T\nemporal\nâœ“ - 1\n61.3 59.7 62.7\n- âœ“ 1 60.4 58.9 61.9\nâœ“ âœ“ 1 61.9 60.4 63.4\nâœ“ âœ“ 2\n60.7 59.3 62.2\nâœ“ âœ“ 3 60.4 59.1 61.7\nTable 5: Ablation Study of MTA Module.\nComponents Block\nNum.\nJ &F J\nF\nEncoder Decoder\nâœ“ - 3\n60.3 58.8 61.9\n- âœ“ 3 61.2 60.0 62.6\nâœ“ âœ“ 3 61.9 60.4 63.4\nâœ“ âœ“ 2\n61.1 59.5 62.6\nâœ“ âœ“ 1 60.8 59.3 62.3\nTable 6: Ablation Study of MTI Module.\nprevious best competitors (J &F 83.0% VS 78.8%; 61.6%\nVS 52.9%) with the same ResNet-50 backbone. We also\nachieve a new state-of-the-art performance with Swin-L (Liu\net al. 2021) backbone. By employing a stronger backbone,\nwe observe consistent performance improvement of MUTR,\nindicating the strong generalization of our approach.\nJoint Training Datasets. We keep most training hyper-\nparameters consistent with our previous text-referred video\nobject segmentation experiments, and adopt ResNet-50 as\nthe visual backbone. Table 3 and 4 present the performance\nof MUTR by joint training on Ref-YouTube-VOS and A VS-\nBench S4, respectively. Therein, ReferFormer, the â€˜Base-\nlineâ€™, and MUTR âˆ— are all trained exclusively on text- or\naudio-referred dataset, while MUTR is trained on the multi-\nmodality joint dataset. As shown, the single unified MUTR\nby joint training can achieve even better performance than\ntheir separate training. This indicates the effectiveness of\nour proposed architecture to serve as a unified framework\nsimultaneously for text and audio input.\nQualitative Results\nThe first two columns of Figure 4 visualize some qualitative\nresults in comparison with ReferFormer (Wu et al. 2022),\nwhich lacks inter-frame interaction in terms of temporal di-\nmension. As demonstrated, along with multiple highly similar\nobjects in the video, ReferFormer (Wu et al. 2022) is easier\nto misidentifies them. In contrast, MUTR can associate all\nthe objects in temporal, which can better track and segment\nall targets accurately.\nThe last column of Figure 4 visualizes the audio-visual\nresult compared with Baseline (Zhou et al. 2022) on A VS-\nBbench S4. With temporal consistency, MUTR success-\nfully tracks and segments challenging situations that are sur-\nrounded or occluded by similar instances.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6454\nReferFormer\n MUTR\nReferFormer MUTR\n BaselineMUTR\nTimeStep\nRef-YouTube-VOS Ref-DAVIS2017 AVSBench\nâ€A fire truck pulling outwith another behind it.â€References â€œA man in a black jacketwearing glasses.\"â€A box in the hands ofa man wearing glasses.\"â€Aman in the center wearinga blue and black jacket.\"\nAmbulance Siren\nFigure 4: Qualitative Results of MUTR. We visualize the results between ReferFormer (Wu et al. 2022) and MUTR on R-VOS\nbenchmarks and between Baseline (Zhou et al. 2022) and MUTR on A V-VOS benchmark. Compared with ReferFormer, MUTR\nperforms better on temporal consistency when segmenting multiple similar objects, i.e., fire truck in Ref-YouTube-VOS and box\nin Ref-DA VIS 2017. Also, compared with the baseline of A V-VOS (Zhou et al. 2022) that denoted as â€˜Baselineâ€™ in this figure,\nMUTR can handle serve occlusion.\nMTA MTI J&F J F FPS Parameters\n- - 60.2 58.7 61.7 19.64 168.1M\n- âœ“ 60.8 59.3 62.2 19.53 176.4M\nâœ“ - 61.5 60.1 63.0 19.44 169.3M\nâœ“ âœ“ 61.9 60.4 63.4 19.37 177.6M\nTable 7: Ablation Study of the MTA and MTI Modules.\nAblation Studies\nIn this section, we perform experiments to analyze the main\ncomponents and hyper-parameters of MUTR. All the ex-\nperiments are conducted with the ResNet-50 backbone and\nevaluate their impact by the Ref-YouTube-VOS performance.\nEffectiveness of Main Componenets. Table 7 demon-\nstrates the effectiveness of MTA and MTI proposed in our\nframework. The performance will be seriously degraded from\n61.9% to 60.2% by removing MTA and MTI modules. Be-\nsides, our MTA and MTI modules introduce a marginal in-\ncrease in inference latency, demonstrating favorable imple-\nmentation and parameter efficiency.\nAblation Study on MTA. In Table 5, if either the single-\nscale temporal aggregation or multi-scale aggregation at the\nimage level are adopted, the performance of MUTR would\nsignificantly drop to 60.4% and 61.3%, respectively, which\ndemonstrates the necessity of the MTA module. We also\nablate the number of MTA blocks. As seen in Table 5, more\nMTA blocks cannot bring further performance improvement,\nsince (1) not enough videos for training; (2) the embedding\nspace of visual and reference is only 256-dimensional, which\nis difficult to optimize so many parameters.\nAblation Study on MTI. As shown in Table 6, the perfor-\nmance of MUTR is improved by using more MTI blocks. A\npossible reason is that the larger the MTI blocks, the more suf-\nficient temporal communication between instance-level can\nbe performed. Moreover, using only the encoder or decoder,\nthe performance of MUTR would both decline.\nConclusion\nThis paper proposes a MUTR, a Multi-modal Unified\nTemporal transformer for Referring video object segmen-\ntation. A simple yet and effective Multi-scale Temporal Ag-\ngregation (MTA) is introduced for multi-modal references\nto explore low-level multi-scale visual information in video-\nlevel. Besides, the high-level Multi-object Temporal Interac-\ntion (MTI) is designed for inter-frame feature communication\nto achieve temporal correspondence between the instance-\nlevel across the entire video. Aided by the MTA and MTI, our\nMUTR achieves new state-of-the-art performance on three R-\nVOS/A V-VOS benchmarks compared to previous solutions.\nWe hope the MTA and MTI will help ease the future study\nof multi-modal VOS and related tasks (e.g., referring video\nobject tracking and video instance segmentation). We do not\nforesee negative social impact from the proposed work.\nAcknowledgements\nThis work was supported by National Natural Science Foun-\ndation of China (Grant No. 62206272). This work was also\nsupported in part by Scientific and Technological Innovation\nAction Plan of Shanghai Science and Technology Committee\n(No.22511101502 and No.21DZ2203300).\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6455\nReferences\nBellver, M.; Ventura, C.; Silberer, C.; Kazakos, I.; Torres, J.;\nand Giro-i Nieto, X. 2020. Refvos: A closer look at referring\nexpressions for video object segmentation. arXiv preprint\narXiv:2010.00263.\nBotach, A.; Zheltonozhskii, E.; and Baskin, C. 2022. End-\nto-end referring video object segmentation with multimodal\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 4985â€“4995.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Computer Visionâ€“ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23â€“28, 2020,\nProceedings, Part I 16, 213â€“229. Springer.\nChen, D.-J.; Jia, S.; Lo, Y .-C.; Chen, H.-T.; and Liu, T.-L.\n2019. See-through-text grouping for referring image seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 7454â€“7463.\nChen, H.; Xie, W.; Afouras, T.; Nagrani, A.; Vedaldi, A.; and\nZisserman, A. 2021. Localizing visual sounds the hard way.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 16867â€“16876.\nChen, W.; Hong, D.; Qi, Y .; Han, Z.; Wang, S.; Qing, L.;\nHuang, Q.; and Li, G. 2022. Multi-Attention Network for\nCompressed Video Referring Object Segmentation. In Pro-\nceedings of the 30th ACM International Conference on Mul-\ntimedia, 4416â€“4425.\nDing, H.; Liu, C.; Wang, S.; and Jiang, X. 2021. Vision-\nlanguage transformer and query generation for referring seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 16321â€“16330.\nDing, H.; Liu, C.; Wang, S.; and Jiang, X. 2022a. VLT:\nVision-Language Transformer and Query Generation for Re-\nferring Segmentation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence.\nDing, Z.; Hui, T.; Huang, J.; Wei, X.; Han, J.; and Liu,\nS. 2022b. Language-bridged spatial-temporal interaction\nfor referring video object segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 4964â€“4973.\nDuke, B.; Ahmed, A.; Wolf, C.; Aarabi, P.; and Taylor, G. W.\n2021. Sstvos: Sparse spatiotemporal transformers for video\nobject segmentation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 5912â€“\n5921.\nFang, R.; Yan, S.; Huang, Z.; Zhou, J.; Tian, H.; Dai, J.;\nand Li, H. 2023. InstructSeq: Unifying Vision Tasks with\nInstruction-conditioned Multi-modal Sequence Generation.\narXiv preprint arXiv:2311.18835.\nGao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.;\nZhang, W.; Lu, P.; He, C.; Yue, X.; et al. 2023. Llama-adapter\nv2: Parameter-efficient visual instruction model. arXiv\npreprint arXiv:2304.15010.\nGuo, Z.; Zhang, R.; Zhu, X.; Tang, Y .; Ma, X.; Han, J.;\nChen, K.; Gao, P.; Li, X.; Li, H.; et al. 2023. Point-bind\n& point-llm: Aligning point cloud with multi-modality for 3d\nunderstanding, generation, and instruction following. arXiv\npreprint arXiv:2309.00615.\nHan, J.; Gong, K.; Zhang, Y .; Wang, J.; Zhang, K.; Lin, D.;\nQiao, Y .; Gao, P.; and Yue, X. 2023a. OneLLM: One Frame-\nwork to Align All Modalities with Language. arXiv preprint\narXiv:2312.03700.\nHan, J.; Zhang, R.; Shao, W.; Gao, P.; Xu, P.; Xiao, H.; Zhang,\nK.; Liu, C.; Wen, S.; Guo, Z.; et al. 2023b. Imagebind-\nllm: Multi-modality instruction tuning. arXiv preprint\narXiv:2309.03905.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 770â€“\n778.\nHeo, M.; Hwang, S.; Oh, S. W.; Lee, J.-Y .; and Kim, S. J.\n2022. Vita: Video instance segmentation via object token\nassociation. arXiv preprint arXiv:2206.04403.\nHershey, S.; Chaudhuri, S.; Ellis, D. P.; Gemmeke, J. F.;\nJansen, A.; Moore, R. C.; Plakal, M.; Platt, D.; Saurous, R. A.;\nSeybold, B.; et al. 2017. CNN architectures for large-scale\naudio classification. In 2017 ieee international conference on\nacoustics, speech and signal processing (icassp), 131â€“135.\nIEEE.\nHu, Z.; Chen, B.; Gao, Y .; Ji, Z.; and Bai, J. 2022. 1st Place\nSolution for YouTubeVOS Challenge 2022: Referring Video\nObject Segmentation. arXiv preprint arXiv:2212.14679.\nHu, Z.; Feng, G.; Sun, J.; Zhang, L.; and Lu, H. 2020. Bi-\ndirectional relationship inferring network for referring image\nsegmentation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 4424â€“4433.\nKamath, A.; Singh, M.; LeCun, Y .; Synnaeve, G.; Misra, I.;\nand Carion, N. 2021. Mdetr-modulated detection for end-\nto-end multi-modal understanding. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n1780â€“1790.\nKhoreva, A.; Rohrbach, A.; and Schiele, B. 2019. Video\nobject segmentation with language referring expressions. In\nComputer Visionâ€“ACCV 2018: 14th Asian Conference on\nComputer Vision, Perth, Australia, December 2â€“6, 2018, Re-\nvised Selected Papers, Part IV 14, 123â€“141. Springer.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo,\nW.-Y .; et al. 2023. Segment anything. arXiv preprint\narXiv:2304.02643.\nLi, D.; Li, R.; Wang, L.; Wang, Y .; Qi, J.; Zhang, L.; Liu, T.;\nXu, Q.; and Lu, H. 2022. You only infer once: Cross-modal\nmeta-transfer for referring video object segmentation. In\nProceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, 1297â€“1305.\nLiang, C.; Wu, Y .; Zhou, T.; Wang, W.; Yang, Z.; Wei, Y .; and\nYang, Y . 2021. Rethinking cross-modal interaction from a\ntop-down perspective for referring video object segmentation.\narXiv preprint arXiv:2106.01061.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and DollÃ¡r, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE international conference on computer vision,\n2980â€“2988.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6456\nLin, Z.; Liu, C.; Zhang, R.; Gao, P.; Qiu, L.; Xiao, H.; Qiu,\nH.; Lin, C.; Shao, W.; Chen, K.; et al. 2023. SPHINX: The\nJoint Mixing of Weights, Tasks, and Visual Embeddings\nfor Multi-modal Large Language Models. arXiv preprint\narXiv:2311.07575.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019.\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n10012â€“10022.\nLiu, Z.; Ning, J.; Cao, Y .; Wei, Y .; Zhang, Z.; Lin, S.; and\nHu, H. 2022. Video swin transformer. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 3202â€“3211.\nLuo, G.; Zhou, Y .; Sun, X.; Cao, L.; Wu, C.; Deng, C.; and Ji,\nR. 2020. Multi-task collaborative network for joint referring\nexpression comprehension and segmentation. In Proceedings\nof the IEEE/CVF Conference on computer vision and pattern\nrecognition, 10034â€“10043.\nMargffoy-Tuay, E.; PÃ©rez, J. C.; Botero, E.; and ArbelÃ¡ez, P.\n2018. Dynamic multimodal instance segmentation guided\nby natural language queries. In Proceedings of the European\nConference on Computer Vision (ECCV), 630â€“645.\nMilletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-net:\nFully convolutional neural networks for volumetric medical\nimage segmentation. In 2016 fourth international conference\non 3D vision (3DV), 565â€“571. Ieee.\nMo, S.; and Tian, Y . 2023. A V-SAM: Segment Anything\nModel Meets Audio-Visual Localization and Segmentation.\narXiv preprint arXiv:2305.01836.\nRezatofighi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.;\nand Savarese, S. 2019. Generalized intersection over union: A\nmetric and a loss for bounding box regression. InProceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 658â€“666.\nSeo, S.; Lee, J.-Y .; and Han, B. 2020. Urvos: Unified refer-\nring video object segmentation network with a large-scale\nbenchmark. In Computer Visionâ€“ECCV 2020: 16th European\nConference, Glasgow, UK, August 23â€“28, 2020, Proceedings,\nPart XV 16, 208â€“223. Springer.\nShi, H.; Li, H.; Meng, F.; and Wu, Q. 2018. Key-word-\naware network for referring expression image segmentation.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), 38â€“54.\nWang, K.; Ren, H.; Zhou, A.; Lu, Z.; Luo, S.; Shi, W.; Zhang,\nR.; Song, L.; Zhan, M.; and Li, H. 2023. MathCoder: Seam-\nless Code Integration in LLMs for Enhanced Mathematical\nReasoning. arXiv preprint arXiv:2310.03731.\nWang, Z.; Lu, Y .; Li, Q.; Tao, X.; Guo, Y .; Gong, M.; and Liu,\nT. 2022. Cris: Clip-driven referring image segmentation. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 11686â€“11695.\nWu, J.; Jiang, Y .; Bai, S.; Zhang, W.; and Bai, X. 2021. Seq-\nFormer: Sequential Transformer for Video Instance Segmen-\ntation. arXiv preprint arXiv:2112.08275.\nWu, J.; Jiang, Y .; Sun, P.; Yuan, Z.; and Luo, P. 2022. Lan-\nguage as queries for referring video object segmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4974â€“4984.\nXu, N.; Yang, L.; Fan, Y .; Yue, D.; Liang, Y .; Yang, J.; and\nHuang, T. 2018. Youtube-vos: A large-scale video object\nsegmentation benchmark. arXiv preprint arXiv:1809.03327.\nYan, S.; Xu, X.; Zhang, R.; Hong, L.; Chen, W.; Zhang, W.;\nand Zhang, W. 2023. PanoVOS: Bridging Non-panoramic\nand Panoramic Views with Transformer for Video Segmenta-\ntion. arXiv e-prints, arXivâ€“2309.\nYang, Z.; Wang, J.; Tang, Y .; Chen, K.; Zhao, H.; and Torr,\nP. H. 2022. Lavt: Language-aware vision transformer for re-\nferring image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n18155â€“18165.\nYe, L.; Rochan, M.; Liu, Z.; and Wang, Y . 2019. Cross-modal\nself-attention network for referring image segmentation. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 10502â€“10511.\nZhang, J.; Xie, J.; Barnes, N.; and Li, P. 2021. Learning\ngenerative vision transformer with energy-based latent space\nfor saliency prediction. Advances in Neural Information\nProcessing Systems, 34: 15448â€“15463.\nZhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li,\nH.; Gao, P.; and Qiao, Y . 2023a. LLaMA-Adapter: Efficient\nFine-tuning of Language Models with Zero-init Attention.\narXiv preprint arXiv:2303.16199.\nZhang, R.; Hu, X.; Li, B.; Huang, S.; Deng, H.; Li, H.; Qiao,\nY .; and Gao, P. 2023b. Prompt, Generate, then Cache: Cas-\ncade of Foundation Models makes Strong Few-shot Learners.\nCVPR 2023.\nZhang, R.; Jiang, Z.; Guo, Z.; Yan, S.; Pan, J.; Dong, H.; Gao,\nP.; and Li, H. 2023c. Personalize segment anything model\nwith one shot. arXiv preprint arXiv:2305.03048.\nZhang, R.; Zhang, W.; Fang, R.; Gao, P.; Li, K.; Dai, J.; Qiao,\nY .; and Li, H. 2022. Tip-Adapter: Training-free Adaption of\nCLIP for Few-shot Classification. In ECCV 2022. Springer\nNature Switzerland.\nZhou, J.; Wang, J.; Zhang, J.; Sun, W.; Zhang, J.; Birchfield,\nS.; Guo, D.; Kong, L.; Wang, M.; and Zhong, Y . 2022. Audioâ€“\nVisual Segmentation. In Computer Visionâ€“ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23â€“27, 2022,\nProceedings, Part XXXVII, 386â€“403. Springer.\nZhu, X.; Zhang, R.; He, B.; Zhou, A.; Wang, D.; Zhao, B.;\nand Gao, P. 2023. Not all features matter: Enhancing few-shot\nclip with adaptive prior refinement. ICCV 2023.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n6457",
  "topic": "Modality (humanâ€“computer interaction)",
  "concepts": [
    {
      "name": "Modality (humanâ€“computer interaction)",
      "score": 0.6035364270210266
    },
    {
      "name": "Computer vision",
      "score": 0.5846472978591919
    },
    {
      "name": "Computer science",
      "score": 0.5799201726913452
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5441085696220398
    },
    {
      "name": "Segmentation",
      "score": 0.5421541333198547
    },
    {
      "name": "Transformer",
      "score": 0.4654702842235565
    },
    {
      "name": "Engineering",
      "score": 0.1351129710674286
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4387153335",
      "name": "China Telecom",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210136246",
      "name": "China Telecom (China)",
      "country": "CN"
    }
  ]
}