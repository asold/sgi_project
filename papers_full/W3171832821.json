{
  "title": "Probing Pre-Trained Language Models for Disease Knowledge",
  "url": "https://openalex.org/W3171832821",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3101323746",
      "name": "Israa Alghanmi",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A2319266098",
      "name": "Luis Espinosa Anke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2189566537",
      "name": "Steven Schockaert",
      "affiliations": [
        "Cardiff University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3017885892",
    "https://openalex.org/W2970688856",
    "https://openalex.org/W3172427031",
    "https://openalex.org/W3116423158",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2997522493",
    "https://openalex.org/W2978491132",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4288265479",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3093553144",
    "https://openalex.org/W4287867803",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2970202851",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2404369708",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3105892552",
    "https://openalex.org/W2970601365",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2970986790",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2946571662",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3111372685"
  ],
  "abstract": "Pre-trained language models such as Clini-calBERT have achieved impressive results on tasks such as medical Natural Language Inference.At first glance, this may suggest that these models are able to perform medical reasoning tasks, such as mapping symptoms to diseases.However, we find that standard benchmarks such as MedNLI contain relatively few examples that require such forms of reasoning.To better understand the medical reasoning capabilities of existing language models, in this paper we introduce DisKnE, a new benchmark for Disease Knowledge Evaluation.To construct this benchmark, we annotated each positive MedNLI example with the types of medical reasoning that are needed.We then created negative examples by corrupting these positive examples in an adversarial way.Furthermore, we define training-test splits per disease, ensuring that no knowledge about test diseases can be learned from the training data, and we canonicalize the formulation of the hypotheses to avoid the presence of artefacts.This leads to a number of binary classification problems, one for each type of reasoning and each disease.When analysing pre-trained models for the clinical/biomedical domain on the proposed benchmark, we find that their performance drops considerably.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3023–3033\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3023\nProbing Pre-Trained Language Models for Disease Knowledge\nIsraa Alghanmi, Luis Espinosa-Anke, Steven Schockaert\nCardiff University, United Kingdom\n{alghanmiia,espinosa-ankel,schockaerts1}@cardiff.ac.uk\nAbstract\nPre-trained language models such as Clini-\ncalBERT have achieved impressive results on\ntasks such as medical Natural Language In-\nference. At ﬁrst glance, this may suggest\nthat these models are able to perform medi-\ncal reasoning tasks, such as mapping symp-\ntoms to diseases. However, we ﬁnd that stan-\ndard benchmarks such as MedNLI contain rel-\natively few examples that require such forms\nof reasoning. To better understand the medi-\ncal reasoning capabilities of existing language\nmodels, in this paper we introduce DisKnE, a\nnew benchmark for Disease Knowledge Eval-\nuation. To construct this benchmark, we an-\nnotated each positive MedNLI example with\nthe types of medical reasoning that are needed.\nWe then created negative examples by corrupt-\ning these positive examples in an adversar-\nial way. Furthermore, we deﬁne training-test\nsplits per disease, ensuring that no knowledge\nabout test diseases can be learned from the\ntraining data, and we canonicalize the formu-\nlation of the hypotheses to avoid the presence\nof artefacts. This leads to a number of binary\nclassiﬁcation problems, one for each type of\nreasoning and each disease. When analysing\npre-trained models for the clinical/biomedical\ndomain on the proposed benchmark, we ﬁnd\nthat their performance drops considerably.\n1 Introduction\nPre-trained language models (LMs) such as BERT\n(Devlin et al., 2019) are currently the de-facto ar-\nchitecture for solving most NLP tasks, and their\nprevalence in general language understanding tasks\nis today indisputable (Wang et al., 2018, 2019).\nBeyond generic benchmarks, it has been shown\nthat LMs are also extremely powerful in domain-\nspeciﬁc NLP tasks, e.g., in the biomedical do-\nmain (Lewis et al., 2020). While there are sev-\neral reasons why they are preferred over standard\nneural architectures, one important (and perhaps\nless obvious) reason is that LMs capture a sub-\nstantial amount of world knowledge. For instance,\nseveral authors have found that LMs are able to\nanswer questions without having access to exter-\nnal resources (Petroni et al., 2019; Roberts et al.,\n2020), or that they exhibit commonsense knowl-\nedge (Forbes et al., 2019; Davison et al., 2019). To\nanalyze the capabilities of LMs in a more system-\natic way, there is a growing interest in designing\nprobing tasks, which are now common across the\nNLP landscape, e.g., for word and sentence-level\nsemantics (Paperno et al., 2016; Conneau et al.,\n2018). In this paper we focus on (generic and\nspecialized) LMs in the biomedical domain, and\nask the following question: what kinds of medi-\ncal knowledge do pre-trained LMs capture? More\nspeciﬁcally, we focus on disease knowledge, which\nencompasses for instance the ability to link symp-\ntoms to diseases, or treatments to diseases.\nAmong the several biomedical LMs (i.e. LMs\nthat have been pre-trained on biomedical text cor-\npora) that exist today, some of the most promi-\nnent are SciBERT (Beltagy et al., 2019), BioBERT\n(Lee et al., 2020) and ClinicalBERT (Alsentzer\net al., 2019). Rather than architectural features,\nthese models differ from each other mostly in the\npre-training corpora: SciBERT was trained from\nscratch on scientiﬁc papers; BioBERT is an adapted\nversion of BERT (Devlin et al., 2019), which was\nﬁne-tuned on PubMed articles as well as some full\ntext biomedical articles; and ClinicalBERT was ini-\ntialized from BioBERT and further ﬁne-tuned on\nMIMIC-III notes (Johnson et al., 2016), which are\nclinical notes describing patients admitted to criti-\ncal care units. These LMs have enabled impressive\nresults on various reading comprehension bench-\nmarks for the medical domain, such as MedNLI\n(Romanov and Shivade, 2018) and MEDIQA-NLI\n(Abacha et al., 2019) for Natural Language Infer-\n3024\nence (NLI), and PubMedQA (Jin et al., 2019b) for\nQA. As an example, Wu et al. (2019) achieved an\naccuracy of 98% on MEDIQA-NLI, which might\nsuggest that medical NLI is essentially a solved\nproblem. This would be exciting, as medical NLI\nintuitively requires a wealth of medical knowledge,\nmuch of which is not available in structured form.\nHowever, a closer inspection of MedNLI, the\nmost well-known medical NLI benchmark, re-\nveals three important limitations, namely: (1) only\nfew test instances actually require medical disease\nknowledge, with instances that (only) require termi-\nnological and lexical knowledge (e.g. understand-\ning acronyms or paraphrases) being more prevalent;\n(2) training and test examples often cover the same\ndiseases, and thus it cannot be determined whether\ngood performance comes from the capabilities of\nthe pre-trained LM itself, or from the fact that the\nmodel can exploit similarities between training and\ntest examples; and (3) hypothesis-only baselines\nperform rather well on MedNLI, which shows that\nthis benchmark has artefacts that can be exploited,\nsimilarly to general-purpose NLI benchmarks (Po-\nliak et al., 2018).\nWe therefore propose DisKnE (Disease Knowl-\nedge Evaluation), a new benchmark for evaluating\nbiomedical LMs. This dataset explicitly addresses\nthe three limitations listed above and thus con-\nstitutes a more reliable testbed for evaluating the\ndisease knowledge captured by biomedical LMs.\nDisKnE is derived from MedNLI and is organized\ninto two top-level categories, which cover instances\nrequiring medical and terminological knowledge\nrespectively. The medical category is furthermore\ndivided into four sub-categories, depending on the\ntype of medical knowledge that is required.\nWe empirically analyse the performance of exist-\ning biomedical LMs, as well as the standard BERT\nmodel, on the proposed benchmark. Our results\nshow that all the considered LMs struggle with NLI\nexamples that require medical knowledge. We also\nﬁnd that the relative performance of the pre-trained\nmodels differs across medical categories, where\nthe best performance is obtained by ClinicalBERT,\nBioBERT, SciBERT or BERT depending on the\ncategory and experimental setting. Conversely, for\nexamples that are based on terminological knowl-\nedge, overall performance is much higher, with\nrelatively little difference between different pre-\ntrained models. The contributions of this paper are\nas follows1:\n• We introduce a new benchmark to assess the\ndisease-centred knowledge captured by pre-\ntrained LMs, organised into categories that\nreﬂect the type of reasoning that is needed,\nand with training-test splits that avoid leakage\nof disease knowledge.\n• We analyze the performance of several clini-\ncal/biomedical BERT variants on each of the\nconsidered categories. We ﬁnd that all con-\nsidered models struggle with examples that\nrequire medical disease knowledge.\n• We ﬁnd that without canonicalizing the hy-\npotheses, hypothesis-only baselines achieve\nthe best results in some categories. This shows\nthat the original MedNLI dataset suffers from\nannotation artefacts, even within the set of\nentailment examples.\n2 Related Work & Background\nKnowledge Encoded in LMsThere is a rapidly\ngrowing body of work that is focused on analyzing\nwhat knowledge is captured by pre-trained LMs.\nA recurring challenge in such analyses is to sep-\narate the knowledge that is already captured by a\npre-trained model from the knowledge that it may\nacquire during a task-speciﬁc ﬁne-tuning step. A\ncommon solution to address this is to focus on zero-\nshot performance, i.e. to focus on tasks that require\nno ﬁne-tuning, such as ﬁlling in a blank (Davison\net al., 2019; Talmor et al., 2020). As an alternative\nstrategy, Talmor et al. (2020) propose to analyse\nthe performance of models that were ﬁne-tuned on\na small training set. Other work has focused on\nextracting structured knowledge from pre-trained\nLMs. Early approaches involved manually design-\ning suitable prompts for extracting particular types\nof relations (Petroni et al., 2019). Recently, how-\never, several authors have proposed strategies that\nautomatically construct such prompts (Bouraoui\net al., 2020; Jiang et al., 2020; Shin et al., 2020).\nFinally, Bosselut et al. (2019) proposed to ﬁne-tune\nLMs on knowledge graph triples, with the aim of\nthen using the model to generate new triples.\n1All code for reconstructing the dataset and replicat-\ning the experiments is available at: https://github.\ncom/israa-alghanmi/DisKnE. License and access to\nMedNLI, MEDIQA-NLI and UMLS will be needed.\n3025\nLMs for Biomedical TextAs already mentioned\nin the introduction, a number of pre-trained LMs\nhave been released for the biomedical domain.\nSeveral authors have analyzed the performance\nof these models, and the impact of including dif-\nferent types of biomedical corpora in particular.\nFor instance, Peng et al. (2019) proposed an eval-\nuation framework for biomedical language un-\nderstanding (BLUE). They obtained the best re-\nsults with a BERT model that was pre-trained on\nPubMed abstracts and MIMIC-III clinical notes.\nAnother large-scale evaluation of biomedical LMs\nhas been carried out by Lewis et al. (2020). To\nevaluate the biomedical knowledge that is captured\nin pre-trained LMs, as opposed to acquired dur-\ning training, Jin et al. (2019a) freeze the trans-\nformer layers during training. They ﬁnd that when\nbiomedical LMs are thus used as ﬁxed feature ex-\ntractors, BioELMo outperforms BioBERT. Most\nclosely related to our work, He et al. (2020) re-\ncently also highlighted the limited ways in which\nbiomedical LMs capture disease knowledge. To\naddress this, they proposed a pre-training objec-\ntive which relies on a weak supervision signal,\nderived from the structure of Wikipedia articles\nabout diseases. Other authors have suggested to\ninclude structured knowledge, e.g. from UMLS,\nduring the pre-training stage of BERT-based mod-\nels (Michalopoulos et al., 2020; Hao et al., 2020).\nAnother strategy is to inject external knowledge\ninto task-speciﬁc models (rather than at the pre-\ntraining stage), for instance in the form of deﬁni-\ntions (Lu et al., 2019) or again UMLS (Sharma\net al., 2019). Kearns et al. (2019) presented a re-\nlated approach to our work in which they categorize\neach sentence pair according to the tense and focus\n(e.g. medication, diseases, procedures, location) of\nthe hypothesis, with the aim of providing a detailed\nexamination of MEDIQA-NLI. Based on this cat-\negorization, they compare the performance of En-\nhanced Sequential Inference Model (ESIM) using\nClinicalBERT, Embeddings of Semantic Predica-\ntions (ESP), and cui2vec. However, their analysis\nwas limited to the MEDIAQ-NLI test set, whereas\nwe include entailment examples from the entire\nMedNLI and MEDIQA-NLI datasets. Moreover,\nwe focus speciﬁcally on the ability of LMs to dis-\ntinguish between closely related diseases, and we\nmove away from the NLI setting to avoid training-\ntest leakage and artefacts.\nAdversarial NLI Several Natural Language In-\nference (NLI) benchmarks have been found to con-\ntain artefacts that can be exploited by NLP systems\nto perform well without actually solving the in-\ntended task (Poliak et al., 2018; Gururangan et al.,\n2018). In particular, it has been found that strong\nresults can often be achieved by only looking at\nthe hypothesis of a (premise, hypothesis) pair. In\nresponse to this ﬁnding, several strategies for cre-\nating harder NLI benchmarks have been proposed.\nOne established approach is to create adversarial\nstress tests (Naik et al., 2018; Glockner et al., 2018;\nAspillaga et al., 2020), in which synthetically gen-\nerated examples are created to speciﬁcally test for\nphenomena that are known to confuse NLI models.\nThis may, for instance, involve the use of WordNet\nto obtain nearly identical premise and hypothesis\nsentences, in which one word is replaced by an\nantonym or co-hyponym. In this paper, we rely\non a somewhat similar strategy, using UMLS to\nreplace diseases in hypotheses. As another strategy\nto obtain hard NLI datasets, Nie et al. (2020) used\nhuman annotators to iteratively construct examples\nthat are incorrectly labelled by a strong baseline\nmodel. While the aforementioned works are con-\ncerned with open-domain NLI, some work on creat-\ning adversarial datasets for the biomedical domain\nhas also been carried out. In particular, Araujo\net al. (2020) studied the robustness of systems for\nbiomedical named entity recognition and seman-\ntic text similarity, by introducing misspellings and\nswapping disease names by synonyms. To the best\nof our knowledge, no adversarial NLI datasets for\nthe biomedical domain have yet been proposed.\n3 Dataset Construction\nIn this section, we describe the process we followed\nfor constructing DisKnE. As we explain in more\ndetail in Section 3.1, this process involved ﬁlter-\ning the entailment instances from the MedNLI and\nMEDIQA-NLI datasets, to select those in which\nthe hypothesis expresses that the patient has (or is\nlikely to have) a particular target disease. These\ninstances were then manually categorized based\non the type of knowledge that is needed for rec-\nognizing the validity of the entailment. Section\n3.2 discusses our strategy for generating negative\nexamples, which were obtained in an adversarial\nway, by replacing diseases occurring in entailment\nexamples with similar ones. Details of the resulting\ntraining-test splits are provided in Section 3.3. In a\n3026\nCategory # inst. Premise Hypothesis\nSymptoms →Disease 112 The patient developed neck pain while training\nwith increasing substernal heaviness and left arm\npain together with sweating.\nThe patient has symptoms of acute\ncoronary syndrome\nTreatments →Disease 60 The patient started on Mucinex and Robitussin. The patient has sinus disease\nTests →Disease 116 Cardiac enzymes recorded CK 363, CK-MB 33,\nTropI 6.78\nThe patient has cardiac ischemia\nA large R hemisphere ICH was revealed when\nthe patent had head CT\nThe patient has an aneurysm\nProcedures →Disease 70 Bloody ﬂuid was removed by pericardiocentesis The patient has hemopericardium.\nTerminological 259 The patient has urinary tract infection The patient has a UTI\nThe patient has high blood pressure Hypertension\nTransfusions in the past could be the cause of\nthe patient having hepatitis C\nThe patient has hepatitis C\nTable 1: Considered categories of disease-focused entailment pairs.\nﬁnal step, we canonicalize the hypotheses of all ex-\namples, as explained in Section 3.4. Note that the\nbenchmark we propose consists of binary classiﬁ-\ncation problems (i.e. predicting entailment or not),\nrather than the standard ternary NLI setting (i.e.\npredicting entailment, neutral, or contradiction),\nwhich is motivated by the fact that natural contra-\ndiction examples are hard to ﬁnd when focusing on\ndisease knowledge.\n3.1 Selecting Entailment Pairs\nWe started from the set of all entailment pairs\n(i.e. premise-hypothesis pairs labelled with the\nentailment category) from the full MedNLI and\nMEDIQA-NLI datasets. We used MetaMap to\nﬁnd those pairs whose hypothesis mentions the\nname of a disease, and to retrieve the UMLS CUI\n(Concept Unique Identiﬁer) code corresponding to\nthat disease. We then manually identiﬁed those\npairs, among the ones whose hypothesis mentions\na disease, in which the hypothesis speciﬁcally ex-\npresses that the patient has that disease. For in-\nstance, in this step, a number of instances were\nremoved in which the hypothesis expresses that\nthe patient does not have the disease. The remain-\ning cases were manually assigned to categories\nthat reﬂect the type of disease knowledge that is\nneeded to identify that the hypothesis is entailed\nby the premise. The considered categories are de-\nscribed in Table 1, which also shows the number\nof (positive) examples we obtained and illustrative\nexamples2. The primary distinction we make is\n2For data protection reasons, we only provide synthetic\nexamples, which are different from but similar in spirit to\nbetween examples that need medical knowledge\nand those that need terminological knowledge. The\nformer category is divided into four sub-categories,\ndepending on the type of inference that is needed.\nFirst, we have the symptoms-to-disease category,\ncontaining examples where the premise describes\nthe signs or symptoms exhibited by the patient, and\nthe hypothesis mentions the corresponding diag-\nnosis. Second, we have the treatments-to-disease\ncategory, where the premise instead describe med-\nications (or other treatments followed by the pa-\ntient). The third category, tests-to-disease, involves\ninstances where the premise describes lab tests and\ndiagnostic tools such as X-rays, CT scans and MRI.\nFinally, the procedures-to-disease category has in-\nstances where the premise describes surgeries and\ntherapeutic procedures that the patient underwent.\nIn the terminological category, the disease is men-\ntioned in both the premise and hypothesis, either as\nan abbreviation, a synonym or within a rephrased\nsentence.\n3.2 Generating Examples\nThe process outlined in Section 3.1 only provides\nus with positive examples. Unfortunately, MedNLI\nand MEDIQA-NLI contain only few negative ex-\namples (i.e. instances of the neutral or contradic-\ntion categories) in which the hypothesis expresses\nthat the patient has some disease. For this rea-\nson, rather than selecting negative examples from\nthese datasets, we generate negative examples by\ncorrupting the positive examples. In particular, to\ngenerate negative examples, we replace the disease\nthose from the original MedNLI dataset.\n3027\nX from a given positive example by other diseases\nY1, ..., Yn that are similar to X, but not ancestors\nor descendants of X in SNOMED CT (Donnelly\net al., 2006). To identify similar diseases, we have\nrelied on cui2vec (Beam et al., 2020), a pre-trained\nclinical concept embedding that was learned from a\ncombination of insurance claims, clinical notes and\nbiomedical journal articles. Apart from the require-\nment that the diseases Y1, ..., Yn should be similar\nto X, it is also important that they are sufﬁciently\ncommon diseases, as including unusual diseases\nwould make the corresponding negative examples\ntoo easy to detect. For this reason, we only consider\nthe diseases that occur in the hypothesis of other\npositive examples as candidates for the negative ex-\namples. Speciﬁcally, among these set of candidate\ndiseases, we selected the n = 10most similar ones\nto X, which were not descendants or ancestors of\nX in SNOMED CT (as ancestors and descendants\nwould not necessarily invalidate the entailment).\nThis resulted in a total of 4133 examples requiring\nmedical knowledge and 2639 examples requiring\nterminological knowledge.\n3.3 Training-Test Splits\nBecause our focus is on evaluating the knowledge\ncaptured by pre-trained language models, we want\nto avoid overlap in the set of diseases in the train-\ning and test splits. In other words, if the model\nis able to correctly identify positive examples for\na target disease X, this should be a reﬂection of\nthe knowledge about X in the pre-trained model,\nrather than knowledge that it acquired during train-\ning. However, any single split into training and\ntest diseases would leave us with a relatively small\ndataset. For this reason, we consider each disease\nX in isolation. Let E be the set of all positive ex-\namples, obtained using the process from Section\n3.1. Furthermore, we write EX for the set of those\nexamples from E in which the target disease in the\nhypothesis is X. Finally, we write neg(X) for the\nset {Y1, ..., Yn} of associated diseases that was se-\nlected to construct negative examples, following\nthe process from Section 3.2.\nFor each target disease X, we deﬁne a corre-\nsponding test set TestX and training set TrainX as\nfollows. TestX contains all the positive examples\nfrom EX. Moreover, for each e ∈ EX and each\nY ∈ neg(X) we add a negative example eX→Y\nto TestX which is obtained by replacing the occur-\nrence of X by Y . If the word before the occurrence\n<Pb >,  <HbY >\n<Pb >, <HbZ >\n<Pc >, <HcZ >\n<Pc >, < HcY >\n<Pa >, <HaX >\n<Pa >, <HaY >\n<Pa >, <HaZ >\n<Pa >, <HaX > \n<Pa >,<HaY >\n<Pa >, <HaZ >\n<Pb >, <HbY >\n<Pb >, <Hb Z >\n<Pb >, <Hb X >\n<Pc >, <HcZ >\n<Pc >, <HcX >\n<Pc >, <HcY >\nFitered \n Dataset \nTarget \n Disease X\n+ \n- \n-  \nP         Premise \nH         Hypothesis \na,b,c    Set of examples \nX,Y,Z   Set of diseases\n+ \n- \n-  \n+ \n- \n-  \n+ \n- \n-  \nTraining\nTesting\n+ \n- \n+ \n- \nFigure 1: Illustration of training-test splitting process.\nof X is a or an, we modify it depending on whether\nY starts with a vowel or consonant. The positive\nexamples in TrainX consist of all examples from\nE in which X is not mentioned. Note that we\nalso remove examples in which these diseases are\nonly mentioned in the premise. Furthermore, we\ncheck for occurrences of all the synonyms of these\ndiseases that are listed in UMLS. The process of\ncreating the training and test set for a given target\ndisease X is illustrated in Figure 1.\n3.4 Canonicalization\nWe noticed that the way in which a given hypoth-\nesis expresses that “the patient has disease X” is\ncorrelated with the type of the disease. For this rea-\nson, as a ﬁnal step, we canonicalize the hypotheses\nin the dataset. Speciﬁcally, we replace each hypoth-\nesis by the name of the corresponding disease X.\nSeveral hypotheses in the dataset already have this\nform. By converting the other hypotheses in this\nformat, we eliminate any artefacts that are present\nin their speciﬁc formulation.\n4 Experiments\nWe experimentally compare a number of pre-\ntrained biomedical LMs on our proposed DisKnE\nbenchmark. In Section 4.1, we ﬁrst describe the\nconsidered LMs and the experimental setup. The\nmain results are subsequently presented in Section\n4.2. This is followed by a discussion in Section 4.3.\n3028\nClinicalBERT\nBioBERT\nSciBERT\nBERT\ncoronary atherosclerosis 0 0 29 10\nchf 67 67 67 67\nacs 04 33 0 05\nstroke 80 56 90 90\nheart disease 80 87 93 100\nmyocardial infarction 0 0 19 0\nheart failure 0 0 22 0\nurinary tract infection 100 100 67 100\ndisorder of lung 89 97 97 100\ncirrhosis of liver 0 11 0 0\nhyperglycemic disorder 27 13 22 0\npneumonia 89 93 67 100\nneurological disease 67 67 80 67\nrespiratory failure 87 70 22 43\npulmonary edema 74 25 0 50\nami 0 0 0 0\ndeep vein thrombosis 47 48 50 48\nacute cardiac ischemia 0 45 17 72\nuri 78 45 67 83\ncholangitis 22 22 33 22\natherosclerosis 66 0 67 0\nMacro-average 46±3.0 42±7.3 43±3.1 46±3.4\nWeighted average 49±3.1 47±6.0 49±2.7 51±2.7\nTable 2: Results for the Symptoms → Disease cate-\ngory in terms of F1 (%) averaged over three runs. Stan-\ndard deviations (over the three runs) of the macro and\nweighted average are also reported.\n4.1 Experimental Setup\nPre-trained LMs. To understand to what extent\nthe pretraining data of an LM affects its perfor-\nmance on our ﬁne-grained evaluation of disease\nknowledge, we used the following BERT variants:\nBERT We use the BERTbase-cased model (Devlin\net al., 2019).\nBioBERT Lee et al. (2019) proposed a model\nbased on BERTbase-cased, which they further\ntrained on biomedical corpora. We use the ver-\nsion where PubMed and PMC were utilized\nfor this further pre-training.\nClinicalBERT Alsentzer et al. (2019) introduced\nfour BERT model variants, trained on vari-\nous clinical corpora. We use the version that\nwas initialized from BioBERT and trained on\nMIMIC-III notes afterwards.\nSciBERT Beltagy et al. (2019) introduced a BERT\nmodel variant that was trained from scratch on\napproximately 1.14M scientiﬁc papers from\nClinicalBERT\nBioBERT\nSciBERT\nBERT\nchf 55 55 53 55\nacs 12 19 0 0\nhypertensive disorder 55 67 54 22\nheart disease 45 22 0 89\nurinary tract infection 100 100 100 100\ndisorder of lung 82 89 100 93\nhyperglycemic disorder 100 69 87 69\npneumonia 60 67 78 57\nanemia 17 17 45 22\nrenal insufﬁciency 69 89 67 72\npulmonary infection 82 77 89 83\ncopd 45 67 61 39\nhyperlipidemia 59 61 61 55\nMacro-average 60±6.1 61±1.4 61 ±3.8 58±1.6\nWeighted average 51 ±5.3 54 ±1.6 51±1.7 45±2.4\nTable 3: Results for the Treatments → Disease cate-\ngory in terms of F1 (%) averaged over three runs. Stan-\ndard deviations (over the three runs) of the macro and\nweighted average are also reported.\nsemantic scholar, 82% of which were biomed-\nical articles. The full text of the papers was\nused for training. We use the cased version.\nTraining Details. For ﬁne-tuning, model hyper-\nparameters were the same across all BERT variants\nsuch as the random seeds, batch size and the learn-\ning rate. In this study, we ﬁx the the learning rate\nat 2e-5, batch size of 8 and we set the maximum\nnumber of epochs to 8 with the use of early stop-\nping. We used 10% of the training set as validation\nsplit.\nEvaluation Protocol. We analyze the results per\ndisease and per category in terms of F1 score for\nthe positive class, reporting results for all diseases\nthat have at least two positive examples for the con-\nsidered category. To this end, for each disease X,\nwe start from its corresponding training-test split,\nwhich was constructed as explained in Section 3.3.\nTo show the results for a particular category, we\nremove from the test set all the examples that do\nnot belong to that category.\n4.2 Results\nThe main results are shown in Tables 2–6. A num-\nber of clear observations can be made. First, the\nresults for the terminological category are substan-\ntially higher than the results for the other categories,\nwhich suggests that the masked language modelling\n3029\nClinicalBERT\nBioBERT\nSciBERT\nBERT\ncoronary atherosclerosis 0 0 0 0\nchf 52 55 52 55\nacs 0 22 0 0\nstroke 87 87 95 77\nhypertensive disorder 09 26 45 21\nmyocardial infarction 28 0 30 14\nheart failure 0 55 40 0\nurinary tract infection 87 90 59 90\nhyperglycemic disorder 81 10 68 33\npneumonia 100 100 89 89\nanemia 0 0 24 0\naortic valve stenosis 11 24 0 27\nsyst. inﬂam. resp. syndr. 76 64 80 80\nacute renal failure syndr. 0 0 0 22\nchronic renal insufﬁciency 0 0 0 0\nkidney disease 22 0 45 0\nischemia 93 100 93 100\nMacro-average 38 ±2.4 37±1.6 42±3.1 36 ±5.0\nWeighted average 31 ±2.6 32±1.2 37±1.5 31 ±3.7\nTable 4: Results for the Tests → Disease category in\nterms of F1 (%) averaged over three runs. Standard de-\nviations (over the three runs) of the macro and weighted\naverage are also reported.\nobjective, which is used as the main pre-training\ntask in all the considered LMs, may not be ideally\nsuited for learning medical knowledge. Second,\nrecall that the main difference between the con-\nsidered biomedical LMs comes from the corpora\nthat were used for pre-training them. As the results\nfor the terminological category (Table 6) reveal,\nthe inclusion of domain-speciﬁc corpora does not\nseem to beneﬁt their ability to model biomedical\nterminology, as similar results for this category\nare obtained with the standard BERT model, which\nwas pre-trained on Wikipedia and a corpus of books\nand movie scripts. For the Symptoms → Disease\ncategory, we see that ClinicalBERT outperforms\nthe other biomedical LMs, although the standard\nBERT model actually achieves the best perfor-\nmance overall. The results suggest that Clini-\ncalBERT is better at distinguishing between rel-\natively rare diseases, but that the focus on ency-\nclopedic text beneﬁts BERT for more common\ndiseases. Intuitively, we can indeed expect that\nthe encyclopedic style of Wikipedia focuses more\non symptoms of diseases than scientiﬁc articles,\nwhich might focus more on treatments, procedures\nand diagnostic tests. This is also in accordance\nwith the ﬁndings from He et al. (2020), who ob-\nClinicalBERT\nBioBERT\nSciBERT\nBERT\ncoronary atherosclerosis 0 0 16 0\nheart disease 83 74 84 84\nheart failure 33 33 50 0\ncirrhosis of liver 0 0 0 0\nend stage renal disease 37 29 70 79\nrespiratory failure 58 27 57 27\nrenal insufﬁciency 100 100 93 100\ncardiac arrest 100 100 93 100\ndisorder of resp. syst. 76 80 80 71\nperipheral vascular dis. 0 0 78 0\nMacro-average 49 ±3.2 44±5.9 62±3.9 46±5.0\nWeighted average 40±3.3 36 ±7.4 55 ±5.6 44 ±4.6\nTable 5: Results for the Procedures → Disease cate-\ngory in terms of F1 (%) averaged over three runs. Stan-\ndard deviations (over the three runs) of the macro and\nweighted average are also reported.\ntained promising results with a disease-centric LM\npre-training task that relies on Wikipedia. On the\nProcedures → Disease and Tests → Disease cat-\negories, we can see that SciBERT achieves the\nbest results, with a particularly wide margin on\nthe Procedures → Disease category. Finally, for\nthe Treatments → Disease category, the relatively\npoor performance of BERT stands out, which con-\nforms with the aforementioned intuition that sci-\nentiﬁc articles put more emphasis on procedures,\ntreatments and tests. BioBERT achieves the best\nresults, although the performance of the other\nbiomedical LMs is quite similar.\n4.3 Discussion\nWhich LM model? Several published works\nhave found ClinicalBERT to outperform the other\nconsidered biomedical LMs on biomedical NLP\ntasks (Alsentzer et al., 2019; Kearns et al., 2019;\nHao et al., 2020). In our results, however, SciBERT\nachieves the most consistent performance, clearly\noutperforming ClinicalBERT on theProcedures →\nDisease and Test → Disease categories, while per-\nforming similar to ClinicalBERT on the remain-\ning categories. However, rather than providing a\nblanket recommendation for SciBERT, our ﬁne-\ngrained analysis highlights the fact that different\nmodels have different strengths. The most surpris-\ning ﬁnding, in this respect, is the performance of\nthe standard BERT model, which achieves the best\nresults on the Symptoms → Disease category and\n3030\nClinicalBERT\nBioBERT\nSciBERT\nBERT\nanemia 95 100 100 93\naortic valve stenosis 100 100 93 100\ncarotid artery stenosis 50 50 60 50\ncoronary atherosclerosis 79 79 76 79\ntype 2 diabetes mellitus 67 56 64 61\ngerd 0 0 0 0\ncardiac arrest 95 97 92 97\nheart disease 100 100 93 80\nheart failure 100 100 100 100\nchf 19 37 35 36\nhyperglycemic disorder 57 63 80 57\nhypertensive disorder 84 87 90 84\nacute renal failure synd. 67 67 58 61\nend-stage renal disease 77 77 78 70\ndisorder of lung 89 76 70 52\ncopd 100 100 97 100\nmyocardial infarction 24 25 25 21\npancreatitis 33 0 22 33\npleural effusion 80 100 100 80\npneumonia 89 93 89 66\npulmonary edema 87 82 56 76\nstroke 81 100 71 100\nurinary tract infection 78 77 78 77\naaa 100 96 100 100\nMacro-average 73 ±2.7 73±0.4 72±2.5 70±3.2\nWeighted average 74±1.8 76 ±1.4 75 ±1.3 72±3.0\nTable 6: Results for the terminological category in\nterms of F1 (%) averaged over three runs. Standard de-\nviations (over the three runs) of the macro and weighted\naverage are also reported.\nperforms comparably to BioBERT on several other\ncategories (with Treatments → Disease being a\nnotable exception).\nDataset Artefacts. As already reported by Ro-\nmanov and Shivade (2018), the original MedNLI\ndataset has a number of annotation artefacts, which\nmean that hypothesis-only baselines can perform\nwell. In our dataset, we tried to address this by\nonly using entailment examples, and creating nega-\ntive examples by corrupting these. However, with-\nout canonicalizing the hypotheses, we found that\nhypothesis-only baselines were still performing\nrather well. This is shown in Table 7, which sum-\nmarizes the results we obtained for a version of\nour dataset without canonicalization, i.e. where the\nfull hypotheses are provided, and the canonicalized\nversion, where the hypotheses were replaced by\nthe disease name only. The table shows results\nfor the standard ClinicalBERT model, as well as\nfor a hypothesis-only variant, which is only given\nthe hypothesis. As can be seen, without canoni-\nStandard Hyp. only\nfull can full can\nMACRO\nSymptoms → Dis. 48 ±0.7 46±3.0 47±4.9 23±0.5\nTreatments → Dis. 64±4.7 60 ±6.1 65±2.5 29±2.1\nTests → Dis. 41±1.7 38±2.4 44±2.3 18±2.0\nProcedures → Dis. 59 ±4.9 49 ±3.2 52±2.6 19 ±3.0\nTerminological 71±2.3 73±2.7 39±1.3 25±0.4\nWEIGHTED\nSymptoms → Dis. 54 ±2.9 49±3.1 53±4.7 23±1.3\nTreatments → Dis. 62±2.8 51±5.3 60±7.1 24±1.0\nTests → Dis. 37±1.4 31±2.6 42±0.2 17±2.8\nProcedures → Dis. 54±6.2 40±3.3 59±5.1 14±2.0\nTerminological 71±1.1 74±1.8 41±2.7 22±0.4\nTable 7: Comparison between a variant with the full\nhypothesis and the proposed canonicalized version. Re-\nsults are for the ClinicalBERT model in terms of F1 (%)\naveraged over three runs. Standard deviations (over the\nthree runs) of the macro and weighted average are also\nreported.\nClinicalBERT\nBioBERT\nSciBERT\nBERT\nMACRO\nSymptoms → Dis. 66±4.0 56±3.2 57±5.2 56±4.1\nTreatments → Dis. 69±4.3 70±2.0 76±4.5 55±4.8\nTests → Dis. 53±0.9 49±3.3 52±1.0 47±0.6\nProcedures → Dis. 60 ±1.8 56±0.8 76±2.6 60±4.5\nTerminological 77±0.9 77±0.6 74±0.6 76±1.0\nWEIGHTED\nSymptoms → Dis. 66 ±5.2 59±3.5 59±4.1 56±4.6\nTreatments → Dis. 64±6.2 59±3.6 68±4.8 46±3.1\nTests → Dis. 53 ±0.6 51±2.4 54±1.6 43±4.0\nProcedures → Dis. 65±3.0 58±1.0 76±0.4 67±4.5\nTerminological 76 ±1.6 77±1.0 75 ±0.4 72 ±0.7\nTable 8: Results for a variant of our benchmark, in\nwhich negative examples were selected at random, in\nterms of F1 (%) averaged over three runs. Standard de-\nviations (over the three runs) of the macro and weighted\naverage are also reported.\ncalization, the hypothesis only baseline performs\nsimilarly to the full model, even outperforming it in\na few cases, with the exception of the Terminologi-\ncal category where a clear drop in performance for\nthe hypothesis-only baseline can be seen. In con-\ntrast, for the canonicalized version of the dataset,\nwe can see that the hypothesis only baseline, which\nonly gets access to the name of the disease in this\ncase, under-performs consistently and substantially.\nNote that the hypothesis-only baseline still achieves\na non-trivial performance in most cases, noting that\nan uninformed classiﬁer that always predicts true\nwould achieve an F1 score of 0.167. However, this\nsimply shows that the model has learned to prefer\n3031\nfrequent diseases over rare ones.\nAdversarial Examples. A key design choice has\nbeen to select negative examples from the diseases\nthat are most similar to the target disease. To anal-\nyse the impact of this choice, we carried out an ex-\nperiment in which negative examples were instead\nrandomly selected. As before, we only consider\ndiseases that are present in the dataset, and we en-\nsure that negative examples are not ancestors or\ndescendants of the target disease in SNOMED CT.\nThe results are presented in Table 8. As expected,\nthe results are overall higher than those from the\nmain experiment. More surprisingly, this easier set-\nting beneﬁts some models more than others. The\nrelative performance of ClinicalBERT in particular\nis now clearly better, with this model achieving\nthe best results for Symptoms → Disease. Fur-\nthermore, the standard BERT model now clearly\nunderperforms the biomedical LMs, except for\nProcedures → Disease where it outperforms Clin-\nicalBERT and BioBERT.\n5 Conclusion\nWe have proposed DisKnE, a new benchmark for\nanalysing the extent to which biomedical language\nmodels capture knowledge about diseases. Posi-\ntive examples were obtained from MedNLI and\nMEDIQA-NLI, by manually identifying and cat-\negorizing hypotheses that express that the patient\nhas some disease. Negative examples were selected\nto be similar to the target disease. To prevent short-\ncut learning, the hypotheses were canonicalized,\nsuch that models only get access to the name of\nthe disease that is inferred. Our empirical analysis\nshows that existing biomedical language models\nparticularly struggle with cases that require medical\nknowledge. The relative performance on the differ-\nent categories suggests that different (biomedical)\nLMs have complementary strengths.\nReferences\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the MEDIQA\n2019 shared task on textual inference, question en-\ntailment and question answering. In Proceedings of\nthe 18th BioNLP Workshop and Shared Task, pages\n370–379.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop, pages\n72–78.\nVladimir Araujo, Andres Carvallo, Carlos Aspillaga,\nand Denis Parra. 2020. On adversarial examples for\nbiomedical NLP tasks. arXiv:2004.11157.\nCarlos Aspillaga, Andr ´es Carvallo, and Vladimir\nAraujo. 2020. Stress test evaluation of transformer-\nbased models in natural language understanding\ntasks. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference , pages 1882–\n1894.\nAndrew L Beam, Benjamin Kompa, Allen Schmaltz,\nInbar Fried, Grifﬁn Weber, Nathan Palmer, Xu Shi,\nTianxi Cai, and Isaac S Kohane. 2020. Clinical con-\ncept embeddings learned from massive sources of\nmultimodal medical data. In Paciﬁc Symposium on\nBiocomputing, volume 25, pages 295–306.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, pages 3613–3618.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli C ¸ elikyilmaz, and Yejin Choi.\n2019. COMET: commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Conference of the Association for\nComputational Linguistics, pages 4762–4779.\nZied Bouraoui, Jos ´e Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom BERT. In Proceedings of the Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence , pages\n7456–7463.\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. arXiv preprint\narXiv:1805.01070.\nJoe Davison, Joshua Feldman, and Alexander M. Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 1173–\n1178.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4171–4186.\nKevin Donnelly et al. 2006. Snomed-ct: The advanced\nterminology and coding system for ehealth. Studies\nin health technology and informatics, 121:279.\n3032\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo neural language representations learn physical\ncommonsense? In Proceedings of the 41th An-\nnual Meeting of the Cognitive Science Society, pages\n1753–1759.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking nli systems with sentences that re-\nquire simple lexical inferences. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics, pages 650–655.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 107–112.\nBoran Hao, Henghui Zhu, and Ioannis Paschalidis.\n2020. Enhancing clinical bert embedding using\na biomedical knowledge base. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 657–661.\nYun He, Ziwei Zhu, Yin Zhang, Qin Chen, and James\nCaverlee. 2020. Infusing disease knowledge into\nBERT for health question answering, medical infer-\nence and disease name recognition. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, pages 4604–4614.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Trans. Assoc. Comput. Linguistics ,\n8:423–438.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019a. Probing biomedical embed-\ndings from language models. In Proceedings of the\n3rd Workshop on Evaluating Vector Space Represen-\ntations for NLP, pages 82–89.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019b. PubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, pages 2567–2577.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc Data, 3(1):1–9.\nWilliam Kearns, Wilson Lau, and Jason Thomas. 2019.\nUW-BHI at MEDIQA 2019: An analysis of repre-\nsentation methods for medical natural language in-\nference. In Proceedings of the 18th BioNLP Work-\nshop and Shared Task, pages 500–509.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomed-\nical and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clin-\nical Natural Language Processing Workshop, pages\n146–157.\nMingming Lu, Yu Fang, Fengqi Yan, and Maozhen Li.\n2019. Incorporating domain knowledge into natural\nlanguage inference on clinical texts. IEEE Access,\n7:57623–57632.\nGeorge Michalopoulos, Yuanxin Wang, Hussam Kaka,\nHelen Chen, and Alex Wong. 2020. UmlsBERT:\nClinical domain knowledge augmentation of contex-\ntual embeddings using the uniﬁed medical language\nsystem metathesaurus. arXiv:2010.10391.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018.\nStress test evaluation for natural language inference.\nIn Proceedings of the 27th International Conference\non Computational Linguistics, pages 2340–2353.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4885–4901, Online. Association\nfor Computational Linguistics.\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazari-\ndou, Ngoc-Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fern ´andez. 2016. The lambada dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1525–1534.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo on ten\nbenchmarking datasets. In Proceedings of the 18th\nBioNLP Workshop and Shared Task, pages 58–65.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models as\nknowledge bases? In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\n3033\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 2463–\n2473.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language in-\nference. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 180–191.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5418–5426.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clin-\nical domain. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, Brussels, pages 1586–1596.\nSoumya Sharma, Bishal Santra, Abhik Jana, Santosh\nTokala, Niloy Ganguly, and Pawan Goyal. 2019. In-\ncorporating domain knowledge into medical NLI\nusing knowledge graphs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, pages 6091–6096.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics - on what language\nmodel pre-training captures. Trans. Assoc. Comput.\nLinguistics, 8:743–758.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. Advances in Neural Informa-\ntion Processing Systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355.\nZhaofeng Wu, Yan Song, Sicong Huang, Yuanhe Tian,\nand Fei Xia. 2019. WTMED at MEDIQA 2019: A\nhybrid approach to biomedical natural language in-\nference. In Proceedings of the 18th BioNLP Work-\nshop and Shared Task, pages 415–426.",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7954307198524475
    },
    {
      "name": "Computer science",
      "score": 0.7809817790985107
    },
    {
      "name": "Inference",
      "score": 0.7206833958625793
    },
    {
      "name": "Artificial intelligence",
      "score": 0.635326087474823
    },
    {
      "name": "Construct (python library)",
      "score": 0.5717752575874329
    },
    {
      "name": "Machine learning",
      "score": 0.5713058114051819
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48437342047691345
    },
    {
      "name": "Natural language processing",
      "score": 0.4558911919593811
    },
    {
      "name": "Natural language understanding",
      "score": 0.43609240651130676
    },
    {
      "name": "Language model",
      "score": 0.4324081242084503
    },
    {
      "name": "Binary classification",
      "score": 0.42373090982437134
    },
    {
      "name": "Test (biology)",
      "score": 0.41570913791656494
    },
    {
      "name": "Natural language",
      "score": 0.4146074056625366
    },
    {
      "name": "Adversarial system",
      "score": 0.41086292266845703
    },
    {
      "name": "Programming language",
      "score": 0.13842275738716125
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Support vector machine",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    }
  ],
  "cited_by": 7
}