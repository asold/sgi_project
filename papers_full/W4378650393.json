{
  "title": "Vision transformer for skin cancer classification",
  "url": "https://openalex.org/W4378650393",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2988090437",
      "name": "Vladyslav Nikitin",
      "affiliations": [
        "National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”"
      ]
    },
    {
      "id": "https://openalex.org/A2306648763",
      "name": "Nataliia Shapoval",
      "affiliations": [
        "National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”"
      ]
    },
    {
      "id": "https://openalex.org/A2988090437",
      "name": "Vladyslav Nikitin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306648763",
      "name": "Nataliia Shapoval",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3151130473"
  ],
  "abstract": "This paper investigates the use of vision transformers (ViT) for skin cancer classification tasks, compared to convolutional models. We propose a novel ViT architecture that effectively classifies skin cancer images. Our findings suggest that ViT models have the potential to outperform convolutional models, especially with larger datasets.",
  "full_text": " \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n449 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \n DOI 10.51582/interconf.19-20.05.2023.039 \n \nVision transformer for skin  \ncancer classification \n \nNikitin Vladyslav1,  \n \nShapoval Nataliia2 \n \n \n1 MSc in Computer Science; \nInstitute for Applied System Analysis, National Technical University of Ukraine  \n«Igor Sikorsky Kyiv Polytechnic Institute»; Ukraine \n \n2 Candidate of Engineering Sciences; \nInstitute for Applied System Analysis, National Technical University of Ukraine  \n«Igor Sikorsky Kyiv Polytechnic Institute»; Ukraine \n \n \n \n \nAbstract. \nThis paper investigates the use of vision transformers (ViT) for skin cancer classification \ntasks, compared to convolutional models. We propose a novel ViT architecture that effectively \nclassifies skin cancer images. Our findings suggest that ViT models have the potential to \noutperform convolutional models, especially with larger datasets.  \n \n \nKeywords:  \nskin cancer \nmachine learning \ncomputer vision \nvision transformer \nattention \nconvolutional neural networks \n \n  \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n450 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nINTRODUCTION \nSkin cancer is an uncontrolled growth of pathological \ncells in the epidermis caused by repaired DNA damage that \ntriggers mutations. Mutat ions force skin cells to multiply \nrapidly and create harmful tumours. The main types of skin \ncancer are basal cell carcinoma, squamous cell carcinoma, and \nmelanoma. All types of skin cancer can be caused by two main \nfactors. First, harmful solar ultraviole t radiation and \nultraviolet solariums . And second, hereditary genetic \npredisposition to this disease [1]. \nWhen this disease is diagnosed at the early stage, \ncomplete removal of the tumour with minimal scarring or no \nscarring at all is possible with a high  probability, not to \nmention the minimal risk to health. This is especially true \nfor cases where the doctor detects a tumour before it \ntransforms into a malignant one and penetrates under the skin \nlayer [1]. \nOn the other hand, late diagnosis is very likely  to \ndetermine a negative outcome. For example, for melanoma, the \n5-year mortality rate can range from 30 to 68% depending on \nthe location of the tumour [2]. In Ukraine, we have slightly \ndifferent but still sad statistics. As of 2018, 2835 cases of \nmelanoma were detected. 10.9% of these patients did not \nsurvive one year from the moment of diagnosis [3]. \nThe automation of the skin cancer detection process could \nsignificantly increase the percentage of patients who were \ndetected at an early stage and according ly increase the \nchances of successful treatment of the disease. Therefore, \nthe purpose of this work is to study the possibility of early \ndetection of skin cancer (in particular, melanoma) using \nvisual transformers – machine learning models derived from \nthe transformer model applied in natural language processing. \nThis new model outperforms standard approaches in many tasks, \nsuch as various types of convolutional neural networks. \nVISION TRANSFORMER AND ATTENTION \nThe article on the vision transformer (ViT) \"An Image is \nWorth 16x16 Words\" [4] demonstrates the implementation of a \npure transformer model without the need for convolutional \nlayers. The article shows how using a vision transformer can \nyield better results than using any convolutional model in \nimage r ecognition tasks, while using relatively fewer \n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n451 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nresources [4]. \nAs can be seen on Fig. 1, the image is getting split to \npatches, flattened and passed to attention func. After getting \nthe attention map, the needed analysis can be performed. \n \n \nFigure 1 \nViT scheme \n \nThe mechanism of attention was introduced in 2014 as a \nsolution to the problem that arises when using a fixed-length \nencoding vector in encoder-decoder type models. The issue is \nthat the decoder may have limited access to the information \npresented in the input model. \nIn general, when computing the attention function, three \nkey components are used: Query, Key, and Value. These can be \nthree identical matrices. The attention value is computed \nusing matrix multiplication of the Query and Key, divided by \nthe square root of the dimension of the Key, followed by the \napplication of the softmax function and matrix multiplication \nwith the Value (Fig. 1) [5]. \nIn this work, the multi-head attention function was used, \nwhich applies multiple attention functions to t he patches \nsimultaneously to extract different features. \nDATASET AND PREPROCESSING \nTo train the model, a dataset synthesised from the \ndatasets PH2 [6], MED -NODE [7], HAM10000 [8], ISIC [9] was \nused (Fig. 2). Mentioned dataset were selectively taken to \ncreate a balanced skin cancer dataset with maximum number of \nimages. \nFor image preprocessing, images from the dataset were \nscaled to 100x100. Options for increasing image contrast and \nconverting to a single channel (black and white) were also \nconsidered. Howev er, training the models on images with \noriginal colours proved to be more effective. To increase the \nsize of the dataset and thus improve the quality of the model, \naugmentation was used, including horizontal flipping and \nscaling of the images. \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n452 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \n \nFigure 2 \nExample of an image from the training dataset \n \nARCHITECTURES \nThree variants of the ViT architecture were considered in \nthis work. The base architecture of the model is presented in \nFigure 4. It includes an input layer, patching, patch \nencoding, 1-10 layers of transformers, and classification. \nThe model takes as input a set of three -channel 100x100 \nimages. Next comes patching. The number and size of patches \nare parameters of the model. For example, suppose we keep the \nimage size at 100x100 and have patches of size 10. Then each \npatch is unfolded into a sequence of pixels (all three \nchannels together), so the resulting image size becomes \n300x100, that is, 100 patches and 300 pixels in one patch \n(three channels). During patch encoding, position embeddings \nare added to the patches. In tables, this architecture will \nbe referred to as base_vit (Fig. 3). \n \n \nFigure 3 \nArchitecture of base_vit \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n453 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nThe second proposed approa ch is using CrossViT [10] \n(Fig. 4). It can be particularly useful for focusing attention \non specific areas of an image, which is especially important \nwhen dealing with skin lesions that may not always fit neatly \nwithin patches. The architecture introduced here will be \nreferred to as cross_vit in tables, and it represents an \ninnovative approach to skin canc er classification using \nvision transformer architectures. \n \n \nFigure 4 \nCrossViT, attention scheme [10] \n \nThe skin lesions may not always be evenly distributed \nacross patches. As a result, the edges may not always fall \nwithin the attention areas. An example c an be seen in \nFigure 5. \n \n \nFigure 5 \nHeatmap of attention for base_vit \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n454 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nTo address this issue, the use of additional larger \npatches was proposed to increase the attention value of the \nsurrounding patches (Fig. 6). However, the attention function \nworks equally in parallel for patches of both sizes at each \napplication of the attention function. Before the next layer \nof the attention function, the attention values of the larger \npatches are projected onto the attention values of the smaller \npatches, added, and no rmalised (Fig. 7). As a result, the \nattention value for the surrounding patches increases \nrelative to the centre. This architecture will be referred to \nas merge_vit in the work. \n \n \nFigure 6 \nThe values of the yellow patches will be added to the smaller patches \nthey touch. As a result, patches on the periphery can have a higher \nattention value \n \nThe basic parameters of the model are img_size, \npatch_size, tfrm_layers, proj_dim, and att_heads. img_size \ncorresponds to the size to which the image will be reduced \nduring training. patch_size corresponds to the size of one \nside of the square patch to which the image will be divided. \ntfrm_layers is how many times the multi -attention function \nwill be applied to the image. proj_dim is the dimension to \nwhich the patch will be reduced. att_heads corresponds to how \nmany single attention functions will be applied to the image. \nFor models that have two types of patches, the patch_size \nparameter will be specified in the format (x, y), where x and \ny are the sizes of the smaller and larger pa tch types, \nrespectively. \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n455 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \n  \nFigure 7 \nThe figure shows a diagram of a vision transformer using larger patches. \nAfter each attention iteration, the values of the larger patches are \nadded to the smaller ones \n \nMETRICS \nThe main metric used to monitor the model during training \nand determine the best model is accuracy, which is the \npercentage of images from the test set that are classified \ncorrectly. \nIn addition, errors of the first and second kind were \ntaken into account for comparison. Since the task is medical, \nthe accepted standard is that a \"first'' kind error is better \nthan a second kind error. In our case, this is also true \nbecause a misclassified nevus as cancer may only have the \nworst consequences as an additional visit to the doctor. On \nthe o ther hand, a second kind error, where melanoma is \nclassified as a nevus, can lead to a delay in the start of \ntreatment, which is the worst possible consequence of an \nerror. \nIt is worth noting that comparing the absolute values of \nerrors of the first and second kind in this study makes sense \nbecause the division of the sample into training and testing \nwas done once, independently of the training of any of the \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n456 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nmodels, so all models have the same training and test sets. \nOther traditional classification metrics  such as \nprecision, recall, f1 -score, support, and training charts \nwill only be provided for the most effective models. \nLEARNING \nAdamW optimizer from the tensorflow -addon framework was \nused. This is a stochastic optimizer for machine learning \nmodels that modifies the standard weight decay in the Adam \nalgorithm, making it less dependent on the updated gradient \nvalue. \nThe table shows the results of training vision \ntransformers with different architectures and parameters. \nIdeally, the same parameters should be selected for different \narchitectures, but hardware powers were limited. Therefore, \nthe model parameters were chosen to be as close to each other \nas possible. \nThe left side of the table shows the parameters with which \nthe model and its architecture were cr eated. On the right \nside, there are three key metrics that were used to select \nthe most efficient models. In the future, when referring to \nany of the models in the table, the notation \narchitecture_size_patch will be used. \nAs can be seen from Table 1, the m ost efficient models \nwere base_vit_12 and merge_vit_12. \n \nTable 1 \nComparative table of models based on vision transformers \nParameters Metrics \nArchitecture \nimg_size \npatch_size \ntfrm_layers \nproj_dim \natt_heads \naccuracy, % errors \nType 1 \nType 2 \nbase_vit 100 10 8 64 4 76.05 35 45 \nbase_vit 72 6 8 64 4 74.55 52 33 \nbase_vit 36 2 6 36 4 72.75 50 41 \nbase_vit 12 1 6 12 3 78.14 45 28 \ncross_vit 100 (10, 20) 8 64 3 67.22 70 39 \ncross_vit 72 (6, 8) 6 64 3 68.34 98 7 \ncross_vit 36 (2, 6) 6 36 2 74.12 81 5 \ncross_vit 12 (1, 4) 6 12 2 72.11 26 67 \nmerge_vit 100 (10, 20) 8 64 3 60.48 132 0 \n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n457 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nTable continuation 1 \nmerge_vit 72 (6, 12) 6 48 3 70.66 74 24 \nmerge_vit 24 (2, 6) 6 24 2 73.65 61 27 \nmerge_vit 12 (1, 3) 8 12 3 81.14 33 30 \n \nRESULTS \nBelow are the extended metrics of the models that showed \nthe best results during training (Fig. 8, Fig. 9, Fig. 10, \nFig. 11, Table 2, Table 3). \n \n \nFigure 8 \nLearning accuracy of the model base_vit_12 \n \n \nFigure 9 \nlearning loss of the model base_vit_12 \n \nTable 2 \nClassification metrics of the model base_vit_12 \n precision recall f1-score samples \nNot Cancer 0.79 0.86 0.83 202 \nCancer  0.76 0.66 0.70 132 \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n458 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \n \nFigure 10 \nLearning accuracy of the model merge_vit_12 \n \n \nFigure 11 \nLearning loss of the model merge_vit_12 \n \nTable 3 \nClassification metrics of the model loss merge_vit_12  \n precision recall f1-score samples \nNot Cancer 0.84 0.85 0.85 202 \nCancer  0.77 0.75 0.76 132 \n \nCOMPARISON WITH CNN \nFor comparison, the architecture from an existing study \non skin cancer classification [11] was taken. The authors of \nthe vision transformer claim that it outperforms \nconvolutional models on large datasets. To compare, training \non subsets of the overall training dataset with sizes of 100, \n200, 500, 1500 (the whole dataset is 3005) is proposed to \n\n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n459 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nevaluate the dependence of key metrics of each model on the \ndataset size. \n \nTable 4 \nComparison of convolution model and ViT on different dataset sizes \nparameters Metrics \nModel Dataset size accuracy, % Errors \nType 1 Type 2 \ncnn 100 60.48 132 0 \ncnn 200 60.48 132 0 \ncnn 500 45.51 80 102 \ncnn 1500 64.37 88 31 \ncnn 3005 68.26 90 16 \nbase_vit_12 100 60.48 132 0 \nbase_vit_12 200 60.48 132 0 \nbase_vit_12 500 60.48 132 0 \nbase_vit_12 1500 68.34 73 33 \nbase_vit_12 3005 78.14 45 28 \nmerge_vit_12 100 60.48 132 0 \nmerge_vit_12 200 60.48 132 0 \nmerge_vit_12 500 68.34 93 13 \nmerge_vit_12 1500 76.35 32 47 \nmerge_vit_12 3005 81.14 33 30 \n \nAs shown in Table 4, the cnn model performed better than \nthe base_vit_12 model for small datasets, but merge_vit_12 \nshowed the best results, parti cularly as the dataset size \nincreased. In conclusion, it can be inferred that the quality \nof vision transformers improves faster with dataset size, \nwhich means that transformer models may potentially have a \nconsiderable advantage over convolutional models in terms of \nefficiency for large datasets. \nCONCLUSION \nIn conclusion, skin cancer is a serious and prevalent \ndisease that can be fatal if not diagnosed and treated early. \nIn recent years, there has been a growing interest in the use \nof deep learning models for skin cancer classification, and \nthe ViT architecture has shown great promise in this field. \nWe have discussed the ViT architecture and its potential \nfor skin cancer classification. Specifically, we have \ncompared the performance of three different ViT models: basic \nViT, ViT with Cross Attention, and a modified version of basic \nViT. Our experiments have shown that ViT outperforms basic \n \n \n \n \nINFORMATION AND WEB TECHNOLOGIES \n460 \n \nThis work is distributed under the terms of the Creative \nCommons Attribution-ShareAlike 4.0 International License \n(https://creativecommons.org/licenses/by-sa/4.0/). \nProceedings of the 1st International \nScientific and Practical Conference \n«Modern Knowledge: Research  \nand Discoveries»  \n \n(May 19-20, 2023).  \nVancouver, Canada \n \n \n \nNo \n155 \nCNN models in terms of accuracy, and that the accuracy of ViT \nmodels grows more quickly with increasing dataset sizes. \nBased on these results, we can conclude that ViT has \nsignificant potential for improving the accuracy of skin \ncancer classification. While more research is needed to fully \nexplore the capabilities of this architecture, our findings \nsuggest that ViT should be considered as a promising tool for \nthis important task. By continuing to investigate and develop \nViT models for skin cancer classification, we can help to \nimprove the accuracy of diagnosis and potentially save lives. \n \nReferences: \n[1] Skin cancer information (2023) The Skin Cancer Foundation. Available \nat: https://www.skincancer.org/skin -cancer-information/ (Accessed: \nMay 1, 2023).  \n[2] Skin cancer (no date) American Academy of Dermatology. Available at: \nhttps://www.aad.org/media/stats-skin-cancer (Accessed: May 1, 2023).  \n[3] Adjusted rates 2018 melanoma of skin С43 table 1 - general rates, \n2018 (no date). Available at: http://www.ncru.inf.ua/publications/ \nBULL_21/PDF_E/38-39-mel.pdf (Accessed: May 1, 2023).  \n[4] Dosovitskiy, A. et al.  (2021) An image is worth 16x16 words:  \nTransformers for image recognition at scale , arXiv.org. Available \nat: https://arxiv.org/abs/2010.11929 (Accessed: May 1, 2023).  \n[5] Vaswani, A. et al. (2017) Attention is all you need, arXiv.org. \nAvailable at: https://arxiv.org/abs/1706.03762 (Accessed: May 1, \n2023).  \n[6] PH² Database (no date) Addi - automatic computer-based diagnosis \nsystem for dermoscopy images . Available at: https://www.fc.up.pt/ \naddi/ph2%20database.html (Accessed: May 1, 2023).  \n[7] MED-NODE (no date) Dermatology database used in Med-node. Available \nat: https://www.cs.rug.nl/~imaging/databases/melanoma_naevi/ \n(Accessed: May 1, 2023).  \n[8] Tschandl, P. (2023) The HAM10000 dataset, a large collection of \nmulti-source dermatoscopic images of common pigmented skin lesions, \nHarvard Dataverse . Harvard Datav erse. Available at: \nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7\n910%2FDVN%2FDBW86T (Accessed: May 1, 2023).  \n[9] Isic Archive (no date) ISIC Archive. Available at: https://www.isic-\narchive.com/ (Accessed: May 1, 2023).  \n[10] Chen, C.-F., Fan, Q. and Panda, R. (2021) Crossvit: Cross-attention \nmulti-scale vision transformer for Image Classification, arXiv.org. \nAvailable at: https://arxiv.org/abs/2103.14899 (Accessed: May 1, \n2023).  \n[11] Tirth Patel (no date) Tirth27/Skin-cancer-classification-using-deep-\nlearning, GitHub. Available at: https://github.com/Tirth27/Skin -\nCancer-Classification-using-Deep-Learning (Accessed: May 1, 2023).   ",
  "topic": "Skin cancer",
  "concepts": [
    {
      "name": "Skin cancer",
      "score": 0.6642985343933105
    },
    {
      "name": "Transformer",
      "score": 0.6245062351226807
    },
    {
      "name": "Computer science",
      "score": 0.6244615316390991
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6140927672386169
    },
    {
      "name": "Architecture",
      "score": 0.585442841053009
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4319571554660797
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4311320185661316
    },
    {
      "name": "Machine learning",
      "score": 0.4087228775024414
    },
    {
      "name": "Cancer",
      "score": 0.24750345945358276
    },
    {
      "name": "Medicine",
      "score": 0.21034353971481323
    },
    {
      "name": "Engineering",
      "score": 0.1865946352481842
    },
    {
      "name": "Internal medicine",
      "score": 0.10278689861297607
    },
    {
      "name": "Geography",
      "score": 0.06994590163230896
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I202483615",
      "name": "National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”",
      "country": "UA"
    }
  ],
  "cited_by": 6
}