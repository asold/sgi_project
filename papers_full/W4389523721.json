{
  "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
  "url": "https://openalex.org/W4389523721",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2224902153",
      "name": "Shubhra Kanti Karmaker Santu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4312130967",
      "name": "Dongji Feng",
      "affiliations": [
        "Auburn University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283172211",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4385573880",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4285139476",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4361866080",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385571886",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4296415404",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4327525855",
    "https://openalex.org/W4225120919",
    "https://openalex.org/W4389519863",
    "https://openalex.org/W4385572876",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W4389520103"
  ],
  "abstract": "While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied and yet to be benchmarked. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, this paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs' performance on a specific complex task.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14197–14203\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTELeR: A General Taxonomy of LLM Prompts for Benchmarking\nComplex Tasks\nShubhra Kanti Karmaker (“Santu”), Dongji Feng\nBig Data Intelligence (BDI) Lab\nDepartment of Computer Science & Software Engineering\nAuburn University, Alabama, USA\n{sks0086, dzf0023}@auburn.edu\nAbstract\nWhile LLMs have shown great success in un-\nderstanding and generating text in traditional\nconversational settings, their potential for per-\nforming ill-defined complex tasks is largely\nunder-studied and yet to be benchmarked. How-\never, conducting such benchmarking studies\nis challenging because of the large variations\nin LLMs’ performance when different prompt\ntypes/styles are used and different degrees of\ndetail are provided in the prompts. To address\nthis issue, this paper proposes a general taxon-\nomy that can be used to design prompts with\nspecific properties in order to perform a wide\nrange of complex tasks. This taxonomy will\nallow future benchmarking studies to report the\nspecific categories of prompts used as part of\nthe study, enabling meaningful comparisons\nacross different studies. Also, by establishing\na common standard through this taxonomy, re-\nsearchers will be able to draw more accurate\nconclusions about LLMs’ performance on a\nspecific complex task.\n1 Introduction\nRecently, conversational Large Language Mod-\nels (LLMs) such as GPT-3 (Brown et al., 2020),\nBard (Thoppilan et al., 2022), LLaMA (Tou-\nvron et al., 2023), BLOOM (Scao et al., 2022),\nPaLM (Chowdhery et al., 2022), etc. have demon-\nstrated exceptional performance in a wide range\nof popular natural language processing (NLP)\ntasks (Bubeck et al., 2023; Dai et al., 2022; Du\net al., 2022; Smith et al., 2022). Prompt, as a stimu-\nlator, refers to a textual input provided to the LLMs\nwith the intention of guiding its output toward a\nspecific task. Unsurprisingly, the quality and ef-\nfectiveness of the prompt can greatly influence the\nperformance of the LLMs for a particular task, and\ntherefore, designing appropriate prompts with the\nright amount of detail has become more important\nthan ever (Liu et al., 2023; Han et al., 2022).\nIn recent years, researchers have spent a signif-\nicant amount of effort proposing different ways\nof designing “appropriate” prompts. For example,\nBrown et al. (2020) showed a standard prompting\ntechnique with question-answer pairs that can re-\nsult in a few-shot effect. Researchers also explored\nother prompt design techniques such as Chain-of-\nthought (CoT) (Wei et al., 2022), Reasoning and\nActing (ReAct) (Yao et al., 2022), and other tech-\nniques (Kojima et al., 2022; Madaan and Yazdan-\nbakhsh, 2022; Press et al., 2022) in terms of im-\nproving the reasoning and acting of LLMs in solv-\ning Question-Answering tasks. Meanwhile, Kim\net al. (2023) proposed a prompting scheme where\nthe agent recursively criticizes and improves its\noutput (RCI) to solve a task. However, these ex-\nperiments primarily emphasized the utilization of\ndiverse prompts to evaluate the ability of LLMs to\nperform “well-defined” NLP tasks, while studies\nwith diverse prompts for ill-defined complex tasks\nare still rare, if not nonexistent.\nWhile conducting multiple benchmarking stud-\nies with various LLMs for complex tasks seems\ninteresting and compelling, conducting such stud-\nies is challenging because of the large variations\nin LLMs’ performance when different prompt\ntypes/styles are used and different degrees of detail\nare provided in the prompts, especially in case of\ncomplex tasks. In this paper, we exclusively focus\non understanding LLMs’ potential for performing\ncomplex tasks that are mostly: 1) ill-defined, 2)\nabstract goal-oriented, 3) highly dependent on sub-\njective interpretation, and 4) very hard to evalu-\nate quantitatively (Khot et al., 2022; Press et al.,\n2022). These complex tasks often involve mul-\ntiple steps/sub-tasks, and designing “appropriate”\nprompts for such tasks is indeed challenging as\nthere is no single rule book to follow in these\ncases (Zelikman et al., 2022; Nye et al., 2021). A\nfurther complication arises if we want to compare\ntwo independent benchmarking studies targeted\n14197\ntowards the same goal (complex) task. Such a com-\nplication arises because, for a given complex task\nand a particular LLM, the performance of the LLM\ncan drastically vary when different types/styles of\nprompts are fed to it. Indeed, the exact details in-\ncluded in the prompt play a big role in how LLMs\nwill perform in solving the goal complex task. This\nindeed creates a problem for evaluation and bench-\nmarking purposes if an apple-to-apple comparison\nis not made in terms of the prompts that are pro-\nvided to the LLMs. In other words, just reporting\naccuracy numbers for LLMs without specifying the\nfiner details of the prompts used in the experiments\nmakes comparisons across LLMs meaningless.\nUnfortunately, every complex task is different,\nand so are the prompts users can try to perform\nthe task; therefore, a general taxonomy that can\ncategorize these diverse kinds of prompts using a\nsingle standard/taxonomy has now become a press-\ning need. The main contribution of this paper is\nto introduce one such general taxonomy (we name\nit TELeR) that can be used by any benchmark-\ning study that leverages LLMs to perform some\ncomplex task. The major benefit of adopting our\nproposed TELeR taxonomy is that it will facili-\ntate more meaningful comparisons among multi-\nple LLMs in terms of their performances across\nvarious complex tasks reported by multiple inde-\npendent groups of researchers/developers and, thus,\nhelp derive more accurate conclusions. TELeR will\nachieve this goal by grounding different types of\nprompts into a common standard and allowing an\napple-to-apple comparison across different prompt\ncategories using the same standard. As such, this\ntaxonomy will serve as an important tool to es-\ntablish a common consensus around the state-of-\nthe-art LLM performance for performing complex\ntasks.\n2 Prompt Engineering for Complex Tasks\n“Prompt Engineering” is a crucial technique\nfor maximizing the utility of LLMs in various\ntasks (Zhou et al., 2022). It involves crafting and\nrevising the query or context in such a way that\nit elicits the desired response or behavior from\nLLMs (Brown et al., 2022). In practice, prompt en-\ngineering is an iterative process requiring multiple\ntrial and error runs (Shao et al., 2023).\nPrompt Engineering becomes even more criti-\ncal and challenging in case of performing complex\ntasks (Tan et al., 2023) with LLMs, as complex\ntasks usually involve multiple steps/sub-tasks re-\nquiring higher levels of semantic understanding,\nplanning, reasoning, and generation of natural lan-\nguage (Fu et al., 2022). These tasks often require\nthe model to go beyond simple pattern recognition\nor retrieval of information and involve simulating\nmore advanced cognitive abilities. In fact, differ-\nences in prompts along several key factors can have\na significant impact on the accuracy and perfor-\nmance of LLMs in complex tasks. Below, we list\nthose key factors of prompt designing.\n• Level of Details in Task Specification : The\nprompt directive should define the task or ques-\ntion being asked in sufficient detail (White et al.,\n2023; Ouyang et al., 2022). For complex tasks,\nproviding a detailed directive typically means\nfollowing the general guidelines below.\n– Clear Goal(s): Specifying clear goals helps\nguide the language model’s understanding of\nthe task or question at hand, increasing the\nchances of receiving the desired information\nor output. Therefore, one should avoid vague\nor ambiguous terms that can lead to inaccurate\nor irrelevant responses (Jiang et al., 2022).\n– Associated Data: Some prompts require\nLLMs to perform a particular task on the data\nprovided by the user in real-time, whereas\nsome prompts do not provide any data and\nrely on the pre-trained model to generate a re-\nsponse based on the background knowledge\nit has already learned. It is very important to\nmake it explicit in LLM prompts whether the\nuser is providing data as part of the prompt or\nnot, and if yes, which part is data vs. directive.\n– Distinct Sub-Tasks: By definition, complex\ntasks consist of multiple steps/ sub-tasks. It is\nimportant to mention these distinct sub-tasks\nin the prompt clearly as separate bullet points\nor numbered items. This visual organization\nhelps LLMs recognize the distinct sub-tasks\nand respond to each one individually.\n– Evaluation Criteria/Few-Shot Examples:\nLLMs can benefit from example-based learn-\ning, where prompts include specific examples\nof the desired input-output pairs (few-shot ex-\namples) (Brown et al., 2020). By incorporating\nrelevant examples, users can train the model\nto follow specific patterns or mimic desired\nbehavior. In the absence of explicit few-shot\nexamples, prompts may describe what consti-\n14198\nFigure 1: Proposed Prompt Taxonomy: TELeR (<Turn, Expression, Level of Details, Role>)\ntutes a “good” response versus what would\nmake a response “bad”. For example, impos-\ning word limits, specifying output formats, or\nrestricting particular data sources etc.\n– Additional Information Fetched via Infor-\nmation Retrieval Techniques: Additional in-\nformation fetched via Information Retrieval\nTechniques enhances Large Language Mod-\nels (LLMs) by providing them with real-time\nand contextually relevant data, improving their\nability to generate up-to-date and accurate re-\nsponses. This helps LLMs stay current and\nadapt to evolving information, making them\nmore valuable for various applications, such\nas chatbots and search engines.\n– Explanation/Justification Seeking: LLMs\nare not only good at generating textual re-\nsponses, but they can also generate explana-\ntions for their output if an explanation is sought\nas part of the prompt explicitly (Rajani et al.,\n2019). This is indeed valuable when a user\nwants to understand why the LLM generated a\nparticular output.\n• Defining Context and Role: Including relevant\ncontext and background information as part of\nthe prompt can provide the model with comple-\nmentary information in order to generate more\naccurate responses. For complex tasks, giving\nthe model a clear understanding of the context\ncan help it make more informed and precise de-\ncisions. Different prompts may provide varying\nlevels of context, which can impact the accuracy\nof the model’s responses.\n• Expression Style: Directives can be expressed\nprimarily in two styles: 1) Questions and 2) In-\nstructions. For complex tasks, one may frame\ndirectives as either a set of questions or instruc-\ntions based on their preference/application need.\n• Interaction Style: Prompts for complex tasks\nusually involve long text descriptions, often in-\ncluding details of associated sub-tasks to be\nperformed step-by-step. Therefore, some users\nmay prefer to provide these step-by-step instruc-\ntions in a multi-turn fashion (like a real dialog),\nwhereas others may prefer to provide all the de-\ntails at a single turn. Such one-turn vs. multi-turn\nprompting can also impact the performance of an\nLLM significantly as the dialog history becomes\ndifferent in generation time for these two cases.\n3 Proposed TELeR Taxonomy\nOur proposed taxonomy is based on the key fac-\ntors we discussed in section 2, whose variations\ncan lead to different outcomes while using LLMs\nto perform complex tasks. To begin with, we rep-\nresent each prompt as a combination of Directive\nand Data. Assuming Data is fixed for a given goal\ntask, the difference between two prompts essen-\n14199\ntially originates from the specifics/details of direc-\ntives they include. More specifically, we propose to\ncategorize LLM prompts for complex tasks along\nthe following four dimensions.\n1. Turn: Based on the number of turns used while\nprompting LLMs in order to perform a complex\ntask, prompts can be either single or multi-turn.\n2. Expression: Based on the expression style of\nthe overall directive as well as the associated\nsub-tasks, prompts can be either question-style\nor instruction-style.\n3. Role: Based on whether a proper system role\nis defined in the LLM system before providing\nthe actual prompt, prompts can be categorized\nas either system-role defined or undefined.\n4. Level of Details: Based on the degree of detail\nprovided in the directive, we divided prompts\ninto seven distinct levels (levels 0-6). Here, the\ndegree of detail is determined by the presence\nor absence of different aspects like clear goals,\nsub-task division, explanation seeking, few-shot\nexamples, etc. By definition, Level “0” means\nminimal details, i.e., no aspects/no directive,\nwhile Level “6” means the highest level of de-\ntails where the directive includes clear goals,\ndistinct sub-tasks/steps, an explicit requirement\nof explanation/justification, well-defined criteria\nfor evaluation, additional information fetched\nvia information retrieval techniques and/or few-\nshot examples. See Figure 1 for the exact defini-\ntions of each of these levels in our taxonomy.\nBecause we used the following four factors, i.e.,\nTurn, Expression, Level of Details and Role, to\ndefine our taxonomy, we name it as TELeR. The\noverall taxonomy is pictorially depicted in Figure 1.\n4 Two Example Use Cases\nIn this section, we present two interesting use cases\nof the proposed TELeR taxonomy that involve\nLLMs for performing a complex task: 1) gener-\nating a meta-review from peer-reviewer comments\non a scholarly work, and 2) Combining multiple\nalternative narratives into a single braided one.\n4.1 Use-Case 1: Meta-Review Generation\nMeta-reviewing is a critical part of the scientific\npeer-review process and is generally a complex\ntask that involves summarizing expert reviews from\nmultiple reviewers (Shen et al., 2022, 2023). It is\na very important and pertinent process for making\ninformed decisions and understanding the consen-\nsus of expert opinions on a submitted manuscript.\nGiven the explosion in the number of research\nmanuscript submissions in recent years and the\nhuge challenge in peer-reviewing timeline manage-\nment (Bansal et al., 2022c; Karmaker Santu et al.,\n2018), it is tempting to leverage LLMs to assist edi-\ntors (for journals)/ program chairs (for conferences)\nin preparing a first-cut draft of the meta-review for\neach manuscript from the individual review texts\nprovided by relevant expert reviewers.\nTo demonstrate the applicability of the proposed\nTELeR taxonomy for categorizing different kinds\nof prompts for this complex task, we show some\nexample prompts with varying levels of detail be-\nlow. For simplicity, we show examples of only\nsingle-turn question-style prompts where the sys-\ntem role is undefined. Other variations are left out\ndue to lack of space. We also assume that three\nreviewers have reviewed the manuscript and pro-\nvided their comments (R1, R2, R3) as the data for\nthe meta-review generation task.\n• Level 0 Prompt: <R1, R2, R3>\n• Level 1 Prompt: Prepare a meta-review by sum-\nmarizing the reviewer comments: <R1, R2, R3>\n• Level 2 Prompt: Prepare a meta-review\nby summarizing the following reviewer com-\nments. The final output should highlight the\ncore contributions of the manuscript, common\nstrengths/weaknesses mentioned by multiple re-\nviewers, suggestions for improvement, and miss-\ning references (if any). The review texts are pro-\nvided below: <R1, R2, R3>\n• Level 3 Prompt: Prepare a meta-review by\nanswering the following questions from the re-\nviewer comments (provided after the questions).\n1. Based on the reviewer’s comments, what are\nthe core contributions made by the authors?\n2. What are the common strengths of this work,\nas mentioned by multiple reviewers?\n3. What are the common weaknesses of this work,\nas highlighted by multiple reviewers?\n4. What suggestions would you provide for im-\nproving this paper?\n5. What are the missing references mentioned by\nthe individual reviews?\nThe review texts are below: <R1, R2, R3>\n• Level 4 Prompt: “Level 3 Prompt” + “A\ngood output should be coherent, highlight major\nstrengths/issues mentioned by multiple reviewers,\n14200\nbe less than 400 words in length, and finally, the\nresponse should be in English only”.\n• Level 5 Prompt: “Level 4 Prompt” + “Below\nare additional information relevant to your goal\ntask. <Information Fetched using Information\nRetrieval Techniques>”.\n• Level 6 Prompt: “Level 5 Prompt” + “Justify\nyour response in detail by explaining why you\nmade the choices you actually made”.\n4.2 Use Case 2: Narrative Braiding\nNarrative braiding, also known as “interweaving”\nor “multi-perspective storytelling” is a literary tech-\nnique that involves the parallel telling of multi-\nple storylines that eventually converge and inter-\nsect (Bancroft, 2018). This technique is often used\nin novels, short stories, films, and television shows\nto create a complex and engaging narrative.\nNarrative braiding is indeed a complex task to\nperform, even for humans, let alone computers, as\nit requires careful planning and execution to ensure\nthat each storyline is fully developed and that the\ndifferent strands of the narrative are balanced and\ncomplement each other. When done well, narrative\nbraiding can create a rich and engaging story that\nkeeps readers or viewers invested. From the recent\npromising results of language models in generating\nhigh-quality controlled text (Bansal et al., 2022a,b),\nit is quite intuitive to test the LLM’s performance\nin narrative braiding tasks.\nNow, we show how one can use the proposed\nTELeR taxonomy to categorize different types of\nprompts for the narrative braiding task. This time,\nwe show examples of only single-turn instruction-\nstyle prompts with system roles undefined. Other\nvariations are left out due to lack of space. We also\nassume two alternative narratives are available that\ndescribe the same event as our data for the braid-\ning task, and the goal is to create a final braided\nnarrative from the two input narratives, N1 and N2.\n• Level 0: <N1, N2>\n• Level 1: Braid a single coherent story from the\nfollowing alternative narratives: <N1, N2>\n• Level 2: Braid a single coherent story from the\nfollowing alternative narratives. The final nar-\nrative should highlight the common information\nprovided by both narratives, interesting, unique\ninformation provided by each individual narra-\ntive, and conflicting information (if any) con-\nveyed in these narratives. The input alternative\nnarratives are provided below: <N1, N2>\n• Level 3: Braid a single coherent story from the\nfollowing alternative narratives provided later\nby performing the following tasks.\n1. Extract overlapping clause pairs from both\nnarratives and paraphrase them.\n2. Extract unique clauses from each narrative\nand identify the interesting ones.\n3. Extract conflicting clause pairs conveyed in\nboth narratives and resolve the conflict.\n4. Generate paragraphs from overlapping-\nunique-conflicting clauses and merge them\ninto a single document.\n5. Reorder sentences of the merged document\ninto a detailed, coherent story.\n6. Summarize the detailed story into a concise,\nbraided narrative.\nThe alternative narratives are below: <N1, N2>\n• Level 4 Prompt: “Level 3 Prompt” + “A good\noutput should be coherent, highlight overlapping-\nunique-conflicting information provided by indi-\nvidual narratives, be less than 1000 words in\nlength, and in English language only”.\n• Level 5 Prompt: “Level 4 Prompt” + “Below\nare additional information relevant to your goal\ntask. <Information Fetched using Information\nRetrieval Techniques>”.\n• Level 6 Prompt: “Level 5 Prompt” + “Provide\njustification for your response in detail by ex-\nplaining why your response contains certain in-\nformation and discards other information of the\ninputs”.\n5 Final Words\nIn this paper, we emphasize the importance of a\nstandardized taxonomy for LLM prompts targeted\ntowards solving complex tasks and, subsequently,\npropose such a general taxonomy, i.e., TELeR,\nwhich can serve as a unified standard for compar-\ning and benchmarking LLMs’ performances re-\nported by multiple independent research studies.\nWe urge the community to use the TELeR taxon-\nomy for designing prompts in their future work and\nreport the specific categories of prompts they exper-\nimented with in their manuscripts. This will enable\nmore meaningful comparisons among LLMs and,\nthereby, help to derive more accurate conclusions\nfrom multiple independent studies. This, in turn,\nwill help the community to reach a consensus on\nstate-of-the-art LLM performances more accurately\nand faster than otherwise.\n14201\n6 Limitations\nThe proposed TeLER taxonomy is exclusively ap-\nplicable to LLM prompts targeted toward solving a\ncomplex task. This taxonomy, especially the seven\nlevels, does not apply to simple tasks. As such,\nTeLER taxonomy will be most useful to researchers\nand developers who are conducting applied LLM\nresearch and development that is focused on per-\nforming complex tasks.\nAlso, the TeLER taxonomy should not be con-\nsidered as an ultimate taxonomy for LLM prompts,\nand further extensions of this taxonomy are cer-\ntainly possible and actually desired. Having said\nthat, the TeLER taxonomy is actually very general\nand can be easily extended by adding more dimen-\nsions for categorization as deemed necessary by\nthe target application.\n7 Acknowledgements\nThis work has been partially supported by the Na-\ntional Science Foundation (NSF) Standard Grant\nAward #2302974 and Air Force Office of Scientific\nResearch Grant/Cooperative Agreement Award\n#FA9550-23-1-0426. We would also like to thank\nAuburn University College of Engineering and the\nDepartment of CSSE for their continuous support\nthrough Student Fellowships and Faculty Startup\nGrants.\nReferences\nCorinne Bancroft. 2018. The braided narrative. Narra-\ntive, 26(3):262–281.\nNaman Bansal, Mousumi Akter, and Shubhra Kanti Kar-\nmaker Santu. 2022a. Learning to generate overlap\nsummaries through noisy synthetic data. In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11765–11777,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nNaman Bansal, Mousumi Akter, and Shubhra Kanti\nKarmaker Santu. 2022b. SEM-f1: an automatic way\nfor semantic evaluation of multi-narrative overlap\nsummaries at scale. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 780–792, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nNaman Bansal, Mousumi Akter, and Shubhra Kanti\nKarmaker Santu. 2022c. Semantic overlap summa-\nrization among multiple alternative narratives: An\nexploratory study. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 6195–6207, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? In 2022 ACM Conference on\nFairness, Accountability, and Transparency, pages\n2280–2292.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui,\nand Furu Wei. 2022. Why can gpt learn in-context?\nlanguage models secretly perform gradient descent as\nmeta optimizers. arXiv preprint arXiv:2212.10559.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.\nGlam: Efficient scaling of language models with\nmixture-of-experts. In International Conference on\nMachine Learning, pages 5547–5569. PMLR.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. arXiv preprint\narXiv:2210.00720.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2022. Ptr: Prompt tuning with rules\nfor text classification. AI Open, 3:182–192.\nEllen Jiang, Edwin Toh, Alejandra Molina, Kristen Ol-\nson, Claire Kayacik, Aaron Donsbach, Carrie J Cai,\nand Michael Terry. 2022. Discovering the syntax and\nstrategies of natural language programming with gen-\nerative language models. In Proceedings of the 2022\nCHI Conference on Human Factors in Computing\nSystems, pages 1–19.\nShubhra Kanti Karmaker Santu, Chase Geigle, Dun-\ncan Ferguson, William Cope, Mary Kalantzis, Du-\nane Searsmith, and Chengxiang Zhai. 2018. Sofsat:\nTowards a setlike operator based framework for se-\nmantic analysis of text. ACM SIGKDD Explorations\nNewsletter, 20(2):21–30.\n14202\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\napproach for solving complex tasks. arXiv preprint\narXiv:2210.02406.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2023. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\nand patterns: For effective chain of thought, it takes\ntwo to tango. arXiv preprint arXiv:2209.07686.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. arXiv preprint arXiv:1906.02361.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nNan Shao, Zefan Cai, Chonghua Liao, Yanan Zheng,\nZhilin Yang, et al. 2023. Compositional task rep-\nresentations for large language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\nChenhui Shen, Liying Cheng, Yang You, and Li-\ndong Bing. 2023. A hierarchical encoding-decoding\nscheme for abstractive multi-document summariza-\ntion. arXiv preprint arXiv:2305.08503.\nChenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing,\nYang You, and Luo Si. 2022. Mred: A meta-\nreview dataset for structure-controllable text genera-\ntion. Findings of the Association for Computational\nLinguistics: ACL 2022, pages 2521–2535.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan\nHu, Yongrui Chen, and Guilin Qi. 2023. Evalu-\nation of chatgpt as a question answering system\nfor answering complex questions. arXiv preprint\narXiv:2303.07992.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,\nCarlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\nSpencer-Smith, and Douglas C Schmidt. 2023. A\nprompt pattern catalog to enhance prompt engineer-\ning with chatgpt. arXiv preprint arXiv:2302.11382.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. Star: Bootstrapping reasoning with rea-\nsoning. Advances in Neural Information Processing\nSystems, 35:15476–15488.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\n14203",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.9277096390724182
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.8190025687217712
    },
    {
      "name": "Computer science",
      "score": 0.7128589749336243
    },
    {
      "name": "Task (project management)",
      "score": 0.5753186941146851
    },
    {
      "name": "Data science",
      "score": 0.4417518079280853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39011386036872864
    },
    {
      "name": "Management science",
      "score": 0.34302443265914917
    },
    {
      "name": "Natural language processing",
      "score": 0.32560116052627563
    },
    {
      "name": "Systems engineering",
      "score": 0.10773387551307678
    },
    {
      "name": "Engineering",
      "score": 0.1008385419845581
    },
    {
      "name": "Ecology",
      "score": 0.0742708146572113
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ]
}