{
  "title": "Probing Image-Language Transformers for Verb Understanding",
  "url": "https://openalex.org/W3177487519",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2115890459",
      "name": "Lisa Anne Hendricks",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117710609",
      "name": "Aida Nematzadeh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3126442315",
    "https://openalex.org/W2962707719",
    "https://openalex.org/W2479423890",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2963644680",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W2214124602",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1551928752",
    "https://openalex.org/W2963668159",
    "https://openalex.org/W3117585461",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2774267535",
    "https://openalex.org/W3034381157",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W3016672431",
    "https://openalex.org/W4287813958",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2463565445",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3184369217",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2883512601",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W3104379732",
    "https://openalex.org/W2423576022"
  ],
  "abstract": "Multimodal image-language transformers have achieved impressive results on a variety of tasks that rely on fine-tuning (e.g., visual question answering and image retrieval).We are interested in shedding light on the quality of their pretrained representationsin particular, if these models can distinguish different types of verbs or if they rely solely on nouns in a given sentence.To do so, we collect a dataset of image-sentence pairs (in English) consisting of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset).We use this dataset to evaluate pretrained image-language transformers and find that they fail more in situations that require verb understanding compared to other parts of speech.We also investigate what category of verbs are particularly challenging.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3635–3644\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3635\nProbing Image–Language Transformers for Verb Understanding\nLisa Anne Hendricks Aida Nematzadeh\nDeepMind\n{lmh, nematzadeh} @google.com\nAbstract\nMultimodal image–language transformers\nhave achieved impressive results on a variety\nof tasks that rely on ﬁne-tuning ( e.g., visual\nquestion answering and image retrieval).\nWe are interested in shedding light on the\nquality of their pretrained representations –\nin particular, if these models can distinguish\ndifferent types of verbs or if they rely solely\non nouns in a given sentence. To do so, we\ncollect a dataset of image–sentence pairs (in\nEnglish) consisting of 421 verbs that are either\nvisual or commonly found in the pretraining\ndata ( i.e., the Conceptual Captions dataset).\nWe use this dataset to evaluate pretrained\nimage–language transformers and ﬁnd that\nthey fail more in situations that require verb\nunderstanding compared to other parts of\nspeech. We also investigate what category of\nverbs are particularly challenging.\n1 Evaluating Verb Understanding\nThe success of image–language models in real-\nworld applications relies on their ability to relate\ndifferent aspects of language (such as verbs or ob-\njects) to images, which we refer to as multimodal\nunderstanding. For example, an image-retrieval\nmodel needs to distinguish between “eating an ap-\nple” and “cutting an apple” and a captioning model\nmust accurately describe the actions in a scene.\nPrevious work shows that image–language\nbenchmarks do not always fully measure such mul-\ntimodal understanding: object retrieval models fail\nto account for linguistic structure (Akula et al.,\n2020), visual question answering (VQA) models\noverly rely on language priors (Goyal et al., 2017;\nAgrawal et al., 2018), and captioning metrics do not\nalways measure if captions “hallucinate” objects\nin an image (Rohrbach et al., 2018). Inspired by\nthis, prior work introduced tasks to speciﬁcally ex-\namine whether models can relate objects to images\n(Shekhar et al., 2017) or classify frequent interac-\ntions associated with objects (Chao et al., 2015).\nHowever, both these datasets are limited to the 80\nobjects in the MSCOCO detection challenge (Lin\net al., 2014).\nTo address this gap, we design a benchmark fo-\ncused on verbs called SVO-Probes for examining\nsubject, verb, object triplets; more speciﬁcally, we\ncollect a set of image–sentence pairs (in English)\nwhere each pair is annotated with whether the sen-\ntence corresponds to the image or not. As shown\nin Fig. 1, for a given sentence, in addition to a pos-\nitive image that matches the sentence, our dataset\nincludes controlled negative images that do not cor-\nrespond to speciﬁc aspects of the sentence ( i.e.,\nsubject, verb, and object). These controlled ex-\namples enable us to probe models for their under-\nstanding of verbs as well as subjects and objects.\nOur dataset consists of 421 verbs and includes over\n48, 000 image–sentence pairs.\nWe use our benchmark to evaluate the recent\nfamily of multimodal (image–language) transform-\ners that have shown impressive results on bench-\nmarks like VQA and image retrieval (Lu et al.,\n2019; Chen et al., 2020; Tan and Bansal, 2019;\nLi et al., 2020b,a; Huang et al., 2020). Our goal\nis to investigate if the good performance of these\nmodels is due to learned representations that suc-\ncessfully relate different aspects of language to\nimages. More speciﬁcally, we evaluate a few archi-\ntectural variations of these models in a zero-shot\nway by using the pretrained models to classify if\nimage–sentence pairs from SVO-Probes match.\nOur results show that the performance of all eval-\nuated models is worst on verbs, with subjects being\neasier than verbs but harder than objects. We ﬁnd\nthat this observation does not depend on the fre-\nquency of test examples in pretraining data. More-\nover, it is considerably harder for all models to\ncorrectly classify image–sentence pairs that do not\n3636\nA woman jogs on the beach.\nA man is jumping into the sea.A person sings at a concert.\nA man jumping into a river.A animal lays in the grass.\nChildren cross the street.\nchild, cross, street lady, cross, street\nanimal, lay, grass woman, lay, grass\nperson, sing, concert person, dance, concert\nman, jump, river man, kayak, river\nman, jump, sea man, jump, mountain\nwoman, jog, beach woman, jog, forest\nPos Neg\nPos\nPos\nPos Pos\nPos\nNeg Neg\nNeg Neg\nNeg\nFigure 1: Examples from SVO-Probes. Images on the left and right show positive and negative image examples\nfor each sentence. Below each image is the ⟨subject, verb, object⟩ triplet corresponding to the image.\nmatch; the image–language transformers overpre-\ndict that sentences corresponds to images.\nAdditionally, we compare an image–language\ntransformer pretrained on a large automatically-\ncurated dataset (i.e., Conceptual Captions, Sharma\net al., 2018) with one pretrained on the smaller but\nmanually-annotated MSCOCO (Chen et al., 2015).\nConceptual Captions is more noisy than MSCOCO\nin that its sentences do not necessarily correspond\nto its images. Interestingly, we observe that the\nmodel pretrained on MSCOCO performs better.\nThis result shows that the image–language trans-\nformers are not robust to dataset noise as they learn\nto predict that somewhat-related image–sentence\npairs correspond to each other.\nDespite their good performance on downstream\ntasks, image–language transformers fail on our\ntask that requires multimodal understanding since\nthey cannot distinguish between ﬁner-grained dif-\nferences between images. Our results highlight\nthat there is still considerable progress to be made\nwhen training multimodal representations, and that\nverbs in particular are an interesting challenge in\nimage–language representation learning.\n2 Related Work\nImage–language transformers build on the trans-\nformer architecture (Vaswani et al., 2017) by in-\ncorporating additional loss functions (to learn im-\nage features and align image and language modal-\nities), using self-attention to combine modalities,\nand training on paired image–text data (Lu et al.,\n2019; Chen et al., 2020; Tan and Bansal, 2019; Li\net al., 2020b,a; Huang et al., 2020). The impres-\nsive performance of these models on many image–\nlanguage benchmarks has inspired recent work that\nstudies different architectural choices made in these\nmodels (Cao et al., 2020; Hendricks et al., 2021).\nCompared to previous image–language models,\nmultimodal transformers both use a new architec-\nture and are frequently trained on a much larger\ndataset – the Conceptual Captions dataset consist-\ning of 3m image–text pairs (Sharma et al., 2018).\nSingh et al. (2020) show that on ﬁne-tuned tasks,\nthe performance of multimodal transformers (i.e.,\nLu et al., 2019; Li et al., 2019) are less sensitive to\ndataset size; the domain match between pretraining\nand ﬁne-tuning datasets is more important.\nDatasets. Our proposed dataset is most similar to\nthe FOIL benchmark (Shekhar et al., 2017) which\ntests if image–language models can differentiate\nbetween sentences that vary with respect to only\none noun. FOIL consists of 64, 300 images from\nMSCOCO (Chen et al., 2015); each image is paired\nwith a corresponding sentence that describes the\nimage (i.e., a positive example) and one that does\nnot (i.e., a negative example). Negative sentences\nare collected by replacing object words in the posi-\ntive sentences with a similar object (e.g., changing\nthe word “dog” to “cat” in “The dog ran.”). Shekhar\net al. (2017) use the FOIL dataset in a few tasks\nincluding a classiﬁcation task where the model is\nasked to classify if a sentence matches the image or\nnot. We use the same task setup because it allows\nus to probe image–language transformers in a zero-\nshot setting as these models are generally trained\nto classify whether an image–text pair match. Our\nwork is different than FOIL in that we focus on verb\nunderstanding as opposed to noun understanding;\nmoreover, our dataset provides different negative\ntypes (by replacing subjects, verbs, or objects).\nOther datasets focus on relationship or interac-\ntion detection (e.g., HICO and VRD; Chao et al.,\n2015; Lu et al., 2016a). These datasets are evalu-\n3637\ndataset Ims Subjs Verbs Objs Sents Negs\nFOIL 32k n/a 0 70 \u0013 \u0013\nHICO 10k n/a 117 80 \u0017 \u0013\nVRD 1k 100 70 100 \u0017 \u0017\nV-COCO 5k n/a 26 48 \u0017 \u0017\nImSitu 25k 950 504 1840 \u0017 \u0017\nSVO-Probes 14k 100 421 275 \u0013 \u0013\nTable 1: Images, Subjects, Verbs, Objects, Sentences, and\nNegatives in other datasets and SVO-Probes. Image numbers\nare for the evaluation set.\nated in a classiﬁcation setting in which the input is\nan image and the output is a detected relationship\n(for HICO, an object and interaction, for VRD two\nobjects and their relationship) and have a limited\nnumber of verbs and objects. V-COCO (Gupta and\nMalik, 2015) and ImSitu (Yatskar et al., 2016) both\nincludes verbs but do not provide negatives for a\ncontrolled evaluation of verb (or noun) understand-\ning. Finally, other work has explored how creating\nhard negatives (e.g., by substituting words in train\nexamples) leads to better test performance (Gupta\net al., 2020; Hendricks et al., 2018; Faghri et al.,\n2017). In contrast, our work focuses on creating\nhard evaluation examples to probe learned repre-\nsentations.\nIn summary, SVO-Probes is unique as it tests\nunderstanding of a broad range of verbs as well as\nsubjects and objects in a controlled way. Further-\nmore, our dataset includes image–sentence pairs;\nthus, it can be used to evaluate image–language\ntransformers that process image–sentence pairs. Fi-\nnally, SVO-Probes is designed as a zero-shot task\nto evaluate pretrained image–language transform-\ners and is collected to have a similar distribution to\nConceptual Captions which is commonly used in\npretraining these models. See Table 1 for a com-\nparison between SVO-Probes and other datasets.\n3 Task Setup and Dataset Collection\nOur goal is to examine verb-understanding in pre-\ntrained multimodal transformers. To do so, we\nneed a task that requires an understanding of a\ngiven verb in a sentence, e.g., a model cannot suc-\nceed at the task by relying on nouns. We also need\nto include a diverse set of verbs, and examine each\nverb in at least a few situations. To test the pre-\ntrained representations, we need to examine the\nmodels in a zero-shot setting (without ﬁne-tuning).\nInspired by the FOIL setup (Shekhar et al., 2017),\nwe use a zero-shot classiﬁcation task where a model\nis asked to identify if a sentence and an image corre-\nspond to each other. As a result, we need a dataset\nthat provides “match” or “not match” labels be-\ntween images and sentences. We collect a dataset\nof image–sentence pairs (SVO-Probes) that given\na sentence, provides such labels for at least two\nimages.1 Some of these images are positive ex-\namples, i.e., the sentence correctly describes them.\nOthers are negative examples where some aspect of\nthe sentence (e.g., verb) does not match the image.\nFigure 1 shows some examples from our dataset.\nWe systematically collect negative examples\nsuch that they only differ from the positive image\nwith respect to the subject, verb, or object of the\nsentence. Finally, we consider sentences whose\nsubjects, verbs, and objects are frequent in the Con-\nceptual Captions (CC) training dataset. Since CC\nis the dataset most frequently used for pretraining\nmultimodal transformers, we can examine what the\npretrained representations capture (in contrast to ex-\namining these models’ generalization ability). We\nnext describe our pipeline to create SVO-Probes.\nCreating a verb list. To ensure that we have a\nlarge number of verbs in our dataset, we ﬁrst cre-\nated a verb list by considering a subset of verbs\nthat occur in the train split of the Conceptual Cap-\ntions dataset (CC-train). More speciﬁcally, we con-\nsider verbs that are visually recognizable in the\nimages; to identify the visual verbs, we use the\nimSitu dataset (Yatskar et al., 2016) that includes\nverbs that annotators marked as reliably recogniz-\nable. Moreover, we include verbs that occur at least\n50 times in CC-train.\nCurating triplets. Given a positive example, we\nneed to systematically generate negatives by re-\nplacing the subject, verb, or the object. As a result,\nwe collect a set of ⟨subject, verb, object ⟩ (SVO)\ntriplets from CC-train sentences for our verbs. We\nextract the subject, verb, and direct object from the\ndependency parse trees. and remove triplets where\nsubjects or objects are pronouns or have less than\ntwo characters. Finally, we discard SVO triplets\nwith frequency smaller than ﬁve.\nWe consider three negative types for a given\ntriplet: a subject-, verb-, or object-negative where\nrespectively, the subject, verb, or object in the\ntriplet are replaced by a different word. For ex-\nample, given the triplet ⟨girl, lie, grass ⟩, exam-\nples of subject-negative, verb-negative, and object-\n1We note that our dataset is limited to English sentences;\nwe simply use “sentences” to refer to English sentences.\n3638\nnegative are ⟨puppy, lie, grass⟩, ⟨girl, sit, grass ⟩,\nand ⟨girl, lie, beach⟩.\nSince our goal is to examine verb understanding,\nwe only keep the triplets that have at least one verb\nnegative. This enables us to evaluate a model’s\ncapacity in distinguishing images that mainly differ\nwith respect to the verb; for example, ⟨girl, lie,\ngrass⟩ vs. ⟨girl, sit, grass⟩. Adding this constraint\nresults in 11230 SVO triplets and 421 verbs. In this\nset, 1840 SVO triplets (and 53 verbs) have at least\ntwo verb and object negatives.\nCollecting images. The next step is collecting\nimages that match the curated SVO triplets. We\nquery for SVO triplets using the Google Image\nSearch API. We retrieve 5 images for each triplet,\nthen remove any images with urls in Conceptual\nCaptions. To make sure that these automatically-\nretrieved images certainly match the triplets, we\nset up an annotation task where we ask workers on\nAmazon Mechanical Turk (AMT) to verify if the\nsubject, verb, and object are present in the image.\nWe ask three people to annotate each image, and\nonly keep images where at least two annotators\nagree that the subject, verb, and object are depicted\nin the image. Moreover, we discard images marked\nas a cartoon by annotators. We ﬁnd that 58% of\nour images pass this initial annotation process. We\npay workers $0.04 per HIT for all tasks.\nCollecting sentences. Multimodal transformer\nmodels are trained on pairs of images and sen-\ntences; to evaluate them, we require image–\nsentence pairs as opposed to image–SVO pairs.\nGiven an image and an SVO triplet, we next ask an-\nnotators to write a sentence that uses all the words\nin the triplet and describes the image. For example,\nas shown in Figure 1 top right, given the triplet\n⟨man, jump, sea⟩, an annotator might write “A man\nis jumping into the sea.”. We ask annotators to\nrefrain from writing additional information to en-\nsure that a collected sentence examines the words\nin the SVO (as opposed to words that we are not\ncontrolling for). Annotators are given the option to\nnot write a sentence if they do not think the subject,\nverb, and object can be combined into a grammati-\ncal sentence that describes the image. 86% of our\nimages pass this phase of our pipeline.\nWe observe that for a given SVO, different im-\nages elicit slightly different sentences. For example,\nthe triplet ⟨person, jog, beach⟩ resulted in the sen-\ntences “A person jogging along the beach.” and “A\nperson jogs at the beach.”. Additionally, annotators\npluralize nouns to ensure the sentence describes\nthe image (e.g., Figure 1 top left, the subject “child”\nis written as “children” in the sentence).\nConﬁrming the negative image. Finally, given\na positive triplet (e.g., ⟨girl, lie, grass⟩) and its nega-\ntive (e.g., ⟨girl, sit, grass⟩), we need to conﬁrm that\nthe positive’s sentence does not match the image\nretrieved for the negative triplet. To do so, we ask\nthree annotators to select which images (positive,\nnegative, neither, or both) match a given sentence.\nImage–sentence pairs where two out of three anno-\ntators agree are accepted into our dataset; 68% of\nthe pairs pass this ﬁnal annotation stage.\n4 Experimental Setup and Results\nWe investigate if current image–language trans-\nformers can relate different aspects of language\n(and in particular verbs) to images by evaluating\nthese models against both FOIL and SVO-Probes.\nMore speciﬁcally, we evaluate a few architectural\nvariations of image–language transformer models\n(based on the implementation of the models by\nHendricks et al., 2021) that differ in their choice\nof multimodal attention and loss functions; this\nway we can examine whether our ﬁndings are sen-\nsitive to these slight differences. The base multi-\nmodal transformer (MMT) closely replicates the\nViLBERT architecture (Lu et al., 2019): this model\nincludes three loss functions, masked language\nmodeling (MLM) and masked region modeling\n(MRM) losses on the language and image inputs\nand an image–text matching (ITM) loss that classi-\nﬁes if an image–sentence pair match. Importantly,\nthe multimodal attention of MMT is similar to the\nhierarchical co-attention in Lu et al. (2016b) where\neach modality ( i.e., image or language) attends\nonly to the other modality. More speciﬁcally, in\nthe multimodal self-attention layer of transformer\n(Vaswani et al., 2017), for queries on the language\ninput, keys and values are taken from images and\nvice versa.\nDifferent interactions of image (language)\nqueries, keys, and values in multimodal self-\nattention results in variations of image–language\ntransformers. We describe the model variations we\nstudy in Table 2. We also consider models that\neither lack the MLM or MRM loss. Models are pre-\ntrained on Conceptual Captions (CC) unless stated\notherwise. For reference, we report the Recall@1\nperformance on the zero-shot image-retrieval task\n3639\nName Multimodal Attention Similar Model MLM MRM ZS Flickr\nMMT Queries from L (I) take values and keys fromonlyI (L) ViLBERT; LXMERT \u0013 \u0013 41.9\nMerged–MMT Queries from L (I) take values and keys frombothL and I UNITER \u0013 \u0013 40.0\nLang–MMT Queries areonlyfrom L (Hendricks et al., 2021) \u0013 \u0013 33.6\nImage–MMT Queries areonlyfrom I (Hendricks et al., 2021) \u0013 \u0013 31.6\nSMT Single-Modality Transformers without multimodal attention \u0013 \u0013 16.9\nNo-MRM–MMT The same as MMT \u0013 \u0017 41.1\nNo-MLM–MMT The same as MMT \u0017 \u0013 20.2\nTable 2: Different variants of the image–language transformer architecture we test. L and I stand for language and\nimage, respectively. We note that models with Merged attention (like UNITER) are also referred to as single-stream\nmodels. ViLBERT: Lu et al. (2019); LXMERT: Tan and Bansal (2019); UNITER: Chen et al. (2020)\non Flickr (ZS Flickr), where a model must retrieve\nan image from the Flickr dataset (Young et al.,\n2014) that matches an input sentence. Since MMT\nperforms best on ZS Flickr, we do most of our\nexperiments on this model unless stated otherwise.\nWe ﬁrst evaluate our image–language transform-\ners on FOIL to examine their noun understanding\nand then test them on SVO-Probes which probes for\nsubject, verb, and object understanding in learned\nrepresentations. Following FOIL, we report the\naccuracy on positive and negative pairs. All our\nmodels have an image-text classiﬁcation output\nused in pretraining to align images and sentences.\nWe calculate accuracy by passing images through\nour models and labeling an image–sentence pair\nas negative if the classiﬁer output is < 0.5 and\npositive otherwise. We report the average over the\ntwo pairs (see Avg columns in Tables 3 and 4) by\nweighting them equally, since we expect models to\nperform well on both positive and negative pairs. In\nFOIL, there are equal positive and negative pairs.\nAnother possible way to set-up our evaluations is\nas image-retrieval (reporting recall@1 as a metric).\nHowever, the retrieval setting does not highlight\nthe difference in performance between positive and\nnegative pairs. For example, a model might rank\nthe pairs correctly even when their scores are very\nclose (positive score is 0.91 and negative one is\n0.9). In this example, the model is wrong about\nthe negative pair (it is assigned a high score) but\nthe retrieval setting does not capture this. However,\nthe classiﬁcation metric will penalize the model\nfor assigning a high score to a negative pair. As a\nresult, the classiﬁcation metric better differentiates\nbetween the models by examining if they correctly\nlabel both the positive and negative pairs.\n4.1 Evaluating Nouns with FOIL\nWe examine noun understanding in image–\nlanguage transformers with the FOIL dataset\n(Shekhar et al., 2017). Given image–sentence pairs\nfrom FOIL, we evaluate the MMT model in a zero-\nshot setting by using it to classify if the image\nand sentence match. Table 3 compares MMT with\nthe best model from the FOIL paper (HieCoAtt\nShekhar et al., 2017) and, to our knowledge, the\nbest-performing model on the task without using\nground-truth annotations (Freq+MM-LSTM from\nMadhyastha et al., 2018). Note that these models\nare trained speciﬁcally for the FOIL task (i.e., on\nthe train split of FOIL), whereas the MMT model\n(pretrained on CC) is tested in a zero-shot setting.\nMMT achieves an accuracy considerably worse\nthan the best models on FOIL (Shekhar et al., 2017;\nMadhyastha et al., 2018) on all pairs; this is sur-\nprising given that image–language transformers\nachieve state-of-the-art results on zero-shot image\nretrieval tasks based on Flickr (Young et al., 2014)\nand MSCOCO (Chen et al., 2015). In particu-\nlar, MMT overpredicts that image–sentence pairs\nmatch, resulting in the highest accuracy on the pos-\nitive pairs (99.0) but the lowest on negative pairs\n(11.8). Thus MMT cannot distinguish between\nsentences that only differ with respect to nouns.\nWe investigate whether this poor performance of\nMMT is due to mismatch between the pretraining\n(i.e., CC) and FOIL test (i.e., MSCOCO) datasets.\nThus, we compare our MMT model pretrained\non Conceptual Captions with one pretrained on\nMSCOCO (MMT-COCO). As expected, MMT-\nCOCO has considerably higher performance on\nall pairs (compare to MMT); however, the accu-\nracy is still signiﬁcantly higher on positive pairs\nthan negative ones, showing that the model overpre-\ndicts that image–sentence pairs match. Our result\nshows that despite their impressive performance\non downstream tasks, image–language transformer\nmodels perform poorly in distinguishing between\nsemantically similar sentences. Next we examine\nhow well these models perform on our proposed\n3640\nModel Avg Pos. Neg.\nHieCoAtt* 64.1 91.9 36.4\nFreq + MM-LSTM † 87.9 86.7 89.0\nMMT 55.4 99.0 11.8\nMMT-COCO 72.0 95.0 49.0\nTable 3: Performance on FOIL averaged over all (Avg),\npositive (Pos.), and negative (Neg.) pairs. *Shekhar\net al. (2017); †Madhyastha et al. (2018).\nprobing dataset which is designed to have a similar\nvocabulary to the CC pretraining dataset.\n4.2 Comparing Models on SVO-Probes\nWe evaluate all models (see Table 2) on SVO-\nProbes and report overall accuracy and accuracy\nfor subject, verb, and object negatives in Table 4.\nThe MMT model (with the best performance on\nZS Flickr) performs poorly on SVO-Probes, achiev-\ning an overall average accuracy of 64.3. The best\noverall average accuracy (No-MRM–MMT; 69.5)\nshows that SVO-Probes is challenging for image–\nlanguage transformers. In particular, models strug-\ngle with classifying negative pairs; Lang–MMT\nachieves the highest accuracy over negative pairs\n(56) which is slightly higher than chance at 50. 2\nThough No-MRM–MMT and MMT perform\nsimilarly on ZS Flickr, No-MRM–MMT performs\nbetter on SVO-Probes. This suggests that the\nmasked region modelling loss is not needed for\ngood performance onZS Flickr; also, it impedes the\nmodel from learning ﬁne-grained representations\nneeded to perform well on SVO-Probes. More sur-\nprisingly, Lang–MMT, which performs worse on\nZS Flickr than MMT, outperforms MMT on SVO-\nProbes. The image representations in Lang–MMT\nare not updated with an attention mechanism. In\nSec. 4.4, we explore if the stronger attention mech-\nanism in MMT leads to overﬁtting of the training\nimages and thus weaker performance.\nWe crafted SVO-Probes such that it includes\nwords from the pretraining dataset of image–\nlanguage transformers ( i.e., CC), whereas FOIL\nis collected from MSCOCO. Comparing the per-\nformance of MMT (with CC pretraining) on FOIL\n2We focus on image–language transformers, but we also\ntested a baseline model where image features are embedded\nwith the detector used in our transformers and language fea-\ntures with BERT. Features are pooled using element-wise\nmultiplication. This baseline achieves 66.3% accuracy overall\nwith 75.4% and 57.3% accuracy on positives and negatives.\nSimilar to transformers, performance on verbs is the worst.\nand SVO-Probes (55.4 in Table 3 vs. 64.3 in Ta-\nble 4), we see that the domain mismatch between\npretraining and test data plays a role in MMT’s\nperformance. Interestingly, comparing the perfor-\nmance of MMT-COCO (MMT with COCO pre-\ntraining) on FOIL to MMT (with CC pretraining)\non SVO-Probes, we ﬁnd that SVO-Probes is more\nchallenging than FOIL when there is no domain\nmismatch (72.0 in Table 3 vs. 64.3 in Table 4).\nWhen comparing different negative types across\nall models, we observe that verbs are harder than\nsubjects and objects; compare average accuracy for\nSubj., Verb, and Obj. Negative columns in Table 4.\nFor example, in MMT, the subject and object neg-\native average accuracies (67.0 and 73.4) are con-\nsiderably higher than the average accuracy for verb\nnegatives (60.8). Moreover, when breaking down\nthe accuracies for positive and negative pairs (Pos.\nand Neg. columns in Table 4), we observe that\nthe accuracies of positive pairs are similar (ranging\nbetween 80.2 and 94.4) across all models except\nSMT (which performs close to chance); however,\nfor negative pairs, there is more variation in accu-\nracy across models especially for verb negatives\n(ranging between 22.4 and 54.6, Neg. columns\nunder “Verb Negative”). These results show that\nnegative pairs are better than positive ones in dis-\ntinguishing between different model architectures.\nWe also ﬁnd that subjects are harder than objects\nacross all models (when comparing average accu-\nracies of subject and object negatives). To better\nunderstand this result, we examined 21 nouns that\noccur both as subjects and objects in SVO-Probes’\nsentences. Interestingly, over these 21 nouns, for\nour MMT model, the accuracies of negative pairs\nare 42.9 and 56.4 for subject and object negatives,\nrespectively. This suggests that the subject posi-\ntion might be more challenging than the object one\nwhich we further explore in Sec. 4.3.\n4.3 Accuracy and Frequency at Training\nOur overall results on SVO-Probes (Table 4) show\nthat for image–language transformers, verb nega-\ntives are more challenging than subject and object\nones, and also subject negatives are harder than\nobject ones. We examine if this observation is due\nto properties of SVO-Probes as opposed to differ-\nences speciﬁc to subjects, verbs, and objects. First,\nwe explore whether the frequency of SVO triplets\nin pretraining data impacts the accuracy of negative\npairs in our MMT model. We focus on negative\n3641\nOverall Subj. Negative Verb Negative Obj. Negative\nAvg Pos. Neg. Avg Pos. Neg. Avg Pos. Neg. Avg Pos. Neg.\n# Examples 48k 12k 36k 8k 3k 5k 34k 11k 23k 11k 3k 8k\nMMT 64.3 93.8 34.8 67.0 94.4 39.5 60.8 93.8 27.8 73.4 94.4 52.4\nMerged–MMT 64.7 94.4 35.0 69.1 94.9 43.2 60.7 94.4 27.0 74.1 94.9 53.3\nLang–MMT 68.1 80.2 56.0 71.5 82.1 60.9 64.5 80.2 48.9 77.7 81.4 74.1\nImage–MMT 64.3 91.6 37.0 68.2 92.1 44.2 59.7 91.6 27.8 75.6 91.5 59.6\nSMT 52.4 49.1 55.6 52.6 47.7 57.5 51.8 49.1 54.6 53.9 50.7 57.0\nNo-MRM–MMT 69.5 85.4 53.7 73.5 87.4 59.7 65.5 85.6 45.5 80.1 86.2 74.1\nNo-MLM–MMT 60.8 92.3 29.3 64.8 93.9 35.8 57.4 92.5 22.4 69.5 93.6 45.5\nTable 4: Results on SVO-Probes on different models for subject, verb, and object negatives. Best results are shown\nin bold; second best results are italicized.\nFigure 2: Accuracy of negative pairs for subject, verb,\nand object negatives given SVO frequencies in CC.\npairs as there is more variation in negative-pair ac-\ncuracies across both models as well as subject, verb,\nand object negatives. We consider the frequency of\npositive and negative SVO triplets: a positive SVO\ncorresponds to a positive image matching a given\nsentence, but a negative SVO and its extracted neg-\native image do not match the sentence.\nWe group SVOs based on their frequency in CC-\ntrain into low (less than 10), medium (between 10-\n200), and high (greater than 200) frequency bins.\nFig. 2 plots the negative-pair accuracy for subject,\nverb, and objects across these different frequency\nbins over positive and negative SVO frequencies.\nWe conﬁrm that our result on the difﬁculty of neg-\native types does not depend on the frequency of\npositive or negative SVOs in pretraining data. In\nboth plots of Fig. 2, the negative types in order of\ndifﬁculty (lower accuracy) are verbs, subjects, and\nobjects independent of the frequency bin.\nSimilarity between SVOs. We examine if the\nsimilarity between the SVO triplets corresponding\nto the negative and positive images can explain\nthe difference in performance of subject, verb, and\nobject negatives. For example, we expect that dis-\ntinguishing ⟨child, cross, street⟩ and ⟨adult, cross,\nstreet⟩ to be harder than differentiating one of them\nfrom ⟨dog, cross, street⟩: “child” and “adult” are\nmore similar to each other than to “dog”. To test\nthis, we measure the similarity between subjects,\nverbs, and objects in their corresponding negative\ntypes using the cosine similarity between word2vec\n(Mikolov et al., 2013) embeddings.\nThe average similarities between subjects, verbs,\nand objects are 0.49, 0.29, 0.27, respectively. Thus,\nsubject words in negative examples tend to be more\nsimilar than object words. Furthermore, we ﬁnd\nthat there is a small positive correlation (as mea-\nsured by Spearman rank correlation) between SVO\nsimilarity and classiﬁer scores for negative pairs –\n.264 and .277 for subjects and objects respectively\n– suggesting that when SVOs corresponding to the\nimage and sentence are similar, the classiﬁer tends\nto assign a higher score (more positive) to the pair.\nThis partially explains why accuracy on subjects\nis lower than on objects in Table 4. Even though\nverb negatives are harder for our model, the simi-\nlarity for verb negatives is similar to that of object\nnegatives. The correlation coefﬁcient between simi-\nlarity and classiﬁer score is weaker (.145) for verbs,\nsuggesting that word similarity factors less in how\nwell the model classiﬁes verb negatives.\n4.4 Similarity to Pretraining Data\nWe next consider the similarity between images\nin SVO-Probes and CC. To measure the similarity\nbetween images, we sample 1 million images from\nCC. For each image in SVO-Probes, we ﬁnd the 10\nnearest neighbors in the feature embedding space\n3642\nFigure 3: Comparing negative scores on MMT and\nLang–MMT for images less or more similar to CC.\nof CC, and average the distance to compute a simi-\nlarity score for the image. Figure 3 plots the aver-\nage score from our classiﬁer for negative pairs with\nimages that are less or more similar to the pretrain-\ning data (since we are classifying negative pairs,\nthe lower score the better). We compare the MMT\nand Lang–MMT models since they have consider-\nably different performance on SVO-Probes and ZS\nFlickr. The difference in average scores between\nless similar and more similar examples for MMT is\n0.083. This is noticeably greater than the difference\nin average scores between less and more similar\nexamples for Lang–MMT (0.024), suggesting that\nthe image similarity inﬂuences Lang–MMT less\nthan MMT. One hypothesis is that the stronger at-\ntention mechanism in MMT overﬁts to the training\nimages which makes the MMT model less robust.\n4.5 The Choice of Pretraining Dataset\nIn Sec. 4.2, we observe that models perform particu-\nlarly poorly in classifying negative pairs. We inves-\ntigate whether the choice of pretraining dataset im-\npacts this observation. Conceptual Captions (CC),\nthe most-common pretraining dataset for image–\nlanguage transformers, is curated by scraping im-\nages and alt-text captions from the web. As a re-\nsult, compared to manually-annotated datasets such\nas MSCOCO, CC is noisy – it contains examples\nwhere the sentence and its corresponding image\ndo not completely align. For example, a sentence\ncan mention objects that are not in the image or, in\nextreme cases, does not describe the image at all.\nWe hypothesize that image–language transform-\ners treat correspondences due to dataset noise as\n“real” relations; in other words, they learn that if\na image–sentence pair is somewhat semantically\nrelated, it should be classiﬁed as a positive match,\neven if some aspects of the sentence do not de-\nscribe the image. At the same time, we can think of\nnegatives in SVO-Probes as examples with noisy\nOverall Neg. Acc.\nTrain Avg. Pos. Neg. S V O\nCC 64.3 93.8 34.8 39.5 27.8 52.4\nCOCO 68.0 75.2 60.9 66.0 55.5 73.4\nTable 5: Comparing performance when training our\nMMT model on COCO and CC.\ncorrespondences where a speciﬁc aspect of a sen-\ntence (e.g., the verb) does not match the image. We\ncompare our MMT model (with CC pretraining) to\none trained on a manually-annotated and less noisy\ndataset, MSCOCO (referred to as MMT-COCO).\nTable 5 reports the overall accuracy of the\ntwo models on SVO-Probes as well as a break-\ndown over subject, verb, and object negatives for\nnegative-pair accuracies. MMT-COCO performs\nbetter than MMT pretrained on CC (avg. accuracy\nof 68 vs 64.3). This is surprising since MMT-\nCOCO has a different image and language distribu-\ntion in its pretraining dataset. The accuracy of pos-\nitive pairs in MMT-COCO is considerably lower\nthan MMT while it performs noticeably better for\nnegative pairs: unlike MMT, the MMT-COCO\nmodel does not overpredict that image–sentence\npairs match. Our results show the image–language\ntransformers are not robust to dataset noise. Less-\nnoisy datasets (such as MSCOCO), despite their\nsmall size and domain mismatch, are more suitable\nfor learning representations that are sensitive to\nﬁner-grained differences in images. Alternatively,\nmodels which are more robust to noise in datasets\ncould be beneﬁcial for tasks like ours.\n4.6 Which Verbs Are the Hardest?\nWe investigate which verbs are hardest for MMT.\nWe consider verbs with many examples in SVO-\nProbes: we keep SVO triplets with at least 30 nega-\ntive images, resulting in a set of 147 verbs and 887\nSVO triplets across 4, 843 images. Table 6 lists the\neasiest and hardest verbs (with highest and lowest\naccuracy for negative pairs) for the MMT model.\nEasy and hard verbs have a diverse set of properties;\nfor example, easy verbs include sporting activities\nlike “tackle” as well as verbs like “lead” that occurs\nin a variety of contexts. We also examine the 20\nmost difﬁcult and easiest verbs for all our models\n(described in Table 2). Most difﬁcult verbs for all\nmodels include: “cut”, “argue”, and“break” and the\neasiest ones include: “direct”, “battle”, “surround”,\n“skate”, and “participate”.\n3643\nEasy Hard\ntackle, reach, arrive, pitch, argue, beat, break,\naccept, congratulate, lead, burn, buy, cast, comb,\npresent, celebrate, attend crash, cut, decorate\nTable 6: Hard and easy verbs for our MMT model\nWe test if verbs that occur in both SVO-Probes\nand imSitu are easier for our model to classify.\nVerbs in imSitu are considered visual as the dataset\ncollection pipeline for imSitu includes an explicit\nannotation step to determine if verbs are visual.\nSurprisingly, we ﬁnd verbs in imSitu are harder for\nour MMT model. On closer inspection, some verbs\nin our dataset but not in imSitu (e.g., “swim”) are\nclearly visual. An interesting future direction is to\ninvestigate which visual properties of a verb make\nit harder or easier for image–language models to\nlearn.\n5 Conclusions\nAlthough image–language transformers achieve\nimpressive results on downstream tasks, previous\nwork suggests performance on these tasks can be\nconfounded by factors such as over-reliance on lan-\nguage priors (Goyal et al., 2017). We collect a\ndataset of image–sentence pairs to examine multi-\nmodal understanding by testing the ability of mod-\nels to distinguish images that differ with respect to\nsubjects, verbs, and objects.\nOur results show that image–language transform-\ners fail at identifying such ﬁne-grained differences;\nthey incorrectly classify image–sentence pairs that\ndo not match. Surprisingly, a model trained on a\nmanually-annotated and smaller dataset does better\non our task, suggesting that models have trouble\nignoring noise in larger but automatically-curated\npretraining datasets. Additionally, verb understand-\ning is harder than subject or object understanding\nacross all models we study. This motivates the\nneed for researchers to not only examines models\non objects, but develop datasets and architectures\nwhich allow for better verb understanding as well.\nAcknowledgements\nWe would like to thank Antoine Miech for detailed\ncomments on our paper. Also, thanks to Phil Blun-\nsom, Laura Rimell, Stephen Clark, Andrew Zis-\nserman, Jean-Baptiste Alayrac and the anonymous\nreviewers for their helpful feedback. We also thank\nSusie Young, Zhitao Gong, and Tyler Liechty for\nsupporting our data collection.\nReferences\nAishwarya Agrawal, Dhruv Batra, Devi Parikh, and\nAniruddha Kembhavi. 2018. Don’t just assume;\nlook and answer: Overcoming priors for visual ques-\ntion answering. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition ,\npages 4971–4980.\nArjun R Akula, Spandana Gella, Yaser Al-Onaizan,\nSong-Chun Zhu, and Siva Reddy. 2020. Words\naren’t enough, their order matters: On the robustness\nof grounding visual referring expressions. arXiv\npreprint arXiv:2005.01655.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun\nChen, and Jingjing Liu. 2020. Behind the scene:\nRevealing the secrets of pre-trained vision-and-\nlanguage models. arXiv preprint arXiv:2005.07310.\nYu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang,\nand Jia Deng. 2015. HICO: A benchmark for recog-\nnizing human-object interactions in images. In Pro-\nceedings of the IEEE International Conference on\nComputer Vision.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Doll ´ar, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server.arXiv preprint\narXiv:1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision (ECCV).\nFartash Faghri, David J Fleet, Jamie Ryan Kiros, and\nSanja Fidler. 2017. Vse++: Improving visual-\nsemantic embeddings with hard negatives. arXiv\npreprint arXiv:1707.05612.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nv in vqa matter: Elevating the role of image under-\nstanding in visual question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6904–6913.\nSaurabh Gupta and Jitendra Malik. 2015. Vi-\nsual semantic role labeling. arXiv preprint\narXiv:1505.04474.\nTanmay Gupta, Arash Vahdat, Gal Chechik, Xi-\naodong Yang, Jan Kautz, and Derek Hoiem. 2020.\nContrastive learning for weakly supervised phrase\ngrounding. ECCV.\nLisa Anne Hendricks, Ronghang Hu, Trevor Darrell,\nand Zeynep Akata. 2018. Grounding visual explana-\ntions. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 264–279.\n3644\nLisa Anne Hendricks, John Mellor, Rosalia Schnei-\nder, Jean-Baptiste Alayrac, and Aida Nematzadeh.\n2021. Decoupling the role of data, attention, and\nlosses in multimodal transformers. arXiv preprint\narXiv:2102.00529.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei\nFu, and Jianlong Fu. 2020. Pixel-bert: Aligning im-\nage pixels with text by deep multi-modal transform-\ners. arXiv preprint arXiv:2004.00849.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In The Thirty-Fourth AAAI Conference on\nArtiﬁcial Intelligence, AAAI.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. 2020b.\nOscar: Object-semantics aligned pre-training for\nvision-language tasks. In European Conference on\nComputer Vision.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nCewu Lu, Ranjay Krishna, Michael Bernstein, and\nLi Fei-Fei. 2016a. Visual relationship detection with\nlanguage priors. In European conference on com-\nputer vision, pages 852–869. Springer.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13–23.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\n2016b. Hierarchical question-image co-attention for\nvisual question answering. In Advances in neural\ninformation processing systems, pages 289–297.\nPranava Madhyastha, Josiah Wang, and Lucia Specia.\n2018. Defoiling foiled image captions. NAACL.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. NeurIPS.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hal-\nlucination in image captioning. EMNLP.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2556–2565.\nRavi Shekhar, Sandro Pezzelle, Yauhen Klimovich,\nAur´elie Herbelot, Moin Nabi, Enver Sangineto, and\nRaffaella Bernardi. 2017. FOIL it! ﬁnd one mis-\nmatch between image and language caption. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers).\nAmanpreet Singh, Vedanuj Goswami, and Devi Parikh.\n2020. Are we pretraining it right? digging deeper\ninto visio-linguistic pretraining. arXiv preprint\narXiv:2004.08744.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. In Empirical Methods in Natural Language\nProcessing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nMark Yatskar, Luke Zettlemoyer, and Ali Farhadi.\n2016. Situation recognition: Visual semantic role\nlabeling for image understanding. In Conference on\nComputer Vision and Pattern Recognition.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7874014377593994
    },
    {
      "name": "Transformer",
      "score": 0.757571816444397
    },
    {
      "name": "Verb",
      "score": 0.7073468565940857
    },
    {
      "name": "Sentence",
      "score": 0.663254976272583
    },
    {
      "name": "Natural language processing",
      "score": 0.6537976264953613
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6358888745307922
    },
    {
      "name": "Noun",
      "score": 0.6089965105056763
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5533860325813293
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}