{
  "title": "Random Language Model",
  "url": "https://openalex.org/W2889786171",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2924436311",
      "name": "E. DeGiuli",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Sorbonne Université",
        "Université Paris Sciences et Lettres",
        "Institut de Physique Théorique",
        "École Normale Supérieure - PSL"
      ]
    },
    {
      "id": "https://openalex.org/A2924436311",
      "name": "E. DeGiuli",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2326418800",
    "https://openalex.org/W4235743066",
    "https://openalex.org/W2093205346",
    "https://openalex.org/W2137640371",
    "https://openalex.org/W2950546693",
    "https://openalex.org/W2021216662",
    "https://openalex.org/W4301223056",
    "https://openalex.org/W2009047999",
    "https://openalex.org/W1507719567",
    "https://openalex.org/W2019588402",
    "https://openalex.org/W2018891628",
    "https://openalex.org/W3102543255",
    "https://openalex.org/W2030646934",
    "https://openalex.org/W2102924625",
    "https://openalex.org/W2108207895",
    "https://openalex.org/W186403072",
    "https://openalex.org/W2095402301",
    "https://openalex.org/W1990767775",
    "https://openalex.org/W2140324965",
    "https://openalex.org/W2570655866",
    "https://openalex.org/W1586060904",
    "https://openalex.org/W1507455327",
    "https://openalex.org/W2015576480",
    "https://openalex.org/W4246648922",
    "https://openalex.org/W2092654472",
    "https://openalex.org/W2002089154",
    "https://openalex.org/W2418771895",
    "https://openalex.org/W1984690265",
    "https://openalex.org/W3101791345",
    "https://openalex.org/W2071759506",
    "https://openalex.org/W1605758875",
    "https://openalex.org/W2609175771",
    "https://openalex.org/W2601798756",
    "https://openalex.org/W2684886669",
    "https://openalex.org/W2170716495"
  ],
  "abstract": "Many complex generative systems use languages to create structured objects. We consider a model of random languages, defined by weighted context-free grammars. As the distribution of grammar weights broadens, a transition is found from a random phase, in which sentences are indistinguishable from noise, to an organized phase in which nontrivial information is carried. This marks the emergence of deep structure in the language, and can be understood by a competition between energy and entropy.",
  "full_text": "Random Language Model\nE. DeGiuli\nInstitut de Physique Th´ eorique Philippe Meyer, ´Ecole Normale Sup´ erieure,\nPSL University, Sorbonne Universit´ es, CNRS, 75005 Paris, France\nMany complex generative systems use languages to create structured objects. We consider a\nmodel of random languages, deﬁned by weighted context-free grammars. As the distribution of\ngrammar weights broadens, a transition is found from a random phase, in which sentences are\nindistinguishable from noise, to an organized phase in which nontrivial information is carried. This\nmarks the emergence of deep structure in the language, and can be understood by a competition\nbetween energy and entropy.\nIt is a remarkable fact that structures of the most\nastounding complexity can be encoded into sequences\nof digits from a ﬁnite alphabet. Indeed, the complex-\nity of life is written in the genetic code, with alphabet\n{A,T,C,G }, proteins are coded from strings of 20 amino\nacids, and human-written text is composed in small, ﬁxed\nalphabets. This ‘inﬁnite use of ﬁnite means’ [1] was for-\nmalized by Post and Chomsky with the notion of gen-\nerative grammar [2, 3], and has been elaborated upon\nsince, both by linguists and computer scientists [4]. A\ngenerative grammar consists of an alphabet of hidden\nsymbols, an alphabet of observable symbols, and a set\nof rules, which allow certain combinations of symbols to\nbe replaced by others. From an initial start symbol S,\none progressively applies the rules until only observable\nsymbols remain; any sentence produced this way is said\nto be ‘grammatical,’ and the set of all such sentences\nis called the language of the grammar. The sequence\nof rule applications is called a derivation. For exam-\nple, the grammar {S → SS,S → (S),S → ()}has a\nsingle hidden symbol S and two observable symbols, (\nand ), and produces the inﬁnite set of all strings of well-\nformed parentheses. A simple derivation in this grammar\nis S → SS → (S)S → (())S → (())(). Besides their\noriginal use in linguistics, where the observable symbols\nare typically taken to be words, and grammars produce\nsentences (Fig 1a) [3, 5], generative grammars have found\napplication in manifold domains: in the secondary struc-\nture of RNA (Fig 1b) [6, 7], in compiler design [4], in\nself-assembly [8], in protein sequence analysis [9], and in\nquasicrystals [10], to name a few.\nThe complexity of a language is limited by conditions\nimposed on its grammar, as described by the Chomsky\nhierarchy, which, in increasing complexity, distinguishes\nregular, context-free, context-sensitive, and recursively\nenumerable grammars [11]. Each class of grammar has a\ncharacteristic graphical structure of its derivations: reg-\nular grammars produce linear derivations, context-free\ngrammars produce trees (Fig 1), and context-sensitive\nand recursively enumerable grammars produce more elab-\norate graphs. Associated with an increase in complexity\nis an increased diﬃculty of parsing [4]. Because biologi-\ncal instantiations of grammars must have been discovered\nby evolution, there is a strong bias toward simpler gram-\nmars; we consider context-free grammars (CFGs), which\nare the lowest order of the Chomsky hierarchy that sup-\nports hierarchical structure.\nDespite their ubiquity in models of complex generative\nsystems, grammars have hitherto played a minor role in\nphysics, and most known results on grammars are theo-\nrems regarding worst-case behavior [12], which need not\nrepresent the typical case. Human languages show Zipf’s\nlaw [13–15], a power-law dependence of word frequency\non its rank, and many sequences, including human text,\nshow long-range information-theoretic correlations [16–\n18], which can be created by a CFG [18]; but are these\ntypical features of some ensemble of grammars? In this\nwork we initiate this research program by proposing and\nsimulating an ensemble of CFGs, so that grammars can\nbe considered as physical systems [19]. We will ﬁnd\nthat CFGs possess two natural ‘temperature’ scales that\ncontrol grammar complexity, one at the surface interface,\nand another in the tree interior. As either of these tem-\nperatures is lowered, there is a phase transition, which\ncorresponds to the emergence of nontrivial information\npropagation. We characterize this phase transition using\nresults from simulations, and understand its location by\na balance between energy and entropy.\nGenerative grammars:A generative grammar is de-\nﬁned by an alphabet χ and a set of rules R. The al-\nphabet has N hidden, ‘non-terminal’ symbols χH, and\nT observable, ‘terminal’ symbols χO. The most gen-\neral rule is of the form a1a2 ...a n →b1b2 ...b m, where\nai ∈χH,bi ∈χ= χH ∪χO. In a CFG the rules are spe-\nS\nNP\nDet\nthe\nN\nbear\nVP\nV\nwalked\nPP\nP\ninto\nNP\nDet\nthe\nN\ncave\nS\ng S\na S u\nc\nS\nc\nS\nu\nS\na\na\ng\nS\nc\nS\nu\nS\ng\na\ng\n(a)\n(b)\nFIG. 1. Illustrative derivation trees for (a) simple English\nsentence, and (b) RNA secondary structure (after [6]). The\nlatter is a derivation of the sequence ‘gacuaagcugaguc’ and\nshows its folded structure. Terminal symbols are encircled.\narXiv:1809.01201v2  [cond-mat.dis-nn]  27 Mar 2019\n2\ncialized to the form a1 →b1b2 ...b m, and we will insist\nthat m≥1, so that there is no ‘empty’ string. Without\nloss of generality, we consider CFGs in Chomsky normal\nform, in which case all rules are of the form [4] a →b c\nor a→A, where a,b,c ∈χH and A∈χO. Note that we\nmay have b = a, or b = c, or a = b = c. Any derivation\nin Chomsky reduced form can be drawn on a binary tree.\nBeginning from the start symbol S ∈χH, rules are ap-\nplied until the string contains only observable symbols.\nSuch a string is called a sentence. The set of all sentences\nis the language of the grammar. Given a string of ob-\nservables S= A1 ...A ℓ and a grammar G, one can ask\nwhether there exists a derivation that produces Sfrom\nthe start symbol S; if so, Sis said to be grammatical.\nA formal grammar as deﬁned above can only distin-\nguish grammatical from ungrammatical sentences. A\nricher model is obtained by giving each rule a non-\nnegative real valued weight. Such a weighted grammar\nis useful in applications, because weights can be contin-\nuously driven by a learning process, and can be used to\ndeﬁne probabilities of parses. Moreover, a weighted gram-\nmar can be put into the Gibbs form, as shown below. For\nCFGs, to every rule of the forma→bcwe assign a weight\nMabc, and to every rule of the form a →A we assign a\nweight OaA.\nEach candidate derivation of a sentence has two dif-\nferent types of degrees of freedom. There is the topol-\nogy T of the tree, namely the identity (terminal or non-\nterminal) of each node, as well as the variables, both ter-\nminal and non-terminal, on the nodes. We write Ω T for\nthe set of internal factors, i.e. factors of the form a→bc,\nand ∂ΩT for the boundary factors, i.e. those associated to\na→A rules. The number of boundary factors is written\nℓT, which is also the number of leaves. Since derivations\nare trees, the number of internal factors is ℓT −1. We\nwill write σfor non-terminal symbols, andofor terminals;\nthese can be enumerated in an arbitrary way 1,...,N and\n1,...,T , respectively. Given T, we can write σi for the\nvalue of the non-terminal on site i, and similarly oj for\nthe terminal on site j. The number of σi is 2ℓT−1, while\nthe number of oj is ℓT. We write Gfor the pair M,O, σ\nfor {σi}, and o for {ot}.\nTo deﬁne a probability measure on derivations, it is\nconvenient to factorize it into the part specifying T, and\nthe remainder. In this way we separate the the tree shape\nfrom the inﬂuence of the grammar on variables. For a\nﬁxed T the weight of a conﬁguration is\nW(σ,o|T,G) =\n∏\nα∈ΩT\nMσα1 σα2 σα3\n∏\nα∈∂ΩT\nOσα1 oα2 , (1)\nwhere each α= (α1,α2,α3) is a factor in the order σα1 →\nσα2 σα3 . Note that Mabc ̸= Macb in general, thus the left\nand right branches are distinguished [20]. We can write\nW = e−E with\nE = −\n∑\na,b,c\nπabc(σ) logMabc −\n∑\na,B\nρaB(σ,o) logOaB (2)\nwhere πabc is the number of times the rule a →bc ap-\npears in the conﬁguration σ, and likewise ρaB is the\nnumber of times the rule a → B appears. This de-\nﬁnes a conditional probability measure on conﬁgurations\nP(σ,o|T,G) = e−E(σ,o|T,G)/Z(T,G) where\nZ(T,G) =\n∑\n{σi,ot}\ne−E(σ,o|T,G). (3)\nAll conﬁgurations have S at the root node. For sim-\nplicity, in this work we consider as a model for the\ntree topology probability P(T|G) = Wtree/Ztree with\nWtree(T) = p|∂ΩT |(1 −p)|ΩT |, where p is the emission\nprobability, the probability that a hidden node becomes\nan observable node. p controls the size of trees; we will\nchoose it such that the tree size distribution is cutoﬀ\nabove a length ξ = 1000. Some facts about the resulting\nbinary trees are recorded in Supplementary Material [21].\nA model with weights of the form (1) is called a\nweighted CFG (WCFG). In the particular case where\n1 = ∑\nb,cMabc = ∑\nAOaA for all a, it is easy to see that\nM and O are conditional probabilities: Mabc = P(a →\nbc | a → non-terminal) and OaA = P(a → A | a →\nterminal). In this case the model is called a probabilistic\nCFG (PCFG). In the main text, we consider a weighted\nCFG, model W; in SI, we show that our results are ro-\nbust in model P, a PCFG. There are tradeoﬀs between\nthese models: model P is easier to sample, because it has\nZ(T,G) = 1 from normalization of probability, and thus\nis factorized. But model W is more amenable to theory,\nsince it is less constrained.\nRandom Language Model: Each grammar deﬁnes\nprobabilities for sentences. To extract the universal prop-\nerties of grammars, which do not depend on all details of\nM and O, we need a measure on the space of grammars.\nWhat is an appropriate measure? From Eq.(2), log M\nand log O are analogous to coupling constants in statisti-\ncal mechanics. A simple model is to assume a Gaussian\ndistribution for these, so that M and O are lognormal.\nThis can be motivated as follows: language evolution is a\ndynamical process, which must be slow in order for lan-\nguage to remain comprehensible at any given moment.\nIf each log Mabc and log OaB are the accumulation of in-\ndependent, additive increments [22], these will lead to a\nlognormal. We deﬁne deep and surface sparsities as, re-\nspectively,\nsd = 1\nN3\n∑\na,b,c\nlog2\n[Mabc\nM\n]\n, ss = 1\nNT\n∑\na,B\nlog2\n[OaB\nO\n]\n(4)\nwhere M = 1 /N2 and O = 1 /T are the correspond-\ning uniform probabilities; it is convenient to use this\nnormalization even for model W where weights are not\nstrictly normalized. A lognormal distribution of gram-\nmar weights is\nPG(M,O) ≡Z−1\nG J e−ϵdsde−ϵsss (5)\n3\n10-3 10-2 10-1 100 101 102 103\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n10-3 10-2 10-1 100 101 102 103\n0.6\n0.7\n0.8\n0.9\n1\n(a) (b)\nFIG. 2. Shannon entropy of random CFGs as functions of\n˜ϵd = ϵd/N3. (a) Block entropy of hidden conﬁgurations for\nindicated k and N. (b) Block entropy of observed strings;\nsymbols as in (a). The constant value for ϵd > ϵ∗ depends\non the surface temperature ϵs. Bars indicate 20 th and 80 th\npercentiles.\nwhere J = e−∑\na,b,clog Mabc−∑\na,Blog OaB, and the space\nof M and O is deﬁned by appropriate normalization and\npositivity constraints. We deﬁne the Random Language\nModel as the ensemble of grammars drawn from Eq.5.\nAn alternative motivation of (5) is that this is the\nmaximum-entropy measure when the grammar-averages\nsd and ss are constrained. sd and ss measure the density\nof rules about their respective median values M and O.\nWhen sd and ss are ﬁnite, all rules must have a ﬁnite\nprobability: this reﬂects the fact that, given any ﬁnite\namount of data, one can only put a lower bound on the\nprobability of any particular rule. In model W the La-\ngrange multipliers ϵd and ϵs satisfy\nsd = N3\n2ϵd\n, ss = NT\n2ϵs\n. (6)\nWhen ϵd →∞, sd →0, which is the value corresponding\nto a completely uniform deep grammar, that is, when for\na non-terminal a, all rules a→bchave the same probabil-\nity 1/N2. This is clearly the limit in which the grammar\ncarries no information. As ϵd is lowered, sd increases,\nand the grammar carries more information. In terms\nof how deterministic the rules are, ϵd plays the role of\ntemperature, with random ↔hot and deterministic ↔\ncold; we will refer to it as the deep temperature. This\nanalogy can also be seen formally: in SI, we show that if\nthe energy E is replaced by βE, then (6) is replaced by\nsd = β2N3/(2ϵd), such that lowering ϵd is equivalent to\nincreasing β. Similarly, ϵs controls information transmis-\nsion at the surface; we call it the surface temperature.\nTo investigate the role of ϵd on language structure,\nwe sampled grammars from the RLM at ﬁxed values\nT = 27 ,ϵs/(NT) = 0 .01. Since the surface sparsity is\nlarge, there is already some simple structure at the sur-\nface; we will explore how deep structure emerges asN and\nϵd are varied. For each value of N and ϵd, we created 120\ndistinct grammars, from which we sample 200 sentences\n(see SI for more details). Altogether approximately 7200\ndistinct languages were constructed.\nThe information content of a grammar G is natu-\nrally encoded by Shannon entropies. For a sequence\n0 0.5 110-3\n10-2\n10-1\n100\n101\n10-3 10-2 10-1 100 101 102 103\n0\n0.5\n1\n1.5\n2\n2.5\n10-3 10-2 10-1 100 101 102 103\n10-2\n10-1\n100\n(a) (b)\n˜ϵd\nFIG. 3. (a) Zipf plot of hidden symbols for N = 40. Here ˜ϵd =\nϵd/N3. (b) Order parameter Q2, with bars indicating 20th and\n80th percentile ranges over grammars at each parameter value.\nInset: same plot in log-log axes.\no1,o2,...,o k the Shannon block entropy rate is\nHs(G; k) = 1\nk\n⟨\nlog 1/P(o1,o2,...,o k|G)\n⟩\n(7)\nFor CFGs we can also consider the block entropy rate of\ndeep conﬁgurations,\nHd(G; k) = 1\nk\n⟨\nlog 1/P(σ1,σ2,...,σ k|G)\n⟩\n(8)\nwhere the symbols are taken from a (leftmost) derivation.\nIn both cases the ensemble average is taken with the ac-\ntual probability of occurrence, P(o|G) for Hs, and P(σ|G)\nfor Hd.\nThe grammar averages Hd(k) and Hs(k) are shown\nin Fig. 2, for k as indicated; here and in the following,\nthe bars show the 20 th and 80 th percentiles, indicating\nthe observable range of Hd and Hs over the ensemble of\ngrammars [23]. The dependence on ϵd is striking: for\nϵd ≳ N3/log2 N, both Hs(1) and Hd(1) are ﬂat. In this\nregime, Hd(1) ≈log N, indicating that although conﬁgu-\nrations strictly follow the rules of a WCFG, deep conﬁgu-\nrations are nearly indistinguishable from completely ran-\ndom conﬁgurations. However, at ϵd = ϵ∗ ≈N3/log2 N\nthere is a pronounced transition, and both entropies be-\ngin to drop. This transition corresponds to the emergence\nof deep structure.\nThe ﬁrst block entropy Hd(G; 1) measures information\nin the single-character distribution, while the diﬀerential\nentropies δHd(G; k) = (k+1)Hd(G; k+1)−kHd(G; k) mea-\nsure incremental information in the higher-order distribu-\ntions [17]. The Shannon entropy rate including all corre-\nlations can either be obtained from lim k→∞Hd(G; k), or\nfrom lim k→∞δHd(G; k). These coincide, but the latter\nconverges faster [17]. In SI, we show that δHd(G; k), and\nthus the limiting rate, appears to collapse with ˜ϵdlog N.\nFor all entropies the sample-to-sample ﬂuctuations de-\ncrease rapidly with k, suggesting that the limiting rates\nare self-averaging.\nTo further investigate the nature of the transition, we\nshow in Fig. 3a a Zipf plot: the frequency of each sym-\nbol, arranged in decreasing order. Fig. 3a shows the Zipf\nplot for deep structure; the Zipf plot for surface struc-\nture is similar, but less dramatic (see SI). We see a sharp\n4\nchange at ϵ∗: for ϵd >ϵ∗, the frequencies of hidden sym-\nbols are nearly uniform, while below ϵ∗, the distribution\nis closer to exponential ( In SI, we show that a power-\nlaw regime for the observable symbols appears when T is\nlarge). The permutation symmetry among hidden sym-\nbols is thus spontaneously broken at ϵ∗.\nWhat is the correct order parameter to describe this\ntransition? The ferromagnetic order parameter is mr =\n⟨Nδσi,r −1⟩, where i is a site. This does not show any\nsignal of a transition, despite the fact that the start sym-\nbol explicitly breaks the replica symmetry. A more inter-\nesting choice is one of Edwards-Anderson type, such as\nQEA\nrs = ⟨Nδσi,r −1⟩⟨Nδσi,s −1⟩where r and s label dif-\nferent sentences produced from the same grammar, and\nσi is a speciﬁed site [24]. However, sentences produced\nby a CFG do not have ﬁxed derivation trees, so we need\nto compare symbols in relative position. For each interior\nrule a→bc we can deﬁne\nQabc(G) = ⟨δσα1 ,a\n(\nN2δσα2 ,bδσα3 ,c −1\n)\n⟩, (9)\naveraged over all interior vertices α, and averaged over\nderivations. Here σα1 is the head symbol at vertex α,\nand σα2 ,σα3 are the left and right symbols, respectively.\nQmeasures patterns in rule application at each branching\nof a derivation tree. It is thus an order parameter for deep\nstructure. Upon averaging over grammars in the absence\nof any ﬁelds, the permutation symmetry must be restored:\nQabc = q0 +δab ql+δac qr+δbc qh+δabδac q∗. As shown in\nSI, these components show a transition, but there is sig-\nniﬁcant noise below ϵ∗, despite there being 120 replicas at\neach point. Evidently, Qabc has large ﬂuctuations below\nϵ∗. This suggests a deﬁnition Q2 ≡∑\na,b,cQ2\nabc, plotted\nin Fig 3b. The signal is clear: on the large scale, Q2 has\na scaling form Q2 ≈N3f(ϵN/ϵ∗) and is small above ϵ∗.\nThe scaling Q2 ∼N3 suggests that below the transition,\nall hidden symbols start to carry information in the deep\nstructure.\nTheory: How can we gain some theoretical in-\nsight into the RLM? Consider the entropy of an observed\nstring of length ℓ, composed of n sentences of length\nℓk,∑\nkℓk = ℓ. The entropy of this string derives from 3\ndistinct combinatorial levels: (i) each sentence can be rep-\nresented by a derivation tree with many diﬀerent topolo-\ngies; (ii) each derivation tree can host a variety of internal\nhidden variables; and (iii) given the hidden variables, the\nobserved symbols can themselves vary.\nSome scaling considerations are useful. Each derivation\ntree can have many topologies: the entropy of binary trees\nscales as ℓklog 4, so that the total tree entropy scales as\nSt ∼ ℓlog 4. Each derivation tree has 2 ℓk −1 hidden\nvariables, so that the total number of hidden DOF is 2ℓ−\nn, and the corresponding deep entropy scales as Sd ∼\n(2ℓ−n) logN. Finally, the sentences have an entropy\nSo ∼ℓlog T.\nWe see that when typical sentences are of length ⟨ℓ⟩≫\n1, so that ℓ−n ∼ℓ, these numbers are independent of\npartitioning, to leading order. For large ⟨ℓ⟩we get the\nscaling S ∼ℓlog(4N2T).\nThis must be compared with the ‘energetic’ terms\nlog Wtree = ( ℓ−n) log(1 −p) + ℓlog p ∼ −2ℓlog 2 for\np near 1/2, and E, Eq.(2). In E, π is positively corre-\nlated with M, since rules with a higher weight are more\nfrequently used; hence we can obtain a simple scaling es-\ntimate E ∼−N3πlog m−NTρ log owhere πis the mean\nvalue of πabc, and log m is the value of a typical positive\nﬂuctuation of logMabc, and similarly forO. From the sum\nrules ∑\na,b,cπabc = |Ω|= ℓ−n and ∑\na,BρaB = |∂Ω|= ℓ\nwe have π= (ℓ−n)/N3,ρ = ℓ/(NT). The mean value of\nlog Mabc is log M, and the mean value of logOaB is log O.\nThese contributions lead to a constant value of E. The\npositive ﬂuctuations in log M and log O that couple to E\nscale as\n√\nN3\n2ϵd\nand\n√\nNT\n2ϵs\n, respectively, leading to\nE ∼−ℓ\n√\nN3\n2ϵd\n−ℓ\n√\nNT\n2ϵs\n+ const (10)\nCombining this with S, the eﬀective free energy F =\nE−log Wtree −S reﬂects a competition between energy\nand entropy. If we consider N and ϵd as varying, then\nthere is a scale ϵ∗ = N3/log2 N where the energetic\nﬂuctuations balance entropy. For ϵd ≫ϵ∗, the energy\nof a conﬁguration is unimportant, and the grammar is\nthus irrelevant: the language produced by the WCFG\nmust then be indistinguishable from random sequences,\nas found empirically above. In contrast, for ϵd ≪ ϵ∗,\nthe language reﬂects those sequences with high intrinsic\nweight, and their entropy is less important. The char-\nacteristic scale ϵ∗ identiﬁed by these simple arguments\nagrees with that found empirically above, and locates the\nemergence of deep structure. However, further work is\nneeded to predict the behavior of Q2, Hs, and Hd.\nLearning human languages: Around 6000 lan-\nguages are spoken around the world [25]; given fractured\nand highly sparse input, how does a child come to learn\nthe precise syntax of one of these many languages? This\nquestion has a long history in linguistics and cognitive sci-\nence [26, 27]. One scenario for learning is known as the\nPrinciples and Parameters (P&P) theory [28]. This posits\nthat the child is biologically endowed with a general class\nof grammars, the ‘principles,’ and by exposure to one\nparticular language, ﬁxes its syntax by setting some num-\nber of parameters, assumed to be binary. For example,\nthe head-directionality parameter controls whether verbs\ncome before or after objects, like English and Japanese,\nrespectively. A vast eﬀort has been devoted to mapping\nout the possible parameters of human languages [25, 29].\nThe richness of the discovered structure has been used\nas criticism of the approach [30]: if the child needs to\nset many parameters, then do these all need to be in-\nnate? This would be a heavy evolutionary burden, and a\nchallenge to eﬃcient learning.\nThe RLM can shed some light on this debate. First,\nsince only 2 living human languages are known to pos-\nsess syntax beyond CFG [31], we consider WCFGs a valid\nstarting point [32]. Following experimental work [27], we\n5\npicture the learning process as follows. Initially, the child\ndoes not know the rules of the grammar, so it begins\nwith some small number of hidden symbols and assigns\nuniform values to the weights M and O. To learn is to\nincrease the likelihood of the grammar by adjusting the\nweights and adding new hidden symbols. As weights are\ndriven away from uniform values, the temperaturesϵd and\nϵs decrease. Eventually the transition to deep structure\nis encountered, and the grammar begins to carry infor-\nmation.\nIn the absence of any bias, this transition would oc-\ncur suddenly and dramatically, spontaneously breaking\nall N3 directions in M space simultaneously, as in Fig. 3b.\nHowever, in realistic child language learning, the child’s\nenvironment acts as a ﬁeld on this likelihood-ascent, and\ncan cause the structure-emerging transitions to occur at\ndiﬀerent critical deep temperatures, depending on their\ncoupling to the ﬁeld. For example, a left-right symmetry\nbreaking could correspond to setting the head direction-\nality parameter.\nAlthough this description is schematic, we insist that\nthe various symmetry-breaking transitions, which could\ngive rise to parameters, are emergent properties of the\nmodel. Thus if there are indeed many parameters to\nbe set, these do not all need to be innate: the child only\nneeds the basic structure of a WCFG, and the rest is\nemergent. The P&P theory is thus consistent with ex-\nistence of many parameters. If the RLM can be solved,\nby which we mean that the partition function Z can be\ncomputed, then the series of symmetry-breaking transi-\ntions that occur in the presence of a ﬁeld can be inferred,\nand a map of syntax in CFGs could be deduced. This is\na tantalizing goal for future work.\nConclusion: We introduced a model of random\nlanguages, which captures the generative aspect of com-\nplex systems. The model has a transition in parameter\nspace that corresponds to the emergence of deep struc-\nture. Since the interaction is long-range, we expect that\nthe RLM, or a variant, is exactly solvable. We hope that\nthis will be clariﬁed in the future.\nThis work beneﬁted from discussions with C. Callan,\nJ. Kurchan, G. Parisi, R. Monasson, G. Semerjian, P.\nUrbani, F. Zamponi, A. Zee, and Z. Zeravcic.\n[1] W. Von Humboldt, Humboldt:’On language’: On the di-\nversity of human language construction and its inﬂuence\non the mental development of the human species (Cam-\nbridge University Press, 1999).\n[2] E. L. Post, American journal of mathematics 65, 197\n(1943).\n[3] N. Chomsky, Syntactic structures (Walter de Gruyter,\nBerlin, 2002).\n[4] J. E. Hopcroft, R. Motwani, and J. D. Ullman, Intro-\nduction to automata theory, languages, and computation ,\n3rd ed. (Pearson, Boston, Ma, 2007).\n[5] N. Chomsky, Aspects of the Theory of Syntax , Vol. 11\n(MIT press, Cambridge, 2014).\n[6] D. B. Searls, Nature 420, 211 (2002).\n[7] B. Knudsen and J. Hein, Nucleic acids research 31, 3423\n(2003).\n[8] E. Winfree, X. Yang, and N. C. Seeman, in DNA\nbased computers II, DIMACS series in discrete mathemat-\nics and theoretical computer science, Vol. 44 (American\nMathematical Soc., Providence, R.I., 1999) p. 191.\n[9] J. P. Barton, A. K. Chakraborty, S. Cocco, H. Jacquin,\nand R. Monasson, Journal of Statistical Physics162, 1267\n(2016).\n[10] J. G. Escudero, in Symmetries in Science IX (Springer,\nBoston, 1997) pp. 139–152.\n[11] M. A. Nowak, N. L. Komarova, and P. Niyogi, Nature\n417, 611 (2002).\n[12] For example, from Ref.4, Theorem 7.17 on the size of\nderivation trees, Theorem 7.31 on the conversion of an\nautomaton to a CFG, and Theorem 7.32 on the complex-\nity of conversion to Chomsky normal form (see below).\n[13] G. K. Zipf, The psycho-biology of language: An introduc-\ntion to dynamic philology (Routledge, Milton Park, 2013).\n[14] R. F. i Cancho and R. V. Sol´ e, Proceedings of the Na-\ntional Academy of Sciences 100, 788 (2003).\n[15] A. Corral, G. Boleda, and R. Ferrer-i Cancho, PloS one\n10, e0129031 (2015).\n[16] W. Ebeling and T. P¨ oschel, EPL (Europhysics Letters)\n26, 241 (1994).\n[17] T. Sch¨ urmann and P. Grassberger, Chaos: An Interdisci-\nplinary Journal of Nonlinear Science 6, 414 (1996).\n[18] H. W. Lin and M. Tegmark, Entropy 19, 299 (2017).\n[19] G. Parisi, Physica A 263, 557 (1999).\n[20] Indeed if the left-right branches are not distinguished,\nCFGs do not have any more expressive power than regular\ngrammars [33].\n[21] Supplementary Material includes details on binary trees,\nsampling methods, robustness in PCFG, diﬀerential en-\ntropies, and equation derivations, and Refs. [34, 35].\n[22] D. Sornette and R. Cont, Journal de Physique I 7, 431\n(1997).\n[23] The error bars in measurements are then smaller by factor\napproximately\n√\n120 ∼11.\n[24] D. Gross, I. Kanter, and H. Sompolinsky, Physical review\nletters 55, 304 (1985).\n[25] M. C. Baker, The atoms of language: The mind’s hidden\nrules of grammar (Basic books, New York, 2008).\n[26] R. C. Berwick, P. Pietroski, B. Yankama, and N. Chom-\nsky, Cognitive Science 35, 1207 (2011).\n[27] C. Yang, S. Crain, R. C. Berwick, N. Chomsky, and J. J.\nBolhuis, Neuroscience and Biobehavioral Reviews (2017).\n[28] N. Chomsky, Lectures on government and binding: The\nPisa lectures, 9 (Walter de Gruyter, 1993).\n[29] U. Shlonsky, Language and linguistics compass 4, 417\n(2010).\n[30] G. Ramchand and P. Svenonius, Language Sciences 46,\n152 (2014).\n[31] Only Swiss-German and Bambara have conﬁrmed fea-\ntures beyond CFG [36, 37].\n[32] Note also that some lexicalized models used for machine\nlearning, such as [38], are WCFGs with multi-indexed\nhidden variables.\n[33] J. Esparza, P. Ganty, S. Kiefer, and M. Luttenberger,\nInformation Processing Letters 111, 614 (2011).\n[34] S. Chib and E. Greenberg, The american statistician 49,\n327 (1995).\n[35] P. Flajolet and R. Sedgewick, Analytic combinatorics\n(cambridge University press, 2009).\n6\n[36] C. Culy, Linguistics and Philosophy 8, 345 (1985).\n[37] S. M. Shieber, in Philosophy, Language, and Artiﬁcial\nIntelligence (Springer, 1985) pp. 79–89.\n[38] M. Collins, Computational linguistics 29, 589 (2003).\nRandom Language Model – Supplementary Information\nE. DeGiuli\nInstitut de Physique Th´ eorique Philippe Meyer, ´Ecole Normale Sup´ erieure,\nPSL University, Sorbonne Universit´ es, CNRS, 75005 Paris, France\nSampling Methods: We consider both WCFGs and\nPCFGs. Since their distribution is factorized, PCFGs are\ntrivial to sample: we begin at the top of the tree with S,\nand choose whether this branches into two non-terminals\nwith probability 1 −p, or becomes a terminal node with\nprobability p. S is replaced by non-terminals or termi-\nnals according to the probabilities speciﬁed in M and O,\nrespectively. This process is then repeated, where we act\nby replacement on the left-most non-terminal (for CFGs\nthe order of replacement does not aﬀect the derivation),\nreplacing non-terminals according to the probabilities M\nor O; we continue until no non-terminals remain. It is\npossible that sequences grow unboundedly; we stop any\nthat go beyond length 4000. This occurs for less than\na fraction 10 −4 of samples. It is clear that the emission\nprobability pcould be absorbed intoM and O: the advan-\ntage of our model is that we strictly control the typical\nsentence length, so that this does not appear as a con-\nfounding variable in the analysis, and we avoid creation\nof inﬁnite trees.\nWCFGs are sampled with the Metropolis-Hastings\nalgorithm [1]. For a given WCFG, we deﬁne a re-\nlated PCFG by ˜Mabc = Mabc/∑\nb′,c′ Mab′c′, ˜OaB =\nOaB/∑\nB′ OaB′, for all a. This PCFG is used as a\ncandidate-generating density. In the Metropolis-Hastings\nrule, the individual factors Mabc,OaB all cancel, leav-\ning only the normalization factors from ∑\nb′,c′ Mab′c′,∑\nB′ OaB′, which have smaller ﬂuctuations than the indi-\nvidual Mabc,OaB. This ensures eﬃcient sampling.\nFor model W, for each value of N and ϵd, we created\n120 distinct grammars, from which we sample 200 sen-\ntences. We ﬁx the emission probability p so that the\nmean sentence length is ⟨ℓ⟩≈ 15, and the total length of\ntext sampled for each grammar is ≈3000.\nBlock entropies are computed using the method of\nGrassberger (Eq. 8 in [2]). Since a deep block entropy of\norder khas a phase space with Nk conﬁgurations, we can\nonly eﬀectively compute entropies for which Nk ≲ 3000,\nand similarly surface block entropies can be computed for\nTk ≲ 3000. Shown entropies in the main text satisfy this\nbound. They are absent from ﬁnite ℓ eﬀects: we saw no\ndependence when entropies were computed using only the\nﬁrst half of the sampled sentences.\nBinary trees: Our model for the probability of a\ntree topology T is\nP(T|G) = 1\nZtree\n(1 −p)|ΩT |p|∂ΩT |, (1)\nFor a binary tree with ℓleaves, |ΩT|= ℓ−1 and |∂ΩT|=\nℓ. The number of binary trees with ℓleaves is given[3] by\n10-3 10-2 10-1 100 101 102 103\n0.4\n0.6\n0.8\n1\n10-3 10-2 10-1 100 101 102\n0.4\n0.6\n0.8\n1\n(a) (b)\nFIG. 1. Diﬀerential entropy of hidden symbols of random\nWCFGs as functions of ˜ϵd = ϵd/N3, for indicated k and N.\n(a) versus ˜ϵd log2 N; (b) versus ˜ϵd log N. Bars indicate 20 th\nand 80th percentiles.\n10-3 10-2 10-1 100 101 102 103\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n10-3 10-2 10-1 100 101 102 103\n0.6\n0.7\n0.8\n0.9\n1\n(a) (b)\nFIG. 2. Shannon entropy of random PCFGs as functions\nof ˜ϵd = ϵd/N3. (a) Block entropy of hidden conﬁgurations\nfor indicated k and N. (b) Block entropy of observed strings.\nSymbols are as in (a), although a diﬀerent subset of param-\neters is shown. The constant value for ϵd > ϵ∗ depends on\nthe surface temperature ϵs. Bars indicate 20 th and 80th per-\ncentiles.\nthe Catalan number\nCℓ−1 = 1\nℓ\n(2(ℓ−1)\nℓ−1\n)\n∼ 4ℓ\n4√π(ℓ−1)3/2 (2)\nleading to\nZtree = p\n∞∑\nℓ=1\nCℓ−1(p(1 −p))ℓ−1 = 2p\n1 + |2p−1|, (3)\nwhere we used the result for the generating function of\nthe Catalan numbers. As expected there is a singularity\nat p = 1/2. Let us write p = 1/2 + ϵ with ϵ >0. Then\nZtree = 1 and one can show that the distribution of tree\nsizes follows\nP(ℓ) ∝e−ℓ/ξ\nℓ3/2 , (4)\nwhere ξ = 1/(4ϵ2). In our numerical results we have set\nξ= 1000.\narXiv:1809.01201v2  [cond-mat.dis-nn]  27 Mar 2019\n2\nScaling symmetry with temperature: We can\ngeneralize the model by adding a bias β to the en-\nergy, such that the probability of a conﬁguration is\nP(σ,o|T,G) = e−βE(σ,o|T,G)/Z(T,G). This is equiva-\nlent to replacing Mabc by Mβ\nabc and OaB by Oβ\naB. If we\nalso rescale M as M\n′\n= M\nβ\n, and O\n′\n= O\nβ\n, then the spar-\nsities rescale as s′\nd = β2sd, s′\ns = β2ss. Thus the rescaled\nequations of state are\nsd\n′= β2N3\n2ϵd\n, ss\n′= β2NT\n2ϵs\n. (5)\nHence decreasing ϵd and ϵs is equivalent to increasing β,\nas well as rescaling the median values M and O. It can\nbe shown that grammar-averages of moments of the par-\ntition function Z(T,G)m also have this scaling property.\nDiﬀerential entropy: Deep diﬀerential entropies are\ndeﬁned from block entropies by\nδHd(G; k) = (k+ 1)Hd(G; k+ 1) −kHd(G; k) (6)\nTo examine the behavior as k is increased, we performed\nadditional simulations over an ensemble of 30 WCFGs at\neach ϵd and N, with 5000 sampled sentences per gram-\nmar. The resulting δHd are plotted in Fig. 1, versus (a)\n˜ϵdlog2 N, and (b) ˜ϵdlog N, where ˜ϵd = ϵd/N3. Collapse\nis better in the latter case, suggesting that the limiting\nrate obtained in the limit k→∞ depends on this reduced\nvariable.\nModel P: We consider PCFGs withT = 27,ϵs/NT =\n0.01, and N = 10 ,20,40,80 with varying ϵd. For each\nparameter set, 300 grammars were constructed, and 200\nsentences were sampled. Altogether 24000 languages were\nconsidered. Results are shown in Figs. 2,3. All qualita-\ntive behavior is identical to that found for model W. Scal-\ning collapses are the same, but the choice of n in lognN\nmay diﬀer: as shown in Fig.3cd, for large N the value\nn = 1 collapses better than n = 2, which is optimal for\nsmall N.\nZipf plots: Zipf plots for the observed symbols are\nshown in Fig.3. At large ϵd, these are not ﬂat, unlike\nthe Zipf plots for hidden symbols, because we have ﬁxed\nϵs/(NT) = 0.01. Still, as the transition at ϵd = ϵ∗ is en-\ncountered, the distributions broaden. In WCFGs with a\nlarger observable alphabet, a power-law regime emerges,\nas shown in Fig.5. For rank Rgreater than N, the expo-\nnent is near −2, while at smaller R, the behavior depends\non N, T, and ϵd. The small R regime also depends on\n˜ϵs (not shown). For comparison, a slope −1, commonly\nobserved in human language [4–6], is also shown. It has\nbeen proposed that this exponent arises from constraints\nof eﬃcient communication [4, 5].\nEquation-of-state: In model W, the grammar par-\ntition function is\nZG =\n∫\ndM\n∫\ndOJ e−ϵdsd e−ϵsss (7)\nOne easily sees that\nsd = −∂log ZG\n∂ϵd\n, ss = −∂log ZG\n∂ϵs\n(8)\nAfter a change of variable mabc = log Mabc/M,oaB =\nlog OaB/O, ZG is Gaussian and we ﬁnd\nZG = J\n(πN3\nϵd\n)1\n2 N3 (πNT\nϵs\n)1\n2 NT\n(9)\nleading to\nsd = N3\n2ϵd\n, ss = NT\n2ϵs\n(10)\nIn model P, we have a normalization δ−function that can\nbe integrated using its Fourier transform. The ﬁnal result\ncannot be put into a very explicit form.\nDeep structure symmetries: In the absence of\nﬁelds, the order parameter Qabc has a grammar average\nthat must respect permutation symmetry. It then is of\nthe form\nQabc = q0 + δab ql + δac qr + δbc qh + δabδac q∗, (11)\nwhere N2q0 +N(ql+qr+qh)+ q∗= 0 from ∑\nb,cQabc = 0.\nq0 and ql are plotted, for model W, in Fig.6. ( qr behaves\nthe same asql, by symmetry. q∗and qh display even larger\nﬂuctuations.) The bands show 20 th and 80th percentiles\nover the 120 grammars sampled at each parameter value.\nWe see that these quantities display wild ﬂuctuations,\nmuch larger than the quantity Q2.\n[1] S. Chib and E. Greenberg, The american statistician 49,\n327 (1995).\n[2] T. Sch¨ urmann and P. Grassberger, Chaos: An Interdisci-\nplinary Journal of Nonlinear Science 6, 414 (1996).\n[3] P. Flajolet and R. Sedgewick, Analytic combinatorics\n(cambridge University press, 2009).\n[4] G. K. Zipf, The psycho-biology of language: An introduc-\ntion to dynamic philology (Routledge, Milton Park, 2013).\n[5] R. F. i Cancho and R. V. Sol´ e, Proceedings of the National\nAcademy of Sciences 100, 788 (2003).\n[6] A. Corral, G. Boleda, and R. Ferrer-i Cancho, PloS one\n10, e0129031 (2015).\n3\n0 0.2 0.4 0.6 0.8 110-3\n10-2\n10-1\n100\n101\n10-3 10-2 10-1 100 101 102 103\n0\n0.5\n1\n1.5\n2\n2.5\n10-3 10-2 10-1 100 101 102 103\n10-2\n10-1\n100\n10-4 10-3 10-2 10-1 100 101 102\n10-2\n100\n(a) (b) (c) (d)\n˜ϵd\nFIG. 3. Results for model P. (a) Zipf plot of hidden symbols for N = 40. Here ˜ϵd = ϵd/N3. (b-d) Order parameter Q2, with\nbars indicating 20th and 80th percentile ranges over grammars at each parameter value. (d) shows a plot vs log N rather than\nlog2 N. The collapse is better at large N but worse at small N.\n0 0.2 0.4 0.6 0.8 110-2\n10-1\n100\n101\n0 0.2 0.4 0.6 0.8 110-2\n10-1\n100\n101\n(a) (b)\nFIG. 4. Zipf plots. Frequency of observed symbols, in decreasing order. (a) WCFG (b) PCFG. In both cases N = 40, and\nlabels are as in Fig. 3a.\n10-1 100 101\n10-4\n10-2\n100\n10-1 100 101\n10-4\n10-2\n100\n(a) (b)\nFIG. 5. Zipf plot in WCFG with (a) T = 1000 and (b) T = 10000. Frequency of observed symbols, in decreasing order,\ntogether with indicated power laws as a function of the rank R.\n(a) (b)\nFIG. 6. Grammar-averages of scalars (a) q0 and (b) ql as functions of ˜ϵd = ϵd/N3. Bands indicate 20 th and 80th percentiles.",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7055433392524719
    },
    {
      "name": "Computer science",
      "score": 0.5901108980178833
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.5880882143974304
    },
    {
      "name": "Grammar",
      "score": 0.5393772721290588
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47634342312812805
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.47032010555267334
    },
    {
      "name": "Phase transition",
      "score": 0.4481396973133087
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4180383086204529
    },
    {
      "name": "Linguistics",
      "score": 0.4015936255455017
    },
    {
      "name": "Natural language processing",
      "score": 0.3969353437423706
    },
    {
      "name": "Physics",
      "score": 0.24376198649406433
    },
    {
      "name": "History",
      "score": 0.0986127257347107
    },
    {
      "name": "Quantum mechanics",
      "score": 0.09732121229171753
    },
    {
      "name": "Philosophy",
      "score": 0.07823875546455383
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210163633",
      "name": "Institut de Physique Théorique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I39804081",
      "name": "Sorbonne Université",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2746051580",
      "name": "Université Paris Sciences et Lettres",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I29607241",
      "name": "École Normale Supérieure - PSL",
      "country": "FR"
    }
  ],
  "cited_by": 29
}