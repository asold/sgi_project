{
    "title": "T3-Vis: visual analytic for Training and fine-Tuning Transformers in NLP",
    "url": "https://openalex.org/W3212246833",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5101464148",
            "name": "Raymond Li",
            "affiliations": [
                "University of British Columbia"
            ]
        },
        {
            "id": "https://openalex.org/A5007294216",
            "name": "Wen Xiao",
            "affiliations": [
                "University of British Columbia"
            ]
        },
        {
            "id": "https://openalex.org/A5025153128",
            "name": "Lanjun Wang",
            "affiliations": [
                "Huawei Technologies (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A5046714396",
            "name": "Hyeju Jang",
            "affiliations": [
                "University of British Columbia"
            ]
        },
        {
            "id": "https://openalex.org/A5049259877",
            "name": "Giuseppe Carenini",
            "affiliations": [
                "University of British Columbia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3038035611",
        "https://openalex.org/W2562979205",
        "https://openalex.org/W2963214037",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W3154594410",
        "https://openalex.org/W2605409611",
        "https://openalex.org/W3009469321",
        "https://openalex.org/W2972458663",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2607223307",
        "https://openalex.org/W2962933129",
        "https://openalex.org/W2888685797",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W3021934057",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2777662428",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2553981914",
        "https://openalex.org/W2965862774",
        "https://openalex.org/W2970863760",
        "https://openalex.org/W2135415614",
        "https://openalex.org/W2952552241",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2142493242",
        "https://openalex.org/W2903996579",
        "https://openalex.org/W2089468765",
        "https://openalex.org/W2516809705",
        "https://openalex.org/W2786672974",
        "https://openalex.org/W3103368673",
        "https://openalex.org/W3101662419",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W3116527904",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2951013084",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2971033911",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W3145755065",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W3103649165",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2657631929"
    ],
    "abstract": "Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model's intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (e.g., hidden states, attention) through interactive visualization, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the framework is useful, and suggest several improvements. Our framework is available at: https://github.com/raymondzmc/T3-Vis.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 220–230\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n220\nT3-Vis: a visual analytic framework for Training and ﬁne-Tuning\nTransformers in NLP\nRaymond Li†, Wen Xiao†, Lanjun Wang‡∗, Hyeju Jang†, Giuseppe Carenini†\n†University of British Columbia, Vancouver, Canada\n{raymondl, xiaowen3, hyejuj, carenini}@cs.ubc.ca\n‡Huawei Cananda Technologies Co. Ltd., Burnaby, Canada\nlanjun.wang@huawei.com\nAbstract\nTransformers are the dominant architecture in\nNLP, but their training and ﬁne-tuning is still\nvery challenging. In this paper, we present\nthe design and implementation of a visual an-\nalytic framework for assisting researchers in\nsuch process, by providing them with valu-\nable insights about the model’s intrinsic prop-\nerties and behaviours. Our framework offers\nan intuitive overview that allows the user to\nexplore different facets of the model (e.g., hid-\nden states, attention) through interactive visu-\nalization, and allows a suite of built-in algo-\nrithms that compute the importance of model\ncomponents and different parts of the input\nsequence. Case studies and feedback from\na user focus group indicate that the frame-\nwork is useful, and suggest several improve-\nments. Our framework is available at:https:\n//github.com/raymondzmc/T3-Vis.\n1 Introduction\nApproaches through neural networks have made\nsigniﬁcant progress in the ﬁeld of NLP, with Trans-\nformer models (Vaswani et al., 2017) rapidly be-\ncoming the dominant architecture due to their\nefﬁcient parallel training and ability to effec-\ntivelymodel long sequences. Following the release\nof BERT (Devlin et al., 2019) along with other\nTransformer-based models pretrained on large cor-\npora (Liu et al., 2019; Lewis et al., 2020; Joshi\net al., 2020; Lee et al., 2020), the most successful\nstrategy on many NLP leaderboards has been to\ndirectly ﬁne-tune such models on the downstream\ntasks (e.g., summarization, classiﬁcation). How-\never, despite the strong empirical performance of\nthis strategy, understanding and interpreting the\ntraining and ﬁne-tuning processes remains a criti-\ncal and challenging step for researchers due to the\ninherent black-box nature of neural models (Koval-\neva et al., 2019; Hao et al., 2019; Merchant et al.,\n2020; Hao et al., 2020).\n∗Corresponding author.\nGenerally speaking, a large number of visual\nanalytics tools have been shown to effectively sup-\nport the analysis and interpretation of deep learn-\ning models (Hohman et al., 2018). For instance,\nto remedy the black-box nature of neural network\nhidden states, previous work has used scatterplots\nto visualize high dimensional vectors through pro-\njection techniques (Smilkov et al., 2016; Kahng\net al., 2017), with Aken et al. (2020) visualizing\nthe differences of token representations from differ-\nent layers of BERT (Devlin et al., 2019). Similarly,\ndespite some limitations regarding the explanatory\ncapabilities of the attention mechanism (Jain and\nWallace, 2019; Wiegreffe and Pinter, 2019), the\nvisualization of its weights has also been shown to\nbe beneﬁcial in discovering learnt features (Clark\net al., 2019; V oita et al., 2019), with promising re-\ncent work focusing on Transformers (Vig, 2019;\nHoover et al., 2020).\nBesides the works on exploring what has been\nlearnt in the pretrained models, there are also sev-\neral visualization tools developed to show saliency\nscores generated by gradient-based (Simonyan\net al., 2013; Bach et al., 2015; Shrikumar et al.,\n2017) or perturbation-based interpretation meth-\nods (Ribeiro et al., 2016; Li et al., 2016), which\ncan help with visualizing the relative importance\nof individual tokens in the input with respect to\na target prediction (Wallace et al., 2019; Johnson\net al., 2020; Tenney et al., 2020). However, only a\nfew studies have instead focused on visualizing the\noverall training dynamics, where support is critical\nfor identifying mislabeled examples or failure cases\n(Liu et al., 2018; Xiang et al., 2019; Swayamdipta\net al., 2020)\nIn essence, the framework we propose in this pa-\nper, namely T3-Vis, synergistically integrates some\nof the interactive visualizations mentioned above\nto support developers in the challenging task of\ntraining and ﬁne-tuning Transformers. This is in\ncontrast with other similar recent visual tools (Ta-\n221\nFigure 1: Overview of the interface: (A) Projection View provides a 2D visualization of the dataset by encoding\neach example as a point on the scatterplot; (B) Data Table allows the user to view the content and metadata (e.g.\nlabel, loss) of the data examples (e.g. document); (C) Attention Head View visualizes the head importance and\nweight matrices of each attention head;(D) Instance Investigation View allows the user to perform detailed analysis\n(e.g. interpretation, attention) on a data example’s input sequence.\nble 1), which either only focus on single data point\nexplanations for uncovering model bias (e.g., Al-\nlenNLP Interpret (Wallace et al., 2019)), or rely\non failed examples to understand the model’s be-\nhaviour (e.g., Language Interpretability Tool (LIT)\n(Tenney et al., 2020)).\nFollowing the well-established Nested Model\nfor visualization design (Munzner, 2009), we ﬁrst\nperform an extensive requirement analysis, from\nwhich we derive user tasks and data abstractions to\nguide the design of visual encoding and interaction\ntechniques. More speciﬁcally, the resulting T 3-\nVis framework provides an intuitive overview that\nallows users to explore different facets of the model\n(e.g., hidden states, attention, training dynamics)\nthrough interactive visualization.\nOur contributions are as follows: (1) An exten-\nsive user requirement analysis on supporting the\ntraining and ﬁne-tuning of Transformer models,\nbased on extensive literature review and interviews\nwith NLP researchers,(2) the design and implemen-\ntation of an open-sourced visual analytic frame-\nwork for assisting researchers in the ﬁne-tuning\nprocess with a suite of built-in interpretation meth-\nods that analyze the importance of model compo-\nnents and different parts of the input sequence, and\n(3) the evaluation of the current design from case\nstudies with NLP researchers and feedback from a\nuser focus group.\n2 Visualization Design\nThe design of our T 3-Vis is based on the nested\nmodel for InfoVis design (Munzner, 2009).\n2.1 User Requirements\nTo derive useful analytical user tasks, we ﬁrst iden-\ntify a set of high-level user requirements (UR)\nthrough interviews with ﬁve NLP researchers as\nwell as surveying recent literature related to the\ninterpretability and the ﬁne-tuning procedures of\npretrained Transformers. In the interviews, we\nprompt participants with the open-ended question\nof \"If a visualization tool is provided to speed up\nyour development (ﬁne-tuning pretrained Trans-\nformers), what information would you like to see\nand explore?\". Combining the interview results\nand insights from the literature review, we organize\nthese ﬁndings into ﬁve high-level requirements,\neach highlighting a different facet of the model for\nvisualization.\nHidden state visualization (UR-1) : Support\nthe exploration for hidden state representations\n222\nFrameworks Components Functions\nDatasetEmbeddings Head\nImportanceAttentionTraining\nDynamicsInterpretationsPruningComparison\nBertViz (Vig, 2019) ✓\nAllenNLP Interpret\n(Wallace et al., 2019) ✓\nexBERT (Hoover et al., 2020)✓ ✓ ✓\nLIT (Tenney et al., 2020)✓ ✓ ✓ ✓ ✓\nInterperT (Lal et al., 2021)✓ ✓ ✓ ✓\nT3-Vis ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓\nTable 1: Comparison with other visual frameworks from recent work.\nfrom the model.\nAttention visualization (UR-2) : Allow users\nto examine and explore the linguistic or positional\npatterns exhibited in the self-attention distribution\nfor different attention heads in the model.\nAttention head importance (UR-3) : Enable\nusers to investigate and understand the importance\nof the attention heads for the downstream task\nand the effects of pruning them on the model’s\nbehaviour.\nInterpretability of models (UR-4): In addition\nto attention maps, support a suite of alternative\nexplanation methods based on token input impor-\ntance, thus allowing users to better understand the\nmodel behaviours during inference.\nTraining dynamics (UR-5) : Assist users in\nidentifying relevant data examples based on their\nroles in the training process.\n2.2 Supported Tasks and Data Model\nBased on these user requirements, we derive nine\nanalytical tasks framed as information seeking\nquestions . In Table 2, we list the tasks along with\nimportant attributes including: When they are rele-\nvant during the ﬁne-tuning process, theGranularity\nof the data that it operates on, corresponding User\nRequirements, and the framework Components that\nit pertains to. We then look at the speciﬁc data to\nwhich the tasks are applied to. We characterize our\ndata model (i.e. data types visualized by the inter-\nface) as comprising the model hidden states, the\ndataset examples along with their label/training fea-\ntures, the attention values, head importance scores,\nand input saliency map. Although our task and data\nmodels are derived for the ﬁne-tuning of pretrained\nmodels, they can naturally be extended to training\nany Transformer models from scratch. Importantly,\nall the questions are invariant to any Transformer-\nbased models for any downstream tasks (e.g. clas-\nsiﬁcation, sequence-generation or labeling).\n2.3 T 3-Vis Components: Visual Encoding\nand Interactive Techniques\nProjection View: To assist users in visualizing the\nmodel’s hidden state representation (UR-1) and to\nidentify the training role of the data examples (UR-\n5), we design the Projection View (Figure 1-(A))\nas the main overview of our interface, and visual-\nize the entire (or a subset of the) dataset on a 2D\nscatterplot, where each data point on the plot en-\ncodes a single data example (e.g. document) in the\ndataset. While the scatterplots can be generated\nin a variety of ways based on the user’s needs, in-\ncluding dimension reduction methods (Wold et al.,\n1987; McInnes et al., 2018) and plotting based on\ntraining dynamics (Li et al., 2018; Toneva et al.,\n2019). Detailed studies examining the effectiveness\nof these methods in the context of visual analytics\nare out of the scope of this paper, but provide a\npromising direction for future work. In T 3-Vis,\nwe provide two implementations (See Figure 2):\n(1) t-SNE projection (Van der Maaten and Hinton,\n2008) of the model’s hidden states, and (2) plotting\nthe examples by their conﬁdence and variability\nacross epochs based on the Data Map technique\n(Swayamdipta et al., 2020). The color of the data\npoints can be selected by the user via a dropdown\nmenu to encode attributes of the data examples,\nwhere color saturation is used for continuous at-\ntributes (e.g. loss, prediction conﬁdence), while\nhue is used for categorical attributes (e.g. labels,\nprediction). The user can also ﬁlter the data points\nby attributes, where a range slider is used for ﬁlter-\ning the data points by continuous attributes, while\na selectable dropdown menu is used to ﬁlter by cat-\negorical attributes. Furthermore, we also introduce\na comparison mode by displaying the two scatter-\nplots side-by-side, which allows for the ﬂexibility\nof comparing across different checkpoints and the\nprojection of different hidden state layers.\nData Table: The Data Table (Figure 1-(B)) lists\n223\n# Question When GranularityUser RequirementsComponents\nT1 How to determine the modelrepresentation for a given NLP task? Before Dataset 1 Projection\nT2 What are the outliers of the dataset?Before, During, AfterDataset 1, 5 Projection\nT3\nWhat types of linguistic or positional attributes do\nthe attention patterns exhibit for each attention heads?Before, During, AfterInstance 2 Projection\nT4\nWhich attention heads are considered important\nfor the task, and what are its functions? After Both 2, 3 Attention HeadInstance Investigator\nT5 How does pruning attention heads affects the model?After Instance 3 Attention Head\nT6\nHow does the model changes at\ndifferent stages of ﬁne-tuning? During, After Both 1, 2, 3 All\nT7\nDoes the model rely on speciﬁc parts of the\ninput sequence when making predictions?After Instance 4 Instance Investigator\nT8 Are there mislabeled examples in the dataset?During, After Both 1, 5 Projection\nT9\nHow can the dataset be augmented to improve\nthe performance and robustness of the model?During, After Both 5 Projection\nTable 2: Supported analytical tasks: questions that our interface helps to answer.\nFigure 2: Interactive scatterplots based on the data ex-\namples’ training dynamics (left), and the t-SNE projec-\ntions of hidden states (right)\nall examples of the dataset in a single scrollable list,\nwhere each entry displays the input text of a data\nexample along with its ground truth label. When\nthe user ﬁlters the dataset in the Projection View,\nthe Data Table is also ﬁltered simultaneously.\nAttention Head View: In order to visualize the\nimportance of the model’s attention heads (UR-3),\nas well as the patterns in the attention weight ma-\ntrices (UR-2), we design the Attention Head View\n(Figure 1-(C)), where each block in thel×h matrix\n(l layers and h heads) represents a single attention\nhead at the respective index for layer and head. In\nthis view, we provide two separate visualization\ntechniques: namely (1) Head Importance and (2)\nAttention Pattern, that can be switched using a tog-\ngle button. The Head Importance technique visual-\nizes the normalized task-speciﬁc head importance\nscore 1 through the background color saturation and\ndisplayed value of the corresponding matrix block\n(See Figure 3a). On the other hand, the Attention\nPattern technique uses heatmaps to visualize the\nmagnitude of the associated self-attention weight\nmatrices (See Figure 3b). We also provide a tog-\ngle button for the user to visualize the importance\n1Details are in A.1 of the Appendix\n(a) Head Importance\n (b) Attention Pattern\nFigure 3: The two visualization techniques in the At-\ntention Head View.\nscore and attention patterns on two scales, where\nthe aggregate-scale visualizes the score and pat-\nterns averaged over the entire dataset, while the\ninstance-scale visualizes the score and patterns for\na selected data example. Lastly, we also offer an\ninteractive technique for the user to dynamically\nprune attention heads and visualize the effects on a\nselected example. By hovering over each attention\nhead block in the view, the user can click on the\nclose icon to prune the respective attention head\nfrom the model.\nInstance Investigation View: After the user se-\nlects a data example from the Projection View or\nData Table, the Instance Investigation View (Fig-\nure 1-(D)) renders the corresponding input text se-\nquence along with the model predictions and labels\nto allow the user to perform detailed analysis on\nthe data example. In this view, each token of the\ninput sequence is displayed in a separate text block,\nwhere the background color saturation of each text\nblock encodes the relative saliency or importance\nof the token based on the interpretation methods.\nOur interface provides two analysis techniques: (1)\nBy selecting a head in the Attention Head View\n(Figure 3), the user can click on the text block of\nany input token to visualize the self-attention dis-\n224\ntribution of the selected token over the input text\nsequence (UR-3). (2) Similarly, the user can visu-\nalize the input saliency map with respect to a model\noutput, by clicking the corresponding output token\n(UR-4). Since our framework allows the user to\nplug in different interpretation techniques based on\ntheir preference, details regarding the meaningful-\nness of such techniques are out of the scope of this\npaper. Our interface provides the implementation\nof two input interpretation methods2 : Layer-wise\nrelevance propagation (Bach et al., 2015), and input\ngradient (Simonyan et al., 2013).\n2.4 Implementation\nData Processing For each model checkpoint,\ndata pertaining to dataset-level visualizations in-\ncluding hidden state projections, prediction conﬁ-\ndence/variability, head importance score, and other\nattributes (e.g. loss, prediction) are ﬁrst processed\nand saved in a back-end directory. The only added\ncomputational overhead to the user’s training pro-\ncess is the dimension reduction algorithm for pro-\njecting hidden state representation, as other visual-\nized values can all be extracted from the forward\n(e.g. conﬁdence, variability, loss) and backward\npass (e.g. head importance, input saliency) of\nmodel training.\nBack-end Our back-end Python server provides\nbuilt-in support for the PyTorch HuggingFace li-\nbrary (Wolf et al., 2020), including methods for\nextracting attention values, head pruning, comput-\ning importance scores, and interpreting the model\npredictions. In order to avoid saving instance-level\ndata (e.g., attention weights, input heatmap, etc.)\nfor all examples in the dataset, the server dynam-\nically computes these values for a selected data\nexample by performing a single forward and back-\nward pass on the model. This requires the server to\nkeep track of the model’s current state, as well as a\ndataloader for indexing the selected data example.\nFront-end Our front-end implementation keeps\ntrack of the current visual state of the interface\nincluding the selections, ﬁlters, and checkpoint.\nThe interface can be accessed through any web\nbrowser, where data is retrieved from the back-end\nserver via the RESTful API. The interactive visual\ncomponents of the interface are implemented using\nthe D3.js framework (Bostock et al., 2011), and\nother UI components (e.g. buttons, sliders) are\n2Details are in A.2 of the Appendix\nimplemented with popular front-end libraries (e.g.\njQuery, Bootstrap).\n3 Iterative Design\n3.1 Focus Group Study\nIn order to collect suggestions and initial feedback\non T3-Vis, we conducted a focus group study with\n20 NLP researchers that work regularly with pre-\ntrained Transformer models. In this study, we ﬁrst\npresented the design of the interface, then gave a\ndemo showing its usage on an example. Through-\nout the process, we gathered responses from the\nparticipants via open discussions.\nMost positive feedback focused on the effective-\nness of our techniques for visualizing self-attention\nespecially on longer documents (in contrast to\nshowing links between tokens (Vig, 2019)). There\nwere also comments on the usefulness of the input\nsaliency map in providing insightful clues on the\nmodel’s decision process.\nSome participants also suggested that the inter-\nface would be more useful for classiﬁcation prob-\nlems with well-deﬁned evaluation metrics since\ndata examples tended to be better clustered in the\nProjection View so that they could be easily ﬁl-\ntered for error analysis. The need of optimizing the\nfront-end to support the visualization of large-scale\ndatasets was also mentioned.\nOn the negative side, some participants were\nconcerned by the information loss intrinsic in the\ndimension reduction methods, whose possible neg-\native effects on the user analysis tasks deﬁnitely\nrequires further study. Encouragingly, at the end, a\nfew participants expressed interest in applying and\nevaluating T3-Vis on their datasets and NLP tasks.\n3.2 Case Studies\nThis section describes two case studies of how T3-\nVis facilitates the understanding and exploration of\nthe ﬁne-tuning process through applications with\nreal-world corpora. These studies provide initial\nevidence on the effectiveness of different visualiza-\ntion components, and serve as examples for how\nour framework can be used.\n3.2.1 Pattern Exploration for an Extractive\nSummarizer\nNLP researchers in our group, who work on sum-\nmarization, applied T3-Vis to the extractive summa-\nrization task, which aims to compress a document\n225\nFigure 4: The self-attention distribution of token\n“photo” in theInstance Analysis View.\nby selecting its most informative sentences. BERT-\nSum, which is ﬁne-tuned from a BERT model (Liu\nand Lapata, 2019), is one of the top-performing\nmodels for extractive summarization, but why and\nhow it works remains a mystery. With our inter-\nface, the researchers explored patterns captured\nby the model that played important roles in model\npredictions. They performed an analysis on the\nCNN/Daily Mail dataset (Hermann et al., 2015),\nwhich is arguably the most popular benchmark for\nsummarization tasks.\nThe ﬁrst step was to ﬁnd the important heads\namong all the heads across all the layers. From\nthe Head Importance View (Figure 1-(C)), the re-\nsearchers selected the attention heads with high\nhead importance scores, so that the corresponding\nattention distribution was available to interact with.\nThen they selected some tokens in the Attention\nView to see which tokens they mostly attended to,\nand repeated this process for multiple other data\nexamples, in order to explore whether there was a\ngeneral pattern across different data examples.\nWhile examining attention heads based on their\nimportance in descending order, the researchers\nobserved that tokens tended to have high attention\non other tokens of the same word on the important\nattention heads. For example, the token “photo\"\nattributed almost all of its attention score to other\ninstances of the token “photo\" in the source doc-\nument (Figure 4). They further found two more\npatterns in other important heads, in which the to-\nkens tended to have more attention on the tokens\nwithin the same sentence, as well as the adjacent\ntokens. These behaviours were consistent across\ndifferent pretrained models, such as RoBERTa (Liu\net al., 2019).\nThese ﬁndings provided useful insights to as-\nsist the researchers in designing more efﬁcient and\naccurate summarization models in the future, and\nserved as a motivation for the researchers to per-\nform similar analysis for other NLP tasks.\nFigure 5: A misclassiﬁed example within a cluster of\nwell-classiﬁed example.\n3.2.2 Error Analysis for Topic Classiﬁcation\nOther researchers in our group explored the inter-\nface for error analysis to identify possible improve-\nments of a BERT-based model for topic classiﬁ-\ncation. The Yahoo Answers dataset (Zhang et al.,\n2015) was used, which contains 10 topic classes.\nResearchers ﬁrst used the Projection View (Fig-\nure 1-(A)) to ﬁnd misclassiﬁed data examples as ap-\nplying ﬁlters to select label and prediction classes.\nFor a selected topic class in the t-SNE projection\nof the model’s hidden states, they found out that\nthe misclassiﬁed data points far away from clusters\nof correctly predicted examples were often misla-\nbeled during annotation. Therefore, misclassﬁed\ndata points within such clusters were of greater in-\nterest to them since such points tends to indicate\nmodel failuresrather than mistakes in annotation\n(Figure 5). Furthermore, data points in the area\nwith low variability and low conﬁdence on the Data\nMap plot were also selected for investigation since\nthey are interpreted as consistently misclassiﬁed\nacross epochs. After selecting the examples, the\nresearchers inspected each instance by using the\nInstance Investigation View (Figure 1-(D)) with\nthe Input Gradient method to visualize the input\nsaliency map for the prediction of each class.\nFrom this analysis, they discovered two scenar-\nios that led to misclassiﬁcation. First, the model\nfocused on unimportant and possibly misleading\ndetails that are not representative of the document’s\noverall topic. For instance, a document about\nBusiness & Finance was classiﬁed into the Sport\ncategory because the model attended to “hockey\nplayer”, “football player”, and “baseball player”,\nwhich were listed as job titles while discussing\navailable jobs in Michigan. Second, the model\nfailed in cases where background knowledge is\nrequired. For example, a document under the En-\ntertainment & Music category mentioned names of\ntwo actors which were key clues for the topic, but\n226\nthe model only attended to other words, and made\na wrong prediction.\nThese ﬁndings helped researchers to gain in-\nsights for future model design where additional\ninformation such as discourse structure (which can\nbetter reveal importance) and encyclopedic knowl-\nedge could be injected into the model’s architecture\nto improve the task performance.\n4 Conclusion\nIn this paper, we presented T 3-Vis, a visual ana-\nlytic framework designed to help researchers better\nunderstand training and ﬁne-tuning processes of\nTransformer-based models. Our visual interface\nprovides faceted visualization of a Transformer\nmodel and allows exploring data across multiple\ngranularities, while enabling users to dynamically\ninteract with the model. Additionally, our imple-\nmentation and design allows ﬂexible customization\nto support a diverse range of tasks and workﬂows.\nOur focus group and case studies demonstrated\nthe effectiveness of our interface by assisting the\nresearchers in interpreting the models’ behaviour\nand identifying potential directions to improve task\nperformances.\nFor future work, we will continue to improve\nour framework through the iterative process of ex-\nploring further usage scenarios and collecting feed-\nback from users. We will extend our framework\nto provide a more advanced visualization for cus-\ntom Transformers. For example, we may want\nto support the visualization of models with more\ncomplex connections (e.g. parallel attention layers)\nor an advanced attention mechanism (e.g. sparse\nattention).\n227\nReferences\nBetty van Aken, Benjamin Winter, Alexander Löser,\nand Felix A Gers. 2020. Visbert: Hidden-state visu-\nalizations for transformers. In Companion Proceed-\nings of the Web Conference 2020, pages 207–211.\nSebastian Bach, Alexander Binder, Grégoire Mon-\ntavon, Frederick Klauschen, Klaus-Robert Müller,\nand Wojciech Samek. 2015. On pixel-wise explana-\ntions for non-linear classiﬁer decisions by layer-wise\nrelevance propagation. PloS one, 10(7).\nMichael Bostock, Vadim Ogievetsky, and Jeffrey Heer.\n2011. D 3 data-driven documents. IEEE trans-\nactions on visualization and computer graphics ,\n17(12):2301–2309.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2020. Inves-\ntigating learning dynamics of BERT ﬁne-tuning. In\nProceedings of the 1st Conference of the Asia-Paciﬁc\nChapter of the Association for Computational Lin-\nguistics and the 10th International Joint Confer-\nence on Natural Language Processing, pages 87–92,\nSuzhou, China. Association for Computational Lin-\nguistics.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Informa-\ntion Processing Systems , volume 28, pages 1693–\n1701. Curran Associates, Inc.\nFred Hohman, Minsuk Kahng, Robert Pienta, and\nDuen Horng Chau. 2018. Visual analytics in deep\nlearning: An interrogative survey for the next fron-\ntiers. IEEE transactions on visualization and com-\nputer graphics, 25(8):2674–2693.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2020. exBERT: A Visual Analysis Tool\nto Explore Learned Representations in Transformer\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 187–196, Online. As-\nsociation for Computational Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nDavid Johnson, Giuseppe Carenini, and Gabriel Mur-\nray. 2020. Njm-vis: interpreting neural joint models\nin nlp. In Proceedings of the 25th International Con-\nference on Intelligent User Interfaces , pages 286–\n296.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nMinsuk Kahng, Pierre Y Andrews, Aditya Kalro, and\nDuen Horng Polo Chau. 2017. Activis: Visual ex-\nploration of industry-scale deep neural network mod-\nels. IEEE transactions on visualization and com-\nputer graphics, 24(1):88–97.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nVasudev Lal, Arden Ma, Estelle Aﬂalo, Phillip Howard,\nAna Simoes, Daniel Korat, Oren Pereg, Gadi Singer,\nand Moshe Wasserblat. 2021. InterpreT: An interac-\ntive visualization tool for interpreting transformers.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: System Demonstrations , pages 135–\n142, Online. Association for Computational Linguis-\ntics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\n228\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and\nTom Goldstein. 2018. Visualizing the loss landscape\nof neural nets. In Neural Information Processing\nSystems.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nDongyu Liu, Weiwei Cui, Kai Jin, Yuxiao Guo, and\nHuamin Qu. 2018. Deeptracker: Visualizing the\ntraining process of convolutional neural networks.\nACM Transactions on Intelligent Systems and Tech-\nnology (TIST), 10(1):1–25.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nL. McInnes, J. Healy, and J. Melville. 2018. UMAP:\nUniform Manifold Approximation and Projection\nfor Dimension Reduction. ArXiv e-prints.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT em-\nbeddings during ﬁne-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP , pages 33–44,\nOnline. Association for Computational Linguistics.\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri\nFrosio, and Jan Kautz. 2019. Importance estimation\nfor neural network pruning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 11264–11272.\nGrégoire Montavon, Wojciech Samek, and Klaus-\nRobert Müller. 2018. Methods for interpreting and\nunderstanding deep neural networks. Digital Signal\nProcessing, 73:1–15.\nTamara Munzner. 2009. A nested model for visualiza-\ntion design and validation. IEEE transactions on vi-\nsualization and computer graphics, 15(6):921–928.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explain-\ning the predictions of any classiﬁer. In Proceed-\nings of the 22nd ACM SIGKDD international con-\nference on knowledge discovery and data mining ,\npages 1135–1144.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017. Learning important features through\npropagating activation differences. In International\nConference on Machine Learning, pages 3145–3153.\nPMLR.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2013. Deep inside convolutional networks: Vi-\nsualising image classiﬁcation models and saliency\nmaps. arXiv preprint arXiv:1312.6034.\nDaniel Smilkov, Nikhil Thorat, Charles Nicholson,\nEmily Reif, Fernanda B Viégas, and Martin Watten-\nberg. 2016. Embedding projector: Interactive visu-\nalization and interpretation of embeddings. arXiv\npreprint arXiv:1611.05469.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A.\nSmith, and Yejin Choi. 2020. Dataset cartography:\nMapping and diagnosing datasets with training dy-\nnamics. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 9275–9293, Online. Associa-\ntion for Computational Linguistics.\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga\nBolukbasi, Andy Coenen, Sebastian Gehrmann,\nEllen Jiang, Mahima Pushkarna, Carey Radebaugh,\nEmily Reif, and Ann Yuan. 2020. The language in-\nterpretability tool: Extensible, interactive visualiza-\ntions and analysis for NLP models. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions, pages 107–118, Online. Association for Com-\nputational Linguistics.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J. Gordon. 2019. An empirical study of exam-\nple forgetting during deep neural network learning.\nIn International Conference on Learning Represen-\ntations.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the transformer model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n37–42, Florence, Italy. Association for Computa-\ntional Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n229\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nEric Wallace, Jens Tuyls, Junlin Wang, Sanjay Sub-\nramanian, Matt Gardner, and Sameer Singh. 2019.\nAllenNLP interpret: A framework for explaining\npredictions of NLP models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP): System Demonstrations , pages\n7–12, Hong Kong, China. Association for Compu-\ntational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 11–20, Hong Kong, China. Associ-\nation for Computational Linguistics.\nSvante Wold, Kim Esbensen, and Paul Geladi. 1987.\nPrincipal component analysis. Chemometrics and\nintelligent laboratory systems, 2(1-3):37–52.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nShouxing Xiang, Xi Ye, Jiazhi Xia, Jing Wu, Yang\nChen, and Shixia Liu. 2019. Interactive correction\nof mislabeled training data. In 2019 IEEE Confer-\nence on Visual Analytics Science and Technology\n(VAST), pages 57–68. IEEE.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. Advances in neural information process-\ning systems, 28:649–657.\n230\nA Appendix\nA.1 Head Importance Score\nAlthough the multi-head self attention mechanism\nin Transformers allows the model to learn multi-\nple types of relationships between input representa-\ntions across a single hidden layer, the importance of\nthe individual attention heads can vary depending\non the downstream tasks. Following previous work,\nwe adapt the Taylor expansion method (Molchanov\net al., 2019) to estimate the error induced from\nremoving a group of parameters from the model.\nIn our implementation, we use the ﬁrst-order ex-\npansion to avoid the overhead from computing the\nHessian, where the gradient with respect to vali-\ndation loss is summed over all parameters of an\nattention head to estimate its importance.\nA.2 Input Interpretation\nInput Gradients The input gradient method (Si-\nmonyan et al., 2013) computes the gradient with\nrespect to each token. During inference, the class-\nscore derivative can be computed through back-\npropagation. The saliency of the token xi for class\nc of output y could therefore be estimated using the\nﬁrst-order Taylor expansion ∂yc\n∂xi\nxi.\nLayer-wise Relevance Propagation Layer-\nwise Relevance Propagation (LRP) (Bach et al.,\n2015) was originally proposed to visualize the\ncontributions of single pixels to predictions for\nan image classiﬁer. By recursively computing\nrelevance from the output layer to the input layer,\nLRP is demonstrated to be useful in unravelling the\ninference process of neural networks and has been\nadopted in recent work to analyze Transformer\nmodels (V oita et al., 2019). The intuition behind\nLRP is that, each neuron of the network is\ncontributed by neurons in the previous layer, and\nthe total amount of contributions for each layer\nshould be a constant during back-propagating,\nwhich is called the conservation principle. LRP\noffers ﬂexibility to design propagation rules to\nexplain various deep neural networks, one example\npropagation rule is shown as follows (Montavon\net al., 2018),\nRi = Σj\naiwij\nΣiaiwij\nRj (1)\nwhere Ri and Rj are relevance scores of two neu-\nrons in consecutive layers, ai is the respective acti-\nvation for neuron i, and wij is the weight between\nneuron i and j."
}