{
  "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories",
  "url": "https://openalex.org/W4385569751",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222674294",
      "name": "Diao, Shizhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A583435494",
      "name": "Xu TianYang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3175415943",
      "name": "Xu Ruijia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A714200083",
      "name": "Wang Jia-wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093305776",
      "name": "Zhang Tong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W4294554825",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W3101893044",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4297971002",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2972119829",
    "https://openalex.org/W3192523796",
    "https://openalex.org/W4385567084",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4287235428",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3205972749",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3098266846",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4301518445",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3176574162",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4307934937",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385573474",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2792643794",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability, scalability, and efficiency of our method. © 2023 Association for Computational Linguistics.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5113–5129\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMixture-of-Domain-Adapters: Decoupling and Injecting Domain\nKnowledge to Pre-trained Language Models’ Memories\nShizhe Diao♡˚, Tianyang Xu ♠˚, Ruijia Xu ♡, Jiawei Wang ♣, Tong Zhang ♡\n♡The Hong Kong University of Science and Technology\n{sdiaoaa, rxuaq, tongzhang}@ust.hk\n♠Wuhan University\ntianyangxu@whu.edu.cn\n♣Shanghai Jiao Tong University\nwjw_sjt@sjtu.edu.cn\nAbstract\nPre-trained language models (PLMs) demon-\nstrate excellent abilities to understand texts in\nthe generic domain while struggling in a spe-\ncific domain. Although continued pre-training\non a large domain-specific corpus is effective,\nit is costly to tune all the parameters on the\ndomain. In this paper, we investigate whether\nwe can adapt PLMs both effectively and effi-\nciently by only tuning a few parameters. Specif-\nically, we decouple the feed-forward networks\n(FFNs) of the Transformer architecture into two\nparts: the original pre-trained FFNs to main-\ntain the old-domain knowledge and our novel\ndomain-specific adapters to inject domain-\nspecific knowledge in parallel. Then we adopt a\nmixture-of-adapters gate to fuse the knowledge\nfrom different domain adapters dynamically.\nOur proposed Mixture-of-Domain-Adapters\n(MixDA) employs a two-stage adapter-tuning\nstrategy that leverages both unlabeled data\nand labeled data to help the domain adap-\ntation: i) domain-specific adapter on unla-\nbeled data; followed by ii) the task-specific\nadapter on labeled data. MixDA can be seam-\nlessly plugged into the pretraining-finetuning\nparadigm and our experiments demonstrate\nthat MixDA achieves superior performance\non in-domain tasks (GLUE), out-of-domain\ntasks (ChemProt, RCT, IMDB, Amazon), and\nknowledge-intensive tasks (KILT). Further\nanalyses demonstrate the reliability, scalabil-\nity, and efficiency of our method.1\n1 Introduction\nPre-trained language models (PLMs) have achieved\na multitude of successful applications in natural\nlanguage understanding (Devlin et al., 2018; Liu\net al., 2019; He et al., 2021b) and generation (Lewis\net al., 2019; Zhang et al., 2020; Yang et al., 2020;\nBrown et al., 2020). The predominant methodol-\nogy for domain adaptation is fine-tuning on labeled\n*Equal Contribution.\n1The code is available at https://github.com/\nAmano-Aki/Mixture-of-Domain-Adapters .\ndomain-specific data or continued pre-training (Gu-\nrurangan et al., 2020) on unlabeled domain-specific\ndata. Although effective, both fine-tuning and con-\ntinued pre-training methods require tuning all the\nparameters of a PLM, raising high costs beyond\nmany institutions’ reach. To mitigate this, multi-\nple parameter-efficient fine-tuning (PEFT) methods\nare proposed, including prompt-based tuning (Gao\net al., 2021; Liu et al., 2021b; Schick and Schütze,\n2021; Li and Liang, 2021; Liu et al., 2021a), and\nadapter-based tuning (Houlsby et al., 2019; Pfeif-\nfer et al., 2020b; Hu et al., 2021). However, they\nare more concerned about task adaptation and it\nis still unclear how to regularly, and inexpensively\ninject domain knowledge into PLMs for different\ndomain-specific tasks. Moreover, directly tuning\nPLMs on a domain-specific corpus with PEFT\nmethods will lead to the catastrophic forgetting\nproblem (Yogatama et al., 2019; Gururangan et al.,\n2020). These limitations highlight an important re-\nsearch question: how to adapt PLMs with the new\ndomain knowledge while keeping the old-domain\nknowledge unmodified?\nInspired by the recent studies (Geva et al., 2021;\nCao et al., 2021; Meng et al., 2022) that found\nknowledge is stored in feed-forward networks\n(FFNs), we decouple the FFNs into two parts:\nthe original pre-trained FFNs to maintain the old-\ndomain knowledge and our novel domain-specific\nadapters to inject domain-specific knowledge in\nparallel. Specifically, we propose Mixture-of-\nDomain-Adapters (MixDA), a mixture of several\ndomain adapters to inject domain-specific knowl-\nedge without affecting the old-domain knowledge.\nOur model has two stages: piq domain-specific\ntuning multiple knowledge adapters on unlabeled\ndata and then piiqtask-specific tuning adapters on\nlabeled data. In the first stage, we train several do-\nmain adapters on both domain-specific corpus and\npre-training corpus simultaneously while keeping\nthe original feed-forward networks unchanged. In\n5113\nthe second stage, we train a mixture-of-adapters\ngate to dynamically select the desired knowledge\nadapter and a task-specific adapter for task adapta-\ntion.\nWe conduct experiments on a broad range\nof tasks, including 4 out-of-domain datasets, 9\nin-domain datasets, and 2 knowledge-intensive\ndatasets. Our experimental results demonstrate\nthe effectiveness of MixDA on 15 datasets, span-\nning biomedical, computer science publications,\nnews, and reviews. Further analysis displays three\nkey properties of our proposed approach: piqRe-\nliability: it shows superior performance on both\nin-domain and out-of-domain tasks. piiqScala-\nbility: it scales well to the increasing number of\ndomains. piiiqEfficiency: it adds only a small\nnumber of parameters per domain. We claim that\nthese properties are helpful for language models\nas a service, where a frozen PLM is served, and\nmultiple adapters are inserted to support different\ncustomized services.\n2 Related Work\nIn this section, we will review four research lines\nrelated to injecting domain knowledge into pre-\ntrained language models: knowledge injection, do-\nmain adaptation, parameter-efficient fine-tuning,\nand mixture-of-adapters.\n2.1 Knowledge Injection\nKnowledge can be injected into PLMs by pre-\ntraining or fine-tuning, each corresponding to a sep-\narate research direction. During pre-training, the\nknowledge carried by knowledge graphs (Zhang\net al., 2019; He et al., 2020), entities (Sun et al.,\n2019; Xiong et al., 2020), n-grams (Diao et al.,\n2020), knowledge embedding (Wang et al., 2021b),\nsynonym and hyponym-hypernym relations in\nWordNet (Lauscher et al., 2019), word-supersense\nknowledge (Levine et al., 2020), and knowledge\nbases (Peters et al., 2019) can be injected into\nPLMs by feeding knowledge inputs and design-\ning new objectives. However, pre-training-based\nmethods are costly, making the application to huge\nPLMs (e.g., models with 175 Billion parameters)\nimpossible. Fine-tuning-based methods only re-\nquire an additional fine-tuning process. Some stud-\nies inject extra information into the input sentences,\nlike knowledge triples from knowledge graphs (Liu\net al., 2020) and knowledge context (Faldu et al.,\n2021), while other studies explored specific model\nand training designs, like knowledge adapter net-\nworks (Wang et al., 2021a), graph convolutional\nnetworks and LSTMs (Lin et al., 2019), and meta-\nlearning (Sinitsin et al., 2020). Zhu et al. (2020)\nformulated knowledge injection as a constrained\noptimization problem by adding a constraint on the\nloss on the unmodified facts. Recent studies (Geva\net al., 2021; Cao et al., 2021; Meng et al., 2022)\nreveal that knowledge is stored in the feed-forward\nnetworks in PLMs. Inspired by these studies, we\npropose a new efficient tuning method to inject do-\nmain knowledge into feed-forward networks with\nminimal costs.\n2.2 Domain Adaptation\nPrevious studies have observed that language mod-\nels suffer from a significant performance drop dur-\ning the domain shift (Beltagy et al., 2019; Alsentzer\net al., 2019; Huang et al., 2019; Lee et al., 2020; Ke\net al., 2022b). Effective strategies that can bridge\nthe domain gap are introduced. Pre-training lan-\nguage models from scratch is effective but costly,\nlike SciBERT (Beltagy et al., 2019), BioBERT (Lee\net al., 2020), and ClinicalBERT (Alsentzer et al.,\n2019). Recent studies explored continued pre-\ntraining (Gururangan et al., 2020) and adapter net-\nworks (Diao et al., 2021) to save time by training\non unlabeled downstream task data. In this paper,\nwe introduce plug-in domain adaptors for domain\nadaptation, which are effective and mitigate catas-\ntrophic forgetting issues because of the explicit\nlearning strategy and efficient model architecture.\n2.3 Parameter-Efficient Fine-tuning\nAnother relevant research direction is parameter-\nefficient fine-tuning (PEFT), which only fine-tunes\na small number of parameters. Existing works\nsolve this problem from two perspectives: prompt-\nbased tuning (Gao et al., 2021; Liu et al., 2021b;\nSchick and Schütze, 2021; Li and Liang, 2021; Liu\net al., 2021a), and adapter-based tuning (Houlsby\net al., 2019; Pfeiffer et al., 2020b; Hu et al.,\n2021). Several works in adapter-based tuning\nare closely related to ours. AdapterFusion (Pfeif-\nfer et al., 2021) aims to combine multiple task\nadapters but does not offer specific architecture\nor training strategies to learn external knowledge.\nDEMix (Gururangan et al., 2022) and MixDA both\ntrain adapters that specialize in domains and use\nmechanisms to route different adapters, but dif-\nfer in routing methods, base models, and training\nstrategies. K-Adapter (Wang et al., 2021a) is re-\n5114\nstricted by its training on T-REx triples and lacks\nthe flexibility to train on unstructured knowledge.\nSimilar to MixDA, CPT (Ke et al., 2022a) inte-\ngrates domain knowledge into LMs, but it employs\na different approach. While MixDA uses domain\nadapters to substitute FFN layers and task adapters\nto perform end tasks, CPT adds CL-Plugins that\nlearn domain knowledge. Recent work by He et al.\n(2021a) presents a unified framework that estab-\nlishes connections across different PEFT methods.\nOur work can leverage any PEFT method and com-\nplement them.\n2.4 Mixture-of-Experts\nMixture-of-Experts (MoE) (Shazeer et al., 2017)\nis introduced with several expert networks, gating\nnetworks, and load-balancing techniques. The fol-\nlowing studies improve MoE on initialization and\ntraining schemes (Fedus et al., 2022), routing mech-\nanisms (Zuo et al., 2021; Yang et al., 2021), and\nload-balancing issues (Lewis et al., 2021; Roller\net al., 2021). AdaMix (Wang et al., 2022) proposed\na mixture of adapters to improve the downstream\ntask performance. Instead of mixing different de-\nsigns of adapters, our domain adapter is a feed-\nforward network specifically designed for domain\nknowledge.\n3 Approach\nGiven a pre-trained language model M, the input\nis a sentence X “ t1t2 ¨¨¨ ti ¨¨¨ tT (ti indicates\nthe i-th token) and the output is the representa-\ntion of each token. The overall architecture of our\nmodel is shown in Figure 1. The training process\nis divided into two-stage. In Stage 1 (Figure 1\n(a)), we inject new feed-forward networks (FFNs)\n(namely domain-adapter) paralleled to the original\npre-trained FFNs in some Transformer layers, act-\ning as a key-value memory. The newly injected\ndomain-adapter is trained on both domain-specific\nunlabeled data and original pre-training unlabeled\ndata to store new factual associations while keeping\nold-domain ones. All modules are frozen except\ndomain-adapter in this stage. In Stage 2 (Figure\n1 (b)), we train a mixture-of-adapters (MoA) gate\nand a task-adapter on downstream tasks with la-\nbeled data, and only these two new modules are\nupdated. The MoA gate receives outputs from the\nold-domain FFNs and domain-adapter, then out-\nputs a weighted sum of them. An additional task-\nadapter is inserted in each Transformer block to\nfacilitate downstream tasks. Figure 1 (c) shows the\nstructures of the domain-adapter and the MoA gate.\nIn this section, we first introduce domain-\nadapter, which learns and stores domain-specific\nknowledge, and then describe task-adapters that\nperform the downstream task. Finally, we discuss\nhow the MoA gate integrates the outputs from the\nFFN and the domain-adapter.\n3.1 Domain-Adapter\nPrevious studies (Geva et al., 2021; Cao et al., 2021;\nMeng et al., 2022) suggest that factual associations\nare stored in the FFNs of some Transformer lay-\ners. To help models learn domain-specific knowl-\nedge, we propose a lightweight domain-adapter\nthat works parallel to the FFNs, and a training\nmethod to learn domain-specific knowledge along-\nside keeping old-domain ones. Domain-adapter\nhas a simple bottleneck architecture consisting of\na down projection layer, a nonlinearity (such as\nReLU (Agarap, 2018)), and an up projection layer.\nThis helps keep the parameter size low (Houlsby\net al., 2019) with competitive performance.\nIn Stage 1, the domain-adapter is trained with\nthe domain-specific and old-domain datasets in one\nbatch. Note that all other parameters are frozen\nexcept the domain-adapter at this stage. Let LK de-\nnote the knowledge loss related to domain-specific\nknowledge, and LS denote the sampling loss re-\nlated to old-domain knowledge. The knowledge\nloss is a cross-entropy loss on predicting masked\ntokens, and the sampling loss is designed to align\nthe latent spaces of the old-domain knowledge and\nnew domain-specific knowledge. The total loss L\nis given by a weighted sum of the two, that is:\nL “λ¨LK `LS, (1)\nwhere λis a weight for the knowledge loss.\nThe knowledge loss is implemented by using\ncross-entropy loss. Given a sentence with M mask\ntokens whose answers are m1,m2,¨¨¨ ,mM , re-\nspectively, the knowledge lossLK is given by\nLK “´ 1\nM\nMÿ\ni“1\nlog ppmiq, (2)\nwhere ppmiqis the probability for token mi output\nby M. Our model accepts two types of domain-\nspecific knowledge as follows, showing improved\nversatility.\n‚ Structured knowledge If the knowledge dataset\nis structured (e.g., ConceptNet (Speer et al.,\n5115\nTask AdaptersAdd & NormMixture-of-Adapters GateTransformer GLUE\nConceptNet\nDomain-Specific Knowledge(Unlabeled)\nOld-Domain Knowledge\n(a)Stage 1\nTransformer \n(b)Stage 2\nGLUE\nDownstream Tasks(Labeled)\nDown ProjectionLayerNonlinearityUp ProjectionLayerDomainAdapterDown Projection MLPMLPUp Projection MLPSigmoidMoA Gate……\nK\nS\n\nBookCorpusCC-NewsOpenWebTextNotesParameters frozen in this stageParameters trained in this stageConcatenation\n(c)StructuresAdd & NormTask Adapters(Houlsby, Pfeiffer...)Add & NormMixture-of-Adapters GateAdd & NormMulti-Head AttentionMulti-Head AttentionAdd & NormDomain AdapterFeedForwardAdd & Norm\nUnlabelled dataLabelled dataDomainAdapter 1FeedForwardDomainAdapter 2Other Domain Adapters\nFigure 1: The overall structure of the model. Our training method includes two stages: (a) In Stage 1, we introduce\ndomain-adapters into the model and freeze other parameters. The model learns from domain-specific knowledge\n(knowledge loss LK) and keeps similar outputs with the FFN on old-domain knowledge (sample loss LS). LK\nand LS are then combined into the total loss L. (b) In Stage 2, we introduce the mixture-of-adapters gate and\ntask-adapters, then freeze the domain-adapter. The model is trained to perform downstream tasks, which gives us\nthe total loss L. (c) shows the detailed structures of the domain-adapter and the MoA gate.\n2016)), we translate each relation into a sentence,\nand then mask out its object. For example, the\nrelation “the Eiffel tower–/r/LocatedAt–Paris” is\ntranslated into “The Eiffel Tower is located at\nParis.”, then “Paris” is substituted with the mask\ntoken, and the model is trained to fill the mask.\n‚ Unstructured knowledge For unstructured\nknowledge (e.g., downstream unlabeled texts),\nwe use the masked language model (MLM) sim-\nilar to RoBERTa pretraining. Some tokens are\nrandomly sampled from the input sentence and\nreplaced with the special token <mask>, and the\nmodel is trained to predict the masked token.\nThe cross-entropy loss is calculated to optimize\nthe model.\nFor old-domain knowledge and sampling loss,\nwe train the model on general corpora including\nWikipedia and BookCorpus (Zhu et al., 2015).\nSpecifically, for each batch, sentences randomly\nsampled from the dataset are input into the model.\nGiven Llayers that have domain-adapters installed,\nfor each such layer l, we collect token representa-\ntions from the FFN Fl, and representations from\nthe domain-adapter Kl. The goal is to keep them\nas similar as possible. Thus, we calculate the sam-\npling loss LS with L2 loss:\nLS “ 1\nL\nLÿ\nl“1\n||Fl ´Kl||2\n2. (3)\n3.2 Task-Adapter\nAfter training domain-adapters, the model is aware\nof the domain knowledge, which is not directly re-\nlated to downstream tasks though. Therefore, we\nadd task-adapters on top of the domain-adapter to\nadapt to downstream tasks. For example, a domain-\nadapter trained in biomedical knowledge can sup-\n5116\nDomain Tasks Domain Knowledge # Tokens Size\nBiomed ChemProt, RCT 2.68K papers about biology and chemistry from S2ORC (Lo et al., 2020) 33.6M 144MB\nReview Amazon, IMDB 24.75K randomly selected Amazon reviews 7.4M 34MB\nID GLUE tasks Corpus of GLUE tasks 29.0M 146MB\nKI FEVER, CSQA Corpus of both CommonsenseQA and FEVER datasets 5.9M 34MB\nTable 1: Domain knowledge in Stage 1 training.\nport different tasks in the domain, while training\nit on a task limits its capability to the specific task.\nTask-adapters can be any adapter architecture or\nother parameter-efficient fine-tuning methods, such\nas the Houlsby adapter (Houlsby et al., 2019), Pfeif-\nfer adapter (Pfeiffer et al., 2020b), prefix-tuning (Li\nand Liang, 2021), and so on. At Stage 2, all pa-\nrameters other than the task-adapters and the MoA\ngate (Section 3.3) are frozen. The training of the\nadapter follows its corresponding approach, despite\nthe addition of domain-adapters. For example, for a\ntext classification task, we add a classification layer\non top of the model, freeze all parameters other\nthan the classification layer, the MoA gate, and the\ntask-adapters, feed input texts into the model, and\nuse cross-entropy as the loss.\n3.3 Mixture-of-Adapters Gate\nOn downstream tasks, it is possible that the output\nfrom the FFN, or a weighted sum of the two, pro-\nduces better results. Therefore, in Stage 2, we train\nan additional mixture-of-adapters (MoA) gate si-\nmultaneously. The MoA gate receives the outputs\nfrom the attention layer q, the domain-adapter K,\nand the FFN F. q is first sent into a multi-layer\nperceptron (MLP):\nh “MLPpqq. (4)\nThe MLP is composed of a down-projection layer\nWd and an up-projection layer Wu, and h “\nWuσpWdqq, where σis the nonlinearity function.\nThen, h is input into a Sigmoid layer to generate the\nweights of the FFNs and other domain-adapters:\nw “Sigmoidphq. (5)\nThe final output o is a weighted sum of the out-\nputs of the FFNs and the domain-adapter:\no “wrK; Fs, (6)\nwhere r; sdenotes matrix concatenation.\n4 Experimental Settings\nIn this section, we first introduce the datasets, then\nthe baseline models, the evaluation metrics, and\nimplementation details in the following four sub-\nsections, respectively.\n4.1 Datasets\nWe conduct experiments on three types of datasets:\nin-domain (ID) tasks that require general-domain\nknowledge; out-of-domain (OOD) tasks that re-\nquire domain-specific knowledge; knowledge-\nintensive (KI) tasks that require commonsense\nknowledge.\n‚ ID: GLUE Benchmark (Wang et al., 2018)\nincluding MNLI (Williams et al., 2017),\nCoLA (Warstadt et al., 2019), MRPC (Dolan\nand Brockett, 2005), SST-2 (Socher et al., 2013),\nRTE (Dagan et al., 2005; Haim et al., 2006; Gi-\nampiccolo et al., 2007; Bentivogli et al., 2009),\nSTS-B (Cer et al., 2017), WNLI (Levesque\net al., 2012), QNLI (Rajpurkar et al., 2016), and\nQQP (Iyer et al., 2017).\n‚ OOD: ChemProt (Kringelum et al., 2016),\nRCT (Dernoncourt and Lee, 2017), IMDB (Maas\net al., 2011), and Amazon (He and McAuley,\n2016). ChemProt is a manually annotated\nchemical-protein interaction dataset extracted\nfrom 5,031 abstractions. RCT is a dataset based\non PubMed for sentence classification. IMDB\nprovides 25,000 movie reviews for sentiment\nanalysis. Amazon is a dataset containing prod-\nuct reviews from Amazon, annotated with user\nratings.\n‚ KI: FEVER (Thorne et al., 2018) and Common-\nsenseQA (CSQA) (Talmor et al., 2019). FEVER\nconsists of 185,445 claims that correspond to\nWikipedia articles and are classified as supported,\nrefuted, and not enough information. Common-\nsenseQA consists of 12,247 questions with 5\nchoices and requires commonsense knowledge\nto predict the correct answers.\nFor Stage 1, we train the domain-adapter with\nunstructured knowledge related to the dataset fol-\nlowing Section 3.1. The unstructured knowledge\n5117\nused is listed in Table 1. We also experiment with\nstructured knowledge in Section 6.2. For Stage 2,\nwe adopt the true few-shot setting following (Perez\net al., 2021) to demonstrate the effectiveness of\nMixDA. For each dataset class, we randomly sam-\nple K “ 16 examples from the original training\nset as the new training set, and another different\nK “16 examples as the validation set. The origi-\nnal validation set will be used as the test set. The\nPfeiffer adapter is used in Stage 2 unless stated\notherwise.\n4.2 Baselines\nIn our experiments, we use the following models\nas the main baselines. For convenience, we refer\nto them with the abbreviations in the parentheses\nlater.\n‚ HOULSBY (HO): Houlsby adapter (Houlsby\net al., 2019) plugged into the RoBERTa-large\nmodel for downstream tasks. Only adapter pa-\nrameters are trained. It adds two adapter blocks\nconsisting of bottleneck networks in each Trans-\nformer block.\n‚ PFEIFFER (PF): Pfeiffer adapter (Pfeiffer et al.,\n2020b) plugged into the RoBERTa-large model.\nThis is similar to the Houlsby adapter, but with a\ndifferent architecture. Pfeiffer adapter has only\none adapter layer in each Transformer block,\nwhile Houlsby has two. Also, Pfeiffer makes\nminor tweaks in the adapter architecture, such as\nthe layer norm and nonlinearity.\n‚ LORA (LO): LoRA (Hu et al., 2021) applied\nto the RoBERTa-large model. LoRA freezes\nthe MLP modules and represents updates to the\nattention weights with two low-rank matrices,\nthus saving space.\n‚ PREFIX -TUNING (PT): Prefix-Tuning (Li and\nLiang, 2021) with the RoBERTa-large model.\nPrefix-Tuning trains a number of prompt em-\nbeddings for each task and pre-pends it before\ntokens.\n‚ FINE -TUNING (FT): Fine-tuning all of the pa-\nrameters of the RoBERTa-large model on down-\nstream tasks.\n4.3 Evaluation Metrics\nWe adopt the Pearson correlation for STS-B since\nit is a regression task. The remaining are text clas-\nsification tasks. Following Wang et al. (2018); Gu-\nrurangan et al. (2020); Diao et al. (2021), we adopt\nmacro-F1 for MRPC and QQP, and micro-F1 for\nothers as evaluation metrics. Macro-F1 computes\nthe F1 independently for each metric, while micro-\nF1 computes an average metric of all classes. To\naccount for the instability of small datasets, we\nreport the average performance and the standard\ndeviation of 3 runs with different random seeds.\n4.4 Implementation\nWe implement our RoBERTa-large model based on\nthe Transformers library from HuggingFace2. The\nHoulsby adapter, the Pfeiffer adapter, and Prefix-\nTuning are implemented based on the adapter-\ntransformers library (Pfeiffer et al., 2020a). LoRA\nis implemented based on OpenDelta (Ding et al.,\n2022). During Stage 1, we train the domain-adapter\nwith learning rate 1e-4, batch size 20, and weight\ndecay 0.05. The knowledge loss factor λis set to\n0.5. We train the 7 and 11 layers of RoBERTa-large\nwith domain-adapter in 10 epochs. In Stage 2, we\nuse the Pfeiffer adapter as the default task-adapter\nand train 20 epochs. All the experiments are con-\nducted on Nvidia 2080Ti GPUs. We find the best\nhyper-parameters through grid search and the best\nresults are listed in Appendix A. The computation\ntime can be found in Appendix B.\n5 Experimental Results\nWe compare the performance of MixDA with our\nbaselines on 15 datasets. First, we train the domain-\nadapter for each domain individually and then per-\nform each task with its corresponding domain-\nadapter, which shows significant improvement over\nour baselines. Next, we plug in several domain-\nadapters trained on different domains parallelly to\nverify the scalability of our model.\n5.1 Single Domain Adapter\nTable 2 shows the performance of a single domain\nadapter compared with baselines. It is only trained\non unstructured knowledge during Stage 1 in the\nfollowing experiments. Results show that Mixture-\nof-Domain-Adapters outperforms our baselines in\nmost datasets, with an average of 3.5% improve-\nment over the best baseline adapter (i.e., Pfeiffer),\nand 3.3% over fine-tuning. Our method even out-\nperforms fine-tuning in most datasets, despite far\nless training time and smaller parameter size. Over\nthe datasets, MixDA shows the most significant im-\nprovement on ChemProt, with 6.9% over Pfeiffer\nand 2.7% over fine-tuning. One possible reason\nis that MixDA learns the necessary knowledge to\n2https://github.com/huggingface/transformers\n5118\nDatasets HO PF LO PT FT MixDA\nOOD\nChemProt 47.1 ˘12.2 53.7˘8.2 24.2˘11.6 17.1˘7.3 57.9˘4.0 60.6˘4.9\nRCT 25.2 ˘2.6 21.9˘2.5 18.5˘3.7 24.4˘3.7 21.0˘3.2 26.4˘0.8\nIMDB 56.0 ˘5.7 55.4˘5.6 43.7˘7.7 53.3˘2.6 46.3˘13.7 58.1˘5.1\nAmazon 48.8 ˘3.2 49.7˘1.4 51.5˘3.9 52.7˘2.8 51.7˘6.2 54.7˘1.6\nAvg. 44.3 45.2 34.5 36.9 44.2 50.0\nID\nMNLI 37.2 ˘0.3 35.7˘0.1 34.8˘1.5 35.4˘0.0 35.3˘0.2 37.3˘1.5\nCOLA 17.6 ˘5.5 9.1˘5.0 7.1˘3.0 12.1˘5.5 21.3˘1.7 20.1˘6.3\nMRPC 81.2 ˘0.2 80.7˘0.6 64.0˘20.5 81.6˘0.5 81.3˘0.1 81.6˘0.5\nSST2 54.7 ˘3.6 53.3˘1.9 50.5˘1.0 52.5˘1.2 54.8˘1.4 56.4˘3.5\nRTE 53.5 ˘1.4 54.1˘1.3 53.4˘2.1 53.4˘1.1 54.7˘1.3 54.9˘1.5\nSTS-B 88.1 ˘1.6 90.6˘0.1 89.5˘0.8 85.6˘4.1 78.4˘8.0 89.8˘0.4\nWNLI 57.3 ˘1.3 58.1˘2.5 59.1˘1.2 57.3˘0.7 58.7˘1.8 60.1˘1.8\nQNLI 53.0 ˘0.1 51.9˘0.8 53.3˘1.3 52.1˘0.9 51.3˘0.1 54.8˘1.8\nQQP 54.7 ˘0.6 55.2˘1.0 53.0˘2.0 55.3˘0.3 55.3˘0.3 56.1˘0.6\nAvg. 55.3 54.3 51.6 53.9 54.6 56.8\nKI\nFEVER 20.2 ˘4.3 27.4˘7.5 22.6˘10.6 31.1˘3.6 36.1˘6.7 32.6˘9.4\nCSQA 27.3 ˘0.7 34.1˘8.7 20.3˘10.9 29.6˘4.2 29.6˘3.0 38.9˘4.0\nAvg. 23.8 30.8 21.5 30.4 32.9 35.8\nAvg. 48.1 48.7 43.0 46.2 48.9 52.2\nTable 2: The overall performance of single MixDA and baselines on the downstream tasks. We useK “16 (per\nclass) for few-shot experiments. The best result for each dataset is made bold. We report mean and standard\ndeviation over 3 runs with different random seeds.\nAmazon IMDB FEVER WNLI QQP RTE MRPC Avg.\nPfeiffer 49.7˘1.4 55.4˘5.6 27.4˘7.5 58.1˘2.5 55.2˘1.0 54.1˘1.3 80.7˘0.6 54.4\nSingle 54.7 ˘1.6 58.1˘5.1 32.6˘9.4 60.1˘1.7 56.1˘0.6 54.9˘1.5 81.6˘0.5 56.9\nParallel 51.6˘2.4 47.9˘2.1 34.5˘0.5 58.7˘1.8 57.8˘3.5 53.8˘0.9 81.0˘0.2 55.0\nTable 3: The performance of parallel domain-adapters on the chosen downstream tasks. Parallel, Single, and Pfeiffer\ndenote parallel domain-adapters, single domain-adapter, and vanilla RoBERTa + Pfeiffer, respectively. The best\nresult for each dataset is made bold.\ndetect the chemical-protein interaction. For exam-\nple, MixDA shows more familiarity with words\nassociated with that field, such as “gefitinib” and\n“tyrosine kinase inhibitor”. In contrast, MixDA\nfalters on STS-B, falling behind Pfeiffer by 0.8%.\nThis is because the knowledge in Stage 1 is not ef-\nfectively utilized. STS-B consists of sentence pairs\nlike “The cat sat on the mat” and “The cat did not sit\non the mat”, with little need for additional knowl-\nedge. Across the three task domains, MixDA has\nan average improvement of 4.8% over RoBERTa\n+ Pfeiffer on out-of-domain tasks, 2.5% on in-\ndomain tasks, and 5.0% on knowledge-intensive\ntasks. It shows that MixDA is not only effective for\nout-of-domain tasks and knowledge-intensive tasks\nthat require additional knowledge but is helpful\nfor general-domain language tasks as well, demon-\nstrating its ability to excel at both in-domain and\nout-of-domain tasks (reliability).\n5.2 Parallel Domain Adapters\nIn the previous section, we explored using a single\ndomain-adapter for each downstream task. Next,\nwe show the scalability of MixDA by using paral-\nlel domain-adapters and only train the MoA layer\nand task-adapters in Stage 2. The training process\nin Stage 2 follows the previous experiments. Ta-\nble 3 shows the comparison across single domain-\nadapter, parallel domain-adapters, and RoBERTa +\nPfeiffer on 7 datasets. On average, parallel domain-\nadapters show an improvement of 0.6% over vanilla\nRoBERTa + Pfeiffer, even though they fall behind\nthe single domain adapter by 1.9%. This could\nbe attributed to the MoA gate choosing the sub-\noptimal domain-adapter for some test data. Still,\nconsidering its improvement over Pfeiffer, the MoA\ngate chooses the correct domain-adapter in most\ncases. Therefore, MixDA demonstrates its scala-\nbility, allowing end users to train Stage 1 on dif-\nferent datasets and combine them later. Overall, in\nboth single and parallel situations, MixDA signifi-\ncantly improves upon the vanilla RoBERTa + Pfeif-\n5119\nDatasets ChemProt IMDB MRPC STS-B CSQA Avg.\nMixDA 60.6 ˘4.9 58.1˘5.1 81.6˘0.5 89.8˘0.4 38.8˘4.0 65.8\n– MoA 55.7˘1.8 49.8˘0.0 80.9˘0.5 88.4˘0.4 28.3˘1.3 60.6\n– Old 54.3˘6.5 41.4˘3.6 78.7˘3.7 90.0˘0.4 27.1˘0.3 58.3\n– DA 21.2˘4.7 56.8˘3.9 81.0˘0.5 80.8˘2.4 27.4˘0.5 53.4\nAdapterFusion 47.7˘0.1 54.4˘2.0 78.0˘1.5 90.3˘0.3 25.0˘1.7 59.1\nK-Adapter 58.2˘5.0 55.6˘4.5 53.9˘5.9 89.7˘0.4 26.2˘4.7 56.7\nCPT 45.9˘0.3 56.1˘5.2 81.0˘0.5 90.2˘0.1 33.7˘2.7 61.4\nTable 4: Ablations of the MoA gate, old-domain knowledge, and the domain-adapter structure and comparisons\nwith other adapter-based tuning methods. For – Old, we omit old-domain knowledge in Stage 1 training. For – DA,\nwe remove the domain-adapter structure and conduct both stages of training only with Pfeiffer adapters. The best\nresults for each dataset are made bold.\nDatasets MRPC STS-B FEVER CSQA Avg.\nMixDA 81.6˘0.5 89.8˘0.4 20.2˘4.3 38.8˘4.0 57.6\n+ ConceptNet 81.7 ˘0.3 90.1˘0.1 30.5˘3.1 40.0˘0.2 60.6\nTable 5: The results of MixDA trained on structured\nand unstructured knowledge. + ConceptNet stands\nfor domain-adapters trained on both the unstructured\nknowledge and ConceptNet.\nfer model with a small increase in model size. This\nis due to the ability of MixDA to capture knowl-\nedge and the MoA to select useful knowledge for\ndownstream tasks.\n6 Analysis\nIn this section, we analyze the respective contribu-\ntions of each part of MixDA through detailed anal-\nysis, including the Stage 1 training, task-adapters\nin Stage 2, and the mixture-of-adapters gate.\n6.1 Ablation Study\nIn this section, we conduct an ablation study to\nreveal the contributions of each part of the model.\nThere are three variants: (1) We remove the MoA\ngate and choose the domain-adapter instead of the\nRoBERTa feed-forward layer (–MoA). (2) We ex-\nclude old-domain knowledge during Stage 1 (–Old).\n(3) To examine whether the training procedures,\nrather than the MixDA structure, contribute the\nmost to our results, we conduct Stage 1 and Stage\n2 training only with task-adapters (–DA). Table 4\nshows the results of the ablation study. As ex-\npected, the average performance drops in all three\nsettings. Without MoA gate, old-domain knowl-\nedge FFNs, and structure knowledge, it is observed\na drop of 5.2%, 7.5%, and 12.4%, respectively,\nshowing that the MoA gate, the old-domain knowl-\nedge, and the MixDA structure are all fundamental\nin the model. Relatively, the MoA has the smallest\nimpact because the old-domain knowledge in Stage\n1 can also help the model retain the knowledge in\nRoBERTa. The domain-adapter has the largest im-\npact since it only stores domain knowledge and\ncan keep it during Stage 2. In contrast, conduct-\ning Stage 1 and 2 training on the Pfeiffer adapter\ncauses catastrophic forgetting.\n6.2 Structured and Unstructured Knowledge\nIn Section 5, the MixDA is only trained on unstruc-\ntured knowledge. As a comparison, we also train\nthe domain adapter on ConceptNet, a structured\nknowledge dataset, and then attach both the un-\nstructured and structured to our model and train the\nMoA layer and the task-adapter during Stage 2.\nTable 5 shows the result of combining structured\nand unstructured knowledge in Stage 1. FEVER\nand CSQA, two knowledge-intensive tasks, have\nthe greatest improvement: 10.3% for FEVER and\n1.2% for CSQA. This is because ConceptNet stores\ncommonsense knowledge that can help both tasks.\nMeanwhile, MRPC and STS-B also obtain im-\nprovement, showing that ConceptNet can benefit\ngeneral language tasks as well. In conclusion, the\nexperiment demonstrates the ability of MixDA to\nutilize structured knowledge, the extensibility of\nour model, and the possible benefits of structured\nknowledge.\n6.3 Effectiveness of Task-Adapters\nIn most experiments of this paper, we adopt Pfeif-\nfer as the task-adapter unless otherwise specified.\nIn this section, we test the performance of MixDA\ncombined with other kinds of task-adapters, includ-\ning Houlsby, Prefix-Tuning, LoRA, and Pfeiffer.\nTable 6 gives the result of different task-adapters.\nPfeiffer surpasses others by at least 6.3%. Even\nthough Houlsby is on par with Pfeiffer, Pfeiffer\nonly requires half the number of newly introduced\n5120\nDatasets ChemProt IMDB MRPC STS-B CSQA Avg.\nHoulsby 47.1 ˘12.2 48.1˘4.5 80.0˘1.5 86.6˘3.2 35.8˘8.9 59.5\nPrefix-Tuning 17.1 ˘7.3 39.1˘7.2 81.6˘0.4 88.6˘0.5 33.3˘0.0 51.9\nLoRA 19.5 ˘11.1 36.1˘4.1 81.2˘0.0 86.7˘1.4 20.3˘10.9 48.8\nPfeiffer 60.6˘4.9 58.1˘5.1 81.6˘0.5 89.8˘0.4 38.8˘4.0 65.8\nTable 6: The results of MixDA combined with different kinds of task-adapters. By default, we use Pfeiffer in\nprevious experiments.\nparameters compared to Houlsby, making it the\noptimal choice of task-adapters in our experiment.\n7 Conclusion\nIn this paper, we proposed MixDA, a mixture of\nadapters for domain adaptation. We first decou-\nple the knowledge modules (i.e., FFNs) into the\nold-domain and domain-specific FFNs. Then we\npropose a two-stage adapter tuning strategy: first\ntuning the domain adapter on each domain and then\ntuning the task adapter on each task. Moreover, our\nmodel could be scaled to multiple domains easily\nwith the introduction of the mixture-of-adapters\ngate. Empirically, MixDA achieved significant im-\nprovement over in-domain tasks, out-of-domain\ntasks, and knowledge-intensive tasks. Further anal-\nyses demonstrate the reliability, scalability, and\nefficiency of our method.\nLimitations\nAlthough MixDA achieves promising results on\ndomain adaptation compared with baseline models,\nthere are certain limitations. MixDA is a two-stage\napproach, which is not fully end-to-end. Our ap-\nproach requires training a domain adapter and task\nadapter, respectively. In the future, we will explore\nthe unifying domain and task adapters by merging\nthem into one.\nAcknowledgments\nWe thank the anonymous reviewers for their valu-\nable suggestions. This work was supported by the\nGeneral Research Fund (GRF) of Hong Kong (No.\n16310222 and No. 16201320). Shizhe Diao and\nRuijia Xu were supported by the Hong Kong Ph.D.\nFellowship Scheme (HKPFS).\nReferences\nAbien Fred Agarap. 2018. Deep learning using rectified\nlinear units (relu). arXiv preprint arXiv:1803.08375.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly Available Clin-\nical BERT Embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT:\nA Pretrained Language Model for Scientific Text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3606–3611.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsification in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308–313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\n5121\nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. Zen: Pre-training chinese\ntext encoder enhanced by n-gram representations.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4729–4740.\nShizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan\nSong, and Tong Zhang. 2021. Taming pre-trained\nlanguage models with n-gram representations for low-\nresource domain adaptation. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3336–3349.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tuning:\nA comprehensive study of parameter efficient meth-\nods for pre-trained language models. arXiv preprint\narXiv:2203.06904.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nKeyur Faldu, Amit Sheth, Prashant Kikani, and Hemang\nAkbari. 2021. Ki-bert: Infusing knowledge context\nfor better language and domain understanding. arXiv\npreprint arXiv:2104.08145.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Journal of\nMachine Learning Research, 23(120):1–39.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5484–5495.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1–9, Prague. Association for\nComputational Linguistics.\nSuchin Gururangan, Mike Lewis, Ari Holtzman, Noah A\nSmith, and Luke Zettlemoyer. 2022. Demix layers:\nDisentangling domains for modular language mod-\neling. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5557–5576.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020. Bert-mk:\nIntegrating graph contextualized knowledge into pre-\ntrained language models. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 2281–2290.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021a. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021b. DEBERTA: Decoding-\nenhanced bert with disentangled attention. In Inter-\nnational Conference on Learning Representations.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative filtering. In Proceedings of\nthe 25th International Conference on World Wide\nWeb, WWW ’16, page 507–517, Republic and Can-\nton of Geneva, CHE. International World Wide Web\nConferences Steering Committee.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-Efficient Transfer Learning for\nNLP. arXiv.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\nof Large Language Models. arXiv.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. ClinicalBERT: Modeling Clinical Notes and\nPredicting Hospital Readmission. arXiv preprint\narXiv:1904.05342.\nShankar Iyer, Nikhil Dandekar, Kornél Csernai, et al.\n2017. First quora dataset release: Question pairs.\ndata. quora. com.\n5122\nZixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu,\nand Bing Liu. 2022a. Continual training of language\nmodels for few-shot learning. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 10205–10216, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nZixuan Ke, Yijia Shao, Haowei Lin, Hu Xu, Lei Shu,\nand Bing Liu. 2022b. Adapting a language model\nwhile preserving its general knowledge. In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10177–10188.\nJens Kringelum, Sonny Kim Kjaerulff, Søren Brunak,\nOle Lund, Tudor I. Oprea, and Olivier Taboureau.\n2016. ChemProt-3.0: a global chemical biology dis-\neases mapping. Database, 2016:bav123.\nAnne Lauscher, Ivan Vuli´c, Edoardo Maria Ponti, Anna\nKorhonen, and Goran Glavaš. 2019. Informing unsu-\npervised pretraining with external linguistic knowl-\nedge.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. BioBERT: A Pre-Trained Biomedical Lan-\nguage Representation Model for Biomedical Text\nMining. Bioinformatics, 36(4):1234–1240.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth international conference on the principles of\nknowledge representation and reasoning.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. Sensebert: Driv-\ning some sense into bert. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4656–4667.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. Base layers:\nSimplifying training of large, sparse models. In In-\nternational Conference on Machine Learning, pages\n6265–6274. PMLR.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. BART:\nDenoising Sequence-to-Sequence Pre-training for\nNatural Language Generation, Translation, and Com-\nprehension. arXiv.\nXiang Lisa Li and Percy Liang. 2021. Prefix-Tuning:\nOptimizing Continuous Prompts for Generation.\narXiv.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. Kagnet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\nArXiv preprint, abs/2107.13586.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-bert: En-\nabling language representation with knowledge graph.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 2901–2908.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nUnderstands, Too. ArXiv preprint, abs/2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. Asso-\nciation for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled\nweight decay regularization.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and Editing Factual Asso-\nciations in GPT. arXiv.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. Ad-\nvances in Neural Information Processing Systems ,\n34:11054–11070.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 43–54.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterfusion: Non-destructive task composition for\ntransfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n487–503.\n5123\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020a. Adapterhub: A\nframework for adapting transformers. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 46–54.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston,\net al. 2021. Hash layers for large sparse models.\nAdvances in Neural Information Processing Systems,\n34:17555–17566.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In International Conference on\nLearning Representations.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2016.\nConceptNet 5.5: An Open Multilingual Graph of\nGeneral Knowledge. arXiv.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction and\nVERification. In NAACL-HLT.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. arXiv\npreprint arXiv:1804.07461.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuan-Jing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-adapter: Infusing\nknowledge into pre-trained models with adapters. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 1405–1418.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176–194.\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee,\nXiaodong Liu, Jing Gao, Ahmed Hassan Awadal-\nlah, and Jianfeng Gao. 2022. Adamix: Mixture-\nof-adaptations for parameter-efficient model tuning.\narXiv preprint arXiv:2210.17451.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference on Learning\nRepresentations.\nAn Yang, Junyang Lin, Rui Men, Chang Zhou,\nLe Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jia-\nmang Wang, Yong Li, et al. 2021. M6-t: Exploring\nsparse expert models and beyond. arXiv preprint\narXiv:2105.15082.\nZe Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi Bai,\nLiran Wang, Wei Wang, and Zhoujun Li. 2020.\nStyleDGPT: Stylized Response Generation with Pre-\ntrained Language Models. ACL Anthology, pages\n1548–1559.\n5124\nDani Yogatama, Cyprien de Masson d’Autume, Jerome\nConnor, Tomas Kocisky, Mike Chrzanowski, Ling-\npeng Kong, Angeliki Lazaridou, Wang Ling, Lei\nYu, Chris Dyer, et al. 2019. Learning and evaluat-\ning general linguistic intelligence. arXiv preprint\narXiv:1901.11373.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and William B Dolan. 2020. DIALOGPT: Large-\nScale Generative Pre-training for Conversational Re-\nsponse Generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1441–\n1451.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\narXiv preprint arXiv:2012.00363.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning Books and Movies:\nTowards Story-like Visual Explanations by Watching\nMovies and Reading Books. arXiv.\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim,\nHany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo\nZhao. 2021. Taming sparsely activated transformer\nwith stochastic experts. In International Conference\non Learning Representations.\n5125\nA Experimental Setup\nOur domain adapter has a reduction factor of 16,\nconsisting of two linear layers 4096 ˆ256 and\n256ˆ1024 (1.31M parameters). With each domain\nadapter also comes a MoA gate which has an FFN\nwith 4096 ˆ2 (number of MixDAs) parameters.\nSince domain adapters are placed in Layers 7 and\n11, they have 2.6M parameters in total. Therefore,\nthe domain adapters (excluding task-adapters) only\nadd 0.7% additional parameters to RoBERTa-large.\nWe preprocess the unstructured data in Stage 1\nsimilar to the masked language model directive in\nRoBERTa. From the text, we choose 15% of to-\nkens uniformly to perform possible alterations. In\nthose tokens, 85% are replaced with <mask>, 10%\nare left unchanged, and 5% are replaced with a ran-\ndom token. The preprocessing is implemented with\nDataCollatorForLanguageModeling in Hug-\ngingface Transformers. In Stage 2, we use few-shot\nsetting with K “16. For each class of the dataset,\nwe randomly select 16 examples before run.\nIn Stages 1 and 2, we use a linear weight\nscheduler. All the models are optimized by\nAdamW (Loshchilov and Hutter, 2017) with weight\ndecay 0.05. The best hyperparameters for Stage\n2 are found with grid search, with batch size\nt2,4,8,16uand learning rate t5e´5,1e´4,5e´\n4u. The details can be found in Tables 7 and 8.\nB Computational Budget\nStage 1 training takes relatively longer time, while\nStage 2 is fast due to the few-shot setting. The train-\ning time of Stage 1 is proportional to the number of\ntokens. For reference, with 4 Nvidia RTX 2080Ti,\nStage 1 training for Biomed (33.6M tokens) takes\n~45min per epoch, and training for Review (7.4M\ntokens) takes ~5min per epoch. Stage 2 training\nis generally fast: The 20-epoch training process\nusually takes less than 5min with 4 Nvidia RTX\n2080Ti.\nC Details of Datasets\nWe conduct experiments on three types of datasets:\nin-domain (ID) tasks that require general-domain\nknowledge; out-of-domain (OOD) tasks that re-\nquire domain-specific knowledge; knowledge-\nintensive (KI) tasks that require commonsense\nknowledge.\nFor in-domain tasks, we evaluate our model\non the GLUE Benchmark (Wang et al., 2018).\nIt includes MNLI (Williams et al., 2017),\nCoLA (Warstadt et al., 2019), MRPC (Dolan\nand Brockett, 2005), SST-2 (Socher et al., 2013),\nRTE (Dagan et al., 2005; Haim et al., 2006; Gi-\nampiccolo et al., 2007; Bentivogli et al., 2009),\nSTS-B (Cer et al., 2017), WNLI (Levesque\net al., 2012), QNLI (Rajpurkar et al., 2016), and\nQQP (Iyer et al., 2017). They are all single-\nsentence or sentence pair classification tasks except\nSTS-B, which is a regression task.\nWe also evaluate our model on several out-of-\ndomain tasks, including ChemProt (Kringelum\net al., 2016), RCT (Dernoncourt and Lee, 2017),\nIMDB (Maas et al., 2011), and Amazon (He and\nMcAuley, 2016). ChemProt is a manually anno-\ntated chemical-protein interaction dataset extracted\nfrom 5,031 abstractions. RCT is a dataset based\non PubMed for sentence classification. IMDB pro-\nvides 25,000 movie reviews for sentiment analysis.\nAmazon is a dataset containing product reviews\nfrom Amazon, annotated with user ratings.\nFor knowledge-intensive tasks, we evaluate our\nmodel on FEVER (Thorne et al., 2018) and\nCommonsenseQA (CSQA) (Talmor et al., 2019).\nFEVER consists of 185,445 claims that correspond\nto Wikipedia articles and are classified as sup-\nported, refuted, and not enough information. Com-\nmonsenseQA consists of 12,247 questions with\n5 choices, each of which requires commonsense\nknowledge to predict the correct answers.\nFor Stage 1, we train domain-adapters with un-\nstructured knowledge related to the dataset follow-\ning Section 3.1. The unstructured knowledge used\nis listed in Table 1. We also experiment with struc-\ntured knowledge in Section 6.2. For Stage 2, we\nadopt the true few-shot setting following (Perez\net al., 2021) to demonstrate the effectiveness of\nMixDA. For each class of each dataset, we ran-\ndomly sample K “16 examples from the original\ntraining set as the new training set, and another\ndifferent K “16 examples as the validation set.\nThe original validation set will be used as the test\nset. The Pfeiffer adapter is used in Stage 2 unless\nstated otherwise.\n5126\nConfig Value\nOptimizer AdamW\nLearning rate 1e-4\nWeight decay 0.05\nOptimizer momentum β1,β2“0.9,0.999\nBatch size {2, 5}\nLearning rate schedule linear decay\ntraining epochs 10\nTable 7: Stage 1 training: experimental setup.\nConfig Value\nOptimizer AdamW\nLearning rate {5e-5, 1e-4, 5e-4}\nWeight decay 0.05\nOptimizer momentum β1,β2“0.9,0.999\nBatch size {2, 4, 8, 16}\nLearning rate schedule linear decay\nwarmup epochs 2\ntraining epochs 20\nTable 8: Stage 2 training: experimental setup.\n5127\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nRefer to Section \"Limitations\" at the end of paper.\n□\u0017 A2. Did you discuss any potential risks of your work?\nThis paper mainly focuses on improving performance on downstream tasks with external knowledge.\nWe have not found risks of the potential use cases.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nRefer to Abstract and Introduction (Section 1).\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nRefer to Section 4.1 and Appendix C.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nRefer to Section 4.1 and Appendix C.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe use publicly available datasets. Non-commercial and academic use of them are approved by their\nrespective authors.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe use publicly available datasets. Non-commercial and academic use of them are approved by their\nrespective authors.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use publicly available datasets.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nRefer to Section 4.1 and Appendix C.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nRefer to Section 4.1, Table 1, and Appendix C.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5128\nC □\u0013 Did you run computational experiments?\nRefer to Sections 5 and 6.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nRefer to Appendix A and B.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nRefer to Appendix A.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nRefer to Sections 5 and 6.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nRefer to Section 4.4.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n5129",
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.8559609651565552
    },
    {
      "name": "Computer science",
      "score": 0.847550630569458
    },
    {
      "name": "Scalability",
      "score": 0.6541125178337097
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.569907546043396
    },
    {
      "name": "Domain adaptation",
      "score": 0.5484163165092468
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5318650007247925
    },
    {
      "name": "Transformer",
      "score": 0.5177634954452515
    },
    {
      "name": "Language model",
      "score": 0.5113746523857117
    },
    {
      "name": "Domain knowledge",
      "score": 0.45473816990852356
    },
    {
      "name": "Decoupling (probability)",
      "score": 0.42851269245147705
    },
    {
      "name": "Machine learning",
      "score": 0.40938934683799744
    },
    {
      "name": "Database",
      "score": 0.1658444106578827
    },
    {
      "name": "Computer hardware",
      "score": 0.13880467414855957
    },
    {
      "name": "Voltage",
      "score": 0.0825437605381012
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Control engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 14
}