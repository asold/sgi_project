{
    "title": "Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)",
    "url": "https://openalex.org/W4403458599",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4282099262",
            "name": "Toro, Sabrina",
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ]
        },
        {
            "id": null,
            "name": "Anagnostopoulos, Anna V",
            "affiliations": [
                "Jackson Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4227329367",
            "name": "Bello, Susan M.",
            "affiliations": [
                "Jackson Laboratory"
            ]
        },
        {
            "id": null,
            "name": "Blumberg, Kai",
            "affiliations": [
                "Beltsville Human Nutrition Research Center"
            ]
        },
        {
            "id": null,
            "name": "Cameron, Rhiannon",
            "affiliations": [
                "Simon Fraser University"
            ]
        },
        {
            "id": "https://openalex.org/A2702654441",
            "name": "Carmody, Leigh",
            "affiliations": [
                "Jackson Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4224623664",
            "name": "Diehl, Alexander D.",
            "affiliations": [
                "University at Buffalo, State University of New York"
            ]
        },
        {
            "id": null,
            "name": "Dooley, Damion M",
            "affiliations": [
                "Simon Fraser University"
            ]
        },
        {
            "id": "https://openalex.org/A4222624761",
            "name": "Duncan, William D.",
            "affiliations": [
                "University of Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2419953839",
            "name": "Fey Petra",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": "https://openalex.org/A2435821828",
            "name": "Gaudet Pascale",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics"
            ]
        },
        {
            "id": "https://openalex.org/A4224335821",
            "name": "Harris, Nomi L",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4297439418",
            "name": "Joachimiak, Marcin P.",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        },
        {
            "id": null,
            "name": "Kiani, Leila",
            "affiliations": [
                "Oldham Council"
            ]
        },
        {
            "id": "https://openalex.org/A4224623658",
            "name": "Lubiana, Tiago",
            "affiliations": [
                "Instituto do Sono"
            ]
        },
        {
            "id": "https://openalex.org/A4227329396",
            "name": "Munoz-Torres, Monica C",
            "affiliations": [
                "University of Colorado Anschutz Medical Campus"
            ]
        },
        {
            "id": null,
            "name": "O'Neil, Shawn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224623663",
            "name": "Osumi-Sutherland, David",
            "affiliations": [
                "Wellcome Sanger Institute"
            ]
        },
        {
            "id": null,
            "name": "Puig-Barbe, Aleix",
            "affiliations": [
                "European Bioinformatics Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4281070841",
            "name": "Reese, Justin T",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        },
        {
            "id": null,
            "name": "Reiser, Leonore",
            "affiliations": [
                "Institute of Bioinformatics"
            ]
        },
        {
            "id": "https://openalex.org/A4284908763",
            "name": "Robb, Sofia MC",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ruemping, Troy",
            "affiliations": [
                "Austin Graduate School of Theology"
            ]
        },
        {
            "id": null,
            "name": "Seager, James",
            "affiliations": [
                "Rothamsted Research"
            ]
        },
        {
            "id": null,
            "name": "Sid, Eric",
            "affiliations": [
                "National Center for Advancing Translational Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A4284908762",
            "name": "Stefancsik, Ray",
            "affiliations": [
                "European Bioinformatics Institute"
            ]
        },
        {
            "id": null,
            "name": "Weber, Magalie",
            "affiliations": [
                "Institut National de Recherche pour l'Agriculture, l'Alimentation et l'Environnement"
            ]
        },
        {
            "id": null,
            "name": "Wood, Valerie",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A4213765529",
            "name": "Haendel, Melissa A",
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ]
        },
        {
            "id": "https://openalex.org/A4222624768",
            "name": "Mungall, Christopher J.",
            "affiliations": [
                "Lawrence Berkeley National Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4224037213",
        "https://openalex.org/W2116034137",
        "https://openalex.org/W2903298154",
        "https://openalex.org/W2096340575",
        "https://openalex.org/W3173802674",
        "https://openalex.org/W2548467786",
        "https://openalex.org/W2462007197",
        "https://openalex.org/W2134009291",
        "https://openalex.org/W2965518110",
        "https://openalex.org/W2084104913",
        "https://openalex.org/W2170282111",
        "https://openalex.org/W2130913281",
        "https://openalex.org/W1965091013",
        "https://openalex.org/W2162385209",
        "https://openalex.org/W2894652650",
        "https://openalex.org/W2523679382",
        "https://openalex.org/W3166265964",
        "https://openalex.org/W4303685003",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2103260426",
        "https://openalex.org/W4394943312",
        "https://openalex.org/W3138773240",
        "https://openalex.org/W4392619039",
        "https://openalex.org/W3106811464",
        "https://openalex.org/W2126434293",
        "https://openalex.org/W2521644033",
        "https://openalex.org/W2021074129",
        "https://openalex.org/W4366451788",
        "https://openalex.org/W3111494011",
        "https://openalex.org/W2068916031",
        "https://openalex.org/W55962619",
        "https://openalex.org/W2192374892",
        "https://openalex.org/W4367672983",
        "https://openalex.org/W4323345674",
        "https://openalex.org/W2624086276",
        "https://openalex.org/W4384154918",
        "https://openalex.org/W3170771812",
        "https://openalex.org/W2963469388",
        "https://openalex.org/W3102771275",
        "https://openalex.org/W4323036511",
        "https://openalex.org/W3214179544"
    ],
    "abstract": null,
    "full_text": "Toro et al. Journal of Biomedical Semantics           (2024) 15:19  \nhttps://doi.org/10.1186/s13326-024-00320-3\nRESEARCH Open Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nJournal of\nBiomedical Semantics\nDynamic Retrieval Augmented Generation \nof Ontologies using Artificial Intelligence \n(DRAGON-AI)\nSabrina Toro1, Anna V. Anagnostopoulos2, Susan M. Bello2, Kai Blumberg3, Rhiannon Cameron4, \nLeigh Carmody5, Alexander D. Diehl6, Damion M. Dooley4, William D. Duncan7, Petra Fey8, Pascale Gaudet9, \nNomi L. Harris10, Marcin P . Joachimiak10, Leila Kiani11, Tiago Lubiana12, Monica C. Munoz‑Torres13, \nShawn O‘Neil1, David Osumi‑Sutherland14, Aleix Puig‑Barbe15, Justin T. Reese10, Leonore Reiser16, \nSofia MC. Robb17, Troy Ruemping18, James Seager19, Eric Sid20, Ray Stefancsik15, Magalie Weber21, \nValerie Wood22, Melissa A. Haendel1 and Christopher J. Mungall10* \nAbstract \nBackground Ontologies are fundamental components of informatics infrastructure in domains such as biomedical, \nenvironmental, and food sciences, representing consensus knowledge in an accurate and computable form. How‑\never, their construction and maintenance demand substantial resources and necessitate substantial collaboration \nbetween domain experts, curators, and ontology experts.\nWe present Dynamic Retrieval Augmented Generation of Ontologies using AI (DRAGON‑AI), an ontology generation \nmethod employing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). DRAGON‑AI can gen‑\nerate textual and logical ontology components, drawing from existing knowledge in multiple ontologies and unstruc‑\ntured text sources.\nResults We assessed performance of DRAGON‑AI on de novo term construction across ten diverse ontologies, \nmaking use of extensive manual evaluation of results. Our method has high precision for relationship generation, \nbut has slightly lower precision than from logic‑based reasoning. Our method is also able to generate definitions \ndeemed acceptable by expert evaluators, but these scored worse than human‑authored definitions. Notably, evalua‑\ntors with the highest level of confidence in a domain were better able to discern flaws in AI‑generated definitions. We \nalso demonstrated the ability of DRAGON‑AI to incorporate natural language instructions in the form of GitHub issues.\nConclusions These findings suggest DRAGON‑AI’s potential to substantially aid the manual ontology construction \nprocess. However, our results also underscore the importance of having expert curators and ontology editors drive \nthe ontology generation process.\nKeywords Ontologies, Large language models, Biocuration, Artificial intelligence, Knowledge graphs, Ontology \nengineering\n*Correspondence:\nChristopher J. Mungall\ncjmungall@lbl.gov\nFull list of author information is available at the end of the article\nPage 2 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \nBackground\nOntologies are structured representations of knowl -\nedge, consisting of a collection of terms organized \nusing logical relationships and textual information. In \nthe life sciences, ontologies such as the Gene Ontology \n(GO) [1 ], Mondo [2 ], Uberon [3 ], and FoodOn [4 ] are \nused for a variety of purposes such as curation of gene \nfunction and expression, classification of diseases, or \nannotation of food datasets. Ontologies are core com -\nponents of major data generation projects such as The \nEncyclopedia of DNA Elements (ENCODE) [5 ] and the \nHuman Cell Atlas [6 ]. The construction and mainte -\nnance of ontologies is a knowledge- and resource-inten -\nsive task, carried out by dedicated teams of ontology \neditors, working alongside the curators who use these \nontologies to curate literature and annotate data. Due \nto the pace of scientific change, the rapid generation \nof diverse data, the discovery of new concepts, and \nthe diverse needs of a broad range of stakeholders, \nmost ontologies are perpetual works in progress. Many \nontologies have thousands, or tens of thousands of \nterms, and are continuously growing. There is a strong \nneed for tools that help ontology editors fulfill requests \nfor new terms and other changes.\nCurrently, most ontology editing workflows involve \nmanual entry of multiple pieces of information (also \ncalled axioms) for each term or class in the ontology. \nThis information includes the unique identifier, a human-\nreadable label, a textual definition, as well as relationships \nthat connect terms to other terms, either in the same \nontology or a different ontology [7]. For example, the Cell \nOntology (CL) [8] term with the ID CL:1001502 has the \nlabel “mitral cell” , a subClassOf (is-a) relationship to the \nterm “interneuron” (CL:0000099), a “has soma location” \nrelationship [9] to the Uberon term “olfactory bulb mitral \ncell layer” (UBERON:0004186), as well as a textual defini-\ntion: The large glutaminergic nerve cells whose dendrites \nsynapse with axons of the olfactory receptor neurons in the \nglomerular layer of the olfactory bulb, and whose axons \npass centrally in the olfactory tract to the olfactory cortex. \nMost of this information is entered manually, using either \na dedicated ontology development environment such as \nProtégé [10] or using spreadsheets that are subsequently \ntranslated into an ontology using tools like ROBOT [11]. \nIn some cases, the assignment of an is-a  relationship \ncan be automated using OWL reasoning [12], but this \nrelies on the ontology developer specifying logical defini -\ntions (a particular kind of axiom) for a subset of terms in \nadvance. This strategy is used widely in multiple different \nbiological ontologies (bio-ontologies), in particular, those \ninvolving many compositional terms, resulting in around \nhalf of the terms having subclass relationships automati -\ncally assigned in this way [13–16].\nExcept for the use of OWL reasoning to infer is-a  rela-\ntionships, the work of creating ontology terms is largely \nmanual. The field of Ontology Learning (OL) aims to use \na variety of statistical and Natural Language Processing \n(NLP) techniques to automatically construct ontologies, \nbut the end results still require significant manual post-\nprocessing and manual curation by experts [17], and cur -\nrently no biological ontologies make use of OL. Newer \nMachine Learning (ML) techniques such as link predic -\ntion leverage the graph structure of ontologies to predict \nnew links, but state-of-the-art ontology link prediction \nalgorithms such as rdf2vec [18] and owl2vec* [19] have \nlow accuracy, and these also have yet to be adopted in \nstandard ontology editing workflows.\nA new approach that shows promise for helping to \nautomate ontology term curation is instruction-tuned \nLLMs [20] such as the gpt-4 model that underpins \nChatGPT [21]. LLMs are highly generalizable tools that \ncan perform a wide range of generative tasks, including \nextracting structured knowledge from text and generat -\ning new text [22, 23]. One area that has seen widespread \nadoption of LLMs is software engineering, where it is \nnow common to use tools such as GitHub Copilot [24] \nthat are integrated within software development environ-\nments and perform code autocompletion. We have previ-\nously noted analogies between software engineering and \nontology engineering and have successfully transferred \ntools and workflows from the former to the latter [25]. \nWe are therefore drawn to the question of whether the \nsuccess of generative AI in software could be applied to \nontologies.\nHere we describe and evaluate DRAGON-AI, an LLM-\nbacked method for assisting in the task of ontology term \ncompletion. Given a portion of an ontology term (for \nexample, the label/name, or the definition), the goal is \nto generate other requisite parts (for example, a textual \ndescription, or relationships to other terms). Our method \naccomplishes this using combinations of latent knowl -\nedge encoded in LLMs, knowledge encoded in one or \nmore ontologies, or semi-structured knowledge sources \nsuch as GitHub issues, using a Retrieval Augmented Gen-\neration (RAG) approach. RAG is a common technique \nused to enhance the reliability of LLMs by combining \nthem with an existing knowledge base or document store \n[26]. RAG is typically implemented by indexing docu -\nments or records as vectors created from textual embed -\ndings—the most similar documents are retrieved in \nresponse to a query, and injected into the LLM prompt. \nWe demonstrate the use of DRAGON-AI to generate \nboth logical relationships and textual definitions over ten \ndifferent ontologies drawn from the Open Biological and \nBiomedical Ontologies (OBO) Foundry [27]. To evaluate \nthe automated textual definitions, we recruited ontology \nPage 3 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \neditors from the OBO community to rank these defini -\ntions according to three criteria.\nWe demonstrate that DRAGON-AI is able to achieve \nmeaningful performance on both logical relationship and \ntext generation tasks.\nImplementation\nDRAGON-AI is a method that allows for AI-based auto-\ncompletion of ontology objects. The input for the method \nis a partially completed ontology term (for example, just \nthe term label, such as “hydroxyprolinuria”), and the out -\nput is a JSON or YAML object that has all desired fields \npopulated, including the text definition, logical defini -\ntion, and relationships.\nThe procedure is shown in Fig.  1. As an initial step, \neach ontology term and any additional contextual \ninformation is translated into a vector embedding, which \nis used as an index for retrieving relevant terms. Addi -\ntional contextual information can include the contents of \na GitHub issue tracker, which might contain text or semi-\nstructured information of relevance to the request. The \nmain ontology completion step works by first construct -\ning a prompt using relevant contextual information. The \nprompt is passed as an input to an LLM, and the results \nare parsed to retrieve the completed term object.\nIndexing ontologies and ontology embeddings\nAs an initial step, DRAGON-AI will create a vector \nembedding [28] for each term. Each term is represented \nas a structured object which is serialized using JSON, fol -\nlowing a schema with the following properties:\nFig. 1 The DRAGON‑AI ontology term completion process. (1) As an initial preprocessing step, knowledge resources (such as ontologies \nand GitHub issues) are indexed in a vector database. (2) A user provides a partial ontology term object (here, a term with only the label \nof the desired term “hydroxyprolinuria” is provided). (3) The vector database is queried for similar terms (e.g. cystathioninuria, hydroxyproline) \nor other relevant pieces of information (e.g. a GitHub issue). (4) A prompt is generated from a template, incorporating the most similar items \nin the vector database. (5) The prompt is provided as textual input to an LLM, which returns a completed JSON object. Either local or remote \nLLMs can be used. (6) The parsed object is returned to the user. Note that this figure uses YAML syntax to represent JSON objects, for the sake \nof compactness\nPage 4 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \n• id: a translated identifier for the term, as described \nbelow\n• label: a string with a human readable label or name \nfor the term\n• definition: an optional string with a human-read -\nable textual definition\n• relationships: a list of relationship objects\n• original_id: the original untranslated identifier\n• logical_definitions: an optional list of rela -\ntionship objects\nA relationship object has the following properties:\n• predicate: a translated identifier for the relation -\nship type. For bio-ontologies, this is typically taken \nfrom the Relation Ontology [29], or is the subClassOf \npredicate, for is-a relations\n• target: a translated identifier for the term that the \nrelationship points to, either in the same ontology, or \na different ontology\nOntology terms are typically referred to using non-\nsemantic numeric identifiers (for example, CL:1001502). \nThese can confound LLMs, which have a tendency to \nhallucinate identifiers [30]. In our initial experiments, \nwe found LLMs tend to perform best if presented with \ninformation in the same way that information is pre -\nsented to humans, presumably as the majority of their \ntraining data is in this form. Therefore, we chose to trans-\nform all identifiers from a non-semantic numeric form \n(e.g. CL:1001502) to a symbol represented by the ontol -\nogy term label in camel case format (e.g. MitralCell). An \nexample is shown in Table 1.\nWe create a vector embedding for each term by first \ntranslating the object to text, and then embedding the \ntext. The text is created by concatenating the label, defi -\nnition, and relationships as key-value pairs. For this \nstudy we used the OpenAI text-embedding-ada-002 text \nembedding model, accessed via the OpenAI API.\nWe store objects and their embeddings using the Chro-\nmaDB database [31]. This allows for efficient queries to \nretrieve the top k matching objects for an input object, using \nthe Hierarchical Navigable Small World graph search algo-\nrithm [32].\nIndexing unstructured and semi‑structured knowledge\nAdditional contextual knowledge can be included in \nDRAGON-AI to inform the term completion pro -\ncess – for example, publications from PubMed, articles \nfrom Wikipedia, or documentation intended for human \nontology editors. One of the most important sources of \nknowledge for ontology terms is the content of GitHub \nissue trackers, where new term requests and other term \nchange requests are proposed and discussed. Information \nin these trackers may be free text, or partially structured.\nWe used the GitHub API to load GitHub issues and \nstore the resulting JSON objects, which are indexed with-\nout any specialized pre-processing. The text-serialized \nform of the GitHub JSON object is used as input for the \nembeddings. We store these JSON objects separately \nfrom the main ontology term objects.\nPrompt generation using Retrieval Augmented Generation\nAt the core of the DRAGON-AI approach is the genera -\ntion of a prompt that is passed as input to an LLM. The \nprompt includes the partial term, and an instruction \nTable 1 Example JSON structure used in DRAGON‑AI\nIn this example, OBO format syntax is shown at the top, including the non-semantic numeric identifier (id), the term label (name), the SubClass_Of relationship (is_a), \nanother relationship using terms from the Relation Ontology (RO) and the Uberon ontology, and the human-readable textual definition\nThe corresponding JSON object form shown is below, including the camel case format of the term label (id), the non-semantic numeric identifier (original_id), \nthe relationships using a predicate and target properties, also in camel case format, and the term definition. It should be noted that the JSON form omits some \ninformation from the OBO Format (e.g. the provenance of the definition)\nid: CL:1,001,502\nname: mitral cell\nis_a: CL:0000099 ! interneuron\nrelationship: RO:0002100 UBERON:0004186 ! has-soma-location\\ olfactory bulb mitral cell layer\ndefinition: “The large glutaminergic nerve cells whose dendrites synapse with axons of the olfactory receptor neurons in the glomerular layer \nof the olfactory bulb, and whose axons pass centrally in the olfactory tract to the olfactory cortex” [MP:0009954]\n{\n“id”: “MitralCell” ,\n“original_id”: “CL:1,001,502”\n“relationships”: [\n{ “predicate”: “SubClassOf” , “target”: “Interneuron”},\n{ “predicate”: “HasSomaLocation” , “target”: “OlfactoryBulbMitralCellLayer”}\n],\n“definition”: “The large glutaminergic nerve cells whose dendrites synapse with axons of the olfactory receptor neurons in the glomerular layer \nof the olfactory bulb, and whose axons pass centrally in the olfactory tract to the olfactory cortex”\n}\nPage 5 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \ndirecting the model to complete the term, filling missing \ninformation, and return as a JSON object.\nIn order to guide the LLM to create a term that is \nsimilar in style to existing terms, and to guide the LLM \nto pick existing terms in relationships, we provide addi -\ntional context within the prompt. This additional context \nincludes existing relevant terms, provided in the same \nJSON format as the intended response. When prompting \nLLMs, it is common to include a small set of examples \nto help guide the model to provide the best responses \n(few-shot learning). One approach here is to use a static \nor fixed set of examples, but the drawback of this is that \nthe pre-selected examples may not be applicable to the \nspecific request from the user. Ideally, examples would be \nselected based on relevancy.\nWe use RAG as the general approach to retrieve the \nmost relevant information. As a first step, the partial \nterm object provided by the user is used as a query to the \nontology terms loaded into the ontology vector index. An \nembedding is created from the text fields of the object \n(using the same embedding model as was used to index \nthe ontology), and this is used to query the top k results \n(k is 10 by default). These form the in-context examples \nfor the prompt. The intent is to retrieve terms that are \nsimilar to the intended term to inform the prediction \nof the completed term; for example, if the query term is \n“hydroxyprolinuria” , then similar terms in the ontology \nsuch as “cystathioninuria” will be informative.\nEach retrieved example forms an input–output training \npair which is concatenated directly into the prompt by \nserializing the JSON object, for example:\n input:\n{“label”: “cystathioninuria”}\noutput:\n{“definition”: “excretion of excessive amounts of cysta -\nthionine in the urine” ,\n“Relationships”: [ {“predicate”: “subClassOf” , “target”: \n“ Aminoaciduria”} ] }\nTo diversify search results, we implement Maximal \nMarginal Relevance (MMR) [33] in order to re-rank \nresults. This helps with inclusion of terms that inform \nmultiple cross-cutting aspects of the requested term, \nincluding terms from other relevant ontologies. For \nexample, if the input is “hydroxyprolinuria” then the \nhighest-ranking terms may be other phenotypes involv -\ning circulating molecules, but by diversifying search \nresults we also include relevant chemical entities from \nChEBI like “hydroxyproline” .\nOptionally, additional information other than the \nsource ontology can be included in the prompt. This \npotentially includes GitHub issues (accessed via the \nGitHub API), documentation written by and for ontology \ndevelopers, and PubMed articles. For this study we only \nmade use of GitHub issues. For these sources we also use \na RAG method to select only the most semantically simi -\nlar documents.\nDifferent LLMs have different limits on the combined \nsize of prompt and response. In order to stay within these \nlimits, we reduce the number of in-context examples to \nthe maximum number that still fits within the limit, or \nthe number provided by the user, whichever is greater.\nPrompt passing and result parsing\nDRAGON-AI allows for a number of different ways \nto extract structured information as a response. These \ninclude using OpenAI function calls, or using a recur -\nsive-descent approach via the SPIRES algorithm [34]. \nFor this study we evaluated a pure RAG-based in-context \napproach, as shown in Fig. 1.\nThis prompt is presented to the LLM, which responds \nwith a serialized JSON object analogous to the in-context \nexamples. This response is parsed using a standard JSON \nparser, with additional preprocessing to remove extrane -\nous preamble text, and the results are merged with the \ninput object to form the predicted object.\nRelationship predictions are further post-processed \nto remove relationships to non-existent terms in the \nontology or imported ontologies. Some of these corre -\nspond to meaningful relationships to terms that have yet \nto be added. In the future, the system may be extended \nto include a step that fills in missing terms, but the cur -\nrent behavior is to be conservative when predicting \nrelationships.\nEvaluation\nWe used 10 different ontologies in our evaluation: the \nCell Ontology (CL) [8], UBERON, the Gene Ontology \n(GO), the Human Phenotype Ontology (HP) [35], the \nMammalian Phenotype Ontology (MP) [36], The Mondo \ndisease ontology (MONDO), the Environment Ontol -\nogy (ENVO) [37], the Food Ontology (FOODON), the \nOntology of Biomedical Investigations (OBI) [38], and \nthe Ontology of Biological Attributes (OBA) [39]. These \nwere selected based on being widely used and impact -\nful and covering a broad range of domains, from basic \nscience through to clinical practice, with representa -\ntion outside biology (the Environment Ontology and \nFoodOn). This selection also represents a broad range of \nontology development styles, from highly compositional \nontologies making extensive use of templated design pat -\nterns (OBA) to more individually structured. All selected \nontologies make use of Description Logic (DL) axioma -\ntizations, allowing for the use of reasoning to auto-clas -\nsify the ontology, providing a baseline for comparison. \nTable 2 shows a summary of which tasks were performed \nPage 6 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \nand evaluated on which ontologies. Table  3 has details of \nthe models that were used in the study.\nWe subdivided each ontology into a core ontology plus \na testing set of 50 terms. Where possible, we selected test \nterms from the set of terms that were added to the ontol -\nogy after November 2022, to minimize the possibility of \ntest data leakage. This was not possible for ENVO, which \nhas a less frequent release schedule, with the most recent \nrelease at the time of analysis being from February 2023, \nso this ontology included terms added in 2021 and 2022. \nUberon also had fewer new terms in 2023, so the test set \nfor this ontology was 40 terms.\nWe chose three tasks: prediction of (1)  relationships, \n(2) definitions, and (3) logical definitions. For each task, \nthe test set consists of ontology term objects with the \nfield to be predicted masked (other fields such as the \nontology term identifier were also masked, as these are \nanother source of training data leakage). For example, to \npredict relationships, the text objects have only labels and \ntext definitions present. We excluded OBI, HP , and MP \nfrom the logical definition analysis as these ontologies \nhave more complex, nested logical definitions that don’t \nconform to the simple style supported in DRAGON-AI. \nWe only evaluated textual definitions for nine of the ten \nontologies based on evaluator expertise.\nWe tested three models (gpt-4, gpt-3.5-turbo, and \nnous-hermes-13b-ggml) against all ontologies for the \nthree tasks. The first two models are proprietary closed \nmodels accessed via an API; the latter model is open, and \nwas executed locally on an M1 MacOS system.\nRelationship prediction evaluation\nOne of the main challenges in ontology learning is evalu -\nation, since the construction of ontologies involves some \nsubjective decisions, and many different valid represen -\ntations are possible [40]. An additional challenge is that \nontologies allow for specification of things at different \nlevels of specificity. For the relationship prediction task, \nwe chose to treat the existing relationships in the ontol -\nogy as the gold standard, recognizing this may penalize \nalternative but valid representations.\nIf a predicted relationship matches a relationship that \nexists in the ontology, this counts as a true positive. If a \npredicted relationship is more general than a relationship \nin the ontology, then we do not count this as a full true \npositive, but instead treat it as an intermediate between \ntrue positive and false negative. We use Information \nContent (IC) based scores, in the same fashion as Criti -\ncal Assessment of Function Annotation (CAFA) evalu -\nations [41]. The IC of an ontology term is calculated as \nthe negative log of the probability of observing that \nTable 2 Ontologies and ontology versions used for evaluation\nFor each ontology we used the standard release product, except for GO, where we used the go-plus version, which has additional relationships to other ontologies. \nWe took the most recent available version of each ontology, and separated the most recent terms into a test set. The minimum (oldest) date of each term is shown\nOntology Version Date of oldest term in \ntest set\nTerms Tested Tasks Performed and Evaluated\nCL 2023–07‑20/cl.owl 2023–01‑10 50 all\nENVO 2023–02‑13/envo.owl 2021–05‑14 50 all\nFOODON 2023–05‑03/foodon.owl 2023–01‑01 50 all\nGO 2023–07‑27/extensions/go‑plus.owl 2023–01‑03 50 all\nHP 2023–07‑21/hp.owl 2023–01‑16 50 relationships, definitions\nMONDO 2023–08‑02/mondo.owl 2023–04‑01 50 all\nMP 2023–08‑09/mo.owl 2023–02‑08 50 relationships, definitions\nOBA 2023–08‑24/oba.owl 2022–11‑26 50 all\nOBI 2023–07‑25/obi.owl 2022–12‑14 50 relationships\nUBERON 2023–07‑25/uberon.owl 2023–01‑18 40 all\nTable 3 Models evaluated, plus their versions/checkpoints\nThe OpenAI training set cutoff dates are based on what is reported on the OpenAI website\nModel Checkpoint / Version Training set cutoff Access Description\ngpt‑3.5‑turbo 0613 2021–09 API Proprietary model from OpenAI\ngpt‑4 0613 2021–09 API Proprietary model from OpenAI\nnous‑hermes‑13b‑ggml 2023–06 2023–02 Local Local quantized model fine‑\ntuned from llama\nPage 7 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \nterm as a subsumer of a random term in the ontology, \nIC(t) = -log(P(t)). We calculate the IC of the broader pre -\ndicted term (ICp) and the narrower expected term (ICe), \nand assign the true positive to be the ratio ICp/ICe, and \nthe false negative as 1-ICp/ICe.\nA relationship (s, p, o) is counted as more general if \nthe target node is traversable from the subject node over \na combination of is-a  (subClassOf) relationship and p \nrelationship.\nAs a baseline, we also include OWL reasoning results \nusing the Elk reasoner [42]. This is only applicable to \nsubsumption (SubClassOf) relationships. For each sub -\nsumption relationship in the ontology, we remove the \nrelationship and use the reasoner to determine if the \nrelationship is recapitulated. We use the OWLTools [43] \ntag-entailed-axioms command to do this. As all ontolo -\ngies use OWL Reasoning as part of their release process, \nthe precision of reasoning, when measured against the \nreleased ontology, is 1.0 by definition. However, recall and \nF1 [44] can be informative to determine breadth of cover-\nage of reasoning.\nDefinition prediction evaluation\nFor the definition prediction task, we could not employ \nthe same strategy as evaluation, as it is very rare for a pre-\ndicted definition to be an exact match for the one that was \nmanually authored in the ontology – however, these can -\nnot be counted as false positives as they may still be good \ndefinitions. We therefore used two methods for evalu -\nating definitions: (1)  measuring the semantic distance \nbetween predicted definition and curated definition using \nBERTScore [45]; (2) manual assessment of predicted and \ncurated definitions. For scoring definitions, we used the \nbert-score package from PyPI, and used default param -\neters (English language, roberta-large as model).\nFor the manual evaluation, we enlisted ontology editors \nand curators to score predicted and curated definitions.\nWe first aggregated all generated definitions using all \nmodels along with the definitions that had previously \nbeen manually curated for the test set terms. We assigned \neach evaluator a task of evaluating a set of definitions by \nscoring using three different criteria. See supplementary \nmethods for the templates used. The three scoring crite -\nria were:\n• Biological accuracy: is the textual definition biologi -\ncally accurate?\n• Internal consistency: is the structure and content of \nthe definition consistent with other definitions in the \nontology, and with the style guide for that ontology?\n• Overall score: overall utility of the definition.\nFor each of these metrics, an ordinal scale of 1–5 was \nused, with 1 being the worst, 3 being acceptable, and \n5 being the best. Assigning a consistency score was \noptional. Evaluators could also choose to use the same \nscore for accuracy and overall score. Additionally, the \nevaluator could opt to provide a confidence score for their \nranking, also on a score ranging from 1 (low confidence) \nto 5 (high confidence). We provided a notes column to \nallow for additional qualitative analysis of the results.\nAt least two evaluators were assigned to each ontology. \nEvaluators received individualized spreadsheets and were \nblinded from the source of the definition. They worked \nindependently, and did not see the results of other evalu -\nators until their task was completed. Evaluators were also \nasked to provide a retrospective qualitative evaluation of \nthe process, which we include in the discussion section.\nTo measure inter-annotator agreement we calculated \nthe Intraclass Correlation Coefficient (ICC) measure. A \none-way analysis of variance (ANOVA) model was fit -\nted to the data, treating the evaluator as a random effect. \nFrom the ANOVA table, we extracted the mean squares \nbetween evaluators (MSB) and the mean squares within \nevaluators (MSE). The ICC was then calculated using the \nformula:\nWe calculated the ICC for three metrics: accuracy, con-\nsistency, and score. As a baseline, values above 0.5 are \nconsidered to indicate moderate consistency, with 0.75 \nand over indicating good consistency.\nAggregating ICCs\nThe overall ICC values for accuracy, consistency, and \nscore were computed by filtering the dataset based on a \nminimum confidence threshold and then applying the \nICC calculation method to each metric. This provided \na robust measure of inter-rater reliability for each of the \nevaluated metrics.\nExecution\nOur workflow is reproducible through our GitHub repos-\nitory [46], also archived on Zenodo [47]. A Makefile is \nused to orchestrate extraction of ontologies, splitting \ntest sets, loading into a vector database, and performing \npredictions. A collection of Jupyter Notebooks is used to \nevaluate and analyze the results.\nResults\nAI‑generated relationships have high precision \nbut moderate recall\nFor each generated term across all 10 ontologies, we eval-\nuated the generated relationships by comparing them to \nICC = (MSB − MSE )/(MSB + (k− 1) × MSE)\nPage 8 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \nexisting relationships in the ontology. We subdivided this \nevaluation into two parts: (1) evaluating only subsuming \nparents (is-a /SubClassOf relationships) and (2)  evaluat -\ning all relationships (making use of heterogeneous rela -\ntionship types). We compared AI-generated relationships \nagainst the use of DL reasoning using the Elk reasoner.\nThe aggregated results of the evaluation are summa -\nrized in Table  4. In all cases, the best performing model \nfor use with DRAGON-AI is gpt-4. On the SubClassOf \nsubtask, the best performing model has high precision \n(0.894), which is comparable with, but less precise than, \nusing DL reasoning (which by its nature always has maxi-\nmal precision). On this subtask, DRAGON-AI has better \nrecall and F1 than using reasoning. For the heterogene -\nous relationship subtask, the overall scores are lower \n(0.802 for precision), but still indicate strong perfor -\nmance. Note that this subtask is outside the capabilities \nof DL reasoning.\nWe also observed that different ontologies may be more \nor less amenable to relationship prediction, as shown in \nFig.  2. However, note that the test set distribution may \nnot be reflective of the overall distribution in the ontol -\nogy, as we limited testing to new terms only.\nAI‑generated definitions score well, but less than existing \ndefinitions\nFor all ontologies, we generated text definitions for each \nterm in the test set, providing only the label and relation -\nships, plus logical definitions if present. We evaluated the \ndefinitions automatically using semantic similarity, and \nmanually using expert evaluation.\nThe BERTScore results are shown in Table  5. We \ninclude as a baseline the score of the definition versus a \nrandomly selected definition across all ontologies. The \nresults show that predicted definitions generally have \ngood correspondence with manually curated definitions \n(with gpt models having an F1 >  = 0.92). While gpt-0.3.5-\nturbo has higher scores than gpt-4, with nous-hermes-\n13b last, the difference is minimal.\nWhile the BERTScore method can be used to rank \nindividual models, it does not directly inform us of the \nquality of the generated definitions. In addition, just \nbecause a generated definition doesn’t semantically \nmatch a curated definition, it doesn’t indicate that the \ngenerated definition is of poorer quality, as there are \nmany valid ways to construct a definition for a concept.\nIn order to understand the quality of the generated def-\ninitions, we employed manual evaluation. Generated def -\ninitions were evaluated by curators and ontology editors \nand scored according to different criteria. The ICC values \nfor accuracy, consistency, and overall score were 0.799, \n0.737, 0.770, indicating moderate to good consistency.\nOverall, definitions authored by human curators scored \nhighest on all three metrics (Table  6). DRAGON per -\nformed acceptably (consistently above a grade of 3, which \nwas considered acceptable) regardless of the underlying \nmodel, with gpt models outperforming the only open \nmodel evaluated (nous-hermes-13b). The performance \ngap between curated definitions and generated defini -\ntions is statistically significant for all score types. The gap \nbetween gpt-3.5-turbo or gpt-4 and the open model was \nalso statistically significant. However, the gap between \ngpt-3.5-turbo and gpt-4 was not significant.\nThe results of the manual evaluation are also available \non HuggingFace [48].\nExperts are more likely to detect flaws in AI‑generated \ndefinitions\nThe difference between the manually authored defini -\ntions and the best AI generated definitions is statistically \nsignificant, yet moderate in effect. We hypothesized that \nthis difference would decrease as the evaluator confi -\ndence decreases – i.e. less confident evaluators would be \nless able to discriminate between a good definition and \nplausible yet flawed definition.\nWhen we plot the performance gap between the best \nperforming model and human curation, we can see a clear \ncorrelation between performance gap and confidence, \nwith the lowest confidence showing no discrimination \nbetween model-generated and human curation  (Fig.  3). \nThe correlation is highly significant (Pearson correlation \ncoefficient 0.973).\nTable 4 DRAGON‑AI results for relationship prediction task\nWe partition into two subtasks: filtered for SubClassOf, and filtered for all relationship types (heterogeneous relationship predictions). We also show OWL DL \nReasoning results for the SubClassOf task. Note that by definition OWL DL reasoning is always completely precise as all entailments follow from existing axioms\nSubClassOf Task All Relationship Types Task\nmethod model precision recall F1 precision recall F1\nDRAGON gpt‑3.5‑turbo 0.846 0.419 0.561 0.758 0.446 0.562\nDRAGON gpt‑4 0.894 0.5 0.642 0.802 0.505 0.62\nDRAGON nous‑hermes‑13b 0.73 0.353 0.476 0.64 0.355 0.457\nReasoner n/a 1.0* 0.337 0.504 n/a n/a n/a\nPage 9 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \nDRAGON‑AI can read and interpret GitHub issues \nto improve performance\nWe investigated whether providing background knowl -\nedge from GitHub issue trackers would improve the \nquality of generated definitions. All of the evalu -\nated ontologies are in the OBO Foundry, and all have \ntheir own issue tracker. These trackers are used by the \nbroader community (domain experts, curators, users \nof the ontology) to file issues requesting changes to \nFig. 2 Metrics for relation prediction across 10 ontologies (gpt‑4 results only, filtered for SubClassOf/is‑a and all relationship types)\nTable 5 DRAGON‑AI BERTScore evaluation\nScoring of generated definition against curated definition using BERTScore\nmethod model name F1 P R\nDRAGON gpt‑3.5‑turbo 0.923 0.933 0.914\nDRAGON gpt‑4 0.920 0.928 0.912\nDRAGON nous‑hermes‑13b 0.916 0.922 0.910\nRandom Selection n/a 0.838 0.845 0.832\nTable 6 DRAGON‑AI performance on definition generation task\nA comparison of base performance of DRAGON on definition generation when \ncompared with existing editor-provided definitions. Evaluator scores shown \nfor three score categories (accuracy, consistency, and overall score). Evaluators \nevaluated definitions generated by three different models, alongside existing \nontology definitions. Evaluators were not shown the source of definitions until \nafter evaluation\nmethod model name accuracy score consistency\nDRAGON gpt‑3.5‑turbo 4.058 3.632 3.735\nDRAGON gpt‑4 3.97 3.567 3.689\nDRAGON nous‑hermes‑13b 3.776 3.389 3.566\ncurator human 4.326 4.069 4.13\nPage 10 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \nthe ontology or new terms. These issues are writ -\nten largely in natural language, sometimes in a semi-\nstructured form. For example, a typical request for the \nCell Ontology is exemplified in issue 2241 [49], which \nrequests a new term “liver-resident natural killer cell” . \nThe requestor also provides a candidate textual defini -\ntion, synonyms, and relationships to other terms. An \nissue may also include further comments from ontology \neditors to others, sometimes with extended discussions \nbefore arriving at consensus. Ontology editors typically \nwork through issues in the GitHub tracker, implement -\ning them manually using the Protégé ontology editing \ntool. Assistance with this task is therefore helpful for \nthe general ontology editing workflow.\nIn order to leverage GitHub requests, we applied \nthe method RAG   + github, in which RAG is used to \nretrieve both the most relevant ontology terms and the \nmost relevant GitHub issues, and both are included in \nthe prompt. We restricted this analysis to two models \n(gpt-4 and gpt-3.5-turbo) and three ontologies (CL with \n2121 issues, UBERON with 2300 issues, and ENVO \nwith 1436 issues indexed).\nIncluding the GitHub issues improved performance \nof all models, although performance was still beneath \nmanually authored definitions (Table  7). The difference \nbetween RAG with and without GitHub is statistically \nsignificant for both accuracy and score.\nOverall, this indicates that generative AI can make \nuse of sources of information intended primarily for \nhumans as a part of their term creation workflow.\nLogical definitions can be generated with high accuracy \nin some ontologies\nWe evaluated the ability of DRAGON-AI to generate \nlogical definitions across four different ontologies. Only \na subset of ontologies was used, as other ontologies did \nnot have a sufficient number of logical definitions in \nnewly added terms to test against, or logical definitions \ndid not conform to the simple genus-differentia form.\nThe results are shown in Fig.  4, demonstrating wide \nvariability.\nFig. 3 Performance gap vs confidence level. If an evaluator lacked confidence in their assessment (lower confidence level), they were more likely \nto assign an LLM‑generated definition a comparable score to a human curated one. As the evaluator confidence increases, the evaluator is more \nlikely to rank the LLM‑generated definition lower than the human one\nTable 7 Comparison of scores when GitHub issues are included \nas background knowledge\nMethod Model name Accuracy Score Consistency\nDRAGON gpt‑3.5‑turbo 4.067 3.626 3.709\nDRAGON + gh gpt‑3.5‑turbo 4.182 3.717 3.733\nDRAGON gpt‑4 4.041 3.608 3.754\nDRAGON + gh gpt‑4 4.241 3.805 3.893\ncurator human 4.439 4.158 4.182\nPage 11 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \nDiscussion\nGenerative AI shows promising capabilities to assist \nin ontology editing workflows, but should be used \nwith caution\nOur results demonstrate the feasibility of incorporating \ngenerative AI into ontology development workflows. For \nrelationship generation, when we compare with exist -\ning ontology relationships, we demonstrate high preci -\nsion, and moderate recall/F1. This indicates that results \nare generally correct, but may be incomplete. Even when \nAI results do not conform to asserted relationships in \nthe ontology, they frequently represent a valid perspec -\ntive that could be incorporated. For definition genera -\ntion, AI-authored definitions rank close to yet lower than \nhuman-authored ones. The DRAGON-AI system is also \nable to leverage textual information from other sources \nto enhance its results. Additionally, DRAGON-AI is able \nto leverage additional textual sources of information such \nas GitHub issues.\nNote that we do not expect AI-generated axioms to \nbe perfect in order for them to be useful. We envision \nDRAGON-AI being used as part of an autocomplete sys -\ntem within existing ontology development environments \nlike Protégé, or in tabular editing environments used in \nconjunction with tools such as ROBOT, or as part of an \nintegrated agent-based development environment such \nas OpenDevin [50]. Here the editor can be presented with \nsuggested axioms to add, based on partial information \nthey have entered, with the ability to easily accept, reject, \nor modify AI-generated suggestions, and ultimately even \nthe ability to interact with the system using natural lan -\nguage in order to hone results. This kind of autocomplete \nparadigm is already widely used in software development \nenvironments through tools such as GitHub Copilot. \nCopilot has been widely adopted by software develop -\ners (over one million paid subscribers), with most users \nself-reporting increased productivity [51]. This is despite \nthe fact that Copilot suggestions are only accepted 30% \nof the time, in contrast with the precision of 82% we have \nachieved for ontology completion.\nHowever, any such tool should be used with caution. \nFor software development, some studies have shown that \nwhile AI tools can boost productivity, they can also be \na liability for novice users [52]. Our results also showed \nthat novice editors are more likely to be “tricked” by \nthe AI. We demonstrated that if an evaluator had lower \nconfidence in a domain, they were more likely to accept \na generated definition on face value, even if incorrect. \nEvaluators with more experience in a domain are more \nlikely to spot subtle problems with generated terms. We \ninformally call this the “gaslighting effect” , and in fact a \nnumber of evaluators commented on the fact that they \nthought they were being “gaslit” by the AI. This is similar \nto a previously observed phenomenon of LLMs “sand -\nbagging” their users [53]. This means that AI should be \nused with caution, particularly in the hands of less expe -\nrienced ontology editors. Of course, it is also important \nto point out that ontology developers can make mistakes \nwithout AI, and all ontologies should employ appropri -\nate centralized QA/QC measures. In fact, assisting with \nFig. 4 F1 score of logical definition prediction across four ontologies\nPage 12 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \nwhole-ontology QA/QC represents a potential useful \nfuture application of AI.\nOverall, our goal is to enhance the experience of ontol -\nogy editors and improve productivity. A recent study \nindicates that generative AI can help restructure tasks \ntowards idea generation and away from tedious repeti -\ntive tasks [54]. Our vision for DRAGON-AI is a tool that \nallows ontology editors and curators to employ their \ndeep understanding of a domain to efficiently translate \nthat knowledge, minimizing tedious tasks such as copy -\ning information from reference sources.\nChallenges in evaluating ontologies by LLMs\nThere are a number of challenges in evaluating the effec -\ntiveness of LLMs in ontology generation, including test \ndata leakage and the inherent subjectivity of ontology \nconstruction.\nTest data leakage occurs when the LLM training sets \nare contaminated with benchmark data. This is common, \ndue to the fact they are trained on internet-sized corpora \n[55, 56]. Most LLMs have effectively memorized most \npublic bio-ontologies. To minimize the possibility of test \ndata leakage, we used only terms that had been added \nto ontologies after the training data cutoff of the major \nGPT models. However, this limited the size of the test \ncorpus, and may have potentially limited the generaliz -\nability of the results, at least within ontologies (for many \nontologies, the terms entered in a one-year period may \nnot be representative of the overall content of the ontol -\nogy). It also makes it hard to evaluate the effectiveness of \nLLMs on de novo ontology construction, since the RAG \napproach makes heavy use of existing terms. Ontologies \nsuch as the GO and the Cell Ontology have existed for \nover two decades, and are the combined efforts of a mas -\nsive number of editors, curators, and domain experts. \nWe consider it a strength of the DRAGON-AI method \nthat it is able to leverage this prior work via RAG when \ngenerating new terms; we consider this task relevant to \nthe day-to-day efforts of biological ontology developers. \nHowever, it also means we did not address the question \nof how well the approach would perform on constructing \nan ontology from scratch, or from an early state. We are \ncurrently exploring the use of DRAGON-AI in the crea -\ntion of de novo ontologies in the environmental domain. \nWe have created an experimental ontology of over a \nthousand environmental variables and parameters for use \nin earth system simulation [57].\nIt is also important to emphasize that the success of AI \nmethods on the ontologies we evaluated depends largely \non the previous work of hundreds of ontology editors and \ncurators working over decades. Ontologies are included \nwithin the datasets used in LLM pre-training, so the \nLLM already comes pre-equipped to recognize common \npatterns employed within these ontologies.\nThe paradigm of using terms added after the training \ndate cutoff for evaluation is likely to become even less \neffective over time, as it becomes easier to update models \nwith new data. The version of gpt-4 and gpt-3.5-turbo we \nused in this evaluation had a training date cutoff of Sep -\ntember 2021, but the latest versions of these models have \nfar more recent cutoffs. The open models we used also \nhad recent cutoffs. Training a model from scratch with an \nolder cutoff for evaluation purposes is simply not feasi -\nble due to the massive costs involved in pre-training. It is \ntherefore vital that we invest in efforts to evaluate ontol -\nogy generation on new domains using terms that have \nnot been used in pre-training.\nThe other challenge involves the inherent subjectiv -\nity of ontology construction; there are different ways to \nrepresent the same thing. For our relationship predic -\ntion task we took the terms that were created by ontology \neditors as the gold standard, but the predicted relation -\nships that do not match these terms are not necessarily \nincorrect. When we manually examined the relationships \ncounted as false positives, many were alternate represen -\ntations, or simply relationships missing from the ontol -\nogy (see Supplementary material).\nTo overcome both these challenges, investing more in \nexpert human evaluation is essential.\nFuture directions\nCustomizing RAG \nOne of the strengths of DRAGON-AI is in-context learn-\ning from existing terms in the ontology. However, not all \nterms in an ontology are of equal quality. Ontologies that \nhave been developed over long time frames may include \n“legacy” terms that do not serve as good exemplars. We \ntherefore plan to extend the approach to allow the user \nto influence the ranking of terms returned by RAG, for \nexample by prioritizing newer terms (presumably these \nare more reflective of current best practice in the ontol -\nogy), or allowing the use of metadata that marks certain \nterms as being good examples of best practice for par -\nticular kinds of terms.\nAnother area we intend to explore is the use of hybrid \nvector store and graph database backends, layered on \nexisting triple stores such as Ubergraph [58]. This would \nallow for precise structured queries in retrieval, in addi -\ntion to retrieval based on semantic similarity of texts.\nIncorporation into ontology editing environments\nIn order for AI methods to be successful, they need to be \nseamlessly integrated into ontology editing workflows. \nFor future development, we are considering a number of \ndifferent potential workflows.\nPage 13 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \nThe first workflow would be integrating AI methods \ninto existing ontology development tools such as Protégé \nvia a plugin. The plugin would function analogous to AI-\nbased code completion in software development envi -\nronments; the editor would create a new term, provide \na label, and the plugin would suggest a completed term, \nwhich the editor could either accept outright, accept and \nthen modify, or reject.\nThe second workflow would be integration into a tabu -\nlar editing environment, where the editable tables are \nused as part of a tabular template-driven workflow, sup -\nported by a tool such as ROBOT templates [11], Dead \nSimple Design Patterns (DOSDPs) [59], OTTR templates \n[60], or LinkML [61].\nThe third workflow would be to design a new kind of \nuser interface that reflects a potential new role for ontol -\nogy editors, with less emphasis on data entry and more \non high level specification of requirements and evalu -\nation and honing of AI generated content. Here the \ninterface may focus on text-oriented interactions, as in \nChatGPT, coupled with easy ways to guide the AI. To \nthis end, we have commenced work on a general-purpose \nAI-driven curation Integrated development environment \n(IDE) called CurateGPT [62]. All of the workflows evalu -\nated in this manuscript are supported in the current UI. \nHowever, a number of challenges need to be overcome to \nmake this IDE usable. Some of these challenges involve \nthe current low latency of LLM prompt completion; \nothers involve making the interface more user friendly, \nwhich will require extensive feedback and iterative test -\ning from ontology editors and curators.\nA fourth workflow is integration directly into LLM chat \ninterfaces. One such mechanism is the “GPTs” feature of \nChatGPT. We have recently developed a ROBOT tem -\nplate GPT helper [63] and used this in the development \nof the Artificial Intelligence Ontology (AIO) [64].\nRegardless of which interface paradigm is followed, \nwe believe the most important functionality is a simple \nand intuitive way to accept, reject, or modify AI gener -\nated suggestions, as well as recording these responses, in \norder to continually improve the system.\nAutomated methods to validate generative AI results\nIn order to increase performance, and in particular, \nquality and reliability of results, we are exploring a two-\npronged approach to including automated validation as \npart of the DRAGON-AI workflow.\nThe first approach is to couple DRAGON-AI with an \nOWL reasoner: this will allow for filtering of redundant \nrelationships, as well as inference of implicit relation -\nships. Note that the recall of OWL reasoning increases \nwith the degree of axiomatization in ontologies, and \nmany ontologies are under-axiomatized. Here we \npropose an approach involving generation of additional \nconstraint style axioms, such as disjointness axioms, that \nwill allow for OWL reasoners to detect errors in gener -\nated terms. It should also be possible for DRAGON-AI \nto both generate and populate ontology design patterns \nsuch as DOSDPs.\nThe second approach involves using methods such as \nRAG to try to find evidence for generated statements in \nthe literature. In many ontologies, statements are accom -\npanied by provenance information, such as bibliographic \nreferences or references to sources such as Wikipedia. \nIn future studies we aim to combine DRAGON-AI with \nthe Evidence Agent in CurateGPT [62], which is able to \nretrieve relevant references and apply as evidence. This \ncould also be used as an additional filter.\nSupport for additional workflows\nIn this paper we demonstrate the use of DRAGON-AI \nfor term generation. However, for many ontologies, new \nterm requests only constitute one part of the overall \nworkflow. Maintenance and correction of existing terms \ncan also be resource-intensive, especially for ontologies \nwith tens of thousands of terms collected over decades. \nOften it is necessary to “refactor” ontologies, where large \nnumbers of terms are modified together, for example, \nas part of an overhaul of how a particular area of biol -\nogy is reflected. We aim to extend DRAGON-AI to sup -\nport these additional workflows, and, in particular, to \nmake use of rich information already present in many \nGitHub issue trackers that couple requested changes with \nenacted changes, in order to build something more like \nan autonomous agent that is able to work through large \nnumbers of requested changes specified in free text, \ninteracting with domain experts and ontology editors \nthrough conversational mechanisms.\nConclusions\nBuilding and maintaining ontologies is time-consuming \nand requires substantial human expertise. DRAGON-AI \ndemonstrates the potential of generative AI approaches, \nin conjunction with human oversight, to facilitate these \ntasks. DRAGON-AI can draw on structured knowl -\nedge from multiple ontologies, as well as textual sources \nincluding GitHub issues that request ontology changes.\nWe tested DRAGON-AI on three ontology editing \ntasks: prediction of relationships, term definitions, and \nlogical definitions. Its performance was evaluated by 24 \nontology editors and curators who worked independently \nof each other; each ontology was reviewed by at least 2 \nevaluators. Based on these evaluations, we found that \nAI-generated relationships had high precision but mod -\nerate recall, suggesting that they were generally correct \nbut incomplete. The AI-generated term definitions were \nPage 14 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \nfound to be decent but not as good as human-generated \ndefinitions. One interesting finding was that the more \nexperienced the evaluator was, the more difference they \ntended to perceive in the quality of the human-generated \nvs. AI-generated definitions.\nWe are investigating ways to incorporate generative AI \napproaches into existing ontology development work -\nflows. Our ultimate goal is not to replace human ontol -\nogy editors, but rather to augment their deep domain \nexpertise with tools that minimize tedious, repetitive \ntasks and make ontology creation and editing more effi -\ncient without sacrificing accuracy.\nAvailability and requirements\nProject name: DRAGON-AI.\nProject home page: https:// github. com/ monar ch- initi \native/ curate- gpt (DRAGON-AI is implemented as part of \nthe CurateGPT suite of tools).\nOperating system(s): Platform independent.\nProgramming language: Python.\nOther requirements: N/A.\nLicense: BSD-3.\nAny restrictions to use by non-academics: none; free to \nreuse and modify.\nAbbreviations\nAI  Artificial Intelligence\nAPI  Application programming interface\nCL  Cell Ontology\nDL  Description Logic\nDRAGON‑AI  Dynamic Retrieval Augmented Generation of Ontologies using \nArtificial Intelligence\nENVO  Environment Ontology\nGO  Gene Ontology\nHP  Human Phenotype Ontology\nICC  Intraclass Correlation Coefficients\nIDE  Integrated development environment\nJSON  JavaScript Object Notation\nLLM  Large Language Model\nML  Machine Learning\nMMR  Maximal Marginal Relevance\nMP  Mammalian Phenotype Ontology\nNLP  Natural Language Processing\nOBA  Ontology of Biological Attributes\nOBI  Ontology of Biomedical Investigations\nOBO  Open Biological and Biomedical Ontologies\nOL  Ontology Learning\nRAG   Retrieval Augmented Generation\nSPIRES  Structured Prompt Interrogation and Recursive Extraction of \nSemantics\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13326‑ 024‑ 00320‑3.\nSupplementary Material 1.\nAcknowledgements\nSupport for title page creation and format was provided by AuthorArranger, a \ntool developed at the National Cancer Institute.\nAuthors’ contributions\nST and CJM led the research and the writing of this manuscript. MPJ, SON,  JTR, \nand CJM contributed to the software used in this study. NLH, MCMT, and MAH \nprovided project support. NLH significantly edited the manuscript. ST and all \nother authors participated in the method evaluation as expert biocurators. All \nauthors read and approved the final manuscript.\nFunding\nThis work was supported by the National Institutes of Health National Human \nGenome Research Institute [HG010860, HG012212, HG010859]; National \nInstitutes of Health Office of the Director [R24 OD011883]; and the Director, \nOffice of Science, Office of Basic Energy Sciences, of the US Department of \nEnergy [DE‑AC0205CH11231 to CJM, NLH, MJ, and JPR]. TR was funded by \nthe National Science Foundation (NSF) under grant number OAC‑2112606 \n(ICICLE program). VW was supported by Wellcome Trust under grant number \n218236/Z/19/Z. TL was supported by a grant from the São Paulo Research \nFoundation (#19/26284–1). We also gratefully acknowledge Bosch Research \nfor their support of this research project.\nAvailability of data and materials\nThe datasets generated and/or analyzed in the current study are available in \nhttps:// github. com/ monar ch‑ initi ative/; a stable version is archived in Zenodo \n[47]. The workflow and Jupyter notebooks for the evaluation in this paper \ncan be found at https:// github. com/ monar ch‑ initi ative/ gpt‑ ontol ogy‑ compl \netion‑ analy sis.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nMAH is a founder of Alamya Health. The other authors declare that they have \nno competing interests.\nAuthor details\n1 University of North Carolina at Chapel Hill, Chapel Hill, NC, USA. 2 The Jackson \nLaboratory, Bar Harbor, ME, USA. 3 Department of Agriculture, Beltsville Human \nNutrition Research Center, Beltsville, MD, USA. 4 Simon Fraser University, \nBurnaby, BC, Canada. 5 The Jackson Laboratory for Genomic Medicine, Farm‑\nington, CT, USA. 6 University at Buffalo, Buffalo, NY, USA. 7 University of Florida, \nGainesville, FL, USA. 8 Northwestern University, Evanston, IL, USA. 9 SIB Swiss \nInstitute of Bioinformatics, Geneva, Switzerland. 10 Lawrence Berkeley National \nLaboratory, Berkeley, CA, USA. 11 Independent Scientific Information Analyst, \nPhiladelphia, USA. 12 University of São Paulo, São Paulo, Brazil. 13 University \nof Colorado Anschutz Medical Campus, Aurora, CO, USA. 14 Sanger Institute, \nHinxton, UK. 15 European Bioinformatics Institute (EMBL‑EBI), Hinxton, UK. \n16 Phoenix Bioinformatics, Newark, CA, USA. 17 Stowers Institute for Medical \nResearch, Kansas City, MO, USA. 18 IC‑FOODS, Austin, TX, USA. 19 Rothamsted \nResearch, Harpenden, UK. 20 National Center for Advancing Translational \nSciences, Bethesda, MD, USA. 21 INRAE, French National Research Institute \nfor Agriculture, Food and Environment, UR BIA, Nantes, France. 22 University \nof Cambridge, Cambridge, UK. \nReceived: 3 June 2024   Accepted: 8 September 2024\n\nPage 15 of 16\nToro et al. Journal of Biomedical Semantics           (2024) 15:19 \n \nReferences\n 1. Gene Ontology Consortium. The gene ontology knowledgebase in 2023. \nGenetics. 2023 Mar 3; Available from:https:// acade mic. oup. com/ genet ics/ \nadvan ce‑ artic le/ doi/ 10. 1093/ genet ics/ iyad0 31/ 70681 18.\n 2. Vasilevsky NA, Matentzoglu NA, Toro S, Flack JE, Hegde H, Unni DR, et al. \nMondo: Unifying diseases for the world, by the world. medRxiv. 2022. \np. 2022–04. https:// doi. org/ 10. 1101/ 2022. 04. 13. 22273 750.\n 3. Mungall CJ, Torniai C, Gkoutos GV, Lewis SE, Haendel MA. Uberon, an \nintegrative multi‑species anatomy ontology. Genome Biol. 2012;13(1):R5.\n 4. Dooley DM, Griffiths EJ, Gosal GS, Buttigieg PL, Hoehndorf R, Lange MC, \net al. FoodOn: a harmonized food ontology to increase global food trace‑\nability, quality control and data integration. NPJ Sci Food. 2018;18(2):23.\n 5. Malladi VS, Erickson DT, Podduturi NR, Rowe LD, Chan ET, Davidson \nJM, et al. Ontology application and use at the ENCODE DCC. Database. \n2015;2015:bav010‑.\n 6. Osumi‑Sutherland D, Xu C, Keays M, Levine AP , Kharchenko PV, Regev \nA, et al. Cell type ontologies of the Human Cell Atlas. Nat Cell Biol. \n2021;23(11):1129–35.\n 7.  Hastings J. Primer on Ontologies. In: Dessimoz C, Škunca N, editors. The \nGene Ontology Handbook. New York: Springer New York; 2017. p. 3–13.\n 8. Diehl AD, Meehan TF, Bradford YM, Brush MH, Dahdul WM, Dougall DS, \net al. The cell ontology 2016: enhanced content, modularization, and \nontology interoperability. J Biomed Semantics. 2016;7(1):44.\n 9. Osumi‑Sutherland D, Reeve S, Mungall CJ, Neuhaus F, Ruttenberg A, \nJefferis GSXE, et al. A strategy for building neuroanatomy ontologies. \nBioinformatics. 2012;28(9):1262–9.\n 10. Horridge M, Knublauch H, Rector A, Stevens R, Wroe C. A Practical guide \nto building OWL ontologies using the protégé‑OWL plugin and CO‑ODE \ntools edition 1.0. University of Manchester. 2004; Available from: http:// \nwww. cse. buffa lo. edu/ facul ty/ shapi ro/ Cours es/ CSE663/ Fall07/ Prote \ngeOWL Tutor ial. pdf.\n 11. Jackson RC, Balhoff JP , Douglass E, Harris NL, Mungall CJ, Overton JA. \nROBOT: a tool for automating ontology workflows. BMC Bioinform. \n2019;20(1):407.\n 12. Rector AL. Modularisation of domain ontologies implemented in descrip‑\ntion logics and related formalisms including OWL. In: Proceedings of \nthe 2nd international conference on Knowledge capture. Sanibel Island: \nACM; 2003. p. 121–8.\n 13. Köhler S, Doelken SC, Mungall CJ, Bauer S, Firth HV, Bailleul‑Forestier I, \net al. The human phenotype ontology project: linking molecular biology \nand disease through phenotype data. Nucleic Acids Res. 2014;42(Data‑\nbase issue):D966‑74.\n 14. Köhler S, Bauer S, Mungall CJ, Carletti G, Smith CL, Schofield P , et al. \nImproving ontologies by automatic reasoning and evaluation of logical \ndefinitions. BMC Bioinformatics. 2011;27(12):418.\n 15. Hill DP , Adams N, Bada M, Batchelor C, Berardini TZ, Dietze H, et al. Dove‑\ntailing biology and chemistry: integrating the Gene Ontology with the \nChEBI chemical ontology. BMC Genomics. 2013;14(1):513.\n 16. Mungall CJ, Bada M, Berardini TZ, Deegan J, Ireland A, Harris MA, et al. \nCross‑product extensions of the Gene Ontology. J Biomed Inform. \n2011;44(1):80–6.\n 17. Asim MN, Wasim M, Khan MUG, Mahmood W, Abbasi HM. A survey of \nontology learning techniques and applications. Database. 2018;2018. \nAvailable from: https:// acade mic. oup. com/ datab ase/ artic le‑ pdf/ doi/ 10. \n1093/ datab ase/ bay101/ 27329 264/ bay101. pdf. Cited 2023 Nov 24.\n 18. Ristoski P , Paulheim H. RDF2Vec: RDF graph embeddings and their \napplications. International semantic web conference. Springer; 2016. p. \n498–514. https:// doi. org/ 10. 1007/ 978‑3‑ 319‑ 46523‑4_ 30.\n 19. Chen J, Hu P , Jimenez‑Ruiz E, Holter OM, Antonyrajah D, Hor‑\nrocks I. OWL2Vec*: embedding of OWL ontologies. Mach Learn. \n2021;110(7):1813–45.\n 20. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P , et al. Train‑\ning language models to follow instructions with human feedback. arXiv \n[cs.CL]. 2022. Available from: http:// arxiv. org/ abs/ 2203. 02155.\n 21. OpenAI. GPT‑4 technical report. arXiv [cs.CL]. 2023. Available from: http:// \narxiv. org/ abs/ 2303. 08774.\n 22. Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, et al. A survey on evalua‑\ntion of large language models. arXiv [cs.CL]. 2023. Available from: http:// \narxiv. org/ abs/ 2307. 03109.\n 23. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al. A survey of large \nlanguage models. arXiv [cs.CL]. 2023. Available from: http:// arxiv. org/ abs/ \n2303. 18223 v12.\n 24. Chen M, Tworek J, Jun H, Yuan Q, de Oliveira Pinto HP , Kaplan J, et al. \nEvaluating Large Language Models Trained on Code. arXiv [cs.LG]. 2021. \nAvailable from: http:// arxiv. org/ abs/ 2107. 03374.\n 25. Matentzoglu N, Goutte‑Gattat D, Tan SZK, Balhoff JP , Carbon S, Caron AR, \nDevelopment O, Kit, et al. A toolkit for building, maintaining and stand‑\nardizing biomedical ontologies. Database. 2022. Available from: https:// \ndoi. org/ 10. 1093/ datab ase/ baac0 87.\n 26. Lewis P , Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, et al. Retrieval‑\naugmented generation for knowledge‑intensive NLP tasks. Adv Neural \nInf Process Syst. 2020;33:9459–74.\n 27. Jackson RC, Matentzoglu N, Overton JA, Vita R, Balhoff JP , Buttigieg PL, \net al. OBO Foundry in 2021: operationalizing open data principles to \nevaluate ontologies. bioRxiv. 2021. p. 2021.06.01.446587. Available from: \nhttps:// www. biorx iv. org/ conte nt/ biorx iv/ early/ 2021/ 06/ 02/ 2021. 06. 01. \n446587. Cited 2023 Dec 9.\n 28. Mikolov T. Efficient estimation of word representations in vector space. \narXiv:1301.3781 [preprint]. 2013. https:// doi. org/ 10. 48550/ arXiv. 1301. \n3781. \n 29. Smith B, Ceusters W, Klagges B, Köhler J, Kumar A, Lomax J, et al. Relations \nin biomedical ontologies. Genome Biol. 2005;6(5):R46.\n 30. Soroush A, Glicksberg BS, Zimlichman E, Barash Y, Freeman R, Charney \nAW, et al. Large language models are poor medical coders — bench‑\nmarking of medical code querying. NEJM AI. 2024;1(5):AIdbp2300040.\n 31. docs.trychroma.com. 2023. Available from: https://docs.trychroma.com/. \nCited 2023 Dec 15.\n 32. Malkov YA, Yashunin DA. Efficient and robust approximate nearest neigh‑\nbor search using Hierarchical Navigable Small World graphs. arXiv [cs.DS]. \n2016. Available from: http:// arxiv. org/ abs/ 1603. 09320.\n 33. Carbonell J, Goldstein J. The use of MMR, diversity‑based reranking for \nreordering documents and producing summaries. In: Proceedings of \nthe 21st annual international ACM SIGIR conference on Research and \ndevelopment in information retrieval. New York, NY, USA: Association for \nComputing Machinery; 1998. p. 335–6. (SIGIR ’98).\n 34. Caufield JH, Hegde H, Emonet V, Harris NL, Joachimiak MP , Matentzoglu \nN, et al. Structured prompt interrogation and recursive extraction of \nsemantics (SPIRES): A method for populating knowledge bases using \nzero‑shot learning. arXiv [cs.AI]. 2023. Available from: http:// arxiv. org/ abs/ \n2304. 02711.\n 35. Köhler S, Gargano M, Matentzoglu N, Carmody LC, Lewis‑Smith D, Vasi‑\nlevsky NA, et al. The human phenotype ontology in 2021. Nucleic Acids \nRes. 2021;49(D1):D1207–17.\n 36. Smith CL, Goldsmith W, Eppig JT. The Mammalian phenotype ontology as \na tool for annotating, analyzing and comparing phenotypic informa‑\ntion. Genome Biol. 2005;6(1). Available from: https:// doi. org/ 10. 1186/ \ngb‑ 2004‑6‑ 1‑ r7.\n 37. Buttigieg PL, Pafilis E, Lewis SE, Schildhauer MP , Walls RL, Mungall CJ. The \nenvironment ontology in 2016: bridging domains with increased scope, \nsemantic density, and interoperation. J Biomed Semantics. 2016;7(1):57.\n 38. Bandrowski A, Brinkman R, Brochhausen M, Brush MH, Bug B, Chibu‑\ncos MC, et al. The ontology for biomedical investigations. PLoS ONE. \n2016;11(4):e0154556.\n 39. Stefancsik R, Balhoff JP , Balk MA, Ball RL, Bello SM, Caron AR, et al. The \nOntology of Biological Attributes (OBA)‑computational traits for the life \nsciences. Mamm Genome. 2023;34(3):364–78.\n 40. Khadir AC, Aliane H, Guessoum A. Ontology learning: grand tour and \nchallenges. Comput Sci Rev. 2021;1(39):100339.\n 41. Gillis J, Pavlidis P . Characterizing the state of the art in the computational \nassignment of gene function: lessons from the first critical assessment \nof functional annotation (CAFA). BMC Bioinform. 2013;14 Suppl 3(Suppl \n3):S15.\n 42. Kazakov Y, Krötzsch M, Simančík F. The Incredible ELK. J Automat Reason. \n2014;53(1):1–61.\n 43. Mungall CJ, Dietze H, Osumi‑Sutherland D. Use of OWL within the gene \nontology. bioRxiv. 2014. p. 010090. Available from: https:// www. biorx iv. \norg/ conte nt/ 10. 1101/ 010090. Cited 2023 Dec 10.\n 44. Powers D. Evaluation: From Precision, Recall and F‑Factor to ROC, \nInformedness, Markedness & Correlation. 2008; Available from: http:// dx. \ndoi. org/.\nPage 16 of 16Toro et al. Journal of Biomedical Semantics           (2024) 15:19 \n 45. Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y. BERTScore: evaluating \ntext generation with BERT. arXiv [cs.CL]. 2019. Available from: http:// arxiv. \norg/ abs/ 1904. 09675.\n 46. dragon‑ai‑results. Github. Available from: https:// github. com/ monar ch‑ \niniti ative/ dragon‑ ai‑ resul ts. Cited 2024 May 21.\n 47. Dragon‑Ai E. DRAGON‑AI Results Analysis. Zenodo. 2023. Available from: \nhttps:// zenodo. org/ recor ds/ 10183 232.\n 48. Toro S, Mungall CJ. Expert rankings of definitions across multiple ontolo‑\ngies. 2024. Available from: https:// huggi ngface. co/ datas ets/ Monar chInit/ \ndragon‑ ai‑ defin ition‑ evals. Cited 2023 Dec 15.\n 49. cell‑ontology. Github. Available from: https:// github. com/ oboph enoty \npe/ cell‑ ontol ogy/ issues/ 2241. Cited 2024 Jul 31.\n 50. Wang X, Li B, Song Y, Xu FF, Tang X, Zhuge M, et al. OpenDevin: an open \nplatform for AI software developers as generalist agents. arXiv [cs.SE]. \n2024. Available from: http:// arxiv. org/ abs/ 2407. 16741. Cited 2024 Jul 28.\n 51. Dohmke T, Iansiti M, Richards G. Sea change in software development: \neconomic and productivity analysis of the ai‑powered developer \nlifecycle. arXiv [econ.GN]. 2023. Available from: http:// arxiv. org/ abs/ 2306. \n15033.\n 52. Dakhel AM, Majdinasab V, Nikanjam A, Khomh F, Desmarais MC, Jiang \nZM. GitHub copilot AI pair programmer: asset or liability? J Syst Softw. \n2023;203:111734.\n 53. Bowman SR. Eight things to know about large language models. arXiv \n[cs.CL]. 2023. Available from: http:// arxiv. org/ abs/ 2304. 00612.\n 54. Noy S, Zhang W. Experimental evidence on the productivity effects of \ngenerative artificial intelligence. 2023. Available from: https:// papers. ssrn. \ncom/ abstr act= 43752 83. Cited 2023 Sep 25.\n 55. Roberts M, Thakur H, Herlihy C, White C, Dooley S. Data contamination \nthrough the lens of time. arXiv [cs.CL]. 2023. Available from: http:// arxiv. \norg/ abs/ 2310. 10628.\n 56. Li C, Flanigan J. Task contamination: language models may not be few‑\nshot anymore. arXiv [cs.CL]. 2023. Available from: http:// arxiv. org/ abs/ \n2312. 16337.\n 57. ecosim‑ontology: EXPERIMENTAL derivation of ontology from ecosim. \nGithub. Available from: https:// github. com/ bioep ic‑ data/ ecosim‑ ontol \nogy. Cited 2024 May 21.\n 58. Balhoff JP , Bayindir U, Caron AR. Ubergraph: integrating OBO ontologies \ninto a unified semantic graph.  http://ceur‑ws  org … . 2022; Available \nfrom: https:// icbo‑ confe rence. github. io/ icbo2 022/ papers/ ICBO‑ 2022_ \npaper_ 5005. pdf.\n 59. Osumi‑Sutherland D, Courtot M, Balhoff JP , Mungall C. Dead simple OWL \ndesign patterns. J Biomed Semantics. 2017;8(1):18.\n 60. Kindermann C, Lupp DP , Sattler U, Thorstensen E. Generating Ontologies \nfrom Templates: A Rule‑Based Approach for Capturing Regularity. :13. \nhttps:// ceur‑ ws. org/ Vol‑ 2211/ paper‑ 22. pdf.\n 61. Moxon S, Solbrig H, Unni D, Jiao D, Bruskiewich R, Balhoff J, Vaidya G, \nDuncan W, Hegde H, Miller M, Brush M, Harris N, Haendel M, Mungall C. \nThe linked data modeling language (LinkML): A general‑purpose data \nmodeling framework grounded in machine‑readable semantics. 2021 \nInternational Conference on Biomedical Ontologies, ICBO 2021, 3073. \n2021. p. 148–151.\n 62. curate‑gpt: LLM‑driven curation assist tool (pre‑alpha). Github. Available \nfrom: https:// github. com/ monar ch‑ initi ative/ curate‑ gpt. Cited 2023 Dec \n14.\n 63. ChatGPT. ChatGPT ‑ ROBOT‑template helper. Available from: https:// chatg \npt. com/g/ g‑ mGG79 L6UW‑ robot‑ templ ate‑ helper. Cited 2024 May 30.\n 64. Joachimiak MP , Miller MA, Harry Caufield J, Ly R, Harris NL, Tritt A, et al. The \nArtificial Intelligence Ontology: LLM‑assisted construction of AI concept \nhierarchies. arXiv [cs.LG]. 2024. Available from: http:// arxiv. org/ abs/ 2404. \n03044.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub‑\nlished maps and institutional affiliations."
}