{
  "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
  "url": "https://openalex.org/W4389524039",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2134104766",
      "name": "Viet Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106993018",
      "name": "Chien Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287855901",
      "name": "Nghia Ngo",
      "affiliations": [
        "University of Oregon"
      ]
    },
    {
      "id": "https://openalex.org/A3105283056",
      "name": "Thuat Nguyen",
      "affiliations": [
        "University of Oregon"
      ]
    },
    {
      "id": "https://openalex.org/A15025084",
      "name": "Franck Dernoncourt",
      "affiliations": [
        "University of Oregon",
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2321607291",
      "name": "Ryan Rossi",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2134526459",
      "name": "Thien Nguyen",
      "affiliations": [
        "University of Oregon"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4386875581",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W3148330722",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4366735744",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4367369802",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4378465252",
    "https://openalex.org/W4389519817",
    "https://openalex.org/W4382246105"
  ],
  "abstract": "Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, Thien Nguyen. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 318–327\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOkapi: Instruction-tuned Large Language Models in Multiple Languages\nwith Reinforcement Learning from Human Feedback\nViet Dac Lai1, Chien Van Nguyen1, Nghia Trung Ngo1, Thuat Nguyen1\nFranck Dernoncourt2, Ryan A. Rossi2, Thien Huu Nguyen1\n1Dept. of Computer Science, University of Oregon, OR, USA\n2Adobe Research, USA\n{vietl@cs,chienn,nghian@cs,thien@cs}@uoregon.edu\n{franck.dernoncourt,ryrossi}@adobe.com\nAbstract\nA key technology for large language models\n(LLMs) involves instruction tuning that helps\nalign the models’ responses with human ex-\npectations to realize impressive learning abili-\nties. Two major approaches for instruction tun-\ning characterize supervised fine-tuning (SFT)\nand reinforcement learning from human feed-\nback (RLHF), which are applied to produce the\nbest commercial LLMs. To improve the ac-\ncessibility of LLMs, various instruction-tuned\nopen-source LLMs have also been introduced\nrecently. However, existing open-source LLMs\nhave only been instruction-tuned for English\nand a few popular languages, thus hindering\ntheir accessibility to many other languages in\nthe world. In addition, SFT has been used as the\nonly approach to instruction-tune open-source\nLLMs for multiple languages. This has left\na significant gap for fine-tuned LLMs based\non RLHF in diverse languages and raised im-\nportant questions on how RLHF can boost the\nperformance of multilingual instruction tuning.\nTo overcome this issue, we present Okapi, the\nfirst system with instruction-tuned LLMs based\non RLHF for multiple languages. Okapi intro-\nduces instruction and response-ranked data in\n26 diverse languages to facilitate the experi-\nments and development of future multilingual\nLLM research. We also present benchmark\ndatasets to enable the evaluation of genera-\ntive LLMs in multiple languages. Our exper-\niments demonstrate the advantages of RLHF\nfor multilingual instruction over SFT for dif-\nferent base models and datasets. Our frame-\nwork with created resources, fine-tuned LLMs,\ninteraction scripts are released at https://\ngithub.com/nlp-uoregon/Okapi. A demo\nvideo to show our framework can also be found\nat: https://youtu.be/QFV2fkPwvi0.\n1 Introduction\nPre-trained on massive data, large language mod-\nels (LLMs) with hundreds of billions of parame-\nters such as GPT-3 (Rae et al., 2021) can unlock\nnew emergent abilities that cannot be achieved\nwith smaller models (Wei et al., 2022; Choi et al.,\n2023; Jiao et al., 2023). However, as LLMs are\ntrained with the autoregressive learning objective,\nthey might exhibit unintended behaviours from hu-\nman expectations (Tamkin et al., 2021; Weidinger\net al., 2021; Kenton et al., 2021). To overcome this\nissue, instruction fine-tuning has been proposed\nas a prominent approach to improve capabilities\nin following human instructions for LLMs and\nalign them with human intentions in conversations\n(Christiano et al., 2017; Stiennon et al., 2020; Sanh\net al., 2021; Ouyang et al., 2022). As such, two ma-\njor techniques for instruction tuning feature super-\nvised fine-tuning (SFT) and reinforcement learning\nfrom human feedback (RLHF) that are leveraged by\nthe best commercial LLMs such as ChatGPT and\nGPT-4 to deliver outstanding dialog performance.\nAnother issue with LLMs pertains to the mas-\nsive scales and closed-source nature of the com-\nmercial LLMs that greatly restrict accessibility and\nthe extent of interactions with the technology. To\nthis end, there have been growing efforts from the\nopen-source community to create more accessible\nLLMs with affordable scales while securing com-\npetitive performance as the proprietary LLMs, e.g.,\nLLaMA (Touvron et al., 2023), StableLM (Stabil-\nityAI, 2023), Falcon (Almazrouei et al., 2023), and\nMTP (MosaicML, 2023). Instruction tuning has\nalso been applied to these open-source LLMs to\nimprove their abilities to engage with human, and\ndifferent instruction datasets have been collected\nto facilitate the process, e.g., Alpaca (Taori et al.,\n2023), Vicuna (Chiang et al., 2023), LaMini-LM\n(Wu et al., 2023), and Dolly (Conover et al., 2023).\nHowever, the instruction-following abilities of\nexisting open-source LLMs have been developed\nmainly for English and some popular languages\n(i.e., using instruction data for those languages),\nfailing to support many other languages of the\nworld to serve a broader population (Taori et al.,\n318\n2023; Wu et al., 2023). To overcome this challenge,\na few contemporary frameworks have explored in-\nstruction tuning of LLMs for multiple languages,\ni.e., Phoenix (Chen et al., 2023) and Bactrian-X (Li\net al., 2023). However, their multilingual instruc-\ntion tuning efforts are limited to only supervised\nfine-tuning, which is unable to examine reinforce-\nment learning with human feedback (RLHF) to fur-\nther boost the performance for multilingual LLMs.\nTo fill in this gap, our work aims to develop\nOkapi, an open-source framework with RLHF-\nbased instruction-tuned LLMs for multiple lan-\nguages to provide resources and shed light on their\nperformance for multilingual LLM learning. Okapi\nwill emphasize on less studied languages and open-\nsource LLMs to better democratize the benefits\nof instruction-tuned LLMs. In particular, an ex-\nample in the instruction datasets involves an in-\nstruction, an input text, and a desired response out-\nput/demonstration. In SFT, the pre-trained LLMs\nare fine-tuned over the instruction triples (instruc-\ntion, input, output) via supervised learning to pro-\nmote their alignment with human expectations.\nIn RLHF, generated outputs from the SFT-tuned\nLLMs are first ranked to provide training signals\nfor the reward functions. Afterward, the SFT-tuned\nmodels will be further optimized via reinforcement\nlearning utilizing rewards from the trained reward\nmodels. As such, RLHF has been successfully em-\nployed to create effective commercial LLMs (e.g.,\nInstructGPT, ChatGPT), owning to its ability to\nlearn beyond positive examples associated with\nonly desired demonstrations. By leveraging the\nreward models, RLHF can observe lower ranking\nscores for less accurate demonstrations to obtain\nricher training signals for LLMs. To our knowl-\nedge, Okapi is the first work to perform instruction\ntuning with RLHF for open-source LLMs over mul-\ntiple languages.\nTo develop Okapi, we need to overcome the\nscarcity of instruction datasets in multiple lan-\nguages to train and evaluate RLHF models. Moti-\nvated by the 52K instructions from Alpaca (Taori\net al., 2023), we leverage Self-Instruct (Wang et al.,\n2023) to generate 106K additional instructions in\nEnglish, introducing a larger dataset to facilitate\nRLHF evaluation. Afterward, we utilize Chat-\nGPT to translate the instructions into a diverse set\nof 26 languages, including high-, medium-, and\nlow-resource languages (e.g., Telugu, Ukrainian,\nNepali, and Kannada) to offer comprehensive re-\nsources and insights for multilingual instruction-\ntuning. In addition, we introduce a translation-\nbased prompt for ChatGPT to produce rankings for\nmultiple responses of the same instructions from\nthe LLMs, which will be used to train the reward\nmodels for RLHF experiments. Finally, we ob-\ntain the multilingual evaluation datasets for our\nfine-tuned LLMs by translating three benchmark\ndatasets for LLMs in the widely-used HuggingFace\nOpen LLM Leaderboard (HuggingFace, 2023; Gao\net al., 2021) into 26 languages, i.e., ARC (Clark\net al., 2018), HellaSwag (Zellers et al., 2019), and\nMMLU (Hendrycks et al., 2021).\nUsing BLOOM (Scao et al., 2022) and LLaMa\n(Touvron et al., 2023) as the base LLMs, our ex-\nperiments illustrate that RLHF generally performs\nbetter than SFT for multilingual instruction tuning.\nWe also highlight the greater challenges of low-\nresource languages for multilingual instruction-\ntuning of LLMs that can be focused in future re-\nsearch. Finally, we release our framework with\nthe created resources and fine-tuned RLHF models.\nWe also provide scripts to interact with our models\nat https://github.com/nlp-uoregon/Okapi.\n2 Data Preparation\nA key requirement for our development of\ninstruction-tuned LLMs with RLHF involves in-\nstruction, ranking, and evaluation datasets in multi-\nple languages. To this end, we perform a compre-\nhensive data collection process to prepare necessary\ndata for our multilingual framework Okapi in 26\nlanguages, divided into four major steps: English\ninstruction generation, instruction translation, rank-\ning data production, and evaluation data creation.\nEnglish Instruction Generation: An instruc-\ntion example to tune LLMs often has three compo-\nnents: an instruction to specify the task, an input\ntext, and an associated output text (i.e., demon-\nstration or label) (Ouyang et al., 2022). As such,\ncurrent public instruction datasets for LLMs mainly\ncover English or some popular languages. Also,\nwe note that a few recent instruction datasets such\nas xP3 (Muennighoff et al., 2022) and Flan (Chung\net al., 2022; Longpre et al., 2023) include mul-\ntilingual data; however, their instructions are still\nwritten in English. Additionally, these datasets tend\nto be converted from NLP datasets with template\ninstructions, which cannot reflect the flexibility of\nhuman-written prompts (Wang et al., 2023). Conse-\nquently, our goal is to develop instruction datasets\n319\nwith instructions, inputs, and output texts in mul-\ntiple languages, including low-resource ones, to\nbetter realize general prompts from human.\nTo achieve this goal, our strategy is to first ob-\ntain English instructions and then translate them\ninto other languages. The benefits of our approach\ninvolve consistent instruction content across lan-\nguages to facilitate performance comparison while\ntaking advantages of translation systems to enable\nexamination for more languages. As such, to con-\nveniently scale our data, we follow the instruction\ngeneration method in Alpaca, which in turn em-\nploys the Self-Instruct procedure in (Wang et al.,\n2023), to produce our English dataset.\nStarting with a pool of 175 human-written seed\ninstructions in English, at each time, Alpaca sam-\nples several instructions from the seeds to form\nan in-context example to prompt the text-davinci-\n003 model of OpenAI for new instruction genera-\ntion. Overall, Alpaca releases 52K instructions\nfor tuning LLMs. In this work, we apply the\nsame Self-Instruct procedure as Alpaca to generate\n106K additional English instructions, resulting in\na larger combined dataset of 158K instructions for\nour RLHF-based models in Okapi. Notably, we\ncondition our generation process on the 52K in-\nstructions from Alpaca so a new instruction is only\nsaved if it is different enough from Alpaca’s and\nprevious instructions per the ROUGE score criteria\nin Alpaca (Taori et al., 2023).\nInstruction Translation: Given the 158K En-\nglish instructions, we aim to translate them into\nmultiple other languages to obtain data for our mul-\ntilingual models in Okapi. Table 1 presents 26\nselected languages in our framework. Using the\ndata ratios rof the languages in CommonCrawl1\nto classify languages as in previous work (Bang\net al., 2023; Lai et al., 2023), our study encom-\npasses a diverse set of languages, including 8 high-\nresource languages (r> 1.0), 11 medium-resource\nlanguages (r> 0.1), and 7 low-resource languages\n(r< 0.1). Notably, several of our languages, such\nas Marathi, Gujarati, and Kannada, have received\nlimited attention in NLP and instruction-tuning.\nWe utilize ChatGPT to translate the 158K En-\nglish instructions into 26 target languages for\nOkapi. Compared to traditional machine trans-\nlation systems, an advantage of ChatGPT is the\nability to use prompts to specify different expec-\ntations for the translated texts to facilitate diverse\n1http://commoncrawl.org\nLanguage CodePop. CC Size B L(M) (%) Cat.\nEnglish en 1,452 45.8786 H ✓ ✓\nRussian ru 258 5.9692 H ✓ ✓\nGerman de 134 5.8811 H ✓ ✓\nChinese zh 1,118 4.8747 H ✓\nFrench fr 274 4.7254 H ✓ ✓\nSpanish es 548 4.4690 H ✓ ✓\nItalian it 68 2.5712 H ✓ ✓\nDutch nl 30 2.0585 H ✓ ✓\nVietnamese vi 85 1.0299 H ✓\nIndonesian id 199 0.7991 M ✓\nArabic ar 274 0.6658 M ✓\nHungarian hu 17 0.6093 M ✓ ✓\nRomanian ro 29 0.5637 M ✓ ✓\nDanish da 6 0.4301 M ✓ ✓\nSlovak sk 7 0.3777 M ✓ ✓\nUkrainian uk 33 0.3304 M ✓ ✓\nCatalan ca 10 0.2314 M ✓ ✓\nSerbian sr 12 0.2205 M ✓ ✓\nCroatian hr 14 0.1979 M ✓ ✓\nHindi hi 602 0.1588 M ✓\nBengali bn 272 0.0930 L ✓\nTamil ta 86 0.0446 L ✓\nNepali ne 25 0.0304 L ✓\nMalayalam ml 36 0.0222 L ✓\nMarathi mr 99 0.0213 L ✓\nTelugu te 95 0.0183 L ✓\nKannada kn 64 0.0122 L ✓\nTable 1: List of 26 non-English languages in Okapi along\nwith their codes, numbers of first and second speakers (the\n“Pop.” column), data ratios in CommonCrawl, and categories.\nThe languages are grouped into categories based on their data\nratios in CommomCrawl: High- (H, > 1%), Medium- (M,\n> 0.1%), and Low-Resource (L, > 0.01%). Columns “ B”\nand “ L” indicate if a language is supported by the LLMs\nBLOOM and LLaMa (respectively) or not.\ntypes of instructions. For example, we can instruct\nChatGPT to preserve code in the instruction ex-\namples about programming as we expect code to\nbe the same in the instructions across natural lan-\nguages. It is important to note that we directly\ntranslate the instruction, input text, and associated\noutput in each English instruction of our data. This\nis in contrast to the other multilingual instruction-\ntuning approaches (Li et al., 2023) that only trans-\nlate instructions and input texts into a target lan-\nguage (using Google Translate), and then prompt\nChatGPT to generate response outputs in the tar-\nget language based on the translated instructions\nand inputs. The intuition for our approach con-\ncerns various potential issues of ChatGPT, e.g.,\nhallucination, bias, mathematical reasoning, and\ntoxic content (Bang et al., 2023; Borji, 2023), that\ncan be exaggerated if ChatGPT is used to produce\nresponses in non-English languages for different\ntasks (Lai et al., 2023). By generating the instruc-\ntions and responses in English, we aim to capitalize\non the greater performance of LLMs for different\n320\nTranslation Prompt:Translate the values in the fol-\nlowing JSON object into <target language> language.\nYou must keep the keys in the JSON object in English.\nIf a value contains programming code, only translate\nthe comments while preserving the code. Your trans-\nlations must convey all the content in the original text\nand cannot involve explanations or other unnecessary\ninformation. Please ensure that the translated text is\nnatural for native speakers with correct grammar and\nproper word choices. Your translation must also use\nexact terminology to provide accurate information even\nfor the experts in the related fields. Your output must\nonly contain a JSON object with translated text and can-\nnot include explanations or other information.\nFigure 1: Translation prompt for ChatGPT for multiple lan-\nguages in Okapi. We organize our instruction examples into\nJSON objects with fields for translation prompts, instructions,\ninputs, and outputs send to ChatGPT. <target language> is\nreplaced with the selected languages in our dataset.\nNLP tasks in English to avoid the exaggeration\nissues and achieve higher quality instructions.\nRanking Data Production: To perform RLHF\nfor a LLM, we need to obtain ranked response out-\nputs from the model for the same instruction and\ninput to train a reward model. Concretely, given\na LLM M and a dataset S = {instk,inputk}N\nk=1\nwith N pairs of instructions instk and input texts\ninputk for a target language, we first prompt\nM to generate T output responses outputk =\n{output1\nk,...,output T\nk}for each pair of instruc-\ntion and input text (instk,inputk) (T >1). After-\nward, the responses in outputk are ranked accord-\ning to their fitness and quality for the instruction\ninstk and input text inputk. This ranking data\n{instk,inputk,outputk}can then be leveraged to\ntrain our reward models in Okapi.\nWe also employ ChatGPT to rank the response\noutputs for multilingual LLMs. Similar to the moti-\nvation for our translation-based approach to obtain\ninstruction data in multiple languages, our rank-\ning strategy first asks ChatGPT to translate the in-\nstructions and responses {instk,inputk,outputk}\nof a target language into English; the ranking of\nthe responses is then done over the translated En-\nglish data to exploit the greater quality of Chat-\nGPT for English (using the translation and rank-\ning prompts in Figure 2). For each example\n{instk,inputk,outputk}, the translation and rank-\ning prompts are wrapped in a two-turn dialog with\nChatGPT to allow the ranking process to condition\non the resulting translations. It also ensures the\nsame output format for the ranking prompts for\nconvenient parsing. Overall, we obtain ranked re-\nsponse outputs for 42K instructions from the 106K\n•Turn 1: Translation PromptYou will be given an in-\nstruction, an input for the instruction, and four possible\nresponses for the instruction. The input can be empty,\nshown as <empty>. You need to translate the provided\ninstruction, input, and responses into English.\nInstruction:. . .\nInput:. . .\nResponse 1:. . .\nResponse 2:. . .\nResponse 3:. . .\nResponse 4:. . .\n• Turn 2: Ranking PromptGiven the translated\ninstruction, input, and responses, you will need to rank\nthe responses according to three factors: correctness\nwith respect to the instruction and input, coherence, and\nnaturalness.\nYou will need to provide an overall rank for each\nresponse when all the three factors are considered. The\noverall rank for a response must be an integer between\n1 and 4 where 1 is for the best response and 4 is the\nworst response. You cannot assign the same rank for\ntwo different responses.\nThe format of your output must be: for each response:\n\"<Response r>: overall rank: <1/2/3/4>\". The\nresponses must be in original order. Do not include\nexplanation in your output.\nAn Example Output from ChatGPT:\nResponse 1: 3\nResponse 2: 1\nResponse 3: 4\nResponse 4: 2\nFigure 2: Prompts to translate and rank responses.\ngenerated instructions for each language in Okapi.\nEvaluation Data Creation: We employ three\ndatasets in the HuggingFace Open LLM Leader-\nboard (HuggingFace, 2023) i.e., ARC (Clark\net al., 2018), HellaSwag (Zellers et al., 2019),\nand MMLU (Hendrycks et al., 2021), to evaluate\nthe model performance for our Okapi framework.\nAll the datasets are organized as multiple-choice\nquestion-answering tasks although they focus on\ndifferent types of knowledge and reasoning aspects.\nARC involves 1170 grade-school science questions;\nHellaSwag provides 9162 commonsense inference\nquestions that are easy for humans, but difficult\nfor many state-of-the-art models; and MMLU as-\nsesses accuracy for 13062 questions over various\nbranches of knowledge (STEM, humanities, social\nsciences, and more). Nevertheless, although the\nLLM community has widely adopted the Hugging-\nFace leaderboard for performance examination, the\ndatasets are only provided for English, thus un-\nable to evaluate LLMs for the languages in our\nwork. To this end, we translate the examples of\nthe three datasets into 26 selected languages using\nChatGPT and the translation prompt in Figure 1.\nThe translated datasets are then reserved to evaluate\nthe LLMs in our Okapi framework.\n321\n3 Instruction-tuning with RLHF\nWe follow three steps to develop a fine-tuned LLM\nwith RLHF for each target language in our Okapi\nframework: supervised fine-tuning, reward model\ntraining, and reinforcement learning.\nSupervised Fine-tuning (SFT): Starting with\na multilingual LLM as the base, e.g., BLOOM\n(Scao et al., 2022), we fine-tune the model with our\ninstruction dataset for the target language using su-\npervised learning with the autoregressive objective.\nHere, we fine-tune the entire base LLM for all of\nits parameters with SFT to accurately understand\nthe model performance for multilingual settings.\nReward Model Training: The goal of this step\nis to train a reward model for the target language\nthat will compute reward signals for reinforcement\nlearning to further optimize the SFT-tuned model\nfrom the previous step. For each pair of a prompt\nand potential response, our reward model returns a\nscalar value to quantify the appropriateness of the\nresponse with respect to the instruction and input\ntext in the prompt. We exploit the instructions with\nmultiple ranked responses in the data collection\nstep for this training step. An example to train our\nreward model for a language involves an instruc-\ntion and an input text (to form a prompt x) along\nwith two sampled responses yc and yr for xfrom\nour datasets. Based on the ranking information, we\ncan assume one of the responses (i.e., yc) is more\npreferable than the other (i.e., yr). In the next step,\nthe binary ranking loss (Ouyang et al., 2022) is em-\nployed to train our reward model, aiming to assign\na higher score r(x,yc) for the preferred response\nyc than the score r(x,yr) for yr: Lreward(θ) =\n−E(x,yc,yr) [log σ(rθ(x,yc) −rθ(x,yr))].\nReinforcement Learning (RL) : With the\nreward model established for the target language,\nthe SFT model undergoes additional fine-tuning\nthrough RL to align it with human preferences.\nFor this purpose, we employ the Proximal\nPolicy Optimization (PPO) algorithm (Ouyang\net al., 2022) that maximizes the mean reward\nof the model via the objective: LRL(ϕ) =\n−Ex∼DRL,y∼πϕ(y|x) [rθ(x,y) −βKL(x,y)].\nHere, DRL corresponds to the prompt distribution,\nand πϕ(y|x) denotes the policy or language\nmodel that requires optimization. πϕ(y|x) is\ninitialized with the SFT-tuned model πϕ(y|x).\nAlso, KL(x,y) =DKL(πϕ(y|x)||π0(y|x)) is the\nKullback–Leibler divergence to penalize large\ndeviation of πϕ from the initial SFT policy π0.\n4 Experiments\nOur Okapi framework utilizes two multilingual\nLLMs: BLOOM (Scao et al., 2022) and LLaMA\n(Touvron et al., 2023) as the base models for\nthe fine-tuning processes. We focus on their 7B-\nparameter versions to facilitate the computing re-\nsources and achieve fairer comparison. For each\nbase model and target language, we carry out both\nSFT-based and RLHF-based instruction-tuning:\n•SFT: The base model is fine-tuned over our\nentire set of 158K translated instructions for the\ntarget language in the supervised manner.\n•RLHF: The base model is first fine-tuned with\nsupervised training over 52K translated instructions\nfrom Alpaca. Afterward, a reward model is trained\nusing the 42K instructions with ranked responses\nobtained in the data collection. Note that the ranked\nresponses are sampled from the SFT-tuned base\nmodel over the 52K Alpaca instructions from pre-\nvious step. Finally, given the reward model, the\nSFT-tuned model is further optimized via reinforce-\nment learning over the 64K remaining translated\ninstructions from our generation set.\nFollowing the HuggingFace Open LLM Leader-\nboard, the Eleuther AI Language Model Evaluation\nHarness framework (Gao et al., 2021) is used to\ncompute the model performance over the trans-\nlated datasets ARC, HellaSwag, and MMLU for\neach language in our framework. As a reference,\nwe also report the performance of the base models\nBLOOM and LLaMA in the experiments. Finally,\nfor BLOOM, we further compare with BLOOMZ\n(Muennighoff et al., 2022), which is the fine-tuned\nversion of BLOOM over the cross-lingual task mix-\nture dataset xP3 with millions of multilingual in-\nstructions to achieve instruction-following ability.\nEvaluation: Tables 2 and 3 present the perfor-\nmance of the models on ARC, HellaSwag, and\nMMLU when BLOOM and LLaMa are used as\nthe base models (respectively). In the tables, for\neach language group (i.e., high-, medium-, and low-\nresource), we report the average performance over\nthe languages and the performance for two exam-\nple languages in the group. We also include the\naverage performance over all languages in Okapi.\nAs some of our languages in Okapi (especially the\nlow-resource ones) are not supported by LLaMA,\nTable 3 will omit those languages (see Table 1).\nFinally, Appendix A provides performance of the\nmodels over all languages and datasets in Okapi.\nThe first observation from the tables is that\n322\nDataLanguage BLOOMBLOOMZSFTRLHF\nARC\nChinese 37.3 37.0 37.9 40.0French 36.7 37.6 37.6 41.2Average High31.5 30.7 32.3 34.0Indonesian 36.0 35.9 37.4 38.8Arabic 31.4 31.2 32.1 33.2Average Medium27.7 26.7 28.0 29.8Bengali 26.2 25.5 26.8 28.9Kannada 24.7 24.6 24.5 24.6Average Low 25.1 24.9 24.7 25.6Average All 28.2 27.4 28.4 30.0\nHellaSwag\nChinese 51.2 42.6 51.8 53.8French 56.6 45.7 55.9 58.7Average High43.8 39.6 44.5 46.6Indonesian 49.5 42.0 50.0 52.2Arabic 43.3 39.5 44.3 47.0Average Medium35.7 33.5 36.9 38.9Bengali 32.8 31.5 33.9 35.4Kannada 30.3 30.9 30.7 32.1Average Low 30.3 30.9 31.2 32.3Average All 36.8 34.7 37.7 39.5\nMMLU\nChinese 29.1 27.2 27.7 28.2French 27.4 27.7 27.7 28.4Average High27.5 26.4 26.9 27.5Indonesian 26.9 26.3 26.8 27.5Arabic 27.5 24.4 27.4 27.7Average Medium27.1 25.8 26.7 27.1Bengali 28.2 25.9 27.1 26.8Kannada 26.7 26.0 26.6 26.8Average Low 26.7 25.9 26.1 26.1Average All 27.1 26.0 26.6 26.9\nTable 2: Performance of the models using BLOOM 7B.\nDataLanguage LLaMASFTRLHF\nARC\nGerman 35.1 37.5 39.7French 37.3 38.4 38.8Average High35.1 36.5 38.7Danish 32.7 35.1 36.8Ukrainian 32.9 35.7 36.4Average Medium32.0 34.3 36.2Average All 33.3 35.2 37.3\nHellaSwag\nGerman 49.9 49.0 52.6French 55.7 55.6 56.9Average High51.4 51.2 53.7Danish 46.7 47.7 51.7Ukrainian 44.1 46.9 47.7Average Medium42.7 44.0 46.5Average All 46.4 47.1 49.6\nMMLU\nGerman 29.9 30.4 31.7French 30.5 31.0 30.7Average High30.1 30.4 30.9Danish 30.0 30.9 31.8Ukrainian 29.4 30.8 31.6Average Medium29.5 29.9 30.7Average All 29.8 30.1 30.8\nTable 3: Performance of the models using LLaMa 7B.\nRLHF is generally better than SFT for multilin-\ngual fine-tuning of LLMs over different datasets,\nbase models, and language groups. It is also evi-\ndent that the RLHF-tuned models can significantly\nimprove the performance of the original base mod-\nels (i.e., BLOOM and LLaMa) for almost all the\nlanguage groups and datasets. In all, it highlights\nthe quality of the generated instruction data and the\neffectiveness of RLHF in Okapi.\nComparing the performance across language\ngroups, the models tend to achieve the highest\nperformance for the high-resource languages, fol-\nlowed by the medium-resource and low-resource\nlanguages. The performance improvement of\nRLHF for low-resource languages is also the least\n(based on BLOOM). Interestingly, our fine-tuned\nBLOOM models with 158K generated instructions\ncan significantly outperform BLOOMZ over al-\nmost all the languages for the ARC, HellaSwag,\nand MMLU datasets using either SFT or RLHF. As\nBLOOMZ has fine-tuned BLOOM over more than\n78M multilingual instructions converted from NLP\ndatasets (Muennighoff et al., 2022), it demonstrates\nthe higher quality of our generated instructions for\nmultilingual instruction tuning of LLMs.\n5 Related Work\nThe most advanced methods for NLP involve fine-\ntuning the pre-trained language models (PLMs) on\ntraining data of the downstream tasks (Min et al.,\n2023). Instruction tuning can be considered as a\nspecial type of fine-tuning techniques for PLMs\nwhere generative PLMs (e.g., GPT) are further\ntrained with instruction data to accomplish the in-\nstruction following abilities. SFT is the most pop-\nular instruction tuning approach that is leveraged\nby most of the existing LLMs, including ChatGPT,\nApaca (Taori et al., 2023), and Vicuna (Chiang\net al., 2023). RLHF can also be used to further en-\nhance LLMs (Wei et al., 2021; Ouyang et al., 2022)\nalthough it has been less explored by current open-\nsource LLMs due to the challenges in obtaining\nranking data for the reward models. For multilin-\ngual learning, instruction tuning is only applied in\nthe form of SFT for non-English languages using\nmultilingual LLMs, e.g., BLOOM and LLaMA, in\na few contemporary work (Chen et al., 2023; Li\net al., 2023; Muennighoff et al., 2022).\n6 Conclusion\nWe present the first framework, called Okapi, on\ninstruction tuning for LLMs in multiple language\nusing RLHF. We introduce instruction, ranked re-\nsponse, and evaluation data in 26 diverse languages\nto enable the training of RLHF methods. Our re-\nsults reveal the benefits of RLHF for multilingual\nfine-tuning of LLMs and the challenging problems\nof low-resource languages in this area.\nAcknowledgement\nThis research has been supported by the Army Re-\nsearch Office (ARO) grant W911NF-21-1-0112,\nthe NSF grant CNS-1747798 to the IUCRC Center\nfor Big Learning, and the NSF grant # 2239570.\n323\nEthical Statement\nOur framework utilizes the multilingual LLMs\nBLOOM-7B and LLaMa-7B to develop instruction-\ntuned models with reinforcement learning from hu-\nman feedback. To obtain necessary resources to\ntrain and evaluation our models, we also apply Self-\nInstruct (Taori et al., 2023) with GPT-3 to generate\nEnglish instruction data, and ChatGPT to translate\nand rank our response data in different languages.\nAs such, the models in our framework might in-\nherit potential issues in the underlying models of\nBLOOM, LLaMa, GPT-3, and ChatGPT, such as\nhallucination, biases, and toxic content. Regret-\ntably, the data required to train such LLMs, even in\nthe case of purportedly open-source models such as\nLLaMa and BLOOM, remains unreleased to enable\nessential investigation into these matters for our\nmodels. Future research can explore open-source\ndatasets, such as CulturaX (Nguyen et al., 2023)\nand RedPajama (Computer, 2023), to develop truly\nopen LLMs, enabling deeper attribution of the prob-\nlems and better understanding of the models’ op-\nerations. To maximally minimize the impacts of\nthese issues in the current work, our framework\nwill fully release the generated instruction, ranking,\nand evaluation data to enable comprehensive ex-\nploration and research for the techniques. We will\nalso restrict the release of our models to research\npurpose, respecting the policy of the underlying\nmodels such as LLaMa and ChatGPT, to facilitate\nfuture research for LLMs while limiting the poten-\ntial ethical issues for the society. Consequently, we\ndo not believe our framework poses any greater\nsocietal risks than existing published research in\nthis area for LLMs (Wang et al., 2023). Finally,\nwe confirm that our work fully complies with the\nACL Ethnics Policy and there is no other ethical\nissues associated with our work, to the best of our\nknowledge.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, and Abdulaziz\nAlshamsi et al. 2023. Falcon-40B: an open large\nlanguage model with state-of-the-art performance.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. ArXiv, abs/2302.04023.\nAli Borji. 2023. A categorical archive of chatgpt fail-\nures. ArXiv, abs/2302.03494.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan\nWang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao\nLiang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang\nWan, Benyou Wang, and Haizhou Li. 2023. Phoenix:\nDemocratizing chatgpt across languages. ArXiv,\nabs/2304.10453.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nJonathan Choi, Kristin Hickman, Amy Monahan, and\nDaniel Schwarcz. 2023. Chatgpt goes to law school.\nAvailable at SSRN.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan\nMartic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nProceedings of the Annual Conference on Neural\nInformation Processing Systems (NeurIPS).\nHyung Won Chung, Le Hou, S. Longpre, and Bar-\nret Zoph et al. 2022. Scaling instruction-finetuned\nlanguage models. ArXiv, abs/2210.11416.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nTogether Computer. 2023. Redpajama: An open source\nrecipe to reproduce llama training dataset.\nMike Conover, Matt Hayes, and Ankit Mathur et al.\n2023. Free dolly: Introducing the world’s first\ntruly open instruction-tuned llm. https://www.\ndatabricks.com.\nLeo Gao, Jonathan Tow, Stella Biderman, and et al.\n2021. A framework for few-shot language model\nevaluation.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Xiaodong Song, and\nJacob Steinhardt. 2021. Measuring massive multitask\nlanguage understanding.\nHuggingFace. 2023. Open llm leaderboard. https:\n//github.com/tatsu-lab/stanford_alpaca.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine. ArXiv,\n2301.08745.\nZachary Kenton, Tom Everitt, Laura Weidinger, Ia-\nson Gabriel, Vladimir Mikulik, and Geoffrey Irv-\ning. 2021. Alignment of language agents. ArXiv,\nabs/2103.14659.\n324\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben\nVeyseh, Hieu Man, Franck Dernoncourt, Trung Bui,\nand Thien Huu Nguyen. 2023. Chatgpt beyond en-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. ArXiv,\nabs/2304.05613.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji,\nand Timothy Baldwin. 2023. Bactrian-x: A multi-\nlingual replicable instruction-following model with\nlow-rank adaptation. ArXiv, abs/2305.15011.\nS. Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won\nChung, Yi Tay, Denny Zhou, Quoc V . Le, Barret\nZoph, Jason Wei, and Adam Roberts. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. ArXiv, abs/2301.13688.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heintz, and Dan Roth. 2023.\nRecent advances in natural language processing via\nlarge pre-trained language models: A survey. ACM\nComputing Survey.\nMosaicML. 2023. Introducing mpt-7b: A new standard\nfor open-source, commercially usable llms. https:\n//www.mosaicml.com/blog/mpt-7b.\nNiklas Muennighoff, Thomas Wang, and Lintang\nSutawika et al. 2022. Crosslingual general-\nization through multitask finetuning. ArXiv,\nabs/2211.01786.\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu\nMan, Nghia Trung Ngo, Franck Dernoncourt, Ryan\nRossi, and Thien Huu Nguyen. 2023. Culturax:\nA cleaned, enormous, and multilingual dataset for\nlarge language models in 167 languages. ArXiv,\nabs/2309.09400.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback. ArXiv,\nabs/2203.02155.\nJack Rae, Sebastian Borgeaud, and et al. 2021. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher. ArXiv, abs/2112.11446.\nVictor Sanh, Albert Webson, and Colin Raffel et al.\n2021. Multitask prompted training enables zero-shot\ntask generalization. ArXiv, abs/2110.08207.\nTeven Scao, Angela Fan, and et al. 2022. Bloom: A\n176b-parameter open-access multilingual language\nmodel. ArXiv, abs/2211.05100.\nStabilityAI. 2023. Stablelm: Stability ai language\nmodels. https://github.com/stability-AI/\nstableLM.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan J. Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul Christiano. 2020. Learn-\ning to summarize from human feedback. ArXiv,\nabs/2009.01325.\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep\nGanguli. 2021. Understanding the capabilities, limi-\ntations, and societal impact of large language models.\nArXiv, abs/2102.02503.\nRohan Taori, Ishaan Gulrajani, and Tianyi Zhang et al.\n2023. Stanford alpaca: An instruction-following\nllama model. https://github.com/tatsu-lab/\nstanford_alpaca.\nHugo Touvron, Thibaut Lavril, and Gautier Izacard et al.\n2023. Llama: Open and efficient foundation lan-\nguage models. ArXiv, abs/2302.13971.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484–13508, Toronto, Canada. Association\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V . Le. 2021. Finetuned language\nmodels are zero-shot learners. In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nLaura Weidinger, John F. J. Mellor, and Maribeth Rauh\net al. 2021. Ethical and social risks of harm from\nlanguage models. ArXiv, abs/2112.04359.\nMinghao Wu, Abdul Waheed, Chiyu Zhang, Muham-\nmad Abdul-Mageed, and Alham Fikri Aji. 2023.\nLamini-lm: A diverse herd of distilled models from\nlarge-scale instructions. ArXiv, abs/2304.14402.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\n325\nA Model Performance\nTables 4, 5, and 6 present the performance of\nthe models on the ARC, HellaSwag, and MMLU\ndatasets (respectively) across all languages when\nBLOOM is used as the base model. Similarly, Ta-\nbles 7, 8, and 9 report the performance with the\nbase model LLaMA over the three datasets. In the\ntables, in addition to the average scores over all\nlanguages for the models, we also include the aver-\nage scores for each group of languages (i.e., rows\n“Ave Group” for high-, medium-, and low-resource\nlanguages) to facilitate the comparisons.\nLanguage BLOOMBLOOMZSFT RLHF\nHigh-Resource\nRussian 27.5 25.5 29.2 30.3\nGerman 26.3 25.4 24.9 25.5\nChinese 37.3 37.0 37.9 40.0\nFrench 36.7 37.6 37.6 41.2\nSpanish 38.1 37.2 39.7 41.5\nItalian 29.0 27.5 29.3 31.3\nDutch 23.1 21.5 24.8 26.1\nVietnamese 33.7 33.5 35.0 36.2\nAve Group 31.5 30.7 32.3 34.0\nMedium-Resource\nIndonesian 36.0 35.9 37.4 38.8\nArabic 31.4 31.2 32.1 33.2\nHungarian 25.9 22.8 25.2 27.5\nRomanian 26.9 23.4 27.5 30.3\nDanish 24.6 24.6 23.6 25.2\nSlovak 24.9 22.5 26.2 27.3\nUkrainian 22.8 23.1 23.6 25.2\nCatalan 34.7 35.8 35.1 38.9\nSerbian 25.1 23.6 25.6 27.8\nCroatian 23.7 22.8 22.7 24.1\nHindi 29.2 28.2 28.5 29.6\nAve Group 27.7 26.7 28.0 29.8\nLow-Resource\nBengali 26.2 25.5 26.8 28.9\nTamil 24.2 25.6 23.7 25.1\nNepali 22.3 22.7 23.4 25.7\nMalayalam 26.4 25.1 24.6 24.7\nMarathi 27.3 24.8 25.8 26.0\nTelugu 24.3 25.8 23.9 24.5\nKannada 24.7 24.6 24.5 24.6\nAve Group 25.1 24.9 24.7 25.6\nAverage 28.2 27.4 28.4 30.0\nTable 4: Performance of the models on the trans-\nlated ARC dataset over different languages in Okapi.\nBLOOM 7B is used as the base LLM.\nLanguage BLOOMBLOOMZSFT RLHF\nHigh-Resource\nRussian 32.5 33.1 32.9 34.2\nGerman 32.4 33.1 34.7 35.9\nChinese 51.2 42.6 51.8 53.8\nFrench 56.6 45.7 55.9 58.7\nSpanish 56.7 48.7 56.1 59.0\nItalian 40.8 40.3 43.1 44.6\nDutch 31.7 32.3 32.6 34.9\nVietnamese 48.3 40.6 49.0 51.3\nAve Group 43.8 39.6 44.5 46.6\nMedium-Resource\nIndonesian 49.5 42.0 50.0 52.2\nArabic 43.3 39.5 44.3 47.0\nHungarian 30.1 29.8 30.8 32.7\nRomanian 31.8 32.3 33.1 35.2\nDanish 31.2 31.5 33.8 35.7\nSlovak 29.8 29.6 31.4 32.9\nUkrainian 30.0 30.4 32.2 33.6\nCatalan 51.2 40.3 50.9 53.8\nSerbian 29.9 30.1 30.7 33.7\nCroatian 30.0 29.4 30.5 31.6\nHindi 36.4 34.0 37.7 39.7\nAve Group 35.7 33.5 36.9 38.9\nLow-Resource\nBengali 32.8 31.5 33.9 35.4\nTamil 29.4 29.5 30.0 30.4\nNepali 30.9 31.9 32.5 34.1\nMalayalam 28.8 29.8 29.7 30.2\nMarathi 31.0 31.9 31.7 32.5\nTelugu 29.2 30.7 30.0 31.7\nKannada 30.3 30.9 30.7 32.1\nAve Group 30.3 30.9 31.2 32.3\nAverage 36.8 34.7 37.7 39.5\nTable 5: Performance of the models on the translated\nHellaSwag dataset over different languages in Okapi.\nBLOOM 7B is used as the base LLM.\n326\nLanguage BLOOMBLOOMZSFT RLHF\nHigh-Resource\nRussian 26.2 25.4 26.5 26.8\nGerman 28.1 25.6 27.0 28.6\nChinese 29.1 27.2 27.7 28.2\nFrench 27.4 27.7 27.7 28.4\nSpanish 28.9 27.1 27.8 28.1\nItalian 25.7 25.8 25.1 26.0\nDutch 26.4 26.0 26.1 26.0\nVietnamese 28.1 26.3 27.0 27.5\nAve Group 27.5 26.4 26.9 27.5\nMedium-Resource\nIndonesian 26.9 26.3 26.8 27.5\nArabic 27.5 24.4 27.4 27.7\nHungarian 26.9 26.1 25.4 26.3\nRomanian 27.4 25.9 27.6 27.4\nDanish 27.1 25.2 27.2 26.9\nSlovak 26.1 26.3 26.4 26.1\nUkrainian 26.6 25.8 25.9 26.4\nCatalan 28.8 26.0 26.7 27.6\nSerbian 27.2 25.7 27.5 27.6\nCroatian 26.0 26.1 26.4 27.7\nHindi 27.5 25.9 26.8 26.5\nAve Group 27.1 25.8 26.7 27.1\nLow-Resource\nBengali 28.2 25.9 27.1 26.8\nTamil 26.6 26.7 26.1 26.0\nNepali 26.6 25.6 25.5 25.2\nMalayalam 26.4 25.2 25.8 25.8\nMarathi 26.3 26.0 26.1 26.1\nTelugu 26.2 25.7 25.4 25.9\nKannada 26.7 26.0 26.6 26.8\nAve Group 26.7 25.9 26.1 26.1\nAverage 27.1 26.0 26.6 26.9\nTable 6: Performance of the models on the trans-\nlated MMLU dataset over different languages in Okapi.\nBLOOM 7B is used as the base LLM.\nLanguage LLaMA SFT RLHF\nHigh-Resource\nRussian 32.1 32.8 37.7\nGerman 35.1 37.5 39.7\nFrench 37.3 38.4 38.8\nSpanish 36.8 38.7 39.3\nItalian 35.8 36.3 39.4\nDutch 33.6 35.2 37.5\nAve Group 35.1 36.5 38.7\nMedium-Resource\nHungarian 29.8 31.4 33.2\nRomanian 32.4 33.8 37.5\nDanish 32.7 35.1 36.8\nSlovak 29.0 34.3 37.2\nUkrainian 32.9 35.7 36.4\nCatalan 35.1 36.8 36.9\nSerbian 30.8 33.5 35.8\nCroatian 33.0 33.8 35.9\nAve Group 32.0 34.3 36.2\nAverage 33.3 35.2 37.3\nTable 7: Performance of the models on the translated\nARC dataset over different languages in Okapi. LLaMA\n7B is used as the base LLM.\nLanguage LLaMA SFT RLHF\nHigh-Resource\nRussian 45.7 46.0 49.1\nGerman 49.9 49.0 52.6\nFrench 55.7 55.6 56.9\nSpanish 56.4 55.7 56.6\nItalian 52.0 52.5 55.9\nDutch 48.7 48.1 51.3\nAve Group 51.4 51.2 53.7\nMedium-Resource\nHungarian 37.9 38.7 41.0\nRomanian 44.9 45.1 48.7\nDanish 46.7 47.7 51.7\nSlovak 35.9 39.5 43.6\nUkrainian 44.1 46.9 47.7\nCatalan 49.6 49.2 49.0\nSerbian 41.1 42.6 45.0\nCroatian 41.1 42.4 45.2\nAve Group 42.7 44.0 46.5\nAverage 46.4 47.1 49.6\nTable 8: Performance of the models on the translated\nHellaSwag dataset over different languages in Okapi.\nLLaMA 7B is used as the base LLM.\nLanguage LLaMA SFT RLHF\nHigh-Resource\nRussian 30.2 30.0 30.6\nGerman 29.9 30.4 31.7\nFrench 30.5 31.0 30.7\nSpanish 30.3 30.4 30.9\nItalian 29.9 30.6 30.4\nDutch 29.8 30.0 31.1\nAve Group 30.1 30.4 30.9\nMedium-Resource\nHungarian 29.0 29.2 30.1\nRomanian 29.7 29.8 30.9\nDanish 30.0 30.9 31.8\nSlovak 29.4 29.6 30.2\nUkrainian 29.4 30.8 31.6\nCatalan 30.2 30.3 30.5\nSerbian 29.2 29.7 30.4\nCroatian 29.3 29.2 30.0\nAve Group 29.5 29.9 30.7\nAverage 29.8 30.1 30.8\nTable 9: Performance of the models on the trans-\nlated MMLU dataset over different languages in Okapi.\nLLaMA 7B is used as the base LLM.\n327",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.607542872428894
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5612425208091736
    },
    {
      "name": "Viet nam",
      "score": 0.49475735425949097
    },
    {
      "name": "Natural language",
      "score": 0.47433632612228394
    },
    {
      "name": "Natural language processing",
      "score": 0.4153536260128021
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4147687554359436
    },
    {
      "name": "Mathematics education",
      "score": 0.36796778440475464
    },
    {
      "name": "Linguistics",
      "score": 0.3651765286922455
    },
    {
      "name": "Psychology",
      "score": 0.19445562362670898
    },
    {
      "name": "Sociology",
      "score": 0.1819671392440796
    },
    {
      "name": "Philosophy",
      "score": 0.08676362037658691
    },
    {
      "name": "Ethnology",
      "score": 0.06412094831466675
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I181233156",
      "name": "University of Oregon",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1306409833",
      "name": "Adobe Systems (United States)",
      "country": "US"
    }
  ],
  "cited_by": 10
}