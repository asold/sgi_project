{
    "title": "Understanding the Potential of FPGA-based Spatial Acceleration for Large Language Model Inference",
    "url": "https://openalex.org/W4393949386",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2635132737",
            "name": "Hongzheng Chen",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2225729394",
            "name": "Jiahao Zhang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2887003153",
            "name": "Yixiao Du",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2765914890",
            "name": "Shaojie Xiang",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A5029785504",
            "name": "Zichao Yue",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2103584509",
            "name": "Niansong Zhang",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2131093075",
            "name": "Yaohui Cai",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2168878782",
            "name": "Zhiru Zhang",
            "affiliations": [
                "Cornell University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321636575",
        "https://openalex.org/W4312037452",
        "https://openalex.org/W2891946740",
        "https://openalex.org/W6800751262",
        "https://openalex.org/W4239385313",
        "https://openalex.org/W4385326807",
        "https://openalex.org/W4319166707",
        "https://openalex.org/W4394998694",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W2798956872",
        "https://openalex.org/W4281758439",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4211147898",
        "https://openalex.org/W3133395503",
        "https://openalex.org/W4308083513",
        "https://openalex.org/W3210312974",
        "https://openalex.org/W4300865759",
        "https://openalex.org/W3130240120",
        "https://openalex.org/W4383749446",
        "https://openalex.org/W4322718253",
        "https://openalex.org/W4280611847",
        "https://openalex.org/W4387321091",
        "https://openalex.org/W2906737788",
        "https://openalex.org/W3129734321",
        "https://openalex.org/W3047848469",
        "https://openalex.org/W3115388607",
        "https://openalex.org/W3086105743",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W4379260375",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3184454880",
        "https://openalex.org/W4310282800",
        "https://openalex.org/W2969388332",
        "https://openalex.org/W3204998121",
        "https://openalex.org/W4293024053",
        "https://openalex.org/W2473344385",
        "https://openalex.org/W4293023328",
        "https://openalex.org/W3162542754",
        "https://openalex.org/W3161542527",
        "https://openalex.org/W4321637273",
        "https://openalex.org/W3206837665",
        "https://openalex.org/W3176468986",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4293025835",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W2565125333",
        "https://openalex.org/W3130920634",
        "https://openalex.org/W4312060029",
        "https://openalex.org/W6838461927",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W4211118386",
        "https://openalex.org/W4389162879",
        "https://openalex.org/W4281651027",
        "https://openalex.org/W3211485653",
        "https://openalex.org/W4362598949",
        "https://openalex.org/W2094756095",
        "https://openalex.org/W2899915146",
        "https://openalex.org/W3199934250",
        "https://openalex.org/W3112948415",
        "https://openalex.org/W6849805803",
        "https://openalex.org/W3117511472",
        "https://openalex.org/W4388093177",
        "https://openalex.org/W4213153339",
        "https://openalex.org/W4241618768",
        "https://openalex.org/W4205983429",
        "https://openalex.org/W2913707927",
        "https://openalex.org/W2047674856",
        "https://openalex.org/W2767510344",
        "https://openalex.org/W2912918068",
        "https://openalex.org/W4206557440",
        "https://openalex.org/W4211095909",
        "https://openalex.org/W4254620533",
        "https://openalex.org/W2560674852",
        "https://openalex.org/W4226064176",
        "https://openalex.org/W4285056663",
        "https://openalex.org/W4206230517",
        "https://openalex.org/W4200208024",
        "https://openalex.org/W2322411027",
        "https://openalex.org/W2913668833",
        "https://openalex.org/W4299994276",
        "https://openalex.org/W2913954081",
        "https://openalex.org/W2963015836",
        "https://openalex.org/W4230315356",
        "https://openalex.org/W4253739894"
    ],
    "abstract": "Recent advancements in large language models (LLMs) boasting billions of parameters have generated a significant demand for efficient deployment in inference workloads. While hardware accelerators for Transformer-based models have been extensively studied, the majority of existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead. This article investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on field-programmable gate arrays (FPGAs). Our approach involves the specialization of distinct hardware units for specific operators or layers, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA. This model can be extended to multi-FPGA settings for distributed inference. Through our analysis, we can identify the most effective parallelization and buffering schemes for the accelerator and, crucially, determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart. To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as open-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT2) on an AMD Xilinx Alveo U280 FPGA device. Experimental results demonstrate our approach can achieve up to 13.4× speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2× speedup compared to Design for Excellence, an FPGA overlay, in the prefill stage, while achieving a 1.9× speedup and a 5.7× improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.",
    "full_text": null
}