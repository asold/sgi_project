{
    "title": "Inserting Information Bottlenecks for Attribution in Transformers",
    "url": "https://openalex.org/W3101068231",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2119712676",
            "name": "Zhiying Jiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2767000278",
            "name": "Raphael Tang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106810651",
            "name": "Ji Xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2163619555",
            "name": "Jimmy Lin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4288351520",
        "https://openalex.org/W4402843978",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W4287907717",
        "https://openalex.org/W2972312591",
        "https://openalex.org/W2810348861",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W3128452405",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2996657638",
        "https://openalex.org/W2516809705",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W2944854690",
        "https://openalex.org/W2626639386",
        "https://openalex.org/W2917322567",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W3035038672",
        "https://openalex.org/W2785760873",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W3102568136",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W1601924930",
        "https://openalex.org/W3035305735",
        "https://openalex.org/W4293861706",
        "https://openalex.org/W3101609372",
        "https://openalex.org/W3176322963",
        "https://openalex.org/W2964159778",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W4298061300",
        "https://openalex.org/W1686946872",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3850–3857\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3850\nInserting Information Bottlenecks for Attribution in Transformers\nZhiying Jiang, Raphael Tang, Ji Xinand Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n{zhiying.jiang, r33tang, ji.xin, jimmylin}@uwaterloo.ca\nAbstract\nPretrained transformers achieve the state of the\nart across tasks in natural language processing,\nmotivating researchers to investigate their in-\nner mechanisms. One common direction is to\nunderstand what features are important for pre-\ndiction. In this paper, we apply information\nbottlenecks to analyze the attribution of each\nfeature for prediction on a black-box model.\nWe use BERT as the example and evaluate our\napproach both quantitatively and qualitatively.\nWe show the effectiveness of our method in\nterms of attribution and the ability to provide\ninsight into how information ﬂows through lay-\ners. We demonstrate that our technique out-\nperforms two competitive methods in degrada-\ntion tests on four datasets. Code is available at\nhttps://github.com/bazingagin/IBA.\n1 Introduction\nIncreasingly prominent is the urge to interpret deep\nneural networks, with the success of these black-\nbox models remaining vastly inexplicable both the-\noretically and empirically. Within natural language\nprocessing (NLP), this desire is particularly true\nfor the pretrained transformer, which has witnessed\nan inﬂux of literature on interpretability analysis.\nSuch papers include visualizing transformer atten-\ntion mechanisms (Kovaleva et al., 2019), probing\nthe geometry of transformer representations (He-\nwitt and Manning, 2019), and explaining the span\npredictions of question answering models (van\nAken et al., 2019).\nIn this paper, we focus on prediction attribution\nmethods. That is, we ask, “Which hidden features\ncontribute the most toward a prediction?” To re-\nsolve this question, a number of methods (Selvaraju\net al., 2017; Smilkov et al., 2017) generate attribu-\ntion scores for features, which provide a human-\nunderstandable “explanation” of how a particular\nprediction is made at the instance level. Specif-\nically, given an instance, these methods assign a\nnumerical score for each hidden feature denoting\nits relevance toward the prediction.\nPrevious papers have demonstrated that gradient-\nbased methods fail to capture all the information as-\nsociated with the correct prediction (Li et al., 2016).\nTo address this weakness, Schulz et al. (2020) insert\ninformation bottlenecks (Tishby et al., 2000) for\nattribution, attaining both stronger empirical per-\nformance and a theoretical upper bound on the in-\nformation used. Additionally, mutual information\nis unconstrained by model and task (Guan et al.,\n2019). Thus, we adopt information bottlenecks for\nattribution (IBA) to interpret transformer models at\nthe instance level. We apply IBA to BERT (Devlin\net al., 2019) across ﬁve datasets in sentiment analy-\nsis, textual entailment, and document classiﬁcation.\nWe show both qualitatively and quantitatively that\nthe method capably captures information in the\nmodel’s token-level features, as well as insight into\ncross-layer behavior.\nOur contributions are as follows: First, we are the\nﬁrst to apply information bottlenecks (IB) for attri-\nbution to explain transformers. Second, we conduct\nquantitative analysis to investigate the accuracy of\nour method compared to other interpretability tech-\nniques. Finally, we examine the consistency of our\nmethod across layers in a case study. Across four\ndatasets, our technique outperforms integrated gra-\ndients (IG) and local interpretable model-agnostic\nexplanations (LIME), two widely adopted predic-\ntion attribution approaches.\n2 Related Work\nIn terms of scope, interpretability methods can be\ncategorized as model speciﬁc or model agnostic.\nModel-speciﬁc methods interpret only one fam-\nily of models, whereas model-agnostic techniques\naim for wide applicability across many families\n3851\nof parametric models. We can roughly separate\nmodel-agnostic methods into three categories: (1)\ngradient-based ones (Li et al., 2016; Fong and\nVedaldi, 2017; Sundararajan et al., 2017); (2) prob-\ning (Ribeiro et al., 2016; Lundberg and Lee, 2017;\nTenney et al., 2019; Clark et al., 2019; Liu et al.,\n2019); (3) information-theoretical methods (Bang\net al., 2019; Guan et al., 2019; Schulz et al., 2020;\nPimentel et al., 2020) .\nGradient-based methods are, however, limited to\nmodels with differentiable neural activations. They\nalso fail to capture all the information associated\nwith the correct prediction (Li et al., 2016). Al-\nthough probing methods provide detailed insight\ninto speciﬁc models, they fail to capture inner\nmechanisms like how information ﬂows through\nthe network (Guan et al., 2019). Information-\ntheoretic methods, in contrast, provide consistent\nand ﬂexible explanations, as we show in this paper.\nGuan et al. (2019) use mutual information to in-\nterpret NLP models across different tokens, layers,\nand neurons, but they lack a quantitative evaluation.\nBang et al. (2019) also propose a model-agnostic\ninterpretable model using IB; however, they limit\nthe information through the network by sampling a\ngiven number of words at the beginning, which re-\nstricts the explanation to neurons only. Our method\nis inspired by Schulz et al. (2020), who use IBA in\nimage classiﬁcation.\n3 Method\nThe idea of IBA is to restrict the information ﬂow-\ning through the network for every single instance,\nsuch that only the most useful information is kept.\nConcretely, given an input X ∈RN and output\nY ∈RM , an information bottleneck is an interme-\ndiate representation T that maximizes the follow-\ning function:\nI(Y; T) −β·I(X; T), (1)\nwhere I denotes mutual information and βcontrols\nthe trade-off between reconstruction I(Y; T) and\ninformation restriction I(X; T). The larger the β,\nthe narrower the bottleneck, i.e., less information\nis allowed to ﬂow through the network.\nWe insert the IB after a given layer l in a pre-\ntrained deep neural network. In this case, X =\nfl(H) represents the chosen layer’s output, where\nH is the input of the layer. We restrict information\nﬂow by injecting noise into the original input:\nT = µ ⊙X + (1 −µ) ⊙ϵ, (2)\nwhere ⊙denotes element-wise multiplication, ϵthe\ninjected noise, X the latent representation of the\nchosen layer, 1 the all-one vector, and µ ∈RN the\nweight balancing signal and noise. For every di-\nmension i, µi ∈[0,1], meaning that when µi = 1,\nthere is no noise injected into the original repre-\nsentation. To simplify the training process, we set\nµi = σ(αi), where σis the sigmoid function and\nα is a learnable parameter vector. In the extreme\ncase, where all the information in T is replaced\nwith noise ( T = ϵ), it’s desirable to keep ϵ the\nsame mean and variance as X in order to preserve\nthe magnitude of the input to the following layer.\nThus, we have ϵ∼N(µX,σ2\nX).\nAfter obtaining T, we evaluate how much infor-\nmation T still contains about X, which is deﬁned\nas their mutual information:\nI(X; T) = EX[DKL[P(T|X)∥P(T)]], (3)\nwhere DKL means Kullback–Leibler (KL) diver-\ngence, P(T|X) and P(T) represent their probabil-\nity distributions. While P(T|X) can be sampled\nempirically, P(T) has no analytical solution since\nit requires integrating over the feature mapP(T) =∫\nP(T|X)P(X)dX. As is standard, we use the\nvariational approximation Q(T) = N(µX,σ2\nX) to\nsubstitute P(T), assuming every dimension ofT is\nindependent and normally distributed. Even though\nthe independence assumption does not hold in gen-\neral, it only overestimates the mutual information,\ngiving a nice upper bound of mutual information\nbetween X and T:\nI(X; T) = EX[DKL[P(T|X)∥Q(T)]]\n−DKL[Q(T)∥P(T)] (4a)\nI(X; T) ≤EX[DKL[P(T|X)∥Q(T)]]. (4b)\nThe complete derivation of Equation 4b is in Ap-\npendix A. Since we expect I(X,T) to be small and\nmutual information to be always nonnegative, the\nupper bound is a desired property.\nIntuitively, the purpose of maximizingI(Y; T)\nis to make accurate predictions. Therefore, instead\nof directly maximizing I(Y; T), we minimize the\nloss function for the original task, e.g., the cross\nentropy LCE for classiﬁcation problems after insert-\ning the information bottleneck.\nCombining the above two parts, our ﬁnal loss\nfunction Lis\nL= DKL[P(T|X)∥Q(T)] + β·LCE. (5)\n3852\nIMDB MNLI Matched MNLI Mismatched AG News RTE\nRandom 0.011 0.106 0.106 0.008 0.012\nLIME 0.038 0.244 0.260 0.033 0.014\nIG 0.090 0.226 0.233 0.036 0.043\nIBA 0.229 0.374 0.367 0.029 0.059\nTable 1: Absolute probability drop for the target class after the top 11% most important tokens removed. The larger\nthe score, the more effective the method.\nNote that we negate the sign for minimization.\nThe βhyperparameter controls the relative impor-\ntance between the two loss components. After the\noptimization process, we obtain for every instance\na compressed representation T.\nWe then calculate DKL[P(T|X)∥Q(T)], indi-\ncating how much information is still kept in T\nabout X, which suggests the contribution of each\ntoken and feature. To generate the attribution map,\nwe sum over the feature–token axis, obtaining the\nattribution score of each token.\nOverall, we try to learn a compressed hidden\nrepresentation T that has just enough information\nabout the input X to predict the output Y. This\ncompression is done by adding noise, which re-\nmoves the least relevant feature-level information,\nwith µ controlling how much to remove.\n4 Experiments\nThrough experimentation, we analyze IBA both\nquantitatively and qualitatively to understand how\nit interprets deep neural network across layers.\n4.1 Experimental Setting\nWe compare our method on BERT with two\nother representative model-agnostic instance-level\nmethods—LIME (Ribeiro et al., 2016), which ex-\nplores interpretable models for approximation and\nexplanation, and integrated gradients (IG) (Sun-\ndararajan et al., 2017), a variation on computing\nthe gradients of the predicted output with respect\nto input features. For a simple baseline, we also\ncompare with “random,” whose attribution scores\nare assigned randomly to tokens. On each dataset,\nwe ﬁne-tune BERT and apply these interpretabil-\nity techniques to the model. We note the test ac-\ncuracy and generate an attribution score for each\ntoken. Details of all parameters are attached in\nAppendix D.\nThere is no consensus on how to evaluate inter-\npretability methods quantitatively (Molnar, 2019).\nLIME’s simulated evaluation leverages the ground\ntruth of already interpretable models like deci-\nsion trees, but the ground truth is unavailable for\nblack-box models like neural networks. There-\nfore, we follow Ancona et al. (2018) and Hooker\net al. (2018) and carry out a degradation teston\nIMDB (Maas et al., 2011), AG News (Gulli, 2004),\nMNLI (Williams et al., 2018), and RTE (Wang\net al., 2018), covering sentiment analysis, natural\nlanguage inference, and text classiﬁcation.\nThe degradation test has the following steps:\n1. Generate attribution scores s for each inter-\npretability method f: s= f(M,x,y ), where\nxis the test instance, yis the target label, and\nMis the model.\n2. Sort tokens by their attribution score in de-\nscending order.\n3. Remove top k tokens to obtain x′, the de-\ngraded instance; kcan be preset.\n4. Test the target class probability p(y|x′) with\nthe original model on the degraded instance.\n5. Repeat steps 3 and 4 until all tokens removed.\nFor the ﬁnal visualization, we average all test in-\nstances at each degradation step to compute¯p(y|x′).\nThen, we normalize the degradation test result\n¯p(y|x′) to [0,1] using the normalized probability\ndrop ¯d = ¯p(y|x′)−m\no−m , where o means the origi-\nnal probability on the nondegraded instance, and\nm means the minimum of the fully degraded in-\nstance’s probability across all interpretability mod-\nels. In this way, the normalized probability drop ¯d\nwill be independent of the original model quality\nand easily comparable across models. Note that,\nfor IBA, we perform the degradation test on the\noriginal model, not the one with the inserted bot-\ntleneck. Thus, a large β does not directly cause\nthe probability to drop. An effective attribution\nmap can ﬁnd the most important tokens, which\nmeans ¯p(y|x′) after the degradation step will drop\nsubstantially.\n3853\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nIBA\nIG\nLIME\nRandom\n(a) IMDB\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nIBA\nIG\nLIME\nRandom (b) MNLI Matched\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nIBA\nIG\nLIME\nRandom (c) MNLI Mismatched\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nIBA\nIG\nLIME\nRandom\n(d) AG News\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nIBA\nIG\nLIME\nRandom (e) RTE\nFigure 1: Degradation test results comparing IBA, IG, LIME, and random.\n4.2 Results and Analysis\nOverall, the results show that our method better\nidentiﬁes the most important tokens compared to\nother model-agnostic interpretability methods.\nQuantitative Analysis. Table 1 shows the abso-\nlute probability drop ∥¯p(y|x) −o∥with the ﬁrst\n11% of the important tokens removed. We fur-\nther plot the normalized probability drop after each\npercentage of the important tokens is removed, as\nshown in Figure 1, indicating how much important\ninformation is lost for prediction: the steeper the\nslope, the better the ability to capture important\ntokens. For this experiment, we insert the infor-\nmation bottleneck after layer 9, and we see that\nremoving important tokens that are identiﬁed by\nour method deteriorates the probability the most on\nIMDB and MNLI Matched/Mismatched.\nOf course, choosing the right layer to insert the\ninformation bottleneck is crucial to the result. It\nalso indicates which layer encodes the most mean-\ningful information for prediction. To investigate\ndifferences in inserting information bottlenecks af-\nter different layers, we carry the degradation test on\n1000 random test samples across layers on IMDB,\nas shown in Figure 2a—see Appendix B for all 12\nlayers. Insertion after layers 1, 8, and 9 generates\nmore meaningful attribution scores. At layer 1, the\ntokens remain distinct (i.e., representations have\nnot been aggregated), and it is likely that the latent\nrepresentation T is essentially capturing per-token\nsentiment values. The big drop of ¯dafter layers 8\nand 9, on the other hand, is interesting. Recently,\nXin et al. (2020) examined early exit mechanisms\nin BERT and found that halting inference at lay-\ners 8 or 9 produces results not much worse than\nfull inference, which suggests that an abundance of\ninformation is encoded in those layers.\nAnother important parameter is β, which con-\ntrols the trade-off between restricting the informa-\ntion ﬂow and achieving greater accuracy. A smaller\nβ allows more information through, and an ex-\ntremely small β has the same effect of using X\nas the attribution map. As Figure 2b shows, when\nβ ≤1e−6, the degradation curve is similar to the\none using X only. Appendix C shows the effects\nof different βon a speciﬁc example.\nQualitative Analysis. The ﬁrst plot in Figure 3\nshows the before and after comparison of IB in-\nsertion, with positive tokens highlighted. The sec-\nond and third plots visualize attribution maps for\ninstances across layers. Consistent with our quanti-\ntative analysis in Figure 2a, these plots demonstrate\n3854\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nL1\nL3\nL6\nL8\nL9\nL12\n(a) IB after different layers.\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\n= 1e 4\n= 1e 5\n= 1e 6\n= 1e 7\nX (b) IB with different β.\nFigure 2: Analysis of different layers and different β.\n[CLS]\ni\nlikedthis\nmovie\na lot\n. it\nreally\nintrigued\nmehowdean##na\nandalicia\nBefore\nAfter\n 0.5\n1.0\n[CLS]\ni\nlikedthis\nmovie\nalot\n. it\nreally\nintrigued\nmehowdean##na\nandalicia\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\ni\ndidthatall\nthroughcollege\nand\nit\nworked\ntoo\n[SEP]\nI\ndidthatall\nthroughcollege\nbut\nit\nneverworked\n[SEP]\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nFigure 3: Illustrations from left to right are as follows: The before and after comparison of inserting an information\nbottleneck after layer 6; attribution for an IMDB example with the positive label; attribution for an MNLI example\nwith the contradiction label.\nthat, for a fully ﬁne-tuned BERT, layers 8 and 9\nseem to encode the most important information\nfor the prediction. For example, in the IMDB in-\nstance, liked and intrigued have the highest attribu-\ntion scores for the prediction of positive sentiment\nacross most layers—see layer 9 in particular. In\nthe MNLI example, never is mostly highlighted\nstarting from layer 7 to predict “contradiction.”\n5 Conclusion\nIn this paper, we adopt an information-bottleneck-\nbased approach to analyze attribution for transform-\ners. Our method outperforms two widely used at-\ntribution methods across four datasets in sentiment\nanalysis, document classiﬁcation, and textual en-\ntailment. We also analyze the information across\nlayers both quantitatively and qualitatively.\nAcknowledgments\nThis research was supported in part by the Canada\nFirst Research Excellence Fund and the Natural Sci-\nences and Engineering Research Council (NSERC)\nof Canada.\nReferences\nBetty van Aken, Benjamin Winter, Alexander L ¨oser,\nand Felix A. Gers. 2019. How does BERT answer\nquestions? A layer-wise analysis of transformer rep-\nresentations. In Proceedings of the 28th ACM Inter-\nnational Conference on Information and Knowledge\nManagement.\nMarco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and\nMarkus Gross. 2018. Towards better understanding\nof gradient-based attribution methods for deep neu-\nral networks. In International Conference on Learn-\ning Representations.\nSeojin Bang, Pengtao Xie, Heewook Lee, Wei Wu,\nand Eric Xing. 2019. Explaining a black-box using\ndeep variational information bottleneck approach.\narXiv:1902.06918.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? An analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n3855\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nRuth C. Fong and Andrea Vedaldi. 2017. Interpretable\nexplanations of black boxes by meaningful perturba-\ntion. In Proceedings of the IEEE International Con-\nference on Computer Vision.\nChaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin\nChen, Di He, and Xing Xie. 2019. Towards a deep\nand uniﬁed understanding of deep neural models\nin NLP. In International Conference on Machine\nLearning.\nAntonio Gulli. 2004. AGNews. http:\n//groups.di.unipi.it/˜gulli/AG_corpus_\nof_news_articles.html.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,\nand Been Kim. 2018. Evaluating feature importance\nestimates. arXiv:1806.10758.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP).\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies.\nScott M. Lundberg and Su-In Lee. 2017. A uniﬁed\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing systems.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies.\nChristoph Molnar. 2019. Interpretable Machine\nLearning. https://christophm.github.io/\ninterpretable-ml-book/.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. arXiv:2004.03061.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. “Why should i trust you?” explain-\ning the predictions of any classiﬁer. In Proceedings\nof the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining.\nKarl Schulz, Leon Sixt, Federico Tombari, and Tim\nLandgraf. 2020. Restricting the ﬂow: Information\nbottlenecks for attribution. In International Confer-\nence on Learning Representations.\nRamprasaath R. Selvaraju, Michael Cogswell, Ab-\nhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based local-\nization. In Proceedings of the IEEE international\nconference on computer vision.\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda\nVi´egas, and Martin Wattenberg. 2017. SmoothGrad:\nremoving noise by adding noise. arXiv:1706.03825.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Interna-\ntional Conference on Machine Learning.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel Bowman, Dipanjan\nDas, and Ellie Pavlick. 2019. What do you learn\nfrom context? Probing for sentence structure in con-\ntextualized word representations. In International\nConference on Learning Representations.\nNaftali Tishby, Fernando C. Pereira, and William\nBialek. 2000. The information bottleneck method.\narXiv:physics/0004057.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. DeeBERT: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics.\n3856\nA Proof of Variational Upper Bound\nI(X; T) = EX[DKL[P(T|X)∥P(T)]]\n=\n∫\nX\np(x)(\n∫\nT\np(t|x) log p(t|x)\np(t) dt)dx\n=\n∫\nX\n∫\nT\np(x,t) log p(t|x)\np(t)\nq(t)\nq(t)dtdx\n=\n∫\nX\n∫\nT\np(x,t) log p(t|x)\nq(t) dtdx\n+\n∫\nX\n∫\nT\np(x,t) log q(t)\np(t)dtdx\n=\n∫\nX\n∫\nT\np(x,t) log p(t|x)\nq(t) dtdx\n+\n∫\nT\np(t)(\n∫\nX\np(x|t)dx) log q(t)\np(t)dt\n= EX[DKL[P(T|X)∥Q(T)]]\n−DKL[Q(T)∥P(T)]\n≤EX[DKL[P(T|X)∥Q(T)]]\nB Degradation Test across 12 Layers\nFigure 4 shows the complete version of the degrada-\ntion test across all 12 layers. In general, the earlier\nwe insert the bottleneck, the larger the probability\ndrop is, except for layers 8 and 9, which are the\nonly two layers with steeper slopes than layer 1.\n0.0 0.2 0.4 0.6 0.8 1.0\nDegradation Fraction\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized Probability Drop\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nL10\nL11\nL12\nFigure 4: Degradation test results across all layers.\nC Visualization of the Effects ofβ\nFigure 5 shows the effects of different βon a spe-\nciﬁc example. As we can see, when βis as small as\n10−7, most information is allowed to ﬂow through\nthe network and thus most parts are highlighted.\nIn contrast, when βis larger, the representation is\nmore restricted.\n[CLS]\ni\nlikedthis\nmovie\na lot . it\nreally\nintrigued\nmehowdean##naandalicia\nbecamefriends\nover\nlayer1\nlayer2\nlayer3\nlayer4\nlayer5\nlayer6\nlayer7\nlayer8\nlayer9\nlayer10\nlayer11\nlayer12\n(a) β = 10−3\n[CLS]\ni\nlikedthis\nmovie\na lot . it\nreally\nintrigued\nmehowdean##naandalicia\nbecamefriends\nover\nlayer1\nlayer2\nlayer3\nlayer4\nlayer5\nlayer6\nlayer7\nlayer8\nlayer9\nlayer10\nlayer11\nlayer12\n(b) β = 10−7\nFigure 5: Comparison of BERT attribution maps with\ndifferent values of β.\nD Detailed Parameters and Dataset\nInformation\nTo keep as much information as possible at the\nbeginning, µi should be set close to 1,∀i, in which\ncase T ≈X. So we initialize with αi = 5 ,∀i\nand therefore µi ≈0.993. In order to stabilize the\nresult, the input of the bottleneck (X) is duplicated\n10 times with different noise added. We set the\nlearning rate to 1 and the number of training steps\nto 10. We use empirical estimation for β ≈10 ×\nLCE\nLIB\n. For IMDB, MNLI Matched/Mismatched,\nand AGNews, we insert the IB after layer 9 and β\nis set to 10−5. For RTE, we insert the IB after layer\n10 and βis set to 10−4.\nWe carry out experiments on NVIDIA RTX 2080\nTi GPUs with 11GB VRAM running PyTorch 1.4.0\nand CUDA 10.0. A full technical description of\nour computing environment is released alongside\nour codebase. For LIME, we set N, the number of\npermuted samples drawn from the original dataset,\nto 100 as this reaches the limitation of GPU mem-\nory. Similarly, the number of steps of integrated\ngradients is set to 10 because it is more memory\nintensive. The average time of running 25000 in-\n3857\nstances on the described GPU is about 10 hours for\nIBA, 13 hours for LIME, and 2 hours for IG.\nDataset Number of Dev/Test\nIMDB 25000\nMNLI Matched 9815\nMNLI Mismatched 9832\nAG News 7600\nRTE 277\nTable 2: Dataset Details.\nWe use the test sets when the label is provided\nand use the dev sets otherwise. See Table 2 for\ndetails. Note that “IMDB” refers to the sentiment\nanalysis dataset provided by Maas et al. (2011).\n“MNLI Matched” means that the training set and the\ntest set have the same set of genres while “MNLI\nMismatched” means that genres that appear in the\ntest set don’t appear in the training set. Detailed\ninformation of the MNLI dataset can be found in\nWilliams et al. (2018)."
}