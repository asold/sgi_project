{
  "title": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View",
  "url": "https://openalex.org/W4393160097",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2889710878",
      "name": "Raphael Schumann",
      "affiliations": [
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2165278438",
      "name": "Wanrong Zhu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2231959515",
      "name": "Weixi Feng",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A4314316502",
      "name": "Tsu-Jui Fu",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A1993280370",
      "name": "Stefan Riezler",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2110566271",
      "name": "William Yang Wang",
      "affiliations": [
        "University of California, Santa Barbara"
      ]
    },
    {
      "id": "https://openalex.org/A2889710878",
      "name": "Raphael Schumann",
      "affiliations": [
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A1993280370",
      "name": "Stefan Riezler",
      "affiliations": [
        "Heidelberg University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2770387316",
    "https://openalex.org/W4288055249",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2903172725",
    "https://openalex.org/W3208883793",
    "https://openalex.org/W6633497868",
    "https://openalex.org/W4310557634",
    "https://openalex.org/W2805984364",
    "https://openalex.org/W3106806814",
    "https://openalex.org/W3172675210",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3211462570",
    "https://openalex.org/W3093241733",
    "https://openalex.org/W4221147783",
    "https://openalex.org/W3099019594",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3200063923",
    "https://openalex.org/W1931877416",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W3117275708",
    "https://openalex.org/W4224329474",
    "https://openalex.org/W2993086250",
    "https://openalex.org/W4382699535",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W2900790749",
    "https://openalex.org/W3089140124",
    "https://openalex.org/W3214506775",
    "https://openalex.org/W3020859112",
    "https://openalex.org/W3038667361",
    "https://openalex.org/W4285069854",
    "https://openalex.org/W4318719538",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4319299816",
    "https://openalex.org/W3106129629",
    "https://openalex.org/W1559723967",
    "https://openalex.org/W4378768622",
    "https://openalex.org/W3100923070",
    "https://openalex.org/W4286973470",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W3155707959",
    "https://openalex.org/W3034578524",
    "https://openalex.org/W4226163860",
    "https://openalex.org/W3175443984",
    "https://openalex.org/W4285224875",
    "https://openalex.org/W3034253961",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4312434279"
  ],
  "abstract": "Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation (VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that VELMA is able to successfully follow navigation instructions in Street View with only two in-context examples. We further finetune the LLM agent on a few thousand examples and achieve around 25% relative improvement in task completion over the previous state-of-the-art for two datasets.",
  "full_text": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language\nNavigation in Street View\nRaphael Schumann1, Wanrong Zhu2, Weixi Feng2, Tsu-Jui Fu2,\nStefan Riezler1,3, William Yang Wang2\n1Computational Linguistics, Heidelberg University, Germany\n2University of California, Santa Barbara\n3IWR, Heidelberg University, Germany\n{rschuman, riezler}@cl.uni-heidelberg.de, {wanrongzhu, weixifeng, tsu-juifu, william}@cs.ucsb.edu\nAbstract\nIncremental decision making in real-world environments is\none of the most challenging tasks in embodied artificial intel-\nligence. One particularly demanding scenario is Vision and\nLanguage Navigation (VLN) which requires visual and nat-\nural language understanding as well as spatial and temporal\nreasoning capabilities. The embodied agent needs to ground\nits understanding of navigation instructions in observations\nof a real-world environment like Street View. Despite the im-\npressive results of LLMs in other research areas, it is an on-\ngoing problem of how to best connect them with an interac-\ntive visual environment. In this work, we propose VELMA,\nan embodied LLM agent that uses a verbalization of the tra-\njectory and of visual environment observations as contextual\nprompt for the next action. Visual information is verbalized\nby a pipeline that extracts landmarks from the human written\nnavigation instructions and uses CLIP to determine their visi-\nbility in the current panorama view. We show that VELMA is\nable to successfully follow navigation instructions in Street\nView with only two in-context examples. We further fine-\ntune the LLM agent on a few thousand examples and achieve\naround 25% relative improvement in task completion over the\nprevious state-of-the-art for two datasets.\nIntroduction\nLarge language models (LLMs), which have shown impres-\nsive reasoning capabilities in traditional natural language\nprocessing tasks, are increasingly used as the reasoning en-\ngine of embodied agents for, e.g., household robots (Shrid-\nhar et al. 2020), video games (Wang et al. 2023) and in-\ndoor navigation (Zhou, Hong, and Wu 2023). These tasks are\nmostly based on simulations that either feature computer-\ngenerated images with a fixed set of displayable objects and\ntextures, or are limited in scale and trajectory length. In this\npaper, we present a verbalization embodiment of an LLM\nagent (VELMA) for urban vision and language navigation\nin Street View. The unique challenge of this task is the com-\nbination of a large-scale environment derived from an actual\nroad network, real-world panorama images with dense street\nscenes, and long navigation trajectories. The agent needs to\nground its understanding of the navigation instructions in\nthe observable environment and reason about the next ac-\ntion to reach the target location. The navigation instructions\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\"Orientate yourself such that a blue bench is on your right, go to the \nend of the block and make a right. Follow the park on your left and \nmake a right at the intersection. Pass the black ﬁre hydrant on your \nright and stop when you get to a gray door on the brown building.\"\nThere is a blue bench on your left.\n1. turn_around\nThere is a blue bench on your right.\n2. forward\nThere is a 3-way intersection.\n3. right\n4. forward\nThere is a park on your left.\n5. forward\nThere is a park on your left.\n6. forward\nThere is a 4-way intersection.\n7. <next word prediction> \nNavigate to the described target location!\nA\nction Space: forward, left, right, turn_around, stop\nNavigation Instructions: \nAction Sequence:\nFigure 1: Prompt sequence used to utilize LLMs for VLN\nin Street View. Verbalized observations of the visual envi-\nronment are in green and appended to the prompt at each\nstep. Agent actions (blue) are acquired by LLM next word\nprediction. Highlighting of text for visual presentation only.\nFull navigation trajectories are, on average, 40 steps long.\nare written by humans and include open-ended landmark\nreferences and directional indications intended to guide the\nagent along the desired path. In order to leverage the rea-\nsoning capabilities of LLMs, we use embodiment by ver-\nbalization, a workflow where the task, including the agent’s\ntrajectory and visual observations of the environment, is ver-\nbalized, thus embodying the LLM via natural language. Fig-\nure 1 shows the verbalization at step 7 of the current trajec-\ntory for a given navigation instance. At each step, the LLM\nis prompted with the current text sequence in order to pre-\ndict the next action. Then the predicted action is executed\nin the environment, and the new observations are verbalized\nand appended to the prompt. This is repeated until the agent\neventually predicts to stop.\nThe main contributions of our work are as follows:\n(i) We introduce VELMA, to our knowledge, the first LLM-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18924\nbased agent for urban VLN. (ii) We report few-shot results\nfor the urban VLN task and achieve new state-of-the-art\nperformance by finetuning our agent on the training set.\n(iii) We address and resolve limitations of the commonly\nused Touchdown environment (Chen et al. 2019), making\nit amenable for few-shot agents.\nRelated Work\nOutdoor VLN Agent models for the outdoor/urban VLN\ntask (Chen et al. 2019) commonly follow a sequence-to-\nsequence architecture where encoded text and image rep-\nresentations are fused for each decoder step (Xiang, Wang,\nand Wang 2020; Mehta et al. 2020; Schumann and Riezler\n2022; Sun et al. 2023). Other proposed agents employ pre-\ntrained vision and language transformers that are finetuned\non task-specific data (Zhu et al. 2021; Armitage, Impett, and\nSennrich 2023). Zhong et al. (2021) represent the visual en-\nvironment by symbols using semantic segmentation and ex-\ntreme downsampling of panorama images, but their agent\ndoes not improve over previous success rates. Other work\nuses CLIP to score the presence of extracted landmarks at\neach panorama node in a graph and uses this information\nto plan a route for given navigation instructions (Shah et al.\n2022). Their non-urban environment has a graph with 300\nnodes, and the navigation path is planned a priori with full\naccess to all panorama images and landmark scores. In con-\ntrast, our agent is embodied and has to plan ad-hoc with ac-\ncess to directly observed information only.\nIndoor VLN Indoor agents (Fried et al. 2018; Wang et al.\n2019; Tan, Yu, and Bansal 2019; Fu et al. 2020; Zhu et al.\n2020; Qi et al. 2020; Hong et al. 2021; Chen et al. 2021; Li,\nTan, and Bansal 2022) are used for navigation datasets like\nR2R (Anderson et al. 2018) and RxR (Ku et al. 2020) or Ob-\njectNav (Ramakrishnan et al. 2021; Zhou et al. 2023). Khan-\ndelwal et al. (2022) showed that using the CLIP encoder for\nimage features improves performance for a range of vision\nand language tasks. Recently, Zhou, Hong, and Wu (2023)\nintroduced an LLM-based agent for R2R that incorporates\nimage information by transcribing its entire content with an\nimage-to-text model. This is feasible because the navigation\ntrajectories are only six steps on average compared to 40\nsteps in the urban VLN task considered in our work. Another\nnotable indoor VLN agent uses CLIP to directly predict the\nnext action by scoring the compatibility of the current sub-\ninstruction with available waypoint images (Dorbala et al.\n2022).\nUrban VLN Environment\nWe use the Touchdown environment introduced by Chen\net al. (2019). The environment is based on Google’s Street\nView and features 29,641 full-circle panorama images con-\nnected by a navigation graph. It covers the dense urban street\nnetwork spanning lower Manhattan. The navigation graph is\na directed graph G = ⟨V, E⟩ where each edge ⟨v, v′⟩ ∈E is\nassociated with α⟨v,v′⟩ which is the heading direction from\nnode v to node v′ ranging from 0◦ to 360◦. The agent state\ns = (v, α) is composed of its current position v ∈ V and\nits heading direction α. The agent can move by executing an\n1\n2\n3\n4 5\n7\n6\n352°\n20°\n288° 95°\n50°\n345°\nFigure 2: The Touchdown environment introduced by Chen\net al. (2019) can require action sequences that are seman-\ntically inconsistent with the correct navigation instructions.\nIn the depicted subgraph, the action sequence to move from\nnode 1 to node 5 is to move FORWARD four times. The se-\nmantically correct sequence of actions would include a right\nturn in between. We fix the problem by modifying the envi-\nronment behavior and selecting the desired direction at in-\ntersections in relation to all outgoing streets.\naction a ∈ {FORWARD, LEFT, RIGHT, STOP}. The state tran-\nsition function st+1 = ϕ(at, st) defines the behavior of the\nagent executing an action. In Chen et al. (2019), the agent’s\nheading αt at position v is restricted to align with the head-\ning of an outgoing edge α⟨v,v′⟩. In case of the RIGHT action,\nthe new state st+1 is (v, α⟨v,⃗ v⟩) where ⃗ vis the neighboring\nnode closest to the right of the agent’s current heading. In\nother words, the agent is rotated in place to the right until it\nsnaps to the direction of an outgoing edge. Likewise, for the\nLEFT action. In the case of the FORWARD action, the agent\nmoves along the edge ⟨v, v′⟩ according to its current head-\ning direction αt = α⟨v,v′⟩. The environment is then forced\nto automatically rotate the agent’s heading towards an out-\ngoing edge: αt+1 = α⟨v′,v∗⟩ where v∗ is the neighbor node\nin the direction closest to the previous heading αt.\nAlignment Inconsistencies in Touchdown\nAs described in Schumann and Riezler (2022), the auto-\nmatic rotation mentioned above can lead to generalization\nproblems, e.g., when moving towards the flat side of a T-\nintersection. For example, if the agent is automatically ro-\ntated towards the right facing street and subsequently ex-\necutes the RIGHT action, it rotates towards the direction it\ncame from instead of clearing the intersection in the in-\ntended direction. The same problem also occurs at intersec-\ntions with more than three directions. Figure 2 gives an il-\nlustrative example that shows the navigation graph at a 4-\nway intersection. Because the environment is derived from\na real-world street layout, the nodes in the graph are not\nperfectly arranged as in an artificial grid world. In order\nto make a right turn at the intersection and to follow the\nroute from v1 to v5, one expects to use the action sequence\n[FORWARD, FORWARD, RIGHT, FORWARD, FORWARD]. However,\nwhen the agent reaches v3, it is automatically rotated to-\nwards the closest outgoing edge, in this case, ⟨v3, v4⟩. This\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18925\nis because the rotation 20◦→50◦ towards v4 is shorter than\nthe rotation 20◦→345◦ towards v7. As such, the required se-\nquence of actions to go from v1 to v5 in Chen et al. (2019)’s\nenvironment is [FORWARD, FORWARD, FORWARD, FORWARD].\nThis is unpredictable and is not correctly aligned with ”turn\nright at the intersection” instructions. 1 To alleviate this\nproblem, Schumann and Riezler (2022) explicitly feed the\nchange of heading at each timestep as additional input to\ntheir model. This enables the agent to anticipate the unex-\npected rotation and to adapt to it. Because adding heading\ndelta values to the text-based interface makes it convoluted\nand unnecessarily difficult for few-shot learning, we propose\na more intuitive way to solve this ambiguity at intersections.\nWe modify the state transition functionϕ such that the agent\nis not automatically rotated when moving FORWARD. This\nmeans the agent’s heading αt is not automatically aligned\nwith an outgoing edge. Instead, the direction is selected in\nrelation to all outgoing edges. The agent at node v3 in Fig-\nure 2 has the nodesv6, v7 and v4 in front. The forward direc-\ntion is selected as the middle one of the three edges, the right\ndirection as the right-most edge, and the left direction as the\nleft-most edge. This means that executing the RIGHT action\nat position v3 will now rotate the agent towards nodev4 and\nallows to use the semantically correct sequence of actions\nfor the depicted route. The proposed modification solves the\nissue of inconsistent action sequences at intersections and\nallows to use agents that are not specifically trained in this\nenvironment.\nTurning Around\nWe additionally introduce the TURN\nAROUND action which\nlets the agent reverse its direction: st+1 = (v, αt − 180◦).\nIn the unmodified environment, this is achieved using the\nLEFT or RIGHT action on regular street segments. The new\naction is better aligned with natural language verbalizations\nof direction reversal and promotes intuitive communication\nwith the environment.\nNavigation Task\nThe objective of the navigation task is to find the goal loca-\ntion by following the given navigation instructions. A navi-\ngation instance is defined by the initial state s1, target node\nˆvT , gold path (ˆv1, ˆv2..., ˆvT ) and navigation instructions text\nn = (w1, w2, ..., wN ). The agent starts ats1 and predicts the\nnext action a1 based on the navigation instructions and cur-\nrent observations. These are the panorama image and num-\nber of outgoing edges at the current position. The environ-\nment processes the action and puts the agent into a new state:\ns2 = ϕ(a1, s1). This is repeated until the agent predicts\nSTOP at the presumed goal location. If the agent stops within\none neighboring node of the target node, the navigation ob-\njective is considered accomplished.\nChallenges\nOne main challenge to successfully follow the navigation\ninstructions is to reliably detect landmarks in the panorama\n1In the Appendix we show more examples for 3-way, 4-way\nand 5-way intersections.\nEgocentric Spatial Reasoning\n1. ... turn so the orange barrier is on your left ...\n2. ... a red truck in front of you ...\n3. ... a playground on the far right corner ahead ...\nAllocentric Spatial Reasoning\n4. ... green metal pole with pink flowers on top ...\n5. ... building with columns around the windows ...\n6. ... stop in between Chase and Dunkin’ Donuts ...\nTemporal Reasoning\n7. ... go straight until you see Chipotle and then ...\n8. ... once you passed the underpass ...\n9. ... stop when the park on your right ends ...\nOther\n10. ... proceed straight through three intersections ...\n11. ... you should see TD Bank on your left ...\n12. ... if you see Dory Oyster Bar, you are too far ...\nTable 1: Reasoning skills the embodied LLM agent must\npossess in order to successfully complete the navigation\ntask. Each with three example snippets from the navigation\ninstructions.\nimages along the route. The landmarks mentioned in the\ninstructions are open-ended and can refer to any object or\nstructure found in street scenes, including vegetation, build-\ning features, vehicle types, street signs, construction utili-\nties, company logos and store names. The agent also needs\nto posses different types of reasoning, most importantly spa-\ntial reasoning to follow general directions, locate landmarks\nand evaluate stopping conditions. The agent also needs to\nunderstand the temporal aspect of the task and reason about\nthe sequence of previous observations and actions. See Ta-\nble 1 for example snippets from the navigation instructions.\nDatasets\nThere are two datasets that provide navigation instruc-\ntions for the environment described in Section : Touch-\ndown (Chen et al. 2019) and Map2seq (Schumann and Rie-\nzler 2021). Each dataset includes around 10k navigation in-\nstances, and we utilize them in the more challenging un-\nseen scenario introduced by Schumann and Riezler (2022).\nThis means that generalization is crucial because the train-\ning routes are located in an area that is geographically sepa-\nrated from the area of development and test routes. The main\ndifference between the two datasets is that Touchdown in-\nstructions were written by annotators who followed the route\nin Street View, while Map2seq instructions were written by\nannotators that saw a map of the route. The Map2seq nav-\nigation instructions were later validated to also be correct\nin Street View. Another difference is that the initial state in\nMap2seq orientates the agent towards the correct direction\nwhich leads to overall better task completion rates than for\nTouchdown instances.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18926\n& Heading\nPanorama\nEnvironment\nStructured Output:\n{ \"landmarks\":   { \"Starbucks\": \"right\" } }\nLandmark Scorer\n\"picture of a mail truck\"\n\"picture of Starbucks\"\n1.29 -1.12 -2.27 4.152.85\n-0.76 -2.20 1.87 1.982.15\nStandardized CLIP Scores (Threshold: 3.5):\nLandmark Extractor\nWrite a list of landmarks in \nthe navigation instructions:\n- Starbucks\n- a mail truck\nThere is a Starbucks on your right.\nVerbalizer\nPrompt Sequence\nNavigate to the described target location!\nAction Space:\nforward, left, right, turn_around, stop\nNavigation Instructions: \n\"Go straight and turn right at the intersection. \nGo straight until there is a Starbucks on your \nright and turn left at the following intersection. \nContinue and stop when a mail truck is left.\"\nAction Sequence:\n1. forward\n2. forward\nThere is a 4-way intersection.\n3. right\n4. forward\nThere is a Starbucks on your right.\n5. <next word prediction> Observation\nLandmarks\nNumber of Edges\nVisible\nLandmarks\nAction\nleft slightly left ahead slightly right right\nThere is a N-way intersection\nFigure 3: Overview of the proposed agent VELMA navigating in the Street View environment. The prompt sequence includes\nthe task description, navigation instructions, and verbalized navigation trajectory up to the current timestep. The next action is\ndecided by next word prediction utilizing an LLM and subsequently executed in the environment. This puts the agent into a\nnew state, and the landmark scorer determines if an extracted landmark is visible in the current panorama view. The verbalizer\ntakes this landmark information along with the information about a potential intersection and produces the current observations\ntext. This text is then appended to the prompt sequence and again used to predict the next action. This process is repeated until\nthe agent stops and the alleged target location.\nLLM Agent\nIn this section, we propose the urban VLN agent that uses\nan LLM to reason about the next action. To this end, we ver-\nbalize the navigation task, especially the environment obser-\nvations. The workflow includes the extraction of landmarks\nthat are mentioned in the instructions and determining their\nvisibility in the current panorama image. The verbalizer then\nintegrates the visible landmarks and street intersections into\nan observation text phrase ot at each step. The complete text\nprompt at timestep t is composed as follows:\nxt = [da, n, db, o1, 1, a1, o2, 2, a2, ..., ot, t], (1)\nwhere [ ] denotes string concatenation, da and db are part\nof the task description and n is the navigation instructions\ntext. Punctuation and formatting are omitted in the notation\nfor brevity. Figure 3 shows a prompt sequence at t = 8 on\nthe left. This formulation of the navigation task enables the\nagent to predict the next action by next word prediction:\nat = arg max\nw∈A\nPLLM (w|xt), (2)\nwhere A are the literals of the five defined actions andPLLM\nis a black-box language model with no vision capabilities.\nLandmark Extractor\nEach navigation instructions text n mentions multiple land-\nmarks for visual guidance. In order to determine if a men-\ntioned landmark is visible in the current panorama view, we\nfirst have to extract them from the instructions text. For this,\nwe create a single prompt that includes five in-context ex-\namples of navigation instructions paired with a list of land-\nmarks (shown in the Appendix). It is used by the LLM to\nautomatically generate the list of landmarks (l1, l2, ..., lL)\nmentioned in the given navigation instructions. The land-\nmark extractor is depicted in the top middle of Figure 3 and\nexecuted before the navigation starts.\nLandmark Scorer\nAt each step, the agent observes a panorama view pα\nv , de-\nfined by its current position v and heading direction α. The\nview is an 800x460 sized image cut from the panorama with\n60◦ field-of-view. In order to determine if a landmark li is\nvisible in the view, we employ a CLIP model (Radford et al.\n2021) to embed the image and the caption: ”picture of [li]”.\nThe similarity score of the two embeddings determines the\nvisibility of the landmark. Because the scores can be biased\ntowards certain types of landmarks, we standardize them us-\ning all views p∗\ntrain of the ∼20k panorama images in the\ntraining area. Recall that we operate in the unseen scenario\nwhere the training area and evaluation area are geographi-\ncally separated. The standardized score of a landmark is:\nz(l, pα\nv ) =CLIP(l, pα\nv ) − µ(Cl)\nσ(Cl)\nwhere Cl = {CLIP(l, pα′\nv′ ) | pα′\nv′ ∈ p∗\ntrain}.\n(3)\nIf the standardized score is larger than the threshold τ, the\nlandmark is classified as visible in the current view. The\nprocess does not require annotations and is completely un-\nsupervised, allowing to score novel landmarks. The thresh-\nold is the only tunable parameter in the landmark scorer.\nFigure 4 shows the distribution of unstandardized CLIP\nscores and views at different threshold values for two ex-\nample landmarks. While the views at τ = 4.0 both show\nthe correct landmark, the view at τ = 3.0 for ”Bank of\nAmerica” shows an HSBC branch, and for ”yellow truck”\nit shows a white truck. This suggests that the optimal thresh-\nold lies between the two values. As depicted on the right\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18927\n2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0\nUnstandardized CLIP Score\nPanorama Views\nτ = 3.0\nτ = 4.0\nτ = 5.0\n4.93 5.29 5.65\nτ = 3.0\nτ = 4.0\nLandmark: Bank of America\n2 3 4 5\nUnstandardized CLIP Score\nPanorama Views\nτ = 3.0\nτ = 4.0\nτ = 5.0\n4.42 4.89 5.35\nτ = 3.0\nτ = 4.0\nLandmark: yellow truck\nFigure 4: Distribution of CLIP scores between a landmark and panorama images in the training area. The CLIP score repre-\nsents the semantic similarity of the panorama image and the text caption ”picture of [landmark]”. The distribution is used to\nstandardize the score of the landmark and a novel panorama. The threshold τ is defined on the standardized score and used to\ndetermine the visibility of the landmark in the novel panorama image.\nin Figure 3, the agent also evaluates views to the left and\nright of the current heading. Each panorama view direction\n(pα−90◦\nv , pα−45◦\nv , pα\nv , pα+45◦\nv , pα+90◦\nv ) is associated with a\nstring literal m valued left, slightly left, ahead, slightly right\nor right, respectively. A visible landmark li and the cor-\nresponding direction literal mi are passed to the verbal-\nizer. A full navigation trajectory includes around 200 im-\nage views (40 steps and 5 view directions per step) and each\nlandmark is typically visible in only one or two views.\nVerbalizer\nThe verbalizer is a template-based component that produces\nenvironment observations in text form. There are two types\nof environment observations. First, there are street intersec-\ntions that are detected based on the number of outgoing\nedges N(v) at the current node v in the navigation graph. If\nthere are three or more outgoing edges at step t, the verbal-\nizer encodes this information into the observation string oe\nt :\n”There is a [N(v)]-way intersection”. Extracting this infor-\nmation directly from the navigation graph is akin to the junc-\ntion type embedding used by the ORAR model (Schumann\nand Riezler 2022) and is motivated by direction arrows dis-\nplayed in the Street View GUI that human navigators used\nduring data collection. The other type of observations are\nlandmarks visible in the panorama view. The landmark name\nli and direction literal mi are used to verbalize the observa-\ntion ol\nt: ”There is [li] on your [mi]”. The complete observa-\ntion is ot = [oe\nt , ol\nt], where the respective string is empty if\nno intersection or landmark is detected. The observation is\nappended to the prompt in Equation 1 and used by the agent\nto decide the next action.\nExperiments\nWe conducted experiments 2 to evaluate the naviga-\ntion performance of the proposed LLM agent in\nfinetuning and in-context learning settings. We used\n2Project page: https://velma.schumann.pub/ and code: https://\ngithub.com/raphael-sch/VELMA\nCLIP-ViT-bigG-14-laion2B-39B-b160k (Schuh-\nmann et al. 2022) as the CLIP model in the landmark\nscorer. We set the threshold τ = 3.5 for all experiments.\nThe threshold was selected by inspecting the distribution of\nCLIP scores (as in Figure 4) for a handful of landmarks. On\npurpose, we did not systematically tune it in order to not\nviolate the premise of few-shot learning.\nLandmark Extraction\nWe ran the landmark extractor once for all instances using\nGPT-3 (Brown et al. 2020) and used the same extracted land-\nmarks in all experiments. On average, 2.7 landmarks were\nextracted from a navigation instructions text. Around 58%\nof the landmarks in the test sets are novel, i.e., they are not\nused in the training instances. In order to estimate the qual-\nity of the automatically extracted landmarks, we annotated\n50 instances of each development set by hand. For Touch-\ndown we calculated an F1-score of 96.3 (precision: 97.2, re-\ncall: 95.4) and the F1-score for Map2seq is 99.6 (precision:\n100, recall: 99.3). This shows that GPT-3 reliably extracts\nlandmarks from the instructions text and reusing them for\nall experiments is minimizing the inaccuracies introduced\nby this workflow step.\nMetrics and Baseline\nWe use three metrics to measure navigation performance.\nThe task completion (TC) rate is a binary metric that mea-\nsures whether the agent successfully stopped within one\nneighboring node of the target location. Shortest-path dis-\ntance (SPD) calculates the shortest path length between the\nstopping location and goal location (Chen et al. 2019). Key\npoint accuracy (KPA) measures the ratio of correct decisions\nat key points. Key points include the initial step, intersec-\ntions along the gold route, and the target location.\nFor baselines, we use the current state-of-the-art agent\nmodel for urban VLN called ORAR (Schumann and Riezler\n2022). The model employs a seq-to-seq architecture where\nthe encoder LSTM reads the navigation instructions text,\nand the multi-layer decoder LSTM receives image feature\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18928\nDevelopment Set Test Set\nTouchdown Map2seq Touchdown Map2seq\nModel SPD↓ KPA↑ TC↑ SPD↓ KPA↑ TC↑ SPD↓ KPA↑ TC↑ SPD↓ KPA↑ TC↑\nORAR-ResNet 20.0 - 15.4±2.2 11.9 - 27.6±1.8 20.8 - 14.9±1.2 13.0 - 30.3±1.8\nORAR♠-ResNet 16.5 64.0 22.6±0.6 10.3 74.4 29.9±1.7 17.4 62.3 19.1±1.0 10.9 74.7 32.5±1.4\nORAR♠-CLIP 17.5 63.7 21.5±0.9 10.0 75.3 32.8±1.5 17.0 63.4 20.0±0.1 10.5 75.1 34.0±0.5\n2-Shot In-Context Learning\nVELMA-Mixtral 28.4 47.2 6.5 21.1 56.8 8.0 - - - - - -\nVELMA-GPT-3 22.2 49.1 6.8 19.1 58.1 9.2 - - - - - -\nVELMA-GPT-4 21.8\n56.1 10.0 12.8 70.1 23.1 - - - - - -\nLLM Finetuning, full training set\nVELMA-FT 18.3 62.0 23.4±0.2 8.7 78.7 41.3±0.9 18.2 62.2 23.5±0.4 9.7 78.0 40.0±1.0\nVELMA-RBL 15.5 63.6 26.0±0.6 8.3 79.5 45.3±0.5 16.0 62.8 26.4±1.7 8.3 79.6 47.5±0.7\nTable 2: Results for the urban VLN task on Touchdown and Map2seq in the unseen scenario, meaning the training area is\ngeographically separated from the area where development and test routes are located. ORAR-ResNet (Schumann and Riezler\n2022) is the previous best model and follows a seq-to-seq architecture that fuses text and image features during decoding. We re-\ntrained this model in our improved environment (ORAR♠-ResNet) and with the same OpenCLIP image embeddings (ORAR♠-\nCLIP) that we use in the landmark scorer. VELMA-GPT-3 and VELMA-GPT-4 models employ our proposed verbalization\nworkflow and are prompted with two in-context examples. Due to cost and data leakage concerns, we evaluate the GPT models\non the development sets only. VELMA-FT is LLaMa-7b finetuned on all training text sequences (around 6k for each dataset).\nThe VELMA-RBL finetuning process is described in Section . All experiments are repeated three times with different random\nseeds (mean/std reported). Bold values are the nominal best results and underlined are best few-shot results.\nvectors of the current panorama view as additional input\nat each action decoding step. The ORAR model is a very\nstrong baseline beating more sophisticated models like the\nVLN Transformer (Zhu et al. 2021). Because the environ-\nment modifications introduced in Section spare the agents\nfrom learning specific irregularities, we additionally retrain\nORAR in the improved environment for a fair comparison.\nFew-Shot Learning Results\nThe proposed text-only interface allows us to use large lan-\nguage models as reasoners without updating their weights\nor fusing image representations. The prompt consists of\na short task description and two in-context examples (2-\nshot). The examples are full text sequences along the gold\nroute for randomly selected navigation instances in the train-\ning set. The two plots in Figure 5 show that performance\nscales with parameter count and varies across model fami-\nlies. The FORWARD-Only baseline reveals that OPT (Zhang\net al. 2022) can barely compete with a basic heuristic, even\nat a model size of 65 billion parameters. LLaMa (Touvron\net al. 2023a) and especially LLaMa-2 (Touvron et al. 2023b)\nshow promising navigation skills reaching 48.3 and 57.7 key\npoint accuracy (KPA) on Touchdown and Map2seq, respec-\ntively. However, this KPA score only translates to task com-\npletion (TC) rates of 2.1 and 3.2, revealing that the model\nis not able to consistently predict correct actions through-\nout the whole navigation trajectory. Mistral-7b performs on\npar with a LLaMA-2 model twice its size, but also fails\nto score task completion rates significantly higher than 3.\nThe only few-shot LLMs that achieve substantial TC rates\n1 3 7 13 30 65 ? ??\n30\n40\n50KPA\nTouchdown\n1 3 7 13 30 65 ? ??\nNumber of Model Parameters in Billion\n30\n40\n50\n60\n70KPA\nMap2seq\nFORWARD-Only\nOPT\nLLaMA-2\nLLaMA\nMixtral\nMistral\nGPT-4\nGPT-3\nFigure 5: Key point accuracy (KPA) for 2-shot in-context\nlearning of large language models with increasing parame-\nter count. The FORWARD-Only baseline predicts walking for-\nward until the average trajectory length is reached and per-\nforms better than predicting random directions.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18929\nTouchdown Map2seq\nModel SPD↓ TC↑ SPD↓ TC↑\nno image 27.4±0.5 14.7±0.5 9.7±0.2 35.2±0.9\nCLIP 21.3±0.5 19.5±0.6 9.8±0.3 37.2±0.5\nOpenCLIP 18.6±0.3 22.6±0.4 9.8±0.5 38.2±0.5\nTable 3: Vision ablation on the development set. We\nfinetune a separate LLaMa-7b model for each ablation.\nCLIP refers to clip-vit-large-patch14 (Radford\net al. 2021). The OpenCLIP image model refers to\nCLIP-ViT-bigG-14-laion2B-39B-b160k (Schuh-\nmann et al. 2022).\nare GPT-3, GPT-4 (OpenAI 2023) and Mixtral (Mistral AI\nTeam 2023). As listed in Table 2, VELMA-GPT-4 achieves\nthe best results for the 2-shot setting. It reaches 44% and\n77% of the TC rate reported for the previous state-of-the-\nart model ORAR♠-ResNet which is a seq-to-seq model that\nhas direct access to image features and was trained on the\nfull training set. In contrast, the LLMs in our work act as a\nblind agent that solely relies on observation descriptions pro-\nduced by the verbalizer. This is remarkable because LLMs\nare not explicitly trained to experience embodiment in a vi-\nsual environment. This is emergent behavior unearthed by\nverbalizing the VLN task. We also observe that GPT-4 in-\nvokes the TURN\nAROUND action in useful ways, e.g. to return\na few steps when it notices that it went past the described\ngoal location. This emphasizes the effectiveness of intuitive\ncommunication with the environment.\nFinetuning Results\nTo further explore the capabilities of the proposed LLM\nagent, we finetune LLaMa-7b on all training instances of the\nrespective dataset, denoted by VELMA-FT in Table 2. Each\ntraining instance is the full text sequence that is produced by\nfollowing the gold path. The visibility of landmarks is deter-\nmined by the landmark scorer during training because gold\nannotations are not available. There are 6,770 training in-\nstances for Touchdown and 5,737 for Map2seq. We finetune\nfor 20 epochs using LoRA (Hu et al. 2022) to adapt query,\nkey and value projections of the attention layer as well as in-\nput and output projection of each transformer layer. The best\nmodel is selected by task completion on the development set.\nThe resulting agent outperforms the previous state-of-the-art\nmodel ORAR* by 10% and 16% relative TC rate. Compar-\ning ORAR* which fuses image features at the vector level\nto VELMA-FT which finetunes on verbalizations of obser-\nvations, shows that the text-based environment observations\nare less prone to overfitting.\nResponse-Based Learning A navigation task is success-\nfully completed if the agent stops at either the goal location\nor an adjacent neighboring node. Training the agent with\nteacher-forcing to exactly follow the gold route penalizes\nthe agent for stopping one step short or one step past the\ntarget node, despite accomplishing the navigation objective.\nFurthermore, the agent can not learn to recover from incor-\nAlgorithm 1: RBL Optimization of Task Completion\nRequire: mixing ratio λ, training step j, model weights θj,\ngold action sequence ˆa, prompt x1\nif random(0, 1) < λthen\naθj = StudentF orcing(θj, x1)\naj = arg maxaθj\nif T askCompletion(aj) = 1then\nlossj = LCE (aθj , aj)\nelse\na∗\nj = Oraclestepwise(aj)\nlossj = LCE (aθj , a∗\nj )\nend if\nelse\naθj = T eacherF orcing(θj, x1, ˆa)\nlossj = LCE (aθj , ˆa)\nend if\nrect decisions during inference. We thus train the agent to\ndirectly optimize the TC metric while also feeding it its own\nactions during training, called VELMA-RBL in Table 2. The\nprocedure for VELMA-RBL is inspired by response-based\nlearning (Clarke et al. 2010) and imitation learning (Ross,\nGordon, and Bagnell 2011) and is outlined in Algorithm 1.\nThe loss for an instance at training step j is either computed\nby teacher forcing the gold action sequence ˆa, or by stu-\ndent forcing, determined by a mixing parameterλ. In student\nforcing, the actions decoded by the current model weights\nθj are executed instead of the gold actions. If this trajectory\nends within one neighboring node of the target location, the\npredicted action sequence aj is considered correct and used\nas the reference to train the agent. If the agent stops at the\nwrong location, an oracle path is computed to provide the\noptimal counterfactual action at each step in the trajectory.\nIn our case, the oracle’s optimal next action is computed as\nthe shortest path to the goal location. We set λ = 0.5 to\ncollect training losses in a batch evenly from both training\nstrategies. Manually inspecting trajectories produced by the\ntrained agent, we found improvements of following instruc-\ntions that have stopping criteria like ”Stop a few steps before\nY.” or ”Stop at X. If you see Y you have gone too far.”. In both\ncases, the agent learned to walk past the uncertain stopping\nlocation and to invoke the TURN\nAROUND action in order to\nwalk back once landmark Y appeared. The described train-\ning procedure leads to a significant increase of task com-\npletion rate by 2.9 and 7.5 for Touchdown and Map2seq,\nrespectively. Overall, our contributions in this work amount\nto a relative increase of task completion by 77% and 57%\nover the previously reported state-of-the-art for urban VLN\non the Touchdown and Map2seq datasets.\nImage Ablation\nIn this section, we ablate the image model used by the\nlandmark scorer. We finetune a LLaMa-7b model accord-\ning to Section and use CLIP (Radford et al. 2021), Open-\nCLIP (Schuhmann et al. 2022) or no image model in the\nlandmark scorer. The latter case means that no landmark\nobservation is passed to the prompt sequence. The results\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18930\nin Table 3 show that OpenCLIP is better suited for detect-\ning landmarks in our navigation task than the original CLIP\nmodel. This is in line with better ImageNet results reported\nby the OpenCLIP authors and suggests that the agent can\ndirectly benefit from further improvements of CLIP models.\nAppending no landmarks to the prompt sequence further de-\ngrades performance, especially on Touchdown.\nConclusion\nWe introduced VELMA, an agent for urban vision and lan-\nguage navigation, which utilizes a large language model to\ninfer its next action. The LLM is continuously queried with\na text prompt that verbalizes the task description, navigation\ninstructions, visual observations, and past trajectory of the\nagent. In order to include observed landmarks in the prompt,\nwe propose an unsupervised pipeline that extracts landmarks\nfrom the instructions and determines their visibility in the\ncurrent panorama view based on thresholded CLIP scores.\nWe evaluate the embodied LLM agent in a modified ver-\nsion of the commonly used Touchdown environment based\non Street View. One proposed modification is fixing a prob-\nlem at intersections that led to incorrect alignments of action\nsequences, and another modification adds the TURN\nAROUND\naction which provides a more intuitive way to communicate\nwith the environment. The proposed agent achieves strong\nfew-shot in-context learning results of 10 and 23 task com-\npletion rates for Touchdown and Map2seq, respectively, and\nyields new state-of-the-art results of 26 and 47 task comple-\ntion rates when finetuned on the full training set. The fine-\ntuning results show that verbalization is not an inherent lim-\nitation for this task and in-context learning with better base\nmodels or improved prompting techniques could outperform\nour reported few shot results.\nAcknowledgments\nThe research reported in this paper was supported by a\nGoogle Focused Research Award.\nReferences\nAnderson, P.; Wu, Q.; Teney, D.; Bruce, J.; Johnson, M.;\nS¨underhauf, N.; Reid, I.; Gould, S.; and Van Den Hen-\ngel, A. 2018. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environ-\nments. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 3674–3683.\nArmitage, J.; Impett, L.; and Sennrich, R. 2023. A Prior-\nity Map for Vision-and-Language Navigation with Trajec-\ntory Plans and Feature-Location Cues. InProceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, 1094–1103.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCurran Associates, Inc.\nChen, H.; Suhr, A.; Misra, D.; Snavely, N.; and Artzi, Y .\n2019. TOUCHDOWN: Natural Language Navigation and\nSpatial Reasoning in Visual Street Environments. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR). Long Beach, California.\nChen, S.; Guhur, P.-L.; Schmid, C.; and Laptev, I. 2021. His-\ntory aware multimodal transformer for vision-and-language\nnavigation. Advances in neural information processing sys-\ntems, 34: 5834–5847.\nClarke, J.; Goldwasser, D.; Chang, M.-W.; and Roth, D.\n2010. Driving Semantic Parsing from the World’s Response.\nIn Proceedings of the Fourteenth Conference on Computa-\ntional Natural Language Learning, 18–27. Uppsala, Swe-\nden: Association for Computational Linguistics.\nDorbala, V . S.; Sigurdsson, G.; Piramuthu, R.; Thomason,\nJ.; and Sukhatme, G. S. 2022. Clip-nav: Using clip for zero-\nshot vision-and-language navigation. In CoRL 2022 Work-\nshop on Language and Robot Learning.\nFried, D.; Hu, R.; Cirik, V .; Rohrbach, A.; Andreas, J.;\nMorency, L.-P.; Berg-Kirkpatrick, T.; Saenko, K.; Klein, D.;\nand Darrell, T. 2018. Speaker-Follower Models for Vision-\nand-Language Navigation. In Neural Information Process-\ning Systems (NeurIPS).\nFu, T.-J.; Wang, X. E.; Peterson, M. F.; Grafton, S. T.; Eck-\nstein, M. P.; and Wang, W. Y . 2020. Counterfactual vision-\nand-language navigation via adversarial path sampler. In\nComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part VI 16,\n71–86. Springer.\nHong, Y .; Wu, Q.; Qi, Y .; Rodriguez-Opazo, C.; and Gould,\nS. 2021. Vln bert: A recurrent vision-and-language bert for\nnavigation. In Proceedings of the IEEE/CVF conference on\nComputer Vision and Pattern Recognition, 1643–1653.\nHu, E. J.; yelong shen; Wallis, P.; Allen-Zhu, Z.; Li, Y .;\nWang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank\nAdaptation of Large Language Models. In International\nConference on Learning Representations.\nKhandelwal, A.; Weihs, L.; Mottaghi, R.; and Kembhavi,\nA. 2022. Simple but Effective: CLIP Embeddings for Em-\nbodied AI. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 14829–\n14838.\nKu, A.; Anderson, P.; Patel, R.; Ie, E.; and Baldridge,\nJ. 2020. Room-Across-Room: Multilingual Vision-and-\nLanguage Navigation with Dense Spatiotemporal Ground-\ning. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 4392–\n4412.\nLi, J.; Tan, H.; and Bansal, M. 2022. Envedit: Environment\nediting for vision-and-language navigation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 15407–15417.\nMehta, H.; Artzi, Y .; Baldridge, J.; Ie, E.; and Mirowski, P.\n2020. Retouchdown: Releasing Touchdown on StreetLearn\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18931\nas a Public Resource for Language Grounding Tasks in\nStreet View. InProceedings of the Third International Work-\nshop on Spatial Language Understanding (SpLU). Online.\nMistral AI Team. 2023. Mixtral of Experts: A High Qual-\nity Sparse Mixture-of-Experts. Mistral AI Blog. Accessed:\nDecember 18, 2023.\nOpenAI. 2023. GPT-4 Technical Report. ArXiv,\nabs/2303.08774.\nQi, Y .; Wu, Q.; Anderson, P.; Wang, X.; Wang, W. Y .; Shen,\nC.; and Hengel, A. v. d. 2020. Reverie: Remote embodied\nvisual referring expression in real indoor environments. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 9982–9991.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision. In\nICML.\nRamakrishnan, S. K.; Gokaslan, A.; Wijmans, E.;\nMaksymets, O.; Clegg, A.; Turner, J. M.; Undersander, E.;\nGaluba, W.; Westbury, A.; Chang, A. X.; Savva, M.; Zhao,\nY .; and Batra, D. 2021. Habitat-Matterport 3D Dataset\n(HM3D): 1000 Large-scale 3D Environments for Embodied\nAI. In Thirty-fifth Conference on Neural Information Pro-\ncessing Systems Datasets and Benchmarks Track (Round\n2).\nRoss, S.; Gordon, G. J.; and Bagnell, J. A. 2011. A Reduc-\ntion of Imitation Learning and Structured Prediction to No-\nRegret Online Learning. In Proceedings of the 14th Inter-\nnational Conference on Artificial Intelligence and Statistics\n(AISTATS). Fort Lauderdale, FL, USA.\nSchuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C. W.;\nWightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis,\nC.; Wortsman, M.; Schramowski, P.; Kundurthy, S. R.;\nCrowson, K.; Schmidt, L.; Kaczmarczyk, R.; and Jitsev, J.\n2022. LAION-5B: An open large-scale dataset for train-\ning next generation image-text models. In Thirty-sixth Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track.\nSchumann, R.; and Riezler, S. 2021. Generating Landmark\nNavigation Instructions from Maps as a Graph-to-Text Prob-\nlem. In Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), 489–502. Online: Association for\nComputational Linguistics.\nSchumann, R.; and Riezler, S. 2022. Analyzing Generaliza-\ntion of Vision and Language Navigation to Unseen Outdoor\nAreas. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 7519–7532. Dublin, Ireland: Association for Com-\nputational Linguistics.\nShah, D.; Osinski, B.; Ichter, B.; and Levine, S. 2022. LM-\nNav: Robotic Navigation with Large Pre-Trained Models of\nLanguage, Vision, and Action. arXiv:2207.04429.\nShridhar, M.; Thomason, J.; Gordon, D.; Bisk, Y .; Han, W.;\nMottaghi, R.; Zettlemoyer, L.; and Fox, D. 2020. ALFRED:\nA Benchmark for Interpreting Grounded Instructions for Ev-\neryday Tasks. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nSun, Y .; Qiu, Y .; Aoki, Y .; and Kataoka, H. 2023. Out-\ndoor Vision-and-Language Navigation Needs Object-Level\nAlignment. Sensors, 23(13).\nTan, H.; Yu, L.; and Bansal, M. 2019. Learning to Navigate\nUnseen Environments: Back Translation with Environmen-\ntal Dropout. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), 2610–2621. Minneapolis, Min-\nnesota: Association for Computational Linguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models. arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nWang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.;\nZhu, Y .; Fan, L.; and Anandkumar, A. 2023. V oyager: An\nOpen-Ended Embodied Agent with Large Language Mod-\nels. arXiv preprint arXiv: Arxiv-2305.16291.\nWang, X.; Huang, Q.; Celikyilmaz, A.; Gao, J.; Shen, D.;\nWang, Y .-F.; Wang, W. Y .; and Zhang, L. 2019. Rein-\nforced Cross-Modal Matching and Self-Supervised Imita-\ntion Learning for Vision-Language Navigation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR).\nXiang, J.; Wang, X.; and Wang, W. Y . 2020. Learning to\nStop: A Simple yet Effective Approach to Urban Vision-\nLanguage Navigation. In Findings of the Association for\nComputational Linguistics (ACL Findings). Online.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhong, V .; Hanjie, A. W.; Wang, S.; Narasimhan, K.; and\nZettlemoyer, L. 2021. SILG: The Multi-domain Symbolic\nInteractive Language Grounding Benchmark. In Ranzato,\nM.; Beygelzimer, A.; Dauphin, Y .; Liang, P.; and Vaughan,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18932\nJ. W., eds.,Advances in Neural Information Processing Sys-\ntems, volume 34, 21505–21519. Curran Associates, Inc.\nZhou, G.; Hong, Y .; and Wu, Q. 2023. NavGPT: Explicit\nReasoning in Vision-and-Language Navigation with Large\nLanguage Models. arXiv:2305.16986.\nZhou, K.; Zheng, K.; Pryor, C.; Shen, Y .; Jin, H.; Getoor,\nL.; and Wang, X. E. 2023. ESC: Exploration with Soft\nCommonsense Constraints for Zero-shot Object Navigation.\narXiv preprint arXiv:2301.13166.\nZhu, W.; Hu, H.; Chen, J.; Deng, Z.; Jain, V .; Ie, E.; and Sha,\nF. 2020. BabyWalk: Going Farther in Vision-and-Language\nNavigation by Taking Baby Steps. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 2539–2556.\nZhu, W.; Wang, X.; Fu, T.-J.; Yan, A.; Narayana, P.; Sone,\nK.; Basu, S.; and Wang, W. Y . 2021. Multimodal Text Style\nTransfer for Outdoor Vision-and-Language Navigation. In\nProceedings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics: Main\nVolume, 1207–1221. Online: Association for Computational\nLinguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18933",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4345128536224365
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4271449148654938
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36879923939704895
    },
    {
      "name": "Computer vision",
      "score": 0.35838884115219116
    },
    {
      "name": "Psychology",
      "score": 0.35117200016975403
    }
  ]
}