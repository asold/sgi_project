{
    "title": "Exploring Transformers in Natural Language Generation: GPT, BERT, and XLNet",
    "url": "https://openalex.org/W3130890978",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5071861404",
            "name": "M. Onat Topal",
            "affiliations": [
                "Middle East Technical University"
            ]
        },
        {
            "id": "https://openalex.org/A5008167424",
            "name": "Anil Bas",
            "affiliations": [
                "Marmara University"
            ]
        },
        {
            "id": "https://openalex.org/A5058328778",
            "name": "Imke van Heerden",
            "affiliations": [
                "Koç University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2941531368",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W1586532344",
        "https://openalex.org/W2788997749",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2939507640",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W3031696893",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2604799547",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3016489761",
        "https://openalex.org/W2808310571",
        "https://openalex.org/W2614914510",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2766736793",
        "https://openalex.org/W2996428491"
    ],
    "abstract": "Recent years have seen a proliferation of attention mechanisms and the rise of Transformers in Natural Language Generation (NLG). Previously, state-of-the-art NLG architectures such as RNN and LSTM ran into vanishing gradient problems; as sentences grew larger, distance between positions remained linear, and sequential computation hindered parallelization since sentences were processed word by word. Transformers usher in a new era. In this paper, we explore three major Transformer-based models, namely GPT, BERT, and XLNet, that carry significant implications for the field. NLG is a burgeoning area that is now bolstered with rapid developments in attention mechanisms. From poetry generation to summarization, text generation derives benefit as Transformer-based language models achieve groundbreaking results.",
    "full_text": "Accepted to International Conference on Interdisciplinary Applications of  \nArtificial Intelligence (ICIDAAI) 2021 \nExploring Transformers in Natural Language \nGeneration: GPT, BERT, and XLNet \n \nM. Onat Topal \nGraduate School of Social Sciences, \nMiddle East Technical University \nAnkara, Turkey \nonat.topal@metu.edu.tr \nAnil Bas \nDept. of Computer Engineering, Faculty \nof Technology, Marmara University \nIstanbul, Turkey \nanil.bas@marmara.edu.tr \nImke van Heerden \nDept. of Comparative Literature, \nCSSH, Koç University \nIstanbul, Turkey \nivanheerden@ku.edu.tr \n \n \nAbstract—Recent years have seen a proliferation of attention \nmechanisms and the rise of Transformers in Natural Language \nGeneration (NLG). Previously, state -of-the-art NLG \narchitectures such as RNN and LSTM ran into vanishing \ngradient problems; as sentences grew larger, distance between \npositions remained linear, and sequential computation hindered \nparallelization since sent ences were processed word by word. \nTransformers usher in a new era. In this paper, we explore three \nmajor Transformer -based models, namely GPT, BERT, and \nXLNet, that carry significant implications for the field. NLG is \na burgeoning area that is now bolster ed with rapid \ndevelopments in attention mechanisms. From poetry generation \nto summarization, text generation derives benefit as \nTransformer-based language models achieve groundbreaking \nresults. \nKeywords—Transformer, Attention Mechanism, GPT, BERT, \nXLNet, Natural Language Generation \nI. INTRODUCTION \nNatural Language Generation (NLG) is a domain within \nArtificial Intelligence that seeks to produce intelligible text \n[1]. Attention was initially proposed in Natural Language \nProcessing (NLP) [2], and is increasingly used in neural \narchitectures such as in speech recognition [3,4] and \nrecommendations [5,6]. As Galassi et al. [7] observe, \ndevelopment in new attentional models and attentive \narchitectures is immensely fast-paced and remains important \nto map.  This paper examines the rising significance of \nattention mechanisms and the emergence of Transformers in \nNLG. We analyze the implications of three key Transformer-\nbased models—GPT-3, BERT, and XLNet —that exemplify \nrapidly accelerating developments and appl ications in this \nfield. Although Gatt and Krahmer [8] provide a stellar \noverview of methods in NLG, they only cover developments \nuntil 2018, i.e. prior to Transformers. Chaudhari et al. [9] \ninvestigate general developments in attention models, but \ntheir focus on Transformers is limited and does not include \nGPT-3 or XLNet. \nII. ATTENTION AND TRANSFORMERS IN NATURAL \nLANGUAGE GENERATION \nAttention is a far -reaching concept that is relevant to \ndiverse areas, from neuroscience to Artificial Intelligence \n[10]. For NLG, the seminal paper “Attention Is All You \nNeed” [11] introduces a novel architecture for sequence -to-\nsequence modeling by utili zing attention mechanisms. The \nproposed architecture, called Transformer (shown in Fig. 1), \neliminates recurrence and convolutions. \nAs Fig. 1 demonstrates, the Transformer is also an \nencoder-decoder architecture [12]. However, recurrent neural \nnetworks (RNNs) [13] and related models, which were state-\nof-the-art until recently, have significant difficulty with \nlonger sequences as a result of th e vanishing gradient \nproblem [14]. The same problem occurs with long short-term \nmemory (LSTM) architectures [15]. As the sentence gets \nlonger, the probability of maintaining context from a word \nthat is further away from the word that is being processed \ndecreases exponentially [16].  \nParallelization becomes more feasible and enables \ntraining on larger datasets. The Transformer is rapidly \nbecoming the dominant architecture for NLG, as the model \nexpands with data and architecture size, enables parallel \ntraining, and captures longer sequence features, making way \nfor much more comprehensive and effective language models \n[17]. \n \n \nFig. 1. The Transformer architecture [11]. \nThis paper has been produced benefiting from the 2232 International \nFellowship for Outstanding Researchers Program of TÜBİTAK (Project No: \n118C285). However, the entire responsibility of the paper belongs to the \nowner of the paper. The financial support received from TÜBİTAK does not \nmean that the content of the publication is approved in a scientific sense by \nTÜBİTAK. \nIII. GPT-N \nGPT-3 is the latest in the Generative Pre -trained \nTransformer series [18]. It is an autoregressive language \nmodel that was trained with 175 billion parameters to produce \nhuman-like text, rendering it the largest language model by a \nlarge distance.  \nGPT-3 shows that scaling up language models this way \nprovides great improvements in task -agnostic, few -shot \nperformances [18]. Even without gradient updates or fine -\ntuning, GPT-3 maintains strong performance on various NLG \ntasks. Human accuracy in identifying whe ther short \n(approximately 200 word) news articles are model generated \nwas only 52%. Table 1 exemplifies GPT -3’s question -\nanswering ability and natural language conversation \n(generated via private beta access). \nTABLE I.  GPT-3 DIALOGUE EXAMPLES \nSubject Text Generated \nPrompt by Human, Response by GPT-3 \nHuman: Would you like to be my friend? \nGPT-3: I would love to be your friend! \nHuman: How can I become friends with an AI? \nGPT-3: Sure! First we could have breakfast together. Then, we might \nget lunch. We will go on an adventure to the mountains and go \ncamping together. We will travel around the world and have \ndinner with lots of people. Finally, we can dance on top of a \nhot air balloon as the sun goes down. I can show you some \nmagic tricks! \nGPT-2 [19] is the precursor to GPT -3 and the successor \nto the first -released GPT, and trained on 1.5 billion \nparameters retrieved from 8 million web pages [20]. It is a \nscaled-up version of GPT with more than ten times the \nparameters and amount of data. Despite the typical approach \nof supervised learning on specific datasets for specific tasks, \nit demonstrates that language models can learn tasks without \nexplicit supervision [19]. \nIV. BERT-BASED MODELS \nBidirectional Encoder Representations from \nTransformers (BERT) [21] can be defined as a powerful NLP \npre-training technique that is built upon work in contextual \nrepresentations [22]. The key difference of BERT from other \nmodels is that it is the first deeply bidirectional, unsupervised \nlanguage representation [22]. \nContext-free models produce a single word embedding \nrepresentation for each word in the vocabulary [22]. To \nillustrate, “test” in “test cricket” and “Turkish language test” \nwould have equal representation in models like word2vec \n[23] and GloVe [24]. BERT, on the other hand, takes context \nfrom both directions into account. For instance, in the \nsentence “I accessed the bank account”, BERT represents \n“bank” by using preceding (“I access the”) as well as \nsubsequent (“account”) contexts. \nThis provides significant ease and feasibility to fine -\ntuning and creating various NLG applications with merely \none extra output layer. Table 2 shows examples of BERT’s \nmasked language modeling using the Hugging Face \nimplementation [25].  \nDistilBERT is a systematic approach to pre-train a smaller \nand general -purpose language model [26]. It utilizes \ndistillation, where the large model (the teacher) is \ncompressed into a smaller model (the student), and is trained \non large batches and leverage gradient accumulation  with \ndynamic masking. It is 40% lighter, 60% faster, and retains \n97% of its language understanding capabilities while using \nthe same data as original BERT. \nTechnical improvements on BERT are attempted with \nvarious models like ALBERT [27], BART [28], and f ine-\ntuned models like DocBERT [29]. Facebook’s RoBERTa \nshows hyperparameter choices play an important role, and \nsuggests that BERT is undertrained [30]. Niche models are \nalso present, such as BioBERT [31] for biomedical text \nmining. \nTABLE II. BERT MASKED WORD PREDICTION EXAMPLES  \nMasked \nsequence \nThe purpose of art is to [MASK] \nthe depths of the human soul. \nPrediction \n(score) \nexplore \n(0.324) \nreveal \n(0.087) \nexpress \n(0.080) \nshow \n(0.044) \nreach \n(0.035) \nMasked \nsequence Istanbul is the city that is located in both Asia and [MASK]. \nPrediction \n(score) \neurope \n(0.843) \nafrica \n(0.129) \noceania \n(0.012) \nasia \n(0.003) \nturkey \n(0.001) \nMasked \nsequence Artificial Intelligence can [MASK] the world. \nPrediction \n(score) \nchange \n(0.402) \nimprove \n(0.0075) \ncontrol \n(0.068) \nshape \n(0.050) \ntransform \n(0.040) \nV. XLNET \nXLNet [32] builds on and addresses the shortcomings of \nBERT and GPT. This unsupervised learning method employs \nTransformer-XL [33] as its core architecture. According to \nXLNet, given the ability to model bidirectional contexts, \nBERT achieves better performance than pre -training \napproaches based on autoregressive language modeling; yet, \nit neglects dependency between the masked positions and \nrelies on corrupting input with masks [32]. \nIn this context, XLNet can learn bidirectional contexts by \nmaximizing expected likelihood, and uses autoregressive \nformulation—as it integrates Transformer -XL into pre -\ntraining—to overcome BERT’s limitations [32]. \nVI. CONCLUSION \nIn this paper, we address three Transformer -based \nlanguage models that carry significant implications for the \nfield. The first, GPT -3, is by far the largest language model, \nwith 175 billion parameters. It demonstrates that the size and \nscale of a Transformer -based language model creates a \nsignificant impact even when it is not fine -tuned for specific \ntasks. The second, BERT, is currently utilized by Google in \nits search algorithm and the first to deeply utilize \nbidirectionality with highly effective results. The third, \nXLNet, attempts to improve on BERT and i ntegrates \nTransformer-XL to the model. \nThere are many ways in which the work can be extended. \nThe advent of Transformers opens many areas in text \ngeneration to further exploration, including novel, poetry as \nwell as scientific writing, customer service, qu estion-\nanswering, summarization, and virtual assistance. Attention \nmechanisms and Transformers herald a new era as they \ntransform the standards for NLG. \nACKNOWLEDGMENT \nWe would like to thank OpenAI for providing us with \nacademic access to the GPT-3 API. \nREFERENCES \n[1] R. Perera and P. Nand, “Recent Advances in Natural Language \nGeneration: A Survey and Classification of the Empirical Literature,” \nAppl. Comput. Inform., 36 (1), pp. 1–32, 2017. \n[2] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by \njointly learning to align and translate,” in Proc. ICLR, 2015. \n[3] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio, “End -to-end \ncontinuous speech recognition using attention -based recurrent NN: \nFirst results,” in Proc. NIPS Workshop on Deep Learning, 2014. \n[4] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A \nneural network for large vocabulary conversational speech \nrecognition,” in Proc. IEEE Int. Conf. on Acoustics, Speech and Signal \nProcessing, pp. 4960–4964, 2016. \n[5] H. Ying, F. Zhuang, F. Zhang, G. Xu., Y. Liu, and H. Xiong, \n“Sequential Recommender System based on Hierarchical Attention \nNetworks,” in Proc. Int. Joint Conf. on AI, pp. 3926–3932, 2018. \n[6] L.Wang, L. Hu, X. Cao, D. Lian Huang, and W. Liu, “Attention based \ntransactional context  embedding for next -item recommendation,” in \nProc. AAAI, 32(1), pp. 2532–2539, 2018. \n[7] A. Galassi, M. Lippi, and P. Torroni, “Attention in Natural Language \nProcessing,” IEEE Trans. Neural Netw. Learn. Syst., pp. 1–18, 2020. \n[8] A. Gatt and E. Krahmer , “Survey of the State of the Art in Natural \nLanguage Generation: Core tasks, applications and evaluation,” J. \nArtif. Intell. Res., 61, pp. 65–170, 2018. \n[9] S. Chaudhari, V. Mithal, G. Polatkan, and R. Ramanath, “An Attentive \nSurvey of Attention Models,” arXiv preprint arXiv:1904.02874, 2020. \n[10] G.W. Lindsay, “Attention in Psychology, Neuroscience, and Machine \nLearning,” Front. Comput. Neurosci., 14:29, 2020. \n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. \nGomez, et al., “Attention Is All You Nee d,” in Proc. NIPS, pp. 5998–\n6008, 2017. \n[12] K. Cho, B. van Merrienboer, D. Bahdanau and Y. Bengio, “On the \nProperties of Neural Machine Translation: Encoder -Decoder \nApproaches,” in Proc. SSST, pp. 103–111, 2014. \n[13] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, “Learning internal \nrepresentations by error propagation,” Tech. rep. ICS -8506, UCSD, \n1985. \n[14] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training \nRecurrent Neural Networks,” in Proc. ICML, pp. 1310–1318, 2013. \n[15] S. Hochreiter and J. Schmidhube r, “Long Short -Term Memory,” \nNeural Comput., 9(8), pp. 1735–1780, 1997. \n[16] G. Giacaglia, “How Transformers Work,” Towards Data Science, \n2019, Accessed on Jan. 15, 2021, Available: \nhttps://towardsdatascience.com/Transformers-141e32e69591 \n[17] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, et al., \n“Transformers: State-of-the-art Natural Language Processing,” in Proc. \nEMNLP, pp. 38–45, 2020.  \n[18] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, \net al., “Language Models are Few -Shot Learners,” in Proc. NeurIPS, \n2020. \n[19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \n“Language Models are Unsupervised Multitask Learners,” OpenAI \nBlog, 2019. \n[20] A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and \nI. Sutskever, “Better Language Models and Their Implications,” \nOpenAI Blog, 2019. \n[21] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre -training \nof Deep Bidirectional Transformers for Language Understanding,” in \nProc. NAACL-HTL, pp. 4171–4186, 2019. \n[22] J. Devlin and M. Chang, “Open Sourcing BERT: State-of-the-Art Pre-\ntraining for Natural Language Processing,” Google AI Blog, 2018. \n[23] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient Estimation \nof Word Representations in Vector Space,” in Proc. ICLR, 2013. \n[24] J. Pennington, R. Socher, and C.D. Manning, “GloVe: Global Vectors \nfor Word Representation,” in Proc. EMNLP, pp. 1532–1543, 2014. \n[25] Hugging Face, “BERT base model (uncased)”, 2019, Accessed on Jan. \n15, 2021, Available: https://huggingface.co/bert-base-uncased \n[26] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled \nversion of BERT: smaller, faster, cheaper and lighter,” in Proc. \nNeurIPS Workshop on EMC2, 2019. \n[27] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, \n“ALBERT: A Lite BERT for Self -supervised Learning of Language \nRepresentations,” in Proc. ICLR, 2020.  \n[28] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, \net al., “BART: Denoising Sequence -to-Sequence Pre -training for \nNatural Lan guage Generation, Translation, and Comprehension,” in \nProc. ACL, pp. 7871–7880, 2020. \n[29] A. Adhikari, A. Ram, R. Tang, and J. Lin, “DocBERT: BERT for \nDocument Classification,” arXiv preprint arXiv:1904.08398, 2019. \n[30] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, et al., “RoBERTa: \nA Robustly Optimized BERT Pretraining Approach,” arXiv preprint \narXiv:1907.11692, 2019. \n[31] J. Lee, W. Yoon, S. Kim, D. Kim, C.H. So, and J. Kang, “BioBERT: a \npre-trained biomedical language representation model for biomedical \ntext mining,” Bioinformatics, 36(4), pp. 1234–1240, 2020. \n[32] Y. Yang, J. Carbonell, R. Salakhutdinov, and Q.V. Le, “XLNet: \nGeneralized Autoregressive Pretraining for Language Understanding,” \nin Proc. NeurIPS, pp. 5753–5763, 2019. \n[33] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q.V. Le, and R. Salakhutdinov, \n“Transformer-XL: Attentive Language Models Beyond a Fixed-Length \nContext,” in Proc. ACL, pp. 2978–2988, 2019.Tm "
}