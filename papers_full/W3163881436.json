{
  "title": "A Spam Transformer Model for SMS Spam Detection",
  "url": "https://openalex.org/W3163881436",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096898954",
      "name": "Xiaoxu Liu",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2784266522",
      "name": "Haoye Lu",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2139535962",
      "name": "Amiya Nayak",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2096898954",
      "name": "Xiaoxu Liu",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2784266522",
      "name": "Haoye Lu",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2139535962",
      "name": "Amiya Nayak",
      "affiliations": [
        "University of Ottawa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2139501017",
    "https://openalex.org/W2750499125",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6640036494",
    "https://openalex.org/W3142036593",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W1930624869",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2900460291",
    "https://openalex.org/W2036166268",
    "https://openalex.org/W2783922631",
    "https://openalex.org/W1670263352",
    "https://openalex.org/W1978515644",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W114517082",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2801332331",
    "https://openalex.org/W6695662000",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3011823135",
    "https://openalex.org/W2576815293",
    "https://openalex.org/W2797652130",
    "https://openalex.org/W2765169965",
    "https://openalex.org/W2971852873",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3085571720",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W6753505025",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6638545294",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1967320885",
    "https://openalex.org/W2020214980",
    "https://openalex.org/W2119821739",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2883265831",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1881647329",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2087347434",
    "https://openalex.org/W1988790447",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2080562691",
    "https://openalex.org/W2501821845",
    "https://openalex.org/W2284289336",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3094253649",
    "https://openalex.org/W3155649056",
    "https://openalex.org/W2912934387",
    "https://openalex.org/W2959400106",
    "https://openalex.org/W2138660131",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2133990480",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2899675781",
    "https://openalex.org/W2056132907",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "With the prosperity of the Short Message Service (SMS), the increasing number of spam messages has become a serious problem. The need to block spam messages requires us to develop new SMS spam detection technologies. The Transformer, an attention- based sequence to sequence model, has achieved excellent results in multiple different tasks recently. In this thesis, we propose a modified Transformer model for SMS spam messages detection. The evaluation of our proposed modified spam Transformer is performed on SMS Spam Collection v.1 dataset and UtkMl’s Twitter Spam Detection Competition dataset, with the benchmark of multiple established classifiers such as Logistic Regression, Na ̈ıve Bayes, Random Forests, Support Vector Machine, and Long Short-Term Memory. In comparison to all other candidates, our experiments show that the proposed modified spam Transformer achieves the best results in terms of almost all selected performance criteria.",
  "full_text": "Received April 28, 2021, accepted May 11, 2021, date of publication May 17, 2021, date of current version June 8, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3081479\nA Spam Transformer Model for\nSMS Spam Detection\nXIAOXU LIU\n , HAOYE LU\n , (Member, IEEE), AND AMIYA NAYAK\n, (Senior Member, IEEE)\nSchool of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON K1N 6N5, Canada\nCorresponding author: Haoye Lu (hlu044@uottawa.ca)\nABSTRACT In this paper, we aim to explore the possibility of the Transformer model in detecting the spam\nShort Message Service (SMS) messages by proposing a modiﬁed Transformer model that is designed for\ndetecting SMS spam messages. The evaluation of our proposed spam Transformer is performed on SMS\nSpam Collection v.1 dataset and UtkMl’s Twitter Spam Detection Competition dataset, with the benchmark\nof multiple established machine learning classiﬁers and state-of-the-art SMS spam detection approaches.\nIn comparison to all other candidates, our experiments on SMS spam detection show that the proposed\nmodiﬁed spam Transformer has the optimal results on the accuracy, recall, and F1-Score with the values\nof 98.92%, 0.9451, and 0.9613, respectively. Besides, the proposed model also achieves good performance\non the UtkMl’s Twitter dataset, which indicates a promising possibility of adapting the model to other similar\nproblems.\nINDEX TERMS SMS spam detection, transformer, attention, deep learning.\nI. INTRODUCTION\nA. MOTIVATION AND OBJECTIVE\nTHE Short Message Service (SMS) has been widely used\nas a communication tool over the past few decades as the\npopularity of mobile phone and mobile network grows. How-\never, SMS users are also suffering from SMS spam. The SMS\nspam, also known as drunk message, refers to any irrelevant\nmessages delivered using mobile networks [1]. There are\nseveral reasons that lead to the popularity of spam messages.\nFirstly, there is a large number of users who use mobile\nphones in the world, making the potential victims of the spam\nmessages attack also high. Secondly, the cost of sending out\nspam messages is low, which could be good news to the\nspam attacker. Last but not least, the capability of the spam\nclassiﬁer on most mobile phones is relatively weak due to the\nshortage of computational resources, which limits them from\nidentifying the spam message correctly and efﬁciently.\nMachine learning is one of the most popular topics in\nthe last few decades, and there are a great number of\nmachine learning based classiﬁcation applications in multiple\nresearch areas. Speciﬁcally, spam detection is a relatively\nmature research topic with several established methods. How-\never, most of the machine learning based classiﬁers were\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Wei Xiang\n .\ndependent on the handcrafted features extracted from the\ntraining data [2].\nAs a class of machine learning techniques, deep learning\nhas been developing rapidly recently thanks to the surprising\ngrowth of computational resources in the last few decades.\nNowadays, deep learning based applications play a signiﬁ-\ncant part in our society, making our lives much easier in many\naspects. As one of the most effective and widely used deep\nlearning architectures, Recurrent Neural Network (RNN),\nas well as its variants such as Long Short-Term Memory\n(LSTM), were applied to spam detection and proved to be\nextremely effective during the last few years.\nThe Transformer [3] is an attention-based sequence-to-\nsequence model that was originally designated for transla-\ntion task, and it achieved great success in English-German\nand English-French translation. Moreover, there are multiple\nimproved Transformer-based models such as GPT-3 [4] and\nBERT [5] proposed recently to address different Natural Lan-\nguage Process (NLP) problems. The accomplishments of the\nTransformer and its successors have proved how powerful\nand promising they are. In this paper, we aim to explore\nwhether it is possible to adapt the Transformer model to\nthe SMS spam detection problem. Therefore, we propose a\nmodiﬁed model based on the vanilla Transformer to identify\nSMS spam messages. Additionally, we analyze and compare\nthe performance of SMS spam detection between traditional\nVOLUME 9, 2021\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 80253\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nmachine learning classiﬁers, an LSTM deep learning solu-\ntion, and our proposed spam Transformer model.\nB. RELATED WORK\nThere are several different machine learning based classiﬁ-\ncation applications proposed in the last few decades [6], [7]\n[8], [9]. In the ﬁeld of SMS spam detection, a great number\nof these approaches are based on traditional machine learning\ntechniques, such as Logistic Regression (LR), Random Forest\n(RF) [10], Support Vector Machine (SVM) [11], Naïve Bayes\n(NB), and Decision Trees (DT). Recently, with the prosperity\nof the deep learning techniques, an increasing number of\nmethods have been introduced to address the SMS spam\nproblem using deep learning based solutions such as Convo-\nlutional Neural Network (CNN), Recurrent Neural Network\n(RNN), and Long Short-Term Memory (LSTM), which is a\nsuccessful variant of RNN.\nIn [12], Gupta et al.compared the performance of 8 differ-\nent classiﬁers including SVM, NB, DT, LR, RF, AdaBoost,\nNeural Network, and CNN. The experimental tests on the\nSMS Spam Collection v.1 [13] dataset that was conducted\nby the authors shows that the CNN and Neural Network are\nbetter compared to other machine learning classiﬁers, and the\nCNN and Neural Network achieved an accuracy of 98.25%\nand 98.00%, respectively.\nIn [14], Jain et al.proposed a method to apply rule-based\nmodels on the SMS spam detection problem. The authors\nextracted 9 rules and implemented Decision Tree (DT), RIP-\nPER [15], and PRISM [16] to identify the spam messages.\nAccording to the experimental results from the authors,\nthe RIPPER outperformed the PRISM and the DT, yielding\na 99.01% True Negative Rate (TNR) and a 92.82% True\nPositive Rate (TPR).\nIn [1], Roy et al. aimed to adapt the CNN and LSTM\nto the SMS spam messages detection problem. The authors\nevaluated the performance of CNN and LSTM by comparing\nthem with Naïve Bayes (NB), Random Forest (RF), Gra-\ndient Boosting (GB) [17], Logistic Regression (LR), and\nStochastic Gradient Descent (SGD) [18]. The experiments\nthat were conducted by the authors showed that the CNN and\nLSTM perform signiﬁcantly better than the tested traditional\nmachine learning approaches when it comes to SMS spam\ndetection.\nIn [2], the authors proposed the Semantic Long Short-Term\nMemory (SLSTM), a variant of LSTM with an additional\nsemantic layer. The authors employed the Word2vec [19],\nthe WordNet [20], and the ConceptNet [21] as the seman-\ntic layer, and combined the semantic layer with the LSTM\nto train an SMS spam detection model. The experimental\nevaluation that was conducted by the authors claimed that\nthe SLSTM achieved an accuracy of 99% on the SMS Spam\nCollection v.1 dataset.\nIn [22], Ghourabi et al.proposed the CNN-LSTM model\nthat consists of a CNN layer and an LSTM layer in order\nto identify SMS spam messages in English and Arabic. The\nauthors evaluated the CNN-LSTM by comparing it with the\nCNN, LSTM, and 9 traditional machine learning solutions.\nThe experimental tests that were conducted by the authors\nshowed that the CNN-LSTM solution performed better than\nother approaches and yield an accuracy of 98.3% and an\nF1-Score of 0.914.\nC. PAPER ORGANIZATION\nThe rest of the paper is organized as follows. Section II pro-\nvides the backgrounds and details of the LSTM and our spam\nTransformer approaches. Concretely, Section II-A introduces\nthe architecture of RNN, followed by one of its most suc-\ncessful variant LSTM in Section II-B. We then introduce\nSequence-to-Sequence in Section II-C, attention mechanism\nin Section II-D, and the original version of Transformer for\ntranslation tasks in Section II-E. Furthermore, Section III\ndiscusses the modiﬁed spam Transformer that we proposed\nin detail. Afterward, Section IV demonstrates the experi-\nment designs, results and analysis. Finally, we conclude in\nSection VI and describes the future work in Section VII.\nII. DEEP LEARNING APPROACHES\nWhile the traditional machine learning techniques do per-\nform well in many ﬁelds, they are still much interference or\nguidance from human specialists required when people try to\napply these technologies to address problems. For instance,\nextracting and representing the features from data is always\na challenging but indispensable work for machine learning\nscientists. In another word, the inadequate capacity of many\ntraditional machine learning classiﬁers is a major limitation\nto a more effective and massive application. However, many\ndeep learning techniques are able to not only learn much\nmore amount of features but also extract more higher-level\nfeatures that are formed by the composition of lower-level\nfeatures. With an effective training process, the deep learning\ntechniques are more capable to consume and make good use\nof a large amount of data and thus perform better especially in\ncoping with difﬁcult jobs compared to the traditional machine\nlearning approaches.\nA. RECURRENT NEURAL NETWORK\nAs is known to all, shufﬂing the order of words in a sentence\ncan severely inﬂuence the meaning of the entire sentence,\nwhich could potentially turn a legitimate message into spam\nmessages, and vice versa. Therefore, in many Natural Lan-\nguage Process (NLP) problems, the order of words is no less\nimportant than the words themselves. To address this prob-\nlem, we need a new kind of model that is capable to effectively\nlearn from prior knowledge to improve the understanding of\nthe data. Although the classical feed-forward neural network\nis a powerful deep learning technique that generally works\nwell in many areas, it cannot utilize the information from the\npast. Derived from the feed-forward neural network, recurrent\nneural network (RNN) [23] has the ability to reuse the saving\ninformation at the time of processing input values. Addi-\ntionally, unlike the traditional feed-forward neural network\n80254 VOLUME 9, 2021\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nFIGURE 1. Structure of a typical Recurrent Neural Network.\nsupports only the input sequence with a ﬁxed length, RNN is\ncapable to handle the input sequence with different length.\nThe Fig. 1 shows the typical structure of RNN models, with\nthe input sequence, output sequence, and the hidden layers at\ntime t are represented by xt , ot , and ht respectively. At time\nt, the current hidden layers state ht is calculated based on\nthe current input sequence xt and the last hidden layers ht−1.\nAfter the calculation of ht is ﬁnished, the output at the current\ntime step ot is generated and the hidden layers state ht will\nget involved in the calculation at the next time step t +1.\nUnlike the normal neural network, where the neurons in the\nsame layer of the hidden layers are independent of each other,\nRNN models usually allow the data ﬂows within the same\nlayer. In another word, connections between neurons in the\nsame layers or even self-connections are allowed generally\nallowed in RNN based models.\nA major advantage of RNN models is that they are able\nto utilize the information from previous input and apply it at\nthe current time, which is signiﬁcantly useful in NLP prob-\nlems since the context can help us understand the sentence\nbetter. However, a major drawback of the vanilla RNN is the\nvanishing and exploding gradients [24]. In back-propagation\ntraining process, the vanishing gradients refers to gradients go\nexponentially close to 0, while the exploding gradients refers\nto the gradients go exponentially increase. The vanishing and\nexploding gradients are usually caused by the multiplication\nof multiple derivatives in training process. Although there are\nseveral approaches [25] existing to address the vanishing and\nexploding gradients problem, in practice, it is still difﬁcult\nfor vanilla RNN to memorize and learn the features from\nlong distance, which is described as long-term dependencies\nproblem. In order to deal with the long-term dependencies\nproblem, many researchers have proposed multiple variants\nof RNN, such as the Long Short-Term Memory (LSTM) [26],\nthe Gated Recurrent Unit (GRU) [27], and the Clockwork\nRNN (CW-RNN) [28].\nB. LONG SHORT-TERM MEMORY\nThe Long Short-Term Memory (LSTM) is a famous variant\nof RNN. The main idea of the LSTM is the introduction of\ngate units, which are the structures that can determine to keep\nor discard the current information. A typical LSTM network\nconsists of multiple memory cells, and each memory cell is\nformed by an input gate, a forget gate, and an output gate.\nIn LSTM, at time t, the state of a memory cell ct is calculated\nbased on the input xt and the last hidden state ht−1. The\nstate of input gate, output gate, and forget gate at time t are\nrepresented as it , ot , ft , respectively. Therefore, the LSTM\ntransition functions are deﬁned as follows [29]:\nit =σ(Wi ·[ht−1,xt ] +bi)\nft =σ(Wf ·[ht−1,xt ] +bf )\nqt =tanh(Wq ·[ht−1,xt ] +bq)\not =σ(Wo ·[ht−1,xt ] +bo)\nct =ft ⊙ct−1 +it ⊙qt\nht =ot ⊙tanh(ct ) (1)\nThe σ denotes the sigmoid function, and the operator ⊙\ndenotes the element-wise multiplication. The sigmoid func-\ntion is a logistic function with the returning value between\n0 and 1. The sigmoid function is deﬁned as follow :\nσ(x) = 1\n1 +e−x (2)\nWhen the output of a gate unit is close to 1, the information\nis more likely to be memorized. On the contrary, a returning\nvalue close to 0 from a gate unit means that the information\nshould not be kept. The input gate it is the gate unit that con-\ntrols how much information should be stored at this time. The\nforget gate ft is responsible to determine to what extent the\nmemory from the last time ct−1 should be kept at time t.\nThe output gate ot at time t is designed to be used in the\ncomputation of the output (hidden state) based on the memory\ncell state.\nIn our LSTM approach for SMS spam detection, the input\nmessage embedding is fed into an LSTM network as an\ninput sequence. Meanwhile, the LSTM network saves the\nimportant features and outputs a sequence with the same\nlength as the input sequence. The output sequence is then\nfed into a feed-forward fully connected layer with a single\nneuron since SMS spam detection is a binary classiﬁcation\nproblem. Finally, a sigmoid function is applied to the output\nof the single neuron to produce a ﬁnal prediction.\nC. SEQUENCE-TO-SEQUENCE MODELS\nSequence-to-sequence (Seq2Seq) [30] was introduced\nin 2014 by Sutskever et al. aiming to ﬁnd a mapping\nbetween two sequences for translation tasks. Seq2Seq models\nemployed the Encoder-Decoder architecture, which consists\nof an encoder stack, a hidden state, and a decoder stack.\nFig. 2 presents a typical Encoder-Decoder architecture. The\nencoders take the input sequence and produce a hidden\nstate with critical information, which is consumed by the\ndecodes to generate the output sequences. One of the crucial\nadvantages of the Encoder-Decoder architecture is that the\ninput sequence and output sequence can be different in terms\nof size or format, which provides much more ﬂexibility and\npossibility. In reality, the Seq2Seq models have been proved\nthemselves in language translation [30], Speech Recogni-\ntion [31], and Video to Text [32]. Undoubtedly, Seq2Seq\nVOLUME 9, 2021 80255\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nFIGURE 2. Structure of Encoder-Decoder architecture.\narchitecture is designed to ﬁt translation tasks exception-\nally well, since it can extract the relationship between the\nsequences in one language and the sequences in a different\nlanguage. The vanilla version of the Seq2Seq model proposed\nin 2014 choose LSTM as both encoder and decoder, because\nLSTM has the ability to successfully learn on data with\nlong-term dependencies [30].\nD. ATTENTION MECHANISM\nThe main purpose of the attention mechanism is to ﬁnd out\nthe most important part from the input sequence. Concretely,\nthe attention mechanism produces weights that represent the\nimportance of the elements based on their correlation with the\ncontext. The attention mechanism makes it possible to focus\non the key elements.\nIn [33], the attention mechanism was introduced as an\nimprovement of the RNN Encoder-Decoder model hidden\nstate in Neural Machine Translation (NMT). The most impor-\ntant contribution of the attention mechanism in NMT is that\nit computes the weights based on all the hidden states gener-\nated by the encoder, and the decoder consumes the weighted\ncombination of all the hidden states instead of focusing only\non the latest one. The introduction of the attention mechanism\ngreatly boosts the performance of NMT.\nThere are also other forms of attention mechanism pro-\nposed. In [34], the attention mechanism is applied to the ﬁeld\nof computer vision by Xu et al., and they also proposed two\ndifferent approaches of attention named ‘‘soft attention’’ and\n‘‘hard attention’’. In [35], Luong et al.proposed global atten-\ntion and local attention. The global attention is similar to the\nmodel of Bahdanau et al.in [33] with a simpler architecture,\nwhile the local attention is a combination of soft and hard\nattention from Xu et al.in [34].\nE. THE TRANSFORMER MODEL\nThe Transformer [3] model is a sequence-to-sequence\n(Seq2Seq) model that was proposed in 2017 by Vaswani et al.,\nas an approach to English-German and English-French trans-\nlation tasks. Compared to those previous Seq2Seq models,\nthe main innovation of Transformer is that it completely relies\non the attention mechanism to efﬁciently learn from the most\ninformative elements [36].\nThough LSTM and some other RNN variants were proved\nto perform well as encoders and decoders in Seq2Seq based\nmodels, the high training consumption of recurrent models\nbecomes a signiﬁcant limitation. At time t, the computation of\nhidden state ht relies on the previous hidden state ht−1, which\nis the sequential computation nature of recurrent models.\nThis sequential computation nature prevents the computing\nof RNN variants from parallelization, leading to the limitation\non computational efﬁciency during the training process.\nIn order to address the computational efﬁciency limitation\nof RNN variants, the Transformer uses only multi-head atten-\ntion mechanism instead of RNN variants as encoders and\ndecoders. This not only greatly reduces the cost of training\nthrough parallelization, but also surprisingly improves the\nperformance in translation tasks as is mentioned in [3].\nIn Transformer, the attention function takes a query Q\nand a set of key-value pairs ( K,V ) as input, and computes\nthe weighted sum of values as output, where the weights\nare calculated based on the queries and keys. Particularly,\nScaled Dot-Product Attention is used in Transformer as the\nattention function. The Scaled Dot-Product Attention is the\ndot-product attention [35] with a scaling factor of 1√dk\n, which\naims to counteract the massive growth of dot-product when\ndimensions of queries and keys dk is large.\nAnother important innovation of Transformer is the\nMulti-Head Attention. In the previous practice, the atten-\ntion is directly performed on the queries, keys, and values,\nwhere their dimension is dmodel . In this way, there is only\na single attention function calculated at one turn. However,\nTransformer ﬁnds an effective way to apply multiple attention\nfunctions at once. Speciﬁcally, the queries, keys, and values\nare sent to some different learned linear layers to be projected\nh times to the dimension of dk , dk , and dv, respectively.\nIn another word, the projection linear layers are individually\nlearned, and output projections have dimensions of dk , dk ,\nand dv, where dk =dv =dmodel /h. After that, a number\nof h attention functions are performed in parallel on these\nprojected queries, keys, and values, resulting in h different\noutput values. Finally, all these h values are concatenated\ntogether and then projected back to a dimension of dmodel . The\nentire process of the attention mechanism in the Transformer\nis deﬁned as follows [3]:\nAttention(Q,K,V ) =softmax(QKT\n√dk\n)V\nMultiHead(Q,K,V ) =Concat(head1,..., headh)W O\nheadi =Attention(QW Q\ni ,KW K\ni ,VW V\ni ) (3)\nThe W Q\ni , W K\ni , and W V\ni are parameters matrices in linear\nprojection layers, where they are used to project dmodel -\ndimension queries, keys, and values to dk , dk , and dv dimen-\nsion, respectively. In both vanilla Transformer and our mod-\niﬁed Transformer for SMS spam detection, dk = dv =\ndmodel /h.\nIn RNN, the computation of the hidden states is based on\nthe previous states, making it available to learn from the order\nof words naturally. However, there is no recurrent or con-\nvolutional structure in Transformer. Therefore, Transformer\n80256 VOLUME 9, 2021\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nintroduces a positional encoding function based on sine and\ncosine functions of different frequencies.\nIn vanilla Transformer model designed for language trans-\nlation tasks, source language texts and shifted right target\nlanguage texts are ﬁrst sent to embedding layers as input\nsequence and output sequence. Secondly, positional infor-\nmation is injected into the input and output sequence in the\npositional encoding layer. After that, the input and output\nsequence is fed into encoders and decoders, respectively.\nThen, the Multi-Head Attention layers and fully-connected\nfeed-forward layers, combined as a single encoder or decoder,\nproduce the output of dimension of dmodel . The results of\ndecoders are passed to a linear layer. Finally, the softmax\nfunction is performed on the output of the linear layer, pro-\nducing the translation in the target language.\nIII. PROPOSED MODIFIED TRANSFORMER MODEL FOR\nSMS SPAM DETECTION\nIn Fig. 3, the main architecture of the modiﬁed Transformer\nmodel for SMS spam detection is described. In order to\nFIGURE 3. Structure of proposed modified Transformer model for SMS\nspam detection. The input messages embeddings and memory (trainable\nparameters) are positional encoded, respectively. Then, the processed\nmessage vectors are passed to encoder layers, where the self-attention is\nperformed. The results of encoder layers are passed to decoder layers.\nIn decoder layers, the Multi-Head Attention is executed based on the\nresults of encoder layers and the processed memory. Then, the decoded\nvectors are sent to some fully-connected linear layers, followed by a final\nactivation function for classification.\napply the Transformer model to the SMS spam detection task,\ntwo major modiﬁcations are done to the vanilla Transformer\nmodel, which is described in Section III-A and Section III-\nB, respectively. After that, several implementation details are\ndiscussed.\nA. MEMORY\nThe ﬁrst modiﬁcation for the SMS spam detection task is the\nintroduction of memory. Since there is no output sequence\n(target sequence) in the SMS spam detection task, we used\na list of trainable parameters named ‘‘memory’’ to be the\nsubstitute for output sequence embedding. The length of the\nmemory is a conﬁgurable hyper-parameter. Each element of\nthe memory is a vector of dimension dmodel so that it can be\nadapted to the Transformer model without any extra projec-\ntion. In other words, the memory is a matrix of dimension\nlenmemory×dmodel . The output embedding layer in the original\nTransformer model is also removed since there are no target\nsequence texts anymore to be mapped to numeric vectors.\nSimilar to the output sequence in the vanilla Transformer\nmodel, the positional information is injected into the mem-\nory at the positional encoding layer before being fed into\ndecoders.\nDuring the training process, the parameters of memory\nare trained, and the memory matrix is expected to contain\nthe important information that can help to predict whether\nor not a message is a spam. Therefore, in the decoders of\nthe modiﬁed spam Transformer model, with the help of the\nattention mechanism, the memory can contribute to locate the\nsigniﬁcant part of the output sequence of the encoder stack\nthat summarized the message, and eventually help to classify\nthe spam SMS messages.\nB. LINEAR LAYERS AND FINAL ACTIVATION FUNCTION\nThe second modiﬁcation is the ﬁnal activation function. In the\nvanilla Transformer, the dimension of outputs of decoder\nlayers is T ×dmodel , where T is the target sequence length\nand dmodel is the model size (number of features). Therefore,\nintuitively, it is a promising approach to use the linear layers\nto map the output to a vector that has the same dimension\nas the number of words in the dictionary and apply a softmax\nfunction on the vector to ﬁnd the closest candidate word from\nthe dictionary.\nHowever, the SMS spam detection task is a binary classi-\nﬁcation problem. Therefore, to convert the output from the\ndecoder stacks with dimension dmodel into a single proba-\nbility of the message being spam, the linear layers after the\ndecoders are also modiﬁed. Instead of mapping the output of\nthe decoder stack to a vector, the linear layer in the modiﬁed\nTransformer model for SMS spam detection has only one sin-\ngle neuron in the last layer. Thus, the outputs of the decoder\nstack are converted into a single numeric probability value.\nAdditionally, the ﬁnal activation function needs to be\nreplaced with a function that can map the result to a\nbinary outcome. Thus, in the modiﬁed Transformer for SMS\nspam detection, a sigmoid function, which is deﬁned in\nVOLUME 9, 2021 80257\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nEquation (2), as the ﬁnal activation function, is applied to the\noutput of the linear layers after decoders, generating a binary\nresult that predicts whether or not the message is spam.\nC. DROPOUT\nDropout [37] is a powerful technique published by Hinton\net al. in 2012 in order to prevent over-ﬁtting in a large\nfeed-forward neural network. Concretely, the Dropout refers\nto randomly omit some nodes in those large feed-forward\nlayers on each speciﬁc training case. The modiﬁed spam\nTransformer model that we proposed employs multiple\nfeed-forward layers. Thus, the Dropout technique is also\nimplemented in the feed-forward layers of our spam Trans-\nformer model. Besides, the Dropout technique is also used in\npositional encoding and calculation of attention function.\nD. BATCHES AND PADDING\nDuring each epoch of training on our proposed models,\nthe whole training set is divided into multiple batches. As the\nlength of the message with the same batch should be the same,\nsome padding words (empty words) should be added into\nthe shorter message vectors, interfering with the detection to\nsome extent. Therefore, the algorithm of dividing the training\nset into batches is designed to minimize the padding words.\nSpeciﬁcally, the training data is sorted by the message length\nﬁrst, and the batches are created to minimize the padding\nwords based on the sorted messages.\nAdmittedly, adding padding words may pose a negative\ninﬂuence on the model. However, using batch has been\nproved to be a good idea for model training as it increases\nthe training speed extraordinarily. In fact, a larger batch size\naccelerates a ton for the training speed. Additionally, the neg-\native inﬂuence of padding words is addressed by minimizing\nthe use of padding words. Besides, the padding masks are also\npassed into the model along with the training batches so that\nthe Transformer model can ignore the padding words during\ntraining.\nE. OPTIMIZATION AND LEARNING RATE\nThe gradient descent is employed to optimize our modiﬁed\nspam Transformer model. The main idea of the gradient\ndescent algorithm is to minimize the loss function of the\nmodel by updating the parameters along the opposite way\nof the gradient to the loss function, where the gradient is\nthe partial derivatives of the loss function of the parameter.\nThere are plenty of variant optimizers of gradient descent.\nWe use the AdamW [38] optimizer for our proposed mod-\niﬁed spam Transformer with β1 = 0.9, β2 = 0.98, and\nϵ = 10−9. Learning rate is a critical hyper-parameter in\nmachine learning. It is deﬁned as the step size of updating\nparameters, which basically represents the speed of learning\nof the model. Having the learning rate set too high will lead to\nthe situation that the model fails to locate the best parameters\n(weights and biases), while a learning rate that is too small\nsticks the model around the local optimal point rather than\nﬁnding a better parameter solution. For the modiﬁed spam\nTransformer model, the same way of determining the learning\nas mentioned in [3] is utilized. The learning rate lr ﬁrst\nincreases linearly until reaching the warmup_steps steps and\nthen decreases proportionally to the square root of the step\nnumbers. Concretely, we used warmup_steps =8000.\nF. DATAFLOW OF MODIFIED TRANSFORMER\nAs is shown in Figure 3, the input messages are ﬁrst converted\ninto word embeddings using the Glove model. Following\nthis, the memory (trainable parameters) and the embeddings\nof the input sequence are positionally encoded, respectively.\nThen, the processed message vectors are passed to encoder\nlayers, where the multi-head self-attention is performed and\nthe important parts of the input sequence are given larger\nweights. The results of encoder layers are passed to decoder\nlayers. In decoder layers, the multi-head self-attention is\ncomputed on the memory. After that, the multi-head attention\nis executed based on the results of encoder layers and the pro-\ncessed memory. Finally, the decoded vectors are sent to some\nfully-connected linear layers, followed by a ﬁnal activation\nfunction for classiﬁcation.\nIV. EXPERIMENT\nA. DATASETS\nIn the experiments, two different datasets are utilized. The\nﬁrst dataset is SMS Spam Collection v.1 [13] dataset, which\nis labeled SMS messages dataset collected for mobile phone\nmessage research. The second one is UtkMl’s Twitter Spam\nDetection Competition (UtkMl’s Twitter) [39] from Kaggle.\nTable 1 shows the overview statistics of the two datasets.\nTABLE 1. The statistics of two datasets.\nAlthough the Twitter posts are not precisely the same as\nthe SMS messages, they are still in some ways common. For\ninstance, they both have approximately less than 100 words.\nPeople tend to use more casual language and abbreviations\nin both Twitter posts and SMS messages. Therefore, UtkMl’s\nTwitter dataset can also be used to test our model. Besides,\nwe can also analyze the extensibility of our model by com-\nparing the performance of our model on these two datasets.\nIn comparison with SMS Spam Collection v.1 [13] dataset,\nUtkMl’s Twitter dataset contains more data in both spam and\nham classes. Besides, UtkMl’s Twitter dataset is balanced\nsince the number of spam messages and ham messages are\napproximately equal. In terms of the language, although they\nare a lot of casual language and abbreviation used in both\ndatasets, casual language and abbreviation appear more fre-\nquently in UtkMl’s Twitter dataset. The reason for this obser-\nvation may be the feature of the Twitter posts. Alternatively,\nit could also because of the date that the dataset was collected,\nas SMS Spam Collection v.1 was published in 2011.\n80258 VOLUME 9, 2021\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nTABLE 2. The confusion matrix.\nB. EVALUATION MEASURES\nIn order to evaluate the performance of the proposed modiﬁed\nspam Transformer model, some metrics such as accuracy,\nprecision, recall, and F1-Score are used in the experiments.\nAll these metrics are calculated based on the confusion\nmatrix. As is mentioned in the previous section, the spam\nmessages in the SMS Spam Collection v.1 dataset are sig-\nniﬁcantly less than the ham messages, which means that\nthe dataset is unbalanced. Therefore, the accuracy is not\nsufﬁcient as a measurement to evaluate the performance of\nthe proposed model, and the F1-Score is employed in the\nexperiments. The accuracy, precision, recall, and F1-Score is\ndeﬁned as follows :\nAccuracy = TP +TN\nTP +FP +FP +FN (4)\nPrecision = TP\nTP +FP (5)\nRecall = TP\nTP +FN (6)\nF1 −Score =2 ×Precision ×Recall\nPrecision +Recall (7)\nThe precision, also known as the positive predictive value,\nrepresents the percentage of the predicted positive cases that\nare actually positive, meaning the possibility that the classi-\nﬁer is correct given that it predicts positive. The recall, also\nknown as sensitivity, denotes the number of true positives\ninstances divided by the number of actual positive instances,\nwhich can also be described as the percentage of the positive\ncases that are identiﬁed successfully. The F1-Score is the\nharmonic mean of precision and recall, which measures the\nperformance of a classiﬁer in terms of precision and recall in\na balanced way.\nC. DATA SPLITTING\nFor the traditional machine learning approaches, the data is\ndivided into training set (70%), and test set (30%). For the\nLSTM and our proposed modiﬁed spam Transformer model,\nthe data is split into training set (50%), validation set (20%),\nand test set (30%), where the validation set is used after each\nepoch of training to help us select the best model and perform\nearly stopping to avoid over-ﬁtting.\nD. DATA PRE-PROCESSING\nThe textual messages in the dataset are ﬁrst tokenized.\nTokenization refers to the task of splitting textual into mean-\ningful words. Speciﬁcally, the SpaCy [40] library is employed\nfor data pre-processing in order to tokenize the data.\nAfter that, the numeric representation vectors (word\nembeddings) are calculated based on the textual messages.\nThere are two major methods of calculating representation\nvectors are employed in our experiments.\n• TF-IDF Representation : The TF-IDF (Term\nFrequency–Inverse Document Frequency) is a widely-\nused numerical statistic in NLP. It is designed to reﬂect\nthe importance of a word to a document in the given\ntext corpus. The Term Frequency (TF) is deﬁned as\nthe number of times that a term occurs in a document.\nA larger TF means the term is referred for more times\nin the given document, showing that the term is more\nrelevant to the document. There are multiple different\nmeans to weigh the TF in order to adapt it in different\napplications. In our experiment, we use the raw count\nof the term in the document as the TF. The Inverse\nDocument Frequency (IDF) is a value to qualify the\nspeciﬁcity of a term, which is normally deﬁned as the\nlogarithmically scaled inverse fraction of the number\nof documents that contain the term. In another word,\nwhen a term occurs in a great number of documents,\nthe IDF is numerically low, leading to a low TF-IDF. For\ninstance, the term ‘‘the’’ occurs in almost every English\ndocument, leading to a document frequency of almost 1\n(100% of the documents in the corpus contain the term\n‘‘the’’). Thus, the IDF of ‘‘the’’ is close to 0, which\nmeans that its importance to any documents in the corpus\nis low.\n• GloVe Representation: GloVe [41] is an unsupervised\nlearning algorithm for obtaining vector representations\nfor words. The main idea is to map words into a mean-\ningful space where the distance between words is related\nto semantic similarity. GloVe produces a vector space\nwith a meaningful substructure, and it can also ﬁnd the\nrelations like synonyms between words.\nIn our experiments, for the deep learning approaches\nsuch as LSTM and our proposed spam Transformer model,\nthe GloVe model is employed to create representation vec-\ntors for them. Speciﬁcally, in our experiments, we used the\n‘‘glove.840B.300d’’, a pre-trained model with 2.2 million\nwords in the dictionary that converts textual data into\n300-dimensional vectors. For benchmark machine learn-\ning algorithms, although the vectors generated by GloVe\nmodel have more dimensions and theoretically contain more\ninformation, presumably due to the limitation of traditional\nmachine learning classiﬁers, the TF-IDF representation per-\nforms better in practice. Therefore, TF-IDF representation\nis used for calculating representation vectors in benchmark\nmachine learning algorithms.\nE. LOSS FUNCTION\nThe loss function we used for deep learning approaches\nincluding LSTM and modiﬁed spam Transformer is Binary\nCross Entropy function, which is deﬁned as follow :\nl(xi,yi) =−wi[yi ·logxi +(1 −yi) ·log(1 −xi)] (8)\nThe weight wi is the rescaling factor for loss. Since\nthe SMS Spam Collection v.1 is unbalanced, where spam\nVOLUME 9, 2021 80259\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nmessages are severely less than ham (legitimate) messages,\na larger weight is given to the actual spam messages to\ncounteract the negative effect of the unbalanced dataset. The\nrescaling weight is calculated based on the ratio between the\nnumber of ham messages and spam messages.\nF. MODEL TRAINING\nWe trained our experiment models on NVIDIA GeForce RTX\n3090 GPU. For the machine learning classiﬁers, the experi-\nments are performed on the Scikit-learn 0.24.0 [42] environ-\nment. For deep learning approaches like LSTM and spam\nTransformer model, the experiments are conducted on the\nUbuntu 20.04 LTS, CUDA 11.1, and PyTorch 1.7.1 [43]\nenvironment. The early stopping technique is implemented\nto ﬁght against the over-ﬁtting. Besides, we also trained and\ntested the CNN-LSTM SMS spam detection model proposed\nin [22] on both datasets as a benchmark to evaluate our\nmodiﬁed spam Transformer model.\nG. HYPER-PARAMETERS TUNING\nIn order to tune the models and ﬁnd the best hyper-parameters\nset, the Ray Tune [44] library is employed. The Ray Tune\nis a hyper-parameter tuning extension tool that supports\nmultiple machine learning frameworks. Given a candidate\nhyper-parameters set, the Ray Tune can ﬁnd the opti-\nmized hyper-parameters set by training multiple models\nwith different settings and comparing the results automat-\nically. In our experiments, with the help of the Ray Tune,\nwe ﬁrst explored optimal settings for the overall architectural\nhyper-parameters such as Encoder layers, Decoder layers,\nand Model size. After that, other hyper-parameters such as the\nrate of dropout and Feed-forward layer size are tuned under\nthe candidate optimal model settings.\nFor the LSTM model, the optimized parameters on both\ndatasets are shown in Table 3. For our modiﬁed spam Trans-\nformer model on SMS Spam Collection v.1, Table 4 presents\nthe initial hyper-parameters that we started from and the opti-\nmized values when the better result was achieved after tuning.\nTable 5 demonstrates the initial as well as the optimized\nhyper-parameters of modiﬁed spam Transformer on UtkMl’s\nTwitter dataset.\nTABLE 3. Optimized hyper-parameters for LSTM.\nV. RESULTS AND ANALYSIS\nA. EVALUATION\nWe demonstrate the performance of the modiﬁed spam Trans-\nformer model by comparing it on two datasets with some\nother typical spam detection classiﬁers, including Logistic\nRegression, Naïve Bayes, Random Forests, Support Vector\nTABLE 4. Initial and optimized hyper-parameters for modified spam\nTransformer on SMS Spam Collection v.1.\nTABLE 5. Initial and optimized hyper-parameters for modified spam\nTransformer on UtkMl’s Twitter.\nTABLE 6. Results obtained on SMS Spam Collection v.1.\nTABLE 7. Results obtained on UtkMl’s Twitter.\nMachine (classiﬁer), and Long Short-Term Memory. Besides,\nfor the SMS Spam Collection v.1 dataset, we also compare\nour models with the CNN-LSTM approaches in [22], since\nthey aim to solve the same problem on the same dataset with\nus.\nTable 6 summarizes the results on SMS Spam Collection\nv.1 dataset. For accuracy, our modiﬁed spam Transformer\nmodel achieved the best value of 98.92%. Concerning pre-\ncision, the best score was from the Random Forests classiﬁer\nwith a value of 1.0, and our proposed spam Transformer\ngot a value of 0.9781. When it comes to recall, the opti-\nmal result came from the spam Transformer model with a\nvalue of 0.9451, and the same value came from the Naïve\nBayes classiﬁer as well. Finally, in terms of F1-Score, our\nspam Transformer also achieved the best value of 0.9613.\nThe experiment of CNN-LSTM [22] that was conducted\n80260 VOLUME 9, 2021\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nTABLE 8. The confusion matrices on SMS Spam Collection v.1.\nTABLE 9. The confusion matrices on UtkMl’s Twitter.\nby Ghourabi et al. on the same dataset, are also included\nin Table 6. In Table 8, we demonstrate the confusion matrix of\nall the approaches that we tested in the experiments on SMS\nSpam Collection v.1 dataset.\nTable 7 summarizes the results on UtkMl’s Twitter dataset.\nThe modiﬁed spam Transformer model outperformed all\nother candidates in all four aspects that we tested with the val-\nues of 87.06%, 0.8746, 0.8576, and 0.8660 on the accuracy,\nprecision, recall, and F1-Score, respectively. The confusion\nmatrix of the modiﬁed spam Transformer model on UtkMl’s\nTwitter is presented in Table 9.\nB. ANALYSIS\nAlthough the experimental results show an improved perfor-\nmance of the proposed spam Transformer model compared\nto other candidates, the false predictions also indicate the\ndrawback of the proposed model. We analyzed the content\nof the false prediction samples including false positive and\nfalse negative samples and found that there were a great\nnumber of the UNK marks in the data passed to the model,\nwhich is produced because the words are never seen in the\ntraining data. In other words, the unknown words obstruct the\nmodel from understanding the messages. Besides, the SMS\nmessages are usually short, which increases the inﬂuence\nof every single word and makes the unknown words more\ninﬂuential. Actually, due to the unknown words, the model\ndid not have enough information to detect spams in many\nfalse prediction cases.\nThough our proposed model performs better than other\ncandidate algorithms on UtkMl’s Twitter dataset, the results\nare still not as good as that in case of SMS Spam Collection\nv.1 dataset. From our observation, the major cause is also\nthe unknown words. Compared to SMS Spam Collection\nv.1 dataset, there are more casual language and abbreviations\nin UtkMl’s Twitter dataset, which may be caused by the\nfeature of Twitter posts or the date of collection of the dataset,\nas is discussed in Section IV-A. Therefore, the negative inﬂu-\nence from casual language and abbreviation is more severe on\nUtkMl’s Twitter dataset, and that is the major cause of more\nunknown words and eventually worse performance from our\nperspective.\nIn addition, Table 8 and Table 9 show the excellent robust-\nness of our model to classify both the spams and hams effec-\ntively on no matter balanced (UtkMl’s Twitter) or unbalanced\n(SMS Spam Collection v.1) datasets.\nVI. CONCLUSION\nIn this paper, we proposed a modiﬁed Transformer model that\naims to identify SMS spam. We evaluated our spam Trans-\nformer model by comparing it with several other SMS spam\ndetection approaches on the SMS Spam Collection v.1 dataset\nand UtkMl’s Twitter dataset. The experimental results show\nthat, compared to Logistic Regression, Naïve Bayes, Random\nForests, Support Vector Machine, Long Short-Term Memory,\nand CNN-LSTM [22], our proposed spam Transformer model\nperforms better on both datasets.\nOn the SMS Spam Collection v.1 dataset, our spam Trans-\nformer has a better performance in terms of accuracy, recall,\nand F1-Score compared to other classiﬁers. Speciﬁcally,\nour modiﬁed spam Transformer approach accomplished an\nexceeding result on F1-Score.\nAdditionally, on the UtkMl’s Twitter dataset, the results\nfrom our modiﬁed spam Transformer model demonstrate its\nimproved performance on all four aspects in comparison to\nother alternative approaches mentioned in this paper. Con-\ncretely, our spam Transformer does exceptionally well on\nrecall, which contributes to a distinct F1-Score.\nVII. FUTURE WORK\nAlthough the experimental results in this paper have shown\nan improvement of our proposed spam Transformer model\nin comparison with some previous approaches on SMS spam\ndetection, we still believe that there is great potential in the\nmodel we proposed.\nFirstly, since our current two datasets contain only thou-\nsands of messages, in the future, we plan to extend our spam\nTransformer model to a larger dataset with more messages\nor even other types of content, for the purpose of better\nperformance.\nBesides, in our proposed model, we ﬂattened the out-\nputs from decoders and applied linear fully-connected lay-\ners before applying the ﬁnal activation function and getting\nthe prediction. We believe that some dedicated designs or\nVOLUME 9, 2021 80261\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\nimplementations instead of simple ﬂattening and linear layers\ncould absolutely boost the performance, which would be one\nof the most important future works.\nAdditionally, although the experimental results show that\nour modiﬁed model based on the vanilla Transformer per-\nforms well on SMS spam detection and conﬁrms the avail-\nability of the Transformer on this problem, the model is still\nfar from optimal. There are some improved models based\non the Transformer with more complex architecture such as\nGPT-3 [4] and BERT [5] that could be explored in the future.\nSpeciﬁcally, the BERT seems to be a promising starting point\nof future work as it has fewer features and is easier to be\nﬁne-tuned.\nFinally, as is discussed in Section V-B, the proposed\nmodel is severely inﬂuenced by the unknown words in many\ncases of false prediction. To address this problem, more data\npre-processing techniques could be applied. For instance,\na larger vocabulary with more words could be a good option,\nand some semantic operations such as replacing unknown\nwords with their synonyms could also be explored. Besides,\nthere are some other data-preprocessing and feature extrac-\ntion techniques that could be done, such as the extraction and\nanalysis of the abbreviation, URLs, tags, or emoji in data.\nREFERENCES\n[1] P. K. Roy, J. P. Singh, and S. Banerjee, ‘‘Deep learning to ﬁlter SMS spam,’’\nFuture Gener. Comput. Syst., vol. 102, pp. 524–533, Jan. 2020.\n[2] G. Jain, M. Sharma, and B. Agarwal, ‘‘Optimizing semantic LSTM for\nspam detection,’’Int. J. Inf. Technol., vol. 11, no. 2, pp. 239–250, Jun. 2019.\n[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5999–6009.\n[4] T. B. Brown et al., ‘‘Language models are few-shot learners,’’ 2020,\narXiv:2005.14165. [Online]. Available: http://arxiv.org/abs/2005.14165\n[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[6] G. Sonowal and K. S. Kuppusamy, ‘‘SmiDCA: An anti-Smishing\nmodel with machine learning approach,’’ Comput. J., vol. 61, no. 8,\npp. 1143–1157, Aug. 2018.\n[7] J. W. Joo, S. Y . Moon, S. Singh, and J. H. Park, ‘‘S-detector: An enhanced\nsecurity model for detecting Smishing attack for mobile computing,’’\nTelecommun. Syst., vol. 66, no. 1, pp. 29–38, Sep. 2017.\n[8] S. Mishra and D. Soni, ‘‘Smishing detector: A security model to detect\nSmishing through SMS content analysis and URL behavior analysis,’’\nFuture Gener. Comput. Syst., vol. 108, pp. 803–815, Jul. 2020.\n[9] C. Li, L. Hou, B. Y . Sharma, H. Li, C. Chen, Y . Li, X. Zhao, H. Huang,\nZ. Cai, and H. Chen, ‘‘Developing a new intelligent system for the diagno-\nsis of tuberculous pleural effusion,’’ Comput. Methods Programs Biomed.,\nvol. 153, pp. 211–225, Jan. 2018.\n[10] T. K. Ho, ‘‘Random decision forests,’’ in Proc. Int. Conf. Document Anal.\nRecognit. (ICDAR), vol. 1, 1995, pp. 278–282.\n[11] C. Cortes and V . Vapnik, ‘‘Support-vector networks,’’ Mach. Learn.,\nvol. 20, no. 3, pp. 273–297, 1995.\n[12] M. Gupta, A. Bakliwal, S. Agarwal, and P. Mehndiratta, ‘‘A comparative\nstudy of spam SMS detection using machine learning classiﬁers,’’ in Proc.\n11th Int. Conf. Contemp. Comput. (IC3), Aug. 2018, pp. 1–7.\n[13] T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami, ‘‘Contributions to the\nstudy of SMS spam ﬁltering: New collection and results,’’ in Proc. 11th\nACM Symp. Document Eng., Sep. 2011, pp. 259–262.\n[14] A. K. Jain and B. B. Gupta, ‘‘Rule-based framework for detection of Smish-\ning messages in mobile environment,’’ Procedia Comput. Sci., vol. 125,\npp. 617–623, 2018.\n[15] W. W. Cohen, ‘‘Fast effective rule induction,’’ in Machine Learning Pro-\nceedings, 1995, pp. 115–123.\n[16] J. Cendrowska, ‘‘PRISM: An algorithm for inducing modular rules,’’ Int.\nJ. Man-Machine Stud., vol. 27, no. 4, pp. 349–370, Oct. 1987.\n[17] J. H. Friedman, ‘‘Greedy function approximation: A gradient boosting\nmachine,’’Ann. Statist., vol. 29, no. 5, pp. 1189–1232, Oct. 2001.\n[18] L. Bottou, ‘‘Large-scale machine learning with stochastic gradient\ndescent,’’ in Proc. COMPSTAT. Physica-Verlag, 2010, pp. 177–186.\n[19] T. Mikolov, K. Chen, G. S. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\nword representations in vector space,’’ in Proc. Int. Conf. Learn. Repre-\nsent., 2013.\n[20] G. A. Miller, ‘‘WordNet: A lexical database for English,’’ Commun. ACM,\nvol. 38, no. 11, pp. 39–41, 1995.\n[21] H. Liu and P. Singh, ‘‘ConceptNet — A practical commonsense reasoning\ntool-kit,’’BT Technol. J., vol. 22, no. 4, pp. 211–226, Oct. 2004.\n[22] A. Ghourabi, M. A. Mahmood, and Q. M. Alzubi, ‘‘A hybrid CNN-LSTM\nmodel for SMS spam detection in arabic and English messages,’’ Future\nInternet, vol. 12, no. 9, p. 156, Sep. 2020.\n[23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‘‘Learning rep-\nresentations by back-propagating errors,’’ Nature, vol. 323, no. 6088,\npp. 533–536, Oct. 1986.\n[24] Y . Bengio, P. Simard, and P. Frasconi, ‘‘Learning long-term dependencies\nwith gradient descent is difﬁcult,’’ IEEE Trans. Neural Netw., vol. 5, no. 2,\npp. 157–166, Mar. 1994.\n[25] R. Pascanu, T. Mikolov, and Y . Bengio, ‘‘On the difﬁculty of training\nrecurrent neural networks,’’ in Proc. 30th Int. Conf. Mach. Learn. (ICML),\n2013, pp. 2347–2355.\n[26] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[27] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN Encoder–Decoder for statistical machine translation,’’ in Proc.\nConf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014,\npp. 1724–1734.\n[28] J. Koutník, K. Greff, F. Gomez, and J. Schmidhuber, ‘‘A clockwork RNN,’’\nin Proc. 31st Int. Conf. Mach. Learn. (ICML), vol. 5, 2014, pp. 3881–3889.\n[29] C. Zhou, C. Sun, Z. Liu, and F. C. M. Lau, ‘‘A C-LSTM neural net-\nwork for text classiﬁcation,’’ 2015, arXiv:1511.08630. [Online]. Available:\nhttp://arxiv.org/abs/1511.08630\n[30] I. Sutskever, O. Vinyals, and Q. V . Le, ‘‘Sequence to sequence learning with\nneural networks,’’ inProc. Adv. Neural Inf. Process. Syst., vol. 4, Sep. 2014,\npp. 3104–3112.\n[31] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and N. Jaitly,\n‘‘A comparison of sequence-to-sequence models for speech recognition,’’\nin Proc. Interspeech, Aug. 2017, pp. 939–943.\n[32] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and\nK. Saenko, ‘‘Sequence to sequence–video to text,’’ in Proc. IEEE Int. Conf.\nComput. Vis. (ICCV), Dec. 2015, pp. 4534–4542.\n[33] D. Bahdanau, K. H. Cho, and Y . Bengio, ‘‘Neural machine translation\nby jointly learning to align and translate,’’ in Proc. 3rd Int. Conf. Learn.\nRepresent. (ICLR), 2015.\n[34] K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov,\nR. S. Zemel, and Y . Bengio, ‘‘Show, attend and tell: Neural image caption\ngeneration with visual attention,’’ in Proc. 32nd Int. Conf. Mach. Learn.,\nvol. 3, 2015, pp. 2048–2057.\n[35] T. Luong, H. Pham, and C. D. Manning, ‘‘Effective approaches to attention-\nbased neural machine translation,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process. (EMNLP), 2015, pp. 1412–1421.\n[36] E. S. D. Reis, C. A. D. Costa, D. E. D. Silveira, R. S. Bavaresco,\nR. D. R. Righi, J. L. V . Barbosa, R. S. Antunes, M. M. Gomes, and\nG. Federizzi, ‘‘Transformers aftermath,’’ Commun. ACM, vol. 64, no. 4,\npp. 154–163, Apr. 2021.\n[37] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, ‘‘Improving neural networks by preventing co-\nadaptation of feature detectors,’’ 2012, arXiv:1207.0580. [Online].\nAvailable: http://arxiv.org/abs/1207.0580\n[38] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’\n2017, arXiv:1711.05101. [Online]. Available: http://arxiv.org/abs/1711.\n05101\n[39] UtkMl’s Twitter Spam Detection Competition | Kaggle, UtkMl.\n[40] M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd, ‘‘spaCy:\nIndustrial-strength natural language processing in python,’’ 2020, doi:\n10.5281/zenodo.1212303.\n[41] J. Pennington, R. Socher, and C. D. Manning, ‘‘GloVe: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n80262 VOLUME 9, 2021\nX. Liuet al.: Spam Transformer Model for SMS Spam Detection\n[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,\nM. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas,\nA. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay,\n‘‘Scikit-learn: Machine learning in python,’’ J. Mach. Learn. Res., vol. 12,\npp. 2825–2830, Nov. 2011.\n[43] A. Paszke et al., ‘‘PyTorch: An imperative style, high-performance deep\nlearning library,’’ in Proc. Adv. Neural Inf. Process. Syst., H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett,\nEds. 2019, pp. 8024–8035.\n[44] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez,\nand I. Stoica, ‘‘Tune: A research platform for distributed model\nselection and training,’’ 2018, arXiv:1807.05118. [Online]. Available:\nhttp://arxiv.org/abs/1807.05118\nXIAOXU LIU received the bachelor’s degree in\ncomputer science and technology from the Nan-\njing University of Posts and Telecommunications,\nChina, and the master’s degree in computer sci-\nence program from the University of Ottawa,\nCanada, in 2019. His research interests include\nmachine learning, deep learning, and natural lan-\nguage processing.\nHAOYE LU (Member, IEEE) received the joint\nB.Sc. degree in computer science and mathemat-\nics, in 2017, and the master’s degree in computer\nscience, in 2019. In 2013, he joined the University\nof Ottawa, Canada. He is currently working as a\nResearch Associate. His research interests include\nartiﬁcial intelligence and network structures.\nAMIYA NAYAK(Senior Member, IEEE) received\nthe B.Math. degree in computer science and com-\nbinatorics and optimization from the University of\nWaterloo, Canada, in 1981, and the Ph.D. degree in\nsystems and computer engineering from Carleton\nUniversity, Canada, in 1991. He is currently a Full\nProfessor with the School of Electrical Engineer-\ning and Computer Science, University of Ottawa.\nHe has over 17 years of industrial experience in\nsoftware engineering, avionics and navigation sys-\ntems, and simulation and system level performance analysis. His research\ninterests include software-deﬁned networking, mobile computing, wireless\nsensor networks, and vehicular ad hoc networks.\nHe has served on the Editorial Board of several journals, including IEEE\nTRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS, International Journal of\nParallel, Emergent and Distributed Systems, Journal of Sensor and Actuator\nNetworks, and EURASIP Journal on Wireless Communications and Network-\ning. He is currently serving on the Editorial Board of the IEEE I NTERNET OF\nTHINGS JOURNAL, IEEE T RANSACTIONS ON VEHICULAR TECHNOLOGY, IEEE O PEN\nJOURNAL OF THE COMPUTER SOCIETY, Future Internet, and International Journal\nof Distributed Sensor Networks.\nVOLUME 9, 2021 80263",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7794893980026245
    },
    {
      "name": "Transformer",
      "score": 0.6557410955429077
    },
    {
      "name": "Spamming",
      "score": 0.4971349537372589
    },
    {
      "name": "Short Message Service",
      "score": 0.44011345505714417
    },
    {
      "name": "Spambot",
      "score": 0.43438586592674255
    },
    {
      "name": "Machine learning",
      "score": 0.43065983057022095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40585893392562866
    },
    {
      "name": "Data mining",
      "score": 0.37097662687301636
    },
    {
      "name": "Computer network",
      "score": 0.1552601456642151
    },
    {
      "name": "World Wide Web",
      "score": 0.12623250484466553
    },
    {
      "name": "The Internet",
      "score": 0.12085038423538208
    },
    {
      "name": "Engineering",
      "score": 0.09599640965461731
    },
    {
      "name": "Voltage",
      "score": 0.0654001235961914
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153718931",
      "name": "University of Ottawa",
      "country": "CA"
    }
  ]
}