{
  "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
  "url": "https://openalex.org/W4389520473",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4224913994",
      "name": "Lin, Hongzhan",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A4221860987",
      "name": "Luo, Ziyang",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A1914841796",
      "name": "Ma Jing",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2107639259",
      "name": "Chen Long",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4310625358",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285606948",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4307106676",
    "https://openalex.org/W3115830738",
    "https://openalex.org/W1541634262",
    "https://openalex.org/W4308243058",
    "https://openalex.org/W2073302931",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4372337842",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4283455941",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3083982479",
    "https://openalex.org/W3116547614",
    "https://openalex.org/W3114509603",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3138656580",
    "https://openalex.org/W3032412857",
    "https://openalex.org/W3111930335",
    "https://openalex.org/W3112525597",
    "https://openalex.org/W3115116211",
    "https://openalex.org/W3195130895",
    "https://openalex.org/W3118051320",
    "https://openalex.org/W3174604160",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3207512843",
    "https://openalex.org/W639708223",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170224286",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2972119347",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4224910065",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3175392088",
    "https://openalex.org/W3199394329",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W3196199406",
    "https://openalex.org/W4224320265",
    "https://openalex.org/W4297816851",
    "https://openalex.org/W4385573042",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9114–9128\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBeneath the Surface: Unveiling Harmful Memes with Multimodal\nReasoning Distilled from Large Language Models\nHongzhan Lin1, Ziyang Luo1, Jing Ma1∗, Long Chen2\n1Hong Kong Baptist University\n2The Hong Kong University of Science and Technology\n{cshzlin,cszyluo,majing}@comp.hkbu.edu.hk, longchen@ust.hk\nAbstract\nThe age of social media is rife with memes.\nUnderstanding and detecting harmful memes\npose a significant challenge due to their implicit\nmeaning that is not explicitly conveyed through\nthe surface text and image. However, existing\nharmful meme detection approaches only rec-\nognize superficial harm-indicative signals in an\nend-to-end classification manner but ignore in-\ndepth cognition of the meme text and image. In\nthis paper, we attempt to detect harmful memes\nbased on advanced reasoning over the inter-\nplay of multimodal information in memes. In-\nspired by the success of Large Language Mod-\nels (LLMs) on complex reasoning, we first con-\nduct abductive reasoning with LLMs. Then\nwe propose a novel generative framework to\nlearn reasonable thoughts from LLMs for better\nmultimodal fusion and lightweight fine-tuning,\nwhich consists of two training stages: 1) Distill\nmultimodal reasoning knowledge from LLMs;\nand 2) Fine-tune the generative framework to\ninfer harmfulness. Extensive experiments con-\nducted on three meme datasets demonstrate that\nour proposed approach achieves superior per-\nformance than state-of-the-art methods on the\nharmful meme detection task.\n1 Introduction\nThe development of social media platforms has\ngiven rise to a new form of multimodal content\nknown as: meme. A meme typically comprises a\npicture that is combined with a concise text com-\nponent. Memes possess the capacity to quickly\nspread across the internet, especially on social me-\ndia platforms, due to their ease of dissemination.\nWhile memes are often seen as humorous, there is a\npotential for harm when the combination of images\nand texts is strategically used to promote political\nand sociocultural divisions. For instance as in Fig-\nure 1(a), during the COVID-19 pandemic, a widely\n∗ Jing Ma is the corresponding author. The first two\nauthors contributed equally to this work.\ncirculated meme falsely claimed that the mRNA\nvaccine would alter human genetic code (DNA)1.\nSuch multimodal disinformation spread caused vac-\ncine safety and effectiveness concerns, hindering\nthe formation of strong immune defenses in im-\npacted areas globally (Basch et al., 2021; Lin et al.,\n2022). Besides, another meme example shown in\nFigure 1(b) perpetuates harmful stereotypes and\ngeneralizations about Asians. Therefore, it is nec-\nessary to develop automatic approaches to facilitate\nharmful meme detection for unveiling the dark side\nof memes.\nHarmful memes2 are generally defined as “mul-\ntimodal units consisting of an image and accom-\npanying text that has the potential to cause harm\nto an individual, an organization, a community, or\nthe whole society” (Sharma et al., 2022). Previ-\nous studies (Kiela et al., 2020; Pramanick et al.,\n2021a,b) attempted to straightforwardly utilize pre-\ntrained vision-language models (Li et al., 2019;\nLu et al., 2019) for harmful meme detection by\ntraining additional task-specific classification lay-\ners. More recently, Cao et al. (2022) proposed a\nprompt-tuning method with the meme text and im-\nage caption as the prompt for masked language\nmodeling (Devlin et al., 2019; Liu et al., 2019).\nHowever, existing harmful meme detection ap-\nproaches oversimplified the problem as an end-to-\nend classification paradigm, which only recognizes\nthe superficial signals conveyed through the sur-\nface text and image. But more in-depth investi-\ngation and cognition on the implicit meaning is\nrequired especially when the image and text are\nnot obviously correlated (Pramanick et al., 2021b).\nIntuitively, the key to harmful meme detection is\nto excavate rich correlations beneath the surface\n1https://www.bbc.com/news/55101238\n2Disclaimer: This paper contains discriminatory content\nthat may be disturbing to some readers, where meme examples\nand words are offensive or hateful in nature. These contents\nare provided for illustrative purposes only and do not repre-\nsent the views and standpoints of the authors.\n9114\n(a) Harmful\n (b) Harmful\n (c) Harmless\nFigure 1: Examples of harmful and harmless memes. Meme text: (a) Chance a virus with a 99.97% recovery rate;\nAlter my DNA from an experimental vaccine, with NO liability, from a corrupt industry.(b) when you date an asian\nboy and you trynna get his family to accept you. (c) you either die a hero, or live long enough to become the villain.\nof the seemly uncorrelated text and image in the\nmeme: 1) For example as in Figure 1(b), the image\nand the text are not harmful when considered in\nisolation, but are harmful when taken as a whole.\nA human checker should cognize that, the “biting”\naction in the image of a young woman with her\npet dog ridicules Asians’ “dog-eating” behavior,\nwhich corresponds to the “asian” word in the text.\n2) In contrast, some harmful signals (e.g., “die” or\n“villain”) are observed in the text of Figure 1(c), but\nthe meme itself actually does not promote hate or\ndiscrimination against a particular group of people.\nBecause the text is a quote from a popular movie\nand is often used as a philosophical statement about\nthe choices people make in life. And the image fur-\nther adds a celebratory and joyful tone to the overall\nmessage. In comparison, conventional detection\nmethods just focused on recognizing shallow harm-\nindicative signals without such multimodal reason-\ning and essential background knowledge considera-\ntion, so the social dynamics of different races or the\norigin of the meme text from classical movie lines\nmay not be well-cognized. Unlike such recognition-\nlevel detection models, we argue that establishing\nreasonable thought between textual and visual in-\nformation can further improve meme understand-\ning with background knowledge for better harmful\nmeme detection.\nInspired by the success of LLMs for reason-\ning at the cognition level with contextual back-\nground knowledge (Wei et al., 2022; Kojima et al.,\n2022; Zhang et al., 2022), we propose a novel ap-\nproach: MR.HARM , by leveraging theMultimodal\nreasoning knowledge distilled from LLMs for\nHarmful meme detection. To this end, we first\nprompt LLMs for abductive reasoning, and then\npropose a two-stage generative framework based\non smaller language models to learn reasonable\nthoughts from LLMs for better multimodal fusion\nand lightweight fine-tuning. More specifically, we\nincorporate the meme text and image into a two-\nstage training paradigm: 1) Reasoning Distillation:\nIn the first stage, we fine-tune our smaller language\nmodels with the interaction of language and vision\nfeatures to distill multimodal reasoning knowledge\nfrom LLMs, which empowers our framework with\nthe ability to conduct cognitive reasoning for the\nharmfulness prediction. 2) Harmfulness Inference:\nIn the second stage, we exploit the fine-tuned small\nlanguage models to infer the final harmfulness pre-\ndiction. In this manner, we augment the harmful\nmeme detection model with multimodal reasoning\nknowledge to unmask the implicit meaning hidden\nin holistic multimodal information from memes.\nWe evaluate our proposed approach based on\nthree public meme datasets. The results not only\nshow that our method outperforms strong harmful\nmeme detection baselines by a large margin, but\nalso provide fine-grained analysis for interpreting\nhow our approach works. Our contributions are\nsummarized as follows in three folds:\n• To our best knowledge, we are the first to al-\nleviate the issue of superficial understanding\nfor harmful meme detection by explicitly uti-\nlizing commonsense knowledge, from a fresh\nperspective on harnessing advanced LLMs.3\n• We propose a novel generative framework to\nfine-tune smaller language models augmented\nwith the multimodal reasoning knowledge dis-\ntilled from LLMs, which facilitates better mul-\ntimodal fusion and lightweight fine-tuning for\nharmfulness prediction.\n• Extensive ablations on three meme datasets\nconfirm that our method could yield superior\n3Our code is available at https://github.com/\nHKBUNLP/Mr.Harm-EMNLP2023\n9115\nperformance than state-of-the-art baselines for\nthe harmful meme detection task.\n2 Related Work\n2.1 Harmful Meme Detection\nHarmful meme detection is a rapidly growing area\nin the research community, driven by the recent\navailability of large meme benchmarks (Kiela et al.,\n2019; Suryawanshi et al., 2020; Pramanick et al.,\n2021a). The Hateful Memes Challenge organized\nby Facebook (Kiela et al., 2020) further encour-\naged researchers to develop solutions for detecting\nharmful memes in hate speech (Das et al., 2020).\nMore recently, Pramanick et al. (2021a) firstly de-\nfined the harmful meme concept and demonstrated\nits dependence on contextual factors. The com-\nplex nature of memes, which often rely on multiple\nmodalities, makes them challenging and struggle to\nyield good performance only using unimodal detec-\ntion methods (Simonyan and Zisserman, 2014; He\net al., 2016; Devlin et al., 2019). Therefore, recent\nstudies in this area attempted to apply multimodal\napproaches on the harmful meme detection task.\nPrevious studies have employed classical two-\nstream models that integrate text and vision fea-\ntures, which are learned from text and image en-\ncoders, using attention-based mechanisms and mul-\ntimodal fusion techniques for classifying harmful\nmemes (Kiela et al., 2019, 2020; Suryawanshi et al.,\n2020). Another branch was to fine-tune pre-trained\nmultimodal models specifically for the task (Lippe\net al., 2020; Muennighoff, 2020; Velioglu and Rose,\n2020; Hee et al., 2022). Recent related efforts have\nalso sought to explore the use of data augmentation\ntechniques (Zhou et al., 2021; Zhu et al., 2022),\nensemble methods (Zhu, 2020; Velioglu and Rose,\n2020; Sandulescu, 2020) and harmful target dis-\nentanglement (Lee et al., 2021). More recently,\nPramanick et al. (2021b) proposed a multimodal\nframework by using global and local perspectives\nto detect harmful memes which achieves state-of-\nthe-art performances. A follow-up prompt-based\napproach (Cao et al., 2022) attempted to concate-\nnate the meme text and extracted image captions\nto fine-tune masked language models (Liu et al.,\n2019) for harmful meme detection. However, ex-\nisting solutions only capture the superficial signals\nof different modalities in memes in an end-to-end\nmanner, which largely ignore explicit deductive rea-\nsoning to guide the model for understanding back-\nground knowledge about the complex and diverse\nrelations between the visual and textual elements.\n2.2 Large Language Models\nRecently, LLMs have demonstrated remarkable ca-\npability in complex reasoning (Brown et al., 2020;\nThoppilan et al., 2022; Rae et al., 2021; Chowdh-\nery et al., 2022), such as generating intermediate\ninference procedures before the final output (Nye\net al., 2021; Wei et al., 2022; Kojima et al., 2022;\nZhang et al., 2022). Unfortunately, the large size of\nLLMs restricts their deployment on detecting harm-\nful memes with different modalities, regardless of\nhow they are enhanced with strategetic text prompt-\ning. Knowledge distillation has been successfully\nused to transfer knowledge from larger, more com-\npetent teacher models into smaller student models\naffordable for practical applications (Buciluˇa et al.,\n2006; Hinton et al., 2015; Beyer et al., 2022). How-\never, existing researches on knowledge distillation\nfrom LLMs (Wang et al., 2022; Ho et al., 2022;\nMagister et al., 2022) only consider the language\nmodality, they are not suitable for harmful meme\ndetection because harmful memes can convey holis-\ntic synergy information through multimodal fea-\ntures. In this work, we conduct abductive reasoning\nwith LLMs, which further advocates a multimodal\nreasoning paradigm to fine-tune smaller language\nmodels (LMs) for harmful meme detection.\n3 Our Approach\nProblem Statement We define a harmful meme\ndetection dataset as a set of memes where each\nmeme M = {y, I, T} is a triplet represent-\ning an image I that is associated with a text\nT, and a ground-truth harmfulness label y ∈\n{harmful, harmless}. In this work, to investigate\nmultimodal reasoning distilled from LLMs, we con-\nvert the harmful meme detection task into a natural\nlanguage generation paradigm, where our model\ntakes the text T and image Ias the input and gen-\nerates a text sequence that contains the label y to\nclearly express whether the meme is harmful.\nOur core idea is to reason and evolve with the\ncognition-level rationale beyond the recognition-\nlevel perception (Davis and Marcus, 2015) by cap-\nturing the inter-relationship between visual and\ntextual elements in memes. The overview of our\nframework is shown in Figure 2, which consists of\nabductive reasoning with LLMs (see Sec. 3.1) and\ntwo training stages, i.e., reasoning distillation (see\nSec. 3.2) and harmfulness inference (see Sec. 3.3).\n9116\nTrm Trm Trm\nTrm Trm TrmLM Decoder\nTrm Trm Trm\nTrm Trm Trm LM Encoder Trm Trm Trm\nTrm Trm Trm\nTrm Trm Trm\nTrm Trm Trm\nLLM\nGiven a text: my black boy friend, which is\nembedded in an image: a woman holds a baby\ngorilla, please provide a rationale for how the meme\nis reasoned as the harmfulness label: harmful \nRationale: \nThe text could be seen as objectifying or reducing a person to their race. While the image of a\nwoman holding a baby gorilla could be interpreted as a comparison between the black boyfriend\nand an animal, reinforcing harmful stereotypes about race. The potential for the overall message\nof the meme to spread harmful or offensive content about race and relationships. \n Vision Extractor\nFusion Fusion\n Vision Extractor\nDistillation The meme is harmful\nThe First Fine-tuning Stage The Second Fine-tuning Stage\nAbductive Reasoning with LLMs\nLM Encoder\nLM Decoder\nFigure 2: The overall pipeline of our method. We first conduct abductive reasoning with LLMs to extract harmfulness\nrationales (pink) by the prompt consisting of the meme text (green), the image caption (blue), and the label (orange).\nWe then use the generated rationales to train small task-specific models with multimodal inputs as the first fine-tuning\nstage and feed the same inputs to the updated model for harmfulness inference as the second fine-tuning stage.\n3.1 Abductive Reasoning with LLMs\nIn this paper, we propose to utilize abductive rea-\nsoning with multimodal inputs to train smaller\ndownstream models. LLMs can produce natural\nlanguage rationales unveiling the implicit meaning\nbeneath the surface of the memes to justify the rea-\nson why the meme is harmful or not. This shares\na similar intuition as heuristic teaching (Pintrich\nand Schunk, 2002) where a teacher who has rich\nexperience and knowledge can impart to students\nthe correct way of thinking and reasoning based\non questions with corresponding answers. The stu-\ndents then learn how to deduce their own ways\nto the correct answers from questions accordingly.\nThus we aim to activate explicit reasoning knowl-\nedge in LLMs as a teacher model, e.g., contextual\nand cultural information related to memes, to guide\nour model to strengthen harmfulness prediction.\nGiven a meme sample M = {y, I, T} from the\ntraining data, to prompt large language models in\nuniform language modality, we first extract the text\ncaption ˜Iof the image Iby off-the-shelf caption-\ning models (Mokady et al., 2021). Then we curate a\ntemplate p that consists of a triplet {y, ˜I, T}as ob-\nserved attributes, to prompt the LLMs to generate\na rationale r that elicits the reasoning knowledge\nabout how to infer the harmfulness label y based\non the interplay of the meme text T and the image\ncaption ˜Ias illustrated in Figure 2. Specifically,\nwe design p as:\n“Given a Text: [ T], which is embedded in an\nImage: [ ˜I]; and a harmfulness label [ y], please\ngive me a streamlined rationale associated with the\nmeme, without explicitly indicating the label, for\nhow it is reasoned as [y].”\nAs we clarify the ground-truth harmfulness label\nin the observed attributes of the prompt, the hallu-\ncination issue (Bang et al., 2023) of LLMs could\nbe effectively alleviated. Because the rich contex-\ntual background knowledge could be activated by\nabductive reasoning based on the ground truth and\ninvalid rationales are naturally filtered out.\n3.2 Reasoning Distillation with Small LMs\nSince we utilize image captions to represent the\nmeme images, we could perform abductive reason-\ning with large language models pre-trained with\nlanguage modality. However, only using the cap-\ntions as opposed to original vision features may\nsuffer from a lack of mutual synergy in the repre-\nsentation space of different modalities in memes\ndue to the inductive bias of possible information\nloss in the captioning process. On the other hand,\nLLMs can be used to conduct abductive reasoning\nonly for the training data whose harmfulness label\nis given in prior but is challenging to be fine-tuned\nfor this task due to the huge amount of model pa-\nrameters. To facilitate the interactions between the\nmeme text and the image, we propose to fine-tune\na smaller language model for the harmful meme\ndetection task, which allows flexibility in adjust-\ning model architectures to incorporate multimodal\nfeatures and is more lightweight for task-specific\nfine-tuning.\nIn this section, we train a small language model\nas a student model distilled from the LLMs with\nmultimodal reasoning knowledge. Specifically, we\nleverage generated rationales from LLMs as in-\nformative supervision, to fine-tune a smaller pre-\ntrained language model to excavate the rich inter-\nrelationship between language and vision modali-\n9117\nties of memes.\nEncoding For a meme sample M from the train-\ning data, we first encode the text T and the image\nIto obtain their embedding vectors as follows:\nH0\nT = TE(T),\nHI= VE(I), (1)\nwhere TE(·) denotes the text embedding layer of\nthe LM Encoder. And H0\nT ∈Rm×d is the token\nembeddings in the Transformer encoder (Vaswani\net al., 2017) where m is the text token length and d\nis the dimension of the hidden states. VE(·) is the\nVision Extractor implemented as frozen pre-trained\nvision Transformers (Radford et al., 2021) to fetch\nthe patch-level features of the image withn patches,\nwhich is projected into the visual representations\nHI∈Rn×d. Next, to support semantic alignment\nbetween the text and the image for better context\nunderstanding, we exploit a cross-attention mech-\nanism (Luo et al., 2022) for multimodal fusion of\nthe textual and visual information:\nQT = Wi\nQHi\nT + bi\nQ,\nKI= Wi\nKHI+ bi\nK,\nVI= Wi\nV HI+ bi\nV ,\nHi\nI= softmax\n( QTK⊤\nI√dk\n)\nVI,\n(2)\nwhere Hi\nT is the input hidden states of each LM\nEncoder layer and Hi\nI is the attended visual fea-\ntures. Then we can fuse Hi\nIwith Hi\nT to attain the\ninterplay representations for a meme:\nHi+1\nT = LMEi (\nHi\nT\n)\n+ Wi\nOHi\nI+ bi\nO, (3)\nwhere LMEi(·) is the i-th layer of the LM Encoder,\nWi\n∗denotes the linear projection, bi\n∗is the bias, and\nˆH = HL\nT is the final interplay representations after\ngoing through an L-layer LM Encoder fused with\nthe visual features.\nDecoding Finally, we feed the interplay represen-\ntations ˆH ∈Rm×d into the LM Decoder, imple-\nmented as a Transformer-based decoder, to gener-\nate the reasonable rationale. Overall, the smaller\nlanguage model f is trained by minimizing the\nfollowing distillation loss:\nLdistill = CE (f(I, T), r) , (4)\nwhere CE(·) denotes the cross-entropy\nloss (Sutskever et al., 2014) between the predicted\ntext and the target rationale r generated by LLMs.\nIn this way, multimodal reasoning knowledge\nabout the meme could be explicitly distilled from\nLLMs and injected into the smaller language\nmodel specific to harmful meme detection.\n3.3 Harmfulness Inference\nDuring the first fine-tuning stage, we conducted\nexplicit deductive reasoning to empower our model\nwith the capability of multimodal reasoning dis-\ntilled from LLMs. As the goal of this task is to\ndetermine whether the meme is harmful or not,\nwe conduct the second fine-tuning stage for Harm-\nfulness Inference, which shares the same model\narchitecture, parameters, and encoding procedure\nas Sec. 3.2 but differs in the decoding output. To\nmake the output consistent with harmfulness pre-\ndiction, the smaller model f is further trained by\nminimizing the following inference loss:\nLinfer = CE (f(I, T), y) , (5)\nwhere the cross-entropy loss is computed between\nthe generated text and ground-truth harmfulness\nlabel y. With the generative objective (Raffel et al.,\n2020) adapted to the previous Reasoning Distilla-\ntion stage, the prior reasoning knowledge absorbed\nin Reasoning Distillation could be well induced for\nHarmfulness Inference.\nModel Training The model training consists of\ntwo fine-tuning stages: 1) Reasoning Distillation\nand 2) Harmfulness Inference, where Reasoning\nDistillation is the predecessor fine-tuning phase of\nHarmfulness Inference. Note that for model testing,\nwe directly input the test sample into our fine-tuned\nlanguage model to predict the meme harmfulness.\n4 Experiments\n4.1 Experimental Setup\nDatasets We use three publicly available meme\ndatasets for evaluation: (1) Harm-C (Pramanick\net al., 2021a), (2) Harm-P (Pramanick et al., 2021b),\nand (3) FHM (Kiela et al., 2020). Harm-C and\nHarm-P consist of memes related to COVID-19\nand US politics, respectively. FHM was released\nby Facebook as part of a challenge to crowd-source\nmultimodal harmful meme detection in hate speech\nsolutions. Different from FHM that each meme\nwas labeled as harmful or harmless, Harm-C and\nHarm-P were originally labeled with three classes:\nvery harmful, partially harmful, and harmless. For\na fair comparison, we merge the very harmful and\npartially harmful memes into harmful ones, follow-\ning the evaluation setting of recent work (Praman-\nick et al., 2021b; Cao et al., 2022). We provide\n9118\nDataset Harm-C Harm-P FHM\nModel Accuracy Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1\nText BERT (Devlin et al., 2019) 70.17 66.25 80.12 78.35 57.12 41.52\nImage-Region (He et al., 2016) 68.74 62.97 73.14 72.77 52.34 34.19\nLate Fusion (Pramanick et al., 2021a)73.24 70.25 78.26 78.50 59.14 44.81\nMMBT (Kiela et al., 2019) 73.48 67.12 82.54 80.23 65.06 61.93\nVisualBERT COCO (Li et al., 2019) 81.36 80.13 86.80 86.07 61.48 47.26\nViLBERT CC (Lu et al., 2019) 78.70 78.09 87.25 86.03 64.70 55.78\nMOMENTA (Pramanick et al., 2021b)83.82 82.80 89.84 88.26 61.34 57.45\nMaskPrompt (Cao et al., 2022) 84.47 81.51 88.17 87.09 72.98 65.24\nMR.HARM 86.16 85.43 89.58 89.57 75.40 75.10\nTable 1: Harmful meme detection results on three datasets. The accuracy and macro-averaged F1 score (%) are\nreported as the metrics. The best and second results are in bold and underlined.\nstatistics of the three datasets in the Appendix.\nBaselines We compare MR.HARM with sev-\neral state-of-the-art harmful meme detection sys-\ntems: 1) Text BERT (Devlin et al., 2019); 2)\nImage-Region (Ren et al., 2016; He et al., 2016);\n3) Late Fusion (Pramanick et al., 2021a); 4)\nMMBT (Kiela et al., 2019); 5) VisualBERT\nCOCO (Li et al., 2019; Lin et al., 2014); 6) ViL-\nBERT CC(Lu et al., 2019; Sharma et al., 2018);\n7) MOMENTA (Pramanick et al., 2021b); 8)\nMaskPrompt (Cao et al., 2022). We use the accu-\nracy and macro-averaged F1 score as the evaluation\nmetrics. More implementation details and baseline\ndescriptions are provided in Appendix.\n4.2 Harmful Meme Detection Performance\nTable 1 shows the performance of our proposed\nmethod versus all the compared methods on the\nHarm-C, Harm-P and FHM datasets. It is observed\nthat 1) The performance of the baselines in the first\ngroup is obviously poor due to only unimodal fea-\ntures like text-only or image-only being captured.\nIn comparison, the other baselines exploit the mul-\ntimodal features from both the text and image in\nmemes. 2) The multimodal models in the second\ngroup outperform the unimodal ones. The early-\nfusion models with multimodal pre-training ( i.e.,\nVisualBERT COCO and ViLBERT CC) outper-\nform that of the simple fusion with unimodal pre-\ntraining (i.e., Late Fusion and MMBT) on Harm-\nC/P datasets, while MOMENTA performs best in\nthe second group by considering global and local\ninformation of memes. 3) However, as the images\nin FHM dataset are more informative and high-\nquality, MaskPrompt yields the best performance\namong all the baselines by incorporating additional\nextracted entities and demographic information of\nthe image into the masked language models, be-\nsides just captioning the image into the prompt.\nOur proposed MR.HARM improves over the\nbest baselines by 2.63%, 1.31%, and 9.86% in\nterms of Macro-F1 score on Harm-C, Harm-P, and\nFHM datasets, respectively. We observe that 1) the\nimprovement on the Harm-P dataset is relatively\nmilder than that on the other two datasets. Mean-\nwhile, all the baselines just have tiny differences\namong their performances on Harm-P. We specu-\nlate the reason falls into the smaller dataset scale of\nHarm-P which only contains politics-related harm-\nful memes. 2) A similar trend can also be observed\nin Harm-C and FHM datasets: the more challeng-\ning the dataset is, the greater performance improve-\nment MR.HARM achieves. Our model performs\nflexibly and stably across all datasets with its keen\njudgment on harmful memes. This is because all\nthe baselines are only designed at the recognition\nlevel, but MR.HARM is further empowered with\nmultimodal reasoning knowledge distilled from\nLLMs to unearth harmful content from the seemly\nuncorrelated text and image modalities of memes.\n4.3 Ablative Study\nWe perform ablative studies on several variants of\nMR.HARM : 1) w/o Reasoning Distillation: Sim-\nply fine-tune the smaller language models in the\nstage of Harmfulness Inference without the stage of\nReasoning Distillation based on LLMs; 2) w/o Vi-\nsual Features: Discard the features from the meme\nimage while keeping those from the meme text;\n3) w/o Multimodal Fusion: Instead of the fusion\nmechanism on the multimodal features in our lan-\nguage model, we only append the lingual features\nfrom image captioning together with the meme text\n9119\nDataset Harm-C Harm-P FHM\nModel Accuracy Macro- F1 Accuracy Macro- F1 Accuracy Macro- F1\nMR.HARM 86.16 85.43 89.58 89.57 75.40 75.10\nw/o Reasoning Distillation 83.33 81.44 88.17 88.17 73.60 73.41\nw/o Visual Features 82.48 80.30 87.04 87.03 58.80 57.01\nw/o Multimodal Fusion 79.38 75.36 87.46 87.45 74.40 74.25\nw/o Two-stage Training 83.05 81.45 63.32 63.32 67.40 65.77\nw/o Fine-tuning Small LMs 71.75 66.86 61.13 60.27 60.00 57.72\nTable 2: Ablation studies on our proposed framework.\nduring encoding; 4) w/o Two-stage Training: Con-\ncatenate the rationales generated from LLMs and\ngolden harmfulness label as the target for model\ntraining, to replace the two-stage training paradigm;\n5) w/o Fine-tuning Small LMs : Directly prompt\nthe representative large language model ChatGPT\nbased on InstructGPT (Ouyang et al., 2022) for\nharmful meme detection.\nAs demonstrated in Table 2, the ablative models\nsuffer different degrees of performance degrada-\ntion, indicating the effectiveness of our proposed\ncomponents for harmful meme detection with mul-\ntimodal reasoning distilled from LLMs. Specifi-\ncally, the performance of MR.HARM significantly\ndecreases in the ‘w/o Reasoning Distillation’ set-\nting due to the lack of multimodal reasoning knowl-\nedge transferred from LLMs about the seemly un-\ncorrelated modalities in memes. The ‘ w/o Visual\nFeatures’ setting also achieves worse performance\nthan MR.HARM , suggesting that the visual repre-\nsentations are complementary to the meme text for\nharm-indicative pattern extraction in the language\nmodel. MR.HARM makes improvements over ‘w/o\nMultimodal Fusion ’, which implies the promot-\ning role of our fusion mechanism that incorporates\noriginal vision features into the language model,\nhardly compromised when there could be severe\ninformation loss in the captioning process. More-\nover, the ‘ w/o Two-stage Training’ setting leads\nto large-margin performance degradation, which\nverifies the effectiveness of our two-stage training\nparadigm. This is because this setting causes mu-\ntual interference between intermediate reasoning\nand final prediction, which affects the convergence\neffect of harmfulness inference and damages the\nmodel’s performance and stability. Compared with\nMR.HARM , the performance of ‘w/o Fine-tuning\nSmall LMs’ also significantly decreases, highlight-\ning the importance of abductive reasoning with\nLLMs to alleviate the hallucination issue during\nIntermediate Reasoning: \nthe meme contains a political\nmessage that promotes a political\nagenda. the text suggests that the\ncoronavirus is nothing compared to\nwindmill cancer, which is a harmful\nand offensive message. the image of\na politician in a suit and tie, possibly\nrelated to the inauguration of\npresident trump, further reinforces\nthe harmful message. the use of a\npolitical message in this context\ncould be seen as insensitive and\ndisrespectful to those who have been\naffected by the pandemic. \nMeme text: DON'T WORRY,\nAMERICA. THE CORONAVIRUS IS\nNOTHING COMPARED TO\nWINDMILL CANCER!\nIntermediate Reasoning: \nthe text in the meme contains a\nderogatory and offensive statement\n\"goat humper\" towards a particular\ngroup of Muslim individuals,\nassociated with an image of a Muslim\nman praying in front of a mosque,\nwhich is highly offensive and\ndiscriminatory. The phrase \"looks like\na bunch of pigs just walked over your\ngrave\" can be interpreted as a\ndisrespectful reference to the Muslim\npractice of abstaining from pork. \nMeme text: it's the goat humper,\nlooks like a bunch of pigs just\nwalked over your grave\nIntermediate Reasoning: \nbased on the text and image it\nappears that the meme is making an\ninflammatory statement about the\ndemocrat party. the image shows a\nclose up of a confederate flag with\nstars, which could be interpreted as\nan implication of the democrat party.\nthe text also mentions the oldest haté\ngroup in the united states, which\ncould be interpreted as a reference to\nthe democrat party being associated\nwith racism and white supremacy. \nMeme text: THE DEMOCRAT PARTY\nAMERIÇAS OLDEST HATÉ GROUP\n(a)\n(b)\n(c)\nFigure 3: Examples of correctly predicted harmful\nmemes in (a) Harm-C, (b) Harm-P, and (c) FHM dataset.\ndeductive reasoning for harmfulness prediction.\n4.4 Cognition-view Reasoning Analysis\nNote that our smaller language model is explicitly\ntrained in the Reasoning Distillation stage for ra-\ntionale generation to distill multimodal reasoning\nknowledge from LLMs. Although intermediate\n9120\nreasoning is not the final target output for harmful\nmeme detection, after the first fine-tuning stage, we\nelicit reasonable thoughts from our smaller lan-\nguage model with the test samples as input, to\nunderstand the cognition view of our proposed\nMR.HARM on the test meme samples more trans-\nparently and intuitively, as exemplified in Figure 3.\nFrom the visualized intermediate reasoning, we\nobserve that 1) our model could understand the\nmultimodal information related to the meme text\n(in green) and image (in blue) with commonsense\nknowledge. For example, in Figure 3(a), the rec-\nognized “politician” in the image could be related\nto “president trump”, which could be linked to\nthe “AMERICA” in the text; in Figure 3(b), the\nrecognized “flag” in the image could be cognized\nto satire “the democrat party” in the text; and in\nterms of Figure 3(c), the “goat humper” and “pigs”\nin the text could be associated with the attacks\nto “a Muslim man” recognized in the image. 2)\nFurthermore, our model learns to cognize the in-\nterplay (in pink) of multimodal information with\nadvanced reasoning. Benefitting from the rich mul-\ntimodal understanding of the memes, the perpet-\nuates harmful stereotypes could be reasoned over\nthe context to the target like “who affected by the\npandemic” in Figure 3(a), “the democrat party” in\nFigure 3(b), and “the Muslim” in Figure 3(c). In\nthis way, the rich correlation beneath the surface of\nthe meme text and image could be excavated to fa-\ncilitate harmfulness inference with better reasoning\nknowledge by harnessing advanced LLMs. Such\nreadable pieces of rationales are also potentially\nvaluable for aiding human checkers to verify the\nfinal answer predicted by our model.\n4.5 Error Analysis\nTo better understand the behavior of our model\nand facilitate future studies, we conduct an error\nanalysis on the wrongly predicted memes by our\nproposed framework. We found that the major er-\nror exists in that our framework still cannot fully\nrecognize the images that require rich background\nknowledge though we exploited the advanced cross-\nattention mechanism by incorporating visual fea-\ntures into the language model. Figure 4 shows\ntwo examples of memes wrongly classified by\nMR.HARM . For the harmful meme in Figure 4(a),\nthe phrase “and for my next class project!” suggests\nthat the image is being used for an academic or ed-\nucational purpose, which can be seen as glorifying\nIntermediate Reasoning: \nbased on the image and text\npresented in the meme, it appears to\nbe a harmless and relatable scenario.\nthe image shows a group of women\nin traditional dress walking on a\nbeach, which is a common and\nharmless activity. the text, \"and for\nmy next class project,\" seems to be a\nplayful and lighthearted comment\nabout the topic of the class project.\nthere is no indication of any harmful\nor offensive content in the image or\ntext.\nMeme text: and for my next class\nproject!\nIntermediate Reasoning: \nthe text in the meme contains a\nderogatory and offensive statement\nabout a politician, jimmy carter, who\ncreated the department of education\nin 1979. the use of such language\nand imagery can be considered\nharmful and offensive to individuals\nwho identify with the politician.\nadditionally, the image of an older\nman with a funny expression on his\nface can be seen as promoting a\nnegative and harmful attitude\ntowards the politician.\nMeme text: in 1979 jimmy carter\ncreated the department of education\nsince then the u.s. has gone from 1st\nto 17th in education\n(a)\n(b)\nFigure 4: Examples of wrongly predicted memes by our\nproposed framework with the ground truth (a) harmful\nand (b) harmless.\nor normalizing the behavior depicted in the im-\nage. The image features “a group of Ku Klux Klan\nmembers walking on a beach”, which is a symbol\nof white supremacy and racism. The combination\nof the phrase in the text and the use of imagery\nassociated with hate groups can contribute to the\nglorification of harmful behaviors and the perpet-\nuation of negative stereotypes, which makes the\nmeme harmful. However, due to the lack of related\nbackground knowledge about the Ku Klux Klan\nmembers and their wear, our framework cannot\nwell recognize the image correctly during the origi-\nnal vision feature extraction, which leads to error\npropagation for wrongly concluding that the meme\nis harmless. Also, in terms of the harmless meme\nin Figure 4(b), the image that “Jimmy Carter with\na smile on his face” is mistakenly recognized as\n“an older man with a funny expression on his face”,\nfurthermore, the model hallucinates that the meme\ntext “can be considered harmful and offensive to\nindividuals who identify with the politician”, result-\ning in the wrong prediction that the meme is harm-\nful. Therefore, it is possible to improve MR.HARM\nby incorporating more informative vision features\nand improving language-vision interaction to be\ncapable of understanding the images with more\ncomplex background knowledge.\n9121\nVersion Harm-C Harm-P FHM\nSmall 84.99 85.33 72.96\nBase 85.43 89.57 75.10\nLarge 84.02 90.14 77.80\nTable 3: Macro-averaged F1 score (%) achieved with\ndifferent versions of our fine-tuned LMs.\n4.6 Discussion\nAs our two-stage training paradigm requires dis-\ntilling the reasoning knowledge and leveraging\noriginal vision features, we utilize the T5 encoder-\ndecoder architecture (Raffel et al., 2020; Chung\net al., 2022) to initialize our generative framework.\nTo test the generality of the benefits of our approach\nto different versions of the backbone, we alter the\nunderlying LMs to other variants in different sizes.\nAs shown in Table 3, one interesting phenomenon\nis that our model has already achieved outstand-\ning performance on the three benchmarks with\nthe Small (about 60M parameters) or Base ((about\n220M parameters)) version as the backbone, which\nhas a smaller size than the state-of-the-art baseline\nMaskPrompt (over 300M parameters). The Large\nversion of our backbone generally achieved better\nperformance than the other two backbone versions\nbecause the larger the fine-tuned LMs, the more it\nalleviates the hallucination issue (Ji et al., 2023).\nOverall, the results show that our framework does\nnot rely excessively on the size of the backbone\nto improve performance and is generally effective\nwith different versions of the backbone model.\n5 Conclusion and Future Work\nIn this paper, we propose to capture implicit mean-\ning that is not explicitly conveyed through the sur-\nface of the text and image in memes for harmful\nmeme detection. We first conduct abductive rea-\nsoning with LLMs. Then we present a novel gen-\nerative framework to distill multimodal reasoning\nknowledge from LLMs, which includes two train-\ning stages: 1) reasoning distillation and 2) harmful-\nness inference. Results on three meme benchmarks\nconfirm the advantages of our proposed framework.\nFor future work, since it is harder to judge the\nquality of the intermediate reasoning, where the\nevaluation is necessarily qualitative, we plan to do\nsome sort of systematic study towards explainable\nharmful meme detection to claim explainability\nthrough a human subjects study for evaluation.\nLimitations\nThere are multiple ways to further improve this\nwork:\n• Despite this work focusing on performance\nimprovement of harmful meme detection, it is\nharder to judge the quality of the intermediate\nreasoning, where the evaluation is necessarily\nqualitative. Considering that our framework\ncould generate readable snippets for cognition-\nview reasoning, we plan to do some sort of\nsystematic study to claim explainability for\nthe evaluation, which would be another more\ntargeted research.\n• New benchmarks to evaluate the reasoning\nability of our framework are demanded. We\nare going to further exploit LLMs toward ex-\nplainable harmful meme detection from the\nperspectives like dataset construction and au-\ntomatic evaluation.\n• We only use the textual prompt to conduct ab-\nductive reasoning with accessible LLMs pre-\ntrained with the language modality. We would\nfurther update our framework by leveraging\nvisual LLMs if accessible in the future to im-\nprove the visual feature extraction for explor-\ning better multimodal reasoning knowledge\ndistillation, and avoid several common defi-\nciencies of existing language models includ-\ning hallucination and limited generalization\nas much as possible.\nAcknowledgements\nThis work was partially supported by Hong Kong\nRGC ECS (Ref. 22200722) and National Natu-\nral Science Foundation of China Young Scientists\nFund(No. 62206233).\nBroader Impact\nThe purpose of this work is to prevent the spread of\nharmful meme information and to ensure that peo-\nple are not subjected to prejudice, racial and gender\ndiscrimination. Nevertheless, we are aware of the\npotential for malicious users to reverse-engineer\nand create memes that go undetected or misunder-\nstood by MR.HARM -trained AI systems. This is\nstrongly discouraged and condemned. Intervention\nwith human moderation would be required in order\nto ensure that this does not occur.\n9122\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nCorey H Basch, Zoe Meleo-Erwin, Joseph Fera, Christie\nJaime, and Charles E Basch. 2021. A global pan-\ndemic in the time of viral memes: Covid-19 vaccine\nmisinformation and disinformation on tiktok. Human\nvaccines & immunotherapeutics, 17(8):2373–2377.\nLucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Mar-\nkeeva, Rohan Anil, and Alexander Kolesnikov. 2022.\nKnowledge distillation: A good teacher is patient and\nconsistent. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 10925–10934.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Proceedings of the 34th International\nConference on Neural Information Processing Sys-\ntems, pages 1877–1901.\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the 12th ACM SIGKDD international\nconference on Knowledge discovery and data mining,\npages 535–541.\nRui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and Jing\nJiang. 2022. Prompting for multimodal hateful meme\nclassification. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 321–332.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAbhishek Das, Japsimar Singh Wahi, and Siyao Li.\n2020. Detecting hate speech in multi-modal memes.\narXiv preprint arXiv:2012.14891.\nErnest Davis and Gary Marcus. 2015. Commonsense\nreasoning and commonsense knowledge in artificial\nintelligence. Communications of the ACM, 58(9):92–\n103.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, pages 4171–\n4186.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nMing Shan Hee, Roy Ka-Wei Lee, and Wen-Haw Chong.\n2022. On explaining multimodal hateful meme de-\ntection models. In Proceedings of the ACM Web\nConference 2022, pages 3651–3655.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nDouwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan\nPerez, and Davide Testuggine. 2019. Supervised\nmultimodal bitransformers for classifying images and\ntext. arXiv preprint arXiv:1909.02950.\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj\nGoswami, Amanpreet Singh, Pratik Ringshia, and\nDavide Testuggine. 2020. The hateful memes chal-\nlenge: detecting hate speech in multimodal memes.\nIn Proceedings of the 34th International Conference\non Neural Information Processing Systems , pages\n2611–2624.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C Berg, Wan-Yen\nLo, et al. 2023. Segment anything. arXiv preprint\narXiv:2304.02643.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022\nWorkshop on Knowledge Retrieval and Language\nModels.\nZhanghui Kuang, Hongbin Sun, Zhizhong Li, Xiaoyu\nYue, Tsui Hin Lin, Jianyong Chen, Huaqiang Wei,\nYiqin Zhu, Tong Gao, Wenwei Zhang, et al. 2021.\nMmocr: a comprehensive toolbox for text detection,\nrecognition and understanding. In Proceedings of the\n29th ACM International Conference on Multimedia,\npages 3791–3794.\nRoy Ka-Wei Lee, Rui Cao, Ziqing Fan, Jing Jiang, and\nWen-Haw Chong. 2021. Disentangling hate in online\nmemes. In Proceedings of the 29th ACM Interna-\ntional Conference on Multimedia, pages 5138–5147.\n9123\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557.\nHongzhan Lin, Jing Ma, Liangliang Chen, Zhiwei Yang,\nMingfei Cheng, and Chen Guang. 2022. Detect ru-\nmors in microblog posts for low-resource domains\nvia adversarial contrastive learning. In Findings\nof the Association for Computational Linguistics:\nNAACL 2022, pages 2543–2556.\nHongzhan Lin, Jing Ma, Mingfei Cheng, Zhiwei Yang,\nLiangliang Chen, and Guang Chen. 2021. Rumor\ndetection on twitter with claim-guided hierarchical\ngraph attention networks. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10035–10047.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer.\nPhillip Lippe, Nithin Holla, Shantanu Chandra, San-\nthosh Rajamanickam, Georgios Antoniou, Ekaterina\nShutova, and Helen Yannakoudakis. 2020. A multi-\nmodal framework for the detection of hateful memes.\narXiv preprint arXiv:2012.12871.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nProceedings of the 33rd International Conference\non Neural Information Processing Systems , pages\n13–23.\nZiyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng\nZhang, and Jing Ma. 2022. I-tuning: Tuning frozen\nlanguage models with image for lightweight image\ncaptioning. ICASSP 2023 - 2023 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP).\nJing Ma and Wei Gao. 2020. Debunking rumors on\ntwitter with tree transformer. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 5455–5466.\nJing Ma, Wei Gao, Shafiq Joty, and Kam-Fai Wong.\n2020. An attention-based rumor detection model\nwith tree-structured recursive neural networks. ACM\nTransactions on Intelligent Systems and Technology\n(TIST), 11(4):1–28.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. arXiv\npreprint arXiv:2212.08410.\nRon Mokady, Amir Hertz, and Amit H Bermano. 2021.\nClipcap: Clip prefix for image captioning. arXiv\npreprint arXiv:2111.09734.\nNiklas Muennighoff. 2020. Vilio: State-of-the-art visio-\nlinguistic models applied to hateful memes. arXiv\npreprint arXiv:2012.07788.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nPaul R Pintrich and Dale H Schunk. 2002. Motivation\nin education: Theory, research, and applications .\nPrentice Hall.\nShraman Pramanick, Dimitar Dimitrov, Rituparna\nMukherjee, Shivam Sharma, Md Shad Akhtar,\nPreslav Nakov, and Tanmoy Chakraborty. 2021a. De-\ntecting harmful memes and their targets. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 2783–2796.\nShraman Pramanick, Shivam Sharma, Dimitar Dim-\nitrov, Md Shad Akhtar, Preslav Nakov, and Tanmoy\nChakraborty. 2021b. Momenta: A multimodal frame-\nwork for detecting harmful memes and their targets.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4439–4455.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International con-\nference on machine learning, pages 8748–8763.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\n9124\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2016. Faster r-cnn: Towards real-time object de-\ntection with region proposal networks. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\n39(6):1137–1149.\nVlad Sandulescu. 2020. Detecting hateful memes us-\ning a multimodal deep ensemble. arXiv preprint\narXiv:2012.13235.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565.\nShivam Sharma, Firoj Alam, Md Shad Akhtar, Dimitar\nDimitrov, Giovanni Da San Martino, Hamed Firooz,\nAlon Halevy, Fabrizio Silvestri, Preslav Nakov, and\nTanmoy Chakraborty. 2022. Detecting and under-\nstanding harmful memes: A survey. In Proceedings\nof the Thirty-First International Joint Conference on\nArtificial Intelligence, pages 5597–5606.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556.\nShardul Suryawanshi, Bharathi Raja Chakravarthi, Mi-\nhael Arcan, and Paul Buitelaar. 2020. Multimodal\nmeme dataset (multioff) for identifying offensive con-\ntent in image and text. In Proceedings of the second\nworkshop on trolling, aggression and cyberbullying,\npages 32–41.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of the 27th International Conference\non Neural Information Processing Systems-Volume 2,\npages 3104–3112.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nRiza Velioglu and Jewgeni Rose. 2020. Detecting hate\nspeech in memes using multimodal deep learning\napproaches: Prize-winning solution to hateful memes\nchallenge. arXiv preprint arXiv:2012.12975.\nPeifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,\nand Xiang Ren. 2022. Pinto: Faithful language rea-\nsoning using prompt-generated rationales. arXiv\npreprint arXiv:2211.01562.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models. arXiv preprint\narXiv:2210.03493.\nYi Zhou, Zhenhao Chen, and Huiyuan Yang. 2021. Mul-\ntimodal learning for hateful memes detection. In\n2021 IEEE International Conference on Multimedia\n& Expo Workshops (ICMEW), pages 1–6. IEEE.\nJiawen Zhu, Roy Ka-Wei Lee, and Wen Haw Chong.\n2022. Multimodal zero-shot hateful meme detection.\nIn 14th ACM Web Science Conference 2022, pages\n382–389.\nRon Zhu. 2020. Enhance multimodal transformer\nwith external label and in-domain pretrain: Hateful\nmeme challenge winning solution. arXiv preprint\narXiv:2012.08290.\n9125\nDatasets Train Test\n#harmful #harmless #harmful #harmless\nHarm-C 1064 1949 124 230\nHarm-P 1486 1534 173 182\nFHM 3050 5450 250 250\nTable 4: Statistics of Datasets.\nA Datasets\nThe detailed statistics of the three datasets are\nshown in Table 4.\nB Implementation Details\nTo separate the text and image in the memes,\nwe first in-paint the memes by combining\nMMOCR (Kuang et al., 2021) with SAM (Kirillov\net al., 2023) to extract the text and pure image.\nThen during the captioning process, since the focus\nof this work is primarily on the multimodal rea-\nsoning for harmful meme detection from a fresh\nperspective on harnessing LLMs, we apply a pre-\ntrained image captioning model ClipCap (Mokady\net al., 2021) used in recent work (Cao et al., 2022),\nto generate textual descriptions about the dominant\nobjects or events in the memes’ image, which is\nutilized as the inputs into LLMs for abductive rea-\nsoning. To generate the rationale for each meme,\nwe employed ChatGPT (Ouyang et al., 2022), a\nwidely used LLM developed by OpenAI, specifi-\ncally utilizing the “gpt-3.5-turbo” version. To make\nour results reproducible, we set the temperature as\n0 and the maximum length as 256.\nFor the system prompt to the “gpt-3.5-turbo”\nmodel, we design the message as:\n“You have been specially designed to perform\nabductive reasoning for the harmful meme detec-\ntion task. Your primary function is that, according\nto a harmfulness label about an image with a text\nembedded, please provide a streamlined rationale,\nwithout explicitly indicating the label, for how it\nis reasoned as the given harmfulness label. The\nimage and the textual content in the meme are often\nuncorrelated, but its overall semantics is presented\nholistically. Thus it is important to note that you\nare prohibited from relying on your own imagina-\ntion, as your goal is to provide the most accurate\nand reliable rationale possible so that people can\ninfer the harmfulness according to your reason-\ning about the background context and relationship\nbetween the given text and image.”.\nMoreover, to prompt the LLMs to generate rea-\nHyper-Parameter Harm-C Harm-P FHM\nFirst-Stage\nepoch 10 10 10\nbatch size 32 32 32\nLearning Rate 5e-5 5e-5 5e-5\nWarmup Step 0.1 0.1 0.1\nWarmup Strategy Linear Linear Linear\nImage Size 224 224 224\nSecond-Stage\nepoch 30 30 30\nbatch size 32 32 32\nLearning Rate 5e-5 5e-4 1e-4\nWarmup Step 0.1 0.1 0.1\nWarmup Strategy Linear Linear Linear\nImage Size 224 224 224\nTable 5: Hyper-parameters.\nsonable rationales with the triplet {y, ˜I, T} as ob-\nserved attributes, we design the template p for the\nuser prompt as:\n“Given a Text: [ T], which is embedded in an\nImage: [ ˜I]; and a harmfulness label [ y], please\ngive me a streamlined rationale associated with the\nmeme, without explicitly indicating the label, for\nhow it is reasoned as [y].”.\nOur MR.HARM model utilizes the T5 encoder-\ndecoder architecture (Raffel et al., 2020; Chung\net al., 2022) as its foundational framework, specif-\nically utilizing the “flan-t5-base” version. For the\nextraction of image features, following previous\nwork (Pramanick et al., 2021b), we adopted the\nstate-of-the-art vision Transformer known as CLIP-\nViT-B/32 (Radford et al., 2021), and this module\nremains static throughout the training process. To\neffectively integrate the multi-modal information,\nwe incorporated a simple one-head cross-attention\nmechanism in each layer of the T5 encoder. Dur-\ning the fusion process, the text features are utilized\nas the query, while the image features act as the\nkey and value. It is noteworthy that these fusion\nmodules were initialized randomly. For the fine-\ntuning phase, we provide a comprehensive list of\nthe hyper-parameters in Table 5. Results are aver-\naged over ten random runs. All experiments were\nconducted using a single V100 32GiB GPU.\nC Baselines\nWe compare our model MR.HARM with several\nstate-of-the-art harmful meme detection systems:\n9126\nDataset Harm-C Harm-P FHM\nVersion Accuracy Macro- F 1 Accuracy Macro- F 1 Accuracy Macro- F 1\nSmall 85.59 84.99 85.35 85.33 73.20 72.96\nBase 86.16 85.43 89.58 89.57 75.40 75.10\nLarge 85.03 84.02 90.14 90.14 78.20 77.80\nTable 6: The detailed results with different sizes of our fine-tuned LMs.\nModels MOMENTA MaskPrompt MR.HARM\nMultimodal Fusion✓ ✗ ✓Prompt Tuning ✗ ✓ ✓Explicit Reasoning✗ ✗ ✓Leveraging LLMs✗ ✗ ✓\nTable 7: Comparison of characteristics between our\nMR.HARM with state-of-the-art models for harmful\nmeme detection.\n1) Text BERT: BERT (Devlin et al., 2019) is uti-\nlized as the unomodal text-only model; 2) Image-\nRegion: a unimodal visual-only model that pro-\ncesses meme images using Faster R-CNN (Ren\net al., 2016) with ResNet-152 (He et al., 2016) to\nfeed into a classification layer; 3) Late Fusion:\na multimodal model uses the average prediction\nscores of BERT and ResNet-152 for harmful meme\ndetection (Pramanick et al., 2021a); 4) MMBT:\na multimodal Bi-Transformer (Kiela et al., 2019)\nthat captures the intra-modal and inter-modal dy-\nnamics of the two modalities; 5) VisualBERT\nCOCO: Visual BERT (Li et al., 2019) pre-trained\non the COCO dataset (Lin et al., 2014); 6) ViL-\nBERT CC: Vision and Language BERT (Lu et al.,\n2019) trained on an intermediate multimodal ob-\njective (Sharma et al., 2018) for task-agnostic joint\nrepresentations of image and text; 7) MOMENTA:\na multimodal harmful meme detection system (Pra-\nmanick et al., 2021b) that takes the global and lo-\ncal information in two modalities of memes into\naccount; 8) MaskPrompt: a prompt learning ap-\nproach (Cao et al., 2022) that converts harmful\nmeme detection as a masked language modeling\nproblem based on RoBERTa-large (Liu et al., 2019).\nWe use accuracy and macro-averaged F1 score as\nthe evaluation metrics, where the macro-averaged\nF1 is the more important metric owing to the im-\nbalanced class prevalence (see Table 4), to cap-\nture competitive performance beyond the majority\nclass.\nWhile LLMs offer strong zero/few-shot perfor-\nmance as shown in the ‘w/o Fine-tuning Small\nFrozen Vision\nTransformer\nCross Attention\nLM Encoder Layer\nAdd\n: Parameters Frozen\n: Parameters Not Frozen\nMeme Text\n... \nFigure 5: The details of our Multimodal Fusion module.\nLMs’ setting in Table 2, they are challenging to\nserve in practice that requires at least 350GB GPU\nmemory using specialized infrastructure for a sin-\ngle 175 billion LLM. This work presents a novel\nparadigm to leverage the reasoning ability and rich\nbackground knowledge of LLMs for better harm-\nful meme detection, but just need to fine-tune the\nsmall language model even with a smaller size than\nthe state-of-the-art baseline. Table 6 illustrates the\ndetailed results on the three meme datasets with dif-\nferent versions of our fine-tuned backbone model.\nTable 7 illustrates the comparison of characteris-\ntics between MR.HARM and the state-of-the-art\nbaselines like MOMENTA and MaskPrompt.\nD Illustration of Multimodal Fusion\nFigure 5 illustrates the details of our multimodal fu-\nsion module in the encoding phase of MR.HARM .\nE Discussion about One-stage Training\nWe further investigate the one-stage training to\nfigure out the intrinsic property of the chain-of-\nthought reasoning. We compare the performance\nwith two proposed variants for the one-stage train-\ning: 1) Explanation where the rationale is utilized\nfor explaining the harmfulness inference; 2) Rea-\nsoning where harmfulness inference is conditioned\n9127\nDataset Harm-C Harm-P FHM\nModel Accuracy Macro- F1 Accuracy Macro- F1 Accuracy Macro- F1\nExplanation 83.05 81.45 63.32 63.32 67.40 65.77\nReasoning 68.93 56.19 56.90 56.67 63.00 59.29\nTable 8: Effects of the one-stage training.\nto the rationale. As shown in Tabel 8, the reasoning\nsetting performs worse than the explanation setting\nwith a large margin. We conjecture that this is be-\ncause the reasoning setting in the one-stage training\ncould lead to error propagation if our small lan-\nguage model generates hallucinated rationales that\nmislead the harmfulness inference, which however\ncould be well avoided by the two-stage training\nparadigm. Meanwhile, as there exists mutual in-\nterference between rationale generation and harm-\nfulness prediction, the explanation setting could\ngive the harmfulness inference higher priority in\nthe sequence generation so that it performs better\nthan the reasoning setting. We argue that such a\none-stage training paradigm could be improved in\nthe future by applying a filtering mechanism, e.g.,\nusing only the effective chain-of-thought reasoning\nto infer the harmfulness of memes and get rid of\nirrelevant rationales. In summary, both settings in\nthe one-stage training paradigm suffer different de-\ngrees of performance degradation, which reaffirms\nthe necessity of our two-stage training paradigm.\nF Future Work\nWe will explore the following directions in the fu-\nture:\n• Considering that our framework could gener-\nate readable snippets for cognition-view rea-\nsoning, we plan to do some sort of system-\natic study to claim explainability (possibly\nthrough a human subjects study) for the eval-\nuation.\n• In this work we target exploring the underly-\ning reasoning process to empower the harmful\nmeme detection model with the ability of ex-\nplicit reasoning, to arrive at correct harmful-\nness predictions. We are going to further ex-\nploit LLMs toward explainable harmful meme\ndetection from perspectives like dataset con-\nstruction on social media with propagation\nstructure (Lin et al., 2021; Ma and Gao, 2020;\nMa et al., 2020), automatic evaluation, and\nhuman evaluation.\n• We would further update our framework by\nleveraging visual LLMs if accessible in the\nfuture to improve the visual feature extraction\nfor better multimodal reasoning, and avoid\nseveral common deficiencies of existing lan-\nguage models including hallucination and lim-\nited generalization as much as possible.\n9128",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5732402801513672
    },
    {
      "name": "Generative grammar",
      "score": 0.5725927948951721
    },
    {
      "name": "Cognition",
      "score": 0.5071189403533936
    },
    {
      "name": "Meaning (existential)",
      "score": 0.4922403395175934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4799487888813019
    },
    {
      "name": "Generative model",
      "score": 0.45536085963249207
    },
    {
      "name": "Harm",
      "score": 0.44763949513435364
    },
    {
      "name": "Task (project management)",
      "score": 0.4319557547569275
    },
    {
      "name": "Visual reasoning",
      "score": 0.41746142506599426
    },
    {
      "name": "Natural language processing",
      "score": 0.37161755561828613
    },
    {
      "name": "Cognitive science",
      "score": 0.361778199672699
    },
    {
      "name": "Psychology",
      "score": 0.26548027992248535
    },
    {
      "name": "Social psychology",
      "score": 0.11043009161949158
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}