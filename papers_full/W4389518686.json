{
  "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
  "url": "https://openalex.org/W4389518686",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2117745523",
      "name": "Katherine Tian",
      "affiliations": [
        "Harvard University Press",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2096617519",
      "name": "Eric Mitchell",
      "affiliations": [
        "Harvard University Press",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2127101740",
      "name": "Allan Zhou",
      "affiliations": [
        "Stanford University",
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2188467301",
      "name": "Archit Sharma",
      "affiliations": [
        "Stanford University",
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2486057653",
      "name": "Rafael Rafailov",
      "affiliations": [
        "Stanford University",
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2781037521",
      "name": "Huaxiu Yao",
      "affiliations": [
        "Harvard University Press",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2107089640",
      "name": "Chelsea Finn",
      "affiliations": [
        "Stanford University",
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2151390485",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Harvard University Press",
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W4290994954",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2073241381",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2169897903",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4285252640",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W2618169590",
    "https://openalex.org/W4385573323",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W1992970544",
    "https://openalex.org/W4281748205"
  ],
  "abstract": "Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher Manning. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5433–5442\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nJust Ask for Calibration: Strategies for Eliciting Calibrated Confidence\nScores from Language Models Fine-Tuned with Human Feedback\nKatherine Tian,∗†Eric Mitchell,∗‡Allan Zhou,‡Archit Sharma,‡Rafael Rafailov‡\nHuaxiu Yao,‡Chelsea Finn,‡Christopher D. Manning‡\n†Harvard University ‡Stanford University\nktian@college.harvard.edu\neric.mitchell@cs.stanford.edu\nAbstract\nA trustworthy real-world prediction system\nshould produce well-calibrated confidence\nscores; that is, its confidence in an answer\nshould be indicative of the likelihood that the\nanswer is correct, enabling deferral to an expert\nin cases of low-confidence predictions. Re-\ncent studies have shown that unsupervised pre-\ntraining produces large language models (LMs)\nwhose conditional probabilities are remarkably\nwell-calibrated. However, the most widely-\nused LMs are fine-tuned with reinforcement\nlearning from human feedback (RLHF-LMs),\nand some studies have suggested that RLHF-\nLMs produce conditional probabilities that are\nvery poorly calibrated. In light of this perceived\nweakness, we conduct a broad evaluation of\nmethods for extracting confidence scores from\nRLHF-LMs. For RLHF-LMs such as ChatGPT,\nGPT-4, and Claude, we find that verbalized\nconfidences emitted as output tokens are typi-\ncally better-calibrated than the model’s condi-\ntional probabilities on the TriviaQA, SciQ, and\nTruthfulQA benchmarks, often reducing the ex-\npected calibration error by a relative 50%.\n1 Introduction\nReal-world prediction systems invariably make er-\nrors. However, some mitigation of these errors is\npossible if the system produces well-calibrated1\nconfidence estimates. In this case, the system’s\nleast confident predictions correspond to those that\nare most likely to be incorrect, potentially allowing\nthese predictions to be skipped or overridden by\na human. In the context of language models, one\nconsequence of poor calibration may be hallucina-\ntion, where a language model confidently asserts\nincorrect facts or reasoning. While the ability of\nvery large LMs to absorb and synthesize knowl-\nedge about the outside world has gained significant\n∗ ∗Equal contribution.\n1i.e., the confidence in a prediction accurately reflects the\nprobability that the prediction is correct (Guo et al., 2017).\nFigure 1: Verbalized confidence scores (blue) are\nbetter-calibrated than log probabilities (orange) for\ngpt-3.5-turbo. Raw model probabilities (top-left) are con-\nsistently over-confident. Verbalized numerical probabilities\n(bottom) are better-calibrated. Considering more answer\nchoices (bottom-right) further improves verbalized calibra-\ntion (as in ‘Considering the Opposite’ in psychology; Lord\net al. (1985)). Verbalized expressions of likelihood (top-right)\nalso provide improved calibration. Bar height is average accu-\nracy of predictions in bin. Darker bars mean more predictions\nfall in that confidence range. Results computed on SciQ.\nattention (Brown et al., 2020; Roberts et al., 2020;\nBubeck et al., 2023), relatively little attention has\nbeen given to their well-calibratedness (Kadavath\net al., 2022). Further, most existing analyses of the\ncalibratedness of LLMs focus on models trained\nwith maximum likelihood, while in practice, the\nmost widely-used LLMs (such as ChatGPT) are\nfine-tuned using methods such as reinforcement\nlearning from human feedback (Christiano et al.,\n2017). Some findings suggest that RLHF-LMs may\nsacrifice well-calibrated predictions for the sake of\ncloser adherence to user instructions in dialogue\n(Kadavath et al., 2022; OpenAI, 2023), as the rein-\nforcement learning objective encourages the model\nto allocate probability mass to the most preferred\nanswer(s), rather than matching the relative fre-\nquency of possible answers.\nThis paper evaluates several methods for ex-\ntracting confidences about model predictions from\n5433\nTriviaQA SciQ TruthfulQA\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40T emperature-scaled ECE\nECE Comparison ( )\nTriviaQA SciQ TruthfulQA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0AUC\nAUC Comparison ( )\nPre-RLHF\nPost-RLHF\nFigure 2: RLHF generally worsens the calibration of\nLlama-70B’s log probabilities, as measured by ECE (lower\nis better) or AUC (higher is better). However, this paper (Ta-\nbles 1-5) will show that for several strong RLHF-LMs, the\nmodel’s verbalized confidence is often better-calibrated than\nits log probabilities, reversing some of this degradation. This\nreversal is strongest for TruthfulQA, an adversarial dataset\ntesting common misconceptions and other difficult queries.\nRLHF-LMs. Due to concerns that RLHF may\ncause systematic overconfidence in the model’s\nprobabilities (Figure 2), as well as the general un-\navailability of per-token log-probabilities in widely\nused RLHF-LMs, we pay particular attention to\nprompts that elicit verbalized probabilities, i.e., the\nmodel expresses its confidence in token-space, as\neither numerical probabilities or another linguistic\nexpression of uncertainty. We find that, surpris-\ningly, popular RLHF-LMs are able to directly ver-\nbalize confidence scores that are better-calibrated\nthan the model’s conditional probabilities (esti-\nmated via sampling), without any fine-tuning to\nlearn verbalization. To further improve calibration,\nwe take inspiration from research in human psy-\nchology showing that overconfidence can be mit-\nigated by considering alternative answers before\nresponding (Lord et al., 1985; Mussweiler et al.,\n2000). We show that prompting a model to produce\nseveral answer choices before giving its confidence\nscores significantly improves calibration of ver-\nbalized probabilities. Combined with temperature\nscaling (Guo et al., 2017), this approach generally\nprovides better calibration than model probabilities\nfor ChatGPT2, GPT-43, and Claude 24 across three\ndatasets, often reducing expected calibration error\n(ECE) by over 50%.\nRelated Work. Several studies have examined\nthe calibration of large LMs (Lin et al., 2022a;\nPark and Caragea, 2022; Kadavath et al., 2022;\nXiao et al., 2022; Kuhn et al., 2023), finding that\ncombining large pre-trained LMs with tempera-\nture scaling (Guo et al., 2017) produces very well-\n2gpt-3.5-turbo, accessed in June 2023.\n3https://cdn.openai.com/papers/gpt-4-system-card.pdf\n4https://www-files.anthropic.com/production/images/Model-\nCard-Claude-2.pdf\ncalibrated predictions (Kadavath et al., 2022; Xiao\net al., 2022; Kuhn et al., 2023). Other work focuses\non the tendency of language and dialogue models\nto use linguistic expressions of uncertainty in a\nwell-calibrated manner (Zhou et al., 2023; Mielke\net al., 2022). However, existing studies focus on\nLMs trained purely with unsupervised learning\n(although Kadavath et al. (2022) briefly examine\nRLHF-LMs), while widely used models in prac-\ntice are fine-tuned with instruction-tuning or RLHF\n(Christiano et al., 2017). RLHF has been shown\nto effectively leverage annotations of human pref-\nerences to control sentiment (Ziegler et al., 2020),\nimprove summarization or instruction-following\nquality (Stiennon et al., 2022; Ouyang et al., 2022),\nand inject behavioral priors of harmlessness (Bai\net al., 2022b,a). However, recent work has raised\nthe question of whether or not RLHF harms cali-\nbration (OpenAI, 2023). Our work is the first to\nshow that verbalized probabilities are often better-\ncalibrated than the model’s conditional probabili-\nties for RLHF-LMs such as ChatGPT, GPT-4, and\nClaude, and Llama-2-70B-Chat.\n2 Evaluating Calibration in RLHF-LMs\nTo study the calibration of RLHF-LMs, we con-\nduct experiments withgpt-3.5-turbo(ChatGPT),\ngpt-4 (GPT-4), claude-1 (Claude 1), claude-2\n(Claude 2), and Llama-2-70b-chat (Llama-2-\n70B-Chat).\nMetrics. We measure calibration with multiple\nmetrics. To measure ECE (expected calibration er-\nror; Guo et al. (2017)), we bin model predictions by\ntheir confidence and measure the average accuracy\nof predictions in each confidence bin. The ECE\nis defined as the average (squared) error between\nthe average accuracy and confidence within each\nbin, where each error is weighted by the fraction of\nsamples falling within the bin. We report raw ECE\nas well as ECE with temperature scaling (ECE-t).\nTemperature scaling fits a single temperature value\nβto the model’s confidences to minimize negative\nlog likelihood (NLL) on the data, giving scaled\nprobability ˜pi of class ias ˜pi ∝ pβ\ni . See Figure 1\nfor a depiction of ECE binning. Although ECE is a\nstandard and interpretable measure of calibration\nerror, it completely fails to capture the confidences’\ndiscriminative power.5 We therefore also report\n5For binary classification, a system that guesses randomly\nand outputs 50% confidence each time has perfect ECE.\n5434\nTriviaQA SciQ TruthfulQA\nMethod ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑\nLabel prob. 0.140 0.097 0.142 0.869 0.256 0.180 0.223 0.752 0.451 0.317 0.345 0.418\n‘Is True’ prob. 0.164 0.159 0.165 0.826 0.312 0.309 0.309 0.677 0.470 0.471 0.476 0.384\nEntropy — — — 0.547 — — — 0.483 — — — 0.236\nVerb. 1S top-1 0.068 0.076 0.138 0.879 0.234 0.084 0.214 0.744 0.389 0.256 0.322 0.545\nVerb. 1S top-2 0.050 0.053 0.139 0.894 0.132 0.050 0.201 0.766 0.361 0.115 0.252 0.485\nVerb. 1S top-4 0.054 0.057 0.144 0.896 0.065 0.051 0.209 0.763 0.203 0.189 0.284 0.455\nVerb. 2S CoT 0.110 0.123 0.168 0.830 0.323 0.246 0.296 0.683 0.419 0.259 0.292 0.551\nVerb. 2S top-1 0.131 0.099 0.148 0.855 0.340 0.203 0.268 0.677 0.431 0.245 0.282 0.483\nVerb. 2S top-2 0.047 0.045 0.147 0.887 0.169 0.040 0.201 0.768 0.395 0.101 0.224 0.517\nVerb. 2S top-4 0.050 0.051 0.156 0.861 0.130 0.046 0.211 0.729 0.270 0.156 0.246 0.463\nLing. 1S human 0.062 0.069 0.137 0.884 0.166 0.087 0.223 0.703 0.306 0.296 0.333 0.503\nLing. 1S-opt. 0.058 0.066 0.135 0.878 0.064 0.068 0.220 0.674 0.125 0.165 0.270 0.492\nTable 1: Measuring calibration of various methods for extracting confidences from gpt-3.5-turbo (ChatGPT). The model’s\nconditional probabilities are relatively poorly calibrated, whether using the model’s conditional probability of the label given the\nquery (Label prob.) or the probability assigned to ‘True’ given the query, proposed answer, and a prompt asking if the answer is\ncorrect (‘Is True’ prob.). Surprisingly, directly verbalizing a probability (Verb. 1S and Verb. 2S) or an expression of confidence\nsuch as ‘highly likely’ (Ling. 1S) yields significantly better-calibrated confidence estimates. 1S refers to one-stage prediction,\nwhere the model provides an answer and confidence probability/expression together. 2S refers to two-stage prediction, where the\nmodel first gives only an answer, and then in a second stage a confidence. To color the table cells, for each column, we demean\nand scale by a constant to obtain a shade in [-1,1], where cyan indicates better and orange worse performance.\nTriviaQA SciQ TruthfulQA\nMethod ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑\nLabel prob. 0.078 0.067 0.077 0.950 0.219 0.165 0.186 0.820 0.445 0.334 0.362 0.462\nVerb. 1S top-1 0.024 0.038 0.084 0.937 0.201 0.084 0.165 0.843 0.350 0.156 0.227 0.622\nVerb. 1S top-2 0.025 0.034 0.084 0.949 0.140 0.048 0.185 0.813 0.315 0.112 0.228 0.623\nVerb. 1S top-4 0.041 0.039 0.081 0.959 0.056 0.059 0.185 0.815 0.198 0.144 0.245 0.619\nLing. 1S-human 0.051 0.041 0.086 0.931 0.148 0.024 0.170 0.835 0.241 0.151 0.228 0.651\nLing. 1S-opt. 0.056 0.051 0.088 0.927 0.028 0.052 0.172 0.828 0.082 0.105 0.212 0.632\nTable 2: gpt-4’s verbalized probabilities are substantially better-calibrated than the model probabilities themselves, even after\ntemperature scaling, similarly to gpt-3.5-turboin Table 1.\nBrier Score (BS; Brier (1950)) on temperature-\nscaled confidences (BS-t), a proper scoring rule\n(Ovadia et al., 2019) that is the mean squared error\nbetween the confidences and the correctness labels.\nFinally, we assess calibration using a metric from\nthe selective classification literature (Geifman and\nEl-Yaniv, 2017), specifically, the area under the\ncurve of selective accuracy and coverage (AUC).\nDatasets. Our experiments use three question-\nanswering datasets assessing factual knowledge.\nTriviaQA (Joshi et al., 2017) contains 650k\nquestion-answer pairs gathered by trivia enthusi-\nasts; SciQ (Welbl et al., 2017) contains approxi-\nmately 14k crowdsourced science exam question-\nanswer pairs; TruthfulQA (Lin et al., 2022b) con-\ntains 817 questions designed to test language mod-\nels’ tendency to mimic human falsehoods. We\nsample 1000 questions from the validation split of\nTriviaQA (rc.web.nocontext) and SciQ and all\n817 questions from the validation split of Truth-\nfulQA (generation) for our experiments.\nEvaluation protocol. For each dataset, we gener-\nate a response and corresponding confidence from\neach method on each of the evaluation questions.\nBecause calibration essentially quantifies the re-\nlationship between model confidence and correct-\nness, computing correctness is crucial to accurate\nmeasurements of calibration. However, we find\ndoing so to be a challenge, especially in datasets\nwhere only a single ground-truth answer (but not\naliases or semantically equivalent rephrases) is pro-\nvided. To avoid excessive false negatives in our\ncorrectness computation as a result of exact-match\nevaluation, we use either GPT-4 or GPT-3.5 to eval-\nuate whether a response is essentially equivalent to\nthe ground truth answer; see Appendix C for the\ncomplete equivalence-checking procedure.\nMethods. We compare a wide variety of methods\nfor extracting confidence estimates from LLMs.\nFor a comprehensive list of the prompts used for\neach method, see Appendix Table 6.\nFirst, we consider two methods that leverage the\ntrue conditional distribution of the model to gener-\n5435\nTriviaQA SciQ TruthfulQA\nMethod ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑\nLabel prob. 0.074 0.079 0.117 0.915 0.216 0.149 0.195 0.786 0.432 0.304 0.335 0.418\nVerb. 1S top-1 0.049 0.059 0.160 0.839 0.265 0.103 0.247 0.663 0.440 0.134 0.204 0.411\nVerb. 1S top-2 0.046 0.047 0.158 0.875 0.207 0.040 0.225 0.693 0.450 0.085 0.197 0.409\nVerb. 1S top-4 0.075 0.079 0.176 0.814 0.151 0.057 0.226 0.667 0.372 0.105 0.183 0.377\nLing. 1S human 0.053 0.050 0.151 0.867 0.253 0.118 0.245 0.664 0.443 0.358 0.340 0.384\nLing. 1S-opt. 0.074 0.060 0.149 0.863 0.089 0.082 0.238 0.623 0.139 0.148 0.228 0.350\nTable 3: Claude-1 produces similar- or better-calibrated log probabilities to gpt-3.5-turbo, but is less able to verbalize\nwell-calibrated confidences, compared to models in the GPT family of RLHF-LMs. Claude-1 has since been deprecated.\nTriviaQA SciQ TruthfulQA\nMethod ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑\nLabel prob. 0.089 0.089 0.137 0.882 0.181 0.176 0.237 0.762 0.409 0.368 0.405 0.319\nVerb. 1S top-1 0.072 0.071 0.141 0.903 0.204 0.054 0.201 0.776 0.345 0.115 0.215 0.573\nVerb. 1S top-2 0.049 0.054 0.133 0.918 0.134 0.041 0.211 0.754 0.359 0.085 0.223 0.491\nVerb. 1S top-4 0.072 0.063 0.158 0.890 0.048 0.052 0.216 0.711 0.274 0.075 0.208 0.473\nLing. 1S human 0.085 0.061 0.151 0.878 0.238 0.026 0.209 0.756 0.381 0.242 0.305 0.530\nLing. 1S-opt. 0.060 0.070 0.151 0.874 0.049 0.056 0.214 0.738 0.099 0.130 0.266 0.446\nTable 4: Claude-2 has weaker conditional probabilities than Claude-1 and GPT-*, but its verbalized calibration provides consistent\nimprovement over conditional probabilities at a level comparable to GPT-3.5 and surpassing GPT-* on TruthfulQA.\nate confidence scores. The simplest is Label prob.,\nwhich uses the conditional probability distribution\np(y|x) of the model given a question x, which we\nestimate using n= 10samples, since many RLHF-\nLMs are closed-source and do not offer per-token\nprobabilities.67 We return the most common an-\nswer, using the LLM-based equivalence function\nto determine when two lexically different answers\nare semantically equivalent. In a variation of the\nmethod described by Kadavath et al. (2022) (again,\nwe use samples since model probabilities are not\navailable), ‘Is True’ prob. samples a single answer\nˆyfrom the model given a question x, and the prob-\nability it is true is estimated by the probability the\nmodel assigns to ‘True’ when asked if the given\nanswer is true (where once again the probabilities\nare estimated via samples), i.e., p(True|x,ˆy).\nNext, we consider methods that extract con-\nfidence scores through verbalization (Lin et al.,\n2022a), i.e., where the model expresses its confi-\ndence in token space, either with numerical prob-\nabilities or linguistic expressions of likelihood. 8\nFirst, Verb. 1S top-k prompts the model to pro-\nduce kguesses and a probability that each is cor-\nrect all in a single response (i.e., ‘1 stage’). We\ntake the highest-probability prediction and its as-\n6We evaluatedgpt-3.5-turbo on all three datasets using\nn = 20 samples, but the calibration did not meaningfully\nimprove, so we always use n = 10to reduce API costs.\n7For each closed LM, we use its default sampling param-\neters (top-p 1.0 for GPT-* and top-p 0.7 for Claude). For\nLlama-2, we use temperature 1.0 and top-p 1.0.\n8However, note that none of the methods described fine-\ntune the model to perform better on verbalization.\nsociated probability as the model’s output and con-\nfidence. Verb. 2S top-k similarly uses numeri-\ncal probabilities, except the model is first asked\nto provide only its answers, and afterwards, in a\nsecond round of dialogue, asked to assign prob-\nabilities of correctness to each answer (i.e., ‘2\nstages’). Verb. 2S CoT uses a chain-of-thought\nprompt before giving a single answer, and in a\nsecond round of dialogue, the model is prompted\nto assign a probability to that answer (with the\nchain of thought present in the model’s context).\nLing. 1S-human uses linguistic likelihood expres-\nsions, rather than numerical probabilities, to ex-\npress uncertainty. The model is prompted to assign\nconfidences to its guesses by choosing from a set\nof linguistic expressions of uncertainty: {Almost\ncertain, Likely, . . . ,Almost no chance}. Each\nlinguistic likelihood expression is mapped to a\nprobability using responses from a human sur-\nvey on social media with 123 respondents (Fagen-\nUlmschneider, 2023). Ling. 1S-opt. uses a held\nout set of calibration questions and answers to com-\npute the average accuracy for each likelihood ex-\npression, using these ‘optimized’ values instead.\nExpressions that are not used for at least 1\nN of\nquestions, where N is the number of calibration\nquestions, simply use the human probability.\n3 Results\nTables 1–5 show the results of evaluating various\nmethods for extracting confidence from RLHF-\nLMs on gpt-3.5-turbo, gpt-4, claude-1,\n5436\nTriviaQA SciQ TruthfulQA\nMethod ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑ ECE↓ ECE-t↓ BS-t↓ AUC↑\nLabel prob. 0.151 0.124 0.156 0.865 0.266 0.189 0.243 0.707 0.405 0.361 0.396 0.407\nVerb. 1S top-1 0.071 0.067 0.186 0.793 0.196 0.053 0.239 0.648 0.386 0.172 0.266 0.502\nVerb. 1S top-2 0.060 0.073 0.194 0.815 0.153 0.032 0.230 0.667 0.340 0.037 0.227 0.440\nVerb. 1S top-4 0.069 0.079 0.182 0.816 0.105 0.043 0.229 0.648 0.231 0.102 0.237 0.465\nLing. 1S human 0.179 0.115 0.195 0.749 0.071 0.101 0.252 0.603 0.376 0.366 0.383 0.407\nLing. 1S-opt. 0.077 0.068 0.186 0.779 0.019 0.042 0.236 0.590 0.047 0.051 0.239 0.435\nTable 5: With Llama2-70B-Chat, verbalized calibration provides improvement over conditional probabilities across some metrics,\nbut the improvement is much less consistent compared to GPT-* and Claude-*.\nclaude-2, and Llama-2-70b-chat, respectively.\nWe distill several key conclusions from these exper-\niments. 1. Large RLHF-LMs can often directly\nverbalize better-calibrated confidences (either a\nnumerical confidence probability or an expres-\nsion such as ‘highly likely’) than the models’\nconditional probabilities. 2. Among the methods\nfor verbalizing probabilities directly, we observe\nthat generating and evaluating multiple hypothe-\nses improves calibration (see Figure 1), similarly\nto humans (Lord et al., 1985), and corroborating\na similar finding in LMs (Kadavath et al., 2022).\n3. Language models can express their uncertainty\nwith numerical probabilities as well or better than\nwith words, which is surprising in light of long-\nstanding difficulties in representing numbers in lan-\nguage models (Thawani et al., 2021). 4. Chain-\nof-thought prompting does not improve verbalized\ncalibration (see Appendix Figure 5 for additional\nCoT results). 5. The calibration of both Claude\nmodels’ conditional probabilities roughly falls be-\ntween gpt-3.5-turboand gpt-4; however, while\nClaude 1 is much weaker at verbalizing its con-\nfidence, Claude 2 is generally a bit stronger than\ngpt-3.5-turbo at verbalizing. The verbal calibra-\ntion of the open source model Llama-2-70b-chat\nis generally weaker than that of closed source mod-\nels but still demonstrates improvement over its con-\nditional probabilities by some metrics, and does so\nmost clearly on TruthfulQA.\n4 Discussion\nIn summary, we study the calibration of widely\nused RLHF-LMs. We first replicate the finding for\nGPT-4 (OpenAI, 2023) that RLHF can worsen the\ncalibration of a model’s conditional probabilities\nusing the open-source Llama-2-70B base and chat\nmodels (Figure 2). To mitigate this regression and\nease extraction of calibrated confidence scores for\nmodels for which log probabilities are not avail-\nable, we propose and study new methods that can\nelicit calibrated confidences from RLHF-LMs by\nprompting the model to verbalize its confidence\nin token space. We find verbalized probabilities\nare better-calibrated than conditional probabilities\nacross several closed models, with mixed results\nfor Llama-2-70B-Chat.\nOur results raise several questions for future\nwork. Most notably, the difference between GPT-*,\nClaude-*, and Llama-2’s ability to verbalize confi-\ndence is significant. What factors are important for\nlearning this skill? Additionally, the 1-stage and\n2-stage verbalized numerical confidence prompts\nsometimes differ drastically in the calibration of\ntheir confidences. How can we reduce sensitivity of\na model’s calibration to the prompt? Going beyond\nquestion-answering, can we leverage good calibra-\ntion in short-answer settings to improve the reliabil-\nity of long-form generations, perhaps by breaking\ndown long-form generation into a sequence of short\nquestions? Finally, to what extent does a language\nmodel’s calibration depend on the domain; do our\nconclusions in the context of factual recall hold in\nthe context of reasoning or arithmetic? Answering\nthese questions provides one path toward building\nmore trustworthy and useful language systems.\nLimitations. While our work demonstrates a\npromising new approach to generating calibrated\nconfidences through verbalization, there are lim-\nitations that could be addressed in future work.\nFirst, our experiments are focused on factual recall-\noriented problems, and the extent to which our ob-\nservations would hold for reasoning-heavy settings\nis an interesting open question. Additionally, the\nlack of technical details available for many state-of-\nthe-art closed RLHF-LMs may limit our ability to\nunderstand what factors enable a model to verbalize\nwell-calibrated confidences and differences in this\nability across different models. Finally, our study\nis limited to short-form question-answering; future\nwork should extend this analysis to longer-form\ngeneration settings.\n5437\nAcknowledgements. CF and CDM are CIFAR Fel-\nlows. EM gratefully acknowledges funding from\na Knight-Hennessy Graduate Fellowship. AZ is\nsupported by the NSF graduate research fellowship\nprogram. This research was supported in part by\nJuniper Networks, Apple, and ONR grant N00014-\n20-1-2675. The authors thank Yoonho Lee and\nNoah Goodman for helpful feedback on calibration\nmetrics and experiment design.\nReferences\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022a. Training a\nhelpful and harmless assistant with reinforcement\nlearning from human feedback.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022b. Constitutional AI: Harmless-\nness from ai feedback.\nGlenn W. Brier. 1950. Verification of Forecasts Ex-\npressed in Terms of Probability. Monthly Weather\nReview, 78(1):1–3.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with GPT-4. ArXiv\npreprint arXiv:2303.12712.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nWade Fagen-Ulmschneider. 2023. Perception of proba-\nbility words. Ms., UIUC, 05-24-2023.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nclassification for deep neural networks. In Proceed-\nings of the 31st International Conference on Neu-\nral Information Processing Systems, NIPS’17, page\n4885–4894, Red Hook, NY , USA. Curran Associates\nInc.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International Con-\nference on Machine Learning , volume 70 of Pro-\nceedings of Machine Learning Research, pages 1321–\n1330. PMLR.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know. Arxiv arxiv:2207.05221.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nIn The Eleventh International Conference on Learn-\ning Representations.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022a.\nTeaching models to express their uncertainty in\nwords. Transactions on Machine Learning Research.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022b.\nTruthfulQA: Measuring how models mimic human\n5438\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nCharles Lord, Mark Lepper, and Elizabeth Preston.\n1985. Considering the opposite: A corrective strat-\negy for social judgment. Journal of personality and\nsocial psychology, 47:1231–43.\nSabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-\nLan Boureau. 2022. Reducing conversational agents’\noverconfidence through linguistic calibration. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:857–872.\nThomas Mussweiler, Fritz Strack, and Tim Pfeiffer.\n2000. Overcoming the inevitable anchoring effect:\nConsidering the opposite compensates for selective\naccessibility. Personality and Social Psychology Bul-\nletin, 26(9):1142–1150.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,\nD. Sculley, Sebastian Nowozin, Joshua V . Dillon,\nBalaji Lakshminarayanan, and Jasper Snoek. 2019.\nCan you trust your model’s uncertainty? evaluating\npredictive uncertainty under dataset shift. In Pro-\nceedings of the 33rd International Conference on\nNeural Information Processing Systems, Red Hook,\nNY , USA. Curran Associates Inc.\nSeo Yeon Park and Cornelia Caragea. 2022. On the cal-\nibration of pre-trained language models using mixup\nguided by area under the margin and saliency. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5364–5374, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul Christiano. 2022. Learning\nto summarize from human feedback.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nArXiv, abs/1707.06209.\nYuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie\nNeiswanger, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2022. Uncertainty quantification\nwith pre-trained language models: A large-scale em-\npirical analysis. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , pages\n7273–7284, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto.\n2023. Navigating the grey area: Expressions of over-\nconfidence and uncertainty in language models.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2020. Fine-tuning lan-\nguage models from human preferences.\n5439\nAlmost No Chance\nHighly Unlikely\nChances are Slight\nLittle Chance\nUnlikely\nProbably Not\nAbout Even\nBetter than Even\nLikely\nProbably\nVery Good Chance\nHighly Likely\nAlmost Certain\nLikelihood Expression\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Fraction of Responses\nUsage of likelihood expressions by 3.5-turbo\nTriviaQA\nSciQ\nTruthfulQA\nFigure 3: gpt-3.5-turbo usage rate of each likelihood ex-\npression; the model displays much lower verbalized confi-\ndence on TruthfulQA than on standard factual recall problems.\nAlmost No Chance\nHighly Unlikely\nChances are Slight\nLittle Chance\nUnlikely\nProbably Not\nAbout Even\nBetter than Even\nLikely\nProbably\nVery Good Chance\nHighly Likely\nAlmost Certain\nLikelihood Expression\n0.0\n0.1\n0.2\n0.3\n0.4Fraction of Responses\nUsage of likelihood expressions by GPT-4\nTriviaQA\nSciQ\nTruthfulQA\nFigure 4: gpt-4 usage rate of each likelihood expression;\nthe model displays markedly lower verbalized confidence on\nTruthfulQA than on standard factual recall problems.\nA Additional Results\nHere, we include the likelihood expression usage\ndistribution for gpt-3.5 and gpt-4 in Figures 3\nand 4, respectively. gpt-3.5is systematically less\nconfident for TruthfulQA. The contrast between\nmodel confidence for TriviaQA and SciQ compared\nwith TruthfulQA is even more stark for gpt-4.\nWe also provide additional calibration results\nfor chain-of-thought methods. We compare a one-\nstage verbalized CoT prompt (Verb. 1S CoT), a\ntwo-stage verbalized CoT prompt (Verb. 2S CoT),\nand a two-stage verbalized method that uses CoT\njust before eliciting the numerical confidence (Verb.\n2S Cot Prob) instead of before the guess, as shown\nfor gpt-3.5on Trivia QA, SciQ, and Truthful QA\nin Figure 5. We find that CoT does not noticeably\nimprove calibration across any setting or dataset.\nB Fitting Procedure for Temperature and\nProbabilities for Linguistic Expressions\nTo fit the temperature that is used to compute ECE-\nt and BS-t we split our total data into 5 folds. For\nFigure 5: Expected calibration error is not consistently im-\nproved for any CoT prompt variant on gpt-3.5-turbo.\neach fold, we use it once to fit a temperature and\nevaluate metrics on the remaining folds. We find\nthat fitting the temperature on 20% of the data\nyields relatively stable temperatures across folds.\nWe report the average temperature-scaled ECE and\nBS as ECE-t and BS-t.\nTo compute ECE and AUC for Ling. 1S-opt., we\nsimilarly split our total data into 5 folds, using 4\nfolds to fit the probabilities behind each linguistic\nexpression of confidence, then evaluating on the\nremaining fold. To compute ECE-t and BS-t for\nLing. 1S-opt, we hold out one of the 5 folds to\nfit temperature. We use 3 folds to fit probabilities\nfor linguistic expressions, compute the temperature\nbased on these probabilities on the temperature\nset, and evaluate metrics on the last fold. We then\naverage metrics across all 20 rotations of folds.\nC Prompt Templates\nThe prompt template for each sampling method\nis provided in Table 6. The question is substi-\ntuted for the variable ${THE_QUESTION} in each\nprompt. To evaluate answer correctness, we\nuse gpt-3.5-turbo for SciQ and TruthfulQA\nand gpt-4 for TriviaQA due to gpt-3.5-turbo’s\n5440\nMethod Template\nLabel prob. Provide your best guess for the following question. Give ONLY the guess, no\nother words or explanation.\\n\\nFor example:\\n\\nGuess: <most likely guess, as\nshort as possible; not a complete sentence, just the guess!>\\n\\nThe question\nis:${THE_QUESTION}\n‘Is True’ prob. Question: ${QUESTION}\\nProposed Answer: ${ANSWER}\\nIs the proposed answer:\\n\\t(A)\nTrue or\\n\\t(B) False?\\n The proposed answer is:\nVerb. 1S top-1 Provide your best guess and the probability that it is correct (0.0 to 1.0) for\nthe following question. Give ONLY the guess and probability, no other words or\nexplanation. For example:\\n\\nGuess: <most likely guess, as short as possible; not\na complete sentence, just the guess!>\\n Probability: <the probability between 0.0\nand 1.0 that your guess is correct, without any extra commentary whatsoever; just\nthe probability!>\\n\\nThe question is: ${THE_QUESTION}\nVerb. 1S top-k Provide your ${k} best guesses and the probability that each is correct (0.0 to\n1.0) for the following question. Give ONLY the guesses and probabilities, no other\nwords or explanation. For example:\\n\\nG1: <first most likely guess, as short as\npossible; not a complete sentence, just the guess!>\\n\\nP1: <the probability between\n0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever; just\nthe probability!> ... G${k}: <${k}-th most likely guess, as short as possible;\nnot a complete sentence, just the guess!>\\n\\nP${k}: <the probability between 0.0\nand 1.0 that G${k} is correct, without any extra commentary whatsoever; just the\nprobability!> \\n\\nThe question is: ${THE_QUESTION}\nVerb. 2S CoT Provide your best guess for the following question. Before giving your answer,\nprovide a step-by-step explanation of your thought process. Then on a new line\ngive the guess with no other words or explanation.\\n\\nFor example:\\n\\nExplanation:\n<one sentence step-by-step explanation of your thought process>\\n\\nGuess: <most\nlikely guess, as short as possible; not a complete sentence, just the guess!>\\n\\nThe\nquestion is: ${THE_QUESTION}\nProvide the probability that your guess is correct. Give ONLY the probability, no\nother words or explanation.\\n\\nFor example:\\n\\nProbability: <the probability between\n0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever;\njust the probability!>\\n\nVerb. 2S top-1 Provide your best guess for the following question. Give ONLY the guess, no\nother words or explanation.\\n\\nFor example:\\n\\nGuess: <most likely guess, as\nshort as possible; not a complete sentence, just the guess!>\\n\\nThe question\nis:${THE_QUESTION}\nProvide the probability that your guess is correct. Give ONLY the probability, no\nother words or explanation.\\n\\nFor example:\\n\\nProbability: <the probability between\n0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever;\njust the probability!>\\n\nVerb. 2S top-k Provide your ${k} best guesses for the following question. Give ONLY the guesses,\nno other words or explanation. For example:\\n\\nG1: <first most likely guess, as\nshort as possible; not a complete sentence, just the guess!>\\n\\nP1: <the probability\nbetween 0.0 and 1.0 that G1 is correct, without any extra commentary whatsoever;\njust the probability!> ... G${k}: <${k}-th most likely guess, as short as possible;\nnot a complete sentence, just the guess!>\\n\\nThe question is:${THE_QUESTION}\nProvide the probability that each of your guesses is correct. Give ONLY\nthe probabilities, no other words or explanation.\\n\\nFor example:\\n\\nP1: <the\nprobability between 0.0 and 1.0 that G1 is correct, without any extra commentary\nwhatsoever; just the probability!>\\n... P${k}: <the probability between 0.0 and\n1.0 that G${k} is correct, without any extra commentary whatsoever; just the\nprobability!>\nLing. 1S Provide your best guess for the following question, and describe how likely it is\nthat your guess is correct as one of the following expressions: ${EXPRESSION_LIST}.\nGive ONLY the guess and your confidence, no other words or explanation. For\nexample:\\n\\nGuess: <most likely guess, as short as possible; not a complete sentence,\njust the guess!>\\nConfidence: <description of confidence, without any extra\ncommentary whatsoever; just a short phrase!>\\n\\nThe question is: ${THE_QUESTION}\nTable 6: Prompt templates for each method evaluated. Methods above the double line use multiple samples in order to estimate\nconfidence scores; methods below the double line use the verbalized confidences directly, requiring only a single sample.\n5441\nhigh disagreement with a human evaluator on\nTriviaQA. Using the ground truth answer as\n${GOLD_ANSWER} and the model-generated answer\nas ${PRED_ANSWER}, we use the following prompt\ntemplate:\nAre the following two answers to my\nquestion Q semantically equivalent?\\n\\nQ:\n${THE_QUESTION}\\nA1: ${GOLD_ANSWER}\\nA2:\n${PRED_ANSWER}\\n\\nPlease answer with a\nsingle word, either “Yes.\" or “No.\", and\nexplain your reasoning.\n5442",
  "topic": "Ask price",
  "concepts": [
    {
      "name": "Ask price",
      "score": 0.809619128704071
    },
    {
      "name": "Calibration",
      "score": 0.6645926237106323
    },
    {
      "name": "Computer science",
      "score": 0.5360892415046692
    },
    {
      "name": "Tian",
      "score": 0.43740853667259216
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3936670422554016
    },
    {
      "name": "Statistics",
      "score": 0.19425556063652039
    },
    {
      "name": "Art",
      "score": 0.15209579467773438
    },
    {
      "name": "Mathematics",
      "score": 0.15001434087753296
    },
    {
      "name": "Literature",
      "score": 0.0835743248462677
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 55
}