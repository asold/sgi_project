{
  "title": "Anomaly Detection of Hyperspectral Images Based on Transformer With Spatial–Spectral Dual-Window Mask",
  "url": "https://openalex.org/W4313590927",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097870016",
      "name": "Song Xiao",
      "affiliations": [
        "Beijing Electronic Science and Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1921975221",
      "name": "Tian Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A4285517025",
      "name": "Zhangchun Xu",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2321546203",
      "name": "Jia-Hui Qu",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2515496140",
      "name": "Shaoxiong Hou",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2155109418",
      "name": "Wen-qian Dong",
      "affiliations": [
        "Xidian University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093600558",
    "https://openalex.org/W2087263574",
    "https://openalex.org/W2900718393",
    "https://openalex.org/W4312289230",
    "https://openalex.org/W2789643644",
    "https://openalex.org/W4285124347",
    "https://openalex.org/W2001045763",
    "https://openalex.org/W1970099214",
    "https://openalex.org/W3007076381",
    "https://openalex.org/W2972614519",
    "https://openalex.org/W2887985146",
    "https://openalex.org/W2047870694",
    "https://openalex.org/W2011147915",
    "https://openalex.org/W2040078680",
    "https://openalex.org/W3196594491",
    "https://openalex.org/W3129083271",
    "https://openalex.org/W3152468234",
    "https://openalex.org/W2163129097",
    "https://openalex.org/W2004491663",
    "https://openalex.org/W3163345093",
    "https://openalex.org/W3168690312",
    "https://openalex.org/W2998493545",
    "https://openalex.org/W4296209273",
    "https://openalex.org/W4312999834",
    "https://openalex.org/W3216217554",
    "https://openalex.org/W2772028350",
    "https://openalex.org/W6780248173",
    "https://openalex.org/W2901555355",
    "https://openalex.org/W2983563481",
    "https://openalex.org/W2969635036",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W1521436688",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2803137490",
    "https://openalex.org/W2740976805",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2288752886",
    "https://openalex.org/W3137199127",
    "https://openalex.org/W3099362990",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Anomaly detection has become one of the crucial tasks in hyperspectral images processing. However, most deep learning-based anomaly detection methods often suffer from the incapability of utilizing spatial&#x2013;spectral information, which decreases the detection accuracy. To address this problem, we propose a novel hyperspectral anomaly detection method with a spatial&#x2013;spectral dual-window mask transformer, termed as S2DWMTrans, which can fully extract features from global and local perspectives, and suppress the reconstruction of anomaly targets adaptively. Specifically, the dual-window mask transformer aggregates background information of the entire image from a global perspective to neutralize anomalies, and uses neighboring pixels in a dual-window to suppress anomaly reconstruction. An adaptive-weighted loss function is designed to further suppress anomaly reconstruction adaptively during network training process. According to our investigation, this is the first work to apply transformer to hyperspectral anomaly detection. Comparative experiments and ablation studies demonstrate that the proposed S2DWMTrans achieves competitive performance.",
  "full_text": "1414 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nAnomaly Detection of Hyperspectral Images Based\non Transformer With Spatial–Spectral\nDual-Window Mask\nSong Xiao , Member, IEEE, Tian Zhang , Zhangchun Xu, Jiahui Qu , Member, IEEE,\nShaoxiong Hou, Graduate Student Member, IEEE, and Wenqian Dong , Member, IEEE\nAbstract—Anomaly detection has become one of the crucial tasks\nin hyperspectral images processing. However, most deep learning-\nbased anomaly detection methods often suffer from the incapabil-\nity of utilizing spatial–spectral information, which decreases the\ndetection accuracy. To address this problem, we propose a novel\nhyperspectral anomaly detection method with a spatial–spectral\ndual-window mask transformer, termed as S2DWMTrans, which\ncan fully extract features from global and local perspectives, and\nsuppress the reconstruction of anomaly targets adaptively. Specif-\nically, the dual-window mask transformer aggregates background\ninformation of the entire image from a global perspective to neu-\ntralize anomalies, and uses neighboring pixels in a dual-window to\nsuppress anomaly reconstruction. An adaptive-weighted loss func-\ntion is designed to further suppress anomaly reconstruction adap-\ntively during network training process. According to our investi-\ngation, this is the ﬁrst work to apply transformer to hyperspectral\nanomaly detection. Comparative experiments and ablation studies\ndemonstrate that the proposed S2DWMTrans achieves competitive\nperformance.\nIndex Terms —Anomaly detection, dual-window mask\ntransformer (DWMTrans), hyperspectral image (HSI).\nI. I NTRODUCTION\nH\nYPERSPECTRAL image (HSI) with rich spectral infor-\nmation has a powerful ability of distinguishing different\nmaterials, and have been widely used in various remote sensing\nManuscript received 26 October 2022; revised 10 December 2022 and 22\nDecember 2022; accepted 23 December 2022. Date of publication 4 January\n2023; date of current version 24 January 2023. This work was supported in part\nby the National Natural Science Foundation of China under Grant 62101414; in\npart by the Young Talent Fund of Xi’an Association for Science and Technology\nunder Grant 095920221320; in part by the Natural Science Basic Research Plan\nin Shaanxi Province of China under Grant 2021JQ-194 and Grant 2021JQ-197;\nin part by the Fundamental Research Funds for the Central Universities under\nGrant XJS210108 and Grant XJS210104; in part by the China Post-Doctoral\nScience Foundation under Grant 2021M702546 and Grant 2021M702548; in\npart by the Scientiﬁc and Technological Activities for Overseas Students of\nShaanxi Province under Grant 2020-017; and in part by the Guangdong Ba-\nsic and Applied Basic Research Foundation under Grant 2020A1515110856.\n(Corresponding author: Song Xiao.)\nSong Xiao is with the Department of Electronic and Communication Engi-\nneering, Beijing Electronic Science and Technology Institute, Beijing 100070,\nChina, and also with the School of Telecommunications Engineering, Xidian\nUniversity, Xi’an 710071, China (e-mail: xs_xidian@163.com).\nTian Zhang, Zhangchun Xu, Jiahui Qu, Shaoxiong Hou, and Wenqian Dong\nare with the State Key Laboratory of Integrated Service Network, Xidian Uni-\nversity, Xi’an 710071, China (e-mail: ztnwu0111@163.com; xzchxx@163.com;\njhqu@xidian.edu.cn; sxhou@stu.xidian.edu.cn; wqdong@xidian.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3232762\ndata analysis applications [1], [2], such as anomaly detection [3],\n[4], image classiﬁcation [5], [6], and target detection [7], [8].\nAmong these applications, anomaly detection utilizes continu-\nous spectral information and spatial information of land covers\nto detect anomalies with signiﬁcant different spectral signatures\nfrom their surrounding environment [9].\nAnomaly detection has the unique advantage of not requiring\nprior spectral information, which prompts a lot of detection\nmethods to be proposed [10], [11]. According to the sep-\naration standards of background and anomaly, hyperspectral\nanomaly detection methods can be divided into three categories:\nstatistical model-based, reconstruction model-based, and deep\nlearning-based. The statistical model-based anomaly detection\nmethods can be traced back to the RX method proposed by\nReed et al. [12], which assumes that the background part can\nbe represented by the Gaussian background distribution model.\nHowever, the HSI background may be very complex and cannot\nbe ﬁtted by the Gaussian model only. In order to improve the\ndetection performance, local RX (LRX) [13], weighted RX\n(WRX) [14], and other methods were proposed. LRX selects\na speciﬁc area as the background, usually in the form of sliding\ndual windows. However, it is difﬁcult to determine the size of\nthe dual window. WRX can better estimate the background by\nassigning low weights to possible anomalous pixels and high\nweights to other pixels. Zhang et al. proposed an isolation forest\nmethod for hyperspectral anomaly detection, which is based on\nOstu and assume that the isolation of the abnormal pixel from\nthe alternative pixel is more sensitive [15]. Sertac et al. designed\nan anomaly detection algorithm for HSIs, which is based on\nnonparametric Bayesian background estimation [16]. Chang\net al. designed an orthogonal subspace projection target detector\nfor hyperspectral anomaly detection, which takes advantage of\nautomatic target generation process, and achieved an outstand-\ning performance [17]. With the development of the compressed\nsensing theory and machine learning, anomaly detection method\nbased on reconstruction has gradually become a research hotpot.\nThe basic idea is to use an overcomplete dictionary to represent\nthe HSI, and the difference between the reconstructed image and\nthe original image is used as the basis for anomaly judgment.\nLi et al. [18] proposed an anomaly detection method based\non background joint sparse representation. The joint sparse\nmodel constructs a sample set that can represent the background\ndistribution, and then, performs sparse representation of the\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nXIAO et al.: ANOMALY DETECTION OF HYPERSPECTRAL IMAGES BASED ON TRANSFORMER 1415\ntest pixel based on the sample set. The degree of difference\nbetween the test pixel and the original pixel reﬂects the anomaly\ndegree of the test pixel. Li et al. [19] described an collaborative\nrepresentation-based detector (CRD), which adopts the local\ndual-window strategy, and uses the pixels between the two\nwindows to carry out collaborative representation of the central\npixels.\nIn recent years, deep learning has achieved excellent results\nin the ﬁelds of computer vision and remote sensing [20], [21].\nFor example, Xie et al. designed a spectral–spatial anomaly\ndetection method for HSI, which is based on band selection.\nSpeciﬁcally, they fully utilized the underlying physical char-\nacteristics to train the unsupervised network [22]. Mihai et al.\nproposed a deep convolutional model to detect anomalies in\nburned area contexts of Sentinel-2 scenes, which adopt a self-\nsupervised paradigm to learn the image representations [23].T h e\nconvolution neural network (CNN) is the typical representative\nof deep learning, which needs sufﬁcient samples for supervised\ntraining [24], [25]. However, hyperspectral anomaly detection\ndoes not have any prior information and the data samples\nare limited. In order to solve this shortcoming, Li et al. [26]\ncombined the CNN with transfer learning to realize anomaly\ndetection. Compared with the CNN, which requires reference\nimages and a large number of data samples, autoencoder (AE),\ngenerative adversarial net (GAN) [27], and adversarial auto\nencoder (AAE) [28] are more suitable for HSI anomaly detec-\ntion. Zhang et al. [29] constructed an adaptive subspace model\nbased on stacked AEs to extract deep features with differences\nfor anomaly detection. Manifold learning is adopted in litera-\nture [30] to extract local spatial information and integrate it with\nAE. Xie et al. [31] introduced spectral constraint into AAE to\nsuppress background while maintaining abnormal characteristic\ninformation, making it easier to distinguish anomalies from\nbackground.\nAt present, the traditional methods and deep learning-based\nmethods both have obtained satisﬁed detection performance.\nHowever, most of the current research only consider spectral\ninformation or local spatial information. The traditional anomaly\ndetection methods have some limitations in extracting image\nfeatures manually, which may ignore a lot of details. Among\nthe deep-learning-based methods, those improved AE occupy\nthe majority and are usually trained by spectral vectors, so the\nexploration of spatial information is insufﬁcient. The anomaly\ndetection methods based on the CNN can only obtain local\nspatial information.\nTo overcome these limitations, this article proposes a hy-\nperspectral anomaly detection method named spatial–spectral\ndual-window mask transformer (S2DWMTrans) to make full\nuse of the spatial–spectral information of the HSI. First, convo-\nlution operation is introduced to give the model local receptive\nﬁelds, and then, the dual perspective spatial–spectral feature\nextraction and fusion module (DPS2FEFM) is used to extract\nspatial–spectral feature under two perspectives. In the global\nperspective, the background information with a high proportion\nis gathered. In the local perspective, the neighbor features in\nthe dual window are reﬁned and fused to reconstruct the image.\nIn addition, due to the relatively large reconstruction error of\nanomalies in the early training stage, the S2DWMTrans designed\nan adaptive weighted loss function (AWLF) to give a small\nweight to the pixels with large reconstruction error to reduce the\ncontribution to the total loss. So, the network not only realizes\nthe full extraction of spatial–spectral information, but also effec-\ntively suppresses anomalies and achieves accurate background\nreconstruction. In order to further improve the detection accu-\nracy, the input and reconstructed images are postprocessed. The\nerror of these two images is used as the weight, and a nonlinear\nmapping is introduced to enlarge the background and anomaly\nin the original image. Finally, the Mahalanobis distance detector\nis used to get the detection result.\nThe main contributions of this article are as follows.\n1) A novel S2DWMTrans is proposed to fully utilize\nthe spatial–spectral information in HSIs for competi-\ntive anomaly detection performance. Speciﬁcally, a dual-\nwindow mask transformer (DWMTrans) is contained in\nthe proposed S2DWMTrans, which can fully exploit the\ncombined spatial–spectral features of the HSI in the global\nand local perspectives for reconstruction, and exert differ-\nent degrees of suppression on anomalies respectively.\n2) An AWLF is designed to further suppress anomaly recon-\nstruction by reducing the weight of potential anomalies\nduring the network training process.\n3) We conduct several comparative experiments and ablation\nstudies on ﬁve datasets to demonstrate the advancement\nand effectiveness of our proposed S2DWMTrans. The de-\ntection performance outperforms many advanced methods\non all the datasets.\nII. R ELATED WORK\nThe transformer was ﬁrst proposed by Google re-\nsearchers [32] and applied in the ﬁeld of natural language\nprocessing, which is similar to AE in overall structure. A trans-\nformer consists of encoder and decoder, and requires input data\nin vector form. Dosovitskiy et al. [33] proposed the vision trans-\nformer model (ViT), which removes the decoder and only uses\nthe encoder structure. In order to meet the strict requirements of\nthe format of the transformer input data, the ViT has a prepro-\ncessing process that ﬂattens the input image into vectors. The\ntransformer despite the traditional CNN structure, whose whole\nnetwork architecture is completely composed of attention. To\nbe precise, the transformer only consists of self-attention mod-\nule and feed forward neural network (FFN). The self-attention\nmodule is the core component of the transformer [34], and has\nbeen applied to the many image recognition task and its speciﬁc\nstructure is shown in Fig. 1 [35], [36].\nThe input data X are ﬁrst transformed into Query (Q), Key\n(K), and Value (V) matrices through three linear transformations,\nwhich can be expressed as\nQ = XWq\nK = XWk\nV = XWv (1)\n1416 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 1. Flowchart of the the self-attention module.\nwhere Wq, Wk, and Wv denote the weight matrices of linear\ntransformations for generating the query, key, and value tensors,\nrespectively, which are trainable and can improve the model ﬁt\nability. Each line of Q, K, and V represents a sample data.\nMultiplying QT and K can calculate the similarity between\nthe extracted features Q and K, which indicates the similarity\nbetween the features of each pixel and other pixels. The inner\nproduct between each row vector of matrices Q and K is\ncalculated to get the weight matrix, which shows how much\nattention each pixel in the input gets when the model focuses\non a particular pixel. Then, the weight matrix is divided by\ndk to prevent the inner product from being too large, and the\nnormalized weight matrix is obtained by softmax function. The\ncomplete calculation formula can be expressed as\nZ = soft max\n(QT K√dk\n)\nV (2)\nwhere dk is the sample coding dimension.\nIII. M ETHODOLOGY\nA. Architecture Overview\nJust as with any other anomaly detection tasks, how to well\nrepresent the background samples while poorly perform the\nanomaly samples is the key to clearly distinguish small anomaly\nsamples. In this article, we propose a hyperspectral anomaly\ndetection method named S2DWMTrans to make full use of the\nspatial–spectral information to suppress the anomaly reconstruc-\ntion.\nThe overall architecture of the proposed S2DWMTrans is\nrepresented in Fig. 2, which mainly consists of three parts, which\nare as follows.\n1) A DWMTrans is constructed for fully digging the spatial–\nspectral feature in global and local perspectives for re-\nconstruction, and applying different degrees of inhibitory\neffect on the anomaly samples.\n2) An AWLF is designed to further suppress anomaly recon-\nstruction by reducing the weights of potential anomalies\nduring network training.\n3) A postprocessing module is applied in order to further\nimprove the detection accuracy.\nB. Dual-Window Mask Transformer (DWMTrans)\nA DWMTrans is constructed for fully extracting spatial–\nspectral features from both global and local perspectives. A local\nshallow feature extraction module (LSFEM) is ﬁrst designed to\ninitially extract local features and inject adjacent information\ninto the spectral vectors, and thus, enhance the information in-\nteraction between image pixels and surrounding areas. Next, the\nspectral vectors are fed into the DPS2FEFM, which consists of\nseveral cascaded dual-window mask encoder block (DWMEB),\nto fuse the spatial–spectral information from two perspectives to\njointly suppress anomaly reconstruction. Speciﬁcally, the mul-\ntihead self-attention (MSA) in DWMEB with a global receptive\nﬁeld is used to gather background information of the whole\nimage to neutralize anomalies. In the local view, the information\nunder the dual-window mask are mined by the constructed\ndual-window mask multihead self-attention (DWM-MSA) in\nDWMEB to suppress anomaly reconstruction. Finally, the re-\nﬁned features are passed through the background reconstruction\nmodule (BRM) to accurately learn the background distribution\nfunction.\n1) Local Shallow Feature Extraction Module (LSFEM):In\norder to enhance the information interaction between pixels and\nsurrounding areas, the input HSI image of the DWMTrans is ﬁrst\npassed through an LSFEM. As shown in Fig. 2, the conv layer of\nthe LSFEM can traverse the entire image to extract local features,\nand gather the feature information in the convolution kernel\narea to the center pixel. The following batch normalization\n(BN) accelerates the speed of the network training and prevents\noverﬁtting. Then, a rectiﬁed liner unit (ReLu) layer is followed\nto learn a nonlinear representation. Since HSIs have multiple\nspectral bands and rich spectral information, the spectral vector\nof a single pixel can act as an input vector of the transformer.\nGiven the HSI patch H ∈ RM×N×B with M × N pixels and B\nspectral bands as the input of the proposed S2DWMTrans, which\ncan also be regarded as M ×N vectors with B dimensions. The\naforementioned process of the LSFEM can be more intuitively\nformulated as\nY = fLSFEM(H)= fconv(H) (3)\ny = Flatten(Y)=[ y1,y2,... yn] (4)\nwhere Y represents the output of the LSFEM, y represents the\nﬂattened vectors as input to the next module. As the input of the\nnext module, all the spectral vectors passing through the LSFEM\ncarry the local spatial information in the vicinity, and thus, fuse\nthe local and global information.\n2) Dual Perspective Spatial–Spectral Feature Extraction and\nFusion Module (DPS2FEFM):Since most regions in HSIs are\nbackground, it is necessary to combine global features of back-\nground to weaken anomaly reconstruction using background\ninformation. At the same time, considering that the spectral\ncharacteristics of abnormal pixels are different from those of\nneighboring pixels, it is necessary to obtain detailed information\nof neighboring regions to represent abnormal pixels and suppress\nanomaly reconstruction.\nWe design a DPS2FEFM that can obtain the global and local\nspatial–spectral information simultaneously. As shown in Fig. 2,\nthe DPS2FEFM is a stack of the DWMEB, whose calculation\nformula is as follows:\nFDPS2FEFM = FDWMEB,N ...F DWMEB,i...F DWMEB,1[y] (5)\nXIAO et al.: ANOMALY DETECTION OF HYPERSPECTRAL IMAGES BASED ON TRANSFORMER 1417\nFig. 2. Architecture of the proposed S2DWMTrans.\nFig. 3. Flowchart of the the DWMEB.\nwhere FDWMEB,i[·] represents the function of the ith DWMEB.\nFig. 3 shows the speciﬁc structure of the DWMEB, which con-\nsists of two consecutive encoder-based structures. The ﬁrst one is\nthe same as transformer encoder in the ViT and is used to extract\nglobal features. The second one designs a DWM-MSA, which\nlimits the calculation of the attention weight of self-attention to\na dual-window region and is used to extract local features. The\nfunction of the DWMEB can be expressed as\nFDWMEB,i = FDWM-MSA,i [FMSA,i (oi−1)] (6)\nwhere FMSA,i(·) represents the function of the ﬁrst encoder,\nFDWM-MSA,i(·) represents the function of the DWM-MSA-based\nstructure, and oi−1 represents the output of i-1th DWMEB.\nFMSA,i(·) is replaced by m for the sake of representation\nm = FMSA,i(oi−1)= oi−1 + fMSA,i(LN(oi−1))\n+ FFN[LN[oi−1 + fMSA,i (LN(oi−1))]] (7)\nwhere fMSA,i(·) represents the function of the MSA block.\nFDWM-MSA,i(·) can be described in detail as follows:\nFDWM-MSA,i = m + fDWM-MSA,i(LN(m))\n+ FFN[LN[m + fDWM-MSA,i(LN(m))]] (8)\nwhere fDWM-MSA,i(·) represents the function of the DWM-MSA\nblock.\nThe MSA is same as the multihead self-attention module in\nthe ViT, whose formula is as follows:\n{\nag = ∑ Ng\na\ni=1 wag\ni ag\ni + ∑ Ng\nb\ni=1 wbg\ni bg\ni\nbg = ∑ Ng\na\ni=1 wag\ni ag\ni + ∑ Ng\nb\ni=1 wbg\ni bg\ni\n(9)\nwhere ag and bg are vectors of abnormal and background pixels,\nrespectively. g is short for global. wag\ni is the attention weights\nof pixels to be reconstructed and abnormal pixels. wbg\ni is the\n1418 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 4. Flowchart of the local attention-based reconstruction.\nattention weights of pixels to be reconstructed and background\npixels. Ng\na and Ng\nb are the total number of abnormal and back-\nground pixels in the corresponding region, respectively, which\nremain unchanged. If the pixel to be reconstructed is abnormal,\nwbg\ni is far less than wag\ni . However, Ng\nb is much larger than\nNg\na , which makes the majority of background pixels affect the\nreconstruction of anomalies. If the pixel to be reconstructed\nis background, most of the background pixels can be well\nrepresented, and only a few small background regions deviating\nfrom the overall background area will be considered as abnormal\npixels.\nDWM-MSA only calculates the attention weight of all pixels\nbetween the inner and outer windows and the central pixels, that\nis, adding a dual-window attention mask B. Fig. 4 shows the\nschematic of B, and its calculation formula is as follows:\nZ = WV = softmax\n[QKT\n√dk\n+ B\n]\nV (10)\nwhere W represents the attention weight between pixels to be\nreconstructed and other pixels, and Z is the output of DWM-\nMSA. The detailed calculation formula of DWM-MSA can be\nexpressed as\nal =\nNl\na∑\ni=1\nwal\ni al\ni +\nNl\nb∑\ni=1\nwbl\ni bl\ni\nbl =\nNl\na∑\ni=1\nwal\ni al\ni +\nNl\nb∑\ni=1\nwbl\ni bl\ni (11)\nwhere l is short for local. Nl\na and Nl\nb will change with the size\nof dual window mask.\nAs the pixel to be reconstructed may be background or\nanomaly, formula (9) will correspond to the following four\nsituations as shown in Fig. 5.\n1) As shown in Fig. 5(a), when the pixel to be reconstructed\nis abnormal and Nl\nb is much larger than Nl\na, similar to\nglobal reconstruction, the background pixels will inhibit\nthe reconstruction of abnormal pixel.\n2) As shown in Fig. 5(b), the pixel to be reconstructed is\nabnormal and Nl\nb is close to Nl\na. This situation is very\nrare and can be solved by adjusting the size of the inner\nand outer windows.\n3) As shown in Fig. 5(c), when the pixel to be reconstructed\nis background and Nl\na is zero, the pixel will be completely\nreconstructed.\nFig. 5. Diagram of four different reconstruction. The red star represents the\nabnormal pixel, and the black and white circles represent the background pixel\nwith different distributions.\n4 ) A ss h o w ni nF i g . 5(d), the pixel to be reconstructed is\nbackground, and Nl\na is the total number of background\npixels with different distributions and abnormal pixels.\nIn the previous MSA, a small number of background pixels\nwith a different distribution than the one to be reconstructed\nwere considered anomalies. This situation will be improved in\nthe DWM-MSA. Since most of pixels in the dual window are\nbackground pixels with similar distribution and wbl is much\nlarger than wal, the small background area will slowly return to\nits own distribution. MSA and DWM-MSA appear in pairs in the\nDWMEB, which fully integrate global and local features, effec-\ntively suppressing anomalies and highlighting the background.\n3) Background Reconstruction Module (BRM): The BRM\nreconstructs the deep features extracted previously. The module\nﬁrst uses a conv layer to adjust the number of spectral bands of\nthe image. The subsequent sigmoid function is used to limit the\nvalue of the output image between 0 and 1, which is convenient\nto calculate the loss with the input image.\nC. Adaptive-Weighted Loss Function (AWLF)\nCompared with the background, anomalies account for a small\nproportion in HSIs and usually exist in irregular small areas,\nwhich are difﬁcult to reconstruct. However, the conv layers\nand DWMEB with strong learning ability have made the total\nloss in the training process decrease continuously. As the data\nﬁtting degree of the network gradually increased, anomalies will\ngradually be reconstructed. In order to curb the training trend and\nfurther suppress the anomalies, the reconstruction errors in the\ntraining process can be used to accurately suppress the potential\nanomalies.\nSince anomaly pixels have a large reconstruction error in the\nearly stage of network training, the AWLF is designed to reduce\nthe weight of areas with large reconstruction errors and increase\nthe weight of other areas when calculating loss. This function\nhelps suppress the reconstruction of anomalies and highlights\nthe reconstruction of the background. The reconstruction error\nXIAO et al.: ANOMALY DETECTION OF HYPERSPECTRAL IMAGES BASED ON TRANSFORMER 1419\nFig. 6. (a) Distribution of anomalies. (b) Distribution of weights.\ncalculation formula of a single pixel can be expressed as\nei,j = ∥xi,j −ˆxi,j∥2 (12)\nwhere xi,j and ˆxi,j are the spectral vectors of the input image\nand reconstructed image at position ( i,j), respectively. Further,\nthe weight of the pixel can be expressed as\nwi,j =\n(\nNorm\n(\n1\nei,j\n)\n+1 −exp (−10 ×ei,j)\n)\n2 (13)\nwhere Norm( ·) represents normalization. The distribution of\npixels and their corresponding weights is shown in the Fig. 6.\nPotential anomalies [yellow areas in Fig. 6(a)] will be given\nsmaller weights [darker blue areas in Fig. 6(b)].\nIn the training process of the DWMTrans, wi,j is updated\nevery 20 epochs. During the ﬁrst 20 epochs, wi,j is initialized to\n1, and its value would not change after ﬁve times of updating.\nThe adaptive-weighted loss function L based on wi,j can be\nexpressed as\nL = 1\nH ×W\nH∑\ni=1\nW∑\nj=1\n∥(xi,j −ˆxi,j) ×wi,j∥2\n+ 1\nπ ×H ×W\nH∑\ni=1\nW∑\nj=1\narccos\n( ⟨xi,j,ˆxi,j⟩\n∥xi,j∥2∥ˆxi,j∥2\n×wi,j\n)\n(14)\nwhere H and W are the height and width of the image, re-\nspectively, and ⟨xi,j,ˆxi,j⟩represents the inner product of two\nspectral vectors. The ﬁrst term of L uses the mean square\nerror (MSE) to quantify spatial differences between the input\nimage and the reconstructed image, while the second term uses\nspectral angle mapper (SAM) to constrain the spectral similarity\nbetween images. Each term is multiplied by the weight wi,j\nwhen calculating the error at a single pixel. Because the weight\ncorresponding to the abnormal pixel is small, its loss contributes\nlittle to the total loss, and thus, the abnormal pixel avoids being\nreconstructed by the network.\nD. Postprocessing\nThe proposed DWMTrans obtains the network mapping func-\ntion FDWMTrans after completing the iterative training process.\nThe input HSI H is passed through FDWMTrans and obtain the\nabnormal inhibited reconstructed image ˆH, which can be ex-\npressed as\nˆH = FDWMTrans(H). (15)\nIn order to improve detection accuracy, postprocessing of input\nand reconstructed images is carried out as shown in Fig. 2.\nThe postprocessing module ﬁrst calculates the reconstruction\nerror between the two images and averages it on the channel\ndimension, so as to obtain the preliminary anomaly detection\nresult D1. D1 can be expressed as,\nω =1 −exp (−10D1) (16)\nwhere ω ranges from 0 to 1. The reconstruction error corre-\nsponding to the abnormal pixel is relatively large, so its weight\nis close to 1, while the reconstruction error corresponding to\nthe background pixel is very small, so its weight is close to\n0. Then, the weight matrix ω is multiplied by the input H\nto increase the separability of background and anomaly. The\nobtained background suppressed image can be expressed as\nH′=( 1−exp (−10D1)) H. (17)\nAfter this operation, the abnormal pixels with large D1 will\nmaintain the spectral vector in H, while the spectral vector of\nbackground pixels with small D1 will be suppressed. Finally,\nthe obtained H′ has great separability in anomaly and back-\nground, and anomaly detection can be realized by calculating\nthe Mahalanobis distance on H′.\nIV . E XPERIMENTAL RESULTS AND ANALYSIS\nA. Datasets and Experimental Setup\nIn this article, ﬁve real hyperspectral datasets are used to verify\nthe effectiveness of the proposed S2DWMTrans 1 [37], whose\npseudocolor and reference images are shown in Fig. 7. Gulfport\ndataset was taken by Airborne Visible/Infrared Imaging Spec-\ntrometer Sensor (A VIRIS) in 2010 over Gulfport, USA, with the\nground spatial resolution of 3.4 m. The size of Gulfport dataset\nand its ground truth is 100 ×100. Three aircraft contained in the\nimage features were taken as abnormal targets, which occupies\n155 60 pixels. After the contaminated bands are removed, 191\nspectral bands are left, covering a spectral range from 400 to\n2500 nm.\nPavia dataset was taken by the reﬂective optics system imag-\ning spectrometer sensor (ROSIS) in Pavia, Italy. The image\ninclude rivers, bridges, soil, and buildings. The resolution of the\nimage is 150 ×150, with 175 spectral bands and a wavelength\nrange of 430 to 860 nm. The ground spatial resolution is 1.3 m.\nThe size of the ground truth of Pavia dataset is 150 ×150, in\nwhich some vehicles on the bridge occupying a total of 68 pixels\nare considered as abnormal targets.\nTexas Coast dataset was acquired by an A VIRIS sensor in\nAugust 2010 over the coast of Texas, USA, with a spatial\nresolution of 17.2 m. The image contains 207 bands, each with a\nsize of 100 ×100, so does its ground truth. There is a parking lot\n1http://xudongkang.weebly.com/\n1420 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 7. Pseudocolor diagram and reference diagram of (a) Gulfport dataset, (b) Pavia dataset, (c) Texas Coast dataset, (d) Los Angeles dataset, and (e ) Muuﬂ\ndataset.\nin the ground object scene, in which some vehicles occupying a\ntotal of 272 pixels are marked as abnormal targets.\nLos Angeles dataset was acquired by the A VIRIS sensor in\nNovember 2011 in Los Angeles, USA, with a spatial resolution\nof 7.1 m. The image scene and its ground truth both cover\n100×100 pixels and has 205 spectral bands, among which some\nnoise bands have been removed. As for Los Angeles dataset,\nsome storage tanks occupying a total of 272 pixels are considered\nas targets.\nMuuﬂ dataset was acquired by the ITRES Research Ltd.\n(ITRES) Compact Airborne Spectrographic Imager (CASI)-\n1500 sensor in 2010, covering the University of Southern Missis-\nsippi Gulfport Campus, Mississippi, USA. 2 The image contains\n64 bands, covering the wavelength range of 367.7–1043.4 nm.\nThe size of Gulfport dataset and its ground truth is 220 ×325,\nand the spatial resolution is 0.54 ×1.0 m. In this article, we crop a\nsubimage of 140 ×280 pixels from the upper right of the whole\nimage, in which four cloths occupying a total of 269 pixels are\nconsidered as targets.\nSome of the anomalies in the ﬁve experimental datasets are in\nthe form of points, some show small irregular regions, and some\nhave obvious structural information. In addition, their spatial\nresolution, number of spectral bands and ground object scenes\nare mostly different. Therefore, experiments on these datasets\ncan effectively verify the effectiveness and robustness of the\nproposed method.\nThe proposed DWMTrans is implemented with the PyTorch\n1.7 and Python 3.7 on Ubuntu, and trained on the GeForce GRX\n3090 GPU. The postprocessing is implemented with the MAT-\nLAB 2018a on Windows10, and trained on the CPU Intel(R)\nCore(TM) i7-9750H. DWMTrans uses Adam [38] optimization\nto ﬁnd the optimal solution, and the learning rate is set as 0.0001.\nEach batch of sample size is the number of pixels in the whole\nimage, and the number of training epochs is 400. The number\nof output nodes of the LSFEM is 128, the dimension of the\n2https://datasets.bifrost.ai/info/1773\nspectral vector is set to 64, and the number of nodes of the two\nfull connection layers in the FFN is 256 and 128, respectively.\nB. Quality Assessment\nThere are qualitative and quantitatively indicators to evaluate\nHSI anomaly detection. One of these indicators is receiver\noperating characteristic (ROC), which is determined by the\nprobability of detection Pd and false alarm rate Pf of abnormal\ndetection results. Pd and Pf can be expressed as\nPd = Nd\nNt\n(18)\nPf = Nf\nNtotal\n(19)\nwhere Nd, Nt, Nf , and Ntotal represent the number of detected\nabnormal pixels, the number of abnormal pixels in the reference\nimage, the number of background pixels mistaken as abnormal\npixels in the detection result, the total number of pixels in HSIs,\nrespectively. Taking Pf as abscissa and Pd as ordinate, the\ncorresponding ROC curve can be drawn. The closer the ROC\ncurve is to the upper left corner, the better the performance of\nthe method. Another metric is area under curve (AUC), namely,\nthe area under the ROC curve, which can be expressed as\nAUC =\n∫ 1\n0\nROC(x)dx. (20)\nThe higher the AUC value is, the better the performance of the\ncorresponding algorithm is. Precision-recall curve (PRC) is also\nan indicator we used. The closer the PRC curve is to the upper\nright corner, the better the performance of the model.\nC. Ablation Study\n1) Parameters Analysis:In order to make the detection result\nof the proposed method as optimal as possible, multiple exper-\niments are conducted to select relatively optimal parameters.\na) Sizes of winin and winout: The most important factors\ninﬂuencing the choice of win in and win out is the size of the\nXIAO et al.: ANOMALY DETECTION OF HYPERSPECTRAL IMAGES BASED ON TRANSFORMER 1421\nTABLE I\nAUC OF DIFFERENT PARAMETER COMBINATIONS FOR PAV I ADATASET\nTABLE II\nAUC OF DIFFERENT PARAMETER COMBINATIONS FOR TEXAS COAST DATASET\nTABLE III\nAUC OF DIFFERENT PARAMETER COMBINATIONS FOR LOS ANGELES DATASET\nTABLE IV\nAUC OF DIFFERENT PARAMETER COMBINATIONS FOR GULFPORT DATASET\nTABLE V\nAUC OF DIFFERENT PARAMETER COMBINATIONS FOR MUUFL DATASET\nanomaly target contained in the image. The value range of\nwinin and winout is set as [3,5,7,9] and [7,9,11,13], respectively.\nConsidering winin < winout, there are 13 different combinations\nof window parameters. To determine the optimal settings for\nwinin and win out, we performed parameters analysis on all ﬁve\ndatasets for different parameter combinations, and AUC values\nare used for judging indicators. Tables I–V show AUC values of\ndifferent parameter combinations for Pavia dataset, Texas Coast\ndataset, Los Angeles dataset, Gulfport dataset, and MUUFL\ndataset, respectively. From all the experimental results, it can be\nFig. 8. AUC of different chead on three datasets. (a) Gulfport. (b) Texas Coast.\n(c) Los Angeles.\nFig. 9. AUC of different cdepth on three datasets. (a) Gulfport. (b) Texas Coast.\n(c) Los Angeles.\nseen that when win in=3,5,7 and win out=13, the value of AUC\nis the optimal.\nb) Number of MSA and DWM-MSA chead: MSA and\nDWM-MSA appear in pairs in the DWMEB, which fully inte-\ngrates global and local features, effectively suppressing anoma-\nlies and highlighting the background. If the number of MSA\nand DWM-MSA chead is too small, the network model will not\npay enough attention to the image information. If chead is too\nlarge, the calculation will increase cost exponentially. So, chead\nis set from 1 to 4. In order to determine the optimal chead,w e\nexperimented with different values of chead on three datasets.\nAUC values are used for judging indicators. Fig. 8 shows AUC\nvalues of different chead on Gulfport dataset, Texas Coast dataset,\nand Los Angeles dataset, respectively.\nc) Number of DWMEB cdepth: The DWMEB can fully\nexcavate the spatial–spectral features of the image from the\nglobal and local perspectives for reconstruction, and exert dif-\nferent degrees of suppression on the anomalies. The number\nof the DWMEB cdepth is a key parameter that determines the\nperformance of the network. If the network is too shallow, it is\nnot enough to extract abstract semantic features. If the network\nis too deep, it is easy to cause overﬁtting. As DWMEB contains\ntwo transformer encoder, the value range of cdepth is set from 1 to\n4. Fig. 9 shows AUC values of different cdepth on three datasets,\nfrom which can be seen that when cdepth = 1o r2 ,t h ev a l u eo f\nAUC has a high probability of obtaining the optimal value.\n2) Component Analysis: In order to verify the effectiveness\nof some modules or components in the proposed S2DWMTrans,\nmultiple ablation experiments are carried out. The experimental\nresults of all the ﬁve datasets of ablation experiments are shown\nin Fig. 10, where Base represents the proposed S2DWMTrans\nwithout any changes, Base-LSFEM represents that the LSFEM\nmodule is removed, Base (MSA) represents that the DWM-MSA\nis replaced by MSA, Base-AWLF represents that the use of\nunweighted loss functions for back propagation of the network.\n1422 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 10. AUC of different variants of the proposed S2DWMTrans on ﬁve datasets. (a) Pavia. (b) Texas Coast. (c) Los Angeles. (d) Gulfport. (e) MUUFL.\na) Effectiveness of the LSFEM:The main function of LS-\nFEM is to obtain the spectral vectors with local spatial infor-\nmation, and serve as the input of subsequent transformer to\npromote the fusion of local and global information. The variable\ncondition of the ﬁrst ablation experiment is whether the LSFEM\nexists. As we can see from Fig. 10, the use of LSFEM has\nresulted in a signiﬁcant increase in AUC values on all datasets.\nFor example, it can be seen from Fig. 10(a) and (b) that the use of\nLSFEM increases AUC by 1.75% and 0.3% on Pavia dataset and\nTexas Coast dataset, respectively, which shows the advantages\nof LSFEM.\nb) Effectiveness of the DWM-MSA:The DWMEB consists\nof two consecutive transformer encoder, of which the second\nencoder adopts DWM-MSA to limit the calculation of attention\nweight to a region between dual window, so as to obtain more\ndetailed local information. The variable condition of the second\nablation experiment is to replace all DWM-MSA in the network\nwith ordinary MSA. As can be seen from Fig. 10, compared\nwith the model without DWMEB, the proposed model achieves\nhigher AUC on all datasets. For example, the use of the DWMEB\nhas resulted in a signiﬁcant increase by 0.56% on Los Angeles\ndataset, which shows the advantages of DWM-MSA.\nc) Effectiveness of the AWLF:The AWLF assigns a small\nweight to potential abnormal pixels with bigger reconstruction\nerror, so as to reduce the contribution of anomalies to the\ntotal loss, and thus, achieve the purpose of inhibiting abnor-\nmal reconstruction. The variable condition of the third ablation\nexperiment is whether different weights are assigned to image\npixels during the loss calculation. As shown in Fig. 10,t h e\ncomparative experimental results between Base-AWLF and the\nproposed model prove the importance of AWLF for improv-\ning AUC. As we can see from Fig. 10, the result of Base is\nalways the best on ﬁve datasets. The addition of three mod-\nules or components will give positive feedback to the network,\nwhich veriﬁes the effectiveness and robustness of the proposed\nS2DWMTrans.\nD. Experimental Result\nIn order to effectively evaluate the detection performance of\nthe proposed S2DWMTrans, ﬁve advanced HSI anomaly detec-\ntion methods are selected for comparison, including RX [14],\nCRD [19], LRaSMD [39],A E D[37], and Auto-AD method [40].\nRX is a pioneering work in the ﬁeld of hyperspectral anomaly\ndetection, which makes use of the statistical characteristics of\nFig. 11. Detection results of different methods on Gulfport dataset.\n(a) Pseudocolor Image. (b) Reference. (c) RX. (d) CRD. (e) LRaSMD. (f) AED.\n(g) Auto-AD. (h) S2DWMTrans.\nimage data to judge the pixels. CRD is a typical detection method\nbased on reconstruction. LRaSMD is a low-rank and sparse ma-\ntrix factorization-based method. AED realizes detection by ﬁl-\ntering and differential operations on spatial attributes. Auto-AD\nuses a CNN with skip connection to reconstruct the background\nand suppress anomalies at the same time. The reconstruction\nerror of each pixel is calculated to obtain the anomaly detection\ngraph.\nThe detection results of Gulfport dataset are shown in Fig. 11,\nin which CRD almost fails to correctly detect abnormal targets.\nLRaSMD and AED detect the trafﬁc dotted line on the highway.\nLRaSMD fails to detect the two small planes, while AED regards\nthe dotted line on the highway as the most likely abnormal target,\nleading to obvious false detection. RX and Auto-AD only detect\nlarger aircraft targets, while the two small aircraft are faintly\nvisible.\nFig. 12 shows the detection results on Pavia dataset. The\ndetection effect of RX, CRD, and LRaSMD is not obvious. AED\nmistakenly detects the large area beach and long strip bridge in\nthe image as abnormal targets. Auto-AD also detects the edge of\nthe beach and bridge, which may be due to the obvious feature\ntransformation at the junction of various scenes.\nThe detection results on Texas Coast dataset are shown in\nFig. 13. Only a few abnormal targets are detected by CRD. RX\nand AED also miss some abnormal targets, and AED misdetects\nin the lower left and right corner of the image where there\nwere no abnormal targets. Although LRaSMD and Auto-AD\nhave detected almost all abnormal targets, their conﬁdence of\nXIAO et al.: ANOMALY DETECTION OF HYPERSPECTRAL IMAGES BASED ON TRANSFORMER 1423\nFig. 12. Detection results of different methods on Pavia dataset. (a) Pseu-\ndocolor Image. (b) Reference. (c) RX. (d) CRD. (e) LRaSMD. (f) AED.\n(g) Auto-AD. (h) S2DWMTrans.\nFig. 13. Detection results of different methods on Texas Coast dataset.\n(a) Pseudocolor Image. (b) Reference. (c) RX. (d) CRD. (e) LRaSMD. (f) AED.\n(g) Auto-AD. (h) S2DWMTrans.\nFig. 14. Detection results of different methods on Los Angeles dataset.\n(a) Pseudocolor Image. (b) Reference. (c) RX. (d) CRD. (e) LRaSMD. (f) AED.\n(g) Auto-AD. (h) S2DWMTrans.\nsome abnormal target regions is relatively lower than that of the\nproposed S2DWMTrans.\nThe detection results on Los Angeles dataset are shown in\nFig. 14. Among them, CRD and LRaSMD only detect two obvi-\nous abnormal targets. The abnormal targets detected by AED are\nvery fuzzy, resulting in misjudgment at the connection between\nFig. 15. Detection results of different methods on MUUFL dataset.\n(a) Pseudocolor Image. (b) Reference. (c) RX. (d) CRD. (e) LRaSMD. (f) AED.\n(g) Auto-AD. (h) S2DWMTrans.\nTABLE VI\nAUC VALUES OF DIFFERENT METHODS\nabnormal targets. RX and Auto-AD have good detection results,\nbut there is also a small area of error detection in the upper right\ncorner of the image.\nFig. 15 shows the detection results on MUUFL dataset. It\ncan be obviously observed that the proposed method obtains\nexcellent performance. It can be seen that CRD hardly works on\nthis image. As for other competitors, AED and Auto-AD both\nhave high false alarm rate, which is easy to judge the background\nas abnormal. RX and LRaSMD all achieve quite nice results.\nThe proposed S2DWMTrans can suppress the reconstruction of\nbackground more effectively so as to highlight the anomalous\ntarget with the least false detection.According to the results of the\nfour datasets, the S2DWMTrans detects more abnormal targets\nand suppresses most of the background. The S2DWMTrans can\neffectively achieve the separation of background and abnormal\ntarget, and has a low rate of missed and false detection.\nBoth qualitative and quantitative indicators are integrated\nto jointly evaluate the proposed method. Table VI lists the\ncorresponding AUC values on ﬁve datasets, where the values\n1424 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 16. ROC curves of different methods. (a) Pavia. (b) Texas Coast. (c) Los Angeles. (d) Gulfport. (e) MUUFL.\nFig. 17. PRC curves of different methods. (a) Pavia. (b) Texas Coast. (c) Los Angeles. (d) Gulfport. (e) MUUFL.\nin bold and underlined represent the optimal and worst results.\nIt can be seen from the table that the proposed S2DWMTrans\nhas the highest AUC value on all datasets, which means that\nthe S2DWMTrans has the best detection ability on the se-\nlected datasets. As can be seen from Fig. 16, the ROC curves\nof the S2DWMTrans are almost always higher than those of\nother methods. Combined with the two quantitative indexes, the\nS2DWMTrans has the best detection effect on the ﬁve datasets.\nFig. 17 shows the PRC curves of different methods on ﬁve\ndatasets. Compared with other methods, it can be observed\nthat the PRC curves of the proposed S2DWMTrans is closer\nto the upper right corner, which reﬂects that the S2DWMTrans\nachieves higher detection performance. In Fig. 17(c), the PRC\ncurve produced by the S2DWMTrans is consistently at the\ntop, indicating that our method is optimal on the Los Angeles\ndataset. The area under the PRC curves of the S2DWMTrans\nfor the all ﬁve datasets are almost bigger than those of RX,\nCRD, LRaSMD, AED, and Auto-AD, which indicates that the\nproposed S2DWMTrans has satisfactory target detection and\nbackground suppression capabilities.\nV. C ONCLUSION\nIn this article, we present a new hyperspectral anomaly de-\ntection method based on the S2DWMTrans to solve the prob-\nlem that the existing methods do not make full use of the\nspatial–spectral information of HSIs. The proposed method fully\nextracts spatial–spectral joint features from global and local\nperspectives by using the constructed DWMTrans, and further\nreconstructs them from these two perspectives. In the global\nperspective, all background information is integrated to weaken\nthe abnormal features. In the local perspective, the neighbor\ninformation is used to greatly constrain the abnormal features,\nso as to effectively inhibit the reconstruction of abnormal targets.\nIn order to further reconstruct image background and suppress\nanomalies, an AWLF is proposed to suppress potential abnormal\nreconstruction precisely. The experiments of the S2DWMTrans\nare carried out on ﬁve datasets to evaluate the effectiveness and\nrobustness of the proposed method.\nREFERENCES\n[1] H. Su, Y . Yu, Z. Wu, and Q. Du, “Random subspace-based k-nearest class\ncollaborative representation for hyperspectral image classiﬁcation,” IEEE\nTrans. Geosci. Remote Sens., vol. 59, no. 8, pp. 6840–6853, Aug. 2021.\n[2] J. M. Bioucas-Dias, A. Plaza, G. Camps-Valls, P. Scheunders, N.\nNasrabadi, and J. Chanussot, “Hyperspectral remote sensing data analysis\nand future challenges,” IEEE Trans. Geosci. Remote Sens. Mag.,v o l .1 ,\nno. 2, pp. 6–36, Jun. 2013.\n[3] Y . Li, J. Wang, X. Liu, N. Xian, and C. Xie “DIM moving target de-\ntection using spatio-temporal anomaly detection for hyperspectral image\nsequences,” inProc. IGARSS IEEE Int. Geosci. Remote Sens. Symp., 2018,\npp. 7086–7089, doi: 10.1109/IGARSS.2018.8517601.\n[4] C.-I. Chang, “Target-to-anomaly conversion for hyperspectral anomaly\ndetection,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–28, 2022,\nArt. no. 5540428, doi: 10.1109/TGRS.2022.3211696.\n[5] M. Zhang, W. Li, and Q. Du, “Diverse region-based CNN for hyperspec-\ntral image classiﬁcation,” IEEE Trans. Image Process., vol. 27, no. 6,\npp. 2623–2634, Jun. 2018.\n[6] W. Dong, T. Zhang, J. Qu, S. Xiao, T. Zhang, and Y . Li, “Multibranch\nfeature fusion network with self- and cross-guided attention for hyper-\nspectral and LiDAR classiﬁcation,” IEEE Trans. Geosci. Remote Sens.,\nvol. 60, pp. 1–12, Jun. 7, 2022, Art. no. 5530612.\n[7] B. Du, L. Zhang, D. Tao, and D. Zhang, “Unsupervised transfer learning\nfor target detection from hyperspectral images,” Neurocomputing,v o l .1 ,\nno. 59, pp. 72–82, 2013.\n[8] D. W. J. Stein, S. G. Beaven, L. E. Hoff, E. M. Winter, A. P. Schaum, and\nA. D. Stocker, “Anomaly detection from hyperspectral imagery,” IEEE\nSignal Process. Mag., vol. 19, no. 1, pp. 58–69, Jan. 2002.\n[9] K. Jiang, W. Xie, Y . Li, J. Lei, G. He, and Q. Du, “Semisupervised\nspectral learning with generative adversarial network for hyperspectral\nanomaly detection,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 7,\npp. 5224–5236, Jul. 2020.\n[10] S. Li, K. Zhang, P. Duan, and X. Kang, “Hyperspectral anomaly detection\nwith kernel isolation forest,” IEEE Trans. Geosci. Remote Sens., vol. 58,\nno. 1, pp. 319–329, Jan. 2020.\n[11] F. Verdoja and M. Grangetto, “Graph Laplacian for image anomaly detec-\ntion,”Mach. Vis. Appl., vol. 1, no. 59, 2020, Art. no. 11.\n[12] I. S. Reed and X. Yu, “Adaptive multiple-band CFAR detection\nof an optical pattern with unknown spectral distribution,” IEEE\nTrans. Acoust. Speech Signal Process., vol. 8, no. 10, pp. 1760–1770,\nOct. 1990.\nXIAO et al.: ANOMALY DETECTION OF HYPERSPECTRAL IMAGES BASED ON TRANSFORMER 1425\n[13] H. Kwon, S. Z. Der, and N. M. Nasrabadi, “Adaptive anomaly detection\nusing subspace separation for hyperspectral imagery,” Opt. Eng.,v o l .1 ,\nno. 59, pp. 3342–3351, 2003.\n[14] Q. Guo, B. Zhang, and Q. Ran, “Weighted-RXD and linear ﬁlter-based\nRXD: Improving background statistics estimation for anomaly detection\nin hyperspectral imagery,”IEEE J. Sel. Topics. Appl. Earth Observ. Remote\nSens., vol. 7, no. 6, pp. 2351–2366, Jun. 2014.\n[15] Y . Zhang, Y . Dong, K. Wu, and T. Chen, “Hyperspectral anomaly detection\nwith Otsu-based isolation forest,” IEEE J. Sel. Topics Appl. Earth Observ.\nRemote Sens., vol. 14, pp. 9079–9088, 2021.\n[16] S. Arisoy and K. Kayabol, “Nonparametric Bayesian background estima-\ntion for hyperspectral anomaly detection,” Digit. Signal Process., vol. 111,\nApr. 2021, Art. no. 102993.\n[17] C.-I. Chang, H. Cao, and M. Song, “Orthogonal subspace projection target\ndetector for hyperspectral anomaly detection,” IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 14, pp. 4915–4932, Mar. 25, 2021.\n[18] J. Li, H. Zhang, L. Zhang, and L. Ma, “Hyperspectral anomaly detec-\ntion by the use of background joint sparse representation,” IEEE J. Sel.\nTopics Appl. Earth Observ. Remote Sens., vol. 8, no. 6, pp. 2523–2533,\nJun. 2015.\n[19] W. Li and Q. Du, “Collaborative representation for hyperspectral\nanomaly detection,” IEEE Trans. Geosci. Remote Sens., vol. 53, no. 3,\npp. 1463–1474, Mar. 2015.\n[20] W. Dong, T. Zhang, J. Qu, S. Xiao, J. Liang, and Y . Li, “Laplacian pyramid\ndense network for hyperspectral pansharpening,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–13, May 14, 2021, Art. no. 5507113.\n[21] W. Dong, S. Hou, S. Xiao, J. Qu, Q. Du, and Y . Li, “Generative dual-\nadversarial network with spectral ﬁdelity and spatial enhancement for\nhyperspectral pansharpening,” IEEE Trans. Neural Netw. Learn. Syst.,\nvol. 33, no. 12, pp. 7303–7317, Dec. 2021.\n[22] W. Xie, Y . Li, J. Lei, J. Yang, C.-I. Chang, and Z. Li, “Hyperspectral band\nselection for spectral–spatial anomaly detection,” IEEE Trans. Geosci.\nRemote Sens., vol. 58, no. 5, pp. 3426–3436, May 2020.\n[23] M. Coca, I. C. Neagoe, and M. Datcu, “Hybrid DNN-dirichlet anomaly\ndetection and ranking: Case of burned areas discovery,” IEEE Trans.\nGeosci. Remote Sens., vol. 60, pp. 1–16, Sep. 16, 2022, Art. no. 4414116.\n[24] W. Dong, T. Zhang, J. Qu, Y . Li, and H. Xia, “A spatial–spectral dual-\noptimization model-driven deep network for hyperspectral and multispec-\ntral image fusion,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–16,\n2022, Art. no. 5542016.\n[25] J. Qu, S. Hou, W. Dong, S. Xiao, Q. Du, and Y . Li, “A dual-branch detail\nextraction network for hyperspectral pansharpening,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–13, Nov. 23, 2021, Art. no. 5518413.\n[26] W. Li, G. Wu, and Q. Du, “Transferred deep learning for hyperspectral\ntarget detection,” in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2017,\npp. 5177–5180.\n[27] J. M. M. Goodfellow and I. Pouget-Abadie, “Generative adversarial nets,”\nin Proc. Conf. Workshop Neural Inf. Process. Syst., vol. 60, 2014, pp. 2672–\n2680.\n[28] A. Makhzani, J. Shlens, and N. Jaitly, “Adversarial autoencoders,” 2016,\narXiv:1511.05644 [cs.LG].\n[29] L. Zhang and B. Cheng, “A stacked autoencoders-based adaptive subspace\nmodel for hyperspectral anomaly detection - sciencedirect,” Infrared Phys.\nTechnol., vol. 60, pp. 52–60, 2019.\n[30] X. Lu, W. Zhang, and J. Huang, “Exploiting embedding manifold of\nautoencoders for hyperspectral anomaly detection,” IEEE Trans. Geosci.\nRemote Sens., vol. 58, no. 3, pp. 1527–1537, Mar. 2020.\n[31] W. Xie, J. Lei, and B. Liu, “Spectral constraint adversarial autoencoders\napproach to feature representation in hyperspectral anomaly detection,”\nNeural Netw., vol. 60, pp. 222–234, 2019.\n[32] A. Vaswani, N. Shazeer, and N. Parmar, “Attention is all you need,” 2017,\narXiv:1706.03762 [cs.CL].\n[33] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2021, arXiv:2010.11929.\n[34] W. Hu, Y . Huang, L. Wei, H. Zhang, and F. Li, “Deep convolu-\ntional neural networks for hyperspectral image classiﬁcation,” J. Sensors,\nvol. 2015, 2021, Art. no. 258619.\n[35] F. Wang et al., “Residual attention network for image classiﬁcation,” in\nProc. Comput. Vis. Pattern Recognit., 2017, pp. 6450–6458.\n[36] L. Drew, S. Dan, and E. Sven, “Global-and-local attention networks for\nvisual recognition,” 2018, arXiv:abs/1805.08819.\n[37] X. Kang, X. Zhang, S. Li, K. Li, J. Li, and J. A. Benediktsson, “Hy-\nperspectral anomaly detection with attribute and edge-preserving ﬁlters,”\nIEEE Trans. Geosci. Remote Sens., vol. 55, no. 10, pp. 5600–5611,\nOct. 2017.\n[38] J. Kingma and D. P. Ba, “Adam: A method for stochastic optimization,”\n2017.\n[39] Y . Zhang, B. Du, L. Zhang, and S. Wang, “A low-rank and sparse ma-\ntrix decomposition-based Mahalanobis distance method for hyperspectral\nanomaly detection,” IEEE Trans. Geosci. Remote Sens., vol. 54, no. 3,\npp. 1376–1389, Mar. 2016.\n[40] S. Wang, X. Wang, L. Zhang, and Y . Zhong, “Auto-AD: Autonomous\nhyperspectral anomaly detection network based on fully convolutional\nautoencoder,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–14,\nMar. 22, 2021, Art. no. 5503314.\nSong Xiao (Member, IEEE) received the M.S. degree\nin communication and information system and the\nPh.D. degree in signal and information processing\nfrom Xidian University, Xi’an, China, in 2001 and\n2004, respectively.\nFrom 2006 to 2007, she was with the Viterbi School\nof Engineering, University of Southern California, as\na Postdoctoral Researcher. She is currently a Pro-\nfessor and Ph.D. Director with Beijing Electronic\nScience and Technology Institute, Beijing, China,\nalso a Professor and Ph.D. Director with the State Key\nLaboratory of Integrated Service Network, Xidian University. She has authored\nmore than 80 international journal and conference papers. Her research interests\ninclude image compression and coding, joint source channel coding, multimedia\ntransmission systems over wired/wireless network, and compressed sensing.\nProf. Xiao is the Secretary-General of Image Application in Military and\nCivilian Integration (IAMCI) Professional Committee of the China Society\nof Image and Graphics. She is a Council Member of the Shaanxi Society of\nImage and Graphics and is a Member of the IEEE Multimedia Communication\nTechnology Committee and the IEEE Signal Processing Society.\nTian Zhang received the B.E. degree in communica-\ntion engineering from Northwest University, Xi’an,\nChina, in 2020. She is currently working toward the\nM.S. degree in communication engineering with the\nState Key Laboratory of Integrated Service Networks,\nXidian University, Xi’an.\nHer research interests include joint classiﬁcation\nof hyperspectral image and LiDAR image, and deep\nlearning.\nZhangchun Xu received the bachelor’s degree in\ncommunication engineering from Hohai University,\nNanjing, China, in 2019. He is currently working to-\nward the master’s degree in information and commu-\nnications engineering with the State Key Laboratory\nof Integrated Service Networks, Xidian University,\nXi’an, China.\nHis research interests include hyperspectral image\nfusion, anomaly detection, and deep learning.\nJiahui Qu (Member, IEEE) received B.S. degree in\ncommunication engineering from Yantai University,\nYantai, China, in 2014, and the Ph.D. degree in\ncommunication and information systems from Xidian\nUniversity, Xi’an, China, in June 2020.\nShe was an exchange Ph.D. Student of Mississippi\nState University from 2018 to 2019, supervised by\nDr. Q. Du. She is currently a Lecturer with the State\nKey Laboratory of Integrated Services Networks, Xi-\ndian University. She has authored and co-authored\nmore than 20 papers in known academic journals and\nconferences, including the IEEE T RANSACTIONS ON GEOSCIENCE AND REMOTE\nSENSING, the IEEE T RANSACTIONS ON NEURAL NETWORKS AND LEARNING\nSYSTEMS, the IEEE G EOSCIENCE AND REMOTE SENSING LETTERS,a n dt h e\nRemote Sensing. Her research interests include hyperspectral image detection,\nimage fusion, neural networks, and deep learning.\n1426 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nShaoxiong Hou (Graduate Student Member, IEEE)\nreceived the bachelor’s degree in communication en-\ngineering from the Xi’an University of Technology,\nXi’an, China, in 2019. He is currently working toward\nthe Ph.D. degree in information and communications\nengineering in information and communications en-\ngineering with the State Key Laboratory of Integrated\nService Networks of Xidian University, Xi’an.\nHis research interests include hyperspectral image\nfusion, hyperspectral anomaly detection, hyperspec-\ntral change detection, and deep learning.\nWenqian Dong (Member, IEEE) received B.S. de-\ngree in communication engineering from Yantai Uni-\nversity, Yantai, China, in 2014, and the Ph.D. degree\nin communication and information systems from Xi-\ndian University, Xi’an, China, in June 2020.\nShe has been a Visiting Scholar with Simon Fraser\nUniversity, Burnaby, BC, Canada, from 2018 to 2019.\nShe is currently a Lecturer with the State Key Labora-\ntory of Integrated Services Networks, Xidian Univer-\nsity. She has authored and co-authored more than 20\npapers in refereed journals and conferences, including\nthe IEEE T RANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, the IEEE\nTRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS,a n dt h e\nRemote Sensing. Her research interests include compressed sensing, machine\nlearning, and hyperspectral remote sensing image processing.",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.8785051107406616
    },
    {
      "name": "Anomaly detection",
      "score": 0.8092892169952393
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6727620363235474
    },
    {
      "name": "Computer science",
      "score": 0.6316956877708435
    },
    {
      "name": "Pixel",
      "score": 0.5691636204719543
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5585809350013733
    },
    {
      "name": "Computer vision",
      "score": 0.5365794897079468
    },
    {
      "name": "Transformer",
      "score": 0.49513640999794006
    },
    {
      "name": "Anomaly (physics)",
      "score": 0.4495748281478882
    },
    {
      "name": "Physics",
      "score": 0.10062143206596375
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Condensed matter physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}