{
  "title": "VLT: Vision-Language Transformer and Query Generation for Referring Segmentation",
  "url": "https://openalex.org/W4307504011",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3035750924",
      "name": "Ding Henghui",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2098561542",
      "name": "Liu Chang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A3091079060",
      "name": "Wang Suchen",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A1623245367",
      "name": "Jiang Xu-dong",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2904910963",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W3034325957",
    "https://openalex.org/W2984121207",
    "https://openalex.org/W6798837711",
    "https://openalex.org/W4312940158",
    "https://openalex.org/W3173859428",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W3168640669",
    "https://openalex.org/W3035097537",
    "https://openalex.org/W6803039797",
    "https://openalex.org/W3110435696",
    "https://openalex.org/W2770129969",
    "https://openalex.org/W2605127024",
    "https://openalex.org/W3034772468",
    "https://openalex.org/W6775188310",
    "https://openalex.org/W6790978476",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W3202576436",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2986803748",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6761380950",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W2894964039",
    "https://openalex.org/W3034692043",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2876852810",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W4214490042",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W2605229288",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W6749526849",
    "https://openalex.org/W2962764817",
    "https://openalex.org/W2558535589",
    "https://openalex.org/W6794092134",
    "https://openalex.org/W3216551675",
    "https://openalex.org/W3104844437",
    "https://openalex.org/W6789753369",
    "https://openalex.org/W4226024706",
    "https://openalex.org/W4312543911",
    "https://openalex.org/W6796280812",
    "https://openalex.org/W3108925098",
    "https://openalex.org/W6697921835",
    "https://openalex.org/W6757135208",
    "https://openalex.org/W2946086442",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3093025045",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3108748824",
    "https://openalex.org/W3004019157",
    "https://openalex.org/W3172522282",
    "https://openalex.org/W2964284374",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W3169998662",
    "https://openalex.org/W6798505901",
    "https://openalex.org/W2798556392",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W3187664142",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W6768730002",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W6804418671",
    "https://openalex.org/W3201770677",
    "https://openalex.org/W4200631575",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3202427362",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3215863079",
    "https://openalex.org/W4289126595",
    "https://openalex.org/W3178075329",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2302548814",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3171098737",
    "https://openalex.org/W3020827971",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2916743882",
    "https://openalex.org/W2962942822",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3153666347",
    "https://openalex.org/W3214642103",
    "https://openalex.org/W3211490618"
  ],
  "abstract": "We propose a Vision-Language Transformer (VLT) framework for referring segmentation to facilitate deep interactions among multi-modal information and enhance the holistic understanding to vision-language features. There are different ways to understand the dynamic emphasis of a language expression, especially when interacting with the image. However, the learned queries in existing transformer works are fixed after training, which cannot cope with the randomness and huge diversity of the language expressions. To address this issue, we propose a Query Generation Module, which dynamically produces multiple sets of input-specific queries to represent the diverse comprehensions of language expression. To find the best among these diverse comprehensions, so as to generate a better mask, we propose a Query Balance Module to selectively fuse the corresponding responses of the set of queries. Furthermore, to enhance the model's ability in dealing with diverse language expressions, we consider inter-sample learning to explicitly endow the model with knowledge of understanding different language expressions to the same object. We introduce masked contrastive learning to narrow down the features of different expressions for the same target object while distinguishing the features of different objects. The proposed approach is lightweight and achieves new state-of-the-art referring segmentation results consistently on five datasets.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1\nVLT: Vision-Language Transformer and Query\nGeneration for Referring Segmentation\nHenghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang, Fellow, IEEE\nAbstract—We propose a Vision-Language Transformer (VLT) framework for referring segmentation to facilitate deep interactions\namong multi-modal information and enhance the holistic understanding to vision-language features. There are different ways to\nunderstand the dynamic emphasis of a language expression, especially when interacting with the image. However, the learned queries\nin existing transformer works are ﬁxed after training, which cannot cope with the randomness and huge diversity of the language\nexpressions. To address this issue, we propose a Query Generation Module, which dynamically produces multiple sets of input-speciﬁc\nqueries to represent the diverse comprehensions of language expression. To ﬁnd the best among these diverse comprehensions, so as\nto generate a better mask, we propose a Query Balance Module to selectively fuse the corresponding responses of the set of queries.\nFurthermore, to enhance the model’s ability in dealing with diverse language expressions, we consider inter-sample learning to\nexplicitly endow the model with knowledge of understanding different language expressions to the same object. We introduce masked\ncontrastive learning to narrow down the features of different expressions for the same target object while distinguishing the features of\ndifferent objects. The proposed approach is lightweight and achieves new state-of-the-art referring segmentation results consistently\non ﬁve datasets.\nIndex Terms—Vision-Language Transformer, Referring Segmentation, Query Generation, Query Balance, Inter-Sample Learning,\nSpatial-Dynamic Fusion, Masked Contrastive Learning.\n!\n1 I NTRODUCTION\nR\nEFERRING segmentation aims at generating a segmentation\nmask for the target object that are referred by a given query\nexpression in natural language [1], [2]. Referring segmentation is\none of the most fundamental while challenging multi-modal tasks,\ninvolving both natural language processing and computer vision.\nIt is intensely demanded in many practical applications, e.g.,\nvideo/image editing, by providing a user-friendly interactive way.\nRecently, many deep-learning-based methods have arisen in this\nﬁeld and achieved remarkable results. However, great challenges\nstill remain: while the query expression implies the target object\nby describing its attributes and its relationships with other objects,\nobjects in referring segmentation images relate to each other in\na complex manner. Therefore, a holistic understanding of the\nimage and language expression is desired. Another challenge is\nthat the diverse objects/images and the unconstrained language\nexpressions bring a high level of randomness, which requires the\nmodal high generalization ability in understanding different kinds\nof images and language expressions.\nFirstly, to address the challenge of complicated correlations\nin the input image and query expression, we propose to en-\nhance the holistic understanding of multi-modal information by\ndesigning a framework with global operations, in which direct\ninteractions are built among all elements, e.g., word-word, pixel-\npixel, and pixel-word. The Fully Convolutional Network (FCN)-\nlike framework [3], [4] is commonly used in existing referring\nsegmentation methods [1], [5]. They usually perform convolu-\n• Henghui Ding, Chang Liu, Suchen Wang and Xudong Jiang are with the\nSchool of Electrical and Electronic Engineering, Nanyang Technological\nUniversity (NTU), Singapore 639798. (e-mail: henghuiding@gmail.com;\nliuc0058@e.ntu.edu.sg; wang.sc@ntu.edu.sg; exdjiang@ntu.edu.sg)\n• Henghui Ding and Chang Liu are co-ﬁrst author.\n• Corresponding author: Henghui Ding.\ntion operations on the fused, e.g., concatenated or multiplied,\nvision-language features to predict the segmentation mask for the\ntarget object. However, the long-range dependency modeling is\nintractable by regular convolution operation as its large receptive\nﬁeld is achieved by stacking many small-kernel convolutions. This\noblique process makes the information interaction between long-\ndistance pixels/words inefﬁcient [6], thus is undesirable for the\nreferring segmentation model to understand the global context\nexpressed by the input image and language [7]. In recent years,\nattention mechanism has gained considerable popularity in the\ncomputer vision community thanks to its advantage in building\ndirect interaction among all elements, which greatly helps the\nmodel in capturing global semantic information. There have been\nsome previous referring segmentation works that use attention\nto alleviate the long-range dependency issues, e.g., [7], [8],\n[9]. However, most of them rely on FCN-like pipelines [10],\n[11] and only use the attention mechanism as auxiliary modules,\nwhich limits their ability to model the global context. In this\nwork, we reformulate the referring segmentation problem as a\ndirect attention problem and re-construct the current FCN-like\nframework using Transformer [12]. We generate a set of query\nvectors from language features using vision-guided attention, and\nuse these vectors to “query” the given image and predict the\nsegmentation mask from the query responses, as shown in Fig. 1.\nThis attention-based framework enables us to implement global\noperation among multi-modal features in each computation stage\nand enhances the network’s ability to capture the global context of\nboth vision and language information.\nSecondly, in order to handle the randomness caused by the\nvarious objects/images and the unrestricted language expressions,\nwe propose to understand the input language expression in differ-\nent ways incorporating vision features. In many existing referring\nsegmentation methods, such as [13], [14], the language self-\narXiv:2210.15871v1  [cs.CV]  28 Oct 2022\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\nDecode\n\"Small elephant on the left\"\nVision-Guided Attention\n0.6\n0.7\n0.3\nQ1 \"SMALL elephant on the LEFT\"\nQ2 \"SMALL ELEPHANT on the left\"\nQ3 \"small ELEPHANT on the LEFT\"\nQuery\nQuery Vectors\nResp. 1\nResp. 2\nResp. 3\nMask\nInput:\n... ... ...\nResponse & Balance\nFig. 1: The proposed method dynamically produces multiple sets\nof input-speciﬁc query vectors to represent the diverse compre-\nhensions of language expression. We use each vector to “query”\nthe input image, generating a response to each query. Then the\nnetwork selectively aggregates the responses to these queries, so\nthat queries that provide better comprehensions are highlighted.\nattention is often used to extract the most informative part and\nemphasized word(s) in the language expression. However, for\nthese methods, their language understanding is derived solely\nfrom the language expression itself without interacting with the\nvision information. As a sequence, they cannot distinguish which\nemphasis is more suitable and effective that can ﬁt a particular\nimage better. Hence, their detected emphases might be inaccu-\nrate or inefﬁcient. On the other hand, in most existing vision-\ntransformer works [15], the queries of the transformer decoder\nare a set of ﬁxed and learned vectors, each of which predicts an\nobject. However, experiments show that each query vector has\nits own operating modes, and is speciﬁcally targeted at certain\nkinds of objects [15], e.g., speciﬁcally targeted at objects of\na certain type or located in a certain area. The ﬁxed queries\nin these works implicitly assume that the objects in the input\nimage are distributed under some certain statistical rules, which\ndoes not well consider the randomness and huge diversity of\nthe referring segmentation, especially the randomness brought by\nunconstrained language expressions. Besides, the learnable queries\nare designed for detecting all the objects in the whole image\ninstead of focusing on the target object indicated by the language\nexpression, thus cannot efﬁciently extract informative representa-\ntion that contains the clues to the target object. To address these\nissues, we propose to generate input-speciﬁc queries that could\nfocus on the clues related to the referred target object. We herein\npropose a Query Generation Module (QGM), which dynamically\nproduces multiple query vectors based on the input language\nexpression and the vision features. Each query vector represents\na speciﬁc comprehension of the language expression and queries\nthe vision features with different emphases. As shown in Fig. 1,\nthree queries focus on different information, respectively. These\ngenerated query vectors produce a set of corresponding masks in\nthe transformer decoder though we only need one mask selected\nfrom them. Besides, we also hope to choose a more reasonable and\nbetter comprehension way from these query vectors. Therefore, we\nfurther propose a Query Balance Module (QBM), which assigns\neach query vector a conﬁdence measure to control its impact on\nmask decoding, and then adaptively selects the output features\nof these queries to better generate the ﬁnal mask. The proposed\nQGM dynamically produces input-speciﬁc queries that focus on\ndifferent informative clues related to the target object, while the\nproposed QBM selectively fuses the corresponding responses by\nthese queries. These two modules work together to prominently\nimprove the diverse ways to understand the image and query\nlanguage and enhance the network’s robustness towards highly\nrandom inputs.\nThirdly, we introduce masked contrastive representation learn-\ning to further enhance the model’s generalization ability and\nrobustness to unconstrained language expressions. With the pro-\nposed Query Generation Module and Query Balance Module, we\nprovide different understandings of a given expression, which can\nbe viewed as a kind of intral-sample learning. Here we further\nconsider inter-sample learning to explicitly endow the model with\nknowledge of different language expressions to one object. For\nthe same target object, there are multiple ways to describe it.\nHowever, the ﬁnal representations that predict the target mask\nshould be the same. In other words, the output features of Query\nBalance Module by different expressions for the same object\nshould be the same. To this end, we utilize contrastive learning\nto narrow down the features of different expressions for a same\ntarget object, while distinguishing the features of different objects.\nWhat’s more, we observe that the model tends to overly rely\non speciﬁc words that provide the most discriminative clues\nor frequently occur in training samples, while ignoring other\ncomplementary information. The excessive reliance on speciﬁc\nwords will damage the model’s generalization ability, for instance,\nthe model may not well understand testing expressions that do not\ncontain common discriminative clues in the training samples. To\naddress this issue, we introduce masked language expressions in\ncontrastive representation learning, which randomly erases some\nspeciﬁc words from the original language expression. The masked\nlanguage expression and the original expression refer to the same\ntarget object, they are considered as a positive pair in the con-\ntrastive representation learning to be close to each other and reach\nthe same representation. The masked contrastive representation\nlearning signiﬁcantly enhances the model’s ability in dealing with\ndiverse language expressions in the wild.\nThe proposed approach builds deep interactions between lan-\nguage and vision information at different levels, which greatly\nenhances the utilization and fusion of multi-modal features. Be-\nsides, the proposed network is lightweight and its parameter scale\nis roughly equivalent to just seven convolution layers. In summary,\nour main contributions are listed as follows:\n• We design a Vision-Language Transformer (VLT) frame-\nwork to facilitate deep interactions among multi-modal\ninformation and enhance the holistic understanding to\nvision-language features.\n• We propose a Query Generation Module (QGM) that\ndynamically produces multiple input-speciﬁc queries rep-\nresenting different comprehensions of the language, and\na Query Balance Module (QBM) to selectively fuse the\ncorresponding responses by these queries.\n• We introduce a masked contrastive representation learning\nto enhance the model’s generalization ability and robust-\nness to deal with the unconstrained language expressions\nby learning inter-sample relationships.\n• The proposed approach is lightweight and achieves new\nstate-of-the-art performance consistently on three referring\nimage segmentation datasets, RefCOCO, RefCOCO+, G-\nRef, and two referring video object segmentation datasets,\nYouTube-RVOS and Ref-DA VIS17.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\n2 R ELATED WORKS\nIn this section, we discuss works that are closely related to the\nproposed approach, including referring segmentation, referring\ncomprehension, and transformer.\n2.1 Referring Segmentation\nReferring segmentation is one of the most fundamental while\nchallenging multi-modal tasks, involving both language and vision\ninformation. Given a natural language expression describing the\nproperties of the target object in the given image, the goal of the\nreferring segmentation is to ground the target object referred by\nthe language and generate a corresponding segmentation mask.\nInspired by the task of referring comprehension [16], [17], [18],\n[19], [20], [21], referring segmentation is introduced by Hu et al.\nin [1]. [1] concatenates the linguistic features extracted by Long\nShort-Term Memory (LSTM) networks and the visual features\nextracted by Convolutional Neural Networks (CNN). Then, the\nfused vision-language features is inputted to a fully convolutional\nnetwork (FCN) [3] to generate the target segmentation mask.\nIn [22], in order to better utilize the information of each word in\nthe language expression, Liu et al. propose a multimodal LSTM\n(mLSTM), which models each word in every recurrent stage to\nfuse the word feature with vision features. Li et al. [23] utilize\nfeatures from different levels in the backbone progressively, which\nfurther improves the performance. To better utilize the language\ninformation, Edgar et al. [5] propose a method that uses the\nfeature of each word in the language expression when extracting\nlanguage features, not just the ﬁnal state of the RNN. Chen\net al. [24] employ a caption generation network to produce a\ncaption sentence that describes the target object, and enforce the\ncaption to be consistent with the input expression. In [13], Luo\net al. propose a multi-task framework to jointly learn referring\nexpression comprehension and segmentation. They build a net-\nwork that contains a referring expression comprehension branch\nand a referring expression segmentation branch, each of which\ncan reinforce the other during training. Jing et al. [25] decouple\nthe referring segmentation to localization and segmentation and\npropose a Locate-Then-Segment (LTS) scheme to locate the target\nobject ﬁrst and then generate a ﬁne-grained segmentation mask.\nFeng et al. [26] propose to utilize the language feature earlier in\nthe encoder stage. Hui et al. [27] introduce a linguistic structure-\nguided context modeling to analyze the linguistic structure for\nbetter language understanding. Yang et al. [28] propose a Bottom-\nUp Shift (BUS) to progressively locates the target object with\nhierarchical reasoning the given expression.\nWith the introduction of attention-based methods [6], [12],\nresearchers have found that the attention mechanism is suitable\nfor the formulation of referring segmentation. For example, Ye et\nal. propose the Cross-Modal Self-Attention (CMSA) model [7]\nto dynamically ﬁnd the most important words in the language\nsentence and the informative image region. Hu et al. [8] propose\na bi-directional attention module to further utilize the features of\nwords. Most of these works are built on FCN-like networks [29],\n[30] and only use the attention as auxiliary modules. Our con-\ncurrent work MDETR [31] employs DETR [15] to build an end-\nto-end modulated detector and reason jointly over language and\nimage. After the proposed VLT [32], transformer-based refer-\nring segmentation architectures receive more attention [33], [34],\n[35], [36], [37]. MaIL [33] follows the transformer architecture\nViLT [38] and utilizes instance mask predicted by Mask R-\nCNN [39] as additional input. Yang et al. [34] propose Language-\nAware Vision Transformer (LA VT) to conduct multi-modal fu-\nsion at intermediate levels of the network. CRIS [35] employs\nCLIP [40] pretrained on 400M image text pairs and transfers CLIP\nfrom text-to-image matching to text-to-pixel matching.\nIn this work we employ a fully attention-based architecture\nand propose a Vision-Language Transformer (VLT) [32] to model\nthe long-range dependencies in the image, as shown in Fig. 2.\nWe further propose to generate input-conditional queries for the\ndecoder of transformer to better understand the unconstrained\nlanguage expressions from different aspects.\n2.2 Referring Comprehension\nReferring comprehension is a highly relevant task to referring\nsegmentation. Referring comprehension also takes an image and\na language expression as inputs and identiﬁes the target object\nreferred by the language expression. However, while referring\nsegmentation aims to output a segmentation mask for the target\nobject, the referring comprehension outputs a grounding box.\nUnlike the FCN-like pipeline of referring segmentation, most\nearlier referring comprehension works are based on the multi-\nstage pipeline [17], [18], [19], [41], [42], [43]. In these works,\noften an out-of-the-box instance segmentation network, e.g., Mask\nR-CNN [39], is ﬁrst applied to the image and generates a set\nof instance proposals, regardless of the language input. Next, the\ncandidate proposals are compared with the language expression,\nto ﬁnd the best match. For example, Yu et al. [14] propose a two-\nstage method that ﬁrst extracts all instances in the image using\nMask R-CNN [39], then employs a modular network to match and\nselect the target object from all the instances detected by Mask R-\nCNN. In recent years, one-stage methods [21], [44], [45] have also\nbeen increasingly adopted in the referring comprehension area,\ne.g., Sadhu et al. propose a “Zero-Shot Grounding” network for\nreferring comprehension [46], and Yang et al. design a recursive\nsub-query construction framework to gradually reason between the\nimage and query language [20].\n2.3 Transformer\nTransformer is ﬁrst proposed by Vaswaniet al. in [12] for machine\ntranslation, it is a sequence-to-sequence deep network architecture\nwith the attention mechanism. Recently, transformer attracts lots\nof attention in Natural Language Processing (NLP) and achieves\ngreat success on many NLP tasks, e.g., machine translation [12],\nquestion answering [47], and language modeling [48]. Besides\nthe NLP, transformer has also been employed to many computer\nvision tasks [49], [50] and has achieved promising results on\nvarious vision tasks such as object detection [15], image recog-\nnition [51], semantic segmentation [52], [53], [54], and human-\nobject interaction [55], [56].\nIn the vision-language ﬁeld, transformer architectures have\nachieved great success in many tasks, e.g., vision-and-language\npre-training [38], [57], [58], image generation [59], visual ques-\ntion answering [60], open-vocabulary detection [61], image re-\ntrieval [62], vision-and-language navigation [63], etc. Lu et\nal. [57] design a co-attention mechanism to incorporate language-\nattended vision features into language features. Kim et al. [38]\npropose to deal with the two modalities in a single uniﬁed\ntransformer architecture. Huang et al. [60] propose a Pixel-BERT\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\nQuery #Nq\nQuery #3\nQuery #2\nQuery #1\n\"Small elephant  \non the left\"\nQuery Balance\nMask\nDecoder\nImage Input\nTarget MaskLanguage  \nFeatures\nVision \nFeatures\n...\nQuery Vectors \nsmall\nelephant\non\nthe\nleft\nVision-Guided\nAttention\nQuery  \nGeneration \n...\nQuery  \nConfidence\nResponse #3\nResponse #2\nResponse #1\nResponse #Nq\nWeighting Query\nLanguage Input\nTransformer\nEncoder\nTransformer\nDecoder\n...\nQuery  \nBalance \nSpatial \nDynamic \nFusion\nPos. \nEmb.\n... ...\n ...MCL\nFig. 2: The overview architecture of the proposed Vision-Language Transformer (VLT). Firstly, the given image and language expression\nare projected into visual and linguistic feature spaces, respectively. A Spatial Dynamic Fusion module is then employed to fuse vision\nand language features, generating multi-modal feature inputted to the transformer encoder. The proposed Query Generation Module\ngenerates a set of input-speciﬁc queries according to the vision and language features. These input-speciﬁc queries are sent to the\ndecoder, producing corresponding query responses. These resulting responses are selected by the Query Balance Module and then\ndecoded to output the target mask by a Mask Decoder. “Pos. Emb.”: Positional Embeddings. “MCL”: Masked Contrastive Learning.\nto align visual features with textual features by jointly learn-\ning visual and textual embedding in a uniﬁed transformer way.\nBased on the Pixel-BERT, Zareian et al. [61] design a vision-to-\nlanguage projection to process visual features before transformer.\nRadford et al. [40] propose a Contrastive Language-Image Pre-\ntraining (CLIP) scheme to jointly train image language encoders.\nWang et al. [35] apply CLIP to referring image segmentation by\ntext-to-pixel alignment. Lei et al. [64] introduce a C LIPBERT to\ntext-to-video retrieval and video question answering. Huet al. [65]\npropose a Uniﬁed Transformer (UniT) model to learn multiple\nvision-language tasks with a uniﬁed multi-modal architecture.\nDifferent from previous works that use a small ﬁxed number of\nlearned position embeddings as object queries, we propose to\ndynamically generate input-speciﬁc queries representing different\ncomprehensions of language and selectively fuse the correspond-\ning responses by these input-speciﬁc queries. With the input-\nspeciﬁc queries, the proposed VLT better captures the informative\nclues hidden in the language expressions and address the high\ndiversity in referring segmentation.\n3 M ETHODOLOGY\nThe overall architecture of the proposed Vision-Language Trans-\nformer (VLT) is shown in Fig. 2. The network takes a language\nexpression and an image as inputs. First, the input image and\nlanguage expression are projected into the linguistic and visual\nfeature spaces, respectively. Then, vision and language features\nare inputted to the proposed Query Generation Module (QGM)\nto generate a set of input-speciﬁc query vectors, which represent\ndifferent understandings of the language expression under the\nguidance of visual clues. Meantime, vision and language fea-\ntures are fused to multi-modal feature by the proposed Spatial\nDynamic Fusion (SDF), and the multi-modal feature is sent to the\nC\nConcat\nConv\nFvr : Raw Vision Feature\nSentence Feature \n 1×C\nTile\nFvr  \nH×W×C \nFused Feature \nH×W×CH×W×2C\nH×W×C \nFig. 3: Tile-and-concatenate fusion. The language feature is iden-\ntically copied to every position across the H×W map.\ntransformer encoder to produce a group of memory features. The\nquery vectors in Q generated by our proposed QGM are employed\nto “query” K and V derived from memory features in trans-\nformer decoder, i.e., Attention (Q,K,V ) = softmax(QKT\n√dk\n)V,\nwhere dk is the dimensionality of K. The resulting responses\nfrom transformer decoder are then selected by a Query Balance\nModule (QBM) with different conﬁdence weights. Finally, the\nmask decoder takes the weighted responses from QBM and the\noutput feature from transformer encoder as inputs and outputs a\nmask for the target object. Masked Contrastive Learning (MCL)\nis used to supervise the features in Mask Decoder to narrow down\nthe features of different expressions for the same target object\nwhile distinguishing the features of different objects. Positional\nembeddings are used to supplement the pixel position information\nin the permutation-invariant transformer architecture.\n3.1 Spatial-Dynamic Multi-Modal Fusion\nAfter backbone features for language and image are extracted, the\nﬁrst step is to preliminarily fuse them together and generate a\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\n(sentence)\nSpatial Dynamic \nLanguage Feature \nH×W×C\nFvr  \nH×W×C \nleft\non\nelephant\nsmall\nSpatial-Dynamic Attention \nH×W×Nt\n...\n...\n...\n...\n× ×\nH×W×2C\nC\nC : Concatenation\nConv\nFused Feature \nH×W×C\n...\nFt\n× : MultiplicationFvr : Raw Vision FeatureLanguage Features\nsmallelephant\nNt×C\n(sentence)on left\nConv\nConvConv\nFig. 4: An illustration of the proposed Spatial-Dynamic Fusion (SDF). Different from the conventional “tile-and-concatenate” fusion,\nthe proposed SDF ﬁnds a word attention set and derives a tailored language feature vector for each pixel in the image feature.\nmulti-modal feature. For referring segmentation, effective multi-\nmodal information fusion is critical and challenging because the\nunconstrained expression of natural language and the diversity\nof objects in scene images bring huge uncertainty to the under-\nstanding and fusion of multi-modal features. However, existing\napproaches conduct multi-modal feature fusion simply by either\nconcatenation [1], [7], [8], [22] or point-wise multiplication [13],\n[25], [32] of vision feature and language feature. The language\nfeature, which is a 1D vector, is usually tiled to every position of\nthe vision feature [1], [7], as shown in Fig. 3. Under the “tile-and-\nconcatenate” operation, the language feature is identically copied\nto every position across the H×W map.\nAlthough such kinds of fusion techniques are simple and have\nachieved reasonable performance, there are a few drawbacks.\nFirstly, the features of individual single words are not fully\nutilized in this step. Secondly, the tiled language feature will be\nidentical for all pixels across the image feature, which weakens\nthe location information carried by the correlation between the\nlanguage information and the visual information. Due to the\ndiversity of objects in the input image, an image usually contains\ndiverse information that can be very complex, where different\nregions may contain different semantic information. Meanwhile,\nthe language expression can be interpreted with different emphases\nfrom different perspectives. We here emphasize the differences\namong pixels/objects, i.e., the vision information across the image\nvaries from place to place. Therefore, the informative words in a\ngiven sentence are different from pixel to pixel. The way of tiling\nignores such differences and simply assigns the same language\nfeature vector to every pixel, resulting in some confusion. It\nis better to make tailored feature fusion speciﬁcally for each\nindividual pixel. In this work, we propose a Spatial-Dynamic\nFusion (SDF) module, which produces different language feature\nvectors for different positions of the image feature according to\nthe interaction between language information and corresponding\npixel information. Each position selects its interested words and\npays more attention to these words during multi-modal fusion.\nAn illustration of the proposed Spatial-Dynamic Fusion (SDF)\nmodule is shown in Fig. 4. The proposed SDF module takes\nlanguage features Ft, including features of each word and the\nwhole sentence, and image feature Fvr as inputs. We ﬁrst use lan-\nguage feature and vision feature to generate the Spatial-Dynamic\nAttention matrix by:\nAsd = softmax( 1√\nC\nConv(Fvr)Conv(Ft)T), (1)\nInput: “The large balloon on the left”\n(a)\n (b)\nFig. 5: An example of one sentence with different emphasis. For\ndifferent images, the informative degree of words “large” and\n“left” are different.\nwhere C is feature channel number and 1√\nC is the scaling factor.\nAsd is with the shape of H ×W ×Nt, where H and W are\nheight and width respectively, and Nt is the number of language\nfeature vectors in Ft. Softmax normalization is applied along\nthe Nt axis of the attention matrix Asd. Each position of the\nspatial-dynamic attention Asd is a weighting vector that indicates\ndifferent importance of the Nt language features at this position.\nTherefore, a spatial dynamic language feature Fsdl is generated\nby:\nFsdl = AsdConv(Ft), (2)\nwhere Fsdl is in the shape ofH×W×C, each vector ofFsdlacross\nH×W is the language feature vector weighted by its correlation\nto the image context at a pixel position. The fused multi-modal\nfeature Ffused is generated by:\nFfused = Conv(Fsdl©Fvr), (3)\nwhere © denotes concatenation. Following previous transformer\nworks [15], [66], we employ ﬁxed sine spatial positional em-\nbeddings to supplement the pixel position information in the\npermutation-invariant transformer. The fused multi-modal feature\nand the positional embeddings are inputted to the transformer\nencoder (see Fig. 2).\n3.2 Query Generation Module\nIn most existing Vision Transformer works, e.g., [15], [52], [53],\n[67], queries for the transformer decoder are usually a set of\nﬁxed learned vectors, each of which is used to predict one object\nand has its own operating mode, e.g., specifying objects of a\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\nNq×C\n...\nNq\nNt\n×\nLanguage Features \nFt\n+\nNt×CNt×C\nNq×C\nQuery Vectors \nFq\nNq×C\nAttention Weights Aqd\nLinear(Wv)Linear(Wa)Linear(Wt)\n...\nNq×HW\nsmall elephant\non the left\nNq×C\n×\n...\n...\nSequential Vision \nFeatures Fvq\nFig. 6: Query Generation Module (QGM). The QGM takes se-\nquential vision feature Fvq and language features Ft as inputs and\ngenerates a group of input-speciﬁc query vectors Fq, which are\nthen sent to the transformer decoder of our VLT.\ncertain kind or located in a certain region. These works with\nﬁxed queries have an implicit assumption that objects in the\ninput image are distributed under some statistic rules. However,\nsuch an assumption does not consider the huge diversity of the\nreferring segmentation. Besides, the learnable queries are designed\nfor detecting all objects in the whole image instead of focusing on\nthe target object indicated by the language expression, thus cannot\neffectively extract informative representation that contains clues\nof the target object.\nFor referring segmentation, the target object described by\nthe given expression can be any part of the image. Because\nboth the input image and language expression are unconstrained,\nthe stochasticity of the target object’s properties is signiﬁcantly\nhigh. Therefore, ﬁxed query vectors, like in most existing ViT\nworks, cannot well represent the properties of the target object.\nInstead, the properties of the target object are hidden in the\ninput language expression, e.g., keywords like “blue/yellow”,\n“small/large”, “right/left”, etc. To capture the informative clues\nand address the high stochasticity in referring segmentation, we\npropose a Query Generation Module (QGM) to adaptively gen-\nerate the input-speciﬁed query vectors online according to the\ngiven image and language expression. Also, it is well known that\nfor a language expression, the importance of different words is\ndifferent. Some existing works address this issue by measuring\nthe importance of each word. For example, [13] gives each word\na weight and [14], [68] deﬁnes a set of groups, e.g., location,\nattribute, entity, and ﬁnds the degree of each word belonging\nto different groups. Most works derive the weights by the lan-\nguage self-attention, which does not utilize the information in\nthe image and only outputs one set of weights. But in practice,\na same sentence may have different understanding perspectives\nand emphasis, and the most suitable and effective emphasis can\nonly be known with the help of the image. We give an intuitive\nexample in Fig. 5. For the same input sentence “The large balloon\non the left” , the word “left” is more informative for the ﬁrst\nimage while the word “large” is more useful for the second\nConv Flatten\nH×W×Nq\nSequential vision \nfeatures Nq×(HW)\n1234\nNq\nRaw vision features \nH×W×C\nFvr\n...\n...\n1 2 3 4 Nq\n...\n...\nFvq...\nFig. 7: The preparation process of the sequential vision features\nfor our Query Generation Module.\nimage. In this case, language self-attention cannot differentiate\nthe importance between “large” and “left”, making the attention\nprocess less effective. In order to let the network learn different\naspects of information and enhance the robustness of the queries,\nwe generate multiple queries with the help of visual information,\nthough there is only one target instance. Each query represents\na speciﬁc comprehension of the given language expression with\ndifferent emphasized word(s).\nThe architecture of the Query Generation Module is shown\nin Fig. 6. It takes language feature Ft ∈RNt×C and raw vision\nfeature Fvr ∈RH×W×C as inputs. In Ft, the i-th vector is the\nfeature vector of the word wi, which is the i-th word in the input\nlanguage expression. Nt denotes the sentence length and is ﬁxed\nover all inputs by zero-padding. This module aims to output Nq\nquery vectors, each of which is a language feature with different\nattention weights guided by the vision information. Speciﬁcally,\nthe vision features are ﬁrstly prepared as shown in Fig. 7. We\nreduce the feature channel dimension size of the raw vision feature\nFvr to query number Nq by three convolution layers, resulting in\nNq feature maps. Each of them will participate in the generation of\none query vector. The feature maps are then ﬂattened in the spatial\ndomain, forming a feature matrix Fvq of size Nq ×(HW), i.e.,\nFvq = Flatten(Conv(Fvr))T. (4)\nNext, we comprehend the language expression from multiple\naspects incorporating the image, forming Nq queries from lan-\nguage. We derive the attention weights for language features Ft\nby incorporating the vision features Fvq,\nAqd = softmax( 1√\nC\nσ(FvqWv)σ(FtWa)T), (5)\nwhere Aqd ∈RNq×Nt is query-dynamic attention matrix, contain-\ning Nq different attention vectors to Ft. Wv ∈R(HW)×C and\nWa ∈RC×C are learnable parameters, σ is activation function\nRectiﬁed Linear Unit (ReLU). The softmax function is applied\nacross all words for each query as normalization. In attention\nmatrix Aqd, each of the Nq vectors consists of a set of attention\nweights for different words. Different queries attend to different\nparts of the language expression. Thus, Nq query vectors focus\non different emphasis or different comprehension ways of the\nlanguage expression. Notably, after this step, for longer sentences,\nwe randomly mask one of the most important words to enhance\nthe generalization ability of the network. The details are shown in\nSec. 3.5.\nNext, the derived attention weights are applied to the language\nfeatures:\nFq = Aqdσ(FtWt) +σ(FvqWv), (6)\nwhere Wt ∈ RC×C and Wv are learnable parameters, Fq ∈\nRNq×C contains Nq query vectors {Fq1,...,F qn,...FqNq }. We\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\nLinear\nTransformer \nDecoder\nC\nLinear\n×2\nCq1 Cq2 Cq3 Cq4 ...\nQuery Vectors Fq\nTransformer Responses Fr\nQuery Confidence Cq\nC : Concatenation\nCqNq Nq×1\nBalanced Responses Fb\nNq×CNq×C\n...\n...\nQuery #1Query #2Query #3Query #4Query #Nq\n...\n...\nNq×C\n...\n...\nWeighting\nResponse\n #1\nResponse\n #2\nResponse\n #3\nResponse\n #4\nResponse #Nq\nFig. 8: Query Balance Module (QBM). For each query vector, a conﬁdence measure parameter Cq is computed to reﬂect how much\nit ﬁts the prediction and the context of the image. The transformer responses Fr is weighted by the corresponding conﬁdences Cq to\ncontrol the inﬂuence of each query vector, generating balanced responses Fb.\nadd a residual connection from vision feature Fvq to enrich the\ninformation in query vectors. Each Fqn is an attended language\nfeature vector guided by vision information and serves as one\nquery vector to the transformer decoder.\n3.3 Query Balance Module\nWe get Nq different query vectors from the proposed Query Gen-\neration Module. Each query represents a speciﬁc comprehension\nof the input language expression under the interactive guidance of\nthe input image information. As we discussed before, both the in-\nput image and language expression are of high arbitrarines. Thus,\nit is desired to adaptively select the better comprehension ways\nand let the network focus on the more reasonable and suitable\ncomprehension ways. On the other hand, as the independence of\neach query vector is kept in the transformer decoder [15] but we\nonly need one mask output, it is desired to balance the inﬂuence\nof different queries on the ﬁnal output. Therefore, we propose a\nQuery Balance Module (QBM) to dynamically assign each query\nvector a conﬁdence measure that reﬂects how much it ﬁts the\nprediction and the context of the image.\nThe architecture of the proposed QBM is shown in Fig. 8.\nSpeciﬁcally, the inputs of Query Balance Module are the query\nvectors Fq from the Query Generation Module and its corre-\nsponding responses from the transformer decoder, Fr, which is\nof the same size as Fq. In the Query Balance Module, the query\nvectors after going through a linear layer and their corresponding\nresponses are ﬁrst concatenated together. The linear layers are\nemployed to derive conﬁdence levels according to the query vec-\ntors Fq and their corresponding responses Fr. Then, a set of query\nconﬁdence levels Cq, in the shape of Nq×1, are generated by two\nconsecutive linear layers. Sigmoid, S(x) = 1\n1+e−x , is employed\nafter the the last linear layer as an activation function to control the\noutput range. Let Frn and Cqn denote the corresponding response\nand query conﬁdence to the n-th query Fqn, respectively. Each\nscalar Cqn shows how much the query Fqn ﬁts the context of\nits prediction, and controls the inﬂuence of its response Frn to\nthe mask decoding. Each response Frn is multiplied with the\ncorresponding query conﬁdence Cqn, i.e., Fbn = FrnCqn. The\nbalanced responses Fb = {Fb1,...,F bn,...,F bNq }are sent for\nmask decoding. The proposed QGM dynamically produces input-\nspeciﬁc queries that focus on different informative clues related\nto the target object, while the proposed QBM selectively fuses\nthe corresponding responses to these queries. These two modules\n...\nResponse #3\nResponse #2\nResponse #1\nResponse #Nq\nQBM Output Fb\n...\nTrasnformer Encoder  \nOutput Fve\nHW×C\nNq×C\n×\n...\nHW×Nq\nOutput Mask\nH×W×Nq\nConv\n×3\nConv\nUpsampling\nFm\nFm\nReshape\nFig. 9: The Mask Decoder takes the outputs of Query Balance\nModule (QBM) Fb and Transformer Encoder Fve to generate the\noutput mask.\nwork together to prominently boost the diversity to understand the\nimage and query language, and enhance the model’s robustness\ntowards highly stochastic inputs.\n3.4 Mask Decoder\nThe output of the Query Balance Module Fb with the size of\nNq×Cis sent to the mask decoder, as shown in Fig. 9. In the mask\ndecoder module, Fb is utilized as a set of mask generation kernel\nto process the vision-dominated feature Fve from the transformer\nencoder, to produce mask feature Fm, i.e.,\nFm = FveFT\nb , (7)\nwhere Fve is with size of HW ×C so that Fm has size of\nHW ×Nq. Then we reshape Fm to H×W ×Nq for the ﬁnal\nmask generation. We use three stacked3×3 convolution layers for\ndecoding followed by one 1 ×1 convolution layer for outputting\nthe ﬁnal predicted segmentation mask. To control the output size\nand generate a higher-resolution mask, upsampling layers are\nplaced after each of the three 3 ×3 convolution layers. To better\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\nSame Image Same Object (SISO)\nDifferent \nImage \n(DI)\n\"man on left\"\n\"leftmost person\"\n ...\n\"man in the middle\" ...\n\"purple shirt\nwoman\"\n\"zebra ...\"\n\"dog ...\"\n\"orange ...\"\n\"man in green on\nleft\"\n\"man in green on\nleft\"\n\"white hair\"\nSame \nImage \nDifferent \nObject \n(SIDO) \nErased\n...\nFig. 10: Different kinds of inter-sample relationships. SISO: Same Image, Same Object (but different expressions). SIDO: Same Image,\nDifferent Object. DI: Different Image. We erase some common word(s) in the long sentences and add such samples into SISO.\nA Training Batch\nSDISDO\nSinit SSO\nSame Image \nSame Object\nInitial Image\nSame Image \nDifferent Object Different Image\n... ...\n ...\nFig. 11: One training batch in inter-sample learning.\ndemonstrate the effectiveness of the proposed transformer module,\nthe Mask Decoding Module in our implementation does not utilize\nany CNN features. We employ the Binary Cross-Entropy loss on\nthe predicted masks to supervise the network training.\n3.5 Masked Contrastive Learning\nHere we further consider inter-sample learning to explicitly en-\ndow the model with the knowledge of different language ex-\npressions to one object. The given expression in natural lan-\nguage is unconstrained. There are multiple ways to describe the\nsame target object, which brings challenges in understanding\nthese expressions. Given an image I that contains NO objects\n{O1,...,O i,...,O NO }, every object Oi in I can be referred\nby NE different expressions {E1\ni,...,E j\ni,...,E NE\ni }. A sample\nS(I,Oi,Ej\ni) of referring segmentation deﬁnes a mapping from\nan expression to the target object: {Ej\ni →Oi|I}. The mappings\nbetween Ei and Oi are in general many-to-one. An object in the\nimage can be described by many different language expressions,\nbut one language expression should unambiguously point to one\nand only one instance. Thus, no matter what kind of expressions\nare given to the target object, the ﬁnal mask is the same, i.e., the\nfeature Fm in Eq. (7) for generating the ﬁnal mask is the same.\nMotivated by this, here we introduce contrastive learning that\nforces the network to narrow the distance of features of different\nexpressions for the same target object while enlarging the distance\nof features for different objects. Furthermore, to provide more\npositive pairs and enhance the model’s generalization ability to\nthe input language, we randomly mask some speciﬁc words in\nthe language expression and add these masked expressions to the\npositive samples of the original expression.\nTo sample the training pairs for contrastive learning, we\nsummarize the inter-sample relationships into three categories: 1)\nDifferent Image (DI), 2) Same Image Different Object (SIDO), 3)\nSame Image Same Object (SISO), as shown in Fig. 10. Unlike\nexisting methods that construct the training batches in a fully\nrandom manner, we intend to let one batch have all kinds of inter-\nsample relationships. Firstly we randomly choose an initial sample\nSinit, as shown in Fig. 11. We denote its SISO images as SSO,\nwhose image Iand object Oiare the same asSinitbut expressions\nEj\ni are different from Sinit, and denote its SIDO images as SDO,\nwhich has the same image I as the initial sample but different\ntarget object Oi. When constructing a mini-batch, we ﬁrst put\nthe initial sample into it. Next we intentionally put at most NSO\nsamples from SSO, and at most NDO samples from SDO. The\nrest of the batch is ﬁlled with the randomly chosen DI samples.\nUnder this mechanism, every training batch will contain all kinds\nof inter-sample relationships, as shown in Fig. 11.\nAs we mentioned earlier, the features of SISO samples for\ngenerating the ﬁnal mask should be the same. In contrast, for\nSIDO items, though they share the same input image so the output\nfeature of the transformer may tend to be similar, the features\nfor generating the mask prediction should be different because\ntheir target outputs are different. From this point, we introduce\ncontrastive learning as feature-level supervision. In our approach,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\nthe Mask Decoder module plays the role in generating the output\nmask, hence we add the contrastive learning on the feature Fm,\nsee Eq. (7), of the Mask Decoder module. Inspired by the InfoNCE\nloss [69], our loss is deﬁned as follows:\nLCL =− 1\nNSO\n∑\nS+∈SSO\nlog exp\n(1\nτ\n⟨\nfS+,fSinit\n⟩)\n∑\nS∈SDO,S+\nexp( 1\nτ ⟨fS,fSinit⟩), (8)\nwhere S+ denotes a SISO sample of the initial sample, τ is a\ntemperature constant, fS is the feature Fm in the Mask Decoder\nmodule, and ⟨,⟩denotes the cosine similarity function. This loss\nfunction forces that the mask feature of the initial sample to be\ncloser to its SISO samples that are supposed to have the identical\noutput feature and mask, and force it to be away from its SIDO\nsamples, which are supposed to have a non-overlap mask with it.\nWhat’s more, a sentence usually has more than one informative\nclue. However, the network tends to capture the most discrimi-\nnative, or easiest clues to reach the training objective, resulting\nin underrating, even ignoring, other information. For example,\ngiven the image in Fig. 10 and an expression “man in green on\nleft”, we have experimentally observed that the network is over-\ninﬂuenced by the word “left” since it is more common in the\ndataset. We argue that overly relying on discriminative/common\nwords harms the model’s generalization ability. To address this\nissue, we propose to randomly mask some prominent words in\nthe language expression and add these masked samples, as SISO\nsamples, to our contrastive learning. Speciﬁcally, we measure the\nimportance of each word when evaluating the language attention\nin the QGM, as mentioned in Eq. (5). For language expression that\nare longer than Nm words, we sum up the word attention vectors\nof all the Nq queries to generate a global attention weight for\nevery words: ai = ∑Nq\nn=1 ani, where ani ∈Aqd represents the\nattention weight of the i-th word in attention vector for the n-th\nquery, ai denotes the global attention weight for i-th word. The\nglobal attention weights {a1,a2,...,a Nt}reveal the importance\nof each word. To enhance the diversity of the training samples,\nwords are chosen to be masked with a probability pm, where the\nprobability pm is determined by the global attention weights by\npmi = eai/∑Nt\nj=1 eaj . This probability-guided random setting\nlets more important words have higher masking chances while\nkeeping the diversity of the training sentences. If a word is masked,\nits corresponding feature in Ft is changed. Thus, we apply a soft-\nmax function in Eq. (5) again to re-normalize it. As a consequence,\na new set of query vectors and query responses are generated. The\nfeature for Mask Decoder by this erased sentence is trained to be\nclose to the original one by adding it as a positive sample S+ in\nEq. (8). In such a way, the network is encouraged to extract the\ninformation from words that are harder or not so discriminative\nrather than always relying on some high-frequency keywords,\nwhich could enhance the network’s versatility in practical usage.\n3.6 Network Architecture\nFeature Extractor. Since the transformer architecture only ac-\ncepts sequential inputs, the original image, and language input\nmust be transformed into feature space before sending to the\ntransformer. For vision features, following [15], we use a CNN\nbackbone for image encoding. We take the features of the last three\nlayers in the backbone as the input for our encoder. By resizing\nthe three sets of feature maps to the same size and summing them\ntogether, we get the raw vision feature Fvr ∈RH×W×C, where\nH,W is the spatial size of features, and C is the feature channel\nnumber. For language features, we ﬁrst use a lookup table to\nconvert each word into word embeddings [70], and then utilize\nan RNN module to achieve contextual understanding of the input\nsentence and convert the word embedding to the same number\nof channels as the vision feature, resulting in a set of language\nfeatures Ft ∈RNt×C. Fvr and Ft are then sent to the Spatial\nDynamic Fusion module and the Query Generation module as\nvision and language features.\nTransformer Module.We use a complete but shallow trans-\nformer to apply the attention operations on input features. The\nnetwork has a transformer encoder and a decoder, each of which\nhas two layers. We use the standard Transformer architecture\nas deﬁned by Vaswani et al. [12], in which each encoder layer\nconsists of one multi-head attention module and one feed-forward\nnetwork (FFN), and each decoder layer consists of two multi-head\nattention modules and one FFN. We ﬂatten the spatial domain of\nthe fused multi-modal feature Ffused into a sequence, forming\nthe multi-modal feature F′\nfused ∈ RNv×C,Nv = HW. The\ntransformer encoder takes F′\nfused as input, deriving the memory\nfeatures about vision information Fmem ∈ RNv×C. Before\nsending it to the encoder, we add a ﬁxed sine spatial positional\nembedding on F′\nfused. Fmem is then sent to the transformer\ndecoder as keys and values, together with Nq query vectors pro-\nduced by the Query Generation Module. The decoder queries the\nvision memory feature with language query vectors and outputs\nNq responses for mask decoding.\n4 E XPERIMENTS\nWe conducted extensive experiments to demonstrate the effective-\nness of our proposed Vision-Language Transformer (VLT) for re-\nferring segmentation. In this section, we introduce implementation\ndetails of our approach, benchmarks we used in the experiments,\nand report both the quantitative and qualitative results of our\nproposed approach compared with other state-of-the-art methods.\n4.1 Implementation Details\nExperiment Settings. Following previous works [13], [14], we\nuse the same experiment settings. Our framework utilizes Darknet-\n53 [71] pretrained on partial MSCOCO as the visual CNN back-\nbone. Images form the validation and test set of the RefCOCO\nseries are excluded in the pretraining. We use bi-GRU [72] as the\nRNN implementation and the Glove Common Crawl 840B [73]\nfor word embedding. The training image size is set to 416 ×416\npixels. Each Transformer block has eight heads, and the hidden\nlayer size in all heads is set to 256. For RefCOCO and RefCOCO+,\nwe set the maximum word number to 15, and for G-Ref, we set\nit to 20 as there are more long sentences. The Adam optimizer is\nused to train the network for 50 epochs, and the learning rate is\nset to λ=0.001. The batch size is 32 on one 32G V100 GPU.\nMetrics. We use two metrics in our experiments: mask\nIntersection-over-Union (IoU) and Precision with thresholds\n(Pr@X). The mask IoU demonstrates the mask quality, which\nemphasizes the model’s overall performance and reveals both\ntargeting and segmenting abilities. The Pr@ X metric computes\nthe ratio of successfully predicted samples using different IoU\nthresholds. Low threshold precision like Pr@0.5 reﬂects the iden-\ntiﬁcation performance of the method, and high threshold precision\nlike Pr@0.9 reveals the ability of generating high-quality masks.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nTABLE 1: Comparison with Convolutional Networks, containing seven 3×3 Conv layers, in terms of parameter size and performance.\nType #params IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\n7 Conv Layers ∼16.6M 60.42 66.44 61.86 53.22 44.72 17.27\nTransformer ∼17.5M 65.24 73.39 68.01 60.83 47.99 20.07\nTABLE 2: Comparison of the proposed Query Generation Module (QGM) with other kinds of query generation ways. “ Ft”: directly\nuse the language features Ft as query vectors. “Learnt”: learnable parameter-queries that are ﬁxed in testing, similar with [15].\nNo. Query Type IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\n1 Ft 60.26 69.88 64.61 56.70 43.62 18.06\n2 Learnt 58.60 67.84 59.98 53.23 44.60 16.33\n3 QGM (ours) 65.24 73.39 68.01 60.83 47.99 20.07\nTABLE 3: Ablation study of Query Numbers Nq. ‡: without\nQuery Balance Module (QBM).\nNq IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\n1 57.34 67.04 60.11 52.03 40.82 10.28\n2 60.78 70.18 63.50 55.41 44.20 16.03\n4 61.58 70.92 64.33 56.02 44.23 15.22\n8 64.35 72.61 66.98 58.83 46.98 19.63\n16 65.24 73.39 68.01 60.83 47.99 20.07\n32 65.12 73.21 67.59 60.13 48.03 18.64\n16‡ 63.80 71.96 67.46 59.73 47.22 19.71\n4.2 Datasets\nThe proposed VLT is evaluated on three public referring segmen-\ntation datasets: RefCOCO, RefCOCO+ [74] and G-Ref [75], [76].\nRefCOCO & RefCOCO+[74] are two of the largest image\ndatasets for referring segmentation. They are also called UNC\n& UNC+ datasets in some literature. 142,209 referring language\nexpressions describing 50,000 objects in 19,992 images are col-\nlected in the RefCOCO dataset, and 141,564 referring language\nexpressions for 49,856 objects in 19,992 images are collected in\nthe RefCOCO+ dataset. The difference between two datasets is\nthat the RefCOCO+ restricts the expression ways for the language\nsentences. For example, descriptions about absolute locations,e.g.,\n“leftmost”, are forbidden in the RefCOCO+ dataset.\nG-Ref [75], [76]. Also called RefCOCOg, it is another\nfamouse and well recognized referring segmentation dataset.\n104,560 referring language expressions for 54,822 objects in\n26,711 images are used in G-Ref. Unlike RefCOCO & Ref-\nCOCO+, the language usage in the G-Ref is more casual but\ncomplex, and the sentence lengthes of G-Ref are also longer in\naverage. Notably, G-Ref has two versions: one is called UMD\nsplit [76], the other is called Google split [75]. The UMD split has\nboth validation and testing set publicly available, but the Google\nsplit only makes its validation set public. We report the results of\nthe proposed VLT on both UMD and Google version.\n4.3 Ablation Study\nIn this section, we conduct ablation studies on the test B of Ref-\nCOCO to demonstrate the effectiveness of the proposed modules\nin our Vision-Language Transformer framework.\nTransformer v.s. ConvNet. To demonstrate the scale of our\nproposed network and verify the effectiveness of the transformer\nmodule, we compare our method with a regular ConvNet in\nterms of the performance and parameter size in TABLE 1. In\nthe experiment, we replace the whole transformer-based modules,\nincluding the transformer encoder-decoder, the Query Generation\nModule, and the Query Balance Module with seven stacked 3 ×3\n12 4 8 16 3255\n59\n63\n67IoU\n12 4 8 16 3266\n68\n70\n72\n74Prec@0.5\nFig. 12: Performance gain by increasing the query number Nq.\nConv layers that have similar parameters size to our transformer-\nbased modules. It shows that the parameter size of our transformer-\nbased module achieves a much superior performance while is only\nnearly equal to 7 convolutional layers. The transformer module\noutperforms the 7 Conv module with ∼5% margin in terms of\nIoU, and ∼7% margin in terms of Precision@0.5. This proves the\neffectiveness of the proposed transformer module.\nQuery Generation.In TABLE 2, we compare different kinds\nof query generation methods, including our proposed Query Gen-\neration Module (QGM), language features Ft as queries, and\nlearned parameters as queries. The Query Generation Module\noutperforms the other two methods with a large margin at about\n5% - 7% in terms of IoU and 4% - 6% in terms of Pr@0.5.\nFirstly, we directly utilize the language features Ft as query\nvectors and send them into the transformer decoder. In detail,\nthe given language expression is processed by an RNN network,\nthen the output for every word, and the output for the whole\nsentence, are used as query vectors. It can be seen in TABLE 2\nthat the performance of Ft as queries is ∼5% worse than QGM,\nwhich is because the information between words is not sufﬁciently\nexchanged and the understanding of language is derived from\nlanguage itself, as we discussed in Sec 3.2. The proposed Query\nGeneration Module has a much superior performance to the “ Ft”\nas queries. This demonstrates that the proposed QGM effectively\nunderstands the language expressions and produces valid attended\nlanguage features under the guidance of visual information. We\nset 16 query vectors that are initialized with uniform distribution\nat the beginning of the training in our experiment, and train these\nquery-parameters together with the network. As the “learnt” in\nTABLE 2, the performance of these learned ﬁxed query vectors\nis not satisfying, only 58.50%, which shows that such learned\nquery-parameters cannot represent the target object as effectively\nas online generated input-speciﬁc queries by the proposed QGM.\nQuery NumberNq. To demonstrate the inﬂuence of the query\nnumber Nq on the results, we evaluate the network’s results with\ndifferent numbers of query vectors. As we can see in TABLE 3\nand Fig. 12, though only one segmentation mask is required in\nthe ﬁnal prediction, multiple queries are desired for providing\ndiverse clues and can achieve better results than a single query. As\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\nTABLE 4: Ablation study of Multi-Modal Fusion.\nType IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\nTile 64.40 72.16 66.82 58.33 47.20 20.03\nTile + Conv×4 64.45 72.19 66.63 58.23 47.40 20.01\nSDF 65.24 73.39 68.01 60.83 47.99 20.07\nTABLE 5: Ablation study of Inter-Sample Learning.\n(a) Experiments on original dataset and masked dataset\nType IoU Pr@0.5\nOriginal Masked Gap Original Masked Gap\nw/o CL 63.43 59.53 -3.90 71.84 67.02 -4.82\nw/ CL 64.70 61.02 -3.68 72.88 68.45 -4.43\nw/ MCL 65.24 64.20 -1.04 73.39 72.19 -1.20\n(b) Cross dataset validation\nType IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\nw/o CL † 49.16 56.06 50.13 41.87 34.11 12.26\nw/ CL † 49.92 57.01 51.25 42.13 35.54 12.81\nw/ MCL† 52.35 60.41 55.12 49.80 39.35 14.76\nNative 56.30 66.03 61.53 56.20 41.22 13.09\nshown in TABLE 3 and Fig. 12, by increasing the query number\nNq, the performance gradually gets higher, and a signiﬁcant\nperformance gain of about 8% is achieved from 1 query to 16\nqueries. The performance gain slows down after the query number\nNq is larger than 8, therefore we select Nq = 16 as the default\nsetting. The performance gain achieved by larger Nq veriﬁes that\nmultiple input-speciﬁc queries produced by the proposed QGM\ndynamically represent the diverse comprehensions of language\nexpression. When the Query Balance Module (QBM) is discarded,\nmarked with ‡in TABLE 3, a performance drop of 1.44% IoU is\nobserved, which proves the advantage of the proposed QBM.\nTile v.s. Spatial-Dynamic Fusion. In TABLE 4, we com-\npare the “tile-and-concatenate” fusion and our proposed Spatial-\nDynamic Fusion (SDF). As we discussed in Section 3.1, the “tile”\noperation does not consider the difference of each pixel but uses\nan identical sentence feature for all pixels across the image. In\ncontrast, the proposed spatial-dynamic fusion customizes a unique\nlanguage feature for every pixel according to the interaction be-\ntween language information and corresponding pixel information.\nAs shown in TABLE 4, compared with “Tile”, the SDF module\nbrings a performance gain of 0.84% IoU and 1.23% Pr@0.5. The\nproposed SDF emphasizes the differences among pixels/objects\nand allows each position to select the more informative words,\nenhancing the multi-modal fusion and producing better multi-\nmodal features. “Tile + Conv ×4” in TABLE 4, which has the\nsame number of parameters as the proposed SDF, does not bring\nbetter performance than “Tile” because our network already has a\nsequential convolution layers after the feature fusion.\nInter-Sample Learning. Here we demonstrate the effective-\nness of our proposed inter-sample learning approach, Masked\nContrastive Learning (MCL). The results are shown in TABLE 5.\nFirstly, we add Contrastive Learning (CL) in the training of our\nnetwork. The CL does not contain masked sentences as SISO\nsamples. From TABLE 5a, on the original testing set, the CL\nbrings a performance gain of 1.27% in terms of IoU and 1.04%\nin terms of Pr@0.5, which demonstrates that the inter-sample\nlearning does enhance the model’s performance. Further, we\nintroduce the samples with masked sentences as SISO samples,\ni.e., positive pairs in contrastive learning. Compared with w/o\nTABLE 6: Ablation study of word selection mechanism in MCL.\nSelect. Type IoU Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9\nNone 49.92 57.01 51.25 42.13 35.54 12.81\nRandom 50.08 57.33 51.30 43.02 35.72 12.60\nThreshold θ 51.57 59.08 52.04 46.13 37.57 13.99\npm 52.35 60.41 55.12 49.80 39.35 14.76\n0% 5% 10% 15%\nPercentage of NDO\n63\n64\n65\n66IoU\n0% 5% 10% 15%\nPercentage of NDO\n71\n72\n73\n74Prec@0.5\nFig. 13: Ablation study of the percentage of NDO in MCL.\nCL, MCL brings a large performance gain of 1.81% IoU on\nthe original dataset. Compared with CL, MCL further brings a\nperformance gain of 0.54% in terms of IoU and 0.51% in terms of\nPr@0.5, which shows the beneﬁts brought by introducing samples\nwith masked expressions in training. To better demonstrate the\nmodel’s ability in dealing with unconstrained and diverse language\nexpressions in the wild, we do another two testings: 1) erase some\ninformative words of the given language expressions in these\ntesting samples, see “Masked” in TABLE 5a; 2) cross datasets\nvalidation between two datasets that have different common clues,\ni.e., training on RefCOCO while testing on the validation set of\nRefCOCO+, marked with w/o CL †, etc. in TABLE 5b. Firstly,\nas shown in TABLE 5a, compared with the original dataset, w/o\nCL drops 3.9% in terms of IoU and 4.82% in terms of Pr@0.5\non the Masked testing samples. The result shows that the w/o\nCL model overly relies on common keywords and is heavily\naffected by the missing of these common clues. While for w/\nMCL, the performance drop on the Masked validation is 1.04%\nand is much less than the performance drop of w/o CL, which\nveriﬁes the model’s robustness and generalization ability brought\nby introducing masked contrastive learning. Next, we do the cross-\ndataset validation on RefCOCO and RefCOCO+ in TABLE 5b. In\nRefCOCO, a large number of samples use absolute location ( e.g.,\n“the left” , “on the right” , etc.) for describing the target object,\nbut such kinds of expressions are not allowed in the RefCOCO+.\nTherefore, the cross datasets validation provides a good simulation\nof a practical scenario, in which the training information and\ntesting are inconsistent, and only partial clues are available for\ntesting. As shown in TABLE 5b, w/ MCL † outperforms w/o\nCL† 3.19% in terms of IoU and 4.35% in terms of Pr@0.5,\nwhich veriﬁes the model’s robustness and generalization ability\nin dealing with diverse language expressions that are different\nfrom training samples. “Native” in TABLE 5b denotes training\n& testing on RefCOCO+. As w/ MCL † v.s. “Native”, we can\nsee that the model trained on RefCOCO with MCL achieves\ncompetitive results on the validation set of RefCOCO+ compared\nto the model trained on RefCOCO+, proving that the proposed\nmasked contrastive learning enhances the model’s generalizability\nunder open-world practical scenarios.\nNext we do an ablation study about the word selection mecha-\nnism in our masked constrastive learning. Apart from the baseline\nmodel that disables MCL, we test three mask-word selection\nmethods: 1) randomly choose a word to mask, 2) randomly choose\na word with the weightaigreater than a thresholdθto mask, 3) the\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\nTABLE 7: Results on Referring Image Segmentation in terms of IoU and Prec@0.5. U: UMD split. G: Google split. Methods pretrained\non large-scale vision-language training datasets are marked with †.\nMethods Visual\nBackbone\nTextual\nEncoder\nRefCOCO RefCOCO+ G-Ref\nval test A test B val test A test B val(U) test(U) val(G)\nDMN [5] DPN92 SRU 49.78 54.83 45.13 38.88 44.22 32.29 - - 36.76\nRRN [23] DeepLab-R101 LSTM 55.33 57.26 53.93 39.75 42.15 36.11 - - 36.45\nMAttNet [14] MaskRCNN-R101 bi-LSTM 56.51 62.37 51.70 46.67 52.39 40.08 47.64 48.61 -\nCMSA [7] DeepLab-R101 None 58.32 60.61 55.09 43.76 47.60 37.89 - - 39.98\nCAC [24] ResNet101 bi-LSTM 58.90 61.77 53.81 - - - 46.37 46.95 44.32\nSTEP [77] DeepLab-R101 bi-LSTM 60.04 63.46 57.97 48.19 52.33 40.41 - - 46.40\nBRINet [8] DeepLab-R101 LSTM 60.98 62.99 59.21 48.17 52.32 42.11 - - 48.04\nCMPC [68] DeepLab-R101 LSTM 61.36 64.53 59.64 49.56 53.44 43.23 - - 39.98\nLSCM [27] DeepLab-R101 LSTM 61.47 64.99 59.55 49.34 53.12 43.50 - - 48.05\nCMPC+ [78] DeepLab-R101 LSTM 62.47 65.08 60.82 50.25 54.04 43.47 - - 49.89\nMCN [13] Darknet53 bi-GRU 62.44 64.20 59.71 50.62 54.99 44.69 49.22 49.40 -\nEFN [26] ResNet101 bi-GRU 62.76 65.69 59.67 51.50 55.24 43.01 - - 51.93\nBUSNet [28] DeepLab-R101 Self-Att 63.27 66.41 61.39 51.76 56.87 44.13 - - 50.56\nCGAN [79] DeepLab-R101 bi-GRU 64.86 68.04 62.07 51.03 55.51 44.06 51.01 51.69 46.54\nISFP [80] Darknet53 Bi-GRU 65.19 68.45 62.73 52.70 56.77 46.39 52.67 53.00 50.08\nLTS [25] Darknet53 bi-GRU 65.43 67.76 63.08 54.21 58.32 48.02 54.40 54.25 -\nVLT (ours) Darknet53 bi-GRU 67.52 70.47 65.24 56.30 60.98 50.08 54.96 57.73 52.02\nReSTR [37] ViT-B Transformer 67.22 69.30 64.45 55.78 60.44 48.27 - - 54.48\nMaIL [33]† ViLT BERT 70.13 71.71 66.92 62.23 65.92 56.06 62.45 62.87 61.81\nCRIS [35]† CLIP-R101 CLIP 70.47 73.18 66.10 62.27 68.08 53.68 59.87 60.36 -\nLA VT [34] Swin-B BERT 72.73 75.82 68.79 62.14 68.38 55.10 61.24 62.09 60.50\nVLT (ours) Swin-B BERT 72.96 75.96 69.60 63.53 68.43 56.92 63.49 66.22 62.80\nVLTDarknet53 (ours) Prec@0.5 77.03 81.01 73.39 66.03 71.87 56.91 62.05 60.96 57.88\nVLTSwin-B (ours) Prec@0.5 85.35 87.76 80.48 74.95 80.98 67.44 77.23 78.03 73.84\n(a). Image (b). \"Man in black coat\"\n(c). \"Man in black coat\",  \nw/o MCL\n(d). \"Man in black coat\",  \nw/ MCL \nFig. 14: Example results of the Masked Contrastive Learning.\nproposed method that words are masked based on the probability\npm. TABLE 6 shows that our method outperforms other mask\nword selection mechanisms.\nFor the setting of NDO in MCL, the ablation study in Fig. 13\nshows that the performance of the network reaches the peak when\nNDO is set to 10% of the batch size. For NSO, as the average\nnumber of expressions for an object is around 3, we can include\nall available Same Object (SO) samples in most cases.\nIn Fig. 14, we provide qualitative examples to show the\neffectiveness of the Masked Contrastive Learning. The original\ninput language expression contains information in two aspects:\ncolor (“black”) and attribute ( “coat”). The model without MCL\noverly relies on the more obvious color information ( “black”), so\nit fails to predict when the word is erased. In contrast, the model\nwith MCL successfully ﬁnds the target with partial information,\nshowing that the MCL enhances the model’s generalization ability\nto various language expressions.\nWe further test the training efﬁciency of our mask contrastive\nlearning approach. We train the network with and without the\nMCL and report the GPU memory usage during training and\nthe training speed of two runs with batch size set to 16. With\nMCL enabled, the GPU memory usage and average training speed\nare 18496MB and 0.479s/iter, respectively. Without MCL, they\nare 17842MB and 0.471s/iter, respectively. The increased training\nmemory and time by MCL are less than 4% and 2%, respectively.\n4.4 Comparison with State-of-the-art Methods\nHere we compare the proposed Vision-Language Transformer\n(VLT) framework with previous state-of-the-art referring image\nsegmentation methods on three commonly-used benchmarks, Re-\nfCOCO, RefCOCO+, and G-Ref. The results are reported in TA-\nBLE 7. It can be seen that the proposed VLT outperforms previous\nstate-of-the-art methods on all three benchmarks. On RefCOCO,\nthe IoU performance of the proposed VLT is better than other\nmethods, e.g., LTS [25], with ∼2% gain on three different testing\nsplits. Then on RefCOCO+, the proposed VLT achieves new state-\nof-the-art result and is around 2% better than previous state-of-\nthe-art method. On the hard benchmark G-Ref that has longer\nlanguage expressions, the proposed VLT consistently achieves\nnew state-of-the-art referring segmentation performance with an\nIoU improvement of about 0.5%-3%, which demonstrates that\nthe proposed VLT has good abilities to deal with hard cases\nand long expressions. We assume the reason is that, on the one\nhand, long and complex expressions usually contain more clues\nand more emphasis, and our proposed Query Generation Module\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13\nP P\n(a)\n (b)\nFig. 15: Visualizations of: (a) the attention map of point P in the\ntransformer encoder; (b) different query vectors Fq.\nand Query Balance Module can produce multiple comprehensions\nwith different emphases and ﬁnd the more suitable ones. On the\nother hand, harder cases also contain complex scenarios that need\na holistic view and understanding of the given language expression\nand image, and the multi-head attention is more appropriate for\nsuch complex scenarios as a global operator. Meantime, compared\nwith other methods that with stronger backbones, e.g., DeepLab-\nR101 [81], MaskRCNN-R101 [39], ResNet101 [82], our backbone\nDarknet53 and our proposed modules are lightweight.\nTo compare with methods using stronger backbones, we fur-\nther provide results with stronger visual and textual encoders in\nTABLE 7. We use the popular vision transformer backbone Swin-\nB [83] as visual encoder and BERT [47] as textual encoder to\nreplace the Darknet53 [71] and bi-GRU [72], respectively. Meth-\nods pretrained on large-scale vision-language datasets are marked\nwith †, e.g., MaIL [33] adopts ViLT [38] pre-trained on four\nlarge-scale vision-language pretraining datasets and CRIS [35]\nemploys CLIP [40] pretrained on 400M image-text pairs. As\nshown in TABLE 7, the proposed approach outperforms MaIL\nand CRIS by around 2% ∼4% IoU without using large-scale\nvision-language datasets in pretraining, which demonstrates the\neffectiveness of our proposed modules with stronger visual and\ntextual encoders. Especially, the proposed approach VLT achieves\nhigher performance gain on more difﬁcult dataset G-Ref that has\na longer average sentence length and more complex and diverse\nword usages, e.g., VLT is ∼4% IoU better than MaIL [33] and\nLA VT [34] on test (U) of G-Ref. It demonstrates the proposed\nmodel’s good ability in dealing with long and complex expres-\nsions with large diversities, which is mainly attributed to input-\nconditional query generation and selection that well cope with the\ndiverse words/expressions, and masked contrastive learning that\nenhances the model’s generalization ability.\n4.5 Qualitative Results and Visualization\nIn Fig. 15a, we extract and visualize an attention map for a position\n“P” from the 2nd layer of our transformer encoder. It shows that\nin a single layer of the transformer, the attention of one output\npixel globally extends to other input pixels far away. We also\nsee that pixel on one instance attends to other instances, showing\nour network is able to capture long-range interactions between\ninstances. In Fig. 15b, we visualize four query vectors Fq (see\nFig. 6 and Eq. (6)). The four query vectors differ from each\nother and have different distributions of response peaks, which\ndemonstrates the diversity of these input-speciﬁc query vectors.\nThen, we visualize some qualitative examples of the proposed\nVLT in Fig. 16. To demonstrate the identifying ability of our VLT,\nwe show the mask predictions of two different input language\nexpressions for every example. Image (a) and (c) are two typical\nexamples that the language expression directly provides the loca-\ntion or color clues of the target object. In the second expression\nof Image (c), “lighter color cat” , it can be seen that the proposed\nTABLE 8: Results on Referring Video Object Segmentation.\nMethods Backbone YouTube-RVOS Ref-DA VIS17\nJ&F J F J&F J F\nCMSA [7] ResNet50 36.4 34.8 38.1 40.2 36.9 43.5\nURVOS [84] ResNet50 47.2 45.3 49.2 51.5 47.3 56.0\nPMINet [85] ResNeSt101 53.0 51.5 54.5 - - -\nCITD [86] Ensemble 61.4 60.0 62.7 - - -\nReferFormer [87] V-Swin-B 62.9 61.3 64.6 61.1 58.1 64.1\nVLT (ours) V-Swin-B 63.8 61.9 65.6 61.6 58.9 64.3\nVLT is able to handle the expressions that indicate the target object\nby providing a comparison of it with other objects, e.g., “lighter”.\nThe examples of image (b) and (d) demonstrate the model’s ability\non understanding the attribute words,e.g., “stripes”, and relatively\nrarer words, e.g., “ﬂoral”. In the second expression of image\n(e), our VLT successfully identiﬁes the target object referred\nby expression describing the relationships between objects, i.e.,\n“Elephant with rider”. Image (f) contains a group of people,\nwhere all instances distribute densely in a complicated layout.\nThe proposed method manages to identify the target instance\nwith difﬁcult language expressions that contain multiple aspects\nof clues, such as direction ( “9 o’clock”), attributes (“white coat”\n& “gray suit” ), and posture (“kneeling”).\n4.6 Results on Referring Video Object Segmentation\nOur proposed approach can also be applied to referring video\nobject segmentation (RVOS) task with minor adaptation. We apply\nour model on each individual frame of the input video clip.\nWe use the average vision features of all frames of a video\nclip as the vision features in the QGM ( Fvq in Fig. 6). This\nenables the query input to be kept identical for all frames in a\nvideo clip, achieving a temporal consistency across frames. When\nperforming the contrastive learning on video data, we sample\ndifferent objects SDO in the ±2 adjacent frames of the initial\nobject. As adjacent frames shares similar image structure with\nthe original frame, we can enlarge the number of negative samples\nwhile keeping a similar behavior with our image model. According\nto the experiments, when only sampling SDO in the same video\nframe, the J&Fperformance is 63.5 while it increases to 63.8\nwhen adding the ±2 adjacent frames.\nIn TABLE 8, we report the quantitative results of the proposed\nVLT on the validation set of the YouTube-RVOS [84] dataset\nand Ref-DA VIS17 [88] dataset. YouTube-RVOS is a large-scale\nreferring video object segmentation benchmark, containing 3,978\nvideo clips with around 15K language expressions. Ref-DA VIS17,\nbuilding based on DA VIS17 [89], contains 90 video clips. The\nresults are reported with three standard evaluation metrics: region\nsimilarity J, contour accuracy F, as well as the mean value of\nthe two metrics J&F= (J+ F)/2.\nTo ensure a fair comparison, we use the Base model of Video\nSwin Transformer (V-Swin-B) [83], [90] as the backbone, the\nsame as ReferFormer [87]. “Ensemble” denotes visual encoder\nensemble of three backbones, including ResNet101 [82], HR-\nNet [91], and ResNeSt101 [92]. As shown in TABLE 8, although\nwe do not design speciﬁc modules and training losses for RVOS\nlike in ReferFormer [87], the proposed VLT achieves new state-\nof-the-art RVOS results consistently on both the YouTube-RVOS\nand Ref-DA VIS17, which demonstrate the effectiveness of the\nproposed VLT on referring video object segmentation.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14\nImage (a)\nImage (c)\nImage (b)\nImage (d)\nImage (e) Image (f)\n\"White bowl on corner\"\"Bowl of carrots\"\n\"Black cat\" \"Lighter color cat\"\n\"Guy with stripes\" \"White shirt\"\n\"Floral pattern\" \"Green shirt\"\n\"Curled tail\" \"Elephant with rider\" \"woman at 9 o'clock with \nwhite coat\"\n\"Man kneeling \nin gray suit\"\nFig. 16: (Best viewed in color) Qualitative examples of the proposed VLT. For each example, the ﬁrst image is the input image, and\ncaptions under the second and third images are the given language expressions.\n5 C ONCLUSION\nIn this work, we address the challenging multi-modal task of\nreferring segmentation by introducing transformer to facilitate\nthe long-range information exchange that is difﬁcult to achieve\nin conventional convolutional networks. We reformulate referring\nsegmentation as a direct attention problem and propose a Vision-\nLanguage Transformer (VLT) framework that exploits the trans-\nformer to perform attention operations. To emphasize the differ-\nences among pixels/objects, we introduce a spatial-dynamic multi-\nmodal fusion to produce a speciﬁc language feature vector for each\nposition of the image feature according to the interaction between\nlanguage information and corresponding pixel information. To\nsolve the problem of ambiguous referring expressions because of\nthe unknown emphasis, we propose a Query Generation Module\nand a Query Balance Module to comprehend the referring sentence\nbetter with the help of the referred image information. These two\nmodules work together to prominently improve the diversity of\nways to understand the image and query language. We further\nconsider inter-sample learning to explicitly endow the model with\nknowledge of understanding different language expressions of one\nobject. Masked contrastive representation learning is proposed to\nnarrow down the features of different expressions for the same\ntarget object while distinguishing the features of different objects,\nwhich signiﬁcantly enhances the model’s ability in dealing with\ndiverse language expressions in the wild. The proposed model is\nlightweight and achieves new state-of-the-art performance on three\npublic referring image segmentation datasets and two referring\nvideo object segmentation datasets.\nREFERENCES\n[1] R. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural lan-\nguage expressions,” in Proc. Eur. Conf. Comput. Vis. Springer, 2016,\npp. 108–124.\n[2] H. Ding, S. Cohen, B. Price, and X. Jiang, “Phraseclick: toward achieving\nﬂexible interactive segmentation by phrase and click,” inProc. Eur. Conf.\nComput. Vis. Springer, 2020, pp. 417–435.\n[3] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2015, pp. 3431–3440.\n[4] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, “Semantic\nsegmentation with context encoding and multi-path decoding,” IEEE\nTrans. Image Processing, vol. 29, pp. 3520–3533, 2020.\n[5] E. Margffoy-Tuay, J. C. P ´erez, E. Botero, and P. Arbel ´aez, “Dynamic\nmultimodal instance segmentation guided by natural language queries,”\nin Proc. Eur. Conf. Comput. Vis., 2018, pp. 630–645.\n[6] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2018, pp. 7794–\n7803.\n[7] L. Ye, M. Rochan, Z. Liu, and Y . Wang, “Cross-modal self-attention\nnetwork for referring image segmentation,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2019, pp. 10 502–10 511.\n[8] Z. Hu, G. Feng, J. Sun, L. Zhang, and H. Lu, “Bi-directional relationship\ninferring network for referring image segmentation,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2020, pp. 4424–4433.\n[9] H. Shi, H. Li, F. Meng, and Q. Wu, “Key-word-aware network for\nreferring expression image segmentation,” in Proc. Eur. Conf. Comput.\nVis., 2018, pp. 38–54.\n[10] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, “Context contrasted\nfeature and gated multi-scale aggregation for scene segmentation,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2393–2402.\n[11] H. Ding, X. Jiang, A. Q. Liu, N. M. Thalmann, and G. Wang, “Boundary-\naware feature propagation for scene segmentation,” in Proc. IEEE Int.\nConf. Comput. Vis., 2019, pp. 6819–6829.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. Adv.\nNeural Inform. Process. Syst., 2017, pp. 5998–6008.\n[13] G. Luo, Y . Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji, “Multi-\ntask collaborative network for joint referring expression comprehension\nand segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2020, pp. 10 034–10 043.\n[14] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg,\n“Mattnet: Modular attention network for referring expression compre-\nhension,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp.\n1307–1315.\n[15] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in Proc.\nEur. Conf. Comput. Vis. Springer, 2020, pp. 213–229.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15\n[16] P. Wang, Q. Wu, J. Cao, C. Shen, L. Gao, and A. v. d. Hengel, “Neigh-\nbourhood watch: Referring expression comprehension via language-\nguided graph attention networks,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2019, pp. 1960–1968.\n[17] D. Liu, H. Zhang, F. Wu, and Z.-J. Zha, “Learning to assemble neural\nmodule tree networks for visual grounding,” in Proc. IEEE Int. Conf.\nComput. Vis., 2019, pp. 4673–4682.\n[18] Z. Yang, B. Gong, L. Wang, W. Huang, D. Yu, and J. Luo, “A fast and\naccurate one-stage approach to visual grounding,” in Proc. IEEE Int.\nConf. Comput. Vis., 2019, pp. 4683–4693.\n[19] B. Zhuang, Q. Wu, C. Shen, I. Reid, and A. Van Den Hengel, “Parallel\nattention: A uniﬁed framework for visual object discovery through\ndialogs and queries,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2018, pp. 4252–4261.\n[20] Z. Yang, T. Chen, L. Wang, and J. Luo, “Improving one-stage visual\ngrounding by recursive sub-query construction,” in Proc. Eur. Conf.\nComput. Vis., vol. 12359. Springer, 2020, pp. 387–404.\n[21] Y . Liao, S. Liu, G. Li, F. Wang, Y . Chen, C. Qian, and B. Li, “A real-\ntime cross-modality correlation ﬁltering method for referring expression\ncomprehension,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\n2020, pp. 10 880–10 889.\n[22] C. Liu, Z. Lin, X. Shen, J. Yang, X. Lu, and A. Yuille, “Recurrent\nmultimodal interaction for referring image segmentation,” in Proc. IEEE\nInt. Conf. Comput. Vis., 2017, pp. 1271–1280.\n[23] R. Li, K. Li, Y .-C. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring\nimage segmentation via recurrent reﬁnement networks,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2018, pp. 5745–5753.\n[24] Y .-W. Chen, Y .-H. Tsai, T. Wang, Y .-Y . Lin, and M.-H. Yang, “Referring\nexpression object segmentation with caption-aware consistency,” inProc.\nBrit. Mach. Vis. Conf., 2019.\n[25] Y . Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan, “Locate then\nsegment: A strong pipeline for referring image segmentation,” in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 9858–9867.\n[26] G. Feng, Z. Hu, L. Zhang, and H. Lu, “Encoder fusion network with co-\nattention embedding for referring image segmentation,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2021.\n[27] T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, “Linguistic\nstructure guided context modeling for referring image segmentation,” in\nProc. Eur. Conf. Comput. Vis. Springer, 2020, pp. 59–75.\n[28] S. Yang, M. Xia, G. Li, H.-Y . Zhou, and Y . Yu, “Bottom-up shift\nand reasoning for referring image segmentation,” in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2021.\n[29] H. Ding, H. Zhang, J. Liu, J. Li, Z. Feng, and X. Jiang, “Interaction\nvia bi-directional graph of semantic region afﬁnity for scene parsing,” in\nProc. IEEE Int. Conf. Comput. Vis., 2021, pp. 15 848–15 858.\n[30] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, “Semantic\ncorrelation promoted shape-variant context for segmentation,” in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 8885–8894.\n[31] A. Kamath, M. Singh, Y . LeCun, I. Misra, G. Synnaeve, and N. Carion,\n“Mdetr – modulated detection for end-to-end multi-modal understand-\ning,” in Proc. IEEE Int. Conf. Comput. Vis., 2021.\n[32] H. Ding, C. Liu, S. Wang, and X. Jiang, “Vision-language transformer\nand query generation for referring segmentation,” in Proc. IEEE Int.\nConf. Comput. Vis., 2021, pp. 16 321–16 330.\n[33] Z. Li, M. Wang, J. Mei, and Y . Liu, “Mail: A uniﬁed mask-image-\nlanguage trimodal network for referring image segmentation,” arXiv\npreprint arXiv:2111.10747, 2021.\n[34] Z. Yang, J. Wang, Y . Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt:\nLanguage-aware vision transformer for referring image segmentation,”\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2022, pp. 18 155–\n18 165.\n[35] Z. Wang, Y . Lu, Q. Li, X. Tao, Y . Guo, M. Gong, and T. Liu, “Cris:\nClip-driven referring image segmentation,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2022, pp. 11 686–11 695.\n[36] K. Jain and V . Gandhi, “Comprehensive multi-modal interactions for\nreferring image segmentation,” arXiv preprint arXiv:2104.10412, 2021.\n[37] N. Kim, D. Kim, C. Lan, W. Zeng, and S. Kwak, “Restr: Convolution-\nfree referring image segmentation using transformers,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2022, pp. 18 145–18 154.\n[38] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language transformer\nwithout convolution or region supervision,” in Proc. Int. Conf. Mach.\nLearn. PMLR, 2021, pp. 5583–5594.\n[39] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in Proc.\nIEEE Int. Conf. Comput. Vis., 2017, pp. 2961–2969.\n[40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in ICML, 2021.\n[41] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko, “Modeling\nrelationships in referential expressions with compositional modular net-\nworks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2017, pp.\n1115–1124.\n[42] H. Zhang, Y . Niu, and S.-F. Chang, “Grounding referring expressions in\nimages by variational context,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2018, pp. 4158–4166.\n[43] R. Hong, D. Liu, X. Mo, X. He, and H. Zhang, “Learning to compose and\nreason with language tree structures for visual grounding,” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 44, no. 2, pp. 684–696, 2022.\n[44] X. Chen, L. Ma, J. Chen, Z. Jie, W. Liu, and J. Luo, “Real-time referring\nexpression comprehension by single-stage grounding network,” arXiv\npreprint arXiv:1812.03426, 2018.\n[45] J. Deng, Z. Yang, T. Chen, W. Zhou, and H. Li, “Transvg: End-to-end\nvisual grounding with transformers,” in Proc. IEEE Int. Conf. Comput.\nVis., 2021, pp. 1769–1779.\n[46] A. Sadhu, K. Chen, and R. Nevatia, “Zero-shot grounding of objects from\nnatural language queries,” in Proc. IEEE Int. Conf. Comput. Vis. , 2019,\npp. 4694–4703.\n[47] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” in Proc.\nNAACL-HLT, vol. 1. Association for Computational Linguistics, 2019,\npp. 4171–4186.\n[48] B. Krause, E. Kahembwe, I. Murray, and S. Renals, “Dynamic evalua-\ntion of transformer language models,” arXiv preprint arXiv:1904.08378,\n2019.\n[49] Y . Cai, J. Lin, H. Wang, X. Yuan, H. Ding, Y . Zhang, R. Timofte, and\nL. Van Gool, “Degradation-aware unfolding half-shufﬂe transformer for\nspectral compressive imaging,” in Proc. Adv. Neural Inform. Process.\nSyst., 2022.\n[50] J. Lin, Y . Cai, X. Hu, H. Wang, Y . Yan, X. Zou, H. Ding, Y . Zhang,\nR. Timofte, and L. Van Gool, “Flow-guided sparse transformer for video\ndeblurring,” in Proc. Int. Conf. Mach. Learn., 2022.\n[51] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” in Proc. Int. Conf. Learn. Represent., 2021.\n[52] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr et al. , “Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers,” in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2021, pp. 6881–6890.\n[53] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efﬁcient design for semantic segmentation with\ntransformers,” in Proc. Adv. Neural Inform. Process. Syst., 2021.\n[54] W. Liang, Y . Yuan, H. Ding, X. Luo, W. Lin, D. Jia, Z. Zhang, C. Zhang,\nand H. Hu, “Expediting large-scale vision transformer for dense predic-\ntion without ﬁne-tuning,” in Proc. Adv. Neural Inform. Process. Syst. ,\n2022.\n[55] S. Wang, Y .-P. Tan, H. Ding, K.-H. Yap, J. Yuan, and J.-Y . Wu,\n“Discovering human interactions with large-vocabulary objects via query\nand multi-scale detection,” in Proc. IEEE Int. Conf. Comput. Vis., 2021.\n[56] S. Wang, Y . Duan, H. Ding, Y .-P. Tan, K.-H. Yap, and J. Yuan, “Learning\ntransferable human-object interaction detector with natural language\nsupervision,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2022,\npp. 939–948.\n[57] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks,” in Proc.\nAdv. Neural Inform. Process. Syst., vol. 32, 2019.\n[58] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y . Choi, and J. Gao,\n“Vinvl: Revisiting visual representations in vision-language models,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 5579–5588.\n[59] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen,\nand I. Sutskever, “Zero-shot text-to-image generation,” inProc. Int. Conf.\nMach. Learn. PMLR, 2021, pp. 8821–8831.\n[60] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, “Pixel-bert: Aligning\nimage pixels with text by deep multi-modal transformers,”arXiv preprint\narXiv:2004.00849, 2020.\n[61] A. Zareian, K. D. Rosa, D. H. Hu, and S.-F. Chang, “Open-vocabulary\nobject detection using captions,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2021, pp. 14 393–14 402.\n[62] S. Goenka, Z. Zheng, A. Jaiswal, R. Chada, Y . Wu, V . Hedau, and\nP. Natarajan, “Fashionvlp: Vision language transformer for fashion\nretrieval with feedback,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., June 2022, pp. 14 105–14 115.\n[63] S. Chen, P.-L. Guhur, C. Schmid, and I. Laptev, “History aware mul-\ntimodal transformer for vision-and-language navigation,” in Proc. Adv.\nNeural Inform. Process. Syst., vol. 34, 2021, pp. 5834–5847.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 16\n[64] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less is\nmore: Clipbert for video-and-language learning via sparse sampling,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2021, pp. 7331–7341.\n[65] R. Hu and A. Singh, “Unit: Multimodal multitask learning with a uniﬁed\ntransformer,” in Proc. IEEE Int. Conf. Comput. Vis. , 2021, pp. 1439–\n1449.\n[66] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proc. IEEE Int. Conf. Comput.\nVis., 2019, pp. 3286–3295.\n[67] B. Cheng, A. G. Schwing, and A. Kirillov, “Per-pixel classiﬁcation is not\nall you need for semantic segmentation,” in Proc. Adv. Neural Inform.\nProcess. Syst., 2021, pp. 17 864–17 875.\n[68] S. Huang, T. Hui, S. Liu, G. Li, Y . Wei, J. Han, L. Liu, and B. Li, “Refer-\nring image segmentation via cross-modal progressive comprehension,”\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 10 488–\n10 497.\n[69] A. Van den Oord, Y . Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv e-prints, pp. arXiv–1807, 2018.\n[70] H. Zhang and H. Ding, “Prototypical matching and open set rejection for\nzero-shot semantic segmentation,” inProc. IEEE Int. Conf. Comput. Vis.,\n2021, pp. 6974–6983.\n[71] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\nonce: Uniﬁed, real-time object detection,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2016, pp. 779–788.\n[72] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Empirical evaluation of\ngated recurrent neural networks on sequence modeling,” arXiv preprint\narXiv:1412.3555, 2014.\n[73] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Proc. of the Conf. on Empirical Methods in\nNatural Language Process., 2014, pp. 1532–1543.\n[74] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling context\nin referring expressions,” in Proc. Eur. Conf. Comput. Vis. Springer,\n2016, pp. 69–85.\n[75] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy,\n“Generation and comprehension of unambiguous object descriptions,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 11–20.\n[76] V . K. Nagaraja, V . I. Morariu, and L. S. Davis, “Modeling context\nbetween objects for referring expression understanding,” in Proc. Eur.\nConf. Comput. Vis. Springer, 2016, pp. 792–807.\n[77] D.-J. Chen, S. Jia, Y .-C. Lo, H.-T. Chen, and T.-L. Liu, “See-through-\ntext grouping for referring image segmentation,” inProc. IEEE Int. Conf.\nComput. Vis., 2019, pp. 7454–7463.\n[78] S. Liu, T. Hui, S. Huang, Y . Wei, B. Li, and G. Li, “Cross-modal\n[88] A. Khoreva, A. Rohrbach, and B. Schiele, “Video object segmentation\nwith language referring expressions,” in Proc. Asi. Conf. Comput. Vis.\nSpringer, 2018, pp. 123–141.\nprogressive comprehension for referring segmentation,” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 44, no. 9, pp. 4761–4775, 2022.\n[79] G. Luo, Y . Zhou, R. Ji, X. Sun, J. Su, C.-W. Lin, and Q. Tian, “Cascade\ngrouped attention network for referring expression segmentation,” in\nACM Int. Conf. Multimedia, 2020, pp. 1274–1282.\n[80] C. Liu, X. Jiang, and H. Ding, “Instance-speciﬁc feature propagation for\nreferring segmentation,” IEEE Trans. Multimedia, 2022.\n[81] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n“Deeplab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs,” IEEE Trans. Pattern Anal.\nMach. Intell., vol. 40, no. 4, pp. 834–848, 2017.\n[82] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[83] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted win-\ndows,” in Proc. IEEE Int. Conf. Comput. Vis., 2021, pp. 10 012–10 022.\n[84] S. Seo, J.-Y . Lee, and B. Han, “Urvos: Uniﬁed referring video object\nsegmentation network with a large-scale benchmark,” in Proc. Eur. Conf.\nComput. Vis. Springer, 2020, pp. 208–223.\n[85] Z. Ding, T. Hui, S. Huang, S. Liu, X. Luo, J. Huang, and X. Wei,\n“Progressive multimodal interaction network for referring video object\nsegmentation,” The 3rd Large-scale Video Object Segmentation Chal-\nlenge, p. 7, 2021.\n[86] C. Liang, Y . Wu, T. Zhou, W. Wang, Z. Yang, Y . Wei, and Y . Yang,\n“Rethinking cross-modal interaction from a top-down perspective for\nreferring video object segmentation,” arXiv preprint arXiv:2106.01061 ,\n2021.\n[87] J. Wu, Y . Jiang, P. Sun, Z. Yuan, and P. Luo, “Language as queries for\nreferring video object segmentation,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2022, pp. 4974–4984.\n[89] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel ´aez, A. Sorkine-Hornung,\nand L. Van Gool, “The 2017 davis challenge on video object segmenta-\ntion,” arXiv preprint arXiv:1704.00675, 2017.\n[90] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin, and H. Hu, “Video\nswin transformer,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. ,\nJune 2022, pp. 3202–3211.\n[91] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y . Zhao, D. Liu, Y . Mu,\nM. Tan, X. Wang et al. , “Deep high-resolution representation learning\nfor visual recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43,\nno. 10, pp. 3349–3364, 2020.\n[92] H. Zhang, C. Wu, Z. Zhang, Y . Zhu, H. Lin, Z. Zhang, Y . Sun, T. He,\nJ. Mueller, R. Manmatha et al. , “Resnest: Split-attention networks,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recog. Worksh., 2022, pp. 2736–\n2746.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8529242873191833
    },
    {
      "name": "Transformer",
      "score": 0.6911175847053528
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6007043123245239
    },
    {
      "name": "Segmentation",
      "score": 0.5701247453689575
    },
    {
      "name": "Natural language processing",
      "score": 0.47702428698539734
    },
    {
      "name": "Randomness",
      "score": 0.47124892473220825
    },
    {
      "name": "Language model",
      "score": 0.413726270198822
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ],
  "cited_by": 110
}