{
  "title": "FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?",
  "url": "https://openalex.org/W4372306494",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5011008635",
      "name": "Shikhar Tuli",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A5042238232",
      "name": "Bhishma Dedhia",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A5024179661",
      "name": "Shreshth Tuli",
      "affiliations": [
        "Imperial College London"
      ]
    },
    {
      "id": "https://openalex.org/A5086131079",
      "name": "Niraj K. Jha",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1965142824",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W3000103182",
    "https://openalex.org/W3041166015",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W3176588845",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W6798306183",
    "https://openalex.org/W6766596047",
    "https://openalex.org/W6775706467",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W2152825437",
    "https://openalex.org/W6795146805",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2939863610",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W7055291151",
    "https://openalex.org/W2972482531",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2785430118",
    "https://openalex.org/W6785906802",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W6681029592",
    "https://openalex.org/W3081305497",
    "https://openalex.org/W2131241448",
    "https://openalex.org/W3213305054",
    "https://openalex.org/W6758657797",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W4205208902",
    "https://openalex.org/W3166244420",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3029385331",
    "https://openalex.org/W2548077621",
    "https://openalex.org/W2981406437",
    "https://openalex.org/W3149288754",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W6966769059",
    "https://openalex.org/W6754929296",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3031420959",
    "https://openalex.org/W3174730533",
    "https://openalex.org/W2916118939",
    "https://openalex.org/W2971381503",
    "https://openalex.org/W2724359148",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2736941579",
    "https://openalex.org/W3015107381",
    "https://openalex.org/W2951245151",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W4287242089",
    "https://openalex.org/W2966284335",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4230563027",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W3034560159",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4321393841",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2142498761",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2737925311",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W3035281298",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W4287725919",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "The existence of a plethora of language models makes the problem of selecting the best one for a custom task challenging. Most state-of-the-art methods leverage transformer-based models (e.g., BERT) or their variants. However, training such models and exploring their hyperparameter space is computationally expensive. Prior work proposes several neural architecture search (NAS) methods that employ performance predictors (e.g., surrogate models) to address this issue; however, such works limit analysis to homogeneous models that use fixed dimensionality throughout the network. This leads to sub-optimal architectures. To address this limitation, we propose a suite of heterogeneous and flexible models, namely FlexiBERT, that have varied encoder layers with a diverse set of possible operations and different hidden dimensions. For better-posed surrogate modeling in this expanded design space, we propose a new graph-similarity-based embedding scheme. We also propose a novel NAS policy, called BOSHNAS, that leverages this new scheme, Bayesian modeling, and second-order optimization, to quickly train and use a neural surrogate model to converge to the optimal architecture. A comprehensive set of experiments shows that the proposed policy, when applied to the FlexiBERT design space, pushes the performance frontier upwards compared to traditional models. FlexiBERT-Mini, one of our proposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9% higher GLUE score. A FlexiBERT model with equivalent performance as the best homogeneous model has 2.6× smaller size. FlexiBERT-Large, another proposed model, attains state-of-the-art results, outperforming the baseline models by at least 5.7% on the GLUE benchmark.",
  "full_text": "Journal of Artiﬁcial Intelligence Research 77 (2023) 39-70 Submitted 05/2022; published 05/2023\nFlexiBERT: Are Current Transformer Architectures too\nHomogeneous and Rigid?\nShikhar Tuli stuli@princeton.edu\nBhishma Dedhia bdedhia@princeton.edu\nDept. of Electrical & Computer Engineering, Princeton University\nPrinceton, NJ 08544 USA\nShreshth Tuli s.tuli20@imperial.ac.uk\nDepartment of Computing, Imperial College London\nLondon, SW7 2AZ UK\nNiraj K. Jha jha@princeton.edu\nDept. of Electrical & Computer Engineering, Princeton University\nPrinceton, NJ 08544 USA\nAbstract\nThe existence of a plethora of language models makes the problem of selecting the best\none for a custom task challenging. Most state-of-the-art methods leverage transformer-based\nmodels (e.g., BERT) or their variants. However, training such models and exploring their\nhyperparameter space is computationally expensive. Prior work proposes several neural\narchitecture search (NAS) methods that employ performance predictors (e.g., surrogate\nmodels) to address this issue; however, such works limit analysis to homogeneous models that\nuse ﬁxed dimensionality throughout the network. This leads to sub-optimal architectures.\nTo address this limitation, we propose a suite of heterogeneous and ﬂexible models, namely\nFlexiBERT, that have varied encoder layers with a diverse set of possible operations and\ndiﬀerent hidden dimensions. For better-posed surrogate modeling in this expanded design\nspace, we propose a new graph-similarity-based embedding scheme. We also propose a novel\nNAS policy, called BOSHNAS, that leverages this new scheme, Bayesian modeling, and\nsecond-order optimization, to quickly train and use a neural surrogate model to converge\nto the optimal architecture. A comprehensive set of experiments shows that the proposed\npolicy, when applied to the FlexiBERT design space, pushes the performance frontier\nupwards compared to traditional models. FlexiBERT-Mini, one of our proposed models, has\n3% fewer parameters than BERT-Mini and achieves 8.9% higher GLUE score. A FlexiBERT\nmodel with equivalent performance as the best homogeneous model has 2.6 ×smaller size.\nFlexiBERT-Large, another proposed model, attains state-of-the-art results, outperforming\nthe baseline models by at least 5.7% on the GLUE benchmark.\n1. Introduction\nIn recent years, self-attention (SA)-based transformer models (Vaswani et al., 2017; Devlin\net al., 2019) have achieved state-of-the-art results on tasks that span the natural language\nprocessing (NLP) domain. Large-scale pre-training datasets, increasing computational\npower, and robust training techniques (Liu et al., 2019) drive this burgeoning success. A\nchallenge that remains is eﬃcient optimal model selection for a speciﬁc task and a set of user\nrequirements. In this context, one should train only models with the maximum predicted\n©2023 The Authors. Published by AI Access Foundation under Creative Commons Attribution License CC BY 4.0.\nTuli, Dedhia, Tuli, & Jha\nperformance. This falls in the domain of neural architecture search (NAS) (Zoph & Le,\n2017).\n1.1 Challenges\nThe design space of transformer models is vast. Rigorous search has proposed several\nmodels in the past. Popular models include BERT, XLM, XLNet, BART, ConvBERT, and\nFNet (Devlin et al., 2019; Conneau & Lample, 2019; Yang et al., 2019; Lewis et al., 2020;\nJiang et al., 2020; Lee-Thorp et al., 2022). Transformer design involves a choice of several\nhyperparameters, including the number of layers, size of hidden embeddings, number of\nattention heads, and size of the hidden layer in the feed-forward network (Khetan & Karnin,\n2020). This leads to an exponential increase in the design space, making a brute-force\napproach to explore the design space computationally infeasible (Ying et al., 2019). The\naim is to converge to an optimal model as quickly as possible by testing the lowest possible\nnumber of datapoints (Pham et al., 2018). Moreover, model performance may not be\ndeterministic, requiring heteroscedastic modeling (Ru et al., 2020).\n1.2 Existing Solutions and Motivation\nRecent NAS advancements use various techniques to explore and optimize diﬀerent models\nin the deep learning domain, from image recognition to speech recognition and machine\ntranslation (Zoph & Le, 2017; Mazzawi et al., 2019). In the computer-vision domain,\nvarious search approaches, such as genetic algorithms, reinforcement learning, and structure\nadaptation, realize diverse convolutional neural network (CNN) architectures. Some even\nintroduce new basic operations (Zhang et al., 2018) to enhance performance on diﬀerent\ntasks. Many works leverage a performance predictor, often called a surrogate model, to\nreliably predict model accuracy. One can train such a surrogate through active learning\nby querying a few models from the design space and regressing their performance to the\nremaining space (under some theoretical assumptions), thus signiﬁcantly reducing search\ntimes (Siems et al., 2020; White et al., 2021b).\nUnlike CNN frameworks (Ying et al., 2019; Tan & Le, 2019), meant for vision tasks,\nthere is no universal framework for NLP that diﬀerentiates among transformer architectural\nhyperparameters. Works that do compare diﬀerent design decisions often do not consider\nheterogeneity and ﬂexibility in their search space and explore the space over a limited\nhyperparameter set (Khetan & Karnin, 2020; Xu et al., 2021; Gao et al., 2022) 1. For\ninstance, Primer (So et al., 2021) only adds depth-wise convolutions to the attention heads;\nAutoBERT-Zero (Gao et al., 2022) lacks deep feed-forward stacks; AutoTinyBERT (Yin et al.,\n2021) does not consider linear transforms (LTs) that outperform traditional SA operations\nin terms of parameter eﬃciency; AdaBERT (Chen et al., 2021) only considers a design space\nof convolution and pooling operations. Most works, in the ﬁeld of NAS for transformers,\ntarget model compression while trying to maintain the same performance (Chen et al., 2021;\nYin et al., 2021; Wang et al., 2020), which is orthogonal to our objectives in this work,\ni.e., searching for novel architectures that push the performance frontier. In addition, all\n1. Here, by heterogeneity, we mean that diﬀerent encoder layers can have distinct attention operations,\nfeed-forward stack depths, etc. By ﬂexibility, we mean that the hidden dimensions for diﬀerent encoder\nlayers in a transformer architecture can be mismatched.\n40\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nFramework Self-AttentionConv.Lin. Transform Flexible no. of\nattn. ops.\nFlexible feed-\nfwd. stacks\nFlexible hidden dim. Search\ntechniqueSDP WMA DFT DCT Ad. width Full ﬂexibility\nPrimer\n(So et al., 2021) ✓ ✓ ES\nAdaBERT\n(Chen et al., 2021) ✓ DS\nAutoTinyBERT\n(Yin et al., 2021)✓ ✓ ✓ ✓ ST\nDynaBERT\n(Hou et al., 2020)✓ ✓ ✓ ✓ ST\nNAS-BERT\n(Xu et al., 2021)✓ ✓ ✓ ST\nAutoBERT-Zero\n(Gao et al., 2022)✓ ✓ ✓ ES\nFlexiBERT (ours)✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ BOSHNAS\nTable 1: Comparison of related works with diﬀerent parameters ( ✓indicates that the corre-\nsponding feature is present). Adaptive width refers to diﬀerent architectures having\npossibly diﬀerent hidden dimensions (albeit each layer within the architecture\nhaving the same hidden dimension). Full ﬂexibility corresponds to each encoder\nlayer having, possibly, a diﬀerent hidden dimension.\nprevious works only consider rigid architectures. For instance, DynaBERT (Hou et al., 2020)\nonly adapts the width of the network by varying the number of attention heads (and not the\nhidden dimension of each head), which is only a simple extension to traditional architectures.\nFurther, their individual models still have the same hidden dimension throughout the network.\nAutoTinyBERT (Yin et al., 2021) and HAT (Wang et al., 2020), among others, ﬁx the input\nand output dimensions for each encoder layer (see Appendix A.1 for a background on the\nSA operation), which leads to rigid architectures.\nTable 1 gives an overview of various baseline NAS frameworks for transformer archi-\ntectures. It presents the aforementioned works and the respective features they include.\nPrimer (So et al., 2021) and AutoBERT-Zero (Gao et al., 2022) exploit evolutionary search\n(ES), which faces various drawbacks that limit elitist algorithms (Dang et al., 2021; White\net al., 2021a; Siems et al., 2020). AdaBERT (Chen et al., 2021) leverages diﬀerentiable\narchitecture search (DS), a popular technique used in many CNN design spaces (Siems et al.,\n2020). On the other hand, some recent works like AutoTinyBERT (Yin et al., 2021), Dyn-\naBERT (Hou et al., 2020), and NAS-BERT (Xu et al., 2021) leverage super-network training,\nwhere they train one large transformer and search its sub-networks in a one-shot manner.\nHowever, this technique is not amenable to diverse design spaces, as the super-network\nsize would drastically increase, limiting the gains from weight transfer to the relatively\nminuscule sub-network. Moreover, previous works limit their search to either the standard\nSA operation, i.e., the scaled dot-product (SDP), or the convolution operation. We extend\nthe basic attention operation to also include the weighted multiplicative attention (WMA).\nTaking motivation from recent advances with LT-based transformer models (Lee-Thorp\net al., 2022), we also add discrete Fourier transform (DFT) and discrete cosine transform\n(DCT) to our design space. AutoTinyBERT and DynaBERT also allow adaptive widths\nin the transformer architectures in their design space. However, each instance still has\nthe same dimensionality throughout the network (in other words, every encoder layer has\n41\nTuli, Dedhia, Tuli, & Jha\nthe same hidden dimension, as explained above). We mathematically detail why this is\ninherently a limitation in traditional transformer architectures in Appendix A.1. FlexiBERT,\nto the best of our knowledge, is the ﬁrst framework to allow full ﬂexibility – not only can\ndiﬀerent transformer instances in the design space have distinct widths, but each encoder\nlayer within a transformer instance can also have diﬀerent hidden dimensions. This results in\na massive design space with 3.32 billion transformer architectures. Searching this space via a\nbrute-force technique would be computationally infeasible. Hence, we leverage a novel NAS\ntechnique, Bayesian Optimization using Second-Order Gradients and Heteroscedastic Models\nfor Neural Architecture Search (BOSHNAS), to search for the best-performing architecture\nin this enormous design space.\n1.3 Our Contributions\nTo address the limitations of homogeneous and rigid models, we make the following technical\ncontributions:\n• We expand the design space of transformer hyperparameters to incorporate heteroge-\nneous architectures that venture beyond simple SA by employing other operations like\nconvolutions and LTs.\n• We propose novel projection layers and relative/trained positional encodings to make\nhidden sizes ﬂexible across layers – hence the name FlexiBERT.\n• We propose Transformer2vec that uses similarity measures to compare computational\ngraphs of transformer models to obtain a dense embedding that captures model\nsimilarity in a Euclidean space.\n• We propose a novel NAS framework, namely, BOSHNAS. It uses a neural network\nas a heteroscedastic surrogate model and second-order gradient-based optimization\nusing backpropagation to input (GOBI) (Tuli et al., 2021) to speed up search for the\nnext query in the exploration process. It leverages nearby trained models to transfer\nweights in order to reduce the amortized search time for every query.\n• Experiments on the GLUE benchmark (Wang et al., 2018) show that BOSHNAS\napplied to the FlexiBERT design space results in a score improvement of 0.4% compared\nto the baseline, i.e., NAS-BERT (Xu et al., 2021). The proposed model, FlexiBERT-\nMini, has 3% fewer parameters than BERT-Mini and achieves 8.9% higher GLUE\nscore. FlexiBERT also outperforms the best homogeneous architecture by 3%, while\nrequiring 2.6×fewer parameters. FlexiBERT-Large, our BERT-Large (Devlin et al.,\n2019) counterpart, outperforms the state-of-the-art models by at least 5.7% average\naccuracy on the ﬁrst eight tasks in the GLUE benchmark (Wang et al., 2018) 2.\nWe organize the rest of the paper as follows. Section 2 presents related work. Section 3\ndescribes the set of steps and decisions that undergird the FlexiBERT framework. In\n2. All the code for the FlexiBERT pipeline is available athttps://github.com/jha-lab/txf_design-space.\nThe code for running BOSHNAS on any tabular dataset of deep learning architectures is available at\nhttps://github.com/jha-lab/boshnas.\n42\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nSection 4, we present the results of design space exploration experiments. Finally, Section 5\nconcludes the article.\n2. Background and Related Work\nWe brieﬂy describe related work next.\n2.1 Transformer Design Space\nTraditionally, transformers have primarily relied on the SA operation (Vaswani et al., 2017).\nNevertheless, several works have proposed various compute blocks to reduce the number of\nmodel parameters and hence computational cost without compromising performance. For\ninstance, ConvBERT uses dynamic span-based convolutional operations that replace SA\nheads to model local dependencies directly (Jiang et al., 2020). Recently, FNet improved\nmodel eﬃciency using LTs instead (Lee-Thorp et al., 2022). MobileBERT, another recent\narchitecture, uses bottleneck structures and multiple feed-forward stacks to obtain smaller\nand faster models while achieving competitive results on well-known benchmarks (Sun et al.,\n2020). For completeness, we present other previously proposed advances to improve the\nBERT model in Appendix A.2.\n2.2 Neural Architecture Search\nNAS is an important machine learning technique that algorithmically searches for new neural\nnetwork architectures within a pre-speciﬁed design space under a given objective (He et al.,\n2021). Prior work implements NAS using various techniques, albeit limited to the CNN\ndesign space. A popular approach is to use a reinforcement learning algorithm, REINFORCE,\nthat is superior to other tabular approaches (Williams, 1992). Other approaches include\nGaussian-Process-based Bayesian Optimization (GP-BO) (Snoek et al., 2012), ES (Real\net al., 2019; Lu et al., 2019), etc. However, these methods come with challenges that limit\ntheir ability to reach state-of-the-art results in the CNN design space (White et al., 2021a).\nRecently, NAS has also seen the application of surrogate models for performance pre-\ndiction in CNNs (Siems et al., 2020). This results in the training of much fewer models to\npredict accuracy for the entire design space under some conﬁdence constraints. However,\nthese predictors are computationally expensive to train. This leads to a bottleneck, especially\nin large design spaces, in the training of subsequent models since we produce new queries\nonly after we train this predictor for every batch of trained models in the search space. Siems\net al. (2020) use a Graph Isomorphism Net (Xu et al., 2019) that regresses performance\nvalues directly on the computational graphs formed for each CNN model.\nAlthough previously restricted to CNNs (Zoph et al., 2018), NAS has recently seen\napplications in the transformer space as well. So et al. (2019) use standard NAS techniques\nto search for optimal transformer architectures. However, their method trains every new\nmodel from scratch. Furthermore, they do not employ knowledge transfer, which transfers\nweights from previously trained neighboring models to speed up subsequent training. This\nis important in the transformer space since pre-training every model is computationally\nexpensive. Further, the attention heads in their model follow the same dimensionality, i.e.,\nare not fully ﬂexible.\n43\nTuli, Dedhia, Tuli, & Jha\nOne of the state-of-the-art NAS techniques, BANANAS, implements Bayesian Opti-\nmization (BO) over a neural network model and predicts performance uncertainty using\nensemble networks that are, however, too compute-heavy (White et al., 2021a). BANANAS\nuses mutation/crossover on the current set of best-performing models and obtains the next\nbest-predicted model in this local space. Instead, we propose using GOBI (Tuli et al., 2021)\nto eﬃciently search for the next query in the global space. Thanks to random cold restarts,\nGOBI can search over diverse models in the architecture space. BANANAS also uses path\nembeddings, which perform sub-optimally for search over a diverse space (Cheng et al.,\n2021).\n2.3 Graph Embeddings that Drive NAS\nMany works on NAS for CNNs use graph embeddings to model their performance predictor.\nEach computational graph has a corresponding embedding, representing a speciﬁc CNN\narchitecture in the design space. A popular approach to learning with graph-structured\ndata is to make use of graph kernel functions that measure similarity between graphs. A\nrecent work, NASGEM (Cheng et al., 2021), uses the Weisfeiler-Lehman (WL) sub-tree\nkernel, which compares tree-like substructures of two computational graphs. This helps\ndistinguish between substructures that other kernels, like random walk, may deem identical\n(Shervashidze et al., 2011). Also, the WL kernel has an attractive computational complexity.\nThis has made it one of the most widely used graph kernels. Graph-distance-driven NAS\noften leads to enhanced representation capacity that yields optimal search results (Cheng\net al., 2021). However, the WL kernel only computes sub-graph similarities based on overlap\nin graph nodes. It does not consider whether or not two nodes are inherently similar. For\nexample, a computational ‘block’ (or its respective graph node) for an SA head with h= 128\nand o= SDP would be closer to another attention block with, say, h= 256 and o= WMA,\nbut would be farther from a block representing a feed-forward layer.\nOnce we have similarities computed between every possible graph pair in the design space,\nwe learn dense embeddings, the Euclidean distance for which should follow the similarity\nfunction. These embeddings would be not only helpful in eﬀective visualization of the design\nspace but also for fast computation of neighboring graphs in the active-learning loop. Further,\na dense embedding helps us practically train a ﬁnite-input surrogate function (as opposed\nto the sparse path encodings used by White et al., 2021a). Many works have achieved this\nusing diﬀerent techniques. Narayanan et al. (2017) train task-speciﬁc graph embeddings\nusing a skip-gram model and negative sampling, taking inspiration from word2vec (Mikolov\net al., 2013). In this work, we take inspiration from GloVe instead (Pennington et al.,\n2014), by applying manifold learning to all distance pairs (Kruskal, 1964). Hence, using\nglobal similarity distances built over domain knowledge and batched gradient-based training,\nwe obtain the proposed Transformer2vec embeddings that are superior to traditional\ngeneralized graph embeddings.\nWe take motivation from NASGEM (Cheng et al., 2021), which showed that training a\nWL kernel-guided encoder has advantages in scalable and ﬂexible search. Thus, we train\na performance predictor on the Transformer2vec embeddings, which not only aid in the\ntransfer of weights between neighboring models but also support better-posed continuous\n44\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nDesign Space \n \nGraph Library \n \nEmbedding Library \n \nBest Architecture \n   \nBOSHNAS \n   \n   \n   \n   \n   \n   \n  \nSearch query\nTransfer \nweights Train model\nTrain surrogate\nfunction\nCrossover with\nneighbors\n(a)\n(b) (c)\n(d)\n(e)\nFigure 1: Overview of the FlexiBERT pipeline.\nperformance approximation. More details on the computation of these embeddings are given\nin Section 3.3.\n3. Methodology\nIn this work, we train a heteroscedastic surrogate model that predicts the performance of a\ntransformer architecture and uses it to run second-order optimization in the design space.\nWe do this by decoupling the training procedure from pre-processing the embedding of every\nmodel in the design space to speed up training. First, we train embeddings to map the\nspace of computational graphs to a Euclidean space ( Transformer2vec) and then train the\nsurrogate model on the embeddings.\nOur work involves exploring a vast and heterogeneous design space and searching for\noptimal architectures with a given task. To this end, we (a) deﬁne a design space via a\nﬂexible set of architectural choices (see Section 3.1), (b) generate possible computational\ngraphs (G; see Section 3.2), (c) learn an embedding for each point in the space using a\ndistance metric for graphs (∆; see Section 3.3), and (d) employ a novel search technique\n(BOSHNAS) based on surrogate modeling of the performance and its uncertainty over the\ncontinuous embedding space (see Section 3.4). In addition, to tackle the enormous design\nspace, we propose a hierarchical search technique that iteratively searches over ﬁner-grained\nmodels derived from (e) a crossover of the best models obtained in the current iteration and\ntheir neighbors. Figure 1 gives a broad overview of the FlexiBERT pipeline, as explained\nabove. We show an unrolled version of this iterative ﬂow below:\nDesign Space →G1\nT− →∆1\nBOSHNAS− −−−−−− →g∗ cross-over−−−−−−→G2\nT− →...\nHowever, for simplicity of notation, we omit the iteration index in further references. We\nnow discuss the key elements of this pipeline in detail.\n45\nTuli, Dedhia, Tuli, & Jha\nDesign Element Allowed Values\nNumber of encoder layers (l) {2,4}\nType of attention operation used (oj) {SA, LT, DSC}\nNumber of operation heads (nj) {2,4}\nHidden size (hj) {128,256}\nFeed-forward dimension (fj) {512,1024}\nNumber of feed-forward stacks {1,3}\nOperation parameters (pj):\nif oj= SA Self-attention type: {SDP, WMA}\nelse ifoj= LT Linear transform type: {DFT , DCT}\nelse ifoj= DSC Convolution kernel size: {5,9}\nTable 2: Design space description. Super-script ( j) depicts the value for layer j.\n3.1 FlexiBERT Design Space\nWe now describe the FlexiBERT design space, i.e., box (a) in Figure 1.\n3.1.1 Set of Operations in FlexiBERT\nThe traditional BERT model comprises multiple layers, each containing a bidirectional\nmulti-headed SA module followed by a feed-forward module. Previous works propose several\nmodiﬁcations to the original encoder, primarily to the attention module. This gives rise to\na richer design space. We consider WMA-based SA in addition to SDP-based operations\n(Luong et al., 2015).\nWe also incorporate LT-based attention in FNet (Lee-Thorp et al., 2022) and dynamic-\nspan-based convolution (DSC) in ConvBERT (Jiang et al., 2020), in place of the vanilla SA\nmechanism. Whereas the original FNet implementation uses DFT, we also consider DCT.\nThe motivation behind using DCT is its widespread application in lossy data compression,\nwhich we believe can lead to sparse weights, thus leaving room for optimizations with\nsparsity-aware machine learning accelerators (Yu & Jha, 2022). Our design space allows\nvariable kernel sizes for convolution-based attention. Consolidating diﬀerent attention\nmodule types that vary in their computational costs into a single design space enables the\nmodels to have inter-layer variance in expression capacity. Inspired by MobileBERT (Sun\net al., 2020), we also consider architectures with multiple feed-forward stacks. We summarize\nthe entire design space with the range of each operation type in Table 2. The ranges of\ndiﬀerent hyperparameters are in accordance with the design space spanned by BERT-Tiny\nto BERT-Mini (Turc et al., 2019), with additional modules included as discussed. We\ncall this the Tiny-to-Mini space. This restricts our curated testbed to models with up to\n3.3 ×107 trainable parameters. This curated parameter space allows us to perform extensive\nexperiments, comparing the proposed approach against various baselines.\nWe can express every model in the design space via a model card, a dictionary containing\nthe chosen value for each design decision. We represent BERT-Tiny (Turc et al., 2019) in\nthis formulation as\n{\nl: 2,o : [SA,SA],h : [128,128],n : [2,2],f : [[512],[512]] ,p : [SDP,SDP]\n}\n,\n46\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nwhere the length of the list for every entry in f denotes the size of the feed-forward stack.\nWe employ the model card to derive the computational graph of the model using smaller\nmodules inferred from the design choice (details in Section 3.2).\n3.1.2 Flexible Hidden Dimensions\nTraditional transformer architectures restrict the ﬂow of information using a constant\nembedding dimension across the network (a matrix of dimensions NT ×h from one layer\nto the next, where NT denotes the number of tokens and h the hidden dimension; more\ndetails in Appendix A.1). Instead, we allow architectures in our design space to have ﬂexible\ndimensions across layers. This enables diﬀerent layers to capture information of diﬀerent\ndimensions, as it learns more abstract features deeper into the network. For this, we make\nthe following modiﬁcations:\n• Projection layers: We add an aﬃne projection network between encoder layers with\ndissimilar hidden sizes to transform encoding dimensionality.\n• Relative positional encoding : The vanilla-BERT implementation uses an absolute\npositional encoding at the input and propagates it ahead through residual connections.\nSince we relax the restriction of a constant hidden size across layers, this does not\napply to many models in our design space (as the learned projections for absolute\nencodings may not be one-to-one). Instead, we add a relative positional encoding at\neach layer (Shaw et al., 2018; Huang et al., 2018; Yang et al., 2019). Such an encoding\ncan entirely replace absolute positional encodings with relative position representations\nlearned using the SA mechanism. Whereas the SA module implementation remains\nthe same as in previous works, for DSC-based and LT-based attention, we learn the\nrelative encodings separately using SA and add them to the output of the attention\nmodule.\nFormally, let Q and V denote the query and the value layers, respectively. Let R\ndenote the relative embedding tensor that the model needs to learn. Let Z and X\ndenote the output and the input tensors of the attention module, respectively. In\naddition, let us deﬁne LT-based attention and DSC-based attention as LT(·) and\nDSC(·), respectively. Then,\nRelativeAttention(X) = softmax\n(\nQR⊤\n√\ndQ\n)\nV (1)\nZLT = LT(X) + RelativeAttention(X) (2)\nZDSC = DSC(X) + RelativeAttention(X) (3)\nOne should note that the proposed approach would only be applicable when the positional\nencodings are trained instead of being predetermined (Vaswani et al., 2017). The proposed\nrelative and trained positional encodings enable us to make the dimensionality of data ﬂow\nﬂexible across the network layers. This also means that each layer in the feed-forward stack\ncan have a distinct hidden dimension.\n47\nTuli, Dedhia, Tuli, & Jha\nInput Token\nEmbeddings\nEncoder Layer\nEncoder Layer\nFine-tuning Head\nOutput Probabilities\nInput\nOutput\no1 - SA / h1 -\n128 / p1 - SDP\no1 - SA / h1 -\n128 / p1 - SDP\nAdd & Norm \nf1 - 512 \nAdd & Norm \nProjection \nPosition \n Embeddings +\nFigure 2: Block-level computation graph for BERT-Tiny in FlexiBERT. The projection layer\nimplements an identity function since the hidden sizes of the input and output\nencoder layers are equal.\n3.2 Graph Library\nWe now describe the graph library, i.e., box (b) in Figure 1.\n3.2.1 Block-level Computational Graphs\nTo learn a lower-dimensional dense manifold of the given design space, characterized by a\nlarge number of FlexiBERT models, we convert each model into a computational graph. We\nformulate this graph based on the forward ﬂow of connections for each compute block. For\nour design space, we take all possible combinations of the compute blocks derived from the\ndesign decisions presented in Table 2 (see Appendix B.1 for a list of possible compute blocks\nsupported in FlexiBERT). Using this design space and the set of compute blocks, we create\nall possible computational graphs within the design space for every transformer model. We\nthen use recursive hashing as follows (Ying et al., 2019). For every node in this graph, we\nconcatenate the hash of its input, that node, and its output, and then take the hash of the\nresult. We use SHA256 as our hashing function. Doing this for all nodes and then hashing\nthe concatenated hashes gives us the resultant hash of a given computational graph. This\nhelps us detect isomorphic graphs and remove redundancy.\nFigure 2 shows the block-level computational graph for BERT-Tiny. Using the connection\npatterns for every possible block permutation, we can generate multiple graphs for the given\ndesign space.\n48\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\n3.2.2 Levels of Hierarchy\nThe total number of possible graphs in the design space with heterogeneous feed-forward\nhidden layers is ∼3.32 billion. This is substantially larger than any transformer design space\nused in the past.\nTo make our approach tractable, we propose a hierarchical search method. We consider\neach model in the design space to be composed of multiple stacks containing at least one\nencoder layer. In the ﬁrst step, we restrict each stack to s= 2 layers, where each layer in\na stack shares the same design conﬁguration. Naturally, this limits the search space size\n(we denote the set of all graphs in this space by G1). Hence, for instance, BERT-Tiny falls\nunder G1 since the two encoder layers have the same conﬁguration. We learn embeddings\nin this space and then run NAS to obtain the best-performing models. In the subsequent\nstep, we consider a design space constituted by a ﬁner-grained neighborhood of these models.\nWe derive the neighborhood using pairwise crossover between the best-performing models\nand their neighbors in a space where the number of layers per stack is s/2 = 1, denoted by\nG2 (detailed explanation of the crossover operation in Appendix B.4). Finally, we include\nheterogeneous feed-forward stacks (s= 1∗) and denote the space by G3.\n3.3 Transformer2vec\nWe now describe the Transformer2vec embedding and how we create an embedding library\nfrom a graph library G, i.e., box (c) in Figure 1.\n3.3.1 Graph Edit Distance\nTaking inspiration from Cheng et al. (2021) and Pennington et al. (2014), we train dense\nembeddings using global distance metrics, such as the Graph Edit Distance (GED) (Abu-\nAisheh et al., 2015). These embeddings enable fast derivation of neighboring graphs in the\nactive learning loop to facilitate the transfer of weights. We call them Transformer2vec\nembeddings. Unlike other approaches like the WL kernel, GED bakes in domain knowledge\nin graph comparisons, as explained in Section 2.3, by using a weighted sum of node insertion,\ndeletion, and substitution costs.\nFor the GED computation, we ﬁrst sort all possible compute blocks in the order of their\ncomputational complexity. Then, we weight the insertion and deletion cost for every block\nbased on its index in this sorted list and the substitution cost between two blocks based on\nthe diﬀerence in the indices in this sorted list. For computing the GED, we use a depth-ﬁrst\nalgorithm that requires less memory than traditional methods (Abu-Aisheh et al., 2015).\n3.3.2 Training Embeddings\nGiven that there are Sgraphs in G, we compute the GED for all possible computational graph\npairs. This gives us a dataset of N =\n(S\n2\n)\ndistances. To train the embedding, we minimize\nthe mean-square error as the loss function between the predicted Euclidean distance and\nthe corresponding GED. For the design space in consideration, we generate d-dimensional\nembeddings for every level of the hierarchy. Concretely, to train embedding T, we minimize\n49\nTuli, Dedhia, Tuli, & Jha\nthe loss\nLT =\n∑\n1≤i≤N,1≤j≤N,i̸=j\n(\nd(T(gi),T(gj)) −GED(gi,gj)\n)2\n, (4)\nwhere d(·,·) is the Euclidean distance and we calculate the GED for the corresponding\ncomputational graphs gi, gj ∈G.\n3.3.3 Weight Transfer among Neighboring Models\nPre-training each model in the design space is computationally expensive. Hence, we rely\non weight sharing to initialize a query model in order to directly ﬁne-tune it and minimize\nexploration time (details in Appendix B.3). We generate k nearest neighbors of a graph in\nthe design space (we use k= 100 for our experiments). Then, naturally, we would like to\ntransfer weights from the corresponding ﬁne-tuned neighbor that is closest to the query, as\nsuch models intuitively have similar initial internal representations.\nWe calculate this similarity using a biased overlap measure that counts the number of\nencoder layers from the input to the output that are common to the current graph ( i.e., have\nexactly the same hyperparameter values). We stop counting the overlap on encountering\ndiﬀerent encoder layers, regardless of subsequent overlaps. In this ranking, there could be\nmore than one graph with the same biased overlap with the current graph. Since the learned\ninternal representations depend on the subsequent set of operations as well, we break ties\nbased on the embedding distance of these graphs with the current graph. This gives us a set\nof neighbors, denoted by Nq for a model q, for every graph that are ranked based on both\nthe biased overlap and the embedding distance. It helps increase the probability of ﬁnding a\ntrained neighbor with high overlap.\nAs a hard constraint, we only consider transferring weights if the biased overlap fraction\n(Of(q,n) = biased overlap/lq, where q is the query model, n ∈ Nq is the neighbor in\nconsideration, and lq is the number of layers in q) between the queried model and its\nneighbor is above a threshold τ. If the query-neighbor pair meets the constraint, we transfer\nthe weights of the shared part from the corresponding neighbor to the query and ﬁne-tune it.\nOtherwise, we pre-train the query. We denote the weight transfer operation by Wq ←Wn.\n3.4 BOSHNAS\nWe now describe the BOSHNAS search policy, i.e., box (d) in Figure 1.\n3.4.1 Uncertainty Types\nTo overcome the challenges of an unexplored design space, it is important to consider the\nuncertainty in model predictions to guide the search process. Predicting model performance\ndeterministically is not enough to estimate the next most probably best-performing model.\nWe leverage the upper conﬁdence bound (UCB) exploration on the predicted performance of\nunexplored models (Russell & Norvig, 2010). This could arise from not only the approxima-\ntions in the surrogate modeling process but also parameter initializations and variations in\nmodel performance due to diﬀerent training recipes. These are called epistemic and aleatoric\nuncertainties, respectively. The former, also called reducible uncertainty, arises from a lack\n50\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nof knowledge or information, and the latter, also called irreducible uncertainty, refers to the\ninherent variation in the system to be modeled.\n3.4.2 Surrogate Model\nIn BOSHNAS, we use Monte-Carlo (MC) dropout (Gal & Ghahramani, 2016) and a\nNatural Parameter Network (NPN) (Wang et al., 2016) to model the epistemic and aleatoric\nuncertainties, respectively. The NPN not only helps with a distinct prediction of aleatoric\nuncertainty that we use for optimizing the training recipe once we are close to the optimal\narchitecture but also serves as a superior model to Gaussian Processes, Bayesian Neural\nNetworks (BNNs), and other Fully-Connected Neural Networks (FCNNs) (Tuli et al., 2021).\nConsider the NPN network fS(x; θ) with a transformer embedding xas input and parameters\nθ. The output of such a network is the pair ( µ,σ) ←fS(x; θ), where µ is the predicted\nmean performance and σ is the aleatoric uncertainty. To model the epistemic uncertainty,\nwe use two deep surrogate models: (1) teacher ( gS) and (2) student ( hS) networks. It is\na surrogate for the performance of a transformer, using its embedding x as an input. The\nteacher network is an FCNN with MC Dropout (parameters θ′). To compute the epistemic\nuncertainty, we generate n samples using gS(x,θ′). The standard deviation of the sample\nset is denoted by ξ. To run GOBI (Tuli et al., 2021) and avoid numerical gradients due to\ntheir poor performance, we use a student network (FCNN with parameters θ′′) that directly\npredicts the output ˆξ←hS(x,θ′′), a surrogate of ξ (Tuli et al., 2022).\n3.4.3 Active Learning and Optimization\nFor a design space G, we ﬁrst form an embedding space ∆ by transforming all graphs in G\nusing the Transformer2vec embedding. Assuming we have the three networks fS,gS, and\nhS for our surrogate model, we use the following UCB estimate:\nUCB = µ+ k1 ·σ+ k2 ·ˆξ=\n(\nfS(x,θ)[0] + k1 ·fS(x; θ)[1]\n)\n+ k2 ·hS(x,θ′′), (5)\nwhere x∈∆, k1, and k2 are hyperparameters.\nTo generate the next transformer to test, we execute GOBI using neural network\ninversion and the AdaHessian optimizer (Yao et al., 2021) that uses second-order updates to\nx (∇2\nxUCB) up till convergence. From this, we get a new query embedding, x′. We ﬁnd the\nnearest transformer architecture based on the Euclidean distance of all available transformer\narchitectures in the design space ∆, giving the next closest model x. We ﬁne-tune this model\n(or pre-train it if there is no nearby trained model with suﬃcient overlap; see Section 3.3) on\nthe required task to obtain the respective performance. Once we receive the new datapoint,\n(x,o), we train the models using the loss functions on the updated corpus δ′:\nLNPN(fS,x,o ) =\n∑\n(x,o)∈δ′\n(µ−o)2\n2σ2 + 1\n2 ln σ2,\nLTeacher(gS,x,o ) =\n∑\n(x,o)∈δ′\n(gS(x,θ′) −o)2,\nLStudent(hS,x) =\n∑\nx,∀(x,o)∈δ′\n(hS(x,θ′′) −ξ)2,\n(6)\n51\nTuli, Dedhia, Tuli, & Jha\nFCNN      \nFCNN \n  \n \nSampling\nNPN    \nUCB\nGOBI\n(MC Dropout)\nFigure 3: Overview of the BOSHNAS pipeline.\nwhere µ,σ = fS(x,θ) and ξ is obtained by sampling gS(x,θ′). The ﬁrst is the aleatoric loss\nto train the NPN model (Wang et al., 2016); the other two are squared-error loss functions.\nWe run multiple random cold restarts of GOBI to get multiple queries for the next step in\nthe search process.\nFigure 3 shows diﬀerent surrogate models in the BOSHNAS pipeline ( fS,gS, and hS) in\nthe order of ﬂow. As explained in Section 3.4, the NPN network (fS) models the performance\nand the aleatoric uncertainty, and the student network (hS) models the epistemic uncertainty\nfrom the teacher network (gS).\nAlgorithm 1 summarizes the BOSHNAS workﬂow. Starting from an initial pre-trained\nset δ in the ﬁrst level of the hierarchy G1, we run until convergence the following steps\nin a multi-worker compute cluster. To trade oﬀ between exploration and exploitation, we\nconsider two probabilities: uncertainty-based exploration (α) and diversity-based exploration\n(β). With probability 1 −α−β, we run second-order GOBI using the surrogate model to\nminimize UCB in Eq. (5). Adding the converged point ( x,o) in δ, we minimize the loss\nvalues in Eq. (6) (line 6 in Algorithm 1). We then generate a new query point, transfer\nweights from a neighboring model, and train it (lines 7-11). With α probability, we sample\nthe search space using the combination of aleatoric and epistemic uncertainties, k1 ·σ+ k2 ·ˆξ,\nto ﬁnd a point where the performance estimate is uncertain (line 15). To avoid getting stuck\nin a localized search subset, we also choose a random point with probability β (line 18).\nOnce we converge in the ﬁrst level, we continue with the second and third levels, G2 and G3,\nas described in Section 3.2.\n4. Experimental Results\nIn this section, we show how the FlexiBERT model obtained from BOSHNAS outperforms\nthe baselines.\n4.1 Setup\nFor our experiments, we set the number of layers in each stack to s= 2 for the ﬁrst level\nof the hierarchy, where models have the same conﬁgurations in every stack. In the second\nlevel, we use s= 1. Finally, we also make the feed-forward stacks heterogeneous ( s= 1∗) in\nthe third level (details given in Section 3.2). For the range of design choices in Table 2 and\n52\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nAlgorithm 1:BOSHNAS\nResult: best architecture\n1 Initialize: overlap threshold (τ), convergence criterion, uncertainty sampling prob.\n(α), diversity sampling prob. ( β), surrogate model ( fS, gS, and hS) on initial\ncorpus δ, design space g∈G⇔ x∈∆;\n2 while convergence criterion not met do\n3 wait till a worker is free\n4 if prob ∼U(0,1) <1 −α−β then\n5 δ←δ ∪{new performance point ( x,o)};\n6 ﬁt(surrogate, δ) using Eqn. (6);\n7 x ←GOBI(fS, hS); /* Optimization step */\n8 for n in Nx do\n9 if n is trained & Of(x,n) ≥τ then\n10 Wx ←Wn;\n11 send x to worker;\n12 break;\n13 else\n14 if 1 −α−β ≤prob. <1 −β then\n15 x ←argmax(k1 ·σ+ k2 ·ˆξ); /* Uncertainty sampling */\n16 send x to worker;\n17 else\n18 send random x to worker; /* Diversity sampling */\nsetting s= 2, we obtain 9312 unique graphs after removing isomorphic graphs. We set the\ndimension of the Transformer2vec embedding to d= 16 after running a grid search. To\ndo this, we minimize the distance prediction error while keeping d small using knee-point\ndetection. We obtain the hyperparameter values in Algorithm 1 through grid search. We\nuse overlap threshold τ = 80%, α= β = 0.1, and k1 = k2 = 0.5 in our experiments. The\nconvergence criterion is met in BOSHNAS when the change in performance is within 10 −4\nfor ﬁve iterations. We give details of the model training process in Appendix B.2.\n4.2 Pre-training and Fine-tuning Models\nWe adapt our pre-training recipe from the one used in RoBERTa, proposed by Liu et al.\n(2019), with slight variations in order to reduce the training budget (details in Appendix B.2).\nWe initialize the architecture space with models adapted from equivalent models presented\nin literature (Turc et al., 2019; Lee-Thorp et al., 2022; Jiang et al., 2020). The 12 initial\nmodels used to initiate the search process are BERT-Tiny, BERT-2/256 (with two encoder\nlayers and a ﬁxed hidden dimension of 256), BERT-4/128, BERT-Mini, FNet-Tiny, FNet-\n2/256, FNet-4/128, FNet-Mini, ConvBERT-Tiny, ConvBERT-2/256, ConvBERT-4/128, and\nConvBERT-Mini (with pj = DFT for FNets and pj = 9 for ConvBERTs adapted from the\noriginal models). These models form the initial set δ in Algorithm 1.\n53\nTuli, Dedhia, Tuli, & Jha\n(a) (b) (c)\nFigure 4: Bar plot comparing all NAS techniques with (a) naive embeddings and a design\nspace of homogeneous models, (b) naive embeddings and an expanded design\nspace of homogeneous and heterogeneous models, and (c) Transformer2vec (T2v)\nembeddings with the expanded design space. Plotted with 90% conﬁdence intervals.\n4.3 Ablation Study\nWe compare BOSHNAS against other popular techniques from the CNN space, namely\nRandom Search (RS), ES, REINFORCE, GP-BO, and a recent state-of-the-art, BANANAS.\nWe present performance on the GLUE benchmark.\nFigure 4 shows the best GLUE scores reached by respective baseline NAS techniques\nalong with BOSHNAS used with naive ( i.e., feature-based one-hot) or Transformer2vec\nembeddings on a representative design space. We use the space in the ﬁrst level of the\nhierarchy (i.e., with 9312 graphs, s= 2) and run all these algorithms in an active-learning\nscenario (all targeted homogeneous models form a subset of this space) over 50 runs for\neach algorithm. The plot highlights the fact that enhancing the richness of the design\nspace enables the algorithms to search for more accurate models (6% improvement averaged\nacross all models). We also see that Transformer2vec embeddings help NAS algorithms\nreach better-performing architectures (9% average improvement). Overall, BOSHNAS with\nthe Transformer2vec embeddings performs the best in this representative design space,\noutperforming the state-of-the-art ( i.e., BANANAS on naive embeddings) by 13%.\nFigure 5(a) shows the best GLUE score reached by each baseline NAS algorithm against\nthe number of models it trained. Again, we perform these runs on the representative\ndesign space described above, using the Transformer2vec encodings. As observed in the\nﬁgure, BOSHNAS reaches the best GLUE score. Ablation analysis justiﬁes the need for het-\neroscedastic modeling and second-order optimization (see Figure 5(b)). The heteroscedastic\nmodel forces the optimization of the training recipe when the framework approaches optimal\narchitectural design decisions. Second-order gradients, on the other hand, help the search\navoid local optima and saddle points and also aid faster convergence.\n54\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\n(a) (b)\nFigure 5: Performance results: (a) best GLUE score with trained models for NAS baselines\nand (b) ablation of BOSHNAS. Plotted with 90% conﬁdence intervals.\nModel ParametersCoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLIAvg.\nBERT-Mini (Turc et al., 2019)16.6M 0 74.8 71.8 84.1 66.4 57.9 85.9 73.3 62.3 64.0\nNAS-BERT10(Xu et al., 2021) 10M 27.8 76.0 81.5 86.3 88.4 66.6 88.6 84.8 53.7∗ 72.6\nFlexiBERT-Mini (ours, w/o S.)7.2M 16.7 72.3 72.9 81.7 76.9 64.1 80.9 77.0 65.3 67.5\nFlexiBERT-Mini (ours, w/o H.)20M 12.3 74.4 72.3 76.4 76.3 59.5 81.2 75.4 67.8 66.2\nFlexiBERT-Mini†(ours) 13.8M 28.7 77.5 82.3 86.9 87.8 67.6 89.7 83.0 51.8 72.7\nFlexiBERT-Mini (ours) 16.1M 23.8 76.1 82.4 87.1 88.7 69.0 81.0 78.9 69.3 72.9\nTable 3: Comparison between FlexiBERT and baselines. We evaluate the models on the\ndevelopment set of the GLUE benchmark. We use Matthews correlation for CoLA,\nSpearman correlation for STS-B, and accuracy for other tasks. We report MNLI\non the matched set. We also include ablation models for BOSHNAS without\nsecond-order gradients (w/o S.) and without using the heteroscedastic model (w/o\nH.). Best (second-best) performance values are in boldface (underlined). ∗Xu et al.\n(2021) do not report the performance of NAS-BERT 10 on the WNLI dataset; we\nobtained it using an equivalent model in our design space. The FlexiBERT-Mini †\nmodel only optimizes performance on the ﬁrst eight tasks for a fair comparison\nwith NAS-BERT.\nTable 3 shows the scores of the ablation models on the GLUE benchmarking tasks. We\nrefer to the best model obtained from BOSHNAS in the Tiny-to-Mini space as FlexiBERT-\nMini. Once we get the best architecture from the search process (using the same, albeit limited\ncompute budget for feasible search times), we pre-train and ﬁne-tune it on a larger compute\nbudget (details in Appendix B.2). According to the table, FlexiBERT-Mini outperforms the\nbaseline, NAS-BERT (Xu et al., 2021), by 0.4% on the GLUE benchmark. Since NAS-BERT\nﬁnds the higher-performing architecture while only considering the ﬁrst eight GLUE tasks\n(i.e., without the WNLI dataset), for a fair comparison, we ﬁnd a neighboring model in the\n55\nTuli, Dedhia, Tuli, & Jha\n×10\n62\n64\n66\n68\n70\n72\n2.6x smaller\n3% higher performance\nFigure 6: Performance frontiers of FlexiBERT on an expanded design space (under the\nconstraints deﬁned in Table 2) and for traditional homogeneous models.\nModel ParametersBoolQ CB COPA MultiRC WiC WSC Avg.\nBERT-Mini (Turc et al., 2019)16.6M 62.1 22.2 40.0 59.1 52.2 61.4 49.5\nFlexiBERT-Mini (ours) 16.1M 62.2 47.4 45.0 63.2 54.3 63.5 55.9\nTable 4: Comparison between BERT-Mini and FlexiBERT-Mini on the SuperGLUE bench-\nmark. For CB we report macro-average F1. We report accuracy for other tasks.\nFlexiBERT design space that only optimizes performance on the ﬁrst eight tasks. We call\nthis model FlexiBERT-Mini†. We see that although FlexiBERT-Mini † does not have the\nhighest GLUE score, it generally outperforms NAS-BERT 10 by signiﬁcant margins on the\nﬁrst eight tasks.\nFigure 6 demonstrates that FlexiBERT pushes to improve the performance frontier\nrelative to traditional homogeneous architectures. In other words, the best-performing\nmodels in the expanded (Tiny-to-Mini) space outperform traditional models for the same\nnumber of parameters. Here, the homogeneous models incorporate the same design decisions\nfor all encoder layers, even with the expanded set of operations ( i.e., including convolutional\nand LT-based attention operations). FlexiBERT-Mini has 3% fewer parameters than BERT-\nMini and achieves 8.9% higher GLUE score. FlexiBERT achieves 3% higher performance\nthan the best homogeneous model while the model with equivalent performance has 2.6 ×\nsmaller size.\nTable 4 shows the performance of FlexiBERT-Mini on SuperGLUE (Wang et al., 2019),\nwhich contains more challenging tasks relative to those in the GLUE benchmark. FlexiBERT-\nMini outperforms BERT-Mini on the tasks in SuperGLUE. We give details of the selected\nset of training hyperparameters in Appendix B.2.\n56\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nModel Parameters GLUE score\nRoBERTa (Liu et al., 2019) 345M 88.5\nFNet-Large (Lee-Thorp et al., 2022) 357M 81.9 ∗\nAutoTinyBERT (Yin et al., 2021) 85M 81.2 ∗\nDynaBERT (Hou et al., 2020) 345M 81.6 ∗\nNAS-BERT60 (Xu et al., 2021) 60M 83.2 ∗\nAutoBERT-Zero Large (Gao et al., 2022) 318M 84.5 ∗\nFlexiBERT-Large (ours) 319M 89.1/90.2∗\nTable 5: Comparison between FlexiBERT-Large (outside of the constraints deﬁned in Ta-\nble 2) and baselines on GLUE score. GLUE ∗scores reported do not consider the\nWNLI dataset.\n4.4 Best Architecture in the Design Space\nAfter running BOSHNAS for each level of the hierarchy, we obtain the respective best-\nperforming models, whose model cards we present in Appendix B.5. From these best-\nperforming models, we can extract the following rules that lead to high-performing trans-\nformer architectures:\n• Models with DCT in the deeper layers are preferable for higher performance on the\nGLUE benchmark. Shallower layers prefer the traditional SDP-based attention heads.\n• Models with more attention heads, but a smaller hidden dimension, are preferable\nin the deeper layers. On the other hand, fewer attention heads with higher hidden\ndimensions are preferable in shallower layers.\n• Feed-forward networks with larger widths, but a smaller depth, are preferable in the\ndeeper layers. Shallower layers prefer the opposite, i.e., smaller width and higher\ndepth.\nUsing these guidelines, we extrapolate the model card for FlexiBERT-Mini to get\nthe design decisions for FlexiBERT-Large, which is an equivalent counterpart of BERT-\nLarge (Devlin et al., 2019). Appendix B.5 presents the approach for extrapolation of\nhyperparameter choices from FlexiBERT-Mini to obtain FlexiBERT-Large. We train\nFlexiBERT-Large with the larger compute budget (see Appendix B.2) and show its GLUE\nscore in Table 5. FlexiBERT-Large outperforms the baseline RoBERTa by 0.6% on the\nentire GLUE benchmarking suite and AutoBERT-Zero Large by 5.7% when only considering\nthe ﬁrst eight tasks.\nJust like FlexiBERT-Large is the BERT-Large counterpart of FlexiBERT-Mini, we\nsimilarly form the BERT-Small and BERT-Base equivalents (Turc et al., 2019). Figure 7\npresents the performance frontier of these FlexiBERT models with diﬀerent baseline works.\nFlexiBERT consistently outperforms the baselines for diﬀerent constraints on model size,\nthanks to its search in a vast, heterogeneous, and ﬂexible design space of architectures.\n57\nTuli, Dedhia, Tuli, & Jha\n(a) (b)\n(c) (d)\nFlexiBERT-Mini†\nFlexiBERT-Large\nAutoBERT-Zero LargeNAS-BERT60\nNAS-BERT5\nAutoTinyBERT-KD-S1\nAutoTinyBERT-KD-S4\nAutoBERT-Zero\nFigure 7: Performance of FlexiBERT and other baseline methods on various GLUE tasks:\n(a) SST-2, (b) QNLI, (c) MNLI (we plot accuracy of MNLI-m), and (d) CoLA.\n5. Conclusion\nIn this work, we presented FlexiBERT, a suite of heterogeneous and ﬂexible transformer\nmodels. We characterized the eﬀects of this expanded design space and proposed a novel\nTransformer2vec embedding scheme to train a surrogate model that searches the design\nspace for high-performance models. We described a novel NAS algorithm, BOSHNAS,\nand showed that it outperforms the state-of-the-art by 13%. The FlexiBERT-Mini model\nsearched in this design space has a GLUE score that is 8.9% higher than BERT-Mini, while\nrequiring 3% fewer parameters. It also outperforms the baseline, NAS-BERT 10 by 0.4%. A\nFlexiBERT model with equivalent performance as the best homogeneous model achieves\n2.6×smaller size. FlexiBERT-Large outperforms the state-of-the-art models by at least\n5.7% average accuracy on the ﬁrst eight tasks in the GLUE benchmark.\n58\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nAcknowledgments\nThis work was supported by NSF Grant No. CNS-1907381 and CCF-2203399. The experi-\nments reported in this paper were substantially performed on the computational resources\nmanaged and supported by Princeton Research Computing at Princeton University. We\nalso thank Xiaorun Wu for the initial discussions.\nAppendix A. Background\nHere, we discuss some supplementary background concepts.\nA.1 Self-attention\nTraditionally, transformers have relied on the SA operation. It is basically a trainable\nassociative memory. We depict the vanilla SA operation as SDP and introduce the WMA\noperation in our design space as well. For a source vector s and a hidden-state vector h:\nSDP := s⊤h√\nd\n, WMA := s⊤Wah (7)\nwhere d is the dimension of the source vector and Wa is a trainable weight matrix in the\nattention layer. Naturally, a WMA layer is more expressive than an SDP layer.\nThe SA mechanism used in the context of transformers also involves the softmax function\nand matrix multiplication. More concretely, in a multi-headed SA operation (with n\nheads), there are four matrices: Wq\ni ∈Rdinp×h/n, Wk\ni ∈Rdinp×h/n, Wv\ni ∈Rdinp×h/n, and\nWo\ni ∈Rh/n×dout. The attention head takes the hidden states of the previous layer as input\nH ∈RNT ×dinp, where i refers to an attention head, dinp is the input dimension, dout is the\noutput dimension, and h is the hidden dimension. We then calculate the output of the\nattention head (Hi ∈RNT ×dout) as follows:\nQi,Ki,Vi = HWq\ni,HWk\ni,HWv\ni (8)\nHi = softmax\n(QiK⊺\ni√\nh\n)\nViWo\ni (9)\nFor traditional homogenous transformer models, dout has to equal dinp (usually, dinp = dout =\nh) due to the residual connections. However, thanks to the relative and trained positional\nencodings and the added projection layer at the end of each encoder ( Wp ∈Rdout×dp), we\ncan relax this constraint. This leads to an increased transformer model ﬂexibility in the\nFlexiBERT design space.\nA.2 Improving BERT’s Performance\nBERT is one of the most widely used transformer architectures (Devlin et al., 2019).\nResearchers have improved BERT’s performance by revamping the pre-training technique.\nRoBERTa proposed a more robust pre-training approach to improve BERT’s performance\nby considering dynamic masking in the Masked Language Modeling (MLM) objective\n(Liu et al., 2019). XLNet introduced Permuted Language Modeling (PLM) (Yang et al.,\n2019) and MPNet extended it by unifying MLM and PLM techniques (Song et al., 2020).\n59\nTuli, Dedhia, Tuli, & Jha\nThese methods achieve functional improvement in pre-training. Other approaches include\ntechniques such as denoising autoencoders (Lewis et al., 2020).\nOn the other hand, Khetan and Karnin (2020) consider optimizing the set of architectural\ndesign decisions for BERT – number of encoder layers l, size of hidden embeddings h, number\nof attention heads a, size of the hidden layer in the feed-forward network f, etc. However, it\nis only concerned with pruning BERT and does not target optimization of accuracy over\ndiﬀerent tasks. Further, it has a limited search space consisting of only homogeneous models.\nAppendix B. Experimental Details\nWe present the details of the experiments performed next.\nB.1 Possible Compute Blocks\nBased on the design space shown in Table 2, we consider all possible compute blocks, as\npresented next:\n• For layer j, when the operation is SA, we have two or four heads among: h-128/SA-\nSDP, h-128/SA-WMA, h-256/SA-SDP, and h-256/SA-WMA. If the encoder layer has\nan LT operation, we have two or four heads among: h-128/LT-DFT, h-128/LT-DCT,\nh-256/LT-DFT, and h-256/LT-DCT; the latter entry being the type of LT operation.\nFor a convolutional (DSC) operation, we have two or four heads among: h-128/DSC-5,\nh-128/DSC-9, h-256/DSC-5, and h-256/DSC-9; the latter integer referring to the\nkernel size.\n• For layer j, the size of the hidden layer in the feed-forward network is either 512\nor 1024. Also, the feed-forward network may either have just one hidden layer or\na stack of three layers. At higher levels of the hierarchy in the hierarchical search\nframework (details in Section 3.2), all the layers in the stack of hidden layers have the\nsame dimension until we relax this constraint in the last leg of the hierarchy.\n• Other blocks: Add&Norm, Input, and Output.\nB.2 Model Training\nWe pre-train our models with a combination of publicly available text corpora, viz.BookCorpus\n(BookC) (Zhu et al., 2015), Wikipedia English (Wiki), OpenWebText (OWT) (Gokaslan\n& Cohen, 2019), and CC-News (CCN) (Mackenzie et al., 2020). We borrow most training\nhyperparameters from RoBERTa. We set the batch size to 256, learning rate warmed up\nover the ﬁrst 10,000 steps to its peak value at 1 ×10−5 that then decays linearly, weight\ndecay to 0.01, Adam scheduler’s parameters β1 = 0.9, β2 = 0.98 (shown to improve stability;\nLiu et al., 2019), ϵ= 1 ×10−6, and run pre-training for 1 ,000,000 steps.\nOnce we ﬁnd the best models, we pre-train and ﬁne-tune the selected models with a\nlarger compute budget. For pre-training, we add the C4 dataset (Raﬀel et al., 2020) and\ntrain for 3,000,000 steps before ﬁne-tuning. We also ﬁne-tune on each GLUE task for 10\nepochs instead of 5 (further details given below). We executed this extended training process\nfor the FlexiBERT-Mini and FlexiBERT-Large models. Table 6 shows the improvement in\nperformance of FlexiBERT-Mini trained using knowledge transfer (where we transfer the\nweights from a nearby trained model) after additional training. When compared to the model\n60\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nModel Pre-training data Pre-training steps Fine-tuning epochs GLUE score\nFlexiBERT-Mini\nw/ knowledge transfer BookC, Wiki, OWT, CCN 1,000,000 5 69.7\n+ pre-training from scratchBookC, Wiki, OWT, CCN 1,000,000 5 70.4\n+ larger compute budgetBookC, Wiki, OWT, CCN, C4 3,000,000 10 72.9\nTable 6: Performance of FlexiBERT-Mini from BOSHNAS after knowledge transfer from\na nearby trained model, and after pre-training from scratch along with a larger\ncompute budget.\nTask Learning rate Batch size Training size Deviation\nCoLA 2.0 ×10−4 64 9K 2.1\nMNLI 9.4 ×10−5 64 393K 3.4\nMRPC 2.23 ×10−5 32 4K 3.5\nQNLI 5.03 ×10−5 128 105K 2.6\nQQP 3.7 ×10−4 64 364K 1.3\nRTE 1.9 ×10−4 128 3K 5.1\nSST-2 1.2 ×10−4 128 67K 6.9\nSTS-B 7.0 ×10−5 32 7K 4.1\nWNLI 4.0 ×10−5 128 634 4.7\nTable 7: Hyperparameters used for ﬁne-tuning FlexiBERT-Mini on the GLUE tasks along\nwith the size of the training set and deviation in performance.\ndirectly ﬁne-tuned after knowledge transfer, we see only a marginal improvement when we\npre-train from scratch. This reaﬃrms the advantage of knowledge transfer that it reduces\ntraining time (see Appendix B.3) with a negligible loss in performance. This is a consequence\nof a high overlap threshold, i.e., 80%, which results in a low performance loss at the cost of\nmaximizing the probability of ﬁnding a pre-trained neighbor. Training with a more signiﬁcant\ncompute budget further improves performance on the GLUE benchmark, validating the\nimportance of data size and diversity in pre-training (Liu et al., 2019). Running a full-ﬂedged\nBOSHNAS on the larger design space ( i.e., with layers from 2 to 24, Tiny-to-Large) can be\nan easy extension of this work.\nWhile running BOSHNAS, we ﬁne-tune our models on the nine GLUE tasks over ﬁve\nepochs and a batch size of 64, where we implement early stopping. We also run automatic\nhyperparameter tuning for the ﬁne-tuning process using the Tree-structured Parzen Estimator\nalgorithm (Akiba et al., 2019). The learning rate is randomly selected logarithmically in\nthe [2 ×10−5, 5 ×10−4] range, and the batch size in {32,64,128}uniformly. Table 7\n(Table 8) shows the best hyperparameters for ﬁne-tuning of each GLUE (SuperGLUE) task\nselected using this auto-tuning technique. This hyperparameter optimization uses random\ninitialization every time, which results in variation in performance each time the model is\nqueried (see aleatoric uncertainty explained in Section 3.4).\nSince some tasks in the GLUE benchmark are very small, one expects a large deviation\nin performance as we change the training recipe. Table 7 also shows the deviation in\n61\nTuli, Dedhia, Tuli, & Jha\nTask Learning rate Batch size\nBoolQ 7 .5 ×10−5 64\nCB 2 .1 ×10−4 64\nCOPA 6 .3 ×10−5 32\nMultiRC 1 .0 ×10−4 128\nWiC 8 .6 ×10−5 128\nWSC 4 .4 ×10−4 64\nTable 8: Hyperparameters used for ﬁne-tuning FlexiBERT-Mini on the SuperGLUE tasks.\nperformance on GLUE tasks (as reported in Table 3). We see a large variation in smaller\ndatasets. We observe marginal deviation in performance in large datasets like MNLI, QNLI,\nand QQP. These deviations correspond to the saliency of aleatoric uncertainty on the training\nrecipe hyperparameters. Our hyperparameter tuning search method chooses the best training\nrecipe resulting in the highest performance.\nWe have included baselines trained with the pre-training + ﬁne-tuning procedure as\nproposed by Turc et al. (2019) for like-for-like comparisons, and not theknowledge distillation\ncounterparts (Xu et al., 2021). Nevertheless, FlexiBERT is orthogonal to (and thus can\neasily be combined with) knowledge distillation because FlexiBERT focuses on searching\nthe best architecture, while knowledge distillation focuses on better training of a given\narchitecture.\nAll models were trained on NVIDIA A100 GPUs and 2.6 GHz AMD EPYC Rome\nprocessors. The entire process of running BOSHNAS for all levels of the hierarchy took\naround 300 GPU-days of training.\nB.3 Knowledge Transfer\nRecent works leverage knowledge transfer. However, these methods are restricted to long\nshort-term memories and simple recurrent neural networks (Mazzawi et al., 2019). Wang\net al. (2020) train a super-transformer and share its weights with smaller models. However,\nthis is not feasible for diverse heterogeneous and ﬂexible architectures. We propose the use\nof knowledge transfer in transformers for the ﬁrst time, to the best of our knowledge, by\ncomparing weights with computational graphs of nearby models. Furthermore, previous\nworks only consider a static training recipe for all the models in the design space, an\nassumption we relax in our experiments. We directly ﬁne-tune models for which nearby\nmodels are already pre-trained. We test for this using the biased overlap metric deﬁned in\nSection 3.3. Figure 8 presents the time gains from knowledge transfer when we ﬁne-tune on\nall GLUE tasks. Since we can directly ﬁne-tune some percentage of models, thanks to their\nneighboring pre-trained models, we were able to speed up the overall training time by 38%.\nB.4 Crossover between Transformer Models\nWe obtain new transformer models of the subsequent level in the hierarchy by taking a\ncrossover between the best models in the previous level (which had layers per stack = s)\nand their neighbors. We choose the stack conﬁguration of the children from all unique\n62\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\n(a) (b) (c)\nFigure 8: Bar plot showing average time for training a transformer model (in GPU-hours)\nwith and without knowledge transfer. (a) Pre-train + Fine-tune: total training\ntime. (b) Direct ﬁne-tuning: training time for a pre-trained model. (c) Knowledge\nTransfer: training using weight transfer from a trained nearby model gives 38%\nspeedup. Plotted with 90% conﬁdence intervals.\nStack A\nStack B\nStack D\nStack C\nStack F\nStack E\nA    D\nB    C\nF\nE\nE\nF\nlayers\nper\nstack\nlayers\nper\nstack\nB    C\nB    C B    C\nA    D\nA    D\nA    D\nFigure 9: Crossover between two parent models yields a ﬁner-grained design space. Each\nstack conﬁguration in the children is derived from the product of the parent design\nchoices at the same depth.\n63\nTuli, Dedhia, Tuli, & Jha\nhyperparameter values present in the parent models at the same depth. We show a simple\nexample of this scheme in Figure 9. First, we compute the design space of permissible\noperation blocks for layers in the stack, s, by the product of the individual design choices\nof the parents for that stack. We then independently form these new layers with the new\nconstraint of s/2 layers having the same choice of hyperparameter values. Expanding\nthe design space in such a fashion retains the original hyperparameters that give good\nperformance while also exploring the internal representations learned by combinations of\nthe hyperparameters at the same level.\nB.5 Best-performing Models\nFrom diﬀerent hierarchy levels ( s = 2,1, and 1 ∗), we get the respective best-performing\nmodels after running BOSHNAS as follows:\ns= 2 :\n{\nl: 4,o : [LT,LT,LT,LT],h : [256,256,256,256],n : [4,4,2,2],\nf : [[1024],[1024],[512,512,512],[512,512,512]],p : [DCT,DCT,DCT,DCT]\n}\ns= 1 :\n{\nl: 4,o : [SA,SA,LT,LT],h : [256,256,128,128],n : [2,2,4,4],\nf : [[512,512,512],[512,512,512],[1024],[1024]],p : [SDP,SDP,DCT,DCT]\n}\nwhere, in the last leg of the hierarchy, the stack length is 1, but the feed-forward stacks\nare also heterogeneous (see Section 3.2). Both s = 1 and s = 1∗ gave the same solution\ndespite ﬁner granularity in the latter case. Thus, the second model card above is that of\nFlexiBERT-Mini.\nWe present the model cards of the FlexiBERT-Mini ablation models, as shown in Table 3,\nbelow:\n(w/o S.) :\n{\nl: 2,o : [SA,SA],h : [128,128],n : [4,4],f : [[1024],[1024]],p : [SDP,WMA]\n}\n(w/o H.) :\n{\nl: 4,o : [LT,LT,SA,SA],h : [256,256,128,128],n : [4,4,4,4],\nf : [[1024,1024,1024],[1024,1024,1024],[512,512,512],[512,512,512]],\np: [DCT,DCT,SDP,SDP]\n}\nFigure 10 shows a working schematic of the design choices in the FlexiBERT-Mini and\nFlexiBERT-Large models. As explained in Section 4.4, we obtain FlexiBERT-Large by extrap-\nolating the design choices in FlexiBERT-Mini to obtain a BERT-Large counterpart (Devlin\net al., 2019).\n64\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nSA-SDP SA-SDP\nAdd & Norm\nF-512\nAdd & Norm\nProjection\nL-DCT L-DCT\nAdd & Norm\nF-1024\nAdd & Norm\nProjection\nL-DCTL-DCT\nx3\nH-256\nH-128\nx2\nx2\nSA-SDP SA-SDP\nAdd & Norm\nF-2048\nAdd & Norm\nProjection\nL-DCT L-DCT\nAdd & Norm\nF-4096\nAdd & Norm\nProjection\nx3\nH-1024\nH-512\nx12\nx12\nx16\nx8\n(a) (b)\nFigure 10: Obtained FlexiBERT models after running the BOSHNAS pipeline: (a)\nFlexiBERT-Mini and its design choices extrapolated to obtain (b) FlexiBERT-\nLarge.\nReferences\nAbu-Aisheh, Z., Raveaux, R., Ramel, J.-Y., & Martineau, P. (2015). An exact graph edit\ndistance algorithm for solving pattern recognition problems. In Proceedings of the\nInternational Conference on Pattern Recognition Applications and Methods , Vol. 1, pp.\n271–278.\nAkiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A next-generation\nhyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining , pp. 2623–2631.\nChen, D., Li, Y., Qiu, M., Wang, Z., Li, B., Ding, B., Deng, H., Huang, J., Lin, W., &\nZhou, J. (2021). AdaBERT: Task-adaptive BERT compression with diﬀerentiable\nneural architecture search. In Proceedings of the 29th International Joint Conference\non Artiﬁcial Intelligence , pp. 2463–2469.\nCheng, H.-P., Zhang, T., Zhang, Y., Li, S., Liang, F., Yan, F., Li, M., Chandra, V., Li,\nH., & Chen, Y. (2021). NASGEM: Neural architecture search via graph embedding\n65\nTuli, Dedhia, Tuli, & Jha\nmethod. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , Vol. 35, pp.\n7090–7098.\nConneau, A., & Lample, G. (2019). Cross-lingual language model pretraining. In Advances\nin Neural Information Processing Systems , Vol. 32, pp. 7059–7069.\nDang, D.-C., Eremeev, A., & Lehre, P. K. (2021). Escaping local optima with non-elitist evo-\nlutionary algorithms. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nVol. 35, pp. 12275–12283.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Vol. 1, pp. 4171–4186.\nGal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model\nuncertainty in deep learning. In Proceedings of The 33rd International Conference on\nMachine Learning, Vol. 48, pp. 1050–1059.\nGao, J., Xu, H., Shi, H., Ren, X., Yu, P. L. H., Liang, X., Jiang, X., & Li, Z. (2022).\nAutoBERT-Zero: Evolving bert backbone from scratch. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , Vol. 36, pp. 10663–10671.\nGokaslan, A., & Cohen, V. (2019). OpenWebText corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nHe, X., Zhao, K., & Chu, X. (2021). AutoML: A survey of the state-of-the-art. Knowledge-\nBased Systems, 212, 106622.\nHou, L., Huang, Z., Shang, L., Jiang, X., Chen, X., & Liu, Q. (2020). DynaBERT: Dynamic\nBERT with adaptive width and depth. In Advances in Neural Information Processing\nSystems, Vol. 33, pp. 9782–9793.\nHuang, C.-Z. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., Dai, A. M.,\nHoﬀman, M. D., Dinculescu, M., & Eck, D. (2018). Music transformer: Generating\nmusic with long-term structure. In Proceedings of the International Conference on\nLearning Representations.\nJiang, Z.-H., Yu, W., Zhou, D., Chen, Y., Feng, J., & Yan, S. (2020). ConvBERT: Improving\nBERT with span-based dynamic convolution. In Advances in Neural Information\nProcessing Systems, Vol. 33, pp. 12837–12848.\nKhetan, A., & Karnin, Z. (2020). schuBERT: Optimizing elements of BERT. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics , pp.\n2807–2818.\nKruskal, J. B. (1964). Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric\nhypothesis. Psychometrika, 29 (1), 1–27.\nLee-Thorp, J., Ainslie, J., Eckstein, I., & Ontanon, S. (2022). FNet: Mixing tokens with\nFourier transforms. In Proceedings of the Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies , pp.\n4296–4313.\n66\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V.,\n& Zettlemoyer, L. (2020). BART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics , pp. 7871–7880.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L.,\n& Stoyanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach.\nCoRR, abs/1907.11692.\nLu, Z., Whalen, I., Boddeti, V., Dhebar, Y., Deb, K., Goodman, E., & Banzhaf, W. (2019).\nNSGA-Net: Neural architecture search using multi-objective genetic algorithm. In\nProceedings of the Genetic and Evolutionary Computation Conference , pp. 419–427.\nLuong, T., Pham, H., & Manning, C. D. (2015). Eﬀective approaches to attention-based\nneural machine translation. In Proceedings of the Conference on Empirical Methods in\nNatural Language Processing, pp. 1412–1421.\nMackenzie, J., Benham, R., Petri, M., Trippas, J. R., Culpepper, J. S., & Moﬀat, A.\n(2020). CC-News-En: A large English news corpus. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management , pp. 3077–3084.\nMazzawi, H., Gonzalvo, X., Kracun, A., Sridhar, P., Subrahmanya, N., Lopez-Moreno, I.,\nPark, H., & Violette, P. (2019). Improving keyword spotting and language identiﬁcation\nvia neural architecture search at scale. In Proceedings of Interspeech, pp. 1278–1282.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed\nrepresentations of words and phrases and their compositionality. In Advances in Neural\nInformation Processing Systems, Vol. 26, pp. 3111–3119.\nNarayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y., & Jaiswal, S. (2017).\ngraph2vec: Learning distributed representations of graphs. CoRR, abs/1707.05005.\nPennington, J., Socher, R., & Manning, C. (2014). GloVe: Global vectors for word represen-\ntation. In Proceedings of the Conference on Empirical Methods in Natural Language\nProcessing, pp. 1532–1543.\nPham, H., Guan, M., Zoph, B., Le, Q., & Dean, J. (2018). Eﬃcient neural architecture\nsearch via parameters sharing. In Proccedings of the 35th International Conference on\nMachine Learning, pp. 4095–4104.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &\nLiu, P. J. (2020). Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research , 21 (140), 1–67.\nReal, E., Aggarwal, A., Huang, Y., & Le, Q. V. (2019). Regularized evolution for image\nclassiﬁer architecture search. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, Vol. 33, pp. 4780–4789.\nRu, R., Esperanca, P., & Carlucci, F. M. (2020). Neural architecture generator optimization.\nIn Advances in Neural Information Processing Systems , Vol. 33, pp. 12057–12069.\nRussell, S., & Norvig, P. (2010). Artiﬁcial Intelligence: A Modern Approach (3rd edition).\nPrentice Hall.\n67\nTuli, Dedhia, Tuli, & Jha\nShaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position repre-\nsentations. In Proceedings of the Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies , Vol. 2, pp.\n464–468.\nShervashidze, N., Schweitzer, P., van Leeuwen, E. J., Mehlhorn, K., & Borgwardt, K. M.\n(2011). Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research ,\n12 (77), 2539–2561.\nSiems, J., Zimmer, L., Zela, A., Lukasik, J., Keuper, M., & Hutter, F. (2020). NAS-Bench-\n301 and the case for surrogate benchmarks for neural architecture search. CoRR,\nabs/2008.09777.\nSnoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine\nlearning algorithms. In Advances in Neural Information Processing Systems , Vol. 25,\npp. 2951–2959.\nSo, D., Ma´ nke, W., Liu, H., Dai, Z., Shazeer, N., & Le, Q. V. (2021). Searching for eﬃcient\ntransformers for language modeling. In Advances in Neural Information Processing\nSystems, Vol. 34, pp. 6010–6022.\nSo, D. R., Liang, C., & Le, Q. V. (2019). The evolved transformer. CoRR, abs/1901.11117.\nSong, K., Tan, X., Qin, T., Lu, J., & Liu, T.-Y. (2020). MPNet: Masked and permuted\npre-training for language understanding. In Advances in Neural Information Processing\nSystems, Vol. 33, pp. 16857–16867.\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y., & Zhou, D. (2020). MobileBERT: A compact\ntask-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pp. 2158–2170.\nTan, M., & Le, Q. (2019). EﬃcientNet: Rethinking model scaling for convolutional neural\nnetworks. In Proceedings of the 36th International Conference on Machine Learning ,\npp. 6105–6114.\nTuli, S., Casale, G., & Jennings, N. R. (2022). GOSH: Task scheduling using deep surrogate\nmodels in fog computing environments. IEEE Transactions on Parallel and Distributed\nSystems, 33 (11), 2821–2833.\nTuli, S., Poojara, S. R., Srirama, S. N., Casale, G., & Jennings, N. R. (2021). COSCO:\nContainer orchestration using co-simulation and gradient based optimization for fog\ncomputing environments. IEEE Transactions on Parallel and Distributed Systems ,\n33 (1), 101–116.\nTurc, I., Chang, M., Lee, K., & Toutanova, K. (2019). Well-read students learn better: The\nimpact of student initialization on knowledge distillation. CoRR, abs/1908.08962.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., &\nPolosukhin, I. (2017). Attention is all you need. In Advances in Neural Information\nProcessing Systems, Vol. 30, pp. 5998–6008.\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., &\nBowman, S. (2019). SuperGLUE: A stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Information Processing Systems, Vol. 32.\n68\nFlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. (2018). GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In\nProceedings of the EMNLP Workshop on BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP , pp. 353–355.\nWang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., & Han, S. (2020). HAT: Hardware-\naware transformers for eﬃcient natural language processing. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics , pp. 7675–7688.\nWang, H., Shi, X., & Yeung, D.-Y. (2016). Natural-parameter networks: A class of probabilis-\ntic neural networks. In Advances in Neural Information Processing Systems , Vol. 29,\npp. 118–126.\nWhite, C., Neiswanger, W., & Savani, Y. (2021a). BANANAS: Bayesian optimization\nwith neural architectures for neural architecture search. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , Vol. 35, pp. 10293–10301.\nWhite, C., Zela, A., Ru, B., Liu, Y., & Hutter, F. (2021b). How powerful are performance\npredictors in neural architecture search?. In Advances in Neural Information Processing\nSystems.\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist\nreinforcement learning. Machine Learning, 8 (3–4), 229–256.\nXu, J., Tan, X., Luo, R., Song, K., Li, J., Qin, T., & Liu, T.-Y. (2021). NAS-BERT:\nTask-agnostic and adaptive-size BERT compression with neural architecture search.\nIn Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining, pp. 1933–1943.\nXu, K., Hu, W., Leskovec, J., & Jegelka, S. (2019). How powerful are graph neural networks?.\nIn Proceedings of the International Conference on Learning Representations .\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). XLNet:\nGeneralized autoregressive pretraining for language understanding. In Advances in\nNeural Information Processing Systems, Vol. 32.\nYao, Z., Gholami, A., Shen, S., Mustafa, M., Keutzer, K., & Mahoney, M. (2021). ADA-\nHESSIAN: An adaptive second order optimizer for machine learning. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence , Vol. 35, pp. 10665–10673.\nYin, Y., Chen, C., Shang, L., Jiang, X., Chen, X., & Liu, Q. (2021). AutoTinyBERT:\nAutomatic hyper-parameter optimization for eﬃcient pre-trained language models. In\nProceedings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language Processing ,\nVol. 1, pp. 5146–5157.\nYing, C., Klein, A., Christiansen, E., Real, E., Murphy, K., & Hutter, F. (2019). NAS-\nBench-101: Towards reproducible neural architecture search. In Proceedings of the\n36th International Conference on Machine Learning , Vol. 97, pp. 7105–7114.\nYu, Y., & Jha, N. K. (2022). SPRING: A sparsity-aware reduced-precision monolithic\n3D CNN accelerator architecture for training and inference. IEEE Transactions on\nEmerging Topics in Computing , 10 (1), 237–249.\n69\nTuli, Dedhia, Tuli, & Jha\nZhang, X., Zhou, X., Lin, M., & Sun, J. (2018). ShuﬄeNet: An extremely eﬃcient convolu-\ntional neural network for mobile devices. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pp. 6848–6856.\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.\n(2015). Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE International Conference on\nComputer Vision, pp. 19–27.\nZoph, B., & Le, Q. (2017). Neural architecture search with reinforcement learning. In\nInternational Conference on Learning Representations.\nZoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018). Learning transferable architectures\nfor scalable image recognition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 8697–8710.\n70",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7533441781997681
    },
    {
      "name": "Hyperparameter",
      "score": 0.6473160982131958
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6250879764556885
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.556527853012085
    },
    {
      "name": "Machine learning",
      "score": 0.49749329686164856
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4629318118095398
    },
    {
      "name": "Embedding",
      "score": 0.4476008415222168
    },
    {
      "name": "Surrogate model",
      "score": 0.4123838543891907
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    }
  ],
  "cited_by": 9
}