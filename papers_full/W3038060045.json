{
    "title": "On the comparability of pre-trained language models",
    "url": "https://openalex.org/W3038060045",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5069469652",
            "name": "Matthias AÃŸenmacher",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5012747021",
            "name": "Christian Heumann",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2806120502",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2949433733",
        "https://openalex.org/W2793353489",
        "https://openalex.org/W131533222",
        "https://openalex.org/W4919037",
        "https://openalex.org/W2989499211",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2740721704",
        "https://openalex.org/W2086161653"
    ],
    "abstract": "Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.",
    "full_text": null
}