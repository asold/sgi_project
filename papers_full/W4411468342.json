{
    "title": "Performance analysis of large language models Chatgpt-4o, OpenAI O1, and OpenAI O3 mini in clinical treatment of pneumonia: a comparative study",
    "url": "https://openalex.org/W4411468342",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5006970522",
            "name": "Zhiwu Lin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100384444",
            "name": "Yuanyuan Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5084693085",
            "name": "Min Wu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100376999",
            "name": "Hongmei Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5056116087",
            "name": "Xinyuan Song",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5101704336",
            "name": "Qian Yu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5062480940",
            "name": "Gui Xiao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5060607239",
            "name": "Jipan Xie",
            "affiliations": [
                "Mianyang Central Hospital",
                "University of Electronic Science and Technology of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385620388",
        "https://openalex.org/W4395052272",
        "https://openalex.org/W4403813762",
        "https://openalex.org/W4376866715",
        "https://openalex.org/W4367186868",
        "https://openalex.org/W4283593734",
        "https://openalex.org/W4213162720",
        "https://openalex.org/W278912771",
        "https://openalex.org/W4388424922",
        "https://openalex.org/W4387331030",
        "https://openalex.org/W4280652908",
        "https://openalex.org/W4406421457",
        "https://openalex.org/W4406141219",
        "https://openalex.org/W4402582321",
        "https://openalex.org/W4401820448",
        "https://openalex.org/W4318069287",
        "https://openalex.org/W3158982765",
        "https://openalex.org/W4389165057",
        "https://openalex.org/W3155832727",
        "https://openalex.org/W3155025797",
        "https://openalex.org/W3129768930",
        "https://openalex.org/W4310568840",
        "https://openalex.org/W4399656810",
        "https://openalex.org/W4404866765",
        "https://openalex.org/W4380090363",
        "https://openalex.org/W4405574034",
        "https://openalex.org/W4229007786",
        "https://openalex.org/W4386293164",
        "https://openalex.org/W4281384654",
        "https://openalex.org/W4200325999",
        "https://openalex.org/W3097031598",
        "https://openalex.org/W4381730539",
        "https://openalex.org/W3088277513",
        "https://openalex.org/W3023620462",
        "https://openalex.org/W4310074180",
        "https://openalex.org/W4296619163",
        "https://openalex.org/W3177810550",
        "https://openalex.org/W4390542087",
        "https://openalex.org/W4403771222",
        "https://openalex.org/W4401413536",
        "https://openalex.org/W4220660066",
        "https://openalex.org/W4391270800",
        "https://openalex.org/W4221012439",
        "https://openalex.org/W4404654936",
        "https://openalex.org/W4229064018",
        "https://openalex.org/W4403024753",
        "https://openalex.org/W4401381646",
        "https://openalex.org/W4226064793"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)\nClinical and Experimental Medicine          (2025) 25:213  \nhttps://doi.org/10.1007/s10238-025-01743-7\nRESEARCH\nPerformance analysis of large language models Chatgpt‑4o, OpenAI \nO1, and OpenAI O3 mini in clinical treatment of pneumonia: \na comparative study\nZhiwu Lin1 · Yuanyuan Li1 · Min Wu1 · Hongmei Liu1 · Xiaoyang Song1 · Qian Yu1 · Guibao Xiao2 · Jiajun Xie3\nReceived: 26 March 2025 / Accepted: 21 May 2025 \n© The Author(s) 2025\nAbstract\nThis study aimed to compare the performance of three large language models (ChatGPT-4o, OpenAI O1, and OpenAI O3 \nmini) in delivering accurate and guideline compliant recommendations for pneumonia management. By assessing both gen-\neral and guideline-focused questions, the investigation sought to elucidate each model’s strengths, limitations, and capacity \nto self-correct in response to expert feedback. Fifty pneumonia-related questions (30 general, 20 guideline-based) were posed \nto the three models. Ten infectious disease specialists independently scored responses for accuracy using a 5-point scale. \nThe two chain-of-thought models (OpenAI O1 and OpenAI O3 mini) were further tested for self-correction when initially \nrated “poor,” with re-evaluations conducted one week later to reduce recall bias. Statistical analyses included nonparamet-\nric tests, ANOVA, and Fleiss’ Kappa for inter-rater reliability. OpenAI O1 achieved the highest overall accuracy, followed \nby OpenAI O3 mini; ChatGPT-4o scored lowest. For “poor” responses, O1 and O3 mini both significantly improved after \ntargeted prompts, reflecting the advantages of chain-of-thought reasoning. ChatGPT-4o demonstrated limited gains upon \nre-prompting and provided more concise, but sometimes incomplete, information. OpenAI O1 and O3 mini offered supe-\nrior guideline-aligned recommendations and benefited from self-correction capabilities, while ChatGPT-4o’s direct-answer \napproach led to moderate or poor outcomes for complex pneumonia queries. Incorporating chain-of-thought mechanisms \nappears critical for refining clinical guidance. These findings suggest that advanced large language models can support \npneumonia management by providing accurate, up-to-date information, particularly when equipped to iteratively refine their \noutputs in response to expert feedback.  \nKeywords Pneumonia management · Clinical decision support · Large language models · AI-driven healthcare · Clinical \naccuracy\nIntroduction\nIn an era marked by rapid advancements in artificial intel-\nligence (AI) and heightened interest in data-driven solu-\ntions for healthcare, sound evidence on the performance of \nlarge language models (LLMs) in clinical practice is cru-\ncial [1 –5]. Pneumonia continues to be a leading cause of \nmorbidity worldwide, spanning a wide range of ages and \nclinical contexts. Although improved vaccination strategies \nand antimicrobial treatments have contributed to positive \noutcomes in some regions, ensuring appropriate, guideline-\naligned care for pneumonia remains an ongoing challenge \n[6–11]. The increasing complexity of medical information, \ncoupled with evolving recommendations on empirical antibi-\notic regimens, underscores the need for robust tools that can \nsynthesize and interpret extensive clinical data in real time. \n * Guibao Xiao \n 1250374226@qq.com\n * Jiajun Xie \n xiejiagao123@163.com\n1 Department of Thoracic Surgery, Ziyang Central Hospital, \nZiyang 641300, China\n2 Department of Infectious Diseases, Ziyang Central Hospital, \nZiyang 641300, China\n3 Department of Respiratory and Critical Care Medicine, \nMianyang Central Hospital, School of Medicine, University \nof Electronic Science and Technology of China, Mianyang, \nChina\n Clinical and Experimental Medicine          (2025) 25:213 \n  213  Page 2 of 10\nResearch focusing on LLMs for clinical decision support—\nparticularly in conditions like pneumonia, which require \ncareful diagnostic evaluation and management—has prolif-\nerated in recent years. These models operate by processing \nhuman language inputs and generating outputs that emulate \nexpert-driven reasoning, often drawing on vast repositories \nof medical literature [12–16]. However, important questions \nremain about their capability to provide evidence-based rec-\nommendations, accurately interpret clinical guidelines, and \nadapt their outputs following targeted feedback from human \nspecialists. Early successes in automated content generation \nhave highlighted the promise of using AI to deliver point-of-\ncare guidance, yet variability in performance across different \nmodel architectures persists [17–22]. Against this backdrop, \nthree large language models—ChatGPT-4o, OpenAI O1, and \nOpenAI O3 mini—have emerged as influential players in \nthe clinical AI landscape. ChatGPT-4o is characterized by a \ndirect-answer architecture, delivering concise responses that \nmay expedite simple information retrieval but sometimes \nlack the structured reasoning steps that might be essential \nin complex medical decisions. By contrast, OpenAI O1 \nemploys a deliberative chain-of-thought module that first \nproduces a structured reasoning draft and then generates \nthe final answer, whereas OpenAI O3 mini uses a lighter, \nsampling-based chain-of-thought that selects from several \nshort reasoning paths before response generation. Both \nmodels therefore expose intermediate logic, but O1’s deeper \nreasoning stack typically yields longer and more granular \nexplanations, while O3 mini sacrifices some depth for speed. \n[23–30]. Proponents of chain-of-thought architectures argue \nthat these systems have a heightened capacity to self-correct, \noffering an iterative approach to resolving inaccuracies or \nincomplete clinical justifications.  \nAlthough such architectural distinctions are theoretically \nrelevant, few studies have investigated how they affect LLM \nperformance on real-world medical questions, especially \nthose requiring alignment with established clinical guide-\nlines. A particular gap exists concerning pneumonia, a con-\ndition in which initial missteps—such as prescribing overly \nbroad-spectrum antibiotics or overlooking pathogen-specific \nrisk factors—can have significant implications for patient \noutcomes and antimicrobial resistance [31– 34]. The guide-\nlines for pneumonia management reflect updated knowledge \non emerging pathogens, targeted therapies, and biomarker-\nbased decision-making strategies, offering an ideal test case \nfor evaluating the adaptability and specificity of LLM out-\nputs. Moreover, the ability of AI tools to evolve and integrate \nnew data is an area of active exploration. Some LLMs may \nsuccessfully assimilate novel recommendations into their \noutputs when prompted, while others may rely too heavily \non their training parameters and thus remain static. In the \ncontext of pneumonia—where local resistance patterns and \ncomorbid conditions often inform therapeutic choices—the \ncapacity for iterative correction is not merely a technical \nadvantage; it can fundamentally alter patient care.\nIn this study, we present a comparative analysis of Chat-\nGPT-4o, OpenAI O1, and OpenAI O3 mini in addressing \npneumonia-related queries. We specifically examine how \nthese models perform on questions derived both from \ngeneral clinical practice and from the guidelines. We then \nevaluate their capacity to self-correct after expert feedback, \nemphasizing the potential of chain-of-thought reasoning in \naligning AI-driven outputs with shifting evidence bases. \nBy illuminating differences in accuracy, completeness, and \nresponsiveness to critique, our aim is to inform clinicians, \nresearchers, and policymakers about the promise and pitfalls \nof integrating advanced AI systems into the complex domain \nof pneumonia management.\nMethods\nStudy design\nA total of 30 frequently asked questions related to pneumo-\nnia (see Supplementary Table 1) were curated from reputa-\nble online medical resources, including the Infectious Dis-\neases Society of America (IDSA), the American Thoracic \nSociety, and various peer-reviewed articles [35– 39]. The \nchosen questions addressed clinical presentation, patho -\ngenesis, diagnostic methods, treatment strategies, preven -\ntion approaches, and risk factors for pneumonia. Candidate \nitems were generated through a three-round modified Delphi \nexercise: Two infectious disease faculty drafted an initial \npool of 72 questions from guideline documents and recent \nreviews, the full panel of ten specialists independently rated \nclinical relevance, and items scoring ≥ 4·0 on a 5-point rel-\nevance scale in the final round ( n = 50) were retained for \nthe study. To further evaluate the large language models’ \nability to incorporate guideline-specific content, an addi-\ntional 20 questions were generated based on the Guidelines \nfor the Management of Pneumonia (see Supplementary \nTable 2) [40–47]. Where management diverged for children, \npediatric-specific recommendations from the 2023 PIDS/\nIDSA guideline were incorporated to ensure age-appropriate \ncontent for the five pediatric questions. Answers to these \nquestion prompts were collected by querying three large \nlanguage models (LLMs): ChatGPT-4o, OpenAI O1, and \nOpenAI O3 mini. To maintain consistency, each question \nwas posed in a separate, reset conversation. The resulting \nresponses were extracted as plain text. Any references to the \nspecific model used or other model identifiers were redacted. \nTen infectious disease specialists, each with over 5 years \nof clinical experience in pneumonia management, were \nenlisted to independently rate the anonymized responses. \nBefore formal scoring, the panel participated in a 30 min \nClinical and Experimental Medicine          (2025) 25:213  \n Page 3 of 10   213 \nvirtual calibration session in which two sample answers were \ndiscussed to align expectations regarding accuracy, com -\npleteness, and guideline citation.\nAccuracy assessment\nTen specialists evaluated the accuracy of each response \nusing a 5-point scale, where 1 indicated that the response \nwas completely incorrect and 5 indicated that the response \nwas fully accurate with thorough elaboration. Each ques-\ntion thus had a maximum possible total score of 50 (5 \npoints × 10 evaluators). Scores were categorized a-priori as \n‘poor’ (< 26), ‘moderate’ (26–38), and ‘excellent’ (> 38); \nthese breakpoints correspond to the lower (≤ 50%) and upper \n(≥ 75%) quartiles of the maximum attainable score in a pre-\nliminary pilot (n = 10 questions, 5 raters) and mirror cut-offs \nused in prior LLM-evaluation studies for clinical QA tasks. \nThe scale was anchored as follows: 1 = completely incorrect \nor misleading; 2 = partly correct but missing key clinical \nconcepts; 3 = largely correct yet lacking important guideline \ndetails; 4 = correct and sufficiently detailed for routine clini-\ncal use; 5 = fully correct, guideline-aligned, and thoroughly \nelaborated. All specialists were affiliated with tertiary hos-\npitals in Sichuan Province, China. Their routine practice \nfocuses predominantly on adult inpatients; therefore, scoring \nnorms may partly reflect regional antimicrobial stewardship \npriorities and adult-oriented guideline familiarity.\nRe‑evaluating the accuracy of self‑correcting \nchain‑of‑thought\nThe two LLMs with chain-of-thought capabilities (OpenAI \nO1 and OpenAI O3 mini) were examined for their abil-\nity to self-correct in instances where they initially scored \nin the “poor” range. For each response that fell below the \nTS threshold of 26, an infectious disease specialist high-\nlighted the incorrect or incomplete segments of text within \nthe answer and prompted the LLM again using a follow-up \nstatement: “This information seems inaccurate. Could you \nre-evaluate and correct your response?” The newly gener -\nated responses were collected, de-identified, randomized, \nand then re-rated by the same ten specialists. During both \nrounds, raters evaluated not only factual accuracy but also \nthe degree to which corrected answers introduced substan-\ntive new information; responses that merely re-phrased \nearlier content without addressing flagged omissions were \ndeemed insufficiently improved. The second round of \nassessment was conducted one week after the initial rating \nto reduce potential recall bias, and evaluators were blinded \nto whether the text was the original or corrected version. \nTo achieve full blinding, all answer blocks were re-labeled \nwith new anonymous IDs by an independent coordinator \nnot involved in scoring, and the presentation order was \nre-randomized with a computer-generated block sequence \n(size = 10), so that raters never saw the same question in the \nsame position or pairing. No evaluator was exposed to his or \nher own previous markings during the second round.\nStatistical analysis\nData were processed using SPSS version 29. For mul-\ntiple comparisons of nonnormally distributed data, the \nKruskal–Wallis H test was employed to detect any sig-\nnificant differences among ChatGPT-4o, OpenAI O1, and \nOpenAI O3 mini. If a distribution appeared normal, an \nindependent sample t test or one-way ANOVA was used, as \nappropriate. Post hoc tests were then performed for pairwise \ncomparisons. Fleiss’ Kappa coefficients were calculated to \nevaluate the inter-rater reliability among the ten specialists, \nwith values closer to 1.0 indicating higher consistency. A p \nvalue less than 0.05 denoted statistical significance through-\nout all analyses.\nResults\nResponse length analysis\nLength of responses to general pneumonia questions\nThe word and character counts of responses for the 30 \ngeneral pneumonia questions are summarized in Table  1. \nAcross these questions, OpenAI O1 consistently gener -\nated the longest replies, whereas ChatGPT-4o produced \nthe fewest words on average. OpenAI O3 mini’s responses \nwere slightly shorter than those from O1 but generally \ncontained more words than those from ChatGPT-4o.As \nshown in Table  1, OpenAI O1’s mean word count ± SD \nwas 324.67 ± 47.91, compared to 302.13 ± 45.20 for Ope-\nnAI O3 mini and 277.60 ± 42.62 for ChatGPT-4o. Char -\nacter counts followed a similar pattern, with O1 produc-\ning the greatest number of characters (2304.10 ± 366.53), \nO3 mini second (2131.27 ± 318.02), and ChatGPT-4o last \n(1967.43 ± 292.80). Table 1 also shows that OpenAI O1’s \nresponses tended to be more expansive, often including sup-\nplementary explanations. In contrast, ChatGPT-4o’s answers \nTable 1  Length of LLM responses (words and characters) for the 30 \ngeneral pneumonia questions\nModel Word Count (Mean ± SD) Character Count \n(Mean ± SD)\nChatGPT-4o 277.60 ± 42.62 1967.43 ± 292.80\nOpenAI O1 324.67 ± 47.91 2304.10 ± 366.53\nOpenAI O3 mini 302.13 ± 45.20 2131.27 ± 318.02\n Clinical and Experimental Medicine          (2025) 25:213 \n  213  Page 4 of 10\nwere concise but occasionally omitted detailed clinical jus-\ntifications, as noted by several specialists.\nLength of responses to guideline‑based questions\nFor the 20 guideline-related questions, a similar trend \nemerged (Table  2). OpenAI O1 generated the most com-\nprehensive responses, with a median word count of \n349.00 (interquartile range [IQR] 318.25–381.50), while \nOpenAI O3 mini’s median word count was 330.00 (IQR \n298.75–362.50). ChatGPT-4o’s median was 298.50 (IQR \n262.75–323.25). Character count results, shown in Table 2, \ncorroborated these findings. Specialists commented that O1 \noffered nuanced discussions reflecting guideline recommen-\ndations in more depth, whereas ChatGPT-4o tended to focus \non key points without extensive elaboration.\nAccuracy and total score assessment\nPerformance on general pneumonia questions\nThe TS distribution for the 30 general questions is presented \nin Table 3. When considering all queries combined, OpenAI \nO1 achieved the highest overall median TS, followed closely \nby OpenAI O3 mini. ChatGPT-4o yielded the lowest median \nTS. To understand performance by specific topic areas—\nnamely clinical presentation, pathogenesis, diagnosis, \ntreatment, prevention, and risk factors—we calculated TSs \nfor each model in each category. As illustrated in Table  3, \nOpenAI O1 consistently scored at or near 40 points (i.e., in \nthe “excellent” range) in most categories, with its highest \nmedian TS recorded in the section on “clinical presenta-\ntion” (41.00 [IQR 39.25–44.00]). OpenAI O3 mini displayed \nstrong but slightly lower scores, ranging predominantly in \nthe 35–40 span, indicating the upper tier of “moderate” or \nlower end of “excellent.” ChatGPT-4o frequently fell into \nthe “moderate” range for most categories, with several top-\nics—particularly “pathogenesis” and “prevention”—show -\ning multiple “poor” individual question scores.\nTable  4 details the percentage breakdown of “poor,” \n“moderate,” and “excellent” responses for the 30 general \npneumonia questions. O1 had the fewest “poor” answers (2 \nout of 30, 7%), whereas O3 mini had 4 (13%), and Chat-\nGPT-4o had 6 (20%). The inter-rater reliability for general \nquestion scoring, as measured by Fleiss’ Kappa, was 0.52 \nacross all items, signifying moderate agreement among the \n10 raters.\nPerformance on guideline‑based questions\nComparisons for the 20 questions drawn from the Guide-\nlines are summarized in Table  5. OpenAI O1 once again \nTable 2  Length of LLM \nresponses (words and \ncharacters) for the 20 guideline-\nbased questions\nModel Word Count, Median (IQR) Character Count, Median (IQR)\nChatGPT-4o 298.50 (262.75–323.25) 2035.50 (1821.00–2214.50)\nOpenAI O1 349.00 (318.25–381.50) 2522.50 (2309.75–2744.25)\nOpenAI O3 mini 330.00 (298.75–362.50) 2374.00 (2188.50–2596.75)\nTable 3  Distribution of total scores (TS) across general pneumonia topics (30 questions)\nTopic ChatGPT-4o, Median (IQR) OpenAI O1, Median (IQR) OpenAI O3 mini, Median (IQR)\nClinical Presentation 30.00 (27.00–34.00) 41.00 (39.25–44.00) 38.00 (34.50–41.75)\nPathogenesis 27.00 (23.00–30.00) 40.00 (36.00–42.00) 36.00 (32.00–38.00)\nDiagnosis 31.00 (26.25–34.75) 42.00 (40.00–45.00) 37.00 (35.00–40.00)\nTreatment 32.00 (29.50–34.00) 43.00 (39.00–44.00) 38.00 (35.25–41.25)\nPrevention 25.00 (22.50–28.00) 39.00 (35.75–41.75) 35.00 (32.00–37.00)\nRisk Factors 28.00 (25.00–30.75) 40.00 (37.50–42.50) 37.00 (34.00–39.00)\nTable 4  Rating distribution of “poor,” “moderate,” and “excellent” \nfor the 30 general pneumonia questions\nModel Poor (TS < 26) Moderate \n(TS 26–38)\nExcellent (TS > 38)\nChatGPT-4o 6 (20%) 20 (67%) 4 (13%)\nOpenAI O1 2 (7%) 10 (33%) 18 (60%)\nOpenAI O3 mini 4 (13%) 16 (53%) 10 (34%)\nTable 5  Total scores (TS) on guideline-based questions (20 items)\nModel Mean ± SD Range\nChatGPT-4o 31.96 ± 7.40 18–39\nOpenAI O1 40.85 ± 5.17 28–48\nOpenAI O3 mini 37.12 ± 6.25 24–45\nClinical and Experimental Medicine          (2025) 25:213  \n Page 5 of 10   213 \nexhibited the strongest performance, with a mean ± SD TS of \n40.85 ± 5.17, which comfortably placed most of its responses \nin the “excellent” category. OpenAI O3 mini ranked sec-\nond, showing an average TS of 37.12 ± 6.25. ChatGPT-4o \nremained in last place, with a mean TS of 31.96 ± 7.40. \nQualitative analysis also indicated that O1 and O3 mini more \nreliably integrated guideline-specific details, such as recom-\nmended antibiotic regimens for typical and atypical pneumo-\nnia pathogens, vaccination schedules, and risk stratification \ncriteria. ChatGPT-4o tended to present general standard-of-\ncare guidance without consistently mapping key points back \nto the Guidelines. Table  6 shows the rating distribution for \nthe 20 guideline-related questions. OpenAI O1 produced \n14 “excellent” answers (70%), O3 mini registered 9 “excel-\nlent” answers (45%), and ChatGPT-4o achieved 5 “excel-\nlent” answers (25%). The proportion of “poor” responses \nwas higher for ChatGPT-4o (40%) compared to 25% for O3 \nmini and 15% for O1.\nRe‑evaluating self‑correction capacities\nIdentification of “poor” initial responses\nA combined total of 14 “poor” responses emerged across all \n50 questions for ChatGPT-4o, while O3 mini had 9 and O1 \nhad 5 (Tables  4 and 6). OpenAI O1 and O3 mini are both \nendowed with chain-of-thought reasoning, enabling them \nto self-correct when inaccuracies or omissions are pointed \nout. ChatGPT-4o lacks this chain-of-thought mechanism, \nbut we nevertheless prompted it to provide revised answers \nfor completeness.\nSelf‑correction outcome for OpenAI O1\nAs shown in Table 7, all five “poor” O1 responses were re-\nrated after self-correction prompts. The average initial TS \nfor these five items was 22.60 ± 1.67, indicating the “poor” \nrange. After re-evaluation, four answers rose to “excellent” \n(TS > 38), while one improved to “moderate” (TS = 35). The \nmean post-correction TS was 39.00 ± 2.83. This improve-\nment was statistically significant (p < 0.01). Specialists noted \nthat O1 effectively targeted the inaccuracies identified in \neach “poor” response, often adding relevant guideline details \nor clarifying ambiguous statements.\nSelf‑correction outcome for OpenAI O3 mini\nOpenAI O3 mini demonstrated similar correction strength, \nas presented in Table 8. Its nine “poor” answers had an aver-\nage TS of 23.33 ± 2.29 initially; after self-correction, five \nrose to the “excellent” category (TS ≥ 39), while four were \ndeemed “moderate” (TS 26–38). The post-correction mean \nTS was 37.78 ± 3.41, representing a significant improvement \n(p < 0.01). In many instances, O3 mini’s corrections intro-\nduced more specific references to the pneumonia guidelines \nor offered more precise dosing details, mimicking the chain-\nof-thought approach used by O1.\nSelf‑correction outcome for ChatGPT‑4o\nUnlike O1 and O3 mini, ChatGPT-4o showed minimal \nimprovement upon re-prompting, which was expected given \nthe absence of a robust chain-of-thought mechanism. In \nTable 9, the average initial TS for ChatGPT-4o’s 14 “poor” \nresponses was 22.21 ± 2.12. After attempting self-correction, \nthe mean TS rose slightly to 24.14 ± 3.55, but 9 of the 14 \nanswers still remained under 26, preserving their “poor” sta-\ntus. No corrected responses exceeded a TS of 31. Although \nsome minor factual errors were addressed, the specialists \nnoted that ChatGPT-4o often repeated the same generalities \nTable 6  Rating distribution for the 20 guideline-based questions\nModel Poor (TS < 26) Moderate \n(TS 26–38)\nExcellent (TS > 38)\nChatGPT-4o 8 (40%) 7 (35%) 5 (25%)\nOpenAI O1 3 (15%) 3 (15%) 14 (70%)\nOpenAI O3 mini 5 (25%) 6 (30%) 9 (45%)\nTable 7  Self-correction outcomes for OpenAI O1 (n = 5 poor \nresponses)\nQuestion ID Initial TS Corrected TS Rating Change\n#4-Gen 25 38 Poor → Excellent\n#11-Gen 24 40 Poor → Excellent\n#6-Guideline 23 35 Poor → Moderate\n#10-Guideline 22 39 Poor → Excellent\n#18-Guideline 19 42 Poor → Excellent\nMean 22.60 ± 1.67 39.00 ± 2.83 p < 0.01 (paired t)\nTable 8  Self-correction outcomes for OpenAI O3 mini (n = 9 poor \nresponses)\nQuestion ID Initial TS Corrected TS Rating Change\n#3-Gen 24 40 Poor → Excellent\n#5-Gen 21 32 Poor → Moderate\n#9-Gen 25 39 Poor → Excellent\n#13-Gen 22 38 Poor → Excellent\n#2-Guideline 23 42 Poor → Excellent\n#7-Guideline 25 37 Poor → Moderate\n#8-Guideline 24 40 Poor → Excellent\n#14-Guideline 22 35 Poor → Moderate\n#20-Guideline 23 39 Poor → Excellent\nMean 23.33 ± 2.29 37.78 ± 3.41 p < 0.01 (paired t)\n Clinical and Experimental Medicine          (2025) 25:213 \n  213  Page 6 of 10\nor simply re-phrased the initial text, resulting in limited qual-\nitative enhancement. In particular, answers that originally \nlacked dosage details, local resistance caveats, or guideline \ncitations showed little progress, whereas items judged ‘poor’ \nonly for isolated factual slips tended to reach the ‘moderate’ \nband after correction. The model’s brevity bias appears to \nconstrain its ability to insert entirely new guideline elements \nduring revision.\nSummary of comparative performance\nCollectively, these findings highlight the superior perfor -\nmance of OpenAI O1, which not only scored highest on both \ngeneral and guideline-based pneumonia questions but also \ndemonstrated the most robust self-correction capabilities. \nOpenAI O3 mini was a close second, displaying commenda-\nble accuracy and noteworthy improvements upon re-prompt-\ning. ChatGPT-4o consistently scored lowest, exhibiting a \nhigher rate of “poor” answers and showing minimal gains \nafter attempted correction. Despite the overall differences in \nresponse quality, all three models provided generally coher-\nent content when addressing standard pneumonia manage -\nment queries. However, in specialized areas requiring close \nadherence to updated guidelines or detailed diagnostic algo-\nrithms, ChatGPT-4o frequently lacked accuracy, whereas O1 \nand O3 mini leveraged their chain-of-thought methodology \nto produce more precise and guideline-aligned information. \nBy reflecting on table-by-table comparisons (Tables 1, 2, \n3, 4, 5, 6, 7, ,8 and 9), it is evident that O1 systematically \ndelivers the highest level of detail, O3 mini offers slightly \nless elaboration though still robust, and ChatGPT-4o exhib-\nits the most concise yet often incomplete coverage of clinical \nrecommendations. The self-correction analysis underscores \nthe advantages of integrating chain-of-thought reasoning, \nenabling iterative refinement for the O1 and O3 mini models.\nDiscussion\nPneumonia persists as a considerable source of morbidity \nand mortality across diverse populations, necessitating reli-\nable and evidence-based guidance for its management. In the \npresent study, we compared three large language models—\nChatGPT-4o, OpenAI O1, and OpenAI O3 mini—across a \nseries of clinically relevant questions, ranging from common \npresentations and pathophysiology to specific guideline-\nfocused recommendations. Our findings confirm that all \nthree models are capable of delivering coherent overviews \non pneumonia management; however, important differences \nemerge in the depth of evidence, alignment with up-to-date \nguidelines, and capacity for iterative correction. A clear out-\ncome of this investigation is the superior performance of \nOpenAI O1, which consistently demonstrated high levels of \naccuracy, detailed discussion of guideline recommendations, \nand robust self-correction following expert feedback. Nota-\nbly, ChatGPT-4o produced more succinct statements but \noften omitted crucial elements of pneumonia care, includ-\ning considerations of local antibiotic resistance patterns or \nindications for advanced diagnostic imaging in complex \npresentations. OpenAI O1 and OpenAI O3 mini generally \nconveyed these updated strategies more completely than \nChatGPT-4o, highlighting that chain-of-thought reasoning \ncan be pivotal when models are prompted to refer back to \nspecific guideline content. While both models share a chain-\nof-thought paradigm, O1 is fine-tuned on a larger, clinician-\nannotated corpus (~ 8 B tokens) and uses a two-pass delib-\neration layer; O3 mini is distilled from O1 with only ~ 2 B \ntokens and a single-pass reasoning sampler. These training-\nset and decoder differences plausibly account for O1’s con-\nsistently richer guideline citations and dosing specifics. The \nchain-of-thought frameworks in O1 and O3 mini effectively \nnavigated these demands, whereas ChatGPT-4o tended to \nrepeat general statements without incorporating the details \nclinicians might need for complex decision-making.\nA particularly illustrative facet of this analysis involves \nthe self-correction mechanism. In the study design, we inten-\ntionally flagged portions of text that specialists deemed inac-\ncurate or incomplete, prompting the two chain-of-thought \nmodels (O1 and O3 mini) and ChatGPT-4o to revise their \nresponses. The capacity for iterative refinement is essential \nin real-life clinical contexts, where a medical professional \nmight flag errors or request clarifications based on newly \nTable 9  Self-correction outcomes for ChatGPT-4o (n = 14 poor \nresponses)\nQuestion ID Initial TS Corrected TS Rating Change\n#2-Gen 25 27 Poor → Moderate \n(border)\n#6-Gen 24 24 Poor → Poor (no \nchange)\n#12-Gen 22 23 Poor → Poor\n#15-Gen 20 22 Poor → Poor\n#21-Gen 23 26 Poor → Moderate \n(barely)\n#27-Gen 20 25 Poor → Poor\n#4-Guideline 19 23 Poor → Poor\n#9-Guideline 24 28 Poor → Moderate\n#11-Guideline 25 29 Poor → Moderate\n#13-Guideline 21 23 Poor → Poor\n#16-Guideline 22 24 Poor → Poor\n#17-Guideline 20 22 Poor → Poor\n#18-Guideline 24 28 Poor → Moderate\n#19-Guideline 22 24 Poor → Poor\nMean 22.21 ± 2.12 24.14 ± 3.55 p = 0.081 (paired t)\nClinical and Experimental Medicine          (2025) 25:213  \n Page 7 of 10   213 \navailable diagnostic findings or emerging guideline revi-\nsions. OpenAI O1 repeatedly showcased an ability to trans-\nform “poor” initial answers into “moderate” or “excellent” \nfinal outputs, primarily by incorporating omitted guideline \ndetails or clarifying ambiguous statements about pneumo-\nnia management. OpenAI O3 mini followed a similar tra-\njectory, albeit with slightly less comprehensive expansions \nupon revision, whereas ChatGPT-4o demonstrated only \nmodest improvements. Conversely, ChatGPT-4o’s marginal \nimprovement (mean ΔTS ≈ + 1.9) suggests that its shorter, \nsummary-oriented decoding strategy leaves insufficient \nlatent space to integrate new cue words, limiting the benefit \nof corrective prompts despite high base fluency. While our \nfindings indicate that LLMs can reliably convey basic points \naround diagnosis and treatment, only the models with robust \nreasoning frameworks consistently adapted to the evolving \ndemands of clinical guidelines. The possibility that clini-\ncians in high-burden or resource-limited settings could refer \nto AI-driven tools for real-time recommendations amplifies \nthe importance of ensuring that these models are both accu-\nrate at baseline and adept at refining their outputs following \nfeedback.\nDespite the clear advantages seen in chain-of-thought-\nequipped models, there remain several overarching chal-\nlenges to integrating AI-based assistance more broadly in \npneumonia care. Chief among these is the need for ongo-\ning alignment of model outputs with contemporary clini-\ncal guidelines, such that emergent threats—whether in the \nform of novel pathogens or rising resistance trends—can be \naddressed in a timely manner. The models evaluated in this \nstudy reflect snapshots of training data up to certain periods, \nwith uncertain capacity to incorporate highly recent devel-\nopments without targeted re-training. This constraint may \nblunt clinical utility when new guideline updates or shifts in \nlocal resistance patterns emerge, because the models cannot \ningest such information in near real-time. For pneumonia \nspecifically, changes in recommended antibiotic stewardship \nstrategies, prophylaxis for recurrent episodes, and vaccina-\ntion scheduling can occur rapidly. Models that cannot fluidly \nupdate their outputs might inadvertently propagate outdated \nor incomplete guidance. Additionally, the spectrum of pneu-\nmonia pathogens is subject to continuous shifts, particularly \nin an era marked by emerging pandemics, compounding the \ncomplexity of delivering a stable, one-size-fits-all approach \nto clinical recommendations. Our evaluations relied chiefly \non North American and international society guidelines; \nreaders should note that performance could differ under \nregional protocols that recommend alternative first-line \nantibiotics. Another salient point is the potential for dif-\nferential performance across subpopulations. Older adults, \nfor instance, often present with more subtle symptoms of \npneumonia or comorbidities that heighten the risk of adverse \noutcomes. Pediatric cases, immunocompromised states, and \npregnancy introduce additional variability in choice of diag-\nnostic imaging and antimicrobial agent safety profiles. Our \nresults demonstrate that the chain-of-thought models are \nbetter equipped to address these population-specific vari-\nations, as they can articulate more detailed reasoning steps \nthat guide antibiotic selection or highlight the importance of \nadvanced diagnostic techniques. By contrast, ChatGPT-4o \nfrequently provided concise summaries that were adequate \nfor general queries but omitted essential steps for more spe-\ncialized populations. It also generated occasional halluci-\nnations—such as outdated fluoroquinolone doses—which \nreinforces the necessity of continuous human oversight. This \ngap is critical, given that pneumonia remains a leading cause \nof pediatric hospitalizations and a frequent driver of acute \ncare visits in older adults.\nNotwithstanding the above strengths, several limitations \nof the current study deserve mention. First, although ten \nindependent infectious disease specialists provided robust \ninput, the moderate Fleiss’ Kappa (0·52) confirms mean-\ningful subjectivity—particularly in deciding whether an \nanswer at the upper end of ‘moderate’ merited promotion \nto ‘excellent’—which could shift individual item classifica-\ntions. The observed Fleiss’ Kappa of 0.52 indicates only \nmoderate inter-rater concordance; thus, a portion of the \nscore spread—especially near the ‘moderate’/‘excellent’ \ncutoff—likely reflects legitimate differences in clinical \nemphasis rather than model error alone. Future studies may \nbenefit from calibration sessions or Delphi rounds to tighten \nconsensus. Our inter-rater reliability suggests moderate \nagreement, implying differences in how experts weigh com-\npleteness versus brevity or the relative importance of certain \nclinical details. The rating scales, while practical, may not \ncapture every subtlety of pneumonia management, such as \nthe integration of patient preference, resource limitations, \nor local pathogen prevalence. Second, the study’s design \nrelied on text-based prompts, which might not replicate the \ndynamic environment of actual patient care, where labora-\ntory findings, imaging results, and point-of-care tests would \nshape each subsequent query to the AI system. Third, the \nextent to which these models can absorb emergent data—\nparticularly the evolving evidence on long-term outcomes, \ncomplications, or novel diagnostic methods—depends on \nthe processes used to update their training sets. Over time, \nthe scope of these data might broaden, but the challenges \nof continuous re-training, verifying changes, and mitigat-\ning potential hallucinations remain areas of active research.\nThese caveats notwithstanding, our comparative find-\nings offer insights into how large language models can \ncomplement clinical decision-making, especially for a \ncommon condition like pneumonia. From the standpoint of \nclinicians and health systems, the chain-of-thought models \nhold promise as a flexible, iterative platform that can inte-\ngrate nuanced feedback from domain experts, especially in \n Clinical and Experimental Medicine          (2025) 25:213 \n  213  Page 8 of 10\nsettings where medical resources are scarce or guideline-\nbased treatment is irregularly enforced. A system capable \nof quickly updating antibiotic recommendations or vac-\ncination schedules, then iterating based on new data or \nspecialist critiques, could help streamline management \ndecisions and potentially reduce the burdens of over-pre-\nscription or delayed care. At the same time, caution must \nbe exercised to avoid over-reliance on any algorithmic \noutput without confirming its validity and alignment with \nestablished clinical practice. Looking ahead, there is scope \nto harness the best attributes of these models more effec-\ntively. Future approaches might extend beyond the text-\nbased paradigm to incorporate patient-specific data—such \nas laboratory values, radiological findings, or comorbidity \nprofiles—within a secure, privacy-conscious framework. \nIntegrating domain-specific knowledge graphs or curated \nrepositories could help LLMs maintain fidelity to the lat-\nest pneumonia guidelines, including antibiotic stewardship \nprinciples and individualized considerations for at-risk \ngroups. Further, the potential to refine and optimize model \nstructures for better interpretability—ensuring that each \nrecommendation is clearly traced to underlying clinical \nevidence—would address a key concern among medical \nprofessionals regarding accountability and reliability.\nConclusion\nIn this study, our findings underscore the value and limi-\ntations of large language models in supporting pneumonia \nmanagement. OpenAI O1 consistently outperformed its \ncounterparts, likely reflecting a robust chain-of-thought \nimplementation, while OpenAI O3 mini exhibited consid-\nerable adaptability after prompted revisions. By contrast, \nChatGPT-4o was comparatively concise and less adept \nat incorporating guideline-based nuances, pointing to the \nimportance of advanced reasoning pathways for challenging \nclinical scenarios. These results highlight how iterative, self-\ncorrecting features can enhance the utility of AI in domains \nwhere evidence-based frameworks evolve rapidly. Nonethe-\nless, any clinical deployment must remain under the super -\nvision of qualified professionals, with automated outputs \ntreated as decision support rather than definitive prescrip-\ntions. Investing in continuous updates, rigorous evaluations, \nand context-specific refinements is imperative if these mod-\nels are to be integrated safely and effectively into the broader \nefforts to reduce pneumonia’s burden. As clinical science \ncontinues to advance, the capacity of AI models to adapt \nand refine their outputs in near real-time could contribute \nmeaningfully to optimizing pneumonia care, provided that \nappropriate caution is exercised in validation and clinical \noversight.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s10238- 025- 01743-7.\nAuthor contributions Conceptualization contributed by ZL, YL, MW, \nHL, XS, QY, GX, JX; methodology contributed by ZL,YL,MW,HL; \nvalidation contributed by XS, QY, GX, JX; formal analysis contrib-\nuted by XS, QY, GX, JX; investigation contributed by ZL, YL, MW; \nresources contributed by ZL, YL, MW; data curation contributed by \nHL, XS, QY; writing—original draft contributed by HL, XS, QY; writ-\ning—review and editing contributed by ZL, YL, MW, HL, XS, QY, \nGX, JX; project administration contributed by ZL, YL, MW, HL, XS, \nQY, GX, JX.\nFunding This work was supported by Multicenter Investigator-Initiated \nTrial Project of Mianyang Central Hospital (Project Number: 2024KY-\nFZC033), Project of the Mianyang Municipal Health Commission \n(Grant No.202207), Incubation Project of Mianyang Central Hospital \n(Grant No.2019FH14).\nData availability The data used to support the findings of this study are \nincluded within the article.\nDeclarations \nConflicts of interest The authors declare no competing interests.\nEthics approval and consent to participate This study was reviewed by \nthe Mianyang Central Hospital Ethics Committee, which determined \nthat this study could be conducted without approval.\nOpen Access  This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material \nderived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and \nyour intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http:// creat iveco \nmmons. org/ licen ses/ by- nc- nd/4. 0/.\nReferences\n 1. Shah NH, Entwistle D, Pfeffer MA. Creation and adoption of large \nlanguage models in medicine. JAMA. 2023;330(9):866–9.\n 2. Meng X, Yan X, Zhang K, et al. The application of large language \nmodels in medicine: a scoping review. Iscience. 2024. https:// doi. \norg/ 10. 1016/j. isci. 2024. 109713.\n 3. Goh E, Gallo R, Hom J, et al. Large language model influence on \ndiagnostic reasoning: a randomized clinical trial. JAMA Netw \nOpen. 2024;7(10):e2440969–e2440969.\n 4. Abd-Alrazaq A, AlSaad R, Alhuwail D, et al. Large language \nmodels in medical education: opportunities, challenges, and future \ndirections. JMIR Med Educat. 2023;9(1):e48291.\n 5. Li H, Moon JT, Purkayastha S, et al. Ethics of large language \nmodels in medicine and medical research. Lancet Digit Health. \n2023;5(6):e333–5.\nClinical and Experimental Medicine          (2025) 25:213  \n Page 9 of 10   213 \n 6. Rojo P, Moraleda C, Tagarro A, et al. Empirical treatment \nagainst cytomegalovirus and tuberculosis in HIV-infected \ninfants with severe pneumonia: study protocol for a multi-\ncenter, open-label randomized controlled clinical trial. Trials. \n2022;23(1):531.\n 7. Alves L, Pullen R, Hurst JR, et al. Conquest: a quality improve-\nment program for defining and optimizing standards of care for \nmodifiable high-risk COPD patients. Patient Relat Outcome Meas. \n2022. https:// doi. org/ 10. 2147/ PROM. S2965 06.\n 8. Aliberti S, Cruz CSD, Amati F, et al. Community-acquired pneu-\nmonia. The Lancet. 2021;398(10303):906–19.\n 9. Cao Q, Wu X, Zhang Q, et al. Mechanisms of action of the BCL-2 \ninhibitor venetoclax in multiple myeloma: a literature review. \nFront Pharmacol. 2023;14:1291920.\n 10. Zhang P, Zhang H, Tang J, et al. The integrated single-cell analy-\nsis developed an immunogenic cell death signature to predict lung \nadenocarcinoma prognosis and immunotherapy. Aging (Albany \nNY). 2023;15(19):10305.\n 11. Klompas M, Branson R, Cawcutt K, et al. Strategies to pre-\nvent ventilator-associated pneumonia, ventilator-associated \nevents, and nonventilator hospital-acquired pneumonia in acute-\ncare hospitals: 2022 Update. Infect Control Hosp Epidemiol. \n2022;43(6):687–713.\n 12. Smith JC, Spann A, McCoy A B, et al. Natural language process-\ning and machine learning to enable clinical decision support for \ntreatment of pediatric pneumonia[C]//AMIA Annual Symposium \nProceedings. 2021, 2020: 1130.\n 13. Lee S, Youn J, Kim H, et al. CXR-LLAVA: a multimodal large \nlanguage model for interpreting chest X-ray images. Europ Radiol. \n2025. https:// doi. org/ 10. 1007/ s00330- 024- 11339-6.\n 14. Chirino A, Cabral G, Cavallazzi R, et al. In patients evaluated in \nthe emergency room with suspected community-acquired pneu-\nmonia, ChatGPT 3.5 may help physicians with assessments and \nplans. Nort Healthc Med J. 2024. https:// doi. org/ 10. 59541/ 001c. \n127927.\n 15. Wang S, Zhao Z, Ouyang X, et al. Interactive computer-aided \ndiagnosis on medical image using large language models. Com-\nmun Eng. 2024;3(1):133.\n 16. Freyer O, Wiest IC, Kather JN, et al. A future role for health appli-\ncations of large language models depends on regulators enforcing \nsafety standards. Lancet Digit Health. 2024;6(9):e662–72.\n 17. Shen Y, Heacock L, Elias J, et al. ChatGPT and other large \nlanguage models are double-edged swords. Radiology. \n2023;307(2):e230163.\n 18. Singh C, Askari A, Caruana R, et al. Augmenting interpretable \nmodels with large language models during training. Nat Commun. \n2023;14(1):7913.\n 19. Liz H, Sánchez-Montañés M, Tagarro A, et al. Ensembles of con-\nvolutional neural network models for pediatric pneumonia diag-\nnosis. Futur Gener Comput Syst. 2021;122:220–33.\n 20. Almaslukh B. A lightweight deep learning-based pneumonia \ndetection approach for energy-efficient medical systems. Wirel \nCommun Mob Comput. 2021;2021(1):5556635.\n 21. El Asnaoui K. Design ensemble deep learning model for pneu-\nmonia disease classification. Int J Multimed Inform Retrieval. \n2021;10(1):55–68.\n 22. Li Y, Wehbe RM, Ahmad FS, et al. A comparative study of pre-\ntrained language models for long clinical text. J Am Med Inform \nAssoc. 2023;30(2):340–7.\n 23. Miyazaki Y, Hata M, Omori H, et al. Performance and Errors of \nChatGPT-4o on the Japanese Medical Licensing Examination: \nSolving All Questions Including Images with Over 90% Accuracy. \nJMIR Med Educ, 2024.\n 24. Tang H, Wang J, Liu Q, et al. Exploring the use of ChatGPT-4o in \nenhancing career development counseling for medical students: a \nstudy protocol. BMJ Open. 2024;14(11):e083697.\n 25. Temsah MH, Jamal A, Alhasan K, et al. OpenAI o1-Preview vs. \nChatGPT in healthcare: A new frontier in medical AI reasoning. \nCureus. 2024;16(10):e70640.\n 26. Jaech A, Kalai A, Lerer A, et al. Openai o1 system card. arXiv \npreprint arXiv: 2412. 16720, 2024.\n 27. You Y, Chen Y, You Y, et al. Evolutionary game analysis of arti-\nficial intelligence such as the generative pre-trained transformer \nin future education. Sustainability. 2023;15(12):9355.\n 28. Arrieta A, Ugarte M, Valle P, et al. Early External Safety Testing \nof OpenAI’s o3-mini: Insights from the Pre-Deployment Evalua-\ntion. arXiv preprint arXiv: 2501. 17749, 2025.\n 29. Ol B, Na AAI. Making IT up. Nature. 2025;637:23.\n 30. Guan MY, Joglekar M, Wallace E, et al. Deliberative alignment: \nReasoning enables safer language models. arXiv preprint arXiv:  \n2412. 16339, 2024.\n 31. Assefa M. Multi-drug resistant gram-negative bacterial pneumo-\nnia: etiology, risk factors, and drug resistance patterns. Pneumo-\nnia. 2022;14(1):4.\n 32. Cao Q, Wu X, Chen Y, et al. The impact of concurrent bacterial \nlung infection on immunotherapy in patients with non-small cell \nlung cancer: a retrospective cohort study. Front Cell Infect Micro-\nbiol. 2023;13:1257638.\n 33. Darie AM, Khanna N, Jahn K, et al. Fast multiplex bacterial PCR \nof bronchoalveolar lavage for antibiotic stewardship in hospital-\nised patients with pneumonia at risk of Gram-negative bacterial \ninfection (Flagship II): a multicentre, randomised controlled trial. \nLancet Respir Med. 2022;10(9):877–87.\n 34. Cillóniz C, Torres A, Niederman MS. Management of pneumo-\nnia in critically ill patients. BMJ. 2021. https:// doi. org/ 10. 1136/ \nbmj- 2021- 065871.\n 35. Giannella M, Bouza E, Viale P. Time to first antibiotic dose for \ncommunity-acquired pneumonia: a challenging balance. Clin \nMicrobiol Infect. 2021;27(3):322–4.\n 36. Ambroggio L, Cotter J, Hall M, et al. Management of pediatric \npneumonia: a decade after the pediatric infectious diseases society \nand infectious diseases society of America guideline. Clin Infect \nDis. 2023;77(11):1604–11.\n 37. Wilson KC, Schoenberg NC, Cohn DL, et  al. Community-\nacquired pneumonia guideline recommendations—impact of a \nconsensus-based process versus systematic reviews. Clin Infect \nDis. 2021;73(7):e1467–75.\n 38. Rhee C, Chiotos K, Cosgrove SE, et al. Infectious Diseases Society \nof America position paper: recommended revisions to the national \nsevere sepsis and septic shock early management bundle (SEP-1) \nsepsis quality measure. Clin Infect Dis. 2021;72(4):541–52.\n 39. Dillon K, Garnick B, Fortier M, et al. The management of infec-\ntious pulmonary processes in the emergency department: pneu-\nmonia. Phys Assistant Clin. 2022;8(1):123.\n 40. Cilloniz C, Ferrer M, Pericàs JM, et al. Validation of IDSA/\nATS guidelines for ICU admission in adults over 80 years old \nwith community-acquired pneumonia. Arch Bronconeumol. \n2023;59(1):19–26.\n 41. Dela Cruz CS, Evans SE, Restrepo MI, et al. Understanding \nthe host in the management of pneumonia. An official Ameri-\ncan thoracic society workshop report. Ann Am Thorac Soc. \n2021;18(7):1087–97.\n 42. He S, Su L, Hu H, et al. Immunoregulatory functions and thera-\npeutic potential of natural killer cell-derived extracellular vesicles \nin chronic diseases. Front Immunol. 2024;14:1328094.\n 43. Jingwen X, et al. The two-sided battlefield of tumour-associated \nmacrophages in glioblastoma: unravelling their therapeutic poten-\ntial. Discov Oncol. 2024;15(1):590. https:// doi. org/ 10. 1007/  \ns12672- 024- 01464-5\n 44. Yatera K, Yamasaki K. Management of the Diagnosis and Treat-\nment of Pneumonia in an Aging Society. Internal Medicine, 2024: \n4203–24.\n Clinical and Experimental Medicine          (2025) 25:213 \n  213  Page 10 of 10\n 45. Tamma PD, Heil EL, Justo JA, et al. Infectious diseases Society of \nAmerica 2024 guidance on the treatment of antimicrobial-resistant \ngram-negative infections. Clin Infect Diseases. 2024. https:// doi. \norg/ 10. 1093/ cid/ ciae4 03.\n 46. Losier A, Cruz CSD. New testing guidelines for community-\nacquired pneumonia. Curr Opin Infect Dis. 2022;35(2):128–32.\n 47. Metersky ML, Kalil AC. Management of ventilator-associated \npneumonia: guidelines. Infect Dis Clin. 2024;38(1):87–101.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}