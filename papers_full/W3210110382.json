{
  "title": "Can Vision Transformers Perform Convolution?",
  "url": "https://openalex.org/W3210110382",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222870637",
      "name": "Li, Shanda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355881896",
      "name": "Chen Xiang-ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098476997",
      "name": "He Di",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222521706",
      "name": "Hsieh, Cho-Jui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W3168489096",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W3172946450",
    "https://openalex.org/W3112479704",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3163120468",
    "https://openalex.org/W3212115192",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W2962749806",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3106009088",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3104613728"
  ],
  "abstract": "Several recent studies have demonstrated that attention-based networks, such as Vision Transformer (ViT), can outperform Convolutional Neural Networks (CNNs) on several computer vision tasks without using convolutional layers. This naturally leads to the following questions: Can a self-attention layer of ViT express any convolution operation? In this work, we prove that a single ViT layer with image patches as the input can perform any convolution operation constructively, where the multi-head attention mechanism and the relative positional encoding play essential roles. We further provide a lower bound on the number of heads for Vision Transformers to express CNNs. Corresponding with our analysis, experimental results show that the construction in our proof can help inject convolutional bias into Transformers and significantly improve the performance of ViT in low data regimes.",
  "full_text": "CAN VISION TRANSFORMERS\nPERFORM CONVOLUTION ?\nShanda Li\nSchool of EECS, Peking University\nlishanda@pku.edu.cn\nXiangning Chen\nDepartment of Computer Science, UCLA\nxiangning@cs.ucla.edu\nDi He\nMicrosoft Research\ndihe@microsoft.com\nCho-Jui Hsieh\nDepartment of Computer Science, UCLA\nchohsieh@cs.ucla.edu\nABSTRACT\nSeveral recent studies have demonstrated that attention-based networks, such\nas Vision Transformer (ViT), can outperform Convolutional Neural Networks\n(CNNs) on several computer vision tasks without using convolutional layers. This\nnaturally leads to the following questions: Can a self-attention layer of ViT ex-\npress any convolution operation? In this work, we prove that a single ViT layer\nwith image patches as the input can perform any convolution operation construc-\ntively, where the multi-head attention mechanism and the relative positional en-\ncoding play essential roles. We further provide a lower bound on the number of\nheads for Vision Transformers to express CNNs. Corresponding with our anal-\nysis, experimental results show that the construction in our proof can help inject\nconvolutional bias into Transformers and signiﬁcantly improve the performance\nof ViT in low data regimes.\n1 I NTRODUCTION\nRecently, the Transformer (Vaswani et al., 2017) architecture has achieved great success in vision\nafter it dominates the language domain (Devlin et al., 2019; Liu et al., 2019). Equipped with large-\nscale pre-training or several improved training strategies, the Vision Transformer (ViT) can outper-\nform CNNs on a variety of challenging vision tasks (Dosovitskiy et al., 2021; Touvron et al., 2021;\nLiu et al., 2021; Chen et al., 2021). As Transformer takes 1D sequences of tokens as input, the com-\nmon manner to transform a 2D image into such a 1D sequence is introduced by Dosovitskiy et al.\n(2021): An image X ∈RH×W×C is reshaped into a sequence of ﬂattened patches ˜X ∈RN×P2C,\nwhere H,W,C are the image height, width, channel, P is the patch resolution, and N = HW/P2\nis the sequence length. By using speciﬁc positional encoding to encode spatial relationship between\npatches, a standard Transformer can therefore be used in the vision domain.\nIt has been observed that when there is sufﬁcient training data, ViT can dramatically outperform\nconvolution-based neural network models (Dosovitskiy et al., 2021) (e.g., 85.6% vs 83.3% ImageNet\ntop-1 accuracy for ViT-L/16 and ResNet-152x2 when pre-trained on JFT-300M). However, ViT still\nperforms worse than CNN when trained on smaller-scale datasets such as CIFAR-100. Motivated by\nthese observations, it becomes natural to compare the expressive power of Transformer and CNN.\nIntuitively, a Transformer layer is more powerful since the self-attention mechanism enables context-\ndependent weighting while a convolution can only capture local features. However, it is still unclear\nwhether a Transformer layer is strictly more powerful than convolution. In other words:\nCan a self-attention layer of ViT (with image patches as input) express any convolution operation?\nA partial answer has been given by Cordonnier et al. (2020). They showed that a self-attention\nlayer with a sufﬁcient number of heads can express convolution, but they only focused on the set-\ntings where the input to the attention layer is the representations of pixels, which is impractical\ndue to extremely long input sequence and huge memory cost. In Vision Transformer and most of\nits variants (Touvron et al., 2021; Dosovitskiy et al., 2021; D’Ascoli et al., 2021), the input is the\nrepresentations of non-overlapping image patches instead of pixels. As a convolution operation can\n1\narXiv:2111.01353v2  [cs.CV]  3 Nov 2021\ninvolve pixels across patch boundaries, whether a self-attention layer in ViT can express convolution\nis still unknown.\nIn this work, we give an afﬁrmative answer to the above-mentioned question. We formally prove\nthat a ViT layer with relative positional encoding and sufﬁcient attention heads can express any\nconvolution even when the input is image patches. This implies that the poor performance of ViT\non small datasets is mainly due to its generalization ability instead of expressive power. We further\nprovide a lower bound on the number of heads required for transforming convolution into a self-\nattention layer. Based on our theoretical ﬁndings, we propose a two-phase training pipeline to inject\nconvolutional bias into Vision Transformers, and empirically demonstrate its effectiveness in low\ndata regimes.\nThe contributions of this paper are summarized below.\n• We provide a constructive proof to show that a 9-head self-attention layer in Vision Trans-\nformers with image patch as the input can perform any convolution operation, where the\nkey insight is to leverage the multi-head attention mechanism and relative positional en-\ncoding to aggregate features for computing convolution.\n• We prove lower bounds on the number of heads for self-attention layers to express con-\nvolution operation, for both the patch input and the pixel input setting. This result shows\nthat the construction in the above-mentioned constructive proof is optimal in terms of the\nnumber of heads. Speciﬁcally, we show that 9 heads are both necessary and sufﬁcient for\na self-attention layer with patch input to express convolution with a K×K kernel, while\na self-attention layer with pixel input must need K2 heads to do so. Therefore, Vision\nTransformers with patch input are more head-efﬁcient than pixel input when expressing\nconvolution.\n• We propose a two-phase training pipeline for Vision Transformers. The key component in\nthis pipeline is to initialize ViT from a well-trained CNN using the construction in our the-\noretical proof. We empirically show that with the proposed training pipeline that explicitly\ninjects the convolutional bias, ViT can achieve much better performance compared with\nmodels trained with random initialization in low data regimes.\n2 P RELIMINARIES\nIn this section, we recap the preliminaries of Convolutional Neural Networks and Vision Transform-\ners, and deﬁne the notations used in our theoretical analysis. We use bold upper-case letters to denote\nmatrices and tensors, and bold lower-case letters to denote vectors. Let [m] = {1,2,··· ,m}. The\nindicator function of Ais denoted by 1 A.\n2.1 C ONVOLUTIONAL NEURAL NETWORKS\nConvolutional Neural Networks (CNNs) are widely used in computer vision tasks, in which the\nconvolutional layer is the key component.\nConvolutional layer. Given an image X ∈RH×W×C, the output of a convolutional layer for\npixel (i,j) is given by\nConv(X)i,j,: =\n∑\n(δ1,δ2)∈∆\nXi+δ1,j+δ2,:WC\nδ1,δ2,:,:, (1)\nwhere WC ∈RK×K×C×Dout is the learnable convolutional kernel, K is the size of the kernel and\nthe set ∆ = {−⌊K/2⌋,··· ,⌊K/2⌋}×{−⌊K/2⌋,··· ,⌊K/2⌋}is the receptive ﬁeld.\n2.2 V ISION TRANSFORMERS\nA Vision Transformer takes sequences of image patches as input. It usually begins with a patch\nprojection layer, followed by a stack of Transformer layers. A Transformer layer contains two\nsub-layers: the multi-head self-attention (MHSA) sub-layer and the feed-forward network (FFN)\nsub-layer. Residual connection (He et al., 2016) and layer normalization (Lei Ba et al., 2016) are\napplied for both sub-layers individually. Some important components are detailed as follows:\n2\nPatch input. Consider an input image X ∈ RH×W×C, where H,W,C is the image height,\nwidth and channel. To feed it into a Vision Transformer, it is reshaped into a sequence of ﬂat-\ntened patches ˜X ∈RN×P2C, where P is the patch resolution, and N = HW/P2 is the sequence\nlength. Formally, a ﬂattened patch is deﬁned as ˜Xi,: = concat\n(\nXhi1,wi1,:,··· ,XhiP2 ,wiP2 ,:\n)\n,\nwhere (hi1,wi1),··· ,(hiP2 ,wiP2 ) are the positions of pixels in the i-th patch. Then a linear pro-\njection is applied on all ﬂattened patches to obtain the input to the Transformer.\nMulti-head self-attention (MHSA) layer. The attention module is formulated as querying a dic-\ntionary with key-value pairs, i.e., Attention(Q,K,V ) = softmax\n(\nQK⊤\n√\nd\n)\nV , where d is the\ndimension of the hidden representations, and Q, K, V are referred to as queries, keys and values\nthat are all produced by linearly projecting the output of the previous layer. The multi-head variant\nof the attention module is popularly used because it allows the model to jointly learn the informa-\ntion from different representation sub-spaces. Formally, an MHSA layer with input H ∈RN×d is\ndeﬁned as:\nMHSA(H) = concat(SA1(H),··· ,SANH (H))WO =\nNH∑\nk=1\nSAk(H)WO\nk (2)\nSAk(H) = Attention(HW Q\nk ,HW K\nk ,HW V\nk ), (3)\nwhere WQ\nk ,WK\nk ,WV\nk ∈ Rd×dH and WO = ( WO⊤\n1 ,··· ,WO⊤\nNH )⊤ ∈ RNHdH×dO are learn-\nable projection matrices 1, NH is the number of heads, dH is the size of each head, and dO is the\ndimensionality of the output.\nRelative positional encoding. Many Vision Transformer models adopt a learnable relative posi-\ntion bias term in computing self-attention scores (Liu et al., 2021; Luo et al., 2021; Li et al., 2021):\nAttention(Q,K,V ) = softmax\n(QK⊤\n√\nd\n+ B\n)\nV , (4)\nwhere Bi,j only depends on the relative position between the i-th patch (query patch) and the j-th\npatch (key patch). More speciﬁcally, assume the position of the ℓ-th patch is (xℓ,yℓ), then Bi,j =\nb(xi−xj,yi−yj). For −H\nP + 1 ≤x≤H\nP −1 and −W\nP + 1 ≤y≤W\nP −1, b(x,y) is a trainable scalar.\n3 E XPRESSING CONVOLUTION WITH THE MHSA LAYER\nIn this section, we consider the question of using the MHSA layer in Vision Transformers to express\na convolutional layer. We mainly focus on the patch-input setting, which is more realistic for current\nViTs. First, we show that a MHSA layer in Vision Transformers can express a convolutional layer in\nthe patch-input setting (Theorem 1). Second, we prove lower bounds on the number of heads for self-\nattention layers to express the convolution operation for both patch and pixel input settings, which\ndemonstrates that the number of heads required in Theorem 1 is optimal. Putting the representation\ntheorem and the lower bounds together, we conclude that MHSA layers with patch input are more\nhead-efﬁcient in expressing convolutions. The dependency on the number of heads is more feasible\nin the patch-input setting for Vision Transformers in practice.\n3.1 A N MHSA LAYER WITH ENOUGH HEADS CAN EXPRESS A CONVOLUTIONAL LAYER\nOur main result in this subsection is that an MHSA layer can express convolution under mild as-\nsumptions in the patch-input setting. To be precise, we present the following theorem:\nTheorem 1. In the patch-input setting, assume dH ≥d and dO ≥P2Dout. Then a multi-head\nself-attention layer with NH =\n(\n2\n⌈K−1\n2P\n⌉\n+ 1\n)2\nheads and relative positional encoding can express\nany convolutional layer of kernel size K×K, and Dout output channels.\n1For simplicity, the bias terms of linear projections are omitted.\n3\nThe patch input poses the major difﬁculty in proving this result: The convolution operation can in-\nvolve pixels across patch boundaries, which makes the problem complicated. To address this, we\nﬁrst aggregate the information from all the relevant patches for calculating the convolution by lever-\nage the relative positional encoding and multi-head mechanism, and then apply a linear projection\non the aggregated features. This idea leads to a constructive proof.\nProof sketch of Theorem 1. Note that the attention calculation with relative positional encod-\ning can be dissected into a context-aware part (which depends on all the input tokens) and a posi-\ntional attention part (which is agnostic to the input): In Equation (4), QK⊤and B correspond to\nthe context-aware part and the positional attention part respectively. Since convolution is context-\nagnostic by nature, we set WQ\nk = WK\nk = 0 (∀k ∈ [NH]) and purely rely on the positional\nattention in the proof. Given any relative position δbetween two patches, we force the query patch\nto focus on exactly one key patch, such that the relative position between the query and the key isδ.\nWe elaborate on this argument in Lemma 2.\nLemma 2. For any relative position δ between two patches, there exists a relative positional en-\ncoding scheme B such that softmax(Bq,:)k = 1 {q−k=δ}, where q,k are the index of the query/key\npatch.2\nLet the receptive ﬁeld of a given patch in K×K convolution be the set of patches that contain at\nleast one pixel in the receptive ﬁeld of any pixel in the given patch. Then it’s easy to see that the\nrelative position between a given patch and the patches in its receptive ﬁeld are\n˜∆ =\n{\n−\n⌈K−1\n2P\n⌉\n,··· ,\n⌈K−1\n2P\n⌉}\n×\n{\n−\n⌈K−1\n2P\n⌉\n,··· ,\n⌈K−1\n2P\n⌉}\n. (5)\nWith Lemma 2, we can force the query patch to attend to the patch at a given relative position in\n˜∆ in each head. By setting WV\nk = ( Id,0d×(dH−d)), the hidden representation (before the ﬁnal\nprojection WO) of the query patch contains the features of all the patches in its receptive ﬁeld.\nFinally, by the linearity of convolution, we can properly set the weights inWO based on the convo-\nlution kernel, such that the ﬁnal output is equivalent to that of the convolution for any pixel, which\nconcludes the proof. We refer the readers interested in a formal proof to Appendix A.2.\nRemark on the positional encoding. In this result, we focus on a speciﬁc form of relative posi-\ntional encoding. In fact, Theorem 1 holds as long as the positional encoding satisﬁes the property in\nLemma 2. It’s easy to check that a wide range of positional encoding have such property (Dai et al.,\n2019; Raffel et al., 2020; Ke et al., 2020; Liu et al., 2021), so our result is general.\nHowever, our construction does not apply to MHSA layers that only use absolute positional encod-\ning. In the prood, we need aseparate context-agnostic termin calculating attention scores. However,\nabsolute positional encoding, which is typically added to the input representation, cannot be sepa-\nrated from context-dependent information and generate the desired attention pattern in Lemma 2.\nRemark on the pixel-input setting. It should be noted that the pixel-input setting is a special\ncase of the analyzed patch-input setting, since patches become pixels when patch resolution P = 1.\nTherefore, the result in Cordonnier et al. (2020) can be viewed as a natural corollary of Theorem 1.\nCorollary 3. In the pixel-input setting, a multi-head self-attention layer with NH = K2 heads of\ndimension dH, output dimensiondO and relative positional encodings can express any convolutional\nlayer of kernel size K×Kand min{dH,dO}output channels.\nPractical implications of Theorem 1. For Vision Transformers and CNNs used in practice, we\ntypically have K <2P, e.g., P ≥16 in most Vision Transformers, and K = 3,5,7 in most CNNs.\nThus, the following corollary is more practical:\nCorollary 4. In the patch-input setting, assume K < 2P, dH ≥d and dO ≥P2Dout. Then\na multi-head self-attention layer with 9 heads and relative positional encoding can express any\nconvolutional layer of kernel size K×Kand Dout output channels.\n2Here we abuse the notation for ease of illustration: When used as subscripts, q, kare scalars in [N]; When\nused to denote the locations of patches, q, kare two-dimensional coordinates in [H/P ] × [W/P ].\n4\nAnother thing that would be important from a practical perspective is that Theorem 1 can be gen-\neralized to other forms of convolution operations, although we focus on the simplest formulation\ndeﬁned in Equation 1. For example, people sometimes use convolution with stride greater than 1, or\ndilated convolution (Yu & Koltun, 2015) in practice. This theorem can be easily generalized to these\ncases. Intuitively, we only use the linearity of convolution in our proof, so any variant of convolution\nthat preserves this property can be expressed by MHSA layers according to our construction.\n3.2 A N MHSA LAYER WITH INSUFFICIENT HEADS CANNOT EXPRESS CONVOLUTION\nIt’s noticeable that the multi-head mechanism plays an essential role in the constructive proof of\nTheorem 1. Thus, it’s natural to ask whether the dependency on the number of heads is optimal in\nthe theorem. In this part, we present lower bounds on the number of heads required for an MHSA\nlayer to express convolution in both pixel-input and patch-input setting, highlighting the importance\nof the multi-head mechanism and showing the optimality of our construction in the previous proof.\n3.2.1 T HE PIXEL -INPUT SETTING\nWe ﬁsrt show that the dependency on the number of heads in Corollary 3 is optimal, i.e., an MHSA\nlayer must need K2 heads to express convolution of kernel size K×K.\nTheorem 5. In the pixel-input setting, suppose NH < min{K2,d}. There exists a convolutional\nkernel weight WC ∈RK×K×d×Dout such that any MHSA layer with NH heads and relative posi-\ntional encoding cannot express conv(·; WC).\nProof. We will prove the theorem in the case whereDout = 1 by contradiction, and consequently\nthe result will hold for anyDout ∈N∗. Since Dout = 1, we viewWC as a three-dimensional tensor.\nIn the convolutional layer, consider the output representation of the pixel at positionγ ∈[H] ×[W]:\nconv(X; WC)γ =\n∑\nδ∈∆\nd∑\ni=1\nXγ+δ,iWC\nδ,i, (6)\nwhere ∆ = {−⌊K/2⌋,··· ,⌊K/2⌋}×{−⌊K/2⌋,··· ,⌊K/2⌋}.\nIn the MHSA layer, assume the attention score between the query pixelγand the key pixel γ+ δin\nthe k-th head is ak\nδ(γ). Let WV\nk WO\nk = wk = (wk\n1 ,··· ,wk\nd)⊤∈Rd×Dout (recall that Cout = 1).\nThen, the output representation of the pixel at position γ ∈[H] ×[W] is\nMHSA(X)γ =\nNH∑\nk=1\n∑\nδ\nah\nδ(γ)\nd∑\ni=1\nXγ+δ,iwk\ni =\n∑\nδ\nd∑\ni=1\nXγ+δ,i\nNH∑\nk=1\nak\nδ(γ)wk\ni. (7)\nPutting Equation 6 and 7 together, in order to ensure conv(X,WC)γ = MHSA(X)γ, we have\nWC\nδ,i =\nNH∑\nk=1\nak\nδ(γ)wk\ni (∀δ∈∆,i ∈[d]) ⇒ ˜WC =\nNH∑\nk=1\nak(γ)wk⊤, (8)\nwhere ak(γ) = ( ak\nδ(γ))δ∈∆ ∈RK2\nis a row vector for any k ∈[NH], and ˜WC ∈RK2×d is\nreshaped from the weights WC ∈RK×K×d×1.\nNote that\nrank\n(NH∑\nk=1\nak(γ)wk⊤\n)\n≤NH <min{K2,d}. (9)\nBy properly choosing convolutional kernel weights WC such that rank( ˜WC) = min{K2,d}, we\nconclude the proof by contradiction.\nRemark. For Vision Transformers and CNNs used in practice, we typically have min{K2,d}=\nK2. Thus this result shows that K2 heads are necessary, and Corollary 3 is optimal in terms of the\nnumber of heads.\n5\n3.2.2 T HE PATCH -INPUT SETTING\nIn the patch-input setting, we show that at least 9 heads are needed for MHSA layers to perform\nconvolution.\nTheorem 6. In the patch-input setting, suppose K ≥3 and NH ≤8. There exists a convolutional\nkernel weight WC ∈RK×K×Din×Dout, such that any MHSA layer with NH heads and relative\npositional encoding cannot express conv(·; WC).\nSimilar to Theorem 5, this theorem is also proven with a rank-based argument. However, the proof\nrequires more complicated techniques to deal with the patch input, so we defer it to Appendix A.3.\nRemark. This result shows that Corollary 4 is also optimal in terms of the number of heads in\npractical cases.\nDiscussions on the theoretical ﬁndings. Our ﬁndings clearly demonstrate the difference between\nthe pixel-input and patch-input setting: patch input makes self-attention require less heads to per-\nform convolution compared to pixel input, especially when K is large. For example, according to\nTheorem 5, MHSA layers with pixel input need at least25 heads to perform 5×5 convolution, while\nthose with patch input only need 9 heads. Usually the number of heads in a MHSA layer is small in\nVision Transformers, e.g., there are only12 heads in ViT-base. Therefore, our theory is realistic and\naligns well with practical settings.\n4 T WO-PHASE TRAINING OF VISION TRANSFORMERS\nOur theoretical results provide a construction that allows MHSA layers to express convolution. In\nthis section, we propose a two-phase training pipeline for Vision Transformers which takes advan-\ntage of the construction. Then we conduct experiments using this pipeline and demonstrate that\nour theoretical insight can be used to inject convolutional bias to Vision Transformers and improve\ntheir performance in low data regimes. We also discuss additional beneﬁts of the proposed training\npipeline from the optimization perspective. Finally, we conclude this section with a discussion on\nthe limitation of our method.\n4.1 M ETHOD AND IMPLEMENTATION DETAILS\nTwo-phase training pipeline. Inspired by the theoretical ﬁndings, we propose a two-phase train-\ning pipeline for Vision Transformers in the low data regime, which is illustrated in Figure 1. Specif-\nically, we ﬁrst train a “convolutional” variant of Vision Transformers, where the MHSA layer is\nreplaced by a K ×K convolutional layer. We refer to this as the convolution phase of training.\nAfter that, we transfer the weights in the pre-trained model to a Transformer model, and continue\ntraining the model on the same dataset. We refer to this as the self-attention phase of training. The\nnon-trivial step in the pipeline is to initialize MHSA layers from well-trained convolutional layers,\nand we utilize the construction in the proof of Theorem 1 to do so. Due to the existence of the con-\nvolution phase, we cannot use a [cls] token for classiﬁcation. Instead, we follow Liu et al. (2021)\nto perform image classiﬁcation by applying global average pooling over the output of the last layer,\nfollowed by a linear classiﬁer. This method is commonly used in CNNs for image classiﬁcation.\nIntuitively, in the convolution phase, the model learns a “convolutional neural network” on the data\nand enjoys the inductive bias including locality and spatial invariance which makes learning easier.\nIn the self-attention phase, the model mimics the pre-trained CNN in the beginning, and gradually\nlearns to leverage the ﬂexibility and strong expressive power of self-attention.\nImplementation details. While our theory focuses on a single MHSA layer, we experiment\nwith 6-layer Vision Transformers to show that our theoretical insight still applies when there are\nstacked Transformer layers. We focus on the low-data regime and train our model on CIFAR-100\n(Krizhevsky et al., 2009). The input resolution is set to 224, and the patch resolution P is set to 16.\nIn the convolution phase, we experiment models with convolutional kernel size K = 3 and 5. To\napply our theory, in the self-attention phase, the number of attention headsNH is set to 9. The input\nand output dimension of MHSA layers dand dO are both set to 768. The size of each head dH is\n6\nFigure 1: Overview of the two-phase training pipeline. See\nSection 4.1 for details.\n20 40 60 80 100\nEpoch\n1.0\n1.2\n1.4\n1.6Validation loss\nConvolutional initializaion\nw/o warm up\nw/ warm up\n20 40 60 80 100\nEpoch\n3.0\n3.4\n3.8\n4.2Validation loss\nRandom initializaion\nw/o warm up\nw/ warm up\nFigure 2: Loss curves of CMHSA\nwith or without the warm-up stage\nunder two initialization schemes.\nset to 768. The dimension of feed-forward layer dFFN is set to 3072. Detailed descriptions of the\nexperimental settings are presented in Appendix B.\n4.2 E XPERIMENTAL RESULTS\nFor ease of illustration, we name our models as CMHSA- K (Convolutionalized MHSA) where K\nis the size of the convolutional kernel in the ﬁrst training phase. To demonstrate the effectiveness of\nour approach, we choose several baselines models for comparison:\n• ViT-base proposed in (Dosovitskiy et al., 2021), which applies Transformers on image\nclassiﬁcation straightforwardly.\n• DeiT-small and DeiT-base proposed in Touvron et al. (2021), which largely improve the\nperformance of ViT using strong data augmentation and sophisticated regularization.\n• CMHSA trained only in convolution phase or self-attention phase. When training directly\nin self-attention phase, the model is initialized randomly. In Table 1, CMHSA- K (1st\nphase) refers to models trained only in convolution phase, and CMHSA (2nd phase) refers\nto models trained only in self-attention phase (Note that Kis irrelevant in this case).\nTo ensure a fair comparison, all the baseline models are trained for 400 epochs, while our models\nare trained for 200 epochs in each phase.\nThe experimental results are shown in Table 1. We evaluate the performance of the models in terms\nof both test accuracy and training cost, and make the following observations on the results:\nThe proposed two-phase training pipeline largely improves performance. It’s easy to see that\nDeiTs clearly outperform ViT, demonstrating the effectiveness of the training strategy employed\nby DeiT. Furthermore, our models with two-phase training pipeline outperform DeiTs by a large\nmargin, e.g., the top-1 accuracy of our CMHSA-5 model is nearly9% higher than that of DeiT-base.\nThis demonstrates that the proposed training pipeline can provide further performance gain on top\nof the data augmentation and regularization techniques in the low-data regime.\nBoth training phases are important. From the last 5 rows of Table 1, we can see that under the\nsame number of epochs, CMHSAs trained with only one phase always underperform those trained\nwith both two phases. The test accuracy of CMHSA (2nd phase), which is a randomly initialized\nCMHSA trained for 400 epochs, is much lower than that of our ﬁnal models (which are trained in\nboth two phases). Therefore, the convolutional bias transferred from the ﬁrst phase is crucial for\nthe model to achieve good performance. Besides, the models trained only in the ﬁrst phase are also\n7\nTable 1: Experimental results on CIFAR-100 dataset.\nModel L N H d d FFN Top-1 Top-5 Training time\nViT-base 12 12 768 3072 60.90 86.66 1.00 ×\nDeiT-small 12 6 384 1536 71.83 90.99 0.57 ×\nDeiT-base 12 12 768 3072 69.98 88.91 1.00 ×\nCMHSA-3 (1st phase) 6 − 768 3072 76.07 93.03 0.45 ×\nCMHSA-5 (1st phase) 6 − 768 3072 76.12 93.13 0.49 ×\nCMHSA (2nd phase) 6 9 768 3072 69.83 91.39 1.48 ×\nCMHSA-3 (ours) 6 9 768 3072 76.72 93.74 0.96 ×\nCMHSA-5 (ours) 6 9 768 3072 78.74 94.40 0.98×\nworse than our ﬁnal models. For example, CMHSA-5 outperforms CMHSA-5 (1st phase) by 2.62%.\nThis shows that the second phase enables the model to utilize the ﬂexibility of MHSA layers to learn\nbetter representations, achieving further improvements upon the convolutional inductive bias.\nThe convolutional phase helps to accelerate training. Training of Transformers is usually time-\nconsuming due to the high computational complexity of the MHSA module. In contrast, CNNs\nenjoy much faster training and inference speed. In our proposed training pipeline, the convolution\nphase is very efﬁcient. Although the self-attention phase is slightly slower, we can still ﬁnish 400\nepochs of training using less time compared with DeiT-base. In Table 1, it is clear that our model\nsigniﬁcantly outperforms other models with a comparable training time.\n4.3 A DDITIONAL BENEFITS OF THE TWO -PHASE TRAINING PIPELINE\nAs mentioned above, the two-phase training pipeline helps improve training efﬁciency and test ac-\ncuracy of Vision Transformers. In this subsection we emphasize an additional beneﬁt of the method:\nThe injected convolutional bias makes the optimization process easier , allowing us to remove\nthe warm-up epochs in training.\nThe warm-up stage is crucial to stabilize the training of Transformers and improve the ﬁnal per-\nformance, but it also slows down the optimization and brings more hyperparameter tuning (Huang\net al., 2020; Xiong et al., 2020). We empirically show that, when initialized with a pretrained CNN,\nour CMHSA model can be trained without warm-up and still obtains competitive performance.\nWe train CMHSA in the self-attention phase with pretrained convolution-phase initialization and\nrandom initialization, and show the loss curves of the ﬁrst 100 epochs in Figure 2. In the ﬁgure, the\nx-axis indicates the number of epoch and the y-axis indicates the validation loss. “Convolutional\nInitialization” refers to the models initialized from a pretrained convolution-phase, while “Random\nInitialization” refers to the models initialized randomly. The only difference in experimental setting\nbetween the “w/o warm up” and “w/ warm up” curves is whether a warm-up stage is applied, and\nall the other hyperparameters are unchanged.\nFrom Figure 2, we can see that when CMHSA is initialized randomly in the self-attention phase,\nthe training is ineffective without the warm-up stage. For example, it takes nearly 80 epochs for\nthe model without warm-up stage to reach the same validation loss achieved in the 20th epoch of\nthe model with warm-up. In contrast, when initialized from the pretrained convolution phase, the\nvalidation losses are similar for models with and without the warm-up stage. After 200 epochs’\ntraining, the model without warm-up stage achieves 78.90% top-1 accuracy, slightly outperforming\nthe model with warm-up stage ( 78.74%). Therefore, the proposed two-phase training pipeline can\ntake advantage of the convolutional inductive bias and make training of Vision Transformers easier,\nwhile enables to remove the warm-up stage and eases the efforts of hyperparameter tuning.\nLimitations. Finally, we point out that our current method cannot enable any ViTs to mimic CNNs\nsince we have some constraints on the ViT architecture. In particular, we require sufﬁcient number\nof heads (≥9). As suggested by our theory, an exact mapping doesn’t exist for smaller number of\nheads, and it would be interesting to study how to properly initialize ViT from CNN even when the\nexact mapping is not applicable.\n8\n5 R ELATED WORK AND DISCUSSIONS\n5.1 E XPRESSIVE POWER OF SELF -ATTENTION LAYERS AND TRANSFORMERS\nAs Transformers become increasingly popular, many theoretical results have emerged to study their\nexpressive power. Transformers are Turing complete (under certain assumptions) (P´erez et al., 2019)\nand universal approximators (Yun et al., 2019), and these results have been extended to Transformer\nvariants with sparse attention (Zaheer et al., 2020; Yun et al., 2020). Levine et al. (2020); Wies\net al. (2021) study how the depth, width and embedding dimension affect the expressive power\nof Transformers. Dong et al. (2021) analyze the limitations of a pure self-attention Transformer,\nillustrating the importance of FFN layers and skip connections.\nThere are also some works focusing on a single self-attention layer or the self-attention matrices.\nBhojanapalli et al. (2020) identify a low-rank bottleneck in attention heads when the size of each\nhead is small. Likhosherstov et al. (2021) proves that a ﬁxed self-attention module can approximate\narbitrary sparse patterns depending on the input when the size of each head d = O(log N), where\nN denotes the sequence length.\nOur work is motivated by the recent success of Vision Transformers, and aims to compare a layer in\nTransformers and in CNNs, which is different from the works mentioned above. The most relevant\nwork is (Cordonnier et al., 2020), which shows that a MHSA layer with K2 heads can express a\nconvolution of kernel size K. However, this result only focuses on the pixel-input setting, which is\ninfeasible for current Vision Transformers to apply, especially on high-resolution images. We study\nthe more realistic patch-input setting (of which the pixel-input setting is a special case). We also\nderive lower bounds on the number of heads for such expressiveness, showing the optimality of our\nresult. Therefore, our work provides a more precise picture showing how a MHSA layer in current\nViTs can express convolution.\n5.2 T RAINING VISION TRANSFORMERS\nDespite Vision Transformers reveal extraordinary performance when pre-trained on large-scale\ndatasets (e.g., ImageNet-21k and JFT-300M), they usually lay behind CNNs when trained from\nscratch on ImageNet, let alone smaller datasets like CIFAR-10/100. Previous methods usually em-\nploy strong augmentations (Touvron et al., 2021) or sophisticated optimizer (Chen et al., 2021) as\nrescues. For instance, Chen et al. (2021) observe that enforcing the sharpness constraint during\ntraining can dramatically enhance the performance of Vision Transformers. Touvron et al. (2021)\nstack multiple data augmentation strategies to manually inject inductive biases. They also propose\nto enhance the accuracy by distilling Vision Transformers from pre-trained CNN teachers.\nSupported by our theoretical analysis, we propose a two-phase training pipeline to inject convolu-\ntional bias into ViTs. The most relevant work to our approach is (D’Ascoli et al., 2021). D’Ascoli\net al. (2021) propose a variant of ViT called ConViT, and they also try to inject convolutional bias\ninto the model by initializing it following the construction in (Cordonnier et al., 2020) so that the\nmodel can perform convolution operation at initialization, which resembles the second phase of our\ntraining pipeline. However, their work differs from ours in several aspects: First, their initializa-\ntion strategy only applies in the pixel-input setting. Thus the models can only perform convolution\non images which are 16×downsampled. By contrast, our construction enables MHSA layers with\npatch input to perform convolution on the original image. Second, they only initialize the attention\nmodule to express a random convolution, while our method explicitly transfers information from\na well-learned CNN into a ViT. Third, ConViT makes architectural changes by introducing Gated\nPositional Self-Attention layers, while we keep the MHSA module unmodiﬁed.\n6 C ONCLUSION\nIn this work, we prove that a single ViT layer can perform any convolution operation construc-\ntively, and we further provide a lower bound on the number of heads for Vision Transformers to\nexpress CNNs. Corresponding with our analysis, we propose a two phase training pipeline to help\ninject convolutional bias into Transformers, which improves test accuracy, training efﬁciency and\noptimization stability of ViTs in the low data regimes.\n9\nREFERENCES\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-\nrank bottleneck in multi-head attention models. In International Conference on Machine Learn-\ning, pp. 864–873. PMLR, 2020.\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets\nwithout pretraining or strong data augmentations, 2021.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In International Conference on Learning Representations ,\n2020. URL https://openreview.net/forum?id=HJlnC1rKPB.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019.\nSt´ephane D’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent\nSagun. Convit: Improving vision transformers with soft convolutional inductive biases. In Marina\nMeila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning Research , pp. 2286–2296. PMLR,\n18–24 Jul 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure\nattention loses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404, 2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nXiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims V olkovs. Improving transformer opti-\nmization through better initialization. In International Conference on Machine Learning , pp.\n4475–4483. PMLR, 2020.\nGuolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In\nInternational Conference on Learning Representations, 2020.\nAlex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nYoav Levine, Noam Wies, Or Sharir, Hoﬁt Bata, and Amnon Shashua. The depth-to-width interplay\nin self-attention. arXiv preprint arXiv:2006.12467, 2020.\nYang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable fourier features for multi-\ndimensional spatial positional encoding. In NeurIPS, 2021.\nValerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of\nself-attention matrices. arXiv preprint arXiv:2106.03764, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n10\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam, 2018.\nShengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang,\nand Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding.\nIn NeurIPS, 2021.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in Neural Information Processing Systems, 32:\n8026–8037, 2019.\nJorge P´erez, Javier Marinkovi ´c, and Pablo Barcel ´o. On the turing completeness of modern neural\nnetwork architectures. In International Conference on Learning Representations , 2019. URL\nhttps://openreview.net/forum?id=HyGBdo0qFm.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research, 21:1–67, 2020.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerve Jegou. Training data-efﬁcient image transformers & distillation through attention. In\nMarina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on\nMachine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 10347–\n10357. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/\ntouvron21a.html.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(eds.), Advances in Neural Information Processing Systems , volume 30. Curran Asso-\nciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nNoam Wies, Yoav Levine, Daniel Jannai, and Amnon Shashua. Which transformer architecture ﬁts\nmy data? a vocabulary bottleneck in self-attention. arXiv preprint arXiv:2105.03928, 2021.\nRoss Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\nIn International Conference on Machine Learning, pp. 10524–10533. PMLR, 2020.\nFisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv\npreprint arXiv:1511.07122, 2015.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are\ntransformers universal approximators of sequence-to-sequence functions? In International Con-\nference on Learning Representations, 2019.\nChulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and\nSanjiv Kumar. O (n) connections are expressive enough: Universal approximability of sparse\ntransformers. In NeurIPS, 2020. URL https://proceedings.neurips.cc/paper/\n2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. In NeurIPS, 2020.\n11\nAPPENDIX\nA O MITTED PROOFS OF THEORETICAL RESULTS\nA.1 P ROOF OF LEMMA 2\nProof. Recall that Bi,j = b(xi−xj,yi−yj) where (xℓ,yℓ) denotes the position the ℓ-th patch. Set\nbδ0 = M and bδ = 0(δ̸= δ0), where M is a scalar. Then\nsoftmax(Bq,:)k =\n{ 1\neM+N−1 q−k̸= δ\neM\neM+N−1 q−k= δ (10)\nNote that\nlim\nM→+∞\neM\neM + N −1 = 1. (11)\nlim\nM→+∞\n1\neM + N −1 = 0. (12)\nTherefore, we only need to setMto be sufﬁciently large number to conclude the proof. For example,\nby setting M = 40 we will have softmax(Bq,:)k = 1 {q−k=δ}up to machine precision.\nA.2 P ROOF OF THEOREM 1\nProof. Assume the input (sequence of ﬂattened image patches) is X. We only need to prove the\nresult for dH = dand dO = P2Dout, since an MHSA layer with larger dH and/or dO is at least as\nexpressive as the one with dH = dand dO = P2Dout.\nDeﬁne the receptive ﬁeld of a given patch inK×Kconvolution be the set of patches which contain\nat least one pixel in the receptive ﬁeld of any pixel in the given patch. Then it’s easy to see that the\nrelative position between a given patch and the patches in its receptive ﬁeld are\n˜∆ =\n{\n−\n⌈K−1\n2P\n⌉\n,··· ,\n⌈K−1\n2P\n⌉}\n×\n{\n−\n⌈K−1\n2P\n⌉\n,··· ,\n⌈K−1\n2P\n⌉}\n. (13)\nNote that NH = |˜∆|. Therefore, for any relative position index δ ∈ ˜∆, we can assign an attention\nhead for it, such that the query patch always attends to the patch at the given relative position δ in\nthis head. We further set WV\nk = (Id) (recall that dH = d). Consequently, the hidden representation\n(before the ﬁnal projection WO) of the query patch is the concatenation of the input features of all\nthe patches in its receptive ﬁeld. Precisely speaking, in Equation 2, we have\nconcat(SA1(X),··· ,SANH (X))q,: = concat(Xq+δ,:)δ∈˜∆. (14)\nIn the convolutional layer deﬁned byWC, the output feature of any pixel in theq-th patch is alinear\nfunction of concat(Xq+δ,:)δ∈˜∆. So the output feature of the whole patch is also a linear function\nof concat(Xq+δ,:)δ∈˜∆. Therefore, there exists a linear projection matrix WO such that\nMHSA(X)q = concat(Xq+δ,:)δ∈˜∆WO = conv(X)q (15)\nMoreover, due to the translation invariance property of the convolution operation, the linear projec-\ntion matrix WO does not depend on q. Therefore, MHSA(X)q = conv(X)q holds for any q. In\nother words, MHSA(X) = conv(X).\nRemark. Indeed, we can presents WO constructively: Assume r ∈ [NH]; s,t ∈ [P2]; i ∈\n[Din]; j ∈ [Dout]. Then WO\n(r−1)d+(s−1)Din+i,(t−1)Dout+j = WC\nx(r,s,t),y(r,s,t),i,j, where\nx(r,s,t),y(r,s,t) ∈[K] ∪{0}are deﬁned as follows:\nLet q be a patch on the image, and let ˜∆ = {δ1,··· ,δNH }. When the s-th pixel in the (q+ δr)-\nth patch is in the receptive ﬁeld of the t-th pixel in the q-th patch, we use (x(r,s,t),y(r,s,t)) to\n12\ndenote its location in the receptive ﬁeld. Otherwise, we let (x(r,s,t),y(r,s,t)) = (0,0), and deﬁne\nWC\nx(r,s,t),y(r,s,t),i,j = 0 in this case.\nThis construction will be useful in our experiment, which requires to transfer the knowledge of a\nconvolutional layer into an MHSA layer (Section 4.1).\nA.3 P ROOF OF THEOREM 6\nProof. We will prove the theorem in the case where Dout = 1 , and consequently the result\nwill hold for any Dout ∈N∗. Furthermore, we assume that Din = 1 since we can set WC\n:,:,2:,: ∈\n0RK×K×(Din−1)×Dout if Din >1. In this way, the convolution computation will ignore all but the\nﬁrst channel.\nAssume the input (sequence of ﬂattened image patches) isX. Recall that a ﬂattened patch is deﬁned\nas the concatenation of the features of all the pixels in it, i.e.,\nXi,: = concat\n(\nXhi1,wi1,:,··· ,XhiP2 ,wiP2 ,:\n)\n(16)\nSince we have assumed that Din = 1, the feature of a pixel Xhip,wip,: is actually a scalar. Thus\n˜Xi,: ∈RP2\n, i.e., d= P2.\nIf the output of the MHSA layer could express convolution, the output representation of a patch must\ncontain the output representations of all the pixels in the convolutional layer. Again, sinceDout = 1,\nwe can assume that the output dimension of the MHSA layer dO = P2. In other words, the output\nrepresentation of a patch is the concatenation of the output representations of all its pixels.\nTherefore, WV\nk WO\nk ∈RP2×P2\n(∀k∈[NH]), and we let WV\nk WO\nk = (wk\npq)p,q∈[P2].\nIn the MHSA layer, assume the attention distribution of query patch γ in the k-th head is ak(γ) =\n(ak\nδ(γ))γ+δ∈[H]×[W], where δ stands for the relative position between the query patch and the key\npatch. Consider the output feature of the pixel at position qin patch γ(qdenotes the location of the\npixel on the patch, and γdenotes the location of the patch on the image). We have\nMHSA(X)γ,q =\nNH∑\nk=1\n∑\nδ\nak\nδ(γ)\nP2\n∑\nq=1\nXγ+δ,qwk\npq (17)\n=\n∑\nδ\nP2\n∑\nq=1\nXγ+δ,q\nNH∑\nk=1\nak\nδ(γ)wk\npq. (18)\nThe above experssion is a linear transformation of X. In the convolutional layer, only pixels in\nthe 9 neighboring patches (including the center patch itself) can be relevant, since P > K. Thus,\nak\nδ(γ) >0 only for δ∈∆ = {−1,0,1}2 := {δ1,··· ,δ9}.\nLet the (ﬂattened) convolutional kernel WC = (wC\n1 ,··· ,wC\nK2 ) ∈RK2\n, and additionally let wC\n0 =\n0. Then for any p,q ∈RP2\n,δ ∈∆, we have\nNH∑\nh=1\nah\nδ(γ)wh\npq = wC\nk(p,q,δ), (19)\nwhere k(p,q,δ ) ∈[K2] ∪{0}is an index dependent on p,q and δ. k(p,q,δ ) ̸= 0 if and only if the\nq-th pixel in the γ + δ-th patch is in the receptive ﬁeld of the p-th pixel in the γ-th patch. When\nk(p,q,δ ) ̸= 0, the value of k(p,q,δ ) only depends on the relative position between the two pixels.\nLet wpq = (w1\npq,··· ,wNH\npq ), and\nW =\n\n\nw11\nw12\n...\nwPP\n\n,A =\n\n\na1\nδ1 ··· a1\nδ9\n... ...\naNH\nδ1\n··· aNH\nδ9\n\n, ˜WC =\n\n\nwC\nk(1,1,δ1) ··· wC\nk(1,1,δ9)\n... ...\nwC\nk(P,P,δ1) ··· wC\nk(P,P,δ9)\n\n. (20)\n13\nThen Eqn 19 can be written in matrix form as WA = ˜WC.\nSince P ≥K, all the column in ˜WC is either a one-hot or a zero vector (pixels at the same position\nin the patches cannot be in the receptive ﬁeld ofone pixel). Besides, none of the 9 rows is zero since\nthey are all needed for the convolution computation. Therefore, we can select 9 columns in ˜WC\nand reorder them properly to form a diagonal sub-matrix of ˜WC, which implies rank( ˜WC) = 9\nas long as all the entries in the convolutional kernel is non-zero.\nOn the other hand, rank(WA) ≤rank(W) ≤NH ≤8, which leads to a contradiction and\nconcludes the proof.\nB D ETAILS ON THE EXPERIMENT SETTINGS\nIn the experiments, we evaluate our CMHSA-3/5 models on CIFAR-100 (Krizhevsky et al., 2009),\nusing the proposed two-phase training pipeline. In both phases, the model is trained for 200 epochs\nwith a 5-epoch warm-up stage followed by a cosine decay learning rate scheduler. In the convolu-\ntional phase, the patch projection layer is ﬁxed as identity.\nTo train our models, we AdamW use as the optimizer, and set its hyperparameter εto 1e−8 and\n(β1,β2) to (0,9,0.999) (Loshchilov & Hutter, 2018). We experiment with peak learning rate in\n{1e−4,3e−4,5e−4}in the convolution phase, and {1e−5,3e−5,5e−5,7e−5}in the self-\nattention phase. The batch size is set to 128 in both phases. We employ all the data augmentation\nand regularization strategies of Touvron et al. (2021), and remain all the relevant hyperparameters\nunmodiﬁed.\nOur codes are implemented based on PyTorch (Paszke et al., 2019) and the timm library (Wight-\nman, 2019). All the models are trained on 4 NVIDIA Tesla V100 GPUs with 16GB memory and\nthe reported training time is also measured on these machines.\n14",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.7940939664840698
    },
    {
      "name": "Transformer",
      "score": 0.7404466271400452
    },
    {
      "name": "Computer science",
      "score": 0.730789065361023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5831085443496704
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5686759352684021
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40430784225463867
    },
    {
      "name": "Computer vision",
      "score": 0.3762663006782532
    },
    {
      "name": "Artificial neural network",
      "score": 0.24203014373779297
    },
    {
      "name": "Engineering",
      "score": 0.10518720746040344
    },
    {
      "name": "Voltage",
      "score": 0.10276240110397339
    },
    {
      "name": "Electrical engineering",
      "score": 0.09925669431686401
    }
  ]
}