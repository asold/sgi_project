{
  "title": "A Multiscale Visualization of Attention in the Transformer Model",
  "url": "https://openalex.org/W2949202705",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2125219408",
      "name": "Jesse Vig",
      "affiliations": [
        "Palo Alto Research Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2610881169",
    "https://openalex.org/W2799118206",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3128232076",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2760327630",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W2890353432",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2962965405"
  ],
  "abstract": "The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37–42\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n37\nA Multiscale Visualization of Attention in the Transformer Model\nJesse Vig\nPalo Alto Research Center\n3333 Coyote Hill Road\nPalo Alto, CA 94304\njesse.vig@parc.com\nAbstract\nThe Transformer is a sequence model that for-\ngoes traditional recurrent architectures in favor\nof a fully attention-based approach. Besides\nimproving performance, an advantage of us-\ning attention is that it can also help to interpret\na model by showing how the model assigns\nweight to different input elements. However,\nthe multi-layer, multi-head attention mecha-\nnism in the Transformer model can be difﬁ-\ncult to decipher. To make the model more ac-\ncessible, we introduce an open-source tool that\nvisualizes attention at multiple scales, each of\nwhich provides a unique perspective on the at-\ntention mechanism. We demonstrate the tool\non BERT and OpenAI GPT-2 and present three\nexample use cases: detecting model bias, lo-\ncating relevant attention heads, and linking\nneurons to model behavior.\n1 Introduction\nIn 2018, the BERT (Bidirectional Encoder Rep-\nresentations from Transformers) language repre-\nsentation model achieved state-of-the-art perfor-\nmance across NLP tasks ranging from sentiment\nanalysis to question answering (Devlin et al.,\n2018). Recently, the OpenAI GPT-2 (Generative\nPretrained Transformer-2) model outperformed\nother models on several language modeling bench-\nmarks in a zero-shot setting (Radford et al., 2019).\nUnderlying BERT and GPT-2 is the Trans-\nformer model, which uses a fully attention-based\napproach in contrast to traditional sequence mod-\nels based on recurrent architectures (Vaswani\net al., 2017). An advantage of using attention\nis that it can help interpret a model by showing\nhow the model assigns weight to different input\nelements (Bahdanau et al., 2015; Belinkov and\nGlass, 2019), although its value in explaining in-\ndividual predictions may be limited (Jain and Wal-\nlace, 2019). Various tools have been developed to\nvisualize attention in NLP models, ranging from\nattention-matrix heatmaps (Bahdanau et al., 2015;\nRush et al., 2015; Rockt ¨aschel et al., 2016) to bi-\npartite graph representations (Liu et al., 2018; Lee\net al., 2017; Strobelt et al., 2018).\nOne challenge for visualizing attention in the\nTransformer is that it uses a multi-layer, multi-\nhead attention mechanism, which produces dif-\nferent attention patterns for each layer and head.\nBERT-Large, for example, which has 24 layers\nand 16 heads, generates 24 ×16 = 384 unique at-\ntention structures for each input. Jones (2017) de-\nsigned a visualization tool speciﬁcally for multi-\nhead attention, which visualizes attention over\nmultiple heads in a layer by superimposing their\nattention patterns (Vaswani et al., 2017, 2018).\nIn this paper, we extend the work of Jones\n(2017) by visualizing attention in the Transformer\nat multiple scales. We introduce a high-level\nmodel view, which visualizes all of the layers and\nattention heads in a single interface, and a low-\nlevel neuron view, which shows how individual\nneurons interact to produce attention. We also\nadapt the tool from the original encoder-decoder\nimplementation to the decoder-only GPT-2 model\nand the encoder-only BERT model.\n2 Visualization Tool\nWe now present a multiscale visualization tool\nfor the Transformer model, available at https:\n//github.com/jessevig/bertviz. The\ntool comprises three views: an attention-head\nview, a model view, and a neuron view. Below, we\ndescribe these views and demonstrate them on the\nGPT-2 and BERT models. We also present three\nuse cases: detecting model bias, locating relevant\nattention heads, and linking neurons to model be-\nhavior. A video demonstration of the tool can be\nfound at https://vimeo.com/340841955.\n38\nFigure 1: Attention-head view for GPT-2, for the input text The quick, brown fox jumps over the lazy. The left and\ncenter ﬁgures represent different layers / attention heads. The right ﬁgure depicts the same layer/head as the center\nﬁgure, but with the token lazy selected.\nFigure 2: Attention-head view for BERT, for inputs the cat sat on the mat(Sentence A) and the cat lay on the rug\n(Sentence B). The left and center ﬁgures represent different layers / attention heads. The right ﬁgure depicts the\nsame layer/head as the center ﬁgure, but with Sentence A→Sentence Bﬁlter selected.\n2.1 Attention-head view\nThe attention-head view visualizes the attention\npatterns produced by one or more attention heads\nin a given layer, as shown in Figure 1 (GPT-21) and\nFigure 2 (BERT2). This view closely follows the\noriginal implementation of Jones (2017), but has\nbeen adapted from the original encoder-decoder\nimplementation to the encoder-only BERT and\ndecoder-only GPT-2 models.\nIn this view, self-attention is represented as lines\nconnecting the tokens that are attending (left) with\nthe tokens being attended to (right). Colors iden-\ntify the corresponding attention head(s), while line\nweight reﬂects the attention score. At the top of\nthe screen, the user can select the layer and one\nor more attention heads (represented by the col-\nored squares). Users may also ﬁlter attention by\n1GPT-2 smallpretrained model.\n2BERT-base, uncasedpretrained model.\ntoken, as shown in Figure 1 (right); in this case\nthe target tokens are also highlighted and shaded\nbased on attention weight. For BERT, which uses\nan explicit sentence-pair model, users may spec-\nify a sentence-level attention ﬁlter; for example, in\nFigure 2 (right), the user has selected theSentence\nA →Sentence Bﬁlter, which only shows attention\nfrom tokens in SentenceA to tokens in SentenceB.\nSince the attention heads do not share param-\neters, each head learns a unique attention mech-\nanism. In the head shown in Figure 1 (left), for\nexample, each word attends to the previous word\nin the sentence. The head in Figure 1 (center),\nin contrast, generates attention that is dispersed\nroughly evenly across previous words in the sen-\ntence (excluding the ﬁrst word). Figure 2 shows\nattention heads for BERT that capture sentence-\npair patterns, including a within-sentence pattern\n(left) and a between-sentence pattern (center).\n39\nFigure 3: Examples of attention heads in GPT-2 that capture speciﬁc lexical patterns: list items (left); verbs\n(center); and acronyms (right). Similar patterns were observed in these attention heads for other inputs. Attention\ndirected toward ﬁrst token is likely null attention (Vig and Belinkov, 2019).\nFigure 4: Attention pattern in GPT-2 related to coreference resolution suggests the model may encode gender bias.\nBesides these coarse positional patterns, atten-\ntion heads also capture speciﬁc lexical patterns,\nsuch as those as shown in Figure 3. Other atten-\ntion heads detected named entities (people, places,\ncompanies), paired punctuation (quotes, brack-\nets, parentheses), subject-verb pairs, and other\nsyntactic and semantic relations. Recent work\nshows that attention in the Transformer corre-\nlates with syntactic constructs such as dependency\nrelations and part-of-speech tags (Raganato and\nTiedemann, 2018; V oita et al., 2019; Vig and Be-\nlinkov, 2019).\nUse Case: Detecting Model Bias\nOne use case for the attention-head view is de-\ntecting bias in the model, which we illustrate for\nthe case of conditional language generation using\nGPT-2. Consider the following continuations gen-\nerated3 from two input prompts that are identical\nexcept for the gender of the pronouns (generated\ntext underlined):\n•The doctor asked the nurse a question. She\nsaid, “I’m not sure what you’re talking about.”\n•The doctor asked the nurse a question. He\nasked her if she ever had a heart attack.\nIn the ﬁrst example, the model generates a con-\ntinuation that implies She refers to nurse. In the\nsecond example, the model generates text that\nimplies He refers to doctor. This suggests that\nthe model’s coreference mechanism may encode\ngender bias (Zhao et al., 2018; Lu et al., 2018).\nFigure 4 shows an attention head that appears to\n3Using GPT-2 small model with greedy decoding.\n40\nperform coreference resolution based on the per-\nceived gender of certain words. The two examples\nfrom above are shown in Figure 4 (right), which\nreveals thatShe strongly attends tonurse, while He\nattends more to doctor. By identifying a source of\npotential model bias, the tool could inform efforts\nto detect and control for this bias.\n2.2 Model View\nThe model view (Figure 5) provides a birds-eye\nview of attention across all of the model’s lay-\ners and heads for a particular input. Attention\nheads are presented in tabular form, with rows rep-\nresenting layers and columns representing heads.\nEach layer/head is visualized in a thumbnail form\nthat conveys the coarse shape of the attention pat-\ntern, following the small multiplesdesign pattern\n(Tufte, 1990). Users may also click on any head to\nenlarge it and see the tokens.\nFigure 5: Model view of BERT, for same inputs as in\nFigure 2. Excludes layers 4–11 and heads 6–11.\nThe model view enables users to quickly browse\nthe attention heads across all layers and to see how\nattention patterns evolve throughout the model.\nUse Case: Locating Relevant Attention Heads\nAs discussed earlier, attention heads in BERT ex-\nhibit a broad range of behaviors, and some may be\nmore relevant for model interpretation than oth-\ners depending on the task. Consider the case of\nparaphrase detection, which seeks to determine if\ntwo input texts have the same meaning. For this\ntask, it may be useful to know which words the\nmodel ﬁnds similar (or different) between the two\nsentences. Attention heads that draw connections\nbetween input sentences would thus be highly rel-\nevant. The model view (Figure 5) makes it easy to\nﬁnd these inter-sentence patterns, which are rec-\nognizable by their cross-hatch shape (e.g., layer 3,\nhead 0). These heads can be further explored by\nclicking on them or accessing the attention-head\nview, e.g., Figure 2 (center). This use case is de-\nscribed in greater detail in Vig (2019).\n2.3 Neuron View\nThe neuron view (Figure 6) visualizes the in-\ndividual neurons in the query and key vectors\nand shows how they interact to produce attention.\nGiven a token selected by the user (left), this view\ntraces the computation of attention from that token\nto the other tokens in the sequence (right).\nNote that the Transformer uses scaled dot-\nproduct attention, where the attention distribution\nat position iin a sequence xis deﬁned as follows:\nαi = softmax\n(qi ·k1√\nd\n,qi ·k2√\nd\n,..., qi ·kN√\nd\n)\n(1)\nwhere qi is the query vector at position i, kj is the\nkey vector at position j, and dis the dimension of\nkand q. N=ifor GPT-2 andN=len(x) for BERT.4\nAll values are speciﬁc to a particular layer / head.\nThe columns in the visualization are deﬁned as\nfollows:\n•Query q: The query vector of the selected\ntoken that is paying attention.\n•Key k: The key vector of each token receiv-\ning attention.\n•q×k (element-wise) : The element-wise\nproduct of the query vector and each key vec-\ntor. This shows how individual neurons con-\ntribute to the dot product (sum of element-\nwise product) and hence attention.\n•q ·k: The dot product of the selected token’s\nquery vector and each key vector.\n•Softmax: The softmax of the scaled dot-\nproduct from previous column. This is the\nattention score.\nWhereas the attention-head view and the model\nview show what attention patterns the model\nlearns, the neuron view shows how the model\nforms these patterns. For example, it can help\nidentify neurons responsible for speciﬁc attention\npatterns, as discussed in the following use case.\n4GPT-2 only considers the context up to position i, while\nBERT considers the entire sequence.\n41\nFigure 6: Neuron view of BERT for layer 0, head 0 (same one depicted in Figure 2, left). Positive and negative\nvalues are colored blue and orange, respectively, with color saturation based on magnitude of the value. As with\nthe attention-head view, connecting lines are weighted based on attention between the words.\nFigure 7: Neuron view of GPT-2 for layer 1, head 10 (same one depicted in Figure 1, center) with last token\nselected. Blue arrows mark positions in the element-wise products where values decrease with increasing distance\nfrom the source token (becoming darker orange or lighter blue).\nUse Case: Linking Neurons to Model Behavior\nTo see how the neuron view might provide ac-\ntionable insights, consider the attention head in\nFigure 7. For this head, the attention (rightmost\ncolumn) decays with increasing distance from the\nsource token. This pattern resembles a context\nwindow, but instead of having a ﬁxed cutoff, the\nattention decays continuously with distance.\nThe neuron view provides two key insights\nabout this attention head. First, the attention\nweights appear to be largely independent of the\ncontent of the input text, based on the fact that\nall the query vectors have very similar values (ex-\ncept for the ﬁrst token). Second, a small number\nof neuron positions (highlighted with blue arrows)\nappear to be mostly responsible for this distance-\ndecaying attention pattern. At these neuron posi-\ntions, the element-wise product q×kdecreases as\nthe distance from the source token increases (ei-\nther becoming darker orange or lighter blue).\n42\nWhen speciﬁc neurons are linked to a tangi-\nble outcome, it presents an opportunity to inter-\nvene in the model (Bau et al., 2019). By altering\nthe relevant neurons—or by modifying the model\nweights that determine these neuron values—one\ncould control the attention decay rate, which might\nbe useful when generating texts of varying com-\nplexity. For example, one might prefer a slower\ndecay rate (longer context window) for a scientiﬁc\ntext compared to a children’s story. Other heads\nmay afford different types of interventions.\n3 Conclusion\nIn this paper, we introduced a tool for visualizing\nattention in the Transformer at multiple scales. We\ndemonstrated the tool on GPT-2 and BERT, and\nwe presented three use cases. For future work, we\nwould like to develop a uniﬁed interface to nav-\nigate all three views within the tool. We would\nalso like to expose other components of the model,\nsuch as the value vectors and state activations. Fi-\nnally, we would like to enable users to manipu-\nlate the model, either by modifying attention (Lee\net al., 2017; Liu et al., 2018; Strobelt et al., 2018)\nor editing individual neurons (Bau et al., 2019).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. ICLR.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2019. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In Proc. ICLR.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. ArXiv Computation and Language.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. CoRR, abs/1902.10186.\nLlion Jones. 2017. Tensor2tensor transformer\nvisualization. https://github.com/\ntensorflow/tensor2tensor/tree/\nmaster/tensor2tensor/visualization.\nJaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim.\n2017. Interactive visualization and manipulation\nof attention-based neural machine translation. In\nEMNLP: System Demonstrations.\nShusen Liu, Tao Li, Zhimin Li, Vivek Srikumar, Vale-\nrio Pascucci, and Peer-Timo Bremer. 2018. Visual\ninterrogation of attention-based models for natural\nlanguage inference and machine comprehension. In\nEMNLP: System Demonstrations.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2018. Gender bias\nin neural natural language processing. CoRR,\nabs/1807.11714.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In EMNLP Workshop:\nBlackboxNLP.\nTim Rockt ¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tomas Kocisky, and Phil Blunsom. 2016.\nReasoning about entailment with neural attention.\nIn Proc. ICLR.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proc. EMNLP.\nH. Strobelt, S. Gehrmann, M. Behrisch, A. Perer,\nH. Pﬁster, and A. M. Rush. 2018. Seq2Seq-Vis:\nA Visual Debugging Tool for Sequence-to-Sequence\nModels. ArXiv e-prints.\nEdward Tufte. 1990. Envisioning Information. Graph-\nics Press, Cheshire, CT, USA.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2tensor for neural machine\ntranslation. CoRR, abs/1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nJesse Vig. 2019. BertViz: A tool for visualizing multi-\nhead self-attention in the BERT model. In ICLR\nWorkshop: Debugging Machine Learning Models.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In ACL Workshop: BlackboxNLP.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In NAACL-HLT.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8011891841888428
    },
    {
      "name": "Computer science",
      "score": 0.7673357129096985
    },
    {
      "name": "Visualization",
      "score": 0.6320752501487732
    },
    {
      "name": "DECIPHER",
      "score": 0.6152623891830444
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48891082406044006
    },
    {
      "name": "Machine learning",
      "score": 0.35483184456825256
    },
    {
      "name": "Engineering",
      "score": 0.11706987023353577
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}