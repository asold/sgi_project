{
    "title": "Training Vision Transformers for Image Retrieval",
    "url": "https://openalex.org/W3128099838",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4222055945",
            "name": "El-Nouby, Alaaeldin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222398475",
            "name": "Neverova, Natalia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742358871",
            "name": "Laptev, Ivan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2296768883",
            "name": "Jégou, Hervé",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1908016767",
        "https://openalex.org/W2991234496",
        "https://openalex.org/W3011927872",
        "https://openalex.org/W2605102252",
        "https://openalex.org/W1797268635",
        "https://openalex.org/W2144935315",
        "https://openalex.org/W2963350250",
        "https://openalex.org/W2895347732",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3098656025",
        "https://openalex.org/W2991645104",
        "https://openalex.org/W2964316188",
        "https://openalex.org/W2885201931",
        "https://openalex.org/W2991581349",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W3080750010",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2955488837",
        "https://openalex.org/W2964271799",
        "https://openalex.org/W2148809531",
        "https://openalex.org/W2141362318",
        "https://openalex.org/W2963166708",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2965158072",
        "https://openalex.org/W1833123814",
        "https://openalex.org/W2953271441",
        "https://openalex.org/W2471768434",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2106053110",
        "https://openalex.org/W2962708773",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3118734924",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2340690086",
        "https://openalex.org/W2961224374",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3027758526",
        "https://openalex.org/W2963588253",
        "https://openalex.org/W2990519439",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2948303601",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W3106778652",
        "https://openalex.org/W2963154697",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963026686",
        "https://openalex.org/W2962723992",
        "https://openalex.org/W2555897561",
        "https://openalex.org/W204268067",
        "https://openalex.org/W2138621090",
        "https://openalex.org/W2964121744"
    ],
    "abstract": "Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.",
    "full_text": "Training Vision Transformers for Image Retrieval\nAlaaeldin El-Nouby1 2 Natalia Neverova1 Ivan Laptev2 Herv´e J´egou 1\nAbstract\nTransformers have shown outstanding results for\nnatural language understanding and, more re-\ncently, for image classiﬁcation. We here extend\nthis work and propose a transformer-based ap-\nproach for image retrieval: we adopt vision trans-\nformers for generating image descriptors and train\nthe resulting model with a metric learning objec-\ntive, which combines a contrastive loss with a\ndifferential entropy regularizer.\nOur results show consistent and signiﬁcant im-\nprovements of transformers over convolution-\nbased approaches. In particular, our method out-\nperforms the state of the art on several public\nbenchmarks for category-level retrieval, namely\nStanford Online Product, In-Shop and CUB-200.\nFurthermore, our experiments on ROxford and\nRParis also show that, in comparable settings,\ntransformers are competitive for particular object\nretrieval, especially in the regime of short vector\nrepresentations and low-resolution images.\n1. Introduction\nOne of the fundamental skills in reasoning is the ability to\npredict similarity between entities even if such entities have\nnot been observed before. In the context of computer vision,\nlearning similarity metric has many direct applications such\nas content-based image retrieval, face recognition and per-\nson re-identiﬁcation. It is also a key component of many\nother computer vision tasks like zero-shot and few-shot\nlearning. More recently, advances in metric learning have\nbeen essential to the progress of self-supervised learning,\nwhich relies on matching two images up to data augmenta-\ntion as a learning paradigm.\nModern methods for image retrieval typically rely on con-\nvolutional encoders and extract compact image-level de-\nscriptors. Some early approaches used activations provided\nby off-the-shelf pre-trained models (Babenko et al., 2014).\nHowever, models trained speciﬁcally for the image retrieval\n1Facebook AI 2ENS/Inria. Correspondence to: Alaaeldin El-\nNouby <aelnouby@fb.com>.\ncontrastive loss +\ndifferential entropyregularization\nCLS CLS \ntransformerencodertransformerencoder\npatchify  + projectpatchify  + projectinput tokensinput tokens\noutputtokensoutputtokens\n=\nCLS \npatchify  &project\ninput tokens\nvisiontransformer\noutputtokens\ninput A\nCLS \nvisiontransformer\ninput B\npatchify  &project\ndescriptor Adescriptor B\noutputspace\nFigure 1.We train a transformer model with a Siamese architec-\nture for image retrieval. Two input images are mapped by the\ntransformers into a common feature space. At training time, the\ncontrastive loss is augmented with an entropy regularizer.\ntask achieve better performance (Radenovi´c et al., 2018b;\nTeh et al., 2020; Wang et al., 2020). A number of efﬁcient ob-\njective functions have been proposed to penalize the discrep-\nancy between computed similarities and the ground truth.\nIn addition, research has been focused on improvements of\nsampling methods and data augmentation strategies.\nThe transformer architecture by Vaswani et al. (2017) has\nbeen successfully used for a number of NLP tasks (Devlin\net al., 2018; Radford et al., 2018), and more recently in the\ncore computer vision task of image classiﬁcation (Dosovit-\nskiy et al., 2021; Touvron et al., 2020). This is an interesting\ndevelopment as transformer-based models adapted for com-\nputer vision come with a different set of inductive biases\ncompared to the currently dominant convolutional architec-\ntures. This suggests that such models may ﬁnd alternative\nsolutions and avoid errors that are typical for convolutional\nbackbones. While there have been some efforts exploring\nattention-based metric learning for images (Kim et al., 2018;\nChen & Deng, 2019), to our knowledge the adoption of a\nplain transformer has not been studied in this context.\nIn this work, we introduce and study Image Retrieval Trans-\nformers (IRT). As illustrated in Figure 1, our IRT model\ninstantiates a Siamese architecture with a transformer back-\nbone. We investigate the adaptation of metric learning tech-\nniques and evaluate how they interplay with transformers.\nIn particular, we adopt a contrastive loss (Hadsell et al.,\narXiv:2102.05644v1  [cs.CV]  10 Feb 2021\nTraining Vision Transformers for Image Retrieval\n2006), which has recently been reafﬁrmed as a very effec-\ntive metric learning objective (Musgrave et al., 2020; Wang\net al., 2020). We also employ a differential entropy regular-\nization that favors uniformity over the representation space\nand improves performance.\nWe perform an extensive experimental evaluation and vali-\ndate our approach by considering two image retrieval tasks.\nFirst, we investigate the task of category-level image re-\ntrieval, which is often used to measure the progress in met-\nric learning (Teh et al., 2020; Musgrave et al., 2020). We\nalso explore retrieval of particular objects, and compare our\nmethod to convolutional baselines in similar settings (same\nresolution and similar complexity).\nThe main contributions of this work are listed below.\n• We propose a simple way to train vision transformers both\nfor category-based level and particular object retrieval,\nand achieve competitive performance when compared to\nconvolutional models with similar capacity.\n• As a result, we establish the new state of the art on three\npopular benchmarks for category-level retrieval.\n• For particular object retrieval, in the regime of short-\nvector representation (128 components), our results on\nROxford and RParis are comparable to those of convnets\noperating at a much higher resolution and FLOPS.\n• We show that the differential entropy regularizer enhances\nthe contrastive loss and improves the performance overall.\n2. Related Work\nTransformers. The transformer architecture was intro-\nduced by Vaswani et al. (2017) for machine translation. It\nsolely relies on self-attention and fully-connected layers,\nand achieving an attractive trade-off between efﬁciency and\nperformance. It has subsequently provided state-of-the-art\nperformance for several NLP tasks (Devlin et al., 2018; Rad-\nford et al., 2018). In computer vision, several attempts have\nbeen devoted to incorporate various forms of attention, for\ninstance in conjunction (Wang et al., 2018) or as a replace-\nment to convolution (Ramachandran et al., 2019). Other\nmethods utilize transformer layers on top of convolutional\ntrunks (Carion et al., 2020) for detection.\nMore recently, convolution-free models that only rely on\ntransformer layers have shown competitive performance\n(Chen et al., 2020; Dosovitskiy et al., 2021; Touvron et al.,\n2020), positioning it as a possible alternative to convolu-\ntional architectures. In particular, the Vision Transformers\n(ViT) model proposed by Dosovitskiy et al. (2021) is the ﬁrst\nexample of a transformer-based method to match or even\nsurpass state-of-the-art convolutional models on the task of\nimage classiﬁcation. Touvron et al. (2020) subsequently im-\nproved the optimization procedure, leading to competitive\nresults with ImageNet-only training (Deng et al., 2009).\nMetric Learning. A ﬁrst class of deep metric learning\nmethods is based on classiﬁcation: these approaches rep-\nresent each category using one (Movshovitz-Attias et al.,\n2017; Teh et al., 2020; Zhai & Wu, 2018; Boudiaf et al.,\n2020) or multiple prototypes (Qian et al., 2019). The simi-\nlarity and dissimilarity training signal is computed against\nthe prototypes rather than between individual instances. An-\nother class of methods operate on pairs methods: the training\nsignal is deﬁned by similarity/dissimilarity between indi-\nvidual instances directly. A contrastive loss (Hadsell et al.,\n2006) aims to push representations of positive pairs closer\ntogether, while representations of negative pairs are encour-\naged to have larger distance. The triplet loss (Weinberger &\nSaul, 2009) builds on the same idea but requires the positive\npair to be closer than a negative pair by a ﬁxed margin given\nthe same anchor. Wu et al. (2017) proposes negative sam-\npling weighted by pair-wise distance to emphasize harder\nnegative examples. Other pair-based losses rely on the soft-\nmax function (Goldberger et al., 2005; Sohn, 2016; Wu et al.,\n2018; Wang et al., 2019), allowing for more comparisons\nbetween different positive and negative pairs.\nWhile a vanilla contrastive loss has been regarded to have a\nweaker performance when compared to its successors like\ntriplet and margin (Wu et al., 2017) losses, recent efforts\n(Musgrave et al., 2020) showed that a careful implemen-\ntation of the contrastive loss leads to results outperform-\ning many more sophisticated losses. Additionally, Wang\net al. (2020) showed that when augmented with an exter-\nnal memory to allow sampling of a sufﬁcient number of\nhard negatives, contrastive loss achieves a state-of-the-art\nperformance on multiple image retrieval benchmarks.\nParticular Image Retrieval has progressively evolved\nfrom methods based on local descriptors to convolutional\nencoders. In this context, an important design choice is\nhow to compress the spatial feature maps of activations into\na vector-shaped descriptor (Babenko & Lempitsky, 2015;\nTolias et al., 2015). Subsequent works have adopted end-to-\nend training (Gordo et al., 2016; Radenovi ´c et al., 2018b;\nRevaud et al., 2019) with various forms of supervision. In a\nconcurrent work, Gkelios et al. (2021) investigated off-the-\nshelf pre-trained ViT models for particular image retrieval.\nDifferential Entropy Regularization. Zhang et al.\n(2017) aim a better utilization of the space by spreading\nout the descriptors through matching ﬁrst and second mo-\nments of non-matching pairs with points uniformly sampled\non the sphere. Wang & Isola (2020) provide a theoretical\nanalysis for contrastive representation learning in terms of\nalignment and uniformity on the hypersphere. In the context\nof face recognition, Duan et al. (2019) argue for spreading\nthe class centers uniformly in the manifold, while Zhao et al.\n(2019) minimize the angle between a class center and its\nnearest neighbor in order to improve inter-class separability.\nTraining Vision Transformers for Image Retrieval\nWe focus our study on pairwise losses, the contrastive loss in\nparticular, aiming to prevent the collapse in dimensions that\nhappens as a byproduct of adopting such an objective. Most\nrelated to out method, Sablayrolles et al. (2019) propose a\ndifferential entropy regularization based on the estimator\nby Kozachenko & Leonenko (1987), in order to spread\nthe vectors on the hypersphere more uniformly, such that\nit enables improved lattice-based quantization properties.\nBell et al. (2020) adopted it as an efﬁcient way to binarize\nfeatures output by convnets in commerce applications.\n3. Methods\nIn this section, after reviewing the transformer architecture,\nwe detail how we adapt it to the category-level and particular\nobject retrieval. Note, in the literature these tasks have\nbeen tackled by distinct techniques. In our case we use the\nsame approach for both of these problems. We gradually\nintroduce its different components, as follows:\n• IRTO – off-the-shelf extraction of features from a ViT\nbackbone, pre-trained on ImageNet;\n• IRTL – ﬁne-tuning a transformer with metric learning, in\nparticular with a contrastive loss;\n• IRTR – additionally regularizing the output feature space\nto encourage uniformity.\n3.1. Preliminaries: Vision Transformer\nLet us review the main building blocks for transformer-\nbased models, and more speciﬁcally of the recently pro-\nposed ViT architecture by Dosovitskiy et al. (2021). The\ninput image is ﬁrst decomposed into M ﬁxed-sized patches\n(e.g. 16 ×16). Each patch is linearly projected into M\nvector-shaped tokens and used as an input to the transformer\nin a permutation-invariant manner. The location prior is\nincorporated by adding a learnable 1-D positional encoding\nvector to the input tokens. An extra learnable CLS token\nis added to the input sequence such that its corresponding\noutput token serves as a global image representation.\nThe transformer consists of Llayers, each of which is com-\nposed of two main blocks: a Multi-Headed Self Attention\n(MSA) layer, which applies a self-attention operation to dif-\nferent projections of the input tokens, and a Feed-Forward\nNetwork (FFN). Both the MSA and FFN layers are preceded\nby layer normalization and followed by a skip connection.\nWe refer the reader to Dosovitskiy et al. (2021) for details.\nArchitectures. Table 1 presents the neural networks mod-\nels used through this paper. They are all pre-trained on\nImageNet1k (Deng et al., 2009) only. In order to have a fair\ncomparison with other retrieval methods, we choose to use\nthe DeiT-Small variant of the ViT architecture introduced by\nTouvron et al. (2020) as our primary model. The DeiT-Small\nmodel has a relatively compact size which makes it compa-\nTable 1.Parameters count, FLOPS and ImageNet Top-1 accuracy\nfor convolutional baselines ResNet-50 (R50) and ResNet-101\n(R101) at resolution 224x224, as well as transformer-based mod-\nels: DeiT-Small (DeiT-S) and DeiT-Base (DeiT-B) (Touvron et al.,\n2020). †: Models pre-trained with distillation with a convnet\ntrained on ImageNet1k.\nModel # params FLOPS (G) Top-1 (%)\nR50 23M 8.3 76.2\nDeiT-S 22M 8.4 79.8\nDeiT-S† 22M 8.5 81.1\nR101 46M 15.7 77.4\nDeiT-B 87M 33.7 81.8\nDeiT-B† 87M 33.8 83.9\nrable to the widely adopted ResNet-50 convolutional model\n(He et al., 2016) in terms of parameters count and FLOPS,\nas shown in Table 1. Additionally, we provide some analysis\nand results of larger models like ResNet-101 and DeiT-Base,\nas well as DeiT variants with advanced pre-training.\n3.2. IRTO: off-the-shelf features with Transformers\nWe ﬁrst consider the naive approach IRTO, where we ex-\ntract features directly from a transformer pre-trained on\nImageNet. This strategy is in line with early works on im-\nage retrieval with convolutional networks (Babenko et al.,\n2014), which were featurizing activations.\nPooling. We extract a compact vector descriptor that rep-\nresents the image globally. In the ViT architecture, pre-\nclassiﬁcation layers output M + 1vectors corresponding to\nM input patches and a class (CLS) embedding.\nIn our referent pooling approach, CLS, we follow the spirit\nof BERT (Devlin et al., 2018) and ViT models, and view this\nclass embedding as a global image descriptor. In addition,\nwe investigate performance of global pooling methods that\nare typically used by convolutional metric learning models,\nincluding average, maximum and Generalized Mean (GeM)\npooling, and apply them to the M output tokens.\nl2-Normalization and Dimensionality Reduction. We\nfollow the common practice of projecting the descriptor\nvector into a unit ball after pooling. In the case when the\ntarget dimensionality is smaller than that provided by the\narchitecture, we optionally reduce the vector by principal\ncomponent analysis (PCA) before normalizing it.\n3.3. IRTL: Learning the Metric for Image Retrieval\nWe now consider a metric learning approach for image re-\ntrieval, denoted by IRTL. It is the dominant approach to\nboth category-level and particular object retrieval. In our\ncase we combine it with transformers instead of convolu-\ntional neural networks. We adopt the contrastive loss with\ncross-batch memory by Wang et al. (2020) and ﬁx the mar-\ngin β= 0.5 by default for our metric learning objective.\nTraining Vision Transformers for Image Retrieval\nThe contrastive loss maximizes the similarity between en-\ncoded low-dimensional representations zi of samples with\nthe same label y(or any other pre-deﬁned similarity rule).\nSimultaneously, it minimizes the similarity between rep-\nresentations of samples with unmatched labels which are\nreferred to as negatives. For the contrastive loss, only neg-\native pairs with a similarity higher than a constant margin\nβ contribute to the loss. This prevents the training signal\nfrom being overwhelmed by easy negatives. Formally, the\ncontrastive loss over a batch of size N is deﬁned as:\nLcontr. = 1\nN\nN∑\ni\n\n ∑\nj:yi=yj\n[\n1 −zT\ni zj\n]\n+\n∑\nj:yi̸ =yj\n[\nzT\ni zj −β\n]\n+\n\n. (1)\nThe representations zi are assumed to be l2-normalized,\ntherefore the inner product is equivalent to cosine similarity.\n3.4. IRTR: Differential Entropy Regularization\nRecently, (Boudiaf et al., 2020) studied connections between\na group of pairwise losses and maximization of mutual in-\nformation between learned representations Z = {zi}and\ncorresponding ground-truth labels Y = {yi}. We are in-\nterested in the particular case of the contrastive loss. The\nmutual information is deﬁned as\nI(Z,Y ) =H(Z) −H(Z|Y). (2)\nThe positive term of the contrastive loss leads to minimiza-\ntion of the conditional differential entropy H(Z|Y), where\nintuitively, samples representations belonging to the same\ncategory are trained to be more similar:\nH(Z|Y) ∝ 1\nN\nN∑\ni\n∑\nj:yi=yj\n[\n1 −zT\ni zj\n]\n. (3)\nOn the other hand, the negative term of this loss is respon-\nsible for preventing trivial solutions where all sample rep-\nresentations are collapsed to a single point. Therefore, it\nmaximizes the entropy of the learned representations:\nH(Z) ∝− 1\nN\nN∑\ni\n∑\nj:yi̸ =yj\n[zT\ni zj −β]+. (4)\nThe margin β plays an important role in the training dy-\nnamics. Low values of β allow exploration of a larger\nnumber of negative samples. Yet in this case easy nega-\ntives can dominate the training and cause the performance\nto plateau. In contrast, higher values of β would only ac-\ncept hard negatives, possibly leading to noisy gradients and\nunstable training (Wu et al., 2017).\nOur regularizer. Motivated by the entropy maximization\nview of the negative contrastive term in Equation 4, we add\nan entropy maximization term that is independent of the\nnegative samples accepted by the margin. In particular, we\nuse the differential entropy loss proposed by Sablayrolles\net al. (2019). It is based on the Kozachenko & Leonenko\n(1987) differential entropy estimator:\nLKoLeo = −1\nN\nN∑\ni\nlog(ρi), (5)\nwhere ρi = mini̸=j∥zi −zj∥. In other words, this regular-\nization maximizes the distance between every point and its\nnearest neighbor, and therefore alleviates the collapse issue.\nWe simply add the regularization term to the contrastive\nloss weighted by a regularization strength coefﬁcient λ:\nL= Lcontr. + λLKoLeo.\nIntuitively, the different entropy regularization prevents the\nrepresentations of different samples from lying too close on\nthe hypersphere, by increasing their distance from positive\nexamples, and the hard negatives as well. Having hard\nnegatives with extremely small distances is a main source of\nnoise in the training signal, as identiﬁed by Wu et al. (2017).\n3.5. Analysis\nWe study the behaviour of the output representation space\nwhen training with a contrastive loss, and how augmenting\nthis loss with a differential entropy regularization impacts\nthe space properties and the model performance.\nPCA in the Embedding Space.In Figure 2 we examine\nthe cumulative energy of the principle components for fea-\ntures from an off-the-shelf, ImageNet pre-trained model, as\nwell as models trained using contrastive loss. We observe\nthat the features after training with the contrastive loss suf-\nfer from a collapse in dimensions compared to an untrained\nmodel. This suggests an ineffective use of the represen-\ntational capacity of the embedding space, as alignment is\nfavored over uniformity while both are necessary for good\nrepresentations (Wang & Isola, 2020). As we augment the\ncontrastive loss with the differential entropy regularization,\nthe cumulative energy spreads across more dimensions (see\nFigure 2 with non-zero values of λ). Higher values of λ\nalleviate the dimensionality collapse problem.\nAnother observation is that the transformer-based architec-\nture is less impacted than convnets by the collapse (see\nFigure 3). Despite having a lower extrinsic dimensionality\ncompared to the ResNet-50 model, the DeiT-Small features\nare more spread over principle components. A possible rea-\nson for that behavior is that in multi-headed attention, each\ninput feature is projected to different sub-spaces before the\nattention operation, reducing the risk of collapse.\nGradient Analysis. As pointed out by Wu et al. (2017),\nvery hard negatives can lead to noisy gradients. We exam-\nine the nuclear norm ∥·∥∗associated with the covariance\nmatrix of the gradients directions γ = ∥Cov(∇zLcontr.)∥∗,\naveraged over all training iterations (see Figure 4). Higher\nvalues of γcould indicate noisy gradients. We observe them\nfor both very high and very low values of marginβwhich\nTraining Vision Transformers for Image Retrieval\n0 500 1000 1500 2000\nPrinciple Components\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Cumulative Energy\n= 0.0\n= 0.3\n= 0.7\n= 1.0\nNo Training\nFigure 2.Cumulative energy of the principle\ncomponents for features extracted using a\nResNet-50 backbone from the SOP dataset,\nwith pre-training on ImageNet, with (red) or\nwithout (blue) ﬁnetuning. The solid red line\nindicates the vanilla contrastive loss with\nβ = 0.5. The features have collapsed to few\ndimensions after training, but the collapse is\nreduced by entropy regularization.\n21 23 25 27 29 211\nPrinciple Components\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Energy\nDeiT-S384 ( = 0.0)\nDeiT-S384 ( = 1.0)\nR502048  ( = 0.0)\nR502048  ( = 1.0)\nFigure 3.Cumulative energy of the principle\ncomponents for ResNet-50 and DeiT-Small\nmodels trained using a margin of β = 0.5.\nWe can see that despite the lower extrinsic\ndimensionality of DeiT-S (384 ≈28.5), it\nhas higher intrinsic dimensionalities than\nResNet-50 after training. This suggests that\nthe transformer-based architectures can be\nmore robust against the feature collapse.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7\nNegative Margin \n3.0\n3.5\n4.0\n4.5|Cov( z contrastive)|*\n1e 4\n= 0.0\n= 0.3\n= 0.7\n= 1.0\nFigure 4.The nuclear norm for the covari-\nance of the gradient direction, computed for\ndifferent margin values βand entropy regu-\nlarization strengths λ. The gradient signal\nis noisier for very high and very low margin\nvalues. The LKoLeo regularization provides\na more stable gradient signal, but, in turn,\nbecomes harmful with high values of λ(e.g.\nλ= 1.0,β = 0.7).\naligns with our understanding that very easy and very hard\nnegatives lead to less informative and less stable training\nsignal. Moreover, we observe a decrease in the γ values\nafter the addition of the entropy regularization term.\n4. Experiments & Ablation Studies\nWe ﬁrst describe datasets and implementation details, and\nthen proceed with discussions of empirical results.\n4.1. Datasets\nCategory-level Retrieval. We report performance on\nthree popular datasets commonly used for category-level\nretrieval. Stanford Online Products (SOP) (Oh Song\net al., 2016) consists of online products images representing\n22,634 categories. Following the split proposed by Oh Song\net al. (2016), we use ﬁrst 11,318 categories for training\nand the remaining 11,316 for testing. CUB-200-2011 (Wah\net al., 2011) contains 11,788 images corresponding to 200\nbird categories. Following Wah et al. (2011), we split this\ndataset into two class-disjoint sets, each with 100 categories\nfor training and testing. In-Shop (Liu et al., 2016) con-\ntains 72,712 images of clothing items belonging to 7,986\ncategories, 3,997 of which used for training. The remain-\ning 3,985 categories are split into 14,218 query and 12,612\ngallery images for testing. We compute the Recall@K evalu-\nation metric for a direct comparison with previous methods.\nParticular Object Retrieval. For training, we use the\nSFM120k dataset (Radenovi´c et al., 2018b) which is ob-\ntained by applying structure-from-motion and 3D recon-\nstruction to large unlabelled image collections (Schonberger\net al., 2015). The positive images are selected such that\nenough 3D points are co-observed with the query image,\nwhile negative images come from different 3D models. We\nuse 551 3D models for training and 162 for validation.\nFor evaluation, we report results using revisited benchmarks\n(Radenovi´c et al., 2018a) of the Oxford and Paris (Philbin\net al., 2007; 2008) datasets. These two datasets each contain\n70 query images depicting buildings, and additionally in-\nclude 4993 and 6322 images respectively in which the same\nquery buildings may appear. The revisited benchmarks con-\ntain 3 splits: Easy (E), Medium (M) and Hard (H), grouped\nby gradual difﬁculty of query/database pairs. (E) ignores\nhard queries, (M) includes both easy and hard ones, while\n(H) considers hard queries only. We report the Mean Aver-\nage Precision (mAP) for the Medium and Hard splits in all\nour experiments.\n4.2. Implementation & Training Details\nCategory-level Retrieval. The transformer-based models\nand their pre-trained weights are based on the public imple-\nmentation1 of DeiT (Touvron et al., 2020) built upon the\nTimm library by Wightman (2019). All models are opti-\nmized using the AdamW optimizer (Loshchilov & Hutter,\n2017) with learning rate 3.10−5, weight decay 5.10−4 and\nbatch size of 64. For all experiments, unless mentioned\notherwise, the contrastive loss margin is set to β = 0.5 and\nthe entropy regularization strength is set to λ = 0.7. We\nshow later in ablation that the results are relatively stable\n(and not overﬁtted) to this hyper-parameter setting. We use\nstandard data augmentation methods of resizing the image\nto 256×256 and then taking a random crop of size 224×224,\n1https://github.com/facebookresearch/deit\nTraining Vision Transformers for Image Retrieval\n0.4\n 0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity\nPositive Pairs\nNegative Pairs\n(a) IRTO: off-the-shelf descriptors.\n0.4\n 0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity\nPositive Pairs\nNegative Pairs (b) IRTL: ﬁnetuned descriptors.\n0.4\n 0.2\n 0.0 0.2 0.4 0.6 0.8 1.0\nCosine Similarity\nPositive Pairs\nNegative Pairs (c) IRTR: ﬁnetuned with λ= 4.0.\nFigure 5.We present histograms for the cosine similarities belonging to positive and negative pairs for three DeiT-Small variants. The\ndescriptors are extracted from the RParis (M) dataset with input images of size 224×224 and descriptor dimensionality of 256-d for all\nmodels. Extracting features using off-the-shelf features (left) results in a highly overlapping positive and negative distribution. Finetuning\nthe model using contrastive loss (middle) reduces the overlap signiﬁcantly. However, the descriptors concentrate relatively close to each\nother, not taking full advantage of the embedding space. Augmenting the contrastive loss with entropy regularization (right) results in a\nmore uniformly spread distribution of descriptors and a better separation of positive and negative pairs based on absolute similarities.\ncombined with random horizontal ﬂipping. Following Wang\net al. (2020), we use a dynamic ofﬂine memory queue of\nthe same size as the dataset (with the exception of In-Shop\ndataset for which the memory size is 0.2 of the dataset size).\nAdditionally, for the In-Shop dataset we adopt a momen-\ntum encoder for the memory entries (similarly to He et al.\n(2020)) with momentum value of 0.999. We have found\nthis was not necessary for SOP and CUB-200-2011. Finally,\nSOP and In-Shop models were trained for 35k iterations and\nthe CUB-200-2011 model was trained for 2000 iterations.\nParticular Object Retrieval. For the particular object re-\ntrieval experiments, we build our implementation on top\nof the public code2 associated with the work by Radenovi´c\net al. (2018b). We follow the same optimization and reg-\nularization procedure. All models, transformer-based and\nconvolutional, are ﬁnetuned using the SFM120k dataset.\nThe input images are resized to have the same ﬁxed width\nand height. We report results for image sizes of 224×224\nand 384×384. For ﬁnetuning, each batch consists of 5 tu-\nples of (1 anchor, 1 positive, 5 negatives). For each epoch,\nwe randomly select 2,000 positive pairs and 22,000 negative\ncandidates (using hard-negative mining). We use the default\nhyper-parameters of Radenovi´c et al. (2018b): the models\nare optimized using Adam (Kingma & Ba, 2015) with small\nlearning rate of 5.10−7 and weight decay of 10−6. The\ncontrastive loss margin is set to β = 0.85. The models are\nﬁnetuned for 100 epochs. All models with GeM pooling\nuse a pooling exponent value of p= 3. The dimensionality\nreduction is achieved using a PCA trained on the SFM120k\ndataset. For the evaluation, all the query and database im-\nages are resized into a square image with the same resolution\nas used during the ﬁnetuning stage.\n2https://github.com/ﬁlipradenovic/cnnimageretrieval-pytorch\n4.3. Results\nCategory-level Retrieval. We present the Recall@K per-\nformance for three public benchmarks for category-level\nretrieval. For the SOP dataset, we can see in Table 2 that our\nIRTR model with DeiT-S384 backbone achieves state-of-the-\nart performance for all values of K, outperforming previous\nmethods by a margin of 2.6% absolute points for Recall@1.\nThe DeiT-S†variant with distillation pre-training achieves\nthe best results on this benchmark. Even when reducing the\ndimensionality to 128-D, our method outperforms all the\nconvnets except at Recall@1000. On the CUB-200-2011\ndataset, the DeiT-S384 model outperforms the current state\nof the art by 2.5% points at Recall@1. The distilled DeiT-S\nmodel provides an additional 1.9% improvement, achieving\nthe best results for all values of K. The DeiT-S128 variant\nwith compressed representation outperforms all previous\nmethods except for the ProxyNCA++ model that uses 2048-\nD descriptors. Similarly, for In-Shop, the DeiT-S384 model\nand its distilled variant outperform all previous models at\nRecall@1 with a margin of 0.2% and 0.6% respectively.\nParticular Object Retrieval. We present the mAP per-\nformance for the Medium and Hard splits of the revisited\nOxford and Paris benchmarks in Table 7. First observe that\nfor input images with size 224×224, the DeiT-S†backbone\noutperforms its ResNet-50 counterpart with the same ca-\npacity, as well as the higher capacity ResNet-101 across all\nbenchmarks and descriptor sizes. The larger DeiT-B†pro-\nvides a signiﬁcant gain in performance and achieves the best\nresult among the reported models. Scaling up the image size\nto 384×384 considerably improves the performance for all\nmodels with the DeiT-B†model retaining its position as the\nstrongest model. In Table 8 we compare our model to strong\nstate-of-the-art methods in particular object retrieval, fol-\nlowing the standard extensive evaluation procedure. Revaud\nTraining Vision Transformers for Image Retrieval\nTable 2.Recall@K performance for the SOP, CUB-200 and In-Shop category-level datasets compared to the state-of-the-art methods.\n⊿128: reduction to 128 components obtained using PCA.\nMethod Backbone #dims SOP (K) CUB-200 (K) In-Shop (K)\n1 10 100 1000 1 2 4 8 1 10 20 30\nA-BIER (Opitz et al., 2018)\nGoogleNet 512\n74.2 86.9 94.0 97.8 57.5 68.7 78.3 86.2 83.1 95.1 96.9 97.5\nABE (Kim et al., 2018) 76.3 88.4 94.8 98.2 60.6 71.5 79.8 87.4 87.3 96.7 97.9 98.2\nSM (Suh et al., 2019) 75.3 87.5 93.7 97.4 56.0 68.3 78.2 86.3 90.7 97.8 98.5 98.8\nXBM (Wang et al., 2020) 77.4 89.6 95.4 98.4 61.9 72.9 81.2 88.6 89.4 97.5 98.3 98.6\nHTL (Ge, 2018)\nInceptionBN 512\n74.8 88.3 94.8 98.4 57.1 68.8 78.7 86.5 80.9 94.3 95.8 97.2\nMS (Wang et al., 2019) 78.2 90.5 96.0 98.7 65.7 77.0 86.3 91.2 89.7 97.9 98.5 98.8\nSoftTriple (Qian et al., 2019) 78.6 86.6 91.8 95.4 65.4 76.4 84.5 90.4\nXBM (Wang et al., 2020) 79.5 90.8 96.1 98.7 65.8 75.9 84.0 89.9 89.9 97.6 98.4 98.6\nHORDE (Jacob et al., 2019) 80.1 91.3 96.2 98.7 66.8 77.4 85.1 91.0 90.4 97.8 98.4 98.7\nMargin (Wu et al., 2017)\nResNet-50\n128\n72.7 86.2 93.8 98.0 63.9 75.3 84.4 90.6 - - - -\nFastAP (Cakir et al., 2019) 73.8 88.0 94.9 98.3 - - - - - - - -\nMIC (Roth et al., 2019) 77.2 89.4 94.6 - 66.1 76.8 85.6 - 88.2 97.0 - 98.0\nXBM (Wang et al., 2020) 80.6 91.6 96.2 98.7 - - - - 91.3 97.8 98.4 98.7\nNSoftmax (Zhai & Wu, 2018) 512 78.2 90.6 96.2 - 61.3 73.9 83.5 90.0 86.6 97.5 98.4 98.8\nProxyNCA++ (Teh et al., 2020) 80.7 92.0 96.7 98.9 69.0 79.8 87.3 92.7 90.4 98.1 98.8 99.0\nNSoftmax (Zhai & Wu, 2018) 2048 79.5 91.5 96.7 - 65.3 76.7 85.4 91.8 89.4 97.8 98.7 99.0\nProxyNCA++ (Teh et al., 2020) 81.4 92.4 96.9 99.0 72.2 82.0 89.2 93.5 90.9 98.2 98.9 99.1\nIRTR (ours) DeiT-S ⊿128 83.4 93.0 97.0 99.0 72.6 81.9 88.7 92.8 91.1 98.1 98.6 99.0\n384 84.0 93.6 97.2 99.1 74.7 82.9 89.3 93.3 91.5 98.1 98.7 99.0\nDeiT-S† 384 84.2 93.7 97.3 99.1 76.6 85.0 91.1 94.3 91.9 98.1 98.7 98.9\nTable 3.Ablation of model components: off-the-shelf performance\n(IRTO), with contrastive learning (IRTL) and ﬁnally regularized\n(IRTR). All methods use a DeiT-S backbone with #dims=384.\nSupervision↓ SOP CUB In-Shop ROx RPar\nM H M H\nIRTO 52.8 58.5 31.3 20.3 5.6 50.2 26.3\nIRTL 83.0 74.2 90.3 32.7 11.4 63.6 37.8\nIRTR 84.0 74.7 91.5 34.0 11.5 66.1 40.2\net al. (2019) use the original resolution of the dataset (i.e.\n1024×768), while Radenovi´c et al. (2018a) utilizes multi-\nscale evaluation. Although these methods outperform our\nDeiT-B†model at resolution 384×384 in mAP, they are ap-\nproximately 248% and 437% more expensive w.r.t. FLOPS.\nFurthermore, we observe that for compressed representa-\ntions of 128-D, our model closes the gap with Radenovi ´c\net al. (2018a), achieving a higher mAP for RParis.\n4.4. Ablations\nDifferent Methods of Supervision. We provide a com-\nparison between different degrees of supervision correspond-\ning to IRTO, IRTL and IRTR in Table 3. We observe\nthat ﬁnetuning substantially improves performance over\noff-the-shelf features, especially for category-level retrieval.\nAugmenting the contrastive loss with differential entropy\nregularization further improves the performance across all\nbenchmarks. Figure 5 demonstrates how the distribution of\nthe cosine similarities between positive and negative pairs is\nimpacted by the different variants we study. We notice that\nﬁnetuning strongly helps to make the positive and negative\ndistributions more separable. The entropy regularization\nterm spreads the similarity values across a wider range.\nChoice of Feature Extractor: Pooling Methods.In Ta-\nble 9, we study different feature aggregation methods, as\ndescribed in Section 3.2. Both for category-level and par-\nticular object retrieval, we observe that utilizing the CLS\ntoken as the image-level descriptor provides the strongest\nperformance (or at least on par) compared to other popu-\nlar pooling methods such as average pooling, max pooling\nand GeM. This suggests that the transformer operates as a\nlearned aggregation operator, thereby reducing the need for\ncareful design of feature aggregation methods.\nPerformance across Objective Functions. The choice\nof the objective function used to train image descriptors is\ncrucially important and is the focus of the majority of the\nmetric learning research. While we adopt the contrastive\nloss as our primary objective function, we additionally inves-\ntigate two objective functions with different properties: (1)\nNormalized Softmax (Zhai & Wu, 2018) as a classiﬁcation-\nbased objective, and (2) Scalable Neighborhood Component\nAnalysis (NCA) (Wu et al., 2018), a pairwise objective\nwith implicit weighting of hard negatives through temper-\nature. Table 4 shows that DeiT-S outperforms its convolu-\ntional counterpart across all different choices of objective\nfunctions. This suggests that transformer-based models are\nstrong metric learners and hence an attractive alternative to\nconvolutional models for image retrieval.\nTraining Vision Transformers for Image Retrieval\nTable 4.Comparison between convo-\nlutional ResNet-50 (R50) and IRTL\nDeiT-Small (DeiT-S) architectures\nacross multiple metric learning objec-\ntive functions, as tested using the SOP\ndataset, β = 0.5 (”Contr.” refers to the\ncontrastive loss we use).\nModel Loss #dims R@1\nR50 NSoftmax2048 79.5\nDeiT-S 384 80.8\nR50 SNCA 2048 78.0\nDeiT-S 384 81.1\nR50 Contr. 2048 79.8\nDeiT-S 384 83.0\nTable 5.Recall@1 results for different values of\nmargins βand entropy regularization strengths\nλon SOP. Entropy regularization consistently\nboosts performance (drops again for λ> 1.0).\nModel β λ\n0.0 0.3 0.5 0.7 1.0\n0.1 78.9 79.2 79.1 78.8 78.8\n0.3 79.3 81.3 81.7 81.7 81.7\nR50 0.5 79.8 80.7 81.0 81.2 81.3\n0.7 79.3 80.5 81.2 81.1 78.5\n0.9 79.1 80.0 31.4 13.4 9.1\n0.1 70.3 70.7 71.4 70.1 71.1\n0.3 82.5 83.0 83.0 83.1 83.1\nDeiT-S 0.5 83.0 83.5 83.6 84.0 84.0\n0.7 82.9 83.8 84.1 84.2 83.8\n0.9 82.6 84.4 80.6 71.2 41.5\nTable 6.Particular object retrieval mAP\nperformance for different entropy regu-\nlarization strengths λ. The results are\nobtained by using DeiT-Small model\nwith CLS token as the feature descriptor\n(#dims=384, trained with contrastive loss).\nModel λ RO RPar\nM H M H\nDeiT-S\n0.0 32.7 11.4 63.6 37.8\n1.0 31.5 9.3 64.7 38.6\n2.0 34.5 11.1 65.7 39.8\n3.0 34.6 11.5 66.1 40.1\n4.0 34.0 11.5 66.1 40.2\n5.0 32.3 10.4 65.6 39.8\nTable 7.Particular object retrieval mAP performance comparison\nbetween different convolutional and IRTL models using different\ndescriptor dimensions. All models are ﬁnetuned the same way.\n⊿128: reduction to 128 components obtained using PCA.\nInput size Model Pooler #dims ROx RPar\nM H M H\n224×224\nR50 GeM\n⊿128\n25.8 8.6 56.7 31.2\nR50 R-MAC 23.6 5.5 56.0 30.8\nR101 GeM 27.8 8.0 59.0 32.2\nR101 R-MAC 27.3 7.4 57.9 31.3\nDeiT-S† CLS 32.1 13.3 63.8 39.3\nDeiT-B† CLS 36.6 14.8 64.4 39.1\n224×224\nR50 GeM\n2048\n28.7 10.9 61.2 35.9\nR50 R-MAC 25.6 7.3 60.6 35.4\nR101 GeM 31.7 11.1 63.4 37.3\nR101 R-MAC 31.0 9.3 62.6 36.5\nDeiT-S† CLS 384 34.5 15.8 65.8 42.0\nDeiT-B† CLS 768 39.5 17.4 67.5 43.6\n384×384\nR101 GeM\n⊿128\n34.1 9.5 62.6 36.3\nR101 R-MAC 31.4 7.4 61.6 35.3\nDeiT-B† CLS 49.0 21.5 68.5 43.8\n384×384\nR101 GeM 2048 38.1 12.5 69.4 45.8\nR101 R-MAC 37.1 10.6 66.0 41.4\nDeiT-B† CLS 768 50.5 22.7 70.6 47.4\nRegularizing Hyper-parameterλ. We explore the dif-\nferential entropy regularization strength and its impact on\nthe improvement of retrieval performance. First, we use the\nSOP dataset for our analysis and show how the Recall@1\nperformance changes with different margin values β and\nentropy regularization strengths λin Table 5.\nAll models, either transformer-based or convolutional,\ntrained with different margins are improved by the LKoLeo\nregularizer. The margins with the best results are those with\nthe lowest γ values in Figure 4. Moreover, we observe a\nsimilar boost in performance for particular object retrieval\nin Table 6, conﬁrming that the differential entropy regular-\nization provides a clear and consistent improvement across\ndifferent tasks and architectures.\nTable 8.Comparison with SoA methods for particular object re-\ntrieval: [1] (Radenovi´c et al., 2018a), [2] (Revaud et al., 2019).\n⊿128: reduction to 128 components obtained using PCA.\n⋆: FLOPS (G) are computed for input images of size 1024×768.\n§: our evaluation using pre-trained models from the authors.\nMethod Model\n{maxres} #dimsFLOPS\n(G)\nROx RPar\nM H M H\nIRTL\nDeiT-B†{384}\n⊿128\n98.8\n49.0 21.5 68.5 43.8\nIRTR 49.1 21.1 68.3 44.1\nIRTL 768 50.5 22.7 70.6 47.4\nIRTR 55.1 28.3 72.7 49.6\n[1]-GeM§ R101{1024} ⊿128 432.2⋆ 53.2 28.9 65.4 36.9\n[1]-GeM 2048 64.7 38.5 77.2 56.3\n[2]-GeM R101{1024} 2048 246.0⋆ 67.2 42.8 80.1 60.5\nTable 9.Performance of different pooling methods on both re-\ntrieval tasks (IRTL model, DeiT-Small backbone, #dims=384).\nPooler SOP CUB In-Shop ROx RPar\nM H M H\nAverage Pool83.0 72.8 90.2 28.3 8.5 61.9 36.0\nMax Pool 82.2 69.2 90.3 25.2 6.8 60.4 34.1\nGeM 82.6 69.1 89.8 26.5 8.5 60.2 33.7\nCLS 83.0 74.4 90.4 32.7 11.4 63.6 37.8\n5. Conclusion\nIn this paper, we have explored how to adapt the transformer\narchitecture to metric learning and image retrieval. In this\ncontext, we have revisited the contrastive loss formulation\nand showed that a regularizer based on a differential entropy\nloss spreading vectors over the unit hyper-sphere improves\nthe performance for transformer-based models, as well as\nfor convolutional models. As a result, we establish the new\nstate of the art for category-level image retrieval. Finally,\nwe demonstrated that, for comparable settings, transformer-\nbased models are an attractive alternative to convolutional\nbackbones for particular object retrieval, especially with\nshort vector representations. Their performance is competi-\ntive against convnets having a much higher complexity.\nTraining Vision Transformers for Image Retrieval\nReferences\nBabenko, A. and Lempitsky, V . Aggregating deep con-\nvolutional features for image retrieval. arXiv preprint\narXiv:1510.07493, 2015.\nBabenko, A., Slesarev, A., Chigorin, A., and Lempitsky, V .\nNeural codes for image retrieval. InEuropean Conference\non Computer Vision, 2014.\nBell, S., Liu, Y ., Alsheikh, S., Tang, Y ., Pizzi, E., Henning,\nM., Singh, K., Parkhi, O., and Borisyuk, F. Groknet:\nUniﬁed computer vision model trunk and embeddings\nfor commerce. In SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2020.\nBoudiaf, M., Rony, J., Ziko, I. M., Granger, E., Pedersoli,\nM., Piantanida, P., and Ayed, I. B. A unifying mutual\ninformation view of metric learning: cross-entropy vs.\npairwise losses. In European Conference on Computer\nVision, 2020.\nCakir, F., He, K., Xia, X., Kulis, B., and Sclaroff, S. Deep\nmetric learning to rank. In Computer Vision and Pattern\nRecognition, 2019.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. In European Conference on Computer Vi-\nsion, 2020.\nChen, B. and Deng, W. Hybrid-attention based decoupled\nmetric learning for zero-shot image retrieval. InComputer\nVision and Pattern Recognition, 2019.\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D.,\nand Sutskever, I. Generative pretraining from pixels. In\nInternational Conference on Machine Learning, 2020.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn Computer Vision and Pattern Recognition, 2009.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations,\n2021.\nDuan, Y ., Lu, J., and Zhou, J. Uniformface: Learning deep\nequidistributed representation for face recognition. In\nComputer Vision and Pattern Recognition, 2019.\nGe, W. Deep metric learning with hierarchical triplet loss.\nIn European Conference on Computer Vision, 2018.\nGkelios, S., Boutalis, Y ., and Chatzichristoﬁs, S. A. Investi-\ngating the vision transformer model for image retrieval\ntasks. arXiv preprint arXiv:2101.03771, 2021.\nGoldberger, J., Hinton, G. E., Roweis, S., and Salakhutdinov,\nR. R. Neighbourhood components analysis. In Saul, L.,\nWeiss, Y ., and Bottou, L. (eds.), Advances in Neural\nInformation Processing Systems. MIT Press, 2005.\nGordo, A., Almaz ´an, J., Revaud, J., and Larlus, D. Deep\nimage retrieval: Learning global representations for im-\nage search. In European Conference on Computer Vision,\n2016.\nHadsell, R., Chopra, S., and LeCun, Y . Dimensionality\nreduction by learning an invariant mapping. In Computer\nVision and Pattern Recognition, 2006.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In Computer Vision and\nPattern Recognition, 2016.\nHe, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Computer Vision and Pattern Recognition,\n2020.\nJacob, P., Picard, D., Histace, A., and Klein, E. Metric learn-\ning with horde: High-order regularizer for deep embed-\ndings. In International Conference on Computer Vision,\n2019.\nKim, W., Goyal, B., Chawla, K., Lee, J., and Kwon, K.\nAttention-based ensemble for deep metric learning. In\nEuropean Conference on Computer Vision, 2018.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations, 2015.\nKozachenko, L. and Leonenko, N. N. Sample estimate\nof the entropy of a random vector. Problemy Peredachi\nInformatsii, 1987.\nLiu, Z., Luo, P., Qiu, S., Wang, X., and Tang, X. Deepfash-\nion: Powering robust clothes recognition and retrieval\nwith rich annotations. In Computer Vision and Pattern\nRecognition, 2016.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017.\nMovshovitz-Attias, Y ., Toshev, A., Leung, T. K., Ioffe, S.,\nand Singh, S. No fuss distance metric learning using\nproxies. In International Conference on Computer Vision,\n2017.\nMusgrave, K., Belongie, S., and Lim, S.-N. A metric learn-\ning reality check. arXiv preprint arXiv:2003.08505, 2020.\nOh Song, H., Xiang, Y ., Jegelka, S., and Savarese, S. Deep\nmetric learning via lifted structured feature embedding.\nIn Computer Vision and Pattern Recognition, 2016.\nOpitz, M., Waltner, G., Possegger, H., and Bischof, H. Deep\nTraining Vision Transformers for Image Retrieval\nmetric learning with bier: Boosting independent embed-\ndings robustly. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2018.\nPhilbin, J., Chum, O., Isard, M., Sivic, J., and Zisserman, A.\nObject retrieval with large vocabularies and fast spatial\nmatching. In International Conference on Computer\nVision, 2007.\nPhilbin, J., Chum, O., Isard, M., Sivic, J., and Zisserman, A.\nLost in quantization: Improving particular object retrieval\nin large scale image databases. In Computer Vision and\nPattern Recognition, 2008.\nQian, Q., Shang, L., Sun, B., Hu, J., Li, H., and Jin, R. Soft-\ntriple loss: Deep metric learning without triplet sampling.\nIn Computer Vision and Pattern Recognition, 2019.\nRadenovi´c, F., Iscen, A., Tolias, G., Avrithis, Y ., and Chum,\nO. Revisiting oxford and paris: Large-scale image re-\ntrieval benchmarking. In Computer Vision and Pattern\nRecognition, 2018a.\nRadenovi´c, F., Tolias, G., and Chum, O. Fine-tuning cnn\nimage retrieval with no human annotation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\n2018b.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding with unsupervised\nlearning. Technical report, OpenAI, 2018.\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. arXiv preprint arXiv:1906.05909, 2019.\nRevaud, J., Almaz ´an, J., Rezende, R. S., and Souza, C.\nR. d. Learning with average precision: Training image\nretrieval with a listwise loss. In International Conference\non Computer Vision, 2019.\nRoth, K., Brattoli, B., and Ommer, B. Mic: Mining in-\nterclass characteristics for improved metric learning. In\nInternational Conference on Computer Vision, 2019.\nSablayrolles, A., Douze, M., Schmid, C., and J ´egou, H.\nSpreading vectors for similarity search. In International\nConference on Learning Representations, 2019.\nSchonberger, J. L., Radenovic, F., Chum, O., and Frahm, J.-\nM. From single image query to detailed 3d reconstruction.\nIn International Conference on Computer Vision, 2015.\nSohn, K. Improved deep metric learning with multi-class\nn-pair loss objective. In Advances in Neural Information\nProcessing Systems, 2016.\nSuh, Y ., Han, B., Kim, W., and Lee, K. M. Stochastic class-\nbased hard example mining for deep metric learning. In\nComputer Vision and Pattern Recognition, 2019.\nTeh, E. W., DeVries, T., and Taylor, G. W. Proxynca++: Re-\nvisiting and revitalizing proxy neighborhood component\nanalysis. arXiv preprint arXiv:2004.01113, 2020.\nTolias, G., Sicre, R., and J ´egou, H. Particular object re-\ntrieval with integral max-pooling of cnn activations.arXiv\npreprint arXiv:1511.05879, 2015.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,\nA., and J ´egou, H. Training data-efﬁcient image trans-\nformers and distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie,\nS. The caltech-ucsd birds-200-2011 dataset. 2011.\nWang, T. and Isola, P. Understanding contrastive represen-\ntation learning through alignment and uniformity on the\nhypersphere. arXiv preprint arXiv:2005.10242, 2020.\nWang, X., Girshick, R., Gupta, A., and He, K. Non-local\nneural networks. In Computer Vision and Pattern Recog-\nnition, 2018.\nWang, X., Han, X., Huang, W., Dong, D., and Scott, M. R.\nMulti-similarity loss with general pair weighting for deep\nmetric learning. In Computer Vision and Pattern Recog-\nnition, 2019.\nWang, X., Zhang, H., Huang, W., and Scott, M. R. Cross-\nbatch memory for embedding learning. In Computer\nVision and Pattern Recognition, 2020.\nWeinberger, K. Q. and Saul, L. K. Distance metric learning\nfor large margin nearest neighbor classiﬁcation. Journal\nof Machine Learning Research, 2009.\nWightman, R. Pytorch image models. https://github.\ncom/rwightman/pytorch-image-models,\n2019.\nWu, C.-Y ., Manmatha, R., Smola, A. J., and Krahenbuhl,\nP. Sampling matters in deep embedding learning. In\nInternational Conference on Computer Vision, 2017.\nWu, Z., Efros, A. A., and Yu, S. Improving generaliza-\ntion via scalable neighborhood component analysis. In\nEuropean Conference on Computer Vision, 2018.\nZhai, A. and Wu, H.-Y . Classiﬁcation is a strong baseline for\ndeep metric learning. arXiv preprint arXiv:1811.12649,\n2018.\nZhang, X., Yu, F. X., Kumar, S., and Chang, S.-F. Learning\nspread-out local feature descriptors. In International\nConference on Computer Vision, 2017.\nZhao, K., Xu, J., and Cheng, M.-M. Regularface: Deep face\nrecognition via exclusive regularization. In Computer\nVision and Pattern Recognition, 2019."
}