{
    "title": "Transformers in Single Object Tracking: An Experimental Survey",
    "url": "https://openalex.org/W4385214091",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5092533093",
            "name": "Janani Kugarajeevan",
            "affiliations": [
                "University of Moratuwa"
            ]
        },
        {
            "id": "https://openalex.org/A4224173619",
            "name": "Thanikasalam Kokul",
            "affiliations": [
                "University of Jaffna"
            ]
        },
        {
            "id": "https://openalex.org/A2101211492",
            "name": "Amirthalingam Ramanan",
            "affiliations": [
                "University of Jaffna"
            ]
        },
        {
            "id": "https://openalex.org/A2274482935",
            "name": "Subha Fernando",
            "affiliations": [
                "University of Moratuwa"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2966535964",
        "https://openalex.org/W3132576305",
        "https://openalex.org/W3089134182",
        "https://openalex.org/W4205466759",
        "https://openalex.org/W3044951323",
        "https://openalex.org/W4210257669",
        "https://openalex.org/W3216295476",
        "https://openalex.org/W4220857797",
        "https://openalex.org/W3132393417",
        "https://openalex.org/W3206934557",
        "https://openalex.org/W2066513304",
        "https://openalex.org/W1993480139",
        "https://openalex.org/W2964111344",
        "https://openalex.org/W1984914017",
        "https://openalex.org/W2284721269",
        "https://openalex.org/W3139921887",
        "https://openalex.org/W2066757459",
        "https://openalex.org/W3115711339",
        "https://openalex.org/W2755780177",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2990187711",
        "https://openalex.org/W3164347164",
        "https://openalex.org/W3127796792",
        "https://openalex.org/W2470394683",
        "https://openalex.org/W2424629859",
        "https://openalex.org/W2171590421",
        "https://openalex.org/W2799058067",
        "https://openalex.org/W2886910176",
        "https://openalex.org/W2963534981",
        "https://openalex.org/W3035511673",
        "https://openalex.org/W3035571898",
        "https://openalex.org/W3035672751",
        "https://openalex.org/W3168663926",
        "https://openalex.org/W3172670627",
        "https://openalex.org/W3195373975",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W3111535274",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6810023147",
        "https://openalex.org/W3167536469",
        "https://openalex.org/W3214586131",
        "https://openalex.org/W4214759957",
        "https://openalex.org/W6795704626",
        "https://openalex.org/W3203857058",
        "https://openalex.org/W3203510176",
        "https://openalex.org/W4312651496",
        "https://openalex.org/W3205280353",
        "https://openalex.org/W4312516520",
        "https://openalex.org/W4294831565",
        "https://openalex.org/W4312472480",
        "https://openalex.org/W4312532041",
        "https://openalex.org/W3208338480",
        "https://openalex.org/W6805147364",
        "https://openalex.org/W4285603016",
        "https://openalex.org/W4312805142",
        "https://openalex.org/W4312323989",
        "https://openalex.org/W4312751983",
        "https://openalex.org/W4375869376",
        "https://openalex.org/W4323345691",
        "https://openalex.org/W4386066394",
        "https://openalex.org/W4386083067",
        "https://openalex.org/W4386065544",
        "https://openalex.org/W4386066459",
        "https://openalex.org/W4386066081",
        "https://openalex.org/W2767302379",
        "https://openalex.org/W1995903777",
        "https://openalex.org/W6753998783",
        "https://openalex.org/W2317001119",
        "https://openalex.org/W3001164154",
        "https://openalex.org/W3172446114",
        "https://openalex.org/W2158592639",
        "https://openalex.org/W2126302311",
        "https://openalex.org/W4237698551",
        "https://openalex.org/W4286251267",
        "https://openalex.org/W4304481542",
        "https://openalex.org/W2891033863",
        "https://openalex.org/W2898200825",
        "https://openalex.org/W2794744029",
        "https://openalex.org/W2518876086",
        "https://openalex.org/W2130026429",
        "https://openalex.org/W7034753983",
        "https://openalex.org/W2917435394",
        "https://openalex.org/W6759215614",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W6792695861",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W4295308583",
        "https://openalex.org/W2964423614",
        "https://openalex.org/W3001584168",
        "https://openalex.org/W3035211844",
        "https://openalex.org/W2998434318",
        "https://openalex.org/W3108519869",
        "https://openalex.org/W3035453691",
        "https://openalex.org/W3204540098",
        "https://openalex.org/W4386075784",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2592463526",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4285091163",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W2089961441",
        "https://openalex.org/W4385815582",
        "https://openalex.org/W3036644940",
        "https://openalex.org/W4224304134",
        "https://openalex.org/W6788684253",
        "https://openalex.org/W6853378651",
        "https://openalex.org/W4309125142",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W4380551209",
        "https://openalex.org/W3135063319",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4290066150",
        "https://openalex.org/W3162457465",
        "https://openalex.org/W4300685237"
    ],
    "abstract": "Single-object tracking is a well-known and challenging research topic in computer vision. Over the last two decades, numerous researchers have proposed various algorithms to solve this problem and achieved promising results. Recently, Transformer-based tracking approaches have ushered in a new era in single-object tracking by introducing new perspectives and achieving superior tracking robustness. In this paper, we conduct an in-depth literature analysis of Transformer tracking approaches by categorizing them into CNN-Transformer based trackers, Two-stream Two-stage fully-Transformer based trackers, and One-stream One-stage fully-Transformer based trackers. In addition, we conduct experimental evaluations to assess their tracking robustness and computational efficiency using publicly available benchmark datasets. Furthermore, we measure their performances on different tracking scenarios to identify their strengths and weaknesses in particular situations. Our survey provides insights into the underlying principles of Transformer tracking approaches, the challenges they encounter, and the future directions they may take.",
    "full_text": "Received 22 June 2023, accepted 13 July 2023, date of publication 24 July 2023, date of current version 4 August 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3298440\nTransformers in Single Object Tracking:\nAn Experimental Survey\nJANANI KUGARAJEEVAN\n 1,2, THANIKASALAM KOKUL\n 3, (Member, IEEE),\nAMIRTHALINGAM RAMANAN\n 3, (Senior Member, IEEE),\nAND SUBHA FERNANDO\n 1, (Member, IEEE)\n1Department of Computational Mathematics, University of Moratuwa, Moratuwa 10400, Sri Lanka\n2Department of Interdisciplinary Studies, University of Jaffna, Jaffna 40000, Sri Lanka\n3Department of Computer Science, University of Jaffna, Jaffna 40000, Sri Lanka\nCorresponding author: Thanikasalam Kokul (kokul@univ.jfn.ac.lk)\nThe author acknowledges the support received from the LK Domain Registry, Sri Lanka, in publishing this paper.\nABSTRACT Single-object tracking is a well-known and challenging research topic in computer vision.\nOver the last two decades, numerous researchers have proposed various algorithms to solve this problem\nand achieved promising results. Recently, Transformer-based tracking approaches have ushered in a new\nera in single-object tracking by introducing new perspectives and achieving superior tracking robustness.\nIn this paper, we conduct an in-depth literature analysis of Transformer tracking approaches by categorizing\nthem into CNN-Transformer based trackers, Two-stream Two-stage fully-Transformer based trackers, and\nOne-stream One-stage fully-Transformer based trackers. In addition, we conduct experimental evaluations\nto assess their tracking robustness and computational efficiency using publicly available benchmark datasets.\nFurthermore, we measure their performances on different tracking scenarios to identify their strengths\nand weaknesses in particular situations. Our survey provides insights into the underlying principles of\nTransformer tracking approaches, the challenges they encounter, and the future directions they may take.\nINDEX TERMS Deep learning, tracking review, transformer tracking, vision transformer, visual object\ntracking.\nI. INTRODUCTION\nVisual object tracking (VOT) algorithms are intended to esti-\nmate the state (i.e., spatial location and size) of an object in\na given video sequence. Given the initial state of the target in\nthe first frame of a video sequence, these algorithms tracks\nthe target’s states in the remaining frames. VOT can be clas-\nsified as single and multi-object tracking [1], [2] approaches\nbased on the number of targets to be tracked. In single-object\ntracking, a single instance of an object class is tracked, while\nin multi-object tracking, multiple instances of an object class\nare tracked.\nSingle object tracking algorithms became popular and\ngained interest in recent years because of its wide range\nof applications in computer vision, including video surveil-\nlance [3], augmented reality [4], automated driving [5],\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Charalambos Poullis\n.\nmobile robotics [6], traffic monitoring [7], sports video\nanalysis [8], scene understanding [9], and human computer\ninteraction [10]. Single object VOT approaches captures the\ntarget’s appearance features in the first frame of a video\nsequence and then use it to locate the target in the remaining\nframes. Although many appearance-based approaches have\nbeen proposed in VOT, it is still far from reaching the tracking\nrobustness of humans with real-time speed due to many chal-\nlenges such as appearance and pose variations, occlusions,\nmotion blurring, background clutter, similar object distrac-\ntors, and deformation.\nSingle object tracking algorithms can be classified and\nanalysed in several ways. Based on the features used in\na tracking model, VOT approaches can be categorized as\nhand-crafted and deep feature-based trackers. Hand-crafted\nfeature-based tracking approaches [11], [12], [13], [14], [15]\nextract the features from images according to a certain manu-\nally predefined algorithm based on expert knowledge. On the\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n80297\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 1. Performance comparison of the state-of-the-art CNN-based, CNN-Transformer based, and fully-Transformer based trackers on LaSOT\nbenchmark dataset. For some trackers, their reported tracking speed may be different than the above graph since this comparison study is conducted\non a NVIDIA Quadro P4000 GPU with a 64GB of RAM.\nother hand, deep feature-based trackers [16], [17], [18], [19]\ncapture the semantic cues from raw images by using the\nConvolutional Neural Networks (CNNs) [20]. Because of the\nhierarchical learning capability, deep features-based appear-\nance trackers significantly outperform hand-crafted feature-\nbased trackers [21]. Based on how trackers differentiate the\ntarget object from its surroundings, they can be discriminative\nand generative trackers. Discriminative trackers [22], [23]\ntreats the VOT as a binary classification task and separate\nthe target object from the background. On the other hand,\nmost of the generative trackers [24], [25] treat VOT as a\nsimilarity matching problem by searching for the best can-\ndidate that closely matches the reference template in each\nframe. In recent trackers, a two branch CNN architecture,\nknown as Siamese network [26], is utilized in similarity\nmatching tracking. Over the past few years, a large number\nof Siamese-based trackers [27], [28], [29], [30], [31], [32],\n[33], [34] have been proposed in VOT since they showed\nexcellent performance with high computational efficiency.\nHowever, discriminative capability of Siamese-based trackers\nare considerably poor [35], since they only focus on learn-\ning a visual representation of the target object to match the\nsimilarity while ignoring the background and target specific\ninformation and hence showed low performance in occlusion\nand deformation scenes.\nTransformer [36] was introduced in the field of Natural\nLanguage Processing (NLP) to capture the long range depen-\ndencies between input and output sequences for machine\ntranslation tasks. Encoder and decoder stacks are the two\nprimary components of a Transformer architecture and they\nare used to learn contextual information of the inputs and to\ngenerate the output sequences, respectively. Based on great\nsuccess of the Transformers in NLP applications, researchers\nmodified the Transformer architecture [37] to solve the com-\nputer vision tasks. In the last three years, several Transformer\narchitectures are proposed and showed state-of-the-art perfor-\nmances in various vision tasks such as object detection [38],\nimage classification [37], [39], semantic segmentation [40],\nand point cloud learning [41]. In addition, a few review\nstudies [42], [43], [44] have also been conducted to ana-\nlyze the performance of Transformers in vision tasks and\nto compare their performance with the state-of-the-art CNN\narchitectures.\nAlthough Transformers and CNNs are two types of artifi-\ncial neural network models, they differ from each other in sev-\neral ways, including their architecture, design principles, and\nperformances. The CNNs and Transformers consume images\ndifferently, with CNNs consuming images as arrays of pixel\nvalues and Transformers consuming images as sequences of\npatches, respectively. Based on the studies [42], Transformers\nare good at capturing global information of an image than\nthe state-of-the-art CNN models. Furthermore, Transformers\nare better equipped to capture long-range dependencies in an\nimage without sacrificing computational efficiency. On the\nother hand, increasing the size of convolutional kernels in\nCNNs can hinder their learning capability. Researchers [37]\nalso found that Transformers are difficult to optimize and\nhence needs more training data than the CNN models. Addi-\ntionally, Transformers have a larger number of parameters,\nwhich can lead to overfitting if not enough training data is\navailable. However, once trained, Transformers can produce\ntesting outputs with fewer computational resources than the\n80298 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\ncorresponding CNN models, as they can process the data in\na parallel manner. In terms of transfer learning, Transformers\nshown promising results and has been gaining popularity in\nrecent years while CNNs are generally better suited for small\nand medium-sized datasets. Overall, Transformer models are\nreplacing the CNNs in computer vision tasks because of their\nattention mechanism and global feature capturing capabili-\nties. Although accuracy of Transformer is modest in small and\nmedium-sized datasets, they are expected to replace CNNs in\nall tasks in the coming years.\nFor the last three years, due to the great success of\nTransformers in other computer vision tasks, a set of Trans-\nformer based VOT approaches [45], [46], [47], [48], [49],\n[50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60],\n[61], [62], [63], [64], [65], [66], [67], [68], [69] have been\nproposed and showed excellent performance on publicly\navailable single object tracking benchmark datasets. They\nhave been proposed as Siamese-based template matching\ntrackers or discriminative approaches and showed outstand-\ning performances than the CNN-based trackers. In some\nearly approaches [45], [46], [47], [48], [49], [50], a hybrid-\ntype tracking model is proposed by combining the CNN\nand Transformer architectures together. As shown in Fig. 1,\nthese hybrid-type trackers, called as CNN-Transformer based\ntrackers, showed better tracking robustness while maintaining\nconsiderable tracking speed than the CNN-based trackers\nsince they combined the attention mechanism of Trans-\nformers with the hierarchical learning capability of CNNs.\nFrom last year, researchers have initiated a new era in VOT\nby proposing a set of trackers that rely solely on Trans-\nformer architecture. These trackers are referred to as fully-\nTransformer based trackers, and they have demonstrated\nexceptional tracking robustness compared to CNN-based and\nCNN-Transformer based trackers because they successfully\nhandle the information flow between target template and\nsearch region features. Overall, the introduction of Trans-\nformers in VOT opens up several new possibilities and\nperspectives. Therefore, there is a need to analyze the perfor-\nmance of Transformers in single object tracking to identify\nfuture research directions.\nVOT research has been conducted for more than two\ndecades and several approaches have been proposed. Over\nthese years, several review studies [14], [21], [35], [70],\n[71], [72], [73], [74], [75], [76], [77], [78], [79], [80]\nhave been conducted to analyze and compare the per-\nformance of VOT approaches. Most of the early review\nstudies [14], [71], [73], [76], [77], [78] were conducted\nto evaluate the performance of hand-crafted feature-based\ntrackers, while recent studies [21], [35], [70], [72], [74],\n[75], [80] focus solely on deep feature-based trackers. Fur-\nthermore, while most studies [14], [35], [71], [72], [73],\n[74], [75], [78] only summarize the literature or compare\nthe reported results of tracking approaches, a few stud-\nies [21], [70], [76], [79] perform an analysis of tracker\nperformance through experiments conducted on the same\ncomputing platform. To the best of our knowledge, no survey\nstudies have been conducted that include Transformer-based\ntrackers.\nIn recent years, several challenging benchmark datasets\n[81], [82], [83], [84] have been introduced with various chal-\nlenging attributes and performance measures. To identify the\nfuture direction of VOT, it is necessary to experimentally\nevaluate and compare all recently proposed trackers on the\nsame computing platform in order to avoid bias. Addition-\nally, even though the reported results of Transformer-based\ntrackers are better than those of CNN-based trackers, there\nis a need to evaluate their performance through detailed\nexperiments in order to identify research gaps and propose\nfuture tracking models. With the aim of achieving these\nobjectives, we have conducted an experimental survey on\nrecently proposed single-object trackers. This study specif-\nically focuses on trackers that utilize the Transformer in their\ntracking pipeline. We have considered all the Transformer\ntrackers that were published in indexed journals and presti-\ngious conferences.\nThis study makes the following notable contributions:\n1) We have conducted a comprehensive review of the\nliterature on Transformer-based tracking approaches.\nNon-Transformer based trackers were not included in\nour literature review as they have already been covered\nby previous studies.\n2) We have experimentally evaluated and then compared\nthe tracking robustness of Transformers in VOT on\nfive challenging benchmark datasets by categorizing\nand analyzing the state-of-the-art trackers as fully-\nTransformer based trackers, CNN-Transformer based\ntrackers, and CNN-based trackers.\n3) The computational efficiencies of the state-of-the-art\ntrackers were evaluated on a common computing plat-\nform using the source code and models provided by the\nauthors for fair comparison.\n4) We have conducted the tracking attribute-wise evalua-\ntion on three benchmark datasets to identify the most\nchallenging scenarios for recent trackers.\n5) Based on the experimental findings, we provide the\nsuggestions for future directions of Transformer based\nsingle object tracking.\nThe rest of this manuscript is structured as follows: The\npreviously conducted review studies are discussed and the\nrationale behind this survey study is established in Section II.\nSection III provides an in-depth overview of the Transformer\narchitecture. The detailed literature review of Transformer-\nbased tracking approaches is presented in Section IV. The\nexperimental results of tracking robustness and computa-\ntional efficiencies are presented in Section V. The sum-\nmary of the findings and the proposed future directions\nfor Transformer-based tracking are discussed in Section VI.\nFinally, the manuscript is concluded in Section VII.\nII. RELATED WORK\nIn this section, we justify the need for this study, despite the\nseveral experimental surveys and review studies conducted\nVOLUME 11, 2023 80299\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nin the past two decades for single object tracking. We have\nsummarized the previous studies in Table 1, categorizing\nthem by their review type and feature type.\nDuring the period from 2006 to 2014, several review\nstudies were conducted on hand-crafted feature-based VOT\napproaches. However, since no benchmark datasets were\ndeveloped during that period, the evaluation of trackers\nwas performed on various video sequences using differ-\nent performance metrics. Therefore, comparing the perfor-\nmance of trackers is a challenging task, and as a result, the\nreview studies focused only on conducting literature surveys.\nYilmaz et al. [71] conducted a literature survey on object\ntracking by classifying the trackers into different categories\nbased on the target representation, feature type, model learn-\ning algorithm, and searching technique. Similar to that work,\nCannons [78] developed a detailed technical report on visual\ntracking and conducted the literature survey by classifying\nthe trackers based on their target representation as point,\ncontour, and region trackers. Li et al. [14] performed a study\non VOT by providing a detailed literature survey. Also, they\nhave compared a few trackers based on the qualitative results\nof some tracking attributes such as occlusion, illumination,\nand deformation. Deori and Thounaojam [73] conducted a\nliterature survey by categorising the trackers based on their\nsearching algorithms. Since deep learning-based trackers out-\nperform hand-crafted trackers by a large margin, these survey\nstudies are no longer useful in the present era.\nThe first experimental survey on single object tracking was\nconducted by Wu et al. [76] based on the evaluation of thirty-\none appearance based trackers. They have developed a bench-\nmark dataset, known as Object Tracking Benchmark (OTB),\nand then evaluated the tracking performances and efficiencies\nof the trackers based on their overall accuracies and tracking\nspeed on a same computing platform. In addition, trackers\nare evaluated and compared in eleven challenging tracking\nattributes (scenarios). Although this study only considers the\nhand-crafted feature based trackers, it is still followed by\nmany researchers due to the important of the OTB dataset.\nIn the last ten years, deep learning-based single object\ntrackers have shown excellent tracking performance com-\npared to hand-crafted feature-based trackers. Currently, only\na few studies have been conducted to review the litera-\nture on deep trackers. Soleimanitaleb et al. [74] analyzed the\nliterature of a few number of hand-crafted and deep track-\ners. Pflugfelder [72] only considered the Siamese based\ndeep trackers and compared their performances based on\ntheir reported results. Similar to their work, Ondrašovič\nand Tarábek [35] considered a detailed literature survey\non Siamese based deep trackers and then compared their\ntracking performances based on their reported results on the\nwell-known OTB100 [76], VOT2015 [85], VOT2016 [86],\nVOT2017[87], VOT2018 [88] and GOT-10k[82] benchmark\ndatasets. Recently, Zhang et al. [75] conducted a survey by\nclassifying the recent trackers as deep trackers and Discrim-\ninative Correlation Filter (DCF) based trackers. Also, based\non the reported results of corresponding trackers, they have\ncompared the tracking performance and efficiency. Another\nsimilar survey study is conducted by Javed et al. [80] by cat-\negorizing the tracking approaches as Siamese based trackers\nand DCF trackers. These review studies have analyzed the\nliterature of the trackers by classifying them into different\ncategories and provided future directions for VOT. How-\never, since they did not conduct experimental evaluations,\ntheir reported results and comparisons are not considered\nacceptable. Moreover, these survey studies did not consider\nthe efficiency comparison of trackers, even though it is an\nimportant aspect in evaluation as the tracking speed is directly\nrelated to the complexity of the tracker. Although some\nsurvey studies compared tracking speeds based on reported\nresults, this comparison is not considered acceptable because\nthe speed of a tracker is mainly dependent on the efficiency\nof the GPU on which it was tested.\nThe first experimental survey on deep-learning-based\ntrackers is conducted by Li et al. [70] by evaluating the per-\nformance and efficiency of twenty-two trackers on OTB100\nand VOT2015 benchmarks. Also, they have compared the\nindividual performance of trackers based on eleven tracking\nattributes and then identified the future directions. Recently,\nMarvasti-Zadeh et al. [21] conducted a comprehensive and\nexperimental survey on deep learning-based VOTs based on\nthe network architecture, tracking methods, long-term track-\ning, aerial-view tracking, and online tracking ability of the\ntrackers. In addition, they have compared tracking bench-\nmarks based on their challenging attributes and conducted the\ntracking speed comparison of trackers. Similar to their work,\nChen et al. [79] conducted an experimental survey on deep\nand hand-crafted trackers on recently developed benchmarks:\nOTB, VOT, LaSOT [81], GOT-10k, and TrackingNet [83].\nAlthough these experimental evaluation studies reviewed\nthe literature and compared the tracking performances of\nrecently proposed deep trackers in several aspects, they did\nnot give much attention to the comparison based on tracking\nefficiency.\nIn the last three years, similar to other computer vision\ntasks, language models have played a crucial role in sin-\ngle object tracking. Transformer-based language models\nhave opened up new possibilities and have shown excellent\ntracking performances and efficiencies compared to CNN-\nbased deep trackers. Since previous review studies have not\nincluded Transformer-based trackers, as they were recently\nproposed, there is a need to categorize and review their model\narchitecture and to identify their strengths and weaknesses\nfor the future directions of single object tracking. To achieve\nthis objective, we conducted a detailed literature review on\nTransformer-based single object trackers and conducted an\nexperimental survey, which clearly showed that they signifi-\ncantly outperformed CNN-based trackers by a large margin.\nIn addition, unlike many other review studies, we have also\nexperimentally compared the efficiency of recent trackers\nbased on their tracking speed, number of floating-point oper-\nations (FLOPs), and the number of parameters in the tracking\nmodel. Furthermore, we have discussed the future directions\n80300 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nTABLE 1. Summary of the review and experimental survey studies in single object tracking.\nof Transformer-based tracking by identifying research gaps\nand providing suggestions.\nIII. TRANSFORMER\nIn this section, we have covered the Transformer [36] and\nVision Transformer (ViT) [37] architectures, as they form the\nfoundation for Transformer-based single object trackers.\nTransformer [36] was initially introduced in machine trans-\nlation tasks and then based on its great success and efficiency\nit was used in other NLP tasks such as document sum-\nmarisation and generation. The Transformer architecture is\nbased on an attention mechanism that enables the model to\nweigh the importance of different parts of the input sequence\nduring processing, leading to improved information flow\nand the capturing of long-range dependencies. As shown in\nFig. 2, the Transformer architecture consists of encoder and\ndecoder components. The encoder component is made up of\nN number of identical encoder layers by stacked them on\ntop of each other and the decoder component is also made\nup of N number of identical decoder layers, which are also\nstacked on each other. Encoder and decoder components of\na Transformer architecture are illustrated in the left and right\nside of Fig. 2, respectively. In the Transformer architecture,\nall encoder layers have two sub-layers: a self-attention layer,\nand a fully connected feed-forward layer. In addition to those\ntwo sub-layers, all decoder layers have an encoder-decoder\nattention layer in the middle.\nTransformer [36] receive the input as a vector sequence\nand use a positional embedding algorithm to add information\nabout the position of each token within that sequence to its\nrepresentation. After the embedding, the input data is fed\nto the self-attention sub-layer of the encoder, as it helps to\ncapture the contextual relationships. On the other side, in the\ndecoder layer, an encoder-decoder attention sub-layer is used\nto concentrate on relevant parts of the input data. The self-\nattention mechanism of the Transformer is described in detail\nin Sections III-A and III-B.\nAfter the self-attention, a fully-connected feed forward\nlayer is used to learn the complex representation of atten-\ntion features. It has a simple architecture with two linear\nFIGURE 2. Architecture of the Transformer [36]model used in machine\ntranslation task.\ntransformations and a non-linear activation in between them.\nThis layer can be described as two convolutions with the\nkernel size of 1. In both encoder and decoder layers, residual\nconnection is included and then it is followed by a normaliza-\ntion layer. Residual connections are used to preserve the cues\nfrom the original input data and to enable the model to learn\nmore accurate representations of the input data.\nAfter the decoder layer stacks, a liner layer is used to\nproduce the output vectors. Finally, a Softmax layer is used\nto produce the probabilities of the outputs.\nA. SELF-ATTENTION\nTransformer architectures are designed based on an atten-\ntion concept known as self-attention. It used to process\nVOLUME 11, 2023 80301\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\ninput sequences in parallel, rather than in a sequential man-\nner like RNNs or LSTMs. In the Transformer architecture,\nself-attention mechanism captures contextual relationships\nbetween input tokens by calculating the attention weights\nbetween each element in the input sequence and all other\nelements in the same sequence. Transformer used ‘‘Query’’,\n‘‘Key’’, and ‘‘Value’’ abstractions to calculate the attention of\nan input sequence.\nIn the first step of the self-attention computation, three\ndistinct vectors: a query vector q, a key vector k, and a\nvalue vector v, are created by multiplying an input vector (x)\nwith three corresponding matrices: WQ, WK , and WV , that\nwere trained in the training phase. Similarly, all input vectors\nare packed together to form an input matrix X and then the\ncorresponding ‘‘Query’’, ‘‘Key’’, and ‘‘Value’’ matrices: Q,\nK, and V are generated, respectively.\nAs the second step of self-attention, a score matrix S is\ncomputed for the input X by taking the dot product of Q and\nK as follows:\nS = Q · KT , (1)\nThe score matrix S provides how much attention should be\ngiven to other parts of the input sequence based on the input\nvector at a particular position. In the next step, score matrix\nS is normalized to obtain more stable gradients as follows:\nSn = S/\n√\ndk, (2)\nwhere Sn is the normalized score matrix, and dk is the dimen-\nsion of the key vector. Then the SoftMax function is applied\nto the normalized score matrix to convert the scores into\nprobabilities (P) as denoted in the following equation:\nP = SoftMax(Sn), (3)\nIn the final step of self-attention, the obtained probabili-\nties (P) are multiplied with the value matrix V to find the\nself-attention output values (Z ) as denoted in the following\nequation:\nZ = P · V , (4)\nIn summary, the entire self-attention mechanism can be uni-\nfied into a single equation as follows:\nAttention(Q, K, V ) = SoftMax\n(Q · KT\n√dk\n)\n· V (5)\nThe encoder-decoder attention sub-layer in the decoder\nlayer is similar to the self-attention sub-layer of the encoder\nexcept the key matrix K, and the value matrix V are obtained\nfrom the encoder block, and the query matrix Q is obtained\nfrom its previous layer.\nB. MULTI-HEAD SELF-ATTENTION\nThe self-attention mechanism is not much capable of focus-\ning on a specific location of input without affecting the\nattention on other equally vital locations at the same time.\nTherefore, the performance of self-attention mechanism is\nFIGURE 3. Architecture of the Vision Transformer (ViT)[37], proposed for\nimage recognition task.\nboosted by using multiple heads and this technique is known\nas multi-head self-attention.\nMultiple sets of weight matrices (W Q, WK , and WV ) are\nused in multi-head self-attention mechanism. They are ran-\ndomly initialized and trained separately since they are used\nto project the same input data into a different subspace.\nThen, the attention function is computed concurrently on each\nof these projected versions of queries, keys, and values to\nproduce the corresponding output values. At the final stage\nof multi-head self-attention, outputs of all attention heads\nare concatenated and then multiplied with another trainable\nweight matrix WO to obtain the multi-head self-attention\n(Matten) as follows:\nMatten = Concat(head1, . . . ,headn) · WO (6)\nwhere head i is the output of the attention head i. Multi-\nhead self-attention layer is used to concentrate on different\npositions of input and to represent the input into different\nsubspaces.\nC. VISION TRANSFORMER (ViT)\nBased on the success of Transformers in NLP tasks, several\nresearchers have attempted to apply them in computer vision\ntasks and have proposed various architectures. Among these\nmodels Vision Transformer (ViT) [37] is more efficient than\nothers with a simple architecture as demonstrated in Fig. 3.\nIn the initial step of ViT, an input image I ∈ RH×W ×C is\nsplit as equal size patches with the size of P × P. Here, H, W ,\nand C represent the height, width, and number of channels of\nthe input image, respectively. Then the patches are flattened\nto form a 2D sequence with the size of (N ×(P2 ·C)), where N\nis the number of extracted patches from an input image. In the\nnext stage of ViT architecture, patches are embedded with\nposition and class information. Then the embedded patches\nare fed to a set of encoder layers in a sequence manner. The\noutput of the encoder layers is then obtained and fed to a\nMulti-Layer Perceptron (MLP) network to produce the class-\nspecific scores. The encoder layers of ViT are much similar\nto the Transformer [36] architecture except GELU [89] non-\nlinear function is used instead of ReLU function.\n80302 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nViT, when trained on large datasets, has demonstrated\nsuperior results compared to the state-of-the-art CNN mod-\nels. Also, authors of ViT tested it in small and medium-\nsized datasets with fine-tuning and showed modest results.\nAfter the success of ViT in image recognition, several vision\nTransformer models have been proposed and it has been\nused in other computer vision tasks. To reduce the compu-\ntational complexity of ViT, Swin Transformer [90] performs\nself-attention locally within non-overlapping windows that\npartition an image and introduces a shifting window par-\ntitioning mechanism for cross-window connections. Unlike\nfixed-size tokens in ViT, Swin Transformer constructs a hier-\narchical representation by starting from small-sized patches\nand then gradually merge the neighboring patches in deeper\nTransformer layers for multi-scale prediction to overcome\nthe scaling problem. Since pure Transformer models are\npoor to capture the local information, CVT [91] incorporates\ntwo convolution-based operations into the ViT architecture,\nnamely convolutional token embedding, and convolutional\nprojection. The predefined positional embedding scheme of\nthe ViT is replaced by a conditional position embedding in the\nCPVT [92] Transformer architecture. The TNT [93] Trans-\nformer further subdivides a 16 × 16 image patch into 4 × 4\nsub-patches using a Transformer-in-Transformer framework.\nInner Transformer blocks and outer Transformer blocks are\nused in TNT to capture the interaction between sub-patches\nand the relationship between patches, respectively. Since ViT\nis less efficient at encoding finer-level features, VOLO [94]\nintroduces light-weight attention mechanism called Out-\nlooker to encode the token representations with finer-level\ninformation efficiently. Overall, ViT has ushered in a new era\nin computer vision tasks.\nIV. TRANSFORMER IN SINGLE OBJECT TRACKING\nBased on the model architecture, feature extraction, and\nfeature integration techniques, recent deep trackers can be\nclassified as three categories: CNN-based trackers [29], [31],\n[32], [33], [34], [95], [96], [97], [98], [99], [100], [101],\nCNN-Transformer based trackers [45], [46], [47], [48], [49],\n[50], [51], [52], [53], [54], [55], [56] and fully-Transformer\nbased trackers [57], [58], [59], [60], [61], [62], [63]. CNN-\nbased trackers rely solely on a CNN architecture for fea-\nture extraction and target detection, while CNN-Transformer\nbased trackers and fully-Transformer based trackers partially\nand fully rely on a Transformer architecture, respectively.\nThe literature on CNN-Transformer based trackers and fully-\nTransformer based trackers is reviewed in this section.\nGenerally, Transformer architectures require a massive\nnumber of training samples [37] to train their models. Since\nthe target is given in the first frame of a tracking sequence,\nobtaining a large number of samples is not possible in VOT\nand hence all of the fully-Transformer based and CNN-\nTransformer based trackers use a pre-trained network and\nconsidered it as the backbone model. In addition, some of\nthese trackers update the target template to capture the tempo-\nral cues during tracking, while others do not. Moreover, they\nFIGURE 4. General architectures of CNN-Transformer, One-stream\nOne-stage fully-Transformer, and Two-stream Two-stage fully-Transformer\nbased trackers. Although ARTrack [68] and SeqTrack [69] belong to the\nOne-stream One-stage category, they are capable of predicting the target\nwithout the need for a prediction head.\ntrained their models on various benchmark datasets such as\nCOCO [103], LaSOT, GOT-10k, TrackingNet, and Youtube-\nBB [104]. We have summarized these details in Table 2 for all\nTransformer-based and CNN-Transformer based approaches,\nproviding information on their backbone network, template\nupdate details, training dataset, and training scheme details.\nWe have conducted a literature review of CNN-\nTransformer based and fully-Transformer based trackers,\nfocusing on their model architecture. As shown in Fig. 4,\nCNN-Transformer based trackers used a CNN backbone for\nfeature extraction and then used a Transformer architecture\nfor feature integration. Fully-Transformer based trackers can\nbe classified further as ‘‘Two-stream Two-stage’’ trackers,\nand ‘‘One-stream One-stage’’ trackers. In Two-stream Two-\nstage trackers, two identical pipeline of network branches\n(two-stream) are used to extract the features of the target\nimage and search image. Also, in this category of trackers,\nfeature extraction and feature fusion of target template and\nsearch region are done in two distinguishable stages (two-\nstage). On the other side, in One-stream One-stage trackers,\na single pipeline of networks are used and the feature extrac-\ntion and feature fusion are done together through a single\nstage. Fig. 5 shows the classification of CNN-Transformer\nbased, and fully-Transformer based trackers.\nA. CNN-TRANSFORMER BASED TRACKERS\nMost of the recent CNN-based trackers [29], [31], [32], [33],\n[34], [95], [96], [97], [98], [99], [100], [101] followed a\nSiamese network architecture by using two identical pipelines\nof CNNs. In these trackers, features of the target template\nand search region are extracted by using two identical CNN\nVOLUME 11, 2023 80303\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nTABLE 2. Summary of CNN-Transformer based and fully-Transformer based trackers.\nbranches. Then the target localization is done by finding the\nsimilarity of target’s features in search region’s features using\na correlation function. Although the correlation operation is\nsimple and fast for feature similarity matching process, it is\nnot good enough to capture the non-linear interaction (occlu-\nsion, deformation, and rotation) between the target template\nand search region and hence the tracker’s performance is\nlimited. To successfully address this issue, researchers have\nstarted incorporating the Transformer for the feature fusion\nprocess, resulting in trackers known as CNN-Transformer\nbased trackers.\nSimilar to most of the CNN-based trackers, CNN-\nTransformer based trackers also use two Siamese-like identi-\ncal pipeline of networks. In the beginning of these pipelines,\ntarget template’s and search region’s features are extracted\nusing a CNN backbone. Then the extracted deep features\nare flattened into vectors and thereafter fed to a Transformer\nto capture the similarity features of the target in the search\nregion.\nThe first CNN-Transformer based tracker was proposed\nby Wang et al. [45] by introducing a Transformer into both\ngenerative and discriminative tracking paradigm. In their\nSiamese-like tracking architecture, a set of template patches\nand search region are fed to a CNN backbone to extract the\ndeep features. Then, as shown in Fig. 6, extracted template\nfeatures are fed to the Transformer’s encoder to capture the\nhigh-quality target’s features using the attention mechanism.\nSimilarly, search region features are fed to the Transformer\ndecoder to produce the decoded features by aggregating the\ninformative target cues from previous frames with the search\n80304 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 5. Classification of CNN-Transformer and fully-Transformer\ntrackers based on their model architecture and tracking pipeline. Multiple\npipeline of networks are used in ‘Two-stream Two-stage’ trackers while\nsingle pipeline of networks are used in ‘One-stream’ trackers. Also,\nfeature extraction and feature fusion are done through two\ndistinguishable stages in ‘Two-stage’ trackers while they are done\ntogether in ‘One-Stage’ trackers.\nFIGURE 6. Architecture of the first CNN-Transformer based tracker[45]\nwith TrSiam and TrDiMP branches. Target template set’s features and\nsearch region features are fed to the Transformer encoder and decoder to\nlocate the target.\nregion features. In the TrSiam pipeline, similar to the SiamFC\n[24] tracker, the target feature is cropped from the encoded\nfeature, and then cross-correlated with the decoded features to\nlocate the target location. In the TrDiMP pipeline, end-to-end\nDiscriminative Correlation Filters(DCF) [96] are applied on\nthe encoded features to generate a target model, which is then\nused to convolve with decoded features to locate the target\nin the search region. Since a collection of target template\nfeatures is fed to the Transformer of this tracker, it is able\nto locate the target even with severe appearance changes.\nSimilar to the TrDiMP [45] tracker, Yu et al. [49] intro-\nduced the encoder-decoder Transformer architecture [36] in\nVOT and their tracker is referred to as DTT. They have also\nused a Siamese-like tracking pipeline and extracted the deep\nfeatures using a backbone CNN architecture. In their track-\ning model, target templates are fed with background scenes\nand then the Transformer architecture captures the most dis-\ncriminative cues of the target. Since their approach involves\nconducting the tracking without the need to train a separate\nFIGURE 7. Architecture of the TransT tracker [46]. It has a CNN backbone\nnetwork, a Transformer network, and a prediction network for deep\nfeature extraction, feature fusion, and target localization, respectively.\ndiscriminative model, it is simple and has demonstrated high\ntracking speed in benchmark datasets.\nRecently, Yang et al. [64] found that the TrDiMP [45]\ntracker is computationally expensive since it computes the\nattention in all possible spatial locations of the target template\nfeature, and hence the model weights are increased with\nrespect to the pixel counts of the feature maps. To address this\nissue, they proposed a border-aware tracking framework with\ndeformable Transformers, referred to as the BANDT tracker.\nInstead of computing attention among all possible locations\nof the target template features, the BANDT tracker only cal-\nculates attention in a few specific locations around the target\nboundary. Based on the inspiration from the Deformable\nDETR Transformer [105], the BANDT tracker extracts a\nsmall set of key sampling points around the border of the\ntarget object in the template image. Since the BANDT tracker\nenhances the border features of the target object, it shows\nbetter performance than the TrDiMP tracker with fewer com-\nputational costs.\nAnother Siamese-like architecture is proposed in TransT\n[46] tracker. As demonstrated in Fig. 7, TransT tracker has\nthree modules: a CNN backbone network, a Transformer\nbased feature fusion network, and a prediction network.\nSimilar to other CNN-Transformer based trackers, target tem-\nplate’s and search region’s features are extracted using the\nResNet50 [106] model. Then these features are reshaped\nusing a 1 × 1 convolutional layer and fed to the feature\nfusion network. The Transformer-based feature fusion net-\nwork has a set of layers, and each layer has an ego-context\naugment module (ECA) and a cross-feature augment (CFA)\nmodule to enhance the self-attention and cross-attention,\nrespectively. Finally, fused features are fed to a prediction\nhead, and it locates the target and find the coordinates\nusing a simple classification and regression branches, respec-\ntively. TransT tracker showed excellent performance than the\nCNN-based trackers by utilizing the Transformer for feature\nfusion instead of the correlation matching of the previous\napproaches.\nSeveral CNN-Transformer trackers are proposed by train-\ning the Transformer to capture the relationship between\nthe target template and search region features. The TrTr\ntracker [48] trained a Transformer to capture the global\nVOLUME 11, 2023 80305\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 8. Architecture of the STARK tracker[47]. It captures the spatial\nand temporal information of the target using the Transformer.\ninformation of the target template and then used that cues to\nfind the accurate correlation between target and search region.\nAnother similar approach is proposed by Hu et al. [107] with\na template update mechanism. Recently, Zhong et al. [54]\nproposed a tracker referred to as CTT, and improved the\nfeature fusion between target and search region features by\nincluding a correlation module into the Transformer archi-\ntecture. CTT tracker avoids the background interference in\nlong-term tracking by using their correlation based Trans-\nformer architecture.\nIn contrast to the previously mentioned CNN-Transformer\nbased trackers, Yan et al. [47] proposed a new Transformer\narchitecture for VOT based on the DETR [38] object detec-\ntion Transformer. Their tracker is referred to as STARK and\nthey have trained the Transformer to capture the spatial and\ntemporal cues of the target object. ResNet [106] is used\nto extract the deep features of the initial target template,\ndynamic target template, and search regions. Then, as shown\nin Fig. 8, these features are flattened, concatenated, and then\nfed to a Transformer with encoder and decoder architectures.\nThe STARK tracker’s Transformer consists of a set of encoder\nlayers, with each layer containing a multi-head self-attention\nmodule and a feed-forward network module. The encoder\ncaptures the feature dependencies between every element in\nthe tracking sequence, and reinforces the original features\nwith global contextual information. This enables the model\nto learn discriminative features for object localization. The\ndecoder of the tracker learns a query embedding to predict the\nspatial positions of the target object by using the Transformer-\nbased detection approach of the DETR. To determine the\ntarget’s bounding box in the current frame, STARK intro-\nduced a corner-based prediction head. Additionally, a score\nhead is learned to control the updates of the dynamic template\nimages. Since the architecture of the STARK is simple and\ncapable of capturing the spatio-temporal information of the\ntarget, it has demonstrated better tracking robustness and\ngood tracking speed compared to other CNN-based trackers.\nSimilar to the STARK tracker, Mayer et al. [51] proposed\na tracker called ToMP, utilizing the DETR object detection\nFIGURE 9. Architecture of the HiFT tracker [50]. It extracted the\nmulti-level convolutional features of the target template and search\nregion and then fed the high-resolution features to encoder and\nlow-resolution features to decoder.\nTransformer. In this tracker, the target template and search\nregion features from the CNN backbone are concatenated and\njointly processed by the Transformer encoder and decoder.\nAdditionally, similar to the DiMP [96] tracker, the ToMP\ntracker incorporates a target model for localizing the tar-\nget. However, unlike DiMP, the weights of the target model\nare obtained using the Transformer architecture instead of\na discriminative correlation filter. According to the reported\nexperimental results, the ToMP tracker demonstrates superior\ntracking performance and speed compared to the STARK\ntracker.\nMost of the CNN-Transformer based trackers [45], [46],\n[47], [49], [51] utilized the deep features from the last con-\nvolutional layer of a backbone CNN. Different from these\ntrackers, as illustrated in Fig. 9, Cao et al. [50] utilized the\nmulti-level convolutional features of the target template and\nsearch region from the last three layers (3 to 5) of the\nAlexNet [20] model. Their tracker is referred to as HiFT\nand it was originally proposed for aerial tracking. In this\ntracker, high resolution cross-correlated features are fed to the\nTransformer’s encoder to capture the spatial cues of the target\nwith different scales. Also, low resolution cross-correlated\nfeatures of the last convolutinal layers of the target template\nand search region are fed to the Transformer’s decoder to\ncapture the semantic features. Finally, a prediction head is\napplied on the Transformer features to locate the target state.\nHiFT tracker showed better performance than the CNN-based\ntrackers because of its strong discriminative capability.\nSimilar to HiFT tracker, Xing et al. [52] proposed an aerial\ntracking approach, referred to as SiamTPN, by utilizing\nthe multi-level convolutional features of a backbone CNN.\nHowever, their Transformer architecture is different than the\nHiFT tracker since they used a Transformer Pyramid Network\n(TPN) to fuse the multi-level features of the target and search\nregion. In addition, instead of the multi-head self attention\nlayer, a pooling attention (PA) layer is used in the Trans-\nformer architecture of this tracker to reduce the computational\ncomplexity and memory load. Based on the reported results,\nSiamTPN tracker is able to track a target in real-time speed\non a CPU.\nAll of the early CNN-Transformer based approaches [45],\n[46], [47], [49], [50], [51], [52] extracted the deep features\n80306 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 10. Architecture of the CSWinTT tracker[55]. Instead of\npixel-level attention, this tracker considered the window-level attention\nby fed the flattened window feature pairs to the Transformer.\nof the target template and search region using a CNN back-\nbone and then flattened them to fed a Transformer network.\nAlthough the Transformer captures the pixel-wise atten-\ntion from the flattened features, the valuable object-level\ncues from the relative pixel positions are lost. Recently,\nSong et al. [55] noticed this issue and proposed a multi-scale\ncyclic shifting window Transformer tracker (CSWinTT) to\ntackle this issue. Similar to other CNN-Transformer based\ntrackers, CSWinTT also extracted the deep features of the\ntarget template and search region using the ResNet-50. There-\nafter, as shown in Fig. 10, these features are split as small\nwindows and then each windows are flattened and then fed\nto the Transformer to capture the window-wise attention.\nIn addition, a multi-head multi-scale attention module is\nused in the Transformer of the CSWinTT to find the rele-\nvance between target template and search region windows\nat a particular scale. Moreover, a cyclic shifting technique\nis proposed in the window-wise attention module of the\nCSWinTT to fuse the outputs of each attention heads with\ndifferent scales. Although the cyclic shifting and window-\nwise attention are computationally expensive, CSWinTT\napproach showed excellent tracking performance than the\nother CNN-Transformer based trackers because it considered\nthe window-level attention instead of the pixel-level attention.\nThe first CNN-Transformer based unified tracker is pro-\nposed by Ma et al. [53] and it is referred to as UTT. This\nunified tracker provided a single model architecture for single\nobject tracking and multi object tracking to track the single\nand multiple targets together. In this tracker, deep features\nof the target template and search region, obtained from a\nbackbone CNN, are fed to a Transformer to locate a target.\nIn this UTT approach, target-specific features are extracted\nby using a decoder component of the Transformer. Similarly,\na proposal decoder component is used to extract the search\nregion features with respect to the target. Finally, these two\nfeatures are fed to a Transformer to predict the coordinates of\nthe target in the search region. Experimental results showed\nthat UTT approach successfully handles the single and multi\nobject tracking through a single tracking architecture.\nIn most of the CNN-Transformer based trackers, CNN\nfeatures of the target template and search regions are enriched\nusing the self-attention mechanism. Then, the relationship\nbetween them is computed using the cross-attention modules.\nBased on the correlation between each query-key pair, the\nFIGURE 11. Architecture of the AiATrack approach [56].\nAn attention-in-attention (AiA) module is used in this tracker to enrich\nthe cross-correlation attention by searching for consensus among all\ncorrelation vectors and hence ignores the noisy correlations.\nhighest correlated pair is selected and then used to identify\nthe target in the search region. Gao et al. [56] found that if a\nkey has high correlation with a query and the neighbors of\nthat key has less correlation with that particular query then\nthat correlation should be a noise. Based on this finding,\nas illustrated in Fig. 11, they have proposed an attention-in-\nattention (AiA) module by including an inner attention mod-\nule in Transformer architecture and their tracker is referred\nto as AiATrack. The proposed inner attention module of the\nAiATrack approach improves the attention by searching for\nconsensus among all correlation vectors and ignores the noisy\ncorrelations. In addition, AiATrack approach introduces a\nlearnable target-background embedding to differentiate the\ntarget from the background while maintaining the contextual\ninformation. Because of the enhanced attention mechanism,\nAiATrack approach successfully avoid the distractor objects\nand hence improves the tracking performance.\nIn summary, CNN-Transformer based trackers extracted\nthe deep features of the target template and search region\nusing a CNN backbone such as ResNet or AlexNet. Then\nthe relationship between these features is computed using the\nattention mechanism of a Transformer. Finally, a prediction\nhead is employed to locate the target using the features gen-\nerated by the Transformer. CNN-Transformer based trackers\nare successfully outperforms the CNN-based trackers since\nthey have used a learnable Transformer instead of the linear\ncross-correlation operation. Although, a few early track-\ners borrowed the Transformer architectures from the object\ndetection task and used them without any modification, recent\napproaches identified the Transformer based tracking issues\nand then modified their architectures accordingly. In overall,\nCNN-Transformer based trackers showed excellent tracking\nperformance than the CNN-based trackers.\nB. FULLY-TRANSFORMER BASED TRACKERS\nAlthough CNN-Transformer based trackers utilize the atten-\ntion mechanism of Transformers for the feature integration of\nVOLUME 11, 2023 80307\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 12. Architecture of the DualTFR tracker [57]. It has a set of local\nattention blocks and a global attention block to capture the attention in a\nsmall sized window and whole image, respectively. At the end of the\nnetwork, a cross-attention block is used to fuse the features of both\nbranches.\nthe target template and search region, they still rely on con-\nvolutional features as they use a backbone CNN for feature\nextraction. Since CNNs capture features through local convo-\nlutional kernels, CNN-Transformer based trackers struggle to\ncapture global feature representations.\nDue to the success of the fully-Transformer architectures\nin other computer vision tasks, researchers start using them\nin single object tracking. Based on the tracking network\nformulation, we classified the fully-Transformer trackers as\nTwo-stream Two-stage trackers and One-stream One-stage\ntrackers and reviewed their literature in the following sub-\nsections.\n1) TWO-STREAM TWO-STAGE TRACKERS\nTwo-stream Two-stage trackers consist of two identical and\nindividual Transformer-based tracking pipelines to extract the\nfeatures of the target template and search region. Another\nTransformer network is then employed to find the relation-\nship between these features. Finally, a prediction head is\nutilized to locate the target by using the attended features.\nThe first fully-Transformer based Two-stream Two-stage\ntracker is proposed by Xie et al. [57], and it is referred to as\nDualTFR. In this tracker, template and search region images\nare split as tokens and then fed to the corresponding feature\nextraction branches. DualTFR has a set of local attention\nblocks (LAB) in feature extraction branches to capture the\nattention in a small size window. Then the extracted features\nare fed to a global attention block (GAB) to capture the long\nrange dependencies. Finally, as demonstrated in Fig. 12, out-\nput features of both branches are fed to a cross-attention block\nto compute the attention between target template and search\nregion. Since the LAB computes attention within a small\nwindow of tokens on the high-resolution feature maps and\nthe GAB computes the attention among all tokens of the same\npicture on the low-resolution feature maps, DualTFR tracker\nsuccessfully achieved the high accuracy while maintaining\nabove real-time speed.\nLin et al. [58] also used a two-branch fully-Transformer\narchitecture for VOT and it is referred to as SwinTrack. They\nhave utilized the Swin Transformer [90] for feature extraction\nsince it showed better performance with fewer computa-\ntional cost in object detection. In Swin Transformer, images\nFIGURE 13. Architecture of the SwinTrack tracker [58]. It utilized the Swin\nTransformer [90] for feature extraction. Then the features are\nconcatenated with their positional encoding and then to fed to the\nencoder and decoder to enhance them using self-attention and\ncross-attention, respectively.\nand features are partitioned into non-overlapping windows\nand then the self-attention is computed within the window.\nAlso, windows are shifted in the next layers to maintain\nthe connectivity. After the feature extraction using the Swin\nTransformer, features of the template and search region are\nconcatenated as shown in Fig. 13. Then the concatenated\nfeatures and their corresponding positional embedding are\nfed to the Transformer encoder to enrich them using the self-\nattention mechanism. Then a Transformer decoder block is\nused to find the relationship between the template and search\nregion features using the cross-attention mechanism. Finally,\nthe target is located by feeding the features to a prediction\nhead, which has a binary classifier and a bounding box regres-\nsion module. Based on the tracking accuracy, SwinTrack\noutperformed CNN-Transformer based trackers, and CNN-\nbased trackers in several benchmark datasets.\nWhile the self-attention mechanism of the Transformer\ncaptures long-range dependencies, it may not adequately cap-\nture target-specific cues, making it susceptible to distraction\nby background clutter. Fu et al. [59] noticed this issue and\nthen proposed a tracker, referred to as SparseTT, by using a\nsparse attention module to provide the target-specific atten-\ntion in the search region features. In this tracker, the Swin\nTransformer is used to extract the target template and search\nregion features and then fed them to a sparse Transformer.\nEncoder blocks of the sparse Transformer are similar to\nthe other Transformer architectures. Sparse multi-head self-\nattention modules are used in the decoder blocks to focus\nthe foreground regions. Finally, enhanced features are fed to\na double-head prediction network to detect the target with\nbounding box coordinates.\nIn overall, Two-stream Two-stage fully-Transformer track-\ners used a backbone Transformer model to extract the features\nof the target template and search region. Another Transformer\nis used to fuse and enhance the extracted futures. Finally,\na small prediction network is used to locate the target. All\nof the Two-stream Two-stage trackers have a simple and\nneat tracking architecture and showed better performance\nthan the CNN-based trackers and CNN-Transformer based\ntrackers since they fully utilized the attention mechanism of\nthe Transformer for tracking a target.\n80308 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 14. Architecture of the MixFormer Tracker[60]. It has a set of\nMixed Attention Modules (MAM) to simultaneously extract and integrate\nthe features of the target template and search region and to extract the\ntarget-specific discriminative cues.\n2) ONE-STREAM ONE-STAGE TRACKERS\nOne-stream One-stage Trackers have a single pipeline of\nfully-Transformer based network architecture. Also, feature\nextraction and feature fusion process are done in a single stage\nin these approaches instead of two stages in the previously\nmentioned tracking methods.\nRecently, Cui et al. [60] found that combining the feature\nextraction and feature fusion processes is important for object\ntracking, as it enables extraction of more target-specific cues\nin the search region and improves correlation. Based on this\nfact, they proposed a fully-Transformer based One-stream\nOne-stage tracker referred to as MixFormer. As illustrated\nin Fig. 14, a set of Mixed Attention Modules (MAM) are\nused in the MixFormer tracker to simultaneously extract and\nintegrate the features of the target template and search region.\nMixFormer Tracker consumes multiple target templates and\nsearch regions are the inputs and locate the target using a sim-\nple fully-convolutional based prediction head network. Also,\nMixFormer utilized the pre-trained CVT Transformer [91]\nto design the MAM modules since CVT is excellent to cap-\nture the local and global feature dependencies in an image.\nInstead of the self-attention mechanism of the CVT, MAM\nemploys a dual attention mechanism on target template and\nsearch region tokens to capture the target-specific and search-\nspecific cues, respectively. In addition, an asymmetric mixed\nattention technique is used in the MAM modules to reduce\nthe computational cost by eliminating the unnecessary cross-\nattention between the tokens of the target and search region.\nBased on the reported results, MixFormer showed excellent\ntracking performance in benchmark datasets. However, Mix-\nFormer showed poor tracking speed since the MAM modules\nare computationally inefficient.\nAnother One-stream One-stage tracker is proposed by\nChen et al. [61] and it is referred to as SimTrack. In this\ntracker, as shown in Fig. 15, the pre-trained ViT [37] model\nis utilized as the backbone Transformer to combine fea-\nture extraction and fusion. In SimTrack approach, target\ntemplate and search region are split as a set of tokens,\nconcatenated, and then fed to the backbone Transformer\nwith their positional embedding. Since the target template\ntokens contain some background regions due to the splitting\nFIGURE 15. Architecture of the SimTrack Approach [61]. In this tracker,\na target template, a search region, and an exact target region from the\ntemplate are split as tokens and then fed to the ViT[37] backbone to\nlocate the target.\nprocess, SimTrack followed a foveal windowing technique\nto accurately capture the target-specific cues. In the foveal\nwindowing technique, a smaller region of the template image\nis cropped with the target in middle and then serialized into\nimage tokens. In addition to the tokens of target template and\nsearch region, foveal sequence also fed to the Transformer to\ncapture more target-specific features.\nOSTrack [62] is another One-stream One-stage approach\nthat combines the feature learning and feature fusion pro-\ncesses using the ViT backbone, as demonstrated in Fig. 16.\nOSTrack utilizes a self-supervised learning-based Masked\nAuto Encoder (MAE) [108] pre-trained model to initialize\nthe ViT backbone. The authors of OSTrack have found that\nsome of the tokens from the search image contain background\ninformation, and hence including these tokens in the tracking\nprocess is unnecessary. Based on this fact, OSTrack includes\nan early candidate elimination module in some of the encoder\nlayers to remove tokens containing background information.\nDue to the candidate elimination module, the tracking speed\nand accuracy of OSTrack are increased. Since OSTrack effi-\nciently uses the information flow between the features of the\ntarget template and search region, target-specific discrimina-\ntive cues are extracted, and unnecessary background features\nare eliminated. Therefore, it has shown excellent tracking\nperformance with high tracking speed in several benchmark\ndatasets.\nDue to the great success of OSTrack in the tracking com-\nmunity, several recent follow-up approaches [63], [65], [66],\n[67] have been proposed, utilizing self-supervised learning\nbased MAE pre-trained model to initialize the backbone net-\nwork. Lan et al. [63] upgraded the performance of OSTrack\nby using a candidate token elimination module and including\na set of dynamic templates to adopt the temporal features of\nthe target. Their tracker, referred to as ProContEXT, is shown\nin Fig. 17. The ProContEXT tracker focuses on capturing the\nspatial and temporal cues of the target templates by utilizing\na context-aware self-attention module. In this tracker, static\nVOLUME 11, 2023 80309\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 16. Architecture of the OSTrack Approach[62]. It used an early\nelimination module in encoder layers to identify and remove the features\nof the background tokens from the search image and hence the\nperformance of the tracker is increased.\nFIGURE 17. Architecture of the ProContEXT Tracker [63]. Spatial and\ntemporal cues are captured using a context-aware self-attention module\nby feeding a set of static and dynamic target templates. Also,\nan upgraded candidate token elimination module is used in this tracker.\ntarget templates, dynamic target templates with spatial and\ntemporal cues, and the search region are split and then fed\nto the attention module. Additionally, the candidate token\nelimination module of ProContEXT performs better than\nOSTrack as it incorporates temporal cues in the removal\nof background tokens. Based on the reported results, the\nProContEXT approach outperformed OSTrack and demon-\nstrated state-of-the-art performance in tracking benchmarks.\nThe success of the One-stream One-Stage trackers lies in\nthe information flow between the features of target template\npatches and search region patches. Although the attention\noperation is used to identify target-specific features in search\nregions, it is not necessary to calculate the attention between\nthe template patches and all of the search patches since\npatches containing background and distractor features can\ndegrade the efficiency and accuracy of the tracker. Although\nthe OSTrack [62] and ProContEXT [63] trackers incorporate\nmodules to eliminate background patches in some encoder\nlayers, there is a need for a more robust and generalized mech-\nanism. Recently, Gao et al. [67] have proposed a Generalized\nRelation Modeling (GRM) mechanism to address this limita-\ntion in One-stream One-stage trackers. In the first stage of\nGRM tracker, a simple MLP network is utilized to determine\nthe probabilities of target template features in search region\npatches. Subsequently, the search region patches are divided\ninto two groups, with only the high probability search patches\nallowed to interact with the template patches in the encoder\nlayers. This adaptive selection of search region patches\nenhances the flexibility of relation modeling and improves\nFIGURE 18. Architecture of the DropMAE [65] masked auto encoder. It is\nused to capture the spatial cues within an individual image and to\ncapture the correlated spatial cues between target and search images.\nThe OSTrack[62] approach showed better performance while fine-tuning\nthe DropMAE as the backbone compared to the MAE[108] backbone.\ndiscrimination capabilities. The experimental results demon-\nstrate that the GRM tracker outperforms OSTrack with an\napproximately 2% higher success rate and improved tracking\nspeed.\nThe state-of-the-art One-stream One-stage trackers [60],\n[61], [62] do not fully utilize the temporal features, mak-\ning them less robust for appearance changes. Recently,\nXie et al. [66] identified this limitation and proposed an end-\nto-end Transformer tracker called VideoTrack, which aims to\nexploit the temporal context. VideoTrack captures the spatio-\ntemporal features of the tracking sequence by feeding patches\nfrom a set of adjacent frames to the Transformer. The Video-\nTrack approach encodes inter-frame and intra-frame patches\nusing distinct encoding schemes and then feeds them to a\nTransformer encoder to capture spatio-temporal features. Due\nto the spatio-temporal feature learning, VideoTrack outper-\nforms OSTrack [62] and MixFormer [60] approaches with a\nconsiderable margin.\nMost single-object tracking approaches traditionally rely\non pre-trained models initialized through supervised learning.\nHowever, recent fully Transformer-based trackers, such as\nOSTrack [62], ProContEXT [63], GRM [67], and Video-\nTrack [66], have demonstrated that initializing the backbone\nwith a self-supervised learning based Masked Autoencoder\n(MAE) [108] pre-trained model can achieve higher tracking\naccuracy compared to models based on supervised learning.\nThis improvement can be attributed to the MAE’s ability\nto capture fine-grained local structures within an image,\nwhich are essential for accurate target localization. Recently,\nWu et al. [65] discovered that the MAE architecture exhibits\na lack of robustness when applied to feature matching tasks\nbetween two images. As a result, a modified variant of\nthe MAE architecture named DropMAE was introduced,\nas depicted in Fig. 18, which is specifically designed to\nimprove feature matching for tracking and segmentation\ntasks. DropMAE captures spatial cues within individual\nimages and also captures the correlated spatial cues between\ntwo frames by randomly masking the input frames and\nprocessing them through the encoder-decoder architecture.\nAdditionally, DropMAE follows an attention dropout mech-\nanism that restricts the interaction between tokens within the\n80310 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nsame frame. This restriction leads to increased interaction\nbetween the tokens of search and target image pairs, resulting\nin more reliable capture of temporal cues. The experimen-\ntal results demonstrated that the OSTrack [62] approach\nachieved superior performance when utilizing DropMAE to\ninitialize the backbone compared to initializing with MAE\nbackbone.\nClosely related to DropMAE, another masked encoder-\nbased pre-trained model has been specifically designed and\ntrained for the tracking task, referred to as MAT [102]. Simi-\nlar to DropMAE, MAT randomly masks patches of template\nand search image pairs, which are then jointly processed\nby the encoder to capture their visual representations. After\nencoding, in contrast to the single decoder of other mod-\nels [65], [108], MAT employs two identical decoders to sep-\narately reconstruct the search image and the target region in\nthe search image. The encoded features of the target template\nare used to reconstruct the target region in the search image\nthat enables the capturing of tracking-specific representations\nin the search image. Finally, a simple and lightweight tracker\n(MATTrack) is designed to evaluate the representation of the\nMAT masked encoder.\nAll present CNN-based and fully-Transformer based\napproaches treat object tracking as a template matching prob-\nlem between the target image and the search region. Although\nsome approaches follow a template update mechanism, the\ntemporal dependencies between the frames are neglected or\nnot fully investigated. Recently, Wei et al. [68] proposed a\ntracker called ARTrack, which treats object tracking as a\ncoordinate sequence interpretation problem. Taking inspi-\nration from language modeling, ARTrack interprets target\ntrajectories by learning from previous states in a sequence of\nframes. The tracking pipeline of ARTrack differs from other\nOne-stream One-stage Transformer trackers as it employs\nan encoder-decoder architecture and lacks a prediction head.\nBy treating tracking as a coordinate sequence interpretation\ntask and adopting a general encoder-decoder architecture,\nARTrack simplifies the tracking pipeline and achieves supe-\nrior performance compared to existing trackers that rely on\nprediction heads.\nClosely related to the ARTrack [68], the recently proposed\nSeqTrack [69] tracker also considers tracking as a sequence\nlearning problem as shown in Fig. 19. This tracker uti-\nlizes an encoder-decoder Transformer architecture to extract\nvisual features and autoregressively generate the bounding\nbox coordinates of the target, respectively. Different from\nARTrack, in SeqTrack, the predicted bounding box values\nrely solely on the target stage in the previous frame, achieved\nthrough the use of an attention masking technique in the\ndecoder. Experimental results shows that SeqTrack better\nperformance than the template matching based trackers.\nIn summary, One-stream One-stage trackers combined the\nfeature extraction and feature fusion process using a fully-\nTransformer architecture. In these trackers, target template\nand search region images are split as tokens and concate-\nnated with their positional embedding, and then fed to a\nFIGURE 19. Architecture of the SeqTeack [69] approach. It treats the\ntracking as a sequence learning problem instead of template matching.\nWithout a prediction head, it is able to locate a target using an\nend-to-end Transformer architecture.\nTransformer. Since these trackers extract features with a\nsingle Transformer network, the features of the template\nand search regions are efficiently integrated, leading to\nmore discriminative features being identified and unnec-\nessary features being eliminated. Based on these facts,\nfully-Transformer based One-stream One-stage trackers have\nshown outstanding performance compared to other types of\ntrackers on all benchmark datasets.\nV. EXPERIMENTAL ANALYSIS\nIn the last two decades, numerous approaches have been\nproposed for single object tracking. Since their performance\nis evaluated on various benchmark datasets using different\nevaluation metrics, it is crucial to experimentally evaluate\nthese approaches to identify future directions, particularly\nafter the introduction of Transformers in VOT. This study\nfocuses on evaluating the tracking robustness and compu-\ntational efficiency performances of CNN-Transformer based\ntrackers and fully-Transformer based trackers. Additionally,\nwe have included 12 recently proposed CNN-based track-\ners in this experimental evaluation study. Five benchmark\ndatasets are utilized, and their details and evaluation metrics\nare described in Section V-A. The results of tracking robust-\nness and efficiency are presented in Section V-B and Section\nV-C, respectively.\nA. BENCHMARK DATASETS AND EVALUATION METRICS\nSeveral benchmark datasets are constructed and publicly\navailable for VOT. Each dataset is different from the others\nbased on the target object class, number of sequences, anno-\ntation method, length of the tracking sequence, attributes,\nand complexity. Additionally, these datasets follow various\nperformance metrics to measure the trackers’ performance.\nTo analyze the performance of recent trackers, we have\nselected five benchmark datasets: OTB100, UA V123, LaSOT,\nTrackingNet, and GOT-10k. The details of these datasets are\nsummarized in Table 3.\nOTB was the first widely used benchmark dataset in VOT.\nIt gained widespread acceptance among researchers due to\nVOLUME 11, 2023 80311\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nTABLE 3. Details of the datasets used in experimental analysis study. The abbreviations are denoted as SV for scale variation, ARC for aspect ratio\nchange, FM for fast motion, LR for low resolution, OV for out-of-view, IV for illumination variation, CM for camera motion, MB for motion blur, BC for\nbackground clutter, SOB for Similar Object, DEF for deformation, ROT for rotation, IPR for in-plane rotation, OPR for out-of-plane rotation, OCC for\nocclusion, FOC for full occlusion, POC for partial occlusion, and VC for viewpoint change.\nits simplicity and better evaluation scheme. It is initially\nproposed with 51 short tracking video sequences and they\nare referred to as OTB2013 [109]. Few years later, another\n49 sequences are included into the benchmark and then the\nentire sequences are referred to as OTB100 [76]. We have\nselected OTB100 to evaluate the performance of the trackers\nwith various tracking attributes (scenarios). Similar to other\nresearchers, precision and success plots are used to mea-\nsure the performance of a tracker in OTB100. Precision in\nOTB100 is computed by calculating the distance between\nthe midpoints of the tracked target bounding box and the\ncorresponding ground truth bounding box. The precision plot\nis used to show the proportion of frames in which the tracking\nlocations are within a threshold distance. In this study, thresh-\nold of 20 pixel distance is used to rank the trackers based\non their precision scores. Success of a tracker is measured\nin OTB100 by measuring the average overlap scores and it is\ncomputed as follows:\nOverlap_Score = | Bt ∩ Bg |\n| Bt ∪ Bg |, (7)\nwhere Bt and Bg are the bounding boxes of tracked target\nand ground-truth, respectively. Also, ∩ and ∪ are denoting\nintersection and union operations, respectively. The area-\nunder-curve (AUC) scores are used to rank the trackers in\nsuccess plots, representing the average number of successful\nframes when the overlapping scores change from 0 to 1.\nUA V123 [84] dataset contains a total of 123 video\nsequences from an aerial viewpoint. All the video sequences\nof UA V123 are captured by using unmanned aerial vehicles\n(UA V). Compare to OTB100 and other benchmark datasets,\ntarget object is small in UA V123 and the tracking sequences\nhave several distractor objects and long occlusions. Since\nboth the OTB100 and UA V123 datasets have a small number\nof sequences with fewer frames, they are not sufficient to train\ndeep learning-based tracking architectures. We have used the\nUA V123 dataset to evaluate the aerial tracking performances\nof trackers and to measure their occlusion handling capability.\nUA V123 dataset follows the same evaluation scheme of\nOTB100 to compare the performance of trackers.\nTrackingNet [83] is the first large scale benchmark dataset\nand contains more than 30K video sequences. Tracking\nvideo sequences of TrackingNet are sampled from real-world\nYouTube videos and 30,132 and 511 numbers of sequences\nare allocated for training and testing, respectively. The Track-\ningNet dataset contains a rich distribution of target object\nclasses, with almost 14 million bounding boxes. The average\nlength of tracking sequences is less than 500 frames, and\nhence the TrackingNet dataset is not suitable to evaluate a\ntracker’s long-term tracking capability. However, we have\nused the TrackingNet dataset to evaluate the performance\nof trackers in real-world scenarios across a wide range of\nclasses. Similar to the OTB100, TrackingNet dataset also\nused the precision and success plots to evaluate the perfor-\nmance of trackers. In addition to these metrics, normalized\nprecision is used since the precision is depending on the sizes\nof the images and bounding boxes. The normalized precision\n(Pn) metric is computed as:\nPn = ∥W(Bt − Bg)∥2 (8)\nwhere W denotes the size of the bounding box.\nRecently, the large-scale single object tracking (LaSOT)\nbenchmark [81] has been developed, and it is one of the\nlargest tracking datasets to date. On average, more than\n2500 frames are in a tracking sequence in LaSOT, and the\ntotal duration of video sequences are more than 32 hours.\nIn addition, target objects are annotated with detail infor-\nmation such as absent label, in which out-of-view and fully\noccluded target are labeled. Furthermore, the LaSOT dataset\nmaintained the class balance in video sequences by contain-\ning equal number of video sequences for each target class.\nSimilar to the TrackingNet dataset, LaSOT dataset used the\nprecision, success, and normalized precision metrics to mea-\nsure the performance of the trackers. In this experimental\nanalysis study, LaSOT benchmark is mainly used to evaluate\nthe long-term tracking performances of the trackers.\n80312 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nGOT-10k [82] is another large scale tracking benchmark\nand it has 10,000 video sequences which were captured from\nreal-world scenarios. Compare to other benchmark datasets,\nGOT-10k has a huge number of target object classes and\nalmost 1.5 million bounding boxes are used in annotation.\nFurthermore, the annotations include motion and absent\nlabels, allowing for the efficient evaluation of trackers that\ncan handle motion and occlusions. The average overlap (AO)\nand success rate are used to evaluate the trackers in GOT-10k\nand they are calculated by measuring the average overlaps\nbetween Bt and Bg, and the percentage of correctly tracked\nframes, respectively. We have evaluated the trackers on GOT-\n10k to measure the one-shot tracking performances of the\ntrackers since the training object classes are not overlapped\nwith testing object classes.\nB. TRACKING PERFORMANCE ANALYSIS\nWe have presented the qualitative results of the trackers in this\nsection. To conduct an unbiased evaluation and compare their\ntracking performances, we reproduced the success and preci-\nsion scores of the trackers and their attribute-wise tracking\nresults using their source codes. Additionally, we considered\nthe reported results of a few recent trackers since their source\ncodes are unavailable.\nWe have selected 37 single-object trackers for this experi-\nmental evaluation study. They were published in the last four\nyears in reputed conferences and indexed journals. Although\nwe have included the ARTrack [68] in the literature review,\nit is not included in the experimental evaluation because the\nraw results or tracking model of this tracker are still not\npublicly available. In this evaluation, these selected trackers\nare categorized as CNN-based trackers, CNN-Transformer\nbased trackers, and fully-Transformer based trackers and then\ntheir category-wise performances are discussed. The overall\nperformances of these trackers are summarized in the Table 4\nand their attribute-wise results are detailed in the Table 5.\nTracking performances of these approaches for each bench-\nmark datasets are discussed in the following subsections.\n1) ANALYSIS ON OTB100 DATASET\nWe used the official toolkit of OTB100 to evaluate the track-\ning performance of the trackers. Success plots were ranked\nbased on the area-under-curve (AUC) scores, while the pre-\ncision (P) scores at a threshold of 20 pixels were used to\nevaluate the trackers. In addition to the overall evaluation, the\nperformances of the trackers were measured based on eleven\ntracking attributes using the toolkit and their success plots,\nas shown in Fig. 20.\nBased on the overall success rates and precision scores\nin Table 4, both CNN-Transformer based trackers and\nCNN-based trackers have achieved overall high perfor-\nmance. TrTr [48] tracker showed excellent performance on\nOTB100 in terms of accuracy and precision by replacing\nthe cross-correlation mechanism of Siamese tracking with\na Transformer. Similarly, TrDiMP [45] showed competi-\ntive success and precision scores by exploiting temporal\ncues with a CNN-Transformer based architecture. On the\nother side, CNN-based Siamese trackers: SiamAttn [97] and\nSiamRN [34] showed high success and precision scores,\nrespectively.\nCompared to other benchmark datasets, the tracking per-\nformances of fully-Transformer based trackers are slightly\nlower than the other two types on OTB100. CNN-\nTransformer based trackers and CNN-based trackers showed\nbetter performances in OTB100 by capturing and match-\ning convolutional features that represent local-regional cues.\nSince most of the OTB videos have fewer frames, the appear-\nance of the target remains the same in many sequences.\nTherefore, CNN-based feature extraction and matching have\nshown excellent tracking results. On the other hand, the\nperformances of fully-Transformer based approaches mainly\nrely on their attention mechanism and global feature captur-\ning capabilities, and their performances are slightly limited on\nOTB100 since most tracking sequences have low-resolution\nvideos with fewer frames.\nThe attribute-based evaluation on OTB100 showed that the\ndataset is no longer challenging for recent trackers, based\non the results in Table 5 and the plots in Fig. 20. The fully-\nTransformer based trackers: MixFormer [60], SparseTT [59],\nand ProContEXT [63] showed better performance than the\nCNN-based trackers in many challenging attributes. In par-\nticular, almost all of the fully Transformer-based trackers\nsuccessfully handled the fast motion (FM) and out-of-view\n(OV) scenarios and outperformed the CNN-based trackers\nby a large margin, due to their long-range feature captur-\ning capability. On the other side, fully-Transformer trackers\nshowed poor performances in background clutters (BC), and\ndeformation (DEF) frames because of their poor discrimi-\nnative capabilities in short-range videos. In overall, CNN-\nTransformer based tracker TrTr [48] successfully handles\nall tracking challenges in OTB100 by combining the CNN\nfeatures with a Transformer architecture.\n2) ANALYSIS ON UAV123 DATASET\nTracking a target in the UA V123 dataset is more difficult\nthan in other benchmark datasets, since the target objects are\nrelatively small in aerial tracking sequences. Therefore, the\ntrackers can capture limited visual cues and are unable to rely\non powerful appearance models. In addition, tracking a target\nin UA V123 is more challenging since the target object and\ncamera frequently change position and orientation. We have\nused the official toolkit of UA V123 to measure the precision\nand success scores of the trackers and to conduct the tracking\nattribute-wise evaluation.\nThe One-stream One-stage trackers showed outstanding\nperformances in UA V123 based on the overall precision and\nsuccess rate scores due to their combined feature learning\nand future fusion process. Especially, the DropTrack [65]\ntracker showed excellent performance in UA V123 with a\nsuccess rate score of 70.9% and a precision score of 92.4%\ndue to its exceptional ability to capture correlated spatial\ncues. The OSTrack [62] and MixFormer [60] approaches are\nVOLUME 11, 2023 80313\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nTABLE 4. Tracking robustness comparison of the recently proposed state-of-the-art approaches on OTB100, UAV123, TrackingNet, LaSOT, and GOT-10k\nbenchmarks. The abbreviations are denoted as SR: Success Rate, P: Precision, Pn: Normalized Precision, AO: Average Overlap, SR0.5: Success Rate at\n0.5 threshold, and SR0.75: Success Rate at 0.75 threshold. All the results are reported in percentage. The evaluation considered the top-performing\nmodels of each tracker. The red, blue, and green colors are used to mark the top three results, respectively. The symbols†and ‡are denoting One-stream\nOne-stage trackers, and Two-stream Two-stage trackers, respectively. The∗ symbol is used to indicate tracking models that are only trained on the\ntraining set of GOT-10k.\nalso showed good results in UA V123 dataset. Among the\nCNN-Transformer based trackers, CSWinTT [55] and AiA-\nTrack [56] approaches showed better success and precision\nscores, respectively. CNN-based tracker: KeepTrack [101]\nobtained third highest precision score because of its distrac-\ntor object handling capability without relying heavily on an\nappearance model.\nFully-Transformer based and CNN-Transformer based\ntrackers showed excellent performance compared to the\nCNN-based trackers in every attribute on the UA V123 bench-\nmark, as shown in Fig. 21 and Table 5. The OSTrack [62] and\nDropTrack [65] trackers successfully handle the background\nclutter (BC), partial occlusion (POC), out-of-view (OV),\nand scale variation (SV) scenarios because of their strong\ndiscriminative capability and the background feature elimi-\nnating technique. On the other hand, CNN-Transformer based\ntrackers: CSWinTT [55] and AiATrack [56] significantly\noutperforms fully-Transformer based trackers in fast motion\n(FM), illumination variation (IV), and aspect ratio change\n(ARC) scenarios.\nBased on the experimental analysis, full occlusion (FOC),\nbackground clutter (BC), and low resolution (LR) are the\nmost challenging attributes in UA V123 dataset since all the\ntrackers are struggling to capture the strong appearance cues\nin these scenarios in aerial tracking videos. To summarize,\nthe evaluation of state-of-the-art trackers on the UA V123\ndataset showed that their performance was only average. This\nfinding highlights the need for further research and innova-\ntion to improve the accuracy and reliability of aerial tracking\nsystems.\n3) ANALYSIS ON LaSOT DATASET\nExperimental analysis on LaSOT dataset is important\nto identify the future direction of VOT since it has\n80314 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nTABLE 5. Tracking attribute-based comparison on OTB100, UAV123, and LaSOT datasets. In each tracking attribute, percentage of success rate (SR) scores\nof best three trackers and their types are listed. The symbols†, ‡, and♢ are denoting One-stream One-stage trackers, Two-stream Two-stage trackers, and\nCNN-Transformer based trackers, respectively.\nlong-term tracking sequences with several challenging\nscenarios. We have utilized the official toolkit of the\nLaSOT dataset to measure the tracking performance of the\napproaches.\nVOLUME 11, 2023 80315\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 20. The success plots of trackers for 11 challenging attributes on OTB100.\nAs summarized in the overall performance results in\nTable 4, One-stream One-stage tracker SeqTrack [69] showed\noutstanding performance in LaSOT in terms of success rate,\nnormalized precision, and precision scores since it trained\nthe Transformer by treating tracking as a sequence learning\nproblem rather than template matching. The DropTrack [65]\napproach also demonstrated excellent performance in LaSOT\ndue to the correlated spatial feature learning capability of\nits backbone network. In addition to these trackers, the Pro-\nContEXT [63] and OSTrack [62] approaches showed good\nperformances since they efficiently used the information flow\nbetween the target template and search region and hence elim-\ninated the background features in feature matching. Overall,\nall One-stream One-stage fully-Transformer based trackers\nsignificantly outperformed other tracker types with a large\nmargin in the LaSOT dataset, as the target-aware Transformer\nfeatures are better suited for long-term tracking than CNN-\nbased local feature extraction.\nBased on the attribute-wise success plots in Fig. 22, and\nTable 5, SeqTrack [69] and DropTrack [65] trackers showed\nbest performance in most of the tracking scenarios in the\nLaSOT benchmark. In particular, the SeqTrack tracker out-\nperforms the competitors with a large scale in background\nclutter (BC), deformation (DFE), scale variation (SV), and\nview point change (VC) scenes because of its sequence\nlearning mechanism. In addition to these trackers, the Pro-\nContEXT [63] tracker showed better performance in camera\nmotion (CM), illumination variation (IV), and motion blur\n(MB) scenarios because of its strong discriminative capability\nand background feature removal mechanism.\nOverall, One-stream One-stage fully-Transformer track-\ners showed excellent performance, while CNN-Transformer\nbased trackers showed considerable performance in the\nLaSOT benchmark. On the other hand, the overall tracking\nand attribute-wise performances of CNN-based approaches\nare very limited in the LaSOT dataset, as they fail to include\ntemporal cues and extract target-specific features in search\nregions. Based on the attribute-wise success rates, fast motion\nand full occlusion are the most challenging attributes for\nstate-of-the-art trackers.\n80316 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 21. The success plots of the trackers for 12 challenging attributes on UAV123 dataset.\n4) ANALYSIS ON TRACKINGNET DATASET\nTrackingNet dataset has more than 30k training and 511 test-\ning video sequences with 14 million and 225k annotations,\nrespectively. Since the TrackingNet dataset has the videos\nwith a large diversity of resolutions, target object classes,\nand frame rates, evaluating the tracking performance on this\ndataset is important for many real-world applications. Similar\nto LaSOT dataset, we ranked the trackers in TrackingNet\ndataset using success rate, precision score, and normalized\nprecision. Since the test set annotations are not publicly avail-\nable, we are unable to conduct the attribute-wise comparison\nin this dataset.\nSimilar to the LaSOT benchmark, the SeqTrack [69]\ntracker showed superior performance in TrackingNet dataset\nwith 85.5% of success rate, 89.8% of normalized precision,\nand 85.8% of precision scores. Also, other One-stream One-\nstage based fully-Transformer trackers: ProContEXT [63]\nand DropTrack [65] approaches also showed competitive\ntracking performances. Moreover, the GRM [67] and Mix-\nFormer [60] achieved the third rank in terms of precision and\nnormalized precision, respectively.\nExperimental analysis on TrackingNet dataset showed that\nfully-Transformer based trackers significantly outperformed\nother two categories by a large margin. Although CNN-\nTransformer based trackers: AiATrack [56] and ToMP [51]\nobtained considerable competitive performances, tracking\naccuracy of other CNN-Transformer based trackers are lower.\nOn the other hand, similar to the LaSOT dataset, all CNN-\nbased trackers are struggling to show good performance in\nTrackingNet dataset. The experimental results clearly show\nthat fully-Transformer based feature extraction and fusion\nsuccessfully handle real-world tracking scenarios, such as\nvarious resolutions and frame rates, better than CNN-based\nfeature extraction and target localization.\n5) ANALYSIS ON GOT-10k DATASET\nEvaluating performance on the GOT-10k dataset is essential\nfor VOT approaches, as it has a large diversity of target object\nclasses. This allows for measuring the one-shot tracking\nperformance of the trackers, i.e., the performance when the\ntarget object class is not included in the training. Furthermore,\nthe evaluation results of this dataset are not biased towards\nVOLUME 11, 2023 80317\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nFIGURE 22. The success plots comparison of trackers on LaSOT for 14 challenging attributes.\nfamiliar objects, as the tracking models are only trained on the\ntraining set of GOT-10k. We strictly adhere to the evaluation\nprotocols to measure the generalization capability of trackers.\nAverage Overlap and success rate at the threshold of 0.5\n(SR0.5) and 0.75 (SR 0.75) are used to rank the trackers in\nGOT-10k dataset.\nBased on the obtained results in Table 4, the Drop-\nTrack [65] tracker achieved the highest average overlap\nscore of 75.9% and SR 0.5 (success rate at 0.5 threshold) of\n86.6%. The ProContEXT [63] and SeqTrack [69] trackers\nalso demonstrated competitive performances. Overall, the\nexperimental results showed that all One-stream One-stage\nfully-Transformer based trackers possess a good generaliza-\ntion capability, as their attention mechanism perform effec-\ntively across a diverse range of target object classes. On the\nother hand, the tracking performances of CNN-based trackers\nare lower in the GOT-10k dataset, as CNNs are not as effective\nin handling unfamiliar objects.\n80318 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nC. TRACKING EFFICIENCY ANALYSIS\nAnalyzing tracking efficiency is as important as analyzing\ntracking performance, as it is crucial for many practical situ-\nations and real-world applications. In this efficiency analysis\ncomparison, we have included all the recent trackers except a\nfew approaches since their source codes and tracking models\nare not publicly available. To conduct the unbiased compari-\nson, efficiency results of each tracker is obtained by executing\ntheir source codes on a computer with a NVIDIA Quadro\nP4000 GPU and a 64GB of RAM. We did not change the\nparameters of the tracking models of these approaches and\nobtained the efficiency results by evaluating the trackers on\nthe LaSOT benchmark dataset.\nWe have evaluated the efficiency of the trackers in terms of\ntheir tracking speed, number of parameters, and the number\nof floating point operations (FLOPs) in their tracking model.\nTracking speed is important for many real-world applications\nand hence we have considered it as an important metric in\nefficiency comparison. It is computed by calculating the aver-\nage number of frames processed by an approach in a second.\nSince the reported tracking speeds of the approaches depend\non the hardware and implementation platforms, we calculated\nthe tracking speed of the trackers on the same hardware\nplatform using the PyTorch deep learning framework.\nThe number of parameters is another efficiency metric\nin deep learning-based tracking approaches, as models with\nfewer parameters are more hardware-efficient and can work\non small devices such as mobiles and tablets. Number of\nparameters of a tracking approach are the total number of\narguments passed to the optimizer and in most situations\nit does not rely on the input size of the tracking approach.\nWe have used the PyTorch default function to measure the\ntotal number of parameters of a tracking model. We have\nconsidered the number of floating point operations (FLOPs)\nas the third metric to measure the efficiency of a tracking\nmodel. Except for fully-Transformer based trackers, the num-\nber of FLOPs of an approach depends on the tracking model\nand corresponding search image size since the target template\nfeatures are computed only in the first frame of a tracking\nsequence. In fully-Transformer based trackers, features of\nthe target template are computed in each and every frame,\nand hence the size of the template influences the FLOPs.\nAlthough the number of FLOPs of some trackers is very\nhigh, they are still able to track a target at high speed since\ntheir models are highly parallel, and nowadays GPUs can\nhandle them successfully. However, these approaches are not\nsuitable for some applications that only run on CPUs and\nmobile devices. The overall efficiency results of the trackers,\nalong with their corresponding search image size and success\nscores on the LaSOT benchmark, are reported in Table 6.\nBased on the obtained efficiency results in Table 6, CNN-\nbased trackers showed better performances than the other\ntwo types of trackers. In particular, SiamDW [95] tracker\nachieved top efficiency results with 52.58 FPS of track-\ning speed and 2.46 million of parameters by using the\nlight-weighted cropping-inside residual units based CNN\nbackbone. The SiamGAT [33] tracker achieved a tracking\nspeed of 41.99 FPS. Its tracking model has 14.23 million\nparameters and 14.23 gigaFLOPs for a search image size of\n287 × 287. Although most of the recent CNN-based track-\ners are computationally efficient, SiamRPN++ [29] tracker\nachieved poor efficiency results with 5.17 FPS of tracking\nspeed since it used a deeper CNN backbone architecture for\nfeature extraction. The SiamRN [34] approach also showed\nthe second-lowest efficiency results among the CNN-based\ntrackers, with an average tracking speed of 6.51 FPS and\n116.87 gigaFLOPs, due to its computationally expensive rela-\ntion detector module.\nOverall, most of the CNN-Transformer based trackers\nsuccessfully balance tracking robustness and computational\nefficiency. In particular, the number of FLOPs of the CNN-\nTransformer based trackers is considerably lower than the\nother two categories because they successfully capture robust\ncues from CNN-based features, even with a lightweight back-\nbone network. Particularly, AiATrack [56] tracker achieved\n31.22 FPS of the average tracking speed and 17.95 million\nparameters while maintaining 69% of success score on the\nLaSOT dataset. Although the AiATrack approach searches\nthe target in a large search region, it has only 9.45 gigaFLOPs\nbecause its model update mechanism uses a feature-reusing\ntechnique to avoid additional computational costs. Among\nthe CNN-Transformer based trackers, HiFT [50] approach\nachieved top efficiency results with 37.06 FPS of the track-\ning speed and 11.07 million of parameters while showing\na considerable tracking accuracy. Since the HiFT tracker\nutilized the lightweight AlexNet [20] as the backbone fea-\nture extraction network, it achieved top efficiency results\nwith fewer FLOPs. Based on its average tracking speed, the\nCSWinTT [55] tracker showed poor results with 8.76 FPS\nbecause the cyclic shifting attention mechanism of this\napproach is computationally expensive.\nAlthough most of the fully Transformer-based trackers\nare computationally inefficient, GRM-B [67], SimTrack-\nB/16 [61], and SparseTT [59] trackers considerably balance\nthe tracking robustness and efficiency. Since the GRM tracker\nsearches for the target in a narrower region compared to other\nfully-Transformer trackers and avoids unnecessary attention\ncomputation by eliminating background patches, it success-\nfully balances accuracy and efficiency. The reason behind the\nbetter efficiency of the SimTrack approach is its simple and\ncomputationally efficient ViT-B/16 backbone Transformer\narchitecture. Similarly, the SparseTT [59] tracking model\nhas a computational load of 9.21 gigaFLOPs since it uti-\nlizes the smallest version of the Swin Transformer [90] as\nthe backbone network. Also, the SparseTT tracker removed\nthe non-similarity image pairs between target template and\nsearch image in the searching process and hence improved\nthe tracking efficiency.\nAmong the fully-Transformer based approaches, Seq-\nTrack [69] achieved lower efficiency results with 5.81 FPS\nof tracking speed, 308.98 million parameters, and 535.85\ngigaFLOPs. Although SeqTrack has shown superior accuracy\nVOLUME 11, 2023 80319\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\nTABLE 6. Tracking efficiency comparison of the recently proposed state-of-the-art trackers on LaSOT dataset. The search region size, number of\nparameters (in million), tracking speed (in frame per second), number of floating point operations (FLOPs) of each tracker are listed. The efficiency\nexperiments are conducted on a NVIDIA Quadro P4000 GPU with a 64GB of RAM. The symbols†and ‡are denoting One-stream One-stage trackers, and\nTwo-stream Two-stage trackers, respectively.\nin several benchmarks, it is computationally expensive since\nit relies on the large-scale ViT-L model. Additionally, unlike\nother trackers, SeqTrack uses a large size (384 × 384) of the\ntarget template and search region, leading to an increase in\nthe number of floating-point operations. The MixFormer-L\n[60] tracker also achieved low efficiency results with a\ntracking speed of 8.02 FPS, 195.40 million parameters, and\n113.02 gigaFLOPs. The tracking model of the MixFormer-L\napproach was created from the wider model of CVT Trans-\nformer [91] (CVT-W24), and hence it achieved poor results\nin terms of efficiency. Although the ProContEXT [63] tracker\nis a follow-up work of OSTrack [62] tracking approach, it can\nable to track a target at the half speed of the OSTrack since\nits context-aware self-attention module is computationally\nexpensive and hence the number of FLOPs are doubled.\nIn summary, CNN-based trackers achieve better results in\nterms of tracking speed, number of parameters, and number\nof floating-point operations. However, their tracking robust-\nness is poorer than the other two categories. On the other\nhand, although fully-Transformer based trackers show excel-\nlent tracking robustness, their efficiency is lower than the\nother two types. Number of floating point operations of\nthe fully-Transformer based approaches are generally higher\nthan the CNN-Transformer based and CNN-based trackers\nbecause of their attention mechanism. CNN-Transformer\nbased trackers successfully balance tracking robustness and\nefficiency by combining CNN-based feature extraction and\nTransformer-based feature fusion.\nVI. DISCUSSIONS\nIn this section, we discuss the summary of the findings based\non the literature review and experimental survey, as well\nas the future directions of Transformer-based single object\ntracking.\nA. SUMMARY OF FINDINGS\nThis survey study focused on analyzing the literature and\nperformances of a subset of visual object trackers that use\nTransformers in their tracking pipeline. We covered differ-\nent types of Transformer-based trackers and analyzed their\nindividual performances based on how they addressed the\ntracking challenges. In addition, we compared the perfor-\nmances of Transformer trackers with state-of-the-art CNN-\nbased trackers to demonstrate how they have surpassed them\nby a significant margin within a short period of time.\nBefore the introduction of the Transformer in object\ntracking, CNN-based trackers dominated the tracking world.\nSpecifically, Siamese-based approaches achieved a consid-\nerable balance between tracking robustness and efficiency\nin benchmark datasets by treating object tracking as a\n80320 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\ntemplate matching problem. However, since CNN-based\nSiamese tracking approaches primarily rely on the corre-\nlation operation, which is a local linear matching process,\ntheir performance is limited in challenging tracking scenar-\nios. Additionally, based on the results of our experimental\nanalysis on the large-scale LaSOT benchmark, the long-term\ntracking capability of CNN-based trackers is very limited.\nThis limitation arises from their disregard for temporal cues\nin the template matching process. Furthermore, attribute-wise\nexperimental results showed that CNN-based trackers still\nstruggle to track a target in fully occluded, rotated, viewpoint-\nchanged, and scale-varied scenarios due to their poor target\ndiscriminative and feature matching capabilities. While all\nCNN-based trackers have demonstrated poor tracking robust-\nness in recently developed challenging datasets, they still\nexhibit excellent performance in the OTB100 dataset. This\nnotable distinction arises from the fact that the targets in\nthe OTB100 dataset do not undergo substantial appearance\nalterations, unlike those in the other challenging datasets.\nThe Transformer was initially introduced to the single-\nobject tracking community as a module with a CNN\nbackbone, and these approaches are referred to as CNN-\nTransformer based trackers in this study. Researchers\nreplaced the correlation operation of Siamese tracking\napproaches with a Transformer architecture in CNN-\nTransformer based trackers, utilizing attention mechanism.\nBased on the results of this study, CNN-Transformer based\ntrackers successfully balanced tracking robustness and effi-\nciency in benchmark datasets. Although their tracking robust-\nness is good in short-term and aerial tracking sequences, their\nlong-term tracking capabilities are considerably lower than\nfully-Transformer based approaches. Compared to CNN-\nbased trackers, CNN-Transformer based approaches suc-\ncessfully utilize the backbone CNN architecture for feature\nextraction, and they show excellent performances even with\nsmaller pre-trained CNN models. However, our attribute-wise\nexperimental analysis reveals that CNN-Transformer based\ntrackers exhibit limited performance in tracking scenarios\ninvolving full occlusion, low resolution, out-of-view targets,\nand illumination variation since their inability to capture the\ntemporal cues and target-specific cues using CNN features.\nRecent VOT approaches fully rely on Transformer archi-\ntectures, leveraging their global feature learning capabili-\nties in object tracking. Based on their model architecture,\nwe classify the literature of fully-Transformer approaches\ninto two categories: Two-stream Two-stage trackers and One-\nstream One-stage trackers. Two-stream Two-stage trackers\nperform feature extraction and fusion in two distinguishable\nstages, utilizing two identical Transformer network branches\nand another Transformer network, respectively. On the other\nhand, One-stream One-stage trackers use a single pipeline\nof a Transformer network. Based on our experiential evalu-\nation on challenging benchmark datasets, fully-Transformer\nbased trackers significantly outperform other approaches\nby a wide margin while maintaining moderate efficiency\nscores.\nOur experimental comparison study clearly shows that\nOne-stream One-stage fully-Transformer trackers signifi-\ncantly outperform other types of trackers and are expected\nto dominate the single object tracking community for the\nnext couple of years. Since the feature extraction and fusion\nare conducted together by a single Transformer network\narchitecture in these trackers, the information flow between\nthe features of target template and search region patches is\nefficiently utilized. As a result, target-specific discriminative\nfeatures are enhanced, while unnecessary background and\ndistractor features are eliminated. Therefore, the One-stream\nOne-stage trackers successfully handle long-term tracking\nscenarios, even when the target undergoes severe appearance\nchanges and full occlusion.\nBased on our experimental study, we have found that most\nOne-stream One-stage fully-Transformer trackers exhibit\npoor efficiency compared to CNN-based trackers in terms\nof FLOPs, tracking speed, and the number of parame-\nters. The primary reason for this is that ViT based Trans-\nformers employ a self-attention mechanism that calculates\nattention between all patches, resulting in a quadratic com-\nputational complexity in the input sequence. Additionally,\ncertain trackers such as SeqTrack [69], which utilize encoders\nand decoders for self and cross-attention, exhibit very poor\ncomputational efficiency. In addition to that reason, most\nTransformer trackers search for the target in a larger search\nregion compared to CNN-based trackers, which results in\nslower tracking speeds.\nThe experimental results of this study clearly indicate that\nthe performance of recent trackers on the OTB100 dataset\nhas reached saturation, as evident from the overall perfor-\nmances and attribute-wise results. As a result, some recent\ntrackers have already observed this fact and started avoiding\nthe evaluation of their performances on OTB100. We have\nalso observed that while the tracking accuracy of some track-\ners is excellent on certain challenging benchmark datasets,\ntheir performances are only average on the GOT-10k dataset\nsince these trackers tend to be biased towards familiar target\nobjects. These findings justify the need for a massive track-\ning dataset comprising long tracking sequences and a large\nnumber of non-overlapping training and testing target object\nclasses, which will truly evaluate the performance of future\ntrackers.\nThe increasing number of Transformer-based approaches\nproposed this year, along with their exceptional perfor-\nmances, clearly indicates that Transformers have replaced\nCNNs in single object tracking. These trackers are expected\nto continue impacting the tracking community in the com-\ning years, as they have introduced novel possibilities and\nperspectives, such as treating object tracking as a sequence\nlearning problem rather than the template matching. The\ninfluence of Transformer-based language models on the field\nhas further inspired researchers to propose similar architec-\ntures for object tracking. Moreover, recent advancements in\nself-supervised learning-based mask autoencoders have moti-\nvated the development of more robust trackers that effectively\nVOLUME 11, 2023 80321\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\naddress data redundancy issues and capture more generalized\nrepresentations. Consequently, it is evident that Transformer-\nbased trackers are positioned to dominate the field of single\nobject tracking in the future.\nB. FUTURE DIRECTIONS OF TRANSFORMER TRACKING\nOur experimental survey clearly shows that One-stream\nOne-stage fully-Transformer based trackers significantly out-\nperform other Transformer-based trackers by a large margin.\nDespite their impressive performance on challenging bench-\nmark datasets, there are still several issues that need to be\naddressed and require further attention in future work. Taking\nthis into account, we provide some recommendations for\nfuture directions by exclusively considering the One-stream\nOne-stage fully-Transformer based trackers.\n1) DESIGNING A NEW SPATIO-TEMPORAL TRANSFORMER\nARCHITECTURE FOR OBJECT TRACKING TO CAPTURE THE\nSPATIAL AND TEMPORAL CUES TOGETHER\nAll of the current Transformer-based trackers utilize the\nVision Transformer (ViT) [37] or a variant of ViT as\ntheir backbone and fine-tune it for tracking. These Trans-\nformer backbone networks were originally designed for\nimage recognition or object classification tasks, primarily\nfocusing on capturing spatial relationships within individ-\nual images. However, they lack the inherent capability to\ncapture temporal cues between a sequence of continuous\nreference frames, which is crucial for object tracking. This\nlimitation stems from the fundamental difference between\nobject classification and tracking tasks. To overcome this\nlimitation, there is a need to design a novel spatio-temporal\nTransformer architecture specifically tailored for tracking\ntasks, which effectively captures spatial and temporal cues\ntogether. These Transformers can be capture spatial cues\nby computing attention between image patches within a\nframe and temporal cues by computing attention between\npatches in adjacent frames. Similar to the recently proposed\nspatio-temporal Transformers in other tasks [110], [111],\na novel spatio-temporal Transformer could be employed to\ncapture spatial-temporal cues from the continuous reference\nframe sequence, leading to the attainment of more robust\nresults.\n2) IMPROVING COMPUTATIONAL EFFICIENCY THROUGH\nUTILIZATION OF LIGHTWEIGHT TRANSFORMER\nARCHITECTURES, QUANTIZATION TECHNIQUES, AND\nFEATURE REUSING TECHNIQUES\nAlthough fully-Transformer based trackers have shown\noutstanding tracking robustness, their computational effi-\nciencies are considerably poor and hence not suitable for\nmany real-world applications. Recently, several lightweight\nTransformers have been proposed [112], [113], [114],\ndemonstrating excellent efficiency scores while maintain-\ning accuracy in various computer vision tasks. By utilizing\nthese lightweight Transformers in VOT, computational costs\ncould be improved. In addition, similar to the work of [115],\nutilizing proper quantization techniques could reduce the\ncomputational complexity of Transformers without com-\npromising performance. Moreover, the current Transformer\ntrackers process the template frame patches in each and\nevery frame using an encoder, resulting in increased com-\nputational costs. To address this issue, an efficient feature\nreusing mechanism could be employed to reduce the model’s\ncomputational complexity.\n3) UTILIZING SELF-SUPERVISED LEARNING BASED MASKED\nAUTOENCODER PRE-TRAINED MODELS TO ENHANCE\nTRACKING PERFORMANCE\nRecent Transformer trackers [62], [63], [67], [69] have\nenhanced their tracking accuracy by utilizing self-supervised\nlearning based masked autoencoder pre-trained models [108]\nto initialize the tracker encoder. These pre-trained models\nhelped the tracker extract more discriminative features by\nfocusing on relevant features and discarding irrelevant or\nnoisy information. In addition, the trackers that utilized these\npre-trained models showed better performance for unseen tar-\nget objects due to the generalization capability of the masked\nautoencoders. Due to these facts, self-supervised learning-\nbased masked autoencoder pre-trained models could be fur-\nther investigated as a potential future direction for object\ntracking. Additionally, since current masked autoencoders are\nprimarily designed to capture spatial cues within an image\nfor image recognition tasks, there is a need to develop a\nself-supervised pre-trained model for object tracking that\ncan effectively capture both spatial and temporal cues. How-\never, it is important to note that developing these powerful\npre-trained mask autoencoder models requires significant\ncomputational resources and can be expensive for real-time\ntracking tasks. Therefore, it is crucial to adopt an efficient\nfine-tuning mechanism, similar to LoRA [116], to reduce the\ncomputational complexity without compromising tracking\nperformance.\n4) ENRICHING THE ACCURACY OF FULLY-TRANSFORMER\nTRACKERS TO TRACK A SMALL TARGET OBJECT WITH LESS\nAPPEARANCE CUES\nThe experimental results on the UA V123 dataset show that\nfully-Transformer based approaches struggle to track small\ntarget objects with limited appearance cues, as other types\nof trackers outperform them in attribute-wise comparisons.\nIn particular, their success scores are poor when tracking\nsmall target objects in full occlusion, background clutter,\nand low resolution scenarios. Since the patch-level atten-\ntion mechanism of the Transformer fails to capture the\ncorrect appearance cues of a small target object, fully-\nTransformer based trackers exhibit limited performance.\nSimilar to the CSWinTT tracker [55], incorporating hierar-\nchical architecture with window-level and target size-aware\nattention mechanism could improve the accuracy of fully-\nTransformer trackers when tracking small target objects.\n80322 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\n5) IMPROVING THE LOCAL FEATURE LEARNING CAPABILITY\nOF THE TRACKERS THROUGH A TARGET-SPECIFIC PATCH\nMERGING TECHNIQUE\nTransformers, while effective at capturing long-range global\nstructures in an image, have considerably poor local fea-\nture learning capabilities due to their division of the input\nimage into equally sized patches, which results in the loss\nof information about local features such as edges and lines.\nTo enhance the local feature learning capability of Transform-\ners, a recent approach called Tokens-To-Token Vision Trans-\nformer (T2T-ViT) [117] aggregates neighboring patches into\na single patch. Since many Transformer trackers [61], [62],\n[63] employ the ViT [37] backbone with hard divided\ninput patches, their target localization capabilities could be\nenhanced by incorporating a patch merging technique similar\nto that employed in the T2T-ViT model. By utilizing a target-\nspecific patch merging technique, it is possible to preserve\nthe local features of the target and improve overall tracking\naccuracy.\n6) ENHANCING THE TOKEN SELECTION MECHANISM TO\nOVERCOME BACKGROUND INTERFERENCE AND\nDISTRACTORS\nIn Transformer tracking approaches, the target template and\nsearch region patches are converted into tokens, and the atten-\ntions between these tokens are computed using a Transformer\narchitecture. Since most trackers fail to remove background\nand distractor tokens and compute attention between all\ntokens, their performance is reduced. Additionally, due to\nunnecessary attention computation, the efficiency of these\ntrackers is also reduced. Although some trackers [62], [63],\n[67] incorporate a mechanism to remove background tokens,\nfurther investigation and enhancement of the token selection\nmechanism is warranted. A more advanced token selection\ntechnique, which incorporates information from previous\nframes, has the potential to significantly enhance tracking\naccuracy and efficiency.\n7) FAST MOTION, FULLY OCCLUSION, AND LOW\nRESOLUTION ARE THE MAJOR CHALLENGES IN\nTRANSFORMER TRACKING\nBased on our evaluation results on various benchmark\ndatasets, we have identified that Transformer trackers strug-\ngle to demonstrate satisfactory performance in fast motion,\nsevere occlusion, and low-resolution scenes. Although fully-\nTransformer based approaches have shown considerable\nimprovement in these scenarios, our evaluation on the chal-\nlenging LaSOT dataset reveals that their success scores\nare lower in frames with fast motion, full occlusion, and\nlow resolution, with the top-performing trackers achiev-\ning only 60.1%, 64.5%, and 65.6%, respectively. To han-\ndle these challenges, several approaches could be taken\nsuch as enlarging the search region with a distractor-\naware mechanism could handle fast-moving targets while\nreducing the impact of distractor objects. Additionally,\nincluding a target re-detection scheme in occlusion scenar-\nios could improve the tracking robustness, and including\nthe temporal cues in tracking could handle low resolution\nsituations.\nVII. CONCLUSION\nIn this study, we conducted a survey on Transformer\ntracking approaches. We analyzed the literature on Trans-\nformer trackers and classified them into three types:\nCNN-Transformer trackers, One-stream One-stage fully-\nTransformer based trackers, and Two-stream Two-stage fully-\nTransformer based trackers. We present the literature of\nthe 26 Transformer trackers in this paper based on how they\naddressed different tracking challenges.\nIn the second phase of this study, we experimen-\ntally evaluated the tracking robustness and computational\nefficiency of the Transformer tracking approaches and\ncompared their performances with CNN-based trackers.\nIn total, we evaluated 37 trackers in our experiments. The\nexperimental results on challenging benchmark datasets\ndemonstrated that One-stream One-stage fully-Transformer\nbased trackers are the state-of-the-art approaches. Also,\nwe found that CNN-Transformer based trackers successfully\nmaintained a balance between robustness and efficiency.\nIn the end, we provide future directions for Transformer\ntracking.\nACKNOWLEDGMENT\nThe author acknowledges the support received from the LK\nDomain Registry, Sri Lanka, in publishing this paper. (Janani\nKugarajeevan and Thanikasalam Kokul contributed equally\nto this work.)\nREFERENCES\n[1] G. Ciaparrone, F. Luque Sánchez, S. Tabik, L. Troiano, R. Tagliaferri,\nand F. Herrera, ‘‘Deep learning in video multi-object tracking: A survey,’’\nNeurocomputing, vol. 381, pp. 61–88, Mar. 2020.\n[2] L. Kalake, W. Wan, and L. Hou, ‘‘Analysis based on recent deep learning\napproaches applied in real-time multi-object tracking: A review,’’ IEEE\nAccess, vol. 9, pp. 32650–32671, 2021.\n[3] S. Jha, C. Seo, E. Yang, and G. P. Joshi, ‘‘Real time object detection and\ntracking system for video surveillance system,’’ Multimedia Tools Appl.,\nvol. 80, pp. 3981–3996, Jan. 2021.\n[4] S. Bhakar, D. P. Bhatt, V . S. Dhaka, and Y . K. Sarma, ‘‘A review on\nclassifications of tracking systems in augmented reality,’’ J. Phys., Conf.\nSer., vol. 2161, no. 1, Jan. 2022, Art. no. 012077.\n[5] C. Premachandra, S. Ueda, and Y . Suzuki, ‘‘Detection and tracking\nof moving objects at road intersections using a 360-degree camera\nfor driver assistance and automated driving,’’ IEEE Access, vol. 8,\npp. 135652–135660, 2020.\n[6] R. Pereira, G. Carvalho, L. Garrote, and U. J. Nunes, ‘‘Sort and deep-\nSORT based multi-object tracking for mobile robotics: Evaluation with\nnew data association metrics,’’ Appl. Sci., vol. 12, no. 3, p. 1319,\nJan. 2022.\n[7] R. Chandrakar, R. Raja, R. Miri, U. Sinha, A. K. S. Kushwaha, and\nH. Raja, ‘‘Enhanced the moving object detection and object tracking for\ntraffic surveillance using RBF-FDLNN and CBF algorithm,’’ Expert Syst.\nAppl., vol. 191, Apr. 2022, Art. no. 116306.\n[8] B. T. Naik, M. F. Hashmi, Z. W. Geem, and N. D. Bokde, ‘‘DeepPlayer-\ntrack: Player and referee tracking with Jersey color recognition in soccer,’’\nIEEE Access, vol. 10, pp. 32494–32509, 2022.\n[9] M. Wallner, D. Steininger, V . Widhalm, M. Schörghuber, and C. Beleznai,\n‘‘RGB-D railway platform monitoring and scene understanding for\nenhanced passenger safety,’’ in Proc. Int. Conf. Pattern Recognit.Cham,\nSwitzerland: Springer, 2021, pp. 656–671.\nVOLUME 11, 2023 80323\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\n[10] Y . Liu, C. B. Sivaparthipan, and A. Shankar, ‘‘Human–computer inter-\naction based visual feedback system for augmentative and alternative\ncommunication,’’ Int. J. Speech Technol., vol. 25, no. 2, pp. 305–314,\nJun. 2022.\n[11] B. Liu, J. Huang, L. Yang, and C. Kulikowsk, ‘‘Robust tracking\nusing local sparse appearance model and K-selection,’’ in Proc. CVPR,\nJun. 2011, pp. 1313–1320.\n[12] S. Zhang and X. Wang, ‘‘Human detection and object tracking based on\nhistograms of oriented gradients,’’ in Proc. 9th Int. Conf. Natural Comput.\n(ICNC), Jul. 2013, pp. 1349–1353.\n[13] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. S. Torr,\n‘‘Staple: Complementary learners for real-time tracking,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 1401–1409.\n[14] X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, and A. V . D. Hengel, ‘‘A survey\nof appearance models in visual object tracking,’’ ACM Trans. Intell. Syst.\nTechnol., vol. 4, no. 4, pp. 1–48, Sep. 2013.\n[15] T. Kokul, A. Ramanan, and U. A. J. Pinidiyaarachchi, ‘‘Online multi-\nperson tracking-by-detection method using ACF and particle filter,’’ in\nProc. IEEE 7th Int. Conf. Intell. Comput. Inf. Syst. (ICICIS), Dec. 2015,\npp. 529–536.\n[16] K. Zhu, X. Zhang, G. Chen, X. Tan, P. Liao, H. Wu, X. Cui, Y . Zuo, and\nZ. Lv, ‘‘Single object tracking in satellite videos: Deep Siamese network\nincorporating an interframe difference centroid inertia motion model,’’\nRemote Sens., vol. 13, no. 7, p. 1298, Mar. 2021.\n[17] L. Wang, T. Liu, G. Wang, K. L. Chan, and Q. Yang, ‘‘Video tracking\nusing learned hierarchical features,’’ IEEE Trans. Image Process., vol. 24,\nno. 4, pp. 1424–1435, Apr. 2015.\n[18] H. Zhao, G. Yang, D. Wang, and H. Lu, ‘‘Deep mutual learning for visual\nobject tracking,’’ Pattern Recognit., vol. 112, Apr. 2021, Art. no. 107796.\n[19] T. Kokul, C. Fookes, S. Sridharan, A. Ramanan, and\nU. A. J. Pinidiyaarachchi, ‘‘Gate connected convolutional neural\nnetwork for object tracking,’’ in Proc. IEEE Int. Conf. Image Process.\n(ICIP), Sep. 2017, pp. 2602–2606.\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classification\nwith deep convolutional neural networks,’’ Commun. ACM, vol. 60, no. 6,\npp. 84–90, May 2017.\n[21] S. M. Marvasti-Zadeh, L. Cheng, H. Ghanei-Yakhdan, and S. Kasaei,\n‘‘Deep learning for visual tracking: A comprehensive survey,’’ IEEE\nTrans. Intell. Transp. Syst., vol. 23, no. 5, pp. 3943–3968, May 2022.\n[22] W. Walid, M. Awais, A. Ahmed, G. Masera, and M. Martina, ‘‘Real-time\nimplementation of fast discriminative scale space tracking algorithm,’’\nJ. Real-Time Image Process., vol. 18, no. 6, pp. 2347–2360, Dec. 2021.\n[23] T. Xu, Z. Feng, X.-J. Wu, and J. Kittler, ‘‘Adaptive channel selection for\nrobust visual object tracking with discriminative correlation filters,’’ Int.\nJ. Comput. Vis., vol. 129, no. 5, pp. 1359–1375, May 2021.\n[24] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,\n‘‘Fully-convolutional siamese networks for object tracking,’’ in Proc.\nEur.Conf. Comput. Vis. Workshops. Springer, 2016, pp. 850–865.\n[25] K. Chen and W. Tao, ‘‘Once for all: A two-flow convolutional neural\nnetwork for visual tracking,’’ IEEE Trans. Circuits Syst. Video Technol.,\nvol. 28, no. 12, pp. 3377–3386, Dec. 2018.\n[26] J. Bromley, I. Guyon, Y . LeCun, E. Säckinger, and R. Shah, ‘‘Signature\nverification using a ‘Siamese’ time delay neural network,’’ in Proc.\nAdv. Neural Inf. Process. Syst., vol. 6. Burlington, MA, USA: Morgan-\nKaufmann, 1993, pp. 1–8.\n[27] B. Li, W. Wu, Z. Zhu, and J. Yan, ‘‘High performance visual tracking\nwith Siamese region proposal network,’’ in Proc. CVPR, Jun. 2018,\npp. 8971–8980.\n[28] Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu, ‘‘Distractor-aware\nSiamese networks for visual object tracking,’’ in Proc. Eur. Conf. Comput.\nVis., Sep. 2018, pp. 101–117.\n[29] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, ‘‘SiamRPN++:\nEvolution of Siamese visual tracking with very deep networks,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,\npp. 4277–4286.\n[30] D. Guo, J. Wang, Y . Cui, Z. Wang, and S. Chen, ‘‘SiamCAR: Siamese\nfully convolutional classification and regression for visual tracking,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2020, pp. 6268–6276.\n[31] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji, ‘‘Siamese box adaptive\nnetwork for visual tracking,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2020, pp. 6667–6676.\n[32] P. V oigtlaender, J. Luiten, P. H. S. Torr, and B. Leibe, ‘‘Siam R-CNN:\nVisual tracking by re-detection,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2020, pp. 6577–6587.\n[33] D. Guo, Y . Shao, Y . Cui, Z. Wang, L. Zhang, and C. Shen, ‘‘Graph atten-\ntion tracking,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 9538–9547.\n[34] S. Cheng, B. Zhong, G. Li, X. Liu, Z. Tang, X. Li, and J. Wang, ‘‘Learn-\ning to filter: Siamese relation network for robust tracking,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 4419–4429.\n[35] M. Ondrašovic and P. Tarábek, ‘‘Siamese visual object tracking: A sur-\nvey,’’IEEE Access, vol. 9, pp. 110149–110172, 2021.\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 5998–6008.\n[37] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, and S. Gelly,\n‘‘An image is worth 16×16 words: Transformers for image recognition\nat scale,’’ in Proc. Int. Conf. Learn. Represen., 2021, pp. 1–21. [Online].\nAvailable: https://openreview.net/forum?id=YicbFdNTTy\n[38] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Proc.\nEur. Conf. Comput. Vis.Cham, Switzerland: Springer, 2020, pp. 213–229.\n[39] C. R. Chen, Q. Fan, and R. Panda, ‘‘CrossViT: Cross-attention multi-\nscale vision transformer for image classification,’’ in Proc. IEEE/CVF\nInt. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 347–356.\n[40] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, ‘‘Segmenter: Transformer\nfor semantic segmentation,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV), Oct. 2021, pp. 7242–7252.\n[41] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,\n‘‘PCT: Point cloud transformer,’’ Comput. Vis. Media, vol. 7, no. 2,\npp. 187–199, Jun. 2021.\n[42] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu, Z. Yang, Y . Zhang, and D. Tao, ‘‘A survey on vision\ntransformer,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1,\npp. 87–110, Jan. 2023.\n[43] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n‘‘Transformers in vision: A survey,’’ ACM Comput. Surv., vol. 54,\npp. 1–41, Jan. 2022.\n[44] K. Islam, ‘‘Recent advances in vision transformer: A survey and outlook\nof recent work,’’ 2022, arXiv:2203.01536.\n[45] N. Wang, W. Zhou, J. Wang, and H. Li, ‘‘Transformer meets tracker:\nExploiting temporal context for robust visual tracking,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 1571–1580.\n[46] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, ‘‘Transformer\ntracking,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 8122–8131.\n[47] B. Yan, H. Peng, J. Fu, D. Wang, and H. Lu, ‘‘Learning spatio-temporal\ntransformer for visual tracking,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2021, pp. 10428–10437.\n[48] M. Zhao, K. Okada, and M. Inaba, ‘‘TrTr: Visual tracking with trans-\nformer,’’ 2021, arXiv:2105.03817.\n[49] B. Yu, M. Tang, L. Zheng, G. Zhu, J. Wang, H. Feng, X. Feng, and H. Lu,\n‘‘High-performance discriminative tracking with transformers,’’ in Proc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 9836–9845.\n[50] Z. Cao, C. Fu, J. Ye, B. Li, and Y . Li, ‘‘HiFT: Hierarchical feature\ntransformer for aerial tracking,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis., Oct. 2021, pp. 15457–15466.\n[51] C. Mayer, M. Danelljan, G. Bhat, M. Paul, D. P. Paudel, F. Yu, and\nL. Van Gool, ‘‘Transforming model prediction for tracking,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 8721–8730.\n[52] D. Xing, N. Evangeliou, A. Tsoukalas, and A. Tzes, ‘‘Siamese trans-\nformer pyramid networks for real-time UA V tracking,’’ in Proc.\nIEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2022,\npp. 2139–2148.\n[53] F. Ma, M. Z. Shou, L. Zhu, H. Fan, Y . Xu, Y . Yang, and\nZ. Yan, ‘‘Unified transformer tracker for object tracking,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 8771–8780.\n[54] M. Zhong, F. Chen, J. Xu, and G. Lu, ‘‘Correlation-based transformer\ntracking,’’ in Proc. Int. Conf. Artif. Neural Netw.Cham, Switzerland:\nSpringer, 2022, pp. 85–96.\n80324 VOLUME 11, 2023\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\n[55] Z. Song, J. Yu, Y .-P. P. Chen, and W. Yang, ‘‘Transformer tracking with\ncyclic shifting window attention,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., Jun. 2022, pp. 8791–8800.\n[56] S. Gao, C. Zhou, C. Ma, X. Wang, and J. Yuan, ‘‘AiATrack: Attention in\nattention for transformer visual tracking,’’ in Proc. Eur. Conf. Comput.\nVis. Cham, Switzerland: Springer, 2022, pp. 146–164.\n[57] F. Xie, C. Wang, G. Wang, W. Yang, and W. Zeng, ‘‘Learning tracking\nrepresentations via dual-branch fully transformer networks,’’ in Proc.\nIEEE/CVF Int. Conf. Comput. Vis. Workshops (ICCVW), Oct. 2021,\npp. 2688–2697.\n[58] L. Lin, H. Fan, Z. Zhang, Y . Xu, and H. Ling, ‘‘SwinTrack: A simple\nand strong baseline for transformer tracking,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 35. Red Hook, NY , USA: Curran Associates, 2022,\npp. 16743–16754.\n[59] Z. Fu, Z. Fu, Q. Liu, W. Cai, and Y . Wang, ‘‘SparseTT: Visual tracking\nwith sparse transformers,’’ in Proc. 31st Int. Joint Conf. Artif. Intell.,\nJul. 2022, pp. 905–912.\n[60] Y . Cui, C. Jiang, L. Wang, and G. Wu, ‘‘MixFormer: End-to-end tracking\nwith iterative mixed attention,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2022, pp. 13598–13608.\n[61] B. Chen, P. Li, L. Bai, L. Qiao, Q. Shen, B. Li, W. Gan, W. Wu, and\nW. Ouyang, ‘‘Backbone is all your need: A simplified architecture for\nvisual object tracking,’’ in Proc. Eur. Conf. Comput. Vis.Cham, Switzer-\nland: Springer, 2022, pp. 375–392.\n[62] B. Ye, H. Chang, B. Ma, S. Shan, and X. Chen, ‘‘Joint feature learning and\nrelation modeling for tracking: A one-stream framework,’’ in Proc. Eur.\nConf. Comput. Vis.Cham, Switzerland: Springer, 2022, pp. 341–357.\n[63] J.-P. Lan, Z.-Q. Cheng, J.-Y . He, C. Li, B. Luo, X. Bao, W. Xiang, Y . Geng,\nand X. Xie, ‘‘Procontext: Exploring progressive context transformer for\ntracking,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\n(ICASSP), Jun. 2023, pp. 1–5.\n[64] K. Yang, H. Zhang, J. Shi, and J. Ma, ‘‘BANDT: A border-aware network\nwith deformable transformers for visual tracking,’’ IEEE Trans. Consum.\nElectron., early access, Mar. 6, 2023, doi: 10.1109/TCE.2023.3251407.\n[65] Q. Wu, T. Yang, Z. Liu, B. Wu, Y . Shan, and A. B. Chan, ‘‘DropMAE:\nMasked autoencoders with spatial-attention dropout for tracking tasks,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2023,\npp. 14561–14571.\n[66] F. Xie, L. Chu, J. Li, Y . Lu, and C. Ma, ‘‘VideoTrack: Learning to track\nobjects via video transformer,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., Jun. 2023, pp. 22826–22835.\n[67] S. Gao, C. Zhou, and J. Zhang, ‘‘Generalized relation modeling for\ntransformer tracking,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., Jun. 2023, pp. 18686–18695.\n[68] X. Wei, Y . Bai, Y . Zheng, D. Shi, and Y . Gong, ‘‘Autoregressive visual\ntracking,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\nJun. 2023, pp. 9697–9706.\n[69] X. Chen, H. Peng, D. Wang, H. Lu, and H. Hu, ‘‘SeqTrack: Sequence to\nsequence learning for visual object tracking,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., Jun. 2023, pp. 14572–14581.\n[70] P. Li, D. Wang, L. Wang, and H. Lu, ‘‘Deep visual tracking: Review\nand experimental comparison,’’ Pattern Recognit., vol. 76, pp. 323–338,\nApr. 2018.\n[71] A. Yilmaz, O. Javed, and M. Shah, ‘‘Object tracking: A survey,’’ ACM\nComput. Surv., vol. 38, no. 4, p. 13, 2006.\n[72] R. Pflugfelder, ‘‘An in-depth analysis of visual tracking with Siamese\nneural networks,’’ 2017, arXiv:1707.00569.\n[73] B. Deori and D. M. Thounaojam, ‘‘A survey on moving object tracking\nin video,’’ Int. J. Inf. Theory, vol. 3, no. 3, pp. 31–46, Jul. 2014.\n[74] Z. Soleimanitaleb, M. A. Keyvanrad, and A. Jafari, ‘‘Object tracking\nmethods: A review,’’ in Proc. 9th Int. Conf. Comput. Knowl. Eng.\n(ICCKE), Oct. 2019, pp. 282–288.\n[75] Y . Zhang, T. Wang, K. Liu, B. Zhang, and L. Chen, ‘‘Recent advances\nof single-object tracking methods: A brief survey,’’ Neurocomputing,\nvol. 455, pp. 1–11, Sep. 2021.\n[76] Y . Wu, J. Lim, and M.-H. Yang, ‘‘Object tracking benchmark,’’ IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 37, no. 9, pp. 1834–1848,\nSep. 2015.\n[77] A. W. M. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan,\nand M. Shah, ‘‘Visual tracking: An experimental survey,’’ IEEE Trans.\nPattern Anal. Mach. Intell., vol. 36, no. 7, pp. 1442–1468, Jul. 2014.\n[78] K. Cannons, ‘‘A review of visual tracking,’’ Dept. Comput. Sci. Eng. York\nUniv., Toronto, ON, Canada, Tech. Rep. CSE-2008-07, 242, 2008.\n[79] F. Chen, X. Wang, Y . Zhao, S. Lv, and X. Niu, ‘‘Visual object track-\ning: A survey,’’ Comput. Vis. Image Understand., vol. 222, Sep. 2022,\nArt. no. 103508.\n[80] S. Javed, M. Danelljan, F. S. Khan, M. H. Khan, M. Felsberg, and\nJ. Matas, ‘‘Visual object tracking with discriminative filters and Siamese\nnetworks: A survey and outlook,’’ IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 45, no. 5, pp. 6552–6574, May 2023.\n[81] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y . Xu, C. Liao, and\nH. Ling, ‘‘LaSOT: A high-quality benchmark for large-scale single object\ntracking,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2019, pp. 5369–5378.\n[82] L. Huang, X. Zhao, and K. Huang, ‘‘GOT-10k: A large high-diversity\nbenchmark for generic object tracking in the wild,’’ IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 43, no. 5, pp. 1562–1577, May 2021.\n[83] M. Müller, A. Bibi, S. Giancola, S. Alsubaihi, and B. Ghanem, ‘‘Track-\ningNet: A large-scale dataset and benchmark for object tracking in the\nwild,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV), Jun. 2018, pp. 300–317.\n[84] M. Mueller, N. Smith, and B. Ghanem, ‘‘A benchmark and simulator\nfor UA V tracking,’’ inProc. Eur. Conf. Comput. Vis.Cham, Switzerland:\nSpringer, 2016, pp. 445–461.\n[85] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Cehovin,\nG. Fernandez, T. V ojir, G. Hager, G. Nebehay, R. Pflugfelder, A. Gupta,\nA. Bibi, A. Lukezic, A. Garcia-Martin, A. Saffari, A. Petrosino, and\nA. S. Montero, ‘‘The visual object tracking VOT2015 challenge results,’’\nin Proc. IEEE Int. Conf. Comput. Vis. Workshop (ICCVW), Dec. 2015,\npp. 564–586.\n[86] S. J. Hadfield, R. Bowden, and K. Lebeda, ‘‘The visual object tracking\nVOT2016 challenge results,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV),\nvol. 9914, 2016, pp. 777–823.\n[87] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, and R. Pflugfelder,\n‘‘The visual object tracking VOT2017 challenge results,’’ in\nProc. IEEE/CVF Int. Conf. Comput. Vis. Workshops, Oct. 2017,\npp. 1949–1972.\n[88] M. Kristan, ‘‘The sixth visual object tracking VOT2018 challenge\nresults,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 1–52.\n[89] D. Hendrycks and K. Gimpel, ‘‘Gaussian error linear units (GELUs),’’\n2016, arXiv:1606.08415.\n[90] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n‘‘Swin transformer: Hierarchical vision transformer using shifted win-\ndows,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 9992–10002.\n[91] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, ‘‘CvT:\nIntroducing convolutions to vision transformers,’’ in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2021, pp. 22–31.\n[92] X. Chu, Z. Tian, B. Zhang, X. Wang, and C. Shen, ‘‘Conditional posi-\ntional encodings for vision transformers,’’ in Proc. 11th Int. Conf. Learn.\nRepresen., 2023, pp. 1–19. [Online]. Available: https://openreview.\nnet/forum?id=3KWnuT-R1bh\n[93] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, ‘‘Transformer\nin transformer,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021,\npp. 15908–15919.\n[94] L. Yuan, Q. Hou, Z. Jiang, J. Feng, and S. Yan, ‘‘VOLO: Vision outlooker\nfor visual recognition,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 45,\nno. 5, pp. 6575–6586, May 2023.\n[95] Z. Zhang and H. Peng, ‘‘Deeper and wider Siamese networks for real-time\nvisual tracking,’’ inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2019, pp. 4586–4595.\n[96] G. Bhat, M. Danelljan, L. Van Gool, and R. Timofte, ‘‘Learning dis-\ncriminative model prediction for tracking,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis. (ICCV), Oct. 2019, pp. 6181–6190.\n[97] Y . Yu, Y . Xiong, W. Huang, and M. R. Scott, ‘‘Deformable Siamese\nattention networks for visual object tracking,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 6727–6736.\n[98] Y . Xu, Z. Wang, Z. Li, Y . Yuan, and G. Yu, ‘‘SiamFC++: Towards\nrobust and accurate visual tracking with target estimation guide-\nlines,’’ in Proc. AAAI Conf. Artif. Intell., vol. 34, no. 7, Apr. 2020,\npp. 12549–12556.\n[99] Z. Zhang, H. Peng, J. Fu, B. Li, and W. Hu, ‘‘Ocean: Object-aware anchor-\nfree tracking,’’ in Proc. Eur. Conf. Comput. Vis.Cham, Switzerland:\nSpringer, 2020, pp. 771–787.\n[100] M. Danelljan, L. Van Gool, and R. Timofte, ‘‘Probabilistic regression for\nvisual tracking,’’ inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2020, pp. 7181–7190.\nVOLUME 11, 2023 80325\nJ. Kugarajeevan et al.: Transformers in Single Object Tracking: An Experimental Survey\n[101] C. Mayer, M. Danelljan, D. Pani Paudel, and L. Van Gool, ‘‘Learning\ntarget candidate association to keep track of what not to track,’’ in Proc.\nIEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 13424–13434.\n[102] H. Zhao, D. Wang, and H. Lu, ‘‘Representation learning for visual object\ntracking by masked appearance transfer,’’ in Proc. IEEE/CVF Conf. Com-\nput. Vis. Pattern Recognit., Jun. 2023, pp. 18696–18705.\n[103] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Dollár, and C. L. Zitnick, ‘‘Microsoft COCO: Common objects in\ncontext,’’ in Proc. Eur. Conf. Comput. Vis.Cham, Switzerland: Springer,\n2014, pp. 740–755.\n[104] E. Real, J. Shlens, S. Mazzocchi, X. Pan, and V . Vanhoucke, ‘‘YouTube-\nBoundingBoxes: A large high-precision human-annotated data set for\nobject detection in video,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jul. 2017, pp. 7464–7473.\n[105] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‘‘Deformable\nDETR: Deformable transformers for end-to-end object detection,’’ in\nProc. Int. Conf. Learn. Represen., 2021, pp. 1–16. [Online]. Available:\nhttps://openreview.net/forum?id=gZ9hCDWe6ke\n[106] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for\nimage recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[107] X. Hu, H. Liu, Y . Hui, X. Wu, and J. Zhao, ‘‘Transformer feature enhance-\nment network with template update for object tracking,’’ Sensors, vol. 22,\nno. 14, p. 5219, Jul. 2022.\n[108] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Girshick,\n‘‘Masked autoencoders are scalable vision learners,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 15979–15988.\n[109] Y . Wu, J. Lim, and M.-H. Yang, ‘‘Online object tracking: A bench-\nmark,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2013,\npp. 2411–2418.\n[110] N. V . Chappa, P. Nguyen, A. H. Nelson, H.-S. Seo, X. Li, P. D. Dobbs, and\nK. Luu, ‘‘Spartan: Self-supervised spatiotemporal transformers approach\nto group activity recognition,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., 2023, pp. 5157–5167.\n[111] E. Aksan, M. Kaufmann, P. Cao, and O. Hilliges, ‘‘A spatio-temporal\ntransformer for 3D human motion prediction,’’ in Proc. Int. Conf. 3D Vis.\n(3DV), Dec. 2021, pp. 565–574.\n[112] G. Luo, Y . Zhou, X. Sun, Y . Wang, L. Cao, Y . Wu, F. Huang, and\nR. Ji, ‘‘Towards lightweight transformer via group-wise transformation\nfor vision-and-language tasks,’’ IEEE Trans. Image Process., vol. 31,\npp. 3386–3398, 2022.\n[113] S. Mehta, M. Ghazvininejad, S. Iyer, L. Zettlemoyer, and H. Hajishirzi,\n‘‘Delight: Deep and light-weight transformer,’’ in Proc. Int. Conf. Learn.\nRepresen., 2021, pp. 1–19. [Online]. Available: https://openreview.\nnet/forum?id=ujmgfuxSLrO\n[114] A. Hatamizadeh, G. Heinrich, H. Yin, A. Tao, J. M. Alvarez, J. Kautz,\nand P. Molchanov, ‘‘FasterViT: Fast vision transformers with hierarchical\nattention,’’ 2023, arXiv:2306.06189.\n[115] Z. Li, L. Ma, M. Chen, J. Xiao, and Q. Gu, ‘‘Patch similarity aware data-\nfree quantization for vision transformers,’’ in Proc. Eur. Conf. Comput.\nVis. Cham, Switzerland: Springer, 2022, pp. 154–170.\n[116] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\nand W. Chen, ‘‘LoRA: Low-rank adaptation of large language models,’’\nin Proc. Int. Conf. Learn. Represen., 2022, pp. 1–13. [Online]. Available:\nhttps://openreview.net/forum?id=nZeVKeeFYf9\n[117] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. H. Tay,\nJ. Feng, and S. Yan, ‘‘Tokens-to-token ViT: Training vision transformers\nfrom scratch on ImageNet,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV), Oct. 2021, pp. 538–547.\nJANANI KUGARAJEEVAN received the B.Sc.\ndegree (Hons.) in computer science from the\nUniversity of Jaffna, Sri Lanka, in 2017. She is\ncurrently pursuing the Ph.D. degree with the Uni-\nversity of Moratuwa, Sri Lanka. She is a Lecturer\nwith the Department of Interdisciplinary Stud-\nies, University of Jaffna. Her research interests\ninclude machine learning, computer vision, and\ndeep learning.\nTHANIKASALAM KOKUL (Member, IEEE)\nreceived the B.Sc. degree (Hons.) in computer\nscience from the University of Jaffna, Sri Lanka,\nin 2009, and the Ph.D. degree in computer science\nfrom the Queensland University of Technology\n(QUT), Australia, in 2019. He is currently a Senior\nLecturer with the Department of Computer Sci-\nence, University of Jaffna. His research interests\ninclude object tracking, computer vision, and deep\nlearning.\nAMIRTHALINGAM RAMANAN (Senior Mem-\nber, IEEE) received the B.Sc. degree (Hons.) in\ncomputer science from the University of Jaffna, Sri\nLanka, in 2002, and the Ph.D. degree in computer\nscience from the University of Southampton, U.K.,\nin 2010. He is a Professor in computer science with\nthe Department of Computer Science, University\nof Jaffna. His research interests include the algo-\nrithmic and applied aspects of machine learning,\ncomputer vision, and deep learning.\nSUBHA FERNANDO (Member, IEEE) received\nthe B.Sc. degree in statistics and computer sci-\nence from the University of Kelaniya, Sri Lanka,\nin 2004, and the M.E. and D.Eng. degrees from\nthe Nagaoka University of Technology, Japan,\nin 2010 and 2013, respectively. She is a Senior\nLecturer with the University of Moratuwa. Her\nresearch interests include deep learning, machine\nlearning, multi-agent systems, and data mining\ntechnologies.\n80326 VOLUME 11, 2023"
}