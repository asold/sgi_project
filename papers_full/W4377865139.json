{
    "title": "Observations on LLMs for Telecom Domain: Capabilities and Limitations",
    "url": "https://openalex.org/W4377865139",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5083371463",
            "name": "Sumit Soman",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5035716225",
            "name": "H. G. Ranjani",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4304697829",
        "https://openalex.org/W4320186815",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W4319301677",
        "https://openalex.org/W4362707004",
        "https://openalex.org/W4307001524",
        "https://openalex.org/W3198690080",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W1975879668",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4366341216",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W4319779057",
        "https://openalex.org/W4360836968"
    ],
    "abstract": "The landscape for building conversational interfaces (chatbots) has witnessed a paradigm shift with recent developments in generative Artificial Intelligence (AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and GPT4), Google's Bard, Large Language Model Meta AI (LLaMA), among others. In this paper, we analyze capabilities and limitations of incorporating such models in conversational interfaces for the telecommunication domain, specifically for enterprise wireless products and services. Using Cradlepoint's publicly available data for our experiments, we present a comparative analysis of the responses from such models for multiple use-cases including domain adaptation for terminology and product taxonomy, context continuity, robustness to input perturbations and errors. We believe this evaluation would provide useful insights to data scientists engaged in building customized conversational interfaces for domain-specific requirements.",
    "full_text": "OBSERVATIONS ON LLM S FOR TELECOM DOMAIN :\nCAPABILITIES AND LIMITATIONS ∗\nSumit Soman\nGlobal AI Accelerator\nEricsson\nBangalore, India\nsumit.soman@ericsson.com\nRanjani H G\nGlobal AI Accelerator\nEricsson\nBangalore, India\nranjani.h.g@ericsson.com\nABSTRACT\nThe landscape for building conversational interfaces (chatbots) has witnessed a paradigm shift with\nrecent developments in generative Artiﬁcial Intelligence (AI) based Large Language Models (LLMs),\nsuch as ChatGPT by OpenAI (GPT3.5 and GPT4), Google’s Bard, Large Language Model Meta AI\n(LLaMA), among others. In this paper, we analyze capabilities and limitations of incorporating such\nmodels in conversational interfaces for the telecommunication domain, speciﬁcally for enterprise\nwireless products and services. Using Cradlepoint’s publicly available data for our experiments, we\npresent a comparative analysis of the responses from such models for multiple use-cases including\ndomain adaptation for terminology and product taxonomy, context continuity, robustness to input\nperturbations and errors. We believe this evaluation would provide useful insights to data scientists\nengaged in building customized conversational interfaces for domain-speciﬁc requirements.\nKeywords Chatbot · Large Language Models · Generative AI · ChatGPT · GPT3.5 · GPT4 · Bard · LLaMA · Telecom ·\nEnterprise Wireless.\n1 Introduction\nThere has been signiﬁcant traction in the development of Large Language Models (LLMs) recently, particularly\ngenerative Artiﬁcial Intelligence (AI) based LLMs. OpenAI introduced ChatGPT2 [1, 2], and subsequently ChatGPT\nPlus3 based on GPT3.5 and GPT4 [3]. Google released Bard4, based on Language Model for Dialogue Applications\n(LaMDA) [4]. Other notable efforts include LLaMA [5], Chinchilla [6], PaLM [7], UL2 [8], Cerebras-GPT [9].\nLiterature on LLM based chat interfaces such as ChatGPT and Bard [10, 11] discuss capabilities and prospects, such as\nfor legal use-cases [12]. With general availability of user interfaces, models and datasets [13], the development of (and\nscope for using) conversational interfaces has gained interest, as these models enable question answering for performing\nvarious tasks [14]. Motivated by recent evaluation reports [15 –17], we investigate domain-speciﬁc capabilities and\nlimitations of some models.\nOur speciﬁc focus in this paper is on conversational assistants (or interfaces) for the telecom domain. A common\nuse-case for organizations with enterprise (as well as consumer grade) products and associated services is to have\na conversational assistant that can help users with tasks such as ﬁnding information about products and services,\ninitial support for installation and conﬁguration, operational use-cases like troubleshooting or performance monitoring,\namong others. In such cases, a conversational interface is the ﬁrst line of interaction with the end-user. Thus, it\nbecomes important for the chatbot (that often use LLMs) to understand domain terminology, concepts and context\nin such conversations. These aspects are considered in our work using a speciﬁc example for enterprise wireless\n∗Submitted for review to GLOBECOM 2023.\n2https://openai.com/blog/chatgpt\n3https://openai.com/blog/chatgpt-plus\n4https://bard.google.com/\narXiv:2305.13102v1  [cs.HC]  22 May 2023\nObservations on LLMs for Telecom Domain\nproducts. However, in principle, the study and ﬁndings can be extended to other domains where training or ﬁne-tuning\ndomain-speciﬁc conversational models would be of interest.\nWe aim to address the following Research Questions (RQ):\n• RQ1: Can conversational interfaces that use generative AI LLMs adapt to questions related to domain speciﬁc\nterminology? E.g. telecom domain and product queries.\n• RQ2: How do these models fare in retaining context(s) across conversations? E.g. co-reference resolution\nfrom queries and long-term context retention.\n• RQ3: Are the recipes (responses provided as steps to be followed by the user) generated by such models\naccurate and reliable, or, do they tend to hallucinate?\n• RQ4: Are these models robust to language or grammatical perturbations and can they adapt to speciﬁc\ndomains?\nTo address these questions, we devise and conduct experiments using multiple generative AI models, including GPT4,\nGPT3.5, Bard (based on LaMDA) and LLaMA provided through HuggingChat5. Our experiments are mapped to the\nrespective RQs raised above. Having identiﬁed the typical requirements for wireless Enterprise chat interfaces, we\nwish to highlight that these requirements involve domain intensive terminology and (organization) speciﬁc products.\nThe purpose of this paper is to share the observations on strengths and limitations of some LLMs as a user facing\ntouch-point. Owing to the domain terminology and speciﬁc products, we think it may be injudicious to evaluate the\nmodels (using APIs) on large internal datasets without uncovering initial ﬁndings. The discussion in this paper is based\non the responses received from the chat interfaces of the respective models. Training data used for different models and\ntheir token lengths are listed in Table 1. We believe this captures the popular spectrum of generative AI LLMs available\nat this point of time (the experiments have been conducted during March - April 2023).\nModel Training data Token Length\nGPT 3.5 (175 B) Sept 2021 4k\nGPT 4 (1 T) Sept 2021 8k/32k\nBard/LaMDA (137 B) 2022 (First Half) 512\noasst-sft-6-llama-30b (65 B) March 2023 4k\nTable 1: Training data cut-off & token length of the LLMs\n2 Experiments\nWe evaluate the models for multiple question-answering tasks, as detailed in the following sub-sections. The experiments\nare categorized into domain Question-and-Answer (Q&A) (E1), product Q&A (E2), context continuity (E3 - emulating\ncontext continuity for a product information query sequence, and E4 - emulating context continuity in a troubleshooting\nscenario) and robustness to spelling (or language) perturbations (E5).\n2.1 Domain Q&A\nThis pertains to question-answering task related to telecom domain (to address RQ1), including aspects such as network\ntechnology (4G, 5G etc.) and capabilities. The answers are evaluated based on the ability to discern such questions\nfrom common vocabulary. This is an important challenge when we use such LLMs for domain speciﬁc tasks. We\nnote that though LLMs may have access to the domain (owing to the large and diverse training data), yet they may\nnot be trained on domain-speciﬁc vocabulary, often due to dependence on Subject Matter Experts (SMEs) or lack of\nstandardized domain taxonomy. We include four questions as shown in Table 2 based on 5G technology6. Questions\n1-3 are related to general telecom domain, while Q4 is related to modem capability. Assessment of responses to these\nquestions may be subjective in nature, as such we also evaluated Mean Opinion Score (MOS) and inter-rater agreement\nto quantitatively substantiate our ﬁndings.\n5https://huggingface.co/chat/\n6The deployment of 5G picked up pace from 2019. Most LLMs are trained using data upto 2021 (1 and hence the models are\nexpected to be able to provide answers.\n2\nObservations on LLMs for Telecom Domain\nE1: Domain Q&A\nQ1 What are the different 5G spectrum layers?\nQ2 What are the different 5G architectures?\nQ3 What are the uses of mid-band 5G?\nQ4 Can we have a single modem, steering between LTE PDNs or 5G network slices?\nTable 2: Questions evaluated for domain Q&A.\n2.2 Product Q&A\nThe other aspect related to domain adaptation is comprehending products (names as well as model names, components,\nspeciﬁcations etc.) correctly. This is relevant since the questions from users may relate to detailed information about\nthe products they use or intend to purchase, or pertain to operational aspects, such as installation, conﬁguration,\ntroubleshooting, among others. There may also be questions related to comparison of products, in which case the LLM\nshould be able to interpret product names and speciﬁcations. We evaluate the seven questions shown in Table 3 for\nrepresentative Cradlepoint products (whose datasheets are available publicly). One of the questions (Q5) pertains to\na non-existent product model number at the time of evaluation. The purpose of placing this question in proximity to\na similar question (Q4) with a correct product model number is to observe the ability of LLMs to discern factually\nincorrect questions, as well as possible conﬁdence of the response henceforth. This experiment maps to RQ1 and RQ3\n(for recipe generation - Q3 and hallucination - Q4 and Q5).\nE2: Product Q&A\nQ1 How many expansion slots are there in E300 and what types of slots are they?\nQ2 What are the operating conditions for IBR900 router?\nQ3 What are the steps to setup IBR900?\nQ4 What is the power consumption of IBR1700 as per product speciﬁcations?\nQ5 What is the power consumption of IBR700 as per product speciﬁcations?\nQ6 Which are the ruggedized routers of Cradlepoint?\nQ7 Compare the E300 and IBR900 router speciﬁcations?\nTable 3: Questions evaluated for product Q&A\n2.3 Context Continuity\nContext continuity is important for conversational interfaces for better user experience, as mentioned in RQ2. This\nrefers to the ability to retain context in user conversation (queries) from previous queries. We evaluate two ﬂavors for\ncontext continuity. Table 4 (E3) relates to queries on product speciﬁcations where subsequent queries need to retain\nproduct information from Q1. In Q3 to Q5 of E3, an intentional input perturbation as typographical error for LTE\nbands (as “let” bands) is also introduced in this evaluation sequence, which is relevant to RQ4. Q4 and Q5 of E3\npertains to the ability to discern differences between product and model. In Table 5 (E4), we simulate a troubleshooting\nconversation where the user asks multiple queries related to issues with a router and the domain context needs to be\nretained across the questions. We expect most of the responses to be drawn from the publicly available quick start\nguides provided for the product.\nE3: Context Continuity (Product Information)\nQ1 Can we monitor cellular health as a service for IBR700?\nQ2 Can we check using web links?\nQ3 Does this inform about let bands used? If yes, list the routers?\nQ4 Does this inform about the let bands used for 1200M modem?\nQ5 Does this inform about the let bands used for 1200M-B modem?\nTable 4: Context Continuity - 1\n2.4 Input Perturbations (Language Errors)\nThis experiment aims to address RQ4. The questions in Table 6 include common language errors, related to both general\nEnglish language usage, as well as domain terminology - protocols like Internet Key Exchange (IKE) and Internet\nProtocol Security (IPSec). The errors, indicated in italics, are also generated based on keyboard distance.\n3\nObservations on LLMs for Telecom Domain\nE4: Context Continuity (Troubleshooting)\nQ1 I have a Cradlepoint E300 router whose cellular health LED shows one\nblinking bar and power LED is yellow. What should I do?\nQ2 Which LED will show signal strength?\nQ3 How do I update its ﬁrmware?\nQ4 Which card can I insert and what should I check for?\nQ5 Can you give me the web link of the document for troubleshooting it?\nQ6 Is it possible to factory reset? Can you point me to the\ndocument URL for my router?\nQ7 What problem was I facing earlier?\nTable 5: Context Continuity - 2\nE5: Language Errors\nQ1 Whact is the vpn tunhel counvt in W2005?\nQ2 Is locatoon servcies included in my subcrption?\nQ3 Can we monitor cellular hdalth as a servkce for IBR1700?\nQ4 Can we condifure ispec?\nQ5 Can we condifure ike?\nQ5 Can we conﬁfure ike?\nQ5 Can we cahnge the aldrting tije peeiod?\nTable 6: Language errors (spelling perturbations).\n2.5 Parameters and Prompts Used\nWe describe the model parameters set for the experiments reported in this work in Table 7. For fairness of comparison,\nwe have chosen default parameter settings for all cases. The trial version of Bard has been used (there is no conﬁguration\noption available in the web-based user interface). Similarly for LLaMA, the interface through HuggingChat provides\noasst-sft-6-llama-30b model and does not include any conﬁgurable parameter settings.\nModel Parameters\nChatGPT Plus (gpt-4) Temperature = 0.5, Max Length = 2048, Top P = 1, Frequency Penalty = 0, Presence\nPenalty = 0\nChatGPT (gpt-3.5-turbo) Temperature = 0.5, Max Length = 2048, Top P = 1, Frequency Penalty = 0, Presence\nPenalty = 0\nBard\nHuggingChat User cannot conﬁgure parameters (as on April 2023).\nTable 7: Model parameters used for experiments.\nThe prompts used are common across all the LLMs for a fair comparison. For E1, the following prompt is used - “You\nare an AI assistant for me. You are given a telecom related question. Provide a short answer. If you don’t know the\nanswer, just say “I do not know. ” Do not try to make up an answer. If the question is not about telecom, politely inform\nthem that you are tuned to only answer questions about telecom. ”.\nFor E2-E5, the prompt used is “You are an AI assistant for me. The documentation is located at\nhttps://customer.cradlepoint.com. You are given a question. Provide a conversational multi-step answer. You should\nonly use content that is in the Cradlepoint URL. If you don’t know the answer, just say “I do not know. ” Do not try to\nmake up an answer. If the question is not about Cradlepoint, politely inform them that you are tuned to only answer\nquestions about Cradlepoint. ”\n3 Results and Discussion\nWe present our observations on the suitability of responses for each RQ based on the corresponding experiment. The\nresponses are analysed for each LLM in this section.\n4\nObservations on LLMs for Telecom Domain\n3.1 RQ1 - E1 and E2\nFor E1, Q1, Bard identiﬁes the three bands as low, mid and high, but does not provide speciﬁc details of frequency bands.\nInstead, it uses comparative adverbs to describe the pros and cons of each band, thus making the answer descriptive.\nGPT3.5 simply names the bands, no details about frequency bands are provided, while GPT4’s response includes both\nthe categorization and frequency range for the respective bands, as well as a description of pros and cons of each band.\nIt identiﬁes the bands as below 1 GHz, 1-6 GHz and above 6 GHz. LLaMA, on the other hand, identiﬁes the frequency\nbands as below 600 MHz, 600 MHz - 24 GHz and above 24 GHz, but does not provide any comparison of pros and\ncons. As the frequency band description may vary across multiple data sources, for a naïve user, the information about\npros and cons of the respective bands would be useful. It is however, equally important, to provide accurate information\nof frequency bands.\nFor Question 2 (related to 5G architecture), Bard identiﬁes Stand-Alone (SA) and Non Stand-Alone (NSA) as possible\narchitectures and elucidates on deﬁnition and beneﬁts. Another observation from Bard’s response is that it includes an\nadditional paragraph to describe “network slicing” and “edge computing” as 5G architectures, with a short deﬁnition.\nGPT3.5 identiﬁes centralized, distributed and cloud RAN as architectures. GPT4, however, identiﬁes SA and NSA, and\nprovides a description for each to discern the fundamental difference. LLaMA identiﬁes SA and NSA, followed by a\ndistinction between the two by describing the SA, thus indirectly implying that NSA relies on existing infrastructure.\nQuestion 3 pertains to applications of mid-band 5G. Here, Bard elucidates on enhanced Mobile BroadBand (eMBB),\nFixed-Wireless Access (FWA) and enterprise use-cases including Augmented/Virtual Reality (AR/VR), automation,\nmachine learning and video streaming. GPT3.5 begins by calling out the advantages related to speed and coverage,\nand mentions possible applications like video streaming/calling, gaming and Internet of Things (IoT) devices. GPT4\nprioritizes possible applications in its reply (includes eMBB, FW A, smart cities, connected vehicles and IoT devices)\nand then mentions the advantages of mid-band 5G. LLaMA calls out the subjective nature of use-cases, and hence\nprioritizes the advantages, followed by brief exemplary use-cases like connected cars, cities and IoT devices. We\nobserve that applications may be subjective and non-restrictive, hence it is important for the user to be provided with\nthe reasons that would help identify types of applications that can be enabled with mid-band 5G. However, GPT4 and\nBard have elaborate answers compared to the others for this question.\nQuestion 4 relates to ability of modems to support LTE and 5G along with support for network slices. Here, Bard\ngives the right answer, the underlying technology is pointed out incorrectly as network slicing (and its advantages is\nelaborately described in another). Further, Bard answers that it is possible to connect to both LTE and 5G at same time\nthrough conﬁgurations, making the answer/justiﬁcation incorrect. GPT3.5 provides a short conﬁrmation, but incorrectly\nrefers to it as dual connectivity. It is of interest to note that both Bard and GPT3.5 are incorrectly associating steering\nwith dual connectivity. GPT4 not only provides the right answer, but also the correct justiﬁcation while referring to\n3GPP release and appropriate justiﬁcation of 5G network slicing based on the application. LLaMA though agrees it is\npossible, also incorrectly associates steering to multi-connectivity. We hypothesize co-occurrence of LTE and 5G can\nresult in spurious correlation to dual-connectivity, hence these LLMs may incorrectly answer these.\n3.1.1 MOS and Fleiss’ kappa\nIn order to quantify the assessment of responses obtained from various models, we asked SMEs to assess the responses\nobtained and individually rate them on a scale of 1 − 5, where a response of 1 corresponds to low relevance and 5\ncorresponds to high relevance (technical correctness and completeness of responses). All SMEs have 10 or more years\nof experience in the telecom domain and have been actively involved with development of telecom AI use cases over\nthe past few years. The Mean Opinion Score (MOS) [18] obtained from independent evaluation by ﬁve SMEs for the\nrespective questions (Q1-Q4) is shown in Table 8.\nQuestion GPT3.5 GPT4 Bard LLaMA\nQ1 2.8 4.4 4.2 2.2\nQ2 2.6 5 3.8 3\nQ3 2.8 4.8 4.2 2.2\nQ4 3.4 4.2 4.2 2.2\nAvg 2.9 4.6 4.1 2.4\nTable 8: Mean Opinion Score for questions\nAcross the questions, we ﬁnd that the MOS is highest for GPT4 responses consistently, while it is lowest for LLaMA.\nWe also compute inter-rater agreement score using Fleiss’ kappa [19], which is obtained as 0.316, indicating fair\n5\nObservations on LLMs for Telecom Domain\nFigure 1: Ground truth for E2, Q2 [Source: IBR900 Datasheet]\nagreement. The scores across the questions are consistent though. It may be noted here that Fleiss’ kappa has been\ncomputed across 16 categories (4 questions for 4 models).\nFor product-related queries, we discuss ﬁndings from E2. Question 1 pertains to number and type of expansion slots\nin Cradlepoint’s E300 router. From the product datasheet, there are 3 expansion slots - for MC400 modular modem\nexpansion, USB2.0 Type A and MC20BT expansion. Bard and GPT3.5 do not correctly identify the expansion slots, in\nfact Bard generates a recipe to access the expansion slots. GPT4 correctly identiﬁes two of the expansion slots and\ngives a link to the speciﬁcation sheet. LLaMA too incorrectly states that one slot type option is available but does not\ngive details.\nQuestion 2 pertains to operating conditions for Cradlepoint’s IBR900 (a snapshot of product datasheet is shown in\nFigure 1). It is possible that the content in product datasheets is updated, hence access to the latest product speciﬁcations\ndatasheet is important for such queries to be answered correctly. However, the updates would pertain to adding\ncertiﬁcations or regulatory approvals. It is very unlikely that operating temperatures would change. Hence, this\nspeciﬁcation attribute has been chosen to test for factual accuracy of the responses.\nAll the LLMs provide factually incorrect ranges. Bard provides temperature, humidity, altitude, shock and vibration\nbut the ranges are incorrect. GPT3.5 also provides power input and consumption, while GPT4 provides operating and\nstorage temperature and humidity. LLaMA provides generic temperature range and humidity rate from Cradlepoint.\nQ3-Q5 of E2 relate to recipe generation (RQ3) and is discussed in the corresponding sub-section 3.3.\nE2, Q6 pertains to ﬁnding products of a speciﬁc classiﬁcation (ruggedized routers) from multiple products. E2, Q7\nrelates to comparison of product speciﬁcations for two routers (E300 and IBR900). Both of these relate to aggregation\nof information from multiple products. A correct response to this question must list 8 products. Responding to E2, Q6,\nBard suggests IBR1700, IBR900 and R2100 (implying 100% on accuracy, 37.5% on completeness); GPT3.5 suggests\nIBR1700, IBR900, IBR600C and IBR200 (100% accuracy, 50% completeness); GPT4 suggests IBR900, IBR1700\nand IBR600C (implying 100% on accuracy, 37.5% on completeness); while LLaMA indicates IBR3000/300B series\nand ECM managed routers, along with MC4000, NetCloud Essential and NetCloud Plus (implying 0% accuracy, 0%\ncompleteness). Although none of the LLMs list the complete set, access to latest information about products determines\nthe response of the model, and hence the low numbers on completeness of the response.\nWith respect to Q7, Bard generates a table of 12 speciﬁcations and compares them across the products, and also\nincludes a textual comparison of important features. 4 of the speciﬁcation ﬁelds are not relevant to the router products\nconsidered and have no source in Cradlepoint domain. Of the remaining 8, none of the answers are correct. GPT3.5\ncompares using 6 speciﬁcation ﬁelds as bullet lists, all of them are relevant. Out of these, only 2 answers are factually\ncorrect. GPT4 compares through 8 speciﬁcation ﬁelds (all relevant and 4 answers being correct for both products)\nand provides links to speciﬁcation sheets (although domain part of url is correct, there is no such sheet available).\nSurprisingly, LLaMA points to a non-existent link that is intended to be comparing the products. We also observe that a\ntabular representation of the speciﬁcations is useful for the reader for a comparison, such as the one provided by Bard.\nConclusions for RQ1:GPT4 and Bard provide better responses for domain-speciﬁc questions. Hallucination is a\npotential risk for cases with high speciﬁcity, such as product models, names, speciﬁcations and components. Fine-tuning\nmodels with domain data corpus and additional pre and post-processing may alleviate some risks. Access to recent data\nis required for addressing completeness in the responses.\n6\nObservations on LLMs for Telecom Domain\n3.2 RQ2\nWe address context retention aspects for conversations in this sub-section, that map to E3 and E4. The expected context\nfor the set of queries is indicated in Figure 2. The boxes with solid lines indicate the context being referred to, and boxes\nwith dotted lines indicate the referring segments from following queries. Each set of referring and referred contexts\nis indicated in a different color (blue, green). For our experiments, we consider retaining context across previous\n2-3 questions as “short-term” context retention, while references following more questions would be considered as\n“long-term” context retention. We also point out here that the conversational interfaces used for our experiments do not\nprovide for any explicit context-retention conﬁguration.\n(a) Context Continuity - E3\n(b) Context Continuity - E4\nFigure 2: Context continuity - solid lines indicate context and dotted boxes connect the referring segments from queries.\nIn Fig. 2a, we refer to context of monitoring cellular health as a service in Q2, while other questions refer to context of\nrouter (product) and models. However, the word “this” here is ambiguous and can either refer to any Cradlepoint web\nlink or the link provided in response to E3, Q2. This scenario is typically encountered in a human interaction and one\nwould expect a counter-question for clariﬁcation. However, current LLMs do not ask for clariﬁcation and hence in\nthis report, we evaluate only the LLM response inspite of the ambiguity. Fig. 2b illustrates a typical troubleshooting\nscenario, that can have both short (blue) and long (green)-term context references.\nFor E3, Bard is able to retain the context throughout the conversation, and all replies are relevant for IBR1700. It\nalso provides the speciﬁc LTE bands supported (88% accurate and 69% complete). Bard doesn’t generate any URLs.\nGPT3.5 and GPT4 also retain context, but they do not list the speciﬁc LTE bands that are supported. However, the links\nprovided are incorrect (and do not exist). Responding to E3, Q3-Q5 set, Bard indicates that LTE band information is\navailable through the NetCloud account. GPT3.5 and GPT4 also indicate the same. This is incorrect. With LLaMA,\n7\nObservations on LLMs for Telecom Domain\ncontext retention is seen between E3, Q1-Q2. The links provided do not exist and hence are incorrect. LLaMA does not\nretain the context between E3,Q2 to E3,Q3 and responds by listing three Cradlepoint routers which support LTE (66%\naccurate) along with the countries supported. Of the two correct routers identiﬁed, one is currently discontinued. We do\nnot assess completeness as this requires access to latest product list. Subsequently, LLaMA responds to E3, Q4 and E3,\nQ5 keeping response of E3, Q3 as the context. Hence, technically, the answers are correct as it only addresses LTE\nbands supported.\nFor E4, Bard retains context related to the router (E300) for which the troubleshooting questions are posed. It points to\nrelevant steps for ﬁrmware upgrade with reference to the router (post disambiguation), though the exact steps for SIM\nCard are not retrieved. It politely refuses to return a document link for troubleshooting the product (Q5). The factory\nreset steps (Q6) are not speciﬁc to the product and the response does not reference the product in Q1. For long term\ncontext (Q7), the top response from Bard does not reﬂect the original problem and the reply indicates that the steps\nsuggested have “solved” the problem, without any such indication being explicit in the query. Overall, 6 of 7 responses\nfrom Bard are categorized as acceptable by SME.\nGPT3.5 has a better response to the E4, Q1 by suggesting steps to debug power source. The context beyond E4, Q3 is\ngeneric, does not have product speciﬁc responses and returns generic router document links for Q6 and Q7. It however,\ndoes retain the long-term context, as it replies with the original problem as described by the user for response to Q7.\nThe links provided do not exist even though the domain is correct. Overall, 3 of the 7 responses from GPT3.5 are\ncategorized as acceptable.\nThe limitations of GPT3.5 on context retention are overcome in the responses from GPT4, where responses and\ndocument links speciﬁc to the product are returned (though the link is incorrect and does not exist). The context is also\nretained through Q7, where the response disambiguates the two distinct issues that were described by the user in Q1. 6\nof the 7 responses from GPT4 for E4 are SME acceptable.\nLLaMA’s responses for E4,Q1 seems largely deviant to the user query. It also does not explicitly retain product-speciﬁc\ncontext for Q1 and Q2, but it does so for Q3 (the answer to Q3 itself may be incorrect). For Q4, it asks for more context\nand does not give any reply (which is a desired feature), while for Q5 it returns irrelevant URLs. For Q6, it appears\nit has lost context to disambiguate the router and hence, it returns links for other products, while for Q7 the issue is\nincorrectly re-stated with additional statements, and it claims to have raised a support ticket for a technician to attend to\nthe issue (while also providing a reference number for the same). 3 of 7 responses from LLaMA are categorized as\nacceptable by SME. Based on these, we primarily ﬁnd GPT4 and Bard’s responses for context retention to be more\npersistent and relevant.\nConclusions for RQ2:Amongst the LLMs considered in this work, GPT4 provide both long and short term context\nretention and can potentially be useful in scenarios which require troubleshooting. However, when it comes to context\nretention, it is worth noting that ambiguous queries can result in ambiguous answers which is true philosophically also,\neven with humans. Hence, context disambiguation must be evaluated as a shared responsibility between the user and\nLLM. In addition, while the current technology push is towards answering all the questions (as \"interpreted\" by the\nLLM), it is desirable that the LLM asks the user for clariﬁcation (like LLaMA) or politely refuse to provide any URL\n(like Bard).\n3.3 RQ3\nWe discuss results for queries that expect recipe generation from the models, such as E2, Q3. The response from\nBard provides 6 steps, including conﬁguring the router through the web-based interface. We ﬁnd this is reliable, and\nconforms to the process of router setup for Cradlepoint products (except that there is no mention of SIM card for cellular\nconnectivity; this, however, may not be the primary concern). GPT3.5 provides 6 steps, and instructs on activation\nof router in steps 3 & 4, which are not adherent to Cradlepoint process. It provides a link to quick start guide where\ndomain is correct, but actual link doesn’t exist. GPT4 provides 10 steps for setting up IBR900. The steps till hardware\nsetup are accurate, but is also incorrect regarding the access/activation step. The steps provided by GPT4 are more\ndetailed and speciﬁc in comparison to GPT3.5. The URL link provided by GPT4 is also incorrect, but the domain is\ncorrect. LLaMA provides a URL, which is “safer” but the URL is non-existent. For a quick comparison of LLMs within\nthis experiment, the accuracy of the steps is 100%, 50%, 70%, 0% for Bard, GPT3.5, GPT4 and LLaMA respectively.\nFor evaluating hallucination, we refer to responses for E2, Q4 and Q5. GPT4 too identiﬁes the incorrect product\nand offers information on another product (IBR600C) as an alternative. These product identiﬁcation issues may also\npossibly be indicative of the signiﬁcance of character-level representations in such models [20, pp 7, col 1, para 2].\nGPT3.5 and Bard are, however, unable to distinguish the incorrect product name and generate responses without any\nindication of their factual incorrectness. LLaMA is unable to identify the ﬁctitious product and indicates that it does not\n8\nObservations on LLMs for Telecom Domain\nhave information on that product as its training cut-off date was September 2021. In this case, we ﬁnd that GPT4 and\nLLaMA are comparatively reliable (less hallucination) with their responses.\nConclusions for RQ3:We can view two major aspects for reliability. First, the LLM steps for recipe generation may\nnot be completely accurate for any LLM, even if there are guardrails provided in the prompt to prevent hallucination.\nThis could be due to the popularity bias from competitor products and their approach for any task. It is desirable that\nLLM outputs are more reliable (like that of Bard), or LLM redirects the users to the source URL (like in LLaMA), with\nthe URL being a valid one. Second, the LLM may not be able to discern products if its architecture doesn’t include\ncharacter-level representation and are mostly based on tokens. It is possible that due to these shortcomings, LLMs may\nnot be directly useful in scenarios which require reliable answers.\n3.4 RQ4\nThese set of experiments (E3, Q4-Q5 and E5) aim to evaluate robustness of models to input perturbations introduced\nthrough deliberate spelling and language typos, the typos covering both English language and those pertaining to\ntelecom domain terminology and products. The errors in E3 (Q4 and Q5) are resolved unambiguously across all the\nmodels evaluated. Bard is able to resolve all errors in the questions in E5. GPT3.5 fails on correcting the errors in E5,\nQ1 and interacts with the user to clarify the query. GPT4 resolves the error, but is not conﬁdent and hence asks for a\nclariﬁcation from the user. Both GPT3.5 and GPT4 resolve errors in the rest of the questions. LLaMA resolves spell\nerrors for Q1 and Q4 only. However, response to these are incorrect as they do not relate to Cradlepoint’s products\n(despite having the prompt as a guardrail). The responses for Windows machines (Q1) and generic Digital Subscriber\nLine (DSL) modems and other products has partial similarity in product names with Cradlepoint products. In Q5,\nLLaMA responds saying “IKE” does not relate to networking. In the subsequent question (Q6), it is able to correctly\nresolve IKE abbreviation, it does not provide any conﬁguration steps (maybe because it could not resolve “conﬁgure”\nperturbation).\nConclusions for RQ4: From the limited experiments, we observe that Bard is more robust to input perturbations.\nGPT4 is also able to correct the typos and also has the desirable characteristic of clarifying from the user when it is not\nconﬁdent (hypothesis). LLaMA seems to be sensitive to input perturbations. It is also observed that the guard rails\nintroduced through prompts do not hold for LLaMA when the query has input perturbations.\n4 Conclusions and Future Work\nWe evaluated four popular LLMs for typical chatbot requirements in the enterprise wireless domain, utilizing Cradlepoint\nofferings for experiments. We used limited datasets so as to understand the suitability of LLMs, before testing it out on\nlarger datasets. The learnings presented in our work would be useful for other domains as well. We have also introduced\nprompts that serve as guardrails to minimize hallucination.\nSome noteworthy observations are as follows:\n• It is observed that for domain related questions, Bard and GPT4 show promise with respect to accuracy and\ncould be useful. However, it is inevitable to lean towards ﬁne-tuning for performance improvements.\n• GPT4 is found to be most suitable for both short and long-term context retention, based on our assessment. A\ndetailed study in this direction is desirable.\n• It is desirable that LLMs ask clariﬁcations when the user query is ambiguous. Of course, the caveat to\ndetermine if the query is ambiguous is another research problem.\n• It is desirable that LLMs refuse to provide URLs because their operational mode is restricted to train-\ning/inference. It is note-worthy that GPT3.5 and GPT4 can include the domain speciﬁed in the prompt in their\nresponses, though exact URL is incorrect. We hypothesize that plugins (or other interface approaches) could\nfacilitate this.\n• Summarization abilities require reliability. Bard seems to be more closer to the requirements assessed here. A\ndetailed study is required to further ascertain these initial observations.\n• It is desirable that LLMs do not hallucinate product name(s) in such use cases. This may requires LLMs to\nhave character aware features to discern such aspects.\n• LLMs appear to be quite robust to domain related spelling perturbations. This could be because of the context\nobtained from the query or previous conversation(s).\n9\nObservations on LLMs for Telecom Domain\nBased on our ﬁndings, we are inclined to conclude that such LLMs can’t be used directly (in the form made available\npublicly) for enterprise use-cases. It is imperative that these models are ﬁne-tuned for domain speciﬁc tasks. Also, it\nhas been hypothesized that spurious correlations could be the cause of some wrong answers, such as those seen in\nassociation of 4G & 5G with dual connectivity in all LLMs except GPT4. Detailed experiments need to be conducted\nto ascertain and mitigate these risks. Licensing, data security and privacy aspects, deployment costs and associated\nconstraints are practical considerations that also need be assessed and evaluated for enterprise-grade applications.\nAcknowledgments\nWe thank Anubhav Arora and Natarajan Venkataraman from Cradlepoint for their timely and valuable inputs. We\nare grateful to Venkatesan Pradeep, Ram Kumar Sharma, Pushpendra Sharma, Satish Kumar Kolli and K. N. S. S.\nYoganand for their evaluation and inputs on the domain speciﬁc responses. We thank Ramana Murthy, Chandramouli\nSargor and Thirumaran Ekambaram for facilitating this evaluation study and for their unwavering support.\nReferences\n[1] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford, Dario\nAmodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information\nProcessing Systems, 33:3008–3021, 2020.\n[2] Leo Gao, John Schulman, and Jacob Hilton. Scaling Laws for Reward Model Overoptimization. arXiv preprint\narXiv:2210.10760, 2022.\n[3] OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.\n[4] Aaron Daniel Cohen, Adam Roberts, Alejandra Molina, Alena Butryna, Alicia Jin, Apoorv Kulshreshtha, Ben\nHutchinson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung-ching Chang, et al. LaMDA: Language\nmodels for dialog applications. CoRR, abs/2201.08239, 2022.\n[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efﬁcient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[6] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[8] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Huaixiu Steven Zheng, et al. UL2: Unifying Language Learning Paradigms, 2022.\n[9] Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness,\net al. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster.\narXiv preprint arXiv:2304.03208, 2023.\n[10] Bal Ram and Pratima Verma. Artiﬁcial Intelligence AI-based Chatbot Study of ChatGPT, Google AI Bard and\nBaidu AI. World Journal of Advanced Engineering Technology and Sciences, 8(01):258–261, 2023.\n[11] Md Rahaman, MM Ahsan, Nishath Anjum, Md Rahman, and Md Naﬁzur Rahman. The AI Race is On! Google’s\nBard and OpenAI’s ChatGPT Head to Head: An Opinion Article. Mizanur and Rahman, Md Naﬁzur, The AI Race\nis on, 2023.\n[12] Thomas Hoppner and Luke Streatfeild. ChatGPT, Bard & Co.: An Introduction to AI for Competition and\nRegulatory Lawyers. An Introduction to AI for Competition and Regulatory Lawyers (February 23, 2023) , 9,\n2023.\n[13] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah\nBarhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyﬁ, et al. OpenAssistant Conversations–Democratizing\nLarge Language Model Alignment. arXiv preprint arXiv:2304.07327, 2023.\n[14] Toran Bruce Richards. Auto-GPT: An Autonomous GPT-xperiment. Python. https://github. com/Torantulino/Auto-\nGPT, 2023.\n10\nObservations on LLMs for Telecom Domain\n[15] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks\nof Artiﬁcial General Intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, March 2023.\n[16] Joel T Martin. Hello, LaMDA! Brain, 146(3):793–795, 2023.\n[17] Terrence J Sejnowski. Large language models and the reverse turing test. Neural Computation, 35(3):309–342,\n2023.\n[18] ITU-T. V ocabulary for Performance, Quality of Service and Quality of Experience, 2017.\n[19] Joseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378, 1971.\n[20] Milad Moradi and Matthias Samwald. Evaluating the robustness of neural language models to input perturbations.\narXiv preprint arXiv:2108.12237, 2021.\n11"
}