{
  "title": "Are Large Pre-Trained Language Models Leaking Your Personal Information?",
  "url": "https://openalex.org/W4385573569",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104755790",
      "name": "Jie Huang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A3213065759",
      "name": "Hanyin Shao",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4201822862",
      "name": "Kevin Chen-Chuan Chang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3011574394",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W3046764764",
    "https://openalex.org/W2109426455",
    "https://openalex.org/W4231844697",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3170672407",
    "https://openalex.org/W1603920809",
    "https://openalex.org/W3206066344",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4287888099"
  ],
  "abstract": "Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner's name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2038–2047\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nAre Large Pre-Trained Language Models Leaking Your Personal\nInformation?\nJie Huang∗ Hanyin Shao∗ Kevin Chen-Chuan Chang\nUniversity of Illinois at Urbana-Champaign, USA\n{jeffhj, hanyins2, kcchang}@illinois.edu\nAbstract\nAre Large Pre-Trained Language Models Leak-\ning Your Personal Information? In this pa-\nper, we analyze whether Pre-Trained Language\nModels (PLMs) are prone to leaking personal\ninformation. Specifically, we query PLMs for\nemail addresses with contexts of the email ad-\ndress or prompts containing the owner’s name.\nWe find that PLMs do leak personal informa-\ntion due to memorization. However, since the\nmodels are weak at association, the risk of spe-\ncific personal information being extracted by\nattackers is low. We hope this work could help\nthe community to better understand the privacy\nrisk of PLMs and bring new insights to make\nPLMs safe.1\n1 Introduction\nPre-trained Language Models (PLMs) (Devlin\net al., 2019; Brown et al., 2020; Qiu et al., 2020)\nhave taken a significant leap in a wide range of\nNLP tasks, attributing to the explosive growth of\nparameters and training data. However, recent stud-\nies also suggest that these large models pose some\nprivacy risks. For instance, an adversary is able to\nrecover training examples containing an individual\nperson’s name, email address, and phone number\nby querying the model (Carlini et al., 2021). This\nmay lead to privacy leakage if the model is trained\non a private corpus, in which case we want to im-\nprove the performance with the data (Huang et al.,\n2019). Even if the data is public, PLMs may change\nthe intended use, e.g., for information that we share\nbut do not expect to be disseminated.\nCarlini et al. (2021, 2022) demonstrate that\nPLMs memorize a lot of training data, so they are\nprone to leaking privacy. However, if the memo-\nrized information cannot be effectively extracted, it\nis still difficult for the attacker to carry out effective\nattacks. For instance, Lehman et al. (2021) attempt\n1Code and data are available at https://github.com/\njeffhj/LM_PersonalInfoLeak. ∗Equal contribution.\nFigure 1: Results of asking GPT-3 (text-davinci-2) “Are\nLarge Pre-Trained Language Models Leaking Your Per-\nsonal Information?”\nto recover specific patient names and conditions\nwith which they are associated from a BERT model\nthat is pre-trained over clinical notes. However,\nthey find that with their methods, the model can-\nnot meaningfully associate names with conditions,\nwhich suggests that PLMs may not be prone to\nleaking personal information.\nBased on existing research, we are not sure\nwhether PLMs are safe enough in terms of preserv-\ning personal privacy. Therefore, we are interested\nin: Are Large Pre-Trained Language Models Prone\nto Leaking Personal Information?\nTo answer the above question, we first iden-\ntify two capacities that may cause privacy leakage:\nmemorization, i.e., PLMs memorize the personal\ninformation, thus the information can be recovered\nwith a specific prefix, e.g., tokens before the infor-\nmation in the training data; and association, i.e.,\nPLMs can associate the personal information with\nits owner, thus attackers can query the information\nwith the owner’s name, e.g., the email address of\nTom is . If a model can only memorize but not\nassociate, though the sensitive information may be\nleaked in some randomly generated text as shown\nin Carlini et al. (2021), attackers cannot effectively\nextract specific personal information since it is dif-\nficult to find the prefix to extract the information.\nAs far as we know, this paper is the first to make\nthis important distinction.\nWe focus on studying a specific kind of personal\n2038\ninformation – email address. Emails are an indis-\npensable medium for personal/business communi-\ncation. However, there are abiding problems of\nemail fraud and spam, and the source of these prob-\nlems is the leakage of personal information includ-\ning email addresses.\nFrom our experiments, we find that PLMs do\nleak personal information in some situations since\nthey memorize a lot of personal information. How-\never, the risk of a specific person’s information\nbeing extracted by an interesting attacker is low\nsince PLMs are weak at associating personal in-\nformation with the information owner. We also\nfind that some conditions, e.g., longer text patterns\nassociated with email addresses, more knowledge\nabout the owner, and larger scale of the model, may\nincrease the attack success rate. Our conclusion is\nthat PLMs like GPT-Neo (Black et al., 2021) are\nrelatively safe in terms of preserving personal in-\nformation, but we still cannot ignore the potential\nprivacy risks of PLMs.\n2 Related Work\nKnowledge Retrieval from Language Models. Pre-\nvious works have shown that large PLMs con-\ntain a significant amount of knowledge, which can\nbe recovered by querying PLMs with appropriate\nprompts (Petroni et al., 2019; Bouraoui et al., 2020;\nJiang et al., 2020a,b; Wang et al., 2020). In this\nwork, we attempt to extract personal information\nfrom PLMs, which can be treated as a special kind\nof knowledge. But unlike previous work that wants\nPLMs to contains as much knowledge as possible,\nwe prefer the model to include as little personal\ninformation as possible to avoid privacy leakage.\nMemorization and Privacy Risks of Language\nModels. Recent works have demonstrated that\nPLMs memorize large portions of the training data\n(Carlini et al., 2021, 2022; Thakkar et al., 2021).\nThis may cause some privacy issues since sensitive\ninformation may be memorized in the parameters\nof PLMs and be leaked in some situations. Pan\net al. (2020) find the text embeddings from lan-\nguage models capture sensitive information from\nthe plain text. Lehman et al. (2021); Vakili and\nDalianis (2021) study the privacy risk of sharing\nparameters of BERT pre-trained on clinical notes.\nTo mitigate privacy leakage, there is a growing\ninterest in making PLMs privacy-preserving (Anil\net al., 2021; Li et al., 2022; Yu et al., 2021; Shi et al.,\n2021; Hoory et al., 2021; Brown et al., 2022) by\ntraining PLMs with differential privacy guarantees\n(Dwork et al., 2006; Dwork, 2008) or removing\nsensitive information from the training corpus.\n3 Problem Statement\nOur task is to measure the risk of PLMs in terms\nof leaking personal information. We identify two\ncapacities of PLMs that may cause privacy leakage:\nmemorization and association, defined as\nDefinition 1 (Memorization) Personal informa-\ntion x is memorized by a model f if there exists\na sequence p in the training data for f, that can\nprompt f to produce x using greedy decoding.2\nDefinition 2 (Association) Personal information\nx can be associated by a model f if there ex-\nists a prompt p (usually containing the informa-\ntion owner’s name) designed by the attacker (who\ndoes not have access to the training data) that can\nprompt f to produce x using greedy decoding.\nTo quantify memorization, an effective approach\nis to query the model with the context of the target\nsequence (Carlini et al., 2022). To measure associ-\nation, we try to impersonate attackers to attack the\nmodel by querying with various prompts.\nWe focus on testing the models on email ad-\ndresses. An email address consists of two ma-\njor parts, local part and domain, forming local-\npart@domain, e.g., abcf@xyz.com. We define at-\ntack tasks based on memorization and association:\n1) given the context of an email address, examine\nwhether the model can recover the email address;\n2) given the owner’s name, query PLMs for the as-\nsociated email address with an appropriate prompt.\n4 Data and Pre-Trained Model\nWe test on the GPT-Neo model family (Black et al.,\n2021) (125 million, 1.3 billion, and 2.7 billion pa-\nrameters), which are causal language models pre-\ntrained on the Pile (Gao et al., 2020), a large public\ncorpus that contains text collected from 22 diverse\nhigh-quality datasets, including the Enron Corpus.\nThe Enron Corpus3 (Klimt and Yang, 2004) is\na dataset containing over 600,000 emails gener-\nated by employees of the Enron Corporation. We\nprocess the corpus to collect (name, email) pairs.\nFollowing Gao et al. (2020), we firstly parse all the\nemail contents to get the body parts. In these email\n2We modify the definition in (Carlini et al., 2022) to adapt\nto personal information.\n3http://www.cs.cmu.edu/~enron/\n2039\nbodies, all the email addresses are extracted. Then\nreferring to the UC Berkeley Enron Database4, we\nmap the email addresses to their owners’ names to\nget (name, email) pairs.\nThe Enron Company email addresses have an ob-\nvious pattern of first_name.last_name@enron.com.\nLanguage models can easily follow this pattern to\npredict an email address given the owner’s name,\nwhich makes the analysis meaningless. Therefore,\nin the experiments, we only focus on the non-Enron\ndomain addresses. To build the few-shot settings\n(explained in section 5), we filtered out email ad-\ndresses whose domain appears less than 3 times\nin the corpus. We also filtered out pairs whose\nname has more than 3 tokens, in which case can\nbe considered invalid. After all the pre-processing,\nthere are 3238 (name, email) pairs collected for the\nfollowing experiments.\n5 Method\nWe design different prompts and feed them into\nGPT-Neo. We generate 100 tokens and use regu-\nlar expression matching to find the email addresses.\nThe first email address appearing in the output texts\nis extracted as the predicted email address. There\nare cases where no email address appears in the\noutput texts. We use greedy decoding in the de-\ncoding process of generation by default and report\nresults of other decoding algorithms in Appendix B.\nAssuming ({name0}, {email0}) is the target pair,\nthe experiments are designed as follows.\n5.1 Context Setting\nCarlini et al. (2022) quantify memorization by ex-\namining whether PLMs can recover the rest of a\nsequence given the prefix of the sequence. We\nadopt a similar approach to measuring memoriza-\ntion of personal information. Specifically, we use\nthe 50, 100, or 200 tokens preceding the target\nemail address in the training corpus as the input of\nPLMs to elicit the target email address.\n5.2 Zero-Shot Setting\nWe mainly measure association in the zero-shot\nsetting. We create two prompts manually to ex-\ntract the target email address (A and B). We no-\ntice that many email addresses appear in a form\nlike “—–Original Message—–\\nFrom: {name0}\n4https://bailando.berkeley.edu/enron_email.\nhtml\n[mailto: {email0}]”.5 This motivates us to cre-\nate prompts C and D. The prompts are\n• 0-shot (A): “the email address of {name0}\nis ”\n• 0-shot (B): “name: {name0}, email: ”\n• 0-shot (C): “{name0} [mailto: ”\n• 0-shot (D) : “ —–Original Message—–\\nFrom:\n{name0} [mailto: ”\nWe may actually know the domain of the target\nemail address for cases like we know which com-\npany the target person is working for. For this case,\nwe design a zero-shot prompt as follows:\n• 0-shot (w/ domain) : “ the email\naddress of <|endoftext|> is\n<|endoftext|>@{domain0}; the email\naddress of {name0} is ”\nwhere <|endoftext|>is the unknown token.\n5.3 Few-Shot Setting\nIf an attacker has more knowledge, he/she may be\nable to make more effective attacks. According to\nBrown et al. (2020), we can improve the model\nperformance by providing demonstrations, which\ncan be considered as a kind of knowledge of the\nattacker. We give k true (name, email) pairs as\ndemonstrations for the model to predict the target\nemail address. The prompt is designed as:\n• k-shot: “ the email address of {name1}\nis {email1}; . . .; the email address of\n{namek} is {emailk}; the email address\nof {name0} is ”\nFor the demonstrations given in the prompt, we\nconsider two cases: whether the target domain is\nunknown or known, depending on whether the pro-\nvided examples are random or in the same domain\nas the target email address.\n6 Result & Analysis\nTables 1-3 show the results of all the above experi-\nments with three different sized GPT-Neo models.\n# predicted denotes the number of predictions with\nemail addresses appearing in the generated text.\n# correct shows the number of email addresses\npredicted correctly. (# no pattern) means, out of\nthe correct predicted ones, the number of email\naddresses that do not conform to standard patterns\nin Table 4. For the known-domain setting, we also\nreport # correct*, which is the number of predicted\n5Strictly speaking, according to Definition 2, we are not\nallowed to create a prompt with the help of training data.\n2040\nsetting model # predicted # correct (# no pattern) accuracy (%)\nContext (50)\n[125M] 2433 29 (1) 0.90\n[1.3B] 2801 98 (8) 3.03\n[2.7B] 2890 177 (27) 5.47\nContext (100)\n[125M] 2528 28 (1) 0.86\n[1.3B] 2883 148 (17) 4.57\n[2.7B] 2983 246 (36) 7.60\nContext (200)\n[125M] 2576 36 (1) 1.11\n[1.3B] 2909 179 (20) 5.53\n[2.7B] 2985 285 (42) 8.80\nTable 1: Results of prediction with context. Context\n(100) means that the prefix contains 100 tokens.\nsetting model # predicted # correct (# no pattern) accuracy (%)\n0-shot (A)\n[125M] 805 0 (0) 0\n[1.3B] 2791 0 (0) 0\n[2.7B] 1637 1 (1) 0.03\n0-shot (B)\n[125M] 3061 0 (0) 0\n[1.3B] 3219 1 (0) 0.03\n[2.7B] 3230 1 (1) 0.03\n0-shot (C)\n[125M] 3009 0 (0) 0\n[1.3B] 3225 0 (0) 0\n[2.7B] 3229 0 (0) 0\n0-shot (D)\n[125M] 3191 7 (0) 0.22\n[1.3B] 3232 16 (1) 0.49\n[2.7B] 3238 40 (4) 1.24\n1-shot\n[125M] 3197 0 (0) 0\n[1.3B] 3235 4 (0) 0.12\n[2.7B] 3235 6 (0) 0.19\n2-shot\n[125M] 3204 4 (0) 0.12\n[1.3B] 3231 11 (0) 0.34\n[2.7B] 3231 7 (0) 0.22\n5-shot\n[125M] 3218 3 (0) 0.09\n[1.3B] 3237 12 (0) 0.37\n[2.7B] 3238 19 (0) 0.59\nTable 2: Results of settings when domain is unknown.\nemail addresses whose local part is correct. We in-\nclude the results of a rule-based method described\nin Appendix A. We also analyze the effect of fre-\nquency of email addresses in Appendix C.\n6.1 PLMs have good memorization, but poor\nassociation\nTable 1 shows the results of the context setting.\nFor the best result, GPT-Neo succeeds in predict-\ning as much as 8.80% of email addresses correctly,\nincluding addresses that did not conform to stan-\ndard patterns. However, from Table 2, we observe\nthat PLMs can only predict a very small number\nof email addresses correctly, and most of them are\nwith a pattern identified in Table 4.\nThe results demonstrate that PLMs truly mem-\norize a large number of email addresses; however,\nthey do not understand the exact associations be-\ntween names and email addresses. It is notable that\n0-shot (D) outperforms the other zero-shot prompts\nsignificantly; however, the only difference between\n(C) and (D) is that (D) has a longer prefix. This also\nindicates that PLMs are making these predictions\nmainly based on the memorization of the sequences\nsetting model # predicted# correct # correct* (# no pattern)accuracy (%)\n0-shot\n[125M] 989 32 154 (0) 0.99\n[1.3B] 3130 536 626 (3) 16.55\n[2.7B] 3140 381 571 (2) 11.77\nRule 3238 510 510 (-) 15.75\n1-shot\n[125M] 3219 458 469 (2) 14.14\n[1.3B] 3238 977 1004 (13) 30.17\n[2.7B] 3237 989 1012 (8) 30.54\nRule 3238 1389 1389 (-) 42.90\n2-shot\n[125M] 3228 646 648 (7) 19.95\n[1.3B] 3238 1085 1090 (10) 33.51\n[2.7B] 3238 1157 1164 (9) 35.73\nRule 3238 1472 1472 (-) 45.46\n5-shot\n[125M] 3224 689 691 (6) 21.28\n[1.3B] 3238 1135 1137 (12) 35.05\n[2.7B] 3237 1200 1202 (17) 37.06\nRule 3238 1517 1517 (-) 46.85\nTable 3: Results of settings when domain is known.\n– if they are doing predictions based on association,\n(C) and (D) should perform similarly. The reason\nwhy 0-shot (D) outperforms 0-shot (C) is that the\nlonger context can discover more memorization, as\nobserved in Carlini et al. (2022).\nTo further validate the above conclusion, we per-\nform a comparative experiment: we extract the\nsame number of email addresses from the Enron\nDatabase to create a test set, where the email ad-\ndresses do not appear in the training corpus. We\nfind that the attack success rate on this dataset de-\ncreases a lot, e.g., the accuracy of 0-shot (D)-[2.7B]\nis 0.19%, compared to 1.24% in Table 2. The re-\nsults mean that when the domain is unknown, many\nemail addresses recovered by the models are due\nto memorization/association; otherwise, the perfor-\nmance on these two datasets should be similar.\n6.2 The more knowledge, the more likely the\nattack will be successful\nFrom Tables 2 and 3, we notice that there is a huge\nperformance improvement when domain is known\nor more examples are provided. This is expected as\nmore examples make the model reinforce its learn-\ning of email address format/pattern and therefore\nachieve higher accuracy.\n6.3 The larger the model, the higher the risk\nFor all the settings, there is usually an improve-\nment in the accuracy when scaling the model. This\nphenomenon can be interpreted from two aspects:\n1) with more parameters, PLMs are able to memo-\nrize more training data. This is reflected mainly in\nTable 1, and also observed in Carlini et al. (2022).\n2) larger models are more sophisticated and able\nto better understand the crafted prompts, and there-\nfore to make more accurate predictions.\n2041\n6.4 PLMs are vulnerable yet relatively safe\nWhen domain is unknown (Table 2), very few email\naddresses are predicted correctly, mostly conform-\ning to the standard patterns in Table 4. An ex-\nception is 0-shot (D), the models do predict some-\nthing meaningful, e.g., abcd efg → efg3@xyz.com,\nthough the accuracy is still very low.\nWhen domain is known (Table 3), although\nPLMs can predict many email addresses correctly,\nthe performance is not better than the simple rule-\nbased method. In addition, most correctly predicted\nemail addresses conform to standard patterns. This\nis not particularly meaningful since attackers can\nalso simply guess them from the pattern.\nFor the context setting (Table 1), PLMs can make\nmore meaningful predictions. However, in practice,\nif the training data is private, attackers have no\naccess to acquire the contexts; if the training data\nis public, PLMs cannot improve the accessibility\nof the target email address since attackers still need\nto find (e.g., via search) the context of the target\nemail address from the corpus first in order to use\nit for prediction. However, if the attacker already\nfinds the context, he/she can simply get the email\naddress after the context without the help of PLMs.\n6.5 We still cannot ignore the privacy risks of\nPLMs\n• Long text patterns bring risks. From the results\nof 0-shot (D), if the training corpus contains long\ntext patterns that are helpful for attackers to ex-\ntract personal information, the models may pre-\ndict specific personal information meaningfully.\n• Attackers may use existing knowledge to ac-\nquire more information. As shown in §6.2,\nPLMs can leverage different kinds of knowledge\nto make more meaningful predictions; thus, at-\ntackers may be able to use existing knowledge to\ngain more information about owners from PLMs.\n• Larger and stronger models may be able to ex-\ntract much more personal information. As dis-\ncussed in §6.3, the larger the model, the more\npersonal information can be recovered. We can-\nnot guarantee that the success rate of the attack\nis still within an acceptable range as we continue\nto scale up language models.\n• Personal information may be accidentally\nleaked through memorization. From the results\nof the context setting, we find that 8.80% of email\naddresses can be recovered correctly with the\nlargest GPT-Neo model through memorization.\nThis means that the email addresses may still be\naccidentally generated, and the threat cannot be\nignored as discussed by Carlini et al. (2021).\n7 Mitigating Privacy Leakage\nNow that we have seen some potential risks of\nPLMs in terms of personal information leakage.\nHere we discuss several possible strategies to miti-\ngate these threats.\nFor training PLMs, we can mitigate privacy risks\nbefore, during, and after model training:\n• Pre-processing. 1) Identify and clear out or blur\nlong patterns that could pose potential risks, e.g.,\nthe pattern of 0-shot (D); 2) deduplicate training\ndata. According to Lee et al. (2022), dedupli-\ncation can substantially reduce memorized text;\ntherefore, less personal information will be mem-\norized by PLMs.\n• Training. As suggested in Carlini et al. (2021)\nand implemented in Anil et al. (2021), we can\ntrain the model with differentially private stochas-\ntic gradient descent (DP-SGD) algorithm (Abadi\net al., 2016) for DP guarantees (Dwork et al.,\n2006; Dwork, 2008).\n• Post-processing. For API-access models like\nGPT-3, include a module to examine whether the\noutput text contains sensitive information. If so,\nrefuse to answer or mask the information.\nFor information owners, taking email addresses\nas an example, we suggest as follows:\n• Do not disclose text form of personal information\ndirectly on the Web. For instance, use a picture\ninstead or rewrite the email address and provide\ninstructions for recovering the email address.\n• Avoid using email addresses with obvious pat-\nterns, since attacks on email addresses with a pat-\ntern have a much higher success rate than those\nwithout a pattern.\n8 Conclusion\nOur paper presents the first distinction between\nmemorization and association in pre-trained lan-\nguage models. The results show that PLMs do leak\npersonal information through memorization; how-\never, the risk of specific personal information being\nleaked by PLMs is low since they cannot associate\npersonal information with the owner meaningfully.\nWe suggest several defense techniques to mitigate\npotential threats and hope this study can give new\ninsights to help the community understand the risk\nof PLMs and make PLMs more trustworthy.\n2042\nLimitations\nIn this paper, we measure the risk of personal in-\nformation being leaked by PLMs. Since this paper\ninvolves personal information, we must be very\ncareful in dealing with the data to avoid privacy\nleakage, which brings some limitations to our re-\nsearch, e.g., the data we can use.\nWe choose email addresses for several reasons:\n1) email addresses are representative personal in-\nformation since emails have penetrated into our\nlives and are an indispensable medium for per-\nsonal/business communication; 2) email addresses\nhave a relatively fixed format that can be easily ex-\ntracted from the corpus (e.g., via regular expression\nmatching) and analyzed (e.g., calculating the accu-\nracy); 3) The Enron Email Dataset is a reasonable\nsource that can be used for our research without\nintroducing any additional privacy cost. Collecting\nother personal information such as phone numbers\nand home addresses may raise unnecessary privacy\nrisks, and the collected data is difficult to be made\npublic. Besides, this additionally requires the con-\nsent of the information owner under privacy laws\nand increases the cost of time and money6.\nWe believe the methods and findings in this pa-\nper can be generalized to other personal informa-\ntion and private data since the models are trained\nin a similar way. Importantly, our study can help\nresearchers distinguish the privacy risk caused by\nmemorization and association. For practical usage,\nwe recommend that researchers use our methods to\nevaluate the privacy risks of their trained models\n(possibly with their private data) before releasing\nthe models to others.\nEthics Statement\nThis work has ethical implications relevant to per-\nsonal privacy. The Privacy Act of 1974 (5 U.S.C.\n552a) protects personal information by preventing\nunauthorized disclosures of such information. As\nwe discussed in §1, the leakage of personal infor-\nmation like email addresses (whether or not it has\nbeen made public) will cause privacy issues such\nas email fraud and spam. This is also a reason why\nthe study in this paper is important.\nTo minimize ethical concerns and make the re-\nsults reproducible, we perform analysis on data\nand models that are already public. We also re-\nplace the real email address with consecutive char-\n6According to Wikipedia, the price of Enron Corpus is\n$10,000.\nacters such as abcd in the writing to protect privacy.\nWe believe that the benefits of this paper far out-\nweigh the potential harms. Although the results\nindicate that specific personal information being\nleaked by PLMs is low since PLMs are weak at\nassociation, we cannot underestimate the threats\nbrought by memorization and ignore the potential\nrisks of association. We still suggest researchers\ntake the privacy risks of PLMs seriously and adopt\nthe strategies as suggested in §7 to mitigate privacy\nleakage.\nAcknowledgements\nWe thank the reviewers for their constructive feed-\nback. This material is based upon work supported\nby the National Science Foundation IIS 16-19302\nand IIS 16-33755, Zhejiang University ZJU Re-\nsearch 083650, IBM-Illinois Center for Cognitive\nComputing Systems Research (C3SR) – a research\ncollaboration as part of the IBM Cognitive Horizon\nNetwork, grants from eBay and Microsoft Azure,\nUIUC OVCR CCIL Planning Grant 434S34, UIUC\nCSBS Small Grant 434C8U, and UIUC New Fron-\ntiers Initiative. Any opinions, findings, and conclu-\nsions or recommendations expressed in this publi-\ncation are those of the author(s) and do not neces-\nsarily reflect the views of the funding agencies.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert. arXiv preprint arXiv:2108.01624.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 7456–7463.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint arXiv:2202.05520.\n2043\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nCynthia Dwork. 2008. Differential privacy: A survey\nof results. In International conference on theory and\napplications of models of computation, pages 1–19.\nSpringer.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006. Calibrating noise to sensitivity\nin private data analysis. In Theory of cryptography\nconference, pages 265–284. Springer.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nShlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell,\nAlon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri\nStemmer, Ayelet Benjamini, Avinatan Hassidim, et al.\n2021. Learning and evaluating a differentially pri-\nvate pre-trained language model. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1178–1189.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-factr:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5943–5959.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nBryan Klimt and Yiming Yang. 2004. The enron corpus:\nA new dataset for email classification research. In\nProceedings of the 15th European Conference on\nMachine Learning, ECML’04, page 217–226, Berlin,\nHeidelberg. Springer-Verlag.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nDublin, Ireland. Association for Computational Lin-\nguistics.\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav Gold-\nberg, and Byron C Wallace. 2021. Does bert pre-\ntrained on clinical notes reveal sensitive data? In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 946–959.\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2022. Large language models can be\nstrong differentially private learners. In International\nConference on Learning Representations.\nXudong Pan, Mi Zhang, Shouling Ji, and Min Yang.\n2020. Privacy risks of general-purpose language\nmodels. In 2020 IEEE Symposium on Security and\nPrivacy (SP), pages 1314–1331. IEEE.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nWeiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou\nYu. 2021. Selective differential privacy for language\nmodeling. arXiv preprint arXiv:2108.12944.\n2044\nOm Dipakbhai Thakkar, Swaroop Ramaswamy, Rajiv\nMathews, and Francoise Beaufays. 2021. Under-\nstanding unintended memorization in language mod-\nels under federated learning. In Proceedings of the\nThird Workshop on Privacy in Natural Language Pro-\ncessing, pages 1–10.\nThomas Vakili and Hercules Dalianis. 2021. Are clini-\ncal bert models privacy preserving? the difficulty of\nextracting patient-condition associations.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs. arXiv\npreprint arXiv:2010.11967.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\net al. 2021. Differentially private fine-tuning of lan-\nguage models. arXiv preprint arXiv:2110.06500.\n2045\nA Rule-Based Method\nID name local part\nA1 abcd abcd\nB1 abcd efg abcd.efg\nB2 abcd efg abcd_efg\nB3 abcd efg abcdefg\nB4 abcd efg abcd\nB5 abcd efg edf\nB6 abcd efg aefg\nB7 abcd efg abcde\nB8 abcd efg eabcd\nB9 abcd efg efga\nB10 abcd efg ae\nC1 abcd hi efg abcd.efg\nC2 abcd hi efg abcd_efg\nC3 abcd hi efg abcdefg\nC4 abcd hi efg abcd.hi.efg\nC5 abcd hi efg abcd_hi_efg\nC6 abcd hi efg abcdhiefg\nC7 abcd hi efg abcd\nC8 abcd hi efg edf\nC9 abcd hi efg aefg\nC10 abcd hi efg abcde\nC11 abcd hi efg eabcd\nC12 abcd hi efg efga\nC13 abcd hi efg ahefg\nC14 abcd hi efg ahiefg\nC15 abcd hi efg abcd.h.efg\nC16 abcd hi efg abcd.hiefg\nC17 abcd hi efg ahe\nTable 4: The list of email address patterns.\nMany email addresses follow patterns of the\ncombination of the owners’ first name, last name,\nand initials (from our analysis, more than half of\nemail addresses in the dataset have significant pat-\nterns). For example, if the owner’s name is abcd,\nwith domain known as xyz.com, its email address is\nlikely to be abcd@xyz.com7; if the owner’s name\nis abcd efg, with domain known as xyz.com, its\nemail might be abcd.efg@xyz.com, aefg@xyz.com,\nabcd@xyz.com, etc.\nBased on this observation, for the settings where\nthe target domain is known, we design a rule-based\nmethod as a baseline. We identify 28 patterns clas-\nsified by the length of the owner’s name in Table\n4. And we use Z to denote email addresses that\ncannot be categorized into these 28 patterns.\nIn the zero-shot setting, we simply use pattern\nA1, B6, and C9 to recover the target email address,\ne.g., abcd efg → aefg@xyz.com. For the k-shot\nsetting, the algorithm first identifies the patterns in\nthe demonstrations, and uses the most frequent pat-\ntern to predict the local part, concatenated with the\nprovided domain. For example, assuming that we\nwant to predict the email address of a person with\na name of length 2, the patterns of the 5 sampled\ndemonstrations are {B3, B5, C2, B5, Z}. Among\nthe patterns, the compatible ones are {B3, B5, B5},\n7In the writing, we replace the real email address with\nconsecutive characters such as abcd to protect privacy.\nwith the most frequent one as B5. The model will\npredict the target email with pattern B5. If none\nof the email patterns is compatible with the target\nname, the model predicts the same email address\nas the zero-shot setting.\nB Effect of Decoding Algorithms\nsetting model # predicted # correct (# no pattern) accuracy (%)\nContext (100)\nGreedy\n[125M] 2528 28 (1) 0.86\n[1.3B] 2883 148 (17) 4.57\n[2.7B] 2983 246 (36) 7.60\nContext (100)\nTop-k\n[125M] 2678 22 (1) 0.68\n[1.3B] 2946 102 (10) 3.15\n[2.7B] 3010 171 (22) 5.28\nContext (100)\nBeam\n[125M] 2413 36 (1) 1.11\n[1.3B] 2728 171 (17) 5.28\n[2.7B] 2827 245 (35) 7.57\n0-shot (D)\nGreedy\n[125M] 3191 7 (0) 0.22\n[1.3B] 3232 16 (1) 0.49\n[2.7B] 3238 40 (4) 1.24\n0-shot (D)\nTop-k\n[125M] 3101 1 (0) 0.03\n[1.3B] 3226 5 (0) 0.15\n[2.7B] 3232 24 (2) 0.74\n0-shot (D)\nBeam\n[125M] 3151 5 (0) 0.15\n[1.3B] 3233 13 (1) 0.40\n[2.7B] 3232 47 (4) 1.45\nTable 5: Results of prediction with different decoding\nalgorithms.\nTo explore the effect of decoding algorithms\nin generation, we also report the results of top- k\nsampling (k = 50, temperature = 0.7) and beam\nsearch (num_beams = 5, with early stopping) in\nTable 5. From the results, we observe that the per-\nformance of top-k sampling is worse than that of\ngreedy decoding, and the performance of beam\nsearch and greedy decoding is close.\nC Effect of Frequency\nsetting mean median\nall 26 6\nContext (50) 125 29\nContext (100) 109 27.5\nContext (200) 108 30\n0-shot (D) 184 20.5\n0-shot (w/ domain) 40 9\n1-shot (w/ domain) 31 7\n2-shot (w/ domain) 28 7\n5-shot (w/ domain) 29 7\nTable 6: Mean and median of frequency of the correctly\npredicted email addresses in different settings. all refers\nto statistics of the entire dataset (3238 email addresses).\nIn Table 6, we report themean and median of fre-\nquency of the correctly predicted email addresses\nin different settings (with GPT-Neo 2.7B). We do\nnot include statistics of settings whose number of\ncorrect predictions is lower than 20 since the num-\nber is too small to analyze the mean and median.\n2046\nWe observe that the mean and median for those\ncorrectly predicted email addresses are higher than\nall the email addresses in the dataset (all), which\nindicates that more frequent email addresses are\nmore likely to be memorized and associated by\nPLMs. Similar findings that repeated strings are\nmemorized more were observed in Carlini et al.\n(2021, 2022); Lee et al. (2022).\n2047",
  "topic": "Personally identifiable information",
  "concepts": [
    {
      "name": "Personally identifiable information",
      "score": 0.7663173675537109
    },
    {
      "name": "Computer science",
      "score": 0.7417483925819397
    },
    {
      "name": "Memorization",
      "score": 0.5606130361557007
    },
    {
      "name": "Language model",
      "score": 0.5156739950180054
    },
    {
      "name": "Personal information management",
      "score": 0.4606386125087738
    },
    {
      "name": "Internet privacy",
      "score": 0.392605185508728
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3261628746986389
    },
    {
      "name": "Computer security",
      "score": 0.3078056871891022
    },
    {
      "name": "Information system",
      "score": 0.23356345295906067
    },
    {
      "name": "Psychology",
      "score": 0.15354081988334656
    },
    {
      "name": "Cognitive psychology",
      "score": 0.10194310545921326
    },
    {
      "name": "Engineering",
      "score": 0.08757933974266052
    },
    {
      "name": "Management information systems",
      "score": 0.08042079210281372
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}