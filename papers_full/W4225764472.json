{
  "title": "Performance of chemical structure string representations for chemical image recognition using transformers",
  "url": "https://openalex.org/W4225764472",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3092156181",
      "name": "Kohulan Rajan",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A2029328209",
      "name": "Christoph Steinbeck",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A210963128",
      "name": "Achim Zielesny",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092156181",
      "name": "Kohulan Rajan",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A2029328209",
      "name": "Christoph Steinbeck",
      "affiliations": [
        "Friedrich Schiller University Jena"
      ]
    },
    {
      "id": "https://openalex.org/A210963128",
      "name": "Achim Zielesny",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950128007",
    "https://openalex.org/W2518838044",
    "https://openalex.org/W1966456689",
    "https://openalex.org/W3092396754",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2895677720",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W3097598035",
    "https://openalex.org/W3203895579",
    "https://openalex.org/W3180840675",
    "https://openalex.org/W2963734039",
    "https://openalex.org/W3138074128",
    "https://openalex.org/W2558999090",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W2148797284",
    "https://openalex.org/W2055888446",
    "https://openalex.org/W1508604947",
    "https://openalex.org/W2146292423",
    "https://openalex.org/W3159789740",
    "https://openalex.org/W3127736057",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W4392468778"
  ],
  "abstract": "The most commonly used molecular string representations in deep learning for chemical image recognition.",
  "full_text": " Digital\n  Discovery\nrsc.li/digitaldiscovery\nVolume 1\nNumber 2\nApril 2022\nPages 73–174\nISSN 2635-098X\nPAPER\nKohulan Rajan et al.\nPerformance of chemical structure string representations for \nchemical image recognition using transformers\nPerformance of chemical structure string\nrepresentations for chemical image recognition\nusing transformers\nKohulan Rajan, a Christoph Steinbeck a and Achim Zielesny *b\nThe use of molecular string representations for deep learning in chemistry has been steadily increasing in\nrecent years. The complexity of existing string representations, and the diﬃculty in creating meaningful\ntokens from them, lead to the development of new string representations for chemical structures. In this\nstudy, the translation of chemical structure depictions in the form of bitmap images to corresponding\nmolecular string representations was examined. An analysis of the recently developed DeepSMILES and\nSELFIES representations in comparison with the most commonly used SMILES representation is\npresented where the ability to translate image features into string representations with transformer\nmodels was speciﬁcally tested. The SMILES representation exhibits the best overall performance whereas\nSELFIES guarantee valid chemical structures. DeepSMILES perform in between SMILES and SELFIES,\nInChIs are not appropriate for the learning task. All investigations were performed using publicly available\ndatasets and the code used to train and evaluate the models has been made available to the public.\nIntroduction\nDeep learning in chemistry is increasingly used to address\nproblems in chemistry and cheminformatics.1 One of these\nproblems is Optical Chemical Structure Recognition (OCSR),\nwhich aims to decode a 2D bitmap image of a chemical struc-\nture into a computer-readable le or string representation.\nOCSR techniques are necessary, for example, to extract chem-\nical structure information buried graphically in the chemical\nliterature and patents\n2 and store it in publicly available data-\nbases to enable their comprehensive retrieval with chemical\nstructure, substructure, or similarity searches. In a recent\nreview paper, we surveyed the available OCSR tools, most of\nwhich rely on rule-based approaches,\n3–5 and proposed deep\nlearning solutions as a promising alternative.6\nOCSR approaches with deep learning utilize complex neural\nnetworks that require appropriate representations of chemical\nstructures to encode and decode molecular information.\nCommonly, a 2D bitmap image of a chemical structure depic-\ntion is converted back into a textual representation– a character\nstring – of that same structure. The human-readable SMILES\n7\nrepresentation is one of the most widely used molecular string\nformats. But for deep learning purposes this line notation was\nshown to consist of several problems 8 which are primarily\ncaused by the tokenization of its character string. As an\nexample, structural branches are introduced with an opening\nbracket “(” and closed at a subsequent string position with\na closing bracket “)”. The same holds for ring openings and\nclosures which are marked by a number where a ring opens or\ncloses. However, once SMILES strings are partitioned into\ntokens based on characters, the precise placement of these\nmarkers at potentially distant positions within the text string\ncauses problems for many deep neural networks. Due to these\napparent ineﬃciencies new textual representations of chemical\nstructures like DeepSMILES\n8 and SELFIES9 have recently been\nFig. 1 SMILES, DeepSMILES, and SELFIES are divided into tokens\nwhich are separated with spaces.\naInstitute for Inorganic and Analytical Chemistry, Friedrich-Schiller-University Jena,\nLessingstr. 8, 07743 Jena, Germany\nbInstitute for Bioinformatics and Chemoinformatics, Westphalian University of Applied\nSciences, August-Schmidt-Ring 10, D-45665 Recklinghausen, Germany. E-mail: achim.\nzielesny@w-hs.de\nCite this:Digital Discovery, 2022,1,8 4\nReceived 17th September 2021\nAccepted 12th January 2022\nDOI: 10.1039/d1dd00013f\nrsc.li/digitaldiscovery\n84 | Digital Discovery, 2022,1,8 4–90 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\ndeveloped to overcome the sketched problems. The Deep-\nSMILES string representation aims at avoiding the problems\ndue to branches in SMILES by using closing brackets only for\nbranches where the number of brackets indicates the branch\nlength. For ring closures a single symbol at the ring-closure\nlocation is used instead of two symbols at the ring-opening\nand ring-closing locations. In contrast to SMILES and Deep-\nSMILES which must be partitioned into single character tokens,\nthe SELFIES representation denes separate enclosed tokens\nwithin square brackets“[.]” so that discrete meaningful tokens\nare provided by the representation itself (see Fig. 1).\nIn a recent OCSR study,\n10 we encountered similar problems\nwith SMILES representations which eventually led to a SELFIES\nbased implementation. By using SELFIES as the output repre-\nsentation, a predicted SELFIES string always converts into\na valid molecule due to the SELFIES decoding algorithm. In\ncontrast, predicted SMILES may be invalid due to syntax errors\nsuch as mismatched binding symbols, branching, or ring\nclosure. Other recent OCSR approaches\n11–14 that used SMILES\nstrings for output representation did not specically address\ntheir inherent problems.\nTo further support OCSR development this work reports\nndings of a comparative case study for chemical image to\nchemical structure translation with SMILES, DeepSMILES and\nSELFIES. In addition, InChIs are included as an output which\nwas proposed by a recent Kaggle competition.\n15\nMethods\nData\nIn this study, all data were taken from ChEMBL16 and Pub-\nChem17 databases. The data was originally downloaded in SDF\nformat. Using the Chemistry Development Kit (CDK) 18 the\nchemical structures were converted into SMILES strings with\nand without stereochemistry information. Aer the SMILES\nconversion, the DECIMER ltering rules\n10 were applied to\nobtain a balanced dataset. Then two datasets were created, one\ncontaining SMILES without stereochemistry and one with\nstereochemistry information.\nThe ltering rules for the datasets without stereochemistry\nincluded the following,\n/C15 have a molecular weight of fewer than 1500 Daltons,\n/C15 not possess counter ions,\n/C15 only contain the elements C, H, O, N, P, S, F, Cl, Br, I, Se\nand B,\n/C15 not contain isotopes of hydrogens (D, T),\n/C15 have 3–40 bonds,\n/C15 only contain implicit hydrogens, except in functional\ngroups,\n/C15 have less than 40 SMILES tokens,\n/C15 no stereochemistry was allowed.\nAer ltering, a total of 1 655 225 molecules were obtained\nfrom ChEMBL. Dataset partitioning into training and test\ndatasets is a challenging task: with a simple random parti-\ntioning, the test dataset may not cover the relevant chemical\nspace which could lead to biased results. To avoid this problem,\nthe RDKit\n19 MaxMin20 algorithm was applied, so that equally\ndiverse training and test subsets were created which cover\na similar chemical space.\nA set of 3 million molecules from PubChem was used to\ninvestigate whether the network performs better with more\ndata. Here, the dataset was twice as large as the ChEMBL\ndataset. The PubChem dataset wasltered using the same rules\nas above, and the RDKit MaxMin algorithm was again applied to\ncreate the test set.\nFor the datasets with stereochemistry, a total of 1 653 833\nmolecules were obtained from ChEMBL and 3 million mole-\ncules from PubChem. Again, the RDKit MaxMin algorithm was\nused to select diverse training and test subsets. Table 1 provides\nan overview of the datasets.\nThe dataset with stereochemistry obtained from ChEMBL\nwas a little smaller than the corresponding dataset without\nstereochemistry since stereochemistry adds new characters to\nSMILES, thereby lowering the number of available molecules\ndue to the applied ruleset. With PubChem, however, the dataset\nsize can be managed, since PubChem is much larger than\nChEMBL.\nTextual data\nThe generated molecule sets were then converted into diﬀerent\ntextual representations of the chemical structures: SMILES,\nDeepSMILES, SELFIES and InChIs\n21 and then split into tokens.\nFor SELFIES this was a straightforward process since they\nalready inherit a token-like word representation. Thus, SELFIES\nwere split into tokens by using a space between the squared\nbrackets “][ ”.\nFor splitting SMILES, DeepSMILES and InChIs into tokens\nanother set of rules had to be applied. They were split aer,\n/C15 every heavy atom,\n/C15 every open bracket and close bracket“(”,“)”,\n/C15 every bond symbol“¼”,”#”,\n/C15 every single-digit number and\n/C15 all the characters inside the squared brackets were retained\nas-is.\nTable 1 Overview of the datasets used in this study\nDatabase name ChEMBL PubChem\nDataset name Dataset 1 Dataset 2 Dataset 3 Dataset 4\nDataset description Without\nstereochemistry\nWith stereochemistry Without\nstereochemistry\nWith stereochemistry\nTrain dataset size 1 536 000 1 536 000 3 072 000 3 072 000\nTest dataset size 119 225 117 833 250 000 250 000\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 4–90 | 85\nPaper Digital Discovery\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nThe “InChI¼1S/” token was kept as one single token. As it is\ncommon in all InChIs, it was not used as a token during\ntraining but was later added to the predicted strings during\npost-processing to evaluate the results.\nIn addition to the token count, the maximum string length\nfound in the datasets was calculated. This refers to the length of\nthe longest string available in each dataset and plays a role\nduring training and testing. During training, the input vocab-\nulary size which the network can handle was determined by\ncomparing the number of tokens with the maximum length. In\ncases where the maximum length found in a dataset was\nsmaller than the number of tokens available in the dataset, the\ninput vocabulary size would be the number of tokens, other-\nwise, it would be the maximum length. During testing, the\nmaximum length was used to determine when to stop predict-\ning a structure if the end token is not met. Table 2 summarizes\nthe number of tokens and the maximum string length found in\neach dataset. Datasets with stereochemistry information\ncontain more tokens than datasets without. SELFIES represen-\ntation led to more tokens than SMILES or DeepSMILES repre-\nsentation. InChIs had the lowest number of tokens but the\nlargest maximum length of the longest string. With datasets 1\nand 2, it became clear that InChIs perform signicantly worse\nthan the other string representations, so they were omitted in\ntraining and testing datasets 3 and 4.\nImage data\nA production-quality bitmap image of each molecule was\ngenerated with the CDK Structure Diagram Generator (SDG) at\na resolution of 300/C2 300 pixels. Each molecule was rotated by\na random angle ranging from 0 to 360\n/C14 and depicted. The\ngenerated images were saved in 8 bit PNG format. Each image\ncontains a single structure only.\nThe features from these images were extracted as vectors by\nusing the pre-trained weights of the‘noisy student’22 trained\nEﬃcientNet-B3 (ref. 23) model. The extracted image features\nwere then saved into NumPy arrays.24 These topics were dis-\ncussed in detail in our previous publication.25\nThe extracted image features combined with the tokenized\ntextual data were then converted into TFRecords.26 TFRecords\nare binary records that can be used to train a model faster using\nCloud Tensor Processing Units (TPUs)\n27 on the Google Cloud\nPlatform (GCP).\nFor training purposes, each TFRecord contains 128 data\npoints consisting of 128 image feature vectors accompanied by\n128 tokenized string representations. The TFRecords were\ngenerated on an in-house server and then moved into a Google\nCloud Storage bucket.\nEach dataset contains the same image data but diﬀerent\nstring representations.\nNetwork, training and testing\nIn this work, we use the same network as in DECIMER Image-\nTransformer,\n25 a transformer-based network model similar to\nthe “Base model” as explained in Google's publication,Attention\nIs All You Need.28 This network uses four encoder–decoder layers\nand eight attention heads. Attention has a dimension size of\n512 and feed-forward networks have a dimension size of 2048.\nThe columns and rows here correspond to the image features\nwe extracted as vectors, which are 10/C2 10 /C2 1536. A dropout\nrate of 10% is used to prevent overtting. According to the\npublication “Attention Is All You Need” the network is trained\nusing the Adam optimizer with a custom learning rate sched-\nuler. The loss is calculated by using sparse categorical cross-\nentropy between the real and predicted SELFIES. The network\nwas coded with Python 3 using TensorFlow 2.3 (ref. 29) on the\nbackend.\nThroughout the training process, all models were trained on\nTPU v3-8 devices in the Google cloud. When comparing the\ntraining speed and network performance, a batch size of 1024\nwas found to be an adequate choice. The models were trained\nuntil the training loss had converged. In total, we trained eight\nmodels on datasets 1 and 2, and six models on datasets 3 and 4.\nOnce the models were fully converged, they were tested on an\nin-house server equipped with a GPU. To determine how many\nof the predictions were identical, the predictions were\ncompared to the original strings. Aer the identical prediction\ncalculations, all the predictions were converted to SMILES.\nAn analysis of the Tanimoto\n30 similarity index was conducted\nbetween the original and predicted SMILES using PubChem\nngerprints available in the CDK. The Tanimoto similarity\nindices help to understand how well the network was able to\nlearn chemical string representations since sometimes the\npredictions were not identical but only similar to the original\nstructures and even for isomorphic structures, there can be\nmany diﬀerent SMILES.\nTable 2 Overview of the token count and the maximum length\nDatabase\nname\nChEMBL PubChem\nDataset 1 (without\nstereochemistry)\nDataset 2 (with\nstereochemistry)\nDataset 3 (without\nstereochemistry)\nDataset 4 (with\nstereochemistry)\nNumber\nof tokens\nMaximum length\nof string\nNumber\nof tokens\nMaximum length\nof string\nNumber of\ntokens\nMaximum length of\nstring\nNumber\nof tokens\nMaximum length\nof string\nSMILES 52 81 104 81 73 87 125 83\nSELFIES 69 80 187 88 98 84 205 90\nDeepSMILES 76 93 127 101 97 93 148 96\nInChI 32 236 41 273 —— — —\n86\n| Digital Discovery, 2022,1,8 4–90 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nResults and discussion\nThe purpose of this study was to examine diﬀerent chemical\nstring representations that are available for deep learning in\nchemistry by their performance on chemical image to string\ntranslation using transformer networks. Predictions were valid\nif the images could be translated into structures correctly.\nAll the test results were assessed as following,\n/C15 Valid DeepSMILES/SELFIES/InChI: the predicted Deep-\nSMILES, SELFIES and InChIs that could decode back into\nSMILES strings. The rest were deemed invalid.\n/C15 Valid SMILES: predicted SMILES and decoded SMILES\nwhich could be parsed to calculate the Tanimoto similarity\ncalculations. The rest were classied as invalid SMILES.\n/C15 Identical predictions: this calculation identied how many\npredictions matched the original string representations. This\nwas accomplished by using a one-to-one character string match.\nIf a single character was wrong in the predicted string, it was\nconsidered as a wrong prediction.\n/C15 Average Tanimoto: the Tanimoto similarity between the\noriginal and predicted SMILES was calculated from the valid\nSMILES and the average Tanimoto similarity index was calcu-\nlated against the entire test dataset.\n/C15 Tanimoto 1.0 Percentage: the percentage of molecule pairs\n(original and predicted) with a Tanimoto similarity index of 1.0,\nwhich was calculated from the valid SMILES of the entire test\ndataset.\nResults for the ChEMBL dataset\nFrom ChEMBL two datasets were obtained to train and test, one\nwith stereochemistry (dataset 1) and one without\nstereochemistry (dataset 2). Table 3 summarizes the test results\nobtained with training on images from dataset 1.\nSMILES performed best in comparison to the other repre-\nsentations. Comparing the identical predictions and the Tani-\nmoto 1.0 count, SMILES based models were more accurate. This\ncould be due to fewer tokens in the SMILES language space.\nAdditionally, the maximum SMILES string was shorter than the\nrest. As a result, the model learns the representations better.\nEven though the InChIs have fewer tokens compared to the\nother representations, having a lesser number of tokens\nincreases the maximum length of each string compared to the\nother representations, which ultimately creates more errors for\nlearning and predicting. In addition, valid InChI predictions\nwere predominantly identical to the original string.\nEven though SELFIES has the most valid structures, the\noverall predictivity of the SELFIES-based model was lower than\nthat of SMILES and DeepSMILES. Overall, SMILES were simpler\nto learn– but for guaranteed valid structures, SELFIES were the\nbest option.\nTo estimate the impact of stereochemistry, the same proce-\ndure was repeated with dataset 2 where the models were trained\nfrom scratch. The results are summarized in Table 4.\nInclusion of stereochemistry information led to a lowered\naccuracy. For DeepSMILES and InChIs, the number of invalid\npredictions increased. Additionally, the fraction of invalid\nSMILES increased for all representations except InChIs. Aer\nparsing all InChIs, there were only valid SMILES.\nSMILES with stereochemistry reduced the overall predict-\nability and accuracy due to the new artefacts added to the\nimages. In addition, one should consider that the overall token\ncount in these datasets increased due to stereochemistry with\nadditional tokens being introduced.\nTable 4 Test results on dataset 2 (with stereochemistry)\nSMILES DeepSMILES SELFIES InChI\nTest dataset size 117 833 117 833 117 833 117 833\nInvalid DeepSMILES/SELFIES/InChI 0.00% 0.11% 0.00% 32.99%\nValid DeepSMILES/SELFIES/InChI 100.00% 99.89% 100.00% 67.01%\nInvalid SMILES 0.81% 0.64% 0.08% 0.00%\nValid SMILES 99.19% 99.25% 99.92% 67.01%\nIdentical predictions (string match) 78.16% 77.07% 66.59% 59.10%\nTanimoto 1.0 percentage (not identical) 85.02% 83.89% 72.07% 63.49%\nAverage Tanimoto 0.97 0.97 0.94 0.66\nTable 3 Test results on dataset 1 (without stereochemistry)\nSMILES DeepSMILES SELFIES InChI\nTest dataset size 119 225 119 225 119 225 119 225\nInvalid DeepSMILES/SELFIES/InChI 0.00% 0.07% 0.00% 30.79%\nValid DeepSMILES/SELFIES/InChI 100.00% 99.93% 100.00% 69.21%\nInvalid SMILES 0.35% 0.10% 0.00% 0.00%\nValid SMILES 99.65% 99.83% 100.00% 69.21%\nIdentical predictions (string match) 80.87% 78.67% 68.85% 64.28%\nTanimoto 1.0 percentage (not identical) 86.30% 84.11% 73.88% 65.53%\nAverage Tanimoto 0.97 0.97 0.95 0.69\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 4–90 | 87\nPaper Digital Discovery\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nSMILES were overall best to get the most accurate predic-\ntions. Since InChIs showed a signicantly inferior performance,\nit was decided to restrict further investigations to SMILES,\nDeepSMILES and SELFIES.\nResults for the PubChem dataset\nIn order to determine model improvement with increasing data\nsize, the training and test data were doubled by utilizing data\nfrom PubChem. As pointed out above, InChIs were omitted in\nsubsequent testing.\nThe number of molecules available in PubChem is currently\na 110 million. For this work, 3 million molecules for training\nand 250 000 molecules for testing were obtained by random\nselection: the resulting tokens were carefully compared to those\nin the ChEMBL dataset to ensure a similar token set. Using the\nPubChem derived datasets with (datasets 3) and without\nstereochemistry (datasets 4), the same training and testing\nprocedures were repeated, and the same evaluation procedure\nwas used as before. For the dataset without stereochemistry\n(dataset 3) the results are summarized in Table 5.\nBy comparison of Table 5 with Table 3, it can be concluded\nthat the data increase improved the model's performance in\ngeneral. Again, SMILES show the best accuracy on the test\nresults and SELFIES still retain 100% valid structures.\nDeepSMILES falls somewhere between these two. Although\nDeepSMILES has more valid structures than SMILES, when\nconsidering overall accuracy, the DeepSMILES format falls\nbehind: comparing DeepSMILES to SELFIES, DeepSMILES has\na better accuracy because of its SMILES like representation, but\nits overall number of valid structures lags behind SELFIES (see\nFig. 2).\nA summary of the results for dataset 4 with stereochemistry\ncan be found in Table 6.\nCompared to Table 4, the results in Table 6 showed that\nincreasing the dataset size also increased the overall accuracy.\nDatasets with stereochemistry did not perform as well as data-\nsets without. However, the overall accuracy did increase\ncompared to the datasets derived from ChEMBL. In addition, all\nof the SELFIES predictions which were decoded back into\nSMILES were valid, providing 100% valid structures in\ncomparison with Table 4. SMILES again performed best in\nterms of predictability and accuracy, see Fig. 3.\nTable 6 Test results on dataset 4 (with stereochemistry)\nSMILES DeepSMILES SELFIES\nTest dataset size 250 000 250 000 250 000\nInvalid DeepSMILES/SELFIES/InChI 0.00% 0.06% 0.00%\nValid DeepSMILES/SELFIES/InChI 100.00% 99.94% 100.00%\nInvalid SMILES 0.34% 0.05% 0.00%\nValid SMILES 99.66% 99.88% 100.00%\nIdentical predictions (string match) 85.80% 83.80% 79.73%\nTanimoto 1.0 percentage (not identical) 91.69% 90.60% 86.00%\nAverage Tanimoto 0.98 0.98 0.97\nTable 5 Test results on dataset 3 (without stereochemistry)\nSMILES DeepSMILES SELFIES\nTest dataset size 250 000 250 000 250 000\nInvalid DeepSMILES/SELFIES/InChI 0.00% 0.08% 0.00%\nValid DeepSMILES/SELFIES/InChI 100.00% 99.92% 100.00%\nInvalid SMILES 0.22% 0.08% 0.00%\nValid SMILES 99.78% 99.84% 100.00%\nIdentical predictions (string match) 88.62% 87.52% 82.96%\nTanimoto 1.0 percentage (not identical) 92.19% 91.08% 86.42%\nAverage Tanimoto 0.98 0.98 0.97\nFig. 2 Comparison of identical predictions, Tanimoto 1.0 count and\naverage Tanimoto of ChEMBL vs. PubChem datasets (without\nstereochemistry).\n88 | Digital Discovery, 2022,1,8 4–90 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nConclusion\nThe performance of diﬀerent textual chemical structure repre-\nsentations for the chemical image to structure translation using\ntransformers was investigated. The most accurate models were\nobtained by using the SMILES representation. Using SELFIES,\nhowever, we were able to produce models that led to predictions\nwith fewer invalid structures. DeepSMILES models always fell\nbetween SMILES and SELFIES. To ensure that the models\nimprove similarly with more data, the datasets were scaled up:\nthe results showed the same comparative performance. For\nmost accurate predictions, models should be trained using\nSMILES, for maximizing valid structures SELFIES should be\nused.\nThe valid structures generated aer decoding from SELFIES\nand DeepSMILES showed that the SELFIES decoding was\nsuperior to DeepSMILES decoding. SMILES and DeepSMILES\nshould always be used with a set of rules on how to split them\ninto meaningful tokens. SELFIES do not require this. There\nwere fewer tokens in DeepSMILES than in SELFIES because the\nrepresentation was similar to that in SMILES.\nSince SELFIES encoding is a promising endeavor under\nactive development, improved SELFIES variants could reach or\neven surpass the SMILES predictivity with the additional\nadvantage of a 100% structural validity.\nAbbreviations\nCDK Chemistry development kit\nDECIMER Deep learning for chemical image recognition\nGCP Google cloud platform\nGPU Graphical processing unit\nInChI International chemical identi er\nPCA Principal component analysis\nSDF Structure data le\nSDG Structure diagram generator\nSELFIES Self-referencing embedded strings\nSMILES Simpli ed molecular-input line-entry system\nTPU Tensor processing units\nFunding\nThe authors acknowledge funding by the Carl-Zeiss-\nFoundation. Open Access funding is enabled and organized\nby Project DEAL.\nData availability\n(1) The code for Performance of chemical structure string\nrepresentations for chemical image recognition using trans-\nformers can be found at https://github.com/Kohulan/\nDECIMER_Short_Communication. The version of the code\nemployed for this study is version 1.0.\n(2) Data and processing scripts for this paper, including in\nSMILES format are available at Zenodo at DOI: 10.5281/\nzenodo.5155037.\nAuthor contributions\nKR designed, developed the so ware, performed the data\nanalysis, and wrote the paper. CS and AZ conceived the project\nand supervised the work. All authors contributed to and\napproved the manuscript.\nConﬂicts of interest\nAZ is co-founder of GNWI – Gesellscha f¨ur natur-\nwissenschaliche Informatik mbH, Dortmund, Germany.\nAcknowledgements\nThe authors like to thank Google for free computing time on\ntheir TensorFlow Research Cloud infrastructure. The support of\nthe Open Access Publication Fund of the Westphalian Univer-\nsity of Applied Sciences is gratefully acknowledged.\nReferences\n1 A. C. Mater and M. L. Coote,J. Chem. Inf. Model., 2019, 59,\n2545–2559.\n2 I. V. Tetko, O. Engkvist and H. Chen,Future Med. Chem.,\n2016, 8, 1801–1806.\n3 I. V. Filippov and M. C. Nicklaus,J. Chem. Inf. Model., 2009,\n49, 740–743.\n4 T. Peryea, D. Katzel, T. Zhao, N. Southall and D.-T. Nguyen,\nAbstracts of Papers of The American Chemical Society, 2019,\np. 258.\n5 V. Smolov, F. Zentsev and M. Rybalkin,TREC, 2011.\nFig. 3 Comparison of identical predictions, Tanimoto 1.0 count and\naverage Tanimoto of ChEMBL vs. PubChem datasets (with\nstereochemistry).\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 4–90 | 89\nPaper Digital Discovery\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n6 K. Rajan, H. O. Brinkhaus, A. Zielesny and C. Steinbeck,J.\nCheminf., 2020,12, 60.\n7 D. Weininger,J. Chem. Inf. Model., 1988,28,3 1–36.\n8 N. O'Boyle and A. Dalke,ChemRxiv, 2018, DOI: 10.26434/\nchemrxiv.7097960.v1.\n9 M. Krenn, F. H¨ase, A. Nigam, P. Friederich and A. Aspuru-\nGuzik, Mach. Learn.: Sci. Technol., 2020,1, 045024.\n10 K. Rajan, A. Zielesny and C. Steinbeck,J. Cheminf., 2020,12,\n65.\n11 D.-A. Clevert, T. Le, R. Winter and F. Montanari,Chem. Sci.,\n2021, 12, 14174–14181.\n12 I. Khokhlov, L. Krasnov, M. Fedorov and S. Sosnin,\nChemRxiv, 2021, DOI: 10.26434/chemrxiv.14602716.v1.\n13 J. Staker, K. Marshall, R. Abel and C. M. McQuaw,J. Chem.\nInf. Model., 2019,59, 1017–1029.\n14 H. Weir, K. Thompson, A. Woodward, B. Choi, A. Braun and\nT. J. Mart´ınez, Chem. Sci., 2021,12, 10622–10633.\n15 Bristol-Myers Squibb – molecular translation , https://\nwww.kaggle.com/c/bms-molecular-translation.\n16 A. Gaulton, A. Hersey, M. Nowotka, A. P. Bento, J. Chambers,\nD. Mendez, P. Mutowo, F. Atkinson, L. J. Bellis, E. Cibri´an-\nUhalte, M. Davies, N. Dedman, A. Karlsson,\nM. P. Magari˜nos, J. P. Overington, G. Papadatos, I. Smit\nand A. R. Leach,Nucleic Acids Res., 2017,45, D945–D954.\n17 S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li,\nB. A. Shoemaker, P. A. Thiessen, B. Yu, L. Zaslavsky,\nJ. Zhang and E. E. Bolton, Nucleic Acids Res., 2019, 47,\nD1102–D1109.\n18 C. Steinbeck, Y. Han, S. Kuhn, O. Horlacher, E. Luttmann\nand E. Willighagen, J. Chem. Inf. Comput. Sci., 2003, 43,\n493–500.\n19 G. Landrum,\nRDKit: Open-Source Cheminformatics Soware,\nhttp://rdkit.org.\n20 M. Ashton, J. Barnard, F. Casset, M. Charlton, G. Downs,\nD. Gorse, J. Holliday, R. Lahana and P. Willett, Quant.\nStruct.-Act. Relat., 2002,21, 598–604.\n21 S. R. Heller, A. McNaught, I. Pletnev, S. Stein and\nD. Tchekhovskoi,J. Cheminf., 2015,7, 23.\n22 Q. Xie, M.-T. Luong, E. Hovy and Q. V. Le, 2020, arXiv pre-\nprint server, arxiv:1911.04252.\n23 M. Tan and Q. Le,presented in part at the Proceedings of the\n36th International Conference on Machine Learning,\nProceedings of Machine Learning Research, 2019.\n24 S. Van Der Walt, S. C. Colbert and G. Varoquaux,Comput. Sci.\nEng., 2011,13,2 2–30.\n25 K. Rajan, A. Zielesny and C. Steinbeck,J. Cheminf., 2021,13,\n61.\n26 TensorFLow, TFRecord and tf.train.Example , https://\nwww.tensorow.org/tutorials/load_data/tfrecord, accessed\nOctober 08, 2021.\n27 T. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon,\nC. Young, N. Jouppi and D. Patterson, IEEE Micro, 2021,\n41,5 6–63.\n28 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, 2017, arXiv pre-\nprint server, arxiv:1706.03762.\n29 M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard,\nY. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg,\nD. Mane, R. Monga, S. Moore, D. Murray, C. Olah,\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas,\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu\nand X. Zheng, 2016, arXiv pre-print server, arxiv:1603.04467.\n30 T. T. Tanimoto, An Elementary Mathematical Theory of\nClassication and Prediction , International Business\nMachines Corporation, 1958.\n90 | Digital Discovery, 2022,1,8 4–90 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 15 January 2022. Downloaded on 11/5/2025 6:18:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5348901152610779
    },
    {
      "name": "String (physics)",
      "score": 0.5168364644050598
    },
    {
      "name": "Computer science",
      "score": 0.49951648712158203
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48777443170547485
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4673405587673187
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41261982917785645
    },
    {
      "name": "Natural language processing",
      "score": 0.35695964097976685
    },
    {
      "name": "Physics",
      "score": 0.25273996591567993
    },
    {
      "name": "Theoretical physics",
      "score": 0.2009250819683075
    },
    {
      "name": "Quantum mechanics",
      "score": 0.07912224531173706
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}