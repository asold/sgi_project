{
  "title": "proScript: Partially Ordered Scripts Generation via Pre-trained Language Models",
  "url": "https://openalex.org/W3154984731",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101067919",
      "name": "Keisuke Sakaguchi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5044250030",
      "name": "Chandra Bhagavatula",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024879161",
      "name": "Ronan Le Bras",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5052882677",
      "name": "Niket Tandon",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081635691",
      "name": "Peter E. Clark",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102992157",
      "name": "Yejin Choi",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2164585080",
    "https://openalex.org/W3093733158",
    "https://openalex.org/W2145374219",
    "https://openalex.org/W2573974208",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2972780142",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2805486818",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W1965142824",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3126134783",
    "https://openalex.org/W2578412240",
    "https://openalex.org/W2593847145",
    "https://openalex.org/W2963134713",
    "https://openalex.org/W3021347125",
    "https://openalex.org/W2142269169",
    "https://openalex.org/W1702669762",
    "https://openalex.org/W2250506749",
    "https://openalex.org/W2252215150",
    "https://openalex.org/W3045911328",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963847835",
    "https://openalex.org/W1549997466",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3093166897",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3120365012",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2742108525"
  ],
  "abstract": "Scripts - standardized event sequences describing typical everyday activities - have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we demonstrate for the first time that pre-trained neural language models (LMs) can be be finetuned to generate high-quality scripts, at varying levels of granularity, for a wide range of everyday scenarios (e.g., bake a cake). To do this, we collected a large (6.4k), crowdsourced partially ordered scripts (named proScript), which is substantially larger than prior datasets, and developed models that generate scripts with combining language generation and structure prediction. We define two complementary tasks: (i) edge prediction: given a scenario and unordered events, organize the events into a valid (possibly partial-order) script, and (ii) script generation: given only a scenario, generate events and organize them into a (possibly partial-order) script. Our experiments show that our models perform well (e.g., F1=75.7 in task (i)), illustrating a new approach to overcoming previous barriers to script collection. We also show that there is still significant room for improvement toward human level performance. Together, our tasks, dataset, and models offer a new research direction for learning script knowledge.",
  "full_text": "proScript: Partially Ordered Scripts Generation\nvia Pre-trained Language Models\nKeisuke Sakaguchi†, Chandra Bhagavatula †, Ronan Le Bras †,\nNiket Tandon†, Peter Clark †, Yejin Choi †‡\n†Allen Institute for AI, ‡University of Washington\n{keisukes, chandrab, rolanlb, nikett, peterc, yejinc}@allenai.org\nAbstract\nScripts - standardized event sequences describ-\ning typical everyday activities - have been\nshown to help understand narratives by pro-\nviding expectations, resolving ambiguity, and\nﬁlling in unstated information. However, to\ndate they have proved hard to author or extract\nfrom text. In this work, we demonstrate for\nthe ﬁrst time that pre-trained neural language\nmodels (LMs) can be be ﬁnetuned to generate\nhigh-quality scripts, at varying levels of gran-\nularity, for a wide range of everyday scenar-\nios (e.g., bake a cake). To do this, we col-\nlected a large (6.4k), crowdsourced partially\nordered scripts (named proScript), which\nis substantially larger than prior datasets, and\ndeveloped models that generate scripts with\ncombining language generation and structure\nprediction. We deﬁne two complementary\ntasks: (i) edge prediction: given a scenario\nand unordered events, organize the events into\na valid (possibly partial-order) script, and (ii)\nscript generation: given only a scenario, gener-\nate events and organize them into a (possibly\npartial-order) script. Our experiments show\nthat our models perform well (e.g., F1=75.7 on\ntask (i)), illustrating a new approach to over-\ncoming previous barriers to script collection.\nWe also show that there is still signiﬁcant room\nfor improvement toward human level perfor-\nmance. Together, our tasks, dataset, and mod-\nels offer a new research direction for learning\nscript knowledge.\n1 Introduction\nScripts, originally introduced by Schank and Abel-\nson (1975), represent structured commonsense\nknowledge about prototypical events in everyday\nsituations/scenarios such as bake a cake and fuel a\ncar (Figure 1). However, while scripts have been\nshown to help understand narratives by providing\nexpectations, resolving ambiguity, and ﬁlling in un-\nstated information (Chambers and Jurafsky, 2008;\nﬁnd the cake recipe\ngather the ingredients\nturn on the oven\nmix the ingredients\nput the cake batter in the oven\nbake for the right amount of time\ntake the cake out of the oven\nScenario: bake a cake\nFigure 1: We collected 6.4k of partially ordered scripts\n(proScript) and developed models that take a sce-\nnario (e.g., bake a cake) as the input and generate a\n(possibly partial-order) script.\nModi et al., 2017, inter alia), they have proved hard\nto author or extract from text, with only small script\ndatabases available (Regneri et al., 2010; Cham-\nbers, 2017; Ostermann, 2020).\nIn this work, we show for the ﬁrst time that\npre-trained neural language models (LMs) can be\nadapted to generate high-quality scripts, including\nappropriately partial ordering events where a spe-\nciﬁc temporal ordering is required only when it is\nnecessary. LMs have previously been shown to\nsuccessfully generate stories (Rashkin et al., 2020),\nsummaries (Lewis et al., 2020), and commonsense\nfacts (Bosselut et al., 2019; Hwang et al., 2020).\nHere we investigate their application to script gen-\neration. First, we collect large amount (6.4k) of\npartially ordered script from crowdsourcing with\na similar but simpliﬁed collection method (Ciosici\net al., 2021). We call the dataset as proScript\n(PaRtial Order SCRIPt for generaTion), and this\nis substantially larger than prior (crowdsourced)\ndataset such as DeScript (Regneri et al., 2010) that\narXiv:2104.08251v1  [cs.CL]  16 Apr 2021\nhas 40 scripts. Since the granularity of scripts\n(and the events) are inherently vague and subjec-\ntive (Modi et al., 2016), we collected wider variety\nof micro and macroscopic scripts than previous\ndatasets. Additionally, temporal duration of each\nevent is also annotated (e.g., take the cake out of\nthe oven typically takes one minute in the bake\na cake script), which will potentially link script\nknowledge with temporal reasoning in future work.\nSecond, with the collected data, we introduce\ntwo complementary tasks: script edge prediction\nand entire script generation. In the edge predic-\ntion task, given a scenario and unordered interme-\ndiate events, models must organize the events as\na valid partial-order script. On the other hand, the\nscript generation task is to generate intermediate\nevents and the partial-order of those events for a\ngiven scenario. This task requires both natural lan-\nguage generation (for nodes) and graph structure\nprediction (for edges).\nFinally, based on our proposed dataset, we de-\nvelop models for both edge prediction and entire\nscript generation tasks. As Chambers (2017) has\nrevealed that models trained and evaluated on miss-\ning events prediction (i.e., narrative cloze) are in-\nsufﬁcient to assess script knowledge, our evalua-\ntion scheme evaluate the entire script. We compare\nthe models against baselines, and show that our\nmodels outperform the baselines for both the edge\nprediction and the script generation tasks. Nonethe-\nless, there is a signiﬁcant room for improvement\ntoward human-level performance – e.g., for edge\nprediction, the best model achieves 75.71 of F1\nscore while human achieves 89.28, and for script\ngeneration, the best model obtains a graph edit dis-\ntance of 4.97 (i.e., number of human edits), while\nhuman-created scripts achieve 2.98 on average.\nOur contributions are thus:\n• A new dataset ( proScript) of crowd-\nsourced scripts that is substantially larger than\nprior (manually crafted) datasets\n• Two complementary task deﬁnitions against\nproScript\n• Two new models for these task, providing the\nﬁrst demonstration that generative models can\nbe successfully applied, although it is still sig-\nniﬁcantly below human levels\n2 Related Work\nScript as narrative chain Mooney and DeJong\n(1985) and Chambers and Jurafsky (2008, in-\nter alia) have investigated automatically inducing\nscripts from (unstructured) corpus. In particular,\nChambers and Jurafsky (2008) introduced scripts\nas narrative chain, where verbs with the partici-\npants information (e.g., ( claimed, subj), and (ac-\ncused, obj) ) named narrative events are partially\nordered according to causal and temporal relations.\nThey also introduced narrative cloze task, where a\nmodel is expected to predict one removed narrative\nevent, given all the other narrative events, while\nour proposed task requires to generate scripts as\na partial-order graph for a given scenario. The\n“script as narrative chain” approach has been ac-\ntively studied (Jans et al., 2012; Modi and Titov,\n2014; Pichotta and Mooney, 2014; Rudinger et al.,\n2015; Granroth-Wilding and Clark, 2016; Weber\net al., 2018; Belyy and Van Durme, 2020), but\nit has its drawbacks. First, the source corpora is\nmainly from a news domain rather than everyday\nscenarios, and a number of verbs in news texts\nare not script-relevant events such as reporting\nverbs (Mostafazadeh et al., 2016; Chambers, 2017).\nSecond, events are highly abstracted as tuples of\nverb and the dependency (subj or obj) (Ostermann,\n2020). Third, the evaluation scheme for the nar-\nrative cloze task is insufﬁcient to evaluate script\nknowledge (Chambers, 2017).\nScript as paraphrase sets Script as paraphrase\nsets (Regneri et al., 2010; Modi et al., 2016; Wan-\nzare et al., 2016) is more recent approach to gather\nscript knowledge, where crowd workers are asked\nto write down a sequence of events for a given ev-\neryday scenario (e.g., bake a cake) and the collected\nsequences (called event sequence description) are\naligned with paraphrased events being clustered.\nThe collected (partially ordered) scripts cover wide\nvariety of everyday situations compared to narra-\ntive chains (news domain), but one shortcoming\nof this approach is the scalability; it is not easy\nto scale because of the cost for manual data col-\nlection (Chambers, 2017; Ostermann, 2020). In\nfact, Modi et al. (2016) crowdsourced 1000 sto-\nries that cover only 10 scripts, and similarly Reg-\nneri et al. (2010) end up with collecting 40 scripts.\nThe limited amount of data hinders learning script\nknowledge by models. Furthermore, they provide\nno evaluation metric on the dataset for assessing\nmodel’s script knowledge.\nStory generation Neural models have been\ndemonstrated to successfully generate stories (Kid-\ndon et al., 2016; Peng et al., 2018; Zhai et al.,\n2019; Rashkin et al., 2020). Our work is related to\nstory generation in terms of generating higher-level\nagenda (or plot) of a story. However, a main differ-\nence between stories and scripts is that stories often\nrequire surprising and incidentalsequence of events\nas well as description about character’s mental\nstates and landscape depiction that make the story\nattractive for readers, whereas our script generation\nexpects generating essentialcore events(Chambers,\n2017) in partial order.\n3 Deﬁnitions\nproScript We deﬁne proScript as a di-\nrected acyclic graph (DAG), G(V, E) with a given\nscenario (s), where V is a set of essential events\n{v1, ...vi, ...v|V |}and E is a set of temporal or-\ndering constraints between events {eij}which\nmeans that the events vi must precede the event\nvj (vi ≺vj).1 DAGs effectively encode the partial-\nordering of core events–crucial for representing\nevents which can be performed in any order. For\nexample, in a bake a cake scenario, one can “gather\nthe ingredients” and “turn on the oven” in any order\n(Figure 1). We emphasize that scripts should not\ninclude non-core events such as discourse related\nevents (e.g., reporting verbs) as Chambers (2017)\nproposed. In proScript, we also exclude alter-\nnative events in a proScript DAG. For exam-\nple, in a bake a cake scenario, “get ingredients”\nand “buy ingredients” are alternative events with\neach other because either one is only necessary in\nthe scenario. By excluding alternative events, we\ncan resolve ambiguity of the edges in partial order\nstructure as temporal relations or alternative paths.\nRegneri et al. (2010) and Modi et al. (2016) do not\ndiscriminate this ambiguity.2\nWith the deﬁnition, we introduce proScript\ntask in two complementary settings: script edge\nprediction and entire script generation.\nEdge Prediction The script edge prediction task\nis to predict a set of partial-ordered edges ( E) of\nthe script G(V, E), given a scenario and a set of\nunordered intermediate events v ∈V .\n1Technically, proScript is a transitive reduction of a\nDAG. In short, transitive reduction of G does not have any\nshort cut edges between nodes. In proScript, we add a\nsingle root node (vr) and scenario (s) as a unique leaf node.\n2In proScript, we focus on events and the (partial-)\nordering, and we leave participants/arguments identiﬁcation\nfor future work.\nScript Generation The script generation task is\nto predict a partial order script G(V, E), but only\nthe scenario is given. Models are additionally ex-\npected to generate events (V ) in natural language.\n4 Datasets\nSource of Scenarios We collected scenarios\nfrom ROCStories (Mostafazadeh et al., 2016),\nDeScript (Wanzare et al., 2016), and Virtual-\nHome (Puig et al., 2018). As ROCStories con-\nsists of sentences instead of scenarios, we extract\nphrases that match the manually curated patterns\n“want(ed) to ... ”, “need(ed) to ... ”, “look(ing) to\n... ”and that do not include personal pronouns or\nperson’s name. The 2,565 scenarios we collected\ninclude both high-level long-term ones (e.g., open\na small business) and ﬁne-grained short-term ones\n(e.g., sign into an email account). DeScript con-\nsists of 40 daily scenarios (e.g., making coffee) and\nwe use all of them. VirtualHome is constructed to\nlearn activities interactively in a household in a 3D\nsimulated world. It has 233 indoor tasks (e.g., turn\non light) and we include them as scenarios.\nCrowdsourcing proScript For the collected\nscenarios, we crowdsource the corresponding\nproScript on the Amazon Mechanical Turk.\nOur crowdsourcing procedure is similar but simpli-\nﬁed method to (Ciosici et al., 2021). First, crowd-\nworkers are required to describe ﬁve to seven core\nevents that they are essential for the given sce-\nnario (Chambers, 2017) with the estimated time\nit takes to complete each event. In the second ques-\ntion, they are asked to sort them in possibly partial\norder (=DAG), which represents the proScript\nfor the scenario.\nDue to the complex nature of this crowdsourc-\ning procedure, it is crucial to maintain the quality.\nTo identify and ﬁlter out noisy instances, two dif-\nferent workers are asked to sort the same set of\nevents in partial order (i.e., the same as the second\nquestion described above). According to our man-\nual analysis, we decided to retain scripts that have\nat least 65.0 F1 score between the workers. 3 To\ncollect proScript with both micro and macro-\nscopic scenarios, we iteratively picked two adjacent\nevents in the DAGs and use them as a source of\nﬁner-grained scenarios.\n3In our crowdsourcing tasks, we maintained a pay rate of\n12$/hr or higher. For example, crowd workers were paid $0.8\nfor the script creation and $0.4 for the validation.\nbuy some new \nclothes (1 hour)\ngo to bathroom (5 mins)\nsign into email \naccount (1 min)\nreplace a closet door (1 day)\nfind a new job\n(1 month)\nopen a small business (1 year)\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nln m (m=minutes)\n0.00\n0.05\n0.10\n0.15\nNormalized Density\nFigure 2: Normalized histogram of time duration in\nproScript dataset. We see the dataset contains\nscripts with various time granularity.\nDataset Statistics In total, we collected 6,414\nvalid scripts that include 311,502 pairs of events,\nand we split the proScript into training (3,252\nscenarios), development (1,085), and test set\n(2,077). The training and development sets con-\nsist of scenarios collected from ROCStories, and\nthe test set consists of those from ROCStories, De-\nScript, and VirtualHome. This helps us evaluate in-\nand out-of-domain performance.\nThe average number of events in proScript\nscenarios is 5.45 and the maximum degrees of\nDAGs in the training set are distributed as follows:\n2,198 scripts (67.6%) for degree 1, 915 scripts\n(28.1%) for degree 2, 108 scripts (3.3%) for de-\ngree 3, 31 scripts (0.9%) for degree 4 and above.\nFigure 2 shows the normalized histogram of the\ntypical time to take for each script in proScript\ndataset. Most of the scripts take between a minute\nand an hour (e.g., “go to bathroom”, “buy some\nnew clothes”), while there are a reasonable amount\nof high-level long-term scripts (e.g., “ﬁnd a new\njob”, “open a small business”).\n5 proScript Edge Prediction\n5.1 Models\nFor the proScript edge prediction task (§3), we\nimplement a two-step approach baseline (pairwise\nmodel) and compare it with our proposed end-to-\nend neural method (proScriptedge-pred).\nPairwise Model We implement a two-step base-\nline where we train a binary classiﬁer to predict the\nprecedence between pairs of events, followed by\nbuilding a partial order script ˆG by aggregating the\npredicted relations across all pairs of events.\nFormally, the classiﬁer takes a pair of events (vi,\nvj) and predicts the precedence ˆeij – i.e. the event\nInput: unordered events\nand scenario (bake a cake)\nStep0: turn on the oven;\nStep1: bake for the right \n           amount of time;\nStep2: mix the ingredients;\nStep3: find the cake recipe;\nStep4: gather the ingredients;\nStep5: put the cake batter \n           in the oven;\nStep6: take the cake out of \n           the oven;\nStep3 --> Step4; \nStep3 --> Step5;\nStep4 --> Step2; \nStep2 --> Step4;\nStep4 --> Step0; \nStep0 --> Step3;\nStep3 --> Step6;\nOutput: Edges\n(in DOT language)\n=\n3\n4\n0\n5\n2\n1\n6\nG\nFigure 3: Example of input and output for the\nproScriptedge-pred model. The input is a ﬂattened se-\nquence of events, and the output is a ﬂattened sequence\nof edges of the (predicted) partial-order script.\nvi precedes (≺) vj .\nˆeij = p(vi ≺vj|vi, vj) (1)\nScores by the classiﬁer are used as weights to\ncreate an adjacency matrix of G which is then auto-\nmatically converted into a partial-order script with\nheuristics – when G contains a cycle, we iteratively\nremove edges by choosing the one with minimum\nweight until we get a valid DAG.\nproScriptedge-pred We propose an an end-to-\nend neural model, which takes all the (unordered)\nevents (v) and the scenario (s) as the input (x) and\npredicts the edges ( ˆE) in a partial-order script ( ˆG)\nat one time. To represent ˆE in a linear format (y),\nwe use DOT, a graph description language as shown\nin Figure 3. 4 By ﬂattening the nodes and edges\nof G (and ˆG), we apply neural encoder-decoder\nmodels. Formally, ﬂattened unordered events and\nscenario as x are embedded as continuous represen-\ntation (emb(x)) by the encoder, then the decoder\nwill generate tokens (y) as follows:\np(y1, . . . , yN |x1, . . . , xM ) = (2)\nN∏\nn=1\np(yn|emb(x1, . . . , xM ), y1, . . . , yn−1).\nCompared to the pairwise model, the\nproScriptedge-pred model uses information\nfrom all the events jointly to build partial-order\nscript with a broader context.\n5.2 Evaluation Metrics\nGiven G(V, E) as a predicted (partial order) script\nand ˆG(V, ˆE) as the correct (oracle) script, the F1\n4Madaan and Yang (2020) have previously shown that\nﬁnetuned LMs can generate valid DOT language.\ndev test (all) test (in domain) test (out domain)\nModels F1 P R F1 P R F1 P R F1 P R\nrandom 21.30 21.08 21.72 21.03 21.00 21.26 20.57 20.52 20.84 21.32 21.27 21.58\nPairwise (RoB) 65.75 67.05 64.71 61.29 62.85 60.06 63.25 64.97 61.89 59.06 60.44 57.98\nPairwise (T5) 70.96 71.93 69.76 67.64 69.44 66.18 69.50 71.41 67.96 65.51 67.20 64.16\nproScr(11B-100) 56.05 56.58 55.75 52.26 52.91 51.89 54.98 55.67 54.59 49.16 49.76 48.83\nproScr(11B-1k) 65.98 66.49 65.71 60.55 61.24 60.15 64.64 65.40 64.20 55.89 56.51 55.54\nproScr(Large) 66.25 66.89 65.83 63.64 64.22 63.27 65.76 66.38 65.35 61.23 61.76 60.91\nproScr(11B-all) 78.20 78.48 78.14 75.71 75.93 75.72 77.75 78.03 77.71 73.37 73.54 73.46\nHuman 89.32 89.60 89.21 89.28 89.91 88.86 90.04 90.54 89.74 88.71 89.44 88.18\nTable 1: Results for proScript edge prediction task. In this table, proScript refers to proScriptedge-pred.\nscore is deﬁned as follows:\nPrecision = |E& ˆE|\n|ˆE|\n, Recall = |E& ˆE|\n|E|\nF1 score = 2 ∗Precision ∗Recall\nPrecision + Recall .\n5.3 Experiments\nSetup For the binary classiﬁer (pairwise model),\nwe use two variants of the Transformer (Vaswani\net al., 2017): RoBERTa-large (Liu et al., 2019) and\nT5-11B (Raffel et al., 2020).\nWhen training (i.e., ﬁne-tuning) RoBERTa, 5\nwe use a grid-search for choosing the best hyper-\nparameters from the best performed model on the\ndevelopment set: epochs {1, 2, 3 }, learning rate\n{1e-5, 1e-6, 1e-7 }, batch size {16, 24, 32 }. For\ntraining the T5 model as the pairwise model, we\nfollowed a default set of hyper-parameters that are\nrecommended in Raffel et al. (2020).\nFor the proScriptedge-pred model, we use the\nT5 with different model sizes (Large and 11B) and\ntraining sizes (100, 1k, and all 3.2k) to see how\nthese factors affect the performance.6 We followed\na default set of hyper-parameters for the T5 models.\nResults The results are shown in Table 1. We\nﬁnd that the pairwise and proScriptedge-pred\nmodels signiﬁcantly outperform the random base-\nline where the edges are randomly assigned. The\nproScriptedge-pred T5-11B model outperforms\nthe pairwise T5-11B model. This indicates that\nthe proScriptedge-pred model beneﬁts from a\nlarger context from the input to predict edges\nmore accurately, although there is still a signiﬁ-\ncant room for improvement toward human-level\nperformance.7 Regarding the difference between\n5We used the implementation from Huggingface Trans-\nformers (Wolf et al., 2019).\n6We also used BART (Lewis et al., 2020), but we found\nthat BART did not perform well on this task.\n7We ﬁnd that 99% of the outputs fromproScriptedge-pred\nare valid DOT language.\n1 2 3+\ndegree\n0\n20\n40\n60\n80\n100F1 score\nproScript Pairwise Human\nFigure 4: Model performance by pairwise (T5-11B),\nproScriptedge-pred (T5-11B) and human according to\nthe maximum degree of the script (DAG).\nin and out of domain, we ﬁnd that the in-domain\nperformance is higher than the out-of-domain per-\nformance, whereas human performance is robust\nregardless of the domain difference. We also see\nthat the training set (100, 1k, all) and model sizes\n(Large, 11B) signiﬁcantly affect the performance\nof proScriptedge-pred.\nFigure 4 shows the performance of the pair-\nwise (T5-11B) model, proScriptedge-pred (T5-\n11B) and human according to the (maximum) de-\ngree of the script DAGs. We ﬁnd that scripts\nwith higher degree are more difﬁcult to predict\nfor both proScriptedge-pred and pairwise mod-\nels, whereas human shows smaller decrease for\npredicting higher-degree scripts.\n6 proScript Generation\n6.1 Models\nproScriptgen The proScript generation\ntask combines natural language generation (i.e.\ngenerating events in natural language) with struc-\nture prediction over the generated events (i.e. or-\nganizing the events into a DAG). Our approach\n(proScriptgen) is to formulate it as an end-to-\nend problem, similar to the proScriptedge-pred\nInput: scenario and # of events\n formatted as natural language question\nYou want to bake a cake. \nHow can you do this in 7 steps?\nOutput: events and edges\n(in DOT language)\n=\n0\n1\n3\n4\n2\n5\n6\nG\nStep0: find the cake recipe;\nStep1: gather the ingredients;\nStep2: mix the ingredients;\n… omitted …\nStep5: bake for the right \n           amount of time;\nStep6: take the cake out of \n           the oven;\nStep0 --> Step1; \nStep0 --> Step3;\n… omitted …\nStep5 --> Step6;\nFigure 5: Example of input and output for\nproScriptgen. The input is a scenario and number\nof events to generate in natural text format, and the out-\nput is a sequence of events and edges of the script.\nfor the proScript edge prediction task ( §5.1).\nGiven a scenario (s) and the number of events to\ngenerate in the script, proScriptgen generates\nevents and edges for the partial-order script ( ˆG)\nin DOT language (Figure 5). Formally, we use\nthe same encoder-decoder framework (eq.2) except\nthat a scenario and number of steps to generate are\ndescribed in natural text as x and the decoder is\nexpected to generate events as well as the edges\n(as y) in the script.\nTransfer learning from WikiHow data Trans-\nfer learning often helps improve the performance\nwhen it is (pre-)trained on a similar task (Peters\net al., 2018; Devlin et al., 2019). As additional\nresource for pre-training proScriptgen, we use\nprocedural texts extracted from WikiHow,8 which\ncontains 130k instances of a sequence of essential\nsteps for a given topic in various categories (e.g.,\nhealth, ﬁnance, hobbies, etc.). It is important to\nnote that all the procedures in WikiHow are for-\nmatted as sequences rather than a partial-order. We\nrefer to this approach as proScriptgen-transfer.\nPipeline approach An alternative approach\nis to use proScriptgen followed by the\nproScriptedge-pred model. The approach re-\nlies on proScriptgen to generate a set of\nevents but allows to ﬁx the predicted edges via\nthe proScriptedge-pred model. We refer to\nthis approach as proScriptgen-pipe, and study\nwhether it can improve the performance over\nproScriptgen.\n6.2 Evaluation Metrics\nChambers (2017) emphasizes the importance of\nhuman annotation for evaluating script knowledge.\n8https://www.wikihow.com/\nHowever, human evaluation for the proScript\ngeneration task is challenging because it involves\nnatural text generation and structured prediction.\nAs in the text generation tasks such as machine\ntranslation and text summarization, there are sev-\neral possible correct answers. Therefore, we use\ntwo complementary evaluation metrics for the\nproScript generation task: (i) graph edit dis-\ntance, and (ii) pairwise comparison. These are the\nabsolute and relative measures of performance, re-\nspectively. Graph edit distance (Abu-Aisheh et al.,\n2015) computes the distance between two graphs.\nFormally, given two graphsG1 and G2,\nGED(G1, G2) = min\nG1\nd1,...,dk\n−−−−−→G2\nk∑\ni=1\ncost(di)\nwhere d1, . . . , dk is a list of graph edit operations\nfrom G1 to G2. The operations include deletion, in-\nsertion, and replacement for vertex and edge. Each\noperation has its cost and we set the cost to be 1 for\nall the operations in our evaluation for simplicity.\nWe use the graph edit distance between a model-\ngenerated script and the revised script by human\nannotators. The graph edit distance is indicative of\nthe quality of the generated scripts; higher-quality\nscripts must have smaller graph edit distances to\nthe gold-standard (i.e. they require a smaller num-\nber of human revisions). In addition, we also em-\nploy pairwise human judgments where we ask hu-\nman annotators to compare the scripts generated\nby proScriptgen with those from the other ap-\nproaches.\n6.3 Experiments\nSetup For our proScriptgen, we use T5-11B.\nSimilarly to the proScriptedge-pred, we follow\nthe default set of hyper-parameters recommended\nin (Raffel et al., 2020). For proScriptgen-transfer,\nwe pre-train the proScriptgen with the 130k\nprocedures, and ﬁnetune it on the proScript\ndataset. For the proScriptgen-pipe, we ﬁrst ob-\ntain the actions generated by proScriptgen (ig-\nnoring the edges), and use the set of events as in-\nput for proScriptedge-pred, which is trained (see\n§5.3) to predict the edges.\nAs deﬁned in §3, we use graph edit distance and\npairwise judgments to evaluate the quality of the\ngenerated scripts. For computing graph edit dis-\ntances, we select 500 scripts (250 for dev and test\nsets) and ask crowdworkers to revise the generated\nscripts as necessary (e.g., add/delete/replace the\nSplit Models Edit Dist V-Del V-Ins V-Rep E-Del E-Ins E-Rep\nproScriptgen 4.73 0.426 0.192 0.581 1.558 1.308 0.671\ndev proScriptgen-transfer 4.79 0.337 0.195 0.679 1.491 1.281 0.775\nproScriptgen-pipe 4.88 0.397 0.159 0.560 1.705 1.407 0.661\nHuman 2.78 0.155 0.161 0.144 1.123 1.011 0.199\nproScriptgen 4.97 0.581 0.142 0.656 1.668 1.184 0.709\ntest proScriptgen-transfer 5.38 0.438 0.213 0.775 1.713 1.402 0.835\nproScriptgen-pipe 5.41 0.594 0.143 0.671 1.880 1.292 0.787\nHuman 2.98 0.168 0.149 0.130 1.276 1.074 0.189\nproScriptgen 4.57 0.513 0.158 0.633 1.471 1.108 0.687\ntest (in domain) proScriptgen-transfer 5.03 0.339 0.299 0.649 1.575 1.496 0.677\nproScriptgen-pipe 5.10 0.561 0.147 0.630 1.765 1.217 0.744\nHuman 3.03 0.168 0.211 0.154 1.223 1.091 0.206\nproScriptgen 5.43 0.659 0.124 0.681 1.894 1.270 0.735\ntest (out domain) proScriptgen-transfer 5.76 0.549 0.115 0.916 1.867 1.296 1.013\nproScriptgen-pipe 5.81 0.659 0.116 0.795 1.961 1.267 0.941\nHuman 2.91 0.170 0.074 0.102 1.340 1.054 0.170\nTable 2: Results for proScript generation task (dev, test, in-domain test and out-of-domain test set). We\nmeasure the average graph edit distance between generated script and the two human revisions (lower the better).\nWe also show the average number of each graph edit operation ( {Delete, Insert, Replace }×{ Vertex, Edge}).\nRandom (edge) baseline shows 11.06 edit distance for the dev set and 10.95 for the test set.\nPipeline-dev\nTransfer-dev\nHuman-dev\nPipeline-test\nTransfer-test\nHuman-test\n20.3\n30.6\n55.3\n25.2\n28.7\n54.0\n56.6\n45.6\n22.7\n56.6\n44.4\n32.7\n23.1\n23.8\n22.0\n18.2\n26.9\n13.3\nbetter than proScript equal worse\nFigure 6: Pairwise judgments (%) between\nproScriptgen and the other approaches.\nevents and the edges).We use the revised scripts as\ngold-standard. Each script is revised by two anno-\ntators, and we compute the average of the graph\nedit distances.\nIn pairwise judgments, we compare the scripts\ngenerated by proScriptgen with those from the\nother approaches. We randomly select 150 pairs,\nand ask three crowdworkers to judge whether\nthe script generated by proScriptgen is better,\nworse, or equal to the other (i.e. transfer, pipeline,\nor human). We use majority vote to decide the ﬁnal\npairwise human judgment between the two scripts.\nResults The pairwise judgment result is shown\nin Figure 6. We see that the pipeline and\ntransfer models show slight preference over the\nproScriptgen (except pipeline-dev), although\nthe difference is not large. We also see that\nthe transfer model constantly have more prefer-\nence over the proScriptgen than the pipeline\n0 1-3 4-6 7-9 10+0\n60\n120\n180 proScript\n0 1-3 4-6 7-9 10+0\n60\n120\n180 Pipeline\n0 1-3 4-6 7-9 10+0\n60\n120\n180 Transfer\n0 1-3 4-6 7-9 10+0\n60\n120\n180 Human\nFigure 7: Histograms of graph edit distance (in dev\nset). The number of scripts (y-axis) according to the\n(binned) graph edit distance (x-axis).\nmodel in both dev and test sets. Regarding the\npairwise comparison with human-created plans,\nproScriptgen still has a signiﬁcant room for im-\nprovement toward human level.\nTable 2 shows the average graph edit distance be-\ntween the generated script and the human revisions.\nWe ﬁnd that neither transfer nor pipeline help to im-\nprove the graph edit distance overproScriptgen,\nindicating that proScriptgen is already a strong\nbaseline (see examples in Appendix). The rea-\nson of no improvement by the transfer approach\nmay be because WikiHow consists of sequences\nrather than partially ordered steps. No improve-\nment by the pipeline approach indicates that the\nproScriptgen can directly generate valid script\nRevision types generated script (subgraph) revised script (subgraph)\nmissing event wait for the plane →exit the plane wait for the plane → get on the plane → exit the\nplane\nincorrect order get off the car →drive to the zoo drive to the zoo →get off the car\nirrelevant or re-\ndundant event\nput clothes in dryer →place clothes into dryer →\ndry clothes\nput clothes in dryer →dry clothes\norder by context get a visa → ... → get off the plane → trip to a\nforeign country\nget off the plane → get a visa (on arrival) → trip\nto a foreign country\ngranularity get out of the bed →go to the kitchen get out of the bed →open the bedroom door →go\nto the kitchen\nparaphrased move into new apartment move to a new apartment\nTable 3: Examples for each revision type.\nRevision types proScript Transfer Pipeline Human\n(edge) incorrect order 15.79 21.62 24.32 10.00\ncrucial errors (node) missing event 5.26 2.70 2.70 0.00\n(node) irrelevant/redundant event 10.53 13.51 2.70 0.00\n(edge) order by context 31.58 32.43 40.54 33.33\nminor revisions (node) granularity 31.58 24.32 21.62 26.67\n(node) paraphrased event 0.00 0.00 5.41 6.67\nwrong revisions 5.26 5.41 2.70 23.33\nTable 4: Revision type distribution (%) by each model.\nin both events and edges. Further studies for im-\nprovements are needed for future work.\nIn terms of the edit types, many of the edits\nare edge-related, suggesting that proScriptgen\nand the variants are all good at generating events\nbut struggles with ordering them. Regarding in-\nand out-of domains in the test sets, we observe\nthat proScriptgen and the variants have slightly\nbetter performance for in-domain scripts than out-\nof domain, while human created scripts are not\naffected by domains. These ﬁndings are consistent\nwith the result in the edge prediction task (§5.3).\nFigure 7 shows a histogram of the graph edit\ndistance. It is evident that human created scripts\nare corrected less often than scripts generate\nby proScriptgen, whereas the scripts from\nproScriptgen and the variants often have a large\nnumber of edits (e.g., 4 or more). It is interesting\nto see that fewer number of scripts have 1 to 3 edits\n(except scripts created by human). The reason is\nbecause one simple revision tends to yield multiple\ngraph edits (e.g., one node insertion yields multiple\nedge insertions).\nError Analysis We performed manual error anal-\nysis for the scripts generated by each model. We se-\nlected 40 random scripts that have non-zero graph\nedit distance and classiﬁed the human revisions\ninto 7 types: (1) incorrect order, (2) missing event,\n(3) irrelevant/redundant event, (4) order by con-\ntext, (5) granularity, (6) paraphrased event, and (7)\nwrong correction (examples are shown in Table 3).\nApproximately, the ﬁrst three types indicate that\nthe script has crucial errors, the next three types are\ntrivial revisions where both generated and revised\nscripts are plausible. The last type of revision is the\none where the revised script is wrong (or worse).\nTable 4 shows the statistics of each error type.\nWe see that edge-related revisions are more fre-\nquent than node-related revisions. This is consis-\ntent with the results in graph edit distance. Overall,\nwe ﬁnd that minor revisions are more frequent than\ncrucial errors, indicating that proScriptgen and\nthe variants generates reasonably good scripts. In\ncontrast, crucial errors are quite rare in human cre-\nated scripts, indicating a signiﬁcant room for future\ninnovation.\n7 Conclusions\nWe show for the ﬁrst time that pre-trained neu-\nral language models can be adapted to generate\npartial order scripts. We collect 6,400 partially or-\ndered script from crowdsourcing ( proScript),\nwhich is substantially larger than prior manually\ncrafted datasets. With the proScript dataset, we\nintroduced two complementary task and models,\nproviding the ﬁrst demonstration that generative\nmodels can be successfully applied to script gener-\nation, although it is still below human performance.\nWe believe that proScript dataset and models\nwould advance future work on various NLP tasks\nsuch as story generation, machine comprehension,\ntemporal reasoning, and high-level planning.\nReferences\nZeina Abu-Aisheh, Romain Raveaux, Jean-Yves\nRamel, and Patrick Martineau. 2015. An Exact\nGraph Edit Distance Algorithm for Solving Pattern\nRecognition Problems. In 4th International Confer-\nence on Pattern Recognition Applications and Meth-\nods 2015, Lisbon, Portugal.\nAnton Belyy and Benjamin Van Durme. 2020. Script\ninduction as association rule mining. In Proceed-\nings of the First Joint Workshop on Narrative Under-\nstanding, Storylines, and Events , pages 55–62, On-\nline. Association for Computational Linguistics.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli C ¸ elikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nNathanael Chambers. 2017. Behind the scenes of an\nevolving event cloze test. In Proceedings of the 2nd\nWorkshop on Linking Models of Lexical, Sentential\nand Discourse-level Semantics, pages 41–45, Valen-\ncia, Spain. Association for Computational Linguis-\ntics.\nNathanael Chambers and Dan Jurafsky. 2008. Unsuper-\nvised learning of narrative event chains. In Proceed-\nings of ACL-08: HLT , pages 789–797, Columbus,\nOhio. Association for Computational Linguistics.\nManuel R. Ciosici, Joseph Cummings, Mitchell De-\nHaven, Alex Hedges, Yash Kankanampati, Dong-\nHo Lee, R. Weischedel, and Marjorie Freedman.\n2021. Machine-assisted script curation. ArXiv,\nabs/2101.05400.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMark Granroth-Wilding and Stephen Clark. 2016.\nWhat happens next? event prediction using a com-\npositional neural network model. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 30.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2020. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. arXiv\npreprint arXiv:2010.05953.\nBram Jans, Steven Bethard, Ivan Vuli ´c, and\nMarie Francine Moens. 2012. Skip n-grams\nand ranking functions for predicting script events.\nIn Proceedings of the 13th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics, pages 336–344, Avignon, France.\nAssociation for Computational Linguistics.\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally coherent text generation with neural\nchecklist models. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 329–339, Austin, Texas. Associa-\ntion for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAman Madaan and Yi-Ming Yang. 2020. Neural lan-\nguage modeling for contextualized temporal graph\ngeneration. ArXiv, abs/2010.10077.\nAshutosh Modi, Tatjana Anikina, Simon Ostermann,\nand Manfred Pinkal. 2016. InScript: Narrative texts\nannotated with script information. In Proceedings\nof the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 3485–\n3493, Portoro ˇz, Slovenia. European Language Re-\nsources Association (ELRA).\nAshutosh Modi and Ivan Titov. 2014. Inducing neu-\nral models of script knowledge. In Proceedings of\nthe Eighteenth Conference on Computational Nat-\nural Language Learning , pages 49–57, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nAshutosh Modi, Ivan Titov, Vera Demberg, Asad Say-\need, and Manfred Pinkal. 2017. Modeling seman-\ntic expectation: Using script knowledge for referent\nprediction. Transactions of the Association for Com-\nputational Linguistics, 5:31–44.\nRaymond J Mooney and Gerald DeJong. 1985. Learn-\ning schemata for natural language processing. In IJ-\nCAI, pages 681–687.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nSimon Ostermann. 2020. Script Knowledge for Nat-\nural Language Understanding . Ph.D. thesis, Saar-\nland University, Germany.\nNanyun Peng, Marjan Ghazvininejad, Jonathan May,\nand Kevin Knight. 2018. Towards controllable story\ngeneration. In Proceedings of the First Workshop on\nStorytelling, pages 43–49, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nKarl Pichotta and Raymond Mooney. 2014. Statisti-\ncal script learning with multi-argument events. In\nProceedings of the 14th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 220–229, Gothenburg, Sweden. As-\nsociation for Computational Linguistics.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba.\n2018. Virtualhome: Simulating household activities\nvia programs. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition ,\npages 8494–8502.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nHannah Rashkin, Asli Celikyilmaz, Yejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state\ntracking. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 4274–4295, Online. Associa-\ntion for Computational Linguistics.\nMichaela Regneri, Alexander Koller, and Manfred\nPinkal. 2010. Learning script knowledge with web\nexperiments. In Proceedings of the 48th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 979–988, Uppsala, Sweden. Associa-\ntion for Computational Linguistics.\nRachel Rudinger, Pushpendre Rastogi, Francis Ferraro,\nand Benjamin Van Durme. 2015. Script induction as\nlanguage modeling. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1681–1686, Lisbon, Portugal. As-\nsociation for Computational Linguistics.\nRoger C. Schank and Robert P. Abelson. 1975. Scripts,\nplans and knowledge. In IJCAI, pages 151–157.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nLilian D. A. Wanzare, Alessandra Zarcone, Stefan\nThater, and Manfred Pinkal. 2016. A crowdsourced\ndatabase of event sequence descriptions for the ac-\nquisition of high-quality script knowledge. In Pro-\nceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC’16) ,\npages 3494–3501, Portoro ˇz, Slovenia. European\nLanguage Resources Association (ELRA).\nNoah Weber, Leena Shekhar, Niranjan Balasubrama-\nnian, and Nathanael Chambers. 2018. Hierarchi-\ncal quantized representations for script generation.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3783–3792, Brussels, Belgium. Association\nfor Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nFangzhou Zhai, Vera Demberg, Pavel Shkadzko, Wei\nShi, and Asad Sayeed. 2019. A hybrid model for\nglobally coherent story generation. In Proceedings\nof the Second Workshop on Storytelling , pages 34–\n45, Florence, Italy. Association for Computational\nLinguistics.\nA Appendices\nA.1 Plans generated by proScriptgen\nWe show some example scripts generated by\nproScriptgen in Figure 8.\nA.2 Reproducibility\nFor the POP model. we use a single CPU with\n4GB memory. This model does not require any\ntraining procedure. For training RoBERTa-large\nas a pairwise model, we use Quadro RTX 8000\n(48GB memory), which takes around 4.5 hours to\ntrain a model. RoBERTa-large consists of 355M\nparameters with 24 layers, 1,024 of hidden embed-\nding size, and 16 of the attention heads. T5-large\nmodel has 770M parameters with 24-layers, 1024-\nhidden-state, 4096 feed-forward hidden-state, and\n16 attention heads. T5-11B models has 11B pa-\nrameters with 24-layers, 1024-hidden-state, 65,536\nfeed-forward hidden-state, 128 attention heads. We\nuse TPU (v3-8) on google cloud platform. It takes\n3 hours in average to train a edge prediction model,\nand 5 hours for plan generation models.\nScenario: play the organ Scenario: audition for a musicalScenario: drink a glass of milk\nFigure 8: Example scripts generated proScriptgen.",
  "topic": "Scripting language",
  "concepts": [
    {
      "name": "Scripting language",
      "score": 0.9243360757827759
    },
    {
      "name": "Computer science",
      "score": 0.8467354774475098
    },
    {
      "name": "Ambiguity",
      "score": 0.6463459730148315
    },
    {
      "name": "Natural language processing",
      "score": 0.6232619881629944
    },
    {
      "name": "Task (project management)",
      "score": 0.5839686393737793
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5341951251029968
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5091518759727478
    },
    {
      "name": "Language model",
      "score": 0.45521777868270874
    },
    {
      "name": "Granularity",
      "score": 0.4482899308204651
    },
    {
      "name": "Programming language",
      "score": 0.17044851183891296
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ]
}