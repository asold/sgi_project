{
    "title": "Continuous Space Language Models using Restricted Boltzmann Machines",
    "url": "https://openalex.org/W2406079600",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2916937209",
            "name": "Niehues, Jan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2915755685",
            "name": "Waibel, Alex",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2116064496",
        "https://openalex.org/W2114211285",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W2251098065",
        "https://openalex.org/W3209717902",
        "https://openalex.org/W83522546",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2105402874",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W2105245376",
        "https://openalex.org/W2143719855",
        "https://openalex.org/W44815768",
        "https://openalex.org/W1970689298",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2099866409",
        "https://openalex.org/W2056590938",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2138706636",
        "https://openalex.org/W2095650036",
        "https://openalex.org/W2083460949",
        "https://openalex.org/W1631260214"
    ],
    "abstract": "We present a novel approach for continuous space language models in statistical machine translation by using Restricted Boltzmann Machines (RBMs). The probability of an n-gram is calculated by the free energy of the RBM instead of a feedforward neural net. Therefore, the calculation is much faster and can be integrated into the translation process instead of using the language model only in a re-ranking step. Furthermore, it is straightforward to introduce additional word factors into the language model. We observed a faster convergence in training if we include automatically generated word classes as an additional word factor. We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures. Instead of replacing the conventional n-gram-based language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way. With this approach we could show improvements of about half a BLEU point on the translation task.",
    "full_text": null
}