{
  "title": "Lexically Constrained Neural Machine Translation with Levenshtein Transformer",
  "url": "https://openalex.org/W3017817427",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288556419",
      "name": "Susanto, Raymond Hendy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288556418",
      "name": "Chollampatt, Shamil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354387089",
      "name": "Tan Li-ling",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2945735543",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2962714778",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2951770285",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2971167892",
    "https://openalex.org/W2963583256",
    "https://openalex.org/W2963352809"
  ],
  "abstract": "This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries. Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.",
  "full_text": "Lexically Constrained Neural Machine Translation\nwith Levenshtein Transformer\nRaymond Hendy Susanto, Shamil Chollampatt, and Liling Tan\nRakuten Institute of Technology Singapore\nRakuten, Inc.\n{first.last}@rakuten.com\nAbstract\nThis paper proposes a simple and effective al-\ngorithm for incorporating lexical constraints in\nneural machine translation. Previous work ei-\nther required re-training existing models with\nthe lexical constraints or incorporating them\nduring beam search decoding with signiﬁ-\ncantly higher computational overheads. Lever-\naging the ﬂexibility and speed of a recently\nproposed Levenshtein Transformer model (Gu\net al., 2019), our method injects terminology\nconstraints at inference time without any im-\npact on decoding speed. Our method does\nnot require any modiﬁcation to the training\nprocedure and can be easily applied at run-\ntime with custom dictionaries. Experiments on\nEnglish-German WMT datasets show that our\napproach improves an unconstrained baseline\nand previous approaches.\n1 Introduction\nNeural machine translation (NMT) systems can\ngenerate higher-quality translations than phrase-\nbased MT systems, but they come at the cost of\nlosing control over how translations are generated.\nWithout the explicit link between the source and the\ntarget vocabulary, enforcing speciﬁc terminologi-\ncal translation in domain-speciﬁc settings becomes\npainfully difﬁcult for NMT systems. Consider an\nexample where we have a Chinese-English NMT\nsystem trained for the E-commerce domain, and\nthere is no prior knowledge of the brand name “红\n米” in the training data, the system would translate\nthe input term literally as “red (红) rice (米)” in-\nstead of “Redmi”. In such scenarios, machine trans-\nlation users often maintain in-domain dictionaries\nto ensure that speciﬁc information is translated ac-\ncurately and consistently.\nA line of previous work that tried to address this\nproblem required re-training the NMT models with\nlexical constraints, either by a placeholder mecha-\nnism (Crego et al., 2016) or via code-mixed train-\ning (Song et al., 2019; Dinu et al., 2019). However,\nthey do not reliably guarantee the presence of the\nconstraints at test time. Another approach focused\non constrained beam search decoding (Hokamp and\nLiu, 2017; Post and Vilar, 2018; Hu et al., 2019).\nAlthough the latter approach has higher control\nover the target constraint terms, they signiﬁcantly\nslow down the decoding.\nDifferent from the existing line of work, we in-\nvoke lexical constraints using a non-autoregressive\napproach.1 To do this, we use Levenshtein Trans-\nformer (LevT) (Gu et al., 2019), an edit-based\ngeneration model that performs deletion and inser-\ntion operations during inference iteratively. LevT\nachieves substantially higher inference speed com-\npared to beam search without affecting quality.\nWe add a constraint insertion step in LevT de-\ncoding to seamlessly decode the target language\nsequence while adhering to speciﬁc lexical con-\nstraints, achieving the same speed as standard LevT\ndecoding.\n2 Related Work\nPrevious approaches integrated lexical constraints\nin NMT either via constrained training or decoding.\nCrego et al. (2016) replaced entities with place-\nholders that remained unchanged during transla-\ntion and placed them back in a post-processing\nstep. Song et al. (2019) trained a Transformer\n(Vaswani et al., 2017) model by augmenting the\ndata to include the constraint target phrases in the\nsource sentence. Dinu et al. (2019) proposed a\nsimilar idea and additionally used factored training.\nOther approaches proposed enforcement of lexical\nconstraints during inference with various improve-\nments to constraint-aware beam search, such as\n1In literature, non-autoregressive NMT decoding mostly\nrefers to those that do not generate tokens sequentially, al-\nthough they perform iterative reﬁnement (Lee et al., 2018).\narXiv:2004.12681v1  [cs.CL]  27 Apr 2020\ngrid beam search (Hokamp and Liu, 2017), dy-\nnamic beam allocation (Post and Vilar, 2018), and\nits optimized vectorized version (Hu et al., 2019).\nHasler et al. (2018) built ﬁnite-state acceptors to in-\ntegrate constraints in a multi-stack decoder. These\nlexically-constrained decoding approaches rely on\nautoregressive inference that generates one target\ntoken at a time, which makes it difﬁcult to par-\nallelize the decoder and monotonically increases\ndecoding time. While being mostly effective at\nforcing the inclusion of pre-speciﬁed terms in the\noutput, these approaches further slow down the\nbeam search process. Post and Vilar (2018) re-\nported 3×slow down compared to standard beam\nsearch.\nNon-autoregressive neural machine translation\n(NAT) (Gu et al., 2018) attempts to move away\nfrom the conventional autoregressive decoding.\nSuch a direction enables parallelization during se-\nquence generation that results in lower inference\nlatency. Recent NAT approaches treat inference\nas an iterative reﬁnement process, ﬁrst proposed\nby Lee et al. (2018). Following this direction, it is\nintuitive to perform decoding using “edit” opera-\ntions, such as insertion (Stern et al., 2019) or both\ninsertion and deletion (LevT, Gu et al. (2019)). The\nLevT model has been shown to outperform existing\nreﬁnement-based models, such as Ghazvininejad\net al. (2019) and performs comparably to autore-\ngressive Transformer models. Our method inte-\ngrates lexical constraints in NAT decoding utilizing\nthe ﬂexibility, speed, and performance of LevT.\n3 Levenshtein Transformer\nLevenshtein Transformer (LevT) (Gu et al., 2019)\nhas an encoder-decoder framework based on Trans-\nformer architecture (Vaswani et al., 2017) with\nmulti-headed self-attention and feed-forward net-\nworks. Unlike token generation in a typical Trans-\nformer model, LevT decoder models a Markov\nDecision Process (MDP) that iteratively reﬁnes\nthe generated tokens by alternating between the\ninsertion and deletion operations. After embed-\nding the source input through a Transformer en-\ncoder block, the LevT decoder follows the MDP\nformulation for each sequence at the k-th itera-\ntion yk = (y1, y2, ..., yn), where y1 and yn are the\nstart (<s>) and end (</s>) symbols. The decoder\nthen generates yk+1 by performing deletion and\ninsertion operations via three classiﬁers that run\nsequentially:\nConstraint Insertion\nPlaceholder Classiﬁer\nToken Classiﬁer\n<s> Nevada hat bereits ein Pilot@@ projekt abgeschlossen . </s>\nDeletion Classiﬁer\n<s> </s>\n<s> Nevada Pilot@@ projekt </s>\n<s> Nevada [PLH] [PLH] [PLH] Pilot@@ projekt  [PLH] [PLH] </s>         \n<s> Nevada Pilot@@ projekt </s>\nFigure 1: Levenshtein Transformer decoding with lex-\nical constraints for English-German MT. The source\nsentence is Nevada has completed a pilot project. and\nthe target constraints are [ Nevada, Pilot@@ projekt].\nEncoder and attention components are not shown.\n1. Deletion Classiﬁer, which predicts for each\ntoken position whether they should be “kept”\nor “deleted”,\n2. Placeholder Classiﬁer , which predicts the\nnumber of tokens to be inserted between every\ntwo consecutive tokens and then inserts the\ncorresponding number of placeholder [PLH]\ntokens,\n3. Token Classiﬁer , which predicts for each\n[PLH] token an actual target token.\nEach prediction is conditioned on the source text\nand the current target text. The same Transformer\ndecoder block is shared among the three classiﬁers.\nDecoding stops when the current target text does\nnot change, or a maximum number of reﬁnement\niterations has been reached.\nThe LevT model is trained using sequence-level\nknowledge distillation (Kim and Rush, 2016) from\na Transformer teacher whose beam search output is\nused as ground truth during training. We refer the\nreaders to (Gu et al., 2019) for a detailed descrip-\ntion of the LevT model and training routine.\n4 Incorporating Lexical Constraints\nFor sequence generation, the LevT decoder typi-\ncally starts the ﬁrst iteration of the decoding pro-\ncess with only the sentence boundary tokens y0 =\n<s></s>. To incorporate lexical constraints, we\npopulate the y0 sequence before the ﬁrst deletion\noperation with the target constraints, as shown in\nFigure 1. The initial target sequence will pass\nthrough the deletion, placeholder, and insertion\nclassiﬁers sequentially, and the modiﬁed sequence\nwill be reﬁned for several iterations. The decoding\nsteps are explained in detail below.\nConstraint Insertion More formally, given a list\nof m target constraints C1, C2, ..., Cm, where each\nconstraint Ci is possibly a multi-token phrase Ci =\nwi\n1, wi\n2, ..., wi\n|Ci|, we insert the constraints into the\ndecoding sequence before the deletion operation to\nform y0 = <s> C1 C2 ... Cn</s>.\nDeletion Operation Next, y0 passes through the\ndeletion classiﬁer to decide which wi\nj token to re-\nmove. If the deletion operation is allowed on the\nconstraint tokens, the presence of each constraint in\nthe ﬁnal output is not guaranteed, especially when\nthe supplied constraints are out of context for the\ndecoder. To mitigate this problem, we optionally\ndisallow the deletion operation on the constraint\ntokens by introducing a constraint mask to indicate\nthe positions of constraint tokens in the sequence.\nWe forcefully set the deletion classiﬁer prediction\nfor all positions in this mask to “keep”. The po-\nsitions in this mask are re-computed accordingly\nafter each deletion and insertion operation.\nInsertion Operation Finally, the y0 passes\nthrough the placeholder classiﬁer to predict the\nnumber of tokens to be inserted and generate the\ncorresponding number of [PLH] tokens and the\ntoken classiﬁer assigns an actual target token for\nevery [PLH] token. Each constraint may contain\nmultiple tokens, and the [PLH] tokens may be\ninserted between the tokens from the same con-\nstraint. To prevent this from happening and to keep\neach constraint intact, we optionally prohibit in-\nserting [PLH] within a multi-token constraint by\nconstraining 0 to the number of such placeholders.\nIn Figure 1, our constraint insertion is executed\nat the ﬁrst pass, and subsequent iterations start from\ndeletion (indicated by a loop in the ﬁgure). We\nnote that this step happens only at inference; dur-\ning training, the original LevT training routine is\ncarried out without the constraint insertion.\n5 Experiments\nWe extend the FAIRSEQ 2 (Ott et al., 2019) imple-\nmentation of the original LevT architecture\n2https://github.com/pytorch/fairseq/commit/2d51e04\nTerm% BLEU Speed\nFull Constr. (sent/sec)\nBaseline LevT 80.23 26.49 29.86 263.11\n+ Constr. Ins. 94.43 26.50 29.93 260.19\n+ No Del. 99.62 26.59 30.43 260.61\n+ No Ins. 100.00 26.60 30.49 254.64\nTable 1: Results of LevT with lexical constraints on\nWMT14 En-De task\nto perform lexically-constrained decoding. All\nTransformer blocks in our LevT model follow the\nbase conﬁguration that contains 6 layers with 8 at-\ntention heads each, with a model sizedmodel = 512\nand feed-forward layer size dﬀ = 2048; the source\nand target embeddings share the same vocabulary.\nThe LevT model is trained using knowledge distilla-\ntion routine using Transformer base output released\nby Gu et al. (2019). We leave more experimental\ndetails in the Appendix.\n5.1 Data and evaluation settings\nWe evaluate our approach on the WMT’14 English-\nGerman (En-De) news translation task (Bojar et al.,\n2014) with En-De bilingual dictionary entries ex-\ntracted from Wiktionary 3 following Dinu et al.\n(2019), by matching the source and target phrases\nof the dictionary entries in the source and target\nsentences, respectively.\nWe also evaluate our approach on two En-De\ntest sets released by Dinu et al. (2019) to compare\nour approach against previous work on applying\nlexical constraints in NMT (Post and Vilar, 2018;\nDinu et al., 2019). The two test sets are subsets\nof WMT’17 En-De test set (Bojar et al., 2017) ex-\ntracted using Wiktionary and the Interactive Termi-\nnology for Europe (IATE) terminology database,4\nrespectively. Both the WMT’14 and WMT’17 En-\nDe datasets are tokenized using the Moses tokeniza-\ntion scripts and segmented into sub-word units us-\ning byte-pair encoding (Sennrich et al., 2016).\n5.2 Results\nWe evaluate the systems using BLEU scores (Pa-\npineni et al., 2002) and term usage rate (Term%),\nwhich is deﬁned as the number of constraints gen-\nerated in the output divided by the total number of\nthe given constraints.\nTable 1 shows the result of (i) the baseline LevT\nmodel, (ii) with the constraint insertion operation\n(+ Constr. Ins.), (iii) with the constraint insertion\n3https://dumps.wikimedia.org/enwiktionary/\n4https://iate.europa.eu/\nSource “We don’t want tocharge that,” she said.\nBaseline LevT “Das wollen wir nicht in Rechnung stellen”, sagte sie.\n+ Constr. Ins. “Das wollen wir nicht verlangen”, sagte sie.\n+ No Del. + No Ins. “Das wollen wir nicht berechnen”, sagte sie.\nReference “Wir m¨ochten diese Summe nicht berechnen”, erkl¨arte sie.\nTable 2: Example translations from the LevT with constraint insertion to enforce the translation of\ncharge→berechnen. When deletion is allowed (+ Constr. Ins. ) the imposed constraint ( berechnen) gets deleted\nduring decoding. But when deletion is disallowed (+ No Del.) and unwanted insertion between constraint tokens\nis prohibited (+ No Ins.), it guarantees the presence of our desired term in the ﬁnal translation. We show more\nexamples in the Appendix.\noperation and forcefully disallowing deletion of\nthe constraints (+ No Del.) and (iv) disallowing\n[PLH] insertion between tokens from the same\nconstraint (+ No Ins. ). Table 2 shows an exam-\nple where prohibiting constraint deletion prevents\ncatastrophic removal of the lexical constraint.\nWe report results on both the ﬁltered test set\nfor sentence pairs that contain at least one target\nconstraint (“Constr.”, 454 sentences) and the full\ntest set (“Full”, 3,003 sentences). The constraint\ninsertion operation increases the term usage rate\nfrom about 80% to over 94%, and further disal-\nlowing deletion of the constraints achieves above\n99% term usage. Prohibiting insertion between\neach constraint’s tokens guarantees a 100% term\nusage. For sentences with lexical constraints, we\nobserve a statistically signiﬁcant improvement of\n0.6 BLEU (p-value < 0.05) based on bootstrap re-\nsampling (Koehn, 2004). On the full test set, the\nBLEU improves by 0.1. The small margin of im-\nprovement is because only 1% of the total reference\ntokens are constraint tokens. Unlike previous work\nthat sacriﬁced decoding speed to enforce lexical\nconstraints (e.g. Hasler et al., 2018; Post and Vilar,\n2018), there is no signiﬁcant difference in the num-\nber of sentences decoded per second between the\nunconstrained and the lexically constrained LevT\nmodels.\nTable 3 presents the comparison to two previous\napproaches: constrained decoding with dynamic\nbeam allocation (Post and Vilar, 2018) and data\naugmentation by replacing the source terms with\ntarget constraints during training (Dinu et al., 2019).\nWe refer to them as POST 18 and DINU 19, respec-\ntively, in Table 3. We evaluate each approach on\nthe WMT’17 En-De test set with constraint terms\nfrom Wiktionary and IATE dictionaries. Note that\nour baseline LevT model with Transformer blocks\nof 6 layers is superior to that of Dinu et al. (2019)\nwho used a 2-layer conﬁguration. Despite having a\nstronger baseline, we obtain higher absolute BLEU\nWiktionary IATE\nTerm% BLEU Term% BLEU\nPrevious work\nBaseline Trans. 76.90 26.00 76.30 25.80\nPOST 18 99.50 25.80 82.00 25.30\nDINU 19 93.40 26.30 94.50 26.00\nThis work\nBaseline LevT 81.11 30.24 80.31 28.97\n+ Constr. Ins. 93.44 30.82 93.81 29.73\n+ No Del. 98.53 31.04 99.12 30.09\n+ No Ins. 100.00 31.20 100.00 30.13\nTable 3: Comparison to previous work. Baseline Trans-\nformer and POST 18 results are from Dinu et al. (2019).\nscore improvements (0.96 and 1.16 BLEU on Wik-\ntionary and IATE, respectively) and achieved 100%\nterm usage. We report additional experiments on\nWMT’16 Romanian-English news translation task\n(Bojar et al., 2016) in the Appendix.\n5.3 Analysis\nTo analyze if our approach inserts the constraints\nat correct positions, we compare it to a baseline ap-\nproach of randomly inserting the constraint terms\nin the output of our baseline LevT model. Note that\nwe only insert those constraints that are not already\npresent in the output. Although this results in a\n100% term usage, we observe that the BLEU score\ndrops from 29.9 to 29.3 on the “Constr.” WMT’14\ntest set, whereas our approach improves the BLEU\nscore. The LevT model with our proposed con-\nstraint insertion seems to inherently have the ability\nto place the constraints at correct positions in the\ntarget sentence.\nAlthough prohibiting constraint deletion im-\nproves term usage in the ﬁnal translation and\nachieves higher BLEU scores, it limits the possibil-\nity of reordering when there is more than one con-\nstraint during inference. For the English-German\ntest sets we evaluated on, 97-99% of the target\nconstraints appear in the same order as the source\nterms. This issue may become more apparent in lan-\nguage pairs with more distinct syntactic differences\nbetween the source and target languages. In prac-\ntice, most of the entries in terminology databases\n(Wiktionary, IATE, etc.) are often nominal. Thus,\nthe reordering of lexical constraints boils down to\nwhether the source and target language share the\nsame argument-predicate order.5 We will explore\npotential strategies to reorder constraints dynami-\ncally in future work.\n6 Conclusion\nWe proposed a non-autoregressive decoding ap-\nproach to integrate lexical constraints for NMT.\nOur constraint insertion step is simple and we have\nempirically validated its effectiveness. The ap-\nproach demonstrated control over constraint terms\nin target translations while being able to decode as\nfast as a baseline Levenshtein Transformer model,\nwhich achieves signiﬁcantly higher decoding speed\nthan traditional beam search. 6 In addition to the\nterminological lexical constraints discussed in this\nwork, future work can potentially modify inser-\ntion or selection operations to handle target transla-\ntions of multiple forms; this can potentially disam-\nbiguate the morphological variants of the lexical\nconstraints.\nReferences\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, Radu Soricut, Lucia Specia, and Aleˇs\nTamchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 conference on machine\ntranslation (WMT17). In Proceedings of the Second\nConference on Machine Translation.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aur ´elie\nN´ev´eol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\n5i.e., Subject (S), Verb (V), and Object (O), SOV , or VSO\nordering.\n6Our implementation will be made publicly available at\nhttps://github.com/raymondhs/constrained-levt.\nZampieri. 2016. Findings of the 2016 conference\non machine translation. In Proceedings of the\nFirst Conference on Machine Translation: Volume\n2, Shared Task Papers.\nJosep Maria Crego, Jungi Kim, Guillaume Klein,\nAnabel Rebollo, Kathy Yang, Jean Senellart,\nEgor Akhanov, Patrice Brunelle, Aurelien Co-\nquard, Yongchao Deng, Satoshi Enoue, Chiyo Geiss,\nJoshua Johanson, Ardas Khalsa, Raoum Khiari,\nByeongil Ko, Catherine Kobus, Jean Lorieux, Leidi-\nana Martins, Dang-Chuan Nguyen, Alexandra Priori,\nThomas Riccardi, Natalia Segal, Christophe Servan,\nCyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing\nZhou, and Peter Zoldan. 2016. SYSTRAN’s pure\nneural machine translation systems. arXiv preprint\narXiv:1610.05540.\nGeorgiana Dinu, Prashant Mathur, Marcello Federico,\nand Yaser Al-Onaizan. 2019. Training neural ma-\nchine translation to apply terminology constraints.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP).\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O. K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In 6th\nInternational Conference on Learning Representa-\ntions, Conference Track Proceedings.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In Advances in Neural In-\nformation Processing Systems 32.\nEva Hasler, Adri `a de Gispert, Gonzalo Iglesias, and\nBill Byrne. 2018. Neural machine translation decod-\ning with terminology constraints. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers).\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers).\nJ. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\nXia, Tongfei Chen, Matt Post, and Benjamin\nVan Durme. 2019. Improved lexically constrained\ndecoding for translation and monolingual rewriting.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers).\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests for\nmachine translation evaluation. In Proceedings of\nthe 2004 Conference on Empirical Methods in Natu-\nral Language Processing.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics.\nMatt Post and David Vilar. 2018. Fast lexically con-\nstrained decoding with dynamic beam allocation for\nneural machine translation. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers).\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers).\nKai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun\nWang, and Min Zhang. 2019. Code-switching for\nenhancing NMT with pre-speciﬁed translation. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers).\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. In Pro-\nceedings of the 36th International Conference on\nMachine Learning.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30.\nA Datasets\nWe train on 3,961,179 distilled sentence pairs re-\nleased by Gu et al. (2019) and evaluate on WMT’14\nEn-De test set (3,003 sentences). The dictionary\nused in this work is created by sampling 10% En-\nDe translation entries from Wiktionary, resulting\nin 10,522 entries. After applying this dictionary to\ngenerate constraints for the test set, we obtain 454\nsentences that contain at least one constraint. The\naverage number of constraints per sentence is 1.15\nand the number of unique source constraints is 220.\nWe use an English frequency list7 to ﬁlter the 500\nmost frequent words. We use the WMT’17 En-De\ntest sets released by Dinu et al. (2019)8 that were\ncreated based on Wiktionary and IATE term en-\ntries exactly matching the source and target. They\ncontain 727 and 414 sentences, respectively.\nB Hyperparameters\nTable 4 shows the hyperparameter settings for our\nLevT model. We learn a joint BPE vocabulary with\n32,000 operations. Their resulting vocabulary size\nis 39,843.\nEmbedding dim. 512\nLearned positional embeddings Yes\nTied embeddings Yes\nTransformer FFN dim. 2,048\nAttention heads 8\nEn/Decoder layers 6\nLabel smoothing 0.1\nDropout 0.3\nWeight decay 0.01\nLearning rate 0.005\nWarmup updates 10,000\nEffective batch size in tokens 64,000\nMax. updates 300,000\nTable 4: LevT hyperparameter settings\nC Additional Experiments\nWe train a LevT model on 599,907 training sen-\ntence pairs from the WMT’16 Romanian-English\n(Ro-En) news translation task (Bojar et al., 2016)\nusing knowledge distillation routine based on\nTransformer base output and evaluate on 1,999 test\nsentences. Similar to En-De, we create a dictionary\n7https://norvig.com/ngrams/count_1w.txt\n8https://github.com/mtresearcher/terminology_\ndataset\nTerm% BLEU Speed\nFull Constr. (sent/sec)\nBaseline LevT 80.33 33.00 35.35 271.32\n+ Constr. Ins. 95.33 33.10 35.96 274.01\n+ No Del. 98.67 33.13 36.09 263.68\n+ No Ins. 100.00 33.13 36.09 264.45\nTable 5: Results of LevT with lexical constraints on\nWMT16 Ro-En task\nby sampling 10% Ro-En translation entries from\nWiktionary, resulting in 3,490 entries. We use this\ndictionary to generate 270 test sentences that con-\ntain at least one constraint. The average number\nof constraints per sentence is 1.11, and the number\nof unique source constraints is 122. Similarly, we\nﬁlter out the 500 most frequent English words.\nWe train our LevT model using the same hyper-\nparameter settings from Table 4. We learn a joint\nBPE vocabulary with 40,000 operations, which re-\nsults in 39,348 vocabulary size. Table 5 shows the\nexperiment results. We observe consistent ﬁndings\nin our En-De experiments in terms of improved\nterm usage rate (from 80% to 100%) and a small\nmargin of improvement of 0.7 BLEU, while being\nable to decode as fast as a baseline LevT model.\nD Examples\nTable 6 shows more example translations of the\nlexically constrained LevT model.\nWMT’14 En-De\nSource Bwelle and his team spend almost every weekend seeing hundreds of patients {spend→verbringen, almost→beinahe}\nBaseline LevT Bwelle und sein Team verbringen fast jedes Wochenende mit Hunderte von Patienten.\n+ Constr. Ins. Bwelle und sein Team verbringen beinahe jedes Wochenende mit Hunderte von Patienten.\n+ No Del. + No Ins. Bwelle und sein Team verbringen beinahe jedes Wochenende mit Hunderte von Patienten.\nReference Bwelle und sein Team verbringen beinahe jedes Wochenende damit, Hunderte von Patienten zu behandeln\nSource There have already been two events held in the brightly lit caf´e. {already→schon}\nBaseline LevT Im hell beleuchteten Caf´e fanden bereits zwei Veranstaltungen statt.\n+ Constr. Ins. Im hell beleuchteten Caf´e fanden bereits zwei Veranstaltungen statt.\n+ No Del. + No Ins. Im hell beleuchteten Caf´e fanden schon zwei Veranstaltungen statt.\nReference Zwei Events gab’s auchschon im hellen Caf´e.\nWMT’17 En-De - Wiktionary\nSource House searches had revealed evidence and drugs, the police revealed on Friday. {evidence→Beweismittel, police→Polizei}\nBaseline LevT Durchsuchungen des Hauses hatten Beweise und Drogen enth¨ullt, die Polizei am Freitag enth¨ullt.\n+ Constr. Ins. Hausdurchfragen hatten Beweismittel und Drogen offenbart, hat die Polizei am Freitag enth¨ullt.\n+ No Del. + No Ins. Durchfragen hatten Beweismittel und Drogen offenbart, die Polizei am Freitag enth¨ullt.\nReference Bei Wohnungsdurchsuchungen seien Beweismittel und Rauschgift sichergestellt worden, teilte die Polizei am Freitag mit.\nSource We always say that it has a lot of Latin American inﬂuences. {Latin American→lateinamerikanisch}\nBaseline LevT Wir sagen immer, dass sie viele lateinamerikanische Einﬂ¨usse hat.\n+ Constr. Ins. Wir sagen immer, dass sie viel lateinamerikanisch beeinﬂusst.\n+ No Del. + No Ins. Wir sagen immer, dass sie viel lateinamerikanisch beeinﬂusst.\nReference Wir sagen immer, dass sie sehr lateinamerikanisch gepr¨agt ist.\nWMT’17 En-De - IATE\nSource What is behind sleep disorders? {sleep disorders→Schlafst¨orungen}\nBaseline LevT Was steckt hinter Schlafkrankheiten?\n+ Constr. Ins. Was steckt hinter Schlafst¨orungen?\n+ No Del. + No Ins. Was steckt hinter Schlafst¨orungen?\nReference Was steckt hinter Schlafst¨orungen?\nSource He said another stepson who lives nearby alerted him. {stepson→Stiefsohn}\nBaseline LevT Er sagte, ein weiterer Stiefson, der in der N¨ahe lebt, alarmierte ihn.\n+ Constr. Ins. Er sagte, ein weiterer Stiefsohn, der in der N¨ahe lebt, alarmierte ihn.\n+ No Del. + No Ins. Er sagte, ein weiterer Stiefsohn, der in der N¨ahe lebt, alarmierte ihn.\nReference Er sagte, dass ihn ein weiterer Stiefsohn, der in der N¨ahe wohnt, gewarnt h¨atte.\nTable 6: More example translations from the LevT with constraint insertion. The constraints are in curly brackets.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8169393539428711
    },
    {
      "name": "Computer science",
      "score": 0.806678831577301
    },
    {
      "name": "Decoding methods",
      "score": 0.7121430039405823
    },
    {
      "name": "Transformer",
      "score": 0.7010859251022339
    },
    {
      "name": "Levenshtein distance",
      "score": 0.6709734201431274
    },
    {
      "name": "Inference",
      "score": 0.5699246525764465
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5526842474937439
    },
    {
      "name": "Terminology",
      "score": 0.5416529774665833
    },
    {
      "name": "Beam search",
      "score": 0.4464593529701233
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.43513017892837524
    },
    {
      "name": "Natural language processing",
      "score": 0.4240341782569885
    },
    {
      "name": "Machine learning",
      "score": 0.39166951179504395
    },
    {
      "name": "Search algorithm",
      "score": 0.22099977731704712
    },
    {
      "name": "Algorithm",
      "score": 0.192496657371521
    },
    {
      "name": "Voltage",
      "score": 0.08109205961227417
    },
    {
      "name": "Engineering",
      "score": 0.06504198908805847
    },
    {
      "name": "Mathematics",
      "score": 0.06488394737243652
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}