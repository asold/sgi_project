{
  "title": "Exploring The Prospects And Challenges Of Large Language Models For Language Learning And Production",
  "url": "https://openalex.org/W4386050906",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2120502545",
      "name": "Anna M. Borghi",
      "affiliations": [
        "Institute of Cognitive Sciences and Technologies"
      ]
    },
    {
      "id": "https://openalex.org/A4378110950",
      "name": "Chiara De Livio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995925208",
      "name": "Francesco Mannella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A717299384",
      "name": "Luca Tummolini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A73201049",
      "name": "Stefano Nolfi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4200018804",
    "https://openalex.org/W3019529248",
    "https://openalex.org/W6809085903",
    "https://openalex.org/W3180556725",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6677981114",
    "https://openalex.org/W4213025562",
    "https://openalex.org/W2904990229",
    "https://openalex.org/W6732685356",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3161997689",
    "https://openalex.org/W2800893588",
    "https://openalex.org/W4328051875",
    "https://openalex.org/W6752473969",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W2190736972",
    "https://openalex.org/W3204667412",
    "https://openalex.org/W2157853130",
    "https://openalex.org/W2117352571",
    "https://openalex.org/W6631502567",
    "https://openalex.org/W2972680241",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W6650144174",
    "https://openalex.org/W3165020050",
    "https://openalex.org/W1969138120",
    "https://openalex.org/W2155721440",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W2805428026",
    "https://openalex.org/W4225309552",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4312143998",
    "https://openalex.org/W2144862731",
    "https://openalex.org/W2076748564",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W3119614544",
    "https://openalex.org/W4317797582",
    "https://openalex.org/W3213516082",
    "https://openalex.org/W7064937209",
    "https://openalex.org/W4287887298",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2969129063",
    "https://openalex.org/W6799913168",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4312143882",
    "https://openalex.org/W2808512304",
    "https://openalex.org/W4306922998",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4309419356",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W4378469174",
    "https://openalex.org/W4220944417",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4366552789",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2579313781",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W4252434862",
    "https://openalex.org/W3199748991",
    "https://openalex.org/W4310957622",
    "https://openalex.org/W4229076925",
    "https://openalex.org/W2168488947",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4239636595",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W2119462124",
    "https://openalex.org/W4312143904",
    "https://openalex.org/W3110909889",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W4386567020",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4380033159",
    "https://openalex.org/W2809358850",
    "https://openalex.org/W4302454383",
    "https://openalex.org/W4386566857",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4389518978",
    "https://openalex.org/W4281566718"
  ],
  "abstract": "The success of Large Language Models (LLMs) in many application domains suggests that they may also change how we conceive cognition. LLMs possess capabilities traditionally considered exclusively human. Can the experience with language alone facilitate the acquisition of other complex cognitive abilities? Do linguistic, sensorimotor, and interoceptive experiences need to be integrated? Are there domains, like that of abstract concepts (e.g., freedom), where linguistic experience suffices to capture meaning? After introducing what LLMs are, we address their potential impact, discussing five differences from human cognition: they are not grounded, lack action, hardly capture pragmatics, are culturally biased, and do not reflect individual characteristics.",
  "full_text": " \nTo be included in V. Bambini and C. Barattieri di San Pietro (eds.), \"Multidisciplinary perspectives on ChatGPT and \nother Artificial Intelligence Models\" / Focus monografico “Prospettive multidisciplinari su ChatGPT e altri \nmodelli di intelligenza artificiale” , Sistemi Intelligenti 2023(2), Special Section. \nISSN: 1120 - 9550. Forthcoming \n \n \n \nEXPLORING THE PROSPECTS  \nAND CHALLENGES OF LARGE LANGUAGE MODELS  \nFOR LANGUAGE LEARNING AND PRODUCTION \n \nAnna M. Borghi*^, Chiara De Livio^, Francesco Mannella^, Luca Tummolini^, & Stefano Nolfi^ \n*Dipartimento di Psicologia Dinamica, Clinica e Salute, Sapienza Università di Roma \n^Istituto di Scienze e Tecnologie della Cognizione, Consiglio Nazionale delle Ricerche, Roma \n \n \n \nAbstract  \nThe success of Large Language Models (LLMs) in many application domains suggests that they may \nalso change how we conceive cognition. LLMs possess capabilities traditionally considered \nexclusively human. Can the experience with language alone facilitate the acquisition of other complex \ncognitive abilities? Do linguistic, sensorimotor, and interoceptive experiences need to be integrated? \nAre there domains, like that of abstract concepts (e.g., freedom), where linguistic experience suffices \nto capture meaning? After introducing what LLMs are, we address their potential impact, discussing \nfive differences from human cognition: they are not grounded, lack action, hardly capture pragmatics, \nare culturally biased, and do not reflect individual characteristics.  \nKeywords: large language models, grounded cognition, abstract concepts, ChatGPT, cultures.  \n \n1. Introduction. Large Language Models: What are they?  \nLLMs such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al. , 2022), and LLaMA (Touvron \net al., 2023) consist of large neural networks  containing hundreds of billions (or more) parameters \nthat are  trained to predict the next word of a massive set of human written text  on the basis of the \npreceding words. More specifically, given a sequence of token x = {x\n1, ….., xn}, where tokens encode \nwords or word parts, LLMs are trained autoregressively to predict the target token xi based on the \npreceding tokens x<i. They are based on the Transformer neural network architecture (Vaswani et al., \n2017) where multi -head attention layers are stacked in  a very deep neural network. The learning \nprocess is self-supervised, i.e., it does not require any additional information besides the written text. \nIndeed, the next token to be predicted can be extracted  by the text itself. They thus learn through a \nform of observation learning which is passive, i.e., they do not interact with a physical or  social \nenvironment and cannot alter their next observations through their actions. The sequence of words \nthat they receive as input is fixed and is determined by the pre-existing written text.  \nLLMs are often fine -tuned by using a reinforcement learning with human feedback method which \nrequires interaction with human instructors who rank the LLM outputs based on their preferences \n(Ouyang et al., 2022). Such fine -tuning does not encode any further semantic knowledge but  it is \nused to align the output of the models to human preferences and/ or to facilitate the elicitation of the \nabilities acquired during the training phase . LLMs can also be trained by using training sets t hat \ninclude images and other forms of non- linguistic data. However, for the purpose of our  argument \nhere, we focus our analysis on the knowledge and abilities extracted  during the prediction learning \nphase and on models that are trained with linguistic data only (written text only).  \nLLMs acquire formal linguistic competence (Mahowald et al., 2023 ) and are capable of producing \ntext that is hard to distinguish from human output. They are capable of correctly discriminating \ngrammatical vs ungrammatical  sentences by passing challenging tests designed by the natural  \nlanguage research community (Warstadt et al., 2020; Warstadt, Bowman, 2022). The quality of the \ncompetence acquired largely surpass what linguists could imagine only five years ago and falsify \npast claims stating that statistical approaches would never be able to capture the com plex syntactic \nand semantic features of language (Pinker, Prince, 1988; Petroni  et al., 2019; Everaert et al., 2015). \nMoreover, LLMs display a  large set of additional competencies which include the ability to perform  \ndynamical semantic operations, i.e., understanding how the meaning of  a sentence alters the context \ndescribed in the preceding sentences (Li et  al., 2021), affordance recognition skills (Jones et al., \n2022), theory of  mind skills that enable them to infer the mental states of the characters  described \nin a story (Kosinski, 2023), and reasoning skills (Talmor et al.,  2022; Creswell, Shanahan, Higgins, \n2022). Remarkably, they also learn and use representations o f the outside world, at least to some \nextent, despite not having direct sensory experiences with it. For instance, it has been shown that they \nacquire internal representations of color words which closely mirror the properties of human color \nperception (Abdou et al., 2021; Patel, Pavlick, 2022; Søgaard, 2023). Moreover, they can internally \nrepresent the spatial layout of the setting of a story (Patel, Pavlick, 2022; Bubeck et al., 2023) and \nupdate such representations as more related information is revealed (Li et al., 2021). These large and \ndiverse sets of skills are all acquired as a side effect of the attempt to predict the next word of written \ntexts. These remarkable results can be explained by considering that, in order to guess the next word \nof a text really well, one should deeply understand the preceding text. And to understand the text \nreally well, one should acquire and use factual knowledge and complex cognitive skills.  \nClearly, these results do not necessarily imply that humans acquire their abilities in the same manner. \nUnlike LLMs, humans interact with their  physical and social environment in an active way and \nexploit such interactions to acquire their knowledge and skills. Further differences can be caused by \nthe fact that humans have limited short -term memory and that the amount of language used to train \nLLMs is much greater than the  amount of language input experienced by humans in the course of \ntheir life. Conversely, the results obtained with the LLMs demonstrate that  extracting factual  \nknowledge, linguistic knowledge, and cognitive skills from the order of words of a human-generated \nlanguage is possible. More importantly, they indicate that humans could also extract part of their  \nknowledge and skills in a similar manner. \n \n2. LLMS and cognitive theories of concepts and language  \nLLMs might be relevant for current theories of meaning. Embodied and grounded theories (Barsalou, \n2008; Gallese, Lakoff, 2005; Pulvermüller, Fadiga 2010), claim that concepts are “grounded” in the \nenvironment via perception, action, and emotion. In contrast with them  (Glenberg, Robertson, 2000), \ndistributional semantics theories (e.g.,  Latent Semantic Analysis – LSA, Landauer, Dumais, 1997) \npropose that  meaning can be grasped based on “the company”  words keep: words  have similar \nmeanings when they have similar distributional patterns  across corpora (Günther et al., 2019). A \nmajor problem of embodied  theories was to account for the formation and use of abstract concepts  \n(Borghi et al., 2017), and syntax (but see Thibault et al., 2022); a major issue for distributional \nsemantic views was the symbol grounding  problem (Harnad, 1990), i.e., the impossibility of \nlinguistic symbols to  acquire meaning without sensory and motor experience. Later, hybrid models \nemerged (e.g., Louwerse, 2018). Now, also thanks to LLMs, understanding the role language plays \nin the formation of concepts and in accessing meaning is becoming urgent. Which is ultimately the \nexplanatory “power” of language?  \nOne possibility is that it differs depending on the domain under consideration. In some domains, \nlanguage might play a more fundamental role in determining meaning. Studies on abstract concepts \nsuggest that this could be the case.  \n2.1. Abstract concepts and language  \nAbstract concepts and the words expressing them (e.g., “fantasy” ), even  if not dichotomously \nopposed to concrete ones, typically differ from them across multiple dimensions (for analyses, Borghi \net al., 2019; Borghi, 2023; Dove, 2022). Abstract concepts typically do not refer to single  bounded \nobjects but to multiple elements, and their members are heterogeneous . Also due to their relational \ncharacter, abstract concepts are more complex, less determined, and more negotiable than concrete \nones. Hence, recent studies have underlined the importance of language to acquire, represent and use \nabstract concepts. For example, to acquire the  concept “justice,” the linguistic label can work as a \nglue holding together various situations; in addition, the linguistic and social scaffolding provided by \nothers who can explain the conceptual meaning is fundamental . Evidence with facilitation and \ninterference tasks shows that  the mouth motor system is more activated with abstract than with \nconcrete concepts, pointing to the m ajor role of language (and inner speech ) in their acquisition, \nrepresentation, and use (Barca et al., 2020; Fini et al., 2022). Consistently, abstract concepts activate \nlinguistic and social neural areas, particularly the left inferior frontal gyrus, invol ved in articulation \nand verbal working memory. Different abstract concepts – emotional, numerical, social, etc. – also \nactivate different areas related to their content (Desai et al., 2018; Conca et al., 2021; Pexman et al., \n2023; Villani et al., 2019).  \nOnce the importance of language for abstract concepts is established, a question arises: do we really \nrepresent them only through language? The answer is no. They also evoke sensory modalities, \nparticularly audition  (Banks, Connell, 2023), interoception, and emotions (even if especially  \nemotional concepts) (Kousta et al., 2011; Winter, 2023). Yet, consider  that the data leading to this \nview are collected through linguistic means  – questionnaires where participants evaluate the \ncontribution of various dimensions, feature listing tasks in which participants report what comes to \ntheir mind with a given word or sentence.  \nTo better address this question, consider how word acquisition occurs, both for concrete and abstract \nwords. During their first year, infants learn to communicate through spoken language (Kuhl, 2010). \nThis process involves  learning an initial set of concrete words that are associated with physical  \nobjects and events. As infants interact with their environment, they begin to connect the sounds they \nhear and the things they see, touch, and experience (Gopnik, Meltzoff, 1997). Over time, they develop \na basic vocabulary that allows them to express their needs and desires, as well as  to understand the \nlanguage used by others. One of the key factors that facilitate language acquisition is social \ninteraction. Infants learn language through exposure to the language used by those around them, \nparticularly their parents and caregivers (Tomasello, 2003). Adults play a critical role in this process \nby providing the necessary contingencies between words and events, which are culturally determined \nand, in a sense , arbitrary. During this time, the baby also engages in sensory and motor  mapping, \nwhich allows them to develop an understanding of how their senses and movements interact with the \nobjects around them. The initial lexicon plays a crucial role in guiding this mapping process which \nhelps them recognize the unique features of objects and ho w they can be manipulated , ultimately \nleading to the categorization of objects based on their differences (Gopnik, Meltzoff, 1997). This \ndeveloping map of the world also functions as a robust structure for arranging the influx of spoken \ninformation into a logical and cohesive format in a clear and efficient manner.  \nEven if language and social interaction influence the acquisition of all concepts, experiments on real \nor mimicked word acquisition show that linguistic and interactive aspects are important for concrete \nconcepts and crucial for abstract ones. Linguistic labels, explanations, and social interaction facilitate \nin particular learning of abstract words. Infants start to understand the first abstract words at around \n10 and 14 months when they develop social abilities such as engaging in joint action and following \nothers’ gaze (Bergelson, Swingley, 2013). As to word production, abstract concepts are hard words \nand children start to use them quite  late (Bellagamba et al., 2022). To acquire them, previous \nlinguistic knowledge (linguistic acquisition modality) and the support of others are needed  (social \nmetacognition, Villani et al., 2019; Mazzuca et al., 2022).  \nIn sum, the answer is “no”: we do not acquire either concrete or abstract  concepts solely thr ough \nlanguage. Abstract concepts are learned by interacting with others and with the environment, even if \nthe mediation of language is essential. Language plays a major role in abstract concepts’ \nrepresentation, but also sensory and particularly interoceptive modalities do. Importantly, however, \nthis last piece of evidence is mostly collected through language. Hence, we can extract information \non embodied aspects constraining abstract (and concrete) concepts learning through language.  \n3. LLMs, concepts, and language  \nWe have seen how language acquisition occurs in children. We have  also seen the impressive \ncapabilities LLMs have. This leads us to a more general question: to what extent can we say that our \nconceptual representation is language-based? Studies on abstract and concrete concepts, but also, \nmore generally, studies on sensory modalities in concepts, pose a problem. There is no real contrast \nbetween sensorimotor and other body- related information with linguistic information, which are \ninstead correlated. Linguistic information can help us access and retrieve other kinds of information.  \nThe reasons behind this alignment may be different. It might happen, as some have argued, because \nlanguage provides an efficient shortcut to meaning (Connell, 2019), particularly for linguistic tasks \nsuch as speaking, describing, and writing (Barsalou et al., 2008). This might be due to the fact  that \nlanguage is easier to activate. It is also possible that language, beyond  favoring direct meaning \nacquisition, facilitates the learning of complex cognitive capacities, such as reasoning, that allow the \ndetection of contextual aspects of meanings. Alternatively, the alignment might be due to the specific \nevolutionary history of language. When we use words in corpora, these words have emerged through \nan evolutionary history based on interactions with the environment.  \nThus, the intake of language seems more crucial i n some domains, such as abstract ones. However, \nwe have seen that learning typically occurs  also through interaction – linguistic interaction with \nothers, and interaction with the world. And yet, it seems that through language, we can retrieve most \ninformation across domains. How is it possible?  \nTo answer this question, studies with individuals with deprivation of their sensory system are critical. \nRecently, Jonauskaite et al. (2021) showed that red/green color -blind people and controls associate \ncolors with the same emotions, both when the stimuli are words and visual patches . Saysani et al. \n(2021) compared blind and sighted people in their  color ratings on a semantic differential scale \nincluding bipolar items (e.g., happy-sad). Besides individual differen ces, they also found striking  \nsimilarities between (some) blind people and sighted ones. Do these  data suggest that direct \nperceptual information is not necessary to develop color concepts? Some possibilities are open. One \ncould object that, while participants of these studies are deprived of one sensory modality, sight, they \ncan still access information through multiple other sensory modalities. Alternatively, language might \nplay a compensatory function in blind or color-blind people, but this is not the way people typically \nacquire and represent color. Finally, it is possible that, because language offers a rich comprehension \nof the domain, studies on blind people tell us how language grants access to meaning for everybody. \nInterestingly, color words are n ot abstract ones. Similar to abstract concepts, they have only one \ncommon dimension (Borghi, 2022), but they are linked to a sensory modality, thus people typically \nevaluate them as neither very concrete nor very abstract.  \nSuppose, now, that we can access  meaning solely through language, and that this is true also for \nwords that are not abstract, like color ones. In our view, language is capable of offering information \nthat is so rich because it has evolved through bodily and linguistic interactions.  \nLet us assume that a large amount of information can be derived from large  linguistic corpora. Are \nthere aspects of language that linguistic models do not capture? Is there something that is distinctive \nof human experience, not reflected in the output of LLMs? How do they deal with grounding, how \ndo they capture the variability of human experiences? We think at least five aspects are worth \nmentioning.  \n3.1. LLMs are not grounded  \nLLMs use a process called word embedding, which involves trans forming words into a topological \nspace. In this space, each word is rep resented as a point in a high-dimensional area, and the distance \nbetween these points reflects the semantic similarity between the corresp onding words (Mikolov et \nal., 2013). Word embedding is a powerful tool that can provide valuable insights into the relationships \nbetween words. However, these relationships are ultimately based only on patterns of usage (Bengio \net al., 2003). Therefore, word embedding can help us understand how words are used in context, but \nit does not necessarily provide information about how the words pick out the referents they do in the \noutside world. Since the input space of LLMs is so limited, how they can be connected to experiences \nwith the outside world remains unclear. One possible answer is that a fundamental predisposition to \ngrounding is inherent in the internal mechanisms of language and is reflected in the interconnections \nbetween words (Barsalou, 2008). This  predisposition likely arises from language being developed \nthrough interactions between individuals who possess a sense of grounding.  Language is not entirely \narbitrary. Instead, it is grounded in the shared experiences and knowledge of the individuals who use \nit. The principles governing language, including syntax and semantics, are shaped by the way words \nrelate to each other and the context in which they are used.  These rules are built upon the collective \nexperiences and knowledge of a community, which form the basis for a shared language (Tomasello, \n2003). Therefore, LLMs can be grounded by linking their input tokens to the actual events in the real \nworld, which can be seen as a type of retrospective grounding (Harnad, 1990). Two separate \nprocesses are necessary for post-hoc grounding of LLMs to occur during language acquisition. The \nfirst process involves learning the inferential relationships between tokens at the expression level. \nThe second process involves grounding by learning the connections between those tokens and real-\nworld events. However, this process of post-hoc grounding raises various issues. One first limitation \nis that the links to the world are established from a perspective that is outside of the agent, and the \nencoding that defines t he meaning of words is not dependent on the agent’s own experiences or  \nperspective (Buzsáki, 2019). This makes it difficult to consider the acquired connection of the model \nas a proper grounding. The second limitation is that, during the initial training phase, inferential rules \nare established  in a predetermined manner based on a fixed dataset. This limitation  restricts the \nagent’s ability to make inferences and comprehend language , hindering any potential modifications \nto the inferential system when  faced with new events and contexts. Lastly, the two procedures \nimplicated in post-hoc grounding – the acquisition of inferential relationships and their grounding – \nare computationally intensive and may not correspond to the way in which humans naturally acquire \nlanguage during their developmental stages, as previously discussed.  \n3.2. LLMs lack action  \nGenerating and executing action plans that are both physically ground ed and semantically correct is \na significant challenge when using LLMs like GPT-3 for motor planning. To address this challenge, \nresearchers have proposed various methods, including LLM -Planner (Song et al., 2022), PaLM -E \n(Driess et al., 2023), and LLM -brain (Mai et al., 2023), which aim to enhance LLMs with physical \ngrounding and corrective re-prompting to improve plan quality and efficiency. LLMs are useful for  \ncreating high -level actions that define task subgoals, such as “Go to the kitchen” or “Open the \nrefrigerator.” These actions are then translated into  low-level motor commands that specify the \nindividual movements of the  agent, such as “Turn left,” “Move forward,” or “Interact.” However, \nthese methods still require external constraints or supervision to ensure that the motor commands are \nexecutable and correct. One limitation of curr ent approaches is that they do not consider how an \nagent’s motor actions relate to higher -level actions and the perception of affordances, i.e. , of the \ninvitations to act the environment offers us. In order for an agent to truly understand how its actions \naffect its environment and adapt accordingly, it is crucial to incorporate the learning of sensorimotor \naffordances into its training process (Mannella, Tummolini, 2022).  \n3.3. LLMs do not capture pragmatics and paralinguistics aspects  \nPragmatic and paralinguistic aspects directly linked to online interaction  might not be captured by \ncurrent versions of LLMs. For example, when responding to sentences involving abstract concepts, \npeople use more “why” questions and uncertainty expressions, employ more turns (Villani et al., \n2022), and make more metaphorical gestures ( Zdrazilova et al., 2018). These aspects might not be \ncaptured by LLMs; the same is true for gaze exchanges, movement synchrony, etc., that occur during \nreal interaction. Similarly, capturing other pragmatic aspects such as communicative intentions in \nmetaphorical language might represent a challenge for LLMs (Domaneschi, Bambini, 2020). LLMs \nare trained on written databases, where uncertainties are less evident, gestures are not  present, and \nthe focus is more on meaning than on how meaning is conveyed. This does not imply that changing \nthe training set, these functions cannot be implemented, but it will likely not be easy.  \n3.4. LLMs are culturally biased  \nThe representation of meaning offered by LLMs might be biased toward cultures where writing \nplays a prominent role and reflects the opinions and views of Western, Educated, Industrial, Rich, \nand Democrats (WEIRD) participants. Being based on written texts, LLMs do not consider many \nindividual differences and mirror the perspective of younger adults, leaving aside that of infants, \nchildren, and also of older adults, who use few online chats. Again, this does not imply that \nchanging the training set of these functions cannot be implemented, but it does not seem such a \nstraightforward endeavor. Internet distribution itself poses some challenges, as it tends to be \nunevenly accessible across demographic groups and countries (Bender et al., 2021). Studies are \nstarting to investigate how cultural biases are expressed and propagated through LLMs. They \nhighlight that language models tend to mimic the majority viewpoints found on the Internet (Venkit \net al., 2023), misrepresenting those of minorities, possibly due to the use of English training data \nand prompts that could reduce the variability in model responses (Cao et al., 2023;  Naous et al., \n2023). Further evidence comes from studies that compare multi- and mono-lingual LLMs. Touileb \net al. (2022) analyze predictions of six pre-trained language models showing that language-specific \nmodels perform better in identifying occupations that are clearly imbalanced in terms of gender. \nHowever, both language-specific and multilingual models struggle to accurately represent gender-\nbalanced occupations.  \n3.5. LLMs do not capture individual characteristics  \nTo be accessible to different populations, LLMs should consider the needs of different users. But \ndo actual LLMs account for the variability of human experiences? It has been shown that LLMs \ntend to collapse the diversity of judgments into a single opinion approximating an average human \njudgment (Dillion et al., 2023). Notably, these results mostly refer to LLMs that are not state-of-\nthe-art, due to the lack of accessibility to training data and code of these models (OpenAI, 2023). \nPrevious studies show that stereotypes and cultural biases are inherently embedded in language \n(Marinucci, Mazzuca, Gangemi, 2023). LLMs designers aim for universal solutions but, despite the \nefforts in debiasing, actual models still engage in stereotypes (Venkit et al., 2023). Having a large \namount of training data does not guarantee diversity and training data need to be de-biased from \nsets of potentially harmful words. This raises concerns about implementing filtered datasets for \ntraining models (Bender et al., 2021). If it is unknown how training data are filtered, it is \nimpossible to understand which cultural biases the model learned. For example, data shows that the \nBERT model is more likely to associate words conveying negative sentiment with phrases referring \nto individuals with disabilities (Hutchinson et al., 2020). De-biasing approaches may end up \noversimplifying the diversity of each disability experience. In a broader perspective, disability \nintersects many other identities and intersectionality poses another challenge to LLMs to \ndistinguish different possible biases and to not reduce them to one (Guo et al., 2023). A common \napproach to de-bias training corpora is the involvement of human annotators to label possibly \nharmful data. But also, in this case, some concerns arise. Davani and colleagues (2023) investigate \nnormative stereotypes held by novice annotators in hate speech annotations and demonstrate that \nstereotypes embedded in language resources contribute to systematic prediction errors in a hate \nspeech classifier against marginalized groups.  \nExisting models struggle to capture diverse opinions, especially when in contrast with the overall \npersonal ideology, which changes through time and is contextually variable (Hwang et al., 2023). \nLLMs also have low  steerability and misalign with real -world opinions, particularly for \nunderrepresented groups like older adults. Reinforcement Learning from Human Feedback (RLHF) \nmodels face similar issues, collapsing diverse  opinions into a single viewpoint for certain groups. \nThe result is a monolithic representation of a certain group ideology consistent with the demographic \ninformation related to the individuals who are part of the  crowdsourcing process (Santurkar et al., \n2023).  \nState-of-the-art LLMs, despite the attempt to avoid biases, cannot fully  account for cultural and \nindividual variability and do not consider the fact that meaning is socially constructed and negotiated \n(Bender et al., 2021).  \n \n4. LLMs as a tool for research  \nDespite their limitations, LLMs can still prove to be useful for experimental practice, especially in \ncognitive psychology, cognitive science , and neuroscience. LLMs models can be employed to \ncompare their  output to human responses to corroborat e experimental evidence with  human \nparticipants and check for robustness and replicability and study specific domains of interest and tasks \n(Dillion et al., 2023). LLMs are  also well-suited to act as “silicon samples” to explore hypotheses \nwhere experiments with human samples can be costly or disadvantageous or to pilot new experiments \n(Dillion et al., 2023). Finally, LLMs have already been adopted for text analysis (Rathje et al., 2023). \nWith state of-the-art LLMs, this approach is restricted to some limitations. Human communication is \ncollaborative, and a text generated by a language model lacks  grounding in communicative intent \n(Bender et al., 2021). Also, communication involves pragmatic aspects that actual LLMs are not able \nto capture. Finally, as mentioned above, LLMs behavior represents a condensed version of individual \ndifferences. While they reveal impressive capabilities in some areas, in others the performance of \nLLMs is not satisfactory when compared to that of humans. For example, Riccardi and Desai (2023) \ncompared human ratings of the meaningfulness of noun-noun combinations (e.g., beach ball makes \nsense, ball beach does not) with ratings of GPT-3.5, GPT-4, and Bard. Even if GPT-4 outperformed \nthe other two, it still provided anomalous ratings for 20% of the sentences. Importantly, this task does \nnot require reasoning abilities, but simply to understand the meaning of the combination constituents, \nand then use word knowledge to determine whether they make sense or not. Similarly, Jones and \ncolleagues (2022) show that GPT-3 is sensitive to affordances, but only to one-third of the effect of \naffordances on human sensibility judgments. In sum, caution should be used, and the performance of \nLLMs should be validated in relation to the specific selected tasks. Formal tests are critical to \ndetermining in which cases LLMs can be used as valid substitutes for human participants.  \n5. Conclusions  \nIn this paper, we showed that LLMs can have enormous potential not  only as tools that can support \nempirical research but also as models that  can account for human cognition. They challenge current \nembodied views showing the many advantages that learning through language can grant. At the same \ntime, we highlighted many differences in the learning process of humans and LLMs, starting from a \ngrounded perspective. We have prospected the possibility that LLMs might better account for some  \nprocesses, e.g., the formation and use of abstract concepts and we questioned their current capability \nto reproduce the modality in which humans  learn. We still aren’t sure whether these different \nmodalities, one based on sensorimotor and linguistic interaction, and another based exclusively on \nlanguage statistics, can be equated. We also proposed that , if in the future LLMs might arrive at the \nsame output as humans, it is because they rely on amounts of linguistic data much larger than those  \nhumans have at their dispos al, and these data have evolved via embodied interactions. At the same \ntime, the larger the dataset is, the less it will be able to reflect individual and cultural differences. In \nthe end, while LLMs can be currently used as support for research, future research will tell us whether \nthey can be considered credible models of human cognition.  \n \nAcknowledgement  \nWe acknowledge financial support from 1) PNRR MUR project PE0000013-FAIR; 2) PNRR project \nPE08, age.it, spoke 4, and 3) EU project  TRAINCREASE (From social interaction to abstract \nconcepts and words: towards human-centered technology development), Proposal n. 952324.  \n \n \n \n \nReferences  \nAbdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., Søgaard, A.  (2021). Can \nlanguage models encode perceptual structure without ground ing? A case study in color. arXiv \npreprint arXiv:2109.06129.  \nBanks, B., Connell, L. (2023). Multi-dimensional sensorimotor grounding of concrete and abstract \ncategories. Philosophical Transactions of the Royal Society B, 378(1870), 20210366. \nBarca, L., Mazzuca, C., Borghi, A.M. (2020). Overusing the pacifier during infancy sets a footprint \non abstract words processing. Journal of Child Language, 47(5), pp. 1084-1099.  \nBarsalou, L.W. (2008). Grounded cognition. Annual Review of Psychology, 59, pp. 617-645.  \nBellagamba, F., Borghi, A.M., Mazzuca, C., Pecora, G., Ferrara, F., Fogel, A.  (2022). Abstractness \nemerges progressively over the second year of life.  Scientific Reports, 12(1), 20940.  \nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S. (2021). On the Dangers of Stochastic \nParrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM conference on \nfairness, accountability, and transparency, pp. 610-623.  \nBengio, Y., Ducharme, R., Vincent, P., Janvin, C. (2003). A neural probabilistic language model. \nJournal of Machine Learning Research, 3, pp. 1137-1155. Bergelson, E., Swingley, D. (2013). The \nacquisition of abstract words by young infants. Cognition, 127(3), pp. 391-397.  \nBorghi, A.M. (2022). Concepts for which we need others more: the case of abstract concepts. \nCurrent directions in psychological science, 31(3), pp.  238-246.  \nBorghi, A.M. (2023). The Freedom of Words: Abstractness and the Power of Language. \nCambridge: Cambridge University Press.  \nBorghi, A.M., Barca, L., Binkofski, F., Castelfranchi, C., Pezzulo, G., Tummolini,  L. (2019). \nWords as social tools: Language, sociality and inner grounding in abstract concepts. Physics of life \nreviews, 29, pp. 120-153.  \nBorghi, A.M., Binkofski, F., Castelfranchi, C., Cimatti, F., Scorolli, C., Tummo lini, L. (2017). The \nchallenge of abstract concepts. Psychological Bulletin, 143(3), pp. 263 ss.  \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., ..., Amodei, D. (2020). \nLanguage models are few-shot learners. Advances in neural information processing systems, 33, \npp. 1877-1901.  \nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., \nLi, Y., Lundberg, S. et al. (2023). Sparks of artificial general intelligence: Early experiments with \nGPT-4. arXiv preprint arX iv:2303.12712.  \nBuzsáki, G. (2019). The Brain from Inside Out. Oxford: Oxford University Press. Cao, Y., Zhou, \nL., Lee, S., Cabello, L., Chen, M., Hershcovich, D. (2023).  Assessing cross-cultural alignment \nbetween chatgpt and human societies:  An empirical study. arXiv preprint arXiv:2303.17466.  \nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ..., Fiedel, N. (2022). \nPalm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.  \nConca, F., Borsa, V.M., Cappa, S.F., Catricalà, E. (2021). The multidimensional ity of abstract \nconcepts: A systematic review. Neuroscience, Biobehavioral Reviews, 127, pp. 474-491.  \nConnell, L. (2019). What have labels ever done for us? The linguistic shortcut in conceptual \nprocessing. Language, Cognition and Neuroscience, 34(10), pp. 1308-1318.  \nCreswell, A., Shanahan, M., Higgins, I. (2022). Selection-inference: Exploiting large language \nmodels for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.  \nDavani, A.M., Atari, M., Kennedy, B., Dehghani, M. (03 2023). Hate Speech Classifiers Learn \nNormative Social Stereotypes. Transactions of the As sociation for Computational Linguistics, 11, \npp. 300-319, doi:10.1162/ tacl_a_00550.  \nDesai, R.H., Reilly, M., van Dam, W. (2018). The multifaceted abstract brain.  Philosophical \nTransactions of the Royal Society B: Biological Sciences, 373(1752), 20170122.  \nDillion, D., Tandon, N., Gu, Y., Gray, K. (2023). Can AI language models replace human \nparticipants?. Trends in Cognitive Sciences.  \nDomaneschi, F., Bambini V. (2020) Pragmatic Competence. In C. Pavese, E.  Fridland (Eds.), \nRoutledge Handbook of Skill and Expertise, Abingdon:  Routledge.  \nDove, G. (2022). Abstract concepts and the embodied mind: rethinking grounded cognition. \nOxford: Oxford University Press.  \nDriess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., ..., Florence, P. (2023). \nPalm-e: An embodied multimodal language model.  arXiv preprint arXiv:2303.03378.  \nEveraert, M.B., Huybregts, M.A., Chomsky, N., Berwick, R.C., Bolhuis, J.J.  (2015). Structures, \nnot strings: Linguistics as part of the cognitive sciences.  Trends in cognitive sciences, 19(12), pp. \n729-743.  \nFini, C., Zannino, G.D., Orsoni, M., Carlesimo, G.A., Benassi, M., Borghi,  A.M. (2022). \nArticulatory suppression delays processing of abstract words:  The role of inner speech. Quarterly \nJournal of Experimental Psychology, 75(7), pp. 1343-1354.  \nGallese, V., Lakoff, G. (2005). The brain’s concepts: The role of the senso ry-motor system in \nconceptual knowledge. Cognitive neuropsychology, 22(3-4), pp. 455-479.  \nGlenberg, A.M., Robertson, D.A. (2000). Symbol grounding and meaning: A comparison of high-\ndimensional and embodied theories of meaning. Journal of memory and language, 43(3), pp. 379-\n401.  \nGopnik, A., Meltzoff, A.N. (1997). Words, thoughts, and theories. Cambridge, MA: MIT Press.  \nGünther, F., Rinaldi, L., Marelli, M. (2019). Vector-space models of semantic representation from a \ncognitive perspective: A discussion of common mis conceptions. Perspectives on Psychological \nScience, 14(6), pp. 1006-1033.  \nGuo, W., Caliskan, A. (2021, July). Detecting emergent intersectional biases:  Contextualized word \nembeddings contain a distribution of human-like biases. In Proceedings of the 2021 AAAI/ACM \nConference on AI, Ethics, and Society, pp. 122-133.  \nHarnad, S. (1990). The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3), pp. \n335-346.  \nHwang, E., Majumder, B.P., Tandon, N. (2023). Aligning Language Models to User Opinions. \narXiv preprint arXiv:2305.14929.  \nHutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y., Denuyl, S.  (2020). Social \nbiases in NLP models as barriers for persons with disabilities.  arXiv preprint arXiv:2005.00813. \nJonauskaite, D., Sutton, A., Cristianini, N., Mohr, C. (2021). English colour terms carry gender and \nvalence biases: A corpus study using word embed dings. PloS one, 16(6), e0251559.  \nJones, C.R., Chang, T.A., Coulson, S., Michaelov, J.A., Trott, S., Bergen, B.  (2022). Distributional \nsemantics still can’t account for affordances. In Proceedings of the Annual Meeting of the \nCognitive Science Society, vol.  44, n. 44.  \nKosinski, M. (2023). Theory of mind may have spontaneously emerged in large language models. \narXiv preprint arXiv:2302.02083.  \nKousta, S.T., Vigliocco, G., Vinson, D.P., Andrews, M., Del Campo, E. (2011).  The representation \nof abstract words: why emotion matters. Journal of Experimental Psychology: General, 140(1), pp. \n14 ss.  \nKuhl, P.K. (2010). Brain mechanisms in early language acquisition. Neuron, 67(5), pp. 713-727.  \nLandauer, T.K., Dumais, S.T. (1997). A solution to Plato’s problem: The latent semantic analysis \ntheory of acquisition, induction, and representation of knowledge. Psychological review, 104(2), \npp. 211 ss.  \nLi, B.Z., Nye, M., Andreas, J. (2021). Implicit representations of meaning in neural language \nmodels. arXiv preprint arXiv:2106.00737.  \nLouwerse, M.M. (2018). Knowing the meaning of a word by the linguistic and perceptual company \nit keeps. Topics in Cognitive Science, 10(3), pp. 573-589. Mahowald, K., Ivanova, A.A., Blank, \nI.A., Kanwisher, N., Tenenbaum, J.B., Fedorenko, E. (2023). Dissociating language and thought in \nlarge language models: A cognitive perspective. arXiv preprint arXiv:2301.06627. Mai, J., Chen, \nJ., Li, B.Z., Qian, G., Elhoseiny, M., Ghanem, B. (2023). LLM as A Robotic Brain: Unifying \nEgocentric Memory and Control. arXiv pre print arXiv:2304.09349.  \nMannella, F., Tummolini, L. (2022). Kick-starting grounded cognition with intrinsic motivation: A \nprocess model of concept formation in continuous domains. Philosophical Transactions of the \nRoyal Society B, 378(20210370).  \nMarinucci, L., Mazzuca, C., Gangemi (2023). Exposing implicit biases and ste reotypes in human \nand artificial intelligence: State of the art and challenges with a focus on gender. AI & Society, \n38(2), pp. 747-761.  \nMazzuca, C., Falcinelli, I., Michalland, A.H., Tummolini, L., Borghi, A.M.  (2022). Bodily, \nemotional, and public sphere at the time of COVID-19. An investigation on concrete and abstract \nconcepts. Psychological research, 86(7), pp. 2266-2277.  \nMikolov, T., Chen, K., Corrado, G., Dean, J. (2013). Efficient estimation of word representations in \nvector space. arXiv preprint arXiv:1301.3781. Naous, T., Ryan, M.J., Xu, W. (2023). Having Beer \nafter Prayer? Measuring   \nCultural Bias in Large Language Models. arXiv preprint arXiv:2305.14456. Ouyang, L., Wu, J., \nJiang, X., Almeida, D., Wainwright, C., Mishkin, P., ..., Lowe, R. (2022). Training language \nmodels to follow instructions with human feedback. Advances in Neural Information Processing \nSystems, 35, pp. 27730-27744.  \nPatel, R., Pavlick, E. (2022). Mapping language models to grounded conceptual spaces. In \nInternational Conference on Learning Representations. Online, 25-29 April 2022, \nhttps://openreview.net/ forum?id=gJcEM8sxHK. Petroni, F., Rocktaschel, T., Lewis, P., Bakhtin, \nA., Wu, Y., Miller, A.H., \nRiedel, S. (2019). Language models as knowledge bases? arXiv preprint arXiv:1909.01066.  \nPexman, P.M., Diveica, V., Binney, R.J. (2023). Social semantics: the organi zation and grounding \nof abstract concepts. Philosophical Transactions of the Royal Society B, 378(1870), 20210363.  \nPinker, S., Prince, A. (1988). On language and connectionism: Analysis of a parallel distributed \nprocessing model of language acquisition. Cognition, 28(1-2), pp. 73-193.  \nPulvermüller, F., Fadiga, L. (2010). Active perception: Sensorimotor circuits as a cortical basis for \nlanguage. Nature reviews neuroscience, 11(5), pp. 351-360. Rathje, S., Mirea, D., Sucholutsky, I., \nMarjieh, R., Robertson, C., Van Bavel, J.J. (2023, May 19). GPT is an effective tool for \nmultilingual psychological text analysis, https://doi.org/10.31234/osf.io/sekf5.  \nRiccardi, N., Desai, R.H. (2023). The Two Word Test: A Semantic Benchmark for Large Language \nModels. arXiv preprint arXiv:2306.04610. Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, \nP., Hashimoto, T.  (2023). Whose opinions do language models reflect?. arXiv preprint arX \niv:2303.17548.  \nSaysani, A., Corballis, M.C., Corballis, P.M. (2021). Seeing colour through language: Colour \nknowledge in the blind and sighted. Visual Cognition, 29(1), pp. 63-71.  \nSøgaard, A. (2023). Grounding the vector space of an octopus: Word meaning from raw text. \nMinds and Machines, 1-22.  \nSong, C.H., Wu, J., Washington, C., Sadler, B.M., Chao, W.L., Su, Y. (2022).  Llm-planner: Few-\nshot grounded planning for embodied agents with large language models. arXiv preprint \narXiv:2212.04088.  \nTalmor, A., Elazar, Y., Goldberg, Y., Berant, J. (2020). oLMpics-on what lan guage model pre-\ntraining captures. Transactions of the Association for Computational Linguistics, 8, pp. 743-758.  \nThibault, S., Py, R., Gervasi, A.M., Salemme, R., Koun, E., Lövden, M., Bou lenger, V., Roy, \nA.C.,Brozzoli, C. (2021). Tool use and language share syntactic processes and neural patterns in \nthe basal ganglia. Science, 374(6569), eabe0874.  \nTomasello, M. (2003). Constructing a language: A usage-based theory of lan guage acquisition. \nCambridge, MA: Harvard University Press. Touileb, S., Øvrelid, L., Velldal, E. (2022, July). \nOccupational biases in Norwe gian and multilingual language models. In Proceedings of the 4th \nWorkshop on Gender Bias in Natural Language Processing (GeBNLP), pp. 200-211. Touvron, H., \nLavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T.,  Rozière, B., Goyal, N., Hambro, E., \nAzhar, F., Rodriguez, A., Joulin, A.,  Grave, E., Lample, G. (2023). LLaMA: Open and Efficient \nFoundation Language Models. arXiv, preprint arXiv:2302.13971.  \nVaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., \nPolosukhin, I. (2017). Attention is all you need. Advances in neural information processing \nsystems, 30.  \nVenkit, P.N., Gautam, S., Panchanadikar, R., Wilson, S. (2023). Nationality Bias in Text \nGeneration. arXiv preprint arXiv:2302.02463.  \nVillani, C., Lugli, L., Liuzza, M.T., Borghi, A.M. (2019). Varieties of abstract concepts and their \nmultiple dimensions. Language and Cognition, 11(3),  pp. 403-430.  \nVillani, C., Orsoni, M., Lugli, L., Benassi, M., Borghi, A.M. (2022). Abstract and concrete \nconcepts in conversation. Scientific Reports, 12(1), 17572. Warstadt, A., Bowman, S.R. (2022). \nWhat artificial neural networks can tell us about human language acquisition. arXiv preprint \narXiv:2208.07998. Warstadt, A., Parrish, A., Liu, H., Mohananey, A., Peng, W., Wang, S-F., Bow \nman, S.R. (2020). BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transactions \nof the Association for Computational Linguistics, 8, pp. 377-392, doi: 10.1162/tacl_a_00321.  \nWinter, B. (2023). Abstract concepts and emotion: cross-linguistic evidence and arguments against \naffective embodiment. Philosophical Transactions of the Royal Society B, 378(1870), 20210368.  \nZdrazilova, L., Sidhu, D.M., Pexman, P.M. (2018). Communicating abstract meaning: concepts \nrevealed in words and gestures. Philosophical Transactions of the Royal Society B: Biological \nSciences, 373(1752), 20170138.  \n \n \n ",
  "topic": "Cognition",
  "concepts": [
    {
      "name": "Cognition",
      "score": 0.6532440185546875
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5922071933746338
    },
    {
      "name": "Pragmatics",
      "score": 0.5181073546409607
    },
    {
      "name": "Action (physics)",
      "score": 0.5100281834602356
    },
    {
      "name": "Psychology",
      "score": 0.47404050827026367
    },
    {
      "name": "Language acquisition",
      "score": 0.4175511598587036
    },
    {
      "name": "Cognitive science",
      "score": 0.4079045057296753
    },
    {
      "name": "Linguistics",
      "score": 0.34199726581573486
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210166865",
      "name": "Institute of Cognitive Sciences and Technologies",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I861853513",
      "name": "Sapienza University of Rome",
      "country": "IT"
    }
  ]
}