{
  "title": "Generalization through Memorization: Nearest Neighbor Language Models",
  "url": "https://openalex.org/W2988841832",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288544679",
      "name": "Khandelwal, Urvashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202210949",
      "name": "Levy, Omer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223899260",
      "name": "Jurafsky, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751234931",
      "name": "Zettlemoyer Luke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2464276071",
      "name": "Lewis, Mike",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2798540241",
    "https://openalex.org/W2964318358",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W1551773846",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2583010282",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2962782699",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2788330850",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963380118",
    "https://openalex.org/W2793165286",
    "https://openalex.org/W2788475065"
  ],
  "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
  "full_text": "Published as a conference paper at ICLR 2020\nGENERALIZATION THROUGH MEMORIZATION :\nNEAREST NEIGHBOR LANGUAGE MODELS\nUrvashi Khandelwal†∗, Omer Levy‡, Dan Jurafsky†, Luke Zettlemoyer‡& Mike Lewis‡\n†Stanford University\n‡Facebook AI Research\n{urvashik,jurafsky}@stanford.edu\n{omerlevy,lsz,mikelewis}@fb.com\nABSTRACT\nWe introduce kNN-LMs, which extend a pre-trained neural language model (LM)\nby linearly interpolating it with a k-nearest neighbors ( kNN) model. The near-\nest neighbors are computed according to distance in the pre-trained LM embed-\nding space, and can be drawn from any text collection, including the original LM\ntraining data. Applying this augmentation to a strong W IKITEXT -103 LM, with\nneighbors drawn from the original training set, ourkNN-LM achieves a new state-\nof-the-art perplexity of 15.79 – a 2.9 point improvement with no additional train-\ning. We also show that this approach has implications for efﬁciently scaling up to\nlarger training sets and allows for effective domain adaptation, by simply varying\nthe nearest neighbor datastore, again without further training. Qualitatively, the\nmodel is particularly helpful in predicting rare patterns, such as factual knowl-\nedge. Together, these results strongly suggest that learning similarity between se-\nquences of text is easier than predicting the next word, and that nearest neighbor\nsearch is an effective approach for language modeling in the long tail.\n1 I NTRODUCTION\nNeural language models (LMs) typically solve two subproblems: (1) mapping sentence preﬁxes to\nﬁxed-sized representations, and (2) using these representations to predict the next word in the text\n(Bengio et al., 2003; Mikolov et al., 2010). We present a new language modeling approach that is\nbased on the hypothesis that the representation learning problem may be easier than the prediction\nproblem. For example, any English speaker knows that Dickens is the author of and Dickens wrote\nwill have essentially the same distribution over the next word, even if they do not know what that\ndistribution is. We provide strong evidence that existing language models, similarly, are much better\nat the ﬁrst problem, by using their preﬁx embeddings in a simple nearest neighbor scheme that\nsigniﬁcantly improves overall performance.\nWe introduce kNN-LM, an approach that extends a pre-trained LM by linearly interpolating its next\nword distribution with a k-nearest neighbors ( kNN) model. The nearest neighbors are computed\naccording to distance in the pre-trained embedding space and can be drawn from any text collec-\ntion, including the original LM training data. This approach allows rare patterns to be memorized\nexplicitly, rather than implicitly in model parameters. It also improves performance when the same\ntraining data is used for learning the preﬁx representations and the kNN model, strongly suggesting\nthat the prediction problem is more challenging than previously appreciated.\nTo better measure these effects, we conduct an extensive empirical evaluation. Applying our kNN\naugmentation to a strong W IKITEXT -103 LM using only the original dataset achieves a new state-\nof-the-art perplexity of 15.79 – a 2.86 point improvement over the base model (Baevski & Auli,\n2019) – with no additional training. We also show that the approach has implications for efﬁciently\nscaling up to larger training sets and allows for effective domain adaptation, by simply varying the\nnearest neighbor datastore. Training a model on 100-million tokens and using kNN search over a\n3-billion token dataset can outperform training the same model on all 3-billion tokens, opening a\n∗Work done while the ﬁrst author was interning at Facebook AI Research.\n1\narXiv:1911.00172v2  [cs.CL]  15 Feb 2020\nPublished as a conference paper at ICLR 2020\nObama was senator for\nBarack is married to\nObama was born in\n…\nObama is a native of\nTraining Contexts\nIllinois\nMichelle\nHawaii\n…\nHawaii\nTargets Representations\n4\n100\n5\n…\n3\nDistances\n0.7\n0.2\n0.1\nNearest k\nHawaii\nIllinois\nHawaii\nNormalization\nHawaii\nIllinois\nHawaii\n3\n4\n5\n0.8\n0.2\nAggregation\nHawaii\nIllinois\nObama’s birthplace is\nTest Context\n?\nTarget Representation\n0.6\n0.2\n…\nInterpolation\nHawaii\nIllinois\n…\n0.2\n0.2\n…\nClassiﬁcation\nHawaii\nIllinois\n…\n…\nFigure 1: An illustration of kNN-LM. A datastore is constructed with an entry for each training set\ntoken, and an encoding of its leftward context. For inference, a test context is encoded, and the k\nmost similar training contexts are retrieved from the datastore, along with the corresponding targets.\nA distribution over targets is computed based on the distance of the corresponding context from the\ntest context. This distribution is then interpolated with the original model’s output distribution.\nnew path for efﬁciently using large datasets in language models. Similarly, adding out-of-domain\ndata to the datastore makes a single LM useful across multiple domains, again without further train-\ning. Qualitatively, we ﬁnd the model is particularly helpful for long-tail patterns, such as factual\nknowledge, which might be easier to access via explicit memory.\n2 N EAREST NEIGHBOR LANGUAGE MODELING\nLanguage models (LMs) assign probabilities to sequences. Given a context sequence of tokens\nct = (w1,...w t−1), autoregressive LMs estimate p(wt|ct), the distribution over the target token\nwt.\nThe kNN-LM involves augmenting such a pre-trained LM with a nearest neighbors retrieval mech-\nanism, without any additional training (the representations learned by the LM remain unchanged).\nThis can be done with a single forward pass over a text collection (potentially including the original\nLM training set), where the resulting context-target pairs are stored in a key-value datastore that is\nqueried during inference, as illustrated in Figure 1.\nDatastore Let f(·) be the function that maps a context cto a ﬁxed-length vector representation\ncomputed by the pre-trained LM. For instance, in a Transformer LM, f(c) could map cto an inter-\nmediate representation that is output by an arbitrary self-attention layer. Then, given thei-th training\nexample (ci,wi) ∈D, we deﬁne the key-value pair (ki,vi), where the key ki is the vector represen-\ntation of the context f(ci) and the value vi is the target word wi. The datastore (K,V) is thus the set\nof all key-value pairs constructed from all the training examples in D:\n(K,V) ={(f(ci),wi)|(ci,wi) ∈D} (1)\nInference At test time, given the input context xthe model generates the output distribution over\nnext words pLM(y|x) and the context representationf(x). The model queries the datastore withf(x)\nto retrieve its k-nearest neighbors Naccording to a distance function d(·,·) (squared L2 distance\nin our experiments, making the similarity function an RBF kernel).Then, it computes a distribution\nover neighbors based on a softmax of their negative distances, while aggregating probability mass\nfor each vocabulary item across all its occurrences in the retrieved targets (items that do not appear\nin the retrieved targets have zero probability):\npkNN(y|x) ∝\n∑\n(ki,vi)∈N\n1 y=vi exp(−d(ki,f(x))) (2)\nFinally, we follow Grave et al. (2017a) and interpolate the nearest neighbor distribution pkNN with\nthe model distribution pLM using a tuned parameter λto produce the ﬁnal kNN-LM distribution:\np(y|x) =λpkNN(y|x) + (1−λ) pLM(y|x) (3)\n2\nPublished as a conference paper at ICLR 2020\nImplementation The datastore contains an entry for each target in the training set, which for LMs\ncan be up to billions of examples. To search over this large datastore, we use FAISS (Johnson et al.,\n2017), an open source library for fast nearest neighbor retrieval in high dimensional spaces. FAISS\nspeeds up search by clustering the keys and looking up neighbors based on the cluster centroids,\nwhile reducing memory usage by storing compressed versions of the vectors. We found in pre-\nliminary experiments that using L2 distance for FAISS retrieval results in better performance for\nkNN-LM, compared to inner product distance.\nRelated Cache Models Prior work (Grave et al., 2017c; Merity et al., 2017) used a similar ap-\nproach to compute similarity to the previous hidden states of test documents, making it easier to\ncopy rare vocabulary items from the recent past. Such techniques have been less popular since the\ndevelopment of Transformers (Vaswani et al., 2017), which can learn to copy recent words using\nself-attention; in Section 4.1, we observe relatively small gains from caching recent items in the\nsame test document `a la Grave et al. (2017c). Most relatedly, Grave et al. (2017a) describe anonline\nlanguage model using nearest neighbor search over all previous hidden states, to improve domain\nadaptation. In our work, we only save training data, with the goal of explicitly memorizing training\nexamples to better generalize to similar cases at test time.\n3 E XPERIMENTAL SETUP\nData Experiments in this paper use the following English corpora:\nWIKITEXT -103 is a standard benchmark by Merity et al. (2017) for autoregressive language mod-\neling with a 250K word-level vocabulary. It consists of 103M tokens of Wikipedia in the training\nset and 250K tokens in each of the development and test sets.\nBOOKS is the Toronto Books Corpus (Zhu et al., 2015), containing 0.7B. Complete books are held\nout for validation/test.\nWIKI -3B is English Wikipedia, containing about 2.87B tokens. Whole articles are held out for\nvalidation/test.\nWIKI -100M is a random 100M token subset of W IKI -3B, consisting of complete articles.\nExcept for W IKITEXT -103, text is tokenized using the byte-pair encoding (Sennrich et al., 2015)\nwith the 29K subword vocabulary from BERT (Devlin et al., 2019).\nModel Architecture kNN-LM is compatible with any model that produces ﬁxed size context\nrepresentations. We use decoder-only Transformers (Vaswani et al., 2017) for language modeling,\nwhich are the current state of the art. Since the kNN-LM makes no changes to the underlying\nLM, we take the exact architecture and optimization described by Baevski & Auli (2019) and use\nit to create a kNN-LM for inference. This model consists of 16 layers, each with 16 self-attention\nheads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers, amounting to\n247M trainable parameters. It processes 3072 tokens of context per example for W IKITEXT -103\nand 1024 tokens for the rest of the corpora. Following Baevski & Auli (2019), we use adaptive\ninputs and an adaptive softmax (Grave et al., 2017b) with tied weights (Press & Wolf, 2017) for\nthe W IKITEXT -103 experiments. On other datasets we do not use adaptive inputs or an adaptive\nsoftmax.\nEvaluation LMs are trained to minimize the negative log-likelihood of the training corpus, and\nevaluated by perplexity (exponentiated negative log-likelihood) on held out data. Following Baevski\n& Auli (2019), 512 tokens are scored per test example, but up to 2560 tokens of extra prior context\nis provided for W IKITEXT -103 and up to 512 tokens of extra prior context is provided for the rest\nof the corpora.\nkNN-LM The keys used for kNN-LM are the 1024-dimensional representations fed to the feed-\nforward network in the ﬁnal layer of the Transformer LM (after self-attention and layernorm; see\nSection 5 for further explanation). We perform a single forward pass over the training set with the\ntrained model, in order to save the keys and values. During this forward pass, each target token is\nprovided a minimum of 1536 tokens of prior context for W IKITEXT -103 and a minimum of 512\n3\nPublished as a conference paper at ICLR 2020\nModel Perplexity ( ↓) # Trainable Params\nDev Test\nBaevski & Auli (2019) 17.96 18.65 247M\n+Transformer-XL (Dai et al., 2019) - 18.30 257M\n+Phrase Induction (Luo et al., 2019) - 17.40 257M\nBase LM (Baevski & Auli, 2019) 17.96 18.65 247M\n+kNN-LM 16.06 16.12 247M\n+Continuous Cache (Grave et al., 2017c) 17.67 18.27 247M\n+kNN-LM + Continuous Cache 15.81 15.79 247M\nTable 1: Performance on W IKITEXT -103. The kNN-LM substantially outperforms existing work.\nGains are additive with the related but orthogonal continuous cache, allowing us to improve the\nbase model by almost 3 perplexity points with no additional training. We report the median of three\nrandom seeds.\nModel Perplexity ( ↓) # Trainable Params\nDev Test\nBase LM (Baevski & Auli, 2019) 14.75 11.89 247M\n+kNN-LM 14.20 10.89 247M\nTable 2: Performance on BOOKS , showing that kNN-LM works well in multiple domains.\ntokens for the rest of the corpora. A FAISS index is then created using 1M randomly sampled keys\nto learn 4096 cluster centroids. For efﬁciency, keys are quantized to 64-bytes. During inference,\nwe retrieve k = 1024 neighbors, and the index looks up 32 cluster centroids while searching for\nthe nearest neighbors. For W IKITEXT -103 experiments, we compute squared L2 distances with\nfull precision keys, but for the other datasets we use the FAISS L2 distances (not squared) between\nquantized keys directly, for faster evaluation. We tune the interpolation parameterλon the validation\nset.1\nComputational Cost Although the kNN-LM requires no training given an existing LM, it does\nadd some other computational overheads. Storing the keys and values requires a single forward pass\nover the training set, which amounts to a fraction of the cost of training for one epoch on the same\nexamples. Once the keys are saved, for WIKITEXT -103 building the cache with 103M entries takes\nroughly two hours on a single CPU. Finally, running on the validation set took approximately 25\nminutes when retrieving 1024 keys. While the cost of building a large cache grows linearly in the\nnumber of entries, it is trivial to parallelize and requires no GPU-based training.\n4 E XPERIMENTS\n4.1 U SING THE TRAINING DATA AS THE DATASTORE\nWe ﬁrst experiment with creating a datastore from the same data used to train the LM. Table 1 shows\nthat kNN-LM improves perplexity on WIKITEXT -103 from 18.65 (Baevski & Auli, 2019) to a new\nstate-of-the-art of 16.12. We also provide reported perplexities from two other recent models that\nalso build upon Baevski and Auli’s, suggesting that further improvements may be possible by aug-\nmenting the kNN-LM with these techniques. We compare with models trained only on the standard\ntraining set, but recent work has shown performance can be improved by training on additional data,\nfrom either the test set (Krause et al., 2019) or large amounts of web text (Shoeybi et al., 2019).\nWe also experiment with a continuous cache model, a related but orthogonal technique from Grave\net al. (2017c), in which the model saves and retrieves neighbors from earlier in the test document,\n1Code is available at: https://github.com/urvashik/knnlm\n4\nPublished as a conference paper at ICLR 2020\nTraining Data Datastore Perplexity ( ↓)\nDev Test\nWIKI -3B - 16.11 15.17\nWIKI -100M - 20.99 19.59\nWIKI -100M W IKI -3B 14.61 13.73\nTable 3: Experimental results on W IKI -3B. The model trained on 100M tokens is augmented with\na datastore that contains about 3B training examples, outperforming the vanilla LM trained on the\nentire WIKI -3B training set.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nSize of datastore (in billions)\n14\n15\n16\n17\n18\n19\n20\n21Perplexity\nWiki-100M\nWiki-3B\nkNN-LM (Wiki-100M + kNN)\n(a) Effect of datastore size on perplexities.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nSize of datastore (in billions)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nOptimal \n(interpolation parameter)\nkNN-LM (Wiki-100M + kNN) (b) Tuned values of λfor different datastore sizes.\nFigure 2: Varying the size of the datastore. (a) Increasing the datastore size monotonically improves\nperformance, and has not saturated even at about 3B tokens. A kNN-LM trained on 100M tokens\nwith a datastore of 1.6B tokens already outperforms the LM trained on all 3B tokens. (b) The optimal\nvalue of λincreases with the size of the datastore.\nrather than the training set. Gains from interpolating with the continuous cache are smaller than\nreported in the original setting that used LSTMs, perhaps because self-attentive language models\ncan learn to perform such queries. Improvements from the continous cache are additive with the\nkNN-LM, pushing our state-of-the-art result to 15.79, a gain of 2.86 over the base model.\nFinally, we repeat the experiment using text from a different domain, B OOKS , to control for the\npossibility that encyclopedic Wikipedia text is somehow uniquely good for caching. Table 2 shows\nan improvement in test set perplexity from 11.89 to 10.89, suggesting that this is not the case.\n4.2 M ORE DATA WITHOUT TRAINING\nSection 4.1 has shown that retrieving neighbors from the training data can signiﬁcantly improve\nlanguage modeling performance. This raises the question: can retrieving nearest neighbors from\ndata be a substitute for training on it? To test this, we train a LM on WIKI -100M and use it to build\na datastore from W IKI -3B, a corpus 30 times larger than the training set. We then compare this\nkNN-LM to a vanilla LM trained on the entire WIKI -3B corpus. 2\nTable 3 shows that, as expected, the model trained on 3B tokens dramatically outperforms the model\ntrained on 100M tokens, improving perplexity from 19.59 to 15.17. However, adding nearest neigh-\nbors retrieval over those 3B examples to the model trained on 100M tokens improves perplexity\nfrom 19.59 to 13.73; i.e. retrieving nearest neighbors from the corpus outperforms training on it .\nThis result suggests that rather than training language models on ever larger datasets, we can use\nsmaller datasets to learn representations and augment them with kNN-LM over a large corpus.\n2The original LM (Baevski & Auli, 2019) was trained for 286K steps on a corpus of similar size to W IKI -\n100M. When scaling up to WIKI -3B, we tuned only the number of updates on the validation set and found that\ntraining for 572K steps (double) produces a slightly stronger baseline.\n5\nPublished as a conference paper at ICLR 2020\nTraining Data Datastore Perplexity ( ↓)\nDev Test\nWIKI -3B - 37.13 34.84\nBOOKS - 14.75 11.89\nWIKI -3B B OOKS 24.85 20.47\nTable 4: Domain adaptation experiments, with results on B OOKS . Adding an in-domain datastore\nto a Wikipedia-trained model improves results by 23 points, approaching in-domain training.\nMulti Headed \nSelf Attention\nFeed Forward Network\n+\nLayer Norm\nLayer Norm\n+\nFigure 3: Transformer LM layer.\nKey Type Dev ppl. ( ↓)\nNo datastore 17.96\nModel output 17.07\nModel output layer normalized 17.01\nFFN input after layer norm 16.06\nFFN input before layer norm 17.06\nMHSA input after layer norm 16.76\nMHSA input before layer norm 17.14\nTable 5: W IKITEXT -103 validation results using dif-\nferent states from the ﬁnal layer of the LM as the rep-\nresentation function f(·) for keys and queries. We re-\ntrieve k=1024 neighbors and λis tuned for each.\nTo understand how the amount of data used forkNN retrieval affects performance, we use the WIKI -\n100M model to create datastores using different amounts of randomly sampled data from WIKI -3B.\nFigure 2a shows that using only 1.6B examples for the datastore already surpasses the performance\nof the model trained on all of W IKI -3B. In addition, performance does not saturate at 3B examples\nin the datastore, suggesting that growing the datastore more could lead to further gains. Figure 2b\nshows the model relies more on the kNN component as the size of the datastore increases.\n4.3 D OMAIN ADAPTATION\nWe also experiment with domain adaptation by creating a datastore on the target domain training\nset. Table 4 shows that an in-domain LM on BOOKS has a relatively low perplexity (11.89), while a\nmodel trained on WIKI -3B performs poorly on the BOOKS domain (34.84 perplexity). Adding kNN\nsearch over BOOKS to the WIKI -3B model reduces perplexity by 14 points (to 20.47), demonstrating\nthat kNN-LM allows a single model to be useful in multiple domains, by simply adding a datastore\nper domain.\n5 T UNING NEAREST NEIGHBOR SEARCH\nWhile the kNN-LM is conceptually straightforward, and requires no additional training, a number of\nhyperparameters are introduced for nearest neighbor search. We experiment with different choices\nhere.\nKey Function For similarity search, we extract a representation of contextcusing an intermediate\nstate of the LM f(c). Transformers compute a number of different intermediate states, and we com-\npare several choices depicted in Figure 3, with results shown in Table 5. While all the instantiations\nof f we tried are helpful, we achieved the largest improvement by using the input to the ﬁnal layer’s\nfeedforward network. We also observe that normalized representations (i.e. taken immediately af-\nter the layer norm) perform better. Repeating the experiment on the second-last transformer layer\nshowed similar trends with slightly worse results (not shown), suggesting that the feedforward layer\nmight be focusing more on the prediction problem, while the onus of representing the input falls\nmore on the self-attention layer.\n6\nPublished as a conference paper at ICLR 2020\n1 2 8 64 256 1024\nk (# nearest neighbors)\n16.2\n16.4\n16.6\n16.8\n17.0\n17.2\n17.4\n17.6Perplexity\nkNN-LM on Wikitext-103\nFigure 4: Effect of the number of nearest neigh-\nbors returned per word on W IKITEXT -103 (val-\nidation set). Returning more entries from the\ndatastore monotonically improves performance.\n0.0 0.2 0.4 0.6 0.8\n (interpolation parameter)\n14\n15\n16\n17\n18In-domain Perplexity\nBooks (In-domain)\nWiki-3B + Books Datastore\n(Domain Adaptation)\n26\n28\n30\n32\n34\n36\nDomain Adaptation Perplexity\nFigure 5: Effect of interpolation parameter λ\non in-domain (left y-axis) and out-of-domain\n(right y-axis) validation set performances. More\nweight on pkNN improves domain adaptation.\nNumber of Neighbors per QueryEach query returns the top- k neighbors. Figure 4 shows that\nperformance monotonically improves as more neighbors are returned, and suggests that even larger\nimprovements may be possible with a higher value of k. Nonetheless, even a small number of\nneighbors (k= 8) is enough to achieve a new state of the art.\nInterpolation Parameter We use a parameterλto interpolate between the base model distribution\nand the distribution from kNN search over the dataset. Figure 5 shows that λ= 0.25 is optimal on\nWIKITEXT -103. However, λ= 0.65 works best for domain adaptation results (Figure 5).\nPrecision of Similarity Function In FAISS, the nearest neighbor search computes L2 distances\nagainst quantized keys. We found results were improved from 16.5 perplexity on W IKITEXT -103\nto 16.06 by computing squared L2 distances with full precision keys for Equation 2.\n6 A NALYSIS\nQualitative Analysis To understand why kNN-LM improves performance, we manually examine\ncases in which pkNN was signiﬁcantly better than pLM. Table 6 shows one such example, along with\nseveral others in Appendix A. The example shows an interesting case where the model matches the\ntrigram impact on the in several retrieved neighbors, but puts almost all weight on the most relevant\nneighbor, thus adding more value than an n-gram LM.\nIn general, we ﬁnd that examples where kNN-LM is most helpful typically contain rare patterns.\nExamples include factual knowledge, names, and near-duplicate sentences from the training set. In\nthese cases, assigning train and test instances similar representations (via f(·)) appears to be an\neasier problem than implicitly memorizing the next word in model parameters.\nSimple vs Neural Representation We observe that many long-tail phenomena manifest as rare\nn-grams (e.g. names). Is it therefore possible to interpolate an n-gram model with a Transformer\nLM, as an alternative to our kNN approach? Figure 7 shows little improvement from using n-gram\nLMs – 0.2 perplexity points (similarly to Bakhtin et al. (2018)). This result highlights the need to\nuse the learned representation function f(·) to measure similarity between more varied contexts.\nImplicit vs Explicit Memory If a neural representation function is crucial for kNN-LM, could\nimplicitly memorizing the training dataset in the neural network parameters replace the explicit\nmemory in the datastore? To test this, we train a Transformer LM with no dropout. Figure 8 shows\nthat this model eventually reaches zero training loss, indicating that it can make perfect predictions\nfor all examples in the training set; the model has memorized the dataset. Naturally, the memorizing\nLM overﬁts, i.e. the training loss drops to 0 while the best validation perplexity is much higher at\n28.59. For comparison, the vanilla Transformer LM (with dropout) has a much higher training loss\n(shown in Figure 8), but also generalizes better with a validation perplexity of 17.96. This result\nshows that the Transformer has sufﬁcient capacity to memorize the training set.\n7\nPublished as a conference paper at ICLR 2020\nTest Context (pkNN = 0.998, pLM = 0.124) Test Target\nit was organised by New Zealand international player Joseph Warbrick,\npromoted by civil servant Thomas Eyton, and managed by James Scott, a\npublican. The Natives were the ﬁrst New Zealand team to perform a haka,\nand also the ﬁrst to wear all black. They played 107 rugby matches during\nthe tour, as well as a small number of Victorian Rules football and associ-\nation football matches in Australia. Having made a signiﬁcant impact on\nthe...\ndevelopment\nTraining Set Context Training\nSet Target\nContext\nProbability\nAs the captain and instigator of the 1888-89 Natives – the ﬁrst New Zealand\nteam to tour the British Isles – Warbrick had a lasting impact on the...\ndevelopment 0.998\npromoted to a new ﬁrst grade competition which started in 1900. Glebe\nimmediately made a big impact on the...\ndistrict 0.00012\ncenturies, few were as large as other players managed. However, others\ncontend that his impact on the...\ngame 0.000034\nNearly every game in the main series has either an anime or manga adap-\ntation, or both. The series has had a signiﬁcant impact on the...\ndevelopment 0.00000092\nFigure 6: Example where the kNN model has much higher conﬁdence in the correct target than the\nLM. Although there are other training set examples with similar local n-gram matches, the nearest\nneighbour search is highly conﬁdent of speciﬁc and very relevant context.\n0 2 4 6 8 10\nn (size of n-gram)\n16.00\n16.25\n16.50\n16.75\n17.00\n17.25\n17.50\n17.75\n18.00Perplexity\nWikitext-103 LM + n-gram LM\nkNN-LM on Wikitext-103\nFigure 7: Interpolating the Transformer LM with\nn-gram LMs on WIKITEXT -103 (validation set).\nUsing kNN-LM gives a much lower perplexity,\nsuggesting that the representations are learning\nmore than just matching local context.\n0 25 50 75 100 125 150 175 200\nEpoch\n0\n1\n2\n3\n4\n5\n6\n7\n8Training loss (base e)\nWith Dropout\nWithout Dropout\nFigure 8: Training curves for the Transformer\nLM with and without dropout. Turning off\ndropout allows the training loss to go to 0, in-\ndicating that the model has sufﬁcient capacity to\nmemorize the training data.\nWe consider whether the memorizing LM can be an effective substitute for nearest neighbor search.\nInterpolating the memorizing LM with the original LM improves validation perplexity by just 0.1\n– compared to 1.9 from kNN-LM. This result suggests that although the Transformer is expressive\nenough to memorize all training examples, learning to do so does not result in context representations\nthat generalize. In contrast, kNN-LM memorizes training data while improving generalization.\nFrom these experiments, we conjecture that kNN-LM improves performance because (1) the Trans-\nformer LM is very good at learning a representation function for contexts with an implicit notion\nof similarity, and (2) while the Transformer has capacity to memorize all training examples, doing\nso causes its representation to generalize less effectively, but (3) the kNN-LM allows the model to\nmemorize the training data while retaining an effective similarity function.\n8\nPublished as a conference paper at ICLR 2020\n7 R ELATED WORK\nWe discuss related uses of caches for language modeling in Section 2.\nSimilar kNN models to ours have been proposed for computer vision tasks (Papernot & McDaniel,\n2018; Orhan, 2018; Zhao & Cho, 2018), primarily motivated by improving interpretability and ro-\nbustness to adversarial attacks. We hypothesize that our method may be particularly effective for\nlanguage modeling, because plentiful unlabeled data allows datastores of billions of tokens, and\nlanguage modeling often requires world knowledge to be learnt from few examples.\nNearest neighbor models have been applied to a number of NLP problems in the past, such as part\nof speech tagging (Daelemans et al., 1996) and morphological analysis (Bosch et al., 2007), but\nthe use of learned representations makes the similarity function much more effective in the case of\nneural models. More recently, Kaiser et al. (2017) have used a similarly differentiable memory that\nis learned and updated during training, and is applied to one-shot learning tasks.\nSeveral models have also improved language generation by using training examples directly at test\ntime. Guu et al. (2018) propose a model that samples training sentences at random and edits them\nwith a sequence-to-sequence model, but does not use a retrieval mechanism such as kNN. Gu et al.\n(2018) introduce a translation model that attends over retrieved training set examples. Weston et al.\n(2018) improve a dialogue response generation model by reﬁning similar instances from the training\nset. kNN-LM differs from these approaches by working at the level of individual tokens instead of\nwhole training sentences, as well as not incorporating the retrieval mechanism into the training\npipeline.\nA general trend in machine learning, and in language modeling in particular, is that adding more\ndata consistently improves performance (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019;\nLiu et al., 2019; Zellers et al., 2019; Shoeybi et al., 2019). Our work offers an alternative method\nfor scaling language models, in which relatively small models learn context representations, and a\nnearest neighbour search acts as a highly expressive classiﬁer.\n8 C ONCLUSION AND FUTURE WORK\nWe have introduced kNN-LMs, which can signiﬁcantly outperform standard language models by\ndirectly querying training examples at test time. The approach can be applied to any neural language\nmodel. The success of this method suggests that learning similarity functions between contexts may\nbe an easier problem than predicting the next word from some given context. Future work should\nexplore explicitly training similarity functions, and reducing the size of the datastore.\nACKNOWLEDGMENTS\nThe authors thank the anonymous reviewers as well as Sida Wang, Kartikay Khandelwal, Kevin\nClark and members of the FAIR Seattle team for helpful discussions and comments.\nREFERENCES\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019.\nAnton Bakhtin, Arthur Szlam, Marc’Aurelio Ranzato, and Edouard Grave. Lightweight adaptive\nmixture of neural and n-gram language models. arXiv preprint arXiv:1804.07705, 2018.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\nAntal van den Bosch, Bertjan Busser, Sander Canisius, and Walter Daelemans. An efﬁcient memory-\nbased morphosyntactic tagger and parser for dutch. LOT Occasional Series, 7:191–206, 2007.\nWalter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. Mbt: A memory-based part of\nspeech tagger-generator. In WVLC, 1996.\n9\nPublished as a conference paper at ICLR 2020\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. In\nACL, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\nEdouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online lan-\nguage modeling with open vocabulary. In NIPS, pp. 6042–6052, 2017a.\nEdouard Grave, Armand Joulin, Moustapha Ciss´e, Herv´e J´egou, et al. Efﬁcient softmax approxima-\ntion for gpus. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pp. 1302–1310. JMLR. org, 2017b.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\ncontinuous cache. In ICLR, 2017c.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. Search engine guided neural machine\ntranslation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\nKelvin Guu, Tatsunori B Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\nediting prototypes. Transactions of the Association for Computational Linguistics , 6:437–450,\n2018.\nJeff Johnson, Matthijs Douze, and Herv ´e J ´egou. Billion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734, 2017.\nŁukasz Kaiser, Oﬁr Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. In\nICLR, 2017.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of trans-\nformer language models. arXiv preprint arXiv:1904.08378, 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nHongyin Luo, Lan Jiang, Yonatan Belinkov, and James Glass. Improving neural language models\nby segmenting, attending, and predicting the future. In ACL, 2019.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. ICLR, 2017.\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Eleventh annual conference of the international speech\ncommunication association, 2010.\nA. Emin Orhan. A simple cache model for image recognition. In NeurIPS, 2018.\nNicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards conﬁdent, interpretable\nand robust deep learning. arXiv preprint arXiv:1803.04765, 2018.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. In ICLR, 2017.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. URL https://d4mucfpksywv.cloudfront.net/better-\nlanguage-models/language-models.pdf, 2019.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\n10\nPublished as a conference paper at ICLR 2020\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nJason Weston, Emily Dinan, and Alexander H Miller. Retrieve and reﬁne: Improved sequence\ngeneration models for dialogue. arXiv preprint arXiv:1808.04776, 2018.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Ali Farhadi, Franziska Roesner, and Yejin Choi.\nDefending against neural fake news. In NeurIPS, 2019.\nJake Zhao and Kyunghyun Cho. Retrieval-augmented convolutional neural networks for improved\nrobustness against adversarial examples. arXiv preprint arXiv:1802.09502, 2018.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer\nvision, pp. 19–27, 2015.\n11\nPublished as a conference paper at ICLR 2020\nA A PPENDIX\nThis section provides several examples wherepkNN places higher probability mass on the true target,\ncompared to pLM.\nTest Context (pkNN = 0.995, pLM = 0.025) Test Target\nFor Australians and New Zealanders the Gallipoli campaign came to sym-\nbolise an important milestone in the emergence of both nations as indepen-\ndent actors on the world stage and the development of a sense of national\nidentity. Today, the date of the initial landings, 25 April, is known as An-\nzac Day in Australia and New Zealand and every year thousands of people\ngather at memorials in both nations, as well as Turkey, to...\nhonour\nTraining Set Context Training\nSet Target\nContext\nProbability\nDespite this, for Australians and New Zealanders the Gallipoli campaign\nhas come to symbolise an important milestone in the emergence of both\nnations as independent actors on the world stage and the development of a\nsense of national identity. Today, the date of the initial landings, 25 April,\nis a public holiday known as Anzac Day in Australia and New Zealand and\nevery year thousands of people gather at memorials in both nations, and\nindeed in Turkey, to ...\nhonour 0.995\nOn the anniversary date of his death, every year since 1997, thousands of\npeople gather at his home in Memphis to...\ncelebrate 0.0086\nTwenty-ﬁve years after Marseille’s death, ﬁghter pilot veterans of World\nWar II gathered to...\nhonour 0.0000041\nTable 6: Another example where thekNN model places much higher probability mass on the correct\ntarget, compared to the LM. The nearest neighbors search has retrieved a training set context that is\nextremely similar to the test context, while very rare and in the long-tail of patterns.\nTest Context (pkNN = 0.959, pLM = 0.503) Test Target\nU2 do what they’re best at, slipping into epic rock mode, playing music\nmade for the arena”. In two other local newspaper reviews, critics praised\nthe song’s inclusion in a sequence of greatest hits. For the PopMart Tour of\n1997–...\n1998\nTraining Set Context Training\nSet Target\nContext\nProbability\nFollowing their original intent, ”Sunday Bloody Sunday” was not played\nduring any of the forty-seven shows on the Lovetown Tour in 1989. The\nsong reappeared for a brief period during the Zoo TV Tour, and late during\nthe second half of PopMart Tour (1997–...\n1998 0.936\nThey are 6 times Champions and they won the Challenge Cup in 1938, and\nhave experienced two previous stretches in the Super League, 1997–...\n2002 0.0071\nAbout $40 million ($61.4 million in 2018 dollars) was spent on the property\nacquisition. After weather-related construction delays due to the El Nino\nseason of the winter of 1997–...\n1998 0.0015\nThis made it the highest-rated season of The X-Files to air as well as the\nhighest rated Fox program for the 1997–...\n98 0.00000048\nTable 7: In this example, the desired date pattern appears in many examples. Yet, the nearest\nneighbors search is able to identify the only training set context which is relevant to the test context\nand assigns it the highest probability mass.\n12\nPublished as a conference paper at ICLR 2020\nTest Context (pkNN = 0.624, pLM = 0.167) Test Target\nLord Strathcona awarded Gauthier a scholarship in 1906 that allowed her\nto return to Europe and continue her vocal studies. She returned there and\ncontinued both to study and give performances. Her ﬁrst operatic perfor-\nmance came in 1909 in Pavia, Italy as Micaela in Bizet’s...\nCarmen\nTraining Set Context Training\nSet Target\nContext\nProbability\nDespite poor relations with the orchestra, Mahler brought ﬁve new operas\nto the theatre, including Bizet’s...\nCarmen 0.356\nThe fourth movement of An die Jugend (1909), for instance, uses two of\nNiccolo Paganini’s Caprices for solo violin (numbers 11 and 15), while the\n1920 piece Piano Sonatina No. 6 (Fantasia da camera super Carmen) is\nbased on themes from Georges Bizet’s...\nopera 0.0937\nIt also hosted the Ballet of her Majesty’s Theatre in the mid-19th century,\nbefore returning to hosting the London premieres of such operas as Bizet’s...\nCarmen 0.0686\nTable 8: In this case, the model is able to memorize the fact that Georges Bizet wrote Carmen.\nTest Context (pkNN = 0.031, pLM = 0.007) Test Target\nMycena maculata bears some resemblance to M. <unk>, but is only as-\nsociated with decaying hardwood logs and stumps, and is found in eastern\nNorth America, and sometimes on oak on the West Coast. In age, it...\ndevelops\nTraining Set Context Training\nSet Target\nContext\nProbability\nMorchella tridentina (=Morchella frustrata) is also rufescent and very sim-\nilar to M. rufobrunnea. It is found in mountainous forests and maquis and\nforms a marked sinus at the attachment of the cap with the stem, which is\npure white. At maturity, it...\ndevelops 0.031\nThe winter bonnet (M. tintinnabulum) is a northern European species that\nis much smaller (cap diameter up to 2.6 cm (1.0 in) across) and has a brown\ncap, and has ragged hairs at the base. It...\ngenerally 0.029\nThe ”bleeding” will distinguish Mycena atkinsoniana from most other\nMycena species commonly encountered. The common and widely dis-\ntributed M. sanguinolenta is another ”bleeder”, but it is smaller than M.\natkinsonia, with a cap diameter ranging from 3 to 15 mm (0.1 to 0.6 in).\nAdditionally, it...\nhas 0.028\nMycena ﬂavoalba bears resemblance to some members of the genus\nHemimycena, such as H. lactea and H. <unk>. It...\ncan 0.018\nTable 9: This is an example where the pkNN distribution is relatively ﬂat, as several words are\nplausible continuations. However, the nearest neighbors search assigns the highest probability to\nthe correct target and a corresponding context that is particularly relevant. In contrast, the LM\nprobability on the correct target is lower.\n13",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8951842188835144
    },
    {
      "name": "Computer science",
      "score": 0.7303037643432617
    },
    {
      "name": "k-nearest neighbors algorithm",
      "score": 0.6928545236587524
    },
    {
      "name": "Generalization",
      "score": 0.6817406415939331
    },
    {
      "name": "Artificial intelligence",
      "score": 0.608466386795044
    },
    {
      "name": "Nearest neighbor search",
      "score": 0.5941723585128784
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5480022430419922
    },
    {
      "name": "Embedding",
      "score": 0.538077175617218
    },
    {
      "name": "Language model",
      "score": 0.5345458388328552
    },
    {
      "name": "Memorization",
      "score": 0.5048506855964661
    },
    {
      "name": "Point (geometry)",
      "score": 0.4804758131504059
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44988304376602173
    },
    {
      "name": "Word (group theory)",
      "score": 0.44901251792907715
    },
    {
      "name": "Scaling",
      "score": 0.42053359746932983
    },
    {
      "name": "Training set",
      "score": 0.4130431115627289
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4099244475364685
    },
    {
      "name": "Natural language processing",
      "score": 0.36269694566726685
    },
    {
      "name": "Mathematics",
      "score": 0.15158268809318542
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}