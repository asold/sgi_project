{
  "title": "SERENGETI: Massively Multilingual Language Models for Africa",
  "url": "https://openalex.org/W4385571607",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5009882700",
      "name": "Ife Adebara",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025365353",
      "name": "AbdelRahim Elmadany",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A5004629670",
      "name": "Muhammad Abdul-Mageed",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A5060761291",
      "name": "Alcides Alcoba Inciarte",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4300427991",
    "https://openalex.org/W3103147437",
    "https://openalex.org/W4283161703",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W2613045005",
    "https://openalex.org/W4285060468",
    "https://openalex.org/W4221155692",
    "https://openalex.org/W3012624518",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W3013055388",
    "https://openalex.org/W2945836649",
    "https://openalex.org/W3185293939",
    "https://openalex.org/W4287855142",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3134918747",
    "https://openalex.org/W4294959075",
    "https://openalex.org/W2783331529",
    "https://openalex.org/W2279705987",
    "https://openalex.org/W4393553944",
    "https://openalex.org/W2091712071",
    "https://openalex.org/W4287028386",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3154997565",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2100960835",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W1819662813",
    "https://openalex.org/W3096755600",
    "https://openalex.org/W2467369108",
    "https://openalex.org/W3209305045",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4319915529",
    "https://openalex.org/W3183430109",
    "https://openalex.org/W2964015429",
    "https://openalex.org/W2902224779",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3117519745",
    "https://openalex.org/W4308614370",
    "https://openalex.org/W3036683019",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W3034987021",
    "https://openalex.org/W2897702578",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W4307310173",
    "https://openalex.org/W4285187193",
    "https://openalex.org/W3031202003",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3041561615",
    "https://openalex.org/W2946730224",
    "https://openalex.org/W4285272463",
    "https://openalex.org/W2256462621",
    "https://openalex.org/W3042196906",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4248423381",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4229014795",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W3154300329",
    "https://openalex.org/W4285208773",
    "https://openalex.org/W3098075008",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W4255213459",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W3003079869",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2279316390",
    "https://openalex.org/W2091746061",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W4307407281",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1563413811",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3036073890",
    "https://openalex.org/W3208906783"
  ],
  "abstract": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.Anonymous link",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1498–1537\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSERENGETI: Massively Multilingual Language Models for Africa\nIfe Adebara1,⋆ AbdelRahim Elmadany1,⋆ Muhammad Abdul-Mageed1,2 Alcides Alcoba1\n1Deep Learning & Natural Language Processing Group, The University of British Columbia\n2Department of Natural Language Processing & Department of Machine Learning, MBZUAI\n{ife.adebara@,a.elmadany@,muhammad.mageed@,alcobaaj@mail.}ubc.ca\nAbstract\nMultilingual pretrained language models\n(mPLMs) acquire valuable, generalizable\nlinguistic information during pretraining\nand have advanced the state of the art on\ntask-specific finetuning. To date, only ∼31\nout of ∼2, 000 African languages are covered\nin existing language models. We ameliorate\nthis limitation by developing SERENGETI, a\nmassively multilingual language model that\ncovers 517 African languages and language\nvarieties. We evaluate our novel models\non eight natural language understanding\ntasks across 20 datasets, comparing to 4\nmPLMs that cover 4 −23 African languages.\nSERENGETI outperforms other models on\n11 datasets across the eights tasks, achieving\n82.27 average F1. We also perform analyses\nof errors from our models, which allows us to\ninvestigate the influence of language genealogy\nand linguistic similarity when the models are\napplied under zero-shot settings. We will\npublicly release our models for research.1\n1 Introduction\nPretraining NLP models with a language model-\ning objective has gained popularity as a precursor\nto task-specific finetuning (Ettinger, 2020). Pre-\ntrained models like BERT (Devlin et al., 2019),\nELMo (Peters et al., 2018), Roberta (Liu et al.,\n2019), GPT (Radford et al., 2018, 2019; Brown\net al., 2020a), and BART (Lewis et al., 2020) have\nadvanced the state of the art in a wide variety of\ntasks, demonstrating how these models acquire\nvaluable, generalizable linguistic information dur-\ning the pretraining process. However, training\nlanguage-specific models is possible for only a few\nlanguages which have large amounts of data. A\npopular alternative has been pretrained multilingual\nlanguage models (mPLM) such as mBERT (Devlin\n1https://github.com/UBC-NLP/serengeti\n⋆ Authors contributed equally.\nFigure 1: All 517 languages in our dataset across the 50\nAfrican countries our data comes from. The language varieties\nare represented as colored pie shapes within each country. We\nzero in on South Africa, Lesotho, Swaziland, and Senegal to\nshow detail. We provide a larger map in Appendix A.1.\net al., 2019) and XML-R (Conneau et al., 2020).\nmPLMs are trained on large amounts of unlabelled\ndata from multiple languages so that low resource\nlanguages may benefit from shared vocabulary and\nother linguistic information from high-resource and\nsimilar languages in the model. The vast majority\nof the world’s ∼7, 000 languages today remain\nuncovered by mPLMs, however.\nAfrican languages are no exception. Although\nthere are few mPLMs that support a small number\nof African languages (Devlin et al., 2019; Ogueji\net al., 2021; Nzeyimana and Niyongabo Rubungo,\n2022; Alabi et al., 2022a; Jude Ogundepo et al.,\n2022; Conneau et al., 2020), these cover only a\ntotal of 31 languages. This is grossly inadequate\nconsidering that Africa is believed to be home to\n∼2, 000 languages (Eberhard et al., 2021). Each of\nthese languages encapsulates unique features that\nare essential in preserving linguistic diversity. The\nsame way every species embodies essential value\nto the natural ecosystem, each language plays a\n1498\ncrucial role in the linguistic ecosystem. That is,\neach language encodes knowledge about people,\ntheir traditions, wisdom, and environment, as well\nas how it is that they interact with the sum of the\nconcepts in their own culture (Adebara and Abdul-\nMageed, 2022). This in turn allows people and\ncommunities to preserve and transmit their know-\nledge, values, unique modes of thinking, mean-\ning and expression, history, culture, traditions, and\nmemory to next generations, while participating\nin society and constructing their future (UNESCO\n66260, 2022).\nLanguage technology plays an important role in\nbuilding inclusive knowledge societies, providing\naccess to education and information, supporting\nfreedom of expression, cultural and linguistic di-\nversity, and further stimulating innovation. This\ntechnology thus has great impact on multiple do-\nmains, including education, government, health,\nrecreation, among others. This motivates adequate\nrepresentation of African languages in the ongoing\ntechnological revolution. This is also likely to con-\nnect Africa to the rest of the world. Building tech-\nnologies for African languages may also aid lan-\nguages that may be at risk of falling into a state of\ndisuse at an alarming rate, thus hopefully prevent-\ning subsequent language death that may become\ninevitable (Adebara and Abdul-Mageed, 2022).\nDeveloping LMs that represent a large number\nof African languages is therefore very crucial for\nachieving progress in Afrocentric NLP (Adebara\nand Abdul-Mageed, 2022) and indeed in address-\ning issues related to representation bias in artifi-\ncial intelligence and linguistic diversity - two re-\nsearch themes of international relevance (Bender\net al., 2021). Motivated by this call for Afrocentric\nNLP, we introduce SERENGETI. SERENGETI is\na massively multilingual language model exploiting\na large manually-curated dataset for 517 African\nlanguages and language varieties. These languages\nbelong to 14 language families and are written in 5\ndifferent scripts. In addition to these African lan-\nguages, SERENGETI is also pretrained on the top\n10 most spoken languages globally.\nWe also introduce AfroNLU, an extensive\nbenchmark exploiting 20 different datasets across\n28 different languages and language varieties for\nvarious NLP tasks. For even richer evaluation, we\nalso apply our models to an African language iden-\ntification task covering all the 517 languages in\nour pretraining. To the best of our knowledge,\nAfroNLU is the most extensive and inclusive evalu-\nation benchmark proposed to date for African NLP.\nOur contributions in this work are as follows:(1)\nwe collect a large dataset of 517 African languages\nand language varieties and exploit it to develop\nSERENGETI. (2) we propose AfroNLU, a new ex-\ntensive benchmark for African NLU that has the\nwidest and most inclusive coverage for African\nNLP today. (3) we benchmark SERENGETI on\nAfroNLU and show through meaningful comparis-\nons how our model excels and acquire new SOTA.\n(4) we offer a linguistically motivated analysis of\nmodel performance substantiated in language gene-\nalogy, allowing us for the first time to derive in-\nsights across the widest range of African languages\nin the African NLP literature to date.\nThe rest of the paper is organized as follows: In\nSection 2 we discuss related work. We describe\ngenealogical information in Section 3. Next, we\ngive a detailed description of SERENGETI in Sec-\ntion 4. In Section 5 we describe AfroNLU, the\nbenchmark we create. We present performance\nof SERENGETI in Section 6 and compare it to\nother mPLMs. We conclude in Section 7, and out-\nline a number of limitations and use cases for our\nwork in Section 8 and Section 9.\n2 Related Work\nAfrocentric NLP .An Afrocentric approach to tech-\nnology development is crucial for African lan-\nguages. An afrocentric approach will mean that\nwhat technologies to build and how to build, evalu-\nate, and deploy them arises from the needs of local\nAfrican communities (Adebara and Abdul-Mageed,\n2022). We provide more details in Section B in the\nAppendix.\nAfrican Language Models.Here, we briefly de-\nscribe language models covering any number of\nAfrican languages. Since we develop encoder-only\nmodels in this work, we will restrict our discussion\nto this category of models. We provide informa-\ntion about the African languages covered by these\nmodels in Table 1.\nAfriBERTa(Ogueji et al., 2021) is trained using a\nTransformer with the standard masked language\nmodelling objective and covers 11 African lan-\nguages. The pretraining corpus for this model\nis small (only 108.8 million tokens), when com-\npared to many other models. AfroLM (Dossou\net al., 2022) supports 23 African languages, the\nlargest number of African languages before SER-\nENGETI. It is trained on a multi-domain dataset\n2\n1499\nLanguage Model African languages represented\nMBERT Afrikaans, Malagasy, Swahili, Yoruba\nXLM-R Afrikaans, Amharic, Hausa, Oromo, Somali, Swahili, Xhosa.\nKinyarBERT Kinyarwanda\nAfriBERTA Afaan Oromoo, Amharic, Gahuza, Hausa, Igbo, Nigerian Pidgin, Somali, Swahili, Tigrinya and Yoruba\nAfro-XLMR Afrikaans, Amharic, Hausa, Igbo, Malagasy, Chichewa, Oromo, Nigerian Pidgin, Kinyarwanda, Kirundi,\nShona, Somali, Sesotho, Swahili, isiXhosa, Yoruba, and isiZulu\nAfroLM Amharic, Afaan Oromoo, Bambara, Ghomala, Ewe, Fon, Hausa, Igbo, Kinyarwanda, Lingala, Luganada,\nLuo, Moore, Chewa, Nigerian Pidgin, Shona, Swahili, Setswana, Akan Twi, Wolof, Xhosa, Yoruba, IsiZulu\nSERENGETI Includes 517 African languages.\nTable 1: Encoder-only models with African languages represented.\nfrom various sources (Adelani et al., 2022a; Alabi\net al., 2022b; Jude Ogundepo et al., 2022; Niy-\nongabo et al., 2020). It uses a self-active learn-\ning framework and achieves SOTA on NER, senti-\nment analysis, and text classification. Afro-XLM-\nR (Alabi et al., 2022a) uses language adaptation\non the 17 most-resourced African languages and\nthree other high-resource foreign languages widely\nused in Africa (i.e., English, French, and Arabic)\nsimultaneously to provide a single model for cross-\nlingual transfer learning. Authors show that Afro-\nXLM-R has competitive results with AfriBERTa\nand XLM-R on NER, topic classification, and news\nclassification. KINYaBERT (Nzeyimana and Niy-\nongabo Rubungo, 2022) uses a two-tier BERT\narchitecture that involves a morphological ana-\nlyzer and explicitly represents morphological in-\nformation for Kinyawanda–a morphologically rich\nAfrican language. Authors show that KINYaBERT\nachieves good convergence and accuracy, and is\nrobust on multiple downstream tasks. mBERT\n(Devlin et al., 2019) is a multilingual variant of\nBERT trained on 104 languages including four\nAfrican languages. XLM-R (Conneau et al., 2020)\nuses a Transformer based architecture and obtains\nSOTA on cross-lingual classification, sequence la-\nbeling, and question answering on 100 languages\nincluding eight African languages.\n3 Genealogy of African Languages\nGenealogical or genetic classification groups lan-\nguages based on their historical and evolutionary\nrelationships. Genetically related languages are\noften classified into similar families in a hierarch-\nical tree like structure that shows the level of sim-\nilarity between the languages. Languages with\na higher degree of similarity belong to the same\nclass while languages with a lower degree of sim-\nilarity are further subdivided into different classes\nand subclasses. Two closely related languages can\ntherefore be viewed as sisters of the same parent\nlanguage/ancestor–they are languages that evolved\nover time and/or space from an older parent lan-\nguage (Gerhardt, 2020). Typological classification\ndiffers from geneological classification in that the\nformer is based on grammatical features or types\n(V ossen, 2020). For instance, a typological classi-\nfication would group tone languages together, or\nsplit languages based on their morphological struc-\nture into, for instance, isolating or agglutinating\nlanguages. Despite this difference, languages that\nbelong to the same family often share similar typo-\nlogical information (Gerhardt, 2020). For example,\nmost Benue-Congo languages are tone languages\n(Williamson, 2006). In the case of African lan-\nguages, where typological information is scarcely\navailable (Adebara and Abdul-Mageed, 2022; Gül-\ndemann, 2018), utilizing genetic classes may be a\nuseful way to determine typological information.\nIf the typological information of one language in\na group is known, we may make a sensible as-\nsumption that other languages in that group perhaps\nshare similar features with minor variations. We\nuse geneological classification information in eval-\nuating SERENGETI’s behaviour. Specifically, we\ninvestigate the relationship between language simil-\narity and model performance in zero-shot scenarios\nfor South African languages in some datasets in\nour benchmark. We use classification information\nfrom Ethnologue (Eberhard et al., 2021) in all our\nanalyses. We provide a broad overview of the fam-\nilies in our models under six broad ancestors in\nSection D in the Appendix.\n4 SERENGETI\n4.1 Pretraining Data\nSERENGETI is pretrained using 42GB of data\ncomprising a multi-domain, multi-script collection\n3\n1500\nVocabulary Training DataModel Tok Vocab Size#Params #Lang.(afr/all) Tokens(afr/all) Size(afr/all) Source\nxlmr SP 250k 270M 8 / 100 UNK/164B UNK/2.4 GB CC-100\nmbert WP 110K 110M 4 / 100 UNK/12.8B UNK/100GB Books, Wiki.\nAfro-XLMR SP 70.6K 270M 17 / 20 — 21.6 GB mC4, CC, BBC, VOA\nAfriBERTa WP 70k 111M 11 / 11 108.8M 0.94 GB BBC, CC\nAfroLM SP 250K 264M 23/23 — 0.73GB mC4, CC, BBC, VOA\nSERENGETI-E110 WP 110K 170M 517 / 527 7.1B/8.6B 40/42GB RT, News, GD, HD, EC\nSERENGETI-E250 WP 250K 277M 517/527 7.1B/8.6B 40/42GB RT, News, GD, HD, EC\nSERENGETI SP 250K 278M 517/527 7.1B/8.6B 40/42GB RT, News, GD, HD, EC\nTable 2: Models with African languages that we compare SERENGETI with. SP: SentencePiece, WP: WordPiece.\nData sources include - CC: CommonCrawl, EC: Existing corpora, GD: Government documents, HD: Health\ndocuments, RT: Religious text, UNK: Unknown.\nof texts that we manually curate. The pretraining\ndata covers 517 African languages and the 10 most\nspoken languages globally (i.e., Arabic, English,\nFrench, German, Greek, Italian, Portuguese, Rus-\nsian, Spanish, and Turkish). The multi-domain\ndataset comprises texts from religious, news, gov-\nernment documents, health documents, and exist-\ning corpora written in five scripts from the set {Ar-\nabic, Coptic, Ethiopic, Latin, and Vai} . For the\ntop ten foreign languages, we randomly select 1M\nparagraphs from Wikipedia for each language to\nuse in our overall pretraining data. We provide fur-\nther details of the pretraining data in Section C in\nthe Appendix. We also show all languages in our\npretraining data in Tables F.1, F.2, and F.3.\n4.2 Preprocessing\nTo prepare the raw data for pretraining, we perform\nlight preprocessing to retain a faithful representa-\ntion of the naturally occurring text. Specifically,\nwe ensure that images and non-text materials are\nnot in our dataset by using regular expression and\nmanual curation techniques. We do not perform\nany further preprocessing of the data before split-\nting the text off into tokens. For tokenization, we\nuse a WordPiece tokenizer (Song et al., 2021). We\nexperiment with two vocabulary sizes, 110K and\n250K.\n4.3 SERENGETI Models\nWe pretrain both Electra style (Clark et al., 2020b;\nChi et al., 2021) as well as XLM-R style (Conneau\net al., 2020) models, as follows.\nSERENGETI-E110 and SERENGETI-E250.\nWe first pretrain Electra (Chi et al., 2021) style\nmodels. Electra uses a multilingual replaced token\ndetection (MRTD) objective for training. Unlike\nother training objectives, the goal of MRTD is to\ndistinguish real input tokens from corrupted tokens.\nModels built with this objective are pretrained as\ndiscriminators rather than generators. We train\nthe models with two vocabulary sizes, 110K and\n250K, and hence refer to them as SERENGETI-\nE110 and SERENGETI-E250. Each of these mod-\nels has 12 layers and 12 attention heads. We pre-\ntrain each model for 40 epochs with a sequence\nlength of 512, a learning rate of 2e −4 and a batch\nsize of 216 and 104 for the SERENGETI-E110\nand SERENGETI-E250, respectively. We pre-train\nthe models on 1 Google Cloud TPU with 8 cores\n(v3.8) from TensorFlow Research Cloud (TFRC).\n2\nSERENGETI Model. Apart form the Electra mod-\nels, we also experiment with an XLM-R base archi-\ntecture. We train the model with a250K vocabulary\nsize for 20 epochs. This model has 12 layers and\n12 attention heads, a sequence length of 512 and\na batch size of 8. We pre-train this model on 80\nM50 AMD Pod GPUs with 16G ram. Our XLM-\nR model has better performance compared to the\nElectra models as we will show. We provide in-\nformation about each model we build and compare\nwith in Table 2.\n5 AfroNLU Benchmark\nOur goal is to evaluate our models extensively, and\nso we combine all available datasets we could ac-\nquire to create an evaluation benchmark that we\nrefer to as AfroNLU. AfroNLU is composed of\nseven different tasks, covering both token and sen-\ntence level tasks, across 18 different datasets. The\nbenchmark covers a total of 32 different languages\nand language varieties . In addition we evaluate\nour best model (SERENGETI) on an African lan-\n2https://www.tensorflow.org/tfrc.\n4\n1501\nCluster Dataset Languages TRAIN DEV TEST\nNER\nmasakaner-v1⋆ amh, hau, ibo, kin, lug, luo, pcm, swh, wol, yor 443,692 60,515 134,126\nmasakaner-v2⋆ bam, bbj, ewe, fon, hau, ibo, kin, lug, mos, nya,\npcm, sna, swa, tsn, twi, wol, xho, yor, zul 2,537,792 362,837 726,830\nmasakaner-east⋆ amh, kin, lug, luo, swh 162,388 21,206 46,407\nmasakaner-eastwest⋆ amh, hau, ibo, kin, lug, luo, pcm, swh, wol, yor 416,113 56,512 126,176\nmasakaner-west⋆ hau, ibo, pcm, wol, yor 253,725 35,306 79,769\nnchlt-ner⋆ afr, nbl, nso, sot, ssw, tsn, tso, ven, xho, zul 1,749,372 219,703 215,616\nyoruba-twi-ner⋆ yor 20,237 2,811 5,471\nwikiann⋆ afr, amh, ibo, mlg, kin, som, swh, yor 9,244 9,240 9,424\nPhrase Chunking phrase-chunk⋆ afr, nso, sot, ssw, tsn, tso, ven, zul 107,492 12,972 13,389\nPOS igbo-pos ⋆ ibo 756,775 94,692 95,048\nNews\namharic-news† amh 41,185 5,148 5,149\nkinnews† kir 15,308 1,701 4,254\nkirnews† run 3,320 369 923\nswahili-news-v0.2† swh 19,986 2,221 7,338\nSentiment Analysis\nbambara-v2† bam 2,436 305 305\npidgin-tweet† pcm 11,200 1,400 1,400\nyosm† yor 800 200 500\nTopic hausa-topic† hau 2,045 290 582\nyoruba-topic† yor 1,340 189 379\nQA qa-swahili † swh 49,881 5,077 499\nLID AfroLID† 517 African Languages 2,496,980 25,850 51,400\nAfri-Senti amh, hau, ibo, pcm, swh, yor -\nTable 3: Distribution of AfroNLU datasets. ⋆ indicates that datasize is measured at token level. † indicates data size\nmeasured at sentence level.\nTasks AfriBERTa Afro-XLMR KinyaBERT SERENGETI\nNER\nPC — — —\nPOS — — —\nNC —\nSA — —\nTC —\nQA — — —\nLID — — —\nGLUE — — —\nTable 4: Tasks evaluation comparison across different\nAfrican language MLMs. NER: named entity recogni-\ntion, PC: phrase chunking, POS: part of speech, NC:\nnews classification, SA: sentiment analysis, TC: topic\nclassification, QA: question answering, LID: language\nidentification.\nguage identification (LID) task covering all the517\nlanguages in our pretraining collection. For LID,\nwe use two datasets to test SERENGETI. This puts\nAfroNLU at a total of 20 different datasets and\neight different tasks. To the best of our know-\nledge, our evaluation benchmark is the most ex-\ntensive compared to previous published research.\nWe provide detailed statistics of the datasets com-\nprising AfroNLU in Table 3. We also provide a\ndetailed comparison of our AfroNLU benchmark\nwith evaluation data from other models in Table 4.\nWe now describe each of the downstream tasks in\nAfroNLU.\n5.1 Named Entity Recognition (NER)\nWe evaluate our models on NER datasets across\nmultiple languages. We use MasakhaNER data (Ife-\noluwa Adelani et al., 2021), WikiAnn (Pan et al.,\n2017; Rahimi et al., 2019), Yoruba-Twi NER data\n(Alabi et al., 2020), Distance Supervision NER (DS\nNER) Data (Hedderich et al., 2020) and multiple\nNER data from SADiLaR. For our experiments, we\nuse the region aggregates on MasakhaNER. Spe-\ncifically, we use MasakhaNER-east, MasakhaNER-\nwest, and MasakhaNER-eastwest. MasakhaNER-\neast includes NER data for Amharic, Kinyawanda,\nLuganda, Luo, and Swahili. MasakhaNER-west in-\ncludes NER data for Hausa, Igbo, Nigerian-Pidgin,\nWolof, and Yoruba. MasakhaNER-eastwest, on\nthe other hand, includes a combination of\nMasakhaneNER-east and MasakhaneNER-west.\nData from SADiLaR cover ten indigenous South\nAfrican languages and is annotated for person, or-\nganisation, location, and miscellaneous named en-\ntities. Miscellaneous named entities refer to all ri-\ngid designators that do not fall into one of the other\ncategories, including temporal expressions (dates\n5\n1502\nand times), URLs, numerical expressions, publica-\ntions, names of languages, nationalities, among oth-\ners. More details about the datasets are in Table 3.\n5.2 Part of Speech Tagging\nWe test our models on POS tagging datasets for\nIgbo taken from IgboNLP (Onyenwe et al., 2018,\n2019). In Table 3, we provide the statistical details\nfor the dataset.\n5.3 Phrase Chunks\nWe evaluate our models on phrase chunks data-\nsets for ten Indigenous languages of South Africa\n(see Table 3). The data has annotations for noun,\nverb, adjective, adverbial, and prepositional phrase\nchunks. Words not belonging to these phrase types\nare labelled with the tag O.\n5.4 Sentiment Analysis\nWe finetune our model on three sentiment analysis\ndatasets, including Bambara Sentiment dataset (Di-\nallo et al., 2021), YOSM–a new Yorùbá Sentiment\nCorpus for Movie Reviews (Shode et al., 2022), and\nthe Nigerian Pidgin sentiment dataset (Oyewusi\net al., 2020), respectively. Some details of these\ndatasets is in Table 3.\n5.5 News classification\nWe use news classification datasets for Amharic\n(Azime and Mohammed, 2021), Kinyarwanda (Niy-\nongabo et al., 2020), Kirundi (Niyongabo et al.,\n2020), and Swahili (David, 2020a,b). The Amharic\ndataset contains six classes–news, sport, politics,\ninternational news, business, and entertainment.\nThe Swahili dataset also has six categories includ-\ning local news, international, finance, health, sports,\nand entertainment. The datasets for Kinyarwanda\nand Kirundi have 14 and 12 categories each, re-\nspectively. Again, data statistics are in Table 3.\n5.6 Topic classification\nWe include topic classification datasets for Yorùbá\nand Hausa (Hedderich et al., 2020). The Yorùbá\nand Hausa datasets contain news titles collected\nfrom VOA Hausa and BBC Yorùbá news sites. The\nYorùbá dataset has seven topics–Nigeria, Africa,\nworld, entertainment, health, sports, and politics,\nwhile the Hausa dataset is categorized into five\ntopics - Nigeria, Africa, world, health, and politics.\nIn Table 3, we provide details about the data split\nsizes.\n5.7 Question Answering\nWe use TYDIA question answering dataset (Clark\net al., 2020a). The dataset has a primary task and a\ngold passage task. The primary task has two sub-\ntasks, one for passage selection and another that is\na minimal answer span. For the passage selection\nsubtask, a list of passages is given and the required\nresponse is either the index of the passage where\nthe answer to the question is or null (if no answer\nexists in the passage). The minimal answer span\nsubtask on the other hand gives a full article and the\nexpected answer is either the start and end byte in-\ndices of the minimal span that answers the question,\nyes or no response, or null (if no minimal answer\nexists). For the gold passage task, a correct answer\nis predicted from a passage containing one answer.\nThis is similar to existing reading comprehension.\nWe use the Kiswahili dataset alone, since it is the\nonly African language in the dataset. Details about\nthe data splits can be found in Table 3.\n5.8 Language Identification\nWe also evaluate SERENGETI on the task of lan-\nguage identification (LID). LID focuses on identi-\nfying the human language a piece of text or speech\nsegment belongs to, making automatic LID an im-\nportant first step in processing human language\nappropriately (Tjandra et al., 2021; Thara and Poor-\nnachandran, 2021). We use datasets from AfroLID\n(Adebara et al., 2022b) for this task. AfroLID\ndata is a multi-genre, multi-script dataset for 517\nAfrican languages. We compare the performance\nof AfroLID data on our models with performance\non AfroLID tool. To ensure a fair comparison, the\ndata used for AfroLID is completely different from\nthe data used for SERENGETI. We also evaluate\nour LID model on AfriSenti dataset (Muhammad\net al., 2022; Yimam et al., 2020).\n6 Experimental Setup and Evaluation\nWe evaluate SERENGETI on eight task clusters\nin the benchmark, and report results on our Test\nset in Table 5. We also report performance on our\nDev set in Table E.1 (Appendix). For each task\ncluster, we finetune for a maximum of 25 epochs\nwith a patience value of five. We compare res-\nults from SERENGETI, SERENGETI-E110, and\nSERENGETI-E250 to encoder-only models cover-\ning any number of African languages. Specifically,\nwe compare with XLMR, mBERT, Afro-XLMR,\nand AfriBERTa. We report the results of each\nexperiment as an average of three runs, showing\n6\n1503\nCluster Dataset SOTA XLMR mBERT Afro-XLMR AfriBERTa SERENGETI-E110 SERENGETI-E250 SERENGETI\nNER\nmasakaner-v1 84.80±0.3‡‡‡81.41±0.2678.57±0.53 84.16±0.45 81.42±0.30 81.23±0.32 81.54±0.68 84.53±0.56\nmasakaner-v2 87.00±1.2‡‡‡87.17±0.1884.82±0.96 88.69±0.12 86.22±0.06 86.57±0.27 86.69±0.29 88.86±0.25\nmasakaner-east 80.62⋆ 80.38±0.5678.33±1.25 83.02±0.31 79.31±0.92 80.53±0.71 81.26±0.68 83.75±0.26\nmasakaner-eastwest 82.34⋆ 82.85±0.3882.37±0.90 86.31±0.30 82.98±0.44 82.90±0.49 83.67±0.44 85.94±0.27\nmasakaner-west 83.11⋆ 82.85±0.79 83.99±0.39 86.78±0.44 84.08±0.32 82.06±0.67 83.45±0.81 86.27±0.94\nnchlt-ner — 71.41 ±0.0770.58±0.26 72.27±0.14 68.74±0.29 64.46±0.37 64.42±0.24 73.18±0.24\nyoruba-twi-ner — 61.18 ±2.1970.37±0.61 58.48±1.85 69.24±3.05 61.77±1.24 57.99±2.61 71.25±1.73\nwikiann 83.82 ±0.3982.65±0.77 86.01±0.83 83.05±0.20 83.17±0.54 84.85±0.53 85.83±0.94\nPhrase Chunking phrase-chunk — 88.86±0.1888.65±0.06 90.12±0.12 87.86±0.20 90.39±0.21 89.93±0.33 90.51±0.04\nPOS igbo-pos — 85.50 ±0.0885.42±0.13 85.39±0.21 85.43±0.05 85.50±0.16 85.61±0.13 85.54±0.08\nNews Classification\namharic-news — 84.97 ±0.5559.01±1.47 86.18±0.85 86.54±1.20 86.50±0.71 86.34±0.30 86.82±0.72\nkinnews 76.58 ±0.7077.45±0.43 79.13±0.53 80.40±1.50 81.43±1.02 80.38±1.36 79.80±0.68\nkirnews — 57.18 ±3.4474.71±2.56 87.67±0.92 89.59±0.27 78.75±3.24 86.60±1.28 87.53±2.31\nswahili-news-v0.2 — 87.50±0.9185.12±0.93 87.49±1.26 87.91±0.36 87.33±0.28 86.12±1.30 88.24±0.99\nSentiment Analysisbambara-v2 64.00† 47.17±1.8364.56±1.71 59.40±0.56 65.06±2.08 65.07±2.59 65.76±2.02 63.36±3.31\npidgin-tweet — 70.42 ±0.6868.59±0.47 71.40±0.51 69.19±0.97 71.06±0.39 70.46±1.02 69.74±0.92\nyosm 87.20 ‡ 85.57±1.0985.25±0.25 87.46±0.42 88.66±0.23 86.86±0.95 85.58±1.51 87.86±0.81\nTopic hausa-topic 48.52†† 85.80±1.4581.38±0.42 88.67±0.30 92.59±0.69 88.52±1.31 89.07±0.95 89.93±0.49\nyoruba-topic 54.93†† 54.69±2.8971.79±1.43 75.13±1.40 81.79±0.66 65.22±4.72 66.34±4.09 79.87±1.61\nQA qa-swahili 81.90 ‡‡ 82.79±1.93 83.40±0.7879.94±0.39 57.3±1.8 79.76±0.52 81.25±1.33 80.01±0.78\nAfroNLU Score 76.91 77.85 81.09 80.37 79.45 79.87 82.44\nTable 5: Performance of models on seven AfroNLU benchmark TEST datasets. (F1) score is the evaluation metric.\nOur model (SERENGETI) significantly outperforms AfriBERTa (the 2nd in row) on 13/18 datasets and achieve\nSOTA on 9/18 datasets. SOTA as reported on ⋆(Ifeoluwa Adelani et al., 2021), †(Diallo et al., 2021), ‡(Shode\net al., 2022), ††(Hedderich et al., 2020) and ‡‡(Clark et al., 2020a), ‡ ‡ ‡(Adelani et al., 2022b). We use a dash (-) to\nrepresent tasks without a known SOTA.\nthe standard deviation. We also evaluate SEREN-\nGETI on language identification and show results\non Afrolid in Table 6 and on Afrisenti in Table 7.\nFor multilingual datasets in each task, we show\nevaluation results per language, comparing the per-\nformance of various models in Table E.4 in the\nAppendix.\nTask AfroLID SERENGETI\nDev 96.14 ⋆ 97.64±0.02\nTest 95.95⋆ 97.41±0.02\nTable 6: Performance of SERENGETI on African LID\n(F1). ⋆ Results as reported in Adebara et al. (2022b).\nAfroLID SERENGETI\nAmharic (amh) 97.00 99.50 ±0.01\nHausa (hau) 89.00 98.09±0.02\nIgbo (ibo) 46.00 95.28±0.00\nNigerian Pidgin (pcm) 56.00 77.73±0.01\nSwahili (swh) 96.00 98.66±0.02\nYoruba (yor) 82.00 98.96±0.00\nTable 7: Comparison between AfroLID (Adebara et al.,\n2022b) and SERENGETIon AfriSenti Dev dataset.\n6.1 Performance Analysis\nWe report the results for seven of our eight tasks in\nTable 5.\nNamed Entity Recognition (NER). SEREN-\nGETI sets a new SOTA on six out of eight datasets\non the NER cluster. The lowest F1 across all mod-\nels are on NCHLT and Yoruba-Twi datasets (on\nboth Dev and Test). SERENGETI achieves best\nperformance on both of these datasets on Test (with\n73.18 F1 on the first and 71.25 on the second).\nPhrase Chunking. SERENGETI outperforms all\nmodels on the phrase chunking task on both Dev\nand Test data, reaching 90.51 F1 on Test.\nPart of Speech (POS) Tagging. In the POS tag-\nging task, SERENGETI outperformed all other\nmodels in the Dev. and Test sets.\nNews Classification. Our SERENGETI outper-\nforms other models on three out of four datasets on\nTest data (and on two datasets on Dev). 3. We do\nnot report SOTA results for Amharic, Kirnews, and\nKinnews datasets because their authors report per-\nformance in accuracy (and so are not comparable\nto our results). We show performance of SEREN-\nGETI on each category in the news classification\ncluster in Figure E.1 in the Appendix.\nSentiment Analysis. SERENGETI-E250 outper-\nforms other models on one out of three tasks in our\nsentiment analysis task cluster. Afro-XMLR and\nAfriBERTa outperform other models on one each.\nTo further investigate performance, we conduct an\nerror analysis on the three sentiment datasets (see\nFigure E.2 in the Appendix).\nTopic Classification. AfriBERTa outperforms\n3Our SERENGETI-E110 outperforms SERENGETI on\none dataset in Dev and Test sets\n7\n1504\nother models on both tasks in our topic classifica-\ntion cluster, followed by SERENGETI. We show\nconfusion matrices for Hausa and Yoruba topic\nclassification in Figure E.3 in the Appendix.\nLanguage Identification. SERENGETI outper-\nforms AfroLID on AfroLID and AfriSenti data\n(see Table 6 and 7 for details). We also compare\nthe performance of SERENGETI to AfroLID, and\nFranc4, on the 88 African languages represented in\nFranc in Table E.3 (Appendix). SERENGETI out-\nperforms AfroLID and Franc with an average F1\nscore of 96.29. SERENGETI outperforms both\nmodels on 59 languages and has similar results\nwith AfroLID on 19 languages. Next, we eval-\nuate the performance of SERENGETI on Creole\nlanguages. Again, we record improvement in res-\nults for Creole languages when compared with\nAfroLID. SERENGETI outperforms AfroLID in 7\nout of 9 languages and acquires similar scores on\n2 languages. We assume that the addition of the\nten most spoken languages to the pretraining data\nfor SERENGETI may have helped the model learn\nthe Creoles better. This is because Creoles share\nsome features including vocabularies and syntax\nwith some of those top ten languages.\n6.2 Error Analysis\nIn the sentiment analysis cluster, best performance\nis recorded for positive categories while negative\ncategories have the worst performance. A fine-\ngrained analysis of the Yoruba sentiment dataset\nfound that SERENGETI failed to correctly categor-\nize sentiment if the polarity item(s) were not seen\nin training, can be associated with both positive\nand negative sentiments, the polarity item(s) is a\nnegation, or if ambivalent markers are present in\nthe sentence. We provide a table showing examples\nof each type of error we found in Table E.2 in the\nAppendix. For the news classification task, polit-\nics and tourism are the best performing classes\nwhile education and relationships have the worst\nperformance on kirnews and kinnews respectively.\nIt is important to mention that the worst performing\ncategories do not have the smallest data sizes. For\nthe topic classification, the best performance is on\nthe world class for Hausa topic modelling while\nentertainment and sport have best performance for\nYoruba. The worst performance is on Nigeria and\nhealth for Hausa and Yoruba topic datasets respect-\nively.\n4A publicly available LID tool covering 88 African lan-\nguages.\n6.3 Imbalanced Distribution\nWe find imbalances in the class distributions for all\ndatasets except YOSM. We find a positive correla-\ntion between the size of each category in a dataset\nand the model accuracy. We also find a positive\ncorrelation with the number of examples in a spe-\ncific class and the accuracy we acquire. We provide\nconfusion matrices that represents the sizes of each\ncategory and the performance of SERENGETI in\nFigures E.4, E.5, and E.6 in the Appendix.\n6.4 Genealogy & Language Contact\nOur preliminary analyses show that language sim-\nilarity may improve model performance in zero-\nshot settings. This we believe is due to high cross-\nlingual transfer information (Conneau et al., 2020)\nfrom similar languages. Similar languages often\nshare many features (e.g., vocabulary, syntax, and\nscript) sometimes up to a point of mutual intel-\nligibility (Nassenstein, 2019; Arndt, 2015; Roy-\nCampbell, 2006). Languages in contact may also\nhave such similarities. By language in contact ,\nwe mean all languages that speakers of a specific\nlanguage interact with and influence. A language\ncan be in contact with another due to trade, geo-\ngraphic proximity, migration, or even colonization.\nLanguages in contact can influence each other in\nmultiple ways, such as borrowing words, grammat-\nical structures, phonology, or orthographic conven-\ntions (Matras, 2009). To illustrate our hypothesis,\nwe select two datasets with South African (SA)\nlanguages in AfroNLU - NCHLT-ner and phrase-\nchunk. We select SA languages because they are\ncontact languages (see Figure D.5 in Appendix for\na genealogical classification tree that highlights the\nSA languages.) (Nassenstein, 2019; Arndt, 2015;\nRoy-Campbell, 2006).\nTo determine the significance of language sim-\nilarity and language contact in our own zero-shot\nsettings, we measure the Jaccard similarity between\nthe pretraining data for the SA languages (see Table\n8). We find strong similarities between some of\nthese languages (see bolded examples in Table 8).\nWe also finetune a BERT model and compare the\nperformance of BERT with MBERT. We do this be-\ncause BERT does not include any similar language\nin its representation.\nXLM-R, mBERT, and AfriBERTa are not trained\non most SA languages but have high scores in\nzero-shot settings see Table 9 and Table E.4 in\nAppendix. We argue that XLM-R in addition to\n8\n1505\nafr nbl nso sot ssw tsn tso ven xho zul kin lug nya run sna som\nafr 1 0.28 0.35 0.26 0.27 0.36 0.29 0.22 0.42 0.38 0.34 0.38 0.26 0.25 0.25 0.43\nnbl 0.28 1 0.47 0.41 0.62 0.26 0.48 0.42 0.41 0.55 0.37 0.35 0.48 0.43 0.46 0.35\nnso 0.35 0.47 1 0.55 0.47 0.38 0.51 0.40 0.42 0.50 0.40 0.38 0.42 0.39 0.39 0.42\nsot 0.26 0.41 0.55 1 0.43 0.27 0.52 0.46 0.31 0.41 0.33 0.29 0.45 0.40 0.39 0.34\nssw 0.27 0.62 0.47 0.43 1 0.25 0.50 0.44 0.38 0.52 0.36 0.33 0.48 0.43 0.43 0.34\ntsn 0.36 0.26 0.38 0.27 0.25 1 0.28 0.21 0.39 0.36 0.31 0.36 0.25 0.24 0.23 0.37\ntso 0.29 0.48 0.48 0.52 0.50 0.28 1 0.47 0.37 0.48 0.38 0.34 0.51 0.44 0.44 0.37\nven 0.22 0.42 0.40 0.46 0.44 0.21 0.47 1 0.27 0.35 0.29 0.26 0.44 0.38 0.41 0.29\nxho 0.42 0.41 0.42 0.31 0.38 0.39 0.37 0.27 1 0.56 0.41 0.47 0.35 0.33 0.32 0.45\nzul 0.38 0.55 0.50 0.41 0.52 0.36 0.48 0.35 0.56 1 0.44 0.44 0.44 0.40 0.39 0.45\nTable 8: Jaccard Similarity for South African languages and some languages that are genealogically similar to them.\nEach of the 10 South African languages are represented on each row. The genealogically similar languages we\nexplore are after the horizontal lines. Specifically, we have: Kinyarwanda (kin), Luganda (lug), Chichewa (nya),\nRundi (run), Shona (sna) and Somali (som). We highlight similarity scores of 0.4 and above in bold face.\nDataset Lang XLMR BERT mBERT Affro-XLMR AfriBERTa SERENGETI\nNCHLT-NER\nafr 80.68±0.75 71.47 80.08 ±0.29 80.55±0.11 74.5 ±0.64 81.57±0.59\nnbl 74.64 ±0.66 61.02 73.48 ±0.18 75.26±0.28 72.28 ±0.67 77.13±0.67\nnso 77.0 ±1.23 64.27 78.75 ±0.45 80.13±0.51 75.45 ±1.09 80.69±0.64\nsot 54.71 ±1.51 49.75 54.68 ±0.49 55.57±0.2 54.09 ±0.98 56.26±1.52\nssw 71.75 ±0.65 65.18 71.24 ±0.75 72.35±1.02 69.38 ±0.58 73.37±0.82\ntsn 77.02 ±0.22 70.96 76.35 ±0.47 77.68±0.96 73.89 ±1.41 79.05±0.75\ntso 74.24 ±0.08 65.09 72.95 ±0.67 74.85 ±0.43 71.05 ±0.9 75.13±0.31\nven 64.06 ±0.31 61.51 63.11 ±1.27 64.39 ±0.36 63.24 ±1.26 65.42±0.76\nxho 70.77±2.45 58.17 68.54 ±1.44 72.37±0.39 67.00 ±1.27 72.92±0.29\nzul 69.44 ±0.62 54.27 67.74 ±1.46 70.28±0.49 67.17 ±0.15 71.20±0.44\nPhrase Chunk\nafr 95.34±0.16 89.92 95.68 ±0.30 95.13±0.06 90.22 ±0.81 96.01±0.14\nnso 96.57 ±0.61 95.26 96.85 ±0.55 98.36±0.2 96.47 ±0.14 98.28±0.1\nsot 82.93 ±0.38 80.59 83.08 ±0.78 85.28±0.61 82.18 ±0.93 85.69±0.76\nssw 82.9 ±1.03 82.09 81.91 ±0.47 84.73±0.18 83.24 ±0.11 83.45±0.12\ntsn 92.77 ±0.16 92.09 92.64 ±0.66 94.11±0.49 92.71 ±0.42 94.03±0.19\ntso 86.42 ±0.46 86.75 86.90 ±0.31 87.39 ±0.18 86.73 ±0.95 89.32±0.43\nven 92.31 ±0.45 92.32 90.47 ±0.32 92.42 ±0.68 92.02 ±0.33 92.54±0.21\nzul 87.30 ±0.26 84.93 87.29 ±1.04 88.67±0.66 85.74 ±0.55 90.05±0.81\nTable 9: Performance of mPLMs and BERT on each language in NCHLT-NER and Phrase-Chunk datasets we use\nfor the genealogy analysis. (F1) score is the evaluation metric. We use Red highlights to indicate languages in\nzero-shot setting. We evaluate BERT, a monolingual model as a sanity check for our evaluation.\ncross-lingual transfers from other languages ac-\nquires representation from afr and xho where xho\nalone shares more than 0.4 similarity with afr, nbl,\nnso, and zul. mBERT also learns representation\nfrom afr while AfriBERTa learns representations\nfrom Gahuza which is a code-mixed variety of KIN\nand RUN. BERT on the other hand significantly\nperforms lower than MBERT in all languages ex-\ncept on ssw, and ven (Phrase chunk). SERENGETI,\nhowever, outperforms other models on these lan-\nguages which demonstrates the impact of pretrain-\ning on each of these languages.\nThese analyses are in no way conclusive, but\ndo provide insights on how linguistic information\nmay impact model performance in zero-shot set-\ntings. Future work can further probe the influence\nof similar languages in a more in-depth fashion.\n(See Appendix F for detailed analysis).\n7 Conclusion\nWe reported our efforts to develop SERENGETI, a\nsuite of three massively multilingual language mod-\nels for African NLP. SERENGETI outperforms4\nmPLMs on 11 datasets across 8 tasks. We provide\nextensive evaluations of model outputs, including\nzero-shot performance of the mPLMs. We also\noffer broad linguistically-motivated analyses of\nmodel performance.\n9\n1506\n8 Limitations\nWe identify the following limitations for our work:\n1. Due to limited access to a wide network of nat-\nive speakers from the majority of languages,\nwe were able to manually inspect only a\nsubset of languages present in our pretrain-\ning data. Specifically, we could only manu-\nally evaluate Afrikaans, Yorùbá, Igbo, Hausa,\nLuganda, Kinyarwanda, Chichewa, Shona,\nSomali, Swahili, Xhosa, Bemba, and Zulu.\nFuture work should focus on increasing the\nsubset of languages evaluated manually in or-\nder to ensure quality. We believe automatic\nanalyses are not sufficient before development\nof models that get deployed in particular ap-\nplications.\n2. Another limitation is related to our inability\nto perform extensive analysis of biases and\nhateful speech present in our pretraining data.\nAgain, this is due to relatively restricted ac-\ncess to native speakers (and even automated\ntools) to perform this analysis. As a result,\nwe cannot fully ensure that our models is free\nfrom biases and socially undesirable effects.\nTherefore, it is important that these models be\nused with care and caution, and be analyzed\nfor biases and socially undesirable effects be-\nfore use.\n3. Additionally, due to unavailability of suffi-\ncient computing resources, we were unable\nto evaluate large language models such as\nBLOOM, even though it covers 22 African\nlanguages.\n4. Finally, even though AfroNLU has diverse\ntasks at the word and sentence level, these\ntasks only cover few African languages. We\ntherefore encourage the creation of more data-\nsets for downstream NLU tasks in more (and\nmore diverse) African languages. We believe\nbroader benchmarks will continue to be im-\nportant for future progress in African NLP.\n9 Ethics Statement and Wider Impacts\nSERENGETI aligns with Afrocentric NLP where\nthe needs of African people is put into consider-\nation when developing technology. We believe\nSERENGETI will not only be useful to speakers\nof the languages supported, but also researchers of\nAfrican languages such as anthropologists and lin-\nguists. We discuss below some use cases for SER-\nENGETI and offer a number of broad impacts.\n1. SERENGETI aims to address the lack of ac-\ncess to technology in about90% of the world’s\nlanguages, which automatically discriminates\nagainst native speakers of those languages.\nMore precisely, it does so by focusing on\nAfrica. To the best of our knowledge, SER-\nENGETI is the first massively multilingual\nPLM developed for African languages and\nlanguage varieties. A model with knowledge\nof 517 African languages, is by far the largest\nto date for African NLP.\n2. SERENGETI enables improved access of im-\nportant information to the African community\nin Indigenous African languages. This is es-\npecially beneficial for people who may not be\nfluent in other languages. This will potentially\nconnect more people globally.\n3. SERENGETI affords opportunities for lan-\nguage preservation for many African lan-\nguages. To the best of our knowledge, SER-\nENGETI consists of languages that have not\nbeen used for any NLP task until now. We\nbelieve that it can help encourage continued\nuse of these languages in several domains, as\nwell as trigger future development of language\ntechnologies for many of these languages.\n4. To mitigate discrimination and bias, we ad-\nopt a manual curation of our datasets. Native\nspeakers of Afrikaans, Yorùbá, Igbo, Hausa,\nLuganda, Kinyarwanda, Chichewa, Shona,\nSomali, Swahili, Xhosa, Bemba, and Zulu\nalso manually evaluated a subset of the data to\nensure its quality. The data collected for this\nwork is taken from various domains to further\nensure a better representation of the language\nusage of native speakers.\n5. Although LMs are useful for a wide range of\napplications, they can also be misused. SER-\nENGETI is developed using publicly available\ndatasets that may carry biases. Although we\nstrive to perform analyses and diagnostic case\nstudies to probe performance of our models,\nour investigations are by no means compre-\nhensive nor guarantee absence of bias in the\ndata. In particular, we do not have access\n10\n1507\nto native speakers of most of the languages\ncovered. This hinders our ability to investigate\nsamples from each (or at least the majority) of\nthe languages.\nAcknowledgements\nMAM gratefully acknowledges support from\nCanada Research Chairs (CRC), the Natural Sci-\nences and Engineering Research Council of Canada\n(NSERC; RGPIN-2018-04267), the Social Sci-\nences and Humanities Research Council of Canada\n(SSHRC; 435-2018-0576; 895-2020-1004; 895-\n2021-1008), Canadian Foundation for Innova-\ntion (CFI; 37771), Digital Research Alliance of\nCanada,5 UBC ARC-Sockeye,6 Advanced Micro\nDevices, Inc. (AMD), and Google. Any opinions,\nconclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not neces-\nsarily reflect the views of CRC, NSERC, SSHRC,\nCFI, the Alliance, AMD, Google, or UBC ARC-\nSockeye.\nReferences\nJulien Abadji, Pedro Javier Ortiz Suárez, Laurent Ro-\nmary, and Benoît Sagot. 2021. Ungoliant: An optim-\nized pipeline for the generation of a very large-scale\nmultilingual web corpus. Proceedings of the Work-\nshop on Challenges in the Management of Large\nCorpora (CMLC-9) 2021. Limerick, 12 July 2021\n(Online-Event), pages 1 – 9, Mannheim. Leibniz-\nInstitut für Deutsche Sprache.\nIfe Adebara and Muhammad Abdul-Mageed. 2022. To-\nwards afrocentric NLP for African languages: Where\nwe are and where we can go. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3814–3841, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nIfe Adebara, Muhammad Abdul-Mageed, and Miikka\nSilfverberg. 2022a. Linguistically-motivated Yorùbá-\nEnglish machine translation. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 5066–5075, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nIfe Adebara, AbdelRahim Elmadany, Muhammad\nAbdul-Mageed, and Alcides Alcoba Inciarte. 2022b.\nAfroLID: A neural language identification tool for\nAfrican languages.\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia\nKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\n5https://alliancecan.ca\n6https://arc.ubc.ca/ubc-arc-sockeye\nDietrich Klakow, Peter Nabende, Ernie Chang, Tajud-\ndeen Gwadabe, Freshia Sackey, Bonaventure F. P.\nDossou, Chris Emezue, Colin Leong, Michael Beuk-\nman, Shamsuddeen Muhammad, Guyo Jarso, Oreen\nYousuf, Andre Niyongabo Rubungo, Gilles Hacheme,\nEric Peter Wairagala, Muhammad Umair Nasir,\nBenjamin Ajibade, Tunde Ajayi, Yvonne Gitau,\nJade Abbott, Mohamed Ahmed, Millicent Ochieng,\nAnuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,\nFatoumata Ouoba Kabore, Godson Kalipe, Derguene\nMbaye, Allahsera Auguste Tapo, Victoire Memd-\njokam Koagne, Edwin Munkoh-Buabeng, Valen-\ncia Wagner, Idris Abdulmumin, Ayodele Awokoya,\nHappy Buzaaba, Blessing Sibanda, Andiswa Bukula,\nand Sam Manthalu. 2022a. A few thousand transla-\ntions go a long way! leveraging pre-trained models\nfor African news translation. In Proceedings of the\n2022 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 3053–3070,\nSeattle, United States. Association for Computational\nLinguistics.\nDavid Ifeoluwa Adelani, Graham Neubig, Se-\nbastian Ruder, Shruti Rijhwani, Michael Beuk-\nman, Chester Palen-Michel, Constantine Lignos,\nJesujoba O. Alabi, Shamsuddeen H. Muhammad,\nPeter Nabende, Cheikh M. Bamba Dione, Andiswa\nBukula, Rooweither Mabuya, Bonaventure F. P. Dos-\nsou, Blessing Sibanda, Happy Buzaaba, Jonathan\nMukiibi, Godson Kalipe, Derguene Mbaye, Amelia\nTaylor, Fatoumata Kabore, Chris Chinenye Emezue,\nAnuoluwapo Aremu, Perez Ogayo, Catherine Gitau,\nEdwin Munkoh-Buabeng, Victoire M. Koagne, Al-\nlahsera Auguste Tapo, Tebogo Macucwa, Vukosi\nMarivate, Elvis Mboning, Tajuddeen Gwadabe, Tosin\nAdewumi, Orevaoghene Ahia, Joyce Nakatumba-\nNabende, Neo L. Mokono, Ignatius Ezeani, Chia-\nmaka Chukwuneke, Mofetoluwa Adeyemi, Gilles Q.\nHacheme, Idris Abdulmumin, Odunayo Ogundepo,\nOreen Yousuf, Tatiana Moteu Ngoli, and Dietrich\nKlakow. 2022b. Masakhaner 2.0: Africa-centric\ntransfer learning for named entity recognition.\nAlham Fikri Aji, Genta Indra Winata, Fajri Koto,\nSamuel Cahyawijaya, Ade Romadhony, Rahmad\nMahendra, Kemal Kurniawan, David Moeljadi,\nRadityo Eko Prasojo, Timothy Baldwin, Jey Han\nLau, and Sebastian Ruder. 2022. One country, 700+\nlanguages: NLP challenges for underrepresented lan-\nguages and dialects in Indonesia. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 7226–7249, Dublin, Ireland. Association for\nComputational Linguistics.\nJesujoba Alabi, Kwabena Amponsah-Kaakyire, David\nAdelani, and Cristina Espana-Bonet. 2020. Massive\nvs. curated embeddings for low-resourced languages:\nThe case of Yorùbá and Twi. In Proceedings of The\n12th Language Resources and Evaluation Confer-\nence, pages 2754–2762.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022a. Adapting\n11\n1508\npre-trained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336–4349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022b. Multilingual\nlanguage model adaptive fine-tuning: A study on\nAfrican languages.\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren\nKirchner. 2018. Machine bias. Nieman reports,\n72(3/4):37.\nJochen S. Arndt. 2015. Missionaries, Africans and\nthe Emergence of Xhosa and Zulu as Distinct Lan-\nguages in South Africa, 1800-54. Ph.D. thesis. Copy-\nright - Database copyright ProQuest LLC; ProQuest\ndoes not claim copyright in the individual underlying\nworks; Last updated - 2022-11-02.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nIsrael Abebe Azime and Nebil Mohammed. 2021. An\nAmharic News Text classification Dataset.\nSolon Barocas and Andrew D. Selbst. 2016. Big data’s\ndisparate impact. California law review, 104(3):671–\n732.\nEmily M. Bender. 2011. On achieving and evaluating\nlanguage-independence in nlp. Linguistic Issues in\nLanguage Technology, 6.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nLaurent Besacier, Etienne Barnard, Alexey Karpov, and\nTanja Schultz. 2014. Automatic speech recognition\nfor under-resourced languages: A survey. Speech\nCommunication, 56:85–100.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Ven-\nkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. In Advances in\nNeural Information Processing Systems, volume 29.\nCurran Associates, Inc.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clem-\nens Winter, Chris Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clem-\nens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020b. Language models are few-shot learners. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems , NIPS’20,\nRed Hook, NY , USA. Curran Associates Inc.\nJoy Buolamwini and Timnit Gebru. 2018. Gender\nshades: Intersectional accuracy disparities in com-\nmercial gender classification. In Proceedings of\nthe 1st Conference on Fairness, Accountability and\nTransparency, volume 81 of Proceedings of Machine\nLearning Research, pages 77–91. PMLR.\nPei-Xuan Cai, Yao-Chung Fan, and Fang-Yie Leu.\n2022. Compare encoder-decoder, encoder-only, and\ndecoder-only architectures for text generation on low-\nresource datasets. In Advances on Broad-Band Wire-\nless Computing, Communication and Applications ,\npages 216–225, Cham. Springer International Pub-\nlishing.\nAylin Caliskan, Joanna J Bryson, and Arvind Naray-\nanan. 2017. Semantics derived automatically from\nlanguage corpora contain human-like biases. Science,\n356(6334):183–186.\nBharathi Raja Chakravarthi and Vigneshwaran Muralid-\naran. 2021. Findings of the shared task on hope\nspeech detection for equality, diversity, and inclu-\nsion. In Proceedings of the First Workshop on Lan-\nguage Technology for Equality, Diversity and Inclu-\nsion, pages 61–72, Kyiv. Association for Computa-\ntional Linguistics.\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma,\nSaksham Singhal, Payal Bajaj, Xia Song, and Furu\nWei. 2021. Xlm-e: Cross-lingual language model\npre-training via ELECTRA.\nJames Clackson. 2007. The Indo-European language\nfamily, Cambridge Textbooks in Linguistics, page\n1–26. Cambridge University Press.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020a. TyDi QA: A bench-\nmark for information-seeking question answering in\n12\n1509\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020b. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learning\nRepresentations.\nBernard Comrie. 2017. Languages of the World ,\nchapter 2. John Wiley & Sons, Ltd.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Fran-\ncisco Guzmán, Edouard Grave, Myle Ott, Luke\nZettlemoyer, and Veselin Stoyanov. 2020. Unsuper-\nvised cross-lingual representation learning at scale.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nDenis Creissels, Gerrit J Dimmendaal, Zygmunt Fra-\njzyngier, and Christa König. 2008. Africa as a\nmorphosyntactic area. A linguistic geography of\nAfrica, 86150.\nBrian Daigle. 2021. Data protection laws in Africa: A\nPan-African survey and noted trends. J. Int’l Com. &\nEcon., page 1.\nDavis David. 2020a. Swahili : News classification data-\nset.\nDavis David. 2020b. Swahili : News classification\ndataset. The news version contains both train and\ntest sets.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMountaga Diallo, Chayma Fourati, and Hatem Had-\ndad. 2021. Bambara language dataset for sentiment\nanalysis.\nGerrit J. Dimmendaal. 2020. 364 Nilo-Saharan and Its\nLimits. In The Oxford Handbook of African Lan-\nguages. Oxford University Press.\nGerrit J Dimmendaal, Colleen Ahland, Angelika Jakobi,\nand Constance Kutsch Lojenga. 2019. Linguistic\nfeatures and typologies in languages commonly re-\nferred to as ‘Nilo-Saharan’. Cambridge Handbook\nof African Languages, pages 326–381.\nSumanth Doddapaneni, Gowtham Ramesh, Mitesh M.\nKhapra, Anoop Kunchukuttan, and Pratyush Kumar.\n2021. A primer on pretrained multilingual language\nmodels.\nBonaventure F. P. Dossou, Atnafu Lambebo Tonja,\nOreen Yousuf, Salomey Osei, Abigail Oppong, Iy-\nanuoluwa Shode, Oluwabusayo Olufunke Awoyomi,\nand Chris Chinenye Emezue. 2022. Afrolm: A\nself-active learning-based multilingual pretrained lan-\nguage model for 23 African languages.\nMatthew S Dryer. 2013. Order of subject, object and\nverb. the world atlas of language structures online, ed.\nby matthew s. dryer and martin haspelmath. leipzig:\nMax planck institute for evolutionary anthropology.\nOnline: https://wals. info.\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer\nReingold, and Rich Zemel. 2011. Fairness through\nawareness. Technical report, Cornell University Lib-\nrary, arXiv.org.\nDavid M Eberhard, F Simons Gary, and Charles D Fen-\nnig (eds). 2021. Ethnologue: Languages of the world.\nTwenty-fourth edition, Dallas, Texas: SIL Interna-\ntional.\nAllyson Ettinger. 2020. What BERT Is Not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics for\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nVirginia Eubanks. 2018. Automating inequality: how\nhigh-tech tools profile, police, and punish the poor,\nfirst edition. St. Martin’s Press, New York, NY .\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy Lipt-\nchinsky, Sergey Edunov, Edouard Grave, Michael\nAuli, and Armand Joulin. 2020. Beyond English-\nCentric multilingual machine translation.\nEduard Fosch-Villaronga and Adam Poulsen. 2022. Di-\nversity and Inclusion in Artificial Intelligence, pages\n109–134. T.M.C. Asser Press, The Hague.\nZygmunt Frajzyngier. 2018. Afroasiatic languages.\nAna Freire, Lorenzo Porcaro, and Emilia Gómez. 2021.\nMeasuring diversity of artificial intelligence confer-\nences. In Proceedings of 2nd Workshop on Diversity\nin Artificial Intelligence (AIDBEI) , volume 142 of\nProceedings of Machine Learning Research, pages\n39–50. PMLR.\nLudwig Gerhardt. 2020. 125 Reflections on the History\nof African Language Classification. In The Oxford\nHandbook of African Languages. Oxford University\nPress.\nDavid Gil and Antoinette Schapper, editors. 2020. Aus-\ntronesian Undressed: How and why languages be-\ncome isolating. John Benjamins.\nJeff Good. 2020. 138139 Niger-Congo, with a Special\nFocus on Benue-Congo. In The Oxford Handbook of\nAfrican Languages. Oxford University Press.\n13\n1510\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale Trans-\nformers for multilingual masked language modeling.\nIn Proceedings of the 6th Workshop on Represent-\nation Learning for NLP (RepL4NLP-2021) , pages\n29–33, Online. Association for Computational Lin-\nguistics.\nTom Güldemann, editor. 2018. The Languages and\nLinguistics of Africa. De Gruyter Mouton.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Jesu-\njoba Alabi, Udia Markus, and Dietrich Klakow. 2020.\nTransfer learning and distant supervision for mul-\ntilingual Transformer models: A study on African\nlanguages. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2580–2591, Online. Association for\nComputational Linguistics.\nSara Hooker. 2021. Moving beyond “algorithmic bias\nis a data problem”. Patterns, 2(4):100241.\nDirk Hovy, Federico Bianchi, and Tommaso Fornaciari.\n2020. “You Sound Just Like Your Father” Commer-\ncial Machine Translation Systems Include Stylistic\nBiases. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1686–1690, Online. Association for Computational\nLinguistics.\nLarry M Hyman. 2003. African languages and phonolo-\ngical theory. Glot International, 7(6):153–163.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig,\nDaniel D’souza, Julia Kreutzer, Constantine Lignos,\nChester Palen-Michel, Happy Buzaaba, Shruti Rijh-\nwani, Sebastian Ruder, et al. 2021. Masakhaner:\nNamed entity recognition for African languages.\narXiv e-prints, pages arXiv–2103.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nOdunayo Jude Ogundepo, Akintunde Oladipo,\nMofetoluwa Adeyemi, Kelechi Ogueji, and Jimmy\nLin. 2022. AfriTeV A: Extending ?small data?\npretraining approaches to sequence-to-sequence\nmodels. In Proceedings of the Third Workshop on\nDeep Learning for Low-Resource Natural Language\nProcessing, pages 126–135, Hybrid. Association for\nComputational Linguistics.\nJohanita Kirsten. 2018. Afrikaans, pages 13–30. Pal-\ngrave Macmillan UK, London.\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-\nlingual constituency parsing with self-attention and\npre-training. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3499–3505, Florence, Italy. Association for\nComputational Linguistics.\nAllison Koenecke, Andrew Nam, Emily Lake, Joe\nNudell, Minnie Quartey, Zion Mengesha, Connor\nToups, John R. Rickford, Dan Jurafsky, and Sharad\nGoel. 2020. Racial disparities in automated speech\nrecognition. Proceedings of the National Academy of\nSciences of the United States of America, 117(14):pp.\n7684–7689.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wa-\nhab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-\nlahsera Tapo, Nishant Subramani, Artem Sokolov,\nClaytone Sikasote, Monang Setyawan, Supheakmun-\ngkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera,\nAnnette Rios, Isabel Papadimitriou, Salomey Osei,\nPedro Ortiz Suárez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias Müller, André Müller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyakeni,\nJamshidbek Mirzakhalov, Tapiwanashe Matangira,\nColin Leong, Nze Lawson, Sneha Kudugunta, Yacine\nJernite, Mathias Jenny, Orhan Firat, Bonaventure\nF. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sak-\nine Çabuk Ballı, Stella Biderman, Alessia Battisti,\nAhmed Baruwa, Ankur Bapna, Pallavi Baljekar, Is-\nrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2021. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. arXiv preprint arXiv:2103.12028.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguist-\nics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nAlex Boniface Makulilo. 2012. Privacy and data protec-\ntion in Africa: a state of the art. International Data\nPrivacy Law, 2(3):163–178.\n14\n1511\nNina Markl. 2022. Language variation and algorithmic\nbias: understanding algorithmic bias in British Eng-\nlish automatic speech recognition. In Proceedings of\n2022 5th ACM Conference on Fairness, Accountabil-\nity, and Transparency (FAccT 2022), pages 521–534.\nACM Association for Computing Machinery.\nJoshua L Martin and Kevin Tang. 2020. Understanding\nracial disparities in automatic speech recognition:\nThe case of habitual\" be\". In INTERSPEECH, pages\n626–630.\nYaron Matras. 2009. Contact languages, Cambridge\nTextbooks in Linguistics, page 275–307. Cambridge\nUniversity Press.\nJosh Meyer, Lindy Rauchenstein, Joshua D. Eisen-\nberg, and Nicholas Howell. 2020. Artie bias cor-\npus: An open dataset for detecting demographic\nbias in speech applications. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 6462–6468, Marseille, France. European\nLanguage Resources Association.\nMargaret Mitchell, Dylan Baker, Nyalleng Moorosi,\nEmily Denton, Ben Hutchinson, Alex Hanna, Timnit\nGebru, and Jamie Morgenstern. 2020. Diversity and\ninclusion metrics in subset selection. In Proceedings\nof the AAAI/ACM Conference on AI, Ethics, and So-\nciety, AIES ’20, page 117–123, New York, NY , USA.\nAssociation for Computing Machinery.\nShamsuddeen Hassan Muhammad, David Ifeoluwa\nAdelani, Sebastian Ruder, Ibrahim Said Ahmad,\nIdris Abdulmumin, Bello Shehu Bello, Monojit\nChoudhury, Chris Chinenye Emezue, Saheed Sa-\nlahudeen Abdullahi, Anuoluwapo Aremu, Alipio Je-\norge, and Pavel Brazdil. 2022. NaijaSenti: A Ni-\ngerian Twitter Sentiment Corpus for Multilingual\nSentiment Analysis.\nNico Nassenstein. 2019. Kinyarwanda and Kirundi:\nOn Colonial Divisions, Discourses of National Be-\nlonging, and Language Boundaries. Modern Africa:\nPolitics, History and Society, 7(1):11–40.\nRubungo Andre Niyongabo, Qu Hong, Julia Kreutzer,\nand Li Huang. 2020. KINNEWS and KIRNEWS:\nBenchmarking cross-lingual text classification for\nKinyarwanda and Kirundi. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 5507–5521, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nAntoine Nzeyimana and Andre Niyongabo Rubungo.\n2022. KinyaBERT: a morphology-aware Kinyar-\nwanda language model. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5347–\n5363, Dublin, Ireland. Association for Computational\nLinguistics.\nZiad Obermeyer, Brian Powers, Christine V ogeli, and\nSendhil Mullainathan. 2019. Dissecting racial bias\nin an algorithm used to manage the health of popu-\nlations. Science (American Association for the Ad-\nvancement of Science), 366(6464):447–453.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data? no problem! exploring the viability\nof pretrained multilingual language models for low-\nresourced languages. In Proceedings of the 1st Work-\nshop on Multilingual Representation Learning, pages\n116–126, Punta Cana, Dominican Republic. Associ-\nation for Computational Linguistics.\nIkechukwu E Onyenwe, Mark Hepple, Uchechukwu\nChinedu, and Ignatius Ezeani. 2018. A basic lan-\nguage resource kit implementation for the IgboNLP\nproject. ACM Trans. Asian Low-Resour. Lang. Inf.\nProcess., 17(2).\nIkechukwu E. Onyenwe, Mark Hepple, Uchechukwu\nChinedu, and Ignatius Ezeani. 2019. Toward an\neffective Igbo part-of-speech tagger. ACM Trans.\nAsian Low-Resour. Lang. Inf. Process., 18(4).\nWuraola Fisayo Oyewusi, Olubayo Adekanmbi, and\nOlalekan Akinsande. 2020. Semantic enrichment\nof Nigerian Pidgin English for contextual sentiment\nclassification. arXiv preprint arXiv:2003.12450.\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. IEEE Transactions on Knowledge\nand Data Engineering, 22(10):1345–1359.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long\nPapers), pages 1946–1958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repres-\nentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nVictor Porkhomovsky. 2020. 269Afro-Asiatic Over-\nview. In The Oxford Handbook of African Languages.\nOxford University Press.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\n15\n1512\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\nTransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019.\nMassively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nZaline Makini Roy-Campbell. 2006. The state of\nAfrican languages and the global language politics:\nEmpowering African languages in the era of global-\nization. In Selected proceedings of the 36th annual\nconference on African linguistics, pages 1–13. Cas-\ncadilla Proceedings Project Somerville, MA.\nSebastian Ruder. 2022. The State of Mul-\ntilingual AI. http://ruder.io/\nstate-of-multilingual-ai/ .\nSebastian Ruder, Matthew E. Peters, Swabha Swayam-\ndipta, and Thomas Wolf. 2019. Transfer learning in\nnatural language processing. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Tu-\ntorials, pages 15–18, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki,\nElizabeth-Jane Pavlick, Suzana Ili’c, Daniel Hesslow,\nRoman Castagn’e, Alexandra Sasha Luccioni, Franc-\ncois Yvon, Matthias Gallé, Jonathan Tow, Alexan-\nder M. Rush, Stella Rose Biderman, Albert Web-\nson, Pawan Sasanka Ammanamanchi, Thomas Wang,\nBenoît Sagot, Niklas Muennighoff, Albert Villanova\ndel Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Beltagy, Huu\nNguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz\nSuarez, Victor Sanh, Hugo Laurenccon, Yacine\nJernite, Julien Launay, Margaret Mitchell, Colin\nRaffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa\nEtxabe, Alham Fikri Aji, Amit Alfassy, Anna Ro-\ngers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao\nMou, Chris C. Emezue, Christopher Klamm, Colin\nLeong, Daniel Alexander van Strien, David Ife-\noluwa Adelani, Dragomir R. Radev, Eduardo G.\nPonferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar\nNatan, Francesco De Toni, Gérard Dupont, Germán\nKruszewski, Giada Pistilli, Hady ElSahar, Hamza\nBenyamina, Hieu Tran, Ian Yu, Idris Abdulmu-\nmin, Isaac Johnson, Itziar Gonzalez-Dios, Javier\nde la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, Jorg Frohberg, Josephine L. To-\nbing, Joydeep Bhattacharjee, Khalid Almubarak,\nKimbo Chen, Kyle Lo, Leandro von Werra, Leon\nWeber, Long Phan, Loubna Ben Allal, Ludovic Tan-\nguy, Manan Dey, Manuel Romero Muñoz, Maraim\nMasoud, Mar’ia Grandury, Mario vSavsko, Max\nHuang, Maximin Coavoux, Mayank Singh, Mike\nTian-Jian Jiang, Minh Chien Vu, Mohammad Ali\nJauhar, Mustafa Ghaleb, Nishant Subramani, Nora\nKassner, Nurulaqilla Khamis, Olivier Nguyen, Omar\nEspejel, Ona de Gibert, Paulo Villegas, Peter Hende-\nrson, Pierre Colombo, Priscilla Amuok, Quentin\nLhoest, Rheza Harliman, Rishi Bommasani, Roberto\nL’opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,\nSebastian Nagel, Shamik Bose, Shamsuddeen Hassan\nMuhammad, Shanya Sharma, S. Longpre, Somaieh\nNikpoor, Stanislav Silberberg, Suhas Pai, Sydney\nZink, Tiago Timponi Torrent, Timo Schick, Tristan\nThrush, Valentin Danchev, Vassilina Nikoulina, Ver-\nonika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Elizabeth Salesky, Sab-\nrina J. Mielke, Wilson Y . Lee, Abheesht Sharma, An-\ndrea Santilli, Antoine Chaffin, Arnaud Stiegler, Deba-\njyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica,\nNihal V . Nayak, Ryan Teehan, Samuel Albanie,\nSheng Shen, Srulik Ben-David, Stephen H. Bach,\nTaewoon Kim, Tali Bers, Thibault Févry, Trishala\nNeeraj, Urmish Thakker, Vikas Raunak, Xiang Tang,\nZheng Xin Yong, Zhiqing Sun, Shaked Brody, Y Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Franc-\ncois Lavall’ee, Rémi Lacroix, Samyam Rajbhandari,\nSanchit Gandhi, Shaden Smith, Stéphane Requena,\nSuraj Patil, Tim Dettmers, Ahmed Baruwa, Aman-\npreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur’elie N’ev’eol,\nCharles Lovering, Daniel H Garrette, Deepak R.\nTunuguntla, Ehud Reiter, Ekaterina Taktasheva, Eka-\nterina V oloshina, Eli Bogdanov, Genta Indra Winata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jekater-\nina Novikova, Jessica Zosa Forde, Jordan Clive,\nJungo Kasai, Ken Kawamura, Liam Hazan, Mar-\nine Carpuat, Miruna Clinciu, Najoung Kim, Newton\nCheng, Oleg Serikov, Omer Antverg, Oskar van der\nWal, Rui Zhang, Ruochen Zhang, Sebastian Gehr-\nmann, S. Osher Pais, Tatiana Shavrina, Thomas\nScialom, Tian Yun, Tomasz Limisiewicz, Verena\nRieser, Vitaly Protasov, Vladislav Mikhailov, Yada\nPruksachatkun, Yonatan Belinkov, Zachary Bamber-\nger, Zdenvek Kasner, Alice Rueda, Amanda Pest-\nana, Amir Feizpour, Ammar Khan, Amy Faranak,\nAnanda Santa Rosa Santos, Anthony Hevia, Anti-\ngona Unldreaj, Arash Aghagol, Arezoo Abdollahi,\nAycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Olusola Ajibade, Bharat Ku-\nmar Saxena, Carlos Muñoz Ferrandis, Danish Con-\ntractor, David M. Lansky, Davis David, Douwe\n16\n1513\nKiela, Duong Anh Nguyen, Edward Tan, Emily\nBaylor, Ezinwanne Ozoani, Fatim T Mirza, Frank-\nline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani\nBhattacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis\nSanz, Karen Fort, Lívia Macedo Dutra, Mairon\nSamagaio, Maraim Elbadri, Margot Mieskes, Marissa\nGerchick, Martha Akinlolu, Michael McKenna, Mike\nQiu, M. K. K. Ghauri, Mykola Burynok, Nafis Abrar,\nNazneen Rajani, Nour Elkott, Nourhan Fahmy, Olan-\nrewaju Modupe Samuel, Ran An, R. P. Kromann,\nRyan Hao, Samira Alizadeh, Sarmad Shubber,\nSilas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-\nCong Le, Tobi Oyebade, Trieu Nguyen Hai Le,\nYoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh\nKashyap, Alfredo Palasciano, Alison Callahan, An-\nima Shukla, Antonio Miranda-Escalada, Ayush Ku-\nmar Singh, Benjamin Beilharz, Bo Wang, Caio\nMatheus Fonseca de Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on\nPerin’an, Daniel Molano, Dian Yu, Enrique Man-\njavacas, Fabio Barth, Florian Fuhrimann, Gabriel\nAltay, Giyaseddin Bayrak, Gully A. Burns, Helena U.\nVrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang,\nJohn Giorgi, Jonas Golde, Jose David Posada, Karthi\nSivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shin-\nzato, Madeleine Hahn de Bykhovetz, Maiko Takeu-\nchi, Marc Pàmies, María Andrea Castillo, Marianna\nNezhurina, Mario Sanger, Matthias Samwald, Mi-\nchael Cullan, Michael Weinberg, M Wolf, Mina\nMihaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg, Nich-\nolas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patricia Haller, R. Chandrasekhar, R. Eis-\nenberg, Robert Martin, Rodrigo L. Canalli, Ros-\naline Su, Ruisi Su, Samuel Cahyawijaya, Samuele\nGarda, Shlok S Deshmukh, Shubhanshu Mishra, Sid\nKiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti\nKumar, Stefan Schweter, Sushil Pratap Bharati, T. A.\nLaud, Th’eo Gigant, Tomoya Kainuma, Wojciech\nKusa, Yanis Labrak, Yashasvi Bajaj, Y . Venkatra-\nman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao\nTan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes\nBelkada, and Thomas Wolf. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\nArXiv, abs/2211.05100.\nGuillaume Segerer. 2008. Closed adjective classes and\nprimary adjectives in African Languages. Working\npaper or preprint.\nIyanuoluwa Shode, David Ifeoluwa Adelani, and Anna\nFeldman. 2022. YOSM: A new Yorùbá Senti-\nment Corpus for Movie Reviews. AfricaNLP 2022\n@ICLR.\nGabriele Sommer. 2020. 889 Pidgin and Creole Lan-\nguages. In The Oxford Handbook of African Lan-\nguages. Oxford University Press.\nXinying Song, Alex Salcianu, Yang Song, Dave Dopson,\nand Denny Zhou. 2021. Fast WordPiece tokenization.\nIn Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n2089–2103, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nEwan Sutherland. 2018. Digital privacy in Africa : cy-\nbersecurity, data protection & surveillance. Data\nProtection & Surveillance (June 22, 2018).\nRachael Tatman. 2017. Gender and dialect bias in You-\nTube’s automatic captions. In Proceedings of the\nFirst ACL Workshop on Ethics in Natural Language\nProcessing, pages 53–59, Valencia, Spain. Associ-\nation for Computational Linguistics.\nRachael Tatman and Conner Kasten. 2017. Effects of\nTalker Dialect, Gender & Race on Accuracy of Bing\nSpeech and YouTube Automatic Captions. In Proc.\nInterspeech 2017, pages 934–938.\nS. Thara and Prabaharan Poornachandran. 2021. Trans-\nformer based language identification for Malayalam-\nEnglish code-mixed text. IEEE Access, 9:118837–\n118850.\nAndros Tjandra, Diptanu Gon Choudhury, Frank Zhang,\nKritika Singh, Alexis Conneau, Alexei Baevski, As-\nsaf Sela, Yatharth Saraf, and Michael Auli. 2021. Im-\nproved language identification through cross-lingual\nself-supervised learning.\nUNESCO 66260. 2022. State of the art - indigenous\nlanguages in research webinar: concept note and\nagenda, 20 may 2022.\nAsahi Ushio and Jose Camacho-Collados. 2021. T-\nNER: An all-round python library for Transformer-\nbased named entity recognition. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 53–62, Online. Association\nfor Computational Linguistics.\nRainer V ossen. 2020. 9091 African Language Types. In\nThe Oxford Handbook of African Languages. Oxford\nUniversity Press.\nAlicia Beckford Wassink, Cady Gansen, and Isabel\nBartholomew. 2022. Uneven success: automatic\nspeech recognition and ethnicity-related dialects.\nSpeech Communication, 140:50–70.\nK. Williamson. 2006. Benue–Congo languages*. In\nKeith Brown, editor, Encyclopedia of Language &\nLinguistics (Second Edition), second edition edition,\npages 734–735. Elsevier, Oxford.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilin-\ngual pre-trained text-to-text Transformer. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n483–498, Online. Association for Computational Lin-\nguistics.\n17\n1514\nSeid Muhie Yimam, Hizkiel Mitiku Alemayehu,\nAbinew Ayele, and Chris Biemann. 2020. Exploring\nAmharic sentiment analysis from social media texts:\nBuilding annotation tools and classification mod-\nels. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 1048–\n1060, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, To-\ndor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster,\nDaniel Simig, Punit Singh Koura, Anjali Sridhar,\nTianlu Wang, and Luke Zettlemoyer. 2022. Opt:\nOpen pre-trained Transformer language models.\n18\n1515\nAppendices\nWe provide an overview of the Appendix.\nIntroduction\n• We share a large map of Africa showing the\n517 Languages covered in our pretraining data\nin Figure A.1.\n• We also share the scripts represented in our\npretraining data in Table A.1.\nLiterature Review\n• We provide a more extensive literature review\nin B. We discuss Afrocentric NLP, multilin-\ngualism in NLP, diversity and inclusion in\nNLP and multilingual language models.\nPretraining Data We discuss the pretraining data\nin more detain in Section C.\nTypology Information for AfroNLUIn Section D\nwe discuss 6 families that cover the languages in18\ndatasets in AfroNLU. For each family, we provide\nvisualizations that cover any number of languages\nin the 18 datasets. We provide visualizations for:\n• Afro-Asiatic in Figure D.1,\n• Austronesian in Figure D.2,\n• Creole in Figure D.3,\n• Indo-European in Figure D.4,\n• Niger-Congo in Figure D.5, and\n• Nilo-Saharan in Figure D.6.\nEvaluation We provide more information about\nthe evaluations. We do the following:\n• We show SERENGETI’s performance on the\nDev. set in Table E.1.\n• We show SERENGETI’s performance on each\nlanguage in our multilingual datasets in Table\nE.4.\n• We perform error analysis and show examples\nof errors in Table E.2. We also show confusion\nmatrices for the news classification, sentiment\nanalysis, and topic classification clusters in in\nFigure E.1, Figure E.2, and Figure E.3.\n• We discuss the implications of imbalanced dis-\ntribution and show confusion matrices for the\nnews classification, sentiment analysis, and\ntopic classification clusters in Figure E.4, Fig-\nure E.5, and Figure E.6.\n• We show results from comparing SEREN-\nGETI with AfroLID and Franc on AfroLID\ntest set in Table 7.\n• Information about the languages in our pre-\ntraining data is provided in Table F.1, Table\nF.2 and Table F.3.\n• We share statistics of the top ten languages\nwith the largest data in SERENGETI and the\nten languages with the least dataset in Table\nF.4.\nGenealogy /Language Contact Analysis We fur-\nther analyize our claim on the interaction of similar\nlanguages and zero-shot settings in Section F.\n• We create a Figure highlighting the languages\ner perform analysis on in Figure E.7.\n• We show the Jaccard similarity scores in Table\n8.\n• Next we show the results of each language in\nzero-shot settings and results for finetuning\non BERT in Table 9.\nA Introduction\nScript Languages\nEthiopic Amharic, Basketo, Maale,\n⋆Oromo, Sebat Bet Gurage\nTigrinya, Xamtanga\nArabic Fulfude Adamawa, Fulfude Caka\nTarifit\nVai Vai\nCoptic Coptic\nTable A.1: Scripts represented in SERENGETI.\nB Literature Review\nRepresentation learning is an integral part of mod-\nern NLP systems. It has significantly improved the\nstate of the art in natural language understanding\n(NLU) and natural language generation (NLG). We\nnow discuss Afrocentric NLP, Multilingualism in\nNLP, Diversity and Inclusion in NLP, MLMs, and\nLMs for African languages.\n19\n1516\nFigure A.1: All 517 languages in our dataset across the 50 African countries our data comes from. The language varieties are\nrepresented as colored pie shapes within each country. We zero in on South Africa, Lesotho, Swaziland, and Senegal to show\ndetail.\nB.1 Afrocentric NLP\nMore than 2, 000 Indigenous languages are spoken\nin Africa, which is about a third of all languages\nspoken in the world (Eberhard et al., 2021). Unfor-\ntunately, the majority of these languages have not\nreceived any NLP attention to date. Rather, most\nNLP research has focused on higher resource lan-\nguages. Most of these resourceful languages are ty-\npologically very different from Indigenous African\nlanguages. Methods used to develop technolo-\ngies for these languages remain Western-centric,\nand may not be directly extensible to Indigenous\nAfrican languages (Adebara and Abdul-Mageed,\n2022). Existing NLP technologies also mostly func-\ntion within the contexts of values and beliefs that re-\nflect western societies and pose unique challenges\nif the technologies are applied within African com-\nmunities.\nAfrocentric NLP adopts a holistic approach to\nNLP throughout the life cycle of NLP policy mak-\ning to model development and deployment. It\ndiscourages the current language data gas flaring\npolicies that have led to the low resource status of\nmany Indigenous African languages. Afrocentric\nNLP entails an understanding of the need for multi-\ndimensional policies that influence the language\npolicy in education, media, government, and other\ndomains to create ever-increasing, multi-domain,\nbig data sources for NLP. During the archival and\ncollection of language data, Afrocentric NLP ne-\ncessitates respect of user consent, data sovereignty,\nwishes of local communities, and privacy (Suther-\nland, 2018; Daigle, 2021; Makulilo, 2012). For\nmodel development, approaches tailored to the\nunique typological features of African languages\n20\n1517\nare of utmost priority. This also means develop-\nment of models that understand simple to com-\nplex tones–a common feature in about 80% of\nAfrican languages–serial verb constructions, and\nmany other features (Hyman, 2003; Creissels et al.,\n2008). Afrocentric NLP also prioritizes deploying\nmodels in formats that people without program-\nming experience can easily use. Furthermore, from\nan Afrocentric approach, development of certain\nNLP applications such as language models, lan-\nguage identification tools, spelling checkers, lan-\nguage specific keyboards, and machine translation\nsystems is crucial to advance NLP for African lan-\nguages.\nB.2 Multilingualism in NLP\nMultilingualism, the ability to handle multiple lan-\nguages within a single system or model, is becom-\ning increasingly important as the amount of text\nand speech data in many languages increase. NLP\nsystems capable of handling multiple languages\ncan provide greater access to information and com-\nmunication for people who speak languages other\nthan those most commonly used or supported by\nNLP.\nMultilingualism in NLP (Ruder, 2022) is mainly\nachieved through building (1) a single model\ntrained on several languages (Devlin et al., 2019;\nConneau et al., 2020) and (2) transfer learning\n(Raffel et al., 2020; He et al., 2022; Ruder et al.,\n2019). In the former, large transformer models\nhave achieved state-of-the-art on many tasks while\nthe latter has enabled the use of low-resource lan-\nguages through finetuned on various NLP tasks.\nDue to lack of adequate (or good quality) pretrain-\ning data (Kreutzer et al., 2021), transfer learning is\noften the most accessible method for a few low re-\nsource languages. Unfortunately, about 94% of the\nworld’s languages are either left-behinds, in that\nit is probably impossible to build NLP resources\nfor them, or scraping-bys with no labelled datasets\n(Joshi et al., 2020). For the left-behinds, labelled\nand unlabelled data is unavailable and even transfer\nlearning approaches are beyond reach. So far, to\nthe best of our knowledge, the largest multilingual\nmodel for African languages is pretrained on only\n28 African languages (Dossou et al., 2022).\nMost multilingual models are often trained with\nno more than 100 languages because increasing\nthe number of language would mean decreasing\nits capacity to learn representations of each lan-\nguage (Conneau et al., 2020). Nevertheless, in-\ncreasing model size was shown to ameliorate this\nproblem (Goyal et al., 2021). In some cases, these\nbenchmarks are translations from English (Artetxe\net al., 2020; Nzeyimana and Niyongabo Rubungo,\n2022; Ponti et al., 2020) and may not necessarily\nbe a good evaluation for the languages. This is be-\ncause translating from a source language may mask\nconcept gaps and differences in linguistic constitu-\nents (Segerer, 2008) in the target language. That is,\ntranslations are at best approximations of the tar-\nget language (Adebara and Abdul-Mageed, 2022;\nJoshi et al., 2020). For example, when translating\ninto English (which marks (in)definiteness mor-\nphologically) from Yorùbá (which uses bare nouns\nbut marks these features contextually), ambiguities\narise (Adebara et al., 2022a).\nFor evaluation of multilingual models, several\nbenchmarks have been created(Artetxe et al., 2020)\nwith most of these supporting English and other\nhigh-resource languages. More recently, a few\nevaluation sets were introduced for African lan-\nguages (Ifeoluwa Adelani et al., 2021; Shode et al.,\n2022; Niyongabo et al., 2020).We include these\nevaluation sets in our benchmark, which we hence-\nforth refer to as AfroNLU.\nWhen evaluating multilingual models, report-\ning model performance for each language in the\nbenchmark is preferred because reporting the\nresults as a single value on all languages may\nmask the model’s performance on individual lan-\nguages (Ruder, 2022). Large pre-training data, fine-\ntuning data, and evaluation benchmarks remain\nopen challenging questions for achieving progress\nin multilingual NLP. For SERENGETI, we report\nresults for each language in each benchmark across\nthe 9 tasks we evaluate on.\nB.3 Diversity and Inclusion in NLP\nDiversity relates to the level of variety within a sys-\ntem. It is the measure of distinctiveness between\nthe various individuals within a group. Inclusion\non the other hand relates to the level of representa-\ntion or alignment of an individual within a group\nand the ability for that individual to function to its\nfullest ability (Fosch-Villaronga and Poulsen, 2022;\nMitchell et al., 2020). Diversity and inclusion in\nNLP has gained increasing attention in recent years.\nIn general, there is an acknowledgement that over-\nrepresentation (and under-representation) of certain\ngroups in the data used to train models (Mitchell\net al., 2020) can be amplified by resulting technolo-\ngies. This raises concerns about the technology and\n21\n1518\nhow it is that it can further existing biases and soci-\netal inequalities. But these biases can be exhibited\nin various ways beyond training data, including the\nalgorithms implemented, the diversity of research-\ners and engineers developing the models, and the\nsocietal and cultural context in which they are used.\nAlthough this is starting to change, often times\nmost of the data exploited in NLP models come\nfrom closely related Western languages. Most\nof these languages are Indo-European (Aji et al.,\n2022; Joshi et al., 2020), and many of them share\nclose geographic proximity and typology. In addi-\ntion, the people who speak these languages have\nsimilar cultures. The implication is that several\nlinguistic phenomena and typologies are under-\nrepresented in NLP data while those prevalent\nin Indo-European languages are over-represented\n(Chakravarthi and Muralidaran, 2021). About\n88.38% of the 2, 679 languages whose typology\nis described in W ALS (Dryer, 2013) have not been\nused in NLP (Joshi et al., 2020). Many ideas and\ntopics, alien to Western cultures have also never\nbeen seen (Adebara and Abdul-Mageed, 2022;\nBender, 2011) in NLP data. African languages–and\nindeed many low resource languages–have rich lin-\nguistic typology, probably not seen in any other\nlanguage in the world (Bender, 2011). An ob-\nvious problem with the current lack of diversity\nin NLP data is that the methods and models de-\nveloped have overfit to these Indo-European typo-\nlogies and cannot generalize to other typologies.\nSimilarly, machine translation systems have been\nfound to exhibit gender, racial (Bolukbasi et al.,\n2016; Caliskan et al., 2017; Chakravarthi and Mur-\nalidaran, 2021) and stylistic biases (Hovy et al.,\n2020) in their outputs perpetuated through the data\nused for training.\nA number of studies have also found that\nalgorithms could exhibit biases (Hooker, 2021;\nBuolamwini and Gebru, 2018; Dwork et al., 2011).\nFor example, a recent study that investigated\nperformance of Amazon Transcribe and Google\nSpeech-To-Text on British English reported not-\nably higher error rates for second language speak-\ners of different varieties of British English (Markl,\n2022). In another study, an evaluation of automatic\nspeech recognition systems show substantial per-\nformance differences between ’standard’ US Eng-\nlish and African American English (AAE) variet-\nies (Koenecke et al., 2020). In this study, commer-\ncial ASR systems developed by Amazon, Apple,\nGoogle, IBM, and Microsoft were evaluated and\nhigher rates of errors were recorded for speakers of\nAAE than speakers of standard US varieties. Sim-\nilar studies have also recorded higher errors in non-\nwhite users of English (Wassink et al., 2022; Martin\nand Tang, 2020). Other studies also reported differ-\nences in the performance of Youtube’s automatic\ncaption in different settings. One study reported\nhigher accuracy in the transcriptions of US English\ncompared with Indian English (Meyer et al., 2020).\nAnother reported lower accuracy scores for women\nand speakers of Scottish English (Tatman, 2017)\nand non-white speakers of English (Tatman and\nKasten, 2017).\nApart from data and algorithmic biases, the di-\nversity crises in AI research is also argued to per-\npetuate historical biases (Freire et al., 2021). A\nmore inclusive and diverse workforce could pro-\nmote the exploration of questions and solutions\nbeyond currently investigated research questions\n(Fosch-Villaronga and Poulsen, 2022). Several ini-\ntiatives have been adopted to increase diversity in\nAI, including providing travel grants to marginal-\nized communities to attend conferences, creating\nmentoring opportunities, special workshops, and\ncommunity diversity chairs. A number of organiza-\ntions have also been developed to promote diversity\nand inclusion in AI and NLP, such as Masakhane,\nBlack in AI, LatinX in AI.\nThe impact of using biased systems in decision\nmaking have been extensively studied. Algorithmic\ndecision-making using biased systems have been\nshown to have significant discriminatory effects in\nhealth (Obermeyer et al., 2019; Eubanks, 2018),\nemployment (Barocas and Selbst, 2016), hous-\ning (Buolamwini and Gebru, 2018; Barocas and\nSelbst, 2016), government benefit allocation (Eu-\nbanks, 2018), policing (Buolamwini and Gebru,\n2018; Barocas and Selbst, 2016; Angwin et al.,\n2018), and freedom (Angwin et al., 2018). Lack\nof diversity also has implication on access to\ntechnology. Currently, due to the use of a few\nhigh resource languages in NLP, there is limited\nglobal access to important applications such as ma-\nchine translation, speech processing, information\nretrieval, and sentiment analysis. These techno-\nlogies play an important role in ensuring a lan-\nguage thrives and offer major contributions to ongo-\ning communication, literacy, education, and trans-\nlation efforts in communities worldwide. These\nlanguages which have barely been used for NLP,\n22\n1519\nusually referred to as low-resource languages, rep-\nresent more than 90% of the world’s 7, 000 lan-\nguages (Joshi et al., 2020). The current focus of\nNLP on resource-rich languages does also have\naggravating effects on the language endangerment\nproblem which has been of serious concern for lin-\nguistics and language policy around the world. An\nalarming 50 −90% of languages have been envis-\naged to go extinct by the end of the century due\nto the domination by some of these resource-rich\nlanguages (Besacier et al., 2014).\nOverall, diversity and inclusion in NLP re-\nmain active areas of research and comprise press-\ning issues of international significance. SEREN-\nGETI contributes to diversity and inclusion in NLP\nas follows: (1) We develop SERENGETI, a suite of\nmassively, multilingual language models that sup-\nport 517 African languages and language varieties.\nTo the best of our knowledge, more than 400 of\nthese languages have never been represented in any\nlanguage model to date. (2) The languages we sup-\nport belong to14 language families. (3) We provide\na massive benchmark covering28 languages across\neight different tasks.\nB.4 Multilingual Language Models\nMLMs have proven effective for cross-lingual NLU\nand NLG, often outperforming monolingual lan-\nguage models (Conneau et al., 2020). Different\nobjectives have been adopted for training (Dod-\ndapaneni et al., 2021), using Transformer architec-\ntures. These LMs use one of the three different vari-\nants of Transformer architectures–encoder-decoder,\nencoder-only and decoder-only (Cai et al., 2022).\nIn the encoder-decoder models, input is encoded\nby the encoder side and the decoder conducts the\noperation to predict the sequence one token at a\ntime or just reconstruct it by denoising. MBART\n(Liu et al., 2020), AfriTeva (Jude Ogundepo et al.,\n2022), M2M100 (Fan et al., 2020), and MT5 (Xue\net al., 2021) are representatives for this architec-\nture. Encoder-only models use only the encoder\npart of the transformer architecture, while decoder-\nonly models use its decoder only. Some examples\nof encoder-only models are BERT (Devlin et al.,\n2019), XLMR (Conneau et al., 2020), and Elec-\ntra (Chi et al., 2021), while BLOOM (Scao et al.,\n2022), GPT (Radford et al., 2018, 2019; Brown\net al., 2020b), OPT (Zhang et al., 2022) are ex-\namples of decoder-only models. Most LMs de-\nveloped for African languages use an encoder-only\narchitecture, except AfriTEV A and AfroT5 which\nuse encoder-decoder architectures.\nThese models are further finetuned on specific\ntasks. Finetuning has demonstrated its effect-\niveness on various NLU and NLG downstream\ntasks including part of speech tagging (Conneau\net al., 2020), named entity recognition (Ushio and\nCamacho-Collados, 2021; Conneau et al., 2020),\nand question answering (Conneau et al., 2020).\nFinetuning follows a transfer learning approach\nwhich attempts to transfer knowledge from other\nsources to benefit a current task. This is based on\nthe premise that previous knowledge may improve\nsolutions for a current task (Pan and Yang, 2010;\nRaffel et al., 2020; He et al., 2022; Ruder et al.,\n2019). Transfer learning allows the domains, tasks,\nand distributions used in training and testing to be\ndifferent thereby enabling a new task to leverage\npreviously acquired domain knowledge. Potential\nbenefits include faster learning, better generaliza-\ntion, and a more robust system. In the real world,\nwe find many examples of transfer learning where\nhumans transfer previous knowledge while learn-\ning or performing a task. For instance, knowing\nhow to play the piano may facilitate learning to play\nthe guitar and knowing how to ride a bicycle may\nfacilitate learning to ride a motorbike. Finetuning\nis thus done by reusing the LM’s parameters as a\nstarting point, while adding one task-specific layer\ntrained from scratch. Finetuning can be done on an\nindividual or joint basis (Kitaev et al., 2019). In the\nformer, a model is finetuned on single language for\na specific downstream task. In the later, training\ndata from a combination of multiple languages can\nbe jointly finetuned in a single model.\nC Pretraining Data\nWe provide details of our pretraining data below:\nReligious Domain. Our religious data is taken\nfrom online Bibles, Qurans, and data crawled from\nthe Jehovah’s witness website. We also include\nreligious texts from the book of Mormon.\nNews Domain. We collect data from online news-\npapers (Adebara and Abdul-Mageed, 2022) and\nnews sites such as V oice of America, V oice of Ni-\ngeria, BBC, Global voices, and DW news sites. We\ncollect local newspapers from 27 languages from\nacross Africa.\nGovernment Documents. We collect government\ndocuments South African Centre for Digital Lan-\nguage Resources (SADiLaR), and the Universal\nDeclaration of human rights (UDHR) in multiple\nlanguages.\n23\n1520\nHealth Documents. We collect multiple health\ndocuments from the Department of Health, State\nGovernment of Victoria, Australia. We collect doc-\numents in Amharic, Dinka, Harari, Oromo, Somali,\nSwahili, and Tigrinya.\nExisting Corpora. We collect corpora available\non the web for different African languages, includ-\ning from Project Gutenberg for Afrikaans, South\nAfrican News data. for Sepedi and Setswana,\nOSCAR (Abadji et al., 2021) for Afrikaans, Am-\nharic, Somali, Swahili, Oromo, Malagasy, and Yor-\nuba. We also used Tatoeba for Afrikaans, Amharic,\nBemba, Igbo, Kanuri, Kongo, Luganda, Malagasy,\nSepedi, Ndebele, Kinyarwanda, Somali, Swahili,\nTsonga, Xhosa, Yoruba, and Zulu; Swahili Lan-\nguage Modelling Data for Swahili; Ijdutse cor-\npus for Hausa; Data4Good corpora for Luganda,\nCC-100 for Amharic, Fulah, Igbo, Yoruba, Hausa,\nTswana, Lingala, Luganada, Afrikaans, Somali,\nSwahili, Swati, North Sotho, Oromo, Wolof,\nXhosa, and Zulu; Afriberta-Corpus for Afaan /\nOromo, Amharic, Gahuza, Hausa, Igbo, Pidgin,\nSomali, Swahili, Tigrinya and Yoruba; mC4 for\nAfrikaans, Amharic, Hausa, Igbo, Malagasy, Chi-\nchewa, Shona, Somali, Sepedi, Swahili, Xhosa,\nYoruba and Zulu.\nD Typology Information for AfroNLU\nSERENGETI consists of languages from 14 famil-\nies including: Afro-Asiatic, Austronesean, Creole-\nEnglish, Creole-French, Creole-Kongo, Creole-\nNgbandi, Creole-Portuguese, khoe-kwadi-Hainum,\nkhoe-kwadi-Nama khoe-kwadi-Southwest, Indo-\nEuropean, Niger-Congo, and Nilo Saharan. We\ndiscuss the classes from AfroNLU which includes\nAfro-Asiatic, Austronesian, Creole-English, Niger-\nCongo, and Nilo-Saharan.\nD.1 Afro-Asiatic\nAfro-Asiatic (aka Hamito-Semitic) is one of the\nlanguage families of Africa. It consists of\nfive or six branches: Berber, Chadic, Cushitic,\nEgyptian, Omotic (or a single Cush-Omotic),\nand Semitic(Porkhomovsky, 2020; Comrie, 2017).\nMany Afro-Asiatic languages are spoken in Cent-\nral, East, North, and West Africa. They are also\nspoken in the Middle East and in scattered com-\nmunities in Europe, the United States, and the Cau-\ncasus (Frajzyngier, 2018). In Figure D.1, we show\nrelationship between the Afro-asiatic languages in\nAfroNLU.\nD.2 Austronesian\nAustronesian languages are found along Main-\nland Southeast Asia, through Indonesia, Western\nNew Guniea, and the Madagascar area in Africa\n(Eberhard et al., 2021). Many of them have been\nshown to exhibit an isolating word structure. This\nmeans that the words in these languages are of min-\nimal morphological complexity (Gil and Schapper,\n2020). In Figure D.2, we show the geneology for\nMalagasy, the only Austronesian language in our\nbenchmark.\nD.3 Creole\nA creole language is one spoken initially only in\nsituations of contact between speakers of two or\nmore mutually unintelligible languages, and not\nas a language within an ethnic group (Sommer,\n2020). Historically, creoles have evolved along\ntrade routes or in colonized communities partic-\nularly when several groups of people without a\ncommon lingua franca are forced to communicate\nin the presence of a dominant language. Creole\nlanguages therefore often include lexical items and\ngrammatical features from multiple contact lan-\nguages. Usually, one dominant language that is\nalso referred to as the lexifier language contributes\na majority of the vocabulary. Creole languages are\nclassified based on their geographical location and\nare further grouped according to their main lexifier\nlanguages, their presumed origins, and the major\nlanguages with which they are in contact (i.e., con-\ntact languages). Figure D.3 shows the geneology\nfor Nigerian Pidgin, the only Creole in our pretrain-\ning collection.\nD.4 Indo-European\nAfrikaans is the only \"Indigenous\" Indo-European\nlanguage spoken in Africa. Although it may also\nbe viewed as not being truly Indigenous to Africa\n(Kirsten, 2018). Indo-European languages were ori-\nginally domiciled in Europe, Iran, Turkey, Western\nAsia and India (Clackson, 2007; Eberhard et al.,\n2021; Comrie, 2017; Kirsten, 2018). However,\ndue to migration, Indo-European languages are\nspoken around the world. In 2003, over 2.5 billion\npeople spoke an Indo-European language (Clack-\nson, 2007). In Figure D.4, we show the geneology\nfor Afrikaans.\nD.5 Niger-Congo\nNiger-Congo, also referred to as Niger-\nKordofanian, is the largest language family\n24\n1521\nFigure D.1: Afro-Asiatic languages in SERENGETI pretraining data. Amharic (amh), Hausa (hau), Oromo (gaz), Somali (som)\nand Tigrinya (tir) are presented in red circles.\nFigure D.2: Austroneasean language family consisting of\nMalagasy (mlg).\nFigure D.3: SERENGETI pretraining data has one creole\nlanguage, Nigerian Pidgin, indicated with ISO-639-3 code\npcm.\nin Africa (Good, 2020; Comrie, 2017). It consists\nof the highest number of languages and speakers\nin Africa. Niger-Congo languages spread across\nsub-Saharan Africa, with Benue-Congo, including\nBantu languages dominating the southern part of\nthe continent. Figure D.5 shows the Niger-congo\nlanguages in our collection. Although we use\nsimilar colours for languages which are sisters of\nthe same parent, only some of those languages are\nmutually intelligible. That is speakers of each indi-\nvidual language understand each other’s language\nwithout learning it. Specifically, Kinyawanda\n(kin) and Kirundi (run) are mutually intelligible\n(Nassenstein, 2019). Ndebele, Siswati, Xhosa,\nand Zulu also share various levels of intelligibility\nmutually intelligible (Arndt, 2015; Roy-Campbell,\n2006). Sepedi, Sotho, and Tswana also share some\nlevels of mutual intelligibility (Roy-Campbell,\n2006).\nD.6 Nilo-Saharan\nNilo-Saharan is subdivided into four branches that\ninclude North Eastern, Central Sudanic and two dis-\nputed branches–Songhay and Koman (Dimmendaal\nFigure D.4: Indo-European language family consisting of\nAfrikaans (afr).\net al., 2019; Dimmendaal, 2020; Comrie, 2017).\nThese branches are further divided into other sub-\ngroups, languages, and dialects. Nilo-Saharan lan-\nguages are spoken predominantly by eastern and\ncentral African pastoralists, and includes in its main\nChari-Nile branch the Central Sudanic and Eastern\nSudanic (also called Nilotic) languages. Figure D.6\nshows the Nilo-saharan languages in our pretrain-\ning data.\nE Evaluation\nE.1 Performance Analysis\nIn this section, we provide more information about\nour evaluation procedure and results using visualiz-\nations and tables. Figure E.1 shows the confusion\nmatrix for the news classification cluster. Figure\nE.2 shows the performance of SERENGETI on the\nsentiment analysis cluster. Each confusion matrix\nrepresents each dataset in the sentiment analysis\ncluster. In Figure E.3, we show SERENGETI per-\nformance on each category in the topic classifica-\ntion datasets.\nE.2 Error Analysis\nIn the sentiment analysis cluster, best performance\nis recorded for positive categories while negative\ncategories have the worst performance. A fine-\ngrained analysis of the Yoruba sentiment dataset\nfound that SERENGETI failed to correctly categor-\n25\n1522\nFigure D.5: Niger Congo Languages in AfroNLU benchmark. Languages which are siblings of the same parent are presented\nin similar colours.\nFigure D.6: Nilo Saharan language family with Luo (luo)\nize sentiment if the polarity item(s) were not seen\nin training, can be associated with both positive\nand negative sentiments, the polarity item(s) is a\nnegation, or if ambivalent markers are present in\nthe sentence. We provide a table showing examples\nof each type of error we found in Table E.2 in the\nAppendix. For the news classification task, polit-\nics and tourism are the best performing classes\nwhile education and relationships have the worst\nperformance on kirnews and kinnews respectively.\nIt is important to mention that the worst performing\ncategories do not have the smallest data sizes. For\nthe topic classification, the best performance is on\nthe world class for Hausa topic modelling while\nentertainment and sport have best performance for\nYoruba. The worst performance is on Nigeria and\nhealth for Hausa and Yoruba topic datasets respect-\nively.\nE.3 Imbalanced Distribution\nWe find imbalances in the class distributions for all\ndatasets except YOSM. We find a positive correla-\ntion between the size of each category in a dataset\nand the model accuracy. The larger the number of\nexamples in a specific class, the better the accuracy,\n26\n1523\nCluster Task SOTA XLMR mBERT Afro-XLMR AfriBERTa Serengeti-E110 Serengeti-E250 Serengeti\nNER\nmasakaner-v1 84.8±0.3 85.59±0.20 82.82±0.10 87.79±0.33 85.19±0.08 86.11±0.27 86.42±0.26 88.82±0.18\nmasakaner-v2 85.7±0.1⋆ 87.00±0.12 85.07±0.83 87.46±0.06 86.19±0.11 86.51±0.22 86.81±0.24 88.98±0.20\nmasakaner-east — 83.52 ±1.03 82.85±0.42 87.28±0.68 83.33±0.56 85.64±0.50 87.12±0.62 88.09±0.57\nmasakaner-eastwest — 87.70±0.30 87.29±0.33 89.34±0.07 87.77±0.34 88.14±0.26 88.96±0.15 90.38±0.17\nmasakaner-west — 89.77±0.53 90.28±0.46 89.97±0.23 89.36±0.46 88.24±0.52 89.44±0.56 91.58±0.08\nnchlt-ner — 72.19 ±0.13 71.44±0.07 73.22±0.2 69.25±0.25 65.67±0.07 65.86±0.16 73.81±0.18\nyoruba-twi-ner — 57.40 ±2.51 75.35±0.78 68.02±2.01 82.40±0.04 65.6±2.87 62.45±1.04 79.68±1.42\nwikiann — 84.82 ±0.24 84.68±0.85 87.00±1.12 84.58±0.46 84.21±0.12 85.64±0.36 86.91±0.31\nPhrase Chunking phrase-chunk — 90.41±0.10 89.62±0.24 91.54±0.24 89.47±0.22 91.99±0.02 91.70±0.27 92.01±0.18\nPOS igbo-pos — 85.40 ±0.04 85.31±0.16 85.23±0.26 85.35±0.07 85.39±0.14 85.54±0.12 85.36±0.18\nNews\namharic-news — 85.83 ±0.56 60.83±0.91 85.97±0.34 87.03±0.35 86.37±0.42 86.13±0.20 86.84±0.32\nkinnews — 76.5 ±0.91 77.98±0.41 79.15±0.57 78.21±0.41 80.09±0.68 79.54±1.00 79.32±1.49\nkirnews — 53.77 ±2.54 66.87±1.48 86.77±1.49 86.72±0.21 73.63±6.66 83.18±1.3 85.39±2.73\nswahili-news-v0.2 — 88.43±0.31 85.28±0.21 88.89±0.58 88.76±0.82 88.09±1.02 86.97±1.31 89.29±0.74\nSentiment Analysisbambara-v2 — 46.22 ±1.94 65.00±2.00 62.81±1.35 60.19±1.61 60.50±0.94 63.90±3.5 63.17±0.51\npidgin-tweet — 69.99 ±0.41 69.00±0.44 71.41±0.16 69.47±0.84 69.98±0.35 69.64±0.23 68.27±1.11\nyosm — 81.18 ±1.63 83.99±0.49 85.50±0.87 87.47±0.53 85.33±0.76 83.00±1.32 84.83±2.93\nTopic hausa-topic — 84.75 ±1.88 83.48±1.52 87.83±0.53 88.41±0.49 87.50±0.11 88.21±0.61 89.52±1.11\nyoruba-topic — 64.37 ±3.15 82.81±1.56 86.60±1.21 85.74±2.23 78.11±4.55 73.07±3.38 83.58±1.68\nAfroNLU Score 77.77 79.54 82.96 80.92 80.03 80.43 83.04\nTable E.1: Performance of models on seven AfroNLU benchmark DEV datasets. (F1) score is the evaluation metric.\nIn QA task, we train the models on English squad TRAIN and DEV datasets. We exclude the QA from AfroNLU\nDEV datasets. We use a dash (-) for tasks without a known SOTA.\nCategory Sentence Gold Prediction\nAmbivalence Markers Kò burú s\n˙\nùgbò\n˙\nn ó ti pé\n˙\njù positive negative\nSinimá tì a lè pè nì ìràwó\n˙\nsinimá tì ò `n\nko\n˙\nmó\n˙\nnà mó\n˙\nnà s\n˙\nùgbò\n˙\nn n tì kò nì ohun ámúye\n˙\nni. negative positive\nNegation Eré síse naa ko dára to, ìtàn naa kò yeni,\nní èrò tèmi òs\n˙\nèré tó daa jù ni ìyá náà negative positive\nS\n˙\ne oun tó o fé\n˙\n. negative positive\nNot seen in training Wo\n˙\nn rí sinima yìí s\n˙\ne, àgbó\n˙\ndò\n˙\nwò ni positive negative\nIrú yádi fíímù. Mo kórìrá gbogbo dídágbé mi\nnìkan kejì tì o. Ìdo\n˙\ntí ´nlá! negative positive\nPolarity item can be either Ìkìlò\n˙\n. O ní láti wo ìparí eré yìí nítorí wípé ´nkan\npositive or negative s\n˙\ne\n˙\nlè\n˙\nní ìparí eré náà. positive negative\nNìkan ní ìpò àwàdà Nollywood gbòòrò. S\n˙\né ó ní\nìdánílójú nítòótó\n˙\n. negative positive\nTable E.2: Error analysis of Yoruba Sentiment analysis dataset. The polarity items are highlighted in red.\n27\n1524\na) Kinnews\n b) Kirnews\nFigure E.1: Confusion matrices showing the performance of SERENGETI for each categories in Kirnews and\nKinnews classification datasets. The categories are (1) politics, (2) sports, (3) economy, (4) health, (5) entertainment,\n(6) history, (7) technology, (8) tourism, (9) culture, (10) fashion (11) religion, (12) environment, (13) education, and\n(14) relationship. Kirnews does not have Class 8 and 10.\na) Bambara Sentiment Analysis\n b) Pidgin Tweets\n c) YOSM Sentiment\nFigure E.2: Confusion matrices showing the performance of SERENGETI for each category in Bambara, Pidgin\ntweets, and YOSM datasets.\na) Hausa Topic Classification.\n b) Yoruba Topic Classification.\nFigure E.3: Confusion matrices showing the performance of SERENGETI for each categories in Hausa and Yoruba\ntopic classification datasets. A=\"Africa\", E=\"Entertainment\", H=\"Health\", N=\"Nigeria\", P=\"Politics\", S=\"Sport\",\nW=\"World\"\n28\n1525\nalthough we find a few exceptions. We provide\nconfusion matrices that represents the sizes of each\ncategory and the performance of SERENGETI in\nFigures E.4, E.5, and E.6.\n29\n1526\na) Kinnews\n b) Kirnews\nFigure E.4: Confusion matrices showing the performance of SERENGETI for each categories in Kirnews and\nKinnews classification datasets.\na) Bambara Sentiment Analysis\n b) Pidgin Tweets\n c) YOSM Sentiment\nFigure E.5: Confusion matrices showing the performance of SERENGETI for each category in Bambara, Pidgin\ntweets, and YOSM datasets.\na) Hausa Topic Classification.\n b) Yoruba Topic Classification.\nFigure E.6: Confusion matrices showing the performance of SERENGETI for each categories in Hausa and Yoruba\ntopic classification datasets. A=\"Africa\", E=\"Entertainment\", H=\"Health\", N=\"Nigeria\", P=\"Politics\", S=\"Sport\",\nW=\"World\"\n30\n1527\nISO-639-3 SERENGETI AfroLID Franc ISO-639-3 SERENGETI AfroLID Franc ISO-639-3 SERENGETI AfroLID Franc\naar 100.00 96.00 74.00 kde 99.00 95.00 60.00 pov 98.00 93.00 82.00\nada 100.00 100.00 98.00 kdh 100.00 99.00 95.00 run 97.00 91.00 68.00\nafr 100.00 97.00 81.00 kea 98.00 96.07 0.00 sag 100.00 100.00 30.00\namh 99.00 97.00 36.00 kin 94.00 89.00 47.00 shk 100.00 100.00 93.00\nbam 92.00 70.00 30.00 kmb 98.00 94.00 71.00 sna 98.00 97.00 91.00\nbba 100.00 100.00 83.00 kng 99.00 98.00 58.00 som 98.00 95.00 89.00\nbci 97.00 98.00 92.00 koo 96.00 96.00 96.00 sot 92.00 88.00 93.00\nbem 98.00 94.00 90.00 kqn 99.00 98.00 84.00 ssw 92.00 86.00 68.00\nbfa 100.00 99.00 91.00 kqs 99.00 95.00 73.00 suk 100.00 99.00 34.00\nbin 100.00 99.00 97.00 ktu 98.00 93.00 19.00 sus 99.00 99.00 96.00\nbum 98.00 97.00 72.00 lia 98.00 97.00 100.00 swh 95.00 77.00 70.00\ncjk 98.00 96.00 56.00 lin 98.00 99.00 98.00 tem 99.00 99.00 88.00\ncrs 97.00 96.00 83.00 lot 100.00 99.00 93.00 tir 100.00 99.00 97.00\ndag 100.00 100.00 100.00 loz 100.00 95.00 92.00 tiv 100.00 100.00 99.00\ndga 98.00 100.00 78.00 lua 98.00 99.00 87.00 toi 98.00 98.00 80.00\ndip 98.00 93.00 86.00 lue 98.00 95.00 68.00 tsn 81.00 76.00 33.00\ndyu 95.00 96.00 0.00 lug 96.00 87.00 64.00 tso 97.00 99.00 94.00\newe 93.00 97.00 97.00 lun 97.00 97.00 86.00 twi 100.00 100.00 87.00\nfat 98.00 98.00 94.00 men 98.00 98.00 99.00 umb 100.00 99.00 76.00\nfon 98.00 97.00 92.00 mfq 92.00 95.00 88.00 vai 100.00 100.00 100.00\nfuf 96.00 93.00 52.00 mos 99.00 97.00 90.00 ven 98.00 95.00 85.00\nfuv 95.00 94.00 61.00 nba 100.00 99.00 61.00 vmw 98.00 97.00 95.00\ngaa 98.00 95.00 97.00 nbl 79.00 74.00 47.00 wol 87.00 81.00 21.00\ngaz 94.00 94.00 96.00 ndo 97.00 96.00 76.00 xho 75.00 67.00 30.00\ngjn 100.00 98.00 99.00 nso 89.00 83.00 59.00 xsm 99.00 99.00 53.00\ngkp 68.00 63.00 69.00 nya 99.00 92.00 75.00 yor 99.00 98.00 66.00\nhau 95.00 88.00 77.00 nym 98.00 99.00 54.00 zdj 98.00 96.00 63.00\nibb 99.00 98.00 84.00 nyn 95.00 92.00 92.00 zul 68.00 50.00 40.00\nibo 97.00 97.00 88.00 nzi 100.00 97.00 98.00\nkbp 100.00 100.00 98.00 pcm 96.00 96.00 82.00\nSERENGETI Average f1_score:96.29 AfroLID Average f1_score:91.63 Franc Average: f1_score74.81\nTable E.3: F1-scores for SERENGETI, AfroLID, and Franc on AfroLID’s dataset for 88 languages.\n31\n1528\nF Detailed Geneaology and Language\nContact Analysis\nIn this Section, we use Figures and Tables to\nprovide evidence for the influence of similar lan-\nguages in zero-shot settings. First, we highlight\nin purple the similar languages that we perform\ngenealogy analysis on in Figure E.7. In the figure,\nthe languages with mutual intelligibility are presen-\nted in similar coloured circles. To determine the\nsignificance of language similarity and language\ncontact in our own zero-shot settings, we measure\nthe Jaccard similarity between the pretraining data\nfor the South African languages in AfroNLU (see\nTable 8). To calculate the Jaccard similarities, we\nremoved digits, emojis, and punctuation marks. We\ndo this to ensure that we reduce interference with\nthe similarity scores. We find strong similarities\nbetween some of these languages as in the bolded\nexamples in Table 8.\nWe find that although XLM-R, mBERT, and\nAfriBERTa are not trained on most most of these\nlanguages, we record high scores in zero-shot set-\ntings see Table E.4). We argue that XLM-R in addi-\ntion to cross-lingual transfers from other languages\nacquires representation from afr and xho where xho\nalone shares more than 0.4 similarity with afr, nbl,\nnso, and zul. mBERT also learns representation\nfrom afr while AfriBERTa learns representations\nfrom Gahuza which is a code-mixed variety of kin\nand run. SERENGETI however, outperforms other\nmodels on these datasets indicating that learning\nthe representation of each language improves per-\nformance.\nNext, we finetune a BERT model and compare\nthe performance of BERT with MBERT. We do\nthis because BERT is a monolingual model and\ndoes not include any similar language in its repres-\nentation. In Table 9, BERT significantly performs\nlower than MBERT in all languages in NCHLT-\nNER. BERT also has lower performance on the\nphrase-chunk dataset in all languages except on\nssw, and ven.\nThis analysis is far from being conclusive and\nfuture work can further probe the influence of sim-\nilar languages in more detail. This is necessary\nto evaluate to what extent similar languages have\nan influence on performance in zero-shot settings\nand why in zero shot settings, some monolingual\nmodels outperform multilingual ones. For example,\nin the case of ssw and ven.\n32\n1529\nFigure E.7: A genetic classification of Niger-Congo languages in AfroNLU. We highlight in purple the list of languages relevant\nto our geneaology and language contact analysis. Languages which share stronger mutual intelligibility is represented in similar\ncolours.\n33\n1530\nCluster Dataset Lang. XLMR mBERT Afro-XLMR AfriBERTa SERENGETI\nNamed Entity Recognition (NER)\nmasakaner-v1\namh 73.98±0.64 0.0±0.0 77.38±0.47 69.61±0.76 74.26±0.54\nhau 91.39±0.24 88.25±0.42 91.92±0.86 91.12±0.37 92.03±0.59\nibo 84.55±0.15 84.44±0.97 87.51±0.92 87.95±0.54 87.82±0.63\nkin 73.54±0.35 71.02±1.34 78.46±0.34 75.07±0.51 78.56±0.34\nlug 78.65±1.25 79.07±2.01 82.11±0.99 77.84±0.4 84.61±0.4\nluo 74.28±1.87 74.47±0.08 75.20±1.23 70.76±1.57 77.28±1.61\npcm 88.89±0.56 88.88±0.91 90.07±0.18 87.65±0.43 89.65±0.63\nswa 87.68±0.98 86.12±0.5 87.77±0.1 87.72±0.13 88.08±0.13\nwol 63.4±0.68 64.25±1.66 68.09±1.65 60.9±1.69 66.26±1.47\nyor 78.97±0.93 79.45±0.36 83.76±0.34 79.89±0.89 83.08±1.18\nmasakaner-v2\nbam 80.66±0.99 79.2±1.43 81.04±0.31 78.55±0.42 82.11±0.53\nbbj 72.82±1.07 62.44±0.59 73.31±0.74 71.97±1.61 73.66±0.87\newe 88.54±0.23 84.19±1.12 89.58±0.54 86.97±0.4 89.75±0.14\nfon 82.34±0.09 77.87±0.47 82.62±0.73 78.66±0.39 82.86±0.53\nhau 86.09±0.61 82.66±1.46 87.29±0.67 86.14±0.38 87.33±0.62\nibo 89.67±0.28 84.04±1.09 91.99±0.11 91.56±0.36 92.28±0.21\nkin 84.04±0.48 83.53±0.81 86.51±0.3 83.22±0.25 86.38±0.35\nlug 86.18±0.22 85.78±1.41 88.17±0.56 85.32±0.49 89.24±0.37\nmos 74.55±0.65 67.75±1.84 75.25±0.71 69.95±0.89 73.74±1.62\nnya 90.23±0.14 88.6±0.65 91.84±0.23 88.83±0.11 91.29±0.19\npcm 89.11±0.1 87.90±1.0 89.27±0.4 87.81±0.45 88.77±0.37\nsna 94.15±0.19 93.06±0.75 95.35±0.16 93.51±0.32 95.92±0.2\nswa 92.37±0.05 91.09±0.33 93.06±0.14 92.43±0.11 92.87±0.33\ntsn 85.69±0.89 85.02±0.85 88.24±0.26 83.58±0.79 88.43±0.1\ntwi 79.60±1.45 78.05±2.3 79.94±1.6 75.35±0.81 80.25±1.1\nwol 85.14±0.34 83.65±1.11 84.60±0.4 81.68±0.38 85.97±0.43\nxho 87.6 ±0.15 86.24±1.2 89.59±0.37 86.18±0.17 88.76±0.76\nyor 86.56±0.36 83.45±1.63 88.91±0.27 87.45±0.17 87.99±0.61\nzul 86.32±0.6 84.16±1.75 89.75±0.16 84.9±0.27 90.41±0.24\nnchlt-ner\nafr 80.68 ±0.75 80.08±0.29 80.55±0.11 74.5±0.64 81.57±0.59\nnbl 74.64±0.66 73.48±0.18 75.26±0.28 72.28±0.67 77.13±0.67\nnso 77.0±1.23 78.75±0.45 80.13±0.51 75.45±1.09 80.69±0.64\nsot 54.71±1.51 54.68±0.49 55.57±0.2 54.09±0.98 56.26±1.52\nssw 71.75±0.65 71.24±0.75 72.35±1.02 69.38±0.58 73.37±0.82\ntsn 77.02±0.22 76.35±0.47 77.68±0.96 73.89±1.41 79.05±0.75\ntso 74.24±0.08 72.95±0.67 74.85±0.43 71.05±0.9 75.13±0.31\nven 64.06±0.31 63.11±1.27 64.39±0.36 63.24±1.26 65.42±0.76\nxho 70.77±2.45 68.54±1.44 72.37±0.39 67.00±1.27 72.92±0.29\nzul 69.44±0.62 67.74±1.46 70.28±0.49 67.17±0.15 71.20±0.44\nWikiann\namh 57.76±0.45 33.96±1.83 64.27±1.91 60.16±2.83 68.11±1.75\nibo 73.6±1.32 70.83±1.86 73.93±1.12 76.14±1.42 75.73±2.78\nkin 69.67±2.07 77.35±4.47 82.24±2.17 79.8±1.06 79.78±1.78\nswh 88.09±0.32 88.00±0.28 88.83±0.47 86.13±0.2 89.16±0.35\nyor 83.8±2.06 81.96±0.88 87.96±1.24 82.77±0.23 85.00±2.42\nPhrase Chunking\nphrase-chunk\nafr 95.34 ±0.16 95.68±0.30 95.13±0.06 90.22±0.81 96.01±0.14\nnso 96.57±0.61 96.85±0.55 98.36±0.2 96.47±0.14 98.28±0.1\nsot 82.93±0.38 83.08±0.78 85.28±0.61 82.18±0.93 85.69±0.76\nssw 82.9±1.03 81.91±0.47 84.73±0.18 83.24±0.11 83.45±0.12\ntsn 92.77±0.16 92.64±0.66 94.11±0.49 92.71±0.42 94.03±0.19\ntso 86.42±0.46 86.90±0.31 87.39±0.18 86.73±0.95 89.32±0.43\nven 92.31±0.45 90.47±0.32 92.42±0.68 92.02±0.33 92.54±0.21\nzul 87.30±0.26 87.29±1.04 88.67±0.66 85.74±0.55 90.05±0.81\nTable E.4: Performance of mPLMs on each language in each task. (F1) score is the evaluation metric. We use Red\nhighlights to indicate languages in zero-shot setting.\n34\n1531\nISO-639-3 Language ISO-639-3 Language ISO-639-3 Language ISO-639-3 Language\naar Afar / Qafar bky Bokyi dow Doyayo gol Gola\naba Abe / Abbey bmo Bambalang dsh Daasanach gqr Gor\nabn Abua bmv Bum dua Douala gso Gbaya, Southwest\nacd Gikyode bom Berom dug Chiduruma gud Dida, Yocoboue\nach Acholi bov Tuwuli dwr Dawro gur Farefare\nada Dangme box Bwamu / Buamu dyi Sénoufo, Djimini guw Gun\nadh Jopadhola / Adhola bqc Boko dyu Jula gux Gourmanchema\nadj Adjukru / Adioukrou bqj Bandial ebr Ebrie guz Ekegusii\nafr Afrikaans bsc Oniyan ebu Kiembu / Embu gvl Gulay\nagq Aghem bsp Baga Sitemu efi Efik gwr Gwere\naha Ahanta bss Akoose ego Eggon gya Gbaya, Northwest\najg Aja bst Basketo eka Ekajuk hag Hanga\nakp Siwu bud Ntcham eko Koti har Harari\nalz Alur bum Bulu eto Eton hau Hausa\namh Amharic bun Sherbro etu Ejagham hay Haya\nann Obolo bus Bokobaru etx Iten / Eten hbb Nya huba\nanu Anyuak / Anuak buy Bullom So ewe Ewe heh Hehe\nanv Denya bwr Bura Pabir ewo Ewondo her Herero\nasa Asu bwu Buli fak Fang hgm Haillom\nasg Cishingini bxk Bukusu fat Fante hna Mina\natg Ivbie North-Okpela-Arhe byf Bete ffm Fulfulde, Maasina ibb Ibibio\nati Attie byv Medumba fia Nobiin ibo Igbo\navn Avatime bza Bandi fip Fipa idu Idoma\navu Avokaya bzw Basa flr Fuliiru igb Ebira\nazo Awing cce Chopi fon Fon ige Igede\nbam Bambara chw Chuabo fub Fulfulde, Adamawa igl Igala\nbav Vengo cjk Chokwe fue Fulfulde, Borgu ijn Kalabari\nbba Baatonum cko Anufo fuf Pular ikk Ika\nbbj Ghomala cme Cerma fuh Fulfulde, Western Niger ikw Ikwere\nbbk Babanki cop Coptic ful Fulah iqw Ikwo\nbci Baoule cou Wamey fuq Fulfulde Central Eastern Niger iri Rigwe\nbcn Bali crs Seychelles Creole fuv Fulfude Nigeria ish Esan\nbcw Bana csk Jola Kasa gaa Ga iso Isoko\nbcy Bacama cwe Kwere gax Oromo, Borana-Arsi-Guji iyx yaka\nbdh Baka daa Dangaleat gaz Oromo, West Central izr Izere\nbds Burunge dag Dagbani gbo Grebo, Northern izz Izii\nbem Bemba / Chibemba dav Dawida / Taita gbr Gbagyi jgo Ngomba\nbeq Beembe dga Dagaare gde Gude jib Jibu\nber Berber dgd Dagaari Dioula gid Gidar jit Jita\nbex Jur Modo dgi Dagara, Northern giz South Giziga jmc Machame\nbez Bena dhm Dhimba gjn Gonja kab Kabyle\nbfa Bari dib Dinka, South Central gkn Gokana kam Kikamba\nbfd Bafut did Didinga gkp Kpelle, Guinea kbn Kare\nbfo Birifor, Malba dig Chidigo gmv Gamo kbo Keliko\nbib Bisa dik Dinka, Southwestern gna Kaansa kbp Kabiye\nbim Bimoba dip Dinka, Northeastern gnd Zulgo-gemzek kby Kanuri, Manga\nbin Edo diu Gciriku gng Ngangam kcg Tyap\nbiv Birifor, Southern dks Dinka, Southeastern gof Goofa kck Kalanga\nbjv Bedjond dnj Dan gog Gogo kdc Kutu\nTable F.1: Languages covered in SERENGETI - Part I.\n35\n1532\nISO-639-3 Language ISO-639-3 Language ISO-639-3 Language ISO-639-3 Language\nkde Makonde laj Lango mfh Matal ngb Ngbandi, Northern\nkdh Tem lam Lamba mfi Wandala ngc Ngombe\nkdi Kumam lap Laka mfk Mofu, North ngl Lomwe\nkdj Ng’akarimojong lee Lyélé mfq Moba ngn Bassa\nkdl Tsikimba lef Lelemi mfz Mabaan ngo Ngoni\nkdn Kunda lem Nomaande mgc Morokodo ngp Ngulu\nkea Kabuverdianu lgg Lugbara mgh Makhuwa-Meetto nhr Naro\nken Kenyang lgm Lega-mwenga mgo Meta’ nhu Noone\nkhy Kele / Lokele lia Limba, West-Central mgq Malila nih Nyiha\nkia Kim lik Lika mgr Mambwe-Lungu nim Nilamba / kinilyamba\nkik Gikuyu / Kikuyu lin Lingala mgw Matumbi nin Ninzo\nkin Kinyarwanda lip Sekpele mif Mofu-Gudur niy Ngiti\nkiz Kisi lmd Lumun mkl Mokole nka Nkoya / ShiNkoya\nkki Kagulu lmp Limbum mlg Malagasy nko Nkonya\nkkj Kako lnl Banda, South Central mlr Vame nla Ngombale\nkln Kalenjin log Logo mmy Migaama nnb Nande / Ndandi\nklu Klao lom Loma mnf Mundani nnh Ngiemboon\nkma Konni loq Lobala mnk Mandinka nnq Ngindo\nkmb Kimbundu lot Latuka moa Mwan nse Chinsenga\nkmy Koma loz Silozi mos Moore nnw Nuni, Southern\nknf Mankanya lro Laro moy Shekkacho nso Sepedi\nkng Kongo lsm Saamya-Gwe / Saamia moz Mukulu ntr Delo\nknk Kuranko lth Thur / Acholi-Labwor mpe Majang nuj Nyole\nkno Kono lto Tsotso mpg Marba nus Nuer\nkoo Konzo lua Tshiluba mqb Mbuko nwb Nyabwa\nkoq Kota luc Aringa msc Maninka, Sankaran nxd Ngando\nkqn Kikaonde lue Luvale mur Murle nya Chichewa\nkqp Kimré lug Luganda muy Muyang nyb Nyangbo\nkqs Kisi lun Lunda mwe Mwera nyd Olunyole / Nyore\nkqy Koorete luo Dholuo / Luo mwm Sar nyf Giryama\nkri Krio lwg Wanga mwn Cinamwanga nyk Nyaneka\nkrs Gbaya lwo Luwo mws Mwimbi-Muthambi nym Nyamwezi\nkrw Krahn, Western maf Mafa myb Mbay nyn Nyankore / Nyankole\nkrx Karon mas Maasai myk Sénoufo, Mamara nyo Nyoro\nksb Shambala / Kishambala maw Mampruli myx Masaaba nyu Nyungwe\nksf Bafia mbu Mbula-Bwazza mzm Mumuye nyy Nyakyusa-Ngonde / Kyangonde\nksp Kabba mck Mbunda mzw Deg nza Mbembe, Tigon\nktj Krumen, Plapo mcn Masana / Massana naq Khoekhoe nzi Nzema\nktu Kikongo mcp Makaa naw Nawuri odu Odual\nkua Oshiwambo mcu Mambila, Cameroon nba Nyemba ogo Khana\nkub Kutep mda Mada nbl IsiNdebele oke Okpe\nkuj Kuria mdm Mayogo ncu Chunburung okr Kirike\nkus Kusaal mdy Maale ndc Ndau oku Oku\nkvj Psikye men Mende nde IsiNdebele orm Oromo\nkwn Kwangali meq Merey ndh Ndali ozm Koonzime\nkyf Kouya mer Kimiiru ndj Ndamba pcm Nigerian Pidgin\nkyq Kenga mev Maan / Mann ndo Ndonga pem Kipende\nkzr Karang mfe Morisyen / Mauritian Creole ndv Ndut pkb Kipfokomo / Pokomo\nlai Lambya mfg Mogofin ndz Ndogo\nTable F.2: Languages covered in SERENGETI - Part II\n36\n1533\nISO-639-3 Language ISO-639-3 Language ISO-639-3 Language\npov Guinea-Bissau Creole tcd Tafi won Wongo\npoy Pogolo / Shipogoro-Pogolo ted Krumen, Tepo xan Xamtanga\nrag Lulogooli tem Timne xed Hdi\nrel Rendille teo Teso xho Isixhosa\nrif Tarifit tex Tennet xnz Mattokki\nrim Nyaturu tgw Senoufo, Tagwana xog Soga\nrnd Uruund thk Tharaka xon Konkomba\nrng Ronga / ShiRonga thv Tamahaq, Tahaggart xpe Kpelle\nrub Gungu tir Tigrinya xrb Karaboro, Eastern\nrun Rundi / Kirundi tiv Tiv xsm Kasem\nrwk Rwa tke Takwane xtc Katcha-Kadugli-Miri\nsag Sango tlj Talinga-Bwisi xuo Kuo\nsaq Samburu tll Otetela yal Yalunka\nsba Ngambay tog Tonga yam Yamba\nsbd Samo, Southern toh Gitonga yao Yao / Chiyao\nsbp Sangu toi Chitonga yat Yambeta\nsbs Kuhane tpm Tampulma yba Yala\nsby Soli tsc Tshwa ybb Yemba\nsef Sénoufo, Cebaara tsn Setswana yom Ibinda\nses Songhay, Koyraboro Senni tso Tsonga yor Yoruba\nsev Sénoufo, Nyarafolo tsw Tsishingini yre Yaoure\nsfw Sehwi ttj Toro / Rutoro zaj Zaramo\nsgw Sebat Bet Gurage ttq Tawallammat zdj Comorian, Ngazidja\nshi Tachelhit ttr Nyimatli zga Kinga\nshj Shatt tui Toupouri ziw Zigula\nshk Shilluk tul Kutule zne Zande / paZande\nsid Sidama tum Chitumbuka zul Isizulu\nsig Paasaal tuv Turkana\nsil Sisaala, Tumulung tvu Tunen\nsna Shona twi Twi\nsnf Noon umb Umbundu\nsng Sanga / Kiluba urh Urhobo\nsnw Selee uth ut-Hun\nsom Somali vag Vagla\nsop Kisonge vai Vai\nsor Somrai ven Tshivenda\nsot Sesotho vid Chividunda\nsoy Miyobe vif Vili\nspp Senoufo, Supyire vmk Makhuwa-Shirima\nssw Siswati vmw Macua\nsuk Sukuma vun Kivunjo\nsus Sosoxui vut Vute\nswa Swahili wal Wolaytta\nswc Swahili Congo wbi Vwanji\nswh Swahili wec Guere\nswk Sena, Malawi wes Pidgin, Cameroon\nsxb Suba wib Toussian, Southern\ntaq Tamasheq wmw Mwani\ntcc Datooga wol Wolof\nTable F.3: Languages covered in SERENGETI - Part III.\n37\n1534\nISO-639-3 #Tokens\nswh 2,912,488,735\nafr 1,264,478,436\nsom 587,549,878\nswa 499,792,448\nhau 286,806,539\namh 241,700,000\nmlg 137,852,716\nzne 89,981,183\nsna 75,413,519\n... ...\nbam 3,262\nhar 3,066\ndyo 1,797\nfvr 1,680\ntbz 1,578\nddn 1,372\nfuc 1,336\nknc 1,097\neot 1,041\ncgg 845\nTable F.4: The sizes of the top 10 and bottom 10 lan-\nguages in SERENGETI pretraining.\n38\n1535\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0013 A2. Did you discuss any potential risks of your work?\n8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1 and 7\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4 and 5\n□\u0013 B1. Did you cite the creators of artifacts you used?\n2, 4 and 5\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n2, 4 and 5\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n9\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use only publicly available data to develop our models. Our data comes from 517 languages and\nlanguage varieties and hence it is challenging to carry out manual investigation on it. However, since\nthe data belong to the public domain, we do not have serious concerns about privacy or anti-social\nlanguage beyond what already exists online and is accessible to anyone.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n2, 3, 4, 5, 6\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n2, 3, 4, 5, 6\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1536\nC □\u0013 Did you run computational experiments?\n5 and 6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n5\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n2, 3, 4, 5, 6, Appendix\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n2, 4, 5, 6, Appendix\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n7\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1537",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8042296767234802
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6389826536178589
    },
    {
      "name": "Natural language processing",
      "score": 0.6076099872589111
    },
    {
      "name": "Task (project management)",
      "score": 0.6047655344009399
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5488390326499939
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.537127673625946
    },
    {
      "name": "Language model",
      "score": 0.5269390344619751
    },
    {
      "name": "Natural language",
      "score": 0.46140921115875244
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}