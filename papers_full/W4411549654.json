{
  "title": "Comparative Analysis of Multimodal Large Language Models GPT-4o and o1 vs Clinicians in Clinical Case Challenge Questions",
  "url": "https://openalex.org/W4411549654",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2100545752",
      "name": "Jae-Won Jung",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2161218792",
      "name": "Hyunjae Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2293024921",
      "name": "SungA Bae",
      "affiliations": [
        "Severance Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2107116369",
      "name": "Jin-Young Park",
      "affiliations": [
        "Severance Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2100545752",
      "name": "Jae-Won Jung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161218792",
      "name": "Hyunjae Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2293024921",
      "name": "SungA Bae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107116369",
      "name": "Jin-Young Park",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2908201961",
    "https://openalex.org/W4205879186",
    "https://openalex.org/W4400897152",
    "https://openalex.org/W4389919444",
    "https://openalex.org/W4395037579",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4400260578",
    "https://openalex.org/W4401715452",
    "https://openalex.org/W4396831262",
    "https://openalex.org/W4392702964",
    "https://openalex.org/W4403024753",
    "https://openalex.org/W4403781156",
    "https://openalex.org/W4404354758",
    "https://openalex.org/W4404985438",
    "https://openalex.org/W4402050919",
    "https://openalex.org/W4390510163",
    "https://openalex.org/W4401171954",
    "https://openalex.org/W4405468609",
    "https://openalex.org/W2029907219",
    "https://openalex.org/W2962052654",
    "https://openalex.org/W2804791494",
    "https://openalex.org/W2570796533",
    "https://openalex.org/W4386045865",
    "https://openalex.org/W4401507156",
    "https://openalex.org/W4401760450",
    "https://openalex.org/W4409755001",
    "https://openalex.org/W4387340031",
    "https://openalex.org/W4406887664",
    "https://openalex.org/W4409834488",
    "https://openalex.org/W4389055737",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W4226070527"
  ],
  "abstract": "SUMMARY Background Generative Pre-trained Transformer 4 (GPT-4) has demonstrated strong performance in standardized medical examinations but has limitations in real-world clinical settings. The newly released multimodal GPT-4o model, which integrates text and image inputs to enhance diagnostic capabilities, and the multimodal o1 model, which incorporates advanced reasoning, may address these limitations. Objective This study aimed to compare the performance of GPT-4o and o1 against clinicians in real-world clinical case challenges. Methods This retrospective, cross-sectional study used Medscape case challenge questions from May 2011 to June 2024 (n = 1,426). Each case included text and images of patient history, physical examination findings, diagnostic test results, and imaging studies. Clinicians were required to choose one answer from among multiple options, with the most frequent response defined as the clinician’s decision. Data-based decisions were made using GPT models (3.5 Turbo, 4 Turbo, 4 Omni, and o1) to interpret the text and images, followed by a process to provide a formatted answer. We compared the performances of the clinicians and GPT models using Mixed-effects logistic regression analysis. Results Of the 1,426 questions, clinicians achieved an overall accuracy of 85.0%, whereas GPT-4o and o1 demonstrated higher accuracies of 88.4% and 94.3% (mean difference 3.4%; P = .005 and mean difference 9.3%; P &lt; .001), respectively. In the multimodal performance analysis, which included cases involving images (n = 917), GPT-4o achieved an accuracy of 88.3%, and o1 achieved 93.9%, both significantly outperforming clinicians (mean difference 4.2%; P = .005 and mean difference 9.8%; P &lt; .001). o1 showed the highest accuracy across all question categories, achieving 92.6% in diagnosis (mean difference 14.5%; P &lt; .001), 97.0% in disease characteristics (mean difference 7.2%; P &lt; .001), 92.6% in examination (mean difference 7.3%; P = .002), and 94.8% in treatment (mean difference 4.3%; P = .005), consistently outperforming clinicians. In terms of medical specialty, o1 achieved 93.6% accuracy in internal medicine (mean difference 10.3%; P &lt; .001), 96.6% in major surgery (mean difference 9.2%; P = .030), 97.3% in psychiatry (mean difference 10.6%; P = .030), and 95.4% in minor specialties (mean difference 10.0%; P &lt; .001), significantly surpassing clinicians. Across five trials, GPT-4o and o1 provided the correct answer 5/5 times in 86.2% and 90.7% of the cases, respectively. Conclusions The GPT-4o and o1 models achieved higher accuracy than clinicians in clinical case challenge questions, particularly in disease diagnosis. The GPT-4o and o1 could serve as valuable tools to assist healthcare professionals in clinical settings.",
  "full_text": " \nComparative Analysis of Multimodal Large \nLanguage Models GPT-4o and o1 vs Clinicians in \nClinical Case Challenge Questions \nJaewon Jung,1,5 Hyunjae Kim,1,5 SungA Bae,2,3,6,* and Jin Young Park,2,4,**  1 \n1Department of Medicine, Yonsei University College of Medicine, Seoul, Republic of Korea 2 \n2Department of Biomedical Systems Informatics, Yonsei University College of Medicine, Yongin, Republic of Korea 3 \n3Division of Cardiology, Department of Internal Medicine, Yonsei University College of Medicine, Yongin Severance 4 \nHospital, Yongin, Republic of Korea 5 \n4Department of Psychiatry, Yongin Severance Hospital, Yonsei University College of Medicine, Yongin, Republic of 6 \nKorea 7 \n5These authors contributed equally  8 \n6Lead contact 9 \n*Correspondence: cardiobsa@yuhs.ac 10 \n**Correspondence: empathy@yuhs.ac  11 \n 12 \nSUMMARY 13 \nBackground: Generative Pre-trained Transformer 4 (GPT-4) has demonstrated strong performance in 14 \nstandardized medical examinations but has limitations in real-world clinical settings. The newly released 15 \nmultimodal GPT-4o model, which integrates text and image inputs to enhance diagnostic capabilities, and 16 \nthe multimodal o1 model, which incorporates advanced reasoning, may address these limitations.  17 \nObjective: This study aimed to compare the performance of GPT-4o and o1 against clinicians in real-18 \nworld clinical case challenges. 19 \nMethods: This retrospective, cross-sectional study used Medscape case challenge questions from May 20 \n2011 to June 2024 (n = 1,426). Each case included text and images of patient history, physical 21 \nexamination findings, diagnostic test results, and imaging studies. Clinicians were required to choose one 22 \nanswer from among multiple options, with the most frequent response defined as the clinician’s decision. 23 \nData-based decisions were made using GPT models (3.5 Turbo, 4 Turbo, 4 Omni, and o1) to interpret the 24 \ntext and images, followed by a process to provide a formatted answer. We compared the performances of 25 \nthe clinicians and GPT models using Mixed-effects logistic regression analysis. 26 \nResults: Of the 1,426 questions, clinicians achieved an overall accuracy of 85.0%, whereas GPT-4o and 27 \no1 demonstrated higher accuracies of 88.4% and 94.3% (mean difference 3.4%; P = .005 and mean 28 \ndifference 9.3%; P < .001), respectively. In the multimodal performance analysis, which included cases 29 \ninvolving images (n = 917), GPT-4o achieved an accuracy of 88.3%, and o1 achieved 93.9%, both 30 \nsignificantly outperforming clinicians (mean difference 4.2%; P = .005 and mean difference 9.8%; P < 31 \n.001). o1 showed the highest accuracy across all question categories, achieving 92.6% in diagnosis (mean 32 \ndifference 14.5%; P < .001), 97.0% in disease characteristics (mean difference 7.2%; P < .001), 92.6% in 33 \nexamination (mean difference 7.3%; P = .002), and 94.8% in treatment (mean difference 4.3%; P = .005), 34 \nconsistently outperforming clinicians. In terms of medical specialty, o1 achieved 93.6% accuracy in 35 \ninternal medicine (mean difference 10.3%; P < .001), 96.6% in major surgery (mean difference 9.2%; P = 36 \n.030), 97.3% in psychiatry (mean difference 10.6%; P = .030), and 95.4% in minor specialties (mean 37 \ndifference 10.0%; P < .001), significantly surpassing clinicians. Across five trials, GPT-4o and o1 38 \nprovided the correct answer 5/5 times in 86.2% and 90.7% of the cases, respectively. 39 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \nConclusions: The GPT-4o and o1 models achieved higher accuracy than clinicians in clinical case 40 \nchallenge questions, particularly in disease diagnosis. The GPT-4o and o1 could serve as valuable tools to 41 \nassist healthcare professionals in clinical settings. 42 \n 43 \nKEYWORDS 44 \nMultimodal large language model; GPT-4 Omni; o1; clinical decision-making; diagnostic accuracy; 45 \nartificial intelligence 46 \n 47 \nINTRODUCTION 48 \nRapid advancements in artificial intelligence (AI) have transformed various sectors, including healthcare, 49 \nwhere the importance of digital health technologies continues to grow.1–3 Among these technologies, 50 \nlarge language models (LLMs) have demonstrated considerable potential in healthcare, particularly in 51 \nanswering medical questions, assisting with diagnoses, and automating administrative tasks.4 Generative 52 \nPre-trained Transformer 4 (GPT-4; OpenAI, San Francisco, CA, USA) is one of the most advanced LLMs, 53 \nconsistently achieving high scores on the United States Medical Licensing Examination and often 54 \nexceeding the passing threshold.5–7 Despite these accomplishments, existing evaluations have primarily 55 \nfocused on text-only formats that often exclude or replace visual elements with descriptions. This 56 \nlimitation reduces GPT-4’s applicability in image-dependent specialties, such as radiology. 57 \nTo address the limitations of unimodal LLMs, the recently released GPT-4 Omni introduces multimodal 58 \ncapabilities, enabling it to process and integrate both textual and visual information.8 This enhancement 59 \nallows for a more comprehensive analysis of clinical cases, particularly in fields such as diagnostic and 60 \ninterventional radiology and nuclear medicine, where early studies have demonstrated its potential.9, 10 61 \nDespite these advancements, existing research has largely focused on simplified exam questions, and no 62 \nstudies have reported significant performance improvements over text-only models in medical 63 \nexaminations.11, 12 This gap highlights the need to evaluate the performance of multimodal models in 64 \nclinical case settings that reflect the complexity of diagnostic decision-making encountered in clinical 65 \npractice. 66 \nIn parallel with these multimodal developments, OpenAI recently introduced the o1 model, which 67 \nincorporates advanced reasoning capabilities designed to enhance safety and robustness. 13 By utilizing 68 \nchain-of-thought reasoning, o1 has demonstrated improved adherence to safety protocols in complex 69 \nscenarios, effectively reducing the risk of generating harmful or biased content. These enhanced 70 \nreasoning capabilities are particularly important in healthcare, where complex problem-solving and data 71 \ninterpretation are critical.14 Several studies have shown that the o1-preview outperforms both GPT-4 and 72 \nGPT-4o models in clinical settings, highlighting its superior reasoning abilities in medical decision-making 73 \nprocesses.15–17 However, the performance of the o1 model in complex real-world clinical cases remains 74 \nunexplored due to its recent release. Accordingly, this study aimed to evaluate the clinical potential of two 75 \nadvanced large language models, GPT-4 Omni and o1, in solving complex clinical case challenge 76 \nquestions that incorporate both textual and visual data and to validate their effectiveness as diagnostic 77 \nsupport tools compared with clinicians. 78 \n 79 \nRESULTS  80 \nQuestion characteristics 81 \nWe evaluated the performance of four language models, GPT-3.5 Turbo, GPT-4 Turbo, GPT-4 Omni, and 82 \no1, using a dataset of 1,426 cases. Of these, 917 questions included images, resulting in 1,475 images. 83 \nFigure S1 in the Supplementary Appendix shows the quiz samples. The number of answer options per 84 \nquestion ranged from two to nine, with the majority of cases (97.2%) offering either four or five answer 85 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \noptions: 906 cases had four options and 480 had five. The random chance of selecting the correct answer 86 \nwas 23.1%. 87 \n 88 \nOverall accuracy and model comparison 89 \nClinicians demonstrated an accuracy of 85.0% across the entire dataset (Figure 1A and Table S1 in the 90 \nSupplementary Appendix). In comparison, GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, and o1 achieved 91 \naccuracies of 60.6% (95% confidence interval [CI], 58.1–63.1), 82.1% (95% CI, 80.1–84.0), 88.4% (95% 92 \nCI, 86.8–90.1), and 94.3% (95% CI, 93.1–95.5), respectively. The performance of GPT-4o and o1 93 \nsurpassed that of clinicians, demonstrating significantly higher accuracy (P = .005 and P < .001, 94 \nrespectively). In contrast, GPT-3.5 Turbo and GPT-4 Turbo performed significantly lower than the 95 \nclinicians (P < .001 and P = .028, respectively). 96 \nTo assess the potential impact of data exposure on model performance, we evaluated the accuracy of 97 \nGPT-4o and o1 on datasets introduced both before (n = 1,219) and after (n = 207) the knowledge cutoff 98 \ndate of October 2023. Both models demonstrated comparable accuracy across the two time periods (P = 99 \n.646 for GPT-4o and P = .249 for o1), indicating that exposure to the test data had minimal to no impact 100 \non model accuracies (Figure S2 in the Supplementary Appendix). 101 \n 102 \nMultimodal model performances on cases with images 103 \nIn the subset of cases that included images (n = 917), clinicians achieved an accuracy of 84.1% (Figure 104 \n1B). GPT-4o performed better, with an accuracy of 88.3% (95% CI, 86.3–90.4), which was significantly 105 \nhigher than that of clinicians (P = .005). o1 exhibited the highest performance, achieving an accuracy of 106 \n93.9% (95% CI, 92.3–95.4), significantly surpassing both clinicians and GPT-4o (P < .001 for both). 107 \n 108 \nPerformance by question category 109 \nThe dataset was stratified based on the question category, with the diagnosis category comprising the 110 \nlargest number of questions (n = 530), followed by disease characteristics (n = 401), treatment (n = 305), 111 \nand examination (n = 190) (Table 1). o1 demonstrated the highest accuracy across all question 112 \ncategories, consistently outperforming clinicians and other GPT models. Overall, it achieved 94.3% 113 \naccuracy, significantly surpassing that of clinicians (85.0%, P < .001) and all other models. 114 \nIn the diagnosis and disease characteristics categories, o1 achieved 92.6% and 97.0% accuracy, 115 \nrespectively, significantly outperforming clinicians (P < .001 for both). In the examination category, o1 116 \nachieved 92.6% accuracy, surpassing that of clinicians, who achieved 85.3% accuracy (P = .002). In the 117 \ntreatment category, o1 also outperformed clinicians, achieving 94.8% accuracy compared to 90.5% for 118 \nclinicians (P = .005). 119 \nIn the diagnosis category, GPT-4o significantly outperformed clinicians, achieving 84.9% accuracy 120 \ncompared to 78.1% for clinicians (P = .002). For categories related to disease characteristics and 121 \nexamination, GPT-4o also demonstrated higher accuracy than clinicians, although the differences were 122 \nnot statistically significant (P = .261 and P = .105, respectively). In the treatment category, clinicians 123 \nshowed greater accuracy than GPT-4o, but the difference was not statistically significant (P = .715). 124 \n 125 \nPerformance by medical specialty 126 \nThe dataset was stratified by medical specialty, with internal medicine comprising 766 questions, followed 127 \nby minor specialties (n = 390), pediatrics (n = 108), major surgery (n = 87), and psychiatry (n = 75) (Table 128 \n2). 129 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \no1 achieved 93.6% accuracy in internal medicine and 95.4% in minor specialties, which were statistically 130 \nsignificant compared to clinicians (p < .001 for both). In major surgery, o1 outperformed clinicians with 131 \n96.6% accuracy, while clinicians achieved 87.4% accuracy (P = .030). In psychiatry, o1 achieved an 132 \naccuracy of 97.3%, surpassing that of clinicians, who achieved 86.7% accuracy (P = .030). Clinicians 133 \ndemonstrated numerically higher performance in pediatrics than in o1, but the difference was not 134 \nstatistically significant (P = .798). 135 \nGPT-4o demonstrated superior accuracy in internal medicine, with 86.9% compared to 83.3% for 136 \nclinicians (P = .037). Similarly, in minor specialties, GPT-4o outperformed clinicians, achieving 91.5% 137 \naccuracy compared to 85.4% (P = .005). In addition, GPT-4o showed higher accuracy than clinicians in 138 \nmajor surgeries, although the difference was not statistically significant (P = .802). In contrast, clinicians 139 \ndemonstrated higher accuracy in pediatrics and psychiatry, but these differences were not statistically 140 \nsignificant (P = .617 and P = .645, respectively). 141 \n 142 \nConsistency of responses 143 \nGPT-4o and o1 were evaluated using the entire dataset over five independent trials to assess response 144 \nconsistency (Figure 2). GPT-4o achieved the correct answer 5/5 times in 86.2% of the cases, while o1 145 \ndemonstrated a slightly higher consistency, with the correct answer 5/5 times in 90.7% of the cases. 146 \nNotably, GPT-4o answered correctly at least once in 90.3% of the cases across the five trials, whereas o1 147 \nachieved at least one correct answer in 95.6% of the cases. 148 \n 149 \nDISCUSSION  150 \nPrincipal results 151 \nThis study demonstrated that recent GPT models, specifically o1 and GPT-4o, can outperform clinicians 152 \nin clinical case challenges.  153 \nComparison with prior work 154 \nPrevious studies using earlier models, such as GPT-3.5 Turbo and GPT-4 Turbo, generally found that the 155 \nperformance on medical cases was comparable to or below that of humans.18, 19 A prior study using 156 \nMedscape clinical case challenges with a text-only model reported performance below 50%, highlighting 157 \nsignificant limitations.20 In contrast, our study utilized multimodal models, resulting in substantial 158 \nperformance improvements. Both GPT-4o and o1 maintained high accuracy in cases involving images, 159 \ndemonstrating their ability to interpret various imaging types, including magnetic resonance imaging 160 \n(MRI), computed tomography (CT), X-ray, ultrasound, and pathological images, across multiple 161 \nspecialties. Additionally, earlier research has shown that the o1-preview model outperformed the GPT-4 162 \nseries in solving medical problems.14, 16, 17 Our findings further support this observation, as o1 163 \noutperformed GPT-4o in complex medical scenarios. 164 \nThe accuracy of o1 remained above 90% across all question categories and medical specialties, with 165 \nparticular strength in diagnosis. The model also performed well in areas such as major surgery and 166 \npsychiatry, with accuracy rates exceeding those of GPT-4o by more than 8%. Research suggests that 167 \no1’s high performance is driven by its “chain-of-thought” reasoning, which systematically breaks down 168 \ncomplex problems, reduces hallucinations, and improves logical reasoning.13, 14 This capability is 169 \nespecially beneficial in clinical cases with abundant patient data, which often include extraneous 170 \ninformation. In such contexts, o1’s ability to filter relevant insights and synthesize data across specialties 171 \naids in understanding disease progression and managing complex multisystem conditions. 172 \nThe o1 model demonstrated stronger performance and greater consistency than GPT-4o in selecting the 173 \nmost accurate option from multiple choices. This finding contrasts with previous studies suggesting similar 174 \ngains between the o1 and GPT-4 models in probabilistic reasoning and critical diagnosis identification.21 175 \nThis divergence may be attributed to differences in prompt design and parameter tuning, which enabled 176 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \no1 to achieve a significant improvement. However, it also suggests that o1’s performance may vary 177 \ndepending on the clinical setting.  178 \nIntegration into clinical workflows and application 179 \nWhile the study shows that AI models outperform clinicians in accuracy, their reasoning process remains 180 \na black box. Understanding how these models reach their conclusions and making this process 181 \ninterpretable for clinicians is a critical consideration for integration into clinical workflows. Figure 3 182 \nillustrates how the AI model interprets imaging studies and synthesizes differential diagnoses. This 183 \napproach enables clinicians to verify, understand, and critically assess AI recommendations, thereby 184 \nfacilitating safer and more informed decision-making. 185 \nIn practical applications, GPT models may serve as valuable tools to support clinicians, particularly in 186 \nsettings where specialized expertise is limited. Clinicians are prone to diagnostic errors and may 187 \noccasionally order unnecessary tests, influenced by factors such as financial incentives while lacking 188 \nadequate tools to monitor such decisions.22–25 Recent models demonstrate potential in assisting with 189 \ndifferential diagnosis and initial treatment planning, especially in fields outside a physician’s primary 190 \nspecialty.26 Furthermore, GPT models show promise in identifying rare diseases that might otherwise be 191 \noverlooked and in recommending appropriate referrals to specialized care.27 In medically underserved 192 \nareas or environments with limited access to specialist consultation, AI systems can help reduce 193 \nhealthcare disparities by providing consistent decision support and expanding diagnostic reach.28 194 \n 195 \nRemaining challenges and ethical considerations 196 \nAlthough AI demonstrates high diagnostic accuracy, the role of clinicians remains indispensable. The 197 \nselection, preprocessing, and input of relevant patient information into AI systems depend heavily on 198 \nclinician expertise.29 Moreover, while the models have demonstrated impressive performance based on 199 \nthe provided case information, real clinical settings are far more complex.30 Patients often provide 200 \nincomplete histories or omit critical information, presenting substantial challenges that current LLMs are 201 \nnot yet fully equipped to manage.31 Further prospective studies using real-world clinical data, including 202 \nfree-form questions and open-ended scenarios, are recommended to evaluate the robustness and 203 \ngeneralizability of these models. 204 \nAs AI technologies, including large multimodal models capable of analyzing imaging data, become 205 \nincreasingly integrated into clinical workflows, new ethical considerations must be addressed. In addition 206 \nto traditional concerns about accountability, transparency, and bias in algorithmic decision-making, the 207 \ninclusion of sensitive medical images heightens the importance of patient privacy and data protection.32 208 \nFurthermore, establishing clear legal frameworks is essential to delineate responsibility in cases where 209 \nAI-assisted decisions result in misdiagnosis.33 Despite the substantial capabilities demonstrated by these 210 \nmodels, it remains crucial to recognize that AI systems currently lack the emotional understanding, 211 \nempathy, and holistic grasp of individual patient cases that clinicians possess; thus, human intervention 212 \nand interaction remain essential. 213 \n 214 \nLimitations of the study 215 \nOur study has certain limitations. First, clinicians were defined as site voters rather than as a standardized 216 \ngroup of certified specialists, and the voting participants included a broad range of users. The absence of 217 \nparticipant credentials and the informal nature of responses, provided without academic or clinical 218 \nconsequences, may have contributed to an underestimation of actual clinician performance. Despite 219 \nthese limitations, the primary aim of the study was not to benchmark against definitive human 220 \nperformance but to illustrate the rapid and consistent progress of LLMs in performing medical tasks. 221 \nGiven the observed performance trajectory suggesting increasing clinical applicability, further studies 222 \ninvolving board-certified clinicians are warranted to evaluate the models’ validity in real-world settings. 223 \nSecond, the dataset was derived from Medscape clinical case challenges, which may not fully capture the 224 \ndiversity and complexity of real-world scenarios. Additionally, because the clinical case dataset is open-225 \nsource, it is possible that the GPT models were exposed to specific cases during training. However, o1 226 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nand GPT-4o were trained only on data available until October 2023, and their accuracy rates showed no 227 \nstatistically significant differences between datasets introduced before and after this date, suggesting that 228 \nany prior exposure had a limited impact on model accuracy. 229 \nThird, the dataset may have been influenced by inherent bias. This includes selection bias, as 230 \nMedscape’s educational objectives often prioritize complex or rare conditions, leading to their preferential 231 \nselection over common clinical presentations. Furthermore, latent biases, such as the overrepresentation 232 \nof specific medical conditions, demographic groups, or clinical scenarios, may have limited the dataset’s 233 \ngeneralizability and reduced its ability to reflect real-world patient diversity.34 234 \n 235 \nCONCLUSIONS 236 \nThe o1 and GPT-4o models achieved higher accuracy than clinicians on clinical case challenge 237 \nquestions, including those involving medical imaging. These findings suggest that both models can serve 238 \nas valuable tools for healthcare professionals, supporting various aspects of patient care and decision-239 \nmaking in clinical settings. 240 \n 241 \nRESOURCE AVAILABILITY 242 \n 243 \nLead contact 244 \nFurther information and requests for resources and reagents should be directed to and will be fulfilled by 245 \nthe lead contact, SungA Bae (cardiobsa@yuhs.ac). 246 \n 247 \nMaterials availability 248 \nThis study did not generate new materials.  249 \n 250 \nData and code availability 251 \nThe publicly available Medscape case challenge questions used for this analysis are available for 252 \ndownload at the cited sources: https://reference.medscape.com/features/casechallenges. The underlying 253 \ncode and sample datasets used in this study are publicly available in the following GitHub repository: 254 \nhttps://github.com/makogirls/GPT-medscape-case-challenge. 255 \n 256 \nACKNOWLEDGMENTS 257 \nThis research was supported by a grant from the Korea Health Technology R&D Project through the 258 \nKorea Health Industry Development Institute (KHIDI), funded by the Ministry of Health and Welfare, 259 \nRepublic of Korea (grant number RS-2023-KH135442). 260 \nAdditionally, this study was supported by a New Faculty Research Seed Money Grant from Yonsei 261 \nUniversity College of Medicine for 2025 (2025-32-0022). 262 \n 263 \nAUTHOR CONTRIBUTIONS 264 \nConceptualization, S. Bae and J.Y. Park; funding acquisition, S. Bae and J.Y. Park; supervision, S. Bae 265 \nand J.Y. Park; data curation, J. Jung and H. Kim; formal analysis, J. Jung and H. Kim; investigation, J. 266 \nJung and H. Kim; programming, J. Jung and H. Kim; verification, J. Jung and H. Kim; visualization, J. 267 \nJung and H. Kim; writing—original draft, all authors; writing—review & editing, all authors; final approval, 268 \nall authors; accountability, all authors. 269 \n 270 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nDECLARATION OF INTERESTS 271 \nNone. 272 \n 273 \nDECLARATION OF GENERATIVE AI AND AI-ASSISTED TECHNOLOGIES 274 \nDuring the preparation of this manuscript, GPT-4o and o1 (OpenAI, 2025) were used as supplementary 275 \ntools to assist with translation, refine sentence structure, and generate vector images for figures. These 276 \ntools were not involved in generating original scientific content, nor did they contribute to the interpretation 277 \nof the study results.  278 \n 279 \nSUPPLEMENTAL INFORMATION 280 \nDocument S1. Figures S1–S2 and Tables S1–S2 281 \nFigure S1. Medscape quiz sample 282 \nFigure S2. Model Performances Before and After the Knowledge Cutoff Date 283 \nTable S1. Comparison of clinician and GPT model responses 284 \nTable S2. Medscape quiz information 285 \n 286 \nFIGURE TITLES AND LEGENDS 287 \nFigure 1. Overall performance of the GPT models 288 \n(A) Accuracy of the models across the entire dataset (n = 1,426). Clinicians achieved an accuracy of 289 \n85.0%, while GPT-3.5 Turbo, GPT-4 Turbo, GPT-4 Omni, and o1 achieved accuracies of 60.6%, 82.1%, 290 \n88.4%, and 94.3%, respectively. Statistical comparisons showed that GPT-4o and o1 significantly 291 \noutperformed clinicians (**P < .01; ***P < .001), while GPT-3.5 Turbo and GPT-4 Turbo performed 292 \nsignificantly worse (***P < .001; *P < .05). Error bars represent 95% confidence intervals.  293 \n(B) Accuracy of models for the dataset subset with images (n = 917). Clinicians achieved an accuracy of 294 \n84.1%, while GPT-4o achieved 88.3%, significantly surpassing clinicians (**P < .01). o1 demonstrated an 295 \naccuracy of 93.9%, significantly outperforming both clinicians and GPT-4o (***P < .001 for both). Error 296 \nbars represent 95% confidence intervals. GPT-4, Generative Pre-trained Transformer 4 297 \n 298 \nFigure 2. Response consistency of GPT-4o and o1 299 \n(A) Performance of GPT-4o across five independent trials, showing that 86.2% of cases had all correct 300 \nresponses (5/5). GPT-4o achieved at least one correct response in 90.3% of cases across the trials.  301 \n(B) Performance of o1 across five independent trials, demonstrating slightly higher consistency, with 302 \n90.7% of cases achieving all correct responses (5/5). o1 achieved at least one correct response in 95.6% 303 \nof cases across the trials. GPT-4, Generative Pre-trained Transformer 4 304 \n 305 \nFigure 3. Multimodal Reasoning Process of GPT in a Clinical Case 306 \nThis figure illustrates how the GPT model integrates textual and visual information to interpret a clinical 307 \ncase. The GPT model is prompted to identify the most likely diagnosis, explain its reasoning, and discuss 308 \ndifferential diagnoses. GPT, Generative Pre-trained Transformer 309 \n 310 \nFigure 4. Study flowchart 311 \nThis flowchart illustrates the workflow for evaluating the diagnostic performances of GPT models 312 \ncompared with clinicians in Medscape case challenge questions. GPT, Generative Pre-trained 313 \nTransformer 314 \n 315 \n  316 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nTABLES  317 \nTable 1. Performance by Question Category 318 \nCategory No. of \nquestions \nAccuracy (95% CI) \nP \nvaluea \nP \nvalueb Clinician \nGPT \n3.5 Turbo 4 Turbo 4 Omni o1 \nDiagnosis 530 78.1 (74.6–\n81.6) \n56.4 \n(52.2–\n60.6) \n76.8 \n(73.2–\n80.4) \n84.9 \n(81.9–\n88.0) \n92.6 \n(90.4–\n94.9) \n.002 < .001 \nDisease \ncharacteristics 401 89.8 (86.8–\n92.7) \n62.6 \n(57.9–\n67.3) \n86.5 \n(83.2–\n90.0) \n91.5 \n(88.8–\n94.2) \n97.0 \n(95.3–\n98.7) \n.261 < .001 \nExamination 190 85.3 (80.2–\n90.3) \n65.8 \n(59.0–\n72.5) \n85.8 \n(80.8–\n90.8) \n89.5 \n(85.1–\n93.8) \n92.6 \n(88.9–\n96.3) \n.105 .002 \nTreatment 305 90.5 (87.2–\n93.8) \n62.0 \n(56.5–\n67.4) \n83.0 \n(78.7–\n87.2) \n89.8 \n(86.4–\n93.2) \n94.8 \n(92.3–\n97.3) \n.715 .005 \nTotal 1426 85.0 (83.1–\n86.8) \n60.6 \n(58.1–\n63.1) \n82.1 \n(80.1–\n84.0) \n88.4 \n(86.8–\n90.1) \n94.3 \n(93.1–\n95.5) \n.005 < .001 \na Clinician vs 4o 319 \nb Clinician vs o1 320 \nCI, confidence interval 321 \n 322 \nTable 2. Performance by Medical Specialty 323 \nCategory No. of \nquestions \nAccuracy (95% CI) \nP \nvalue\na \nP \nvalueb \nClinician \nGPT \n3.5 Turbo 4 Turbo 4 Omni o1 \nInternal \nmedicine 766 \n83.3 \n(80.6–\n85.9) \n59.0 \n(55.5–\n62.5) \n80.2 \n(77.3–\n83.0) \n86.9 \n(84.6–\n89.3) \n93.6 \n(91.9–\n95.3) \n.037 < .001 \nMajor Surgery 87 \n87.4 \n(80.4–\n94.3) \n64.4 \n(54.3–\n74.4) \n79.3 \n(70.8–\n87.8) \n88.5 \n(81.8–\n95.2) \n96.6 \n(92.7–\n100.0) \n.802 .030 \nPediatrics 108 \n92.6 \n(87.7–\n97.5) \n61.1 \n(51.9–\n70.3) \n84.3 \n(77.4–\n91.1) \n90.7 \n(85.3–\n96.2) \n91.7 \n(86.5–\n96.9) \n.617 .798 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nPsychiatry 75 \n86.7 \n(79.0–\n94.4) \n60.0 \n(48.9–\n71.1) \n82.7 \n(74.1–\n91.2) \n84.0 \n(75.7–\n92.3) \n97.3 \n(93.7–\n100.0) \n.645 .030 \nMinor 390 \n85.4 \n(81.9–\n88.9) \n62.8 \n(58.0–\n67.6) \n85.6 \n(82.2–\n89.1) \n91.5 \n(88.8–\n94.3) \n95.4 \n(93.3–\n97.5) \n.005 < .001 \nTotal 1426 \n85.0 \n(83.1–\n86.8) \n60.6 \n(58.1 –\n63.1) \n82.1 \n(80.1–\n84.0) \n88.4 \n(86.8–\n90.1) \n94.3 \n(93.1–\n95.5) \n.005 < .001 \na Clinician vs 4o 324 \nb Clinician vs o1 325 \nCI, confidence interval 326 \n 327 \nTable 3. Prompt template for GPT models 328 \nModels Prompt \n  \nGPT-3.5 Turbo \nGPT-4 Turbo \nYou are a specialist clinician. The data provided is about one patient, and it \nmight involve a rare disease, so consider many possibilities. As a clinician, \nyou must make data-based \ndecisions, highly recommended to be based on journals and medical \ntextbooks. Examination findings are very important! \n \n[Question] \n[Options] \n[All textual information] \n  \n  \nGPT-4 Omni \no1 \nYou are a specialist clinician. The data provided is about one patient, and it \nmight involve a rare disease, so consider many possibilities. As a clinician, \nyou must make data-based decisions, highly recommended to be based on \njournals and medical textbooks. Examination findings are very important! \n \n[Question] \n[Options] \n[All textual information] \n[Images] \n  \n  329 \n 330 \n  331 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nREFERENCES 332 \n1. Topol, E.J. (2019). High-performance medicine: the convergence of human and artificial 333 \nintelligence. Nat Med 25, 44–56. 10.1038/s41591-018-0300-7. 334 \n2. Yoon, D. (2022). Preparing for a New World: Making Friends with Digital Health. Yonsei Med J 335 \n63, S108–S111. 10.3349/ymj.2022.63.S108. 336 \n3. Lim, C.Y., Sohn, B., Seong, M., Kim, E.Y., Kim, S.T., and Won, S.Y . (2024). Need for 337 \nTransparency and Clinical Interpretability in Hemorrhagic Stroke Artificial Intelligence Research: 338 \nPromoting Effective Clinical Application. Yonsei Med J 65, 611–618. 10.3349/ymj.2024.0007. 339 \n4. The Lancet Digital Health. (2024). Large language models: a new chapter in digital health. 340 \nLancet Digit Health 6, e1. 10.1016/S2589-7500(23)00254-6. 341 \n5. Shieh, A., Tran, B., He, G., Kumar, M., Freed, J.A., and Majety, P . (2024). Assessing ChatGPT 342 \n4.0's test performance and clinical diagnostic accuracy on USMLE STEP 2 CK and clinical case 343 \nreports. Sci Rep 14, 9330. 10.1038/s41598-024-58760-x. 344 \n6. Brin, D., Sorin, V., Vaid, A., Soroush, A., Glicksberg, B.S., Charney, A.W., Nadkarni, G., and 345 \nKlang, E. (2023). Comparing ChatGPT and GPT-4 performance in USMLE soft skill 346 \nassessments. Sci Rep 13, 16492. 10.1038/s41598-023-43436-9. 347 \n7. Nori, H., King, N., McKinney, S.M., Carignan, D., and Horvitz, E. Capabilities of GPT-4 on 348 \nmedical challenge problems. Preprint at arXiv, https://doi.org/10.48550/arXiv.2303.13375. 349 \n8. OpenAI. GPT-4o System Card [Internet]. 2024. Available from: https://openai.com/index/gpt-4o-350 \nsystem-card/. Accessed 2025 Jun 18. 351 \n9. Sonoda, Y., Kurokawa, R., Nakamura, Y., Kanzawa, J., Kurokawa, M., Ohizumi, Y., Gonoi, W., 352 \nand Abe, O. (2024). Diagnostic performances of GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro in 353 \n\"Diagnosis Please\" cases. Jpn J Radiol 42, 1231–1235. 10.1007/s11604-024-01619-y. 354 \n10. Ebel, S., Ehrengut, C., Denecke, T., Gossmann, H., and Beeskow, A.B. (2024). GPT-4o's 355 \ncompetency in answering the simulated written European Board of Interventional Radiology 356 \nexam compared to a medical student and experts in Germany and its ability to generate exam 357 \nitems on interventional radiology: a descriptive study. J Educ Eval Health Prof 21, 21. 358 \n10.3352/jeehp.2024.21.21. 359 \n11. Hirano, Y ., Hanaoka, S., Nakao, T., Miki, S., Kikuchi, T., Nakamura, Y ., Nomura, Y., Yoshikawa, 360 \nT., and Abe, O. (2024). GPT-4 Turbo with Vision fails to outperform text-only GPT-4 Turbo in the 361 \nJapan Diagnostic Radiology Board Examination. Jpn J Radiol 42, 918–926. 10.1007/s11604-362 \n024-01561-z. 363 \n12. Nakao, T., Miki, S., Nakamura, Y., Kikuchi, T., Nomura, Y., Hanaoka, S., Yoshikawa, T., and Abe, 364 \nO. (2024). Capability of GPT-4V(ision) in the Japanese National Medical Licensing Examination: 365 \nEvaluation Study. JMIR Med Educ 10, e54393. 10.2196/54393. 366 \n13. OpenAI. OpenAI o1 System Card [Internet]. 2024. Available from: 367 \nhttps://openai.com/index/openai-o1-system-card/. Accessed 2025 Jun 18 368 \n14. Temsah, M.H., Jamal, A., Alhasan, K., Temsah, A.A., and Malki, K.H. (2024). OpenAI o1-369 \nPreview vs. ChatGPT in Healthcare: A New Frontier in Medical AI Reasoning. Cureus 16, 370 \ne70640. 10.7759/cureus.70640. 371 \n15. Xie, Y., Wu, J., Tu, H., Yang, S., Zhao, B., and Zong, Y. A preliminary study of o1 in medicine: 372 \nare we closer to an AI doctor? Preprint at arXiv, https://doi.org/10.48550/arXiv.2409.15277. 373 \n16. Nori, H., Usuyama, N., King, N., McKinney, S.M., Fernandes, X., and Zhang, S. From 374 \nMedprompt to o1: exploration of run-time strategies for medical challenge problems and beyond. 375 \nPreprint at arXiv, https://doi.org/10.48550/arXiv.2411.03590. 376 \n17. Xu, S., Zhou, Y ., Liu, Z., Wu, Z., Zhong, T., and Zhao, H. Towards next-generation medical 377 \nagent: how o1 is reshaping decision-making in medical scenarios. Preprint at arXiv, 378 \nhttps://doi.org/10.48550/arXiv.2411.14461. 379 \n18. Liu, C.L., Ho, C.T., and Wu, T.C. (2024). Custom GPTs Enhancing Performance and Evidence 380 \nCompared with GPT-3.5, GPT-4, and GPT-4o? A Study on the Emergency Medicine Specialist 381 \nExamination. Healthcare (Basel) 12. 10.3390/healthcare12171726. 382 \n19. Barile, J., Margolis, A., Cason, G., Kim, R., Kalash, S., Tchaconas, A., and Milanaik, R. (2024). 383 \nDiagnostic Accuracy of a Large Language Model in Pediatric Case Studies. JAMA Pediatr 178, 384 \n313–315. 10.1001/jamapediatrics.2023.5750. 385 \n20. Hadi, A., Tran, E., Nagarajan, B., and Kirpalani, A. (2024). Evaluation of ChatGPT as a 386 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \ndiagnostic tool for medical learners and clinicians. PLoS One 19, e0307383. 387 \n10.1371/journal.pone.0307383. 388 \n21. Brodeur, P .G., Buckley, T.A., Kanjee, Z., Goh, E., Ling, E.B., and Jain, P . Superhuman 389 \nperformance of a large language model on the reasoning tasks of a physician. Preprint at arXiv, 390 \nhttps://doi.org/10.48550/arXiv.2412.10849. 391 \n22. Singh, H., Giardina, T.D., Meyer, A.N., Forjuoh, S.N., Reis, M.D., and Thomas, E.J. (2013). 392 \nTypes and origins of diagnostic errors in primary care settings. JAMA Intern Med 173, 418-425. 393 \n10.1001/jamainternmed.2013.2777. 394 \n23. Newman-Toker, D.E., Schaffer, A.C., Yu-Moe, C.W., Nassery, N., Saber Tehrani, A.S., Clemens, 395 \nG.D., Wang, Z., Zhu, Y ., Fanai, M., and Siegal, D. (2019). Serious misdiagnosis-related harms in 396 \nmalpractice claims: The \"Big Three\" - vascular events, infections, and cancers. Diagnosis (Berl) 397 \n6, 227–240. 10.1515/dx-2019-0019. 398 \n24. Bindraban, R.S., Ten Berg, M.J., Naaktgeboren, C.A., Kramer, M.H.H., Van Solinge, W.W., and 399 \nNanayakkara, P.W.B. (2018). Reducing Test Utilization in Hospital Settings: A Narrative Review. 400 \nAnn Lab Med 38, 402–412. 10.3343/alm.2018.38.5.402. 401 \n25. Brownlee, S., Chalkidou, K., Doust, J., Elshaug, A.G., Glasziou, P ., Heath, I., Nagpal, S., Saini, 402 \nV., Srivastava, D., Chalmers, K., and Korenstein, D. (2017). Evidence for overuse of medical 403 \nservices around the world. Lancet 390, 156–168. 10.1016/S0140-6736(16)32585-5. 404 \n26. Rao, A., Pang, M., Kim, J., Kamineni, M., Lie, W., Prasad, A.K., Landman, A., Dreyer, K., and 405 \nSucci, M.D. (2023). Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: 406 \nDevelopment and Usability Study. J Med Internet Res 25, e48659. 10.2196/48659. 407 \n27. Du, X., Zhou, Z., Wang, Y ., Chuang, Y.W., Yang, R., Zhang, W., Wang, X., Zhang, R., Hong, P., 408 \nBates, D.W., and Zhou, L. (2024). Generative Large Language Models in Electronic Health 409 \nRecords for Patient Care Since 2023: A Systematic Review. medRxiv 2024. 410 \n10.1101/2024.08.11.24311828. 411 \n28. Dankwa-Mullan, I. (2024). Health Equity and Ethical Considerations in Using Artificial 412 \nIntelligence in Public Health and Medicine. Prev Chronic Dis 21, E64. 10.5888/pcd21.240245. 413 \n29. Chew, B.H., and Ngiam, K.Y . (2025). Artificial intelligence tool development: what clinicians need 414 \nto know? BMC Med 23, 244. 10.1186/s12916-025-04076-0. 415 \n30. Ahmed, M.I., Spooner, B., Isherwood, J., Lane, M., Orrock, E., and Dennison, A. (2023). A 416 \nSystematic Review of the Barriers to the Implementation of Artificial Intelligence in Healthcare. 417 \nCureus 15, e46454. 10.7759/cureus.46454. 418 \n31. Roustan, D., and Bastardot, F. (2025). The Clinicians' Guide to Large Language Models: A 419 \nGeneral Perspective With a Focus on Hallucinations. Interact J Med Res 14, e59823. 420 \n10.2196/59823. 421 \n32. J, B.R., Sood, A., Pattnaik, T., Malhotra, R., Nayyar, V., Narayan, B., Mishra, D., and Surya, V. 422 \n(2025). Medical imaging privacy: A systematic scoping review of key parameters in dataset 423 \nconstruction and data protection. J Med Imaging Radiat Sci 56, 101914. 424 \n10.1016/j.jmir.2025.101914. 425 \n33. Cestonaro, C., Delicati, A., Marcante, B., Caenazzo, L., and Tozzo, P. (2023). Defining medical 426 \nliability when artificial intelligence is applied on diagnostic algorithms: a systematic review. Front 427 \nMed (Lausanne) 10, 1305756. 10.3389/fmed.2023.1305756. 428 \n34. Obermeyer, Z., Powers, B., Vogeli, C., and Mullainathan, S. (2019). Dissecting racial bias in an 429 \nalgorithm used to manage the health of populations. Science 366, 447–453. 430 \n10.1126/science.aax2342. 431 \n35. Clinical challenges. Medscape website. 432 \nhttps://reference.medscape.com/features/casechallenges. Accessed 2025 Jun 18 433 \n 434 \n 435 \n  436 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nSTAR/i1 METHODS 437 \nKEY RESOURCES TABLE 438 \nREAGENT OR RESOURCE SOURCE IDENTIFIER \nDeposited data \nMedscape case challenge questions https://reference.medscape.com/features/ca\nsechallenges \nThe Medscape case challenge scenarios \nand questions are publicly available on the \nwebsite. \nSoftware and algorithms \nPython https://www.python.org/ Version 3.11.3 \npandas https://pandas.pydata.org/ Version 2.2.2 \nopenai https://platform.openai.com/docs Version 1.86.0 \nGPT-3.5 Turbo https://platform.openai.com/docs/models/gpt\n-3-5 \nVersion gpt-3.5-turbo-0613 \nGPT-4 Turbo https://platform.openai.com/docs/models/gpt\n-4 \nVersion gpt-4-0613 \nGPT-4 Omni https://platform.openai.com/docs/models/gpt\n-4o \nVersion gpt-4o-2024-05-13 \no1 https://platform.openai.com/docs/models/o1  Default Version (queried in Dec 2024) \nR https://www.r-project.org/ Version 4.5.0 \n 439 \n 440 \nEXPERIMENTAL MODEL AND STUDY PARTICIPANT DETAILS 441 \nStudy Design 442 \nThis retrospective study analyzed Medscape clinical case challenge questions from May 2011 to June 443 \n2024 (Figure 4 and Table S2 in the Supplementary Appendix).35 Medscape clinical case challenges 444 \nare interactive educational tools designed to help healthcare professionals assess and improve their 445 \nclinical knowledge and decision-making skills. Exclusion criteria included non-question contents and 446 \nmultiple choice questions. Case data were collected using a custom Python-based web scraper that 447 \nretrieved publicly available content from the Medscape website. Only publicly accessible cases were 448 \nincluded, and no personal data were collected. Ethical approval was not required due to the retrospective 449 \nnature of the study and the use of open-source data. 450 \nEach case presented a realistic clinical scenario, including patient history, physical examination findings, 451 \ndiagnostic test results, and imaging studies. Participants were tasked with selecting the most appropriate 452 \nanswer from a set of multiple-choice options covering aspects of diagnosis, disease characteristics, 453 \nexaminations, and treatment strategies. Clinicians’ responses were defined as the most frequently 454 \nselected option for each question. Each question had a predefined correct answer established by the 455 \nMedscape case author based on the confirmed diagnosis and clinical guidelines. The cases were 456 \ncategorized by medical specialty, including internal medicine (allergy, cardiology, endocrinology, 457 \ngastroenterology, hematology, infectious diseases, nephrology, oncology, pulmonology, and 458 \nrheumatology), major surgery (obstetrics/gynecology and surgery), pediatrics, psychiatry, and minor 459 \nspecialties (dermatology, emergency medicine, neurology, neurosurgery, ophthalmology, orthopedics, 460 \notorhinolaryngology, and urology). 461 \nMETHOD DETAILS 462 \nLarge Language Models and Prompt 463 \nEach GPT model (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4 Omni, and o1) was accessed through the 464 \nOpenAI application programming interface (API) between August and December 2024. Eash model was 465 \ntasked to generate a formatted response based on the options available for each question. To ensure 466 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n \nconsistency and replicability, the models were queried using predefined prompt templates that simulated 467 \nthe clinical reasoning scenarios. Table 3 outlines the prompts used to query the models, including their 468 \nstructures and formatting. For questions that included images, both textual and visual data were provided 469 \nto the models, where applicable, enabling multimodal input processing in GPT-4o and o1. The original 470 \nimages uploaded by Medscape were used without any preprocessing. The chat session was reset before 471 \neach new question to eliminate the influence of memory retention or in-context learning, ensuring that all 472 \nquestions were processed independently and that responses remained unbiased. GPT-3.5 Turbo, GPT-4 473 \nTurbo, and GPT-4 Omni were configured with a temperature of 0.1 and top-p of 1.0, while o1 used its 474 \ndefault settings of a temperature of 1.0 and top-p of 1.0. 475 \n 476 \nQUANTIFICATION AND STATISTICAL ANALYSIS 477 \nWe compared the performances of the four GPT models (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4 Omni, 478 \nand o1) with those of clinicians using mixed-effects logistic regression analysis to evaluate differences in 479 \naccuracy. The model type was included as a fixed effect to compare performance across models, and a 480 \nrandom intercept was added for each case to account for potential correlations among nested questions 481 \nwithin the same case. We calculated the accuracy and 95% confidence intervals (CIs) for each model and 482 \nclinician and performed pairwise comparisons between them. For cases with images, a focused analysis 483 \nof GPT-4o and o1 was conducted to specifically assess their multimodal capability in interpreting visual 484 \ndata. The dataset was further analyzed by question category (diagnosis, characteristics, examination, and 485 \ntreatment) and medical specialty (internal medicine, major surgery, pediatrics, psychiatry, and minor 486 \nspecialties). GPT-4o and o1 were tested over five independent trials using the same set of questions to 487 \nevaluate response consistency. All analyses were conducted using R (version 4.5.0, R Foundation for 488 \nStatistical Computing, Vienna, Austria), with a P-value < .05 considered statistically significant. 489 \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint \n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 23, 2025. ; https://doi.org/10.1101/2025.06.22.25330068doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.41929566860198975
    },
    {
      "name": "Natural language processing",
      "score": 0.3439343571662903
    },
    {
      "name": "Psychology",
      "score": 0.3333018720149994
    },
    {
      "name": "Linguistics",
      "score": 0.32345688343048096
    },
    {
      "name": "Philosophy",
      "score": 0.18827223777770996
    }
  ],
  "institutions": [],
  "cited_by": 1
}