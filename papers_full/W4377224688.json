{
  "title": "Dimensionality and Ramping: Signatures of Sentence Integration in the Dynamics of Brains and Deep Language Models",
  "url": "https://openalex.org/W4377224688",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2922131337",
      "name": "Theo Desbordes",
      "affiliations": [
        "Cognitive Neuroimaging Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2404476342",
      "name": "Yair Lakretz",
      "affiliations": [
        "Cognitive Neuroimaging Lab"
      ]
    },
    {
      "id": "https://openalex.org/A1837702824",
      "name": "Valérie Chanoine",
      "affiliations": [
        "Centre de Recherche et d’Enseignement de Géosciences de l’Environnement"
      ]
    },
    {
      "id": "https://openalex.org/A209329885",
      "name": "Maxime Oquab",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209548128",
      "name": "Jean-Michel Badier",
      "affiliations": [
        "Institut de Neurosciences de la Timone",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2619038954",
      "name": "Agnès Trébuchon",
      "affiliations": [
        "Assistance Publique Hôpitaux de Marseille",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2114318131",
      "name": "Romain Carron",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Assistance Publique Hôpitaux de Marseille"
      ]
    },
    {
      "id": "https://openalex.org/A3175547144",
      "name": "Christian G. Bénar",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Institut de Neurosciences des Systèmes"
      ]
    },
    {
      "id": "https://openalex.org/A126417355",
      "name": "Stanislas Dehaene",
      "affiliations": [
        "Université Paris-Saclay",
        "Commissariat à l'Énergie Atomique et aux Énergies Alternatives",
        "Université Paris Sciences et Lettres"
      ]
    },
    {
      "id": "https://openalex.org/A4226093966",
      "name": "Jean-Remi King",
      "affiliations": [
        "École Normale Supérieure - PSL",
        "Cognitive Neuroimaging Lab",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2922131337",
      "name": "Theo Desbordes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2404476342",
      "name": "Yair Lakretz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1837702824",
      "name": "Valérie Chanoine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A209329885",
      "name": "Maxime Oquab",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209548128",
      "name": "Jean-Michel Badier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2619038954",
      "name": "Agnès Trébuchon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114318131",
      "name": "Romain Carron",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3175547144",
      "name": "Christian G. Bénar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A126417355",
      "name": "Stanislas Dehaene",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226093966",
      "name": "Jean-Remi King",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3022734720",
    "https://openalex.org/W3178228406",
    "https://openalex.org/W2760196906",
    "https://openalex.org/W4293223172",
    "https://openalex.org/W2168306877",
    "https://openalex.org/W2042003045",
    "https://openalex.org/W2156663106",
    "https://openalex.org/W2123480151",
    "https://openalex.org/W3024993598",
    "https://openalex.org/W2170567676",
    "https://openalex.org/W2021032538",
    "https://openalex.org/W2158113120",
    "https://openalex.org/W3039556919",
    "https://openalex.org/W3154773080",
    "https://openalex.org/W1974803102",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2037693308",
    "https://openalex.org/W3013369344",
    "https://openalex.org/W2101135654",
    "https://openalex.org/W3194734757",
    "https://openalex.org/W4285392847",
    "https://openalex.org/W2586691160",
    "https://openalex.org/W3009009806",
    "https://openalex.org/W2526072446",
    "https://openalex.org/W1988812422",
    "https://openalex.org/W2995567445",
    "https://openalex.org/W2960869174",
    "https://openalex.org/W2621961142",
    "https://openalex.org/W2766284800",
    "https://openalex.org/W2040739363",
    "https://openalex.org/W4220949944",
    "https://openalex.org/W2892053535",
    "https://openalex.org/W2169918686",
    "https://openalex.org/W2513463761",
    "https://openalex.org/W2164051460",
    "https://openalex.org/W2978950289",
    "https://openalex.org/W2091507471",
    "https://openalex.org/W3211949750",
    "https://openalex.org/W6720212515",
    "https://openalex.org/W4293750141",
    "https://openalex.org/W4289638300",
    "https://openalex.org/W1965248225",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2528108497",
    "https://openalex.org/W6751425215",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2157108417",
    "https://openalex.org/W2153076044",
    "https://openalex.org/W2079092292",
    "https://openalex.org/W2037504148",
    "https://openalex.org/W3038027488",
    "https://openalex.org/W2082842332",
    "https://openalex.org/W1661597557",
    "https://openalex.org/W2922523190",
    "https://openalex.org/W1978415413",
    "https://openalex.org/W2107265154",
    "https://openalex.org/W2955321733",
    "https://openalex.org/W2149158842",
    "https://openalex.org/W2047125104",
    "https://openalex.org/W2077723500",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2983769105",
    "https://openalex.org/W2121393656",
    "https://openalex.org/W3088418428",
    "https://openalex.org/W2024579655",
    "https://openalex.org/W2606837722",
    "https://openalex.org/W2073146149",
    "https://openalex.org/W4214909510",
    "https://openalex.org/W3019166713",
    "https://openalex.org/W1995672192",
    "https://openalex.org/W2905623858",
    "https://openalex.org/W2177117292",
    "https://openalex.org/W1965092590",
    "https://openalex.org/W2157306293",
    "https://openalex.org/W2107627409",
    "https://openalex.org/W1969634671",
    "https://openalex.org/W2332216477",
    "https://openalex.org/W2978450937",
    "https://openalex.org/W2994855366",
    "https://openalex.org/W2950514661",
    "https://openalex.org/W2101295242",
    "https://openalex.org/W3120392574",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4300402905",
    "https://openalex.org/W2049647213",
    "https://openalex.org/W3210923133",
    "https://openalex.org/W2030831236",
    "https://openalex.org/W2997938465",
    "https://openalex.org/W2046557060",
    "https://openalex.org/W2013494846",
    "https://openalex.org/W3136636939",
    "https://openalex.org/W1567277581",
    "https://openalex.org/W3142226569",
    "https://openalex.org/W2795349950",
    "https://openalex.org/W2775133029",
    "https://openalex.org/W2423225835",
    "https://openalex.org/W2129753520",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W6763027002",
    "https://openalex.org/W2037287753",
    "https://openalex.org/W2765508388",
    "https://openalex.org/W3041725488",
    "https://openalex.org/W2083893109",
    "https://openalex.org/W3107793556",
    "https://openalex.org/W4211115742",
    "https://openalex.org/W1510964369",
    "https://openalex.org/W2754478492",
    "https://openalex.org/W4390692365",
    "https://openalex.org/W2470334021",
    "https://openalex.org/W4306643150",
    "https://openalex.org/W3139046810",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W3159684727",
    "https://openalex.org/W4389085015",
    "https://openalex.org/W2947012833",
    "https://openalex.org/W3102539921",
    "https://openalex.org/W2409950528",
    "https://openalex.org/W3134545286",
    "https://openalex.org/W2805003518",
    "https://openalex.org/W3104335155"
  ],
  "abstract": "A sentence is more than the sum of its words: its meaning depends on how they combine with one another. The brain mechanisms underlying such semantic composition remain poorly understood. To shed light on the neural vector code underlying semantic composition, we introduce two hypotheses: (1) the intrinsic dimensionality of the space of neural representations should increase as a sentence unfolds, paralleling the growing complexity of its semantic representation; and (2) this progressive integration should be reflected in ramping and sentence-final signals. To test these predictions, we designed a dataset of closely matched normal and jabberwocky sentences (composed of meaningless pseudo words) and displayed them to deep language models and to 11 human participants (5 men and 6 women) monitored with simultaneous MEG and intracranial EEG. In both deep language models and electrophysiological data, we found that representational dimensionality was higher for meaningful sentences than jabberwocky. Furthermore, multivariate decoding of normal versus jabberwocky confirmed three dynamic patterns: (1) a phasic pattern following each word, peaking in temporal and parietal areas; (2) a ramping pattern, characteristic of bilateral inferior and middle frontal gyri; and (3) a sentence-final pattern in left superior frontal gyrus and right orbitofrontal cortex. These results provide a first glimpse into the neural geometry of semantic integration and constrain the search for a neural code of linguistic composition. SIGNIFICANCE STATEMENT Starting from general linguistic concepts, we make two sets of predictions in neural signals evoked by reading multiword sentences. First, the intrinsic dimensionality of the representation should grow with additional meaningful words. Second, the neural dynamics should exhibit signatures of encoding, maintaining, and resolving semantic composition. We successfully validated these hypotheses in deep neural language models, artificial neural networks trained on text and performing very well on many natural language processing tasks. Then, using a unique combination of MEG and intracranial electrodes, we recorded high-resolution brain data from human participants while they read a controlled set of sentences. Time-resolved dimensionality analysis showed increasing dimensionality with meaning, and multivariate decoding allowed us to isolate the three dynamical patterns we had hypothesized.",
  "full_text": "Behavioral/Cognitive\nDimensionality and Ramping: Signatures of Sentence\nIntegration in the Dynamics of Brains and Deep Language\nModels\nThéo Desbordes,1 Yair Lakretz,2\n Valérie Chanoine,3 Maxime Oquab,4 Jean-Michel Badier,5 Agnès Trébuchon,6\nRomain Carron,7 Christian-G. Bénar,8 Stanislas Dehaene,9p and Jean-Rémi King1,10p\n1Meta AI Research, Paris 75002, France; and Cognitive Neuroimaging Unit NeuroSpin center, 91191, Gif-sur-Yvette, France, 2Cognitive\nNeuroimaging Unit NeuroSpin center, Gif-sur-Yvette, 91191, France, 3Institute of Language, Communication and the Brain, Aix-en-\nProvence, 13100, France; and Aix-Marseille Université, Centre National de la Recherche Scientifique, LPL, Aix-en-Provence, 13100,\nFrance, 4Meta AI Research, Paris 75002, France,5Aix Marseille Université, Institut National de la Santé et de la Recherche Médicale, CNRS,\nLPL, Aix-en-Provence 13100; and Inst Neurosci Syst, Marseille, 13005, France, 6Aix Marseille Université, Institut National de la Santé et\nde la Recherche Médicale, CNRS, LPL, Aix-en-Provence 13100, France; and Inst Neurosci Syst, Marseille, 13005, France; and Assistance\nPublique Hopitaux de Marseille, Timone hospital, Epileptology and Cerebral Rythmology, Marseille, 13385, France, 7Aix Marseille\nUniversité, Institut National de la Santé et de la Recherche Médicale, CNRS, LPL, Aix-en-Provence 13100, France; and Inst Neurosci Syst,\nMarseille, 13005, France; and Assistance Publique Hopitaux de Marseille, Timone hospital, Functional and Stereotactic Neurosurgery,\nMarseille, 13385, France, 8Aix Marseille Université, Institut National de la Santé et de la Recherche Médicale, CNRS, LPL, Aix-en-Provence\n13100, France; and Inst Neurosci Syst, Marseille, 13005, France, 9Université Paris Saclay, Institut National de la Santé et de la Recherche\nMédicale, Commissariat à l’Energie Atomique, Cognitive Neuroimaging Unit, NeuroSpin center, Saclay, 91191, France; and Collège de\nFrance, PSL University, Paris, 75231, France, and 10LSP, École normale supérieure, PSL (Paris Sciences & Lettres) University, CNRS,\n75005 Paris, France\nA sentence is more than the sum of its words: its meaning depends on how they combine with one another. The brain\nmechanisms underlying such semantic composition remain poorly understood. To shed light on the neural vector\ncode underlying semantic composition, we introduce two hypotheses: (1) the intrinsic dimensionality of the space of\nneural representations should increase as a sentence unfolds, paralleling the growing complexity of its semantic repre-\nsentation; and (2) this progressive integration should be reflected in ramping and sentence-final signals. To test these\npredictions, we designed a dataset of closely matched normal and jabberwocky sentences (composed of meaningless\npseudo words) and displayed them to deep language models and to 11 human participants (5 men and 6 women)\nmonitored with simultaneous MEG and intracranial EEG. In both deep language models and electrophysiological data,\nwe found that representational dimensionality was higher for meaningful sentences than jabberwocky. Furthermore,\nmultivariate decoding of normal versus jabberwocky confirmed three dynamic patterns: (1) a phasic pattern following\neach word, peaking in temporal and parietal areas; (2) a ramping pattern, characteristic of bilateral inferior and mid-\ndle frontal gyri; and (3) a sentence-final pattern in left superior frontal gyrus and right orbitofrontal cortex. These\nresults provide a first glimpse into the neural geometry of semantic integration and constrain the search for a neural\ncode of linguistic composition.\nKey words:dimensionality; integration; intracranial; language; ramping; semantic composition\nReceived June 14, 2022; revised Feb. 7, 2023; accepted Feb. 19, 2023.\nAuthor contributions: T.D., V.C., C.-G.B., S.D., and J.-R.K. designed research; T.D., V.C., J.-M.B., A.T., R.C.,\nand C.-G.B. performed research; T.D., Y.L., and J.-R.K. analyzed data; T.D. wrote the first draft of the paper;\nT.D., Y.L., V.C., C.-G.B., S.D., and J.-R.K. edited the paper; T.D. and J.-R.K. wrote the paper; M.O. contributed\nunpublished reagents/analytic tools.\nResearchers from the Convergence Institute of Language, Communication and the Brain(ILCB), were supported\nby grants from France 2030 (ANR-16-CONV-0002) and the Excellence Initiative of Aix-Marseille University\n(A*MIDEX). C.-G.B. was supported in part by Agence Nationale de la recherche“SCALES” ANR-17-HBPR-0005, FLAG\nERA/HBP. S.D. was supported by Institut National de la Santé et de la Recherche Médicale, Commissariat à l’Energie\nAtomique, Collège de France, the Bettencourt-Schueller foundation and an ERC grant “NeuroSyntax.” Data\nacquisition was performed on a platform member of France Life Imaging network Grant ANR-11-INBS-0006.\nThe authors declare no competing financial interests.\nCorrespondence should be addressed to Stanislas Dehaene at stanislas.dehaene@gmail.com.\nhttps://doi.org/\n10.1523/JNEUROSCI.1163-22.2023\nCopyright © 2023 the authors\n5350  The Journal of Neuroscience, July 19, 2023 43(29):5350– 5364\nSignificance Statement\nStarting from general linguistic concepts, we make two sets of predictions in neural signals evoked by reading multiword sen-\ntences. First, the intrinsic dimensionality of the representation should grow with additional meaningful words. Second, the\nneural dynamics should exhibit signatures of encoding, maintaining, and resolving semantic composition. We successfully\nvalidated these hypotheses in deep neural language models, artificial neural networks trained on text and performing very\nwell on many natural language processing tasks. Then, using a unique combination of MEG and intracranial electrodes, we\nrecorded high-resolution brain data from human participants while they read a controlled set of sentences. Time-resolved\ndimensionality analysis showed increasing dimensionality with meaning, and multivariate decoding allowed us to isolate the\nthree dynamical patterns we had hypothesized.\nIntroduction\nDuring sentence comprehension, the human brain must bind\nsuccessive words into an integrated representation. The neural\nbasis of such compositionality has been partially studied for\ntwo word phrases (Pylkkänen, 2019, 2020) but remains largely\nunknown for longer constituents. Here, combining ideas from\nthe neural population framework (\nGeorgopoulos et al., 1986;\nYuste, 2015; Ebitz and Hayden, 2021) and vector models of lin-\nguistic composition (Smolensky, 1990), we hypothesize that the\nneural manifold representing sentences should grow with the\nprogressive addition of meaningful words. Two sets of predic-\ntions unfold from this vector framework (\nFig. 1).\nFirst, for each word that the subject successfully integrates, we\npredict an increase in intrinsic dimensionality (ID) of the corre-\nsponding neural representation, that is, the number of independent\ndimensions that participate in the encoding (\nCarreira-Perpinán,\n1997)o r “the dimensionality of the manifold that approximately\nembeds the data” (Del Giudice, 2021). ID should increase as new\nmeaning elements are put together because each word, once bound\nto its syntactic role, should occupy a distinct axis in neural vector\nspace (\nSmolensky, 1990): when we combine real words with one\nanother, we generate a meaning that is more than the sum of its\nparts and thus requires additional dimensions. Intuitively, one can\nthink of concept cells (\nQuiroga et al., 2005)b e i n gr e c r u i t e dt o\nencode incoming elements and their relationships. For jabber-\nwocky sentences, where meaningless pseudowords replace actual\nwords (\nMazoyer et al., 1993; Hahne and Jescheniak, 2001), we pre-\ndict the recruitment of a reduced set of semantic dimensions\nbecause pseudowords carry little or no meaningful information.\nOur second set of predictions relates to the dynamics of com-\nposition. We predict that meaningful composition will lead to a\ngrowing superposition of neural codes each recruiting additional\ndimensions, and thus leading to ramping neural activity over the\nc o u r s eo ft h es e n t e n c e(\nFig. 1). Several studies support this predic-\ntion (Bastiaansen et al., 2010; Pallier et al., 2011; Fedorenko et al.,\n2016; Nelson et al., 2017), but they did not clearly separate multi-\nword integration from other semantic processes of lexical access\nand sentence wrap-up. Here, we used multivariate decoding with\ntemporal generalization (\nKing and Dehaene, 2014; Fyshe, 2020)t o\ndisentangle them. We derived theoretical generalization matrices\nfor the predicted dynamics of classifiers trained to separate normal\nand jabberwocky sentences for each process (\nFig. 2D). Those three\nstages are not tied to a particular theory of sentence processing\n(\nJust and Carpenter, 1980; Seidenberg and McClelland, 1989;\nSteedman, 2001; Lewis and Vasishth, 2005); rather, we propose\nthem as necessary steps in sentence comprehension.\nFirst, lexical access is predicted to elicit a phasic, transient\nresponse that separates normal words from jabberwocky (Just\nand Carpenter, 1980; Caramazza, 1997)( Fig. 2D, red), especially\nin the anterior fusiform gyrus (FuG), superior temporal gyrus\n(STG), and middle temporal gyrus (MTG) (Nobre et al., 1994;\nBinder et al., 2003; Woolnough et al., 2020).\nSecond, multiword integration is predicted to elicit ramping\ndynamics, characterized by an increasingly strong square pattern\nin the temporal generalization matrix (Fig. 2D,b l u e ) .T h i sp a t -\ntern is predominantly expected in inferior frontal gyrus (IFG)\nand part of superior temporal sulcus (STS) (\nPallier et al., 2011;\nFedorenko et al., 2016; Nelson et al., 2017).\nLast, sentence-final wrap-up processes are predicted to occur\nafter the last word and elicit a sentence-final separation of normal\nversus jabberwocky sentences (\nFig. 2D, green). Indeed, readers\nare known to pause at the end of sentences to integrate, interpret,\nand incorporate the constituting elements into the general con-\ntext of the discourse (\nJust and Carpenter, 1980). Behavioral eye-\ntracking studies have evidenced an increased reading time for\nsentence-final words (\nWarren et al., 2009; Kuperman et al., 2010).\nEnd-of-sentence neural signatures of in-sentence grammatical\ngender violation (\nMolinaro et al., 2008) and syntactic complexity\n(Pattamadilok et al., 2016)h a v ea l s ob e e no b s e r v e d .\nWe tested the above predictions in both humans and artificial\nneural networks. Convergent findings would support the view\nthat those properties are general features of linguistic composition.\nMaterials and Methods\nEthics. Eleven right-handed individuals (5 men and 6 women; age\nrange = 25-57 yr, mean = 40 yr, SD = 9.4 yr) with intracranial stereotactic\nelectrodes implantation as part of their treatment for refractory epilepsy\ngave their informed consent to participate in our study, in accordance\nwith the ethic evaluation RCB 2018-A02363-52. All patients were\nimplanted with depth electrodes for clinical purpose (presurgical evalua-\ntion) in the Epileptology and Cerebral Rythmology Department of the\nTimone Hospital (Marseille, France). Neuropsychological assessment\nindicated that all patients had intact language functions. Their reading\nability was controlled by means of a French version of a reading test (test\nMalabi, Unité Institut National de la Santé et de la Recherche Médicale-\nCommissariat à l’Energie Atomique de Neuroimagerie Cognitive).\nStimuli and task.Eight-word-long sentences were presented to the\nparticipants in a Rapid Stream Visual Presentation with an SOA of\n400 ms. Each sentence was preceded and followed by visual masks\n(####) to keep forward and backward masking constant (\nFig. 2A).\nThe stimuli were generated using a custom sentence generator script\nthat constructs a wide range of sentences from a finite vocabulary set,\nrespecting several constraints. (a) All sentences were 8 words long. (b)\nEach sentence comprised a systematic alternation of short function\nwords (determiners and auxiliary) and longer content words (nouns and\nverbs). (c) The sentences consisted of a noun phrase (NP) followed by a\nverb phrase (VP). The NP consisted of a determiner, a noun (the subject\nof the sentence), and optionally one or two prepositions. The VP con-\nsisted of an auxiliary, a verb, and optionally a determiner and a noun\n(the object of the verb) and one or two prepositions. (d) There were\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5351\nFigure 2. Experimental design.A, Example stimuli for the three conditions: normal text, jabberwocky, and list of consonant strings (stringlist). Masks of‘#’ were presented before and after\neach sentence to keep visual masking approximately constant.B, Extraction of activations from Neural Language Models (NLMs) and combined MEG-sEEG brain recordings.C, Example of the\nthree syntactic structures used in this study, varying in the relative size of the Noun Phrase (NP) and the Verb Phrase (VP).D, Theoretical temporal generalization patterns. Regions involved in\nlexical processing of single words would differentiate normal content words from jabberwocky content words based on lexical access mechanisms (thefunction words are the same in both con-\nditions), yielding a phasic pattern. Regions involved in compositional processes should exhibit a ramping generalization pattern, where normal and jabberwocky sentences become more and\nmore differentiated with each incoming word. Finally, regions involved in wrap-up processes would separate normal and jabberwocky only after the sentence is finished, leading to a sentence-\nfinal pattern. For electrode placement, see Extended DataFigure 2-1.\nFigure 1. Hypotheses. As successive words are integrated into a sentence-level representation, a variety of models predict that they are integrated into an in ternal repre-\nsentation, which combines word identity with syntactic role, and sums the resulting filler-role bindings ( Smolensky, 1990 ). Assuming that these neural vectors are sparse,\nthis schema makes two predictions: (1) average neural activity, as indexed for instance by high- g activity, should increase; and (2) the dimensionality of the neural manifold\nevoked by a variety of such sentences should increase because across a variety of words, the vectors point in a variety of directions and therefore span al a r g e rs p a c e .S u c h\nincreases would not be expected (or to a lower extent) for a list of meaningless consonant strings or for a jabberwocky sentence – the latter being constantly represented as\n{agent:meaningless} 1 {complement:meaningless} 1 {verb:meaningless}.\n5352  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping\nthree possible syntactic structures that varied in the size of the NP and\nVP. They could both be of size 2, 4, or 6 words, while their sum was\nalways equal to 8 words (Fig. 2C).\nThe vocabulary consisted of 9 determiners (i.e., three for each gender\nin the singular form and three for the plural form); 10 verbs that could\nappear in singular or plural, in the present or past tense (40 different\nforms); and 75 nouns, among those 46 could appear in the singular or\nplural form (the others were always singular), 46 were masculine (26\nwere used as subjects and objects, the rest appeared in prepositional\nphrases), 26 were feminine (all were used as subjects and objects), and 3\ncould appear in either masculine or feminine form (used as subjects\nonly).\nThe total number of distinct sentences the script could generate was\n791,754. For each subject, in the normal and jabberwocky conditions, we\nsampled an equal number of each syntactic structure, as well as an equal\nnumber of feminine/masculine and singular/plural subjects and objects\nand an equal number of present/past tense for the verb.\nThe jabberwocky stimuli were designed by hand, by changing one or\ntwo letters of each content word to create a corresponding nonword,\nwhile keeping the morphologic markers present.\nStrings lists consisted of strings of consonants with the same length\nas the original words. These letter strings did not have any morphosyn-\ntactic information and thus constituted a low-level, mainly visual, con-\ntrol to the linguistic stimuli used in the experiment.\nThe task was for the participant to detect the presence of target words\nin the sentences and press the response button as fast as possible when\nthe target was present. The target was present in 1 in 11 sentences.\nParticipants performed 330 trials in total (cut down in 6 blocks), com-\nposed of 300 nontarget trials (120 normal sentences, 120 jabberwocky\nsentences, and 60 strings of consonants) and 30 target sentences of a ran-\ndomly selected condition containing the target words, that were dis-\ncarded from the analyses.\nStereotactic EEG (sEEG) and MEG data acquisition and preprocessing.\nMEG and sEEG recordings took place simultaneously in a dimly illumi-\nnated, magnetically shielded room. Recordings were obtained from sub-\njects in supine position to limit the movement during the recording.\nMEG signals were acquired with a 248-channel biomagnetometer\nsystem (Magnetometers, 4D Neuroimaging, located in the MEG facility,\nTimone Hospital, Marseille). The data were recorded continuously with\na band-DC-800 Hz bandwidth with a sampling rate of 2034.51 Hz.\nsEEG recordings were performed using intracerebral multiple con-\ntact electrodes (10-15 contacts, length: 2 mm, diameter: 0.8 mm, 1.5 mm\napart from edge to edge) placed intracranially according to Talairach’s\nstereotactic method (Bancaud et al., 1970; Talairach et al., 1992).\nsEEG as well as EOG and ECG signals, to facilitate the ulterior rejection\nof eye movements, blinks, and cardiac artifacts, were simultaneously\nrecorded with MEG (Badier et al., 2017) with a band-0.01-1000 Hz band-\nwidth with a sampling rate of 2500 Hz using a 256-channel BrainAmp am-\nplifier system (Brain Products). sEEG was then interpolated at the sampling\nrate of the MEG thanks to triggers.\nIn order to determine the location of the head with respect to the\nMEG helmet, five coils were fixed on the subject’s head. The position of\nthese coils as well as the surface of the head were digitized with a 3-D\ndigitizer (Polhemus Fastrack, Polhemus), and head position was meas-\nured at the beginning and at the end of each run. The head shape\nobtained from the digitization of the head was used to check and eventu-\nally compensate for differences in head position between runs or to\nmatch to the participant’s MRI. All stimuli were presented to the subjects\non a mirror by a back-projection system where an LCD projector was\nplaced outside the magnetically shielded room to avoid interfering elec-\ntrical apparatus. The distance between the participant’s eyes and the\nscreen on which stimuli were displayed was similar across patients. A\ntrigger square invisible to the participant was projected onto a photo-\ndiode which was used to signal the presence of a stimulus on-screen and\nto synchronize the MEG and EOG/ECG recordings.\nAmong the 11 patients, 9 underwent an MEG recording at the same\ntime.\nThe sEEG and MEG data were bandpass filtered at 0.3-500 Hz, notch\nfiltered at 50 h, and the first three harmonics to remove line noise. The\ndata were then downsampled to 100 Hz and clipped at 10 times the SD\neither side of the median value, separately for each channel. We then\nused an automatic detection procedure for bad channels in which the\ntemporal variance is computed for each channel and a value above or\nbelow 25 times the median variance over channels leads to rejection.\nEpochs were constructed keeping time points from/C0 0.5 to 5.5 s after\nthe onset of the first mask. We then used a procedure to reject bad\nepochs similar to the one we used for channels: the variance was now\ncomputed over time and the remaining channels, separately for each\nepoch, and a value above or below 5 times the median over epochs lead\nto rejection. Baseline correction was then applied using the 400 ms inter-\nval between the onsets of the first mask and the first word. The data\nwere then smoothed using a 100 ms Hanning window. Finally, Common\nMedian Referencing was applied using all channels but the ones that\nwere marked as bad. All the data preprocessing was done using the\nMNE-Python software (\nGramfort et al., 2013).\nThe high-g activity (inFig. 3) was extracted using a Morlet trans-\nform and 8 frequency bands linearly spaced between 70 and 150 Hz,\nthen combined with principal component analysis (PCA), which we\nfound to be more robust than simple averaging. For all subsequent\nanalyses, we used raw voltage instead high-\ng because we found the\noverall decoding performance to be better. However, we replicated the\ndecoding and dimensionality results with high-g and did not find\nmajor differences.\nLocalizer test. To boost the statistical power, we selected language-\nspecific electrodes using a two-sample temporal cluster permutation test.\nSpecifically, we tested whether the conditions (normal text, jabberwocky)\nand (stringlist) were different at the whole-epoch level. We kept electro-\ndes that contained at least one cluster after the permutation test and\nFalse Discovery Rate (FDR) correction. We used 1000 permutations and\nthreshold-free cluster enhancement (\nSmith and Nichols, 2009)w i t ha\nstarting threshold of 0 and a step of 0.1.\nDimensionality analysis.Within the large dimensionality of the over-\nall neural space (equal to the number of relevant neurons), only a much\nsmaller vector subspace is actually used for encoding (Ebitz and Hayden,\n2021). To quantify this ID, we used a previously reported method based\non PCA (Gao et al., 2017; Elmoznino and Bonner, 2022), sometimes\ncalled the participation ratio (Sorscher et al., 2022). This method quanti-\nfies ID as follows:\nD ¼\nXM\ni¼1ki\n/C16/C17 2\nXM\ni¼1ki2\nwhere ki are the eigenvalues of the neural covariance matrix (i.e., the\neigenvalues whose corresponding eigenvectors are the principal compo-\nnents of the dataset), andM is the number of channels (electrodes or\nmagnetometers). This gives a continuous measure of the number of\nprincipal components needed to explain most of the variance in a data-\nset. Intuitively, one can check that, if the data vary only along a single\ndimension, all of the variance will be explained by the first principal\ncomponent; hence, a single eigenvaluek\n1 will be non-zero, and therefore\nthe formula implies thatD = 1. On the contrary, if the signals in all chan-\nnels vary independently of each other (and with similar magnitude),\nsuch that each principal component explains an equal part of the var-\niance, thenD will be equal to the number of channels. Between those\ntwo extremes,D estimates the approximate number of dimensions that\nvary significantly in the brain signals.\nTo calculateD, we computed a PCA on 0.4 s sliding time windows,\ncombining time points and trials (such that the PCA’s input is a n_times/C2\nn_trials by n_channels matrix), separately for each of the three conditions\n(normal, Jabber, stringlist), and computed the ID of the resulting\neigenspectrum using the aforementioned formula. This analysis was\nrepeated 10 times with different (non-overlapping) parts of the data to get\nan averageD a n dt h ec o r r e s p o n d i n gs t a n d a r de r r o rb a r s .\nThis measure of ID has been used for some time in statistics and\nmachine learning (\nCarreira-Perpinán, 1997; Campadelli et al., 2015)a n d\nhas recently gained traction in the neuroscience domain, where low-\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5353\ndimensional intrinsic dynamics were found in high-dimensional neural\nrecordings (Machens et al., 2010; Churchland et al., 2012; M a n t ee ta l . ,\n2013; Xie et al., 2022). These low-dimensional dynamics have also been\nfound to emerge in trained neural networks (Laje and Buonomano,\n2013; Recanatesi et al., 2021) and have been assigned important roles in\nvarious theories of neural computation (Gallego et al., 2017; Gao et al.,\n2017; Vyas et al., 2020; Ebitz and Hayden, 2021; Sorscher et al., 2021).\nImportantly, ID is not equivalent to overall neural activity. Rather,\nthe two characteristics may vary independently: very high activity in a\nsingle dimension will bring a low dimensionality, whereas the same\namount of activity distributed across more dimensions yields a high\ndimensionality. Furthermore, while overall neural activity is an average\nmeasure, dimensionality reflects the dispersion of activity over a set of\ntrials (here, sentences). It is only because the successive words in senten-\nces convey many possible meanings that we expect the internal vector\nthat encodes them to span an increasingly larger portion of neural space.\nWhile we predict increasing activity and ID due the recruitment of\nadditional dimensions in normal sentences compared with jabberwocky,\nthere are a number of alternative hypotheses. Predictive coding theories\nof language processing (\nShain et al., 2020; Goldstein et al., 2022;\nHeilbron et al., 2022) predict increased brain activations to surprising\nwords, which in our setup should lead to normal sentences having the\nlowest responses. Likewise, if brain activity relates to processing diffi-\nculty (\nJust et al., 1996; Carpenter et al., 1999), then jabberwocky should\nlead to the highest response. Thus, the predicted increase in dimension-\nality, greater for normal than for jabberwocky sentences, should be\nfound if and where brain signals are dominated by compositional\nsemantics. Here, we do not make any specific claim about which charac-\nteristics of the stimuli influence the ID; future studies could test how pa-\nrameters, such as part-of-speech or conceptual specificity, impact the\ndimensionality of neural representations.\nMultivariate decoding.We trained a logistic regression (\nPedregosa et\nal., 2011) to separate normal and jabberwocky sentences at each time\npoint using MEG and sEEG single-trial data. Such a decoding analysis\ninforms us about whether and when our two conditions are differently\nrepresented in neural signals: if at time t the classifier reaches above-\nchance performance, it means that the brain (or the specific ROI) segre-\ngates normal and jabberwocky stimuli at this time. These classifiers were\nthen tested at each other time point according to the temporal general-\nization method (\nKing and Dehaene, 2014). This extension of the tradi-\ntional within-time decoding analysis allows to test for the consistency of\nneural patterns over time: if a classifier trained at time t generalizes to\ntime T, it means that the neural pattern is somewhat similar between\ntime t and T. On the other hand, within-time decoding could be high at\nboth t and T, but with no generalization between t and T. This would\nmean that the brain segregates normal and jabberwocky stimuli at both\ntime points, but with a different pattern of activations. In other words,\nthe within-time decoding performance (trained and tested at the same\ntime, i.e., the diagonal of the temporal generalization matrix) informs us\nabout the content of brain signals, while the across-time decoding per-\nformance (trained and test at different times, i.e., the off-diagonal ele-\nments) tells us about the stability of these representations. Obviously,\nFigure 3. Illustrative profiles of human sEEG responses compatible with the postulated phasic, ramping, and sentence-final patterns. Four examples of electrodes located in the left Fusiform Gyrus (A),\nInferior Parietal Lobule (B), Inferior Frontal Gyrus (C) and Superior Frontal Gyrus (D), responding to normal sentences (blue), jabberwocky sentences (orange), and string of consonants (gray). Each line\nindicates the local field potential (voltage, left) and the high-g (HG power, right) for the same electrode. The eight vertical lines represent the onset of each word in the sentence. For additional illustrative\nprofiles of human sEEG responses, see Extended DataFigure 3-1. For activations from the transformer that exhibit similar dynamics, see Extended DataFigure 3-2.\n5354  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping\nmany different levels of internal representation may allow a classifier to\ncategorize the incoming stimulus as a normal or a jabberwocky sentence.\nHowever, the evolution of these representations over time should, at\nleast partially, separate them.\nBefore training the classifiers, the data were subtracted from its me-\ndian and scaled using the interquartile range, that is, the range between\nthe first quartile (25th quantile) and the third quartile (75th quantile).\nWe used a stratified k-fold cross-validation procedure with 10 folds. We\naverage the classifiers’ performances across all splits and report the aver-\nage performance across subjects. We also stored the area under the curve\n(AUC) for 100 permutations of the test labels to assess significance of\nthe regional pattern regression analysis. To infer the overall tendency to\ngeneralize, we averaged each line of the temporal generalization matrix.\nTemplate regression. We trained a linear regression to predict\nempirical AUC matrices (averaged over patients) from templates.\nSpecifically, each 601 /C2 6 0 1m a t r i x( w eh a v e6 0 1t i m ep o i n t s )w a s\nflattened to a 6012 vector, used in the regression analysis. For each\ntemplate, we made a grid search to select the best parameters\n(delay and width, see Extended Data\nFig. 7-1). We thus tested, for\neach empirical matrix, 100 candidates for each kind of template\n(phasic, ramping, sentence-final). The best template was the one\nwith the highest likelihood in the regression model.\nRegional pattern analysis. ROIs were extracted from the Harvard-\nOxford Cortical Atlas (Desikan et al., 2006). The whole decoding and\npattern regression pipeline was repeated for each region. We only con-\nsidered regions where at least three subjects had electrodes. Thep values\nreported in this section have been FDR-corrected at the region level.\nNLMs. Neural networks have long been used to model natural lan-\nguage processing (Rumelhart and McClelland, 1986; Pater, 2019), and\nNLMs trained on next-word prediction have recently undergone a re-\nvival as models of human language processing (McClelland et al., 2020;\nHale et al., 2021) and learning (Warstadt and Bowman, 2022; but see\nLakretz et al., 2021a; Oh et al., 2022). Here we use NLMs as a testbed to\ncheck whether our hypotheses can be verified in a noise-free language\nprocessing system that has major implementational differences com-\npared with biological neural networks. Several researchers have started\nto analyze activations from NLMs in an attempt to shed light on the neu-\nral codes for language (\nClark et al., 2019; Lakretz et al., 2019; Tenney et\nal., 2019; Rogers et al., 2020), and the present work contributes to this\nfield by introducing the temporal generalization method.\nWe report the results of home-trained character-based transformers\nand long-short term memory (LSTM) (Hochreiter and Schmidhuber,\n1997) models and of CamemBERT (Martin et al., 2020), a BERT (Devlin\net al., 2019) model trained on a very large French dataset. The activations\nextracted from the CamemBERT models were obtained by giving the\nsentence piece by piece to the model and averaging the activation of\neach wordpiece composing a word. Thus, although the model is bidirec-\ntional, we only gave it information about the past up to the current\nword.\nThe character-based LSTM models had 2 layers of 1024 units, while\nthe character-based transformer models had 12 layers and 768 units per\nlayer. Both were trained on a 2GB sample of the French Wikipedia\n(\nMerity et al., 2016). We trained the models for 20 epochs, an initial\nlearning rate of 20, a batch size of 128, a dropout rate of 0.2, and a\nsequence length of 35. At the end of an epoch, if no improvement was\nseen on the validation set, the learning rate was halved. For both LSTMs\nand transformers models, we used 10 instantiations of the model with\ndifferent random seeds and report the performance averaged over all\nseeds. To obtain a single activation vector per word, we used the average\nactivation evoked by each character belonging to the word for the char-\nacter embedding layer, and the activations at the last character of the\nword for each upper layer. Untrained models were initialized with ran-\ndom weights (all sampled uniformly between/C0 0.1 and 0.1), and directly\nunderwent the same procedure for extracting their activations. We chose\ncharacter-based models because word-based models cannot generalize\nto jabberwocky (they only take trained words as input), whereas charac-\nter-based models can take any string as input.\nFor the decoding analysis, we used a sample of 1000 sentences of\neach condition, generated using the same script as the subjects.\nResults\nThe present work aimed to address three main questions: how\ndoes the dimensionality of the neural representation evolve dur-\ning sentence processing? Can phasic, ramping, and sentence-\nfinal signals be disentangled in brain dynamics? Do they occur in\nseparate brain regions? We start with a quick overview of the\ndiversity of neural signals in our dataset. We then present the\nID analysis, followed by multivariate decoding in artificial\nNLMs and real brain signals. Finally, we quantify the presence\nof each theoretical pattern in the empirical generalization mat-\nrices by means of multiple regression and replicate the decod-\ning analysis in multiple brain regions. Overall, our results back\nthe idea that learning language is associated with a predictable\nshaping of the representational manifold, such that meaningful\nsentences evoke ramping signals and a larger number of repre-\nsentational dimensions.\nDiversity of sEEG responses during sentence processing\nEleven patients with intracranial sEEG electrodes implanted for\nclinical purposes (sEEG, totaln = 2243 electrodes; Extended Data\nFig. 2-1) and simultaneously recorded MEG (n = 276 sensors per\nsubject) read 240 sentences in rapid stream visual presentation,\nwith an SOA of 400 ms. Among them, half were normal French\nsentences, and the other half were the syntactically matched jab-\nberwocky sentences. Each sentence consists of eight words alter-\nnating between function words (determiners and auxiliary in\npositions 1, 3, 5, and 7) and content words (nouns and verbs or\ntheir equivalent jabberwocky pseudowords in positions 2, 4, 6,\nand 8). These stimuli were mixed with 60“string lists” of similar\nlength, which consist of meaningless sequences of strings (\nFig.\n2A) and were used as a low-level control. Specifically, as a local-\nizer test, before all analyses, we selected language-selective chan-\nnels using temporal cluster permutation test: only the channels\nwith at least one significant cluster, after FDR correction at the\nelectrode level, when comparing (1) normal or jabberwocky sen-\ntences to (2) string lists were kept for subsequent analyses.\nWe begin with a quick descriptive overview of the diversity\nof brain signals across electrodes and patients.\nFigure 3 illus-\ntrates some of these responses, both in the evoked broadband\ndomain and in the highg frequency range (.70 Hz). In each\nelectrode, we assessed the variation of brain responses with our\nexperimental conditions using a temporal cluster permutation\ntest. Electrodes with at least one significant cluster, after FDR\ncorrection at the electrode level, when comparing (1) normal\nand (2) jabberwocky sentences, for either broadband or high\ng\nsignals, were considered. We then manually selected represen-\ntative electrodes from this pool. We do not claim these results\nto be exhaustive; rather, we find it helpful to hold in mind these\nillustrative neural signals when examining the subsequent anal-\nyses. Unless specified otherwise, all subsequent statistical tests\nin this section are two-sample Wilcoxon– Mann– Whitney tests.\nFirst, several visual electrodes exhibited a fast phasic broad-\nband response, triggered indifferently by all visual stimuli, but\nmodulated by stimulus length. As previously reported in\nother datasets (\nAgrawal et al., 2020 ; King et al., 2020 ;\nWoolnough et al., 2020 ), short function words triggered a\ns m a l l e rr e s p o n s et h a nl o n g e rc o n t e n tw o r d s(p ,0.01 based\non the average activity between 50 and 300 ms following\neach stimulus presentation, electrode E6142, Extended\nData Fig. 3-1A). This channel is the only exception to the\nselection rule stated above: its activity did not differentiate\nbetween normal and jabberwocky sentences, neither in\nbroadband nor high g power.\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5355\nSecond, in the FuG, the evoked responses of electrode E1042\n(Fig. 3A) was similar between normal and jabberwocky, but sig-\nnificantly different from string lists (p ,0.0001), consistent with\nthis region’s sensitivity to written words and word-like stimuli\n(Woolnough et al., 2020). Interestingly, highg power from the\nsame electrode exhibited phasic responses that differed for nor-\nmal and jabberwocky (p ,0.01, Fig. 3A, right), compatible with\na role in lexical access (Woolnough et al., 2020).\nThird, in regions such as the inferior parietal lobule (IPL,\nFig. 3B), high g responses to normal sentences showed a clear\nphasic effect. For example, in electrode E0064, it peaks 300 ms\nafter each content word. In this electrode, responses to jabber-\nwocky were very similar for the first pseudoword but then\nquickly dropped.\nWhile most electrodes exhibited stronger responses to nor-\nmal sentences than jabberwocky sentences, some responded\nspecifically to jabberwocky (e.g., superior temporal electrode\nE1263, Extended Data\nFig. 3-1B). Such differences are compati-\nble with the dual-route model of reading ( Marshall and\nNewcombe, 1973; Jobard et al., 2003; Coltheart, 2005), while\nwords evoke additional lexical, syntactic, semantic and, ulti-\nmately, compositional processes, pseudowords may also elicit\nspecific processes associated for instance with attention and\ngrapheme-phoneme conversion (\nRumsey et al., 1997; Binder et\nal., 2003; Taylor et al., 2013).\nFourth, in IFG (Fig. 3C) and medial frontal gyrus (MFG,\nExtended Data Fig. 3-1C), we observed ramping responses:\neach additional content word led to an increase of the\nbroadband and high g responses (linear regression on the\ndifference in HG activity between normal and jabberwocky\nfor electrode E0878 (IFG) from 0 s to 3.5 s: slope = 0.086,\nr =0 . 7 8 ,p ,0.0001; and for electrodes E3652 (MFG) on the\nbroadband: slope = 4.99e-6,r = 0.64,p ,0.0001). This is com-\npatible with a role in linguistic integration, in line with previous\nstudies (\nFedorenko et al., 2016; Nelson et al., 2017).\nLast, in left superior frontal gyrus (SFG, Fig. 3D)a n d\nright orbitofrontal cortex (OFC, Extended Data Fig. 3-1D),\nwe observed sentence-final effects: for example, electrode\nE6062 (OFC), was mostly silent during the sentence, started\nto increase toward its ending, and exhibited a sharp peak\n.1sa f t e rt h el a s tw o r d’s onset (Extended Data\nFig. 3-1D).\nOn the other hand, electrode E0404 (SFG) responded simi-\nlarly to both normal and jabberwocky words, but these con-\nditions ultimately diverged after the last word (\nFig. 3D).\nFigure 4. ID is higher for normal sentences than jabberwocky.A, ID computed from the broadband sEEG signals from all subjects as a function of the time window used (sliding time win-\ndow of 0.4 s width). Right, Bar plot represents the ID computed using the whole sentence (a full 4 s time window).B, C, Same analysis applied to MEG signals and transformer activations\n(last layer, averaged over all stimuli for each condition). For the transformer, bar plot represents the ID computed using the 8 word time window. For ID computed from high-g sEEG, LSTM,\nand CamemBERT, see Extended DataFigure 4-1. For the ID from untrained LSTM and transformer models, see Extended DataFigure 4-2.\n5356  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping\nOverall, the broad spectrum of functional responses illustrates\nthe difficulty of interpreting the neural bases of language.\nInterestingly, a similar diversity of responses can be observed in\nindividual units of deep language models (Extended DataFig. 3-\n2). In the following sections, we use multivariate dimension\nreduction and decoding tools to evaluate whether our theoretical\nframework can account for the latent structure underlying these\ncomplex neural signals.\nEvaluating the ID hypothesis\nAs detailed in the Introduction, our framework predicts that\nstring lists, jabberwocky sentences, and normal sentences should\nlead to neural representations that systematically increase in their\nID. To compute the ID, we follow previous studies in neuro-\nscience outside of the language domain (Gao et al., 2017; Sorscher\net al., 2021; Elmoznino and Bonner, 2022)\na n du s eam e t h o db a s e do nP C A( s e e\nMaterials and Methods).\nW ep e r f o r m e dt h i sa n a l y s i so n4 0 0 m s\ntime windows from/C0 0.4 to 5.2 s.Figure 4\nshows the dimensionality estimate as a\nfunction of window onset. Three findings\nfit with our predictions. First, in all condi-\ntions for sEEG and for the normal text\ncondition in MEG, the estimate increased\nwith window onset, thus showing that as\nthe successive words unfolded, the dimen-\nsionality of the brain signals increased\n(Pearson correlation with time, normal:\nR = 0.98, jabber:R = 0.94; stringlist:R =\n0.89, p ,0.0001 for each for broadband\nsEEG,\nFig. 4A;n o r m a l :R =0 . 9 1 ,p ,\n0.0001, jabber:R =0 . 5 0 ,p .0.05, stringlist:\nR =0 . 4 7 ,p .0.05 for MEG,Fig. 4B,F D R -\ncorrected). Second, the ID was overall\nlarger for normal sentences than for jab-\nberwocky sentences and string lists in the\nsEEG signals (p ,0.001 for normal vs jab-\nber, p ,0.001 for normal vs string lists,\nWilcoxon– Mann– Whitney using the 4 s,\nFDR-corrected,\nFig. 4A, right). This was\nalso the case for the MEG (p ,0.001 for\nnormal vs jabber,p ,0.001 for normal vs\nstring lists, FDR-corrected,Fig. 4B,r i g h t ) .\nThird, the difference increased as the\nsentence unfolded, as determined by a sig-\nnificant Pearson correlation between the\ndifference (normal– jabber) and time (R =\n0.96, p ,0.0001 for sEEG;R = 0.92,p ,\n0.0001 for MEG). We replicated these re-\nsults with sEEG high\ng power and found\nhighly similar results (Extended Data\nFig. 4-1A).\nComparable effects were also observed\nin NLMs, such as causal transformers (Fig.\n4C; p ,0.001 for both pairwise compari-\nsons;p ,0.001 for all correlations), LSTMs\n(Extended DataFig. 4-1B; p ,0.001 for\nboth pairwise comparisons;p ,0.001 for\nall correlation), and CamemBERT (Exten-\nded DataFig. 4-1C; p ,0.05 for normal vs\njabber,p ,0.001 for normal vs string list;\np ,0.0001 for all correlations). Crucially,\nuntrained NLMs did not exhibit any signifi-\ncant differences between normal and jabberwocky (Extended Data\nFig. 4-2). Thus, in models, language learning is associated with a\nreshaping of the representational manifold, with an attribution of\nmeaning to specific dimensions.\nNLMs exhibit phasic, ramping, and sentence-final responses\nWe next tested our second prediction (i.e., the existence of dis-\ntinct phasic, ramping, and sentence-final responses to sentences).\nTo put these predictions to a test, we first evaluated whether\nthese putative stages of semantic composition could be identified\nin NLMs using multivariate decoding and generalization. For\nthis, we extracted the activations of each model in response to\nour stimuli and trained, at each word relative to sentence onset, a\nlogistic regression across its artificial neurons to classify normal\nFigure 5. Decoding normal versus jabberwocky sentences in NLMs shows lexical, persistent, and ramping patterns. Left,\nTemporal generalization matrices for a decoder trained to distinguish normal sentences from jabberwocky using the activity of\nthe input word layer (A) and the final 12th layer (B) in a transformer language model. The AUC is the average over the 10\nmodels trained on the same corpus but instantiated with different random seeds. In non-contextualized word embeddings, we\nonly see the lexical pattern, whereas contextualized layers exhibit a superposition of multiple theoretical patterns. Although\nthe performance on the diagonal is at ceiling (AUC = 1), the generalization pattern is consistent with the ramping model.\nRight, Generalization of individual decoders (i.e., horizontal slices from the temporal generalization matrices on the left). These\nslices from each matrix show in more details the temporal dynamics of sentence processing. Filled lines indicate significant\ntime points, tested with Wilcoxon– Mann– Whitney test against chance (0.5) and FDR correction. The first time point corre-\nsponds to the onset of the visual mask preceding the sentence. Vertical gray lines indicate the onsets of successive words.\nSmall colored diamonds represent the time where the decoders shown were trained. For decoding normal versus jabberwocky\nsentences in LSTMs and CamemBERT, see Extended Data\nFigure 5-1. For the ramping tendency in LSTMs and transformer\nNLMs, see Extended DataFigure 5-2.\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5357\nversus jabberwocky sentences. We then evaluated these logistic\nregressions at each other time point, including the representa-\ntions after the end of the sentence (obtained by feeding the\nmodel with four additional“space” tokens and extracting the\ncorresponding activations). This temporal generalization analysis\n(King and Dehaene, 2014)r e s u l t e di na1 2/C2 12 training/C2 test-\ning matrix of classification scores summarizing (1) where and\nwhen information distinguishing normal and jabberwocky sen-\ntences is linearly represented in the network, and (2) whether the\nunderlying representations change as the sentence unfolds.\nThe first computational step in NLMs is an“embedding”\nlayer, where each vocabulary item (i.e., each character) is mapped\nonto a unique vector. Consequently, we expected temporal gen-\neralization to reveal a lexical signature in this embedding layer\n(i.e., a transient and phasic score rising after content words) (Fig.\n2D). Our analysis confirmed this prediction: decoding led to a\nrelatively small above-chance decoding performance at each con-\ntent word (Fig. 5A, mean AUC over all content words: 0.79,\nWilcoxon– Mann– Whitney test against chance:p ,0.0001). As\nexpected, this signature disappeared for function words, as they\nwere identical for normal and jabberwocky sentences.\nThe deep layers of NLMs integrate information from multiple\ntokens. Because of this integration of the preceding context, we\nexpected temporal generalization to reveal an increasingly strong\n“square” of decoding performance. Indeed, we observed a tem-\nporal generalization not just between content words but also for\nfunction words as well as after the sentence. This was true for\ncausal transformers (Fig. 5B), LSTMs (Extended DataFig. 5-1A),\nand CamemBERT (Extended DataFig. 5-1B). We show results\nfor the last layer, where we found the overall decoding perform-\nance to be the strongest; nevertheless, the earlier layers exhibited\nsimilar dynamics (Extended DataFig. 5-2). Because the diagonal\nperformance was at ceiling (AUC = 1) after the first content\nword, we could not directly test whether temporal generalization\nsignificantly increases over the course of the sentence, as pre-\ndicted by a ramping processing stage (Fig. 2D). However, the\nFigure 6. Decoding normal from jabberwocky in human sEEG and MEG shows phasic, ramping, and sentence-final patterns. Left, Temporal generalization matricesfor a decoder trained to\ndistinguish normal sentences from jabberwocky using in human sEEG (A)a n dM E G(B). The AUC is the average over the 11 subjects for sEEG and 9 subjects for the MEG. Right, Generalization\nof individual decoders (i.e., horizontal slices from the temporal generalization matrices on the left). These slices from each matrix show in more details the temporal dynamics of sentence proc-\nessing. Filled lines indicate significant time points, tested with cluster permutation test and FDR correction. The first time point is the onset of the visual mask preceding the sentence. Each ver-\ntical gray line indicates a word onset. Small colored diamonds represent the time where the decoders are trained. For the diagonal decoding performance for normal versus jabberwocky\nsentences in human sEEG and MEG, see Extended DataFigure 6-1.\n5358  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping\ngeneralization performance of individual decoders (i.e., the aver-\nage of each line from the matrix) increased over the course of the\nsentence, as revealed by a linear regression on the average of\neach line from word 1 to word 8 for layer 6 (slope = 0.017,\nr =0 . 7 8 ,p ,0.0001). This finding suggests that NLMs demon-\nstrate a ramping activity pattern that varies with semantic\ncomposition. Furthermore, this tendency to ramping in-\ncreased in the upper layers of LSTM and causal trans-\nformer models (Extended Data\nFig. 5-2 ), suggesting that\nhigher-level linguistic information is characterized by a\nstronger ramping signature.\nFinally, looking at activations after the end of the sentence\n(i.e., when NLMs are input with spaces,Fig. 5B, top right), we\nobserved a strong square generalization pattern (mean AUC for\nmodels trained and tested on tokens 9-12: 0.97; Wilcoxon–\nMann– Whitney test against chance:p ,0.0001) that generalizes\nonly modestly to the preceding words (mean AUC of classifiers\ntrained on tokens 9-12, generalized to all preceding words: 0.59;\nWilcoxon– Mann– Whitney test against chance: p ,0.0001).\nConsistent with the predictions of a wrap-up processing stage,\nthis result suggests that sentence-final representations partially\ndiffer from those generated during online sentence processing.\nOverall, these analyses confirm that temporal generalization\ncan isolate the three putative processing stages of semantic com-\nposition in NLMs: a phasic effect at the earliest processing stage\nof the network, and ramping and end-of-sentence effects at\nhigher processing levels. In the next section, we apply these anal-\nyses on the sEEG and MEG responses to the same sentences, to\ntest whether and where these processing stages occur in the\nhuman brain.\nPhasic, ramping, and sentence-final patterns in time-\nresolved multivariate decoding\nApplying multivariate decoding and temporal generalization to\nhuman sEEG (\nFig. 6A) and MEG recordings (Fig. 6B) yielded a\nsuperposition of patterns. First, the sEEG and MEG diagonal\ndecoding performance reached significance at ;250 ms after\nword onset (Extended DataFig. 6-1; cluster permutation test on\nsEEG: significant cluster from 0.69 to 4.10 s,p = 0.002, and MEG:\nfirst significant cluster from 0.63 to 1.10 s,p = 0.016), consistent\nwith studies comparing ERPs evoked by words and pseudowords\n(Poldrack et al., 1999 ; Woolnough et al., 2020 ). Decoding\nreached a peak at;500 ms after stimulus onset, reaching up to\n0.65 AUC 6 0.03 (SEM) for sEEG (Wilcoxon– Mann– Whitney\ntest against chance:p ,0.01) and 0.58 AUC6 0.03 for MEG\n(Wilcoxon– Mann– Whitney test against chance:p ,0.01). The\nmultivariate pattern that separated normal and jabberwocky was\nsimilar across the four positions where content words appear in\nthe sentence, resulting in a 4/C2 4 grid of decoding generalization,\nsimilar to the one observed in NLMs (\nFig. 5). The resulting grid\npattern means that, although the neural activity evolved over the\ncourse of the sentence, after each content word, it transiently\nreached a similar state that dissociated normal and jabberwocky\nsentences. This phasic pattern is consistent with a lexical process\n(\nFig. 2D).\nSecond, to examine the presence of ramping effects, we\ntested whether decoding and generalization performance\n(average performance of a decoder over all time points)\nincreased with sentence unfolding (from 0.4 and 4 s). We\nfound a positive effect in sEEG diagonal performance (lin-\near regression on the average AUC across subjects: slope =\n0.013, r = 0.39,p ,0.0001), and the generalization performance\n(slope = 0.0068,r = 0.61, p ,0.0001). In MEG, there was a\nsignificant effect for the diagonal performance (slope = 0.0053,\nr = 0.25,p ,0.0001), but no effect for the generalization per-\nformance (slope = 0.000072,r = 0.014,p = 0.81).\nFinally, decoding performance remained significant for more\nthan 1 s after the end of the sentence (cluster permutation test on\nsEEG: significant cluster from 0.69 to 4.10 s,p ,0.01, the last\nword’s onset being at 2.8 s, Extended Data\nFig. 6-1). For example,\nthe purple line inFigure 6A corresponds to a sEEG classifier\ntrained 1 s after the last word’s onset. Despite being trained this\nlate, it reached an AUC of 0.596 0.03 and generalized to a few\nseconds before, with a clear ramping pattern (cluster permuta-\ntion test: 2 significant clusters from 1.9 to 2.8 s,p =0 . 0 1 4 , a n d\nfrom 3 to 4.3 s,p ,0.01). Similarly, the MEG classifier trained\n2 s after the onset of the last word (t =4 . 9s ,Fig. 6B, brown line),\nhence much later than sensory and lexical processes, still showed\na high decoding performance (training time AUC = 0.566 0.03,\ncluster permutation test: one significant cluster from 4.3 to 5 s,\np ,0.01), now quite restricted over time and hence supporting\nthe sentence-final wrap-up hypothesis (Fig. 2D).\nTo summarize, at the whole-brain level, we observed a linear\nsuperposition of the three patterns (Fig. 2D) hypothesized to par-\nticipate in semantic composition. Together, these results suggest\nthat the brain integrates semantic information across multiple\nwords and, after the end of the sentence, reaches a state that still\ndifferentiates normal and jabberwocky sentences for a long pe-\nriod of time.\nSuperposition and regional specialization of phasic,\nramping, and sentence-final effects\nTo quantify the extent to which each of the three dynamic pat-\nterns was present in the empirical generalization matrices, we fit\na linear regression using the (linearized) template matrices as\npredictors:\n^y ¼ bphasic /C2 Mphasic 1bramping /C2 Mramping 1bsentence/C0 final\n/C2 Msentence/C0 final\nIn this equation,^y is the predicted AUC matrix (average of all\nsubjects), Mphasic, Mramping,a n dMsentence/C0 final are the template\nmatrices shown inFigure 2D, and the betas are the correspond-\ning estimated coefficients. To account for varying time delays\nand intrinsic dynamics, we performed a systematic grid search\nover template matrices, individually varying the onset and width\nof each peak (Extended DataFig. 7-1). The model with the high-\nest likelihood was selected. We thus obtained ab coefficient for\neach template, quantifying the degree to which the dynamic pat-\ntern was present in the empirical matrix. This analysis is coarse:\nbecause we average patients’ data before fitting the regression,\nonly consistent patterns across patients will show up. The reason\nfor this averaging is the small number of patients (11) and the\nfact that each patient does not have electrodes in every region,\nthus making the total number of data points too small for fitting\na regression per patient followed by statistical testing across\npatients. Instead, we assess statistical significance with permuta-\ntion tests (by shuffling the classifier’s labels at the time of training\nthe decoder).\nApplying this method to the whole-brain temporal general-\nization matrix (Fig. 6B), we obtained significant coefficients\n(i.e., bigger than the coefficients fit on the AUC matrices from\nshuffled labels) for the three patterns:bphasic ¼ 0:11ðp ,0:01),\nbramping ¼ 0:06ðp ,0:01Þ; bsentence/C0 final ¼ 0:12ðp ,0:01Þ,\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5359\nconfirming the findings of the previous section. The corre-\nsponding template matrices are shown inFigure 7A.\nTo evaluate whether the three dynamics revealed by temporal\ngeneralization arise from distinct brain regions, we repeated this\nregression analysis on subsets of electrodes belonging to anatom-\nically defined ROIs, and predicted to be involved in distinct lan-\nguage-specific processes (Hickok and Poeppel, 2007; Friederici,\n2011; Hagoort, 2019; Matchin and Hickok, 2020). We then plot-\nted these results for each ROI by assigning a red, green, and blue\nvalue corresponding, respectively, to theb (normalized across\nregions to be between 0 and 1) of the phasic, ramping, and sen-\ntence-final patterns. Example empirical matrices are shown in\nFigure 7C and the resulting whole-brain maps inFigure 7D, E.\nFirst, we expected the ventral occipito-temporal visual\npathway, and particularly the FuG, to exhibit phasic (lexical)\neffects, with no ramping or delayed patterns. Our results are\nbroadly consistent with this prediction as shown by the strong\npresence of the lexical component (bphasic ¼ 0:84; p ,0:01),\nbut not ramping (bramping ¼ 0:14; p ¼ 0:12) nor sentence-final\n(bsentence/C0 final ¼ 0:07; p ¼ 0:09) components in the FuG (Fig.\n7D,E). To verify that the ramping pattern was not present in\nthese regions, we checked that the performance increased only\nmarginally and non-significantly over the course of the sentence\n(linear regression AUC for each subject: average slope = 0.0018,\nWilcoxon– Mann– Whitney test against null slope:p =0 . 4 3 f o r\nthe left precentral gyrus).\nFigure 7. Phasic, ramping, and sentence-final patterns are found in varying degrees in each region in human sEEG.A, Template matrices selected by the grid search for whole-brain human\nsEEG. For each template, 20 different template matrices with varying delays and widths (Extended DataFig. 7-1) were tested against the data, and the best fit was kept.B, Empirical matrix\nfor whole-brain human sEEG.C, Empirical matrices in human sEEG for three most relevant ROIs. Arrows indicate the corresponding brain regions.D, Surface brains maps represent the strength\nof the temporal generalization patterns for phasic (red, left), ramping (blue, middle), and sentence-final (green, right) processes in each ROI. Brain maps represent the corresponding red, blue,\nor green value, with a transparency value proportional to the regression coefficient of the corresponding pattern.E, Surface brain map represents the combined strength of the three patterns\nin each ROI. The color of each ROI reflects the red, blue, and green values of the phasic, ramping, and sentence-final patterns, respectively. For illustrative templates used in the grid search for\nthe template regression analysis, see Extended DataFigure 7-1. For the lack of syntactic modulation of the ramping pattern, see Extended DataFigure 7-2.\n5360  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping\nSecond, we expected the MTG and STG to be responsible for\naccessing lexical representations (Hart et al., 2000; Binder et al.,\n2003; Tranel, 2009) and starting to compose them according to\nsentential context (Lau et al., 2008; Pallier et al., 2011; Price et\nal., 2015, 2016), thus showing a combination of the phasic and\nramping patterns. Our results are consistent with this predic-\ntion (left STG:bphasic ¼ 0:54; p ,0:01, bramping ¼ 0:53; p ,0:01,\nbsentence/C0 final ¼ 0:17; p ¼ 0:1; left MTG:bphasic ¼ 0:42; p ,0:01,\nbramping ¼ 0:66; p ,0:01, bsentence/C0 final ¼ 0:12; p ¼ 0:08): the\nnormal versus jabberwocky decoding results revealed a strong\nresponse starting 200 ms and peaking 400 ms after each content\nword onset (e.g., peak performance for the first word att =0 . 8 s :\nmean AUC = 0.566 0.02 Wilcoxon– Mann– Whitney test against\nchance:p ,0.01), with good generalization to all words in the sen-\nt e n c ea l o n gw i t ha ni n c r e a s eo fp e r f o r m a n c eo v e rt i m e( l i n e a r\nregression for each subject: average slope = 0.0039, Wilcoxon–\nMann– Whitney test against null slope: p ,0.01) and an\nincrease of the generalization performance (average\nslope = 0.0047, Wilcoxon– Mann– Whitney test against null\nslope: p ,0.01). Interestingly, parietal regions such as the IPL,\nshowed a similar pattern (Fig. 7C,l e f t ) :bphasic ¼ 1; p ,0:01,\nbramping ¼ 0:83; p ,0:01, bsentence /C0 final ¼ 0:18; p ¼ 0:07, con-\nfirming their involvement in word composition (Bemis and\nPylkkänen, 2013; P r i c ee ta l . ,2 0 1 5, 2016).\nThird, we expected the PFC to be more specifically involved in\ncombinatorial computations (Hagoort, 2005; Pallier et al., 2011;\nFriederici, 2011; Fedorenko et al., 2016; Nelson et al., 2017; Matchin\nand Hickok, 2020). In the left IFG ( bphasic ¼ 0:2; p ¼ 0:08,\nbramping ¼ 0:85; p ,0:01, bsentence/C0 final ¼ 0:69; p ,0:01) and in\nthe left MFG (Fig. 7C,m i d d l e ;bphasic ¼ 0:64; p ,0:01; bramping ¼\n1; p ,0:01, bsentence/C0 final ¼ 0:58; p ,0:01), we observed a ramp-\ning activity profile, starting at;350 ms after the first content word’s\nonset, and increasing without discontinuity until 4.2 s (i.e., 1.4 s af-\nter the last word onset; linear regression for each subject: average\nslope = 0.0065, Wilcoxon– Mann– Whitney test against null slope:\np , 0.001 for left IFG). The ramping pattern was also visible in the\ngeneralization performance (average slope = 0.0054, Wilcoxon–\nMann– Whitney test against null slope:p ,0.01). Similar ramping\np r o f i l e sw e r ef o u n di nt h er i g h tI F Ga n dM F G(\nFig. 7D,E), but the\nevidence for phasic and sentence-final patterns was weaker. This ac-\ntivity profile is consistent with a linear integrator (Pallier et al., 2011;\nFedorenko et al., 2016), whereby IFG/MFG would combine each\nincoming word with the previous ones.\nLast, in the right OFC (bphasic ¼ 0:26, p =0 . 1 5 ,bramping ¼\n0; p ¼ 0:96, bsentence/C0 final ¼ 0:71; p ,0:01) and the left SFG\n(Fig. 7 C, bphasic ¼ 0:09; p ¼ 0:47, bramping ¼ 0:34; p ¼ 0:05,\nbsentence /C0 final ¼ 1; p ,0:01), decoding performance stayed at\nchance level for the most part of the sentence, but significantly\nincreased after the last word, and stayed above chance until\n1.6 s after the end of the sentence (cluster permutation test on\nthe diagonal performance: single significant cluster from 3.9 to\n4.4 s, p ,0.01 for left SFG and single significant cluster from\n4.0 to 4.4 s,p ,0.01 for right OFC). This sentence-final effect is\nconsistent with a wrap-up process.\nIn sum, we successfully identified a set of regions exhibiting\nsignatures of phasic, ramping, and sentence-final processes, and\nthus provide a path to a systematic decomposition of sentence\ncomposition in the brain.\nDiscussion\nTo clarify how sentences are composed by the human brain,\nwe introduced and tested a simple yet powerful vector coding\nframework in NLMs and human electrophysiological recordings.\nFirst, we predicted that the dimensionality of brain signals\nshould increase as successive words get added to an evolving rep-\nresentation of sentential meaning, and that this effect should\nbe larger for meaningful than for meaningless materials\n(jabberwocky or lists of meaningless strings). Indeed, we\nfound that representations of meaningful sentences evoke\nneural signals of higher dimensionality than jabberwocky\nsentences or string lists. Noteworthy, there was an increase\nof dimensionality with time in all conditions; but crucially,\nit was highest for normal sentences. These effects were\nabsent in untrained NLMs, supporting the idea that, with\nlearning, coding dimensions are assigned distinctive mean-\ning in distributed semantic spaces.\nTo further characterize the dynamics of brain activity during\nsentence processing, we used multivariate decoding and tempo-\nral generalization, a method that has recently been advocated for\nin the context of language processing (\nFyshe, 2020; He et al.,\n2022). The results indicate that the representations generated in\nbrains and NLMs follow similar dynamics, despite their dif-\nferences in implementation and timescale. Specifically, we\nobserved three distinct dynamic signatures: phasic, ramp-\ning, and sentence-final, which we interpret as the reflection\nof single-word processing, multiword composition, and\nsentence wrap-up, respectively. ROI analysis showed that\nthe FuG exhibited pure lexical patterns, confirming its role\nin written word identification. On the other hand, regions,\nsuch as MTG, STG, and IPL, displayed mixed lexical and\nramping patterns, suggesting that access single-word mean-\nings and start to combine them. Frontal regions had mixed\nsignatures of ramping and sentence-final (for IFG), as well\nas a phasic component (for MFG), or a pure sentence-final\neffect (for right OFC and left SFG). Overall, we observe a\nwide variety of combinations of each signature, witnessing\nthe dynamic flow of information during sentence process-\ning. Although this pattern-based analysis is coarse and is\ncorrelational rather than causal, it is corroborated by sin-\ngle-channel evoked activities (e.g.,\nFig. 3) and fits with the\nprevious literature.\nLexical access, in particular, has been studied extensively\nand is thought to be supported by the FuG and temporal\nregions, where we found strong phasic signatures. Curiously,\nthis was also the case of parietal regions, such as IPL and the\nprecentral and postcentral gyri, suggesting a stronger\ninvolvement in lexical access than previously thought. The\nramping pattern also appeared as a marker of compositional\nprocesses in several previous studies.\nPallier et al. (2011)\nobserved that fMRI activity in IFG and posterior STS\nincreased in direct proportion to the number of elements in\nthe current syntactic phrase, for both normal text and jabber-\nwocky. They proposed a simple model in which each consec-\nutive word or phrase adds a fixed amount of activity to a\ncompositional representation, which therefore builds up\nacross time.\nNelson et al. (2017)and Fedorenko et al. (2016)\nthen showed, with the higher resolution of intracranial EEG,\nthat high-g activity does indeed increase after each word in a\nconstituent word phrase.\nThe exact computational role of this ramping activity is,\nhowever, still unknown. Because of the engagement of addi-\ntional semantic processes, the neural assemblies recruited\nduring the processing of normal sentences should be larger\nand the neural dynamics richer, compared with jabberwocky.\nComputational models of the neural encoding of compositional\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5361\nstructures (Smolensky, 1990; Plate, 1995; Gayler, 2004) indeed pre-\ndict that the neural code can be characterized as a sum of represen-\ntations for each constituent (technically, a sum of tensor products of\nthe vectors representing each word’s role and filler) and should\ntherefore increase as their number increases. Nevertheless, direct\nevidence for such a neural code is still missing. Testing whether\nbrain activity during sentence processing contains signatures of ten-\nsor product representations is a promising avenue for future work.\nThe sentence-final pattern seen in SFG and OFC may be asso-\nciated with several cognitive processes, collectively dubbed wrap-\nup processes. The classical view holds that sentence wrap-up\neffects reflect higher-level integration and, if necessary, reanalysis\nand conflict resolution of the multiple possible meanings of\nwords (\nJust and Carpenter, 1980; Molinaro et al., 2008). Until\nrecently, few neuroimaging studies examined sentence-final acti-\nvations, for fear that wrap-up effects would confound regular\nprocesses happening at the last word (Stowe et al., 2018). The\npresent study shows one way that they can be disentangled.\nHere, we also used varied syntactic structures (Fig. 2C)a sa n\ninitial attempt to look for modulations of the dynamic patterns\nby syntax. However, no such modulations were found (Extended\nData\nFig. 7-2), suggesting that the integrative processes are similar\nin the three structures used. This does not preclude a modulation\nin more complex structures, for example, sentences including ad-\nverbial phrases or embedded clauses.\nThe present ID hypothesis stems from much research in the\npast decades, which has emphasized how biological neural net-\nworks use high-dimensional vector spaces to encode complex\nstructures (Quiroga et al., 2005; Gorban et al., 2019; Tyukin et\nal., 2019; Calvo Tapia et al., 2020). For instance, human MEG\nsignals can be decomposed into several dimensions that reflect\nthe various contents of a visual sequence (Liu et al., 2019;\nQuentin et al., 2019; Al Roumi et al., 2021). Similarly, recordings\nof thousands of monkey prefrontal neurons can be decomposed\ninto orthogonal vector subspaces storing the successive ele-\nments of a spatial sequence in working memory (\nXie et al.,\n2022). The present ID analysis extended this idea to sentences.\nNonlinear alternative measures of dimensionality are available\n(Granata and Carnevale, 2016; Facco et al., 2017; Landa et al.,\n2021), but the advantages of the measure of ID used here are its\nsimplicity and wide acceptance.\nDistributed word representations from NLMs were shown to\nalign with cortical responses to words by means of linear encod-\ning models (Huth et al., 2016; Jain and Huth, 2018; Toneva and\nWehbe, 2019; Caucheteux and King, 2020; Caucheteux et al.,\n2021; Schrimpf et al., 2021; Goldstein et al., 2022). Such partial\nsimilarities in brain activity, in performance on various linguistic\ntasks (Otter et al., 2021) and in error types (Coenen et al., 2019;\nGoldberg, 2019; Jawahar et al., 2019; Lakretz et al., 2020, 2021b),\nprovide an interesting, although incomplete, means to study neu-\nral representations of sentences during language processing in a\nsystem that is (1) less noisy compared with brain data, (2) fully ac-\ncessible to manipulation and recordings, and (3) notap r i o r itied to\na normative linguistic theory but, rather, learned from large-scale\nlinguistic corpora. The present decoding approach shows that the\nsimilarities between brains and artificial neural networks lie not\nonly in their representations, but also in their dynamics (i.e., the\norder in which the representations are combined with one another).\nPerhaps even more surprisingly, the resemblance holds for nonca-\nnonical stimuli, such as jabberwocky sentences, which are out-of-\ndomain for both brains and NLMs. Of note, the interpretation of\nthe sentence-final pattern in the models is not straightforward, and\nwe remain careful to interpret it as a wrap-up process.\nA limiting factor in our work is the limited coverage of some\nbrain regions (e.g., STS). Furthermore, we had to pool together\nelectrodes in relatively large brain regions to achieve decent\ndecoding performance. This choice greatly limited the spatial ac-\ncuracy of our analyses. For example, subregions of the IFG are\nknown to have functional specializations and interindividual var-\niability (\nFedorenko and Blank, 2020): for example, the pars trian-\ngularis (BA45) may be most sensitive to syntax (Nelson et al.,\n2017). Similarly, we had to aggregate the temporal pole, which\nhas been associated with 2-words composition ( Bemis and\nPylkkänen, 2011; Fyshe et al., 2019; Pylkkänen, 2019), with the\ntemporal gyri. Higher-resolution recordings, for instance using\nsmaller electrode arrays (\nS z o s t a ke ta l . ,2 0 1 7; Steinmetz et al., 2021),\nwill be needed to further study the functional specialization of these\nregions. Furthermore, our study considers semantic composition in\nthe broadest sense and does not afford any claim regarding the spe-\ncific subprocesses underlying the observed dynamics.\nTogether, our results suggest that a succession of processing\nstages, separated by their distinct brain signatures, underlie the\ncomposition of sentence-level semantics. They allow us to specu-\nlate that incoming lexical information arising from the FuG\n(\nWoolnough et al., 2020) is first passed to the temporal lobe and\nIPL, where semantic information is accessed, stored, and begins\nto be combined. The IFG/MFG exhibits relatively selective ramp-\ning and sentence-final signals, suggesting that it may play a key\nrole in merging individual words into constituents that encode\ntheir compositional meaning. The final sentential representation\nmay then stay present in the activity pattern of the left SFG and\nright OFC for several seconds, a duration that might have been\nextended if we had presented multiple sentences forming part of a\nlarger discourse. Finally, the construction of these compositional\nmeanings is associated with an increased dimensionality of the rep-\nresentations. These results bring us one step closer to understanding\nhow the human brain composes and understands sentences.\nReferences\nAgrawal A, Hari K, Arun SP (2020) A compositional neural code in high-\nlevel visual cortex can explain jumbled word reading. Elife 9:e54846.\nAl Roumi F, Marti S, Wang L, Amalric M, Dehaene S (2021) Mental com-\npression of spatial sequences in human working memory using numerical\nand geometrical primitives. Neuron 109:2627– 2639.e4.\nBadier JM, Dubarry AS, Gavaret M, Chen S, Trébuchon AS, Marquis P, Régis\nJ, Bartolomei F, Bénar CG, Carron R (2017) Technical solutions for si-\nmultaneous MEG and sEEG recordings: towards routine clinical use.\nPhysiol Meas 38:N118– N127.\nBancaud J, Angelergues R, Bernouilli C, Bonis A, Bordas-Ferrer M,\nBresson M, Buser P, Covello L, Morel P, Szikla G, Takeda A,\nTalairach J (1970) Functional stereotaxic exploration (sEEG) of epi-\nlepsy. Electroencephalogr Clin Neurophysiol 28:85 – 86.\nBastiaansen M, Magyari L, Hagoort P (2010) Syntactic unification operations\nare reflected in oscillatory dynamics during on-line sentence comprehen-\nsion. J Cogn Neurosci 22:1333– 1347.\nBemis DK, Pylkkänen L (2011) Simple composition: a magnetoencephalogra-\nphy investigation into the comprehension of minimal linguistic phrases. J\nNeurosci 31:2801– 2814.\nBemis DK, Pylkkänen L (2013) Basic linguistic composition recruits the left\nanterior temporal lobe and left angular gyrus during both listening and\nreading. Cereb Cortex 23:1859– 1873.\nBinder JR, McKiernan KA, Parsons ME, Westbury CF, Possing ET, Kaufman\nJN, Buchanan L (2003) Neural correlates of lexical access during visual\nword recognition. J Cogn Neurosci 15:372– 393.\nCalvo Tapia C, Tyukin I, Makarov VA (2020) Universal principles justify the\nexistence of concept cells. Sci Rep 10:7889.\nCampadelli P, Casiraghi E, Ceruti C, Rozza A (2015) Intrinsic dimension\nestimation: relevant techniques and a benchmark framework. Math Probl\nEng 2015:e759567.\n5362  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping\nCaramazza A (1997) How many levels of processing are there in lexical\naccess? Cogn Neuropsychol 14:177– 208.\nCarpenter PA, Just MA, Keller TA, Eddy WF, Thulborn KR (1999) Time\ncourse of fMRI-activation in language and spatial networks during sen-\ntence comprehension. Neuroimage 10:216– 224.\nCarreira-Perpinán MA (1997) A review of dimension reduction techniques.\nSheffield, England: CS– 96– 09.\nCaucheteux C, King JR (2020) Language processing in brains and deep neural\nnetworks: computational convergence and its limits. bioRxiv 186288.\nhttps://doi.org/10.1101/2020.07.03.186288 .\nCaucheteux C, Gramfort A, King JR (2021) GPT-2’s activations predict the\ndegree of semantic comprehension in the human brain. bioRxiv 440622.\nhttps://doi.org/10.1101/2021.04.20.440622.\nChurchland MM, Cunningham JP, Kaufman MT, Foster JD, Nuyujukian P,\nRyu SI, Shenoy KV (2012) Neural population dynamics during reaching.\nNature 487:51– 56.\nClark K, Khandelwal U, Levy O, Manning CD (2019) What does BERT look\nat? An analysis of BERT ’s attention. Proceedings of the 2019 ACL\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP,p p2 7 6– 286.\nCoenen A, Reif E, Yuan A, Kim B, Pearce A, Viégas F, Wattenberg M (2019)\nVisualizing and measuring the geometry of BERT. arXiv:1906.02715.\nColtheart M (2005) Modeling reading: the dual-route approach. In: The sci-\nence of reading: a handbook, pp 6– 23. New York: Wiley.\nDel Giudice M (2021) Effective dimensionality: a tutorial. Multivariate Behav\nRes 56:527– 542.\nD e s i k a nR S ,S é g o n n eF ,F i s c h lB ,Q u i n nB T ,D i c k e r s o nB C ,B l a c k e rD ,B u c k n e r\nRL, Dale AM, Maguire RP, Hyman BT, Albert MS, Killiany RJ (2006) An\nautomated labeling system for subdividing the human cerebral cortex on\nMRI scans into gyral based regions of interest. Neuroimage 31:968– 980.\nDevlin J, Chang MW, Lee K, Toutanova K (2019) BERT: pre-training of deep\nbidirectional transformers for language understanding. Proceedings of\nthe 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pp 4171 – 4186, Minneapolis, Minnesota:\nAssociation for Computational Linguistics.\nEbitz RB, Hayden BY (2021) The population doctrine in cognitive neuro-\nscience. Neuron 109:3055– 3068.\nElmoznino E, Bonner MF (2022) High-performing neural network models\nof visual cortex benefit from high latent dimensionality. bioRxiv 499969.\nhttps://doi.org/10.1101/2022.07.13.499969.\nFacco E, d’Errico M, Rodriguez A, Laio A (2017) Estimating the intrinsic dimen-\nsion of datasets by a minimal neighborhood information. Sci Rep 7:1.\nFedorenko E, Blank IA (2020) Broca’s area is not a natural kind. Trends\nCogn Sci 24:270– 284.\nFedorenko E, Scott TL, Brunner P, Coon WG, Pritchett B, Schalk G,\nKanwisher N (2016) Neural correlate of the construction of sentence\nmeaning. Proc Natl Acad Sci USA 113:E6256– E6262.\nFriederici AD (2011) The brain basis of language processing: from structure\nto function. Physiol Rev 91:1357– 1392.\nFyshe A (2020) Studying language in context using the temporal generaliza-\ntion method. Philos Trans R Soc Lond B Biol Sci 375:20180531.\nFyshe A, Sudre G, Wehbe L, Rafidi N, Mitchell TM (2019) The lexical seman-\ntics of adjective– noun phrases in the human brain. Hum Brain Mapp\n40:4457– 4469.\nGallego JA, Perich MG, Miller LE, Solla SA (2017) Neural manifolds for the\ncontrol of movement. Neuron 94:978– 984.\nGao P, Trautmann E, Yu B, Santhanam G, Ryu S, Shenoy K, Ganguli S\n(2017) A theory of multineuronal dimensionality, dynamics and mea-\nsurement. BioRxiv 214262.\nhttps://doi.org/10.1101/214262.\nGayler RW (2004) Vector symbolic architectures answer Jackendoff’sc h a l -\nlenges for cognitive neuroscience. arXiv:cs/0412059.\nGeorgopoulos AP, Schwartz AB, Kettner RE (1986) Neuronal population\ncoding of movement direction. Science 233:1416– 1419.\nGoldberg Y (2019) Assessing BERT’s syntactic abilities. arXiv:1901.05287.\nGoldstein A, et al. (2022) Shared computational principles for language proc-\nessing in humans and deep language models. Nat Neurosci 25:369– 380.\nGorban AN, Makarov VA, Tyukin IY (2019) The unreasonable effectiveness of\nsmall neural ensembles in high-dimensional brain. Phys Life Rev 29:55– 88.\nGramfort A, Luessi M, Larson E, Engemann DA, Strohmeier D, Brodbeck C,\nGoj R, Jas M, Brooks T, Parkkonen L, Hämäläinen M (2013) MEG and\nEEG data analysis with MNE-Python. Front Neurosci 7:267.\nGranata D, Carnevale V (2016) Accurate estimation of the intrinsic dimen-\nsion using graph distances: unraveling the geometric complexity of data-\nsets. Sci Rep 6:31377.\nHagoort P (2005) On Broca, brain, and binding: a new framework. Trends\nCogn Sci 9:416– 423.\nHagoort P (2019) The neurobiology of language beyond single-word process-\ning. Science 366:55– 58.\nHahne A, Jescheniak JD (2001) What’s left if the Jabberwock gets the semantics?\nAn ERP investigation into semantic and syntactic processes during auditory\nsentence comprehension. Brain Res Cogn Brain Res 11:199– 212.\nHale JT, Campanelli L, Li J, Bhattasali S, Pallier C, Brennan JR (2021) Neuro-\ncomputational models of language processing. Annu Rev Linguistics 8:\n427– 446.\nHart J, Kraut MA, Kremen S, Soher B, Gordon B (2000) Neural substrates of\northographic lexical access as demonstrated by functional brain imaging.\nNeuropsychiatry Neuropsychol Behav Neurol 13:1– 7.\nHe Y, Sommer J, Hansen-Schirra S, Nagels A (2022) Negation impacts sentence\nprocessing in the N400 and later time windows: evidence from multivariate\npattern analysis of EEG. PsyArXiv.\nhttps://doi.org/10.31234/osf.io/8rbw3.\nHeilbron M, Armeni K, Schoffelen JM, Hagoort P, de Lange FP (2022) A hi-\nerarchy of linguistic predictions during natural language comprehension.\nProc Natl Acad Sci USA 119:e2201968119.\nHickok G, Poeppel D (2007) The cortical organization of speech processing.\nNat Rev Neurosci 8:393– 402.\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neural\nComput 9:1735– 1780.\nHuth AG, Lee T, Nishimoto S, Bilenko NY, Vu AT, Gallant JL (2016)\nDecoding the semantic content of natural movies from human brain ac-\ntivity. Front Syst Neurosci 10:81.\nJain S, Huth A (2018) Incorporating context into language encoding models\nfor fMRI. Adv Neural Information Processing Systems 31:6629– 6638.\nJawahar G, Sagot B, Seddah D (2019) What does BERT learn about the struc-\nture of language? Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pp 3651– 3657.\nJobard G, Crivello F, Tzourio-Mazoyer N (2003) Evaluation of the dual route\ntheory of reading: a metanalysis of 35 neuroimaging studies. Neuroimage\n20:693– 712.\nJust MA, Carpenter PA (1980) A theory of reading: from eye fixations to\ncomprehension. Psychol Rev 87:329– 354.\nJust MA, Carpenter PA, Keller TA, Eddy WF, Thulborn KR (1996) Brain\nactivation modulated by sentence comprehension. Science 274:114– 116.\nKing JR, Dehaene S (2014) Characterizing the dynamics of mental representa-\ntions: the temporal generalization method. Trends Cogn Sci 18:203– 210.\nKing JR, Charton F, Lopez-Paz D, Oquab M (2020) Back-to-back regression:\ndisentangling the influence of correlated factors from multivariate obser-\nvations. Neuroimage 220:117028.\nKuperman V, Dambacher M, Nuthmann A, Kliegl R (2010) The effect of\nword position on eye-movements in sentence and paragraph reading. Q J\nExp Psychol (Hove) 63:1838– 1857.\nLaje R, Buonomano DV (2013) Robust timing and motor patterns by taming\nchaos in recurrent neural networks. Nat Neurosci 16:925– 933.\nLakretz Y, Kruszewski G, Desbordes T, Hupkes D, Dehaene S, Baroni M\n(2019) The emergence of number and syntax units in LSTM language\nmodels. Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Vol 1 (Long and Short Papers), pp 11– 20.\nLakretz Y, Hupkes D, Vergallito A, Marelli M, Baroni M, Dehaene S (2020)\nExploring processing of nested dependencies in neural-network language\nmodels and humans. arXiv:2006.11098.\nLakretz Y, Desbordes T, Hupkes D, Dehaene S (2021a) Causal transformers\nperform below chance on recursive nested constructions, unlike humans.\narXiv:2110.07240.\nLakretz Y, Desbordes T, King JR, Crabbé B, Oquab M, Dehaene S (2021b)\nCan RNNs learn recursive nested subject-verb agreements? arXiv:2101.02258.\nLanda B, Zhang TT, Kluger Y (2021) Biwhitening reveals the rank of a count\nmatrix. arXiv:2103.13840.\nLau EF, Phillips C, Poeppel D (2008) A cortical network for semantics: (de)\nconstructing the N400. Nat Rev Neurosci 9:920– 933.\nLewis RL, Vasishth S (2005) An activation-based model of sentence process-\ning as skilled memory retrieval. Cogn Sci 29:375– 419.\nLiu Y, Dolan RJ, Kurth-Nelson Z, Behrens TE (2019) Human replay sponta-\nneously reorganizes experience. Cell 178:640– 652\n.e14.\nDesbordes et al.· Dimensionality and Ramping J. Neurosci., July 19, 2023  43(29):5350– 5364  5363\nMachens CK, Romo R, Brody CD (2010) Functional, but not anatomical, separa-\ntion of‘what’and ‘when’in prefrontal cortex. J Neurosci 30:350– 360.\nMante V, Sussillo D, Shenoy KV, Newsome WT (2013) Context-dependent\ncomputation by recurrent dynamics in prefrontal cortex. Nature 503:78– 84.\nMarshall JC, Newcombe F (1973) Patterns of paralexia: a psycholinguistic\napproach. J Psycholinguist Res 2:175– 199.\nMartin L, Muller B, Suárez PJ, Dupont Y, Romary L, de la Clergerie ÉV,\nSeddah D, Sagot B (2020) CamemBERT: a Tasty French Language\nModel. Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp 7203– 7219.\nMatchin W, Hickok G (2020) The cortical organization of syntax. Cereb\nCortex 30:1481– 1498.\nMazoyer BM, Tzourio N, Frak V, Syrota A, Murayama N, Levrier O,\nSalamon G, Dehaene S, Cohen L, Mehler J (1993) The cortical represen-\ntation of speech. J Cogn Neurosci 5:467– 479.\nMcClelland JL, Hill F, Rudolph M, Baldridge J, Schütze H (2020) Placing lan-\nguage in an integrated understanding system: next steps toward human-\nlevel performance in neural language models. Proc Natl Acad Sci USA\n117:25966– 25974.\nMerity S, Xiong C, Bradbury J, Socher R (2016) Pointer sentinel mixture\nmodels. arXiv:1609.07843.\nMolinaro N, Vespignani F, Job R (2008) A deeper reanalysis of a superficial\nfeature: an ERP study on agreement violations. Brain Res 1228:161– 176.\nNelson MJ, Karoui IE, Giber K, Yang X, Cohen L, Koopman H, Cash SS,\nNaccache L, Hale JT, Pallier C, Dehaene S (2017) Neurophysiological dy-\nnamics of phrase-structure building during sentence processing. Proc\nNatl Acad Sci USA 114:E3669– E3678.\nNobre AC, Allison T, McCarthy G (1994) Word recognition in the human\ninferior temporal lobe. Nature 372:260– 263.\nOh BD, Clark C, Schuler W (2022) Comparison of structural parsers and neural\nlanguage models as surprisal estimators. Front Artif Intell 5:777963.\nOtter DW, Medina JR, Kalita JK (2021) A survey of the usages of deep learning\nfor natural language processing. IEEE Trans Neural Netw Learn Syst 32:604–\n624.\nPallier C, Devauchelle AD, Dehaene S (2011) Cortical representation of the con-\nstituent structure of sentences. Proc Natl Acad Sci USA 108:2522– 2527.\nPater J (2019) Generative linguistics and neural networks at 60: foundation,\nfriction, and fusion. Language 95:e41– e74.\nPattamadilok C, Dehaene S, Pallier C (2016) A role for left inferior frontal\nand posterior superior temporal cortex in extracting a syntactic tree from\na sentence. Cortex 75:44– 55.\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O,\nBlondel M, Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A,\nCournapeau D, Brucher M, Perrot M, Duchesnay É (2011) Scikit-learn:\nmachine learning in Python. J Mach Learn Res 12:2825– 2830.\nPlate TA (1995) Holographic reduced representations. IEEE Trans Neural\nNetw 6:623– 641.\nPoldrack RA, Wagner AD, Prull MW, Desmond JE, Glover GH, Gabrieli JD\n(1999) Functional specialization for semantic and phonological process-\ning in the left inferior prefrontal cortex. Neuroimage 10:15– 35.\nPrice AR, Bonner MF, Peelle JE, Grossman M (2015) Converging evidence\nfor the neuroanatomic basis of combinatorial semantics in the angular\ngyrus. J Neurosci 35:3276– 3284.\nPrice AR, Peelle JE, Bonner MF, Grossman M, Hamilton RH (2016) Causal\nevidence for a mechanism of semantic integration in the angular gyrus as\nrevealed by high-definition transcranial direct current stimulation. J\nNeurosci 36:3829– 3838.\nPylkkänen L (2019) The neural basis of combinatory syntax and semantics.\nScience 366:62– 66.\nPylkkänen L (2020) Neural basis of basic composition: what we have learned\nfrom the red– boat studies and their extensions. Philos Trans R Soc Lond\nB Biol Sci 375:20190299.\nQuentin R, King JR, Sallard E, Fishman N, Thompson R, Buch ER, Cohen\nLG (2019) Differential brain mechanisms of selection and maintenance\nof information during working memory. J Neurosci 39:3728– 3740.\nQuiroga RQ, Reddy L, Kreiman G, Koch C, Fried I (2005) Invariant visual repre-\nsentation by single neurons in the human brain. Nature 435:1102– 1107.\nRecanatesi S, Farrell M, Lajoie G, Deneve S, Rigotti M, Shea-Brown E (2021)\nPredictive learning as a network mechanism for extracting low-dimen-\nsional latent space representations. Nat Commun 12:1.\nRogers A, Kovaleva O, Rumshisky A (2020) A primer in bertology: what we\nknow about how bert works. Trans Assoc Computational Linguistics 8:\n842– 866.\nRumelhart DE, McClelland JL (1986) On learning the past tenses of English verbs.\nIn: Parallel distributed processing: Explorations in the microstructure of cogni-\ntion: Foundations. MIT.\nRumsey JM, Horwitz B, Donohue BC, Nace K, Maisog JM, Andreason P\n(1997) Phonological and orthographic components of word recognition :\na PET-rCBF study. Brain 120:739– 759.\nSchrimpf M, Blank IA, Tuckute G, Kauf C, Hosseini EA, Kanwisher N,\nTenenbaum JB, Fedorenko E (2021) The neural architecture of language:\nintegrative modeling converges on predictive processing. Proc Natl Acad\nSci USA 118:e2105646118.\nSeidenberg MS, McClelland JL (1989) A distributed, developmental model of\nword recognition and naming. Psychol Rev 96:523– 568.\nShain C, Blank IA, van Schijndel M, Schuler W, Fedorenko E (2020) fMRI\nreveals language-specific predictive coding during naturalistic sentence\ncomprehension. Neuropsychologia 138:107307.\nSmith SM, Nichols TE (2009) Threshold-free cluster enhancement: address-\ning problems of smoothing, threshold dependence and localisation in\ncluster inference. Neuroimage 44:83– 98.\nSmolensky P (1990) Tensor product variable binding and the representation\nof symbolic structures in connectionist systems. Artif Intell 46:159– 216.\nSorscher B, Ganguli S, Sompolinsky H (2021) The geometry of concept learn-\ning. BioRxiv 436284.\nhttps://doi.org/10.1101/2021.03.21.436284.\nSorscher B, Ganguli S, Sompolinsky H (2022) Neural representational geom-\netry underlies few-shot concept learning. Proc Natl Acad Sci USA 119:\ne2200800119.\nSteedman M (2001) The syntactic process. Cambridge, MA: Massachusetts\nInstitute of Technology.\nSteinmetz NA, et al. (2021) Neuropixels 2.0: a miniaturized high-density\nprobe for stable, long-term brain recordings. Science 372:eabf4588.\nStowe LA, Kaan E, Sabourin L, Taylor RC (2018) The sentence wrap-up\ndogma. Cognition 176:232– 247.\nSzostak KM, Grand L, Constandinou TG (2017) Neural interfaces for intra-\ncortical recording: requirements, fabrication methods, and characteris-\ntics. Front Neurosci 11:665.\nTalairach J, Bancaud J, Bonis A, Szikla G, Trottier S, Vignal JP, Chauvel P,\nMunari C, Chodkievicz JP (1992) Surgical therapy for frontal epilepsies.\nAdv Neurol 57:707– 732.\nTaylor JS, Rastle K, Davis MH (2013) Can cognitive models explain brain\nactivation during word and pseudoword reading? A meta-analysis of 36\nneuroimaging studies. Psychol Bull 139:766– 791.\nTenney I, Das D, Pavlick E (2019) BERT rediscovers the classical NLP pipe-\nline. Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pp 4593– 4601.\nToneva M, Wehbe L (2019) Interpreting and improving natural-language\nprocessing (in machines) with natural language-processing (in the brain).\nAdv Neural Information Process Syst 32:14928– 14938.\nTranel D (2009) The left temporal pole is important for retrieving words for\nunique concrete entities. Aphasiology 23:867– 884.\nTyukin I, Gorban AN, Calvo C, Makarova J, Makarov VA (2019) High-\ndimensional brain: a tool for encoding and rapid learning of memories\nby single neurons. Bull Math Biol 81:4856– 4888.\nVyas S, Golub MD, Sussillo D, Shenoy KV (2020) Computation through neu-\nral population dynamics. Annu Rev Neurosci 43:249– 275.\nWarren T, White SJ, Reichle ED (2009) Investigating the causes of wrap-up\neffects: evidence from eye movements and E – Z Reader. Cognition\n111:132– 137.\nWarstadt A, Bowman SR (2022) What artificial neural networks can tell us\nabout human language acquisition. arXiv 2208.07998.\nWoolnough O, Donos C, Rollo PS, Forseth KJ, Lakretz Y, Crone NE,\nFischer-Baum S, Dehaene S, Tandon N (2020) Spatiotemporal dynamics\nof orthographic and lexical processing in the ventral visual pathway. Nat\nHum Behav 5:389– 398.\nXie Y, Hu P, Li J, Chen J, Song W, Wang XJ, Yang T, Dehaene S, Tang S,\nMin B, Wang L (2022) Geometry of sequence working memory in maca-\nque prefrontal cortex. Science 375:632– 639.\nYuste R (2015) From the neuron doctrine to neural networks. Nat Rev\nNeurosci 16:487– 497.\n5364  J. Neurosci., July 19, 2023 43(29):5350– 5364 Desbordes et al. · Dimensionality and Ramping",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.7308759093284607
    },
    {
      "name": "Computer science",
      "score": 0.616352379322052
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5382570624351501
    },
    {
      "name": "Natural language processing",
      "score": 0.47971147298812866
    },
    {
      "name": "Representation (politics)",
      "score": 0.47730767726898193
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.46979817748069763
    },
    {
      "name": "Magnetoencephalography",
      "score": 0.4542267322540283
    },
    {
      "name": "Psychology",
      "score": 0.2976154685020447
    },
    {
      "name": "Electroencephalography",
      "score": 0.26027894020080566
    },
    {
      "name": "Neuroscience",
      "score": 0.16085276007652283
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210143253",
      "name": "Cognitive Neuroimaging Lab",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I21491767",
      "name": "Aix-Marseille Université",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I154526488",
      "name": "Inserm",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210087487",
      "name": "Assistance Publique Hôpitaux de Marseille",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I187986737",
      "name": "Collège de France",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2738703131",
      "name": "Commissariat à l'Énergie Atomique et aux Énergies Alternatives",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2746051580",
      "name": "Université Paris Sciences et Lettres",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I277688954",
      "name": "Université Paris-Saclay",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I29607241",
      "name": "École Normale Supérieure - PSL",
      "country": "FR"
    }
  ]
}