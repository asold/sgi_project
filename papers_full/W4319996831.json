{
  "title": "Assessment of chemistry knowledge in large language models that generate code",
  "url": "https://openalex.org/W4319996831",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105096877",
      "name": "Andrew D. White",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2298549838",
      "name": "Glen M. Hocky",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2888500310",
      "name": "Heta A. Gandhi",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3153332890",
      "name": "Mehrad Ansari",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2566604316",
      "name": "Sam Cox",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3041959245",
      "name": "Geemi P. Wellawatte",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A4311130878",
      "name": "Subarna Sasmal",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2167460620",
      "name": "Ziyue Yang",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3171243909",
      "name": "Kangxin Liu",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2133605956",
      "name": "Yuvraj Singh",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3211103814",
      "name": "Willmor J. Peña Ccoa",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2105096877",
      "name": "Andrew D. White",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2298549838",
      "name": "Glen M. Hocky",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2888500310",
      "name": "Heta A. Gandhi",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3153332890",
      "name": "Mehrad Ansari",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2566604316",
      "name": "Sam Cox",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3041959245",
      "name": "Geemi P. Wellawatte",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A4311130878",
      "name": "Subarna Sasmal",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2167460620",
      "name": "Ziyue Yang",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3171243909",
      "name": "Kangxin Liu",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2133605956",
      "name": "Yuvraj Singh",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3211103814",
      "name": "Willmor J. Peña Ccoa",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W4280597794",
    "https://openalex.org/W4281619372",
    "https://openalex.org/W4287117648",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4288804596",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4221159410",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4288805334",
    "https://openalex.org/W3035965352",
    "https://openalex.org/W1965092590",
    "https://openalex.org/W2949223833",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W4225000967",
    "https://openalex.org/W4240061226",
    "https://openalex.org/W3099878876"
  ],
  "abstract": "In this work, we investigate the question: do code-generating large language models know chemistry? Our results indicate, mostly yes.",
  "full_text": "Assessment of chemistry knowledge in large\nlanguage models that generate code†\nAndrew D. White, *ab Glen M. Hocky, *cd Heta A. Gandhi, a Mehrad Ansari, a\nSam Cox, a Geemi P. Wellawatte, e Subarna Sasmal, c Ziyue Yang, a\nKangxin Liu, c Yuvraj Singh c and Willmor J. Peña Ccoa c\nIn this work, we investigate the question: do code-generating large language models know chemistry? Our\nresults indicate, mostly yes. To evaluate this, we introduce an expandable framework for evaluating\nchemistry knowledge in these models, through prompting models to solve chemistry problems posed as\ncoding tasks. To do so, we produce a benchmark set of problems, and evaluate these models based on\ncorrectness of code by automated testing and evaluation by experts. Weﬁnd that recent LLMs are able\nto write correct code across a variety of topics in chemistry and their accuracy can be increased by 30\npercentage points via prompt engineering strategies, like putting copyright notices at the top ofﬁles.\nOur dataset and evaluation tools are open source which can be contributed to or built upon by future\nresearchers, and will serve as a community resource for evaluating the performance of new models as\nthey emerge. We also describe some good practices for employing LLMs in chemistry. The general\nsuccess of these models demonstrates that their impact on chemistry teaching and research is poised to\nbe enormous.\nI. Introduction\nLarge language models (LLMs) are multi-billion parameter\ntransformer neural networks1 that are trained on enormous\ncollections of documents (a ‘corpus’) without supervision or\nlabels.2 LLMs can perform multiple tasks like classifying natural\nlanguage, translating text, and document search. Perhaps the\nmost remarkable task of LLMs is to complete an input string of\ntext; via this mechanism (called causal language modeling),\nLLMs can write unit tests, document function, write code from\na doc string, answer questions, and complete stoichiometric\nequations.\n3,4\nWe previously discussed the outlook of LLMs in chemistry.5\nIn the few months since then, LLMs have been both developed\nfor specic chemistry problems\n6,7 and general LLMs have been\napplied in chemistry.8,9 On Nov 30, 2022, OpenAI released an\ninteractive interface to an LLM termed ChatGPT (ref. 10) which\nsubstantially increased interest in this area as well as use by\nscientists for coding and writing tasks. An open question for\nLLMs such as GPT-3,\n3 T5,11 or GPT-neo (ref. 12) that are trained\non very large and varied textual data is if they can be applied in\ndomains like chemistry, which have specialized language and\nknowledge. In our initial work,\n5 we found that relationships\nbetween SMILES and natural language is possible with GPT-3.\nSMILES is the standard method of representing molecules as\nstrings.\n13 It is even possible to loosely edit structuresvia natural\nlanguage (see Fig. 6).14,15 However, the extent to which LLMs can\nbe directly applied in chemistry in the broad context of research\nand teaching is unexplored. The large amount of speci c\ndomain knowledge required to solve chemistry problems may\nlimit applicability of general LLMs. For example, recent work\nhas found that knowledge of the periodic table of elements\nrequires very high parameter counts.\n4\nRecent comparisons of LLMs that generate code can be\nfound in ref. 16. Here, we focus our study on whether LLMs that\ngenerate code\n17 can be applied to chemistry tasks of a compu-\ntational nature (both computational chemistry problems, and\ngeneral tasks which can be expressed as simple computer\nprograms, such as ranking elements by ionic radius). Most\nLLMs that generate computer code are causal decoder-only\nmodels\n17–19— a user provides a sequence of text (called the\nprompt) and it proposes a continuation of the text (the\ncompletion).\n20 There are LLMs trained on code that can inll or\nmatch encoder/decoder natural language to code like Code-\naDepartment of Chemical Engineering, University of Rochester, USA. E-mail: andrew.\nwhite@rochester.edu\nbVial Health Technology, Inc., USA\ncDepartment of Chemistry, New York University, USA. E-mail: hockyg@nyu.edu\ndSimons Center for Computational Physical Chemistry, New York University, USA\neDepartment of Chemistry, University of Rochester, USA\n† Electronic supplementary information (ESI) available: Supporting gures,\ntables, and text. Accuracy data are available as comma separated value les.\nContexts are available as a markup le. The responses from the model\n(completions) which were the basis for expert evaluators are available in HTML\nformat at https://doi.org/10.5281/zenodo.6800475. See DOI:\nhttps://doi.org/10.1039/d2dd00087c\nCite this:Digital Discovery,2 0 2 3 ,2,\n368\nReceived 17th August 2022\nAccepted 19th January 2023\nDOI: 10.1039/d2dd00087c\nrsc.li/digitaldiscovery\n368 | Digital Discovery,2 0 2 3 ,2,3 6 8–376 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nBERT,21 but they are typically used for embedding code for tasks\nlike classication, document retrieval, or translating code to\nnatural language. Because it is not reasonable to use encoder–\ndecoder or encoder-only models to generate code or answer\nquestions with open-ended length, this paper explores solely\ndecoder-only causal language models.\nEvaluating LLMs' knowledge of chemistry should be distin-\nguished from capability to reason or understand. LLMs can\nmake compelling completions, but are incapable of reasoning\nand demonstrate supercial understanding.\n22,23 Our goal is to\nevaluate LLMs' ability to correlate natural language, equations,\ncode, and heuristics of chemistry.\nII. Methods\nWe have compiled a categorized set of chemistry and related\nexample prompts for benchmarking code-generating LLMs in\na public repository.\n24 To generate these problems, we rst\ndecided upon a list of categories of chemistry and chemical\nengineering knowledge, listed in Table 1, and set a goal of\nhaving at least 10 examples in each category for our initial\ndatabase of problems. Members of our research groups (the\nauthors of this paper), who we consider to have su ﬃcient\nexpertise in these areas due to formal schooling, research, and\nteaching experience, contributed the prompts and reference\nsolutions for these categories.\nThe examples in this table span a range of topics that we\nconsider common questions across chemistryelds. There is\nsome representation of computational chemistry research topics\n(categories corresponding to performing chemical simulations\n(sim), analyzing molecular dynamics simulations (md), chemical\ninformatics (cheminf), and some quantum mechanics (qm)), but\nthis constitutes less than half of the initial prompts created by us.\nThe rest correspond to typical questions that one might\nencounter in general chemistry (genchem), biochemistry (bio),\nphysical chemistry (thermodynamics, quantum mechanics, and\nspectroscopy), and in laboratory classes (plotting and statistics).\nWithin this set of topics, some examples were labeled as only\nexpert evaluable, where automated evaluation is infeasible or\ninsuﬃcient (e.g. plotting). The total number of examples is 84,\nof which 25 were expert evaluable, and the accuracy is 75% for\nthe best performing model.\nThere is a strong correlation between the model parameter\ncount and accuracy,\n25 so we focus only on the largest models\nwith more than 1B parameters. The architectures of models are\nall decoder-only like GPT-3 (ref. 3) with the ability to insert\ncompletions,\n26 (except when noted). Therst model is a GPT-3\n12B ne-tuned on code (Codex) abbreviated as“cushman”.I t\nis known as code-cushman-001 in the OpenAI API.27 This is\nmodied from the original one in Austinet al.17 somewhat and\nis described as“a stronger, multilingual version of the Codex\n12B model.”.28 We also used code-davinci-002, abbreviated as\n“davinci”. This model is part of the category of “GPT-3.5”\nmodels that are derived from GPT-3.29 The number of parame-\nters in davinci-class models is not public information, but may\nmatch the 175B parameters of the model described in the GPT-\n3.5 paper.\n30 We also considered the recent text-davinci-003\nmodel which is derived from code-davinci-002 with a reinforce-\nment-learning adaption from human user feedback\n30 –\nalthough this model became available only aer human evalu-\nation (below) was complete, so our analysis is reported only on\nautomated evaluations. This model is denoted as ‘davinci3’\nhere. Finally, from publicly available information we know that\nChatGPT is based on a slightly modied version of GPT-3.5, and\nso we expect its performance to be comparable to that of the\nmodel; however, it does not have an API that would allow us to\nsystematically probe any diﬀerences in our study. One example\nuse of ChatGPT is given in the ESI.†\nWe also study two “incoder” models from Fried et al.\n18\ntrained on code only. We chose incoder because it is able to\ninll code in addition to completing code prompts, which gives\na more direct comparison, and it has generally good perfor-\nmance. Lastly, we consider the ‘codegen’ model,31 which is\nanother decoder-only model trained on a similar dataset to\n‘incoder’. It was not trained for in lling, because it was\ndesigned for back-and-forth code synthesis with natural\nlanguage. Although it is not exactly analogous to the other\nmodels, it is one of very few competitive models that can\ngenerate working code, and so we include it here for\ncomparison.\n31\nRecent benchmarks show that davinci is the best or nearly\nthe best for general programming tasks.16,32 Incoder was used as\nimplemented in HuggingFace transformers.33 To avoid library\nchanges since 2021 inuencing the accuracy, our evaluations\nare performed using the python version and packages from June\n2021. The chosen date was based on the reported training range\nfrom ref. 32 and comes before the training time in ref. 18.\nTable 1 The number of prompts by topic and best accuracy achiev-\nable in this work.“Expert” is the number within a topic that must be\nevaluated by an expert. We used the“copyright” context for incoder-\n6B, “authority” for codegen-16B, and“insert” for davinci andT = 0.2\n(best for all models). Accuracies are averaged (macro-averaging)\nacross top-k sampling (we consider correct if valid prompt appeared in\ntop-k results). Expert accuracies are macro-averaged across topics/\nprompts\nTopic N Expert Incoder Codegen Davinci Davinci3\nBio 13 2 0% 29% 43% (0%) a 86%\nCheminf 10 0 20% 20% 50% 50%\nGenchem 11 0 29% 86% 86% 86%\nmd 11 3 0% 13% 63% (81%) 88%\nPlot 10 10 —— — (57%) —\nqm 8 3 20% 60% 100% (59%) 100%\nsim 8 5 0% 0% 100% (64%) 100%\nspect 11 1 30% 20% 50% (12%) 40%\nstats 11 1 40% 80% 70% (88%) 60%\nThermo 10 0 10% 10% 80% 70%\nTotal 84\nb 23 17% 35% 72% (57%) 75%\na Expert evaluator scores are in parentheses.b Some prompts appear in\nmultiple topics. The abbreviations of topics are biochemistry (bio),\ncheminformatics (cheminf), general chemistry (genchm), molecular\ndynamics & simulation (md), quantum mechanics (qm), methods of\nsimulation (sim), spectroscopy (spect), statistics (stats), and\nthermodynamics (thermo)\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,3 6 8–376 | 369\nPaper Digital Discovery\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nWhen developing example prompts and solutions, the\nprompts were tested and modied using davinci. Some prompt\nengineering was inevitable through this process.3,34,35 However,\nprompts were not designed to get a correct answer and some\nprompts ( e.g., two atom harmonic oscillator) were never\ncorrectly completed. We do emphasize that the reported accu-\nracy is not what one would expect of the rst prompt con-\nstructed on-the- y for a given problem. Rather, they are\nconstructed to answer “how much chemistry do these LLMs\nknow?” These gures should not be construed as upper bounds\neither, as recent work on prompt engineering shows that\nmultiple steps (sometimes known as using“scratchpads”)\n19 or\neliciting multiple steps can further improve accuracy.29\nFollowing Chen et al.,32 a prompt completion is accurate if\nthe code functions correctly, not if it matches a reference\nimplementation. Most examples have both a prompt and unit\ntests. The accuracy of expert evaluable prompts for which there\nare no unit tests is not reported, unless speci ed. Five\ncompletions were generatedvia top-k sampling\n36 and multiple\ntemperatures atT = 0.05, 0.2, 0.5 (somax scaling). We explored\nnucleus sampling,37 but found it to be no diﬀerent compared to\nadjusting the temperature for balancing the diversity and\ncorrectness of completions. We chose k = 5 for all models,\nexcept for incoder-6B where GPU memory limitations prevented\nsampling more thank = 1. Thus, these results may be slightly\ninated since accuracy is reported on only a most likely output.\nError bars in all plots are 95% condence intervals generated\nfrom bootstrap resampling across top-k.\nExpert evaluation was performed onk = 3 outputs of davinci\n(T = 0.2, “insert” context) and accessed through a web inter-\nface.\n38 Each example contains a link to a custom Google form\nwhich could be used to evaluate that example, with results saved\nin a spreadsheet. The multiple choice questions in the form\nwere: “Is this question: Easy; Medium; Hard”, “Is the solution:\nPerfect; Correct but not perfect; Runs and is almost correct;\nDoes not run but is almost correct; Is far from correct”. There\nwas also a box for extra comments. This evaluation did breakout\nmore detailed information like alignment between the prompt\nand completion or hazards of completion, similar to that\nrecently proposed by Khlaaf.\n39 The full set of evaluations, with\npersonally identiable information (student emails) removed,\nis available as a comma separated value (CSV)le in the ESI.† To\nmake a numerical evaluation of this data as shown in Fig. 3, we\nassigned scores from 1–5 with 5 being the best (“Perfect”) and 1\nbeing the worst (“Is far from correct”). To compute an overall\naccuracy as reported in Table 1, we assigned “Perfect”, and\n“Correct but not perfect” a value of 1.0, and all others 0.0, and\nthen computed the mean score for each prompt separately. It\nshould be noted that each assessor had a diﬀerent level of\nexpertise on each topic, as well as a diﬀerent level of python\nprogramming experience, although we feel all were suﬃciently\nexpert to evaluate each prompt with suﬃcient authority.\nIII. Results\nA. Example problems\nTo illustrate the kinds of tasks and impressive (if not always\ncorrect) results produced by LLMs, we show the output for one\n‘sim’category task in Fig. 1. To standardize our tasks, each task\nis phrased as a function to belled in, as in the top box. This\nprompt includes arst line which loads the numerical python\n(numpy\n40) library, which gives additional‘context’(see below).\nThe rest of the information for the LLM is contained in two\nplaces, the names of the variables given as inputs‘n_steps’, ‘T’,\n‘k’, and a comment string which says what the function does/\nshould do. In this case, the function should perform Metrop-\nolis Monte Carlo for a harmonic potential. Implicit in the\ninstruction by the creator is that k represents the spring\nconstant, and so this code should produce samples from the\nFig. 1 Example prompt and code generated for database example‘mc_harmonic’. Full output is the prompt with‘[insert]’replaced by code in the\nlower box. The asterisk indicates a line that is faulty. The inset box shows equivalent lines from two other solutions that are correct, if not\nnecessarily optimal. This example is discussed in Sec. III A.\n370\n| Digital Discovery,2 0 2 3 ,2,3 6 8–376 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nenergy functionUðxÞ¼ 1\n2 kðx /C0 x0Þ2, withx0 = 0 since it was not\nspecied as an input, and also that reduced units are used, such\nthat Boltzmann's constantkB = 1.0. We can see that— with quite\nminimal instruction— the code in the output is correct except\nfor an error in the line indicated with a‘*’; in this line, the\nposition of the particle is completely resampled from scratch on\nthe range [−1,1). This code would actually bene if the system\nwere constrained to be within a box of length 2, and in the limit\nof k [ 1 it will also appear to give correct results. The inset\nshows the equivalent line in two other outputs of the model,\nboth of which are acceptable; one displaces the position by\na Gaussian random number withm = 0 and s\n2 = 1, and the\nsecond chooses a new position from a Gaussian with the mean\ncentered at the current position ands\n2 = 1. Note that neither of\nthese is optimized for the choice of (k,T), ass2 = 1 may be too\nlarge or too small to be eﬃcient, depending on the spring\nconstant and temperature. Finally, in one of the other two\noutputs for this example (available in the ESI† or on the result\nwebsite), k is interpreted as Boltzmann's constant, and the\nharmonic system is given a spring constant of 1.0 implicitly;\nthis is a reasonable inference of the model. It illustrates how the\nauthor must be careful about what is implicit in their prompt\nand what is stated explicitly (e.g. here, thatT is the temperature).\nFig. 2 shows an additional example to highlight how the\ndavinci-codex model internally contains knowledge of chem-\nistry topics (in this case, general chemistry pertaining to phase\nequilibrium). The output shows that the model“knows” the\nrelevant rearrangement of the Claussius–Clapeyron equation,\nand returns the appropriate result, assuming that the heat of\nvaporization (‘Hvap’) was given in joules mol\n−1. One gure in\nthe ESI† shows that we can use ChatGPT to solve the same\nproblem, either by asking it toll in the“[insert]” text with the\ncorrect solution, or by describing the problem conversationally.\nWhen an API for ChatGPT is available, we would expect the\nperformance in the former mode to be very similar to that of the\nunderlying GPT-3.5 model.\nB. Expert evaluations\nDavinci, the best performing model, does have broad knowl-\nedge of equations and common calculations across multiple\ndomains of chemistry. Table 1 gives the overall accuracy across\nthe topics, models, and expert evaluable topics. Both models\ncan correctly answer prompts across a range of topics, with\ndavinci performing the best. About 30 percentage points of\naccuracy are from prompt engineering, which is discussed\nfurther below.\nOn average, the accuracy for human evaluable topics is\nlower, re ecting their increased di ﬃculty. These prompts\ninclude tasks like writing an inputle for NWChem,\n41 imple-\nmenting a Monte Carlo simulation of a harmonic oscillator\n(Fig. 1), and generating a complex multi-panel plot. Fig. 3 shows\na breakdown of diﬃculty from the individual evaluations. There\nis a balance of easy and hard prompts in the dataset, as judged\nby experts. Our primary result here is that the accuracy of the\nmodel is negatively correlated with perceived prompt diﬃculty,\nas might be expected but did not necessarily have to be the case.\nWe did not perform any randomization or controls; each eval-\nuator was able to see all prompts and all outputs, and so we\nacknowledge that scores could be biased by factors such as the\norder of the prompts on the website, and the order that results\nfor a given prompt were presented on the website. In the rest of\nthis article, we focus only on prompts whose correctness can be\nevaluated by comparison with an expected solution in an\nautomated fashion.\nC. How to improve performance\nThere is a large accuracy gain when using basic prompt engi-\nneering strategies. Fig. 4 shows the eﬀect of diﬀerent “contexts”\non accuracy across models. A context here is code prepended\nbefore all prompts, or all prompts within a topic. The contexts\nare given both in the ESI † and our accompanying code.\n“Custom” includes two pieces: some imports related to the topic\n(e.g., rdkit\n42 for cheminf) and a single example to teach the\nmodel how to indicate the end of a prompt completion. The\nFig. 2 Example prompt and code generated for the database example\n‘claussius’. Full output is the prompt with‘[insert]’ replaced by code in\nthe lower box. Davinci passed our automated check for this example\non three out ofﬁve tries.\nFig. 3 650 evaluations of davinci completions by the nine coauthors\nwho are postdoctoral scholars or PhD students in chemistry or\nchemical engineering. Scoring is described in Sec. II. Weﬁnd that the\ntypical result quality (white dot) drops from‘Perfect,’ to ‘Correct but\nnot perfect’,t o ‘Runs and is almost correct’ as perceived diﬃculty\nincreased.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,3 6 8–376 | 371\nPaper Digital Discovery\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nimports are not just to prevent errors due to failure to include\nrelevant libraries— they inuence the completions and give\ncontext. For example, a“structure” aer importing rdkit means\na bonded arrangement of atoms; in contrast, a structure aer\nimporting openmm43 (a molecular dynamics simulation code)\nwould implicitly mean a 3D arrangement of atoms,e.g. obtained\nfrom a PDBle.\nThe completion example is a one line statement ( e.g.,\nprinting the version number of an imported package) with\na comment above and #end below. This causes the LLM to end\ncompletions with #end. We tried to ad hoc look for certain\nkeywords such as new function defs, returns, or comments as\ncompletion ends, but these heuristics were oen violated. The\ncompletion example is signicant for the Cushman model,\nwhich can only perform completions but not insertions. For the\ndavinci and incoder models, we can replace this with the\n“insert” contexts which have the same imports but use a model\ncapability to inll at a special insert token (as in Fig. 1).\nAvoiding our completion example in the context seems to be\ninsignicant for davincni, but important for incoder.\nLLMs seem to be very susceptible to conditioning contexts,\nlike adding the word“very” many times to improve a comple-\ntion\n44 or stating that the code“has no bugs”. We explored this in\nour benchmarks in two ways. We tried inserting copyright\nnotices and found, as shown in Fig. 4 and 5 that it does\nsignicantly improve accuracy at higher temperatures. This\nmakes intuitive sense; lowering the temperature makes the LLM\nchoose more likely completions and a copyright notice would\nmore oen be included with standard/quality code, thus giving\na similar eﬀect to lowering the temperature. The best per-\nforming model/temperature combination was not improved\nbecause it already had a low temperature. We also tried\ninserting the statement “This is written by an expert Python\nprogrammer” as suggested by Austin,\n45 and saw slightly less\nimprovement. A similar recent work has found context or\nspecic phrases (e.g., “let's think step by step”) that elicit chain-\nof-thought outputs which can give large accuracy improve-\nments.\n29,46 Fried et al.18 and Wei et al.35 have recently explored\nusing metadata, including popularity of code, as a mechanism\nto condition completions, so that we do not need to usead hoc\nprompt engineering. Interestingly, the results from davinci3\nshow that the improvements to the NLP model through human\nfeedback removed some of the observed sensitivity\n30 to prompt\nengineering on our examples.\nAside from contexts, there are a few strategies to ensure that\na prompt aligns the intent of a user with the completion. If the\nprompt contains programming mistakes or spelling mistakes,\nthen the completion will be of similar quality. So a correctly\nspelled and intelligible prompt is necessary.\nThe LLM tries to agree with each word in the prompt. If\na prompt is a function declaration and uses the phrase\n“compute the moment”, the model will probably not return the\nvalue. Thus, the word“return” should be used. If a package is\nimported in the prompt, the model will try to make use of it.\nThis can lead to problems if many packages are imported– it\ncan be unexpected as to which packages the model will use, or if\nthe model thinks it must use all of them.\nA major source of the errors in some of the categories such as\n‘md’ is the improper use of functions from a package such as\nmdtraj, in particular, improper knowledge of how many and\nwhat type of values are returned by that function; this could be\na simple error or due to training on an earlier version of the\nFig. 4 A comparison of accuracy of the LLMs compared in this study across diﬀerent contexts, broken down by category. Adding context– short\ncomments/imports – generally improves accuracy across topics and models. Error bars are 95% conﬁdence intervals from bootstrapping across\nindividual prompts and temperatures, and from multiple completions.\nFig. 5 Comparison of the context eﬀect across models and temper-\natures. Having a custom context is most important. Note that insert,\ncopyright, and authority include the“custom” context. Error bars are\n95% con ﬁdence intervals from bootstrapping across individual\nprompts and temperatures, and from multiple completions. Cushman\nand codegen cannot perform insertions.\n372 | Digital Discovery,2 0 2 3 ,2,3 6 8–376 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nmodule; these results may be able to be improved in the future\nby ‘ne tuning’the LLM on examples from a particular package\nthat is frequently used in one's work, or by adding additional\ncontext.\nD. Molecular structures\nOur goal is to evaluate how much chemistry LLMs know.\nBesides evaluating tasks that can be expressed as programs, we\nalso explored whether LLMs can connect natural language\ndirectly with molecular structures. We tested both\nInstructGPT\n30 and davinci in these examples, but found\nInstructGPT to work better. Neither could convert from molec-\nular SMILES to the name of the molecule, as demonstrated with\n0% accuracy on 100 random molecules from pubchem\n47 when\nwe tried a SMILES length of less than 60 characters (relatively\nsmall/simple molecules). The attempt from InstructGPT is\nshown in the ESI.† InstructGPT was able to convert a sentence\ndescribing a molecule into SMILES, as shown with examples in\nFig. 6. InstructGPT is able to connect functional groups from\nSMILES to natural language. The molecules are not exact\nmatches, but there is some correlation (e.g., oxygen near a ring\nfor phenol and amine). It is also able to correlate molecular\nproperties like lipophilicity with SMILES. InstructGPT rarely\ngenerates invalid SMILES; only therst molecule in Fig. 6 had\na single invalid character (see the ESI† for SMILES). It appears\nthat InstructGPT or other LLMs could be trained/ne-tuned on\nthe connection between natural language and chemical struc-\ntures. Recently, specic models that can translate between\nmolecular structure and natural language have also been\ntrained from scratch.\n48\nE. Discussion\nDavinci seems to not reason well about computational chem-\nistry. If we prompt davinci to use a“highly accurate single-\npoint” quantum calculation in pyscf,49 it will frequently use\nrelativistic Hartree –Fock regardless of the property being\ncomputed because it has memorized that“relativistic” is asso-\nciated with accurate. Another example is in the“force constant”\nprompt which is meant to compute the force constant for a two-\natom harmonic oscillator with diﬀerent masses given a wave-\nlength. Perhaps because this is an unusual variant of a common\nquestion (converting between the force constant and wave-\nlength), davinci always fails on this question and is unable to\nrearrange the equation to take a harmonic mean of masses.\nDavinci may also hallucinate functions that do not exist. If\nad iﬃcult prompt is given, for example“return the residual\ndipole couplings given a SMILES string,” the model will simply\ntry to use a non-existent method MolToRDC. As reported\npreviously,\n22 LLMs are not able to perform chemical reasoning\nwhen completing prompts.\nWe would like to anecdotally note that the LLMs could\nperform many of the benchmark problems if the natural\nlanguage was in Chinese, German, or Spanish. We did not\nexplore this in depth, but a few example prompts written in\nMandarin can be found in the ESI.† The use of LLMs with\nprompts that are not in English may be a valuable tool for\nlowering the barrier for employing computational tools for\nthose who are not native English speakers, and who therefore\nmay have a harder time interpreting documentation and\nprogramming forums.\nIV. Conclusions\nLLMs are now easily availablevia tools like tabnine,50 copilot,51\nor ChatGPT.52 We have found high accuracy on chemistry\nquestions, and it is inevitable that students and researchers will\nbegin using these tools. From our results, high accuracy should\nbe expected with reasonable prompts. We emphasize that our\nresults only give lower bounds on the chemistry knowledge in\nthese models, since they cover only the specic topics so far\nincluded in our database, and further prompt engineering or\nother strategies for evaluating this knowledge besides python\nfunction writing could elicit even better results.\nTricks like inserting copyright notices at the top of a source\nle seems to be another way to improve accuracy, althoughne-\ntuning with human feedback mitigates this eﬀect,\n30 as seen in\ndavinci3. We found that humans are able to gauge accuracy for\neasy to medium prompts, but care should be taken if using\ncompletions of diﬃcult prompts. The seeming ability to always\ngenerate syntactically valid code means LLMs oen produce\nsomething, but it is up to the user to assess it. We also found\nsomewhat unexpected capabilities like generating molecules\nfrom natural language and accurate completions with non-\nEnglish prompts. For a broader discussion of what impact\nFig. 6 Generating molecules with InstructGPT (text-davinci-002).\nPrompts are shown in annotations. The strongly lipophilic molecule is\nC\n505, a polystyrene that is indeed strongly lipophilic. Most examples\ncontain mistakes, but were mostly valid. The top-left example had an\nambiguous ring indicator index which was removed prior to drawing.\nAll structures do not match the prompt exactly (indicated by\na crossed-icon), but do have details correlated with the prompt.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,3 6 8–376 | 373\nPaper Digital Discovery\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nthis will have on education, we refer interested readers to our\nearlier perspective article.5\nData availability\nAccuracy data is available as comma separated valueles in the\nESI.† Contexts are available as a markuple included in the\nESI.† The responses from the model (completions) which were\nthe basis for expert evaluators are available in HTML format at\nhttps://doi.org/10.5281/zenodo.6800475. Code used to create\ncompletions with contexts is available at https://github.com/\nwhitead/nlcc. Incoder model is available at https://\ngithub.com/dpfried/incoder/blob/main/README.md. OpenAI\nCodex requires an access key to use and its model and\nanalysis are discussed inhttps://arxiv.org/abs/2107.03374.\nAuthor contributions\nA. D. W. and G. M. H. wrote NLCC soware and designed the\nnlcc-database, website, and human evaluation form. They\ncontributed examples to the nlcc-data repository, performed\ndata analysis, and draed the manuscript. All other authors\ncontributed examples to the nlcc-data repository, participated\nin the expert evaluation, and assisted in writing the manuscript.\nConﬂicts of interest\nAer submission of this manuscript, A. D. W. worked as a paid\nconsultant for OpenAI, the developers of some of the models\npresented in this work.\nAcknowledgements\nResearch reported in this work was supported by the National\nInstitute of General Medical Sciences of the National Institutes of\nHealth under award number R35GM137966 (to A. D. W.) and\nR35GM138312 (to G. M. H.). HAG was supported by NSF award\n1751471. MA, SC, and Z. Y. were supported by NIH award\nR35GM137966. G. P. W. was supported by NSF award 1764415. S.\nS. and Y. S. were partially supported by NIH award R35GM138312,\nWJPC by R35GM138312-02S1, and K. L. partially by Department\nof Energy award DESC0020464. S. S. and K. L. were also partially\nsupported by the Simons Foundation Grant No. 839534. We\nthank Drs Sanjib Paul, David Gomez, and Navneeth Gokul who\nalso contributed some examples to the repository.\nReferences\n1 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you\nneed, Adv. Neural Inf. Process. Syst., 2017, vol. 30.\n2 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, Bert: pre-\ntraining of deep bidirectional transformers for language\nunderstanding, arXiv, 2018, preprint, arXiv:1810.04805,\nDOI: 10.48550/arXiv.1810.04805.\n3 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al., Language models are few-shot learners,Adv. Neural\nInf. Process. Syst., 2020,33, 1877.\n4 A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid,\nA. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-\nAlonso, et al., Beyond the imitation game: quantifying and\nextrapolating the capabilities of language models, arXiv,\n2022, preprint, arXiv:2206.04615, DOI: 10.48550/\narXiv.2206.04615.\n5 G. M. Hocky and A. D. White, Natural language processing\nmodels that automate programming will transform\nchemistry research and teaching,Digit. Discovery, 2022,1, 79.\n6 S. Wang, Y. Guo, Y. Wang, H. Sun and J. Huang, Smiles-bert:\nlarge scale unsupervised pre-training for molecular property\nprediction, in Proceedings of the 10th ACM international\nconference on bioinformatics, computational biology and\nhealth informatics, 2019, pp. 429–436.\n7 N. Frey, R. Soklaski, S. Axelrod, S. Samsi, R. Gomez-\nBombarelli, C. Coley and V. Gadepally, Neural scaling of\ndeep chemical models, ChemRxiv, 2022, preprint, DOI:\n10.26434/chemrxiv-2022-3s512.\n8 D. Flam-Shepherd, K. Zhu and A. Aspuru-Guzik, Language\nmodels can learn complex molecular distributions, Nat.\nCommun., 2022,13,1 .\n9 J. Ross, B. Belgodere, V. Chenthamarakshan, I. Padhi,\nY. Mroueh and P. Das, Do large scale molecular language\nrepresentations capture important structural information?,\narXiv, 2021, preprint, arXiv:2106.09553, DOI: 10.48550/\narXiv.2106.09553.\n10 https://openai.com/blog/chatgpt/.\n11 C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, P. J. Liu,et al., Exploring the\nlimits of transfer learning with a uni ed text-to-text\ntransformer, J. Mach. Learn. Res., 2020,21,1 .\n12 L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe,\nC. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,et al.,\nThe pile: An 800 gb dataset of diverse text for language\nmodeling, arXiv, 2020, preprint, arXiv:2101.00027, DOI:\n10.48550/arXiv.2101.00027.\n13 D. Weininger, Smiles, a chemical language and information\nsystem. 1. introduction to methodology and encoding rules,\nJ. Chem. Inf. Comput. Sci., 1988,28\n, 31.\n14 C. Nantasenamat,“would be cool to have gpt-3 generate new\nchemical structures in smiles notation? ”, Twitter,\n1516794237391863810, 2022 A. D. White,“as suggested by\n@thedataprof, gpt-3 can actually generate molecules. very\nclever idea! prompt was ”the smiles for this drug-like\nmolecular are:”, Twitter, 1516795519284228106, 2022 P.\nIsola, “language-conditional models can act a bit like\ndecision transformers, in that you can prompt them with\na desired level of “reward”. e.g., want prettier #dalle\ncreations? ”just ask ” by adding ”[very]^n beautiful ”:”,\nTwitter, 1532189616106881027, 2022 J. Austin, “we found\nthat code models get better when you prompt them with\ni’m an expert python programmer. the new anthropic\npaper did something similar, pre xing the model ’s\nresponse with i’ve tested this function myself so i know\nthat it’s correct:, Twitter, 1515063524258627586, 2022.\n374 | Digital Discovery,2 0 2 3 ,2,3 6 8–376 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n15 C. Nantasenamat,“would be cool to have gpt-3 generate new\nchemical structures in smiles notation? ”, Twitter,\n1516794237391863810, 2022 A. D. White,“as suggested by\n@thedataprof, gpt-3 can actually generate molecules. very\nclever idea! prompt was ”the smiles for this drug-like\nmolecular are:”, Twitter, 1516795519284228106, 2022 P.\nIsola, “language-conditional models can act a bit like\ndecision transformers, in that you can prompt them with\na desired level of “reward”. e.g., want prettier #dalle\ncreations? ”just ask ” by adding ”[very]^n beautiful ”:”,\nTwitter, 1532189616106881027, 2022 J. Austin, “we found\nthat code models get better when you prompt them with\ni’m an expert python programmer. the new anthropic\npaper did something similar, pre xing the model ’s\nresponse with i’ve tested this function myself so i know\nthat it’s correct:, Twitter, 1515063524258627586, 2022.\n16 F. F. Xu, U. Alon, G. Neubig and V. J. Hellendoorn, A\nsystematic evaluation of large language models of code, in\nProceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, 2022, pp. 1–10.\n17 J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski,\nD. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le,et al., Program\nsynthesis with large language models, arXiv, 2021,\npreprint, arXiv:2108.07732, DOI:10.1145/3520312.3534862.\n18 D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi,\nR. Zhong, W.-t. Yih, L. Zettlemoyer and M. Lewis, Incoder:\na generative model for code inlling and synthesis, arXiv,\n2022, preprint, arXiv:2204.05999, DOI: 10.48550/\narXiv.2204.05999.\n19 E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\nS. Savarese and C. Xiong, A conversational paradigm for\nprogram synthesis, arXiv, 2022, preprint, arXiv:2203.13474,\nDOI: 10.48550/arXiv.2203.13474.\n20 A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever,\net al., Language models are unsupervised multitask learners,\nOpenAI blog, 2019, vol. 1, p. 9.\n21 Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,\nL. Shou, B. Qin, T. Liu, D. Jiang,et al., Codebert: A pre-\ntrained model for programming and natural languages,\narXiv, 2020, preprint, arXiv:2002.08155, DOI: 10.48550/\narXiv.2002.08155.\n22 E. M. Bender and A. Koller, Climbing towards nlu: on\nmeaning, form, and understanding in the age of data, in\nProceedings of the 58th annual meeting of the association for\ncomputational linguistics, 2020, pp. 5185–5198.\n23 E. M. Bender, T. Gebru, A. McMillan-Major and\nS. Shmitchell, On the dangers of stochastic parrots: Can\nlanguage models be too big?, in Proceedings of the 2021\nACM Conference on Fairness, Accountability, and\nTransparency, 2021, pp. 610–623.\n24 https://github.com/ur-whitelab/nlcc-data.\n25 P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu,\nM. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar,\net al., Holistic evaluation of language models,arXiv, 2022,\npreprint, arXiv:2211.09110, DOI:10.48550/arXiv.2211.09110.\n26 M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey,\nJ. Tworek and M. Chen, E ﬃcient training of language\nmodels to ll in the middle, arXiv, 2022, preprint,\narXiv:2207.14255, DOI:10.48550/arXiv.2207.14255.\n27 https://Openai.com.\n28 https://beta.openai.com/docs/model-index-for-researchers.\n29 T. Kojima, S. S. Gu, M. Reid, Y. Matsuo and Y. Iwasawa, Large\nlanguage models are zero-shot reasoners, arXiv, 2022,\npreprint, arXiv:2205.11916, DOI:10.48550/arXiv.2205.11916.\n30 L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,et al.,\nTraining language models to follow instructions with\nhuman feedback, arXiv, 2022, preprint, arXiv:2203.02155,\nDOI: 10.48550/arXiv.2203.02155.\n31 E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\nS. Savarese and C. Xiong, A conversational paradigm for\nprogram synthesis, arXiv, 2022, preprint, arXiv:2203.13474,\nDOI: 10.48550/arXiv.2203.13474.\n32 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto,\nJ. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,\net al., Evaluating large language models trained on code,\narXiv, 2021, preprint, arXiv:2107.03374, DOI: 10.48550/\narXiv.2107.03374.\n33 T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,et al.,\nHuggingface's transformers: state-of-the-art natural\nlanguage processing, arXiv, 2019, preprint,\narXiv:1910.03771, DOI:10.48550/arXiv.1910.03771.\n34 S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Ra ﬀel,\nN. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry,et al.,\nPromptsource: an integrated development environment\nand repository for natural language prompts,arXiv, 2022,\npreprint, arXiv:2202.01279, DOI:10.48550/arXiv.2202.01279.\n35 J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le and\nD. Zhou, Chain of thought prompting elicits reasoning in\nlarge language models, arXiv, 2022, preprint,\narXiv:2201.11903, DOI:10.48550/arXiv.2201.11903.\n36 A. Fan, M. Lewis and Y. Dauphin, Hierarchical neural story\ngeneration, arXiv, 2018, preprint, arXiv:1805.04833, DOI:\n10.48550/arXiv.1805.04833.\n37 A. Holtzman, J. Buys, L. Du, M. Forbes and Y. Choi, The\ncurious case of neural text degeneration, arXiv, 2019,\npreprint, arXiv:1904.09751, DOI:10.48550/arXiv.1904.09751.\n38 https://ur-whitelab.github.io/nlcc-data/.\n39 H. Khlaaf, A hazard analysis framework for code synthesis\nlarge language models, arXiv, 2022, preprint,\narXiv:2207.14157, DOI:10.48550/arXiv.2207.14157.\n40 C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gommers,\nP. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg,\nN. J. Smith,et al., Array programming with numpy,Nature,\n2020, 585, 357.\n41 M. Valiev, E. J. Bylaska, N. Govind, K. Kowalski,\nT. P. Straatsma, H. J. J. Van Dam, D. Wang, J. Nieplocha,\nE. Apr`a, T. L. Windus, et al., Nwchem: a comprehensive\nand scalable open-source solution for large scale molecular\nsimulations, Comput. Phys. Commun., 2010,181, 1477.\n42 G. Landrum, et al. , Rdkit: A So ware Suite for\nCheminformatics, Computational Chemistry, and Predictive\nModeling, Greg Landrum, 2013.\n© 2023 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 3 ,2,3 6 8–376 | 375\nPaper Digital Discovery\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n43 P. Eastman, J. Swails, J. D. Chodera, R. T. McGibbon,\nY. Zhao, K. A. Beauchamp, L.-P. Wang, A. C. Simmonett,\nM. P. Harrigan, C. D. Stern, et al., Openmm 7: rapid\ndevelopment of high performance algorithms for\nmolecular dynamics,PLoS Comput. Biol., 2017,13, e1005659.\n44 C. Nantasenamat,“would be cool to have gpt-3 generate new\nchemical structures in smiles notation? ”, Twitter,\n1516794237391863810, 2022 A. D. White,“as suggested by\n@thedataprof, gpt-3 can actually generate molecules. very\nclever idea! prompt was ”the smiles for this drug-like\nmolecular are:”, Twitter, 1516795519284228106, 2022 P.\nIsola, “language-conditional models can act a bit like\ndecision transformers, in that you can prompt them with\na desired level of “reward”. e.g., want prettier #dalle\ncreations? ”just ask ” by adding ”[very]^n beautiful ”:”,\nTwitter, 1532189616106881027, 2022 J. Austin, “we found\nthat code models get better when you prompt them with\ni’m an expert python programmer. the new anthropic\npaper did something similar, pre xing the model ’s\nresponse with i’ve tested this function myself so i know\nthat it’s correct:, Twitter, 1515063524258627586, 2022.\n45 C. Nantasenamat,“would be cool to have gpt-3 generate new\nchemical structures in smiles notation? ”, Twitter,\n1516794237391863810, 2022 A. D. White,“as suggested by\n@thedataprof, gpt-3 can actually generate molecules. very\nclever idea! prompt was ”the smiles for this drug-like\nmolecular are:”, Twitter, 1516795519284228106, 2022 P.\nIsola, “language-conditional models can act a bit like\ndecision transformers, in that you can prompt them with\na desired level of “reward”. e.g., want prettier #dalle\ncreations? ”just ask ” by adding ”[very]\n^n beautiful ”:”,\nTwitter, 1532189616106881027, 2022 J. Austin, “we found\nthat code models get better when you prompt them with\ni’m an expert python programmer. the new anthropic\npaper did something similar, pre xing the model ’s\nresponse with i’ve tested this function myself so i know\nthat it’s correct:, Twitter, 1515063524258627586, 2022.\n46 Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\nD. Drain, S. Fort, D. Ganguli, T. Henighan,et al., Training\na helpful and harmless assistant with reinforcement\nlearning from human feedback, arXiv, 2022, preprint,\narXiv:2204.05862, DOI:10.48550/arXiv.2204.05862.\n47 S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li,\nB. A. Shoemaker, P. A. Thiessen, B. Yu, et al., Pubchem\n2019 update: improved access to chemical data, Nucleic\nAcids Res., 2019,47, D1102.\n48 C. Edwards, T. Lai, K. Ros, G. Honke and H. Ji, Translation\nbetween molecules and natural language, arXiv, 2022,\npreprint, arXiv:2204.11817, DOI:10.48550/arXiv.2204.11817.\n49 Q. Sun, T. C. Berkelbach, N. S. Blunt, G. H. Booth, S. Guo,\nZ. Li, J. Liu, J. D. McClain, E. R. Sayfutyarova, S. Sharma,\net al., Pyscf: the python-based simulations of chemistry\nframework, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 2018,\n8, e1340.\n50 https://www.tabnine.com/.\n51 https://github.com/features/copilot.\n52 https://openai.com/blog/chatgpt/.\n376 | Digital Discovery,2 0 2 3 ,2,3 6 8–376 © 2023 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 26 January 2023. Downloaded on 11/5/2025 1:44:10 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6294587850570679
    },
    {
      "name": "Code (set theory)",
      "score": 0.5939085483551025
    },
    {
      "name": "Programming language",
      "score": 0.42902424931526184
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}