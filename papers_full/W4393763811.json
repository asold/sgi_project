{
  "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
  "url": "https://openalex.org/W4393763811",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2227977723",
      "name": "Yuji Cao",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2102854265",
      "name": "Huan Zhao",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2542390697",
      "name": "Yu Heng Cheng",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2126558564",
      "name": "Ting Shu",
      "affiliations": [
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2099858462",
      "name": "Yue Chen",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2124436185",
      "name": "Guolong Liu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2318092400",
      "name": "Gaoqi Liang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2238333488",
      "name": "Junhua Zhao",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2122072271",
      "name": "Jinyue Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112108313",
      "name": "Yun Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2145339207",
    "https://openalex.org/W6741002519",
    "https://openalex.org/W6747473740",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W6772005887",
    "https://openalex.org/W6922480057",
    "https://openalex.org/W2989847975",
    "https://openalex.org/W4313332633",
    "https://openalex.org/W2960876848",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6764088478",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W6782858274",
    "https://openalex.org/W2964915587",
    "https://openalex.org/W6799150178",
    "https://openalex.org/W4391759936",
    "https://openalex.org/W6839888651",
    "https://openalex.org/W3157893055",
    "https://openalex.org/W4225417724",
    "https://openalex.org/W4382239386",
    "https://openalex.org/W4287164755",
    "https://openalex.org/W6766952794",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4396986140",
    "https://openalex.org/W4399120818",
    "https://openalex.org/W6870761175",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W6850885490",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W6854372151",
    "https://openalex.org/W4386215566",
    "https://openalex.org/W6782465632",
    "https://openalex.org/W4310509152",
    "https://openalex.org/W6849861922",
    "https://openalex.org/W6849843017",
    "https://openalex.org/W6856012975",
    "https://openalex.org/W6859584208",
    "https://openalex.org/W6850833530",
    "https://openalex.org/W6850101715",
    "https://openalex.org/W6753243525",
    "https://openalex.org/W6784098896",
    "https://openalex.org/W2938421504",
    "https://openalex.org/W6743368274",
    "https://openalex.org/W6772285785",
    "https://openalex.org/W3038822267",
    "https://openalex.org/W4407736288",
    "https://openalex.org/W6846476806",
    "https://openalex.org/W6849851706",
    "https://openalex.org/W3081310128",
    "https://openalex.org/W4385430559",
    "https://openalex.org/W6857462693",
    "https://openalex.org/W4391215636",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6810220367",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W6802669662",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6873795324",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W6853465110",
    "https://openalex.org/W4393160302",
    "https://openalex.org/W3135642434",
    "https://openalex.org/W3198999478",
    "https://openalex.org/W3009172038",
    "https://openalex.org/W4388081402",
    "https://openalex.org/W4226512186",
    "https://openalex.org/W6857151620",
    "https://openalex.org/W6810640255",
    "https://openalex.org/W6849277338",
    "https://openalex.org/W4252279978",
    "https://openalex.org/W6780470247",
    "https://openalex.org/W6838876373",
    "https://openalex.org/W6853528043",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W4405470166",
    "https://openalex.org/W4402753642",
    "https://openalex.org/W4402670522",
    "https://openalex.org/W3039732322",
    "https://openalex.org/W3175599258",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4402959528",
    "https://openalex.org/W4390367337",
    "https://openalex.org/W6861112638",
    "https://openalex.org/W4401018111",
    "https://openalex.org/W3119264617",
    "https://openalex.org/W6730038592",
    "https://openalex.org/W6791862963",
    "https://openalex.org/W6849548236",
    "https://openalex.org/W6849895545",
    "https://openalex.org/W6857487902",
    "https://openalex.org/W6855653055",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W6857037581",
    "https://openalex.org/W6810080435",
    "https://openalex.org/W6839256673",
    "https://openalex.org/W3205786327",
    "https://openalex.org/W6860855263",
    "https://openalex.org/W6853664340",
    "https://openalex.org/W6851275496",
    "https://openalex.org/W6856437238",
    "https://openalex.org/W4388997064",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W6846930601",
    "https://openalex.org/W6861451011",
    "https://openalex.org/W6846853244",
    "https://openalex.org/W3109097593",
    "https://openalex.org/W6804244202",
    "https://openalex.org/W6810156098",
    "https://openalex.org/W6858444883",
    "https://openalex.org/W6854282728",
    "https://openalex.org/W6810089409",
    "https://openalex.org/W6776601253",
    "https://openalex.org/W6851261902",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W3099204253",
    "https://openalex.org/W2998557583",
    "https://openalex.org/W6851860556",
    "https://openalex.org/W6870850844",
    "https://openalex.org/W6867621267",
    "https://openalex.org/W1845972764",
    "https://openalex.org/W3185083385",
    "https://openalex.org/W6810117489",
    "https://openalex.org/W6850227584",
    "https://openalex.org/W6858415753",
    "https://openalex.org/W4400524848",
    "https://openalex.org/W6850886341",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6771217966",
    "https://openalex.org/W6783988234",
    "https://openalex.org/W4224220194",
    "https://openalex.org/W6796289742",
    "https://openalex.org/W6842654171",
    "https://openalex.org/W6850540853",
    "https://openalex.org/W6804859080",
    "https://openalex.org/W6809850638",
    "https://openalex.org/W6756256016",
    "https://openalex.org/W6859455233",
    "https://openalex.org/W6810494806",
    "https://openalex.org/W6857057181",
    "https://openalex.org/W4390188019",
    "https://openalex.org/W3216656735",
    "https://openalex.org/W6881462737",
    "https://openalex.org/W6856860864",
    "https://openalex.org/W6868270293",
    "https://openalex.org/W6854268352",
    "https://openalex.org/W4392737061",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W4392173735",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W6860640815",
    "https://openalex.org/W2126316555",
    "https://openalex.org/W6864723252",
    "https://openalex.org/W6852792400",
    "https://openalex.org/W6810932996",
    "https://openalex.org/W6852749991",
    "https://openalex.org/W6853359188",
    "https://openalex.org/W4384282814",
    "https://openalex.org/W4390871722",
    "https://openalex.org/W6852648873",
    "https://openalex.org/W6810921705",
    "https://openalex.org/W6859298233",
    "https://openalex.org/W4383604511",
    "https://openalex.org/W4403706044",
    "https://openalex.org/W6870442082",
    "https://openalex.org/W6875712328",
    "https://openalex.org/W6869609294",
    "https://openalex.org/W6882843459"
  ],
  "abstract": "With extensive pretrained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects, such as multitask learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared with conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated and provide insights into future directions. Finally, the comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications, such as robotics, autonomous driving, and energy systems.",
  "full_text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 1\nSurvey on Large Language Model-Enhanced\nReinforcement Learning: Concept, Taxonomy, and\nMethods\nYuji Cao, Huan Zhao, Member, IEEE, Yuheng Cheng, Student Member, IEEE, Ting Shu, Yue Chen,\nMember, IEEE, Guolong Liu, Member, IEEE, Gaoqi Liang, Member, IEEE, Junhua Zhao, Senior Member, IEEE,\nJinyue Yan, Yun Li, Fellow, IEEE\nAbstract—With extensive pre-trained knowledge and high-level\ngeneral capabilities, large language models (LLMs) emerge as a\npromising avenue to augment reinforcement learning (RL) in as-\npects such as multi-task learning, sample efficiency, and high-level\ntask planning. In this survey, we provide a comprehensive review\nof the existing literature in LLM-enhanced RL and summarize\nits characteristics compared to conventional RL methods, aiming\nto clarify the research scope and directions for future studies.\nUtilizing the classical agent-environment interaction paradigm,\nwe propose a structured taxonomy to systematically categorize\nLLMs’ functionalities in RL, including four roles: information\nprocessor, reward designer, decision-maker, and generator. For\neach role, we summarize the methodologies, analyze the specific\nRL challenges that are mitigated, and provide insights into future\ndirections. Lastly, comparative analysis of each role, potential\napplications, prospective opportunities and challenges of the\nLLM-enhanced RLare discussed. By proposing this taxonomy, we\naim to provide a framework for researchers to effectively leverage\nLLMs in the RL field, potentially accelerating RL applications in\ncomplex applications such as robotics, autonomous driving, and\nenergy systems.\nIndex Terms—Reinforcement learning (RL), large language\nmodels (LLM), vision-language models (VLM), multimodal RL,\nLLM-enhanced RL.\nI. I NTRODUCTION\nCorresponding author: Huan Zhao, Junhua Zhao.\nYuji Cao and Yue Chen are with the Department of Mechan-\nical and Automation Engineering, The Chinese University of Hong\nKong, Hong Kong SAR, 999077, China (email: yjcao@mae.cuhk.edu.hk,\nyuechen@mae.cuhk.edu.hk)\nHuan Zhao and Jinyue Yan are with the Department of Building En-\nvironment and Energy Engineering, The Hong Kong Polytechnic Univer-\nsity, Hong Kong, 100872, China (email: huan-paul.zhao@polyu.edu.hk, j-\njerry.yan@polyu.edu.hk)\nYuheng Cheng, Guolong Liu, and Junhua Zhao are with the School of\nScience and Engineering, The Chinese University of Hong Kong, Shenzhen,\n518172, China, and also with the Center for Crowd Intelligence, Shen-\nzhen Institute of Artificial Intelligence and Robotics for Society (AIRS),\nShenzhen, 518129, China (email: yuhengcheng@link.cuhk.edu.cn, liuguo-\nlong@cuhk.edu.cn, zhaojunhua@cuhk.edu.cn)\nTing Shu is with Guangdong-Hongkong-Macao Greater Bay Area Weather\nResearch Center for Monitoring Warning and Forecasting (Shenzhen In-\nstitute of Meteorological Innovation), Shenzhen, 518125, China. (email:\nshuting@gbamwf.com)\nGaoqi Liang is with the School of Mechanical Engineering and Automation,\nHarbin Institute of Technology, Shenzhen, 518055, China (e-mail: liang-\ngaoqi@hit.edu.cn)\nYun Li is with the Shenzhen Institute for Advanced Study, University of\nElectronic Science and Technology of China, Shenzhen, 518110, China, and\nalso with i4AI Ltd., WC1N 3AX London, U.K (email: Yun.Li@ieee.org)\nR\nEINFORCEMENT learning (RL) is a powerful learning\nparadigm that focuses on control and decision-making,\nwhere an agent learns to optimize a specified target through\ntrial and error interactions with the environment. Traditional\nRL methods, however, often struggled with high-dimensional\nstate spaces and complex environments [1]. The integration\nof deep learning techniques with RL, known as deep RL, has\nled to significant breakthroughs. In 2015, Deep Q-Networks\n(DQN) [2] marked a turning point, demonstrating human-level\nperformance on Atari games using raw pixel inputs. Subse-\nquent innovations such as proximal policy optimization [3]\nand soft actor-critic [4] have further expanded the capabilities\nof deep RL. In different realms, deep RL algorithms have\nachieved promising performance, such as real-time strategy\ngames [5], [6], board games [7], [8], energy management [9]\nand imperfect information games [10], [11]. Concurrent ad-\nvancements in natural language processing (NLP) and com-\nputer vision (CV) [12], [13], have fostered new RL paradigms,\nsuch as language-conditional RL [14], which uses natural\nlanguage to instruct agents, and vision-based RL [15], where\nagents learn from high-dimensional visual inputs.\nThe integration of language and vision capabilities into deep\nRL has introduced new challenges, as the agent has to learn\nthe high-dimensional features and a control policy jointly. To\nreduce the burden of visual feature learning, reference [16] de-\ncoupled representation learning from RL. To handle language-\ninvolved tasks, a survey [17] called for potential uses of NLP\ntechniques in RL. Nevertheless, the capabilities of language\nmodels were limited at that time and the following four chal-\nlenges still have not been addressed: 1) Sample inefficiency :\nLanguage and vision tasks involve large, complex state-action\nspaces, making it challenging for RL agents to learn effective\npolicies. Moreover, agents must understand tasks and connect\nthem to corresponding states, necessitating even more exten-\nsive interactions [18], [19], [20]. 2) Reward function design: In\nlanguage and vision tasks, designing effective reward functions\nis particularly challenging. These functions must capture subtle\nlinguistic nuances and complex visual features, significantly\nincreasing the complexity of an already difficult process.\nMoreover, aligning rewards with high-level task objectives in\nthese domains often requires domain expertise and extensive\ntrial-and-error [21], [22], [1], [23]. 3) Generalization: RL\nagents often overfit to training data, especially in vision-based\nenvironments, leading to poor performance when deployed in\narXiv:2404.00282v3  [cs.LG]  30 Oct 2024\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 2\nstates with interventions (e.g., added noise). Agents must learn\ninvariant features robust to such interventions, enabling gen-\neralization across varied linguistic contexts and visual scenes.\nHowever, the complexity of these domains makes extracting\nsuch features and adapting to new environments particularly\nchallenging [24], [25]. 4) Natural language understanding :\ndeep RL faces difficulties in natural language processing and\nunderstanding scenarios, where the nuances and complexities\nof human language present unique challenges that are not\nadequately addressed by current RL methodologies [26].\nThe field of NLP has undergone a revolutionary transfor-\nmation since the introduction of the Transformer architecture\nin 2017 [12]. This breakthrough paved the way for the devel-\nopment of Large Language Models (LLMs), with landmark\nmodels such as BERT [27], GPT [28], and more recent itera-\ntions such as GPT-3 [29] and PaLM [30] marking significant\nmilestones. The emergence of these LLMs has demonstrated\npowerful capabilities across various real-world applications,\nincluding medicine [31], chemical research [32], energy sys-\ntem [33], [34], [35] and embodied control in robotics [36].\nThese models have not only advanced the field of NLP but\nalso shown remarkable potential in tackling complex, multi-\ndisciplinary challenges. Compared to small language models,\nLLMs have emergent capabilities that are not present in small\nlanguage models [37], such as in-context learning [38], reason-\ning ability [39] etc. Additionally, leveraging the vast amounts\nof training data, pre-trained LLMs are equipped with a broad\nspectrum of world knowledge [40]. Benefiting from these\ncapabilities, the applications of language models have been\nshifted from language modeling to task-solving, ranging from\nbasic text classification and sentiment analysis to complex\nhigh-level task planning [41] and decision-making [42], [43].\nWith emergent capabilities, the potential of LLMs to address\nthe inherent challenges of RL has recently gained popu-\nlarity [44], [45]. The capabilities of LLMs, particularly in\nnatural language understanding, reasoning, and task planning,\nprovide a unique approach to solving the above-mentioned\nRL issues. For sample inefficiency, reference [46] proposed\na framework where LLMs can be employed to improve the\nsample efficiency of RL agents by providing rich, contextually\ninformed predictions or suggestions, thereby reducing the need\nfor extensive environment interactions. For reward function\ndesign, LLMs can aid in constructing more nuanced and\neffective reward functions, enhancing the learning process by\noffering a deeper understanding of complex scenarios [47]. For\ngeneralization, reference [48] proposed a framework that lever-\nages language-based feedback for improving the generalization\nof RL policy in unseen environments. For natural language\nunderstanding, Pang et al. [49] used LLMs to translate\ncomplex natural language-based instructions to simple task-\nspecified languages for RL agents. These works show that\nLLM is a promising and powerful role that can contribute to\nthe longstanding RL challenges.\nDespite the advancements in the domain of integrating\nLLMs into the RL paradigm, there is currently a notable\nabsence of comprehensive review in this rapidly evolving area.\nAdditionally, though various methods are proposed to integrate\nLLMs into the RL paradigm, there is no unified framework for\nsuch integration. Our survey paper seeks to fill these gaps by\nproviding an extensive review of the related literature, defining\nthe scope of the novel paradigm called LLM-enhanced RL, and\nfurther proposing a taxonomy to categorize the functionalities\nof LLMs in the proposed paradigm.\nA. Contributions\nThis survey makes the following contributions:\n• LLM-enhanced RL paradigm: This paper presents the first\ncomprehensive review in the emerging field of integrating\nLLM into the RL paradigm. To clarify the research scope\nand direction for future works, we define the term LLM-\nenhanced RL to encapsulate this class of methodologies,\nsummarize the characteristics and provide a correspond-\ning framework that clearly illustrates 1) how to integrate\nLLMs in classical agent-environment interaction and 2)\nthe multifaceted enhancements that LLMs offer to the\nconventional RL paradigm.\n• Unified taxonomy: Further classifying the functionalities\nof LLMs in the LLM-enhanced RL paradigm, we propose\na structured taxonomy to systematically categorize LLMs\nwithin the classical agent-environment paradigm, where\nLLMs are classified as information processors, reward\ndesigners, decision-makers, and generators. By such a\ncategorization, a clear view of how LLMs integrate into\nthe classical RL paradigm is offered.\n• Algorithmic review : For each role of LLM, we review\nemerging works in this direction and discuss different\nalgorithmic characteristics from the perspective of ca-\npabilities. Based on this foundation, future applications,\nopportunities, and challenges of LLM-enhanced RL are\nanalyzed to provide a potential roadmap for advancing\nthis interdisciplinary field.\nB. Text Organization\nThe remaining sections are organized as follows. Sec-\ntion II provides foundational knowledge of both RL and\nLLM. Section III presents the concept of LLM-enhanced\nRL and provides its overall framework. Following this, Sec-\ntions IV, V, VI, and VII offer an in-depth analysis of LLMs\nwithin the RL context, exploring their roles as information\nprocessor, reward designer, decision-maker, and generator,\nrespectively. Last, Section VIII discusses the application,\nopportunities and challenges of LLM-enhanced RL. Finally,\nSection IX concludes the survey.\nII. B ACKGROUND\nIn this section, we provide a concise overview of the classi-\ncal RL paradigm and related challenges. Next, we explore the\nprevailing trend in RL—specifically, the fusion of multimodal\ndata sources, including language and visual information. Fol-\nlowing this, we offer an introductory background on LLMs and\noutline the key capabilities that can enhance the RL learning\nparadigm.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 3\nA. Background of Reinforcement Learning\n1) Classical Reinforcement Learning: In a classical RL\nparadigm shown in Fig. 1, the agent interacts with an en-\nvironment through a trial-and-error process to maximize the\nspecified rewards in the trajectory. In each step, the agent\ntakes an action a based on the observed state s from the\nenvironment. By optimizing the policy π (action controller),\nthe agent maximizes the cumulative rewards. Such an opti-\nmization problem is usually formalized through the concept\nof Markov Decision Process (MDP), defined by the quintuple\n⟨S, A, T , R, γ⟩. Here, S denotes a set comprising all possible\nstates, A denotes a set of all possible actions, T represents the\nstate transition probability function T : S × A × S →[0, 1],\nR is a reward function R : S × A × S →R, and γ (with\n0 ≤ γ ≤ 1) is the discount factor. The objective in RL is to\noptimize the policy π(a|s) such that the cumulative returnsP∞\nk=0 γkrk+1 is maximized.\nAgent\nPolicy\ncontroller EnvironmentAction \nObserved state \nReward \nFig. 1. Classical reinforcement learning paradigm.\n2) Challenges of Reinforcement Learning: While RL\nalgorithms have made remarkable performance in recent\nyears [2], [7], [50], there are still several longstanding chal-\nlenges that limit the real-world applicability of RL:\n• Generalization in Unseen Environment: Generalization in\nunseen environments remains a significant challenge in\nthe field of RL [51]. The core issue lies in the ability of\nRL algorithms to transfer learned knowledge or behaviors\nto new, previously unseen environments. RL models are\noften trained in simulated or specific settings, excelling\nin those scenarios but struggling to maintain performance\nwhen faced with novel or dynamic conditions. This\nlimitation hinders the practical application of RL in real-\nworld situations, where environments are rarely static or\nperfectly predictable. Achieving generalization requires\nmodels to not only learn specific task solutions but also\nto understand underlying principles that can be adapted\nto a range of situations.\n• Reward Function Design : Reward function is the princi-\npal contributing factor to the performance of an RL agent.\nDespite their fundamental importance, reward functions\nare known to be notoriously difficult to design, especially\nin contexts involving sparse reward environments and\ncomplex scenarios [1]. In sparse reward settings, where\nfeedback is limited, reward shaping becomes essential\nto guide agents toward meaningful behaviors; however,\nthis introduces the risk of inadvertently biasing the agent\ntowards sub-optimal policies or overfitting to specific\nscenarios [52], [53]. Conversely, for complex tasks, high-\nperformance reward functions usually require massive\nmanual trial-and-error since most designed rewards are\nsub-optimal [23] or lead to unintended behavior [54].\n• Compounding Error in Model-based Planning : Model-\nbased RL is prone to the issue of compounding errors\nduring planning. As the prediction horizon extends, errors\nin the model’s predictions accumulate, leading to signifi-\ncant deviations from optimal trajectories [55], [56]. This\nproblem is particularly acute in complex environments\nwith high-dimensional state spaces. With their advanced\npredictive capabilities and understanding of sequential\ndependencies, LLMs could help mitigate these errors,\nleading to more accurate and reliable planning in model-\nbased RL.\n• Multi-task Learning: Multi-task RL faces several key\nchallenges that limit its effectiveness. One major issue is\nmanaging varying task difficulties, where simpler tasks\ncan overshadow learning of more complex ones, leading\nto negative transfer [57]. Task interference is another\ncritical problem, as shared parameters or data between\ntasks can result in suboptimal performance on individual\ntasks [58]. Determining optimal parameter-sharing strate-\ngies is complex, as it must balance learning efficiency\nwith task-specific requirements [59]. Sample efficiency\nremains a significant hurdle, with traditional data-sharing\napproaches not fully leveraging learned behaviors across\ntasks [60]. Finally, effectively transferring knowledge\nbetween tasks without negative interference is an ongoing\nchallenge that impacts the agent’s ability to accelerate\nlearning across multiple objectives [61].\n3) Multimodal Reinforcement Learning: With the advances\nin both CV and NLP, pattern recognition in vision and natural\nlanguage has become increasingly powerful, and multimodal\ndata has been involved in the RL paradigm recently. Visual\ndata is commonly involved in the observation space of RL\nwhen agents receive image-based information from the envi-\nronment, e.g. in applications such as robots [62], video game\ncontrol [2] etc. Compared to visual data, natural languages\nare usually included when RL agents are given specific tasks\nwhen interacting with the environments. The use of natural\nlanguages in RL can be divided into the following two\ncategories [17]:\n• Language-conditional RL : In language-conditional RL,\nthe problem itself requires the agent to interact with\nthe environment through language. Specifically, there are\ntwo ways to integrate natural language in RL: 1) task\ndescription: the task or instruction is described in natural\nlanguages, e.g. instruction following, where the agents\nlearn to interpret the instructions first and then execute\nactions; 2) action space or observation space : natural\nlanguage is part of the state and action space, e.g.. text\ngames, dialogue systems, and question answering (Q&A).\nThis class of RL leverages natural language as a direct\ncomponent of the RL process, guiding the agent’s actions\nand decisions within the language environment.\n• Language-assisted RL : In language-assisted RL, natural\nlanguage is used to facilitate learning but not as a\npart of problem formulation. Two usages of language-\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 4\nassisted RL are: 1) communicating domain knowledge :\nthe text containing task-related information can be helpful\nfor agents. Therefore, wikis and manuals related to the\nenvironment can potentially assist the agents in such\ncases; 2) structuring policies : structuring policies is to\ncommunicate information about the state or dynamics\nof the environment based on language instead of rep-\nresentations of the environment or models. In such cases,\nlanguage can be leveraged to shape representations to-\nwards a generalizable abstraction, such as using “avoid\nhitting the wall” instead of representations of a policy.\nThis approach represents a more indirect use of natural\nlanguage, serving as a guide or enhancer to the primary\nRL tasks.\nThe integration of multimodal data challenges the RL\nparadigm since the agent has to simultaneously learn how to\nprocess complex multimodal data and optimize the control\npolicy in the environment [16]. Issues such as natural lan-\nguage understanding [63] and visual-based reward function\ndesign [64] require to be addressed.\nB. Background of Large Language Models\nLLMs typically refer to the Transformer-based language\nmodels [65] containing billions of parameters and being\ntrained on massive text data (i.e., several terabytes (TB)\nscale) [66], such as GPT-3 [29] and LLaMA [67]. The ex-\ntensive number of parameters and internet-scale training data\nenable LLMs to master a diverse array of tasks, resulting\nin enhanced capabilities in language generation, knowledge\nrepresentation, and logical reasoning, as well as improved\ngeneralization to novel tasks.\nThe development and effectiveness of LLMs are largely\ndriven by Scaling Laws , i.e., as these models grow in size\n– both in terms of their parameter count and the data they\nare trained on – they tend to exhibit emergent abilities that\nare not present in small models [68], [69], [70], such as\nin-context learning, reasoning, and generalization. Here, we\nbriefly introduce such capabilities of LLMs in detail:\n• In-context Learning: In-context learning capability elim-\ninates the need for explicit model retraining or gradient\nupdate [29], as it can generate better responses or perform\ntasks by inputs cueing examples or related knowledge.\nSpecifically, task-related texts are included in the prompts\nas context information, helping the LLMs to understand\nthe situations and execute instructions.\n• Instruction Following : Leveraging diverse task-specific\ndatasets formatted with natural language descriptions\n(also called instruction tuning ), LLMs are shown to\nperform well on unseen tasks that are also described\nin the form of natural language [71], [72], [73]. There-\nfore, this capability equips LLMs with the ability to\ncomprehend instructions for new tasks and effectively\ngeneralize across tasks not previously encountered, even\nin the absence of explicit examples.\n• Step-by-step Reasoning : For smaller models, tackling\nmulti-step tasks, such as solving math word problems,\noften proves to be challenging. However, large language\nmodels can address the complex task effectively with so-\nphisticated prompting strategies such as Chain of Thought\n(CoT) [74], Tree of Thought (ToT) [75], and Graph\nof Thought (GoT) [76]. These strategies structure the\nproblem-solving process into sequential or hierarchical\nsteps, facilitating a more articulated and understandable\nreasoning pathway. Additionally, prompts designed for\nplanning enable LLMs to output sequences that reflect\na progression of thoughts or actions, proving invaluable\nfor tasks demanding logical sequence or decision-making\noutputs.\nIII. L ARGE LANGUAGE MODEL -ENHANCED\nREINFORCEMENT LEARNING\nA. Definition\nRL agents are often tasked with making robust and de-\nliberate decisions using multimodal information in real-real\napplications, whether in the MDP setting or within the context\nof specific task descriptions. Examples include robots designed\nto follow natural language instructions while navigating phys-\nical environments or visual games with tasks described in\nnatural language [77], [78], [79]. However, it is challenging for\nconventional RL methods as the agent is required to simultane-\nously interpret complex multimodal data and optimize control\npolicies amidst ever-changing environments [80]. Compound-\ning these challenges are issues like sample inefficiency, the\ndifficulty of crafting reward functions that accurately reflect\nmultimodal inputs, and the need for robust generalization\nacross varied tasks and settings.\nThe rapid advancements in LLMs present a viable solution\nto these challenges, thanks to their potent natural language\nunderstanding and reasoning abilities, coupled with recent\nprogress in incorporating visual data processing [81]. This dual\ncapability enables LLMs to interpret and act upon complex\nmultimodal information effectively, serving as a robust helper\nfor enhancing the RL paradigm for real-world applications.\nNevertheless, despite the powerful functionalities of LLMs,\nthe current studies are varied and lack a standard concept\ncorrectly specifying the systematic methodology, which im-\npedes the advancement of research in this area. Therefore, we\nintroduce the concept called LLM-enhanced RL as follows:\nLLM-enhanced RL refers to the methods that utilize the\nmultimodal information processing, generating, reasoning,\nand other high-level cognitive capabilities of pre-trained,\nknowledge-inherent LLM models to assist the RL paradigm.\nLLM-enhanced RL differs from traditional model-based RL\nby leveraging knowledge-rich LLM models. This approach\nprovides two key advantages: First, LLM equips the agent\nwith substantial pre-trained capabilities at the beginning of\nthe learning process, such as reasoning and high-level plan-\nning, etc. Second, it offers superior generalization capabilities.\nPre-trained on diverse data, LLMs can effectively transfer\nknowledge across domains, enabling better adaptation to un-\nseen environments than conventional data-driven models. Last,\nLLM-enhanced RL addresses a key limitation of pre-trained\nmodels: their inability to interact with environments to expand\ntheir knowledge and ground themselves in specific domains.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 5\nInformation Processor\nDecision-maker\nGenerator\nSpeciﬁcation\nLanguage\nExtracted\nRepresentation\nAction-making\nAction\nCandidates\nExpert\nActions\nAction-guiding\nTrajectory\nSequence\nModelling\nPolicy\nModel SimulatorPolicy Interpreter\nInterpretation\nReal\nWorld\nSim.\nWorld\nWorld Model\nActionRewardState\nLLM as\nInformation\nProcessor\nLLM as\nReward\nDesigner\nLLM as\nWorld Model\nSimulator\nLLM as\nDecision-\nmaker\nLLM as\nPolicy\nInterpreter\nEnv\nCode\nGeneration\nReward Designer\nReward Model\nModel\nAgent\nFig. 2. Framework of LLM-enhanced RL in classical Agent-Environment interactions, where LLM plays different roles in enhancing RL.\nThrough environmental interactions, this approach generates\ntask-specific data, grounds the LLM in particular domains\nby in-context learning, and eventually helps them adapt to\ndynamic changes with continuous learning.\nB. Framework\nThe framework of LLM-enhanced RL is illustrated in the\ncenter of Fig. 2, which is founded on the classical agent-\nenvironment interaction paradigm. Along with the trial-and-\nerror learning process, LLM processes the state information,\nredesigns the reward, assists in action selection, and interprets\nthe policy after the action selection.\nSpecifically, on the one hand, when the agent receives the\nstate and reward information from the environment, LLM\nis able to process or modify the information to either filter\nunnecessary natural language-based information or design ap-\npropriate rewards to accelerate the learning process, based on\nthe natural language understanding and reasoning capabilities.\nOn the other hand, when the agent is about to choose an\naction based on the observation, LLM can assist the action\nselection process by either simulating a world model or serving\nas the policy network to generate reasonable actions based\non the modeling capability and common-sense knowledge.\nAdditionally, after the action selection process, integrating\nstate, reward, and action information, LLM can interpret the\nunderlying possible reasons behind the policy selection, which\nhelps human supervisors understand the scenarios for further\nsystem optimization.\nBased on the functions of LLM in the framework, we extract\ncharacteristics of LLM-enhanced RL and further divide four\ndifferent LLM roles in LLM-enhanced RL, including informa-\ntion processor, reward designer, generator, and decision-maker,\nwhich will be elaborated in the next subsections.\nC. Characteristics\nThe LLM-enhanced RL paradigm enhances the vanilla RL\nparadigm with the following characteristics:\n• Multimodal Information Understanding : LLMs en-\nhance RL agents’ comprehension of scenarios involving\nmultimodal information, enabling them to learn from\ntasks or environments described in natural language and\nvision data more effectively.\n• Multi-task Learning and Generalization : Benefiting\nfrom the multi-disciplinary pre-trained knowledge and\npowerful sequence modeling capability, LLMs empower\nRL agents by providing a high-capacity model capable\nof accommodating task variances and transferring knowl-\nedge across multiple tasks, assisting in handling multiple\ntasks.\n• Improved Sample Efficiency : Given the inherent ex-\nploratory nature, the RL paradigm demands significant\nsamples to learn. Pre-trained LLM can enhance data\ngeneration by simulation or leverage the prior knowledge\nto improve the sample efficiency of RL.\n• Long-Horizon Handling: RL becomes more challenging\nas the length of trajectory increases, due to the credit as-\nsignment problem. LLMs can decompose complex tasks\ndown into sub-tasks to assist RL agents in planning\nover longer temporal horizons, aiding in the decision-\nmaking process for complex, multi-step tasks such as the\nMinecraft game.\n• Reward Signal Generation : Based on the context un-\nderstanding and domain knowledge, LLMs contribute to\nthe reward shaping and reward function designing, which\nhelp guide the RL towards effective policy learning in\nsparse-reward environments.\nD. Taxonomy\nIn this subsection, we illustrate the different roles of LLMs\nwithin the above framework, by detailing their functions and\ncorresponding issues of RL they address:\n• Information Processor : When observation or task de-\nscription involves language or visual features, it is chal-\nlenging for the agent to comprehend the complex infor-\nmation and optimize the control policy simultaneously.\nTo release the agent from the burden of understanding\nthe multimodal data, LLM can serve as an information\nprocessor for environment information or task instruction\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 6\ninformation by 1): extracting meaningful feature repre-\nsentations for speeding up network learning; 2) trans-\nlating natural language-based environment information\nor task instruction information into formal specific task\nlanguages to reduce learning complexity.\nApplication example : In instruction-following RL for\nrobots, tasks can have unbounded natural language forms\ndue to users’ diverse speaking habits, which can impede\nRL learning performance. LLMs transform these varied\nnatural language instructions into a unique task language,\nenabling more robust RL performance [49].\n• Reward Designer: In complex task environments where\nthe reward is sparse or a high-performance reward func-\ntion is hard to define, using the prior world knowledge,\nreasoning abilities, and code generation ability, LLM can\nserve as two roles: 1) an implicit reward model to provide\nreward values based on the environment information, ei-\nther by training or prompting; 2) an explicit reward model\nthat generates executable codes of reward functions that\ntransparently specifies the logical calculation process of\nreward scalars based on the environment specifications\nand language-based instructions or goals.\nApplication example: For complex robotic control prob-\nlems, such as dexterous manipulation, reward design\nrequires both expertise and trial and error. LLM designs\nreward functions based on knowledge and iteratively\nimproves it based on the performance [82].\n• Decision-maker: RL faces challenges such as sample\ninefficiency and exploration inefficiency. To address these\nchallenges, LLMs can be leveraged as decision-makers\nin RL, offering promising solutions through two main\napproaches: 1) action-making: LLMs treat offline RL\nas a sequence modeling problem, using rewards for the\nconditional generation of actions. Pretrained on diverse,\ninternet-scale data, LLMs possess advanced semantic\nunderstanding capabilities, which can be exploited to\naccelerate offline RL learning. 2) action-guiding: LLMs\nact as expert instructors to produce a reduced set of\naction candidates or expert actions. The action candidates\nconstrain the original action space, thereby enhancing\nexploration efficiency. Expert actions encapsulate prior\nknowledge from LLMs. When incorporated to regularize\nthe policy learning, this expert knowledge is distilled into\nan RL agent, resulting in better sample efficiency.\nApplication example : In embodied robot, given human-\ninstruction and language-based description, LLM gener-\nates potential actions for the robot to choose from [83].\n• Generator: Model-based RL hinges on precise world\nmodels to learn accurate environment dynamics and simu-\nlate high-fidelity trajectories. Additionally, interpretability\nremains another important issue in RL. Using the mul-\ntimodal information understanding capability and prior\ncommon-sense reasoning ability, LLMs can be 1) a\ngenerator to generate accurate trajectories in model-based\nRL; 2) generate policy explanations with the prompts of\nrelated information in explainable RL. Application exam-\nple: In Minecraft item crafting, LLMs generate Abstract\nWorld Models—hypothesized sequences of subgoals for\na given task. These LLM-generated world models guide\nRL agents’ exploration and learning and the RL agent\nverifies and corrects the world model through gameplay,\ncombining LLM knowledge with grounded experience to\nachieve an order of magnitude improvement in sample\nefficiency over traditional methods [84].\nIV. LLM AS INFORMATION PROCESSOR\nThe normal way for deep RL with language or visual\ninformation is to jointly process the information and learn\na control policy, end-to-end. However, this demands the RL\nagent to learn to comprehend the information and manage\nthe task simultaneously. Additionally, learning the language\nor visual features by simply relying on the reward function is\nchallenging and may narrow the learned features to a narrow\nutility, hampering the generalization ability of the agent [16].\nWith the advances in unsupervised techniques and large-\nscale pre-trained models for CV and NLP, the decoupled\nstructure, where the encoders are separately trained, has gained\npopularity [16], [85], [86]. Utilizing the powerful represen-\ntation ability and prior knowledge, the pre-trained LLM or\nvision-language model (VLM) model can serve as an informa-\ntion processor for RL. They can extract observation represen-\ntations for downstream networks or translate natural language\ninto formal specifications, enabling the execution of multiple\ntasks. This multi-task capability improves sample efficiency\nand zero-shot performance, allowing agents to generalize\neffectively across diverse and sparse-reward environments.\nA. Feature Representation Extractor\nAdopting the large pre-trained models in CV and NLP, the\nlearned feature representation can be a scaffold embedding\nfor downstream network learning and increase the sample\nefficiency. As illustrated in Fig. 3 (i), the usages can be further\ndivided into two categories according to whether the model is\ntrained simultaneously. One way is to directly use the frozen\npre-trained model to extract embeddings from the observation\nOt and another way is to further fine-tune the pre-trained\nmodel using contrastive learning with a contrastive loss Lc\nt\nto achieve better adaptation in new environments.\n1) Frozen Pre-trained Model: Using the frozen large-scale\npre-trained model is the straightforward way. In reference [87],\nthe author proposed History Compression via Language Mod-\nels (HELM) to utilize a frozen pre-trained Language Trans-\nformer to extract history representation and compression and\nthus addresses the problem of partially observed MDP by\napproximating the underlying MDP with the past representa-\ntion. Specifically, the framework first uses FrozenHopfield, a\nfrozen associative memory to map observations[ot−2, ot−1, ot]\nto a compressed representation ht and then concatenate it\nwith a learned encoding of the current observation via a\nconvolutional neural network. Such a method solves the prob-\nlem of how to effectively utilize the compressed history for\npolicy optimization. After that, Semantic HELM [88] proposed\na human-readable memory mechanism that summarizes past\nvisual observations in human language and uses multimodal\nmodels to associate visual inputs with language tokens. The\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 7\nActor-Critic\nNetwork\n(i) Feature Representation Extractor\nLLM / VLM\nEncoder\n(ii) Language Translator\nOpen the door.\n[0 1 0]\nInstruction Information Translator\ndirectly exposed to NL\ntask language translation\nEnvironment Information Translator\nYou'll die if hitting a\nwall. \nEffect HittingWall:\n  if Near(wall) and a == walk:\n    R <= -1\n    S <= S_initial\n    t = 0 ...\nTransition State\nPolicies Reward\nMDP translator\nFine-tune\naccording to the task \nInvariant\nFeature\nFig. 3. LLM as an information processor. (i) Feature Representation Extractor:\nfrozen/fine-tuned LLM extracts meaningful representations for downstream\nRL networks. In the fine-tuning process, given observation ( Ot), invariant\nfeature abstraction ( ˜St) is learned with the contrastive loss ( Lc\nt). Then,\nthe invariant is fed into the actor-critic network. After fine-tuning, given\ndifferent observations (Ot) and ( O′\nt) with appearance variation, the extracted\nrepresentation is invariant, leading to robust RL performance. (ii) Language\nTranslator: LLM interprets diverse natural language inputs, converting them\ninto a standardized, task-specific format that the RL agent can efficiently\nprocess and act upon.\nmemory is a semantic database S constructed by encoding\nprompt-augmented tokens from the vocabularies of Contrastive\nLanguage-Image Pre-training (CLIP) [89] and the pre-trained\nlanguage models. Given an observation ot, the agent retrieves\ntop-k embeddings from S as actor-critic input to assist the\npolicy optimization. Such a memory mechanism provides a\nhuman-readable representation of the past and helps the agent\nto cope with partially observable environments. In the exper-\niment, they used the proximal policy optimization (PPO) [3]\nalgorithm and pretrained Transformer XL [90] model. When\ntesting on the partially observable environments, they found\nthe extracted semantics help the memory-less agent obtain\ncomparable scores with memory-based methods trained on\nlong trajectories. However, one limitation of such frozen pre-\ntrained models is that the representations cannot dynamically\nadjust according to the task and environment.\n2) Fine-tuning Pre-trained Model: When trained RL agents\nare deployed in real-world applications, their performance\noften deteriorates under significant appearance variations (out-\nof-distribution data) due to overfitting to training scenarios. For\nexample, robots with vision-based navigation tasks may fail\nwhen the environment color changes. Invariant feature repre-\nsentations serve as a form of state abstraction that remains con-\nsistent across out-of-distribution appearance variations such\nas added noise, brightness changes, or slight rotations. When\nencountering appearance changes or out-of-distribution data,\nthough the observation changed, the representation (feature\nembedding) that fed into the policy/value network is nearly\nunchanged, leading to robust RL performance and increased\ngeneralization in unseen environments.\nContrastive learning is a common way to learn the invariant\nfeature representation. It learns representations from high-\ndimensional data by contrasting positive examples against\nnegatives. Given a query q, the goal is to match query q\nmore closely to a positive key k+ than to any negative keys\nK\\{k+} in a set K. This process is modeled using similarity\nmeasures, such as the dot product (qT k) or the bilinear\nproduct (qT W k), where W is a weight matrix. To effectively\nlearn these representations, a contrastive loss function such as\nInfoNCE [91] is used:\nLc = log exp(qT W k+)\nexp(qT W k+) +PK−1\nk exp(qT W ki)\n(1)\nwhere exp is the exponential symbol and the whole Lc can\nbe viewed as the log-loss of a softmax classifier, treating the\nmatching of q to k+ as a multi-class classification problem\namong K classes. By maximizing alignment between different\nchanges of the same observation via the above loss, the model\ncan learn the invariant representations.\nWhen combining with RL, given different RL tasks, the re-\nquired invariant feature representations should be adjusted ac-\ncordingly. Therefore, researchers have explored different ways\nto improve contrastive learning. In reference [92], to achieve\nthe zero-shot capability of embodied agents, the author devised\na visual prompt-based contrastive learning framework that uses\na pre-trained VLM to learn the visual state representations.\nThe visual prompts are learned on expert demonstrations from\ndomain factors such as camera settings and stride length. By\ncontrastively training the VLM on a pool of visual prompts\nalong with the RL policy learning process, the learned repre-\nsentations are robust to the variations of environments, leading\nan increase of 18-20% success rates on unseen scenarios and\nimproved generalization capability. Based on the contrastive\nlearning, another method ReCoRe [93] added an intervention-\ninvariant regularizer in the form of an auxiliary task such as\ndepth prediction and image denoising to explicitly enforce\ninvariance of learned representations to environment changes.\nB. Language Translator\nThe unbounded and diverse representation of natural lan-\nguages in both human instruction and environmental infor-\nmation impedes policy learning. LLM can be leveraged as\na language translator to reduce the additional burden of\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 8\ncomprehending natural language for RL agents and increase\nsample efficiency. As illustrated in Fig. 3 (ii), LLM transforms\nthe diverse and informal natural language information into\nformal task-specific information, such as feature representation\nor task-specific languages, thus assisting the learning process\nof the RL agent.\n1) Instruction Information Translation: One application\nof LLM is to translate natural language-based instructions\nfor instruction-following applications. In reference [49], the\nauthor investigated an inside-out scheme for natural language-\nconditioned RL by training an LLM that translates the natural\nlanguage to a task-related unique language. Such an inside-\nout scheme prevents the policy from being directly exposed to\nnatural language instructions and helps efficient policy learn-\ning. Another literature STARLING [94] used LLM to translate\nnatural language-based instructions to game information and\nexample game metadata. The translated information is then\nfed forward to Inform7, an interactive fiction game engine, to\ndevelop a large amount of text-based games for RL agents to\nmaster the desired skills.\n2) Environment Information Translation: On the other\nhand, LLM can also be used to translate the natural language\nenvironment information into formal domain-specific language\nthat can specify MDP information, which converts natural\nlanguage sentences into grounded usable knowledge for the\nagent. Previous works generally ground natural language into\nindividual task components such as task objectives descrip-\ntion [95], rewards [96] and polices [97], [98]. To unify the\ninformation about all the components of a task, [99] introduces\nRLang, a grounded formal language capable of expressing\ninformation about every element of an MDP and solutions such\nas policies, plans, reward functions and transition functions.\nBy using LLM to translate natural language to RLang and train\nRL agents upon RLang, the agents are capable of leveraging\ninformation and avoid having to learn tabula rasa.\nC. Summarization and Outlook\nFor information processing, LLM is used to accelerate RL\nlearning processing by decoupling the information processing\ntask and the controlling task, where LLM extracts feature\nrepresentations or handles the natural language-based infor-\nmation.\nWhen multimodal data are involved in the environment,\ne.g., robot manipulation tasks, the information processing task\nbecomes more challenging. For one thing, the misalignment\nor contradiction between different modalities may exist [100].\nModality weighting techniques that adaptively learn the impor-\ntance of different modalities by attention mechanisms provide\na potential solution for this problem [101]. In addition, CLIP-\nbased multimodal foundation models using large-scale image-\ntext pairs have shown outstanding zero-shot ability in various\nmultimodal tasks [89]. By learning cross-modal and task\nsemantics, the misaligned modality can be replaced with a\nvirtually generated modality [102]. For another, how to effec-\ntively combine the information between different modalities\ninto a unified representation, i.e., multimodal fusion, is also an\nimportant issue. In reference [103], a multimodal contrastive\nlearning with attention mechanisms, which learns the intra\nand inter-modal representations, was proposed. The different\nattention heads align the agreement from one modality to\nanother and vice versa, providing an informative representation\nfor the RL agent. However, most multimodal learning methods\ndo not consider the task information and only focus on the\nmodality alignment, remaining an area to be explored.\nIn the following, we list potential directions for future\nresearch.\n• Feature Representation Extractor : Although the use of\nLLMs as feature representation extractors has shown\npromise in enhancing RL, several challenges persist in\nfuture research. Short-term goals include developing a\ncomputationally efficient feature extractor and improving\nthe generalization of LLM-derived representations. In the\nlong term, researchers should focus on exploiting task\ncompositionality for better generalization and creating\nadaptive extraction methods for diverse control tasks.\n• Language Translator : Current existing works are still\nlimited. Short-term objectives involve exploring LLMs’\nability to handle more tasks and improving translation\nefficiency and accuracy in RL contexts. Long-term goals\ninclude developing multimodal translation capabilities\nand integrating these with RL algorithms, achieving a\nmore general translator, and helping agent learning.\nV. LLM AS REWARD DESIGNER\nThe reward signal is the most important information to\ninstruct agent learning in RL [1]. However, despite the fun-\ndamental importance, high-performing reward functions are\nknown to be notoriously difficult to design [104]. First,\nspecifying human notions of desired behavior is difficult via\ndesigned reward functions or requires huge expert demon-\nstrations. Moreover, dense rewards that accurately provide\nlearning signals require either manually decomposing the\ngeneral goal into sub-goals [105] or rewarding interesting\nauxiliary objectives [106]. Nevertheless, both of these methods\nsuffer from the need for expert input and meticulous manual\ncrafting [23].\nBenefiting from pre-trained common-sense knowledge, code\ngeneration, and in-context learning ability, LLM has the poten-\ntial to design or shape reward functions for DRL by leveraging\nnatural language-based instructions and environment informa-\ntion. In this section, we review recent literature in which LLMs\nact as reward models that implicitly provide reward values or\nexplicitly write executable reward function codes detailing the\ncalculation process of reward scalars.\nA. Implicit Reward Model\nA large pre-trained model can be an implicit reward model\nthat directly provides auxiliary or overall reward value based\non the understanding of task objectives and observations. The\nmethods are illustrated in Fig. 4 (i). One way is by directly\nprompting with language descriptions and another way is by\nscoring the alignment between the feature representation of\nthe visual observations and language-based instructions.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 9\n(i) Implicit Reward Model\nLanguage\nVisual\nSimilarityAlignment Score\nPrompting\nDesigned\nReward\nDirectly Prompting Alignment Scoring\ndef calc_reward(obj, s,\na):\n    reward = ...\nclass GridEnv:\n  def calc_obs(self):\n    self.vol = ...\n    self.current = ...\n    self.num_bus = ...\n    self.load = ...\n    ...\nEvaluation\nSelf-reﬁne Loop\nLLM Function\nGenerator\n(ii) Explicit Reward Model\nFig. 4. LLM as a reward designer. (i) Implicit Reward Model: LLMs\nprovide rewards through direct prompting or alignment scoring between\nlanguage instructions and visual observations. (ii) Explicit Reward Model:\nLLMs generate executable code for reward functions, with potential for self-\nrefinement through evaluation loops.\n1) Direct Prompting: Reference [107] simplified the man-\nual reward design by prompting an LLM as a proxy reward\nfunction with examples of desirable behaviors and the pref-\nerences description of the desired behaviors. Reference [108]\nproposed a Read and Reward framework that utilizes LLMs\nto read instruction manuals to boost the learning policies\nof specific tasks. The framework includes a Question &\nAnswer (QA) extraction module for information retrieval\nand summarization and a Reasoning module for evaluation.\nExperimentally, they show RL algorithms, by their design, can\nobtain significant improvement in performance and training\nspeed. In reference [45], Carta et al. proposed an automated\nreward shaping method where the agent extracts auxiliary\nobjectives from the general language goal. Using a question\ngeneration and QA system, the framework guides the agent\nin reconstructing partial information about the global goal\nand provides an intrinsic reward signal for the agent. This\nintrinsic reward incentivizes the agent to produce trajectories\nthat help them reconstruct the partial information about the\ngeneral language goal. To acquire a generalizable policy by\ncontinually learning a set of tasks is challenging for RL agents\nsince the agent is required to retain the previous knowledge\nwhile quickly adapting to new tasks. Reference [109] intro-\nduced the Lafite-RL (Language agent feedback interactive\nRL) framework that provides interactive rewards mimicking\nhuman feedback based on LLMs’ real-time understanding of\nthe agent’s behavior. By designing two prompts, one to let\nLLM understand the scenario and the other to instruct it about\nthe evaluation criterion, their framework can accelerate the RL\nprocess while freeing up human effort during the interaction\nbetween agent and environment.\nAlgorithm 1 Language Reward Modulated Pretraining\n(LAMP) in [110]\n1: Initialize parameters for Masked World Models (MWM)\n2: Load pretrained DistilBERT [111] for language prepro-\ncessing\n3: Load pretrained R3M visual encoder and score predictor\n4: Initialize empty replay buffer\n5: Populate language prompt buffer and synonym buffers\nwith predefined samples\n6: for each training episode do\n7: Randomize scene textures from Ego4D and RLBench\ndatasets\n8: Sample ShapeNet objects and language prompts\n9: Insert ShapeNet objects into the scene\n10: Generate and process language embeddings using Dis-\ntilBERT\n11: Execute policy to collect transitions\n12: Assign LAMP rewards using R3M score predictos\n13: Update buffers and train MWM with augmented re-\nwards\n14: end for\n2) Alignment Scoring: For visual RL, some literature uti-\nlizes vision-language models as reward models to align multi-\nmodal data and calculate the similarity score using metrics\nsuch as cosine similarity. Rocamonde et al. employs the\nCLIP model as a zero-shot reward model to specify tasks via\nnatural language [64]. They first compute the probability pot,l\nthat the agent achieves a goal given by language description\nl out of a set of potential goals l′ ∈ L in the task set\nL using the softmax computation with temperature τ over\nthe cosine similarity between visual state embeddings fθ(ot)\nand language description embeddings gθ(l) across the set of\npotential goals l′:\npot,l = exp(fθ(ot) · gϕ(l)/τ)P\nl′ exp(fθ(ot) · gϕ(l′)/τ). (2)\nThen the reward is obtained by a binary reward function rt =\nr(ot+1, l) =I[pot+1,l ≥ β], which thresholding the probability.\nTheir framework only requires a single-sentence text prompt\ndescription of the desired task with minimal prompt engineer-\ning. Another work [112] constructed reward signals based on\nthe similarity between natural language-based description and\nembeddings from the pre-trained VLM encoder. By labeling\nthe expert demonstration with the reward signals, the frame-\nwork effectively mitigates the problem of mis-generalization.\nIn reference [110], the authors proposed the Language Reward\nModulated Pretraining (LAMP) framework as a pertaining\nutility for RL as opposed to a downstream task reward to\nwarm-start sample-efficient learning. The framework leverages\nfrozen, pre-trained VLMs such as R3M [113] to generate\nnoisy, albeit shaped exploration rewards by computing the\nalignment score between instructions and image observations.\nThe algorithm is presented in Algorithm 1. They used Masked\nWorld Model [114], a visual model-based RL algorithm for\nrobot manipulation based on image and instructions. The\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 10\nimages were downloaded from Ego4D [115] and the language\ninstructions were obtained by querying ChatGPT. The reward\nis then calculated from the R3M alignment score. After that,\nthe generated rewards are optimized with standard novelty-\nseeking exploration rewards for language-conditioned policy.\nReference [116] explored preference-based RL, where the\nagent learns a reward function from preference labels over\nthe behaviors. A VLM is leveraged to generate preference\nlabels given visual observations and a text description of the\ntask goal. The evaluation is based on a series of vision-based\nmanipulation tasks. Results suggested that prompting VLMs\nto produce preference labels for reward learning leads to better\nperformance, in contrast to treating them as reward functions\nto produce raw reward scores.\nB. Explicit Reward Model\nAnother way to design reward functions is by generating\nexecutable codes that explicitly specify the details of the cal-\nculation process, as illustrated in Fig. 4 (ii). Compared to the\nimplicit reward value provision, this explicit way transparently\nreflects the reasoning and logical process of LLMs and thus\nis readable for humans to further evaluate and optimize.\nTo help robots learn low-level actions, reference [117]\nharnessed the code generation ability of LLMs to define the\nlower-level reward parameters based on high-level instructions.\nUsing such a reward design paradigm, their work bridges\nthe gap between high-level language instructions to low-level\nrobot actions and can reliably tackle 90% of the designed\ntasks compared to the 50% of the baseline. Motivated by the\ncapability of LLM for self-refinement [118], reference [119]\nproposed a framework with a self-refinement mechanism for\nautomated reward function design, including initial design,\nevaluation and self-refinement loop. Their results indicate that\nthe LLM-designed reward functions are able to rival or surpass\nmanually designed reward functions. Similarly, Eureka [82]\ndeveloped a reward optimization algorithm with self-reflection.\nThe algorithm is outlined in Algorithm 2. In each iteration,\nit uses an environment source code and task description to\nsample different reward function candidates from a coding\nLLM. Then the candidates are used to instruct RL training.\nAfter training, the results are used to calculate the scores of the\nreward candidates. Then the best reward function code are se-\nlected for reflection, where LLM uses the reasoning capability\nto progressively improve the reward code. In the experiment,\nresults show that their proposed method can achieve human-\nlevel performance on reward design and solve dexterous\nmanipulation tasks that were previously infeasible by manual\nreward engineering. Another work Text2Reward [120] gener-\nated shaped dense reward functions as executable programs\nbased on the environment description. Given the sensitivity of\nRL training and the ambiguity of language, the RL policy may\nfail to achieve the goal. Text2Reward addresses the problem\nby executing the learned policy in the environment, requesting\nhuman feedback and refining the reward accordingly.\nC. Summarization and Outlook\nThe use of LLMs as reward designers in RL offers a more\nnatural and efficient way to design complex reward functions.\nAlgorithm 2 Eureka [82]\nRequire: Task description l, environment code C, coding\nLLM Mc, evaluation function E, initial prompt prompt,\noptimization iterations N, iteration batch size K\n1: for i ← 1 to N do\n2: // Sample K reward code candidates\nfrom coding LLM Mc\n3: R1, . . . , RK ← sample(l, Mc, C,prompt)\n4: // Evaluate reward candidates\n5: si\n1 = E(Ri\n1), si\n2 = E(Ri\n2), . . . , si\nK = E(Ri\nK)\n6: // Select the best reward code\n7: best = arg maxk(si\n1, . . . , si\nK)\n8: // Reward reflection\n9: prompt← prompt:Reflection(Ri\nbest, si\nbest)\n10: // Optimize Eureka reward code\n11: if si\nbest > sEureka then\n12: REureka, sEureka ← (Ri\nbest, si\nbest)\n13: end if\n14: end for\n15: return REureka\nBy leveraging natural language processing capabilities, LLMs\nsimplify the traditionally challenging task of reward function\ndesign, enhancing both the efficiency and effectiveness of RL\nalgorithms.\nHowever, the inherent biases in LLMs may transfer to the\ndesigned reward functions, potentially resulting in suboptimal\nor harmful behaviors [121]. This presents a potential risk re-\nquiring careful consideration. While existing mitigation efforts\nprimarily address biases in demographic, cultural, and political\nbeliefs [122], task-specific biases remain understudied, thus\nlimiting the applicability of LLM-designed reward functions.\nToward this end, we list some potential solutions. First, in\npreference-based RL, when an RL agent optimizes against\nbiased reward models generated by LLMs to predict human\npreferences, overoptimization and overfitting may occur [123],\nimpeding the learning of the true reward function. Reward\nfunction regularization [124] includes the agent preference\ngenerated by the value function as a regularization term to\nmitigate the risk, which helps recover the true underlying\nreward function. Secondly, human-in-the-loop approaches are\nanother direction to prevent harmful behaviors from designed\nreward functions. One viable solution is to design an evalu-\nation module where humans can intervene and correct unde-\nsired behaviors and reward functions when the agent’s action\nviolates a predefined set of rules. Finally, principles from\nensemble learning suggest that combining the reward functions\nfrom different LLM models mitigates bias from individual\nLLMs, leading to improved unbiased performance compared\nto a single LLM [125].\nIn the future, we expect advancements in the following\nareas:\n• Implicit Reward Model : An immediate focus may con-\nsider improving how well LLM-generated rewards align\nwith human intentions. This involves refining the qual-\nity of language instructions to reduce ambiguities and\ninaccuracies, ensuring that the reward functions align\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 11\nprecisely with human notions of desired behavior. In the\nlonger term, generalization and transferability of LLM-\ngenerated rewards across different tasks and environ-\nments, especially in complex, high-dimensional visual\nenvironments, is also an important direction to explore.\n• Explicit Reward Model : A key limitation in reward code\ngeneration is its dependency on pre-trained common-\nsense knowledge, which can be restrictive for highly\nspecialized tasks not covered in training data. Therefore,\nshort-term goals may include enhancing prompts with de-\ntailed, task-specific information and external knowledge.\nAdditionally, the reliance on manually designed templates\nfor motion descriptions limits the adaptability. Looking\nfurther ahead, researchers might develop automated or\nunified processes for designing templates, moving beyond\nthe current limitations of manual motion description\ntemplates.\nVI. LLM AS DECISION -MAKER\nLLMs trained on a massive amount of data show impres-\nsive results in language understanding tasks [29], instruction-\nfollowing [126], vision-language navigation [127] and tasks\nrequiring planning and sequential reasoning [83]. Such success\nmotivates researchers to explore the potential of LLMs for\ndecision-making problems. In this section, we divide the role\nof LLM as 1) the action-maker that generates actions; and 2)\nthe action-guider that instructs the actions.\n(i) Action-making\nPretrained LLM as General Scoffold\nTask-Speciﬁc Model\nNext Action\nTrajectory Goal / Instruction\nPretrained LLM\nEnv & Trajectory\nHistory\nAction\nCandidates\nCritic Network\nEnv\nInst.\nAgent\nReplay\nUpdate Policy\n(ii) Action-guiding\nAction Candidates Expert Actions\nFig. 5. LLM as a decision-maker. (i) Action-Making: given a T-length\ntrajectory τ = ( ˆR1, s1, a1, . . . ,ˆRT , sT , aT ) as a sequence of ordered\nreturn-to-go ˆR, action a, and states s, LLM learns to predict future action\na′\nt by minimizing the mean squared error loss L = P\nt ∥at − a′\nt∥2\n2. (ii)\nAction-Guiding: LLM generates a reduced set of action candidates for agents\nor generates expert actions to regularize RL learning.\nA. Action-Making\nTransformer-based models such as Decision Transformer\n(DT) [128] have shown great potential in offline RL domain.\nInstead of using the traditional trial-and-error way, these mod-\nels treat offline RL as a sequence modeling problem, yielding\npromising results. As LLM itself is a large-scale Transformer-\nbased model, a natural thought is to leverage the pre-trained\npower of LLM within this paradigm.\nWe term this function of LLM as action-making, and\nidentify two typical approaches, as illustrated in Fig. 5 (i).\nIn the first approach (left figure), pre-trained LLM is fine-\ntuned and then employed for action generation. In the second\napproach (right figure), goal/instruction along with trajectory\nare fed to pre-trained LLM. Moreover, a task-specific smaller\nmodel is appended after the fine-tuned LLM to facilitate rapid\nadaptation to diverse tasks.\nPre-trained LLMs outperform basic DT in generalization\nand sample efficiency, especially for sparse-reward and long-\nhorizon tasks. LLMs’ latent representations, learned from di-\nverse linguistic data, provide valuable prior knowledge for new\ntasks. This knowledge enables LLMs to solve unseen tasks\nwith less training data, by transferring knowledge from similar\ntasks and predicting high-reward actions even with sparse\nfeedback. For instance, comparing pre-trained LLM with basic\nDT, Li et al. [129] reported a 43.6% improvement in out-of-\ndistribution (novel) task completion rates while requiring less\ntraining data (e.g., 500 vs 10K). Additionally, Shi et al. [130]\ndemonstrated a 50% performance gain in sparse-reward envi-\nronments like Kitchen and Reacher2d. For long-horizon tasks,\npre-trained representations encode future information, guiding\ndecision-making over extended sequences. For instance, in\nAntMaze, a long-horizon navigation environment, pre-trained\nrepresentations yield five times higher scores compared to non-\npre-trained counterparts [131].\nSeveral studies have further explored the application of\nLLMs in offline RL, demonstrating their versatility and effec-\ntiveness across various tasks and benchmarks. Reference [132]\ninvestigated the transferability of general language models on\nspecific RL tasks. Fined-tuned on offline RL tasks (control,\ngames), these general language models outperform Decision\nTransformer and reduce training time by 3-6x on D4RL\nbenchmark [133]. Reference [129] used pre-trained LLM as a\ngeneral scaffold for task-specific model learning, where goals\nwere added along with observations as the input for LLM.\nResults on embodied decision-making tasks demonstrate that\ntheir proposed method outperforms others with less training\ndata, especially when generalizing to novel tasks. In addition,\nthey found representations in pre-trained language models\ncan aid learning and generalization even outside of language.\nTo unify language reasoning with actions in a single policy,\nreference [134] generated textual captions interleaved with\nactions when training the Transformer-based policy. Results\nshow that by using captions describing the next subgoals,\nthe reasoning policy can consistently outperform the caption-\nfree baseline. For scenarios where data collection is costly\nand risky, reference [130] proposed a general framework to\neffectively use pre-trained LLM for offline RL. The pre-\ntrained LLM are based on DT. To combine the pre-trained\nknowledge and task-related domain knowledge, they fine-\ntuned pre-trained LLM with Low-Rank Adaptation (LoRA)\nmethod. The architecture of Transformer is based on GPT-\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 12\n2 model with 12 layers and 12 attention heads. To fine-tune\nthe LLM, they obtained the trajectory data from the D4RL\ndataset. Results show their method achieves state-of-the-art\nperformance in sparse-reward tasks with limited data samples.\nTo integrate multimodal data, e.g., vision and language, into\nthe offline RL, reference [135] co-fine-tuned vision-language\nmodels on both robotic trajectory data and Internet-scale\nvision-language tasks, e.g., visual question answering. In the\nframework, they incorporate the actions as natural language\ntokens and co-fine-tune models on both vision and language\ndatasets. Results show that such co-fine-tune methods can\nincrease generalization performance and the chain of thought\nreasoning can help the agent perform multi-stage semantic\nreasoning and solve complex tasks.\nB. Action-Guiding\nThe action-guider role is illustrated in Fig. 5 (ii). As\nan action-guider, LLM guides the action selection by either\ngenerating reasonable action candidates or expert actions.\nBy instructing the action selection, LLM improves sample\nefficiency and exploration efficiency posed by enormous action\nspaces and natural language.\n1) Action Candidates: In environments such as text-based\ngames, action spaces are large and only a tiny fraction of\nactions are accessible. Although RL agents can learn through\nextensive trials, they often face exploration efficiency issues,\nespecially in multi-task settings where agents must manage\nvarious tasks simultaneously. LLMs address this challenge\nby generating a reduced set of action candidates based on\ntask understanding. These candidates are likely to yield high\nrewards and are applicable across multiple tasks, enhancing\nexploration efficiency and reducing the need for ineffective\nexploration. Reference [136] trained a GPT-2 model to gener-\nate the candidates. To maximize long-term rewards, another\nneural network is used to calculate the Q-values of these\ncandidates. When tested in 28 man-made text games from\nJericho framework [137], they found the method excludes\nnon-useful actions, speeds up the exploration and consistently\nachieves higher scores by more than 20%. Following this work,\nanother study [83] proposed the SayCan framework, where\ninstruction-following robots are integrated with an embodied\nLLM to understand tasks. When receiving instructions, the\nembodied LLM generates a high-level step-by-step plan. When\nacting, LLM produces action candidates based on task prompts\nand then the candidate with the largest critic value is executed.\nBeing evaluated in an office kitchen with real-world robotic\ntasks from 101 instructions, their method can complete long-\nhorizon, abstract, natural language instructions on a mobile\nmanipulator.\n2) Expert Actions: Traditional RL agents cannot converge\nto desirable equilibrium in human-AI collaboration or learn\nefficiently for complex tasks, due to the lack of expert\ndemonstrations. With the understanding of human behavior\nand general knowledge, LLM solves the issues by producing\nhigh-quality expert actions to regularize RL agents. In human-\nAI collaboration, instructRL [138] used LLM to generate prior\npolicy based on human instructions and uses this prior to\nregularizing the RL objective. Specifically, instructRL aug-\nments the policy update function with an auxiliary term\npLLM[lang(at)|lang(τi\nt ), inst], the probability of choosing an\naction based on the trajectory and instructions. Experiments\nshow instructRL converges to policies aligned with human\npreferences. Similarly, to address the sample inefficiency issue\nof RL, Zhou et al. [139] included the policy difference between\nthe student model and LLM-based teacher into RL learning\nloss. Their experiments on simulation platforms demonstrate\nthat the method reduced training iterations by a factor of 1\nto 9. In reference [140], LLM was leveraged to generate a\nhigh-level expert motion plan of robotics tasks, guiding RL\npolicies to efficiently solve robotics control tasks. The high-\nlevel language plan breaks long-horizon tasks into stages to\nexecute. Then, a single RL policy was trained across all states\nand stepped through the language plan. Their results show the\nproposed method solves long-horizon tasks from raw visual\ninput spanning different benchmarks at success rates of over\n85%, out-performing classical, language-based, and end-to-\nend approaches.\nC. Summarization and Outlook\nSample inefficiency and exploration inefficiency remain\nlong-standing challenges for deep RL, particularly in envi-\nronments with sparse rewards or where data collection is\nexpensive or risky. LLM provides three ways to solve the\nproblems. First, LLM as action-makers treat RL as a condi-\ntional sequence modeling problem. The supervised way fine-\ntunes pre-trained LLMs to predict future actions. Benefiting\nfrom learned prior knowledge, LLM can perform well even\nin out-of-distribution, sparse-reward, and long-horizon tasks.\nSecond, LLM as action-guiders generates potential action\ncandidates for RL. With task comprehension, these action\ncandidates promote agents to explore potentially high task-\nvalue states, thus increasing exploration efficiency. Last, LLM\ngenerates expert actions to help RL learn from demonstrations.\nBy incorporating demonstrations from LLM, and RL learns\nspecific prior knowledge and improves sample efficiency.\nSafety issues are another important topic when using LLMs\nas decision-makers in RL, particularly for costly and risky\ntasks [141]. For action-making, DT-based offline RL learns\nthe optimal policy from pre-collected datasets. Recently, inte-\ngrating safety constraints has been explored by some works\nusing methods such as pessimistic estimations [142] and\nstationary distribution correction [143]. These methods set a\nconstant constraint threshold before training. To dynamically\nadjust the threshold during deployment, a constrained deci-\nsion transformer that dynamically relabels the reward based\non safety-reward trade-offs was proposed [144]. For action-\nguiding, LLMs are viewed as instructors to provide expert\nactions. Ensuring the safety of actions generated by LLMs\ndiffers from that in action-making. Leveraging ideas from\nLLM-based agent research, we propose several potential solu-\ntions. First, LLMs can be equipped with testing modules that\nexecute code to evaluate action safety and feasibility [145].\nSecond, implementing a carefully designed human-in-the-\nloop framework enables safety intervention through human\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 13\noversight [146]. Finally, developing memory modules with\nreasoning mechanisms that adaptively learn safety boundaries\nfrom past experiences provides a continual learning-based\napproach to ensuring long-term safety [147].\nIn the following, we list the challenges and future directions\nof the two roles as below:\n• Action-making: Directly employing pre-trained large-\nscale LLM to generate actions demands huge computa-\ntional resources and huge data for fine-tuning it in specific\ntasks. In the short term, future work may consider more\ncost-effective methods such as the Low-Rank Adaptation\n(LoRA) [148] to exploit the power of LLM in direct\ndecision-making. Long-term goals involve developing\ninnovative techniques to efficiently exploit the power\nof LLMs in direct decision-making, potentially creating\nhybrid models that combine the strengths of LLMs with\nmore lightweight, task-specific architectures.\n• Action-guiding: Since the LLM acts as an instructor\nto provide expert actions, the bias and limitations are\nalso inherited by the agent. In addition, LLM itself\ncannot inherently interrogate or intervene in the environ-\nment, which limits LLMs’ capabilities to intentionally\naggregate information. Short-term goal is to address\nthe inherited biases and limitations when LLMs act as\ninstructors providing expert actions, focusing on meth-\nods to filter or correct biased information. In the long\nterm, it is crucial to develop mechanisms for LLMs to\nactively interrogate and intervene in the environment,\nenabling them to intentionally aggregate information and\nimprove their capabilities through real-world interactions.\nTherefore, how to use the information gained from real-\nworld interactions to improve the LLM itself in terms of\nactuality and reasoning is another important problem.\nVII. LLM AS GENERATOR\nThe generative capability of LLMs can be applied to en-\nvironmental simulation and behavior explanation. On the one\nhand, growing interests in applying RL to the real world and\nrisky data-collection process suggests a need for imaginary\nrollouts [149], [150]. Possessed with a powerful modeling\ncapability and world knowledge, LLMs can serve as a world\nmodel simulator to learn complex environmental dynamics\nwith high fidelity by iteratively predicting the next state and\nreward, thus increasing the sample efficiency in model-based\nRL [151], [152]. On the other hand, in RL, interpretability\nremains an important security issue in current black-box AI\nsystems as they are increasingly being deployed to help end-\nusers with everyday tasks. Explanations of policy can improve\nthe end-user understanding of the agent’s decision-making and\ninform the reward function design for agent learning. In such\naspects, LLM can act as a policy interpreter based on their\nknowledge and reasoning ability. In this section, we classify\nthe above two roles of LLM as a generator and review recent\nrelated works.\nA. World Model Simulator\nServing as a world model simulator, LLM is trained as a\n1) trajectory rolloutor, which auto-regressively generates ac-\nCollect\nAcquire\nReal\nWorld\nModelGuide\nReal Data\nKnowledge\nRender\nDynamics\nReconstruction\nEnhance\nSim.\nWorld\nTrajectory \nRollout\nPolicy\nLearning\nWorld Model\nSimulator\n(i) World Model Simulator\nGuideState\nAction\nHistory\nPolicy\nInterpreter Interpretation\nPrompts\n(ii) Policy Interpreter\nFurther Prompts\nFig. 6. LLM as a generator. (i) World Model Simulator: LLM uses real-\nworld data and knowledge to model dynamics, generate simulated worlds, and\nassist policy learning. (ii) Policy Interpreter: LLM generates interpretations of\nagent behavior based on state-action history and prompts, potentially leading\nto explainable RL.\ncurate trajectories for the agent to learn and plan; 2) dynamics\nrepresentation learner, which predicts the latent representation\nof the world using representation learning. A flow chart of\nthe world model is illustrated in Fig. 6 (i). From the real\nworld, knowledge and real data can be collected to construct\na world model simulator, which further models the dynamics\nrepresentation of the world, generates trajectories and helps\nthe policy learning of the agent in the real world.\n1) Trajectory Rolloutor: Similar to the decision trans-\nformer [153] in offline RL, pre-trained large-scale models\nwere used in model-based to synthesize trajectories. In 2022,\nMicheli et al. proposed IRIS, an agent that employs a discrete\nautoencoder and an autoregressive Transformer to learn the\nworld model for Atari games [154]. With the equivalent of\ntwo hours of gameplay in the Atari 100k benchmark, the\nproposed method outperforms humans in 10 out of 26 games.\nSimilarly, reference [155] applied a transformer to build a\nsample-efficient world model for Atari games. Utilizing such\na Transformer-based world model (TWM), the RL agent can\nsolve the long-term dependency and train a state-of-the-art pol-\nicy on the Atari 100k benchmark based on generated meaning-\nful experiences from TWM. Visual RL enables the RL agent\nto learn from visual observations effectively. Reference [156]\nproposed TransDreamer, a Transformer-based model-based RL\nagent that leverages a Transformer for dynamics predictions\nin 2D and 3D visual RL tasks. Experiments showed the\nTransDreamer agent can outperform Dreamer with long-range\nmemory access and memory-based reasoning. Based on the\ndevelopment of supervised pre-training methods, Seo et al.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 14\nproposed a framework that learns world model dynamics from\naction-free video representations [157]. The framework added\na video-based intrinsic bonus for better-guiding exploration to\neffectively encourage agents to learn diverse behaviors. The\nexperimental results demonstrated their proposed method can\nimprove the performances of vision-based RL on manipulation\nand locomotion tasks by transferring the pre-trained represen-\ntations from unseen domains.\n2) Dynamics Representation Learner: Using representation\nlearning techniques, the latent representation of the future\ncan be learned to assist decision-making. Reference [114]\nintroduced a visual model-based RL framework that decou-\nples visual representation learning and dynamic learning by\ntraining an autoencoder with a vision Transformer to recon-\nstruct pixel-given masked observations and learn the dynamics\nfrom the latent space. By such a decoupling approach, their\nproposed method achieved state-of-the-art performance on\nvisual robotic tasks from Meta-world and RLBench. Utilizing\nthe fact that language contains rich information signals and\ncan help agents to predict the future, Lin et al. proposed\nDynalang [46], where an agent that learns a multimodal world\nmodel to predict future text and image representations and\nthereby instruct the decision-making process. The algorithm\nis presented in Algorithm 3. In the training part, a LLM-\nbased world model implemented with Recurrent State Space\nModel (RSSM) [158] computes the state representation zt\nbased on the collected transitions. After that, the representation\nzt and the action at are fed into the RSSM to predict the\nfuture representations zt+1. In addition, the language ˆlt and\nimages ˆxt are predicted to reconstruct the original state. Then,\nthe world model is trained to learn the next representation\nand reconstruct observations from the representations. After\nupdating, the world model imagined (sampled) rollouts and the\npolicy is trained to maximize the imagined rewards. Compared\nto other works that use language only for predicting actions,\nmultimodal information enables Dynalang to handle tasks\nthat require grounded language generation, obtaining higher\nscore than methods provided with only task descriptions.\nTo solve the out-of-distribution generalization problem in\nvisual control tasks of RL, reference [159] proposed the\nLanguage Grounded World Model (LanGWM), which focuses\non learning language-grounded visual features to enhance the\nworld model learning. To improve the generalization of the\nlearned visual features, they masked the bounding boxes and\npredicted them with given language descriptions. Utilizing the\nexpressing ability of language in higher-level concepts and\nglobal contexts, the proposed LanGWM method yields state-\nof-the-art results on out-of-distribution tests.\nB. Policy Interpreter\nExplainable RL (XRL) is an emerging subfield of both\nexplainable machine learning and RL that has attracted consid-\nerable attention recently. XRL aims to elucidate the decision-\nmaking process of learning agents. According to a survey\nin explainable RL [160], the categories of XRL include the\nfeature importance, learning process and MDP, and policy\nlevel. Currently, the usage of LLMs in XRL has only been\nAlgorithm 3 Training part of Dynalang [46]\nRequire: Rewards rt, episode continue flag ct, images xt,\nlanguage tokens lt, actions at, model state (ht, zt).\n1: while training do\n2: Draw batch of transitions {(rt, ct, xt, lt, at)} from re-\nplay buffer.\n3: Use world model to compute multimodal representa-\ntions zt, future predictions ˆzt+1, and decode ˆxt, ˆlt, ˆrt,\nˆct.\n4: Update world model to minimize Lpred + Lrepr.\n5: Imagine rollouts from all zt using π.\n6: Update actor to minimize Lπ.\n7: Update critic to minimize LV .\n8: end while\nlimited to the policy level, i.e., as the policy interpreter.\nTherefore, though there is limited literature related to this area,\nwe think this field is important and requires more attention in\nthe future. The following will introduce the role of LLMs as\npolicy interpreters and an outlook regarding other categories\nof XRL will be provided in the last subsection.\nAs the policy interpreter, LLM generates explanations with\nthe prompts of state and action description or trajectory\ninformation. An illustration is depicted in Fig. 6. Using the\ntrajectory history of states and actions as context information,\nLLMs can be prompts to generate readable interpretations of\ncurrent policies or situations for humans.\nDas et al. [161] proposed a unified framework called\nState2Explanation (S2E), that learns a joint embedding model\nbetween state-action pairs and concept-based explanation.\nBased on the learned models, the explanation can help inform\nreward shaping during an agent’s training and provide insights\nto end-users at deployment. Another work [162] first distilled\nthe policy into a decision tree, derives the decision path,\nand then prompts an LLM to generate a natural language\nexplanation based on the decision path. Additionally, Lu et\nal. [163] introduced a framework that decomposes the overall\nreward into multiple sub-rewards based on specific object\nproperties, defines actions as high-level motion primitives exe-\ncuted at precise 3D positions to simplify decision-making, and\nintegrates LLMs to enable interactive and flexible querying of\nexplanations.\nC. Summarization and Outlook\nAs a generator, LLMs can be integrated into model-based\nRL or explainable RL, i.e., serving as world model simulators\nor policy interpreters, respectively. As world model simulators,\nLLMs enhance model-based RL by auto-regressively generat-\ning accurate trajectories (trajectory rollout) and by predicting\nlatent world representations (world representation learners),\nsignificantly improving sample efficiency and decision-making\naccuracy. In the realm of explainable RL, LLMs provide\nvaluable insights for both end-user understanding and reward\nshaping by generating explanations based on trajectory infor-\nmation. However, the current usages of LLMs in explainable\nRL are rather limited and have great potential for future work.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 15\nBelow are discussions on the key limitations and future work\ndirections of the two roles:\n• World Model Simulator : LLMs encounter challenges in\naligning their abstract knowledge with the specific re-\nquirements of different environments, leading to limi-\ntations in functional competence and grounding. This\nmisalignment affects their effectiveness in generating\ntrajectories and interacting with the environment. Addi-\ntionally, the current model-based agents, which rely on\npurely observational world models, present difficulties for\nhuman adaptation. These models are typically modifiable\nonly through observational data, which is not an effective\nmeans for humans to communicate complex intentions or\nadjustments. Furthermore, while LLMs hold promise for\nenhancing multi-task learning by generating trajectories\nand dynamic representations applicable to multiple tasks,\nthere remains a scarcity of research exploring this poten-\ntial.Looking ahead, in the short term, researchers might\nprioritize improving LLMs’ alignment with specific en-\nvironment requirements. For the long term, a promising\ndirection would be to integrate language instructions or\nan adapter into the world model. This approach could\nlead to a more flexible and adaptive world model that can\nbetter accommodate human intentions and adjustments, as\nwell as support more effective multi-task learning through\nenhanced trajectory generation and dynamic representa-\ntions.\n• Policy Interpreter: As policy interpreters, the quality of\nexplanations depends on the LLM’s understanding of\nfeature representations and implicit logic of policy. How\nto utilize domain knowledge or examples to improve the\nunderstanding of a complex policy is still a major issue.\nIn the short term, researchers could focus on enhancing\nLLMs’ ability to interpret the correlation between obser-\nvations and policy selection, providing insights into the\ndecision-making process of RL agents. This could involve\ndeveloping techniques to better leverage domain knowl-\nedge and examples for improving LLMs’ understanding\nof complex policies. In the long-term, the field could\nexplore advanced applications of LLMs in explainable\nRL, such as analyzing the learning process and MDP to\nreveal influences on agent behavior. Researchers could\ndevelop sophisticated prompting techniques for LLMs to\nanswer nuanced “why” and “why-not” questions with\nMDP context, providing deeper explanations of agent\ndecision-making.\nVIII. D ISCUSSION\nIn previous sections, we introduced the concept “LLM-\nenhanced RL” and developed a corresponding framework,\nwhich we extended to the integration of multimodal AI\nmodels such as visual-language models. We then discussed\nthe different LLM-enhanced RL approaches. This section\nprovides a comprehensive analysis of the LLM-enhanced RL\napproach. We begin with a comparative analysis of the differ-\nent LLM roles, highlighting their advantages and limitations.\nFollowing this, we explore potential real-world applications of\nthe LLM-enhanced RL paradigm. Finally, we discuss future\nopportunities and challenges, taking into account both the\nuntapped capabilities and inherent limitations of LLMs, with a\nparticular focus on their application in multimodal information\nenvironments.\nA. Comparison of Different LLM-Enhanced RL Approaches\nThis section provides a comparative analysis of the four\nLLM-enhanced RL approaches, helping researchers under-\nstand the strengths and limitations of each approach.\n• Information Processor : As an information processor,\nLLM excels in handling complex, multimodal inputs,\nparticularly in translating natural language instructions\nor environment information into a format more readily\nusable by RL agents. This role significantly enhances the\nagent’s ability to understand and interact with complex\nenvironments. However, it faces challenges in computa-\ntional efficiency and may struggle with highly specialized\ndomain knowledge not covered in its pre-training.\n• Reward Designer : LLMs serving as reward designers\noffer a more intuitive and flexible approach to defining\nreward functions, especially in complex or sparse-reward\nenvironments. This role can significantly improve the\nalignment of RL objectives with human intentions. The\nmain limitation lies in ensuring that generated rewards\naccurately reflect task-specific nuances and long-term\ngoals, particularly in highly specialized domains.\n• Decision-Maker: As decision-makers, LLMs can either\ndirectly generate actions or guide action selection, lever-\naging their vast knowledge base to improve sample ef-\nficiency and exploration in RL. This role is particularly\neffective in tasks requiring complex reasoning or long-\nterm planning. However, it may face challenges in real-\ntime decision-making scenarios due to computational\noverhead and may inherit biases present in the LLM’s\ntraining data.\n• Generator: In the generator role, LLMs can simulate\ncomplex environments for model-based RL and provide\ninterpretable explanations of RL policies. This capability\nis invaluable for improving sample efficiency and making\nRL more transparent and understandable. The main chal-\nlenges include aligning generated simulations with real-\nworld dynamics and ensuring the relevance and accuracy\nof policy explanations.\nB. Applications of LLM-Enhanced RL\nBased on the characteristics of LLM-enhanced RL, such as\nmultimodal information understanding and multi-task learning\nand generation, we believe that LLM-enhanced RL opens up\na wide array of potential applications. Here we list several\napplications to inspire researchers.\n• Robotics: RL is widely used in robots to learn how to\nmake decisions and execute actions to achieve goals. Uti-\nlizing natural language understanding and general logical\nreasoning abilities, LLM-enhanced RL can 1) improve\nthe efficiency of human-robot interaction, 2) help robots\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 16\nunderstand human needs and behavioral logic, and 3)\nenhance decision-making and planning capabilities.\n• Autonomous Driving : Autonomous driving uses RL to\nmake decisions in complex, ever-changing environments\nthat involve understanding both sensor data (visual, lidar,\nradar) and contextual information (traffic laws, human\nbehavior). LLM-enhanced RL could employ LLMs to 1)\nprocess this multimodal information and natural language\ninstructions; or 2) design comprehensive rewards based\non multi-disciplinary metrics such as safety, efficiency,\nand passenger comfort.\n• Energy Management: In the energy system, operators or\nusers apply RL to efficiently manage the usage, trans-\nportation, conversion and storage of multiple energy with\nhigh uncertainty brought by renewable resources. LLM-\nenhanced RL in such cases can 1) improve the RL agents’\nability to handle multi-objective tasks, such as economy,\nsafety, and low carbon, by reward function designing,\nand 2) increase the sample efficiency for the new energy\nsystem.\n• Healthcare Recommendation : RL is used to learn rec-\nommendations or suggestions in healthcare [164]. LLMs\ncan utilize the domain knowledge to analyze the vast\namount of patient data and medical histories, therefore, 1)\naccelerating the learning process of RL recommendation\nand 2) providing more accurate diagnostic and treatment\nrecommendations in healthcare.\nC. Opportunities for LLM-Enhanced RL\nAlthough current works in LLM-enhanced RL have already\nshown better performance in several aspects, more unexplored\nareas remain to be explored and may lead to significant im-\nprovement. Below, we summarize the potential opportunities\nof LLM-enhanced RL from both the perspectives of RL and\nLLM capabilities, respectively.\n• RL: Existing work such as [138], [134], [129] mainly fo-\ncus on the general RL while various specialized branches\nof RL are still under-exploited, e.g. multi-agent RL,\nsafe RL, transfer RL, explainable RL, multi-task RL,\nin-context RL and human-centric RL. In the multi-\nagent area, compared to various works about multi-\nLLM-agent collaboration [165], [166], LLM-based multi-\nagent RL remains largely unexplored [167]. A recent\nsurvey discussed some open research problems within this\nfield [168]. LLM-enhanced strategies could be employed\nto facilitate communication and collaboration among RL\nagents. The natural language understanding capabilities\nof LLMs can be used to interpret and generate instruc-\ntions or strategies among agents, enhancing cooperative\nbehaviors or competition strategies; Safe RL could benefit\nfrom the reasoning and predictive capabilities of LLMs\nto design cost functions that encourage safety criteria\ncompliance, reducing the risks of dangerous exploration;\nIn transfer RL, LLMs can assist in identifying simi-\nlarities between tasks or environments, leveraging their\nvast knowledge base to facilitate knowledge transfer and\nthus improve learning efficiency and adaptability across\ndifferent tasks; For multi-task RL, LLMs enhance agents\nby processing diverse entity representations and aligning\ninstructions (Information Processor), generating reduced\naction sets and expert actions for various tasks (Decision-\nMaker), and creating task-specific trajectories and dy-\nnamic representations (Generator). Recently, in-context\nRL has emerged as a promising field by leveraging the\nin-context learning capability of LLMs to solve decision-\nmaking problems [169]. Given a query state and an in-\ncontext offline dataset, a trained LLM exhibits both on-\nline exploration and offline conservation while avoiding\nextensive trial and error and is sample-efficient. Addi-\ntionally, human-centric RL is another prominent field in\nhealthcare and robotics, where AI and humans learn and\ncommunicate with each other for collaboration [170]. In\nthis setting, an LLM naturally serves as a perfect mediator\nto facilitate bidirectional communication through natural\nlanguage interactions.\n• LLM: While LLMs have been integrated with RL in\nvarious methods, several promising directions remain to\nbe explored to further enhance LLM capabilities for RL.\nIt can be divided into the language model view and the\nlanguage agent view as follows:\n– From the model perspective : LLM could be en-\nhanced by an external knowledge base and contin-\nual learning. Retrieval-augmented generation (RAG)\ntechniques help LLM to retrieve the most relevant\ndata in an external database, which could be used\nto attach knowledge in the task domains [171].\nContinual learning techniques are also a hot field\nthat enables models to continuously acquire new\nknowledge while retaining previously learned ca-\npabilities [172]. This technique allows both LLMs\nand RL agents to continuously evolve and adapt to\nnew tasks or environments while maintaining their\nfundamental pre-trained abilities, leading to more\nflexible and robust learning systems.\n– From the agent perspective : Equipping LLM-\nbased agents with specialized modules—namely,\nplanning modules, memory modules, and action\nmodules—can significantly enhance the capabilities\nof LLMs [173], [174]. In the planning module, multi-\nstep reasoning techniques such as CoT-based reason-\ning [74] and Monte Carlo tree search (MCTS) [175]\ncan be used to enhance LLM’s long-term planning\nability, improving the long-term task decomposition\nability of RL agents; for the memory module, long-\nand short-term human-memories and corresponding\nmemory retrieval mechanisms provide a way to\nstore previous experiences and learn adaptively along\nwith the RL agents [176]; for the action module,\ntool integration presents another promising direc-\ntion to unlock new possibilities for LLMs. Exter-\nnal tools such as mathematical solvers [177] and\ninternet browsers [178] can augment LLMs’ capa-\nbilities in RL tasks requiring complex mathematical\ncomputations and real-time information processing,\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 17\nrespectively. Furthermore, collaboration of multiple\nLLM agents [179] is another promising direction.\nWith each LLM playing different roles and acquir-\ning different information, multiple distinctive LLM\nagents solve complex problems by communicating\nand exchanging information. In the context of LLM-\nenhanced RL, different LLMs could potential serves\ndifferent roles and work together to guiding RL. For\nexample, in a group of three LLM agents, one LLM\nfocuses on guiding short-term or immediate problem-\nsolving for RL; another LLM is responsible for long-\nterm planning for RL; the last LLM serves as a\ncoordinator responsible for overseeing the group and\nadjusting horizon consideration accordingly.\nD. Challenges of LLM-Enhanced RL\nWhile LLM improves different issues in the RL paradigm,\nthe success of LLM-enhanced RL is inherently tied to the ca-\npabilities and limitations of the underlying LLM. The primary\nconcerns revolve around the inherent limitations of LLMs,\nadaptability of LLMs in RL environments, computational\ndemands, and the broader implications of deploying such\nsystems in real-world scenarios.\n• Inherent Limitations of LLMs: The effectiveness of LLM-\nenhanced RL systems is fundamentally constrained by\nthe inherent capabilities and limitations of the underlying\nlanguage models. The presence of systematic biases and\npotential hallucinations in pre-trained LLMs can signifi-\ncantly impact the reliability of multimodal input interpre-\ntation, potentially compromising the overall performance\nof the RL agent. This necessitates the development of\nrobust evaluation frameworks to systematically charac-\nterize and delineate the capability boundaries of LLMs\ngiven specified RL contexts. Furthermore, incorporating\nuncertainty quantification methods aids in identifying un-\nreliable answers, increasing the trustworthiness of LLMs’\nresponses [180].\n• Adaptability of LLMs in RL Environments : Despite the\nvast knowledge base of LLMs, they may struggle to\nadapt to specific or novel RL task environments that are\nnot well-represented in their training data. This calls for\nmethods to expand the task-related knowledge for LLM\nand ground LLMs’ inherent knowledge in specific do-\nmains. In terms of expanding the task-related knowledge,\nRAG-related techniques attach LLMs with the external\ndomain-knowledge without further training [171]. When\nfine-tuning LLMs, employing data augmentation tech-\nniques such as generating synthetic data is another way\nto expand the diversity of training scenarios and improve\ngeneralization to novel environments [181]. Additionally,\ncontinual learning mechanisms help preserve previously\nacquired knowledge while adapting to new informa-\ntion [182]. For domain-specific knowledge grounding,\napproaches include training value functions to evaluate\nthe long-term utility of LLM instructions [83]. Expert\ntrajectories provide another valuable source of informa-\ntion, allowing LLMs to learn optimal decision-making\npatterns through in-context learning.\n• Computational Demands : The integration of LLMs into\nthe RL learning process introduces complexities in terms\nof computational overhead and inference times, which\nslow down the learning process of RL. To address this\nchallenge, several potential solutions have been proposed\nacross different levels. At the data level, input compres-\nsion techniques such as prompt pruning [183] can be em-\nployed to directly shorten the model input, significantly\nreducing inference time without substantial loss in perfor-\nmance. At the model level, efficient architecture designs\nlike mixture-of-experts (MoE) [184] enable conditional\ncomputation, activating only relevant parts of the model\nfor each input, thus reducing computational costs. Sim-\nilarly, structured state space models (SSM) [185] offer\nlinear-time complexity for sequence modeling, providing\na more efficient alternative to traditional attention mech-\nanisms. At the system level, advanced caching strate-\ngies [186] and asynchronous processing techniques [187]\ncan be implemented to reuse intermediate computations\nand parallelize the execution, respectively.\n• Ethical, Legal, and Safety Concerns : In practical usages,\nthe use of LLMs involves complex ethical, legal, and\nsafety concerns. Data privacy, intellectual property, and\naccountability for AI decisions should be carefully dis-\ncussed. To address these issues, researchers are devel-\noping robust frameworks for responsible AI deployment.\nAt the privacy level, differential privacy techniques [188]\nare being implemented to protect individual data during\ntraining and inference. For transparency, efforts focus\non developing interpretable AI systems, allowing stake-\nholders to audit AI-driven decisions [189]. To enhance\nsystem robustness, adversarial training methods are being\nexplored to strengthen LLM-RL systems against potential\nattacks [190]. Regarding ethical and legal issues, a recent\nsurvey also analyzed potential solutions to integrate eth-\nical standards and societal values into LLM [191].\nIX. C ONCLUSION\nLLMs, with their pre-trained knowledge bases and powerful\ncapabilities such as reasoning and in-context learning, present\nas a viable solution to enhance RL in terms of natural lan-\nguage understanding, multi-task generalization, task planning,\nand sample efficiency. In this survey, we have defined this\nparadigm as LLM-enhanced RL and summarized its character-\nistics, together with opportunities and challenges. To formalize\nthe research scope and methodology of LLM-enhanced RL, we\npropose a structured framework to systematically categorize\nthe roles of LLM based on the functionalities within the\nclassical agent-environment interaction paradigm. According\nto functionalities, we categorize the roles of LLMs into\ninformation processor, reward designer, decision-maker, and\ngenerator. For each category, we review current literature based\non their methods and applications, outline the methodologies,\ndiscuss the addressed issues, and provide insights into future\ndirections. These are listed below:\n• Information Processor: LLMs extract observational rep-\nresentations and formal specification languages for RL\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 18\nagents, thus increasing the sample efficiency. Future\ndirections include incorporating goal-based information\nand integrating multimodal environmental information to\nobtain stronger information extraction ability.\n• Reward Designer: For intricate or un-quantifiable tasks,\nLLMs can leverage pre-trained knowledge to generate\nhigh-performing rewards that are notoriously difficult\nfor humans to design. Current methods still require\nconsistently modifying the prompts and instructions of\nLLMs, calling for future work on automated self-evolving\nframeworks without human intervention.\n• Decision-Maker: LLMs generate direct actions or indirect\nadvice for the agent to improve exploration efficiency.\nHuge computational overhead is a major issue in online\ninteraction. Cost-effective methods to reduce the huge\ncomputational overhead of LLM in online RL is an\nimportant direction.\n• Generator: With the generative capability and world\nknowledge, LLMs are used as 1) a high-fidelity world\nmodel to reduce real-world learning cost; and 2) a\nlanguage-based policy interpreter to explain the agent\npolicy. Leveraging human instructions to improve the\naccuracy and generalizability of the world model and\npolicy interpreter would be a crucial direction.\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[2] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller et al. , “Human-level control\nthrough deep reinforcement learning,” Nature, vol. 518, no. 7540, pp.\n529–533, 2015.\n[3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[4] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in International conference on machine learning. PMLR, 2018,\npp. 1861–1870.\n[5] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\nJ. Chung, D. H. Choi, R. Powell et al., “Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning,” Nature, vol. 575, no. 7782,\npp. 350–354, 2019.\n[6] C. Berner, G. Brockman, B. Chan, V . Cheung, P. D ´eak, C. Dennison,\nD. Farhi, Q. Fischer et al., “Dota 2 with large scale deep reinforcement\nlearning,” arXiv preprint arXiv:1912.06680 , 2019.\n[7] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou et al. , “Mastering the\ngame of go with deep neural networks and tree search,” Nature, vol.\n529, no. 7587, pp. 484–489, 2016.\n[8] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,\nS. Schmitt, A. Guez, E. Lockhart et al. , “Mastering atari, go,\nchess and shogi by planning with a learned model,” arXiv preprint\narXiv:1911.08265, 2020.\n[9] H. Zhao, Z. Liu, X. Mai, J. Zhao, J. Qiu, G. Liu, Z. Y . Dong, and\nA. M. Ghias, “Mobile battery energy storage system control with\nknowledge-assisted deep reinforcement learning,” Energy Conversion\nand Economics, vol. 3, no. 6, pp. 381–391, 2022.\n[10] N. B. Schmid, A. Botev, A. Hennig, A. Lerer, Q. Wu, D. Yarats,\nJ. Foerster, T. Rockt ¨aschel et al., “Rebel: A general game playing ai,”\nScience, vol. 373, no. 6556, pp. 664–670, 2021.\n[11] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Superhuman ai for\nmultiplayer poker,” Science, vol. 365, no. 6456, pp. 885–890, 2020.\n[12] A. Vaswani, “Attention is all you need,” Advances in Neural Informa-\ntion Processing Systems , 2017.\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” Advances in neural infor-\nmation processing systems , vol. 25, 2012.\n[14] Y . Jiang, S. S. Gu, K. P. Murphy, and C. Finn, “Language as an\nabstraction for hierarchical deep reinforcement learning,” Advances in\nNeural Information Processing Systems , vol. 32, 2019.\n[15] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S ¨underhauf,\nI. Reid, S. Gould et al., “Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 3674–3683.\n[16] A. Stooke, K. Lee, P. Abbeel, and M. Laskin, “Decoupling representa-\ntion learning from reinforcement learning,” in International Conference\non Machine Learning . PMLR, 2021, pp. 9870–9879.\n[17] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas, E. Grefen-\nstette, S. Whiteson, and T. Rockt ¨aschel, “A Survey of Reinforcement\nLearning Informed by Natural Language,” Jun. 2019.\n[18] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni,\nL. Fei-Fei, S. Savarese et al. , “What matters in learning from offline\nhuman demonstrations for robot manipulation,” 2021.\n[19] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch,\nT. Armstrong, and P. Florence, “Interactive language: Talking to robots\nin real time,” 2022.\n[20] Z. Yang, K. Ren, X. Luo, M. Liu, W. Liu, J. Bian, W. Zhang, and\nD. Li, “Towards applicable reinforcement learning: Improving the\ngeneralization and sample efficiency with policy ensemble,” in Inter-\nnational Joint Conference on Artificial Intelligence , 2022. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:248887230\n[21] W. B. Knox, A. Allievi, H. Banzhaf, F. Schmitt, and P. Stone, “Reward\n(mis) design for autonomous driving,” Artificial Intelligence, vol. 316,\np. 103829, 2023.\n[22] F. Dworschak, S. Dietze, M. Wittmann, B. Schleich, and S. Wartzack,\n“Reinforcement learning for engineering design automation,” Advanced\nEngineering Informatics, vol. 52, p. 101612, 2022.\n[23] S. Booth, W. B. Knox, J. Shah, S. Niekum, P. Stone, and A. Al-\nlievi, “The perils of trial-and-error reward design: misdesign through\noverfitting and invalid task specifications,” in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 37, no. 5, 2023, pp. 5920–\n5929.\n[24] L. L. Di Langosco, J. Koch, L. D. Sharkey, J. Pfau, and D. Krueger,\n“Goal misgeneralization in deep reinforcement learning,” in Interna-\ntional Conference on Machine Learning . PMLR, 2022, pp. 12 004–\n12 019.\n[25] R. Yang, X. Sun, and K. Narasimhan, “A generalized algorithm for\nmulti-objective reinforcement learning and policy adaptation,” Ad-\nvances in neural information processing systems , vol. 32, 2019.\n[26] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas,\nE. Grefenstette, S. Whiteson, and T. Rockt ¨aschel, “A survey of re-\ninforcement learning informed by natural language,” arXiv preprint\narXiv:1906.03926, 2019.\n[27] J. Devlin, “Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding,” arXiv preprint arXiv:1810.04805 , 2018.\n[28] A. Radford and K. Narasimhan, “Improving language understanding\nby generative pre-training,” 2018. [Online]. Available: https://api.\nsemanticscholar.org/CorpusID:49313245\n[29] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam et al. , “Language models are few-shot\nlearners,” Advances in neural information processing systems , vol. 33,\npp. 1877–1901, 2020.\n[30] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung et al., “Palm: Scaling language modeling with\npathways,” Journal of Machine Learning Research , vol. 24, no. 240,\npp. 1–113, 2023.\n[31] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.\nTan, and D. S. W. Ting, “Large language models in medicine,” Nature\nmedicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[32] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes, “Autonomous\nchemical research with large language models,” Nature, vol. 624, no.\n7992, pp. 570–578, 2023.\n[33] G. Jiang, Z. Ma, L. Zhang, and J. Chen, “Eplus-llm: A large language\nmodel-based computing platform for automated building energy mod-\neling,” Applied Energy, vol. 367, p. 123431, 2024.\n[34] H. Tan, Z. Guo, Z. Lin, Y . Chen, D. Huang, W. Yuan, H. Zhang, and\nJ. Yan, “General generative ai-based image augmentation method for\nrobust rooftop pv segmentation,” Applied Energy, vol. 368, p. 123554,\n2024.\n[35] X. Zhou, H. Zhao, Y . Cheng, Y . Cao, G. Liang, G. Liu, and J. Zhao,\n“Elecbench: a power dispatch evaluation benchmark for large language\nmodels,” arXiv preprint arXiv:2407.05365 , 2024.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 19\n[36] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, “Code as policies: Language model programs for em-\nbodied control,” in 2023 IEEE International Conference on Robotics\nand Automation (ICRA) . IEEE, 2023, pp. 9493–9500.\n[37] T. Webb, K. J. Holyoak, and H. Lu, “Emergent analogical reasoning\nin large language models,” Nature Human Behaviour, vol. 7, no. 9, pp.\n1526–1541, 2023.\n[38] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu\net al. , “Larger language models do in-context learning differently,”\narXiv preprint arXiv:2303.03846 , 2023.\n[39] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\nmodels: A survey,” arXiv preprint arXiv:2212.10403 , 2022.\n[40] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-Li, X. Lv, H. Peng,\nZ. Yao et al., “Kola: Carefully benchmarking world knowledge of large\nlanguage models,” 2023.\n[41] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,\nD. Fox, J. Thomason et al. , “Progprompt: program generation for\nsituated robot task planning using large language models,” Autonomous\nRobots, pp. 1–14, 2023.\n[42] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. V oss,\nA. Radford, D. Amodei et al. , “Learning to summarize from human\nfeedback,” 2022.\n[43] E. Aky ¨urek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou, “What\nlearning algorithm is in-context learning? investigations with linear\nmodels,” 2023.\n[44] Y . Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta,\nand J. Andreas, “Guiding Pretraining in Reinforcement Learning with\nLarge Language Models,” Sep. 2023.\n[45] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P.-Y . Oudeyer,\n“Grounding Large Language Models in Interactive Environments with\nOnline Reinforcement Learning,” Sep. 2023.\n[46] J. Lin, Y . Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and\nA. Dragan, “Learning to Model the World with Language,” Jul. 2023.\n[47] H. Li, X. Yang, Z. Wang, X. Zhu, J. Zhou, Y . Qiao, X. Wang, H. Li\net al. , “Auto mc-reward: Automated dense reward design with large\nlanguage models for minecraft,” 2023.\n[48] S. Chakraborty, K. Weerakoon, P. Poddar, M. Elnoor, P. Narayanan,\nC. Busart, P. Tokekar, A. S. Bedi et al. , “RE-MOVE: An Adaptive\nPolicy Design for Robotic Navigation Tasks in Dynamic Environments\nvia Language-Based Feedback,” Sep. 2023.\n[49] J.-C. Pang, X.-Y . Yang, S.-H. Yang, and Y . Yu, “Natural language-\nconditioned reinforcement learning with inside-out task language\ndevelopment and translation,” 2023. [Online]. Available: https:\n//arxiv.org/abs/2302.09368\n[50] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,\nD. Quillen, E. Holly et al. , “Qt-opt: Scalable deep reinforcement\nlearning for vision-based robotic manipulation,” 2018.\n[51] K. Wang, B. Kang, J. Shao, and J. Feng, “Improving generalization\nin reinforcement learning with mixture regularization,” Advances in\nNeural Information Processing Systems, vol. 33, pp. 7968–7978, 2020.\n[52] R. Devidze, P. Kamalaruban, and A. Singla, “Exploration-guided\nreward shaping for reinforcement learning under sparse rewards,”\nAdvances in Neural Information Processing Systems, vol. 35, pp. 5829–\n5842, 2022.\n[53] A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine, “End-to-\nend robotic reinforcement learning without reward engineering,” arXiv\npreprint arXiv:1904.07854, 2019.\n[54] D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan,\n“Inverse reward design,” Advances in neural information processing\nsystems, vol. 30, 2017.\n[55] C. Xiao, Y . Wu, C. Ma, D. Schuurmans, and M. M ¨uller, “Learning\nto combat compounding-error in model-based reinforcement learning,”\narXiv preprint arXiv:1912.11206 , 2019.\n[56] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker et al. , “Model-\nbased reinforcement learning: A survey,” Foundations and Trends® in\nMachine Learning, vol. 16, no. 1, pp. 1–118, 2023.\n[57] M. Cho, J. Park, S. Lee, and Y . Sung, “Hard tasks first: Multi-\ntask reinforcement learning through task scheduling,” in Forty-first\nInternational Conference on Machine Learning , 2024. [Online].\nAvailable: https://openreview.net/forum?id=haUOhXo70o\n[58] J. Feng, M. Chen, Z. Pu, T. Qiu, and J. Yi, “Efficient multi-task\nreinforcement learning via task-specific action correction,” 2024.\n[Online]. Available: https://arxiv.org/abs/2404.05950\n[59] L. Sun, H. Zhang, W. Xu, and M. Tomizuka, “Paco: Parameter-\ncompositional multi-task reinforcement learning,” Advances in Neural\nInformation Processing Systems , vol. 35, pp. 21 495–21 507, 2022.\n[60] G. Zhang, A. Jain, I. Hwang, S.-H. Sun, and J. J. Lim, “Efficient\nmulti-task reinforcement learning via selective behavior sharing,” 2024.\n[Online]. Available: https://openreview.net/forum?id=LYGHdwyXUb\n[61] N. Vithayathil Varghese and Q. H. Mahmoud, “A survey of multi-task\ndeep reinforcement learning,” Electronics, vol. 9, no. 9, 2020.\n[Online]. Available: https://www.mdpi.com/2079-9292/9/9/1363\n[62] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman,\nS. Levine, and J. Tompson, “Robotic Skill Acquisition via Instruction\nAugmentation with Vision-Language Models,” Jul. 2023.\n[63] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu,\n“Plan4mc: Skill reinforcement learning and planning for open-world\nminecraft tasks,” arXiv preprint arXiv:2303.16563 , 2023.\n[64] J. Rocamonde, V . Montesinos, E. Nava, E. Perez, and D. Lindner,\n“Vision-Language Models are Zero-Shot Reward Models for Rein-\nforcement Learning,” Oct. 2023.\n[65] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Neural Information Processing Systems , 2017. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:13756489\n[66] M. Shanahan, “Talking about large language models,” ArXiv, vol.\nabs/2212.03551, 2022. [Online]. Available: https://api.semanticscholar.\norg/CorpusID:254366666\n[67] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal et al. , “Llama: Open and\nefficient foundation language models,” ArXiv, vol. abs/2302.13971,\n2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:\n257219404\n[68] J. Kaplan, S. McCandlish, T. J. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford et al., “Scaling laws for neural language\nmodels,” ArXiv, vol. abs/2001.08361, 2020. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:210861095\n[69] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. de Las Casas, L. A. Hendricks et al. , “Training\ncompute-optimal large language models,” ArXiv, vol. abs/2203.15556,\n2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:\n247778764\n[70] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma et al., “Emergent abilities of large language\nmodels,” Trans. Mach. Learn. Res. , vol. 2022, 2022. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:249674500\n[71] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler et al. , “Multitask prompted training enables\nzero-shot task generalization,” arXiv preprint arXiv:2110.08207, 2021.\n[72] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal et al. , “Training language models to follow\ninstructions with human feedback,” 2022.\n[73] Y . Cheng, H. Zhao, X. Zhou, J. Zhao, Y . Cao, and C. Yang, “Gaia–\na large language model for advanced power dispatch,” arXiv preprint\narXiv:2408.03847, 2024.\n[74] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi,\nQ. Le et al. , “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” 2023.\n[75] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\nlarge language models,” 2023.\n[76] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi,\nJ. Gajda, T. Lehmann, M. Podstawski et al. , “Graph of thoughts:\nSolving elaborate problems with large language models,” 2023.\n[77] Z. Rao, Y . Wu, Z. Yang, W. Zhang, S. Lu, W. Lu, and Z. Zha, “Visual\nnavigation with multiple goals based on deep reinforcement learning,”\nIEEE Transactions on Neural Networks and Learning Systems, vol. 32,\nno. 12, pp. 5445–5455, 2021.\n[78] C. Huang, R. Zhang, M. Ouyang, P. Wei, J. Lin, J. Su, and L. Lin,\n“Deductive reinforcement learning for visual autonomous urban driving\nnavigation,” IEEE Transactions on Neural Networks and Learning\nSystems, vol. 32, no. 12, pp. 5379–5391, 2021.\n[79] M. Yang, W. Huang, W. Tu, Q. Qu, Y . Shen, and K. Lei, “Multitask\nlearning and reinforcement learning for personalized dialog genera-\ntion: An empirical study,” IEEE transactions on neural networks and\nlearning systems, vol. 32, no. 1, pp. 49–62, 2020.\n[80] Z. He, J. Li, F. Wu, H. Shi, and K.-S. Hwang, “Derl: Coupling\ndecomposition in action space for reinforcement learning task,” IEEE\nTransactions on Emerging Topics in Computational Intelligence, 2023.\n[81] Y . Liu, Y . Zhang, Y . Wang, F. Hou, J. Yuan, J. Tian, Y . Zhang, Z. Shi\net al., “A survey of visual transformers,” IEEE Transactions on Neural\nNetworks and Learning Systems , 2023.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 20\n[82] Y . J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman,\nY . Zhu, L. Fan et al. , “Eureka: Human-Level Reward Design via\nCoding Large Language Models,” Oct. 2023.\n[83] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu et al., “Do As I Can, Not As I Say: Grounding Language\nin Robotic Affordances,” Aug. 2022.\n[84] K. Nottingham, P. Ammanabrolu, A. Suhr, Y . Choi, H. Hajishirzi,\nS. Singh, and R. Fox, “Do Embodied Agents Dream of Pixelated Sheep:\nEmbodied Decision Making using Language Guided World Modelling,”\nApr. 2023.\n[85] A. Srinivas, M. Laskin, and P. Abbeel, “CURL: Contrastive Unsuper-\nvised Representations for Reinforcement Learning,” Sep. 2020.\n[86] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. Courville, and\nP. Bachman, “Data-efficient reinforcement learning with self-predictive\nrepresentations,” arXiv preprint arXiv:2007.05929 , 2020.\n[87] F. Paischer, T. Adler, V . Patil, A. Bitto-Nemling, M. Holzleitner,\nS. Lehner, H. Eghbal-zadeh, and S. Hochreiter, “History Compression\nvia Language Models in Reinforcement Learning,” Feb. 2023.\n[88] F. Paischer, T. Adler, M. Hofmarcher, and S. Hochreiter, “Semantic\nhelm: A human-readable memory for reinforcement learning,” Ad-\nvances in Neural Information Processing Systems , vol. 36, 2024.\n[89] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell et al., “Learning transferable visual models from\nnatural language supervision,” in International conference on machine\nlearning. PMLR, 2021, pp. 8748–8763.\n[90] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\n“Transformer-xl: Attentive language models beyond a fixed-length\ncontext,” 2019. [Online]. Available: https://arxiv.org/abs/1901.02860\n[91] A. v. d. Oord, Y . Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” arXiv preprint arXiv:1807.03748, 2018.\n[92] W. K. Kim, S. Kim, H. Woo et al. , “Efficient policy adaptation with\ncontrastive prompt ensemble for embodied agents,” Advances in Neural\nInformation Processing Systems , vol. 36, 2024.\n[93] R. P. Poudel, H. Pandya, S. Liwicki, and R. Cipolla, “Recore:\nRegularized contrastive representation learning of world model,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 22 904–22 913.\n[94] S. Basavatia, K. Murugesan, and S. Ratnakar, “Starling: Self-supervised\ntraining of text-based reinforcement learning agent with large language\nmodels,” arXiv preprint arXiv:2406.05872 , 2024.\n[95] R. Patel, E. Pavlick, and S. Tellex, “Grounding language to non-\nmarkovian tasks with no supervision of task specifications.” in\nRobotics: Science and Systems , vol. 2020, 2020.\n[96] T. R. Sumers, M. K. Ho, R. D. Hawkins, K. Narasimhan, and T. L.\nGriffiths, “Learning rewards from linguistic feedback,” in Proceedings\nof the AAAI Conference on Artificial Intelligence , vol. 35, no. 7, 2021,\npp. 6002–6010.\n[97] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, “Code as Policies: Language Model Programs for\nEmbodied Control,” May 2023.\n[98] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y . Su,\n“Llm-planner: Few-shot grounded planning for embodied agents with\nlarge language models,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023, pp. 2998–3009.\n[99] B. A. Spiegel, Z. Yang, W. Jurayj, B. Bachmann, S. Tellex, and\nG. Konidaris, “Informing reinforcement learning agents by grounding\nlanguage to markov decision processes,” in Workshop on Training\nAgents with Foundation Models at RLC 2024 , 2024. [Online].\nAvailable: https://openreview.net/forum?id=uFm9e4Ly26\n[100] B. Gordon, Y . Bitton, Y . Shafir, R. Garg, X. Chen, D. Lischinski,\nD. Cohen-Or, and I. Szpektor, “Mismatch quest: Visual and textual\nfeedback for image-text misalignment,” in European Conference on\nComputer Vision. Springer, 2025, pp. 310–328.\n[101] Y . Wang, J. He, D. Wang, Q. Wang, B. Wan, and X. Luo, “Multimodal\ntransformer with adaptive modality weighting for multimodal sentiment\nanalysis,” Neurocomputing, vol. 572, p. 127181, 2024.\n[102] X. Zhao, S. Poria, X. Li, Y . Chen, and B. Tang, “Toward robust\nmultimodal learning using multimodal foundational models,” arXiv\npreprint arXiv:2401.13697, 2024.\n[103] F. Lygerakis, V . Dave, and E. Rueckert, “M2curl: Sample-efficient\nmultimodal reinforcement learning via self-supervised representation\nlearning for robotic manipulation,” arXiv preprint arXiv:2401.17032 ,\n2024.\n[104] J. Eschmann, “Reward function design in reinforcement learning,”\nReinforcement Learning Algorithms: Analysis and Applications , pp.\n25–33, 2021.\n[105] J. Andreas, D. Klein, and S. Levine, “Modular multitask reinforcement\nlearning with policy sketches,” in International conference on machine\nlearning. PMLR, 2017, pp. 166–175.\n[106] S. Mirchandani, S. Karamcheti, and D. Sadigh, “Ella: Exploration\nthrough learned language abstraction,” Advances in Neural Information\nProcessing Systems, vol. 34, pp. 29 529–29 540, 2021.\n[107] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, “Reward Design\nwith Language Models,” in The Eleventh International Conference on\nLearning Representations, Sep. 2022.\n[108] Y . Wu, Y . Fan, P. P. Liang, A. Azaria, Y . Li, and T. M. Mitchell,\n“Read and Reap the Rewards: Learning to Play Atari with the Help of\nInstruction Manuals,” Oct. 2023.\n[109] K. Chu, X. Zhao, C. Weber, M. Li, and S. Wermter, “Accelerating\nReinforcement Learning of Robotic Manipulations via Feedback from\nLarge Language Models,” Nov. 2023.\n[110] A. Adeniji, A. Xie, C. Sferrazza, Y . Seo, S. James, and P. Abbeel,\n“Language reward modulation for pretraining reinforcement learning,”\narXiv preprint arXiv:2308.12270 , 2023.\n[111] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” 2020. [Online].\nAvailable: https://arxiv.org/abs/1910.01108\n[112] C. Kim, Y . Seo, H. Liu, L. Lee, J. Shin, H. Lee, and K. Lee, “Guide\nYour Agent with Adaptive Multimodal Rewards,” Oct. 2023.\n[113] S. Nair, A. Rajeswaran, V . Kumar, C. Finn, and A. Gupta, “R3m: A\nuniversal visual representation for robot manipulation,” 2022. [Online].\nAvailable: https://arxiv.org/abs/2203.12601\n[114] Y . Seo, D. Hafner, H. Liu, F. Liu, S. James, K. Lee, and P. Abbeel,\n“Masked World Models for Visual Control,” May 2023.\n[115] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar,\nJ. Hamburger, H. Jiang et al., “Ego4d: Around the world in 3,000 hours\nof egocentric video,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 18 995–19 012.\n[116] Y . Wang, Z. Sun, J. Zhang, Z. Xian, E. Biyik, D. Held,\nand Z. Erickson, “Rl-vlm-f: Reinforcement learning from vision\nlanguage foundation model feedback,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.03681\n[117] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-\nT. L. Chiang, T. Erez et al. , “Language to rewards for robotic skill\nsynthesis,” arXiv preprint arXiv:2306.08647 , 2023.\n[118] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe,\nU. Alon, N. Dziri et al. , “Self-refine: Iterative refinement with self-\nfeedback,” 2023.\n[119] J. Song, Z. Zhou, J. Liu, C. Fang, Z. Shu, and L. Ma, “Self-Refined\nLarge Language Model as Automated Reward Function Designer for\nDeep Reinforcement Learning in Robotics,” Oct. 2023.\n[120] T. Xie, S. Zhao, C. H. Wu, Y . Liu, Q. Luo, V . Zhong,\nY . Yang, and T. Yu, “Text2reward: Reward shaping with language\nmodels for reinforcement learning,” in The Twelfth International\nConference on Learning Representations , 2024. [Online]. Available:\nhttps://openreview.net/forum?id=tUM39YTRxH\n[121] E. Ferrara, “Should chatgpt be biased? challenges and risks of bias in\nlarge language models,” arXiv preprint arXiv:2304.03738 , 2023.\n[122] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim,\nF. Dernoncourt, T. Yu, R. Zhang et al. , “Bias and fairness in large\nlanguage models: A survey,” Computational Linguistics , pp. 1–79,\n2024.\n[123] L. Gao, J. Schulman, and J. Hilton, “Scaling laws for reward model\noveroptimization,” in Proceedings of the 40th International Conference\non Machine Learning, ser. Proceedings of Machine Learning Research,\nA. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\nJ. Scarlett, Eds., vol. 202. PMLR, 23–29 Jul 2023, pp. 10 835–10 866.\n[124] S. Chakraborty, A. Bhaskar, A. Singh, P. Tokekar, D. Manocha,\nand A. S. Bedi, “Rebel: A regularization-based solution for reward\noveroptimization in reinforcement learning from human feedback,”\narXiv preprint arXiv:2312.14436 , 2023.\n[125] A. Radwan, L. Zaafarani, J. Abudawood, F. AlZahrani, and F. Fourat,\n“Addressing bias through ensemble learning and regularized fine-\ntuning,” arXiv preprint arXiv:2402.00910 , 2024.\n[126] Y . Inoue and H. Ohashi, “Prompter: Utilizing large language model\nprompting for a data efficient embodied instruction following,” arXiv\npreprint arXiv:2211.03267, 2022.\n[127] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and\nD. Batra, “Improving vision-and-language navigation with image-text\npairs from the web,” in Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI\n16. Springer, 2020, pp. 259–274.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 21\n[128] M. Janner, Q. Li, and S. Levine, “Offline reinforcement learning as\none big sequence modeling problem,” Advances in neural information\nprocessing systems, vol. 34, pp. 1273–1286, 2021.\n[129] S. Li, X. Puig, C. Paxton, Y . Du, C. Wang, L. Fan, T. Chen, D.-A.\nHuang et al. , “Pre-trained language models for interactive decision-\nmaking,” Advances in Neural Information Processing Systems , vol. 35,\npp. 31 199–31 212, 2022.\n[130] R. Shi, Y . Liu, Y . Ze, S. S. Du, and H. Xu, “Unleashing the power of\npre-trained language models for offline reinforcement learning,” arXiv\npreprint arXiv:2310.20587, 2023.\n[131] Z. Zeng, C. Zhang, S. Wang, and C. Sun, “Goal-conditioned predictive\ncoding for offline reinforcement learning,” Advances in Neural Infor-\nmation Processing Systems , vol. 36, 2024.\n[132] M. Reid, Y . Yamada, and S. S. Gu, “Can Wikipedia Help Offline\nReinforcement Learning?” Jul. 2022.\n[133] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:\nDatasets for deep data-driven reinforcement learning,” arXiv preprint\narXiv:2004.07219, 2020.\n[134] L. Mezghani, P. Bojanowski, K. Alahari, and S. Sukhbaatar, “Think\nbefore you act: Unified policy for interleaving language reasoning with\nactions,” arXiv preprint arXiv:2304.11063 , 2023.\n[135] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart\net al. , “Rt-2: Vision-language-action models transfer web knowledge\nto robotic control,” in Conference on Robot Learning . PMLR, 2023,\npp. 2165–2183.\n[136] S. Yao, R. Rao, M. Hausknecht, and K. Narasimhan, “Keep calm and\nexplore: Language models for action generation in text-based games,”\narXiv preprint arXiv:2010.02903 , 2020.\n[137] M. Hausknecht, P. Ammanabrolu, M.-A. C ˆot´e, and X. Yuan, “Interac-\ntive fiction games: A colossal adventure,” in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 34, no. 05, 2020, pp. 7903–\n7910.\n[138] H. Hu and D. Sadigh, “Language Instructed Reinforcement Learning\nfor Human-AI Coordination,” Jun. 2023.\n[139] Z. Zhou, B. Hu, P. Zhang, C. Zhao, and B. Liu, “Large language model\nis a good policy teacher for training reinforcement learning agents,”\narXiv preprint arXiv:2311.13373 , 2023.\n[140] M. Dalal, T. Chiruvolu, D. S. Chaplot, and R. Salakhutdinov,\n“Plan-seq-learn: Language model guided RL for solving long\nhorizon robotics tasks,” in The Twelfth International Conference\non Learning Representations , 2024. [Online]. Available: https:\n//openreview.net/forum?id=hQVCCxQrYN\n[141] J. Garcıa and F. Fern ´andez, “A comprehensive survey on safe rein-\nforcement learning,” Journal of Machine Learning Research , vol. 16,\nno. 1, pp. 1437–1480, 2015.\n[142] H. Xu, X. Zhan, and X. Zhu, “Constraints penalized q-learning for safe\noffline reinforcement learning,” in Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 36, no. 8, 2022, pp. 8753–8760.\n[143] J. Lee, C. Paduraru, D. J. Mankowitz, N. Heess, D. Precup, K.-E.\nKim, and A. Guez, “Coptidice: Offline constrained reinforcement learn-\ning via stationary distribution correction estimation,” arXiv preprint\narXiv:2204.08957, 2022.\n[144] Z. Liu, Z. Guo, Y . Yao, Z. Cen, W. Yu, T. Zhang, and D. Zhao, “Con-\nstrained decision transformer for offline safe reinforcement learning,”\nin International Conference on Machine Learning . PMLR, 2023, pp.\n21 611–21 630.\n[145] S. Naihin, D. Atkinson, M. Green, M. Hamadi, C. Swift, D. Schonholtz,\nA. T. Kalai, and D. Bau, “Testing language model agents safely in the\nwild,” arXiv preprint arXiv:2311.10538 , 2023.\n[146] W. Huang, H. Liu, Z. Huang, and C. Lv, “Safety-aware human-in-\nthe-loop reinforcement learning with shared control for autonomous\ndriving,” IEEE Transactions on Intelligent Transportation Systems , pp.\n1–12, 2024.\n[147] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\nand S. Yao, “Reflexion: Language Agents with Verbal Reinforcement\nLearning,” Oct. 2023.\n[148] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\n2021.\n[149] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to Control:\nLearning Behaviors by Latent Imagination,” Mar. 2020.\n[150] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba, “Mastering Atari with\nDiscrete World Models,” Feb. 2022.\n[151] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to control:\nLearning behaviors by latent imagination,” 2020.\n[152] Y . Matsuo, Y . LeCun, M. Sahani, D. Precup, D. Silver, M. Sugiyama,\nE. Uchibe, and J. Morimoto, “Deep learning, reinforcement learning,\nand world models,” Neural Networks, vol. 152, pp. 267–275, 2022.\n[153] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin,\nP. Abbeel, A. Srinivas et al. , “Decision transformer: Reinforcement\nlearning via sequence modeling,” Advances in neural information\nprocessing systems, vol. 34, pp. 15 084–15 097, 2021.\n[154] V . Micheli, E. Alonso, and F. Fleuret, “Transformers are Sample-\nEfficient World Models,” in The Eleventh International Conference on\nLearning Representations, Sep. 2022.\n[155] J. Robine, M. H ¨oftmann, T. Uelwer, and S. Harmeling, “Transformer-\nbased World Models Are Happy With 100k Interactions,” Mar. 2023.\n[156] C. Chen, Y .-F. Wu, J. Yoon, and S. Ahn, “TransDreamer: Reinforce-\nment Learning with Transformer World Models,” Feb. 2022.\n[157] Y . Seo, K. Lee, S. James, and P. Abbeel, “Reinforcement Learning\nwith Action-Free Pre-Training from Videos,” Jun. 2022.\n[158] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and\nJ. Davidson, “Learning latent dynamics for planning from pixels,” in\nInternational conference on machine learning . PMLR, 2019, pp.\n2555–2565.\n[159] R. P. K. Poudel, H. Pandya, C. Zhang, and R. Cipolla, “LanGWM:\nLanguage Grounded World Model,” Nov. 2023.\n[160] S. Milani, N. Topin, M. Veloso, and F. Fang, “A survey of explainable\nreinforcement learning,” 2022.\n[161] D. Das, S. Chernova, and B. Kim, “State2explanation: Concept-based\nexplanations to benefit agent learning and user understanding,”\nin Thirty-seventh Conference on Neural Information Processing\nSystems, 2023. [Online]. Available: https://openreview.net/forum?id=\nxGz0wAIJrS\n[162] J. Lin, Y . Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and\nA. Dragan, “Learning to model the world with language,” arXiv\npreprint arXiv:2308.01399, 2023.\n[163] W. Lu, X. Zhao, S. Magg, M. Gromniak, M. Li, and S. Wermterl,\n“A closer look at reward decomposition for high-level robotic expla-\nnations,” in 2023 IEEE International Conference on Development and\nLearning (ICDL). IEEE, 2023, pp. 429–436.\n[164] C. Yu, J. Liu, S. Nemati, and G. Yin, “Reinforcement learning in\nhealthcare: A survey,”ACM Computing Surveys (CSUR), vol. 55, no. 1,\npp. 1–36, 2021.\n[165] S. Agashe, Y . Fan, and X. E. Wang, “Evaluating multi-agent\ncoordination abilities in large language models,” arXiv preprint\narXiv:2310.03903, 2023.\n[166] S. S. Kannan, V . L. Venkatesh, and B.-C. Min, “Smart-llm: Smart\nmulti-agent robot task planning using large language models,” arXiv\npreprint arXiv:2309.10062, 2023.\n[167] O. Slumbers, D. H. Mguni, K. Shao, and J. Wang, “Leveraging large\nlanguage models for optimised coordination in textual multi-agent\nreinforcement learning,” 2023.\n[168] C. Sun, S. Huang, and D. Pompili, “Llm-based multi-agent rein-\nforcement learning: Current and future directions,” arXiv preprint\narXiv:2405.11106, 2024.\n[169] J. Lee, A. Xie, A. Pacchiano, Y . Chandak, C. Finn, O. Nachum, and\nE. Brunskill, “Supervised pretraining can learn in-context reinforce-\nment learning,” Advances in Neural Information Processing Systems ,\nvol. 36, 2024.\n[170] Z. Buc ¸inca, S. Swaroop, A. E. Paluch, S. A. Murphy, and K. Z.\nGajos, “Towards optimizing human-centric objectives in ai-assisted\ndecision-making with offline reinforcement learning,” arXiv preprint\narXiv:2403.05911, 2024.\n[171] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis et al. , “Retrieval-augmented generation\nfor knowledge-intensive nlp tasks,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2005.11401\n[172] L. Wang, X. Zhang, H. Su, and J. Zhu, “A comprehensive survey of\ncontinual learning: theory, method and application,” IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2024.\n[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\nJ. Tang et al., “A survey on large language model based autonomous\nagents,” Frontiers of Computer Science, vol. 18, no. 6, p. 186345, 2024.\n[174] Y . Cheng, C. Zhang, Z. Zhang, X. Meng, S. Hong, W. Li, Z. Wang,\nZ. Wang et al. , “Exploring large language model based intelli-\ngent agents: Definitions, methods, and prospects,” arXiv preprint\narXiv:2401.03428, 2024.\n[175] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling,\nP. Rohlfshagen, S. Tavener, D. Perez et al., “A survey of monte carlo\ntree search methods,”IEEE Transactions on Computational Intelligence\nand AI in games , vol. 4, no. 1, pp. 1–43, 2012.\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 22\n[176] Z. Zhang, X. Bo, C. Ma, R. Li, X. Chen, Q. Dai, J. Zhu, Z. Dong et al.,\n“A survey on the memory mechanism of large language model based\nagents,” 2024. [Online]. Available: https://arxiv.org/abs/2404.13501\n[177] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla:\nLarge language model connected with massive apis,” arXiv preprint\narXiv:2305.15334, 2023.\n[178] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse,\nS. Jain et al. , “Webgpt: Browser-assisted question-answering with\nhuman feedback,” arXiv preprint arXiv:2112.09332 , 2021.\n[179] Y . Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing\nthe power of intelligent llm agents,” arXiv preprint arXiv:2306.03314,\n2023.\n[180] Z. Lin, S. Trivedi, and J. Sun, “Generating with confidence:\nUncertainty quantification for black-box large language models,”\n2024. [Online]. Available: https://arxiv.org/abs/2305.19187\n[181] R. S. Y . C. Tan, Q. Lin, G. H. Low, R. Lin, T. C. Goh, C. C. E.\nChang, F. F. Lee, W. Y . Chanet al., “Inferring cancer disease response\nfrom radiology reports using large language models with data augmen-\ntation and prompting,” Journal of the American Medical Informatics\nAssociation, vol. 30, no. 10, pp. 1657–1664, 2023.\n[182] Q. Gao, C. Zhao, Y . Sun, T. Xi, G. Zhang, B. Ghanem, and J. Zhang, “A\nunified continual learning framework with general parameter-efficient\ntuning,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2023, pp. 11 483–11 493.\n[183] W. Zhou, Y . E. Jiang, R. Cotterell, and M. Sachan, “Efficient\nprompting via dynamic in-context learning,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2305.11170\n[184] Z.-F. Gao, P. Liu, W. X. Zhao, Z.-Y . Lu, and J.-R. Wen, “Parameter-\nefficient mixture-of-experts architecture for pre-trained language\nmodels,” 2022. [Online]. Available: https://arxiv.org/abs/2203.01104\n[185] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling\nwith selective state spaces,” 2024. [Online]. Available: https:\n//arxiv.org/abs/2312.00752\n[186] L. D. Corro, A. D. Giorno, S. Agarwal, B. Yu, A. Awadallah,\nand S. Mukherjee, “Skipdecode: Autoregressive skip decoding with\nbatching and caching for efficient llm inference,” 2023. [Online].\nAvailable: https://arxiv.org/abs/2307.02628\n[187] Y . Chen, Z. han Ding, Z. Wang, Y . Wang, L. Zhang, and S. Liu,\n“Asynchronous large language model enhanced planner for autonomous\ndriving,” 2024. [Online]. Available: https://arxiv.org/abs/2406.14556\n[188] Z. Charles, A. Ganesh, R. McKenna, H. B. McMahan, N. Mitchell,\nK. Pillutla, and K. Rush, “Fine-tuning large language models\nwith user-level differential privacy,” 2024. [Online]. Available:\nhttps://arxiv.org/abs/2407.07737\n[189] E. Cambria, L. Malandri, F. Mercorio, N. Nobani, and A. Seveso,\n“Xai meets llms: A survey of the relation between explainable\nai and large language models,” 2024. [Online]. Available: https:\n//arxiv.org/abs/2407.15248\n[190] S. Xhonneux, A. Sordoni, S. G ¨unnemann, G. Gidel, and L. Schwinn,\n“Efficient adversarial training in llms with continuous attacks,” 2024.\n[Online]. Available: https://arxiv.org/abs/2405.15589\n[191] C. Deng, Y . Duan, X. Jin, H. Chang, Y . Tian, H. Liu, H. P. Zou,\nY . Jinet al., “Deconstructing the ethics of large language models from\nlong-standing issues to new-emerging dilemmas,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2406.05392",
  "topic": "Taxonomy (biology)",
  "concepts": [
    {
      "name": "Taxonomy (biology)",
      "score": 0.6439266800880432
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6134066581726074
    },
    {
      "name": "Computer science",
      "score": 0.5299611687660217
    },
    {
      "name": "Reinforcement",
      "score": 0.45072177052497864
    },
    {
      "name": "Natural language processing",
      "score": 0.3875667452812195
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35902589559555054
    },
    {
      "name": "Psychology",
      "score": 0.25320082902908325
    },
    {
      "name": "Social psychology",
      "score": 0.08666303753852844
    },
    {
      "name": "Ecology",
      "score": 0.05988532304763794
    },
    {
      "name": "Biology",
      "score": 0.05105516314506531
    }
  ],
  "institutions": [],
  "cited_by": 47
}