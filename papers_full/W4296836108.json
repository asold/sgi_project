{
  "title": "Analysis of CT scan images for COVID-19 pneumonia based on a deep ensemble framework with DenseNet, Swin transformer, and RegNet",
  "url": "https://openalex.org/W4296836108",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098689762",
      "name": "Lihong Peng",
      "affiliations": [
        "Hunan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098292663",
      "name": "Chang Wang",
      "affiliations": [
        "Hunan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135031755",
      "name": "Geng Tian",
      "affiliations": [
        "Cipher Gene (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2123439527",
      "name": "Guangyi Liu",
      "affiliations": [
        "Hunan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105140785",
      "name": "Gan Li",
      "affiliations": [
        "Hunan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2558360690",
      "name": "Yuankang Lu",
      "affiliations": [
        "Hunan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2166560538",
      "name": "Jialiang Yang",
      "affiliations": [
        "Cipher Gene (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1986933015",
      "name": "Min Chen",
      "affiliations": [
        "Hunan Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2146048970",
      "name": "Zejun Li",
      "affiliations": [
        "Hunan Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098689762",
      "name": "Lihong Peng",
      "affiliations": [
        "Hunan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098292663",
      "name": "Chang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135031755",
      "name": "Geng Tian",
      "affiliations": [
        "Annoroad Gene Technology (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2123439527",
      "name": "Guangyi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105140785",
      "name": "Gan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2558360690",
      "name": "Yuankang Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166560538",
      "name": "Jialiang Yang",
      "affiliations": [
        "Annoroad Gene Technology (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1986933015",
      "name": "Min Chen",
      "affiliations": [
        "Hunan Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2146048970",
      "name": "Zejun Li",
      "affiliations": [
        "Hunan Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3083753334",
    "https://openalex.org/W3168937842",
    "https://openalex.org/W4200523415",
    "https://openalex.org/W4212883601",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2799307902",
    "https://openalex.org/W2888358121",
    "https://openalex.org/W2962928958",
    "https://openalex.org/W3161263649",
    "https://openalex.org/W4207005326",
    "https://openalex.org/W2998957378",
    "https://openalex.org/W3004906315",
    "https://openalex.org/W2955805844",
    "https://openalex.org/W2970602317",
    "https://openalex.org/W4206930139",
    "https://openalex.org/W3109728240",
    "https://openalex.org/W2070493638",
    "https://openalex.org/W3039785832",
    "https://openalex.org/W1968969471",
    "https://openalex.org/W3163598698",
    "https://openalex.org/W3002256141",
    "https://openalex.org/W2104548316",
    "https://openalex.org/W4207078076",
    "https://openalex.org/W3198147676",
    "https://openalex.org/W4220859562",
    "https://openalex.org/W3093070524",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W3006643024",
    "https://openalex.org/W3009045559",
    "https://openalex.org/W3215296137",
    "https://openalex.org/W3119527628",
    "https://openalex.org/W4205415778",
    "https://openalex.org/W2789758093",
    "https://openalex.org/W3100327638",
    "https://openalex.org/W3121263745",
    "https://openalex.org/W4285076853",
    "https://openalex.org/W4214881290",
    "https://openalex.org/W4225501828",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2794284562",
    "https://openalex.org/W4200110750",
    "https://openalex.org/W3005079553",
    "https://openalex.org/W3102469298",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W4224254299",
    "https://openalex.org/W3179572443",
    "https://openalex.org/W3095676075",
    "https://openalex.org/W3004280078",
    "https://openalex.org/W3131348115",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W3165656738",
    "https://openalex.org/W3136933888",
    "https://openalex.org/W2765950793"
  ],
  "abstract": "COVID-19 has caused enormous challenges to global economy and public health. The identification of patients with the COVID-19 infection by CT scan images helps prevent its pandemic. Manual screening COVID-19-related CT images spends a lot of time and resources. Artificial intelligence techniques including deep learning can effectively aid doctors and medical workers to screen the COVID-19 patients. In this study, we developed an ensemble deep learning framework, DeepDSR, by combining DenseNet, Swin transformer, and RegNet for COVID-19 image identification. First, we integrate three available COVID-19-related CT image datasets to one larger dataset. Second, we pretrain weights of DenseNet, Swin Transformer, and RegNet on the ImageNet dataset based on transformer learning. Third, we continue to train DenseNet, Swin Transformer, and RegNet on the integrated larger image dataset. Finally, the classification results are obtained by integrating results from the above three models and the soft voting approach. The proposed DeepDSR model is compared to three state-of-the-art deep learning models (EfficientNetV2, ResNet, and Vision transformer) and three individual models (DenseNet, Swin transformer, and RegNet) for binary classification and three-classification problems. The results show that DeepDSR computes the best precision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, AUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and significantly outperforms other methods. Furthermore, DeepDSR obtains the best precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score of 0.9695 under three-classification problem, further suggesting its powerful image identification ability. We anticipate that the proposed DeepDSR framework contributes to the diagnosis of COVID-19.",
  "full_text": "Frontiers in Microbiology 01 frontiersin.org\nAnalysis of CT scan images for \nCOVID-19 pneumonia based on \na deep ensemble framework \nwith DenseNet, Swin \ntransformer, and RegNet\nLihong Peng 1,2, Chang Wang 1, Geng Tian 3, Guangyi Liu 1, \nGan Li 1, Yuankang Lu 1, Jialiang Yang 3, Min Chen 4* and \nZejun Li 4*\n1 School of Computer Science, Hunan University of Technology, Zhuzhou, China, 2 College of Life \nSciences and Chemistry, Hunan University of Technology, Zhuzhou, China, 3 Geneis (Beijing) Co., \nLtd., Beijing, China, 4 School of Computer Science, Hunan Institute of Technology, Hengyang, China\nCOVID-19 has caused enormous challenges to global economy and public \nhealth. The identification of patients with the COVID-19 infection by CT scan \nimages helps prevent its pandemic. Manual screening COVID-19-related CT \nimages spends a lot of time and resources. Artificial intelligence techniques \nincluding deep learning can effectively aid doctors and medical workers to \nscreen the COVID-19 patients. In this study, we developed an ensemble deep \nlearning framework, DeepDSR, by combining DenseNet, Swin transformer, \nand RegNet for COVID-19 image identification. First, we  integrate three \navailable COVID-19-related CT image datasets to one larger dataset. Second, \nwe  pretrain weights of DenseNet, Swin Transformer, and RegNet on the \nImageNet dataset based on transformer learning. Third, we continue to train \nDenseNet, Swin Transformer, and RegNet on the integrated larger image \ndataset. Finally, the classification results are obtained by integrating results \nfrom the above three models and the soft voting approach. The proposed \nDeepDSR model is compared to three state-of-the-art deep learning models \n(EfficientNetV2, ResNet, and Vision transformer) and three individual models \n(DenseNet, Swin transformer, and RegNet) for binary classification and three-\nclassification problems. The results show that DeepDSR computes the best \nprecision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, \nAUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and \nsignificantly outperforms other methods. Furthermore, DeepDSR obtains the \nbest precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score \nof 0.9695 under three-classification problem, further suggesting its powerful \nimage identification ability. We  anticipate that the proposed DeepDSR \nframework contributes to the diagnosis of COVID-19.\nKEYWORDS\nCOVID-19 pneumonia, CT scan image, deep ensemble, DenseNet, Swin transformer, \nRegNet\nTYPE Original Research\nPUBLISHED 23 September 2022\nDOI 10.3389/fmicb.2022.995323\nOPEN ACCESS\nEDITED BY\nQi Zhao,  \nUniversity of Science and Technology \nLiaoning, China\nREVIEWED BY\nCangzhi Jia,  \nDalian Maritime University,  \nChina\nBing Wang,  \nAnhui University of Technology, China\n*CORRESPONDENCE\nMin Chen  \nchenmin@hnit.edu.cn  \nZejun Li  \nlzjfox@hnit.edu.cn\nSPECIALTY SECTION\nThis article was submitted to  \nSystems Microbiology,  \na section of the journal  \nFrontiers in Microbiology\nRECEIVED 15 July 2022\nACCEPTED 22 August 2022\nPUBLISHED 23 September 2022\nCITATION\nPeng L, Wang C, Tian G, Liu G, Li G, Lu Y, \nYang J, Chen M and Li Z (2022) Analysis of \nCT scan images for COVID-19 pneumonia \nbased on a deep ensemble framework with \nDenseNet, Swin transformer, and RegNet.\nFront. Microbiol. 13:995323.\ndoi: 10.3389/fmicb.2022.995323\nCOPYRIGHT\n© 2022 Peng, Wang, Tian, Liu, Li, Lu, Yang, \nChen and Li. This is an open-access article \ndistributed under the terms of the Creative \nCommons Attribution License (CC BY). The \nuse, distribution or reproduction in other \nforums is permitted, provided the original \nauthor(s) and the copyright owner(s) are \ncredited and that the original publication in \nthis journal is cited, in accordance with \naccepted academic practice. No use, \ndistribution or reproduction is permitted \nwhich does not comply with these terms.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 02 frontiersin.org\nIntroduction\nIn December 2019, a novel acute atypical respiratory disease, \nCOVID-19, has broken in Wuhan, China (Ksiazek et al., 2003; \nZhou et al., 2020). COVID-19 was defined as a global pandemic \nby the World Health Organization on 3 November 2020. Till 26 \nJune 2022, this disease has infected over 541 million individuals \nand caused over 6.3 million deaths (COVID Live—Coronavirus \nStatistics—Worldometer, 2022). COVID-19 has exacerbated \nhuman suffering, damaged the global economy, and seriously \naffected the health, environmental and social fields worldwide \n(Mofijur et  al., 2021). It has still indirectly affected the global \neducational and religions level. Moreover, it has caused healthcare \nservice resources to the brink in many countries and regions and \nwill deeply affect medical research ( Harper et  al., 2020 ). \nFurthermore, middle-income countries especially low-income \ncountries remain more vulnerable in preventing COVID-19 and \nneed to face more serious challenges (Peters et al., 2020).\nThe COVID-19 pandemic has caused severe challenges to \nglobal public health ( Wang et al., 2020; Sun et al., 2022a ). The \nscreening of massive samples each day overwhelms laboratories \nworldwide ( Agaoglu et  al., 2022 ). Detection of SARS-CoV-2 \nthrough RT-PCR from a nasopharyngeal swab sample is the most \ncommon avenue to diagnose COVID-19. However, RT-PCR does \nnot demonstrate powerful sensitivity and specificity ( Pu et al., \n2022). Moreover, it need spend about 6 h for sampling and \nconsecutive tests to distinguish false positives and false negatives \n(Lee et  al., 2022 ). Multiple patients demonstrate clinical, \nlaboratorial, and radiological features related to COVID-19, \nhowever, their RT-PCR test results are negative (Saad Menezes \net al., 2022).\nMany evidences have suggested that chest Computer \nTomography (CT) is an accurate and efficient COVID-19 \ndiagnosis avenue (Chung et al., 2020; Pan et al., 2020; Wang C C \net al., 2021; Wang B et al., 2021). It has high sensitivity and low \nmisdiagnosis rate, thus is an efficient complement to RT-PCR \n(Fields et al., 2021). Although it is vital to rapidly detect patients \nwith the COVID-19 infection by CT images, expert thoracic \nradiologists are not likely to immediately diagnose positive cases \nat all times, which may not only cause treatment delay, but also \nurge further transmission of COVID-19 because the COVID-19 \npatients are not promptly isolated (Jin et al., 2020; Shorten et al., \n2021; Afshar et  al., 2022 ). In this situation, it is especially \nimportant to aid doctors and health care workers to distinguish \nCOVID-19-related CT images from non-COVID-19-realted CT \nimages using artificial intelligence techniques.\nMany studies have suggested that artificial intelligence (AI) \ntechniques including machine learning obtained enormous \nsuccess in bioinformatics and medical image analysis (Chen et al., \n2018a,b, 2019; Wang B et al., 2021; Wang C C et al., 2021; Zhang \net  al., 2021; Y ang et  al., 2022; Liu et  al., 2022a). Over the last \ndecade years, deep learning techniques have outperformed \nnumerous state-of-the-art machine learning algorithms and \ndemonstrated excellent learning ability in many fields including \nimage recognition (Voulodimos et al., 2018; Wang B et al., 2021; \nWang C C et al., 2021;Sun et al., 2022; Liu et al., 2022a,b).\nUnder the situation of no standardization, artificial \nintelligence technologies, especially deep learning, have been \nwidely applied to data collection and performance evaluation \nfor COVID-19 ( Roberts et  al., 2021 ). Abbas et  al. (2021)  \nproposed a novel convolutional neural network (CNN) model, \nDeTraC, to classify COVID-19-related chest X-ray images based \non feature extraction, decomposition and class composition. \nShalbaf and Vafaeezadeh (2021)  designed a deep transfer \nlearning-based ensemble model with different pre-trained CNN \narchitectures to detect CT images for novel COVID-19 \ndiagnosis. Zhang et al. (2020)  developed a deep learning-based \nanomaly detection system to screen COVID-19 from chest \nx-ray images. Zhou et al. (2021)  explored an ensemble deep \nlearning framework to detect COVID-19 from CT images. \nKarbhari et  al. (2021)  introduced an auxiliary classifier \ngenerative adversarial network to generate synthetic chest X-ray \nimages and further detect COVID-19 based on custom-made \ndeep learning model. Chouat et  al. (2022)  exploited deep \ntransfer learning algorithm to screen COVID-19 positive \npatients based on CT scan and chest X ray images. Fan et al. \n(2022) proposed a branch network model by combining CNN \nand transformer structure for the identification of COVID-19 \nusing CT scan images. Ter-Sarkisov (2022) built a COVID-CT-\nMask-Net model to diagnose COVID-19 through regional \nfeatures from chest CT scan images. Chieregato et al. (2022)  \npresented a deep learning-based COVID-19 prognostic hybrid \nmodel to support clinical decision making.\nThese models are mainly based on CNN and attention \nmechanism and effectively classify COVID-19-related images and \nnon-COVID-19-related ones. However, they remain to improve \nthe classification performance. In this study, we  developed an \nensemble deep learning framework (DeepDSR) by integrating \nthree state-of-the-art neural networks including DenseNet, Swin \ntransformer, and RegNet for the COVID-19 diagnosis.\nMaterials and methods\nMaterials\nWe use three available CT image datasets related to COVID-19 \nto investigate the performance of our proposed DeepDSR model. \nDataset 1 can be  downloaded from https://www.kaggle.com/\ndatasets/plameneduardo/a-covid-multiclass-dataset-of-ct-scans. \nIt contains publicly available 4,173 CT scan images from 210 \ndifferent patients, out of which 2,168 images are from 80 patients \ninfected by COVID-19 and confirmed by RT-PCR in hospitals \nfrom Sao Paulo, Brazil ( Soares et  al., 2020 ). Dataset 2 can \nbe  downloaded from https://www.kaggle.com/datasets/\nplameneduardo/sarscov2-ctscan-dataset. It contains 1,252 CT \nscan images from patients infected by COVID-19 and 1,230 CT \nscan images for patients non-infected by COVID-19 in hospitals \nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 03 frontiersin.org\nfrom Sao Paulo, Brazil ( Soares et  al., 2020 ). Dataset 3 can \nbe downloaded from https://github.com/UCSD-AI4H/COVID-\nCT. It contains 349 COVID-19 CT images from 216 patients and \n463 non-COVID-19 CT images (Zhao et al., 2020).\nTo boost the generalization ability of our proposed DeepDSR \nmodel, we integrate the above three datasets to one larger dataset. \nConsequently, DeepDSR can be used to effectively classify CT \nimages in both individual datasets and other datasets. And \nwe remove images with poor imaging and ones nonconforming to \nspecifications. Finally, we obtain one dataset with 7,398 pulmonary \nCT images, which include 3,768 CT images from patients with the \nCOVID-19 infections, 1,247 ones with other pneumonia \ninfections, and 2,383 ones from normal lungs. We  use 3,768 \nCOVID-19-related images and 2,383 normal CT images to train \nthe models for binary classification problems and use all 7,398 \nimages for three classification problems. As shown in Figure 1, \nLines 1–3 show pulmonary CT images from patients with \nCOVID-19 infections, normal lungs, and patients with other \npneumonia infections, respectively.\nThe pipeline of DeepDSR\nIt is difficult to obtain the best prediction accuracy when only \nthousands of images are trained. Thus, we design an ensemble \nmodel to reduce the limitation of lack of data through transfer \nlearning. The ensemble model integrates three state-of-the-art and \ndifferent network architectures, that is, DenseNet, Swin \ntransformer and RegNet. The pipeline is shown as Figure 2. As \nshown in Figure 2, first, we preprocess data by integrating three \navailable COVID-19-related CT image datasets to one larger \ndataset. Second, we  pretrain weights of DenseNet, Swin \ntransformer, and RegNet on the ImageNet dataset based on \ntransformer learning. Third, we continue to train DenseNet, Swin \nTransformer, and RegNet on the integrated larger dataset. Finally, \nthe classification results are obtained by integrating results from \nthe above three models and the soft voting approach.\nDenseNet\nCNNs can implement accurate and efficient train when they \ncontain shorter connections between layers close to the input and \nthose close to the output. Traditional convolutional networks \ncomposed of L  layers connect each layer to its subsequent layer. \nInspired by the model proposed by Huang et  al. (2017) , \nwe  introduced a Dense convolutional Network, DenseNet, to \nclassify COVID-19-related CT scan images. DenseNet implements \nconnection between each layer in a feed-forward fashion to \naccurately and efficiently train the model. DenseNet with L  \nlayers has 1\n2 1LL +()  direct connections. At each layer, as shown \nin Figure 3A, the CT image feature maps from all previous layers \nare taken as its inputs and its outputs are taken as the inputs at \nnext layer. For ResNet ( Radosavovic et  al., 2020), the original \nfeatures and the new features are added by element by element to \nachieve the sample features. Differed from ResNet, DenseNet \nobtains shortcut through direct concatenation. DenseNet reduces \nthe vanishing-gradient problem, boosts feature propagation, \nadvances feature reuse while greatly decrease the number \nof parameters.\nSwin transformer\nTransformer has difficulty in application from language to \nvision because of differences between the two areas. Thus, Liu \net  al. developed a hierarchical transformer to obtain data \nrepresentation by shifted windows (Liu et al., 2021). For an image, \nfirst, transformer splits it into fixed-size patches. Second, the \npatches are linearly embedded and added position embeddings. \nThird, the embedded results are feed to a standard Transformer \nencoder. Finally, an extra learnable “classification token” is added \nto the sequence to classify images. Inspired by model proposed by \nLiu et al. (2021), we use the window-shift technique and design a \nSwin transformer to classify COVID-19-related CT scan images.\nThe window-shift technique and the sliding window approach \nare similar in modeling ability, but the former is beneficial for \nall-MLP architectures and has much lower latency than the latter. \nSwin transformer focuses on shifting window partition between \nconsecutive self-attention layers. As shown in Figure  3B, the \nshifted windows connect with the windows in the previous layer, \nthus significantly enhancing the modeling ability. The window-\nshift technique limits self-attention computation to \nnon-overlapping local windows as well as supports cross-window \nconnection, thereby effectively improving the image classification \nability of models. Furthermore, Swin transformer utilizes the \nwindow-shift technique and demonstrates the flexibility when \nFIGURE 1\nImage examples in dataset.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 04 frontiersin.org\nmodeling on COVID-19-related image identification as well as \ncomputational complexity linearly with image size.\nRegNet\nNeural architecture search and RegNet are two representative \nneural network design paradigms. The two complementary design \nparadigms can improve the efficiency of search algorithms while \ndevelop better models. Neural architecture search mainly focuses \non the search strategy to more efficiently find the best network \ninstances in a fixed and manually designed search space. In \ncontrast, RegNet ( Radosavovic et  al., 2020 ) more focuses on \ndesigning paradigms on novel design spaces.\nRegNet is a novel neural network design paradigm. It used a \nresidual network to simplify the deeper network training. It can \nboost the understanding of network design and further investigate \ndesign principles with strong generalize abilities across different \nsettings. Instead of concentrating on individual network instance \ndesign, RegNet designs network design spaces that can \nparameterize network populations. The design process is similar \nto manually design network while advances the design space level. \nConsequently, we  can obtain a low-dimensional design space \ncomposed of multiple simple and regular networks.\nFIGURE 2\nThe pipeline for COVID-19-related CT image classification based on an ensemble of DenseNet, RegNet, and Swin transformer.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 05 frontiersin.org\nIn this study, RegNet composes a stem with the stride of 2 and \n32 33×  convolution kernels, followed by a network body \ncomposed of a series of stages, and finally a head. In the network \nbody, each stage operates at gradually reduced resolution. It \nconsists of multiple identical blocks with the stride of 1 except that \nthe first block uses stride-two convolution kernel. The head is \ncomposed of an average pooling layer and a fully connected layer. \nIt is used to output n  classes.\nIn addition, RegNet contains RegNetX and RegNetY \ncomposed of RegNetX and squeeze-and-excitation network. As \nshown in Figure 3C, the squeeze-and-excitation network generally \ncomposed of one global average pooling layer and two fully \nconnection layers that separately use ReLU and sigmoid as \nactivation functions.\nEnsemble\nAlthough machine learning techniques achieve significant \nsuccesses in knowledge discovery, they fail to obtain powerful \nperformance because of imbalanced, high-dimensional and noisy \nfeatures of data. Consequently, ensemble learning, which \neffectively integrates the prediction results from multiple \nindividual classifiers, has been widely applied to image processing \n(Sagi and Rokach, 2018).\nEnsemble learning methods first generate multiple weak \npredictive results using different machine learning models, \nand obtain better performance by ensemble of the results from \neach individual model with different voting strategies. It \ncomposes of five main types: bagging, AdaBoost, gradient \nA\nB\nC\nFIGURE 3\n(A) The DenseNet Block; (B) Shifted-Window technique; (C) The Squeeze-and-Excitation network.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 06 frontiersin.org\nboosting, random forest, and random sub-space ( Dong et al., \n2020). Bagging generates sample subsets based on the random \nsampling approach, and train basic learners in a parallel \nmanner (Breiman, 1996 ). AdaBoost concentrates on improving \nclassification ability of individual models via iteratively \nadjusting weights for all misclassified samples ( Hastie et al., \n2009). Gradient boosting achieves sample subsets based on the \nrandom sampling approach, and trains each classifier to \nalleviate the residuals caused by the previous model. Thus, \ngradient boosting better fits the real data ( Friedman, 2002 ). \nRandom forest takes decision trees as predictors and separately \ntrains multiple models to reduce the overfitting problem \n(Breiman, 2001 ). Random subspace constructs a set of feature \nsubspaces based on the random sampling approach, and trains \nlearners on the feature subspace set. Finally, it obtains the final \nclassification by combining the results from each individual \nclassifier ( Ho, 1995).\nEnsemble learning utilizes different ensemble strategies to \nensemble results from individual models. For regression \nestimation, it gains the final results via averaging all \npredictions. For classification, ensemble learning uses the \nvoting method to achieve the final classification by combining \neach individual classifier. The absolute majority voting \napproach takes the same classification result as one from more \nthan half of individual classifiers as the final result, and the \nrelative majority voting approach takes the classification result \nwhere the number of individual predictors involved in a \ncertain prediction is the largest as the final result. Therefore, \nwe combine DenseNet, Swin transformer, and RegNet and \ndevelop an ensemble deep learning model, DeepDSR, to \nimprove the COVID-19 classification performance of \nthe model.\nThe classification scores from the three individual classifiers \nare integrated based on the soft voting approach. Given a query \nimage, for a binary classification problem, suppose that its scores \nclassified to COVID-19-related image by DenseNet, Swin \ntransformer, and RegNet are S1\n19covid - , S2\n19covid - , and \nS3\n19covid - , respectively, its final score S final\ncovid -19  classified to \nCOVID-19-related sample can be represented by Eq. (1):\n S SSSfinal\ncovidc ovid covidc ovid- ---=+ +19\n1\n19\n2\n19\n3\n19\n (1)\nSimilarly, its final score S final\nnon covid-- 19  classified to \nnon-COVID-19-related image can be represented by Eq. (2):\n \n19 19 19\n12\n19\n3\n-- -- --\n--\n=+\n+\nnon covid non covid non covid\nfinal\nnon covid\nSSS\nS  (2)\nThe image will be  taken as COVID-19 related when \nSSfinal\ncovid\nfinal\nnon covid-- ->19 19 , it will be taken as non-COVID-19 \nrelated, otherwise.\nFurthermore, for a three-classification problem, suppose that \nits scores classified to COVID-19 related by DenseNet, Swin \ntransformer, and RegNet are S1\n19covid - , S2\n19covid - , and \nS3\n19covid - , respectively, its final score S final\ncovid -19  classified to \npositive sample can be computed by Eq. (3):\n S SSSfinal\ncovidc ovid covid pcovid-- - -=+ +19\n1\n19\n2\n19\n3\n19\n (3)\nSimilarly, its final score S final\nothe r  classified to other pneumonia \ncan be computed by Eq. (4):\n S SSSfinal\nothe ro ther othe ro ther=+ +12 3  (4)\nAnd its final score S final\nnorma l  from normal lung can \nbe computed by Eq. (5):\n S SSSfinal\nnormal normal normal norma l=+ +12 3  (5)\nFinally, the image will be taken as COVID-19 related when \nS final\ncovid -19  is larger than S final\nothe r  and S final\nnorma l ; it will be taken as \nother pneumonia when S final\nothe r  is larger than other two values; it \nis from normal lung otherwise.\nTransfer learning and pre-training\nCNNs usually need to train a mass of parameters. However, it \nis almost impossible to learn such massive parameters only \nthrough a few training images ( Zhuang et al., 2020; Zhu et al., \n2021). In particular, transfer learning can utilize existing \nknowledge and transfer knowledge from source domains to the \ntarget domain and thus has been widely applied to solve problems \nin different while relevant fields (Pan and Y ang, 2009; Weiss et al., \n2016). It usually pretrains weights on a large-scale dataset using a \nstandard neural architecture and then fine-tunes the weights on a \ntarget dataset. It has been successfully applied to medical image \nclassification, for instance, cancer classification, pneumonia \ndetection, and skin lesion identification ( Chang et  al., 2017 ; \nDeepak and Ameer, 2019 ; Khalifa et  al., 2019 ; Chouhan \net al., 2020).\nFurthermore, existing lung CT scan images do not satisfy the \nneed of a powerful image identification model because most of \nlung CT images are not publicly available. In addition, a image \nprocessed by random affine transformation, random crop or flip \nmay not be a complete lung CT image because of the specificity of \nCT scanning techniques. The above two situations may easily \nproduce the overfitting problem of image classification models in \nsmall datasets. Therefore, we  want to pretrain the proposed \nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 07 frontiersin.org\nDeepDSR model by transfer learning to advance the training \nspeed, reduce overfitting, alleviate problems produced by \ninsufficient data, and further improve the classification \nperformance (Hijab et al., 2019; Cherti and Jitsev, 2021; Mustafa \net al., 2021).\nFinally, we developed an ensemble deep model (DeepDSR) to \nanalyze COVID-19 CT images by combining DenseNet, Swin \ntransformer, and RegNet. First, we  integrate three COVID-19 \nimage dataset to one larger dataset. Second, we pretrain weights \nof DenseNet, Swin Transformer, and RegNet on the ImageNet \ndataset. Third, we repeatedly select 80% of CT images from the \nintegrated larger dataset as the training set and the remaining 20% \nas the test set. Fourth, the training set is used to train DenseNet, \nSwin transformer, and RegNet, respectively. The test set is used to \ntest the performance of DenseNet, Swin transformer, and RegNet, \nrespectively. Finally, the final classification results are obtained by \nintegrating the results from the above three models.\nResults\nExperimental evaluation and parameter \nsettings\nTo evaluate the performance of the proposed DeepDSR \nframework, we use six measurement metrics: precision, recall, \naccuracy, F1-score, AUC and AUPR. Suppose that True Positive \n(TP), True Negative (TN), False Negative (FN), and False Negative \n(FN) are defined as Table 1. We can compute precision, recall, \naccuracy, F1-score, True Positive Rate (TPR), and False Positive \nRate (FGR) as follows:\n \nPrecision TP\nTP FP= +  \n(6)\n \nRecall TP\nTP FN= +  \n(7)\n \nAccuracy TP TN\nTP TN FP FN= +\n++ +  \n(8)\n \n1\n2××-= +\nPrecision RecallF score Precision Recall  \n(9)\n \nTPR TP\nTP FN= +  \n(10)\n \nFPR FP\nTN FP= +  \n(11)\nAnd AUC is the area under the TPR-FPR curve, and AUPR is \nthe area under the precision-recall curve. For each sample (image), \nits classification scores from three individual networks (DenseNet, \nSwin transformer, and RegNet) are computed by the softmax layer, \nrespectively. Its final classification probability is obtained by \naveraging the scores from the three single models. AUC and \nAUPR can be  computed based on the obtained final \nclassification probability.\nMoreover, the six metrics are not equally important to \nCOVID-19 CT image classification. The results caused by false \nnegatives are more severe than ones caused by false positives for \nmedical image classification. Therefore, recall and AUPR are more \nimportant compared to the other four evaluation metrics.\nThe experiments are performed for 100 epochs to obtain the \noptimal parameter settings. In addition, DenseNet and RegNet use \nstochastic gradient descent algorithm and Swim transformer uses \nAdamW as optimizer to update parameters. The detailed \nparameters are set in Table  2. In Table  2 and the following \nTables 2–5, the bold font in each column represents the best \nperformance computed by corresponding method.\nThe performance comparison of \nDeepDSR with other three models for \nCOVID-19 image binary classification\nWe compare the proposed DeepDSR method to three \nstate-of-the-art deep learning models (efficientNetV2, ResNet, \nand Vision transformer) when classifying CT scan images to \ntwo classes: COVID-19 related or non-COVID-19 related. \nEfficientNetV2 ( Tan and Le, 2021 ) aims to solve the problem \nof slow training when the size of the training image is large in \nefficientNetV1. Moreover, it replaced some MBConv \nstructures in shallow layers with Fused-MBConv structures \nand found the optimal combination through neural \narchitecture search technology to improve the network \ntraining speed. Finally, efficientNetV2 used a non-uniform \nscaling strategy to scale the model and thus make the model \nmore reasonable.\nResNet (He et al., 2016) aims to solve the vanishing gradient \nand network degradation problems in traditional neural networks. \nTABLE 1 The confusion matrix.\nTrue results\nPositive Negative\nPredicted results Positive TP FP\nNegative FN TN\nTABLE 2 Parameter settings.\nModel Parameter setting\nSwin transformer epochs = 100, batch_size = 8, lr = 0.0001\nRegNet epochs = 100, batch_size = 16, lr = 0.001, lrf = 0.01\nDenseNet epochs = 100, batch_size = 16, lr = 0.001, lrf = 0.01\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 08 frontiersin.org\nResNet solved the vanishing gradient problem through data \npreprocessing and batch normalization layer, and reduced the \nnetwork degradation problem through a residual structure. \nResNet used a connection model of shortcut to add interlayers in \nthe feature matrix and thus greatly improve the depth of \nthe network.\nTransformer (Vaswani et al., 2017) has been broadly used in \nthe natural language processing field. Attention mechanism has \nbeen widely used in the computer vision field. Inspired by the \ntransformer mechanism, Vaswani et al. divided each image into \npatches, and took the linear embedded sequence of these image \nblocks as the input of the transformer. The processing method of \nimage patches is the same as marks in NLP applications. Vision \ntransformer (Dosovitskiy et al., 2020) achieved excellent results \nwhen both pretraining on a sufficient scale dataset and migrating \nto tasks with fewer data points.\nWe first selected 80% images as training set and 20% as \ntest set from the integrated COVID-19-related CT scan \nimages. We then train DeepDSR, efficientNetV2 ( Tan and Le, \n2021), ResNet ( He et  al., 2016 ), and Vision transformer \n(Dosovitskiy et al., 2020 ) for 100 epochs, respectively. The \nresults are shown in Table 3 and Figure 4A . We can find that \nDeepDSR significantly outperforms efficientNetV2 in terms \nof precision, recall, accuracy, F1-score, AUC and AUPR. For \nexamples, DeepDSR outperforms 21.93% and 33.42% \ncompared to efficientNetV2 based on AUC and AUPR, \nrespectively. DeepDSR also performs better than ResNet and \nVision transformer although the improvement is slight. \nFigures 4B ,C illustrate the AUC and AUPR values of DeepDSR \nand other models when classifying COVID-19-related CT \nimages to two classes. The above results show that DeepDSR \ncan efficiently identify CT scan images for patients infected by \nCOVID-19.\nThe performance comparison of \nDeepDSR and three individual models for \nCOVID-19 image binary classification\nTo investigate the image classification performance of the \nproposed DeepDSR model with DenseNet, Swin transformer, and \nRegNet, we conduct experiment for 100 epochs. At each epoch, \nwe  select 80% samples to train DeepDSR, DenseNet, Swin \ntransformer, and RegNet and the remaining 20% to test their \nperformance. Table 4 and Figure 5A demonstrate the prediction \nresults of the above four models. The results show that the \nproposed ensemble model, DeepDSR, outperforms other three \nindividual models in terms of precision, recall, accuracy, F1-score, \nAUC, and AUPR. Figures 5B,C illustrate the AUC and AUPR \nvalues obtained from the above four models. We  find that \nDeepDSR, ensemble of DenseNet, Swin transformer, and RegNet, \ncan more effectively classify CT images to two classes: COVID-\n19-related or not.\nStatics of true positives/negatives and \nfalse positives/negatives\nWe investigate the classification results on 1,231 COVID-19-\nrelated CT images from the test set to more intuitively illustrate \nthe affect of DeepDSR on CT image identification performance. \nTable 5 and Figure 6 give the number of true positives (TP), false \npositives (FP), false negatives (FN), and true negatives (TN) \ncomputed by DeepDSR, DenseNet, Swin transformer, and RegNet, \nrespectively.\nThe results show that DeepDSR, DenseNet, Swin transformer, \nand RegNet misclassify a few samples. DeepDSR computes the \nmost TPs and TNs while the least FPs and FNs. Furthermore, \nefficientNetV2, ResNet, and Vision transformer compute much \nmore FPs and FNs compared with DeepDSR, demonstrating \nhigher error rates. Moreover, DeepDSR, ensemble of DenseNet, \nSwin transformer, and RegNet, outperforms all other three \nindividual models. Thus, the neural network, combining the \npredictions obtained from all the base models, can significantly \nimprove the CT image classification performance of models. In \naddition, the stacking ensemble consisting of all three base models \noutperforms all other combinations. DeepDSR is tuned to utilize \nthose predictions that help improve the classification performance \nand ignore the wrong predictions made by the base models.\nTABLE 4 The performance comparison of DeepDSR and three \nindividual models for binary classification problem.\nPrecision Recall Accuracy F1-\nscore\nAUC AUPR\nSwin \ntransformer\n0.9619 0.9539 0.9675 0.9579 0.9943 0.9924\nRegNet 0.9571 0.9832 0.9764 0.9700 0.9963 0.9949\nDenseNet 0.9770 0.9790 0.9829 0.9780 0.9981 0.9973\nDeepDSR 0.9833 0.9895 0.9894 0.9864 0.9991 0.9986\nThe bold fonts represent the best performance in each column.\nTABLE 3 The performance comparison of DeepDSR and other models for COVID-19 image binary classification.\nPrecision Recall Accuracy F1-score AUC AUPR\nEfficientNetV2 0.5077 0.9015 0.6231 0.6495 0.7800 0.6649\nResNet 0.9786 0.9602 0.9764 0.9693 0.9960 0.9943\nVision transformer 0.9811 0.9769 0.9838 0.9790 0.9982 0.9975\nDeepDSR 0.9833 0.9895 0.9894 0.9864 0.9991 0.9986\nThe bold fonts represent the best performance in each column.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 09 frontiersin.org\nThe affect of transfer learning on the \nperformance\nIn the above sections, we pretrain the weights of DenseNet, \nSwin transformer, and RegNet on the ImageNet dataset and \ncontinue to train the three models on the integrated larger dataset \nfor 100 epochs. We set up a group of control experiments without \npretraining (100 epochs and 200 epochs) to validate the \nimportance of pretraining weights of the models for 100 epochs. \nThe results are shown in Table 6 and Figure 7.\nFrom Table  6 and Figure  7, we  can observe that the \nperformance of network architecture with the pretrained weights \nis much better than that of the network without pretraining \nweights for 100 epochs and 200 epochs. For example, under 100 \nepochs, the pretrained network computes accuracy of 0.9894, \nAUC of 0.9991, and AUPR of 0.9986, outperforming 7.88%, \n2.83%, and 5.61% than the network without pretraining, \nrespectively. In addition, we also investigate the performance of \nDeepDSR with pretraining for 100 epochs and ones without \npretraining for 200 epochs. The results show that the pretrained \nnetwork significantly outperforms the network without \npretraining even for 200 epochs. Accuracy, AUC, and AUPR \nTABLE 5 Statistical analyses of four models on 1,231 images.\nDenseNet Swin \ntransformer\nRegNet DeepDSR\nTN 743 736 733 746\nFN 10 22 8 5\nFP 11 18 21 8\nTP 467 455 469 472\nThe bold fonts represent the best performance in each column.\nA\nBC\nFIGURE 4\n(A) The performance comparison of DeepDSR and other models for COVID-19 image binary classification. (B,C) The AUC and AUPR values of \nDeepDSR and other models for COVID-19 image binary classification.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 10 frontiersin.org\ncomputed by the pretrained network are better 3.83%, 1.27%, and \n1.68% than ones without pretraining for 200 epochs, respectively. \nThe results demonstrate that pretraining based on transfer \nlearning can reduce the training time while improve the \nclassification performance. Finally, when adding epochs on the \npretrained network, however, the performance improvement is \nnot obvious. On the contrary, it even produces drifts and thus \ncauses poorer performance.\nPerformance comparison for \nthree-classification problem\nFinally, we classify CT scan images to three classes to further \nevaluate the robustness and credibility of DeepDSR. We use 7,398 \nlung CT scan images, which contain 3,768 lung CT scan images \nfrom patients infected by COVID-19, 2,383 ones from normal \nlung, and 1,247 ones from patients infected by other pneumonia. \nAnd 80% images are selected the training set and the remaining \nimages are the test set. We  repeatedly conduct the three-\nclassification experiments on the obtained 7,398 images for 100 \nepochs. Table 7 and Figure 8 give precision, recall, accuracy, and \nF1-socre of DeepDSR, other three comparative methods, and \nthree individual models.\nThe results from Table  7 and Figure  8  show that the \nproposed DeepDSR framework significantly outperforms \nefficientNet-V2 and Vision transformer in terms of precision, \nrecall, accuracy, and F1-score. DeepDSR is also better than \nResNet and three individual models based on the above \nmeasurement metrics. For example, DeepDSR computes the \nbest precision of 0.9740, recall of 0.9653, accuracy of 0.9737, \nand F1-score of 0.9695, outperforming 1.93%, 1.27%, 1.31%, \nA\nBC\nFIGURE 5\n(A) The performance comparison of DeepDSR and three individual models for COVID-19 binary classification problem; (B,C) The AUC and AUPR \nvalues of DeepDSR and three individual models for COVID-19 binary classification problem.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 11 frontiersin.org\nand 1.59 compared the second-best methods (DenseNet, \nDenseNet, RegNet, and DenseNet), respectively. The results \ndemonstrate that DeepDSR has better generalization ability \nand can thus be  applied to classify COVID-19-related CT \nscan images.\nConclusion\nCOVID-19 detection through CT scan images has the \ncharacteristics of high sensitivity, low misdiagnosis rate, and high \ncommercial practicability. Therefore, it has been a research \nhotspot to detect COVID-19 through CT scan images based on \ndeep learning. In this study, we developed a deep ensemble model, \nDeepDSR to identify CT scan images for patients infected by \nCOVID-19. DeepDSR combined three different state-of-the-art \nnetwork architectures, DenseNet, Swin transformer, and RegNet. \nFIGURE 6\nStatistical analysis of four methods on 1,231 images.\nTABLE 6 The affect of transfer learning on the performance.\nPrecision Recall Accuracy F1-\nscore\nAUC AUPR\nWith \npre-train\n0.9833 0.9895 0.9894 0.9864 0.9991 0.9986\nWithout \npre-train\n0.8773 0.914 0.9171 0.8953 0.9716 0.9455\nWithout \npre-train \n(200 \nepoch)\n0.9544 0.9224 0.9529 0.9382 0.9866 0.9821\nThe bold fonts represent the best performance in each column.\nFIGURE 7\nThe affect of transfer learning on the performance.\nTABLE 7 The performance of DeepDSR and other models for three-\nclassification problem.\nPrecision Recall Accuracy F1-\nscore\nEfficientNet V2 0.4023 0.4479 0.5132 0.3736\nResNet 0.9487 0.9397 0.9541 0.9439\nVision \ntransformer\n0.7112 0.6264 0.7373 0.6301\nSwin \ntransformer\n0.9488 0.9371 0.9548 0.9424\nRegNet 0.9492 0.9463 0.9568 0.9476\nDenseNet 0.9552 0.953 0.9608 0.9541\nDeepDSR 0.974 0.9653 0.9737 0.9695\nThe bold fonts represent the best performance in each column.\nFIGURE 8\nThe performance of DeepDSR and other models for three-\nclassification problem.\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 12 frontiersin.org\nIt obtained the best performance compared to three classical deep \nlearning models (efficientNetV2, ResNet, and Vision transformer) \nas well as three individual models when classifying CT images to \ntwo classes (COVID-19-related or non-COVID-19-related) or \nthree classes (COVID-19-related, normal pneumonia, and \nhealthy lung).\nEfficientNetV2, ResNet, and Vision transformer are three \nstate-of-the-art deep learning models with different network \narchitectures. The proposed DeepDSR model computed the \nbest measurement values compared with the three network \narchitectures, demonstrating its optimal image classification \nability. Moreover, DeepDSR aggregated three individual deep \nmodels, DenseNet, Swin transformer, and RegNet. Lower \ncorrelations between the three individual models more \nobviously reduced the variance of DeepDSR. In addition, \nDeepDSR also reduced its variance due to the ensemble \nnature. Therefore, DeepDSR, ensemble of different single \nmodels, significantly outperforms the three individual models, \nthereby suggesting its powerful performance.\nOur proposed DeepDSR has three advantages: first, three \nCOVID-19-related CT image datasets were fused to boost the \ngeneralization ability of DeepDSR. Moreover, multiple \nmethods including batch normalization were adopted to \nprevent overfitting. Finally, DeepDSR, ensemble of DenseNet, \nSwin transformer, and RegNet, can more accurately classify \nCT images and thus improve the classification performance. \nHowever, the training of DeepDSR was more complex than \nsingle model, it also spend more time to train and test the \nmodel, and more parameters need to be  adjusted, thereby \nrequiring more computing resources. In the future, we will \ndesign more robust ensemble deep learning models to \naccurately classify images for query diseases including \nCOVID-19 and cancer. In particular, we will further consider \ndeep heterogeneous ensemble framework to accurately \nidentify images for related diseases by ensemble of deep \nlearning model and supervised learning model.\nData availability statement\nThe original contributions presented in the study are included \nin the article/supplementary material. Source code is freely \ndownloadable at: https://github.com/plhhnu/DeepDSR/. Datasets \n1-3 can be downloaded from the following three links: https://\nwww.kaggle.com/datasets/plameneduardo/a-covid-multiclass-  \ndataset-of-ct-scans; https://www.kaggle.com/datasets/plamene \nduardo/sarscov2-ctscan-dataset ; https://github.com/UCSD-\nAI4H/COVID-CT.\nAuthor contributions\nLP , CW , MC, and ZL: conceptualization. LP , CW , and ZL: \nmethodology. CW and MC: software. LP , CW , GT, GLiu: \nvalidation. LP , MC, and ZL: investigation. CW , GLi, and YL: data \ncuration. LP and CW: writing—original draft preparation. LP , GT, \nand JY: writing—review and editing. LP: supervision. LP , CW , and \nMC: project administration. LP , and MC: funding acquisition. All \nauthors contributed to the article and approved the \nsubmitted version.\nFunding\nZL was supported by National Natural Science Foundation of \nChina under grant no. 62172158. LP was supported by the National \nNatural Science Foundation of China under grant no. 61803151. \nGLiu and YL were supported by the Innovation and \nEntrepreneurship Training Program for College Students of Hunan \nProvince under grant no. S202111535031 and the Innovation and \nEntrepreneurship Training Program for College Students of Hunan \nUniversity of Technology under grant no. 20408610119.\nConflict of interest\nGT and JY were employed by Geneis (Beijing) Co. Ltd.\nThe remaining authors declare that the research was conducted \nin the absence of any commercial or financial relationships that \ncould be constructed as a potential conflict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the \nauthors and do not necessarily represent those of their affiliated \norganizations, or those of the publisher, the editors and the \nreviewers. Any product that may be evaluated in this article, or \nclaim that may be made by its manufacturer, is not guaranteed or \nendorsed by the publisher.\nReferences\nAbbas, A., Abdelsamea, M. M., and Gaber, M. M. (2021). Classification of \nCOVID-19 in chest X-ray images using DeTraC deep convolutional neural network. \nAppl. Intell. 51, 854–864. doi: 10.1007/s10489-020-01829-7\nAfshar, P ., Rafiee, M. J., Naderkhani, F ., Heidarian, S., Enshaei, N., Oikonomou, A., \net al. (2022). Human-level COVID-19 diagnosis from low-dose CT scans using a \ntwo-stage time-distributed capsule network. Sci. Rep.  12, 1–11. doi: 10.1038/\ns41598-022-08796-8\nAgaoglu, N. B., Yildiz, J., Dogan, O. A., Kose, B., Alkurt, G., Demirkol, Y . K., et al. \n(2022). COVID-19 PCR test performance on samples stored at ambient temperature. \nJ. Virol. Methods 301:114404. doi: 10.1016/j.jviromet.2021.114404\nBreiman, L. (1996). Bagging predictors. Mach. Learn. 24, 123–140. doi: 10.1007/\nBF00058655\nBreiman, L. (2001). Random forests. Mach. Learn.  45, 5–32. doi: \n10.1023/A:1010933404324\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 13 frontiersin.org\nChang, J., Yu, J., Han, T., Chang, H. J., and Park, E. (2017). A method for \nclassifying medical images using transfer learning: A pilot study on histopathology \nof breast cancer. In 2017 IEEE 19th international conference on e-health networking, \napplications and services (Healthcom) (pp. 1–4). IEEE.\nChen, X., Xie, D., Wang, L., Zhao, Q., Y ou, Z. H., and Liu, H. (2018b). BNPMDA: \nbipartite network projection for MiRNA–disease association prediction[J]. \nBioinformatics 34, 3178–3186. doi: 10.1093/bioinformatics/bty333\nChen, X., Yin, J., Qu, J., and Huang, L. (2018a). MDHGI: matrix \ndecomposition and heterogeneous graph inference for miRNA-disease \nassociation prediction[J]. PLoS Comput. Biol.  14:e1006418. doi: 10.1371/\njournal.pcbi.1006418\nChen, X., Zhu, C. C., and Yin, J. (2019). Ensemble of decision tree reveals potential \nmiRNA-disease associations[J]. PLoS Comput. Biol.  15:e1007209. doi: 10.1371/\njournal.pcbi.1007209\nCherti, M., and Jitsev, J. (2021). Effect of Pre-Training Scale on Intra-and Inter-\nDomain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest \nImages. arXiv [Preprint] arXiv:2106.00116.\nChieregato, M., Frangiamore, F ., Morassi, M., Baresi, C., Nici, S., Bassetti, C., et al. \n(2022). A hybrid machine learning/deep learning COVID-19 severity predictive \nmodel from CT images and clinical data. Sci. Rep.  12, 1–15. doi: 10.1038/\ns41598-022-07890-1 \nChouat, I., Echtioui, A., Khemakhem, R., Zouch, W ., Ghorbel, M., and \nHamida, A. B. (2022). COVID-19 detection in CT and CXR images using deep \nlearning models. Biogerontology 23, 65–84. doi: 10.1007/s10522-021- \n09946-7\nChouhan, V ., Singh, S. K., Khamparia, A., Gupta, D., Tiwari, P ., Moreira, C., et al. \n(2020). A novel transfer learning based approach for pneumonia detection in chest \nX-ray images. Appl. Sci. 10:559. doi: 10.3390/app10020559\nChung, M., Bernheim, A., Mei, X., Zhang, N., Huang, M., Zeng, X., et al. (2020). \nCT imaging features of 2019 novel coronavirus (2019-nCoV). Radiology 295, \n202–207. doi: 10.1148/radiol.2020200230\nDeepak, S., and Ameer, P . M. (2019). Brain tumor classification using deep CNN \nfeatures via transfer learning. Comput. Biol. Med.  111:103345. doi: 10.1016/j.\ncompbiomed.2019.103345\nDong, X., Yu, Z., Cao, W ., Shi, Y ., and Ma, Q. (2020). A survey on ensemble \nlearning. Front. Comp. Sci. 14, 241–258. doi: 10.1007/s11704-019-8208-z\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., \nUnterthiner, T., et al. (2020). An image is worth 16x16 words: transformers for \nimage recognition at scale. arXiv [Preprint] arXiv:2010.11929.\nFan, X., Feng, X., Dong, Y ., and Hou, H. (2022). COVID-19 CT Image Recognition \nAlgorithm based on Transformer and CNN. Displays 72:102150. doi: 10.1016/j.\ndispla.2022.102150\nFields, B. K., Demirjian, N. L., Dadgar, H., and Gholamrezanezhad, A. (2021). \nImaging of COVID-19: CT, MRI, and PET. Semin. Nucl. Med. 51, 312–320. doi: \n10.1053/j.semnuclmed.2020.11.003\nFriedman, J. H. (2002). Stochastic gradient boosting. Comput. Stat. Data Analy. \n38, 367–378. doi: 10.1016/S0167-9473(01)00065-2\nHarper, L., Kalfa, N., Beckers, G. M. A., Kaefer, M., Nieuwhof-Leppink, A. J., \nFossum, M., et al. (2020). The impact of COVID-19 on research[J]. J. Pediatr. Urol. \n16, 715–716. doi: 10.1016/j.jpurol.2020.07.002\nHastie, T., Rosset, S., Zhu, J., and Zou, H. (2009). Multi-class adaboost. Statistics \nand its. Interface 2, 349–360. doi: 10.4310/SII.2009.v2.n3.a8\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image \nrecognition. In Proceedings of the IEEE conference on computer vision and pattern \nrecognition (pp. 770–778).\nHijab, A., Rushdi, M. A., Gomaa, M. M., and Eldeib, A. (2019). Breast cancer \nClassification in Ultrasound Images using Transfer Learning. In 2019 Fifth \ninternational conference on advances in biomedical engineering (ICABME) (pp. 1–4). \nIEEE.\nHo, T. K. (1995). Random Decision Forests. In Proceedings of 3rd international \nconference on document analysis and recognition (Vol. 1, pp. 278–282). IEEE.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017). Densely \nconnected convolutional networks. In Proceedings of the IEEE conference on \ncomputer vision and pattern recognition (pp. 4700–4708).\nJin, S., Wang, B., Xu, H., Luo, C., Wei, L., Zhao, W ., et al. (2020). AI-assisted CT \nimaging analysis for COVID-19 screening: building and deploying a medical AI \nsystem in four weeks. medRxiv [Preprint].\nKarbhari, Y ., Basu, A., Geem, Z. W ., Han, G. T., and Sarkar, R. (2021). Generation \nof synthetic chest X-ray images and detection of COVID-19: A deep learning based \napproach. Diagnostics 11:895. doi: 10.3390/diagnostics11050895\nKhalifa, N. E. M., Loey, M., Taha, M. H. N., and Mohamed, H. N. E. T. (2019). \nDeep transfer learning models for medical diabetic retinopathy detection. Acta \nInformatica Medica 27, 327–332. doi: 10.5455/aim.2019.27.327-332\nKsiazek, T. G., Erdman, D., Goldsmith, C. S., Zaki, S. R., Peret, T., Emery, S., et al. \n(2003). A novel coronavirus associated with severe acute respiratory syndrome. N. \nEngl. J. Med. 348, 1953–1966. doi: 10.1056/NEJMoa030781\nLee, Y ., Kim, Y . S., Lee, D. I., Jeong, S., Kang, G. H., Jang, Y . S., et al. (2022). The \napplication of a deep learning system developed to reduce the time for RT-PCR in \nCOVID-19 detection. Sci. Rep. 12, 1–10. doi: 10.1038/s41598-022-05069-2\nLiu, W ., Jiang, Y ., Peng, L., Sun, X., Gan, W ., Zhao, Q., et al. (2022a). Inferring \ngene regulatory networks using the improved Markov blanket discovery \nalgorithm. Interdiscipl. Sci. Comput. Life Sci.  14, 168–181. doi: 10.1007/\ns12539-021-00478-9\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., et al. (2021). Swin transformer: \nhierarchical vision transformer using shifted windows. In Proceedings of the IEEE/\nCVF International Conference on Computer Vision (pp. 10012–10022).\nLiu, W ., Lin, H., Huang, L., Peng, L., Tang, T., Zhao, Q., et al. (2022b). Identification \nof miRNA-disease associations via deep forest ensemble learning based on \nautoencoder. Brief. Bioinform. 23:bbac104. doi: 10.1093/bib/bbac104\nMofijur, M., Fattah, I. R., Alam, M. A., Islam, A. S., Ong, H. C., Rahman, S. A., \net al. (2021). Impact of COVID-19 on the social, economic, environmental and \nenergy domains: lessons learnt from a global pandemic. Sustain. Product. Consumpt. \n26, 343–359. doi: 10.1016/j.spc.2020.10.016\nMustafa, B., Loh, A., Freyberg, J., MacWilliams, P ., Wilson, M., McKinney, S. M., \net al. (2021). Supervised Transfer Learning at scale for Medical Imaging. arXiv \n[Preprint] arXiv:2101.05913.\nPan, S. J., and Y ang, Q. (2009). A survey on transfer learning. IEEE Trans. Knowl. \nData Eng. 22, 1345–1359. doi: 10.1109/TKDE.2009.191\nPan, F ., Y e, T., Sun, P ., Gui, S., Liang, B., Li, L., et al. (2020). Time course of lung \nchanges on chest CT during recovery from 2019 novel coronavirus (COVID-19) \npneumonia. Radiology 295, 715–721. doi: 10.1148/radiol.2020200370\nPeters, A., Vetter, P ., Guitart, C., Lotfinejad, N., and Pittet, D. (2020). \nUnderstanding the emerging coronavirus: what it means for health security and \ninfection prevention. J. Hosp. Infect. 104, 440–448. doi: 10.1016/j.jhin.2020.02.023\nPu, R., Liu, S., Ren, X., Shi, D., Ba, Y ., Huo, Y ., et al. (2022). The screening value of \nRT-LAMP and RT-PCR in the diagnosis of COVID-19: systematic review and meta-\nanalysis. J. Virol. Methods 300:114392. doi: 10.1016/j.jviromet.2021.114392\nRadosavovic, I., Kosaraju, R. P ., Girshick, R., He, K., and Dollár, P . (2020). \nDesigning network design spaces. In Proceedings of the IEEE/CVF conference on \ncomputer vision and pattern recognition (pp. 10428–10436).\nRoberts, M., Driggs, D., Thorpe, M., Gilbey, J., Y eung, M., Ursprung, S., et al. \n(2021). Common pitfalls and recommendations for using machine learning to \ndetect and prognosticate for COVID-19 using chest radiographs and CT scans. Nat. \nMachine Intell. 3, 199–217. doi: 10.1038/s42256-021-00307-0\nSaad Menezes, M. C., Santinelli Pestana, D. V ., Ferreira, J. C., Ribeiro de \nCarvalho, C. R., Felix, M. C., Marcilio, I. O., et al. (2022). Distinct outcomes in \nCOVID-19 patients with positive or negative RT-PCR test. Viruses 14:175. doi: \n10.3390/v14020175\nSagi, O., and Rokach, L. (2018). “Ensemble learning: A survey, ” in Data Mining \nand Knowledge Discovery, Vol. 8. ed. W . Pedrycz (Hoboken, New Jersey: Wiley \nInterdisciplinary Reviews) e1249\nShalbaf, A., and Vafaeezadeh, M. (2021). Automated detection of COVID-19 \nusing ensemble of transfer learning with deep convolutional neural network based \non CT scans. Int. J. Comput. Assist. Radiol. Surg.  16, 115–123. doi: 10.1007/\ns11548-020-02286-w\nShorten, C., Khoshgoftaar, T. M., and Furht, B. (2021). Deep learning applications \nfor COVID-19. J. Big Data 8, 1–54. doi: 10.1186/s40537-020-00392-9\nSoares, E., Angelov, P ., Biaso, S., Froes, M. H., and Abe, D. K. (2020). SARS-CoV-2 \nCT-scan dataset: A large dataset of real patients CT scans for SARS-CoV-2 \nidentification. medRxiv [Preprint].\nSun, F ., Sun, J., and Zhao, Q. (2022). A deep learning method for predicting \nmetabolite-disease associations via graph neural network. Brief. Bioinform. 23, 1–11.  \ndoi: 10.1093/bib/bbac266\nSun, H., Wang, A., Wang, L., Wang, B., Tian, G., Y ang, J., et al. (2022a). Systematic \ntracing of susceptible animals to SARS-CoV-2 by a bioinformatics framework. Front. \nMicrobiol. 13:781770. doi: 10.3389/fmicb.2022.781770\nTan, M., and Le, Q. (2021). Efficientnetv2: Smaller models and faster training. In \nInternational Conference on Machine Learning (pp. 10096–10106). PMLR.\nTer-Sarkisov, A. (2022). Covid-ct-mask-net: prediction of covid-19 from CT scans \nusing regional features. Appl. Intell. 52, 1–12. doi: 10.1007/s10489-021-02731-6 \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. \n(2017). Attention is all you need. Adv. Neural Inf. Proces. Syst. 30, 5998–6008. doi: \n10.48550/arXiv.1706.03762 \nVoulodimos, A., Doulamis, N., Doulamis, A., and Protopapadakis, E. (2018). \nDeep learning for computer vision: A brief review. Comput. Intell. Neurosci. 2018, \n1–13. doi: 10.1155/2018/7068349\nPeng et al. 10.3389/fmicb.2022.995323\nFrontiers in Microbiology 14 frontiersin.org\nWang, C. C., Han, C. D., Zhao, Q., and Chen, X. (2021). Circular RNAs and \ncomplex diseases: from experimental results to computational models. Brief. \nBioinform. 22:bbab286. doi: 10.1093/bib/bbab286\nWang, D., Hu, B., Hu, C., Zhu, F ., Liu, X., Zhang, J., et al. (2020). Clinical \ncharacteristics of 138 hospitalized patients with 2019 novel coronavirus–infected \npneumonia in Wuhan, China. JAMA 323, 1061–1069. doi: 10.1001/jama.2020. \n1585\nWang, B., Jin, S., Y an, Q., Xu, H., Luo, C., Wei, L., et al. (2021). AI-assisted CT \nimaging analysis for COVID-19 screening: building and deploying a medical AI \nsystem. Appl. Soft Comput. 98:106897. doi: 10.1016/j.asoc.2020.106897\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. \nJ. Big Data 3, 1–40. doi: 10.1186/s40537-016-0043-6\nWorldometer (2022). COVID live  - coronavirus statistics  - Worldometer. \nAvailable at: https://www.worldometers.info/coronavirus/ (Accessed July 11,  \n2022).\nY ang, M., Y ang, H., Ji, L., Hu, X., Tian, G., Wang, B., et al. (2022). A multi-omics \nmachine learning framework in predicting the survival of colorectal cancer \npatients[J]. Comput. Biol. Med.  146:105516. doi: 10.1016/j.compbiomed.2022. \n105516\nZhang, J., Xie, Y ., Li, Y ., Shen, C., and Xia, Y . (2020). Covid-19 Screening on chest x-ray \nImages using deep Learning based Anomaly Detection. arXiv [Preprint] arXiv:2003.12338, 27.\nZhang, L., Y ang, P ., Feng, H., Zhao, Q., and Liu, H. (2021). Using network distance \nanalysis to predict lncRNA-miRNA interactions. Interdiscipl. Sci. Comput. Life Sci. \n13, 535–545. doi: 10.1007/s12539-021-00458-z\nZhao, J., Zhang, Y ., He, X., and Xie, P . (2020). Covid-CT-Dataset: a CT scan \nDataset about Covid-19. arXiv [Preprint] arXiv:2003.13865, 490.\nZhou, T., Lu, H., Y ang, Z., Qiu, S., Huo, B., and Dong, Y . (2021). The ensemble \ndeep learning model for novel COVID-19 on CT images. Appl. Soft Comput.  \n98:106885. doi: 10.1016/j.asoc.2020.106885\nZhou, P ., Y ang, X. L., Wang, X. G., Hu, B., Zhang, L., Zhang, W ., et al. (2020). A \npneumonia Outbreak Associated with a new Coronavirus of Probable bat origin. \nNature 579, 270–273. doi: 10.1038/s41586-020-2012-7\nZhu, W ., Braun, B., Chiang, L. H., and Romagnoli, J. A. (2021). Investigation of \ntransfer learning for image classification and impact on training sample size. \nChemom. Intel. Lab. Syst. 211:104269. doi: 10.1016/j.chemolab.2021.104269\nZhuang, F ., Qi, Z., Duan, K., Xi, D., Zhu, Y ., Zhu, H., et al. (2020). A comprehensive \nsurvey on transfer learning. Proc. IEEE 109, 43–76. doi: 10.1109/JPROC.2020.3004555",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7036461234092712
    },
    {
      "name": "Transformer",
      "score": 0.6991785764694214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6560518741607666
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.6355423331260681
    },
    {
      "name": "Deep learning",
      "score": 0.5673860907554626
    },
    {
      "name": "Majority rule",
      "score": 0.5579927563667297
    },
    {
      "name": "Binary classification",
      "score": 0.5489311218261719
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44710099697113037
    },
    {
      "name": "F1 score",
      "score": 0.4259566068649292
    },
    {
      "name": "Recall",
      "score": 0.41249826550483704
    },
    {
      "name": "Machine learning",
      "score": 0.41076135635375977
    },
    {
      "name": "Support vector machine",
      "score": 0.18941757082939148
    },
    {
      "name": "Medicine",
      "score": 0.11826115846633911
    },
    {
      "name": "Engineering",
      "score": 0.11055392026901245
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I49934816",
      "name": "Hunan University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210139220",
      "name": "Cipher Gene (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I96478251",
      "name": "Hunan Institute of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 31
}