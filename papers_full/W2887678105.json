{
  "title": "Doubly Attentive Transformer Machine Translation",
  "url": "https://openalex.org/W2887678105",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Arslan, Hasan Sait",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287100636",
      "name": "Fishel Mark",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2295802463",
      "name": "Anbarjafari Gholamreza",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2800782462",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2963344439",
    "https://openalex.org/W2516756687",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2950019618",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2144600658",
    "https://openalex.org/W2159243025",
    "https://openalex.org/W2345720230",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2513263213",
    "https://openalex.org/W338621447",
    "https://openalex.org/W2593341061",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2229833550",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W2293344577",
    "https://openalex.org/W2395259056",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2951638509",
    "https://openalex.org/W2952688536",
    "https://openalex.org/W2172589779",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2247931231"
  ],
  "abstract": "In this paper a doubly attentive transformer machine translation model (DATNMT) is presented in which a doubly-attentive transformer decoder normally joins spatial visual features obtained via pretrained convolutional neural networks, conquering any gap between image captioning and translation. In this framework, the transformer decoder figures out how to take care of source-language words and parts of an image freely by methods for two separate attention components in an Enhanced Multi-Head Attention Layer of doubly attentive transformer, as it generates words in the target language. We find that the proposed model can effectively exploit not just the scarce multimodal machine translation data, but also large general-domain text-only machine translation corpora, or image-text image captioning corpora. The experimental results show that the proposed doubly-attentive transformer-decoder performs better than a single-decoder transformer model, and gives the state-of-the-art results in the English-German multimodal machine translation task.",
  "full_text": "Doubly Attentive Transformer Machine Translation\nHasan Sait Arslan Mark Fishel\nInstitute of Computer Science\nUniversity of Tartu, Estonia\n{hasan90,fishel}@ut.ee\nGholamreza Anbarjafari\nInstitute of Technology\nUniversity of Tartu, Estonia\nshb@ut.ee\nAbstract\nIn this paper a doubly attentive transformer\nmachine translation model (DATNMT) is pre-\nsented in which a doubly-attentive transformer\ndecoder normally joins spatial visual features\nobtained via pretrained convolutional neural\nnetworks, conquering any gap between im-\nage captioning and translation. In this frame-\nwork, the transformer decoder ﬁgures out\nhow to take care of source-language words\nand parts of an image freely by methods for\ntwo separate attention components in an En-\nhanced Multi-Head Attention Layer of dou-\nbly attentive transformer, as it generates words\nin the target language. We ﬁnd that the\nproposed model can effectively exploit not\njust the scarce multimodal machine transla-\ntion data, but also large general-domain text-\nonly machine translation corpora, or image-\ntext image captioning corpora. The experi-\nmental results show that the proposed doubly-\nattentive transformer-decoder performs better\nthan a single-decoder transformer model, and\ngives the state-of-the-art results in the English-\nGerman multimodal machine translation task.\n1 Introduction\nNeural Machine Translation (NMT) has been ef-\nfectively handled as a sequence to sequence learn-\ning issue (Kalchbrenner and Blunsom, 2013; Cho et\nal., 2014; Sutskever et al., 2014), where each train-\ning example consists of one source and one target\nvariable-length sequences, with no earlier data on\nthe arrangement between the two.\nWith regards to NMT, (Bahdanau et al., 2014)\ninvestigated the fact that the use of a ﬁxed length\nvector is a bottleneck in enhancing the execution of\nthe essential encoder-decoder architecture in neural\nmachine translation, and proposed to broaden this\nby enabling a model to naturally scan for parts of\na source sentence that are pertinent to predicting an\nobjective word, without forming these parts as a hard\nsection expressly. Also (Xu et al., 2015) proposed\nan attention based model for the task of neural im-\nage captioning (NIC) where a model ﬁgures out how\nto take care of particular parts of an image repre-\nsentation as it creates its caption (target) in normal\nlanguage. Similarly, (Calixto et al., 2017) proposed\nan attention based approach for the task of Multi-\nmodal Neural Machine Translation (MNMT) where\na model ﬁgures out when and where to attend to the\nimage or the source text when generating the words\nin the target language.\nLately the attention mechanism has been used\nin addition to NMT also in NIC and MNMT. In\nthis work, we propose an end-to-end attention-\nbased MNMT approach based on the transformer\nmodel (Vaswani et al., 2017), which fuses together\ntwo independent attention vectors, one over source-\nlanguage words and the other over various areas of\nan image.\nOur main contributions are:\n•We propose a novel transformer-attention-\nbased MNMT which makes use of spatial vi-\nsual features and transformer encoder output\nfeatures.\n•We additionally use more than 90 M text-only\ncorpora from OPUS (Tiedemann and Nygaard,\n2004), and image captioning dataset Flickr30k\narXiv:1807.11605v1  [cs.CL]  30 Jul 2018\n(Plummer et al., 2015) to pretrain machine\ntranslation and image captioning models and\nshow that our MNMT model can efﬁciently ex-\nploit them.\n•We demonstrate that images bring helpful data\ninto transformer NMT model, in circumstances\nin which sentences describe objects represented\nin the image.\nThe rest of this paper is organized as takes after.\nWe ﬁrst examine past related work in Section 2, then\ndescribe the related background information (Sec-\ntion 3) and introduce our approach (Section 4). In\nSection 5, we introduce the datasets we use for train-\ning and assessing our models, in Section 6 we talk\nabout our exploratory setup and investigate and ex-\namine our outcomes. At last, in Section 7 we present\nconclusions and list a few directions for future work.\n2 Related work\nThere has been a lot of work on natural language\ngeneration from non-textual inputs. (Mao et al.,\n2014) introduced a multimodal Recurrent Neural\nNetwork model for creating sentence depictions to\nclarify the substance of images. It speciﬁcally mod-\nels the likelihood of creating a word given past\nwords and the image. Image captions are produced\nby sampling from this distribution. The model com-\nprises of two sub-networks: a deep recurrent neural\nnetwork for sentences and a deep convolutional neu-\nral network for images. (Vinyals et al., 2014) intro-\nduced a generative model in view of a deep recur-\nrent design that consolidates late advances in com-\nputer vision and machine translation and that can\nbe used to create regular sentences depicting an im-\nage. The model is trained to augment the probabil-\nity of the target description sentence given a train-\ning image. (Elliott et al., 2015) displayed a way to\ndeal with multi-language image description uniting\nbits of knowledge from neural machine translation\nand neural image captioning. To make a descrip-\ntion of an image for a given target language, their se-\nquence generation models condition on feature vec-\ntors from the image, the depiction from the source\nlanguage, as well as a multimodal vector computed\nover the image and a description in the source lan-\nguage. (Venugopalan et al., 2015) propose an end-\nto-end sequence-to-sequence model to create subti-\ntles for video recordings. For this they use recurrent\nneural networks, particularly LSTMs. Their LSTM\nmodel is trained on video-sentence matches and ﬁg-\nures out how to relate a sequence of video frames\nto a sequence of words so as to produce a depic-\ntion of the occasion in the video cut. Their model\nnormally can take in the ﬂeeting structure of the se-\nquence of frames in addition the sequence model of\nthe produced sentences. (Xu et al., 2015) presented\nan attention based model that naturally ﬁgures out\nhow to depict the substance of images. They de-\nscribe how they can train this model in a determinis-\ntic way using standard back-propagation procedures\nand stochastically by amplifying a variational lower\nbound. At long last, (Zhu et al., 2018) used trans-\nformer model (Vaswani et al., 2017) for image cap-\ntioning task, by feeding spatial visual features of im-\nages directly to transformer decoder, by using CNN\nencoder for images instead of transformer encoder\nfor text.\nWith regards to NMT, (Dong et al., 2015) ex-\nplored the issue of learning a machine translation\nmodel that can at the same time translate sentences\nfrom one source language to numerous target lan-\nguages. They expanded the neural machine transla-\ntion model to a multi-task learning structure which\nshares source language representation and isolates\nthe modeling of various target language translation.\nTheir structure can be connected to circumstances\nwhere either large amount of parallel data or re-\nstricted parallel data is accessible. (Firat et al., 2016)\nproposed a multi-way, multilingual neural machine\ntranslation. Their proposed approach empowers a\nsolitary neural translation model to translate be-\ntween numerous languages, with various parameters\nthat becomes just directly with the quantity of lan-\nguages. This is made conceivable by having a single\nattention mechanism that is shared over all language\nsets. (Luong et al., 2015) analyzed three multi-task\nlearning settings for sequence to sequence models:\n1) The one-to-many setting where the encoder is\nshared between a few tasks. 2) The many-to-one\nsetting where just the decoder can be shared, as on\naccount of translation and image captioning. 3) The\nmany-to-many setting where various encoders and\ndecoders are shared, which is the situation with un-\nsupervised destinations and translation. In spite of\nthe fact that it is not an NMT model, (Hitschler and\nRiezler, 2016) exhibited a way to deal with enhance\nstatistical machine translation of image depictions\nby multimodal turns characterized in visual space.\nTheir key thought is to perform image recovery over\na database of images that are inscribed in the objec-\ntive language, and use the captions of the most com-\nparable images for crosslingual reranking of trans-\nlation outputs. Their approach does not rely upon\nthe accessibility of a lot of in-domain parallel data,\nhowever just depends on accessible huge datasets\nof monoligually captioned images, and on convolu-\ntional neural networks to ﬁgure image similarities.\nDistinctive research groups have proposed to in-\ncorporate global and spatial visual features in re-\nranking n-best lists created by an SMT framework\nor straightforwardly in an NMT structure (Caglayan\net al., 2016; Calixto et al., 2016; Huang et al.,\n2016; Libovick ´y et al., 2016; Shah et al., 2016).\n(Huang et al., 2016) proposed to use global visual\nfeatures extracted with the VGG19 network (Si-\nmonyan and Zisserman, 2014) for a whole image,\nand furthermore for areas of the image acquired us-\ning the RCNN of (Girshick et al., 2013). (Calixto et\nal., 2017) proposed the ﬁrst attention-based MNMT\nmodels, and doubly-attentive model, where two at-\ntention mechanisms are incorporated into one multi-\nmodal decoder, and the entire-model trained jointly\nend-to-end.\nWith our work, we propose the ﬁrst doubly at-\ntentive transformer MNMT model, where two at-\ntention mechanisms are fused together under scaled\ndot-product attention level. Also, differently from\nthe previous model, our model is able to train stan-\ndalone image-captioning, language, machine trans-\nlation, multi-modal machine translation models, or\nthe mix of all these data types homogeneously, and\nthis pretraining leads to signiﬁcant improvement on\nMNMT translation quality.\n3 Background and Notation\n3.1 Transformer NMT\nWe described the transformer NMT model (Figure\n1.) presented by (Vaswani et al., 2017) in this sec-\ntion. Given a source sequence X = (x1, x2, ..., xN )\nand its translation Y = (y1, y2, ..., yM ), an NMT\nmodel means to construct a solitary neural network\nthat translates X into Y by speciﬁcally learning to\nmodel p(Y |X). The entire network consists of one\nencoder and one decoder. Each xi is a row index in\na word embedding matrix Ex ∈R|Vx|×dx , as well as\neach yj being an index in a target word embedding\nmatrix Ey ∈R|Vy|×dy , Vx and Vy are source and tar-\nget vocabularies, anddx and dy are source and target\nword embeddings dimensionalities, respectively.\nThe encoder is made out of a heap of N = 6\nidentical layers. Each layer has two sub-layers.\nThe ﬁrst is a multi-head self-attention component,\nand the second is a basic, position-wise fully con-\nnected feed-forward network. There is a residual\nconnection (He et al., 2015) around every one of\nthe two sub-layers, trailed by layer normalization\n(Ba et al., 2016). That is, the output of each sub-\nlayer is LayerNorm (x + SubLayer(x)), where\nSublayer(x) is simply the function actualized by\nthe sub-layer itself. To encourage these residual con-\nnections, all sub-layers in the model, and addition-\nally the embedding layers, create outputs of dimen-\nsion dmodel = 512.\nThe decoder is likewise made out of a heap of\nN = 6 identical layers. In addition to the two sub-\nlayer in each encoder-layer, the decoder embeds a\nthird sub-layer, which performs multi-head attention\nover the output of the encoder stack. Like the en-\ncoder, there are residual connections around every\none of the sub-layers, trailed by layer normalization.\nThere is a masking system in self-attention sub-layer\nin the decoder stack to keep positions from attending\nto consequent positions. This masking, joined with\nthe fact that output embeddings are counterbalanced\nby one position, guarantees that the predictions for\nposition i can depend just on the known outputs at\npositions before i.\n3.2 Multi-Head Attention\nIn transformer, the attention function is a query map-\nping and an arrangement of key-value sets to an out-\nput, where the query, keys, values, and output are all\nvectors. The output is processed as a weighted sum\nof the values, where the weight allocated to each\nvalue is computed by a compatibility function of the\nquery with the corresponding key.\nThe particular attention in transformer is ”Scaled\nDot-Product Attention” (Figure 2.). The input in-\nvolves queries and keys of dimensiondk, and values\nFigure 1: The Transformer - model architecture.\nFigure 2: Scaled dot product attention architecture.\nFigure 3: Multi-Head Attention architecture.\nof dimension dv. Each query is multiplied with all\nkeys by dot product multiplication, and divided by√dk, and softmax function is applied on output to\ngain the weights on the values.\nPractically, the attention function is simultane-\nously calculated on a set of queries, keys and values\nand packed together into a matrix Q, K and V . The\noutput matrix is computed as:\nAttention(Q, K, V) =softmax(QKT\n√dk\n)V (1)\nWith Multi-Head attention, rather than\nplaying out a single attention work with\ndmodel−dimensional keys, values and queries,\nthese parameters are linearly projected h times with\nvarious, learned linear projections to dk, dk and\ndv dimensions, separately. On every one of these\nanticipated variants of queries, keys and values, the\nattention function is performed in parallel, yielding\ndv−dimensional output values. The output values\nare projected and concatenated for the ﬁnal values\nas in Figure 3.\nMulti-head attention enables the model to mutu-\nally take care of data from various representation\nsubspaces at various positions. With a single atten-\ntion head, averaging restrains this.\nMultiHead (Q, K, V) =\nConcat(head1, ..., headh)WO (2)\nwhere\nheadi = Attention(QWQ\ni , KWK\ni , V WV\ni ) (3)\nwhere the projections are parameter matrices\nWQ\ni ∈Rdmodel×dk ,\nWK\ni ∈Rdmodel×dk ,\nWV\ni ∈Rdmodel×dv ,\nWO ∈Rhdv×dmodel\n(4)\n4 Doubly Attentive Transformer NMT\nOur doubly attentive transformer NMT model (Fig-\nure 4.) can be viewed as an extension of the trans-\nformer NMT system depicted in §2.1.\nWe use openly accessible pre-trained CNNs for\nimage feature extraction. In particular, we extract\nspatial image features for all images in our dataset\nusing the 152-layer Residual network (ResNet-152)\nof (He et al., 2015).\nThese spatial feature are the activations of the\nres4f layer, which can be viewed as encoding an im-\nage in a 14x14 grid, where every one of the sections\nin the network is represented by a 1024D element\nvector that exclusive encodes data about that partic-\nular district of the image. We vectorise this 3-tensor\ninto a 196x1024 network A = ( a1, a2, ..., aL) ∈\nR1024 where every one of the L = 196 rows com-\nprises of a 1024D element vector and every column\nrepresents one column in the image.\n4.1 Doubly Attentive Transformer Decoder\nDoubly attentive transformer integrates two sepa-\nrate attention mechanisms over the source-language\nwords and visual features in a single transformer\ndecoder. Our doubly-attentive transformer decoder\nfuses the scaled-dot production attention vectors\nof target words between encoder output features,\nFigure 4: Doubly attentive Transformer NMT model ar-\nchitecture.\nand between spatial visual features under Enhanced\nMulti-Head Attention layer.\nAttention(Q, Kt, Vt, Kv, Vv) =\n(softmax(QKT\nt√dk\n)Vt)\n+(softmax(QKT\nv√dk\n)Vv)\n(5)\nIf attention to a target word at position i is very\nstrong from one of the sources (image, text), and\nvery weak on another, summing two attention out-\nputs as at (Equation 5.) still gives chance to the tar-\nget word to have high probability to be learned by\nFigure 5: Enhanced scaled dot product attention architec-\nture. Since there is no padding on spatial visual features\nvector, we don’t use masking over visual vector.\nthe model. In this way, if one of the sources does not\nhave feature representing the target word, the model\ndecides based on the other source.\nIn this way, Enhanced Multi-Head Attention is\nrepresented as follows:\nMultiHead (Q, Kt, Vt, Kv, Vv) =\nConcat(head1, ..., headh)WO (6)\nwhere\nheadi = Attention(QWQ\ni , KtWKt\ni , VtWVt\ni ,\nKvWKv\ni , VvWVv\ni , )\n(7)\nwhere the projections are parameter matrices\nFigure 6: Enhanced Multi-Head attention architecture.\nWQ\ni ∈Rdmodel×dk ,\nWKt\ni ∈Rdmodel×dk ,\nWVt\ni ∈Rdmodel×dv ,\nWKv\ni ∈Rdmodel×dk ,\nWVv\ni ∈Rdmodel×dv ,\nWO ∈Rhdv×dmodel\n(8)\n5 Data\nThe Flickr30K (Plummer et al., 2015) dataset con-\ntains 30k images and 5 descriptions in English and\nGerman for each image. In this work, we use\nMulti30k dataset (Elliott et al., 2016).\nFor every one of the 30k images in the Flickr30k,\nthe M30k has one of the English depictions manu-\nally translated into German by an expert translator.\nTraining, validation and test sets contain 29k, 1014\nand 1000 pictures individually, each joined by a sen-\ntence match (the ﬁrst English sentence and its trans-\nlation into German).\nWe use the whole M30k training set for train-\ning our models, its validation set for model choice\nwith BLEU (Papineni et al., 2002) and its test set\nfor assessment. In addition, for pretraining models,\nwe use whole English-German corpora from OPUS\nTable 1: Datasets provided by OPUS. The corpora involves around 98 million sentence pairs, around 1.4 billion\nEnglish and around 1.5 billion German tokens.\nCorpus Sent’s En tokens De tokens\nParaCrawl 36.4M 534.4M 560.9M\nEUbookshop (Skadin ¸ˇs et al., 2014) 9.6M 337.4M 380.2M\nOpenSubtitles2018 (Lison and Tiedemann, 2016) 24.4M 176.3M 191.3M\nOpenSubtitles2016 (Lison and Tiedemann, 2016) 15.4M 110.5M 118.8M\nDGT (Tiedemann, 2012) 3.2M 55.4M 72.9M\nEuroparl (Tiedemann, 2012) 2.0M 54.7M 57.7M\nWikipedia (Tiedemann, 2012) 2.5M 43.5M 58.4M\nJRC-Acquis (Steinberger et al., 2006) 0.7M 30.7M 34.1M\nEMEA (Tiedemann, 2009) 1.2M 11.5M 12.0M\nTanzil (Tiedemann, 2012) 0.5M 11.1M 11.3M\nNews-Commentary11 (Tiedemann, 2012) 0.2M 6.5M 6.6M\nMultiUN (Tiedemann, 2012) 0.2M 5.7M 6.2M\nNews-Commentary (Tiedemann, 2012) 0.2M 5.0M 5.0M\nGNOME (Tiedemann, 2012) 0.8M 4.4M 5.5M\nTatoeba (Tiedemann, 2012) 0.1M 2.4M 3.6M\nECB (Tiedemann, 2009) 0.1M 2.8M 3.1M\nTED2013 (Tiedemann, 2012) 0.1M 2.7M 2.8M\nKDE4 (Tiedemann, 2009) 0.3M 1.9M 2.4M\nGlobalV oices (Tiedemann, 2012) 57.5k 1.3M 1.3M\nBooks (Tiedemann, 2012) 52.3k 1.1M 1.3M\nUbuntu (Tiedemann, 2012) 0.2M 0.6M 0.8M\nOpenSubtitles (Tiedemann, 2009) 76.0k 0.5M 0.6M\nOpenOfﬁce (Tiedemann, 2009) 42.9k 0.5M 0.5M\nWMT-News (Tiedemann, 2012) 19.6k 0.5M 0.5M\nPHP (Tiedemann, 2009) 42.2k 0.3M 0.5M\nEUconst (Tiedemann, 2009) 9.0k 0.2M 0.2M\nRF (Tiedemann, 2012) 0.2k 4.5k 4.4k\nTotal 98.5M 1.4G 1.5G\n(Tiedemann and Nygaard, 2004) Table 1, and whole\nFlickr30k German descriptions.\nWe use tokenizer script in the Moses SMT Toolkit\n(Koehn et al., 2007) to tokenize English and German\ndescriptions, and we also convert space-separated\ntokens into word-pieces with unigram model (Kudo,\n2018) by using google sentencepiece model1.\nIf the sentences in English or German are longer\nthan 100 tokens, they are removed. We train models\nto translate from English to German and report as-\nsessment of cased, tokenized sentences with punctu-\nation.\n1https://github.com/google/sentencepiece\n6 Experiments and Results\n6.1 Experimental Setup\nIn our experiments, we mainly focus on the effect\nof incorporating visual features into the transformer\nNMT model, for this reason we use the same hy-\nperparameter settings on original transformer work\n(Vaswani et al., 2017).\nVisual features are extracted by feeding the im-\nages to the pretrained ResNet-152 and using the acti-\nvations of the res4f layer (He et al., 2015). We apply\nlinear projection on visual features to reduce dimen-\nsion from 1024 to 512 to have the same size with\nword embeddings. We apply dropout of 0.5 on lin-\near projection.\nWe use sentence-minibatches of 32, where each\ntraining instance consists of one English sentence,\none German sentence and one image for MNMT,\none English sentence and one German sentence for\nNMT, and one image and one German sentence for\nimage captioning. We use a homogeneous mixture\nof training instances for pretraining models. We\ntrain and save MNMT models for 100 epochs, and\nselect the best model based on BLEU4 score on val-\nidation set. While, we select the best image cap-\ntioning pretraining model based on best perplexity\n(Jelinek et al., 1977) on validation set, our NMT\npretraining model is trained for 1 epoch. While the\nnumber of warmup steps in all the training setups\nis 4000, we see that this hyperparameter leads to\noverﬁtting for MNMT training with NMT pretrained\nmodel. In order to proceed ﬁnetuning with MNMT,\nwe change this hyperparameter to 8000.\nThe translation quality of our models is assessed\nquantitatively with BLEU4 (Papineni et al., 2002),\nMETEOR (Denkowski and Lavie, 2014) and TER\n(Snover et al., 2006). We report statistical signiﬁ-\ncance with approximate randomisation for the three\nmeasurements using the MultEval tool (Clark et al.,\n2011).\n6.2 Results and Discussions\nIn Table 2, we present results and comparisons,\nincluding Transformer NMT (TNMT), and our\nDoubly-Attentive Transformer NMT (DATNMT)\nresults obtained without pretraining models. Tables\n3 and 4 demonstrate the results of TNMT and DAT-\nNMT after pretraining models with Flickr30k, and\nlarge OPUS corpora, respectively.\nTraining on M30k: One primary ﬁnding is\nthat our model reliably beats the equivalent model\nof NMT(SRC+IMG) (Calixto et al., 2017), with\nchange of +4.5 BLEU. The experimental results\nshow that while the text-only trained model with\nTNMT outperforms text-only trained model with\nNMT (RNN)(Sennrich et al., 2017) by+4.9 BLEU4\npoints, DATNMT outperforms TNMNT by +2.4\nBLEU4 points. Moreover, Table 2 illustrates that\nNMT(SRC+IMG) performs better at recall-oriented\nmetrics, i.e. METEOR and TER, whereas our model\nis better at precision-oriented ones, i.e. BLEU4.\nPre-training: We presently talk about outcomes\nfor models pre-trained using distinctive data sets.\nFirst an image captioning model was pre-trained\nwith our DATNMT on Flickr30k German data set,\na medium-sized in-domain image description data\nset (145k training sentences). In addition, an NMT\nmodel on the English-German parallel sentences of\nmuch larger MT data sets on OPUS (Table 1) was\npre-trained. From Table 2, it is clear that model\nDATNMNT can learn from both in-domain, image\ncaptioning data sets as well as text-only, general do-\nmain ones.\nPre-training on Flickr30k: On Table 3, When\npre-training on Flickr30k, the precision-oriented\nBLEU4 shows a difference on TNMT. Although\nTNMNT does not use image information, the pre-\ntrained language model on Flickr30k data set brings\nimprovement to TNMT. However, the same im-\nprovement cannot be seen on DATNMT, although\nit has +0.1 BLEU point difference between TNMT.\nPre-training on OPUS corpora: In addition\nour models was pre-trained on OPUS corpora (Ta-\nble 1) for 1 epoch, which took 10 days. Results\nshow that DATNMT improves signiﬁcantly over the\nTNMT baseline according to BLEU4, and is con-\nsistently the best according to all metrics. This is\na strong indication that model DATNMT can ex-\nploit the additional pre-training data efﬁciently on\ngeneral-domain text corpora.\nTextual and visual attention:In Figure 7, the\nvisual and textual attention weights for a sentence\ntranslation of the M30k test set are shown by us-\ning the SoftAlignments tool (Rikters et al., 2017).\nIn the visual attention, it can be seen that when\nthe words ”Teenager” and ”Trompete” are gener-\nated, the heatmap is focused on areas around the\nteenager’s head. In addition, it can be observed that\nthe target words which have strong attention with\nsource words also shows strong attention on image\nparts.\nTable 2: BLEU4, METEOR (higher is better) and TER scores (lower is better) on the translated Multi30k (M30k) test\nset without pretraining model. Best overall results appear in bold.\nModel Training data BLEU4 METEOR TER\nNMT (RNN) M30k 33.7 52.3 46.7\nPBSMT M30k 32.9 54.3 45.1\n(Huang et al., 2016) M30k 35.1 52.2 -\n+ RCNN M30k 36.5 54.1 -\nNMT(src+img) M30k 36.5 55.0 43.7\nTNMT M30k 38.6 55.0 51.6\nDATNMT M30k 41.0 53.5 48.4\nTable 3: BLEU4, METEOR (higher is better) and TER scores (lower is better) on the translated Multi30k (M30k) test\nset after ﬁnetuning with image captioning pretraining model. Best overall results appear in bold.\nModel Training data BLEU4 METEOR TER\nTNMT M30k 39.6 54.1 49.9\nDATNMT M30k 39.7 52.5 50.5\nTable 4: BLEU4, METEOR (higher is better) and TER scores (lower is better) on the translated Multi30k (M30k) test\nset after ﬁnetuning with machine translation pre-training model. Best overall results appear in bold.\nModel Training data BLEU4 METEOR TER\nTNMT M30k 41.3 57.1 48.4\nDATNMT M30k 42.2 57.2 45.9\n7 Conclusion and Future Work\nWe have presented a novel transformer NMT model\nto fuse spatial visual data into NMT. We have re-\nported that enhancing scaled-dot product attention\nlayer with 2 attention vectors leads improvement on\ntranslation quality. We also pretrained our model ho-\nmogeneous training sample types, and reported that\nour model is able to exploit general domain text-\nonly or image captioning datasets for improving the\ntranslation accuracy.\nIn the future, we are planning to continue with\nmultimodal image transformer, which will be the\nenhancement of image transformer (Parmar et al.,\n2018). We would like to see the affect of standalone\ntext or speech, or their mixture on pixel-wise image\ngeneration.\nFigure 7: Textual attention map between source and target words and visual attention colormap between target words\nand subregions on the image for a translation example in Multi30k test set. The intense of the line color and spots on\nthe image show the strength of the attention on text and image parts.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473.\nOzan Caglayan, Walid Aransa, Yaxing Wang, Marc\nMasana, Mercedes Garc ´ıa-Mart´ınez, Fethi Bougares,\nLo¨ıc Barrault, and Joost van de Weijer. 2016. Does\nmultimodality help human and machine for translation\nand image captioning? CoRR, abs/1605.09186.\nIacer Calixto, Desmond Elliott, and Stella Frank. 2016.\nDcu-uva multimodal mt system report. InProceedings\nof the First Conference on Machine Translation, WMT\n2016, colocated with ACL 2016, August 11-12, Berlin,\nGermany, pages 634–638. Association for Computa-\ntional Linguistics (ACL), 8.\nIacer Calixto, Qun Liu, and Nick Campbell. 2017.\nDoubly-attentive decoder for multi-modal neural ma-\nchine translation. CoRR, abs/1702.01287.\nKyunghyun Cho, Bart van Merrienboer, C ¸ aglar G¨ulc ¸ehre,\nFethi Bougares, Holger Schwenk, and Yoshua Ben-\ngio. 2014. Learning phrase representations using\nRNN encoder-decoder for statistical machine transla-\ntion. CoRR, abs/1406.1078.\nJonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.\nSmith. 2011. Better hypothesis testing for statistical\nmachine translation: Controlling for optimizer insta-\nbility. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies: Short Papers - Volume\n2, HLT ’11, pages 176–181, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation.\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and\nHaifeng Wang. 2015. Multi-task learning for multi-\nple language translation. In ACL.\nDesmond Elliott, Stella Frank, and Eva Hasler. 2015.\nMulti-language image description with neural se-\nquence models. CoRR, abs/1510.04709.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lucia\nSpecia. 2016. Multi30k: Multilingual english-german\nimage descriptions. CoRR, abs/1605.00459.\nOrhan Firat, KyungHyun Cho, and Yoshua Bengio.\n2016. Multi-way, multilingual neural machine trans-\nlation with a shared attention mechanism. CoRR,\nabs/1601.01073.\nRoss B. Girshick, Jeff Donahue, Trevor Darrell, and Ji-\ntendra Malik. 2013. Rich feature hierarchies for\naccurate object detection and semantic segmentation.\nCoRR, abs/1311.2524.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recogni-\ntion. CoRR, abs/1512.03385.\nJulian Hitschler and Stefan Riezler. 2016. Multi-\nmodal pivots for image caption translation. CoRR,\nabs/1601.03916.\nPo-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean Oh,\nand Chris Dyer. 2016. Attention-based multimodal\nneural machine translation. In WMT.\nF. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker.\n1977. Perplexity – a measure of the difﬁculty of\nspeech recognition tasks. Journal of the Acoustical\nSociety of America, 62:S63, November. Supplement\n1.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of the\n2013 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1700–1709. Association\nfor Computational Linguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran, Richard\nZens, Chris Dyer, Ond ˇrej Bojar, Alexandra Con-\nstantin, and Evan Herbst. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Proceed-\nings of the 45th Annual Meeting of the ACL on Inter-\nactive Poster and Demonstration Sessions, ACL ’07,\npages 177–180, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. CoRR, abs/1804.10959.\nJindrich Libovick ´y, Jindrich Helcl, Marek Tlust ´y, Pavel\nPecina, and Ondrej Bojar. 2016. CUNI system for\nWMT16 automatic post-editing and multimodal trans-\nlation tasks. CoRR, abs/1606.07481.\nPierre Lison and J ¨org Tiedemann. 2016. Opensubti-\ntles2016: Extracting large parallel corpora from movie\nand tv subtitles.\nMinh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2015. Multi-task se-\nquence to sequence learning. CoRR, abs/1511.06114.\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L.\nYuille. 2014. Explain images with multimodal recur-\nrent neural networks. CoRR, abs/1410.1090.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic\nevaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, pages 311–318, Strouds-\nburg, PA, USA. Association for Computational Lin-\nguistics.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, and Alexander Ku. 2018. Im-\nage transformer. CoRR, abs/1802.05751.\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-to-\nsentence models. CoRR, abs/1505.04870.\nMat¯ıss Rikters, Mark Fishel, and Ond ˇrej Bojar. 2017.\nVisualizing neural machine translation attention and\nconﬁdence. The Prague Bulletin of Mathematical Lin-\nguistics, 109(1):39–50.\nRico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-\ndra Birch, Barry Haddow, Julian Hitschler, Marcin\nJunczys-Dowmunt, Samuel L ¨aubli, Antonio Vale-\nrio Miceli Barone, Jozef Mokry, et al. 2017. Nematus:\na toolkit for neural machine translation. arXiv preprint\narXiv:1703.04357.\nKashif Shah, Josiah Wang, and Lucia Specia. 2016.\nShef-multimodal: Grounding machine translation on\nimages. In WMT.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. CoRR, abs/1409.1556.\nRaivis Skadin ¸ˇs, J ¨org Tiedemann, Roberts Rozis, and\nDaiga Deksne. 2014. Billions of parallel words\nfor free: Building and using the eu bookshop cor-\npus. In Proceedings of the 9th International Confer-\nence on Language Resources and Evaluation (LREC-\n2014), Reykjavik, Iceland, May. European Language\nResources Association (ELRA).\nMatthew Snover, Bonnie J. Dorr, Richard F. Schwartz,\nand Linnea Micciulla. 2006. A study of translation\nedit rate with targeted human annotation.\nRalf Steinberger, Bruno Pouliquen, Anna Widiger,\nCamelia Ignat, Tomaz Erjavec, Dan Tuﬁs, and D ´aniel\nVarga. 2006. The jrc-acquis: A multilingual aligned\nparallel corpus with 20+ languages. arXiv preprint\ncs/0609058.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nCoRR, abs/1409.3215.\nJ¨org Tiedemann and Lars Nygaard. 2004. The opus\ncorpus-parallel and free: http://logos. uio. no/opus. In\nLREC. Citeseer.\nJ¨org Tiedemann. 2009. News from OPUS - A collec-\ntion of multilingual parallel corpora with tools and in-\nterfaces. In N. Nicolov, K. Bontcheva, G. Angelova,\nand R. Mitkov, editors, Recent Advances in Natural\nLanguage Processing, volume V , pages 237–248. John\nBenjamins, Amsterdam/Philadelphia, Borovets, Bul-\ngaria.\nJ¨org Tiedemann. 2012. Parallel data, tools and interfaces\nin opus. In Nicoletta Calzolari (Conference Chair),\nKhalid Choukri, Thierry Declerck, Mehmet Ugur Do-\ngan, Bente Maegaard, Joseph Mariani, Jan Odijk, and\nStelios Piperidis, editors, Proceedings of the Eight In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), Istanbul, Turkey, may. Euro-\npean Language Resources Association (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nSubhashini Venugopalan, Marcus Rohrbach, Jeff Don-\nahue, Raymond J. Mooney, Trevor Darrell, and Kate\nSaenko. 2015. Sequence to sequence - video to text.\nCoRR, abs/1505.00487.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. 2014. Show and tell: A neural image\ncaption generator. CoRR, abs/1411.4555.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron C. Courville, Ruslan Salakhutdinov, Richard S.\nZemel, and Yoshua Bengio. 2015. Show, attend and\ntell: Neural image caption generation with visual at-\ntention. CoRR, abs/1502.03044.\nXinxin Zhu, Lixiang Li, Jing Liu, Haipeng Peng,\nand Xinxin Niu. 2018. Captioning transformer\nwith stacked attention modules. Applied Sciences,\n8(5):739.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8378416299819946
    },
    {
      "name": "Machine translation",
      "score": 0.8318279981613159
    },
    {
      "name": "Computer science",
      "score": 0.8107728958129883
    },
    {
      "name": "Closed captioning",
      "score": 0.7015269994735718
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5369503498077393
    },
    {
      "name": "Exploit",
      "score": 0.4955636262893677
    },
    {
      "name": "Natural language processing",
      "score": 0.4930884540081024
    },
    {
      "name": "Speech recognition",
      "score": 0.43235525488853455
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24202659726142883
    },
    {
      "name": "Voltage",
      "score": 0.15315604209899902
    },
    {
      "name": "Engineering",
      "score": 0.09205570816993713
    },
    {
      "name": "Electrical engineering",
      "score": 0.08438214659690857
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}