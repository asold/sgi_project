{
  "title": "More is more: Addition bias in large language models",
  "url": "https://openalex.org/W4407690577",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5107784046",
      "name": "Luca Santagata",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2341956539",
      "name": "Cristiano De Nobili",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6787300336",
    "https://openalex.org/W4387952963",
    "https://openalex.org/W3147377228",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W2134273450",
    "https://openalex.org/W7042697309",
    "https://openalex.org/W2269742369",
    "https://openalex.org/W6855664720",
    "https://openalex.org/W6852860699",
    "https://openalex.org/W6810156098",
    "https://openalex.org/W6796715840",
    "https://openalex.org/W4206590911",
    "https://openalex.org/W6859136814",
    "https://openalex.org/W6776644801",
    "https://openalex.org/W6870632548",
    "https://openalex.org/W6852178813",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W3119920397",
    "https://openalex.org/W6857123525",
    "https://openalex.org/W6782465632",
    "https://openalex.org/W4292157289",
    "https://openalex.org/W6781979233",
    "https://openalex.org/W6849963380",
    "https://openalex.org/W4362601642",
    "https://openalex.org/W4386726578",
    "https://openalex.org/W4389524012",
    "https://openalex.org/W4387430847",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4401955911",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4389519799",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W4221159132",
    "https://openalex.org/W4306766067",
    "https://openalex.org/W4386302153",
    "https://openalex.org/W4234059792",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W4301958211",
    "https://openalex.org/W4386246835",
    "https://openalex.org/W4385572383"
  ],
  "abstract": "In this paper, we investigate the presence of addition bias in Large Language Models (LLMs), drawing a parallel to the cognitive bias observed in humans where individuals tend to favor additive over sub-tractive changes [3]. Using a series of controlled experiments, we tested various LLMs, including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, MathΣtral, and Llama 3.1, on tasks designed to measure their propensity for additive versus subtractive modifications. Our findings demonstrate a significant preference for additive changes across all tested models. For example, in a palindrome creation task, Llama 3.1 favored adding let-ters 97.85% of the time over removing them. Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick 76.38% of the time rather than remove one. In a text summarization task, Mistral 7B pro-duced longer summaries in 59.40%–75.10% of cases when asked to improve its own or others’ writing. These results indicate that, similar to humans, LLMs exhibit a marked addition bias, which might have im-plications when LLMs are used on a large scale. Addittive bias might increase resource use and environmental impact, leading to higher eco-nomic costs due to overconsumption and waste. This bias should be con-sidered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.3953920006752014
    },
    {
      "name": "Econometrics",
      "score": 0.33559662103652954
    },
    {
      "name": "Linguistics",
      "score": 0.32247328758239746
    },
    {
      "name": "Economics",
      "score": 0.2490200698375702
    },
    {
      "name": "Philosophy",
      "score": 0.15959027409553528
    }
  ],
  "institutions": [],
  "cited_by": 3
}