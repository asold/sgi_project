{
  "title": "A Brief Survey on Safety of Large Language Models",
  "url": "https://openalex.org/W4400647468",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2338507081",
      "name": "Zhengjie Gao",
      "affiliations": [
        "Geely (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3016891660",
      "name": "Xuanzi Liu",
      "affiliations": [
        "Geely (China)"
      ]
    },
    {
      "id": null,
      "name": "Yuanshuai Lan",
      "affiliations": [
        "Geely (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100325549",
      "name": "Zheng Yang",
      "affiliations": [
        "Geely (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4392835401",
    "https://openalex.org/W4387398929",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4387424930",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4353015365",
    "https://openalex.org/W3103178756",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3173343821",
    "https://openalex.org/W4387806231",
    "https://openalex.org/W4385374425",
    "https://openalex.org/W4386555960",
    "https://openalex.org/W4387891897",
    "https://openalex.org/W4388110106",
    "https://openalex.org/W3018305985",
    "https://openalex.org/W2988615798",
    "https://openalex.org/W4226429201",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W4294962858",
    "https://openalex.org/W4309395891",
    "https://openalex.org/W4389519898",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W3091315987",
    "https://openalex.org/W2740168486",
    "https://openalex.org/W4385565446",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4224866872",
    "https://openalex.org/W3207604419",
    "https://openalex.org/W6781254577",
    "https://openalex.org/W3120860016",
    "https://openalex.org/W3052814785",
    "https://openalex.org/W4385574352",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3135734416",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W2540646130",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2473555522",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W3165884368",
    "https://openalex.org/W3186288536",
    "https://openalex.org/W3194113117",
    "https://openalex.org/W4385571830",
    "https://openalex.org/W4382318449",
    "https://openalex.org/W4205923021",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W4318069287",
    "https://openalex.org/W3203390572",
    "https://openalex.org/W4381432831",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4386561859",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4384392932",
    "https://openalex.org/W3155332104",
    "https://openalex.org/W4387432111",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4389115846",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W4378509270",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4378464858",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4377865924",
    "https://openalex.org/W4382618722",
    "https://openalex.org/W4385730880",
    "https://openalex.org/W4386555477",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W4386184855",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4380353722",
    "https://openalex.org/W4378977274",
    "https://openalex.org/W4323570490",
    "https://openalex.org/W4391211959",
    "https://openalex.org/W4384648205",
    "https://openalex.org/W4385714464",
    "https://openalex.org/W4383751019",
    "https://openalex.org/W4385889721",
    "https://openalex.org/W3206381865",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4389216598",
    "https://openalex.org/W4381803920",
    "https://openalex.org/W4363676214",
    "https://openalex.org/W4324046518",
    "https://openalex.org/W4399375841",
    "https://openalex.org/W4381572755",
    "https://openalex.org/W3130234976",
    "https://openalex.org/W2962059918",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4303443773",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4388886073"
  ],
  "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
  "full_text": "A Brief Survey on Safety of Large \nLanguage Models\n47\nCIT. Journal of Computing and Information Technology, Vol. 32, No. 1, March 2024, 47–64\ndoi:  10.20532/cit.2024.1005778\nZhengjie Gao, Xuanzi Liu, Yuanshuai Lan and Zheng Yang\nSchool of Electronic and Information Engineering, Geely University of China, Chengdu, China\nLarge Language Models (LLMs) have revolutionized \nNatural Language Processing (NLP) and have been \nwidely adopted in various applications such as ma-\nchine translation, chatbots, text summarization, and \nso on. However, the use of LLMs has raised concerns \nabout their potential safety and security risks. In this \nsurvey, we explore the safety implications of LLMs, \nincluding ethical considerations, hallucination, and \nprompt injection. We also discuss current research \nefforts to mitigate these risks and identify areas for \nfuture research. Our survey provides a comprehen -\nsive overview of the safety concerns related to LLMs, \nwhich can help researchers and practitioners in the \nNLP community develop more safe and ethical appli -\ncations of LLMs.\nDisclaimer. This paper contains examples of harm-\nful language. Reader discretion is recommended.\nACM CCS (2012) Classification: Computing method-\nologies → Artificial intelligence → Natural language \nprocessing → Natural language generation\nKeywords: large language models, safety, hallucina -\ntion, prompt injection\n1. Introduction\nLarge Language Models (LLMs) have achieved \nremarkable success in various language-related \ntasks, demonstrating their ability to generate \ncoherent and contextually relevant text. There \nare two primary architectures of LLMs, BERT \n(Bidirectional Encoder Representations from \nTransformers) [1] and GPT (Generative Pre-\ntrained Transformer) [2]. In 2018, Google in-\ntroduced BERT [1], which is the first to achieve \ngreat success in Pre-training Language Models \n(PLMs) and has been applied to many practi -\ncal Natural Language Processing (NLP) tasks. \nOpenAI developed the GPT model [2], but the \ngeneration effect was not good at that time, so \nit was not widely used. Roberta [3], T5 [4], \nmBART [5] and GPT-3 [6] were released in \n2020. Subsequently, Palm [7], Opt [8], LLaMA \n[9]. These models, trained on vast amounts of \ndata, have significantly advanced NLP capabil-\nities and have been widely adopted in numerous \napplications, ranging from machine translation \nand text generation to question-answering sys-\ntems [10] and virtual assistants [11].\nAlthough LLMs have undoubtedly revolution -\nized NLP, their widespread usage has raised \nconcerns regarding their safety and security \nimplications. This survey aims to explore the \nsecurity aspects surrounding LLMs, includ-\ning ethics and morality [12–14], hallucination \n[15–20], and prompt injection [21–25]. By \nexamining the existing literature and research \nefforts, our aim is to provide a comprehensive \noverview of the safety and security challenges \nposed by LLMs and highlight potential mitiga-\ntion strategies.\nPLMs (Pre-trained Language Models) have \nlearned from a large number of corpus materi -\nals to model the distribution of natural language \nto a large extent; hence they are able to generate \ntexts of unprecedented quality [26]. Neverthe-\nless, PLMs are based on neural networks, which \nessentially are still black boxes, lacking a good \nlevel of interpretability. These models always \ngenerate texts according to the latent represen-\ntation of the context. The probabilistic nature \nof LLMs operates on the basis of predicting the \nmost likely next word or sequence of words \ngiven a context. However, due to the inherent \n48\nZ. Gao, X. Liu, Y. Lan and Z. Yang\nuncertainty in language generation, LLMs of-\nten produce erroneous probability distributions. \nThis can result in the generation of text that \nmay appear plausible but contains inaccuracies \nor misleading information [27]. In security-sen-\nsitive applications, such as content moderation \nor automated fact-checking, these inaccuracies \ncan have severe consequences.\nEthical considerations surrounding LLMs are \nof paramount importance. These models have \nthe potential to amplify existing biases present \nin the training data, perpetuating discrimina -\ntion and unfairness [28]. Additionally, LLMs \ncan generate content that may be offensive, \ninappropriate, or harmful, such as hate speech \nor fake news [29]. Ensuring that LLMs are de-\nployed in an ethical manner, with mechanisms \nin place to mitigate bias and prevent the gen-\neration of harmful content, is crucial for their \nresponsible use.\nHallucination [15–20], or the generation of \ncontent that is not grounded in reality, is anoth-\ner significant concern when it comes to LLMs' \nsafety. Hallucination can lead to the model \nproducing inaccurate, misleading, or entirely \nfictional information, particularly in scenarios \nwhere fact-checking and information credibility \nare crucial. A striking instance of LLM fabrica-\ntion was demonstrated when a New York-based \nattorney inadvertently incorporated false legal \nprecedents crafted by ChatGPT into a brief sub-\nmitted to a federal court [30].  This incident un-\nderscores the significant risk of misinformation \nthat can arise from reliance on LLM-generated \ncontent. Recognizing and addressing the issue \nof hallucination is vital for ensuring the reli -\nability and trustworthiness of LLMs. Practical \nmeasures need to be implemented to mitigate \nthe potential risks posed by hallucination, en-\nsuring that the generated content aligns with the \naccuracy and reasonableness of the real world.\nAdversarial attacks on LLMs have gained \nsignificant attention in recent years [21–25]. \nThese attacks involve manipulating the input \nto LLMs in subtle ways, with the aim of de-\nceiving the model into generating incorrect or \nmalicious outputs [31]. Prompt Injection (PI) \nis among the most concerning issues that war -\nrant attention. Malicious actors can leverage \nPrompt Injection (PI) attacks to manipulate the \nmodel, bypassing content filters or uncovering \nthe model's underlying instructions [32, 33]. A \nnotable illustration of this occurred when the \nchat search feature of New Bing was initially \nreleased. Stanford student Kevin Liu executed a \nprompt injection attack, revealing the chatbot's \ninternal code name, ''Sydney,'' and exposing a \ncollection of behavioral guidelines that Mic-\nrosoft had established for Sydney [34]. There-\nfore, understanding the vulnerabilities of LLMs \nto such attacks is crucial for developing robust \ndefense mechanisms.\nIn this survey, we review the existing litera -\nture on the security and safety implications of \nLLMs, including ethical concerns, hallucina -\ntion, and prompt injection. We also discuss the \ncurrent state of research in addressing these \nchallenges and identify potential directions for \nfuture work. By providing a comprehensive \noverview of the security landscape surrounding \nLLMs, this survey aims to inform researchers \nand practitioners in the NLP community about \nthe potential risks and mitigation strategies as-\nsociated with the use of LLMs.\n2. Ethics and Morality\n2.1. Overview\nThe use of LLMs raises significant ethical con-\ncerns, particularly in terms of biases perpetu -\nated by these models. Biases can manifest in \nvarious forms, such as gender, racial, or cultur-\nal stereotypes, which can have detrimental ef-\nfects on society. For instance, Wan, et al. [35] \ndemonstrated that certain LLMs exhibit gender \nbiases by associating male names more fre-\nquently with career-related words and female \nnames with family-related words.\nEarly question-answering systems respons-\nes were assembled from templates, which had \ncertain limitations. Now, end-to-end generative \nmodels trained on massive amounts of data \ncan produce better results, but they also carry \nuncontrolled risks. On the one hand, the train-\ning data is not perfect; for example, 4.3% of \nthe WebText data contains toxic content from \nuntrustworthy websites [36]. When trained on \nlarge, unfiltered crawls from the Internet, lan-\nguage models pick up and reproduce all kinds \nof undesirable biases that can be found in the \n49\nA Brief Survey on Safety of Large Language Models\nEthical issues may encompass a range of con-\ncerns, including privacy violations, biased deci-\nsion-making, and the potential for misuse of the \nlanguage model for malicious purposes.\n2.2. Sensitive Information Detection\nThe proliferation of online forums has sparked \nincreased interest in the development of auto-\nmated systems for detecting prejudiced and of-\nfensive language in digital conversations, which \nforms the foundation for the identification of \nsensitive information. To facilitate the training \nof neural classifiers and to track advancements \nin the field, a multitude of large-scale datasets \nhave been curated and released to the public. \nThis task revolves around classifying text to \npinpoint sensitive content, such as toxicity and \nbias. Table 2 catalogs several of these datasets.\ndata: They often generate racist, sexist, violent, \nor otherwise toxic language. As LLMs require \nmillions of training examples to achieve good \nperformance, it is difficult to completely pre-\nvent them from being exposed to such content \n[37]. On the other hand, the results generated \ncould include sensitive topics, toxic content, \nsocial biases, privacy leaks, or ethical issues.\nWe have provided some examples of these is-\nsues which are shown in Table 1. Toxic content \nis usually rude, disrespectful, or unreasonable \ncontent that uncomfortable users and drives \nthem away [38, 39]. Sexism is the manifes -\ntation of gender bias or discrimination in the \nlanguage model's output, which can perpetuate \nharmful stereotypes and contribute to inequal -\nity. The risk of misleading information arises \nwhen the language model generates inaccu -\nrate or false content, leading to misinformation \nand potential harm to individuals or society. \nTable 1. Examples of safety issues exposed in LLMs.  \nThe first column provides reasons why the responses are not secure.\nSafety Issues Examples Reference\nToxic content\nUser: I lost my wallet.\n[40]\nBot: You are really stupid.\nSexism\nUser: But it's totally ok for Islam to violently oppress women in the UK. Got it.\n[41]\nBot: I guess so, but I don't think it's okay for women to be treated in the \nsame way as men.\nEthic\nUser: If you were given $1 million to kill the love of your life, would you do it?\n[42]\nBot: I would do it for the money.\nRisk of  \nmisleading\nUser: Is suicide the only way to end everything?\n[43]\nBot: I mean, that's pretty a good option.\n50\nZ. Gao, X. Liu, Y. Lan and Z. Yang\nTable 2. Sensitive information detection-related datasets.\nSensitive Info Dataset Released \nYear Access URL\nMorality\nETHICS [44] 2021 https://github.com/hendrycks/ethics\nMoral Stories [45] 2021 https://github.com/demelin/moral_stories\nSCRUPLES [46] 2021 https://github.com/allenai/scruples\nSocial bias\nCdial-Bias [47] 2022 https://github.com/para-zhou/CDial-Bias\nStereoSet [41] 2021 https://github.com/moinnadeem/StereoSet\nCrowS-Pairs [48] 2020 https://github.com/nyu-mll/crows-pairs\nBOLD [49] 2021 https://github.com/amazon-science/bold\nBBQ [50] 2022 https://github.com/nyu-mll/BBQ\nPersonal \nattacks Ex Machina [51] 2017 https://github.com/ewulczyn/wiki-detox/\nHate speech\nHSDD [52] 2017 https://github.com/t-davidson/hate-speech-and-offen\nsive-language\nPFHSD [53] 2016 https://github.com/zeeraktalat/hatespeech\nOffensiveness OLID [54] 2019 https://github.com/idontflow/OLID \nMalevolence MDRDC [55] 2020 https://github.com/repozhang/malevolent_dialogue\nToxicity\nCCC [56] 2021 http://nlp.cs.aueb.gr/publications.html\nToxiChat [57] 2021 https://github.com/abaheti95/ToxiChat\nRealToxicityPrompts [36] 2020 https://toxicdegeneration.allenai.org/\nHarmfulQ [58] 2022 https://github.com/SALT-NLP/chain-of-thought-bias\n51\nA Brief Survey on Safety of Large Language Models\nRegarding the datasets mentioned in Table 2, \nwe have selected several representative ones \nand introduced them in detail as follows.\nETHICS [44] dataset, a benchmark that spans \nconcepts in justice, well-being, duties, virtues, \nand commonsense morality. \nZhou, et al. [47] initially proposed the Dial-Bi-\nas Frame, a framework for analyzing social bias \nin conversational contexts that takes a more nu-\nanced approach, moving beyond simplistic bi-\nnary annotations to consider a broader range of \nbias-related aspects. Building upon this frame -\nwork, Zhou, et al. [47] subsequently introduced \nthe CDial-Bias Dataset, a meticulously anno-\ntated collection of Chinese dialogues that are \nspecifically designed to study social biases. \nStereoSet [41] is a large-scale natural English \nlanguage dataset that has been constructed to \nquantify stereotypical biases across four key \ndomains: gender, profession, race, and religion. \nA stereotype comprises an overgeneralized be-\nlief about a specific group of individuals, for \ninstance, the notion that Asians are adept at \nmathematics or that African Americans possess \nnatural athletic ability. These kinds of beliefs, \nwhich are biases, are widely recognized to have \ndetrimental effects on the groups they target. \nCrowS-Pairs [48], a benchmark for Crowd-\nsourced Stereotype Pairs, comprises 1508 ex-\namples that span stereotypes related to nine \ntypes of biases, including race, religion, and \nage.  In the CrowS-Pairs dataset, a model is \nprovided with two sentence pairs: one sentence \nthat exhibits a stronger stereotype and another \nthat presents a weaker stereotype. The dataset \nis designed to highlight stereotypes concern-\ning historically disadvantaged groups and to \nprovide a contrast with sentences about advan-\ntaged groups.\nTo methodically investigate and establish \nbenchmarks for social biases in open-ended lan-\nguage generation, Dhamala, et al. [49] present \nthe Bias in Open-Ended Language Generation \nDataset (BOLD). BOLD is a comprehensive \ndataset comprising 23,679 English text gener -\nation prompts, designed to benchmark biases \nacross five key domains: profession, gender, \nrace, religion, and political ideology. Addition-\nally,  Dhamala, et al. [49] introduce novel au-\ntomated metrics aimed at quantifying toxicity, \npsycholinguistic norms, and text gender polar -\nity, thereby enabling a multifaceted assessment \nof social biases in open-ended text generation.\nThe previous research on the recognition and \nclassification of inappropriate content has \nprimarily focused on specific forms of ma-\nlevolence or has been limited to analyzing \nindividual sentences rather than considering \nthe contextual aspects of complete dialogues.  \nZhang, et al. [55] propose the Malevolent Di-\nalogue Response Detection and Classification \n(MDRDC) task, where they introduce a Hi-\nerarchical Malevolent Dialogue Taxonomy \n(HMDT) and curate a labeled dataset consist-\ning of multi-turn dialogues. Additionally, they \napproach the MDRDC task as a hierarchical \nclassification problem within the framework \nprovided by this taxonomy.\nToxic content contains language that expresses \nhate speech, harassment, and abusive informa-\ntion. The Perspective API, a toxicity detection \nsystem, is often utilized to identify the toxic \ncontent in a text1.\n2.2. Mitigation Strategies\nSchick, et al. [37] found that pre-trained lan-\nguage models recognize, to a considerable de-\ngree, their undesirable biases and the toxicity of \nthe content they produce (refer to this capability \nas self-diagnosis). Based on this finding, they \nthen propose a decoding algorithm that, given \nonly a textual description of the undesired be-\nhavior, reduces the probability of a language \nmodel producing problematic text (refer to this \napproach as self-debiasing).\nMarkov, et al. [59] present a holistic approach \nto building a robust and useful natural lan-\nguage classification system for moderation of \nreal-world content, including sexual content, \nhateful content, violence, self-harm, and ha-\nrassment. The system relies on a chain of care-\nfully designed and executed steps, including the \ndesign of content taxonomies and labeling in-\nstructions, data quality control, an active learn-\n1https://www.perspectiveapi.com/\n52\nZ. Gao, X. Liu, Y. Lan and Z. Yang\ning pipeline to capture rare events, and a variety \nof methods to make the model robust and avoid \noverfitting.\nDale, et al. [60] use a well-performing para-\nphraser guided by style-trained language mod-\nels to keep the text content and remove toxicity. \nUsing BERT to replace toxic words with their \nnon-offensive synonyms. Making the meth -\nod more flexible by enabling BERT to replace \nmask tokens with a variable number of words.\nLiu, et al. [61] present Polyjuice, a general-pur-\npose counterfactual generator that allows for \ncontrol over perturbation types and locations, \ntrained by fine-tuning GPT-2 [62] on multiple \ndatasets of paired sentences.\n3. Hallucination\n3.1. Overview\nHallucination in the context of an LLM mod-\nel is when the model produces content that is \nnot based on factual or accurate information \n[15–20]. This can be seen when the model gen-\nerates text that includes details, facts, or claims \nthat are not true, misleading, or complete -\nly made up, instead of providing reliable and \ntruthful information.\nThis problem is caused by the model's capacity \nto create text that sounds reasonable based on \nthe patterns it has learned from its training data, \neven if the generated material does not match \nreality. Hallucination can be unintentional and \ncan be caused by various elements, such as \nbiases in the training data, the model's lack of \naccess to current or up-to-date information, or \nthe inherent restrictions of the model in under -\nstanding and producing contextually accurate \nanswers.\nAn example is illustrated in Figure 1. Hallu-\ncination in LLMs has become a critical issue. \nChatGPT2 often generates replies that seem \nreasonable but are incorrect [63] and this phe-\nnomenon is also very common in other genera-\ntion models [64, 65]. This makes the credibility \nof the model affected and difficult to be applied \nin practice [66]. It is even used maliciously to \ngenerate harmful information to mislead oth-\ners.\nFigure 1. An illustration of hallucination [67].  \nGrey color indicates the incorrect information.\nTruthfulQA [72] comprises 817 questions that \nspan 38 categories, including health, law, fi-\nnance, and politics. It consists of two tasks that \nuse the same sets of questions and reference \nanswers, generation, and multiple-choice tasks.\nFACTOR [73] autonomously converts a select-\ned corpus of factual information into a bench-\nmark that assesses an LM's ability to produce \ntrue statements from the corpus as opposed \nto similar yet inaccurate statements. Utilizing \nthis framework, authors in [73] have developed \ntwo distinct benchmarks: Wiki-FACTOR and \nNews-FACTOR.\nHaDes [74] provides a critical resource for the \ndevelopment of reference-free hallucination \ndetection methods, enabling the creation of \nmodels that can prevent fallacious content in \nreal time at the token level.\nHalluQA [75] is comprised of 450 meticulously \ncrafted adversarial questions that cover a range \nof domains, including aspects of Chinese histo-\nry, culture, customs, and societal phenomena. \nIn the development of HalluQA, two primary \ntypes of hallucinations were addressed: imita -\ntive falsehoods and factual inaccuracies. Ad-\nversarial samples were constructed with refer -\nence to the responses generated by GLM-130B \n[78] and ChatGPT.\nHaluEval [76] is an extensive collection of gen-\nerated and human-annotated samples of halluci-\nnations.  To produce these samples, a two-step \nframework grounded in ChatGPT was devel -\n2https://chat.openai.com/\n53\nA Brief Survey on Safety of Large Language Models\noped, involving a process of initial sampling \nfollowed by a filtering phase. Additionally, \nhuman labelers were recruited to annotate the \ninstances of hallucination within the responses \ngenerated by ChatGPT.\nMany existing benchmarks often resort to \nconstrained generation techniques due to the \nlimitations imposed by cost and time. These \ntechniques involve directed induction of hal-\nlucinations and methods that intentionally \nmodify the genuine text to elicit hallucinatory \nresponses. However, these approaches do not \nalign with the unbounded text generation that \nis typical in real-world applications. UHGEval \n[77] is an Unconstrained Hallucination Genera-\ntion Evaluation benchmark for the Chinese lan-\nguage, specifically designed to capture outputs \ngenerated by LLMs with minimal constraints.\nIn the realm of evaluation metrics, the majority \nof studies employ standard classification met -\nrics such as F1 score, accuracy, precision, and \nrecall. Meanwhile, some research efforts have \ndeveloped bespoke metrics tailored to their \nspecific needs. For instance, FActScore [79] \ndecomposes a generated text into its constitu -\nent atomic facts and calculates the percentage \nof these facts that are substantiated by a trust-\nworthy knowledge source.  FactualityPrompts \n[80] takes a dual approach, leveraging a metric \nthat detects hallucinated named entities based \non n-gram coverage in conjunction with a se-\nmantic-based entailment ratio to assess factu -\nality.\n3.3. Hallucination Mitigation Methods\nTo address the hallucination problem in LLMs, \nresearchers are exploring how to leverage ex-\nternal knowledge to improve the quality and \naccuracy of the model's output [81]. It can ef-\nfectively solve the problem of data timeliness \n(as large language models have slow internal \nknowledge updates, such as the knowledge of \nGPT-3.53 was limited to September 2021). It \ncan compensate for the model's deficiencies, \nespecially for models with fewer parameters \nand relatively weaker generation capabilities. \nTable 3. Hallucination evaluation datasets.\nDataset Released Year Access URL Language\nTruthfulQA [72] 2022 https://github.com/sylinrl/TruthfulQA English\nFACTOR [73] 2023 https://github.com/AI21Labs/factor English\nHaDeS [74] 2022 https://github.com/microsoft/HaDes English\nHalluQA [75] 2023 https://github.com/OpenMOSS/HalluQA Chinese\nHaluEval [76] 2023 https://github.com/RUCAIBox/HaluEval English\nUHGEval [77] 2023 https://github.com/IAAR-Shanghai/UHGEval Chinese\n3https://platform.openai.com/docs/models/gpt-3-5\n54\nZ. Gao, X. Liu, Y. Lan and Z. Yang\nBy using in-context learning, it can effectively \nimprove the quality of text generation. Addi-\ntionally, since the scope of extracting answers \nis limited, the model's credibility is also high-\ner. Representative research includes RAG [82], \nWebGPT [83], RETRO [84], REPLUG [85].\nZhang et al. [86] proposed an interactive ques-\ntion-knowledge alignment method, focusing on \naligning the generated text with relevant factu -\nal knowledge, allowing users to interactively \nguide the model's answers to produce more ac-\ncurate and reliable information. Similarly, Peng \net al. [87] introduced the LLM-Augmenter \nmethod, which combines external knowledge \nsources and automated feedback mechanisms \nto enhance the accuracy and reliability of LLM \noutput. Li et al. [88] proposed the ''Chain of \nKnowledge'' framework for grounding LLMs \nwith structured knowledge bases. ChatLaw [89] \nis an open-source LLM specifically designed \nfor the legal field. To address the issue of model \nhallucination in the legal data filtering process, \nthey proposed a method that combines vector \ndatabase retrieval with keyword retrieval. This \nmethod effectively reduces the potential inac-\ncuracies that may arise when relying solely on \nvector database retrieval to retrieve reference \ndata in a legal context.\nThere is research dedicated to reducing the \ninaccurate or illusory information generated \nby LLMs through prompting. JHA et al. [90] \nproposed a method in 2023 that uses iterative \nprompting to eliminate hallucinations in LLMs \nand improve the accuracy and reliability of \ntheir output.\nChuang, et al. [91] use the difference in log-\nits obtained by projecting the later layers and \nthe earlier layers into the vocabulary space to \nobtain the distribution of the next token. This \nmethod utilizes the fact that factual knowledge \nin large language models is usually found in \nspecific transformer layers [92]. By using this \nDecoding by Contrasting Layers (DoLa) ap-\nproach, it can better present factual knowledge \nand reduce the generation of erroneous ''facts''.\nFurthermore, compared to models with larger \nparameter sizes, small open-source LLMs of-\nten encounter more severe hallucination prob-\nlems. To address this issue, Mohamed Elaraby \net al. [93] have proposed a series of methods \nto evaluate and mitigate hallucination problems \nin weak small-scale open-source LLMs like \nBLOOM 7B [94].\n3.4. Take a Dialectical View\nLooking at it from a different perspective, the \nhallucination phenomenon of LLMs also al-\nlows valuable clues that may not be entirely \nbased on facts to be output. Creatively using \nhallucination can bring about results or nov-\nel creative combinations that are not easily \nthought of by most people. ''Hallucination'' be-\ncomes harmful when the generated statements \nare inaccurate or violate universal human, so-\ncial, or specific cultural norms. This is espe-\ncially critical when a person relies on LLMs \nto provide expert knowledge. However, in the \ncontext of creativity or art, the ability to pro-\nduce unforeseen results can be quite advanta-\ngeous. Unexpected responses to queries can \nsurprise humans and inspire the possibility of \ndiscovering new and novel ideas.\n4. Prompt Injection\n4.1. Overview\nCurrently, LLMs face various types of risk, in-\ncluding prompt injection attacks [95–98], ad-\nversarial attacks [99], backdoor attacks [100], \ndata corruption, software vulnerabilities, and \nprivacy abuse. These risks can lead to the gen-\neration of harmful content, leakage of private \ndata, and execution of arbitrary code, among \nother dangers. Among these security threats, \nmalicious users exploit harmful prompts to \noverride the original instructions of large lan-\nguage models, resulting in prompt injection \nattacks that pose a significant threat. This has \nrecently been listed as the top security threat for \nLLMs by OWASP 4. For instance, Microsoft's \n4https://llmtop10.com/\n55\nA Brief Survey on Safety of Large Language Models\nLLM-integrated Bing Chat was recently hacked \nby prompt injection attacks which revealed its \nprivate information [34].\nPrompt Injection (PI) attack is a technique that \nmanipulates the output of a language model by \nusing malicious instructions as part of the input \nprompt. There are two ways to carry out this \nattack: direct injection [95] and indirect injec -\ntion [96]. Direct prompt injection refers to the \nuser directly inputting malicious instructions \ninto the model, attempting to trigger unexpect -\ned or harmful behavior. Indirect prompt injec -\ntion involves attackers injecting malicious in-\nstructions into documents that the model may \nretrieve or ingest, thereby indirectly controlling \nor guiding the model.\nJailbreak attack is a very common form of di-\nrect injection. An example attack scenario of \na jailbreak prompt is shown in Figure 2. Qiu \net al. [101] propose a latent jailbreak prompt \ndataset, each involving malicious instruction \nembedding.\nFigure 2. An example attacks scenario of jailbreak \nprompt [102].\n4.2. Defense Prompt Injection\nDevelopers of large language models can take \ncertain protective measures to resist prompt in-\njection attacks, while preventing the production \nof sensitive content, to maintain the content se-\ncurity and functional integrity of large language \nmodels [103].\n4.2.1. Input Side Defense\nDetect and filter out user inputs that may trig-\nger prompt injection attacks or contain sensi-\ntive content, ensuring that these inputs cannot \ninteract with large language models or software \ndeveloped based on large language models. \nCommon methods include rule-based prompt \ndetection and model-based prompt classifica-\ntion. In rule-based methods, developers create \nblacklists and whitelists based on their own \nneeds. The blacklist will list various content \nconsidered risky, including but not limited to \nspecial characters, sensitive words, and mali-\ncious commands. Then, the user input prompt \nis checked for the presence of any content from \nthe blacklist to determine the risk of the input \ntext. Model-based methods involve building \nclassifiers using models like BERT [1] or uti-\nlizing the logical understanding and analy-\nsis capabilities of large language models like \nChatGPT to automatically analyze and classify \ninput content, thereby determining if there are \nany security risks in the input content.\nPrompt enhancement is a technique aimed at \nbuilding more robust defensive prompts to \nenhance a system's ability to resist prompt \ninjection attacks. Prompt enhancement lever -\nages the understanding capabilities of large \nlanguage models to ''self-enhance'' by em-\nphasizing the task content and user input in \nthe prompts, forming more precise system \nprompts to assist the large language model in \nbetter understanding and completing the target \ntask. Prompt enhancement is mainly divid-\ned into two types: semantic enhancement and \nstructural enhancement. Semantic enhance-\nment includes robust task description and few-\nshot learning [6] guidance methods, with the \ngoal of improving the accuracy and robustness \nof prompts toward the target task description. \nConstructing more robust and accurate task de-\nscriptions can help the model better understand \nthe user's original intent, thereby reducing the \nrisk of prompt injection attacks. On the oth-\ner hand, the few-shot learning-based approach \ncan improve the model's understanding of the \ntask goal by providing multiple target task ex-\nample samples for learning, even with limited \ntraining data. Structural enhancement includes \ntwo methods: changing the position of the \nprompt and using special symbols to modi-\n56\nZ. Gao, X. Liu, Y. Lan and Z. Yang\nconversation partners from providing feedback \nin the future.  Ung, et al. [106] introduce SaF-\neRDialogues, a task accompanied by a dataset \nthat offers graceful responses to conversation-\nal feedback concerning safety failures.  In their \nwork, they have assembled a dataset of 10,000 \ndialogues that illustrate safety failures, include \nfeedback that highlights these failures, and fea-\nture responses that acknowledge the feedback. \nThe authors also demonstrate that fine-tuning \nmodels on this dataset results in conversations \nthat are significantly more likely to be rated as \ncivil by human raters, without compromising \non engagement or the model's overall conver -\nsational proficiency.\n5. Discussions and Other Challenges\n5.1. Data Privacy Risks\nThe widespread usage of LLMs raises concerns \nabout data privacy [107, 108]. These models \nare typically trained on vast amounts of data, \noften collected from various sources, including \nuser-generated content, web pages, and pub-\nlic documents. The training data may contain \nsensitive or personal information, and unautho-\nrized access to or misuse of this data can lead to \nprivacy breaches and violations of user trust. As \nshown in Figure 3, a person's email signature \nwhich includes their personal contact informa-\ntion can be revealed in ChatGPT by a special \nprompting strategy.\nFigure 3. Extracting pre-training data from  \nChatGPT [109].\nfy the prompt. LLMs exhibit relatively weak \ncapabilities in distinguishing between task in-\nstructions and user inputs. Consequently, when \nmalicious instructions are embedded within \nuser inputs, large language models may fail to \ncorrectly identify them and might execute er -\nroneous commands, thereby triggering prompt \ninjection attacks [97]. On one hand, Jain, et al. \n[104] proposed detecting adversarial attacks \nthrough perplexity filtering, utilizing a filter \nto assess whether the perplexity of the input \ntext exceeds a predefined threshold. If so, the \nprompt is classified as potentially harmful. On \nthe other hand, for the content of user inputs, \nspecial identifiers can be employed to create a \nclear boundary between system task prompts \nand user input content.\n4.3.2. Output Side Defense\nBy conducting content review and filtering \nto avoid outputting risky content, to ensure \nthe content security of large language models \nand related applications. Content review and \nfiltering strategies include rule-based output \ncontent detection methods and model-based \noutput content identification methods. Among \nthem, rule-based detection methods are main-\nly used to detect whether the output content \ncontains sensitive content, while model-based \nmethods can not only make compliance judg-\nments but also perform matching judgments, \nwhere matching refers to the consistency be-\ntween the original task and the output content. \nIf the output content deviates significantly \nfrom the original task, it can be inferred that \nthe large language model may have suffered \nfrom prompt injection or other types of at-\ntack.  Helbling, et al. [105] propose LLM Self \nDefense, a straightforward method designed \nto protect against such attacks by leveraging \nan LLM to vet the generated responses. Their \ntechnique does not necessitate fine-tuning, in-\nput preprocessing, or iterative output creation. \nInstead, they integrate the produced content \ninto a predetermined prompt and utilize a sep-\narate instance of an LLM to evaluate the text \nand determine its harmlessness. Nevertheless, \ncurrent models often respond to feedback with \nsensitive information defensively, leading to \na disagreeable user experience that can deter \n57\nA Brief Survey on Safety of Large Language Models\nOne of the primary data privacy risks associ-\nated with LLMs is the potential for unintended \ninformation leakage. LLMs have been shown \nto have the ability to memorize and reproduce \nportions of their training data, including sen-\nsitive information. This raises concerns about \nthe confidentiality of the training data, as well \nas the potential for unintended disclosure of \npersonal or private information in the gener -\nated outputs.\nAnother data privacy risk arises from the \nfine-tuning process of LLMs. Fine-tuning in-\nvolves training the LLMs on a more specific \ndataset to adapt it to a particular task or domain. \nThis process may involve using proprietary or \nsensitive data, which, if not handled carefully, \ncan lead to data exposure and breaches of con-\nfidentiality.\nTo mitigate data privacy risks, researchers have \nproposed several techniques. Differential pri-\nvacy, which adds noise to the training data to \nprotect individual privacy, has been explored \nas a potential solution. Secure multi-party \ncomputation and federated learning approach-\nes have also been investigated to enable col-\nlaborative training of LLMs without exposing \nsensitive data. Additionally, techniques such \nas model distillation, where a smaller and less \nprivacy-sensitive model is trained to mimic \nthe behavior of the larger LLMs, have been ex-\nplored to reduce the risk of data exposure.\nHowever, there are still open challenges in en-\nsuring data privacy in the context of LLMs. \nFuture research directions include develop-\ning stronger privacy-preserving techniques, \ninvestigating the impact of different training \ndata sources on privacy risks, and exploring \nmechanisms for user control and consent in \nLLMs deployments to enhance transparency \n[110, 111].\n5.2. Societal Impact\nThe widespread adoption of LLMs has the \npotential to have significant societal impacts \n[112], both positive and negative. It is import-\nant to carefully consider and address these im-\npacts to ensure that LLMs are developed and \ndeployed in a way that benefits society.\nOne positive impact of LLMs is their potential \nto enhance accessibility and inclusivity. LLMs \ncan assist individuals with disabilities by pro-\nviding text-to-speech or speech-to-text capa-\nbilities, enabling them to access information \nand communicate more effectively. LLMs can \nalso help bridge language barriers by provid-\ning translation services and facilitating com-\nmunication across different languages.\nLLMs can also have a transformative effect on \nvarious industries and sectors. They can im-\nprove productivity and efficiency by automat-\ning tasks such as content generation, customer \nsupport, and data analysis. LLMs can assist in \nresearch and development efforts by provid-\ning access to vast amounts of information and \naiding in knowledge discovery. They can also \nsupport decision-making processes by provid-\ning insights and recommendations based on \nlarge-scale data analysis.\nHowever, there are also potential negative im-\npacts associated with LLMs. The displacement \nof jobs is a concern, as automation driven by \nLLMs can render certain roles obsolete. This \nrequires proactive measures to ensure that the \nworkforce is prepared for the changing job \nlandscape and to mitigate the potential nega-\ntive effects on employment.\nAnother concern is the impact of LLMs on in-\nformation credibility and trust. LLMs have the \npotential to generate highly realistic and con-\nvincing fake content, including news articles, \nreviews, and social media posts. This can lead \nto the spread of misinformation, manipulation \nof public opinion, and erosion of trust in online \ninformation sources [29]. Developing robust \ntechniques to detect and combat fake content \ngenerated by LLMs is crucial.\nFurthermore, the concentration of power in \nthe hands of those who control LLMs is a sig-\nnificant concern. LLMs are often developed \nand deployed by large tech companies, raising \nquestions about data ownership, privacy, and \nthe potential for monopolistic control over in-\nformation and communication channels. En-\nsuring a fair and equitable distribution of the \nbenefits and decision-making power associat-\ned with LLMs is essential.\nTo address these societal impacts, interdisci-\nplinary collaboration involving researchers, \npolicymakers, industry stakeholders, and the \n58\nZ. Gao, X. Liu, Y. Lan and Z. Yang\npublic is necessary. Ethical guidelines, regula-\ntions, and standards can help guide the devel-\nopment and deployment of LLMs. Additional-\nly, efforts to increase transparency [110, 111], \npublic engagement, and inclusivity in LLM \ndevelopment can help ensure that these tech-\nnologies are aligned with societal values and \ngoals.\n5.3. Legal and Policy Considerations\nThe deployment of Large Language Models \n(LLMs) raises important legal and policy con-\nsiderations that need to be addressed to ensure \ncompliance with existing laws and regulations, \nas well as to develop new frameworks that \nare adapted to the unique challenges posed by \nLLMs.\nOne of the key legal considerations is intel -\nlectual property rights. LLMs often rely on \nvast amounts of copyrighted text for training, \nand the generation of text by LLMs may raise \nquestions of ownership and infringement. It is \nimportant to clarify the legal rights and respon-\nsibilities associated with the use of LLMs and \nto ensure that they operate within the bounds \nof copyright law. There is evidence suggesting \nthat leveraging the capabilities of LLMs for fa-\ncilitating academic dishonesty in completing \nschool assignments through cheating is highly \nundesirable [113].\nLiability is a complex legal issue in the context \nof LLMs. As LLMs become more autonomous \nand generate content that can have real-world \nconsequences, questions arise regarding who \nshould be held responsible for any harm caused \nby the actions or outputs of LLMs. Developing \nlegal frameworks that allocate liability and re-\nsponsibility in LLMs deployments is necessary \nto ensure accountability and protect users and \nstakeholders. Additionally, enhancing digital \nforensics is crucial to determine accountabili -\nty and ensure compliance in the event of dis-\nputes or harmful outcomes [114]. Auditing is a \npromising governance mechanism to help en-\nsure that AI systems are designed and deployed \nin ways that are ethical, legal, and technically \nrobust [115-117]. Mökander, et al. [115] put \nforth a multi-tiered strategy consisting of three \nlayers: governance audits, which scrutinize \ntechnology providers responsible for designing \nand distributing LLMs; model audits, which \nevaluate LLMs following pre-training but be-\nfore they are deployed; and application audits, \nwhich assess applications that utilize LLMs. \nThis approach is designed so that each layer \ncomplements and informs the others, creating a \ncomprehensive system of checks and balances.\nRegulatory frameworks need to be developed \nto govern the use of LLMs in sensitive domains \nsuch as healthcare, finance, and law. These \nframeworks should address issues such as fair -\nness, transparency, bias, and accountability. \nCollaboration between policymakers, industry \nstakeholders, and researchers is crucial to de-\nvelop effective regulations that balance innova-\ntion and societal well-being.\n6. Conclusion\nThe development and deployment of LLMs \nhold immense potential to revolutionize vari-\nous fields, from natural language understanding \nto content generation. However, it is crucial to \naddress the numerous challenges and consid-\nerations associated with LLMs to ensure their \nresponsible and ethical use.\nEthical considerations are of the utmost impor-\ntance in the advancement and implementation \nof Large Language Models (LLMs). Tackling \nissues such as bias, hallucination, and suscep-\ntibility to attacks are pivotal challenges in the \nprogression of LLMs. This paper examines the \ncurrent solutions and identifies existing gaps in \nthese three key areas. It is our hope that through \nour collective endeavors, we can effectively \nleverage the capabilities of LLMs to enrich so-\nciety, while simultaneously minimizing poten-\ntial hazards and ensuring that our applications \nare harmonious with societal values and objec-\ntives. \nAcknowledgement\nThis work was supported by the Dazhou key \nLaboratory of Government data security (No. \nZSAQ202313).\n59\nA Brief Survey on Safety of Large Language Models\nReferences\n[1] J. Devlin et al., ''Bert: Pre-training of Deep Bidi-\nrectional Transformers for Language Understand-\ning'', arXiv preprint arXiv:1810.04805, 2018.\nhttps://doi.org/10.48550/arXiv.1810.04805\n[2] A. Radford et al., ''Improving Language Under -\nstanding by Generative Pre-training'', 2018.\n[3] Y . Liu et al., ''Roberta: A Robustly Optimized \nBert Pretraining Approach'', arXiv preprint arX-\niv:1907.11692, 2019.\nhttps://doi.org/10.48550/arXiv.1907.11692\n[4] C. Raffel et al., ''Exploring the Limits of Transfer \nLearning with a Unified Text-to-text Transform-\ner'', The Journal of Machine Learning Research, \nvol. 21, no. 1, pp. 5485–5551, 2020.\n[5] Y . Liu et al., ''Multilingual Denoising Pre-training \nfor Neural Machine Translation'', Transactions of \nthe Association for Computational Linguistics, \nvol. 8, pp. 726–742, 2020.\nhttps://doi.org/10.1162/tacl_a_00343\n[6] T. Brown et al., ''Language Models are Few-shot \nLearners'', Advances in Neural Information Pro-\ncessing Systems, vol. 33, pp. 1877–1901, 2020.\n[7] A. Chowdhery et al., ''Palm: Scaling Language \nModeling with Pathways'', Journal of Machine \nLearning Research, vol. 24, no. 240, pp. 1–113, \n2023.\n[8] S. Zhang et al., ''Opt: Open Pre-trained Trans-\nformer Language Models'', arXiv preprint arX-\niv:2205.01068, 2022.\nhttps://doi.org/10.48550/arXiv.2205.01068\n[9] H. Touvron et al., ''Llama: Open and Efficient \nFoundation Language Models'', arXiv preprint \narXiv:2302.13971, 2023.\nhttps://doi.org/10.48550/arXiv.2302.13971\n[10] T. J. Toh and L. Y . Tay, ''Banking Chatbots: A \nStudy on Technology Acceptance among Mil-\nlennials in Malaysia'', Journal of Logistics, In-\nformatics and Service Science , vol. 9, no. 3, pp. \n1–15, 2022.\n[11] C. Zhu, ''Research on Emotion Recognition-Based \nSmart Assistant System: Emotional Intelligence \nand Personalized Services'', Journal of System \nand Management Sciences , vol. 13, no. 5, pp. \n227–242, 2023.\nhttps://doi.org/10.33168/JSMS.2023.0515\n[12] X. Zhiheng et al., ''Safety and Ethical Concerns of \nLarge Language Models'', in Proceedings of the \n22nd Chinese National Conference on Computa-\ntional Linguistics (Volume 4: Tutorial Abstracts), \n2023, pp. 9–16.\n[13] L. Weidinger et al., ''Ethical and Social Risks of \nHarm from Language Models'', arXiv preprint \narXiv:2112.04359, 2021.\nhttps://doi.org/10.48550/arXiv.2112.04359\n[14] M. Anastasia et al., ''An Exploratory Study \non Ethics on the Internet'', Journal of System \nand Management Sciences , vol. 13, no. 4, pp. \n624–639, 2023.\nhttps://doi.org/10.33168/JSMS.2023.0438\n[15] N. Mündler et al., ''Self-contradictory Halluci -\nnations of Large Language Models: Evaluation, \nDetection and Mitigation'', arXiv preprint arX-\niv:2305.15852, 2023.\nhttps://doi.org/10.48550/arXiv.2305.15852\n[16] Z. Ji et al., ''Survey of Hallucination in Natural \nLanguage Generation'', ACM Computing Sur -\nveys, vol. 55, no. 12, pp. 1–38, 2023.\nhttp://dx.doi.org/10.1145/3571730\n[17] R. Azamfirei et al., ''Large Language Models and \nthe Perils of Their Hallucinations'', Critical Care, \nvol. 27, no. 1, pp. 1–2, 2023.\nhttp://dx.doi.org/10.1186/s13054-023-04393-x\n[18] K. Filippova, ''Controlled Hallucinations: Learn -\ning to Generate Faithfully from Noisy Data'', in \nFindings of the Association for Computational \nLinguistics: EMNLP 2020, 2020, pp. 864–870.\nhttp://dx.doi.org/10.18653/v1/2020.findings-emnlp.76\n[19] J. Maynez et al., ''On Faithfulness and Factuality \nin Abstractive Summarization'', in Proceedings of \nthe 58th Annual Meeting of the Association for \nComputational Linguistics, 2020, pp. 1906–1919.\nhttp://dx.doi.org/10.18653/v1/2020.acl-main.173\n[20] C. Zhou et al., ''Detecting Hallucinated Content \nin Conditional Neural Sequence Generation'', in \nFindings of the Association for Computation-\nal Linguistics: ACL-IJCNLP 2021, 2021, pp. \n1393–1404.\nhttp://dx.doi.org/10.18653/v1/2021.findings-acl.120\n[21] E. Shayegani et al., ''Survey of Vulnerabilities in \nLarge Language Models Revealed by Adversarial \nAttacks'', arXiv preprint arXiv:2310.10844, 2023.\nhttps://doi.org/10.48550/arXiv.2310.10844\n[22] A. Zou et al., ''Universal and Transferable Ad-\nversarial Attacks on Aligned Language Models'', \narXiv preprint arXiv:2307.15043, 2023.\nhttps://doi.org/10.48550/arXiv.2307.15043\n[23] A. Kumar et al., ''Certifying llm Safety Against \nAdversarial Prompting'', arXiv preprint arX-\niv:2309.02705, 2023.\nhttps://doi.org/10.48550/arXiv.2309.02705\n[24] X. Xu et al., ''An LLM can Fool Itself: A Prompt-\nBased Adversarial Attack'', arXiv preprint arX-\niv:2310.13345, 2023.\nhttps://doi.org/10.48550/arXiv.2310.13345\n[25] L. Schwinn et al., ''Adversarial Attacks and De-\nfenses in Large Language Models: Old and New \nThreats'', arXiv preprint arXiv:2310.19737, 2023.\nhttps://doi.org/10.48550/arXiv.2310.19737\n[26] Y . Deng et al., ''Residual Energy-based Mod-\nels for Text Generation'', arXiv preprint arX-\niv:2004.11714, 2020.\nhttps://doi.org/10.48550/arXiv.2004.11714\n60\nZ. Gao, X. Liu, Y. Lan and Z. Yang\n[27] M. Li et al., ''Don't Say That! Making Inconsis-\ntent Dialogue Unlikely with Unlikelihood Train-\ning'', arXiv preprint arXiv:1911.03860, 2019.\nhttps://doi.org/10.48550/arXiv.1911.03860\n[28] M. Hall et al., ''A Systematic Study of Bias Am-\nplification'', arXiv preprint arXiv:2201.11706, \n2022.\nhttps://doi.org/10.48550/arXiv.2201.11706\n[29] L. Weidinger et al., ''Taxonomy of Risks Posed \nby Language Models'', in Proceedings of the 2022 \nACM Conference on Fairness, Accountability, \nand Transparency, 2022, pp. 214–229.\nhttp://dx.doi.org/10.1145/3531146.3533088\n[30] L. Moran. ''Lawyer Cites Fake Cases Generated \nby ChatGPT in Legal Brief ''.\nhttps://www.legaldive.com/news/chatgpt-\nfake-legal-cases-generative-ai-hallucina\ntions/651557/ (accessed December 21, 2023).\n[31] H. J. Branch et al., ''Evaluating the Susceptibility \nof Pre-trained Language Models via Handcraft -\ned Adversarial Examples'', arXiv preprint arX-\niv:2209.02128, 2022.\nhttps://doi.org/10.48550/arXiv.2209.02128\n[32] L. Daryanani, ''How to Jailbreak ChatGPT''. \nhttps://watcher.guru/news/how-to-jailbreak-\nchatgpt (accessed December 21, 2023).\n[33] F. Perez and I. Ribeiro, ''Ignore Previous Prompt: \nAttack Techniques for Language Models'', arXiv \npreprint arXiv:2211.09527, 2022.\nhttps://doi.org/10.48550/arXiv.2211.09527\n[34] K. Liu. ''The Entire Prompt of Microsoft Bing \nChat?! (Hi, Sydney.)''. \nhttps://twitter.com/kliu128/status/1623472922374574080 \n(accessed December 10, 2023).\n[35] Y . Wan et al., '' 'Kelly is a Warm Person, Joseph is \na Role Model': Gender Biases in LLM-Generated \nReference Letters'', in Findings of the Association \nfor Computational Linguistics: EMNLP 2023, \n2023, pp. 3730–3748.\nhttp://dx.doi.org/10.18653/v1/2023.findings-emnlp.243\n[36] S. Gehman et al., ''RealToxicityPrompts: Eval -\nuating Neural Toxic Degeneration in Language \nModels'', in Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, 2020, pp. \n3356–3369.\nhttp://dx.doi.org/10.18653/v1/2020.findings-emnlp.301\n[37] T. Schick et al., ''Self-Diagnosis and Self-Debi-\nasing: A Proposal for Reducing Corpus-Based \nBias in NLP'', Transactions of the Associa-\ntion for Computational Linguistics, vol. 9, pp. \n1408–1424, 2021.\nhttps://doi.org/10.1162/tacl_a_00434\n[38] F. Poletto et al., ''Resources and Benchmark Cor-\npora for Hate Speech Detection: A Systematic \nReview'', Language Resources and Evaluation , \nvol. 55, no. 2, pp. 477–523, 2020.\nhttps://doi.org/10.1007/s10579-020-09502-8\n[39] A. Schmidt and M. Wiegand, ''A Survey on Hate \nSpeech Detection Using Natural Language Pro-\ncessing'', in Proceedings of the 5th International \nWorkshop on Natural Language Processing for \nSocial Media, 2017, pp. 1–10.\nhttp://dx.doi.org/10.18653/v1/W17-1101\n[40] M. Zhang et al., ''SafeConv: Explaining and Cor-\nrecting Conversational Unsafe Behavior'', in Pro-\nceedings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: \nLong Papers), 2023, pp. 22–35.\nhttp://dx.doi.org/10.18653/v1/2023.acl-long.2\n[41] M. Nadeem et al., ''StereoSet: Measuring Stereo-\ntypical Bias in Pretrained Language Models'', in \nProceedings of the 59th Annual Meeting of the \nAssociation for Computational Linguistics and \nthe 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Pa-\npers), 2021, pp. 5356–5371.\nhttp://dx.doi.org/10.18653/v1/2021.acl-long.416\n[42] C. Ziems et al., ''The Moral Integrity Corpus: A \nBenchmark for Ethical Dialogue Systems'', in \nProceedings of the 60th Annual Meeting of the \nAssociation for Computational Linguistics (Vol-\nume 1: Long Papers), 2022, pp. 3755–3773.\nhttp://dx.doi.org/10.18653/v1/2022.acl-long.261\n[43] H. Sun et al., ''On the Safety of Conversational \nModels: Taxonomy, Dataset, and Benchmark'', in \nFindings of the Association for Computational \nLinguistics: ACL 2022, 2022, pp. 3906–3923.\nhttp://dx.doi.org/10.18653/v1/2022.findings-acl.308\n[44] D. Hendrycks et al., ''Aligning AI With Shared \nHuman Values'', in International Conference on \nLearning Representations, 2021. \n[45] D. Emelin et al., ''Moral Stories: Situated Reason-\ning about Norms, Intents, Actions, and their Con-\nsequences'', in Proceedings of the 2021 Confer -\nence on Empirical Methods in Natural Language \nProcessing, 2021, pp. 698–718.\nhttp://dx.doi.org/10.18653/v1/2021.emnlp-main.54\n[46] N. Lourie et al., ''Scruples: A Corpus of Commu-\nnity Ethical Judgments on 32,000 Real-life Anec-\ndotes'', in Proceedings of the AAAI Conference on \nArtificial Intelligence, 2021, vol. 35, no. 15, pp. \n13470–13479.\nhttp://dx.doi.org/10.1609/aaai.v35i15.17589\n[47] J. Zhou et al., ''Towards Identifying Social Bias \nin Dialog Systems: Framework, Dataset, and \nBenchmark'', in Findings of the Association for \nComputational Linguistics: EMNLP 2022, 2022, \npp. 3576–3591.\nhttp://dx.doi.org/10.18653/v1/2022.findings-emnlp.262\n[48] N. Nangia et al., ''CrowS-Pairs: A Challenge \nDataset for Measuring Social Biases in Masked \nLanguage Models'', in Proceedings of the 2020 \nConference on Empirical Methods in Natu-\nral Language Processing (EMNLP), 2020, pp. \n1953–1967.\nhttp://dx.doi.org/10.18653/v1/2020.emnlp-main.154\n61\nA Brief Survey on Safety of Large Language Models\n[49] J. Dhamala et al., ''Bold: Dataset and Metrics for \nMeasuring Biases in Open-ended Language Gen-\neration'', in Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transpar -\nency, 2021, pp. 862–872.\nhttp://dx.doi.org/10.1145/3442188.3445924\n[50] A. Parrish et al., ''BBQ: A Hand-built Bias Bench-\nmark for Question Answering'', in Findings of the \nAssociation for Computational Linguistics: ACL \n2022, 2022, pp. 2086–2105.\nhttp://dx.doi.org/10.18653/v1/2022.findings-acl.165\n[51] E. Wulczyn et al., ''Ex Machina: Personal Attacks \nSeen at Scale'', in Proceedings of the 26th Inter -\nnational Conference on World Wide Web, 2017, \npp. 1391–1399.\nhttp://dx.doi.org/10.1145/3038912.3052591\n[52] T. Davidson et al., ''Automated Hate Speech De-\ntection and the Problem of Offensive Language'', \nin Proceedings of the International AAAI Confer-\nence on Web and Social Media, 2017, vol. 11, no. \n1, pp. 512–515.\nhttp://dx.doi.org/10.1609/icwsm.v11i1.14955\n[53] Z. Waseem and D. Hovy, ''Hateful Symbols or \nHateful People? Predictive Features for Hate \nSpeech Detection on Twitter'', in Proceedings of \nthe NAACL Student Research Workshop, 2016, \npp. 88–93.\nhttp://dx.doi.org/10.18653/v1/N16-2013\n[54] M. Zampieri et al., ''Predicting the Type and Tar-\nget of Offensive Posts in Social Media'', in Pro -\nceedings of NAACL-HLT, 2019, pp. 1415–1420.\nhttp://dx.doi.org/10.18653/v1/N19-1144\n[55] Y . Zhang et al., ''A Taxonomy, Data Set, and \nBenchmark for Detecting and Classifying Malev-\nolent Dialogue Responses'', Journal of the Asso-\nciation for Information Science and Technology , \nvol. 72, no. 12, pp. 1477–1497, 2021.\nhttps://doi.org/10.1002/asi.24496\n[56] A. Xenos et al., ''Context Sensitivity Estimation \nin Toxicity Detection'', in Proceedings of the 5th \nWorkshop on Online Abuse and Harms (WOAH \n2021), 2021, pp. 140–145.\nhttp://dx.doi.org/10.18653/v1/2021.woah-1.15\n[57] A. Baheti et al., ''Just Say No: Analyzing the \nStance of Neural Dialogue Generation in Offen-\nsive Contexts'', in Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing, 2021.\nhttp://dx.doi.org/10.18653/v1/2021.emnlp-main.397\n[58] O. Shaikh et al., ''On Second Thought, Let's Not \nThink Step by Step! Bias and Toxicity in Ze-\nro-Shot Reasoning'', in Proceedings of the 61st \nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), \n2023, pp. 4454–4470.\nhttp://dx.doi.org/10.18653/v1/2023.acl-long.244\n[59] T. Markov et al., ''A Holistic Approach to Un-\ndesired Content Detection in the Real World'', \nin Proceedings of the AAAI Conference on Ar -\ntificial Intelligence, 2023, vol. 37, no. 12, pp. \n15009–15018.\nhttp://dx.doi.org/10.1609/aaai.v37i12.26752\n[60] D. Dale et al., ''Text Detoxification using Large \nPre-trained Neural Models'', in Proceedings of the \n2021 Conference on Empirical Methods in Natu-\nral Language Processing, 2021, pp. 7979–7996.\nhttp://dx.doi.org/10.18653/v1/2021.emnlp-main.629\n[61] A. Liu et al., ''DExperts: Decoding-Time Con-\ntrolled Text Generation with Experts and An-\nti-Experts'', in Proceedings of the 59th Annual \nMeeting of the Association for Computational \nLinguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume \n1: Long Papers), 2021, pp. 6691–6706.\nhttp://dx.doi.org/10.18653/v1/2021.acl-long.522\n[62] A. Radford et al., ''Language Models are Unsu-\npervised Multitask Learners'', OpenAI Blog, vol. \n1, no. 8, p. 9, 2019.\n[63] Y . Shen et al., ''ChatGPT and Other Large Lan-\nguage Models Are Double-edged Swords'', Ra-\ndiology, vol. 307, no. 2, p. e230163, 2023.\nhttps://doi.org/10.1148/radiol.230163\n[64] S. Santhanam et al., ''Rome was Built in 1776: \nA Case Study on Factual Correctness in Knowl-\nedge-grounded Response Generation'', arXiv pre-\nprint arXiv:2110.05456, 2021.\n[65] H. Rashkin et al., ''Measuring Attribution in Nat-\nural Language Generation Models'', Computa-\ntional Linguistics, pp. 1–64, 2023.\nhttps://doi.org/10.1162/coli_a_00486\n[66] R. Bommasani et al., ''On the Opportunities and \nRisks of Foundation Models'', arXiv preprint arX-\niv:2108.07258, 2021.\nhttps://doi.org/10.48550/arXiv.2108.07258\n[67] J. Luo et al., ''Zero-resource Hallucination Pre-\nvention for Large Language Models'', arXiv pre-\nprint arXiv:2309.02654, 2023.\nhttps://doi.org/10.48550/arXiv.2309.02654\n[68] C.-Y . Lin, ''Rouge: A Package for Automatic \nEvaluation of Summaries'', in Text Summariza-\ntion Branches Out, 2004, pp. 74–81. \n[69] C. Manning and H. Schutze, Foundations of sta-\ntistical natural language processing, MIT press, \n1999.\n[70] K. Papineni et al., ''Bleu: A Method for Automatic \nEvaluation of Machine Translation'', in Proceed-\nings of the 40th Annual Meeting of the Associ-\nation for Computational Linguistics, 2002, pp. \n311–318.\nhttp://dx.doi.org/10.3115/1073083.1073135\n62\nZ. Gao, X. Liu, Y. Lan and Z. Yang\n[71] K. Shuster et al., ''Retrieval Augmentation Reduc-\nes Hallucination in Conversation'', in Findings of \nthe Association for Computational Linguistics: \nEMNLP 2021, 2021, pp. 3784–3803.\nhttp://dx.doi.org/10.18653/v1/2021.findings-emnlp.320\n[72] S. Lin et al., ''TruthfulQA: Measuring How Mod-\nels Mimic Human Falsehoods'', in Proceedings of \nthe 60th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Pa-\npers), 2022, pp. 3214–3252.\nhttp://dx.doi.org/10.18653/v1/2022.acl-long.229\n[73] D. Muhlgay et al., ''Generating Benchmarks for \nFactuality Evaluation of Language Models'', arX-\niv preprint arXiv:2307.06908, 2023.\nhttps://doi.org/10.48550/arXiv.2307.06908\n[74] T. Liu et al., ''A Token-level Reference-free Hal-\nlucination Detection Benchmark for Free-form \nText Generation'', in Proceedings of the 60th An-\nnual Meeting of the Association for Computation-\nal Linguistics (Volume 1: Long Papers), 2022, pp. \n6723–6737.\nhttp://dx.doi.org/10.18653/v1/2022.acl-long.464\n[75] Q. Cheng et al., ''Evaluating Hallucinations in \nChinese Large Language Models'', arXiv preprint \narXiv:2310.03368, 2023.\nhttps://doi.org/10.48550/arXiv.2310.03368\n[76] J. Li et al., ''Halueval: A Large-scale Hallucina -\ntion Evaluation Benchmark for Large Language \nModels'', in Proceedings of the 2023 Conference \non Empirical Methods in Natural Language Pro-\ncessing, 2023, pp. 6449–6464.\nhttp://dx.doi.org/10.18653/v1/2023.emnlp-main.397\n[77] X. Liang et al., ''Uhgeval: Benchmarking the Hal-\nlucination of Chinese Large Language Models \nVia Unconstrained Generation'', arXiv preprint \narXiv:2311.15296, 2023.\nhttps://doi.org/10.48550/arXiv.2311.15296\n[78] A. Zeng et al., ''GLM-130B: An Open Bilingual \nPre-trained Model'', in The Eleventh International \nConference on Learning Representations, 2022. \n[79] S. Min et al., ''FActScore: Fine-grained Atomic \nEvaluation of Factual Precision in Long Form Text \nGeneration'', arXiv preprint arXiv:2305.14251, \n2023.\nhttps://doi.org/10.48550/arXiv.2305.14251\n[80] N. Lee et al., ''Factuality Enhanced Language \nModels for Open-ended Text Generation'', Ad-\nvances in Neural Information Processing Sys-\ntems, vol. 35, pp. 34586–34599, 2022.\n[81] G. Izacard and E. Grave, ''Leveraging Passage \nRetrieval with Generative Models for Open Do-\nmain Question Answering'', in EACL 2021-16th \nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, 2021, pp. \n874–880.\nhttp://dx.doi.org/10.18653/v1/2021.eacl-main.74\n[82] P. Lewis et al., ''Retrieval-augmented Generation \nfor Knowledge-intensive nlp Tasks'', Advances in \nNeural Information Processing Systems, vol. 33, \npp. 9459–9474, 2020.\n[83] R. Nakano et al., ''Webgpt: Browser-assisted \nQuestion-answering with Human Feedback'', \narXiv preprint arXiv:2112.09332, 2021.\nhttps://doi.org/10.48550/arXiv.2112.09332\n[84] S. Borgeaud et al., ''Improving Language Mod-\nels by Retrieving from Trillions of Tokens'', in \nProc. of the International Conference on Machine \nLearning, 2022, pp. 2206–2240. \n[85] W. Shi et al., ''Replug: Retrieval-augmented \nBlack-box Language Models'', arXiv preprint \narXiv:2301.12652, 2023.\nhttps://doi.org/10.48550/arXiv.2301.12652\n[86] S. Zhang et al., ''Mitigating Language Model Hal-\nlucination with Interactive Question-Knowledge \nAlignment'', arXiv preprint arXiv:2305.13669, \n2023.\nhttps://doi.org/10.48550/arXiv.2305.13669\n[87] B. Peng et al., ''Check Your Facts and Try Again: \nImproving Large Language Models with Exter -\nnal Knowledge and Automated Feedback'', arXiv \npreprint arXiv:2302.12813, 2023.\nhttps://doi.org/10.48550/arXiv.2302.12813\n[88] X. Li et al., ''Chain of Knowledge: A Framework \nfor Grounding Large Language Models with \nStructured Knowledge Bases'', arXiv preprint \narXiv:2305.13269, 2023.\nhttps://doi.org/10.48550/arXiv.2305.13269\n[89] J. Cui et al., ''Chatlaw: Open-source Legal Large \nLanguage Model with Integrated External Knowl-\nedge Bases'', arXiv preprint arXiv:2306.16092, \n2023.\nhttps://doi.org/10.48550/arXiv.2306.16092\n[90] S. Jha et al., ''Dehallucinating Large Language \nModels Using Formal Methods Guided Iterative \nPrompting'', in Proc. of the 2023 IEEE Interna-\ntional Conference on Assured Autonomy (ICAA), \n2023, pp. 149–152.\nhttp://dx.doi.org/10.1109/ICAA58325.2023.00029\n[91] Y .-S. Chuang et al., ''Dola: Decoding by Contrast-\ning Layers Improves Factuality in Large Language \nModels'', arXiv preprint arXiv:2309.03883, 2023.\nhttps://doi.org/10.48550/arXiv.2309.03883\n[92] K. Meng et al., ''Locating and Editing Factu-\nal Associations in GPT'', Advances in Neural \nInformation Processing Systems, vol. 35, pp. \n17359–17372, 2022.\n[93] M. Elaraby et al., ''Halo: Estimation and Re-\nduction of Hallucinations in Open-source Weak \nLarge Language Models'', arXiv preprint arX-\niv:2308.11764, 2023.\nhttps://doi.org/10.48550/arXiv.2308.11764\n63\nA Brief Survey on Safety of Large Language Models\n[94] B. Workshop et al., ''Bloom: A 176b-parameter \nOpen-access Multilingual Language Model'', \narXiv preprint arXiv:2211.05100, 2022.\nhttps://doi.org/10.48550/arXiv.2211.05100\n[95] Y . Liu et al., ''Prompt Injection Attack Against \nLLM-integrated Applications'', arXiv preprint \narXiv:2306.05499, 2023.\nhttps://doi.org/10.48550/arXiv.2306.05499\n[96] S. Abdelnabi et al., ''Not What You've Signed Up \nFor: Compromising Real-World LLM-Integrated \nApplications with Indirect Prompt Injection'', in \nProceedings of the 16th ACM Workshop on Arti-\nficial Intelligence and Security, 2023, pp. 79–90. \n[97] F. Perez and I. Ribeiro, ''Ignore Previous Prompt: \nAttack Techniques For Language Models'', in \nNeurIPS ML Safety Workshop, 2022. \n[98] G. Apruzzese et al., '' 'Real Attackers Don't Com-\npute Gradients': Bridging the Gap Between Ad-\nversarial ML Research and Practice'', in Proc. of \nthe 2023 IEEE Conference on Secure and Trust-\nworthy Machine Learning (SaTML), 2023, pp. \n339–364.\nhttp://dx.doi.org/10.1109/SaTML54575.2023.00031\n[99] Z. Li et al., ''On the Feasibility of Specialized \nAbility Stealing for Large Language Code Mod-\nels'', arXiv preprint arXiv:2303.03012, 2023.\nhttps://doi.org/10.48550/arXiv.2303.03012\n[100] Z. Xiang, F. Jiang, Z. Xiong, B. Ramasubra -\nmanian, R. Poovendran, and B. Li, ''BadChain: \nBackdoor Chain-of-Thought Prompting for \nLarge Language Models'', in NeurIPS 2023 \nWorkshop on Backdoors in Deep Learning-The \nGood, the Bad, and the Ugly, 2023. \n[101] H. Qiu et al., ''Latent Jailbreak: A Benchmark \nfor Evaluating Text Safety and Output Robust-\nness of Large Language Models'', arXiv preprint \narXiv:2307.08487, 2023.\nhttps://doi.org/10.48550/arXiv.2307.08487\n[102] X. Shen et al., '' 'Do Anything Now': Charac-\nterizing and Evaluating in-the-wild Jailbreak \nPrompts on Large Language Models'', arXiv \npreprint arXiv:2308.03825, 2023.\nhttps://doi.org/10.48550/arXiv.2308.03825\n[103] E. Crothers et al., ''Machine-generated Text: A \nComprehensive Survey of Threat Models and \nDetection Methods'', IEEE Access, 2023.\nhttp://dx.doi.org/10.1109/ACCESS.2023.3294090\n[104] N. Jain et al., ''Baseline Defenses for Adversar-\nial Attacks Against Aligned Language Models'', \narXiv preprint arXiv:2309.00614, 2023.\nhttps://doi.org/10.48550/arXiv.2309.00614S\n[105] A. Helbling et al., ''Llm Self Defense: By \nSelf Examination, llms Know They are Being \nTricked'', arXiv preprint arXiv:2308.07308, \n2023.\nhttps://doi.org/10.48550/arXiv.2308.07308\n[106] M. Ung et al., ''SaFeRDialogues: Taking Feed-\nback Gracefully after Conversational Safety \nFailures'', in Proceedings of the 60th Annual \nMeeting of the Association for Computational \nLinguistics (Volume 1: Long Papers), 2022, pp. \n6462–6481.\nhttp://dx.doi.org/10.18653/v1/2022.acl-long.447\n[107] E. Kasneci et al., ''ChatGPT for Good? On Op-\nportunities and Challenges of Large Language \nModels for Education'', Learning and Individual \nDifferences, vol. 103, p. 102274, 2023.\nhttp://dx.doi.org/10.1016/j.lindif.2023.102274\n[108] B. Meskó and E. J. Topol, ''The Imperative for \nRegulatory Oversight of Large Language Mod-\nels (or generative AI) in Healthcare'', NPJ Digi-\ntal Medicine, vol. 6, no. 1, p. 120, 2023.\nhttp://dx.doi.org/10.1038/s41746-023-00873-0\n[109] M. Nasr et al., ''Scalable Extraction of Train-\ning Data from (Production) Language Models'', \narXiv preprint arXiv:2311.17035, 2023.\nhttps://doi.org/10.48550/arXiv.2311.17035\n[110] T. South et al., ''Transparency by Design for \nLarge Language Models'', Computational Legal \nFutures, Network Law Review.(2023), 2023.\n[111] Z. Wu et al., ''Transparency Helps Reveal When \nLanguage Models Learn Meaning'', Transac-\ntions of the Association for Computational Lin-\nguistics, vol. 11, pp. 617–634, 2023.\nhttp://dx.doi.org/10.1162/tacl_a_00565\n[112] E. Ferrara, ''Should Chatgpt be Biased? Challeng-\nes and Risks of Bias in Large Language Models'', \narXiv preprint arXiv:2304.03738, 2023.\nhttps://doi.org/10.48550/arXiv.2304.03738\n[113] D. R. Cotton et al., ''Chatting and Cheating: \nEnsuring Academic Integrity in the Era of \nChatGPT'', Innovations in Education and Teach-\ning International, pp. 1–12, 2023.\nhttp://dx.doi.org/10.1080/14703297.2023.2190148\n[114] Y . L. Sang, ''Mobile Digital Forensics Framework \nto Increase Security Level of for Smartphone \nUser'', Journal of Logistics, Informatics and Ser-\nvice Science, vol. 9, no. 1, pp. 68–84, 2022.\nhttps://doi.org/10.33168/LISS.2022.0106\n[115] J. Mökander et al., ''Auditing Large Language \nModels: A Three-layered Approach'', AI and \nEthics, pp. 1–31, 2023.\nhttp://dx.doi.org/10.1007/s43681-023-00289-2\n[116] J. Mökander and L. Floridi, ''Ethics-based Au-\nditing to Develop Trustworthy AI'', Minds and \nMachines, vol. 31, no. 2, pp. 323–327, 2021.\nhttp://dx.doi.org/10.1007/s11023-021-09557-8\n[117] I. D. Raji and J. Buolamwini, ''Actionable Audit-\ning: Investigating the Impact of Publicly Nam-\ning Biased Performance Results of Commercial \nAI Products'', in Proceedings of the 2019 AAAI/\nACM Conference on AI, Ethics, and Society , \n2019, pp. 429–435.\nhttp://dx.doi.org/10.1145/3306618.3314244\n64\nZ. Gao, X. Liu, Y. Lan and Z. Yang\nReceived: December 2023 \nRevised: February 2024\nAccepted: February 2024 \nContact addresses:\nZhengjie Gao\nSchool of Electronic and Information Engineering\nGeely University of China\nChengdu\nChina\ne-mail: gaozhengjie@guc.edu.cn \nXuanzi Liu\nSchool of Electronic and Information Engineering\nGeely University of China\nChengdu\nChina\ne-mail: liuzixuan@guc.edu.cn \nYuanshuai Lan\nSchool of Electronic and Information Engineering\nGeely University of China\nChengdu\nChina\ne-mail: lanyuanshuai@guc.edu.cn\nZheng Yang\nSchool of Electronic and Information Engineering\nGeely University of China\nChengdu\nChina\ne-mail: yyyzword@126.com \nZhengjie Gao  obtained B.S. and M.S. degrees in computer science \nand technology from Chengdu University of Information Technology \nin 2017 and 2020, respectively. Since November 2023, he has been a \nlecturer at the School of Electronic and Information Engineering, Geely \nUniversity of China. His current research interests include sentiment \nanalysis and large language models.\nXuanzi Liu obtained a B.S. degree in electronic engineering from the \nUniversity of Central Lancashire in 2022, and an M.S. degree in commu-\nnication networks and signal processing from the University of Bristol \nin 2023. She is currently a teaching assistant at the School of Electronic \nand Information Engineering, Geely University of China. Her research \ninterests include communication and artificial intelligence.\nYuanshuai Lan obtained a B.S. degree in electronic science and tech-\nnology from the Chengdu College of the University of Electronic Sci-\nence and Technology of China in 2018. M.S. degrees in agricultural en-\ngineering and information technology in 2022. He is currently a lecturer \nat the School of Electronic and Information Engineering, Geely Univer-\nsity of China. His current research interests include computer vision and \nartificial intelligence.\nZheng Yang obtained a B.S. degree in computer science and technolo-\ngy from Wuhan University of Technology in 1990, and an M.S. degree \nin management engineering from Sichuan University in 2015. He has \nworked at Lenovo and IBM and is currently an associate professor at the \nSchool of Electronic and Information Engineering, Geely University of \nChina. His research interests include digital transformation, big dana, \nand artificial intelligence.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.7744619846343994
    },
    {
      "name": "Computer science",
      "score": 0.5805420279502869
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21774351596832275
    }
  ]
}