{
  "title": "Research on Application of Financial Large Language Models",
  "url": "https://openalex.org/W4405809121",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2979319137",
      "name": "Mingting Du",
      "affiliations": [
        "Southwestern University of Finance and Economics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "With the increasing use of large language models such as chatgpt, it is not difficult to apply their capabilities to the research of natural language processing in the financial field, including but not limited to text extraction, sentiment analysis, etc. This paper analyzes the construction ideas and applications of three financial big language models, including BloombergGPT, PIXIU and FinBERT, and concludes that the current application of big language models in the financial field is possible, multi-faceted and suitable, but there are still shortcomings in ethics, data processing and other aspects. The application of large language models in the field of finance is still something to look forward to. Through this study and the comparative exploration of various models, we hope to provide valuable modeling experience for practitioners in the field of finance or computer. At the same time, it is hoped that each researcher can follow the ideas of these model-making teams to make up for the shortcomings in their own models and make their own financial big language models better.",
  "full_text": "Highlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n628 \nResearch on Application of Financial Large Language Models \nMingting Du * \nSchool of mathematics, Southwestern University of Finance and Economics, Chengdu, Sichuan, \nChina \n* Corresponding Author Email: 42234005@smail.swufe.edu.cn \nAbstract. With the increasing use of large language models such as chatgpt, it is not difficult to apply \ntheir capabilities to the research of natural language processing in the financial field, including but \nnot limited to text extraction, sentiment analysis, etc. This paper analyzes the construction ideas and \napplications of three financial big language models, including BloombergGPT, PIXIU and FinBERT, \nand concludes that the current application of big language models in the fi nancial field is possible, \nmulti-faceted and suitable, but there are still shortcomings in ethics, data processing and other \naspects. The application of large language models in the field of finance is still something to look \nforward to . Through this study and the comparative exploration of var ious models, we hope to \nprovide valuable modeling experience for practitioners in the field of finance or computer. At the \nsame time, it is hoped that each researcher can follow the ideas of these model -making teams to \nmake up for the shortcomings in their own models and make their own financial big language models \nbetter. \nKeywords: Large Language Models, BloombergGPT, PIXIU, FinBERT, natural language \nprocessing. \n1. Introduction \nGPT-3, released in 2020, demonstrates the power of in -context learning and is a significant \nmilestone for LLMs, with large language models still under development gaining great attention in \nrecent years [1]. Practitioners in the financial field began to think about whether big language models \ncould be applied in the financial field, such as stock trend prediction, corporate financial report \ninterpretation and other scenarios that require a lot of data processing power. \nThis paper selects three mature financial big language models BloombergGPT, PIXIU and \nFinBERT, and analyses their model -building ideas, application scenarios, experimental results and \nshortcomings respectively, so as to draw conclusions on the application research of big language \nmodels in the financial field based on their similarities. \n2. Brief introduction of large language models \nTo predict the probability distribution of lexical sequences, a statistical model known as a language \nmodel is trainable on a large corpus database. The original model is called the n -gram model, which \nrepresents the sequence of terms as Markov processes, assuming that the probability of the next term \ndepends only on the previous term [2]. Subsequently, language models based on Recurrent Neural \nnetworks (RNN) such as LSTM and GRU were born. \nThen, i n 2017, the Transformer architecture was unveiled, which sparked the development of \nseveral more language models that, for instance, perform better at translation tasks than recurrent \nneural networks. For effective training on big data sets, the Transforme r architecture models word \nassociations using a \"self -attention\" method. Currently, a famous Transformer -based models is \nGenerative Pre-trained Transformer Model, which is also called GPT, created by OpenAI. It is a \nsizable language model that only has enc oders and uses location embedding and a self -focusing \ntechnique to get dependencies in text [3].  With a deep bidirectional architecture and the ability to \nlearn context representations, Bidirectional Encoder Representations from Transformers, known as \nBERT, is a decoder -only framework [4]. FinBERT is a big language model that focuses on finance \nand is based on BERT. Its sentiment analysis capability is improved by pre -training BERT financial \nHighlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n629 \ntexts. Furthermore, BLOOM was developed using the Tansformer archi tecture and a vast array of \nparameters that were trained on around 366 billion (1.6TB) tokens [5]. The open -source BLOOM \nmodel supports several languages, and its financial application version is known as BLOOMGPT. \n3. Large language models in finance \n3.1. BloombergGPT \nMany financial tasks are supported by the 50 billion parameter language model of BloombergGPT \n[6]. In order to maintain competitive performance on conventional LLM benchmarks while achieving \nbest-in-class results on financial benchmarks, the B loombergGPT team developed a model. The \ngroup achieved this by utilizing Bloomberg's current capabilities for data development, gathering, \nand maintenance to create the largest domain-specific dataset to date. \n3.1.1. Data construction of FinPILE \nFinPILE is an extensive collection of financial papers in the English language that were taken from \nthe Bloomberg archive, including news, documents, press releases, financial documents that were \ncrawled from the internet, and social media [6]. \nBloombergGPT's training expertise on a large, clean, well -collated domain-specific dataset may \noffer valuable insights into developing large language models for finance, even if the team has said \nthat FinPILE would not be made public. \nFor almost four decades, the bloomberg terminal has given a broad and diversified set of organized \nand unstructured financial data and analytics. Using this extensively gathered and updated dataset, \nBloomberg experts assembled a collection of financial papers to develop FinPile, a collection of \ncorporate records, financial news, and other information pertaining to the financial markets. FinPILE \ncontains web data made out of The majority of FinPile is made up of online material that \nBloombergGPT finds and gathers, which includes hundreds of transcri pts of Bloomberg TV news; \nnews data, which is a collection of news items authored by Bloomberg journalists;  financial \nstatements, largely consisting of the files in this dataset originate from EDGAR, the SEC's web \ndatabase; news, which generally consists p ress releases made publicly by financially relevant firms. \nPress releases, as opposed to filings, are written in a manner like to news stories. Bloomberg is the \nprimary source for news as well as other papers the company has produced, including opinion and  \nanalysis. \"Bloomberg News\" and \"Bloomberg First Word\" are the primary sources. \nThe PILE, the data set used in GPT -neo, GPT -J, and GPT -neox (20B), has been used to \nsuccessfully train LLMs and includes training that diversifies data and may even support fin ancial \ndata; C4, known as Colossal Clean Crawled Corpus, a frequently used data for train LLMs, was one \nof the three well -known and publicly available datasets used in the model. The techniques used for \ncleaning and processing C4 data are varied; Wikipedia , particularly the most recent entries, may \nenhance the model's legitimacy. \n3.1.2. Model construction \nThe BloombergGPT, based on BLOOM, is a decoder-only causal language model. The size of this \nmodel is based on Chinchilla scaling laws [7]. Due to the limited data, it is necessary to select a model \nas large as possible, and the parameter of the final model is 50B [ 6]. A total of 139,200 steps (about \n53 days) were trained, with 569B tokens of 709B tokens available in training [6]. \n3.1.3. Evaluation \nFor both financial and general reasons, the effectiveness of BloombergGPT was assessed across \ntwo major job areas. The notion that training on high quality financial particular data would result in \ngreater performance on the financial problem is tested in part by the Financial particular task. The \nGeneral Task looks at whether the model's output may be directly compared to results that have \nalready been published. \nHighlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n630 \nCompared with broader NLP tasks, NLP tasks in finance possess unique qualities and difficulties \nwhile working with financial data. \nExternal Financial tasks include Any news that might be favorable or bad for investors is classified \nas positive or negative in the Financial Phrasebank Dataset (FPB), which contains the sentiment \nclassification job of finan cial news phrases; otherwise, it is classified as neutral; FiQA SA is based \non the 2018 Financial Q&A and Opinion Mining Challenge and is used to anticipate the sentiment of \nparticular features in English financial news and Weibo [8];  Determines if a headl ine in the gold \ncommodities sector has specific information using the binary classification task Headline; Credit risk \nassessments for the named entity recognition (NER) task on financial data are derived from financial \nagreements filed with the SEC; ConvFinQA was tasked with responding to conversational questions \nrequiring numerical reasoning based on S&P 500 earnings report inputs, which included text and \nmore than one table containing financial data. \nAccording to the final trial results, BloombergGPT com es in second in NER and performs better \nthan all other models for four of the five tasks, which include ConvFinQA, FiQA SA, FPB, and \nHeadline [6]. \nOne of the internal financial tasks is Equity News Sentiment, which assesses how readers will feel \nabout a co mpany based on news stories; a \"positive,\" \"negative,\" or \"neutral\" note indicates that \nreaders' long-term belief in the company may rise, fall, or remain unchanged; Similar to \"Equity News \nSentiment,\" but using English-language social media material pertaining to finance instead of news, \nis Equity Social Media Sentiment, as reported by BloombergGPT; Similar to \"Equity News \nSentiment,\" however instead of news, the team used a transcript from a press conference held by the \nfirm; Country News Sentiment, which  differs from different sentiment tasks in that its aim is to \nforecast the emotions conveyed for a country in articles, and the dataset includes Bloomberg's news \nstories in English, premium news, and web content [6]. ES News Sentiment, which is to predict \nsentiment on specific aspects of a company expressed in news reports; the aim is to aviod indicating \nthe impact on investor’s belief. \nThis section's baseline definition is the goal for which the model can produce the right answer \ngiven the information in the rendered input text. The data sets include BoolQ, OpenBookQA, RACE, \nand some other data sets. \nThe final experiment result is that except for OpenBookQA, the performance of BloombergGPT \nis the highest among BLOOM176B, GPT-NeoX, and OPT66B [6]. \nA situation that isn't immediately related to a user-facing program is called a linguistic job. These \nresponsibilities include assessing implication, syntax, and disambiguation. The purpose of these \nchallenges is to evaluate the model's language comprehension directly. \nThe data sets include Recognizing Textual Entailment, Adversarial NLI, Commitment Bank , \nChoice of Plausible Alternatives, Words in Context, Winograd, Winograd, HellaSWAG, Story Cloze. \nThe final experiment result is that BloombergGPT falls slightly behind GPT -3 and outperforms \nthe other models [6]. \n3.1.4. Advantages of BloombergGPT \nIn the comparison, BloombergGPT fared better than the other models with the approximate size \nof parameters. Furthermore, it can match or even outperform bigger models under some situations. \nEven yet, the model improved its capacity to handle generic data, beating models of a comparable \nsize and, occasionally, equal or surpassing bigger models. \n3.2. PIXIU \nPIXIU is a thorough framework that includes FinMA, the first financial LLM, which is built on \noptimizing LLMa using data from multimodal and multitasking teaching. The researchers developed \ndistinct task-specific instructions written by domain experts for each task u sing publicly available \ntraining data from a variety of tasks, including financial sentiment analysis, news headline \nclassification, named entity recognition, question and answer sets, and stock movement predictions.  \nHighlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n631 \n3.2.1. Raw Data Processing \nCreate a fi nancial instruction tuning dataset (FIT) for a range of financial NLP and forecasting \napplications using open-source data. \nAnalyzing the emotional data entered into financial texts is the goal of the long-standing financial \nsentiment analysis endeavor. The  Financial Phrase Bank (FPB) dataset and FiQA -SA are the two \ndatasets used in this model [8]. \nThe purpose of the news headline categorization task is to assess various types of information, \nincluding changes in price in financial language. The algorithm makes use of a dataset of golden news \nheadlines that spans the years 2000 to 2019 and includes news stories about \"gold\" together with the \nnine associated tags (Resembling the external financial task's headline part in Bloomberg GPT). \nSorting each label in each data sample into a binary class is the task. \nNamed Entity Recognition (NER) tasks identify important financial entities, such as individuals, \nbusinesses, and locations, that may be utilized to create a financial knowledge map. The model \nleverages the F IN dataset, which comprises manually annotated entity types from LOCATION, \norganization, and PERSON. \nThe process of automatically providing financial answers to inquiries based on the data supplied \nis known as question answering. FinQA and Con -vFinQA are the two datasets that the researchers \nused. \nPredicting the movement of stocks is a fundamental financial job that has significant practical \nsignificance in areas like investment strategy. The price change is allocated to the positive sample “1” \nif it exceeds 0.55% and to the negative sample “ -1” if it falls below -0.5%. Three widely used data \nsets—BigData22, ACL18, and CIKM18—are employed in the model. \n3.2.2. Instruction Construction \nThe model constructs its instruction tuning samples using the following template for the FPB, \nFiQA-SA, Headline, NER, BigData22, ACL18, and CIKM18 datasets: \"[task prompt]  Text: [input \ntext] Response: [output]” [9]. The prompt for each data is denoted by \" [task prompt]\", the input \nfinancial data from each data is denoted by \"[input text]\", and the corresponding output for each data \nis indicated by \"[output]\", such as the label of the input text from “Netural”, “Positive” and \n“Negative”. \nThe following templa te is built by this model for FinQA and ConvFinQA. The task prompt is \nnearly the same but includes a part of “Context Input”. For every data sample, the input contextual \ninformation is \"[input context]\". For instance, the text in the FinQA files can be used to populate the \ninput context. \n3.2.3. FinMA – Financial large language model \nThe researchers used FIT fine -tuning of LLaMA to create FinMA, and they fine -tuned LLaMA \n7B and 30B checkpoints to acquire instruction tuning data pertaining to NLP tasks to create FINMA-\n7B and FINMA -30B. By fine -tuning LLaMA 7B to get comprehensive instruction tuning data, \nFinMA-7B-full is constructed [9]. \n3.2.4. Experiments and Results \nIn this section, Investigators compare FIT -fine-tuned FinMA with BloombergGPT, GPT -4, \nChatGPT, BLOOM, GPT-NeoX, OPT-66B, Vicuna-13B [10-13]. \nOn the FPB, FiQA -SA, and Headline datasets, the experiment's end result shows that the fine -\ntuning model FinMA performs noticeably better than other LLMS, highlighting the significance of \ndomain-specific inst ruction tweaking for enhancing domain -specific large language model \nperformance [9]. \nHighlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n632 \n3.3. FinBERT \nOwing to the specific terminology employed in the financial domain,  generic models are not \neffective enough. The researchers introduced FinBERT, a Bert-based language model, to handle NLP \ntasks in the financial field. \n3.3.1. Preliminaries \nThe following techniques were employed in the study to get the model ready for training: long \nshort-term memory, a kind of recurrent neural network that uses \"forget\" and \"update\" gates to let \nlong-term dependencies in the sequence remain in the network;  ELMo embedding, which is \ncontextualized word representation because surrounding words affect word representation [14]; \nULMFit, a downstream NLP task transfer learning model  pre-trained with language models [15]; \nTransformer is an attention -based sequence information model architecture. BERT, essentially a \nlanguage model, consists of a set of Transformer encoders stacked on top of each other. \n3.3.2. Data sets \n1.8 million news articles published by Reuters between 2008 and 2010 make up TRC2 -financial; \n4845 English sentences selected at random from financial news found on the LexisNexis database \n[16] make up Financial PhraseBank; and FiQA Sentiment, a dataset created for conference financial \nopinion mining and question answering challenge, are among the data sets used in the experiment. \n1,174 financial news items and tweets with related sentiment scores are among the data used by the \nresearchers [17]. \n3.3.3. Conclusions \nThree indicators were employed in the assessment process: the macro F1 average, the cross entropy \nloss, and the accuracy. FinBERT outperforms the models published in other studies (LPS, HSC, \nFinSSLX) as well as the approaches we constructed ourselves (LSTM and ULMFit) for every tested \nmetric [17]. \n4. Challenges and Limitations \n4.1. Ethic Level \nFinance is a delicate area of technology, and guaranteeing accurate, reliable information is crucial \nfor goods, consumers, and a company's reputation in the marketplace. Similarly, as makers of models, \nthey take extra care with whatever they produce, whether from humans or machines. You need to \nkeep an eye on the model's tendency to generate inappropriate content, and make it cleaner and \ncontain less obviously biased or offensive content output. \n4.2. Data Level \nHandle financial data with high dimensions: Compared with ordinary text, financial data contains \nmore dimensions, including time series and other information, and the higher the dimension, the more \ncomplex the processing. Processing information in high-dimensional financial data sets is a challenge \nto computing power and the choice of processing methods. \nEnsure data purity and diversity: There are many ways to obtain information today, and the data \ncapture process for large language models needs further supervision to ensure the reliability of data \nsources and prevent the capture of data from misinformation from affecting experimental results. In \naddition, obtaining data from multiple aspects is crucial to the establishment of financial big language \nmodel, and ensuring the diversity of data helps to further enhance the data processing capability of \nfinancial big language model. \nHighlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n633 \n5. Conclusion \nStarting from the big language model, this study introduces BloombergGPT, PIXIU and FinBERT \nfinancial big language models based on big language models, and introduces their data set \nconstruction ideas, model processing ability, experimental process results and shortcomings. Through \nefficient processing speed, in terms of obtaining financial i nformation and analyzing market \nsentiment, financial big language model greatly improves the information collection and processing \nability of practitioners in the financial industry, simplifies many tedious information processing \nprocesses, improves the acquisition speed of financial information, and helps users to obtain market \ninformation after analysis in a timely manner. \nHowever, on the moral level, the output content of the model, as well as the objectivity and \nrationality of the judgment given by the model, deserve further supervision and consideration. At the \ndata level, the processing of high-dimensional data, as well as the purity and diversity of the data sets \nused for training, require further thinking and changes to optimize the capabilities and output of the \nfinancial big language model. \nAt present, with the further development of artificial intelligence and the continuous training of \nlarge language model data sets, this paper believes that financial large language model still has a very \nlarge space for development, and it is hoped to achieve more intelligent data analysis, processing and \noutput capabilities, and further provide users with better quality and more helpful results. \nReferences \n[1] T. Brown, B. Mann, N. Ryder, et al. Language models are few-shot learners. NeurIPS, 33: 1877 – 1901. \n(2020). \n[2] T. Almutiri, F. Nadeem, Markov models applications in natural language processing: a survey. Int. J. Inf. \nTechnol. Comput. Sci 2, 1 – 16 (2022). \n[3] Tom B. Brown et al. Language Models are Few-Shot Learners, arXiv:2005.14165 (2020). \n[4] J. Devlin, M. Chang, K. Lee, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language \nUnderstanding. arXiv: 1810.04805 (2019). \n[5] T. Scao, A. Fan, C. Akiki, et al. Bloom: A 176b -parameter open-access multilingual language model. \narXiv preprint arXiv: 2211.05100. (2022). \n[6] S. Wu, O. Irsoy, S. Lu, et al. BloombergGPT: A Large Language Model for Finance, arXiv: 2303.17564v3 \n(2023). \n[7] J. Hoffmann, S. Borgeaud, A. Mensch et al. (2022). An empirical analysis of compute -optimal large \nlanguage model training. Adv. neural inf. process. syst, 35, 30016 - 30030 (2022). \n[8] M.  Maia, S. Handschuh, A. e Freitas, et al. financial opinion mining and question answering. The Web \nConference 2018, pages 1941 – 1942 (2018). \n[9] Q. Xie, W. Han, X. Zhang, et al. PIXIU: A Large Language Model, Instruction Data and Evaluation \nBenchmark for Finance, arXiv: 2306.05443v1 (2023). \n[10] OpenAI. GPT-4 Technical Report. arXiv: 2303.08774 (2023). \n[11] T. Le Scao, A. Fan, C. Akiki, et al. Bloom: A 176b-parameter open-access multilingual language model. \narXiv preprint arXiv: 2211.05100 (2022). \n[12] S. Zhang, S. Roller, N. Goyal, et al. Opt: Open pre -trained transformer language models. arXiv preprint \narXiv: 2205.01068 (2022). \n[13] Z. Zhang, H. Zhang, K. Chen, et al. Mengzi: Towards lightweight yet ingenious pre -trained models for \nchinese. arXiv preprint arXiv: 2110.06696 (2021). \n[14] M. E Peters, M. Neumann, M. Iyyer, et al. Deep contextualized word representations. https:  \n//doi.org/10.18653/v1/N18- 1202 arXiv: 1802.05365 (2018). \n[15] J. Howard, S. Ruder. Universal Language Model Fine- tuning for Text Classification. arXiv: 1801.06146 \nhttp://arxiv.org/abs/ 1801.06146 (2018). \nHighlights in Business, Economics and Management EMFRM 2024 \nVolume 45 (2024)  \n \n634 \n[16] P. Malo, A. Sinha, P. Korhonen, et al. good debt or bad debt: Detecting semantic orientations in economic \ntexts. J. Assoc. Inf. Sci. Technol. 65, 4, 782 – 796 (2014). \n[17] D. Tan Araci, FinBERT: Financial Sentiment Analysis with Pre-trained Language Models. arXiv preprint \narXiv: 1908.10063 (2019). ",
  "topic": "Business",
  "concepts": [
    {
      "name": "Business",
      "score": 0.41615280508995056
    },
    {
      "name": "Computer science",
      "score": 0.3622678518295288
    }
  ],
  "institutions": [],
  "cited_by": 1
}