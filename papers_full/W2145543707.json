{
  "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation",
  "url": "https://openalex.org/W2145543707",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5021878400",
      "name": "Noam Shazeer",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5071928196",
      "name": "Joris Pelemans",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5068010225",
      "name": "Ciprian Chelba",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2020382207",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2951793508",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W36903255",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W10704533",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W115367774",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2024592335",
    "https://openalex.org/W2078549297",
    "https://openalex.org/W1539309091"
  ],
  "abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM $n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as $n$-gram LMs do.",
  "full_text": "arXiv:1412.1454v2  [cs.LG]  26 Jun 2015\nSkip-gram Language Modeling Using Sparse\nNon-negative Matrix Probability Estimation\nNoam Shazeer Joris Pelemans\nCiprian Chelba\nGoogle, Inc., 1600 Amphitheatre Parkway\nMountain View, CA 94043, USA\n{noam,jpeleman,ciprianchelba}@google.com\nAbstract\nWe present a novel family of language model (LM) estimation techniques\nnamed Sparse Non-negative Matrix (SNM) estimation.\nA ﬁrst set of experiments empirically evaluating it on the One Billion\nWord Benchmark [Chelba et al., 2013] shows that SNMn-gram LMs per-\nform almost as well as the well-established Kneser-Ney (KN)models. When\nusing skip-gram features the models are able to match the state-of-the-art re-\ncurrent neural network (RNN) LMs; combining the two modeling techniques\nyields the best known result on the benchmark.\nThe computational advantages of SNM over both maximum entropy and\nRNN LM estimation are probably its main strength, promisingan approach\nthat has the same ﬂexibility in combining arbitrary features effectively and\nyet should scale to very large amounts of data as gracefully as n-gram LMs\ndo.\n1 Introduction\nA statistical language model estimates probability valuesP (W ) for strings of\nwords W in a vocabularyV whose size is in the tens, hundreds of thousands and\nsometimes even millions. Typically the stringW is broken into sentences, or other\nsegments such as utterances in automatic speech recognition, which are often as-\nsumed to be conditionally independent; we will assume thatW is such a segment,\nor sentence.\nEstimating full sentence language models is computationally hard if one seeks\na properly normalized probability model1 over strings of words of ﬁnite length in\n1We note that in some practical systems the constraint on using a properly normalized language\n1\nV∗. A simple and sufﬁcient way to ensure proper normalization of the model is to\ndecompose the sentence probability according to the chain rule and make sure that\nthe end-of-sentence symbol</s>is predicted with non-zero probability in any\ncontext. WithW = w1, w 2, . . . , w n we get:\nP (W ) =\nn∏\ni=1\nP (wi|w1, w 2, . . . , w i−1) (1)\nSince the parameter space ofP (wk|w1, w 2, . . . , w k−1) is too large, the lan-\nguage model is forced to put thecontextWk−1 = w1, w 2, . . . , w k−1 into anequiv-\nalence classdetermined by a functionΦ( Wk−1). As a result,\nP (W ) ∼\n=\nn∏\nk=1\nP (wk|Φ( Wk−1)) (2)\nResearch in language modeling consists of ﬁnding appropriate equivalence\nclassiﬁersΦ and methods to estimateP (wk|Φ( Wk−1)). The most successful paradigm\nin language modeling uses the(n − 1)-gram equivalence classiﬁcation, that is, de-\nﬁnes\nΦ( Wk−1) .= wk−n+1, w k−n+2, . . . , w k−1\nOnce the formΦ( Wk−1) is speciﬁed, only the problem of estimatingP (wk|Φ( Wk−1))\nfrom training data remains.\nPerplexity as a Measure of Language Model Quality\nA statistical language modelcan be evaluated by how well it predicts a string of\nsymbols Wt—commonly referred to astest data—generated by the source to be\nmodeled.\nA commonly used quality measure for a given modelM is related to the en-\ntropy of the underlying source and was introduced under the name of perplexity\n(PPL):\nP P L(M) = exp(− 1\nN\nN∑\nk=1\nln [PM (wk|Wk−1)]) (3)\nFor an excellent discussion on the use of perplexity in statistical language mod-\neling, as well as various estimates for the entropy of English the reader is referred\nto [Jelinek, 1997], Section 8.4, pages 141-142 and the additional reading suggested\nin Section 8.5 of the same book.\nmodel is side-stepped at a gain in modeling power and simplicity.\n2\nVery likely, not all words in the test stringWt are part of the language model\nvocabulary. It is common practice to map all words that are out-of-vocabulary to a\ndistinguishedunknown word symbol, and report the out-of-vocabulary (OOV) rate\non test data—the rate at which one encounters OOV words in thetest stringWt—\nas yet another language model performance metric besides perplexity. Usually the\nunknown word is assumed to be part of the language model vocabulary—open vo-\ncabularylanguage models—and its occurrences are counted in the language model\nperplexity calculation, Eq. (3). A situation less common inpractice is that ofclosed\nvocabularylanguage models where all words in the test data will always be part of\nthe vocabularyV.\n2 Skip-gram Language Modeling\nRecently, neural network (NN) smoothing [Bengio et al., 2003], [Emami, 2006], [Schwenk, 2007],\nand in particular recurrent neural networks [Mikolov, 2012] (RNN) have shown\nexcellent performance in language modeling [Chelba et al.,2013]. Their excellent\nperformance is attributed to a combination of leveraging long-distance context, and\ntraining a vector representation for words.\nAnother simple way of leveraging long distance context is touse skip-grams. In\nour approach, a skip-gram feature extracted from the contextWk−1 is characterized\nby the tuple(r, s, a ) where:\n• r denotes number of remote context words\n• s denotes the number of skipped words\n• a denotes the number of adjacent context words\nrelative to the target wordwk being predicted. For example, in the sentence,\n<S> The quick brown fox jumps over the lazy dog </S>\na (1, 2, 3) skip-gram feature for the target worddogis:\n[brown skip-2 over the lazy]\nFor performance reasons, it is recommended to limits and to limit either(r+a)\nor limit bothr and s; not setting any limits will result in events containing a set\nof skip-gram features whose total representation size is quintic in the length of the\nsentence.\nWe conﬁgure the skip-gram feature extractor to produce all featuresf, deﬁned\nby the equivalence classΦ( Wk−1), that meet constraints on the minimum and max-\nimum values for:\n• the number of context words usedr + a;\n3\n• the number of remote wordsr;\n• the number of adjacent wordsa;\n• the skip lengths.\nWe also allow the option of not including the exact value ofs in the feature\nrepresentation; this may help with smoothing by sharing counts for various skip\nfeatures. Tied skip-gram features will look like:\n[curiousity skip-*the cat]\nIn order to build a good probability estimate for the target word wk in a con-\ntextWk−1 we need a way of combining an arbitrary number of skip-gram features\nfk−1, which do not fall into a simple hierarchy like regularn-gram features. The\nfollowing section describes a simple, yet novel approach for combining such pre-\ndictors in a way that is computationally easy, scales up gracefully to large amounts\nof data and as it turns out is also very effective from a modeling point of view.\n3 Sparse Non-negative Matrix Modeling\n3.1 Model deﬁnition\nIn the Sparse Non-negative Matrix (SNM) paradigm, we represent the training\ndata as a sequence of eventsE = e1, e 2, ... where each evente ∈ E consists of a\nsparse non-negative feature vectorf and a sparse non-negative target word vectort.\nBoth vectors are binary-valued, indicating the presence orabsence of a feature or\ntarget words, respectively. Hence, the training data consists of|E||P os(f)|positive\nand |E||P os(f)|(|V| − 1) negative training examples, whereP os(f) denotes the\nnumber of positive elements in the vectorf.\nA language model is represented by a non-negative matrixM that, when ap-\nplied to a given feature vectorf, produces a dense prediction vectory:\ny = Mf ≈ t (4)\nUpon evaluation, we normalizey such that we end up with a conditional probabil-\nity distributionPM(t|f) for a modelM. For each wordw ∈ V that corresponds\nto indexj in t, and its feature vectorf that is deﬁned by the equivalence class\nΦ applied to the historyh(w) of that word in a text, the conditional probability\nPM(w|Φ( h(w))) then becomes:\nPM(w|Φ( h(w))) = PM(tj |f) = yj\n∑ |V|\nu=1 yu\n=\n∑\ni∈P os(f) Mij\n∑\ni∈P os(f)\n∑ |V|\nu=1 Miu\n(5)\n4\nFor convenience, we will writeP (tj |f) instead ofPM(tj|f) in the rest of the paper.\nAs required by the denominator in Eq. (5), this computation involves sum-\nming over all of the present features for the entire vocabulary. However, if we\nprecompute the row sums∑ |V|\nu=1 Miu and store them together with the model, the\nevaluation can be done very efﬁciently in only|P os(f)|time. Moreover, only the\npositive entries inMi need to be considered, making the range of the sum sparse.\n3.2 Adjustment function and metafeatures\nWe let the entries ofM be a slightly modiﬁed version of the relative frequencies:\nMij = eA(i,j ) Cij\nCi∗\n(6)\nwhere C is a feature-target count matrix, computed over the entire training corpus\nand A(i, j ) is a real-valued function, dubbedadjustment function. For each feature-\ntarget pair(fi, t j ), the adjustment function extractsk new featuresα k, called\nmetafeatures, which are hashed as keys to store corresponding weightsθ(hash(α k))\nin a huge hash table. To limit memory usage, we use a ﬂat hash table and allow\ncollisions, although this has the potentially undesirableeffect of tying together\nthe weights of different metafeatures. Computing the adjustment function for any\n(fi, t j ) then amounts to summing the weights that correspond to its metafeatures:\nA(i, j ) =\n∑\nk\nθ(hash[α k(i, j )]) (7)\nFrom the given input features, such as regularn-grams and skipn-grams, we con-\nstruct our metafeatures as conjunctions of any or all of the following elementary\nmetafeatures:\n• feature identity, e.g.[brown skip-2 over the lazy]\n• feature type, e.g.(1, 2, 3) skip-grams\n• feature countCi∗\n• target identity, e.g.dog\n• feature-target countCij\nwhere we reused the example from Section 2. Note that the seemingly absent\nfeature-target identity is represented by the conjunctionof the feature identity\nand the target identity. Since the metafeatures may involvethe feature count and\n5\nfeature-target count, in the rest of the paper we will writeα k(i, j, C i∗, C ij ). This\nwill become important later when we discuss leave-one-out training.\nEach elementary metafeature is joined with the others to form more complex\nmetafeatures which in turn are joined with all the other elementary and com-\nplex metafeatures, ultimately ending up with all25 − 1 possible combinations of\nmetafeatures.\nBefore they are joined, count metafeatures are bucketed together according to\ntheir (ﬂoored)log2 value. As this effectively puts the lowest count values, of which\nthere are many, into a different bucket, we optionally introduce a second (ceiled)\nbucket to assure smoother transitions. Both buckets are then weighted according\nto thelog2 fraction lost by the corresponding rounding operation. Note that if we\napply double bucketing to both the feature and feature-target count, the amount of\nmetafeatures per input feature becomes27 − 1.\nWe will come back to these metafeatures in Section 4.4 where we examine their\nindividual effect on the model.\n3.3 Loss function\nEstimating a modelM corresponds to ﬁnding optimal weightsθk for all the metafea-\ntures for all events in such a way that the average loss over all events between the\ntarget vectort and the prediction vectory is minimized, according to some loss\nfunctionL. The most natural choice of loss function is one that is basedon the\nmultinomial distribution. That is, we considert to be multinomially distributed\nwith|V|possible outcomes. The loss functionLmulti then is:\nLmulti(y, t) = − log(Pmulti(t|f)) = − log( yj\n∑ |V|\nu=1 yu\n) = log(\n|V|∑\nu=1\nyu) − log(yj )\n(8)\nAnother possibility is the loss function based on the Poisson distribution2 : we\nconsider eachtj int to be Poisson distributed with parameteryj. The conditional\nprobability ofPP oisson(t|f) then is:\nPP oisson(t|f) =\n∏\nj∈t\nytj\nj e−yj\ntj! (9)\n2Although we do not use it at this point, the Poisson loss also lends itself nicely for multiple target\nprediction which might be useful in e.g. subword modeling.\n6\nand the corresponding Poisson loss function is:\nLP oisson(y, t) = − log(PP oisson(t|f)) = −\n∑\nj∈t\n[tj log(yj) − yj − log(tj !)]\n=\n∑\nj∈t\nyj −\n∑\nj∈t\ntj log(yj ) (10)\nwhere we dropped the last term, sincetj is binary-valued3 . Although this choice is\nnot obvious in the context of language modeling, it is well suited to gradient-based\noptimization and, as we will see, the experimental results are in fact excellent.\n3.4 Model Estimation\nThe adjustment function is learned by applying stochastic gradient descent on the\nloss function. That is, for each feature-target pair(fi, t j) in each event we need to\nupdate the parameters of the metafeatures by calculating the gradient with respect\nto the adjustment function.\nFor the multinomial loss, this gradient is:\n∂(Lmulti(Mf, t))\n∂(A(i, j )) = ∂(log(∑ |V|\nu=1(Mf)u) − log(Mf)j )\n∂(Mij )\n∂(Mij )\n∂(Aij )\n= [ ∂(log(∑ |V|\nu=1(Mf)u))\n∂(Mij ) − ∂(log(Mf )j )\n∂(Mij ) ]Mij\n= [ ∂(∑ |V|\nu=1(Mf)u)\n∑ |V|\nu=1(Mf)u∂(Mij )\n− ∂(Mf)j\n(Mf)j ∂(Mij )]Mij\n= ( fi\n∑ |V|\nu=1(Mf)u\n− fi ·tj\nyj\n)Mij\n= fiMij( 1\n∑ |V|\nu=1 yu\n− tj\nyj\n) (11)\nThe problem with this update rule is that we need to sum over the entire vocab-\nularyV in the denominator. For most featuresfi, this is not a big deal asCiu = 0 ,\nbut some features occur with many if not all targets e.g. the empty feature for un-\nigrams. Although we might be able to get away with this by re-using these sums\nand applying them to many/all events in a mini batch, we choseto work with the\nPoisson loss in our ﬁrst implementation.\n3In fact, even in the general case wheretk can take any non-negative value, this term will disap-\npear in the gradient, as it is independent ofM.\n7\nIf we calculate the gradient of the Poisson loss, we get the following:\n∂(LP oisson(Mf, t))\n∂(A(i, j )) = ∂(∑ |V|\nu=1 (Mf)u − ∑ |V|\nu=1 tu log(Mf)u)\n∂(Mij )\n∂(Mij )\n∂(A(i, j ))\n= [ ∂(∑ |V|\nu=1 (Mf)u)\n∂(Mij ) − ∂(∑ |V|\nu=1 tu log(Mf)u)\n∂(Mij ) ]Mij\n= [ fi − tj\n(Mf)j\n∂(Mf)j\n∂(Mij ) ]Mij\n= [ fi − tjfi\n(Mf)j\n]Mij\n= fiMij (1 − tj\nyj\n) (12)\nIf we were to apply this gradient to each (positive and negative) training exam-\nple, it would be computationally too expensive, because even though the second\nterm is zero for all the negative training examples, the ﬁrstterm needs to be com-\nputed for all|E||P os(f)||V|training examples.\nHowever, since the ﬁrst term does not depend onyj , we are able to distribute\nthe updates for the negative examples over the positive onesby adding in gradients\nfor a fraction of the events wherefi = 1 , buttj = 0 . In particular, instead of\nadding the termfiMij , we addfitj Ci∗\nCij\nMij:\nCi∗\nCij\nMij\n∑\ne=(fi,t j )∈E\nfitj = Ci∗\nCij\nMijCij = Mij\n∑\ne=(fi,t j )∈E\nfi (13)\nwhich lets us update the gradient only on positive examples.We note that this\nupdate is only strictly correct for batch training, and not for online training since\nMij changes after each update. Nonetheless, we found this to yield good results as\nwell as seriously reducing the computational cost. The online gradient applied to\neach training example then becomes:\n∂(LP oisson(Mf, t))\n∂(A(i, j )) = fitjMij (Ci∗\nCij\n− 1\nyj\n) (14)\nwhich is non-zero only for positive training examples, hence speeding up compu-\ntation by a factor of|V|.\nThese aggregated gradients however do not allow us to use additional data to\ntrain the adjustment function, since they tie the update computation to the relative\nfrequenciesCi∗\nCij\n. Instead, we have to resort to leave-one-out training to prevent\n8\nthe model from overﬁtting the training data. We do this by excluding the event,\ngenerating the gradients, from the counts used to compute those gradients. So, for\neach positive example(fi, t j ) of each evente = ( f, t), we compute the gradient,\nexcludingfi from Ci∗ and fitj from Cij . For the gradients of the negative exam-\nples on the other hand we only excludefi from Ci∗ and we leaveCij untouched,\nsince here we did not observetj . In order to keep the aggregate computation of\nthe gradients for the negative examples, we distribute themuniformly over all the\npositive examples with the same feature; each of theCij positive examples will\nthen compute the gradient ofCi∗ −Cij\nCij\nnegative examples.\nTo summarize, when we do leave-one-out training we apply thefollowing gra-\ndient update rule on all positive training examples:\n∂(LP oisson(Mf, t))\n∂(A(i, j )) = fitj\nCi∗ − Cij\nCij\nCij\nCi∗ − 1e\n∑\nk θ(hash[α k(i,j,C i∗ −1,C ij )])\n+ fitj\nCij − 1\nCi∗ − 1\ny′\nj − 1\ny′\nj\ne\n∑\nk θ(hash[α k(i,j,C i∗ −1,C ij −1)]) (15)\nwhere y′\nj is the product of leaving one out for all the relevant features i.e.y′\nj =\n(M′f)j and M′\nij = e\n∑\nk θ(hash[α k(i,j,C i∗ −1,C ij −1)]) Cij −1\nCi∗ −1 .\n4 Experiments\n4.1 Corpus: One Billion Benchmark\nOur experimental setup used the One Billion Word Benchmark corpus4 made avail-\nable by [Chelba et al., 2013].\nFor completeness, here is a short description of the corpus,containing only\nmonolingual English data:\n• Total number of training tokens is about 0.8 billion\n• The vocabulary provided consists of 793471 words includingsentence bound-\nary markers<S>,<\\S>, and was constructed by discarding all words with\ncount below 3\n• Words outside of the vocabulary were mapped to<UNK>token, also part of\nthe vocabulary\n• Sentence order was randomized\n4http://www.statmt.org/lm-benchmark\n9\nModel 5 6 7 8\nKN 67.6 64.3 63.2 62.9\nKatz 79.9 80.5 82.2 83.5\nSNM 70.8 67.0 65.4 64.8\nKN+SNM 66.5 63.0 61.7 61.4\nTable 1: Perplexity results for Kneser-Ney, Katz and SNM, aswell as for the linear\ninterpolation of Kneser-Ney and SNM. Optimal interpolation weights are always\naround0. 6 − 0. 7 (KN) and0. 3 − 0. 4 (SNM).\n• The test data consisted of 159658 words (without counting the sentence be-\nginning marker<S>which is never predicted by the language model)\n• The out-of-vocabulary (OoV) rate on the test set was 0.28%.\n4.2 SNM for n-gram LMs\nWhen trained using solely n-gram features, SNM comes very close to the state-\nof-the-art Kneser-Ney [Kneser and Ney, 1995] (KN) models. Table 1 shows that\nKatz [Katz, 1995] performs considerably worse than both SNMand KN which\nonly differ by about 5%. When we interpolate these two modelslinearly, the added\ngain is only about 1%, suggesting that they are approximately modeling the same\nthings. The difference between KN and SNM becomes smaller when we increase\nthe size of the context, going from 5% for 5-grams to 3% for 8-grams, which\nindicates that SNM is better suited to a large number of features.\n4.3 Sparse Non-negative Modeling for Skipn-grams\nWhen we incorporate skip-gram features, we can either builda ‘pure’ skip-gram\nSNM that contains no regularn-gram features, except for unigrams, and interpolate\nthis model with KN, or we can build a single SNM that has both the regularn-\ngram features and the skip-gram features. We compared the two approaches by\nchoosing skip-gram features that can be considered the skip-equivalent of 5-grams\ni.e. they contain at most 4 words. In particular, we used skip-gram features where\nthe remote span is limited to at most 3 words for skips of length between 1 and 3\n(r = [1 .. 3],s = [1 .. 3],r + a = [1 .. 4]) and where all skips longer than 4 are tied\nand limited by a remote span length of at most 2 words (r = [1 .. 2], s = [4 .. ∗],\nr + a = [1 .. 4]). We then built a model that uses both these features and regular\n5-grams (SNM5-skip), as well as one that only uses the skip-gram features (SNM5-\nskip (no n-grams)).\n10\nModel Num. Params PPL\nSNM5-skip (no n-grams) 61 B 69.8\nSNM5-skip 62 B 54.2\nKN5+SNM5-skip (no n-grams) 56.5\nKN5+SNM5-skip 53.6\nTable 2: Number of parameters (in billions) and perplexity results for SNM5-skip\nmodels with and without n-grams, as well as perplexity results for the interpolation\nwith KN5.\nAs it turns out and as can be seen from Table 2, it is better to incorporate\nall the features into one single SNM model than to interpolate with a KN 5-gram\nmodel (KN5). Interpolating the all-in-one SNM5-skip with KN5 yields almost no\nadditional gain.\nThe best SNM results so far (SNM10-skip) were achieved using10-grams,\ntogether with untied skip features of at most 5 words with a skip of exactly 1 word\n(s = 1 , r + a = [1 .. 5]) as well as tied skip features of at most 4 words where\nonly 1 word is remote, but up to 10 words can be skipped (r = 1 ,s = [1 .. 10],\nr + a = [1 .. 4]).\nThis mixture of rich short-distance and shallow long-distance features enables\nthe model to achieve state-of-the-art results, as can be seen in Table 3. When\nwe compare the perplexity of this model with the state-of-the art RNN results\nin [Chelba et al., 2013], the difference is only 3%. Moreover, although our model\nhas more parameters than the RNN (33 vs 20 billion), trainingtakes about a tenth\nof the time (24 hours vs 240 hours). Interestingly, when we interpolate the two\nmodels, we have an additional gain of 20%, and as far as we know, the perplexity\nof 41.3 is already the best ever reported on this database, beating the previous best\nby 6% [Chelba et al., 2013].\nFinally, when we optimize interpolation weights over all models in [Chelba et al., 2013],\nincluding SNM5-skip and SNM10-skip, the contribution of the other models as\nwell as the perplexity reduction is negligible, as can be seen in Table 3, which also\nsummarizes the perplexity results for each of the individual models.\n4.4 Ablation Experiments\nTo ﬁnd out how much, if anything at all, each metafeature contributes to the ad-\njustment function, we ran a series of ablation experiments in which we ablated one\nmetafeature at a time. When we experimented on SNM5, we found, unsurprisingly,\nthat the most important metafeature is the feature-target count. At ﬁrst glance, it\ndoes not seem to matter much whether the counts are stored in 1or 2 buckets, but\n11\nModel Num. Params PPL interpolation weights\nKN5 1.76 B 67.6 0.06 0.00\nHSME 6 B 101.3 0.00 0.00\nSBO 1.13 B 87.9 0.20 0.04\nSNM5-skip 62 B 54.2 0.10\nSNM10-skip 33 B 52.9 0.4 0.27\nRNN256 20 B 58.2 0.00 0.00\nRNN512 20 B 54.6 0.13 0.07\nRNN1024 20 B 51.3 0.6 0.61 0.53\nSNM10-skip+RNN1024 41.3\nPrevious best 43.8\nALL 41.0\nTable 3: Number of parameters (in billions) and perplexity results for each of the\nmodels in [Chelba et al., 2013], and SNM5-skip and SNM10-skip, as well as inter-\npolation results and weights.\nAblated feature PPL\nNo ablation 70.8\nFeature 71.9\nFeature type 71.4\nFeature count 70.6\nFeature count: second bucket70.3\nLink count 73.2\nLink count: second bucket 70.6\nTable 4: Metafeature ablation experiments on SNM5\nthe second bucket really starts to pay off for models with a large number of single-\nton features e.g. SNM10-skip5. This is not the case for the feature counts, where\nhaving a single bucket is always better, although in generalthe feature counts do\nnot contribute much. In any case, feature counts are deﬁnitely the least important\nfor the model. The remaining metafeatures all contribute more or less equally, all\nof which can be seen in Table 4.\n5Ideally we want to have the SNM10-skip ablation results as well, but this takes up a lot of time,\nduring which other development is hindered.\n12\n5 Related Work\nSNM estimation is closely related to alln-gram LM smoothing techniques that rely\non mixing relative frequencies at various orders. Unlike most of those, it combines\nthe predictors at various orders without relying on a hierarchical nesting of the con-\ntexts, setting it closer to the family of maximum entropy (ME) [Rosenfeld, 1994],\nor exponential models.\nWe are not the ﬁrst ones to highlight the effectiveness of skip n-grams at cap-\nturing dependencies across longer contexts, similar to RNNLMs; previous such\nresults were reported in [Singh and Klakow, 2013].\n[Chelba and Jelinek, 2000] attempts to capture long range dependencies in lan-\nguage where the skipn-grams are identiﬁed using a left-to-right syntactic parser.\nApproaches such as [Bellegarda, 2000] leverage latent semantic information, whereas\n[Tan et al., 2012] integrates both syntactic and topic-based modeling in a uniﬁed\napproach.\nThe speed-ups to ME, and RNN LM training provided by hierarchically pre-\ndicting words at the output layer [Goodman, 2001b], and subsampling [Xu et al., 2011]\nstill require updates that are linear in the vocabulary sizetimes the number of words\nin the training data, whereas the SNM updates in Eq. (15) for the much smaller ad-\njustment function eliminate the dependency on the vocabulary size. Scaling up\nRNN LM training is described in [Chelba et al., 2013] and [Williams et. al.].\nThe computational advantages of SNM over both Maximum Entropy and RNN\nLM estimation are probably its main strength, promising an approach that has the\nsame ﬂexibility in combining arbitrary features effectively and yet should scale to\nvery large amounts of data as gracefully asn-gram LMs do.\n6 Conclusions and Future Work\nWe have presented SNM, a new family of LM estimation techniques. A ﬁrst em-\npirical evaluation on the One Billion Word Benchmark [Chelba et al., 2013] shows\nthat SNMn-gram LMs perform almost as well as the well-established KN models.\nWhen using skip-gram features the models are able to match the stat-of-the-art\nRNN LMs; combining the two modeling techniques yields the best known result\non the benchmark.\nFuture work items include model pruning, exploring richer features similar\nto [Goodman, 2001a], as well as richer metafeatures in the adjustment model, mix-\ning SNM models trained on various data sources such that theyperform best on\na given development set, and estimation techniques that aremore ﬂexible in this\nrespect.\n13\nReferences\n[Bellegarda, 2000] J. Bellegarda. 2000.Exploiting Latent Semantic Information\nin Statistical Language Modeling. Proceedings of the IEEE, vol. 88, no. 8,\n1279-1296.\n[Bengio et al., 2003] Y . Bengio, R. Ducharme, and P. Vincent.2003. A neu-\nral probabilistic language model. Journal of Machine Learning Research,\n3:1137-1155.\n[Brown et al., 1992] P. F. Brown, P. V . deSouza, R. L. Mercer, V. J. Della Pietra,\nand J. C. Lai. 1992.Class-Based N-gram Models of Natural Language.\nComputational Linguistics, 18, 467-479.\n[Chelba and Jelinek, 2000] C. Chelba and F. Jelinek. 2000.Structured language\nmodeling. Computer Speech and Language, 14(4):283-332.\n[Chelba et al., 2013] Ciprian Chelba and Tomas Mikolov and Mike Schuster and\nQi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson. 2013.\nOne Billion Word Benchmark for Measuring Progress in Statistical Language\nModeling. Google Tech Report 41880.\n[Emami, 2006] A. Emami. 2006. A Neural Syntactic Language Model. Ph.D.\nthesis, Johns Hopkins University.\n[Goodman, 2001a] J. T. Goodman. 2001a.A bit of progress in language modeling,\nextended version. Technical report MSR-TR-2001-72.\n[Goodman, 2001b] J. T. Goodman. 2001b.Classes for fast maximum entropy\ntraining. In Proceedings of ICASSP.\n[Jelinek, 1997] Frederick Jelinek 1997.Statistical Methods for Speech Recogni-\ntion. MIT Press, Cambridge, MA, USA.\n[Jelinek and Mercer, 1980] Frederick Jelinek and Robert Mercer 1980.Interpo-\nlated estimation of Markov source parameters from sparse data. In Pattern\nRecognition in Practice, (381–397), Gelsema and Kanal (eds.).\n[Katz, 1995] S. Katz. 1987.Estimation of probabilities from sparse data for the\nlanguage model component of a speech recognizer. In IEEE Transactions on\nAcoustics, Speech and Signal Processing.\n[Kneser and Ney, 1995] R. Kneser and H. Ney. 1995.Improved Backing-Off For\nM-Gram Language Modeling. In Proceedings of ICASSP.\n14\n[Mikolov, 2012] T. Mikolov. 2012.Statistical Language Models based on Neural\nNetworks. Ph.D. thesis, Brno University of Technology.\n[Morin and Bengio, 2005] F. Morin and Y . Bengio. 2005.Hierarchical Proba-\nbilistic Neural Network Language Model. In Proceedings of AISTATS.\n[Rosenfeld, 1994] R. Rosenfeld. 1994.Adaptive Statistical Language Modeling:\nA Maximum Entropy Approach. Ph.D. thesis, Carnegie Mellon University.\n[Schwenk, 2007] H. Schwenk. 2007.Continuous space language models. Com-\nputer Speech and Language, vol. 21.\n[Singh and Klakow, 2013] M. Singh, D. Klakow. 2013.Comparing RNNs and\nlog-linear interpolation of improved skip-model on four Babel languages:\nCantonese, Pashto, Tagalog, Turkish. In Proceedings of ICASSP.\n[Sundermeyer et al., 2012] M. Sundermeyer, R. Schluter, andH. Ney. 2012.LSTM\nNeural Networks for Language Modeling. In Proceedings of Interspeech.\n[Tan et al., 2012] M. Tan, W. Zhou, L. Zheng and S. Wang. 2012.A Scalable Dis-\ntributed Syntactic, Semantic, and Lexical Language Model. In Computational\nLinguistics, vol. 38, no. 3, 631-671.\n[Teh, 2006] Y . W. Teh. 2006.A hierarchical Bayesian language model based on\nPitmanYor processes. In Proceedings of Coling/ACL.\n[Williams et. al.] W. Williams, N. Prasad, D. Mrva, T. Ash andT. Robinson.\n2015. Scaling Recurrent Neural Network Language Models. In CoRR,\nabs/1502.00512.\n[Xu et al., 2011] Puyang Xu, A. Gunawardana, and S. Khudanpur. 2011. Efﬁ-\ncient Subsampling for Training Complex Language Models. In Proceedings\nof EMNLP.\n15",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.6580001711845398
    },
    {
      "name": "Computer science",
      "score": 0.6482226252555847
    },
    {
      "name": "n-gram",
      "score": 0.6154675483703613
    },
    {
      "name": "Language model",
      "score": 0.5825533866882324
    },
    {
      "name": "Gram",
      "score": 0.5355736017227173
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4839182496070862
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.45884570479393005
    },
    {
      "name": "Algorithm",
      "score": 0.4511220157146454
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4312431216239929
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4240129590034485
    },
    {
      "name": "Gramian matrix",
      "score": 0.4160161018371582
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.41185128688812256
    },
    {
      "name": "Artificial neural network",
      "score": 0.3309191167354584
    },
    {
      "name": "Mathematics",
      "score": 0.2354419231414795
    },
    {
      "name": "Statistics",
      "score": 0.16530922055244446
    },
    {
      "name": "Eigenvalues and eigenvectors",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Bacteria",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}