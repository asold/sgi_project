{
  "title": "A Review on the Use of Large Language Models as Virtual Tutors",
  "url": "https://openalex.org/W4397034170",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "García Méndez, Silvia",
      "affiliations": [
        "Universidade de Vigo"
      ]
    },
    {
      "id": null,
      "name": "de Arriba Perez, Francisco",
      "affiliations": [
        "Universidade de Vigo"
      ]
    },
    {
      "id": null,
      "name": "Somoza López, María del Carmen",
      "affiliations": [
        "Universidade de Vigo"
      ]
    },
    {
      "id": null,
      "name": "García Méndez, Silvia",
      "affiliations": []
    },
    {
      "id": null,
      "name": "de Arriba Perez, Francisco",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Somoza López, María del Carmen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4286282840",
    "https://openalex.org/W4313564992",
    "https://openalex.org/W4210363535",
    "https://openalex.org/W4389437528",
    "https://openalex.org/W4285778680",
    "https://openalex.org/W4378228477",
    "https://openalex.org/W2810503719",
    "https://openalex.org/W6885101463",
    "https://openalex.org/W6803096969",
    "https://openalex.org/W4360980141",
    "https://openalex.org/W4292575147",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W7061537676",
    "https://openalex.org/W4308947461",
    "https://openalex.org/W4321153003",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W4323256981",
    "https://openalex.org/W4321786597",
    "https://openalex.org/W2907838557",
    "https://openalex.org/W4226255929",
    "https://openalex.org/W2797544798",
    "https://openalex.org/W4304140101",
    "https://openalex.org/W4327936519",
    "https://openalex.org/W2809678125",
    "https://openalex.org/W3174122384",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W3008941975",
    "https://openalex.org/W3147517805",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W3213918006",
    "https://openalex.org/W2991356836",
    "https://openalex.org/W4380564189",
    "https://openalex.org/W4323526518",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W4385573935",
    "https://openalex.org/W4287878175",
    "https://openalex.org/W4385223296",
    "https://openalex.org/W2025595753",
    "https://openalex.org/W2133436118",
    "https://openalex.org/W4294768175",
    "https://openalex.org/W4324139371",
    "https://openalex.org/W4312083290",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4313678819",
    "https://openalex.org/W4321605350",
    "https://openalex.org/W3126934640",
    "https://openalex.org/W4285778599",
    "https://openalex.org/W4297412105",
    "https://openalex.org/W4361004927",
    "https://openalex.org/W2337155942",
    "https://openalex.org/W4317910584",
    "https://openalex.org/W4392151399",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4225716729",
    "https://openalex.org/W4212964203",
    "https://openalex.org/W4323767542",
    "https://openalex.org/W3204666440",
    "https://openalex.org/W2623703952",
    "https://openalex.org/W3165860937",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2889032597",
    "https://openalex.org/W4313470113",
    "https://openalex.org/W4287887667",
    "https://openalex.org/W4380551750",
    "https://openalex.org/W4366967540",
    "https://openalex.org/W3209080096",
    "https://openalex.org/W3020988228",
    "https://openalex.org/W4322753490",
    "https://openalex.org/W4382318920"
  ],
  "abstract": "Abstract Transformer architectures contribute to managing long-term dependencies for natural language processing, representing one of the most recent changes in the field. These architectures are the basis of the innovative, cutting-edge large language models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out. Accordingly, these generative artificial intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning. Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan. To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs. As expected, the most common role of these systems is as virtual tutors for automatic question generation. Moreover, the most popular models are GPT-3 and BERT. However, due to the continuous launch of new generative models, new works are expected to be published shortly.",
  "full_text": "Science & Education (2025) 34:877–892\nhttps://doi.org/10.1007/s11191-024-00530-2\nSI: EPISTEMIC INSIGHT & ARTIFICIAL INTELLIGENCE\nA Review on the Use of Large Language Models as Virtual\nTutors\nSilvia García-Méndez 1 · Francisco de Arriba-Pérez 1 ·\nMaría del Carmen Somoza-López 2\nAccepted: 26 April 2024 / Published online: 18 May 2024\n© The Author(s) 2024\nAbstract\nTransformer architectures contribute to managing long-term dependencies for natural lan-\nguage processing, representing one of the most recent changes in the ﬁeld. These architectures\nare the basis of the innovative, cutting-edge large language models (LLMs) that have pro-\nduced a huge buzz in several ﬁelds and industrial sectors, among the ones education stands\nout. Accordingly, these generative artiﬁcial intelligence-based solutions have directed the\nchange in techniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of LLMs, this\nreview seeks to provide a comprehensive overview of those solutions designed speciﬁcally\nto generate and evaluate educational materials and which involve students and teachers in\ntheir design or experimental plan. To the best of our knowledge, this is the ﬁrst review of\neducational applications (e.g., student assessment) of LLMs. As expected, the most common\nrole of these systems is as virtual tutors for automatic question generation. Moreover, the\nmost popular models are GPT-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.\n1 Introduction\nArtiﬁcial intelligence (AI) refers to the synthetic capabilities of computer science applications\nto perform tasks that usually require human intelligence (e.g., adaptation, learning, reasoning)\n(Sarker, 2022; Cooper, 2023). The recent technological advancements within the AI ﬁeld have\nled to relevant changes in business and research, the economy, and society (i.e., mega-trends)\nthat are predicted to continue (Estigarribia et al., 2022; Haluza & Jungwirth, 2023;R a s a\net al., 2023).\nB Silvia García-Méndez\nsgarcia@gti.uvigo.es\nFrancisco de Arriba-Pérez\nfarriba@gti.uvigo.es\nMaría del Carmen Somoza-López\ncarmensomoza@uvigo.gal\n1 Information Technologies Group, atlanTTic, University of Vigo, Vigo, Spain\n2 Applied Mathematics I Department, University of Vigo, Vigo, Spain\n123\n878 S. García-Méndez et al.\nThe most relevant change that perfectly exempliﬁes the impact of the mega-trends above is\nthe transformer architectures that contribute to managing long-term dependencies for natural\nlanguage processing (NLP) (Tay et al., 2023). They are the basis of the innovative, cutting-\nedge large language models (LLMs) that have produced a huge buzz in several ﬁelds and\nindustrial sectors (MacNeil et al., 2022a). In this line, ChatGPT achieved more than 1 million\nusers within the ﬁrst 5 days of its release.\n1 Accordingly, LLMs have been used in economy and\nﬁnance (Alshater, 2022), journalism (Pavlik, 2023), medicine (O’Connor & ChatGPT, 2023),\nand education (Sallam, 2023), among others. However, as other technological advancements,\nLLMs have experienced the community’s resistance, a common evolutionary and social\npsychology phenomenon (Tobore, 2019).\nRegarding the learning ﬁeld, during the twenty-ﬁrst century, education has experienced\na profound change in methods and content. Speciﬁcally, a ﬂexible and multidisciplinary\nenvironment is sought, where the student can actively participate in their learning process,\npromoting more autonomous and ubiquitous studying thanks to AI advancements (Baidoo-\nAnu & Ansah, 2023;L ie ta l . , 2023). The scientiﬁc community has researched the use of AI\ntechniques like machine learning (ML) models for training purposes ever since their incep-\ntion (Hochberg et al., 2018; Talan, 2021; Huang & Qiao, 2022), causing progressive advances\ntowards autonomous high-quality learning (Han, 2018; Demircioglu et al., 2022). Mainly, AI\nhas driven the technological transition in this ﬁeld regarding the instructional applications,\ncontents, platforms, resources, techniques, tools, and network infrastructure (Roll & Wylie,\n2016). This transition also involves changes in the leading roles of the education systems,\nteachers, and students since this new digital education environment requires new digital com-\npetencies and reasoning patterns (Jensen et al., 2018; Zhou et al., 2023). However, although\npromising, the advances offered by AI are still far from becoming a standardized tool in the\neducational ﬁeld due to its early state and the need for training in using these solutions to\ntake the most advantage of them.\nOf particular interest is the impact of those applications that leverage LLMs, framed\nwithin the generative AI ﬁeld and based on ML techniques. They enable hands-on learning\nand are common practice in the classroom nowadays. Compared to previous AI solutions\nand traditional methodologies, which focused primarily on modifying the textual input using\ncorrection, paraphrasing, and sentence completion techniques, LLMs generate on-the-ﬂy\nhuman-like utterances, hence its popularity, especially among students and teachers (Rudolph\net al., 2023). Current advanced LLMs can enhance pedagogical practice and provide person-\nalized assessment and tutoring (Sok & Heng, 2023). Consideration should be given to the\ncooperation between LLMs-based systems and humans, provided the experience and scien-\ntiﬁc knowledge along with the capabilities of the human-agents for creativity and emotional\nintelligence (Zhang et al., 2020; Korteling et al., 2021). Note that these AI-based systems\npresent advantages in speciﬁc educational tasks as self-learning tools and virtual tutors.\nSpeciﬁcally, they enable automatic answer grading (Ahmed et al., 2022), explanation gen-\neration (Humphry & Fuller, 2023), question generation (Bhat et al., 2022), and problem\nresolution (Zong & Krishnamachari, 2022). Furthermore, when used for text summarization\n(Phillips et al., 2022), they help synthesize content and improve the student’s abstraction\ncapabilities. Their use as learning software in virtual assistants is highly relevant to ﬂexible\nlearning (Wang et al., 2022; Yamaoka et al., 2022). Furthermore, their language intelligence\ncapabilities make them an appropriate tool for code correction (MacNeil et al., 2022b).\n1 Available at https://twitter.com/gdb/status/1599683104142430208, April 2024.\n123\nA Review on the Use of Large Language Models... 879\nMore in detail, LLMs are trained with massive textual data sets to create human-like utter-\nances. They perform a wide variety of NLP taking advantage of ﬁne-tuning and pre-training\npipelines (Kasneci et al., 2023). Note the relevance of both the pre-training and prompt engi-\nneering development. The ﬁrst concept refers to training LLMs with miscellaneous large\ndata sets, while the second refers to speciﬁc ﬁne-tuning on a particular task (Kasneci et al.,\n2023). Consequently, the quality of the LLMs output highly depends on the input data and\nprompt designed, aka prompt engineering (Cooper, 2023). The latter technique ranges from\nzero-shot learning, widely popular when applied to LLMs (Russe et al., 2024), to few-shot\nlearning. Note that the model follows task instructions in zero-shot learning since the end-\nuser provides no examples. In contrast, in few-shot learning, the model learns from the\ndemonstrations available (i.e., few-shot text prompts).\nAmong the most popular LLMs, BERT (Devlin et al., 2019), GPT-3 (Brown et al.,\n2020), GPT-3.5\n2,G P T - 43, and T5 (Raffel et al., 2020) deserve attention. BERT (Bidirec-\ntional Encoder Representations from Transformers) was released by Google in October 2018\n(slightly after GTP-1 dated June 2018). It is a pre-trained transformer-based encoder model\nthat can be ﬁne-tuned on speciﬁc NLP tasks such as named entity recognition (NER), question\nanswering, and sentence classiﬁcation. Moreover, GPT-3, GPT-3.5, and GPT-4 (generative\npre-trained transformer) models were released by OpenAI in 2020, 2022, and 2023, respec-\ntively. More in detail, GPT-4 is already deployed in ChatGPT application, which compared\nto other LLMs can generate context-aligned responses and interact naturally with the end-\nusers as a peer. This model goes beyond producing reports and translating assessments by\ncreating source code (Haleem et al., 2022) and responding to complex questions posed by\nthe students in real time (George et al., 2023). It can also show creativity to some extent\nin writing (Baidoo-Anu & Ansah, 2023). T5 (Text-to-text Transfer Transformer) model was\nreleased by Google following the encoder-decoder transformer architecture in 2020. Even\nthough its conﬁguration is similar to BERT, it differs in some steps of the pipeline, like\npre-normalization (Pipalia et al., 2020).\nGiven the widespread of AI-based solutions in our everyday lives and particularly the\npopularity of advanced NLP-based chatbots for learning purposes to generate and evaluate\neducational materials, this review seeks to provide a comprehensive overview of the sys-\ntems that exploit LLMs and were explicitly designed for educational purposes (i.e., virtual\ntutors for question generation and assessment). Thus, involving students or teachers at the\ndesign or evaluation levels, excluding those works in which the application of the solution for\neducational use cases was feasible but not initially designed for that purpose. The ultimate\nobjective is to promote the advancement of these existing solutions in a collaborative envi-\nronment between academia (i.e., researchers and developers) and end-users (i.e., students and\nteachers). To the best of our knowledge, this is the ﬁrst review in this regard. Note that there\nexist few review works that focused on speciﬁc related ﬁelds such as health care education\n(Sallam, 2023) or speciﬁc features like the responsible and ethical use of LLMs (Mhlanga,\n2023) and their impact on academic integrity (Perkins, 2023).\nThe rest of this paper is organized as follows. Section 2 describes the methods and materials\nused in the review. Section 3 presents the discussion on the selected relevant works. Finally,\nSection 4 concludes the article and details future research.\n2 Available at https://platform.openai.com/docs/models/gpt-3-5 ,a sr e f e r r e dt ot h e text-davinci-003\nand GPT-3.5-turbo models, April 2024.\n3 Available at https://platform.openai.com/docs/models/gpt-4 , April 2024.\n123\n880 S. García-Méndez et al.\nFig. 1 Review pipeline\n2 Methodology\nThe review methodology followed is composed of two steps: ( i) data gathering (Section 2.1)\nand ( ii) screening and eligibility criteria (Section 2.2). Figure 1 details the methods and\nmaterial used. More in detail, this review aims to gather knowledge to answer the following\nresearch questions:\n RQ1: Which solutions based on LLMs are being developed (e.g., for assessment tasks)\n(i.e., excluding multidisciplinary solutions that were not speciﬁcally intended for learning\nassistance)?\n RQ2: Which educational solutions based on LLMs involved students or teachers at any\nlevel of the development process (e.g., design, evaluation)?\n2.1 Data Gathering\nThe data were extracted using Google Scholar 4 with two search queries, specially designed\nto gather works within the educational ﬁeld that leverage LLMs:\n1. \"education\" AND \"student\" AND (\"large language model\" OR\n\"GPT-3\" OR \"GPT-3.5\" OR \"GPT-4\" OR \"ChatGPT\") -\"review\"\n2. \"education\" OR \"student\" AND (\"large language model\" OR\n\"GPT-3\" OR \"GPT-3.5\" OR \"GPT-4\" OR \"ChatGPT\") -\"review\"\nBoth queries have been restricted temporally since 2020, and the second query was applied\nto the title content exclusively. Note that duplicated elements and works that do not use LLMs\nor do not indicate which model is exploited were not considered. The same applies to the\nworks that assess the performance of LLMs. In the end, 342 records were identiﬁed.\n2.2 Screening and Eligibility Criteria\nThis process was designed to identify works within the ﬁeld of study that were written in\nEnglish while at the same time discarding theoretical and review contributions (i.e., those\nthat do not propose an LLM-based solution but review existing solutions or hypothesize on\nthe impact of LLMs for educational purposes). The manual screening based on the above\neligibility criteria resulted in 29 records. Note that this process distinguishes between pub-\nlished articles and conferences from pre-printed and non-peer-reviewed records. The criteria\nfor selection and exclusion are presented in Table 1.\n4 Available at https://scholar.google.com, April 2024.\n123\nA Review on the Use of Large Language Models... 881\nTable 1 Criteria for selection and\nexclusion Criteria Description\nPublication date Before 2020\nAfter 2020\nWriting language Not in English\nEnglish\nScope General\nEducational purpose\nType of contribution Theoretical or review\nVirtual tutor proposal\nLanguage intelligence Not involving LLMs\nInvolving LLMs\nFigures 2 and 3 detail the distribution of the LLMs used and applications in the works\nselected. Firstly, the most popular model is BERT, followed by GPT-3, T5, and GPT-3.5.\nThe low representativeness of the last GPT model contrasts with its popularity. The latter is\ndue to the fact that the data gathering corresponds to the ﬁrst quarter of 2023, that is, shortly\nafter it was released. Thus, new works exploiting it are expected to be published shortly.\nFurthermore, the most common tasks these models perform in the selected works are as\nvirtual assistants and question generation, as shown in Fig. 3, followed by answer grading\nand code explanation/correction. Note that most works were published in 2022, with few\nrecords in 2021, showing a growth trend in 2023.\nFig. 2 Distribution of the LLMs in the records selected\n123\n882 S. García-Méndez et al.\nFig. 3 Distribution of the tasks in the records selected (AG answer grading, CE code explanation, EG explana-\ntion generation, LS learning software, PR problem resolution, QG question generation, TS text summarization,\nV A virtual assistant)\n3 Analysis and Discussion\nTable 2 lists the articles published in journals and the proceedings of conferences, taking\ninto account their application, the model used, and code and data availability. Note that just\nthe works by Liu et al. ( 2022); Mendoza et al. ( 2022); Tyen et al. ( 2022); Zong & Krishna-\nmachari (2022); Humphry & Fuller ( 2023); Nasution ( 2023) provide enough information for\nreproducibility, while Bhat et al. ( 2022) ;E s s e le ta l .( 2022); Mendoza et al. ( 2022); Moore\net al. ( 2022); Phillips et al. ( 2022); Yamaoka et al. ( 2022); Nasution ( 2023) involved either\nteachers or students in the design or experimental plan.\nRegarding answer grading applications, Ahmed et al. ( 2022) used the BERT model. They\nexploited a modiﬁed version of the model based on triplets and the Siamese network, specially\ndesigned to generate sentences through semantically meaningful embeddings. The data set\nused is the one presented by Mohler & Mihalcea ( 2009). The authors applied the question\ndemoting technique as part of the preprocessing, thus removing from the answer those words\nalso contained in the question. The authors performed the experiments with two different\ncombinations of input data: ( i) the reference and student answers and ( ii) the concatenation\nof the question and the reference answer, plus the answer provided by the student. Evaluation\nmetrics include Pearson’s correlation coefﬁcient (PCC) and root mean square error (RMSE).\nThe results are approximately 0.8 PCC and 0.7 RMSE. Moore et al. ( 2022) presented another\nanswer grading solution based on GPT-3. Unlike Ahmed et al. ( 2022), the input data were\ngathered from an introductory chemistry course at the university level with almost 150 stu-\ndents. Moreover, the GPT-3 model was trained with the LearningQ data set (Chen et al.,\n2018), as in Bhat et al. ( 2022). Based on the assessment of the questions posed to experts in\nthe chemistry ﬁeld, the model was able to correctly evaluate 32% of the questions.\nFew works exist on code explanation and general explanation generation, learning soft-\nware, and problem resolution. Firstly, MacNeil et al. ( 2022b) proposed a GPT-3-based\n123\nA Review on the Use of Large Language Models... 883\nTable 2 Selected articles published in journals or presented at conferences\nApplication Authorship Model Code/data\navailability\nAnswer grading Ahmed et al. ( 2022)B E R T N o\nMoore et al. ( 2022) GPT-3 No\nCode explanation MacNeil et al. ( 2022b) GPT-3 No\nExplanation generation Humphry & Fuller ( 2023) GPT-3.5 Yes\nLearning software Yamaoka et al. ( 2022) GPT-3 No\nProblem resolution Zong & Krishnamachari ( 2022) GPT-3 Yes\nQuestion generation Bhat et al. ( 2022) GPT-3 & T5 No\nDijkstra et al. ( 2022) GPT-3 No\nSharma et al. ( 2022)T 5 N o\nNasution ( 2023) GPT-3.5 Yes\nText summarization Phillips et al. ( 2022) GPT-3 No\nPrihar et al. ( 2022) BERT, SBERT, and\nMathBERT\nNo\nVirtual assistant Sophia & Jacob ( 2021) Dialogﬂow No\nBaha et al. ( 2022) CamemBERT No\nCalabrese et al. ( 2022)D e B E R T a N o\nEssel et al. ( 2022) FlowXO No\nLiu et al. ( 2022)R o B E R T a Y e s\nDistillBERT Yes\nMahajan ( 2022)R o B E R T a N o\nMendoza et al. ( 2022) Dialogﬂow Yes\nTopsakal & Topsakal ( 2022) GPT-3.5 No\nTyen et al. ( 2022)R o B E R T a Y e s\nWang et al. ( 2022)B E R T , A l B E R T , a n d\nDistilBERT\nNo\nsolution for code explanation based on 700 prompts. Note that it does not identify or correct\nerrors. The main functionalities of the system encompass ( i) execution tracing, ( ii) identi-\nfying and explaining common bugs, and ( iii) output prediction. However, no results were\nprovided. Humphry & Fuller ( 2023) proposed a solution based on GPT-3.5 to write con-\nclusion statements about chemistry laboratory experiments. The evaluation of the solution\nrelied on a discussion of features like readability and orthographic correctness of the gen-\nerated text. Unlike the works above, which focused on textual input data, Yamaoka et al.\n(2022) used the GPT-3 model to exploit social media data, particularly from Instagram, for\nlearning purposes. The proposed pipeline comprises ( i) detecting the relevant objects in the\nimages, ( ii) extracting keywords to generate sentences related to those keywords, and ( iii)\nproviding linguist information about the words that composed the sentence. The ultimate\nobjective was to acquire new vocabulary. The experiments consisted of a small pilot study\nwith three students from Osaka Metropolitan University. The only results reported were the\naverage of unknown words, 2.2 in the generated sentences. Finally, Zong & Krishnamachari\n(2022) used GPT-3 to identify and generate math problems involving systems of two lin-\near equations. The experiments consisted of ( i) problem classiﬁcation into ﬁve categories,\n(ii) equation extraction from word problems, and ( iii) generation of similar exercises. The\n123\n884 S. García-Méndez et al.\nauthors prepared the input data ad hoc . The accuracy of the results obtained in each of the\nthree tasks above was 75% (averaging the ﬁve categories); 80% (with ﬁne-tuning), and 60%\n(also averaging the ﬁve categories), respectively.\nRegarding question generation, several representative examples were found in the liter-\nature. Bhat et al. ( 2022) used both GPT-3 and T5 models, GPT-3 for question generation\ncombined with a concept hierarchy extraction model, and T5 for the evaluation in terms of\nlearning usefulness of the generated questions. The input data consisted of textual learn-\ning materials from a university data science course. More in detail, the concept hierarchy\nextraction method exploited the MOOCCubeX pipeline (Y u et al., 2021), which extracts key\nconcepts following a semi-supervised approach. Note that evaluation also involved computing\nthe information score metric and manual assessment by human annotators. The experimental\nresults obtained with the LearningQ data set (Chen et al., 2018) show that almost 75% of the\ngenerated questions were considered useful by the GPT-3 model, with an agreement slightly\nhigher than 65% when compared to manual evaluation. Similarly, Dijkstra et al. ( 2022) created\nEduQuiz with GPT-3, a multi-choice quiz generator for reading comprehension exploiting the\nEQG-RACE data set\n5 (Jia et al., 2021). The authors evaluated the performance of EduQuiz\nusing standard metrics, BLEU-4, ROUGE-l, and METEOR. Results attained 36.11, 11.61,\nand 25.42 for these metrics, respectively. Additionally, Sharma et al. ( 2022) proposed a\nﬁne-tuning pipeline composed of context recognition and paraphrasing, ﬁltering irrelevant\noutput, and translation to other languages for question generation at different levels using the\nT5 model. The authors used the data set by Mohler et al. ( 2011) (an updated version of the\ndata set used in Ahmed et al. ( 2022)). The evaluation metrics computed were BLUE (Papineni\net al., 2002) and METEOR (Lavie & Agarwal, 2007). The results for the two metrics above\nwere 0.52 and 57.66, respectively. Thus, compared with the question generation solution by\nDijkstra et al. ( 2022), Sharma et al. ( 2022) obtained a more competitive METEOR value.\nUltimately, Nasution ( 2023) used GPT-3.5 for question generation. To assess the generated\nquestions’ reliability or internal consistency, the Cronbach’s alpha coefﬁcient (Taber, 2018)\nwas computed, resulting in 0.65. Answers from a survey performed to almost 300 students\nshow that 79% of the generated questions were relevant, 72% were moderately clear, and\n71% were of enough depth.\nIn contrast, Phillips et al. ( 2022) used GPT-3 to create summaries of students’ chats\nin collaborative learning. Moreover, this solution detected confusion and frustration in the\nstudent’s utterances. Input data was gathered from secondary school students in an ecosystem\ngame. The authors brieﬂy discussed how the system could provide advantageous knowledge\nto teachers about their interaction in a collaborative learning environment, but no further\nanalysis or results were provided. Conversely, Prihar et al. ( 2022) proposed a learning assistant\nbased on the BERT model and its variations (i.e., SBERT and MathBERT) to generate support\nmessages from chat logs obtained from fundamental interactions between a live UPchieve\ntutor available at the ASSISTments learning platform and the students. Even though 75%\nof the generated messages were identiﬁed as relevant by manual human evaluation, these\nmessages had a negative impact on the student’s learning process, as the authors explained.\nThe most common application uses LLMs as virtual assistants. Sophia & Jacob ( 2021)\ncreated EDUBOT exploiting Dialogﬂow. Its main limitation lies in the basic language under-\nstanding capabilities (i.e., low variability in the responses provided), particularly regarding the\nuser’s emotions. Baha et al. ( 2022) developed Edu-Chatbot exploiting the Xatkit framework.\nThe system comprises an encoder based on CamemBERT and a decoding module for student\nintent recognition. Unfortunately, the intent classiﬁcation decoder is based on a pre-deﬁned\n5 Available at https://github.com/jemmryx/EQG-RACE , April 2024.\n123\nA Review on the Use of Large Language Models... 885\nset of recognized actions (e.g., simple questions, animations, videos, and quizzes). Thus, the\nlanguage intelligence of the solution is limited. Furthermore, no evaluation was performed.\nCalabrese et al. ( 2022) presented a virtual assistant prototype for Massive Online Open\nCourses (MOOCs). Their objective was to reduce the teaching load and maintain the quality\nof learning. Thus, its architecture allows the teacher to intervene in those questions that have\nnot been resolved satisfactorily. More in detail, they used a personalized version of BERT. The\nquestions answered by the teacher are included in an additional document and allow the BERT\nmodel to be improved. In contrast, Essel et al. ( 2022) involved 68 undergraduate students in\nevaluating the solution developed using FlowXO and integrated into WhatsApp. Qualitative\nevaluation on the end-user’s preferences of the virtual assistant instead of traditional interac-\ntion approaches with the teachers reached 58.8%. Additionally, Liu et al. ( 2022)p r e s e n t e d\na virtual assistant for online courses to resolve general and repetitive doubts about content\nand teaching materials. This system incorporates a sentiment analysis module to analyze the\nresponse’s satisfaction based on the student’s dialogue. They used two ﬁne-tuning versions\nof BERT model with an accuracy of 82% and 90% for the correct detection of the content and\nstudent’s sentiment, respectively. Moreover, Mahajan ( 2022) created a system for students\nto improve their knowledge of the English language that allows them to obtain information\non the meaning of words, make translations, and resolve pronunciation doubts. The authors\nexploited the RoBERTa model with an accuracy greater than 98% in communication intent\ndetection. Similarly, Mendoza et al. ( 2022) created a virtual assistant intended for academic\nand administrative tasks but exploiting Dialogﬂow.\n6 Cronbach’s alpha coefﬁcients during the\nevaluation of the system exceeded 0.7. In contrast, Topsakal & Topsakal ( 2022)p r e s e n t e d\na foreign language virtual assistant based on the GPT-3.5 model combined with augmented\nreality. The authors claim that this combination attracted students’ attention and motivated\nthem through entertaining learning thanks to gamiﬁcation. In this case, the language model\nwas used to establish a dialogue with the end-users. Unfortunately, no results were discussed.\nMoreover, Tyen et al. ( 2022) proposed a virtual assistant for second language learning with\ndifﬁculty level adjustment in the decoder module and evaluated by experienced teachers.\nThe system exploits RoBERTa ﬁne-tuned with a Cambridge exams data set. The system\nattained Spearman and Pearson coefﬁcients of 0.755 and 0.731, respectively. Finally, Wang\net al. ( 2022) developed an educational domain-speciﬁc chatbot. Its goal is to reduce pressure\non teachers in virtual environments and improve response times by easing communication\nbetween students and teachers. They used natural language understanding (NLU) techniques\non variations of the BERT model for the classiﬁcation of intents and response generation. It\npresented an accuracy of 88% in detecting intents. However, its values are lower than 50%\nregarding semantic analysis.\nTable 3 lists the selected pre-printed or non-peer-reviewed works, taking into account\ntheir application, model used, and reproducibility feature. In this case, da Silva et al. ( 2022),\n7\nZhang et al. ( 2022),8 and Christ ( 2023)9 provide enough information for reproducibility,\nwhile Zhang et al. ( 2022)8 involved either teachers or students in the design or experimental\nplan.\n6 Available at https://dialogﬂow.cloud.google.com, April 2024.\n7 Available at https://arc.cct.ie/cgi/viewcontent.cgi?article=1026&context=ict, April 2024.\n8 Available at https://arxiv.org/pdf/2209.14876.pdf, April 2024.\n9 Available at https://fbmn.h-da.de/ﬁleadmin/Dokumente/Studium/DS/WS2022_MDS_Thesis_Paul_Christ_\nTHE.pdf, April 2024.\n123\n886 S. García-Méndez et al.\nTable 3 Selected pre-printed or non peer-reviewed records\nApplication Authorship Model Code/data\navailability\nAnswer grading Hardy ( 2021)10 SBERT No\nCode correction Zhang et al. ( 2022)8 Codex Yes\nPhung et al. ( 2023)14 Codex No\nProblem resolution Cobbe et al. ( 2021)16 GPT-3 No\nQuestion generation da Silva et al. ( 2022)7 T5 Yes\nRaina & Gales ( 2022)17 GPT-3 and T5 No\nChrist ( 2023)9 DistilBERT Yes\nThe distribution of applications is similar to the peer-reviewed records. Hardy ( 2021)10\ndeveloped an automatic evaluation system for reading and writing exercises. The system uses\nthe SBERT model, among others, to capture semantic data and provide valuable insights\nrelated to the student’s skills, using ASAP-AES\n11 and ASAP-SAS 12 data sets. Particularly,\nthey exploited the passage-dependent sentence-BERT model trained using curricular learning\n(Graves et al., 2016). The results from the quadratic weighted kappa (QWK) metric reached\n0.76 on average.\nRegarding code correction, Zhang et al. ( 2022)\n8 presented MMAPR, an error identiﬁcation\nand correction system for code development based on the OpenAI Codex model. 13 The system\nﬁxes semantic and syntax errors by combining iterative querying, multi-modal prompts,\nprogram chunking, and test-case-based selection of a few shots. Results obtained with almost\n300 students reached 96.50% in corrected code rate with the few-shots-based approach. Phung\net al. ( 2023)\n14 presented a similar solution to MMAPR for code correction named PyFiXV .\nThe main difference is that the Codex model, combined with prompt engineering, explains\nthe detected errors. Moreover, the explanations are also validated in terms of suitability for\nthe students. The system has been tested with TigerJython (Kohn & Manaris, 2020)a n d\nCodeforces\n15 data sets. The precision attained 76% in the most favorable scenario with the\nTigerJython data set.\nCobbe et al. ( 2021)16 elaborated a data set of 8.5 K elementary school mathematical\nproblems called GSM8K. Then, the GPT-3 model was used to generate comprehensible\nexplanations of these problems, combining natural language and mathematical expressions.\nThe authors trained veriﬁers to enhance the performance of the model beyond ﬁne-tuning.\nUltimately, they concluded that this approach enhanced the overall performance.\nSubsequently, da Silva et al. ( 2022)\n7 developed an automatic questionnaire generation\nsystem using the T5 model and applying the ﬁne-tuning technique, named QUERAI. The\nT5 model was evaluated with skip thought vectors (STV), embedding average cosine simi-\nlarity (EACS), vector extrema cosine similarity (VECS), and greedy matching score (GMS)\n10 Available at https://arxiv.org/ftp/arxiv/papers/2112/2112.11973.pdf , April 2024.\n11 Available at https://www.kaggle.com/c/asap-aes , April 2024.\n12 Available at https://www.kaggle.com/c/asap-sas , April 2024.\n13 Available at https://openai.com/blog/openai-codex , April 2024.\n14 Available at https://arxiv.org/pdf/2302.04662.pdf, April 2024.\n15 Available at https://codeforces.com, April 2024.\n16 Available at https://arxiv.org/pdf/2110.14168.pdf?curius=520, April 2024.\n123\nA Review on the Use of Large Language Models... 887\nmetrics, with results higher than 0.8 except for VECS. Summing up, the accuracy of the pay-\nper-subscription solution is 91%. Similar to da Silva et al. ( 2022),7 Raina & Gales ( 2022)17\ndeveloped a multiple-choice question generation solution to generate both questions and the\nset of possible answers using apart from T5, the GPT-3 model, both trained with the RACE++\n(Liang et al., 2019) data set composed of middle, high, and college level questions. The results\nobtained are similar between the two models with an accuracy of 80% (11 percentage points\nlower than the solution by da Silva et al. ( 2022)\n7). Note that the authors also measured the\nnumber of grammatical errors and other features like diversity and complexity. The lowest\nvalues are related to the diversity of the questions generated. In the best scenario, the T5\nmodel attained 60% accuracy, approximately. More recently, Christ ( 2023)\n9 used BERT to\ngenerate SQL-Query exercises automatically. Experiments with knowledge graphs and nat-\nural language building were also performed as a baseline. The authors concluded that the\nDistilBERT-based approach generates descriptions that are, on average, almost 50% shorter\nand with a 20% decrease in term frequency compared to the NLP baseline.\nGPT-3 and the different adaptations of the BERT model are the most popular alternatives in\nthe sample regarding answer grading, code explanation and general explanation generation,\nlearning software, problem resolution, question generation, and text summarization. When\nit comes to their use as virtual assistants, the variety of models used increases. The current\nlower costs of the GPT-3.5 model will motivate a rapid increase in its use in the coming\nyears. However, BERT robustness as an entity detector, with evaluation metrics above 80%\nin several of the discussed works, made it a reference for developing educational software\ntools. Unfortunately, most works reviewed do not provide the code or data used for their\nanalysis, making reproducibility difﬁcult. Finally, regarding the risks of exploiting LLMs\nfor educational tasks, they are transversal (e.g., for automatic question generation and as\nvirtual assistants, the two most popular applications identiﬁed). The lack of transparency\nof the models (i.e., the rationale behind their functioning, such as difﬁculty adjustment in\nquestion generation) could negatively impact the end-users. Regarding their use as virtual\ntutors, reinforcement learning from human feedback is essential to gain control over their\noperation and ensure fairness. Ultimately, the risk of poor accuracy must be palliated by\nincluding the probabilistic conﬁdence of their response.\n4 Conclusion\nLLMs represent an undeniably mega-trend in the current century in many ﬁelds and industrial\nsectors. In the particular case of learning, these generative AI-based solutions have produced\na considerable buzz. Accordingly, they enable hands-on learning and are commonly used\nin classrooms nowadays. Compared to previous AI solutions and traditional methodologies,\nwhich focused primarily on modifying the textual input, advanced LLMs can generate on-\nthe-ﬂy human-like utterances, enhancing pedagogical practice and providing personalized\nassessment and tutoring.\nGiven the popularity of LLMs, this work is the ﬁrst to contribute with a comprehensive\noverview of their application within the educational ﬁeld, paying particular attention to those\nthat involved students or teachers in the design or experimental plan. From the 342 records\nobtained during data gathering, 29 works passed the screening stage by meeting the eligibility\ncriteria. They were discussed, taking into account their application within the educational\nﬁeld, the model used, and code and data availability features. Results show that the most\n17 Available at https://arxiv.org/pdf/2209.11830.pdf, April 2024.\n123\n888 S. García-Méndez et al.\ncommon tasks performed as virtual assistants are question generation, answer grading, and\ncode correction and explanation. Moreover, the most popular model continues to be BERT,\nfollowed by GPT-3, T5, and GPT-3.5 models. In the end, this review identiﬁed 9 reproducible\nworks and 8 solutions that involved either teachers or students in the design or experimental\nplan.\nDue to the recent launch of the GPT-4 model within the ChatGPT application, new works\nare expected to be published soon and will be analyzed as part of future work. Moreover,\nas future work, we will study the ethical implications of LLMs (i.e., their transparency and\nfairness behavior caused by the training data and privacy) and how the solutions discussed\ncan be integrated into the education curricula, as well as their shortcomings and risk to\nacademic integrity (e.g., plagiarism concerns). Finally, attention will be paid to those works\nthat propose innovative teaching practices with LLMs and explore the use of ad hoc solutions\nthrough personal language models in the ﬁeld.\nAuthor Contribution Silvia García-Méndez: conceptualization, methodology, software, validation, formal\nanalysis, investigation, resources, data curation, writing—original draft, writing—review and editing, visual-\nization, supervision, project administration, funding acquisition. Francisco de Arriba-Pérez: conceptualization,\nmethodology, software, validation, formal analysis, investigation, resources, data curation, writing—original\ndraft, writing—review and editing, visualization, supervision, project administration, funding acquisition.\nCarmen Somoza-López: conceptualization, writing—review and editing.\nFunding Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature. This\nwork was partially supported by ( i) Xunta de Galicia grants ED481B-2021-118 and ED481B-2022-093, Spain,\nand ( iii) University of Vigo/CISUG for open access charge.\nAvailability of Data and Material The used data is openly available.\nDeclarations\nEthics Approval Not applicable\nConsent to Participate Not applicable\nConsent for Publication Not applicable\nConﬂict of Interest The authors declare that they have no conﬂict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAhmed, A., Joorabchi, A., & Hayes, M. J. (2022). On the application of sentence transformers to automatic\nshort answer grading in blended assessment. In: Proceedings of Irish Signals and Systems Conference.\nIEEE, pp 1–6, https://doi.org/10.1109/ISSC55427.2022.9826194\nAlshater M (2022) Exploring the role of artiﬁcial intelligence in enhancing academic performance: A case\nstudy of ChatGPT. SSRN Electronic Journal pp 1–22. https://doi.org/10.2139/ssrn.4312358\n123\nA Review on the Use of Large Language Models... 889\n123\nBaha, T. A., Hajji, M. E., Es-Saady, Y ., et al. (2022). Towards highly adaptive Edu-Chatbot. Procedia Computer\nScience, 198 , 397–403. https://doi.org/10.1016/j.procs.2021.12.260\nBaidoo-Anu D, Ansah LO (2023) Education in the era of generative artiﬁcial intelligence (AI): Understanding\nthe potential beneﬁts of ChatGPT in promoting teaching and learning. Journal of AI 7:52–62. https://doi.\norg/10.61969/jai.1337500\nBhat, S., Nguyen, H. A., Moore, S., et al (2022). Towards automated generation and evaluation of questions in\neducational domains. In: Proceedings of the International Conference on Educational Data Mining, vol\n701. The International Educational Data Mining Society, pp 701–704, https://doi.org/10.5281/zenodo.\n6853085\nBrown, T. B., Mann, B., Ryder, N., et al (2020) Language models are few-shot learners. In: Proceedings of\nthe Conference on Neural Information Processing Systems, vol 33. NeurIPS Inc., pp 1877–1901\nCalabrese, A., Rivoli, A., Sciarrone, F., et al. (2022). An intelligent chatbot supporting students in massive\nopen online courses. In: Proceedings of the International Symposium on Emerging Technologies for\nEducation, vol 13869. Springer, pp 190–201, https://doi.org/10.1007/978-3-031-33023-0_17\nChen, G., Yang, J., Hauff, C., et al. (2018). LearningQ: A large-scale dataset for educational question generation.\nIn: Proceedings of the International AAAI Conference on Web and Social Media, vol 12. AAAI Press,\npp 481–490, https://doi.org/10.1609/icwsm.v12i1.14987\nChrist, P . (2023). Generation of meaningful SQL-Query exercises using large language models and knowledge\ngraphs\nCobbe, K., Kosaraju, V ., Bavarian, M., et al. (2021) Training veriﬁers to solve math word problems\nCooper, G. (2023). Examining science education in ChatGPT: An exploratory study of generative artiﬁcial\nintelligence. Journal of Science Education and Technology, 32, 444–452. https://doi.org/10.1007/s10956-\n023-10039-y\nda Silva, E., da Silva, F. A., Womg, K. J., et al. (2022). QUERAI - A smart quiz generator\nDemircioglu, T., Karakus, M., & Ucar, S. (2022). Developing students’ critical thinking skills and argumen-\ntation abilities through augmented reality-based argumentation activities in science classes. Science &\nEducation, 32 , 1165–1195. https://doi.org/10.1007/s11191-022-00369-5\nDevlin, J., Chang, M. W., Lee, K., et al. (2019). BERT: Pre-training of deep bidirectional transformers for lan-\nguage understanding. In: Proceedings of the Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, vol 1. Association for Computational\nLinguistics, pp 4171–4186\nDijkstra, R., Genç, Z., Kayal, S., et al. (2022). Reading comprehension quiz generation using generative\npre-trained transformers. In: Proceedings of International Workshop on Intelligent Textbooks, vol 3192.\nCEUR, pp 4–17\nEssel, H. B., Vlachopoulos, D., Tachie-Menson, A., et al. (2022). The impact of a virtual teaching assistant\n(chatbot) on students’ learning in Ghanaian higher education. International Journal of Educational\nTechnology in Higher Education, 19 , 57–75. https://doi.org/10.1186/s41239-022-00362-6\nEstigarribia, L., Chalabe, J. K. T., Cisnero, K., et al. (2022). Co-design of a teaching-learning sequence to\naddress COVID-19 as a socio-scientiﬁc issue in an infodemic context. Science & Education, 31 , 1585–\n1627. https://doi.org/10.1007/S11191-022-00362-Y/TABLES/2\nGeorge, A. S., George, A. H., & Martin, A. (2023). A review of ChatGPT AI’s impact on several business\nsectors. Partners Universal International Innovation Journal, 1 , 9–23. https://doi.org/10.5281/zenodo.\n7644359\nGraves, A., Wayne, G., Reynolds, M., et al. (2016). Hybrid computing using a neural network with dynamic\nexternal memory. Nature, 538, 471–476. https://doi.org/10.1038/nature20101\nHaleem, A., Javaid, M., & Singh, R. P . (2022). An era of ChatGPT as a signiﬁcant futuristic support tool: A\nstudy on features, abilities, and challenges. BenchCouncil Transactions on Benchmarks, Standards and\nEvaluations, 2 , 100089–100096. https://doi.org/10.1016/j.tbench.2023.100089\nHaluza, D., & Jungwirth, D. (2023). Artiﬁcial intelligence and ten societal megatrends: An exploratory study\nusing GPT-3. Systems, 11 , 1–18. https://doi.org/10.3390/systems11030120\nHan, L. (2018). Analysis of new advances in the application of artiﬁcial intelligence to education. In: Proceed-\nings of the International Conference on Education, E-learning and Management Technology. Atlantis\nPress, pp 608–611, https://doi.org/10.2991/iceemt-18.2018.118\nHardy, M. (2021). Toward educator-focused automated scoring systems for reading and writing\nHochberg, K., Kuhn, J., & Müller, A. (2018). Using smartphones as experimental tools-Effects on interest,\ncuriosity, and learning in physics education. Journal of Science Education and Technology, 27 , 385–403.\nhttps://doi.org/10.1007/s10956-018-9731-7\nHuang, X., & Qiao, C. (2022). Enhancing computational thinking skills through artiﬁcial intelligence educa-\ntion at a STEAM high school. Science & Education, 33 , 383–403. https://doi.org/10.1007/s11191-022-\n00392-6\n890 S. García-Méndez et al.\n123\nHumphry, T., & Fuller, A. L. (2023). Potential ChatGPT use in undergraduate chemistry laboratories. Journal\nof Chemical Education., 100 , 1434–1436. https://doi.org/10.1021/acs.jchemed.3c00006\nJensen, J. L., Holt, E. A., Sowards, J. B., et al. (2018). Investigating strategies for pre-class content learning\nin a ﬂipped classroom. Journal of Science Education and Technology, 27 , 523–535. https://doi.org/10.\n1007/s10956-018-9740-6\nJia, X., Zhou, W., Sun, X., et al. (2021). EQG-RACE: Examination-type question generation. In: Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, vol 35. AAAI Press, pp 13,143–13,151, https://doi.\norg/10.1609/aaai.v35i14.17553\nKasneci, E., Sessler, K., Küchemann, S., et al. (2023). ChatGPT for good? On opportunities and challenges of\nlarge language models for education. Learning and Individual Differences 103 :102,274–102,282. https://\ndoi.org/10.1016/j.lindif.2023.102274\nKohn, T., & Manaris, B. (2020). Tell me what’s wrong. In: Proceedings of the ACM Technical Symposium on\nComputer Science Education. Association for Computing Machinery, pp 1054–1060, https://doi.org/10.\n1145/3328778.3366920\nKorteling, J. E. H., Boer-Visschedijk, G. V . D., Blankendaal, R. A., et al. (2021). Human versus artiﬁcial\nintelligence. Frontiers in Artiﬁcial Intelligence, 4 , 1–13. https://doi.org/10.3389/frai.2021.622364\nLavie, A., & Agarwal, A. (2007). METEOR: An automatic metric for MT evaluation with high levels of\ncorrelation with human judgments. In: Proceedings of the Workshop on Statistical Machine Translation.\nAssociation for Computing Machinery, pp 228–231\nLi, X., Li, Y ., & Wang, W. (2023). Long-lasting conceptual change in science education. Science & Education,\n32, 123–168. https://doi.org/10.1007/s11191-021-00288-x\nLiang, Y ., Li, J., & Yin, J. (2019). A new multi-choice reading comprehension dataset for curriculum learning.\nIn: Proceedings of the Asian Conference on Machine Learning, vol 101. PMLR, pp 742–757\nLiu, S., Man, S., & Song, L. (2022). An NLP-empowered virtual course assistant for online teaching and\nlearning. In: Proceedings of the IEEE International Conference on Teaching, Assessment and Learning\nfor Engineering. IEEE, pp 373–380, https://doi.org/10.1109/TALE54877.2022.00068\nMacNeil, S., Kim, J., Leinonen, J., et al. (2022). The implications of large language models for CS teachers and\nstudents. In: Proceedings of the ACM Technical Symposium on Computer Science Education. Association\nfor Computing Machinery, pp 1255–1257, https://doi.org/10.1145/3545947.3573358\nMacNeil, S., Tran, A., Mogil, D., et al. (2022). Generating diverse code explanations using the GPT-3 large lan-\nguage model. In: Proceedings of the ACM Conference on International Computing Education Research,\nvol 2. Association for Computing Machinery, pp 37–39, https://doi.org/10.1145/3501709.3544280\nMahajan, M. (2022). BELA: Bot for English language acquisition. In: Proceedings of the Second Workshop\non NLP for Positive Impact. Association for Computational Linguistics, pp 142–148, https://doi.org/10.\n18653/v1/2022.nlp4pi-1.17\nMendoza, S., Sánchez-Adame, L. M., Urquiza-Yllescas, J. F., et al. (2022). A model to develop chatbots for\nassisting the teaching and learning process. Sensors, 22, 5532–5552. https://doi.org/10.3390/s22155532\nMhlanga, D. (2023). Open AI in education, the responsible and ethical use of ChatGPT towards lifelong\nlearning. In: FinTech and Artiﬁcial Intelligence for Sustainable Development. Springer, p 1–19, https://\ndoi.org/10.1007/978-3-031-37776-1_17\nMohler, M., & Mihalcea, R. (2009). Text-to-text semantic similarity for automatic short answer grading. In:\nProceedings of the Conference of the European Chapter of the Association for Computational Linguistics.\nAssociation for Computational Linguistics, pp 567–575, https://doi.org/10.3115/1609067.1609130\nMohler, M., Bunescu, R., & Mihalcea, R. (2011). Learning to grade short answer questions using seman-\ntic similarity measures and dependency graph alignments. In: Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics: Human Language Technologies, vol 1. Association for\nComputing Machinery, p 752-762\nMoore, S., Nguyen, H. A., Bier, N., et al. (2022). Assessing the quality of student-generated short answer\nquestions using GPT-3. In: Proceedings of the European Conference on Technology Enhanced Learning,\nvol 13450. Springer, pp 243–257, https://doi.org/10.1007/978-3-031-16290-9_18\nNasution, N. E. A. (2023). Using artiﬁcial intelligence to create biology multiple choice questions for higher\neducation. Agricultural and Environmental Education, 2 , 1–11. https://doi.org/10.29333/agrenvedu/\n13071\nO’Connor, S., & ChatGPT. (2023). Open artiﬁcial intelligence platforms in nursing education: Tools for\nacademic progress or abuse? Nurse Education in Practice, 66 , 103537–103538. https://doi.org/10.1016/\nj.nepr.2022.103537\nPapineni, K., Roukos, S., Ward, T., et al. (2002). BLEU: A method for automatic evaluation of machine\ntranslation. In: Proceedings of the Annual Meeting of the Association for Computational Linguistics.\nAssociation for Computing Machinery, pp 311–318\nA Review on the Use of Large Language Models... 891\n123\nPavlik, J. V . (2023). Collaborating With ChatGPT: Considering the implications of generative artiﬁcial intel-\nligence for journalism and media education. Journalism & Mass Communication Educator , 78 , 84–93.\nhttps://doi.org/10.1177/10776958221149577\nPerkins, M. (2023). Academic integrity considerations of AI large language models in the post-pandemic era:\nChatGPT and beyond. Journal of University Teaching and Learning Practice 20:2–26. https://doi.org/10.\n53761/1.20.02.07\nPhillips, T., Saleh, A., Glazewski, K. D., et al. (2022). Exploring the use of GPT-3 as a tool for evaluating text-\nbased collaborative discourse. In: Proceedings of the International Conference on Learning Analytics &\nKnowledge. Society for Learning Analytics Research, pp 54–56\nPhung T, Cambronero J, Gulwani S, et al (2023) Generating high-precision feedback for programming syntax\nerrors using large language models\nPipalia, K., Bhadja, R., & Shukla, M. (2020). Comparative analysis of different transformer based archi-\ntectures used in sentiment analysis. In: Proceedings of the International Conference System Modeling\nand Advancement in Research Trends. IEEE, pp 411–415, https://doi.org/10.1109/SMART50582.2020.\n9337081\nPrihar, E., Moore, A., & Heffernan, N. (2022). Identifying explanations within student-tutor chat logs. In:\nProceedings of the International Conference on Educational Data Mining. International Educational\nData Mining Society, pp 773–777, https://doi.org/10.5281/ZENODO.6852938\nRaffel, C., Shazeer, N., Roberts, A., et al. (2020). Exploring the limits of transfer learning with a uniﬁed text-\nto-text transformer. Journal of Machine Learning Research, 21 , 5485–5551. https://doi.org/10.5555/\n3455716.3455856\nRaina, V ., & Gales, M. (2022). Multiple-choice question generation: towards an automated assessment frame-\nwork\nRasa T, Lavonen J, Laherto A (2023) Agency and transformative potential of technology in students’ images\nof the future. Science & Education pp 1–25. https://doi.org/10.1007/s11191-023-00432-9\nRoll, I., & Wylie, R. (2016). Evolution and revolution in artiﬁcial intelligence in education. International\nJournal of Artiﬁcial Intelligence in Education, 26 , 582–599. https://doi.org/10.1007/s40593-016-0110-\n3\nRudolph, J., Tan, S., & Tan, S. (2023). ChatGPT: Bullshit spewer or the end of traditional assessments in\nhigher education? Journal of Applied Learning & Teaching, 6 , 342–363. https://doi.org/10.37074/jalt.\n2023.6.1.9\nRusse, M. F., Reisert, M., Bamberg, F., et al. (2024). Improving the use of LLMs in radiology through\nprompt engineering: from precision prompts to zero-shot learning. RöFo - Fortschritte auf dem Gebiet\nder Röntgenstrahlen und der bildgebenden V erfahren pp 1–5. https://doi.org/10.1055/a-2264-5631\nSallam, M. (2023). ChatGPT utility in healthcare education, research, and practice: Systematic review\non the promising perspectives and valid concerns. Healthcare, 11 , 887–906. https://doi.org/10.3390/\nhealthcare11060887\nSarker, I. H. (2022). AI-based modeling: Techniques, applications and research issues towards automa-\ntion, intelligent and smart systems. SN Computer Science, 3 , 1–20. https://doi.org/10.1007/s42979-022-\n01043-x\nSharma, S., Agarwal, R., & Mittal, A. (2022). Generating educational questions with similar difﬁculty level.\nIn: Proceedings of the International Conference on Innovative Computing & Communication. SSRN\nElectronic Journal, pp 1–9, https://doi.org/10.2139/ssrn.4033499\nSok, S., & Heng, K. (2023). ChatGPT for education and research: A review of beneﬁts and risks. SSRN\nElectronic Journal, 3 , 110–121. https://doi.org/10.2139/ssrn.4378735\nSophia, J., & Jacob, T. (2021). EDUBOT-A chatbot for education in COVID-19 pandemic and VQAbot com-\nparison. In: Proceedings of the International Conference on Electronics and Sustainable Communication.\nIEEE, pp 1707–1714, https://doi.org/10.1109/ICESC51422.2021.9532611\nTaber, K. S. (2018). The use of Cronbach’s alpha when developing and reporting research instruments in\nscience education. Research in Science Education, 48 , 1273–1296. https://doi.org/10.1007/s11165-016-\n9602-2\nTalan, T. (2021). Artiﬁcial intelligence in education: A bibliometric study. International Journal of Research\nin Education and Science, 7 (3), 822–837. https://doi.org/10.46328/ijres.2409\nTay, Y ., Dehghani, M., Bahri, D., et al. (2023). Efﬁcient transformers: A survey. ACM Computing Surveys, 55 ,\n1–28. https://doi.org/10.1145/3530811\nTobore, T. O. (2019). On energy efﬁciency and the brain’s resistance to change: The neurological evolution\nof dogmatism and close-mindedness. Psychological Reports, 122 , 2406–2416. https://doi.org/10.1177/\n0033294118792670\n892 S. García-Méndez et al.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123\nTopsakal, O., & Topsakal, E. (2022). Framework for a foreign language teaching software for children utilizing\nAR, V oicebots and ChatGPT (large language models). The Journal of Cognitive Systems 7:33–38. https://\ndoi.org/10.52876/jcs.1227392\nTyen, G., Brenchley, M., Caines, A., et al. (2022). Towards an open-domain chatbot for language practice.\nIn: Proceedings of the Workshop on Innovative Use of NLP for Building Educational Applications.\nAssociation for Computational Linguistics, pp 234–249, https://doi.org/10.18653/v1/2022.bea-1.28\nWang, Y ., Liu, S., & Song, L. (2022) Designing an educational chatbot with joint intent classiﬁcation and slot\nﬁlling. In: Proceedings of the IEEE International Conference on Teaching, Assessment and Learning for\nEngineering. IEEE, pp 381–388, https://doi.org/10.1109/TALE54877.2022.00069\nYamaoka, K., Watanabe, K., Kise, K., et al. (2022). Experience is the best teacher: Personalized vocabulary\nbuilding within the context of Instagram posts and sentences from GPT-3. In: Proceedings of the ACM\nInternational Joint Conference on Pervasive and Ubiquitous Computing. Association for Computing\nMachinery, pp 313–316, https://doi.org/10.1145/3544793.3560382\nY u, J., Wang, Y ., Zhong, Q., et al. (2021). MOOCCubeX: A large knowledge-centered repository for adaptive\nlearning in MOOCs. In: Proceedings of the ACM International Conference on Information & Knowledge\nManagement. Association for Computing Machinery, pp 4643–4652, https://doi.org/10.1145/3459637.\n3482010\nZhang, F., Markopoulos, P ., & Bekker, T. (2020). Children’s emotions in design-based learning: A systematic\nreview.Journal of Science Education and Technology, 29, 459–481. https://doi.org/10.1007/s10956-020-\n09830-y\nZhang, J., Cambronero, J., Gulwani, S., et al. (2022). Repairing bugs in Python assignments using large\nlanguage model\nZhou, L., Meng, W., Wu, S., et al. (2023). Development of digital education in the age of digital transformation:\nCiting China’s practice in smart education as a case study. Science Insights Education Frontiers, 14 ,\n2077–2092. https://doi.org/10.15354/sief.23.or095\nZong, M., & Krishnamachari, B. (2022). Solving math word problems concerning systems of equations with\nGPT-3. In: Proceedings of the Symposium on Educational Advances in Artiﬁcial Intelligence, vol 37.\nAssociation for the Advancement of Artiﬁcial Intelligence, pp 15,972–15,979, https://doi.org/10.1609/\naaai.v37i13.26896",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.49060049653053284
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I6289922",
      "name": "Universidade de Vigo",
      "country": "ES"
    }
  ],
  "cited_by": 22
}