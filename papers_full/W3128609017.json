{
  "title": "Deepfake Video Detection Using Convolutional Vision Transformer",
  "url": "https://openalex.org/W3128609017",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4384614642",
      "name": "Wodajo, Deressa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4292018122",
      "name": "Atnafu, Solomon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2906447902",
    "https://openalex.org/W3034964422",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W3088451619",
    "https://openalex.org/W2792259108",
    "https://openalex.org/W2979894294",
    "https://openalex.org/W2806757392",
    "https://openalex.org/W2963890275",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2737559518",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3015756412",
    "https://openalex.org/W3045755384",
    "https://openalex.org/W2894295011",
    "https://openalex.org/W2990452356",
    "https://openalex.org/W2962134292",
    "https://openalex.org/W3036644531",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963720850",
    "https://openalex.org/W3105763085",
    "https://openalex.org/W2899901572",
    "https://openalex.org/W2963791174",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W3006335338",
    "https://openalex.org/W2786055572",
    "https://openalex.org/W2904419556",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2898877033",
    "https://openalex.org/W2791895881",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3081974451",
    "https://openalex.org/W2790214987",
    "https://openalex.org/W2888499542",
    "https://openalex.org/W2738406145",
    "https://openalex.org/W3030681797",
    "https://openalex.org/W2902962850",
    "https://openalex.org/W3125405799",
    "https://openalex.org/W2904573504",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2930789748",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W3080632971",
    "https://openalex.org/W2155653793",
    "https://openalex.org/W3075596981",
    "https://openalex.org/W3017837134",
    "https://openalex.org/W3110422536",
    "https://openalex.org/W3007968293",
    "https://openalex.org/W2991309320",
    "https://openalex.org/W3112437058",
    "https://openalex.org/W2976451995",
    "https://openalex.org/W2519796145",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2984700035",
    "https://openalex.org/W2891145043",
    "https://openalex.org/W2909336075",
    "https://openalex.org/W2852084320",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W3125803510",
    "https://openalex.org/W2789600509",
    "https://openalex.org/W2904367110",
    "https://openalex.org/W2367460255",
    "https://openalex.org/W3036576316",
    "https://openalex.org/W3009852267",
    "https://openalex.org/W2099471712"
  ],
  "abstract": "The rapid advancement of deep learning models that can generate and synthesis hyper-realistic videos known as Deepfakes and their ease of access to the general public have raised concern from all concerned bodies to their possible malicious intent use. Deep learning techniques can now generate faces, swap faces between two subjects in a video, alter facial expressions, change gender, and alter facial features, to list a few. These powerful video manipulation methods have potential use in many fields. However, they also pose a looming threat to everyone if used for harmful purposes such as identity theft, phishing, and scam. In this work, we propose a Convolutional Vision Transformer for the detection of Deepfakes. The Convolutional Vision Transformer has two components: Convolutional Neural Network (CNN) and Vision Transformer (ViT). The CNN extracts learnable features while the ViT takes in the learned features as input and categorizes them using an attention mechanism. We trained our model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5 percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our contribution is that we have added a CNN module to the ViT architecture and have achieved a competitive result on the DFDC dataset.",
  "full_text": "arXiv:2102.11126v3  [cs.CV]  11 Mar 2021\nDeepfake Video Detection Using Convolutional Vision T rans former\nDeressa W odajo\nJimma University\nderessa.wodajo@ju.edu.et\nSolomon Atnafu\nAddis Ababa University\nsolomon.atnafu@aau.edu.et\nAbstract\nThe rapid advancement of deep learning models that\ncan generate and synthesis hyper-realistic videos known as\nDeepfakes and their ease of access have raised concern on\npossible malicious intent use. Deep learning techniques\ncan now generate faces, swap faces between two subjects\nin a video, alter facial expressions, change gender , and\nalter facial features, to list a few . These powerful video\nmanipulation methods have potential use in many ﬁelds.\nHowever , they also pose a looming threat to everyone if\nused for harmful purposes such as identity theft, phishing,\nand scam. In this work, we propose a Convolutional V i-\nsion T ransformer for the detection of Deepfakes. The Con-\nvolutional V ision T ransformer has two components: Con-\nvolutional Neural Network (CNN) and V ision T ransformer\n(V iT). The CNN extracts learnable features while the V iT\ntakes in the learned features as input and categorizes them\nusing an attention mechanism. W e trained our model on the\nDeepF ake Detection Challenge Dataset (DFDC) and have\nachieved 91.5 percent accuracy, an AUC value of 0.91, and\na loss value of 0.32. Our contribution is that we have added\na CNN module to the V iT architecture and have achieved a\ncompetitive result on the DFDC dataset.\n1. Introduction\nT echnologies for altering images, videos, and audios are\ndeveloping rapidly [12, 62]. T echniques and technical ex-\npertise to create and manipulate digital content are also ea s-\nily accessible. Currently, it is possible to seamlessly gen er-\nate hyper-realistic digital images [28] with a little resou rce\nand an easy how-to-do instructions available online [30, 9] .\nDeepfake is a technique which aims to replace the face of a\ntargeted person by the face of someone else in a video [1]. It\nis created by splicing synthesized face region into the orig -\ninal image [62]. The term can also mean to represent the\nﬁnal output of a hype-realistic video created. Deepfakes ca n\nbe used for creation of hyper-realistic Computer Generated\nImagery (CGI), V irtual Reality (VR) [7], Augmented Re-\nality (AR), Education, Animation, Arts, and Cinema [13].\nHowever, since Deepfakes are deceptive in nature, they can\nalso be used for malicious purposes.\nSince the Deepfake phenomenon, various authors have\nproposed different mechanisms to differentiate real video s\nfrom fake ones. As pointed by [10], even though each pro-\nposed mechanism has its strength, current detection meth-\nods lack generalizability. The authors noted that current e x-\nisting models focus on the Deepfake creation tools to tackle\nby studying their supposed behaviors. For instance, Y uezun\net al . [33] and T ackHyun et al . [25] used inconsistencies in\neye blinking to detect Deepfakes. However, using the work\nof Konstantinos et al . [58] and Hai et al . [46], it is now\npossible to mimic eye blinking. The authors in [58] pre-\nsented a system that generates videos of talking heads with\nnatural facial expressions such as eye blinking. The author s\nin [46] proposed a model that can generate facial expression\nfrom a portrait. Their system can synthesis a still picture t o\nexpress emotions, including a hallucination of eye-blinki ng\nmotions.\nW e base our work on two weaknesses of Deepfake de-\ntection methods pointed out by [10, 11]: data preprocess-\ning, and generality. Polychronis et al . [11] noted that cur-\nrent Deepfake detection systems focus mostly on presenting\ntheir proposed architecture, and give less emphasis on data\npreprocessing and its impact on the ﬁnal detection model.\nThe authors stressed the importance of data preprocess-\ning for Deepfake detections. Joshual et al . [10] focused\non the generality of facial forgery detection and found that\nmost proposed systems lacked generality. The authors de-\nﬁned generality as reliably detecting multiple spooﬁng tec h-\nniques and reliably spooﬁng unseen detection techniques.\nUmur et al . [13] proposed a generalized Deepfake de-\ntector called FakeCatcher using biological signals (inter nal\nrepresentations of image generators and synthesizers). Th ey\nused a simple Convolutional Neural Network (CNN) clas-\nsiﬁer with only three layers. The authors used 3000 videos\nfor training and testing. However, they didn’t specify in de -\ntail how they preprocessed their data. From [31, 52, 21], it\nis evident that very deep CNNs have superior performance\nthan shallow CNNs in image classiﬁcation tasks. Hence,\nthere is still room for another generalized Deepfake detec-\ntor that has extensive data preprocessing pipeline and also\nis trained on a very deep Neural Network model to catch as\nmany Deepfake artifacts as possible.\nTherefore, we propose a generalized Convolutional V i-\nsion Transformer (CV iT) architecture to detect Deepfake\nvideos using Convolutional Neural Networks and the Trans-\nformer architecture. W e call our approach generalized for\nthree main reasons. 1) Our proposed model can learn local\nand global image features using the CNN and the Trans-\nformer architecture by using the attention mechanism of the\nTransformer [6]. 2) W e give equal emphasis on our data pre-\nprocessing during training and classiﬁcation. 3) W e propos e\nto train our model on a diverse set of face images using the\nlargest dataset currently available to detect Deepfakes cr e-\nated in different settings, environments, and orientation s.\n2. Related W ork\nWith the rapid advancement of the CNNs [4, 20], Gen-\nerative Adversarial Networks (GANs) [18], and its vari-\nants [22], it is now possible to create hyper-realistic im-\nages [32], videos [61] and audio signals [53, 15] that are\nmuch harder to detect and distinguish from real untampered\naudiovisuals. The ability to create a seemingly real sound,\nimages, and videos have caused a steer from various con-\ncerned stakeholders to deter such developments not to be\nused by adversaries for malicious purposes [12]. T o this ef-\nfect, there is currently an urge in the research community to\ncome with Deepfake detection mechanisms.\n2.1. Deep Learning T echniques for Deepfake Video\nGeneration\nDeepfake is generated and synthesized by deep genera-\ntive models such GANs and Autoencoders (AEs) [18, 37].\nDeepfake is created by swapping between two identities\nof subjects in an image or video [56]. Deepfake can\nalso be created by using different techniques such as face\nswap [43], puppet-master [53], lip-sync [49, 47], face-\nreenactment [14], synthetic image or video generation, and\nspeech synthesis [48]. Supervised [45, 24, 51], and un-\nsupervised image-to-image translation [19] and video-to-\nvideo translation [59, 35] can be used to create highly re-\nalistic Deepfakes.\nThe ﬁrst Deepfake technique is the FakeAPP [42] which\nused two AE network. An AE is a Feedforward Neural Net-\nwork (FFNN) with an encoder-decoder architecture that is\ntrained to reconstruct its input data [60]. FakeApp’s encod er\nextracts the latent face features, and its decoder reconstr ucts\nthe face images. The two AE networks share the same en-\ncoder to swap between the source and target faces, and dif-\nferent decoders for training.\nMost of the Deepfake creation mechanisms focus on the\nface region in which face swapping and pixel-wise editing\nare commonly used [28]. In the face swap, the face of a\nsource image is swapped on the face of a target image. In\npuppet-master, the person creating the video controls the\nperson in the video. In lip-sync, the source person controls\nthe mouse movement in the target video, and in face reen-\nactment, facial features are manipulated [56]. The Deep-\nfake creation mechanisms commonly use feature map rep-\nresentations of a source image and target image. Some of\nthe feature map representations are the Facial Action Cod-\ning System (F ACS), image segmentation, facial landmarks,\nand facial boundaries [37]. F ACS is a taxonomy of human\nfacial expression that deﬁnes 32 atomic facial muscle ac-\ntions named Action Units (A U) and 14 Action Descriptors\n(AD) for miscellaneous actions. Facial land marks are a\nset of deﬁned positions on the face, such as eye, nose, and\nmouth positions [36].\n2.1.1 Face Synthesis\nImage synthesis deals with generating unseen images from\nsample training examples [23]. Face image synthesis tech-\nniques are used in face aging, face frontalization, and pose\nguided generation. GANs are used mainly in face synthe-\nsis. GANs are generative models that are designed to create\ngenerative models of data from samples [3, 18]. GANs\ncontain two adversarial networks, a generative model G ,\nand discriminative model D . The generator and the dis-\ncriminator act as adversaries with respect to each other to\nproduce real-like samples [22]. The generator’s goal is to\ncapture the data distribution. The goal of the discriminato r\nis to determine whether a sample is from the model distribu-\ntion or the data distribution [18]. Face frontalization GAN s\nchange the face orientation in an image. Pose guided face\nimage generation maps the pose of an input image to an-\nother image. GAN architecture, such as StyleGAN [26] and\nFSGAN [43], synthesize highly realistic-looking images.\n2.1.2 Face Swap\nFace swap or identity swap is a GAN based method that\ncreates realistic Deepfake videos. The face swap process\ninserts the face of a source image in a target image of which\nthe subject has never appeared [56]. It is most popularly\nused to insert famous actors in a variety of movie clips [2].\nFace swaps can be synthesized using GANs and traditional\nCV techniques such as FaceSwap (an application for swap-\nping faces) and ZA O (a Chines mobile application that\nswaps anyone’s face onto any video clips) [56]. Face Swap-\nping GAN (FSGAN) [43], and Region-Separative GAN\n(RSGAN) [39] are used for face swapping, face reenact-\nment, attribute editing, and face part synthesis. Deepfake\nFaceSwap uses two AEs with a shared encoder that recon-\nstructs training images of the source and target faces [56].\nThe processes involve a face detector that crops and aligns\nthe face using facial landmark information [38]. A trained\n2\nencoder and decoder of the source face swap the features of\nthe source image to the target face. The autoencoder out-\nput is then blended with the rest of the image using Poisson\nediting [38].\nFacial expression (face reenactment) swap alters one’s\nfacial expression or transforms facial expressions among\npersons. Expression reenactment turns an identity into a\npuppet [37]. Using facial expression swap, one can transfer\nthe expression of a person to another one [27]. V arious fa-\ncial reenactments have proposed through the years. Cycle-\nGAN is proposed by Jun-Y an et al . [63] for facial reenact-\nment between two video sources without any pair of train-\ning examples. Face2Face manipulates the facial expression\nof a source image and projects onto another target face in\nreal-time [54]. Face2Face creates a dense reconstruction\nbetween the source image and the target image that is used\nfor the synthesis of the face images under different light se t-\ntings [38].\n2.2. Deep Learning T echniques for Deepfake Video\nDetection\nDeepfake detection methods fall into three categories\n[34, 37]. Methods in the ﬁrst category focus on the physical\nor psychological behavior of the videos, such as tracking\neye blinking or head pose movement. The second category\nfocus on GAN ﬁngerprint and biological signals found in\nimages, such as blood ﬂow that can be detected in an im-\nage. The third category focus on visual artifacts. Methods\nthat focus on visual artifacts are data-driven, and require a\nlarge amount of data for training. Our proposed model falls\ninto the third category. In this section, we will discuss var -\nious architectures designed and developed to detect visual\nartifacts of Deepfakes.\nDarius et al . [1] proposed a CNN model called MesoNet\nnetwork to automatically detect hyper-realistic forged\nvideos created using Deepfake [40] and Face2Face [54].\nThe authors used two network architectures (Meso-4 and\nMesoInception-4) that focus on the mesoscopic properties\nof an image. Y uezun and Siwei [34] proposed a CNN ar-\nchitecture that takes advantage of the image transform (i.e .,\nscaling, rotation and shearing) inconsistencies created d ur-\ning the creation of Deepfakes. Their approach targets the\nartifacts in afﬁne face warping as the distinctive feature t o\ndistinguish real and fake images. Their method compares\nthe Deepfake face region with that of the neighboring pix-\nels to spot resolution inconsistencies that occur during fa ce\nwarping.\nHuy et al . [41] proposed a novel deep learning approach\nto detect forged images and videos. The authors focused on\nreplay attacks, face swapping, facial reenactments and ful ly\ncomputer generated image spooﬁng. Daniel Mas Montser-\nrat et al . [38] proposed a system that extracts visual and tem-\nporal features from faces present in a video. Their method\ncombines a CNN and RNN architecture to detect Deepfake\nvideos.\nMd. Shohel Rana and Andrew H. Sung [50] proposed a\nDeepfakeStack, an ensemble method (A stack of different\nDL models) for Deepfake detection. The ensemble is com-\nposed of XceptionNet, InceptionV3, InceptionResNetV2,\nMobileNet, ResNet101, DenseNet121, and DenseNet169\nopen source DL models. Junyaup Kim et al . [29] proposed\na classiﬁer that distinguishes target individuals from a se t of\nsimilar people using ShallowNet, VGG-16, and Xception\npre-trained DL models. The main objective of their system\nis to evaluate the classiﬁcation performance of the three DL\nmodels.\n3. Convolutional Vision T ransformer\nIn this section, we present our approach to detect Deep-\nfake videos. The Deepfake video detection model consists\nof two components: the preprocessing component and the\ndetection component. The preprocessing component con-\nsists of the face extraction and data augmentation. The\ndetection components consist of the training component,\nthe validation component, and the testing component. The\ntraining and validation components contain a Convolutiona l\nV ision Transformer (CV iT). The CV iT has a feature learn-\ning component that learns the features of input images and\na V iT architecture that determines whether a speciﬁc video\nis fake or real. The testing component applies the CV iT\nlearning model on input images to detect Deepfakes. Our\nproposed model is shown in Figure 1.\n3.1. Preprocessing\nThe preprocessing component’s function is to prepare\nthe raw dataset for training, validating, and testing our\nCV iT model. The preprocessing component has two sub-\ncomponents: the face extraction, and the data augmentation\ncomponent. The face extraction component is responsible\nfor extracting face images from a video in a 224 x 224 RGB\nformat. Figure 2 and Figure 3 shows a sample of the ex-\ntracted faces.\n3.2. Detection\nThe Deepfake detection process consists of three sub-\ncomponents: the training, the validation, and the testing\ncomponents. The training component is the principal part\nof the proposed model. It is where the learning occurs. DL\nmodels require a signiﬁcant time to design and ﬁne-tune to\nﬁt a particular problem domain into its model. In our case,\nthe foremost consideration is to search for an optimal CV iT\nmodel that learns the features of Deepfake videos. For this,\nwe need to search for the right parameters appropriate for\ntraining our dataset. The validation component is similar\nto that of the training component. The validation compo-\nnent is a process that ﬁne-tunes our model. It is used to\n3\nFigure 1. Convolutional Vision Transformer.\nFigure 2. Sample extracted fake face images.\nFigure 3. Sample extracted real face images.\nevaluate our CV iT model and helps the CV iT model to up-\ndate its internal state. It helps us to track our CV iT model’s\ntraining progress and its Deepfake detection accuracy. The\ntesting component is where we classify and determine the\nclass of the faces extracted in a speciﬁc video. Thus, this\nsub-component addresses our research objectives.\nThe proposed CV iT model consists of two components:\nFeature Learning (FL) and the V iT . The FL extracts learn-\nable features from the face images. The V iT takes in the FL\nas input and turns them into a sequence of image pixels for\nthe ﬁnal detection process.\nThe Feature Learning (FL) component is a stack of con-\nvolutional operations. The FL component follows the struc-\nture of VGG architecture [52]. The FL component differs\nfrom the VGG model in that it doesn’t have the fully con-\nnected layer as in the VGG architecture, and its purpose is\nnot for classiﬁcation but to extract face image features for\nthe V iT component. Hence, the FL component is a CNN\nwithout the fully connected layer.\nThe FL component has 17 convolutional layers, with a\nkernel of 3 x 3 . The convolutional layers extract the low\nlevel feature of the face images. All convolutional layers\nhave a stride and padding of 1. Batch normalization to nor-\nmalize the output features and the ReLU activation function\nfor non-linearity are applied in all of the layers. The Batch\nnormalization function normalizes change in the distribu-\ntion of the previous layers [41], as the change in between\nthe layers will affect the learning process of the CNN ar-\nchitecture. A ﬁve max-pooling of a 2 x 2 -pixel window\nwith stride equal to 2 is also used. The max-pooling oper-\nation reduces dimension of image size by half. After each\nmax-pooling operation, the width of the convolutional laye r\n(channel) is doubled by a factor of 2, with the ﬁrst layer\n4\nhaving 32 channels and the last layer 512.\nThe FL component has three consecutive convolutional\noperations at each layer, except for the last two layers,\nwhich have four convolutional operations. W e call those\nthree convolutional layers as CONV Block for simplicity.\nEach convolutional computation is followed by batch nor-\nmalization and the ReLU nonlinearity. The FL component\nhas 10.8 million learnable parameters. The FL takes in an\nimage of size 224 x 224 x 3 , which is then convolved at\neach convolutional operation. The FL internal state can be\nrepresented as (C ,H ,W ) tensor, where C is the channel,\nH is the height, and W is the width. The ﬁnal output of the\nFL is a 512 x 7 x 7 spatially correlated low level feature of\nthe input images, which are then fed to the V iT architecture.\nOur V ision Transformer (V iT) component is identical to\nthe V iT architecture described in [16]. V ision Transformer\n(V iT) is a transformer model based on the work of [57].\nThe transformer and its variants (e.g., GPT -3 [44]) are pre-\ndominantly used for NLP tasks. V iT extends the applica-\ntion of the transformer from the NLP problem domain to\na CV problem domain. The V iT uses the same compo-\nnents as the original transformer model with slight modi-\nﬁcation of the input signal. The FL component and the V iT\ncomponent makes up our Convolutional V ision Transformer\n(CV iT) model. W e named our model CV iT since the model\nis based on both a stack of convolutional operation and the\nV iT architecture.\nThe input to the V iT component is a feature map of the\nface images. The feature maps are split into seven patches\nand are then embedded into a 1 x 1024 linear sequence.\nThe embedded patches are then added to the position em-\nbedding to retain the positional information of the image\nfeature maps. The position embedding has a 2 x 1024 di-\nmension.\nThe V iT component takes in the position embedding and\nthe patch embedding and passes them to the Transformer.\nThe V iT Transformer uses only an encoder, unlike the orig-\ninal Transformer. The V iT encoder consists of MSA and\nMLP blocks. The MLP block is an FFN. The Norm normal-\nizes the internal layer of the transformer. The Transformer\nhas 8 attention heads. The MLP head has two linear layers\nand the ReLU nonlinearity. The MLP head task is equiva-\nlent to the fully connected layer of a typical CNN architec-\nture. The ﬁrst layer has 2048 channels, and the last layer\nhas two channels that represent the class of Fake or Real\nface image. The CV iT model has a total of 20 weighted\nlayers and 38.6 million learnable parameters. Softmax is\napplied on the MLP head output to squash the weight val-\nues between 0 and 1 for the ﬁnal detection purpose.\n4. Experiments\nIn this section, we present the tools and experimental\nsetup we used to design and develop the prototype to imple-\nment the model. W e will present the results acquired from\nthe implementation of the model and give an interpretation\nof the experimental results.\n4.1. Dataset\nDL models learn from data. As such, careful dataset\npreparation is crucial for their learning quality and predi c-\ntion accuracy. BlazeFace neural face detector [5], MTCNN\n[55] and face\nrecognition [17] DL libraries are used to ex-\ntract the faces. Both BlazeFace and face recognition are\nfast at processing a large number of images. The three DL\nlibraries are used together for added accuracy of face detec -\ntion. The face images are stored in a JPEG ﬁle format with\n224 x 224 image resolution. A 90 percent compression\nratio is also applied. W e prepared our datasets in a train,\nvalidation, and test sets. W e used 162,174 images classiﬁed\ninto 112,378 for training, 24,898 for validation and 24,898\nfor testing with 70 :15 :15 ratios, respectively. Each real and\nfake class has the same number of images in all sets.\nW e used Albumentations for data augmentation. Albu-\nmentations is a python data augmentation library which has\na large class of image transformations. Ninety percent of\nthe face images were augmented, making our total dataset\nto be 308,130 facial images.\n4.2. Evaluation\nThe CV iT model is trained using the binary cross-\nentropy loss function. A mini-batch of 32 images are nor-\nmalized using mean of [ 0 .485 , 0 .456 , 0 .406 ] and stan-\ndard deviation of [ 0 .229 , 0 .224 , 0 .225 ]. The normalized\nface images are then augmented before being fed into the\nCV iT model at each training iterations. Adam optimizer\nwith a learning rate of 0 .1e-3 and weight decay of 0 .1e-6\nis used for optimization. The model is trained for a total of\n50 epochs. The learning rate decreases by a factor of 0.1 at\neach step size of 15.\nThe classiﬁcation process takes in 30 facial images and\npasses it to our trained model. T o determine the classiﬁca-\ntion accuracy of our model, we used a log loss function. A\nlog loss described in Equation 1 classiﬁes the network into\na probability distribution from 0 to 1, where 0 > y < 0 .5\nrepresents the real class, and 0 .5 ≥ y < 1 represents the\nfake class. W e chose a log loss classiﬁcation metric because\nit highly penalizes random guesses and conﬁdent false pre-\ndictions.\nLogLoss = − 1\nn\nn∑\ni=1\n[yi log(ˆ yi ) +log(1 − yi )log(1 − ˆ yi )]\n(1)\nAnother metric we used to measure our model capacity\nis the ROC and A UC metrics [8]. The ROC is used to visu-\nalize a classiﬁer to select the classiﬁcation threshold. A U C\n5\nis an area covered by the ROC curve. A UC measures the\naccuracy of a classiﬁer.\nW e present our result using accuracy, A UC score, and\nloss value. W e tested the model on 400 unseen DFDC\nvideos and achieved 91.5 percent accuracy, an A UC value of\n0.91, and a loss value of 0.32. The loss value indicates how\nfar our models’ prediction is from the actual target value.\nFor Deepfake detection, we used 30 face images from each\nvideo. The amount of frame number we use affects the\nchance of Deepfake detection. However, accuracy might\nnot always be the right measure to detect Deepfakes as we\nmight encounter all real facial images from a fake video\n(fake videos might contain real frames).\nW e compared our result with other Deepfake detection\nmodels, as shown in T able 1, 2, and 3. From T able 1,\n2, and 3, we can see that our model performed well on\nthe DFDC, UADFV , and FaceForensics++ dataset. How-\never, our model performed poorly on the FaceForensics++\nFaceShifter dataset. The reason for this is because visual\nartifacts are hard to learn, and our proposed model likely\ndidn’t learn those artifacts well.\nDataset Accuracy\nFaceForensics++ FaceSwap 69%\nFaceForensics++ DeepFakeDetection 91%\nFaceForensics++ Deepfake 93%\nFaceForensics++ FaceShifter 46%\nFaceForensics++ NeuralT extures 60%\nT able 1. CViT model prediction accuracy on FaceForensics++\ndataset\nMethod V alidation T est\nCNN and RNN-GRU [38] [47] 92.61% 91.88%\nCV iT 87.25 91.5\nT able 2. Accuracy of our model and other Deepfake detection\nmodels on the DFDC dataset\nMethod V alidation FaceSwap Face2Face\nMesoNet 84.3% 96% 92%\nMesoInception 82.4% 98% 93.33%\nCV iT 93.75 69% 69.39%\nT able 3. AUC performance of our model and other Deepfake de-\ntection models on UADFV dataset. * FaceForensics++\n4.3. Effects of Data Processing During Classiﬁcation\nA major potential problem that affects our model accu-\nracy is the inherent problems that are in the face detection\nDL libraries (MTCNN, BlazeFace, and face\nrecognition).\nFigure 4, Figure 5 and Figure 6 show images that were mis-\nclassiﬁed by the DL libraries. The ﬁgures summarize our\npreliminary data preprocessing test on 200 videos selected\nrandomly from 10 folders. W e chose our test set video in\nall settings we can found in the DFDC dataset: indoor, out-\ndoor, dark room, bright room, subject sited, subject stand-\ning, speaking to side, speaking in front, a subject moving\nwhile speaking, gender, skin color, one person video, two\npeople video, a subject close to the camera, and subject\naway from the camera. For the preliminary test, we ex-\ntracted every frame of the videos and found the 637 nonface\nregion.\nFigure 4. face recognition non face region detection.\nFigure 5. BlazeFace non face region detection.\nFigure 6. MTCNN non face region detection.\nW e tested our model to check how its accuracy is af-\nfected without any attempt to remove these images, and\nour models’ accuracy dropped to 69.5 percent, and the loss\nvalue increased to 0.4.\nT o minimize non face regions and prevent wrong predic-\ntions, we used the three DL libraries and picked the best\nperforming library for our model, as shown in T able 4. As a\nsolution, we used face\nrecognition as a “ﬁlter” for the face\nimages detected by BlazeFace. W e chose face recognition\nbecause, in our investigation, it rejects more false-posit ive\n6\nthan the other two models. W e used face recognition for\nﬁnal Deepfake detection.\nDataset Blazeface f rec ** MTCNN\nDFDC 83.40% 91.50% 90.25%\nFaceSwap 56% 69% 63%\nFaceShifter 40% 46% 44%\nNeuralT extures 57% 60% 60%\nDeepFakeDetection 82% 91% 79.59\nDeepfake 87% 93% 81.63%\nFace2Face 54% 61% 69.39%\nUADF 74.50% 93.75% 88.16%\nT able 4. DL libraries comparison on Deepfake detection accu racy .\n** face recognition\n5. Conclusion\nDeepfakes open new possibilities in digital media, VR,\nrobotics, education, and many other ﬁelds. On another spec-\ntrum, they are technologies that can cause havoc and distrus t\nto the general public. In light of this, we have designed and\ndeveloped a generalized model for Deepfake video detec-\ntion using CNNs and Transformer, which we named Con-\nvolutional V ison Transformer. W e called our model a gen-\neralized model for three reasons. 1) Our ﬁrst reason arises\nfrom the combined learning capacity of CNNs and Trans-\nformer. CNNs are strong at learning local features, while\nTransformers can learn from local and global feature maps.\nThis combined capacity enables our model to correlate ev-\nery pixel of an image and understand the relationship be-\ntween nonlocal features. 2) W e gave equal emphasis on our\ndata preprocessing during training and classiﬁcation. 3) W e\nused the largest and most diverse dataset for Deepfake de-\ntection.\nThe CV iT model was trained on a diverse collection of\nfacial images that were extracted from the DFDC dataset.\nThe model was tested on 400 DFDC videos and has\nachieved an accuracy of 91.5 percent. Still, our model has a\nlot of room for improvement. In the future, we intend to ex-\npand on our current work by adding other datasets released\nfor Deepfake research to make it more diverse, accurate,\nand robust.\nReferences\n[1] Darius Afchar, Vincent Nozick, Junichi Y amagishi, and I sao\nEchizen. MesoNet: a Compact Facial Video Forgery Detec-\ntion Network. pages 1–7, 2018.\n[2] Shruti Agarwal, Hany Farid, Y uming Gu, Mingming He,\nKoki Nagano, and Hao Li. Protecting W orld Leaders Against\nDeep Fakes. In CVPR W orkshops, 2019.\n[3] Charu C. Aggarwal. Neural Networks and Deep Learning:\nA T extbook. Springer International Publishing, Switzerland,\n2020.\n[4] Md Zahangir Alom, T arek M. T aha, Chris Y akopcic, Ste-\nfan W estberg, Paheding Sidike, Mst Shamima Nasrin, Mah-\nmudul Hasan, Brian C. V an Essen, Abdul A. S. A wwal, and\nVijayan K. Asari. A State-of-the-Art Survey on Deep Learn-\ning Theory and Architectures. Electronics, 8(3):292, 2019.\n[5] V alentin Bazarevsky , Y ury Kartynnik, Andrey V akunov ,\nKarthik Raveendran, and Matthias Grundmann. BlazeFace:\nSub-millisecond Neural Face Detection on Mobile GPUs.\narXiv preprint arXiv:1907.05047v2 , 2019.\n[6] Irwan Bello, Barret Zoph, Ashish V aswani, Jonathon Shle ns,\nand Quoc V . Le. Attention augmented convolutional net-\nworks. In 2019 IEEE/CVF International Conference on\nComputer V ision (ICCV) , pages 3285–3294, 2019.\n[7] A vishek Joey Bose and Parham Aarabi. Virtual Fakes: Deep -\nFakes for Virtual Reality . In 2019 IEEE 21st International\nW orkshop on Multimedia Signal Processing (MMSP) , pages\n1–1. IEEE, 2019.\n[8] Andrew P . Bradley . The use of the area under the ROC curve\nin the evaluation of machine learning algorithms. P attern\nRecognition, 30(7):1145–1159, 1997.\n[9] John Brandon. T errifying high-tech porn: Creepy\n‘deepfake’ videos are on the rise, 2018. A vailable\nat https://www .foxnews.com/tech/terrifying-high-tech -porn-\ncreepy-deepfake-videos-are-on-the-rise.\n[10] Joshua Brockschmidt, Jiacheng Shang, and Jie Wu. On the\nGenerality of Facial Forgery Detection. In 2019 IEEE 16th\nInternational Conference on Mobile Ad Hoc and Sensor Sys-\ntems W orkshops (MASSW) , pages 43–47. IEEE, 2019.\n[11] Polychronis Charitidis, Giorgos Kordopatis-Zilos, S ymeon\nPapadopoulos, and Ioannis Kompatsiaris. Investigating\nthe Impact of Pre-processing and Prediction Aggrega-\ntion on the DeepFake Detection T ask. arXiv preprint\narXiv:2006.07084v1, 2020.\n[12] Bobby Chesney and Danielle Citron. Deep Fakes A Loom-\ning Challenge for Privacy , Democracy , and National Secu-\nrity , 2019. A vailable at https://ssrn.com/abstract=3213 954.\n[13] Umur A ybars Ciftci, Ilke Demir, and Lijun Yin. Fake-\nCatcher: Detection of Synthetic Portrait Videos using Bio-\nlogical Signals. arXiv preprint arXiv:1901.02212v2 , 2019.\n[14] Sourabh Dhere, Suresh B. Rathod, Sanket Aarankalle, Y a sh\nLad, and Megh Gandhi. A Review on Face Reenactment\nT echniques. In 2020 International Conference on Industry\n4.0 T echnology (I4T ech), pages 191–194, Pune, India, 2020.\nIEEE.\n[15] Chris Donahue, Julian J. McAuley , and Miller S. Puckett e.\nAdversarial Audio Synthesis. In 7th International Confer-\nence on Learning Representations, ICLR 2019, New Or-\nleans, LA, USA, May 6-9, 2019 , New Y ork, NY , USA, 2019.\nOpenReview .net.\n[16] Alexey Dosovitskiy , Lucas Beyer, Alexander Kolesniko v ,\nDirk W eissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly , Jakob Uszkoreit, and Neil Houlsby . An Image is\nW orth 16x16 W ords: Transformers for Image Recognition at\nScale. arXiv preprint arXiv:2010.11929v1 , 2020.\n[17] Adam Geitgey . The world’s simplest facial recogni-\ntion api for Python and the command line. A vailable at\nhttps://github.com/ageitgey/face_recognition.\n7\n[18] Ian J. Goodfellow , Jean Pouget-Abadie, Mehdi Mirza, Bi ng\nXu, David W arde-Farley , Sherjil Ozairy , Aaron Courville,\nand Y oshua Bengio. Generative Adversarial Nets. In Pro-\nceedings of the 27th International Conference on Neural In-\nformation Processing Systems - V olume 2 , page 2672–2680,\nCambridge, MA, USA, 2014. MIT Press.\n[19] Arushi Handa, Prerna Garg, and Vijay Khare. Masked\nNeural Style Transfer using Convolutional Neural Net-\nworks. In 2018 International Conference on Recent Innova-\ntions in Electrical, Electronics Communication Engineeri ng\n(ICRIEECE), pages 2099–2104, 2018.\n[20] Rahul Haridas and Jyothi R L. Convolutional Neural Net-\nworks: A Comprehensive Survey . International Journal\nof Applied Engineering Research (IJAER) , 14(03):780–789,\n2019.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In 2016\nIEEE Conference on Computer V ision and P attern Recog-\nnition (CVPR) , pages 770–778. IEEE, 2016.\n[22] Y ongjun Hong, Uiwon Hwang, Jaeyoon Y oo, and Sungroh\nY oon. How Generative Adversarial Networks and Their\nV ariants W ork: An Overview. volume 52, New Y ork, NY ,\nUSA, 2019. Association for Computing Machinery .\n[23] He Huang, Phillip S. Y u, and Changhu W ang. An Introduc-\ntion to Image Synthesis with Generative Adversarial Nets.\narXiv preprints arXiv:1803.04469v1 , 2018.\n[24] Xun Huang, Ming-Y u Liu, Serge Belongie, and Ming-Y u\nLiu. Multimodal Unsupervised Image-to-Image Translation .\nIn Computer V ision – ECCV 2018 , pages 179–196, Cham,\n2018. Springer International Publishing.\n[25] T ackHyun Jung, SangW on Kim, and KeeCheon Kim. Deep-\nVision: Deepfakes Detection Using Human Eye Blinking\nPattern. IEEE Access , 8:83144–83154, 2020.\n[26] T ero Karras, Samuli Laine, and Timo Aila. A Style-\nBased Generator Architecture for Generative Adversarial\nNetworks. arXiv preprints arXiv:1812.04948 , 2018.\n[27] Hasam Khalid and Simon S. W oo. OC-FakeDect: Classify-\ning Deepfakes Using One-class V ariational Autoencoder. In\n2020 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition W orkshops (CVPRW) , pages 2794–2803,\n2020.\n[28] Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja ,\nand Pankaj W asnik. Fake face detection methods: Can they\nbe generalized? In 2018 International Conference of the Bio-\nmetrics Special Interest Group (BIOSIG) , pages 1–6. IEEE,\n2018.\n[29] Junyaup Kim, Siho Han, and Simon S. W oo. Classifying\nGenuine Face images from Disguised Face Images. In 2019\nIEEE International Conference on Big Data (Big Data) ,\npages 6248–6250, 2019.\n[30] Pavel Korshunov and Sebastien Marcel. DeepFakes: a New\nThreat to Face Recognition? Assessment and Detection.\narXiv preprints arXiv:1812.08685 , 2018.\n[31] Alex Krizhevsky , Ilya Sutskever, and Geoffrey E. Hinto n.\nImageNet Classiﬁcation with Deep Convolutional Neural\nNetworks. Commun. ACM , 60(6):84–90, 2017.\n[32] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caba llero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan T ejani, Johannes T otz, Zehan W ang, and W en-\nzhe Shi. Photo-Realistic Single Image Super-Resolution\nUsing a Generative Adversarial Network. arXiv preprint\narXiv:1609.04802v5, 2017.\n[33] Y uezun Li, Ming-Ching Chang, and Siwei Lyu. In Ictu\nOculi: Exposing AI Generated Fake Face Videos by Detect-\ning Eye Blinking. arXiv preprint arXiv:1806.02877v2 , 2018.\n[34] Y uezun Li and Siwei Lyu. Exposing DeepFake Videos\nBy Detecting Face W arping Artifacts. arXiv preprint\narXiv:1811.00656v3, 2019.\n[35] Arun Mallya, Ting-Chun W ang, Karan Sapra, and Ming-Y u\nLiu. W orld-Consistent Video-to-Video Synthesis. In Com-\nputer V ision – ECCV 2020 , pages 359–378, Cham, 2020.\nSpringer International Publishing.\n[36] Brais Martinez, Michel F . V alstar, Bihan Jiang, and Maj a\nPantic. Automatic Analysis of Facial Actions: A Survey .\nIEEE Transactions on Affective Computing , 10(3):325–347,\n2019.\n[37] Yisroel Mirsky and W enke Lee. The Creation and Detectio n\nof Deepfakes: A Survey. ACM Comput. Surv . , 54(1), 2021.\n[38] Daniel Mas Montserrat, Hanxiang Hao, S. K. Y arlagadda,\nSriram Baireddy , Ruiting Shao, Janos Horvath, Emily Bar-\ntusiak, Justin Y ang, David Guera, Fengqing Zhu, and Ed-\nward J. Delp. Deepfakes Detection with Automatic Face\nW eighting. In 2020 IEEE/CVF Conference on Computer V i-\nsion and P attern Recognition W orkshops (CVPRW) , pages\n2851–2859, 2020.\n[39] Ryota Natsume, T atsuya Y atagawa, and Shigeo Morishima .\nRSGAN: Face Swapping and Editing Using Face and Hair\nRepresentation in Latent Spaces. In ACM SIGGRAPH 2018\nP osters, SIGGRAPH ’18, New Y ork, NY , USA, 2018. Asso-\nciation for Computing Machinery .\n[40] Huy H. Nguyen, Ngoc-Dung T . Tieu, Hoang-Quoc Nguyen-\nSon, Vincent Nozick, Junichi Y amagishi, and Isao Echizen.\nModular Convolutional Neural Network for Discriminating\nbetween Computer-Generated Images and Photographic Im-\nages. In Proceedings of the 13th International Conference on\nAvailability, Reliability and Security , New Y ork, NY , USA,\n2018. Association for Computing Machinery .\n[41] Huy H. Nguyen, Junichi Y amagishi, and Isao Echizen.\nCapsule-forensics: Using Capsule Networks to Detect\nForged Images and Videos. In ICASSP 2019 - 2019 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) , pages 2307–2311, 2019.\n[42] Thanh Thi Nguyen, Cuong M. Nguyen, Dung Tien Nguyen,\nDuc Thanh Nguyen, and Saeid Nahavandi. Deep Learn-\ning for Deepfakes Creation and Detection. arXiv preprint\narXiv:1909.11573v1, 2019.\n[43] Y uval Nirkin, Y osi Keller, and T al Hassner. FSGAN: Sub-\nject Agnostic Face Swapping and Reenactment. In 2019\nIEEE/CVF International Conference on Computer V ision,\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019 , pages 7183–7192. IEEE, 2019.\n[44] OpenAI. OpenAI API, 2020. A vailable at\nhttps://openai.com/blog/openai-api.\n[45] T aesung Park, Ming-Y u Liu, Ting-Chun W ang, and Jun-Y an\nZhu. Semantic Image Synthesis With Spatially-Adaptive\n8\nNormalization. In 2019 IEEE/CVF Conference on Computer\nV ision and P attern Recognition (CVPR) , pages 2332–2341.\nIEEE, 2019.\n[46] Hai X. Pham, Y uting W ang, and Vladimir Pavlovic. Gen-\nerative Adversarial T alking Head: Bringing Portraits to Li fe\nwith a W eakly Supervised Neural Network. arXiv preprint\narXiv:1803.07716, 2018.\n[47] K R Prajwal, Rudrabha Mukhopadhyay , Vinay P . Nambood-\niri, and C V Jawahar. A Lip Sync Expert Is All Y ou Need for\nSpeech to Lip Generation In the Wild , page 484–492. As-\nsociation for Computing Machinery , New Y ork, NY , USA,\n2020.\n[48] Mike Price and Matt Price. Playing Of-\nfense and Defense with Deepfakes, 2019.\nA vailable at lhttps://www .blackhat.com/us-\n19/brieﬁngs/schedule/playing-offense-and-defense-wi th-\ndeepfakes-14661.\n[49] Prajwal K R, Rudrabha Mukhopadhyay , Jerin Philip, Ab-\nhishek Jha, Vinay Namboodiri, and C V Jawahar. T o-\nwards Automatic Face-to-Face Translation. In the 27th ACM\nInternational Conference on Multimedia (MM ’19) , page\n1428–1436, New Y ork, NY , USA, 2019. Association for\nComputing Machinery .\n[50] Md. Shohel Rana and Andrew H. Sung. DeepfakeStack: A\nDeep Ensemble-based Learning T echnique for Deepfake De-\ntection. In 2020 7th IEEE International Conference on Cyber\nSecurity and Cloud Computing (CSCloud)/2020 6th IEEE\nInternational Conference on Edge Computing and Scalable\nCloud (EdgeCom) , pages 70–75, 2020.\n[51] Kuniaki Saito, Kate Saenko, and Ming-Y u Liu. COCO-\nFUNIT: Few-Shot Unsupervised Image Translation with a\nContent Conditioned Style Encoder. In Computer V ision –\nECCV 2020 , pages 382–398, Cham, 2020. Springer Interna-\ntional Publishing.\n[52] Karen Simonyan and Andrew Zisserman. V ery Deep Con-\nvolutional Networks for Large-Scale Image Recognition. In\nY oshua Bengio and Y ann LeCun, editors, 3rd International\nConference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Proceed-\nings, 2015.\n[53] Supasorn Suwajanakorn, Steven M. Seitz, and Ira\nKemelmacher-Shlizerman. Synthesizing Obama: Learning\nLip Sync from Audio. ACM Trans. Graph. , 36(4):780–789,\n2017.\n[54] Justus Thies, Michael Zollh ¨ ofer, Marc Stamminger, Ch ris-\ntian Theobalt, and Matthias Nießner. Face2Face: Real-Time\nFace Capture and Reenactment of RGB Videos. Commun.\nACM, 62(1):96–104, 2018.\n[55] Timesler. Pretrained Pytorch face detection (MTCNN)\nand recognition (InceptionResnet) models. A vailable at\nhttps://github.com/timesler/facenet-pytorch.\n[56] Ruben T olosana, Ruben V era-Rodriguez, Julian Fierrez ,\nA ythami Morales, and Javier Ortega-Garcia. DeepFakes and\nBeyond: A Survey of Face Manipulation and Fake Detection.\nInf. Fusion , 64:131–148, 2020.\n[57] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko -\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is All Y ou Need. In Proceedings\nof the 31st International Conference on Neural Information\nProcessing Systems , NIPS’17, page 6000–6010. Curran As-\nsociates Inc., 2017.\n[58] Konstantinos V ougioukas, Stavros Petridis, and Maja P an-\ntic. Realistic Speech-Driven Facial Animation with GANs.\nInternational Journal of Computer V ision , 128:1398–1413,\n2020.\n[59] Ting-Chun W ang, Ming-Y u Liu, Jun-Y an Zhu, Guilin Liu,\nAndrew T ao, Jan Kautz, and Bryan Catanzaro. Video-\nto-Video Synthesis. In Proceedings of the 32nd Interna-\ntional Conference on Neural Information Processing Sys-\ntems, NIPS’18, page 1152–1164, Red Hook, NY , USA,\n2018. Curran Associates Inc.\n[60] M. Arif W ani, Farooq Ahmad Bhat, Saduf Afzal, and\nAsif Iqbal Khan. Advances in Deep Learning , volume 57\nof Studies in Big Data . Springer Nature, Singapore, 2020.\n[61] Egor Zakharov , Aliaksandra Shysheya, Egor Burkov , and\nVictor Lempitsky . Few-Shot Adversarial Learning of\nRealistic Neural T alking Head Models. arXiv preprint\narXiv:1905.08233v2, 2019.\n[62] Lilei Zheng, Ying Zhang, and Vrizlynn L.L. Thing. A surv ey\non image tampering and its detection in real-world photos.\nElsevier, 58:380–399, 2018.\n[63] Jun-Y an Zhu, T aesung Park, Phillip Isola, and Alexei A.\nEfros. Unpaired Image-to-Image Translation Using Cycle-\nConsistent Adversarial Networks. In 2017 IEEE Interna-\ntional Conference on Computer V ision (ICCV) , pages 2242–\n2251, 2017.\n9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.662025511264801
    },
    {
      "name": "Transformer",
      "score": 0.5774276256561279
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5207809805870056
    },
    {
      "name": "Computer vision",
      "score": 0.4951125681400299
    },
    {
      "name": "Engineering",
      "score": 0.11318275332450867
    },
    {
      "name": "Electrical engineering",
      "score": 0.0804833471775055
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1443707",
      "name": "Jimma University",
      "country": "ET"
    }
  ],
  "cited_by": 137
}