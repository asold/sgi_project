{
  "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
  "url": "https://openalex.org/W4385571325",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2161984462",
      "name": "Jennifer Hu",
      "affiliations": [
        "Institute of Cognitive and Brain Sciences",
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2902098333",
      "name": "Sammy Floyd",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences",
        "Sarah Lawrence College"
      ]
    },
    {
      "id": "https://openalex.org/A1995351802",
      "name": "Olessia Jouravlev",
      "affiliations": [
        "Carleton University"
      ]
    },
    {
      "id": "https://openalex.org/A2071363772",
      "name": "Evelina Fedorenko",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences",
        "McGovern Institute for Brain Research"
      ]
    },
    {
      "id": "https://openalex.org/A2143199609",
      "name": "Edward Gibson",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100501376",
    "https://openalex.org/W2322554741",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2398427208",
    "https://openalex.org/W3111044367",
    "https://openalex.org/W4220985655",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2027970824",
    "https://openalex.org/W3035287090",
    "https://openalex.org/W1749601410",
    "https://openalex.org/W3041623293",
    "https://openalex.org/W1970491474",
    "https://openalex.org/W2093410327",
    "https://openalex.org/W4225090118",
    "https://openalex.org/W4231480720",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W4237236107",
    "https://openalex.org/W3035599593",
    "https://openalex.org/W2963372062",
    "https://openalex.org/W2963542100",
    "https://openalex.org/W2154851666",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385574160",
    "https://openalex.org/W2012803076",
    "https://openalex.org/W2373419227",
    "https://openalex.org/W3030833841",
    "https://openalex.org/W1993979041",
    "https://openalex.org/W3133527511",
    "https://openalex.org/W655927372",
    "https://openalex.org/W2159035740",
    "https://openalex.org/W4250133014",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2885800924",
    "https://openalex.org/W3013060191",
    "https://openalex.org/W2264742718",
    "https://openalex.org/W3176448872",
    "https://openalex.org/W1878980182",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W1976589030",
    "https://openalex.org/W3193548172",
    "https://openalex.org/W2748523847",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2131843096",
    "https://openalex.org/W2618295620",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W3171213613",
    "https://openalex.org/W2785836990",
    "https://openalex.org/W2790062600",
    "https://openalex.org/W2183724832",
    "https://openalex.org/W4308427224",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W1975348552",
    "https://openalex.org/W2964306921",
    "https://openalex.org/W2058081143",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2102411706",
    "https://openalex.org/W2405730947",
    "https://openalex.org/W1532059969",
    "https://openalex.org/W4385573818",
    "https://openalex.org/W1930725873",
    "https://openalex.org/W4254519752",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W4226016984",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3167919460",
    "https://openalex.org/W2136086156",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W2900456377",
    "https://openalex.org/W2893002232",
    "https://openalex.org/W2942499223",
    "https://openalex.org/W2000129504",
    "https://openalex.org/W2943694311",
    "https://openalex.org/W4220889970",
    "https://openalex.org/W4306247398",
    "https://openalex.org/W2889107415",
    "https://openalex.org/W2970536767",
    "https://openalex.org/W2947371815",
    "https://openalex.org/W3181653988",
    "https://openalex.org/W1576350636",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4285105218",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2905898952",
    "https://openalex.org/W4286987939"
  ],
  "abstract": "Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4194–4213\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nA fine-grained comparison of pragmatic language understanding\nin humans and language models\nJennifer Hu1, Sammy Floyd1,2, Olessia Jouravlev3, Evelina Fedorenko1,4, Edward Gibson1\n1Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology\n2Department of Psychology, Sarah Lawrence College\n3Department of Cognitive Science, Carleton University\n4McGovern Institute for Brain Research, Massachusetts Institute of Technology\n{jennhu,samfloyd,evelina9,egibson}@mit.edu\nolessiajouravlev@cunet.carleton.ca\nAbstract\nPragmatics and non-literal language under-\nstanding are essential to human communica-\ntion, and present a long-standing challenge for\nartificial language models. We perform a fine-\ngrained comparison of language models and\nhumans on seven pragmatic phenomena, us-\ning zero-shot prompting on an expert-curated\nset of English materials. We ask whether\nmodels (1) select pragmatic interpretations of\nspeaker utterances, (2) make similar error pat-\nterns as humans, and (3) use similar linguistic\ncues as humans to solve the tasks. We find\nthat the largest models achieve high accuracy\nand match human error patterns: within incor-\nrect responses, models favor literal interpreta-\ntions over heuristic-based distractors. We also\nfind preliminary evidence that models and hu-\nmans are sensitive to similar linguistic cues.\nOur results suggest that pragmatic behaviors\ncan emerge in models without explicitly con-\nstructed representations of mental states. How-\never, models tend to struggle with phenomena\nrelying on social expectation violations.\n1 Introduction\nNon-literal language understanding is an essential\npart of communication. For example, in everyday\nconversations, humans readily comprehend the non-\nliteral meanings of metaphors (My new coworker\nis a block of ice ), polite deceits ( I love the gift ),\nindirect requests (It’s a bit cold in this room), and\nirony (Classy pajamas, dude!). These phenomena\nfall under the broad label of pragmatics, which\nencompasses the aspects of meaning that go beyond\nthe literal semantics of what is said (Horn, 1972;\nGrice, 1975; Yule, 1996; Levinson, 2000).\nA long-standing challenge for NLP is to build\nmodels that capture human pragmatic behaviors.\nCode and data: https://github.com/jennhu/lm-pragmatics\nThe remarkable abilities of modern language mod-\nels (LMs) have triggered a recent effort to investi-\ngate whether such models capture pragmatic mean-\ning, both through philosophical arguments (Bisk\net al., 2020; Bender and Koller, 2020; Potts, 2020;\nMichael, 2020) and empirical evaluations (Jeretic\net al., 2020; Zheng et al., 2021; Tong et al., 2021;\nLiu et al., 2022; Ruis et al., 2022; Stowe et al.,\n2022). However, prior empirical studies have pri-\nmarily evaluated LMs based on a binary distinction\nbetween pragmatic and non-pragmatic responses,\nproviding limited insights into models’ weaknesses.\nA model could fail to reach the target pragmatic\ninterpretation in multiple ways – for example, by\npreferring a literal interpretation, or by preferring a\nnon-literal interpretation that violates certain social\nnorms. Understanding these error patterns can sug-\ngest specific directions for improving the models,\nand foreshadow where pragmatics might go awry\nin user-facing settings (e.g., Saygin and Cicekli,\n2002; Dombi et al., 2022; Kreiss et al., 2022).\nFrom a cognitive perspective, understanding the\npragmatic abilities of LMs could also offer insights\ninto humans. Human pragmatic language compre-\nhension involves a variety of mechanisms, such as\nbasic language processing, knowledge of cultural\nand social norms (Trosborg, 2010), and reason-\ning about speakers’ mental states (Brennan et al.,\n2010; Enrici et al., 2019; Rubio-Fernandez, 2021).\nHowever, it remains an open question when lan-\nguage understanding relies on explicit mentaliz-\ning – which may be cognitively effortful – versus\nlower-cost heuristics (e.g., Butterfill and Apperly,\n2013; Heyes, 2014). Because LMs lack explicit,\nsymbolic representations of mental states, they can\nserve as a tool for investigating whether pragmatic\ncompetence can arise without full-blown mentaliz-\ning (e.g., belief updates in the Rational Speech Act\nframework; Frank and Goodman, 2012).\n4194\nIn this paper, we perform a fine-grained compar-\nison of humans and LMs on pragmatic language\nunderstanding tasks. Adopting the approach of\ntargeted linguistic evaluation (e.g., Linzen et al.,\n2016; Futrell et al., 2019; Hu et al., 2020), our\nanalysis serves two goals: assessing the pragmatic\ncapabilities of modern LMs, and revealing whether\npragmatic behaviors emerge without explicitly con-\nstructed mental representations. Our test materials\nare a set of English multiple-choice questions cu-\nrated by expert researchers (Floyd et al., In prep),\ncovering seven diverse pragmatic phenomena. We\nuse zero-shot prompting to evaluate models with\nvarying sizes and training objectives: GPT-2 (Rad-\nford et al., 2019), Tk-Instruct (Wang et al., 2022),\nFlan-T5 (Chung et al., 2022), and InstructGPT\n(Ouyang et al., 2022).\nThrough model analyses and human experi-\nments, we investigate the following questions: (1)\nDo models recover the hypothesized pragmatic in-\nterpretation of speaker utterances? (2) When mod-\nels do not select the target response, what errors\ndo they make – and how do these error patterns\ncompare to those of humans? (3) Do models and\nhumans use similar cues to arrive at pragmatic\ninterpretations? We find that Flan-T5 (XL) and\nOpenAI’s text-davinci-002 achieve high accuracy\nand mirror the distribution of responses selected\nby humans. When these models are incorrect, they\ntend to select the incorrect literal (or straightfor-\nward) answer instead of distractors based on low-\nlevel heuristics. We also find preliminary evidence\nthat models and humans are sensitive to similar\nlinguistic cues. Our results suggest that some prag-\nmatic behaviors emerge in models without explic-\nitly constructed representations of agents’ mental\nstates. However, models perform poorly on humor,\nirony, and conversational maxims, suggesting a dif-\nficulty with social conventions and expectations.\n2 Related work\nPrior work has evaluated LMs’ ability to recognize\nnon-literal interpretations of linguistic input, such\nas scalar implicature (Jeretic et al., 2020; Schuster\net al., 2020; Li et al., 2021) or figurative language\n(Tong et al., 2021; Liu et al., 2022; Gu et al., 2022;\nStowe et al., 2022). In a broad-scale evaluation,\nZheng et al. (2021) test five types of implicatures\narising from Grice’s (1975) conversational max-\nims, and evaluate their models after training on the\ntask. In our work, we consider Gricean implica-\ntures as one of seven phenomena, and we evaluate\npre-trained LMs without fine-tuning on our tasks.\nSimilar to our work, Ruis et al. (2022) also use\nprompting to evaluate LMs on pragmatic interpre-\ntation tasks. They formulate implicature tests as\nsentences ending with “yes” or “no” (e.g., “Esther\nasked “Can you come to my party on Friday?” and\nJuan responded “I have to work”, which means\nno.”). A model is considered pragmatic if it assigns\nhigher probability to the token that makes the sen-\ntence consistent with an implicature. In our work,\nmodels must select from multiple interpretations,\nenabling a detailed error analysis and comparison\nto humans. Ruis et al.’s materials also focus on in-\ndirect question answering as an implicature trigger,\nwhereas we consider a broader range of pragmatic\nphenomena and utterance types.\nSince pragmatic language understanding often\ndraws upon knowledge of social relations, our tasks\nare conceptually related to benchmarks for evalu-\nating social commonsense (e.g., Sap et al., 2019;\nZadeh et al., 2019). These evaluations focus on the\ninterpretation of actions and events, whereas we\nfocus on the interpretation of speaker utterances.\nAnother hypothesized component of pragmatics is\nTheory of Mind (ToM; Leslie et al., 2004; Apperly,\n2011), or the ability to reason about others’ mental\nstates. Benchmarks for evaluating ToM in models\n(e.g., Nematzadeh et al., 2018; Le et al., 2019; Sap\net al., 2022) primarily focus on false-belief tasks\n(Baron-Cohen et al., 1985), which assess whether\na model can represent the beliefs of another agent\nthat are factually incorrect but consistent with that\nagent’s observations. LMs have been shown to suc-\nceed on some ToM tests (Kosinski, 2023) while\nfailing on others (Sap et al., 2022; Ullman, 2023).\n3 Evaluation materials\n3.1 Overview of stimuli\nOur evaluation materials are taken from Floyd\net al.’s (In prep) experiments,1 covering seven phe-\nnomena. Each item is a multiple choice question,\nwith answer options representing different types of\ninterpretation strategies. For most of the tasks, the\nquestion has three parts: a short story context (1-3\nsentences), an utterance by one of the characters,\nand a question about what the character intended to\nconvey.2 Table 1 shows an example item for each\n1Materials can be found at https://osf.io/6abgk/?view_\nonly=42d448e3d0b14ecf8b87908b3a618672.\n2The exceptions are Humor and Coherence.\n4195\nTask Example query Example answer options\nDeceits Henry is sitting at his desk and watch-\ning TV , and reluctantly switches off the\nTV with the remote control and picks\nup a textbook. Shortly after, his mother\ncomes in the room and asks, \"What\nhave you been doing up here?\" Henry\nresponds: \"Reading.\" Why has Henry\nresponded in such a way?\n1. Correct He does not want to get into trouble for not studying.\n2. Literal He has been reading for some time.\n3. DistractorLexicalOverlap He does not want to offend his mom by not reading the books\nthat she gave him.\n4. DistractorSocialConvention He wants his mom to believe that he has been watching TV .\nIndirect\nspeech\nNate is about to leave the house. His\nwife points at a full bag of garbage and\nasks: \"Are you going out?\" What might\nshe be trying to convey?\n1. Correct She wants Nate to take the garbage out.\n2. Literal She wants to know Nate’s plans.\n3. DistractorAssociative She wants Nate to bring his friends over.\n4. DistractorLexicalOverlap She wants Nate to spend more time with the family.\nIrony It is a holiday. Stefan and Kim are sit-\nting in the backseat of the car. They are\nfighting all the time. Their father says:\n\"Oh, it is so pleasant here.\" What did\nthe father want to convey?\n1. Correct He does not want to listen to his kids’ arguments.\n2. Literal He enjoys listening to his kids fighting.\n3. DistractorAssociative AC gives them some needed cool.\n4. DistractorNonSequitur He remembers about his wife’s birthday.\nMaxims Leslie and Jane are chatting at a cof-\nfee shop. Leslie asks, \"Who was that\nman that I saw you with last night?\"\nJane responds, \"The latte is unbeliev-\nable here.\" Why has Jane responded\nlike this?\n1. Correct She does not want to discuss the topic that Leslie has raised.\n2. Literal She thinks that it is the best latte in the town.\n3. DistractorAssociative The man who Leslie saw makes unbelievable lattes.\n4. DistractorNonLiteral A coffee break is not a good time to discuss men.\nMetaphor Andrew and Bob were discussing the\ninvestment company where Andrew\nworks. Bob said: “The investors are\nsquirrels collecting nuts.” What does\nBob mean?\n1. Correct They buy stocks hoping for future profit.\n2. Literal Squirrels were hired to work in the company.\n3. DistractorNonLiteral The investors dress and eat well.\n4. DistractorNonSequitur Bob is allergic to nuts.\n5. DistractorPlausibleLiteral The investors enjoy picking nuts as much as squirrels do.\nHumor Martha walked into a pastry shop. After\nsurveying all the pastries, she decided\non a chocolate pie. \"I’ll take that one,\"\nMartha said to the attendant, \"the whole\nthing.\" \"Shall I cut it into four or eight\npieces?\" the attendant asked.\n1. Correct Martha said, \"Four pieces, please; I’m on a diet.\"\n2. Literal Martha said: \"Well, there are five people for dessert tonight, so eight pieces will\nbe about right.\"\n3. DistractorAssociative Martha said, \"You make the most delicious sweet rolls in town.\"\n4. DistractorFunny Then the attendant squirted whipped cream in Martha’s face.\n5. DistractorNeutral Martha said, \"My leg is hurting so much.\"\nCoherence Mary’s exam was about to start. Her\npalms were sweaty. 1. Correct Coherent\n2. Incorrect Incoherent\nTable 1: Sample item from each task in our evaluation. All items are originally curated by Floyd et al. (In prep).\ntask, with annotated answer options. Green labels\nindicate the target pragmatic interpretation.3 Blue\nlabels indicate the literal interpretation. Red labels\nindicate incorrect non-literal interpretations, which\nare based on heuristics such as lexical similarity to\nthe story, thus serving as distractor options.\nEach task has 20-40 items, which were manu-\nally curated by expert researchers to cover a broad\nrange of non-literal phenomena and elicit individ-\nual differences among humans. The stimuli were\nnot specifically designed to require Theory of Mind\n3We refer to these answer options as “Correct” throughout\nthe paper. However, these answers are only “correct” in the\nsense of a normative evaluation. We acknowledge the wide\nvariation in individual humans’ abilities and tendencies to use\nnon-literal language, which is not captured in our analyses.\nWe thank an anonymous reviewer for highlighting this point.\nreasoning (ToM). However, behavioral and neural\nevidence suggests that many of the tested phenom-\nena rely on mentalizing processes. In Section 3.2,\nwe briefly describe the role of ToM for each tested\nphenomenon, and how LMs’ training corpora may\nprovide linguistic cues to perform the tasks.\n3.2 Tested phenomena\nDeceits. Humans produce polite deceits (“white\nlies”) in the service of social and personal relation-\nships (e.g., Camden et al., 1984). Behavioral stud-\nies in young children suggest that understanding\nwhite lies requires interpretive ToM, or the abil-\nity to allow different minds to interpret the same\ninformation in different ways (Hsu and Cheung,\n2013). Furthermore, the tendency to produce white\n4196\nlies is linked to emotional understanding abilities,\n(Demedardi et al., 2021), and moral judgments\nabout white lies are linked to second-order false-\nbelief understanding (Vendetti et al., 2019).\nThe Deceits task presents a story with a white lie,\nand asks why the speaker has used this utterance.\nThe underlying intentions behind polite deceits are\nrarely explicitly explained in text. As a result, it\nis unlikely that LMs learn a direct connection be-\ntween the utterance and the speaker’s intention dur-\ning training on static texts. However, instances\nof polite deceits in text corpora may be accompa-\nnied by descriptions of characters’ emotional states,\nwhich may indicate that speakers’ intentions differ\nfrom what is literally conveyed by their utterance.\nThis highlights the importance of context in inter-\npreting deceits, which we return to in Section 5.3.1.\nIndirect speech. Humans often use language in a\nperformative sense, such as indirectly requesting an\naction from other individuals (e.g., Austin, 1975;\nSearle, 1975). Indirect or polite speech compre-\nhension has been captured by Rational Speech Act\n(RSA; Frank and Goodman, 2012) models, which\ncharacterize listeners as performing Bayesian in-\nference about a speaker who chooses utterances\nbased on a tradeoff between epistemic and social\nutility (Brown and Levinson, 1987; Yoon et al.,\n2016, 2020; Lumer and Buschmeier, 2022).\nThe IndirectSpeech task presents a story with an\nindirect request, and asks what the speaker intends\nto convey. Like deceits, it’s unlikely that indirect\nspeech acts are explained in text data. However,\nindirect requests may be followed by descriptions\nof the completion of the implied request – for ex-\nample, that someone closed a window after hearing\nthe utterance “It’s cold in here”. Therefore, models\nmay learn relationships between the utterances and\ndesired outcomes through linguistic experience.\nIrony. Humans use irony to convey the opposite\nof the semantic content of their utterance (Booth,\n1974; Wilson and Sperber, 1992; Attardo, 2000;\nWilson and Sperber, 2012). As such, irony has\nlong been hypothesized to rely on social reasoning\nand perspective-taking (e.g., Happé, 1993; Andrés-\nRoqueta and Katsos, 2017). Indeed, human irony\ncomprehension behaviors are captured by Bayesian\nreasoning models that take into account speakers’\naffective goals (Kao and Goodman, 2014). In addi-\ntion, neuroimaging studies suggest that irony inter-\npretation relies on brain regions that are implicated\nin classic ToM tasks (Spotorno et al., 2012).\nThe Irony task presents a story with an ironic\nstatement, and asks what the character intends to\nconvey. While ironic statements are also rarely\nexplained in text, models could leverage accom-\npanying cues such as descriptions of characters’\nemotional states or a mismatch in sentiment.\nMaxims of conversation. Grice (1975) proposes\nthat communication follows a set of maxims: be\ntruthful; be relevant; be clear, brief, and orderly;\nand say as much as needed, and no more. A prevail-\ning theory is that listeners derive implicatures by\nexpecting speakers to be cooperative (i.e., abide by\nthe maxims) and reasoning about speakers’ beliefs\nand goals. Indeed, there is extensive evidence for\nRSA models capturing these implicatures, such as\nthose arising from the maxims of quantity (Potts\net al., 2016; Frank et al., 2018; Degen, 2023) and\nmanner (Bergen et al., 2016; Franke and Jäger,\n2016; Tessler and Franke, 2018).\nThe Maxims task presents a story with a charac-\nter flouting one of Grice’s maxims, and asks why\nthe character has responded in such a way. Based\non linguistic input, it may be easy for LMs to rec-\nognize when a speaker is flouting a maxim – for\nexample, if an utterance is particularly long, fea-\ntures an uncommon syntactic construction, or di-\nverges semantically from the context. However, it\nis unclear whether LMs will be able to recover the\nspeaker’s underlying intentions.\nMetaphor. Metaphors (Lakoff and Johnson,\n1980) are used to draw comparisons between en-\ntities in a non-literal sense. Metaphor understand-\ning has been hypothesized to require mentalizing\n(Happé, 1993), and fine-grained metaphor compre-\nhension behaviors are captured by RSA models\nwhere listeners and speakers reason about each oth-\ners’ beliefs and goals (Kao et al., 2014).\nThe Metaphor task presents a story with a\nmetaphor, and asks what the speaker intends to con-\nvey. For models, the challenges of metaphor com-\nprehension include accessing world knowledge and\nforming abstract relationships between domains.\nHowever, it is possible that the relevant proper-\nties of the entities under comparison could emerge\nthrough linguistic experience.\nHumor. Humor is one of the most distinctive\naspects of human conversation, reflecting com-\nmunicative goals with complex social function\n(Veatch, 1998; Martin and Ford, 2018). Neu-\n4197\nroimaging studies suggest that joke understanding\nis supported by regions in the ToM brain network\n(Kline Struhl et al., 2018). Behavioral tests also re-\nveal associations between ToM and humor abilities\n(Aykan and Nalçacı, 2018; Bischetti et al., 2019).\nThe Humor task presents a joke and asks which\npunchline makes the joke the funniest.4 Some the-\nories argue that humor is triggered by linguistic in-\ncongruency effects (e.g., Deckers and Kizer, 1975),\nwhich might be straightforward for LMs to detect.\nRecent work has also shown that LMs can explain\ncertain jokes (Chowdhery et al., 2022). However,\nsome of Floyd et al.’s Humor items require com-\nplex world knowledge – for example, that slicing\na pie into four versus eight pieces does not change\nthe total amount of pie (see Table 1). As such,\nselecting the funniest punchline is a nontrivial task.\nCoherence inferences. Humans also make prag-\nmatic inferences beyond the sentence level – for\nexample, by assuming that consecutive sentences\nform a logical or sequential relationship. Moss and\nSchunn (2015) and Jacoby and Fedorenko (2020)\nfind that constructing these discourse relationships\nloads on regions of the ToM brain network, sug-\ngesting a role of ToM in coherence inferences.\nThe Coherence task presents a pair of sentences,\nand asks whether the pair forms a coherent story.5\nWe assume that LMs’ training data, which consists\nof naturalistic text, is primarily coherent. There-\nfore, we expect LMs to be able to distinguish be-\ntween coherent and incoherent sentence pairs (for\nan in-depth study, see Beyer et al., 2021).\n4 Experiments\n4.1 Evaluation paradigm\nOur evaluation paradigm uses zero-shot prompting.\nPrompting can easily be adapted to all of our seven\ntasks, allowing us to compare performance across\ntasks within a model. Prompting also allows us to\npresent models with inputs that are nearly identical\nto the stimuli seen by humans in Floyd et al.’s\nexperiments, whereas other methods would require\nconverting the stimuli into task-specific formats.\nWe choose zero-shot prompts in order to evaluate\nthe knowledge that emerges through training, and\nnot through in-context adaptation to the task.\n4Unlike the other tasks, there is no speaker utterance.\n5This task differs from the others in that there is no speaker\nutterance, and the answer options are identical across items\n(“Coherent” or “Incoherent”).\nModel # parameters Training\nGPT-2 117M Autoregressive LM\nTk-Instruct (3B) 3B Multitask\nTk-Instruct (11B) 11B Multitask\nFlan-T5 (base) 250M Multitask\nFlan-T5 (XL) 3B Multitask\nInstructGPT-3 (ada) 350M (est.) Multitask, human feedback\ntext-davinci-002 Unknown FeedME\nTable 2: Models tested in our experiments.\nPrompt structure. Each prompt consists of two\nparts: task instructions, and a query. The instruc-\ntions are nearly identical to the instructions pre-\nsented to humans in Floyd et al.’s experiments,\nprepended with the keyword “Task:”. The only\nother modification is that the original instructions\nhad a final sentence of “Please answer as quickly\nas possible”, which we replaced with a sentence\nlike “The answer options are 1, 2, 3, or 4”.6\nFor all tasks except Humor, the query consists of\nthe scenario (prepended with keyword “Scenario:”)\nand question, and then the numbered answer op-\ntions (prepended with “Options:”). 7 The prompt\nconcludes with the keyword “Answer:”. Full exam-\nple prompts are given in Appendix A.\nEvaluation. To evaluate a model on a given item,\nwe feed the prompt to the model, and measure the\nmodel’s probability distribution over tokens condi-\ntioned on the prompt. We compare the probabilities\nof each answer token (e.g., “1”, “2”, “3”, or “4”)\nunder this distribution. The model is considered\ncorrect on a given item if it assigns highest prob-\nability to the correct answer token, among all the\npossible answer tokens for that item.\nWe generated 5 versions of each item by random-\nizing the order of answer options. This was done\nto control for the base probabilities of the answer\ntokens. Since we do not analyze generated text, the\nmodel results themselves are deterministic.\n4.2 Models\nWe test seven models across four model families,\nsummarized in Table 2.8 As a baseline, we first test\na base GPT-2 model (117M parameters; Radford\net al., 2019), which is trained on an autoregressive\nlanguage modeling objective.\nSecond, we test a set of models which are based\non T5 (Raffel et al., 2020) and instruction-finetuned\n6The exact answer options changed according to the task.\n7For the Humor task, the joke is prepended with “Joke:”,\nand the answer options are prepended with “Punchlines:”.\n8All non-OpenAI models were accessed via Huggingface\n(Wolf et al., 2020) and run on a single NVIDIA A100 GPU.\n4198\nCoherence Deceits Humor IndirectSpeech Irony Maxims Metaphor\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Proportion correct\nGPT-2\nTk-Instruct (3B)\nTk-Instruct (11B)\nFlan-T5 (base)\nFlan-T5 (XL)\nInstructGPT-3 (ada)\ntext-davinci-002\nHuman\nRandom\nFigure 1: Accuracy for each task. Error bars denote 95% CI. Dashed line indicates task-specific random baseline.\n102 103 104\nMillions of parameters (log scale)\n0.2\n0.3\n0.4\n0.5Mean accuracy\nGPT-2 Tk-Instruct (3B)\nTk-Instruct (11B)\nFlan-T5 (base)\nFlan-T5 (XL)\nInstructGPT-3 (ada)\nFigure 2: Mean accuracy vs. millions of parameters.\nVertical dashed line indicates 1 billion parameters. text-\ndavinci-002 was excluded from this analysis, as the\nnumber of parameters is unknown.\non a diverse collection of tasks (Wei et al., 2022).\nThis set of models consists of two Tk-Instruct\nmodels (3B and 11B; Wang et al., 2022), which\nwere fine-tuned on 1.6K tasks, and two Flan-T5\nmodels (base: 250M parameters; XL: 3B param-\neters; Chung et al., 2022), which were fine-tuned\non 1.8K tasks. The fine-tuning tasks cover a wide\nrange of categories, such as commonsense reason-\ning, translation, mathematics, and programming.\nFinally, we test two InstructGPT-based models\n(Ouyang et al., 2022) via the OpenAI API: text-\nada-001 (350M parameters), which we refer to as\nInstructGPT-3 (ada); and text-davinci-002, which\ncomes from the GPT-3.5 family of models. 9,10\nThese models are fine-tuned to follow instructions\nand align with human feedback.\nWe compare models to a baseline from 374 hu-\nmans, collected by Floyd et al. (In prep). Their\nexperiments presented multiple choice questions to\nhumans in nearly identical format to our prompts.\n5 Results\nWe now return to the three questions posed in the\nIntroduction, in each of the following subsections.\n9Parameter estimates come from https://blog.eleuther.ai/\ngpt3-model-sizes/. Although the size of text-davinci-002 is\nunknown, we assume that it is larger than InstructGPT-3 (ada).\n10The OpenAI model results might not be reproducible, but\n5.1 Do models choose the target pragmatic\ninterpretation?\nFigure 1 shows the proportion of trials where\nmodels and humans select the pragmatic answer.\nThe smallest models (GPT-2, Flan-T5 (base),\nInstructGPT-3 (ada)) fail to perform above chance.\nThe largest models (T k-Instruct (11B), Flan-T5\n(XL), text-davinci-002) perform above chance on\nall tasks (except T k-Instruct (11B) on Maxims),\nand in some cases near human-level. Overall, mod-\nels perform worst at the Humor, Irony, and Max-\nims tasks. Interestingly, these phenomena involve\nspeakers violating listeners’ expectations in some\nway: producing a funny punchline to a mundane\nstory (Humor), stating the direct opposite of the\nspeaker’s belief (Irony), or disobeying one of the\nassumed rules of conversation (Maxims). It may\nbe that models fail to represent certain social ex-\npectations that are maintained by human listeners.\nNext, we investigated the relationship between\nmodel size and accuracy. Figure 2 shows the mean\naccuracy achieved by each model (averaged across\ntasks) vs. millions of parameters. The line and error\nbars denote the mean and 95% CIs, while points\nrepresent individual models. We find a coarse ef-\nfect of model size: there is a stark jump in accuracy\nafter 1B parameters (dashed line). However, model\nsize does not fully explain variance in accuracy: all\nmodels with <1B parameters achieve similar ac-\ncuracy, and Flan-T5 (XL) outperforms Tk-Instruct\n(3B), despite both having 3B parameters.\n5.2 Do models and humans make similar\ntypes of errors?\nRecall from Section 3 that each item has a set of\nanswer options that correspond to different strate-\ngies (Table 1).11 In addition to the target pragmatic\nanswer (Correct), each item also has a plausible but\nunlikely literal answer (Literal), as well as distrac-\ntimestamps of API calls can be found in Appendix B.\n11The exception is Coherence, which is excluded here.\n4199\nCorrect Literal Distractor\nLexicalOverlap\nDistractor\nSocialConvention\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Response probability\nDeceits\nCorrect Literal Distractor\nAssociative\nDistractor\nFunny\nDistractor\nNeutral\nHumor\nCorrect Literal Distractor\nAssociative\nDistractor\nLexicalOverlap\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Response probability\nIndirectSpeech\nCorrect Literal Distractor\nAssociative\nDistractor\nNonSequitur\nIrony\nCorrect Literal Distractor\nAssociative\nDistractor\nNonLiteral\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Response probability\nMaxims\nGPT-2 Tk-Instruct (3B)\nTk-Instruct (11B)\nFlan-T5 (base)\nFlan-T5 (XL)\nInstructGPT-3 (ada)\ntext-davinci-002\nHuman\nRandom\nCorrect Literal Distractor\nNonLiteral\nDistractor\nNonSequitur\nDistractor\nPlausibleLiteral\nMetaphor\nFigure 3: Response distributions across models and humans. Answer options for each task are shown on the x-axis.\nFor models, y-axis denotes probability assigned to each answer option. For humans, y-axis denotes empirical\nfrequency of each answer option being selected. Error bars denote 95% CI. Dashed line indicates random baseline.\ntors based on lexical overlap or semantic associa-\ntions (Distractor*). For each item, we computed the\nhuman empirical distribution over answer choices,\nand compared it to models’ probability assigned to\nthe answer tokens (e.g., “1”, “2”, “3”, and “4”).\nFigure 3 shows the answer distributions for each\ntask. Across tasks, humans primarily select the\nCorrect option, occasionally the Literal option, and\nrarely the distractors. We find a similar pattern\nfor text-davinci-002, although the model is more\nlikely to select the Literal option in general. The\nother large models (T k-Instruct (11B), Flan-T5\n(XL)) also generally assign highest probability to\nthe Correct and Literal options, although the dis-\ntribution looks less human-like. The next-largest\nmodels (Tk-Instruct (3B), Flan-T5 (base)) prefer\nthe Literal option, and the remaining models (GPT-\n2, InstructGPT-3 (ada)) are at chance. These results\nshow that larger models consistently identify the lit-\neral interpretation of an utterance, suggesting that\ntheir pragmatic failures are unlikely to be explained\nby a failure to represent basic semantic meaning\n(for our test materials).\nHowever, even high-performing models occa-\nsionally do select the distractor answers, reveal-\ning interesting behaviors. For example, in the\nMetaphor task, text-davinci-002 and Flan-T5 (XL)\nprefer the DistractorPlausibleLiteral option – which\nis a figurative reading of the utterance – over the\nLiteral option – which is completely non-figurative.\nSimilarly, in the Humor task, text-davinci-002 is\nmuch more likely to select the DistractorFunny\noption over the other (non-humorous) distractors.\nThis suggests a coarse sensitivity to humor, even if\nthe model selects the human-preferred punchline\nonly 55% of the time (see Figure 1). We take this\nanalysis to illustrate the value of looking beyond\nbinary pragmatic/non-pragmatic response distinc-\ntions, and using controlled distractor items to eval-\nuate models’ abilities (e.g., McCoy et al., 2019).\n5.3 Are models and humans sensitive to\nsimilar linguistic cues?\nHaving found qualitatively similar response pat-\nterns between humans and models, we now ask\nhow models and humans arrive at pragmatic inter-\npretations, and whether they use similar types of\ninformation. We begin with a broad evaluation of\nthe extent to which models and humans rely on\nlinguistic context (Section 5.3.1). We then take a\n4200\nmore granular approach and ask whether model and\nhuman performance is correlated at the item level –\ni.e., if models and humans exhibit similar sensitiv-\nity to the cues that make a non-literal interpretation\nmore or less likely (Section 5.3.3).\n5.3.1 The role of context\nMany cues for enriched language understanding\ncome from the context in which the speaker makes\ntheir utterance. However, some aspects of non-\nliteral comprehension might arise given the utter-\nance in isolation, while others are highly sensitive\nto specific contextual details (e.g., Levinson, 2000).\nTherefore, we expect that the degree to which hu-\nmans rely on context to select non-literal interpre-\ntations will vary across the tested tasks.\nTo investigate this variation, we created a new\nset of stimuli by removing the context stories, leav-\ning only the speaker utterance and final question\n(e.g., Dan says, “The dog knocked it over. ” Why\nhas Dan responded in such a way?).12 We re-ran\nthe human experiment on 30 participants, follow-\ning the protocols of Floyd et al. (In prep)’s original\nexperiment using the no-context modified materi-\nals.13 We also re-ran the three models that achieved\nhighest accuracy on the original items: Tk-Instruct\n(11B), Flan-T5 (XL), and text-davinci-002.\nFigure 4 shows the mean accuracy difference\non the original versus no-context versions of each\nitem.14 We find that models and humans exhibit\na similar qualitative pattern: removing the story\nleads to the largest degradation for Irony, followed\nby Deceits and Maxims. This aligns with our in-\ntuitions, because in these cases, speakers’ utter-\nances can be interpreted either literally or as the\ncomplete opposite, based on the specific social sit-\nuation (e.g., “It is so pleasant here”). In contrast,\nthere are smaller degradations for IndirectSpeech\nand Metaphor. This suggests that some indirect\nrequests are conventionalized (e.g., “I am getting\ncold”), although their interpretations may be fa-\ncilitated by context (e.g., Gibbs, 1979). Similarly,\nthis suggests that metaphor interpretation may draw\nmore upon global knowledge than local context.\n5.3.2 Scrambling\nNext, we tested whether models rely on syntac-\ntic and discourse-level information from the con-\n12This manipulation is not compatible with the Humor and\nCoherence tasks, so they are excluded from this analysis.\n13Details can be found in Appendix C.1.\n14See Figure 6 in Appendix C.2 for comparison of raw\naccuracy scores on the original and no-context items.\nIronyDeceitsMaxims\nIndirectSpeech\nMetaphor\n0.00\n0.25\n0.50\n0.75\nAccuracy delta\n(original  no story)\nTk-Instruct (11B)\nFlan-T5 (XL)\ntext-davinci-002\nHuman\nFigure 4: Mean by-item difference in accuracy once\nstory context was removed.\ntext, or whether they can perform the tasks when\nordering cues are removed. We constructed two\nscrambled versions of each item by randomizing\nthe order of sentences and words. In both versions,\nthe instructions, final question (e.g., Why has Dan\nresponded in such a way? ), and answer options\nwere unmodified and remained in their original po-\nsitions. Again, we only tested the best-performing\nmodels on these items.\nWe found that models maintain reasonable per-\nformance for most tasks, with the notable exception\nof Metaphor (Figure 7; Appendix D). This robust-\nness to scrambling accords with prior evidence that\nmodels often rely on lexical information without\nhuman-like compositionality (e.g., Dasgupta et al.,\n2018; Nie et al., 2019; McCoy et al., 2019). We\nexpect that scrambling, especially at the word-level,\nwould likely disrupt human performance, but this\nremains an open empirical question. We leave an\ninvestigation of human performance to future work.\n5.3.3 Item-level alignment\nUp to this point, we analyzed differences across\nphenomena by averaging over items. However,\nthere is also variance within each phenomenon in\nthe types of cues that suggest how the utterances\nshould be interpreted. For example, some items\ncontain explicit descriptions of characters’ emo-\ntional states (e.g., “Sarah becomes angry”). If mod-\nels and humans leverage these cues in similar ways,\nthen we would expect to see correlations between\nmodel and human performance at the item level.\nFor each task and model, we compute the Pear-\nson correlation between by-item mean accuracy\nachieved by humans and by-item mean probability\nthat models assigned to the correct answer (Fig-\nure 5). In general, the larger models (T k-Instruct\n(11B), Flan-T5 (XL), text-davinci-002) are better\naligned with humans, and the strongest correla-\ntions occur for IndirectSpeech, Irony, Maxims, and\nMetaphor. This suggests that for those tasks, mod-\n4201\nCoherenceDeceitsHumorIndirectSpeechIronyMaximsMetaphor\nGPT-2\nTk-Instruct (3B)\nTk-Instruct (11B)\nFlan-T5 (base)\nFlan-T5 (XL)\nInstructGPT-3 (ada)\ntext-davinci-002\nModel\n* **\n** * * **\n* ** *** ** 0.2\n0.0\n0.2\n0.4\n0.6\nFigure 5: Pearson correlation coefficients between by-\nitem human accuracy and model probability of the cor-\nrect answer. Cells are marked with significance codes.\nels and humans are similarly sensitive to cues that\nmake a non-literal interpretation likely.\n6 Discussion\nWe used an expert-curated set of materials (Floyd\net al., In prep) to compare LMs and humans on\nseven pragmatic phenomena. We found that Flan-\nT5 (XL) and text-davinci-002 achieve high accu-\nracy and match human error patterns: within in-\ncorrect responses, these models tend to select the\nliteral interpretation of an utterance over heuristic-\nbased distractors. We also found preliminary evi-\ndence that LMs and humans are sensitive to similar\nlinguistic cues: model and human accuracy scores\ncorrelate at the item level for several tasks, and\ndegrade in similar ways when context is removed.\nOur results suggest that language models can\nconsistently select the pragmatic interpretation of a\nspeaker’s utterance – but how? The models tested\nin our experiments reflect a variety of learning pro-\ncesses through which pragmatic knowledge could\nemerge. GPT-2 is trained to learn the distribution\nof linguistic forms; the T k-Instruct and Flan-T5\nmodels are pre-trained on a denoising task and\nfine-tuned on thousands of instruction-based tasks;\nand the OpenAI models receive signal from human\nfeedback. Our experiments are not designed to\ntease apart the contributions of these training pro-\ncedures to models’ behaviors. Therefore, we do not\nintend to make strong claims about the mechanisms\nby which models learn pragmatics.\nA shared feature of our tested models is the lack\nof explicitly constructed mental state representa-\ntions. In this sense, our results are potentially com-\npatible with two hypotheses. One possibility is\nthat the models do not have an ability that can be\nconsidered an analog of Theory of Mind (ToM).\nThis view is supported by evidence that language\nmodels perform poorly on social commonsense\nand false-belief tasks (Sap et al., 2022), and are\nremarkably brittle to small perturbations of classic\ntests (Ullman, 2023). If models truly lack ToM,\nthen their pragmatic behaviors might be explained\nby inferences based on low-level linguistic cues.\nTaken a step further, this finding could potentially\nsuggest that certain human pragmatic behaviors\narise through inferences based on language statis-\ntics, with no need for mental state representations.\nA second possibility is that models do have a\nheuristic version of ToM, which is not explicitly\nengineered but instead emerges as a by-product of\noptimizing for other objectives (such as linguistic\nprediction). Since language contains many descrip-\ntions of agents’ beliefs, emotions, and desires, it\nmay be beneficial – perhaps even necessary – to in-\nduce representations of these mental states in order\nto learn a generative model of linguistic forms. In-\ndeed, Andreas (2022) argues that whereas language\nmodels have no explicit representation of commu-\nnicative intents, they can infer approximate repre-\nsentations of the mental states of the agents that\nproduce a given linguistic context. If this hypoth-\nesis is true, however, it would still remain unclear\nwhether ToM is necessary to support the pragmatic\nbehaviors tested in our evaluation materials.\nOur experiments do not differentiate between\nthese two hypotheses. However, fine-grained be-\nhavioral evaluations – such as those presented in\nthis work – are important for revealing models’ ca-\npabilities and weaknesses, and offer a first step\ntoward understanding how pragmatic behaviors\ncan be supported. A promising direction for fu-\nture work is to test models with a wider range of\ntraining objectives, or even new architectures, such\nas distinct language and social reasoning modules\n(see Mahowald et al., 2023). In addition, although\nthere is evidence for the role of mentalizing in our\ntested pragmatic phenomena (see Section 3.1), one\nlimitation of our stimuli is that they were not specif-\nically designed to require ToM. New datasets that\nperform targeted manipulations of ToM alongside\ntests of language comprehension could help reveal\nhow linguistic experience and ToM jointly support\npragmatic behaviors.\nAcknowledgments\nWe would like to thank the anonymous review-\ners as well as Roger Levy, Christopher Potts, and\n4202\nJosh Tenenbaum for their constructive feedback.\nWe also thank Quinn Langford for help with cod-\ning details of the stimuli. This work was in part\nsupported by a grant from the Simons Founda-\ntion to the Simons Center for the Social Brain at\nMIT. J.H. is supported by an NSF Graduate Re-\nsearch Fellowship (#1745302) and an NSF Doc-\ntoral Dissertation Research Improvement Grant\n(BCS-2116918). S.F. is funded by the NSF SPRF\n(#2105136). E.F. was additionally supported by\nNIH award R01-DC016950 and by research funds\nfrom the McGovern Institute for Brain Research\nand the Department of Brain and Cognitive Sci-\nences.\nLimitations\nWe note several methodological limitations with\nour experiments. First, since the evaluation mate-\nrials were manually crafted, there is a rather small\nnumber of items (compared to the size of automati-\ncally generated NLP benchmarks). Small evalua-\ntion sets can introduce issues of statistical power\n(Card et al., 2020) and introduce bias based on lex-\nical items. We feel this is not a major concern,\nbecause (1) our materials are validated by expert\nresearchers; (2) models can be directly compared\nto humans in Floyd et al.’s experiments; and (3)\nin practice, there is enough signal to distinguish\nbetween the tested models.\nSecond, we only evaluate models on English-\nlanguage materials, and some of the tasks were\ndesigned based on norms of communication and so-\ncial interaction in Western cultures. As pragmatics\ncan vary widely across language and cultures (Li,\n2012; Rubio-Fernandez and Jara-Ettinger, 2020;\nFloyd, 2021; Brown et al., 2021; Dideriksen et al.,\n2022), an important direction for future work is to\nevaluate pragmatics beyond English (Ameka and\nTerkourafi, 2019; Blasi et al., 2022).\nThird, aside from the OpenAI API models, we\nwere only able to test models with ≤11B parame-\nters due to limited computational resources. Mod-\nels with parameter sizes between 11B and the size\nof text-davinci-002 could exhibit qualitatively dif-\nferent behaviors.\nFinally, we emphasize that it is impossible to pre-\ndict how models will respond to an arbitrary input.\nTherefore, we caution against extrapolating from\nour results and expecting that models will behave\n“pragmatically” in downstream applications. This is\nespecially true for models behind the OpenAI API,\nand text-davinci-002 in particular, for which very\nlittle is publicly known about the training protocol.\nEthics statement\nLanguage technologies have the potential to cause\nharm at the individual and societal levels. Large\nlanguage models (LLMs), which are typically\ntrained on vast amounts of internet text, have been\nshown to perpetuate stereotypes based on gender,\nrace, and sexual orientation. Applications using\nLLMs could reinforce systematic discrimination\nand amplify existing socioeconomic inequities. For\nexample, LLMs could perpetuate social biases by\nassisting with hiring decisions or legal rulings.\nThe remarkable fluency of LLM-generated text\nalso poses risks for the general public. LLMs have\nlong been used to generate text that is difficult to\ndistinguish from human-written text, raising con-\ncerns about detecting fake news and misinforma-\ntion. Recently, LLMs have been used to synthesize\nknowledge – for example, by answering scientific\nquestions (Taylor et al., 2022) or acting as search\nengines (Shah and Bender, 2022). Using LLMs as\nknowledge-providers could tremendously impact\nthe nature of human collaboration and work, raising\nthe need for model transparency and explainability.\nReferences\nFelix K. Ameka and Marina Terkourafi. 2019. What\nif. . . ? Imagining non-Western perspectives on prag-\nmatic theory and practice. Journal of Pragmatics,\n145:72–82.\nJacob Andreas. 2022. Language Models as Agent Mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 5769–5779, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nClara Andrés-Roqueta and Napoleon Katsos. 2017. The\nContribution of Grammar, V ocabulary and Theory of\nMind in Pragmatic Language Competence in Chil-\ndren with Autistic Spectrum Disorders. Frontiers in\nPsychology, 8.\nIan Apperly. 2011. Mindreaders: The cognitive basis of\n\"Theory of Mind\". Psychology Press, New York.\nSalvatore Attardo. 2000. Irony as relevant inappropri-\nateness. Journal of Pragmatics, 32(6):793–826.\nJohn L. Austin. 1975. How to do things with words.\nSimge Aykan and Erhan Nalçacı. 2018. Assessing The-\nory of Mind by Humor: The Humor Comprehension\nand Appreciation Test (ToM-HCAT). Frontiers in\nPsychology, 9.\n4203\nSimon Baron-Cohen, Alan M. Leslie, and Uta Frith.\n1985. Does the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On Meaning, Form, and Understand-\ning in the Age of Data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5185–5198, Online. Association\nfor Computational Linguistics.\nLeon Bergen, Roger Levy, and Noah D. Goodman. 2016.\nPragmatic reasoning through semantic inference. Se-\nmantics and Pragmatics, 9.\nAnne Beyer, Sharid Loáiciga, and David Schlangen.\n2021. Is Incoherence Surprising? Targeted Evalua-\ntion of Coherence Prediction from Language Models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4164–4173, Online. Association for Computa-\ntional Linguistics.\nLuca Bischetti, Irene Ceccato, Serena Lecce, Elena Cav-\nallini, and Valentina Bambini. 2019. Pragmatics and\ntheory of mind in older adults’ humor comprehension.\nCurrent Psychology.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,\nAngeliki Lazaridou, Jonathan May, Aleksandr Nis-\nnevich, Nicolas Pinto, and Joseph Turian. 2020. Ex-\nperience Grounds Language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718–8735,\nOnline. Association for Computational Linguistics.\nDamián E. Blasi, Joseph Henrich, Evangelia Adamou,\nDavid Kemmerer, and Asifa Majid. 2022. Over-\nreliance on English hinders cognitive science. Trends\nin Cognitive Sciences, 26(12):1153–1170. Publisher:\nElsevier.\nW.C. Booth. 1974. A Rhetoric of Irony . Litera-\nture/Criticism - The University of Chicago Press.\nUniversity of Chicago Press.\nSusan E. Brennan, Alexia Galati, and Anna K. Kuhlen.\n2010. Two Minds, One Dialog: Coordinating Speak-\ning and Understanding. In Brian H. Ross, editor,\nPsychology of Learning and Motivation, volume 53,\npages 301–344. Academic Press.\nPenelope Brown and Stephen C. Levinson. 1987. Po-\nliteness: Some Universals in Language Usage. Cam-\nbridge University Press.\nPenelope Brown, Mark A. Sicoli, and Olivier Le Guen.\n2021. Cross-speaker repetition and epistemic stance\nin Tzeltal, Yucatec, and Zapotec conversations. Jour-\nnal of Pragmatics, 183:256–272.\nStephen A. Butterfill and Ian A. Apperly. 2013. How\nto Construct a Minimal Theory of Mind. Mind &\nLanguage, 28(5):606–637. Publisher: John Wiley &\nSons, Ltd.\nCarl Camden, Michael T. Motley, and Ann Wilson. 1984.\nWhite lies in interpersonal communication: A taxon-\nomy and preliminary investigation of social motiva-\ntions. Western Journal of Speech Communication,\n48(4):309–325. Publisher: Routledge.\nDallas Card, Peter Henderson, Urvashi Khandelwal,\nRobin Jia, Kyle Mahowald, and Dan Jurafsky. 2020.\nWith Little Power Comes Great Responsibility. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9263–9274, Online. Association for Computa-\ntional Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling Language\nModeling with Pathways. arXiv preprint.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling Instruction-Finetuned Language Mod-\nels. arXiv preprint.\nIshita Dasgupta, Demi Guo, Andreas Stuhlmüller,\nSamuel J. Gershman, and Noah D. Goodman. 2018.\nEvaluating Compositionality in Sentence Embed-\ndings. In Proceedings of the Cognitive Science Soci-\nety.\nLambert Deckers and Philip Kizer. 1975. Humor and\nthe Incongruity Hypothesis. The Journal of Psychol-\nogy, 90(2):215–218.\nJudith Degen. 2023. The Rational Speech Act Frame-\nwork. Annual Review of Linguistics, 9(1):519–540.\nMarie-Julie Demedardi, Claire Brechet, Edouard Gen-\ntaz, and Catherine Monnier. 2021. Prosocial lying\n4204\nin children between 4 and 11 years of age: The role\nof emotional understanding and empathy. Journal of\nExperimental Child Psychology, 203:105045.\nChristina Dideriksen, Morten H Christiansen, Mark\nDingemanse, Malte Højmark-Bertelsen, Christer Jo-\nhansson, Kristian Tylén, and Riccardo Fusaroli. 2022.\nLanguage specific constraints on conversation: Ev-\nidence from Danish and Norwegian. PsyArXiv\npreprint.\nJudit Dombi, Tetyana Sydorenko, and Veronika Timpe-\nLaughlin. 2022. Common ground, cooperation,\nand recipient design in human-computer interactions.\nJournal of Pragmatics, 193:4–20.\nIvan Enrici, Bruno G. Bara, and Mauro Adenzato. 2019.\nTheory of Mind, pragmatics and the brain: Converg-\ning evidence for the role of intention processing as a\ncore feature of human communication. Pragmatics\n& Cognition, 26(1):5–38.\nSammy Floyd, Olessia Jouravlev, Zachary Mineroff,\nLeon Bergen, Evelina Fedorenko, and Edward Gib-\nson. In prep. Deciphering the structure of pragmatics:\nA large-scale individual differences investigation.\nSimeon Floyd. 2021. Conversation and Culture. Annual\nReview of Anthropology, 50(1):219–240. Publisher:\nAnnual Reviews.\nMichael C Frank, Andrés Goméz Emilsson, Benjamin\nPeloquin, Noah D. Goodman, and Christopher Potts.\n2018. Rational speech act models of pragmatic rea-\nsoning in reference games. PsyArXiv preprint.\nMichael C. Frank and Noah D. Goodman. 2012. Pre-\ndicting Pragmatic Reasoning in Language Games.\nScience, 336(6084):998–998.\nMichael Franke and Gerhard Jäger. 2016. Probabilistic\npragmatics, or why Bayes’ rule is probably important\nfor pragmatics. Zeitschrift für Sprachwissenschaft,\n35(1):3–44.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 32–42, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nRaymond W. Gibbs. 1979. Contextual effects in un-\nderstanding indirect requests. Discourse Processes,\n2(1):1–10. Publisher: Routledge.\nHerbert P. Grice. 1975. Logic and Conversation. In\nPeter Cole and Jerry L. Morgan, editors, Syntax and\nSemantics: Speech Acts, volume 3, pages 41–58. Aca-\ndemic Press.\nYuling Gu, Yao Fu, Valentina Pyatkin, Ian Magnus-\nson, Bhavana Dalvi Mishra, and Peter Clark. 2022.\nJust-DREAM-about-it: Figurative Language Under-\nstanding with DREAM-FLUTE. In Proceedings of\nthe 3rd Workshop on Figurative Language Process-\ning (FLP), pages 84–93, Abu Dhabi, United Arab\nEmirates (Hybrid). Association for Computational\nLinguistics.\nFrancesca G.E. Happé. 1993. Communicative compe-\ntence and theory of mind in autism: A test of rele-\nvance theory. Cognition, 48(2):101–119.\nCecilia Heyes. 2014. Submentalizing: I Am Not Really\nReading Your Mind. Perspectives on Psychological\nScience, 9(2):131–143.\nLaurence R. Horn. 1972. On the semantic properties of\nlogical operators in English. PhD Thesis, University\nof California Los Angeles.\nYik Kwan Hsu and Him Cheung. 2013. Two mental-\nizing capacities and the understanding of two types\nof lie telling in children. Developmental Psychology,\n49:1650–1659.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A Systematic Assessment of\nSyntactic Generalization in Neural Language Mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1725–1744, Online. Association for Computational\nLinguistics.\nNir Jacoby and Evelina Fedorenko. 2020. Discourse-\nlevel comprehension engages medial frontal Theory\nof Mind brain regions even for expository texts. Lan-\nguage, Cognition and Neuroscience, 35(6):780–796.\nPaloma Jeretic, Alex Warstadt, Suvrat Bhooshan, and\nAdina Williams. 2020. Are Natural Language Infer-\nence Models IMPPRESsive? Learning IMPlicature\nand PRESupposition. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8690–8705, Online. Association\nfor Computational Linguistics.\nJustine T. Kao, Leon Bergen, and Noah D. Goodman.\n2014. Formalizing the Pragmatics of Metaphor Un-\nderstanding. In Proceedings of the 36th Annual Meet-\ning of the Cognitive Science Society.\nJustine T. Kao and Noah D. Goodman. 2014. Let’s\ntalk (ironically) about the weather: Modeling verbal\nirony. In Proceedings of the 36th Annual Meeting of\nthe Cognitive Science Society.\nMelissa Kline Struhl, Jeanne Gallée, Zuzanna Balewski,\nand Evelina Fedorenko. 2018. Understanding jokes\ndraws most heavily on the Theory of Mind brain\nnetwork. PsyArXiv preprint.\nMichal Kosinski. 2023. Theory of Mind May Have\nSpontaneously Emerged in Large Language Models.\narXiv preprint.\n4205\nElisa Kreiss, Fei Fang, Noah Goodman, and Christo-\npher Potts. 2022. Concadia: Towards image-based\ntext generation with a purpose. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4667–4684, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nG. Lakoff and M. Johnson. 1980. Metaphors We Live\nBy. University of Chicago Press.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the Evaluation of Theory of Mind\nthrough Question Answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877, Hong Kong,\nChina. Association for Computational Linguistics.\nAlan M. Leslie, Ori Friedman, and Tim P. German. 2004.\nCore mechanisms in ‘theory of mind’. Trends in\nCognitive Sciences, 8(12):528–533.\nStephen Levinson. 2000. Presumptive meaning: The\ntheory of generalized conversational implicature .\nMIT Press.\nElissa Li, Sebastian Schuster, and Judith Degen. 2021.\nPredicting Scalar Inferences From \"Or\" to \"Not Both\"\nUsing Neural Sentence Encoders. In Proceedings of\nthe Society for Computation in Linguistics, volume 4.\nJin Li. 2012. Cultural Foundations of Learning: East\nand West. Cambridge University Press, Cambridge.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the Ability of LSTMs to Learn\nSyntax-Sensitive Dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535. Place: Cambridge, MA Publisher: MIT Press.\nEmmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham\nNeubig. 2022. Testing the Ability of Language Mod-\nels to Interpret Figurative Language. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4437–4452,\nSeattle, United States. Association for Computational\nLinguistics.\nEleonore Lumer and Hendrik Buschmeier. 2022. Mod-\neling Social Influences on Indirectness in a Rational\nSpeech Act Approach to Politeness. In Proceedings\nof the 44th Annual Conference of the Cognitive Sci-\nence Society.\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank,\nNancy Kanwisher, Joshua B. Tenenbaum, and\nEvelina Fedorenko. 2023. Dissociating language\nand thought in large language models: A cognitive\nperspective. arXiv preprint.\nR.A. Martin and T. Ford. 2018. The Psychology of\nHumor: An Integrative Approach. Academic Press.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the Wrong Reasons: Diagnosing Syntactic Heuris-\ntics in Natural Language Inference. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3428–3448, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nJulian Michael. 2020. To Dissect an Octopus: Making\nSense of the Form/Meaning Debate.\nJarrod Moss and Christian D. Schunn. 2015. Com-\nprehension through explanation as the interaction of\nthe brain’s coherence and cognitive control networks.\nFrontiers in Human Neuroscience, 9.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Tom Griffiths. 2018. Evaluating The-\nory of Mind in Question Answering. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2392–2400,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYixin Nie, Yicheng Wang, and Mohit Bansal. 2019. An-\nalyzing Compositionality-Sensitivity of NLI Models.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 33(01):6867–6874.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nChristopher Potts. 2020. Is it possible for language\nmodels to achieve language understanding?\nChristopher Potts, Daniel Lassiter, Roger Levy, and\nMichael C. Frank. 2016. Embedded Implicatures as\nPragmatic Inferences under Compositional Lexical\nUncertainty. Journal of Semantics, 33(4):755–802.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPaula Rubio-Fernandez. 2021. Pragmatic markers: the\nmissing link between language and Theory of Mind.\nSynthese, 199(1):1125–1158.\nPaula Rubio-Fernandez and Julian Jara-Ettinger. 2020.\nIncrementality and efficiency shape pragmatics\nacross languages. Proceedings of the National\nAcademy of Sciences, 117(24):13399–13404.\n4206\nLaura Ruis, Akbir Khan, Stella Biderman, Sara Hooker,\nTim Rocktäschel, and Edward Grefenstette. 2022.\nLarge language models are not zero-shot communi-\ncators. arXiv preprint.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural Theory-of-Mind? On the Limits\nof Social Intelligence in Large LMs. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3762–3780,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense Reasoning about Social Interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAyse Pinar Saygin and Ilyas Cicekli. 2002. Pragmat-\nics in human-computer conversations. Journal of\nPragmatics, 34(3):227–258.\nSebastian Schuster, Yuxing Chen, and Judith Degen.\n2020. Harnessing the linguistic signal to predict\nscalar inferences. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5387–5403, Online. Association for\nComputational Linguistics.\nJohn R. Searle. 1975. Indirect Speech Acts. In Speech\nActs, pages 59–82. Brill, Leiden, The Netherlands.\nChirag Shah and Emily M. Bender. 2022. Situating\nSearch. In ACM SIGIR Conference on Human Infor-\nmation Interaction and Retrieval, CHIIR ’22, pages\n221–232, New York, NY , USA. Association for Com-\nputing Machinery. Event-place: Regensburg, Ger-\nmany.\nNicola Spotorno, Eric Koun, Jérôme Prado, Jean-\nBaptiste Van Der Henst, and Ira A. Noveck. 2012.\nNeural evidence that utterance-processing entails\nmentalizing: The case of irony. NeuroImage,\n63(1):25–39.\nKevin Stowe, Prasetya Utama, and Iryna Gurevych.\n2022. IMPLI: Investigating NLI Models’ Perfor-\nmance on Figurative Language. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5375–5388, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A Large Language Model for Science.\narXiv preprint.\nMichael Henry Tessler and Michael Franke. 2018. Not\nunreasonable: Carving vague dimensions with con-\ntraries and contradictions. In Proceedings of the 40th\nAnnual Conference of the Cognitive Science Society.\nXiaoyu Tong, Ekaterina Shutova, and Martha Lewis.\n2021. Recent advances in neural metaphor process-\ning: A linguistic, cognitive and social perspective.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4673–4686, Online. Association for Computa-\ntional Linguistics.\nAnna Trosborg, editor. 2010. Pragmatics across Lan-\nguages and Cultures. De Gruyter Mouton.\nTomer Ullman. 2023. Large Language Models Fail on\nTrivial Alterations to Theory-of-Mind Tasks. arXiv\npreprint.\nThomas C. Veatch. 1998. A theory of humor. Humor,\n11(2):161–216.\nCorrie Vendetti, Deepthi Kamawar, and Katherine E.\nAndrews. 2019. Theory of mind and preschoolers’\nunderstanding of misdeed and politeness lies. Devel-\nopmental Psychology, 55(4):823–834.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022. Super-NaturalInstructions: Generaliza-\ntion via Declarative Instructions on 1600+ NLP Tasks.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5085–5109, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V . Le. 2022. Finetuned Language\nModels are Zero-Shot Learners. In International\nConference on Learning Representations.\nD. Wilson and D. Sperber. 2012. Meaning and Rele-\nvance. Cambridge University Press.\nDeirdre Wilson and Dan Sperber. 1992. On verbal irony.\nLingua, 87(1):53–76.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\n4207\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-Art Natural Language Processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nErica J. Yoon, Michael Henry Tessler, Noah D. Good-\nman, and Michael C. Frank. 2016. Talking with tact:\nPolite language as a balance between informativity\nand kindness. In Proceedings of the Annual Meeting\nof the Cognitive Science Society.\nErica J. Yoon, Michael Henry Tessler, Noah D. Good-\nman, and Michael C. Frank. 2020. Polite Speech\nEmerges From Competing Social Goals. Open Mind,\n4:71–87.\nGeorge Yule. 1996. Pragmatics, 1 edition. Oxford\nIntroduction to Language Study. Oxford University\nPress.\nAmir Zadeh, Michael Chan, Paul Pu Liang, Edmund\nTong, and Louis-Philippe Morency. 2019. Social-\nIQ: A Question Answering Benchmark for Artificial\nSocial Intelligence. In 2019 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 8799–8809.\nZilong Zheng, Shuwen Qiu, Lifeng Fan, Yixin Zhu,\nand Song-Chun Zhu. 2021. GRICE: A Grammar-\nbased Dataset for Recovering Implicature and Con-\nversational rEasoning. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 2074–2085, Online. Association for Computa-\ntional Linguistics.\nA Example prompts\nThis section contains example prompts for each\ntask in our experiments. See Section 3 and Table 1\nfor details on the materials, and Section 4.1 for\ndiscussion of how prompts were constructed.\nA.1 Deceits\nTask: You will read short stories that describe two\ncharacters interacting with each other. Each story will\nbe followed by a multiple-choice question. Read each\nstory and choose the best answer to each question. Your\ntask is to decide why the character in the story responds\nin a certain way. The answer options are 1, 2, 3, or 4.\nScenario: Henry is sitting at his desk and watching TV , and\nreluctantly switches off the TV with the remote control and\npicks up a textbook. Shortly after, his mother comes in the\nroom and asks, \"What have you been doing up here?\" Henry\nresponds: \"Reading.\" Why has Henry responded in such a\nway?\nOptions:\n1) He has been reading for some time.\n2) He does not want to offend his mom by not reading the\nbooks that she gave him.\n3) He does not want to get into trouble for not studying.\n4) He wants his mom to believe that he has been watching TV .\nAnswer:\nA.2 IndirectSpeech\nTask: You will read short stories that describe everyday\nsituations. Each story will be followed by a multiple-choice\nquestion. Read each story and choose the best answer.\nYour task is to decide what the character in the story is\ntrying to convey. The answer options are 1, 2, 3, or 4.\nScenario: Nate is about to leave the house. His wife points at\na full bag of garbage and asks: \"Are you going out?\" What\nmight she be trying to convey?\nOptions:\n1) She wants Nate to spend more time with the family.\n2) She wants to know Nate’s plans.\n3) She wants Nate to take the garbage out.\n4) She wants Nate to bring his friends over.\nAnswer:\nA.3 Irony\nTask: You will read short stories that describe everyday\nsituations. Each story will be followed by a multiple-choice\nquestion. Read each story and choose the best answer.\nYour task is to decide what the character in the story is\ntrying to convey. The answer options are 1, 2, 3, or 4.\nScenario: It is a holiday. Stefan and Kim are sitting in the\nbackseat of the car. They are fighting all the time. Their father\nsays: \"Oh, it is so pleasant here.\" What did the father want to\nconvey?\nOptions:\n1) He enjoys listening to his kids fighting.\n2) He remembers about his wife’s birthday.\n3) He does not want to listen to his kids’ arguments.\n4) AC gives them some needed cool.\nAnswer:\nA.4 Maxims\nTask: You will read short stories that describe everyday\nsituations. Each story will be followed by a multiple-choice\nquestion. Read each story and choose the best answer. Your\ntask is to decide why the character in the story responds\nin a certain way. The answer options are 1, 2, 3, or 4.\nScenario: Leslie and Jane are chatting at a coffee shop. Leslie\nasks, \"Who was that man that I saw you with last night?\"\nJane responds, \"The latte is unbelievable here.\" Why has Jane\nresponded like this?\nOptions:\n1) She does not want to discuss the topic that Leslie has raised.\n2) The man who Leslie saw makes unbelievable lattes.\n3) She thinks that it is the best latte in the town.\n4) A coffee break is not a good time to discuss men.\nAnswer:\nA.5 Metaphor\nTask: You will read short stories that describe everyday\nsituations. Each story will be followed by a multiple-choice\nquestion. Read each story and choose the best answer to\neach question. The answer options are 1, 2, 3, 4, or 5.\nScenario: Andrew and Bob were discussing the investment\ncompany where Andrew works. Bob said: \"The investors are\nsquirrels collecting nuts.\" What does Bob mean?\nOptions:\n1) The investors dress and eat well.\n2) Squirrels were hired to work in the company.\n3) Bob is allergic to nuts.\n4) They buy stocks hoping for future profit.\n5) The investors enjoy picking nuts as much as squirrels do.\n4208\nAnswer:\nA.6 Humor\nTask: You will read jokes that are missing their punch\nlines. A punch line is a funny line that finishes the\njoke. Each joke will be followed by five possible\nendings. Please choose the ending that makes the\njoke funny. The answer options are 1, 2, 3, 4, or 5.\nJoke: Martha walked into a pastry shop. After surveying all\nthe pastries, she decided on a chocolate pie. \"I’ll take that\none,\" Martha said to the attendant, \"the whole thing.\" \"Shall I\ncut it into four or eight pieces?\" the attendant asked.\nPunchlines:\n1) Martha said, \"My leg is hurting so much.\"\n2) Martha said, \"Four pieces, please; I’m on a diet.\"\n3) Martha said: \"Well, there are five people for dessert tonight,\nso eight pieces will be about right.\"\n4) Then the attendant squirted whipped cream in Martha’s\nface.\n5) Martha said, \"You make the most delicious sweet rolls in\ntown.\"\nAnswer:\nA.7 Coherence\nTask: You will read pairs of sentences. Reach\neach pair and decide whether they form a co-\nherent story. The answer options are 1 or 2.\nScenario: Cleo brushed against a table with a vase on it. She\ndecided to study harder to catch up.\nOptions:\n1) Incoherent\n2) Coherent\nAnswer:\nB Timestamps of OpenAI model queries\nTable 3 shows timestamps of requests sent to the\nOpenAI API.\nModel Phenomenon Timestamp\ntext-ada-001 Coherence 2022-10-11 12:28 -0400\ntext-ada-001 Deceits 2022-10-11 12:28 -0400\ntext-ada-001 IndirectSpeech 2022-10-11 12:28 -0400\ntext-ada-001 Irony 2022-10-11 12:28 -0400\ntext-ada-001 Humor 2022-10-11 12:28 -0400\ntext-ada-001 Maxims 2022-10-11 12:29 -0400\ntext-ada-001 Metaphor 2022-10-11 12:29 -0400\ntext-davinci-002 Coherence 2022-10-11 11:56 -0400\ntext-davinci-002 Deceits 2022-10-11 11:55 -0400\ntext-davinci-002 IndirectSpeech 2022-10-11 11:55 -0400\ntext-davinci-002 Irony 2022-10-11 11:54 -0400\ntext-davinci-002 Humor 2022-10-11 11:53 -0400\ntext-davinci-002 Maxims 2022-10-11 11:56 -0400\ntext-davinci-002 Metaphor 2022-10-11 11:57 -0400\nTable 3: Timestamps of OpenAI API model queries.\nC No-context analysis\nC.1 Details of human experiments\nBelow, we discuss details of the no-context human\nexperiments described in Section 5.3.1. This study\nwas approved by the Institutional Review Board\nat the home institution of the authors (protocol\n2010000243).\nParticipants. We collected data from 30 partici-\npants using Amazon.com’s Mechanical Turk. All\nparticipants were recruited from IP addresses in\nthe US, Canada, and other English-speaking coun-\ntries and passed a brief English proficiency task to\nparticipate. We pre-screened participants using a\nqualification task in which they were asked to per-\nform 10 simple sentence completions, which were\njudged for basic levels of coherence and grammati-\ncality. Participants were paid 7 USD for complet-\ning the study, which took around 20 minutes to\ncomplete. The resulting hourly rate was around 21\nUSD, which is well above federal minimum wage\nin the United States.\nProcedure. Participants completed these tests\nduring one individual testing session. After giving\ninformed consent, which included assurance of\nanonymity, participants were shown instructions\nand a training trial, in which they were told they\nwould be answering questions about a character\nin a short interaction. They then saw 105 trials\n(similar to those described in Appendix A), without\nthe scenario context. For example:\nBob said: \"The investors are squirrels collecting nuts.\" What\ndoes Bob mean?\n1) The investors dress and eat well.\n2) Squirrels were hired to work in the company.\n3) Bob is allergic to nuts.\n4) They buy stocks hoping for future profit.\n5) The investors enjoy picking nuts as much as squirrels do.\nItems were presented within blocks according to\ntheir phenomenon, as in Floyd et al.’s (In prep) orig-\ninal experiments. Blocks and items were presented\nin a random order.\nC.2 Raw accuracy scores\nFigure 6 shows accuracy scores achieved by hu-\nmans and the three best-performing models on the\noriginal (shaded bars) and no-context (empty bars)\nversions of the test items.\nD Sentence- and word-level scrambling\nFigure 7 shows accuracy scores achieved by the\nthree best-performing models on each task, across\nthree scrambling conditions: none (original, un-\nmodified items), sentence-level, and word-level.\nExample prompts are provided below.\n4209\n0.0\n0.5\n1.0Prop. correct\n0.0\n0.5\n1.0Prop. correct\n0.0\n0.5\n1.0Prop. correct\nIrony Deceits Maxims IndirectSpeech Metaphor\n0.0\n0.5\n1.0Prop. correct\nOriginal No context\nHumanTk-Instruct (11B)Flan-T5 (XL)text-davinci-002\nFigure 6: Proportion of items where humans and models\nselect the correct pragmatic answer, on both original\n(shaded bars) and no-context (empty bars) versions.\nD.1 Sentence-level scrambled prompt\nTask: You will read short stories that describe two\ncharacters interacting with each other. Each story will\nbe followed by a multiple-choice question. Read each\nstory and choose the best answer to each question. Your\ntask is to decide why the character in the story responds\nin a certain way. The answer options are 1, 2, 3, or 4.\nScenario: Dan says,\"The dog knocked it over.\" The vase falls\ndown on the floor and breaks. He brushes against his mother’s\nvase. When Dan’s mother comes home, she asks Dan: \"What\nhappened to my vase?\" Dan is playing in the living room. Why\nhas Dan responded in such a way?\nOptions:\n1) Dan does not want his mom to be angry with him for\nbreaking the vase.\n2) Dan finds this vase ugly and wants to get rid of it.\n3) Dan wants his mom to know that he knocked it over.\n4) Dan thinks that the dog has knocked over the vase.\nAnswer:\nD.2 Word-level scrambled prompt\nTask: You will read short stories that describe two\ncharacters interacting with each other. Each story will\nbe followed by a multiple-choice question. Read each\nstory and choose the best answer to each question. Your\ntask is to decide why the character in the story responds\nin a certain way. The answer options are 1, 2, 3, or 4.\nScenario: to happened Dan \"The against in it she comes \"What\nliving Dan the vase floor on down The Dan: He dog my\nbrushes vase?\" mother When falls breaks. vase. and playing\nroom. his asks knocked says, home, over.\" the mother’s is\nDan’s Why has Dan responded in such a way?\nOptions:\n1) Dan does not want his mom to be angry with him for\nbreaking the vase.\n2) Dan finds this vase ugly and wants to get rid of it.\n3) Dan wants his mom to know that he knocked it over.\n4) Dan thinks that the dog has knocked over the vase.\nAnswer:\n4210\nnone sent word\n0.25\n0.50\n0.75\n1.00Proportion/uni00A0correct\nCoherence\nnone sent word\nDeceits\nnone sent word\nHumor\nnone sent word\nScramble/uni00A0condition\nIndirectSpeech\nnone sent word\nIrony\nnone sent word\nMaxims\nnone sent word\nMetaphor\nmodel\nTk/uni00ADInstruct/uni00A0(11B)\nFlan/uni00ADT5/uni00A0(XL)\ntext/uni00ADdavinci/uni00AD002\nFigure 7: Model performance across scrambling conditions (none = original, unmodified items). Error bars denote\n95% CI. Dashed line indicates random baseline.\n4211\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n\"Limitations\" section after Section 6\n□\u0013 A2. Did you discuss any potential risks of your work?\n\"Limitations\" and \"Ethics statement\" sections after Section 6\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract; Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3; Section 4.2\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAll artifacts are open to scientiﬁc research use.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAll data and models used in our study were intended for scientiﬁc research.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4212\n□\u0017 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe only used publicly available pre-trained models.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix C\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nAppendix C\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nAppendix C\n□\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nAppendix C\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nAppendix C\n4213",
  "topic": "Literal (mathematical logic)",
  "concepts": [
    {
      "name": "Literal (mathematical logic)",
      "score": 0.761096715927124
    },
    {
      "name": "Computer science",
      "score": 0.7438385486602783
    },
    {
      "name": "Pragmatics",
      "score": 0.6700448989868164
    },
    {
      "name": "Heuristic",
      "score": 0.6281780004501343
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6076438426971436
    },
    {
      "name": "Language model",
      "score": 0.5727498531341553
    },
    {
      "name": "Natural language processing",
      "score": 0.5217741131782532
    },
    {
      "name": "Artificial intelligence",
      "score": 0.519008994102478
    },
    {
      "name": "Human language",
      "score": 0.4561653733253479
    },
    {
      "name": "Linguistics",
      "score": 0.35318851470947266
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Algorithm",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}