{
  "title": "<i>LMFingerprints</i>: Visual Explanations of Language Model Embedding Spaces through Layerwise Contextualization Scores",
  "url": "https://openalex.org/W4288761543",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5112855257",
      "name": "R. Sevastjanova",
      "affiliations": [
        "University of Konstanz"
      ]
    },
    {
      "id": "https://openalex.org/A4288762352",
      "name": "A. Kalouli",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2095659657",
      "name": "C. Beck",
      "affiliations": [
        "University of Konstanz"
      ]
    },
    {
      "id": "https://openalex.org/A2018215478",
      "name": "H. Hauptmann",
      "affiliations": [
        "Utrecht University"
      ]
    },
    {
      "id": "https://openalex.org/A4288763256",
      "name": "M. El‐Assady",
      "affiliations": [
        "ETH Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3021934057",
    "https://openalex.org/W2412593044",
    "https://openalex.org/W2995755016",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3127507520",
    "https://openalex.org/W2054901814",
    "https://openalex.org/W3048018176",
    "https://openalex.org/W3125516434",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2786028390",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2149386833",
    "https://openalex.org/W3087570533",
    "https://openalex.org/W3015766957",
    "https://openalex.org/W2951689902",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3049654111",
    "https://openalex.org/W3092557781",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W2869198903",
    "https://openalex.org/W3038035611",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W1892979790",
    "https://openalex.org/W2339562889",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2141115795",
    "https://openalex.org/W2146695947",
    "https://openalex.org/W3126961594",
    "https://openalex.org/W2751627669",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2889883176",
    "https://openalex.org/W3116286104",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2989195139",
    "https://openalex.org/W2343885978",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2805083708",
    "https://openalex.org/W2996005046",
    "https://openalex.org/W2142120379",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2922466929",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W3173876819",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2969821795",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W3099954305",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2156136663",
    "https://openalex.org/W3157950068"
  ],
  "abstract": "Abstract Language models, such as BERT, construct multiple, contextualized embeddings for each word occurrence in a corpus. Understanding how the contextualization propagates through the model's layers is crucial for deciding which layers to use for a specific analysis task. Currently, most embedding spaces are explained by probing classifiers; however, some findings remain inconclusive. In this paper, we present LMFingerprints, a novel scoring‐based technique for the explanation of contextualized word embeddings. We introduce two categories of scoring functions, which measure (1) the degree of contextualization, i.e., the layerwise changes in the embedding vectors, and (2) the type of contextualization, i.e., the captured context information. We integrate these scores into an interactive explanation workspace. By combining visual and verbal elements, we provide an overview of contextualization in six popular transformer‐based language models. We evaluate hypotheses from the domain of computational linguistics, and our results not only confirm findings from related work but also reveal new aspects about the information captured in the embedding spaces. For instance, we show that while numbers are poorly contextualized, stopwords have an unexpected high contextualization in the models' upper layers, where their neighborhoods shift from similar functionality tokens to tokens that contribute to the meaning of the surrounding sentences.",
  "full_text": null,
  "topic": "Contextualization",
  "concepts": [
    {
      "name": "Contextualization",
      "score": 0.9675168991088867
    },
    {
      "name": "Computer science",
      "score": 0.7488424777984619
    },
    {
      "name": "Embedding",
      "score": 0.7143293619155884
    },
    {
      "name": "Workspace",
      "score": 0.5894156694412231
    },
    {
      "name": "Natural language processing",
      "score": 0.5601708889007568
    },
    {
      "name": "Language model",
      "score": 0.5479807257652283
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46688973903656006
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4508107900619507
    },
    {
      "name": "Construct (python library)",
      "score": 0.44309675693511963
    },
    {
      "name": "Linguistics",
      "score": 0.3889126479625702
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Robot",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}