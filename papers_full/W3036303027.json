{
  "title": "Differentiable Language Model Adversarial Attacks on Categorical Sequence Classifiers",
  "url": "https://openalex.org/W3036303027",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5047409045",
      "name": "Ivan Fursov",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5011905327",
      "name": "Alexey Zaytsev",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5075777588",
      "name": "Nikita Kluchnikov",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5065777566",
      "name": "Andrey Kravchenko",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088950452",
      "name": "Evgeny Burnaev",
      "affiliations": [
        "Skolkovo Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963600562",
    "https://openalex.org/W3010186808",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3020242586",
    "https://openalex.org/W3015001695",
    "https://openalex.org/W2552767274",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2735135478",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2950159395",
    "https://openalex.org/W2979889262",
    "https://openalex.org/W2963834268",
    "https://openalex.org/W3013520104",
    "https://openalex.org/W2962982907",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W3012613466",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2963809642",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2885318751",
    "https://openalex.org/W2803831897",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2963178695",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W2915238810",
    "https://openalex.org/W2908442265",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2998277219",
    "https://openalex.org/W2963083752"
  ],
  "abstract": "An adversarial attack paradigm explores various scenarios for the vulnerability of deep learning models: minor changes of the input can force a model failure. Most of the state of the art frameworks focus on adversarial attacks for images and other structured model inputs, but not for categorical sequences models. Successful attacks on classifiers of categorical sequences are challenging because the model input is tokens from finite sets, so a classifier score is non-differentiable with respect to inputs, and gradient-based attacks are not applicable. Common approaches deal with this problem working at a token level, while the discrete optimization problem at hand requires a lot of resources to solve. We instead use a fine-tuning of a language model for adversarial attacks as a generator of adversarial examples. To optimize the model, we define a differentiable loss function that depends on a surrogate classifier score and on a deep learning model that evaluates approximate edit distance. So, we control both the adversability of a generated sequence and its similarity to the initial sequence. As a result, we obtain semantically better samples. Moreover, they are resistant to adversarial training and adversarial detectors. Our model works for diverse datasets on bank transactions, electronic health records, and NLP datasets.",
  "full_text": "Differentiable Language Model Adversarial Attacks\non Categorical Sequence Classiﬁers\nI. Fursov\nivan.fursov@skoltech.ru\nA. Zaytsev\na.zaytsev@skoltech.ru\nN. Kluchnikov\nnikita.klyuchnikov@skoltech.ru\nA. Kravchenko\nandrey.kravchenko@deepreason.ai\nE. Burnaev\ne.burnaev@skoltech.ru\nAbstract\nAn adversarial attack paradigm explores various scenarios for the vulnerability of\ndeep learning models: minor changes of the input can force a model failure. Most\nof the state of the art frameworks focus on adversarial attacks for images and other\nstructured model inputs, but not for categorical sequences models.\nSuccessful attacks on classiﬁers of categorical sequences are challenging because\nthe model input is tokens from ﬁnite sets, so a classiﬁer score is non-differentiable\nwith respect to inputs, and gradient-based attacks are not applicable. Common\napproaches deal with this problem working at a token level, while the discrete\noptimization problem at hand requires a lot of resources to solve.\nWe instead use a ﬁne-tuning of a language model for adversarial attacks as a gener-\nator of adversarial examples. To optimize the model, we deﬁne a differentiable loss\nfunction that depends on a surrogate classiﬁer score and on a deep learning model\nthat evaluates approximate edit distance. So, we control both the adversability of a\ngenerated sequence and its similarity to the initial sequence.\nAs a result, we obtain semantically better samples. Moreover, they are resistant to\nadversarial training and adversarial detectors. Our model works for diverse datasets\non bank transactions, electronic health records, and NLP datasets.\n1 Introduction\nAdversarial attacks [39] in all application areas including computer vision [2, 17], NLP [40, 35], and\ngraphs [31] make use of non-robustness of deep learning models. An adversarial attack generates an\nobject that fools a deep learning model by inserting changes into the initial object, undetectable by\na human eye. The deep learning model misclassiﬁes the generated object, whilst for a human, it is\nevident that the object’s class remains the same [20].\nFor images, we can calculate derivatives of class probabilities with respect to the pixels’ colors in\nan input image. Thus, moving along the gradient we apply slight alterations to a few pixels and\nget a misclassiﬁed adversarial image close to the input image. If we have access to the gradient\ninformation, it is easy to construct an adversarial example.\nThe situation is different for discrete sequential data, due to its discrete categorical nature [40, 35].\nWhilst an object representation can lie in a continuous space, when we go back to the space of\nsequences, we can move far away from the initial object, rendering partial derivatives useless. Many\nPreprint. Under review.\narXiv:2006.11078v1  [cs.LG]  19 Jun 2020\nOriginal Hotﬂip DILMA (ours)\nhow did socrates die fear did socrates die how did jaco die\nwhat were the what were the what were the\nﬁrst frozen foods origin frozen foods second frozen foods\nwhat country’s people are what country why caused are what company s people are\nthe top television watchers the top television watchers the top television watchers\nwhat is the difference between what fear the which between what is the novel between\na median and a mean a median and a mean a bachelor and a play\nwhat s the literary term what s the origin term what s the human term\nfor a play on words for a play on words for a play on words\nTable 1: Examples of generated adversarial sequences for the TREC dataset [34] evaluated on the\nHotFlip and DILMA approaches. HotFlip often selects origin and fear words that corrupt the\nsemantics of a sentence. DILMA is more ingenious.\napproaches that accept the initial space of tokens as input attempt to modify these sequences using\noperations like addition, replacement, or switching of tokens [28, 21, 7]. The discrete optimisation\nproblem is often hard, and existing approaches are greedy or similar ones [7]. Another idea is to move\ninto an embedding space and leverage on gradients and optimisation approaches in that space [29, 27].\nThis approach requires training of a large model from scratch and has many parameters to tune.\nMoreover, in this type of models without attention mechanism the embedding space is a bottleneck,\nas we constrain all information about input sequence to the embedding vector. Thus, this type of\nmodels is inaccurate for long and complex sequences.\nWe propose an adversarial attack model DILMA (DIfferentiable Language Model Attack) that can\nalleviate the aforementioned problems with differentiability. Moreover, our approach beneﬁts from\nstrong pre-trained language models, making adversarial model generation easier for new domains.\nOur model for an adversarial attack works in two regimes. The ﬁrst regime is a random sampling\nthat produces adversarial examples by chance. The second regime is a targeted attack that modiﬁes\nthe language model by optimisation of the loss related to misclassiﬁcation by the target model and\nsmall difference between an initial sequence and its adversarial counterpart. For the second regime\nwe introduce a differentiable loss function as the weighted sum of the distance between the initial\nsequence and the generated one and the difference between the probability scores for these sequences.\nWe use a trained differentiable version of the Levenshtein distance [22] and Gumbel-Softmax heuristic\nto pass the derivatives through our sequence generation layers. The number of hyperparameters in\nour method is small, and the selection of them is not crucial for obtaining high performance scores.\nAs our loss is differentiable, we can adopt any gradient-based adversarial attack. The training and\ninference procedure is summarised in Figure 1.\nAs a generative model for adversarial attacks we use a transformer sequence2sequence masked\nlanguage model based on BERT [6], thus allowing the constructed model to be reused for generating\nadversarial attacks. The validation of our approaches includes testing on diverse datasets from NLP,\nbank transactions, and electronic healthcare records domains. Examples of generated sequences for\nthe TREC dataset [34] are presented in Table 1.\nTo sum up, the main contributions of this work are the following:\n• An adversarial attack based on a masked language model (MLM). We ﬁne tune parameters\nof MLM by optimising a weighted sum of two differentiable terms based on a surrogate\ndistance between sequences and a surrogate classiﬁer model scores.\n• A simple, but powerful baseline “SamplingFool” attack that requires only a pre-trained\nMLM to work.\n• Our algorithms are resistant to common defense strategies, whilst existing approaches fail to\nfool models after they defende themselves. To validate our approach we use diverse datasets\nfrom NLP, bank transactions, and electronic health records areas.\n• We show a necessity of validation adversarial attacks for sequential data using different\ndefense strategies. Some state-of-the-art approaches lack the ability to overcome simple\ndefences. Our methods perform well in these scenarios.\n2\nSurrogate classiﬁer\n Deep Levenshtein\nLoss function\nSampler: Straight-Through (ST) Gumbel\nEstimator\nGenerator: Transformer\nBackpropagation of gradients\nFigure 1: Training of the DILMA architecture consists of the following steps. Step 1: obtain logits P\nfrom a pre-trained Language Model for input x. Step 2: sample x′from multinomial distribution P\nusing the Gumbel-Softmax estimator. To improve generation quality we can sample many times.Step\n3: obtain surrogate probability C(x′) and approximate edit distance DL(x′,x). Step 4: calculate\nloss, do a backward pass. Step 5: update parameters of the language model using these gradients.\n2 Related work\nThere exist adversarial attacks for different types of data: the most mainstream being images\ndata [32, 12], graph data, [42] and sequences [25]. We refer a reader to recent surveys on adversarial\nattacks for sequences [40, 35], and highlight below only related issues.\n[25] is one of the ﬁrst published works on generation of adversarial attacks for discrete sequences.\nThe authors identify two main challenges for adversarial attacks on discrete sequence models: a\ndiscrete space of possible objects and a complex deﬁnition of a semantically coherent sequence. Their\napproach focuses on a white-box adversarial attack for binary classiﬁcation. We consider a similar\nproblem statement, but focus on black-box adversarial attacks for sequences, see also [10, 21, 16].\nAuthors of [10] identify certain pairs of tokens and then permute their positions within those pairs,\nthus working directly on a token level. A black-box approach [21] also performs a direct search for\nthe most harmful tokens to fool a classiﬁer.\nAs stated in Section 1 there are two main classes of approaches. In the ﬁrst one they directly apply\nnatural modiﬁcations like deletion or replacement of a token in a sequence trying to generate a good\nadversarial example [28, 37]. This idea leads to an extensive search among the space of possible\nsequences, thus making the problem computationally challenging, especially if the inference time of\na model is signiﬁcant [8]. Moreover, we have little control on the semantic closeness of the initial\nand modiﬁed sequences. An approach [ 3] to white-box adversarial attacks is very similar except\none more modiﬁcation: the authors additionally use differentiable distance between the initial and\ngenerated sequences to control their similarity.\nThe second type of methods is similar to the common approaches used by adversarial attacks\npractitioners, namely, to use gradients of a cost function to modify the initial sequence. However,\nthe authors in [ 29] limit directions of perturbations in an embedding space by modifying only a\nspeciﬁc token, which seems too restrictive. [ 27] elaborates a V AE-based model with GRU units\ninside to create a generator for adversarial examples. However, due to usage of RNNs seq2seq models\ncomplexity and length of sequences they can process is limited. In our opinion, the state of the art\nmechanisms such as attention [ 33] should be used instead of standard RNNs to improve seq2seq\nmodels. Authors of [9] move in the right direction, but the performance metrics are worse than of\ncompetitors’.\n3\nFrom the current state of the art, we see a remaining need to identify an effective way to generate\nadversarial categorical sequences. Existing approaches use previous generations of LMs based on\nrecurrent architectures, and stay at a token level or use V AE, despite known limitations of these\nmodels [38, 33]. Moreover, as most of the applications focus on NLP-related tasks, it makes sense to\nwiden the scope of application domains for adversarial attacks on categorical sequences.\n3 Methods\n3.1 General description of the approach\nWe generate adversarial examples using two consecutive components: a masked language model\n(MLM, we call it language model or LM below for brevity) with parameters θ that provides for\nan input sequence x, conditional distribution pθ(x′|x), and a sampler from this distribution such\nthat x′∼pθ(x′|x). Thus, we can generate sequences x′by consecutive application of a LM and a\nsampler.\nFor this sequence to be adversarial, we optimise a proposed differentiable loss function that forces\nthe LM to generate semantically similar but adversarial examples by modifying the LM parameters\nθ. The loss function consists of two terms: the ﬁrst term corresponds to a surrogate classiﬁer C(x′)\nthat outputs a probability of belonging to a target class, and the second term corresponds to a Deep\nLevenstein distance DL(x,x′) that approximates the edit distance between sequences.\nThe general scheme of our approach is given in Figure 1. More details are given below. We start with\nthe description of the LM and the sampler in Subsection 3.2. We continue with the description of the\nloss function in Subsection 3.3. The formal description of our algorithm is given in Subsection 3.4. In\nlater subsections 3.5 and 3.6 we provide more details on the use of the target and surrogate classiﬁers\nand the Deep Levenstein model correspondingly. Detailed descriptions of the architectures and\ntraining procedures is provided in supplementary materials B.1.\n3.2 Language model\nThe language model (LM) lies at the heart of our approach. The LM is a model that takes a sequence of\ntokens (e.g. words) as inputx = {x1,...,x t}and outputs logits for tokensP = {p1,..., pt}∈ Rd·t\nfor each index 1,...,t , where dis a size of the dictionary of tokens. In this work we use transformer\narchitecture as the LM [33]. We pre-train a transformer encoder [ 33] in a BERT [6] manner on a\ncorresponding domain. We use all available data to train such this kind of model.\nThe sampler is deﬁned as follows: the LM learns a probability distribution over sequences, so we\ncan sample a new sequence x′= {x′\n1,...,x ′\nt}based on the vectors of token logits P. In this work\nwe use Straight-Through Gumbel Estimator ST(P) : P →x′for sampling [15]. To get the actual\nprobabilities qij from logits pij we use a softmax with temperature τ [13]:\nqij = exp (pij/τ)\n∑d\nk=1 exp (pik/τ)\n. (1)\nValue τ > 1 produces a softer probability distribution over classes. As τ → ∞, the original\ndistribution approaches a uniform distribution. If τ →0, then sampling from (1) reduces to setting\nx′\ni = arg maxj pi = arg max{pi1,...,p id}⊤.\nOur ﬁrst method SamplingFool samples sequences from the categorical distribution with qij proba-\nbilities. The sampled examples turn out to be good-looking and can easily fool a classiﬁer, as we\ndiscuss in Section 4. The method is similar to the random search algorithm and serves as a baseline\nfor the generation of adversarial examples for discrete sequences.\n3.3 Loss function\nThe Straight-Through Gumbel sampling allows propagation of the derivatives through the softmax\nlayer. So, we optimise parameters θ of our MLM to improve quality of generated adversarial\nexamples.\nA loss function should take into account two terms: the ﬁrst term represents the probability score\ndrop (1 −Ct\ny(x′)) of the target classiﬁer for the considered class y, the second term represents the\n4\nedit distance DL(x,x′) between the initial sequence x and the generated sequence x′. We should\nmaximise the probability drop and minimise the edit distance, so it is as close to 1 as possible.\nIn our black-box scenario we do not have access to the true classiﬁer score, so we use a substitute\nclassiﬁer score Cy(x′) ≈Ct\ny(x′). More details on classiﬁers are given in Subsection 3.5. As a\ndifferentiable alternative to edit distance, we use Deep Levenstein model proposed in [22] — a deep\nlearning model that approximates the edit distance: DL(x,x′) ≈D(x,x′). More details on the\nDeep Levenstein model are given in Subsection 3.6.\nIn that way we get the following differentiable loss function:\nL(x′,x,y) = β(1 −DL(x′,x))2 −log(1 −Cy(x′)), (2)\nwhere Cy(x′) is the probability of the true class yfor sequence x′and βis a weighting coefﬁcient.\nThus, we penalise cases when more than one modiﬁcation is needed in order to get x′ from x.\nSince we focus on non-target attacks, the Cy(x′) component is included in the loss. The smaller the\nprobability of an attacked class, the smaller the loss.\nWe estimate derivatives of L(x′,x,y) in a way similar to [ 14]. Using these derivatives we do a\nbackward pass and update the weights θ of the Language Model. We ﬁnd that updating the whole set\nof parameters θ is not the best strategy and a better alternative is to update only the last linear layer\nand the last layer of the transformer.\n3.4 DILMA algorithm\nNow we are ready to group the introduced components into a formal algorithm with the architecture\ndepicted in Figure 1.\nThe proposed approach for the generation of adversarial sequences has the following inputs: a\nlanguage model with parameters θ = θ0, learning rate α, temperature for sampling τ, coefﬁcient β\n(if β = 0, we only decrease a classiﬁer score and don’t pay attention to the edit distance part of the\nloss function) for the normalised classiﬁer score in (2). For these inputs the algorithm performs the\nfollowing steps at iterations i= 1,2,...,k for a given sequence x.\nStep 1. Pass the sequence x through the pre-trained LM. Obtain logits P = LMθi−1 (x).\nStep 2. Sample an adversarial sequence x′from the logits using the Gumbel-Softmax Estimator.\nStep 3. Calculate the probability Cy(x′) and Deep Levenstein distance DL(x′,x). Calculate the\nloss value L(x′,x,y) (2).\nStep 4. Do a backward pass to update LM’s weights θi−1 using gradient descent and get new\nweights θi.\nStep 5. Obtain an adversarial sequence x′\ni.\na) DILMA: by setting x′\ni to be the most probable value according to P = LMθi (x), or\nb) DILMA w/ sampling: by sampling based on (1) with τ >0 and P.\nNote that the algorithm decides by itself which tokens should be replaced. The classiﬁcation\ncomponent changes the sequence in a direction where the probability score Cy(x′) is low, and\nthe Deep Levenstein distance keeps the generated sequence close to the original one. The update\nprocedure for each x starts from the pre-trained LM parameters θ0.\nLet us note that for the DILMA w/ sampling approach we can sample m> 1 adversarial examples\nfor each iteration with almost no additional computational cost. As we will see in Section 4, this\napproach works better than the original DILMA approach across all datasets.\nAfter all iterations i= 1,2,...,k of the algorithm we obtain a set of adversarial sequences {x′\ni}k\ni=1\nin case of the original DILMA; for DILMA w/ sampling we get madversarial sequences on each\niteration of the algorithm. The last sequence in this set is not always the best one. Therefore among\ngenerated sequences that are adversarial w.r.t. the substitute classiﬁer Cy(x′) we select x′\nopt with\nthe lowest Word Error Rate (WER). Here we calculate WER relative to the initial sequencex. If all\nexamples are not adversarial w.r.t. Cy(x′), then we select x′\nopt with the smallest target class score.\nAs we use only the substitute classiﬁer, DILMA is a blackbox-type adversarial attack.\n3.5 Classiﬁcation model\nIn all experiments we use two classiﬁers: a target classiﬁer Ct(x) that we attack and a substitute\nclassiﬁer C(x) that provides differentiable surrogate classiﬁer scores.\n5\nClasses Avg. Max Train Test Targeted GRU Substitute CNN\nlength length size size accuracy accuracy\nAG 4 6.61 19 120000 7600 0.87 0.86\nTREC 6 8.82 33 5452 500 0.85 0.85\nSST-2 2 8.62 48 76961 1821 0.82 0.80\nMR 2 18.44 51 9595 1067 0.76 0.70\nEHR 2 5.75 20 314724 34970 0.99 0.98\nTr.Age 4 8.98 42 2649496 294389 0.46 0.45\nTr.Gender 2 10.26 20 256325 28481 0.68 0.67\nTable 2: The comparison of evaluated datasets. We try to attack classiﬁers for diverse NLP- and\nnon-NLP problems\nThe target classiﬁer is a combination of a bi-directional Gated Recurrent Unit (GRU) [4] RNN and\nan embeddings layer for tokens before it. The hidden size for GRU is 128, the dropout rate is 0.1\nand the embedding size 100. The surrogate classiﬁer is Convolutional Neural Networks (CNN) for\nSentence Classiﬁcation [18] and an embeddings layer of size 100 for tokens before it.\nThe substitute classiﬁer has access only to 50% of the data, whilst the target classiﬁer uses the whole\ndataset. We split the dataset into two parts with stratiﬁcation. Both models demonstrate comparable\nresults for all datasets.\n3.6 Deep Levenstein\nTo make gradient-based updates of parameters, we need a differentiable version of the edit distance\nfunction. We use an approach based on training a deep learning model “Deep Levenshtein distance”\nDL(x,x′) for evaluation of the Word Error Rate between two sequencesx and x′. It is similar to the\napproach proposed in [22]. In our case, the WER is used instead of the Levenstein distance, since we\nwork at the word level instead of the character level for NLP tasks, and for non-textual tasks there are\nno levels other than “tokens”.\nThe Deep Levenstein model receives two sequences (x,y). It encodes them into a dense represen-\ntation of ﬁxed length lusing the shared encoder zx = E(x), zy = E(y). Then it concatenates the\nrepresentations and the absolute difference between them in a vector (zy,zy,|zy −zy|) of length 3l.\nAt the end the model uses a fully-connected layer. The architecture is similar to the one proposed\nin [5]. To estimate the parameters of the encoder and the fully connected layer we use the L2 loss\nbetween true and model values of the WER distance. We form a training sample of size of about two\nmillion data points by sampling sequences and their modiﬁcations from the training data.\n4 Experiments\nIn this section we describe our experiments. The datasets and the source code are published online1.\nAn ablation study and an algorithm to select hyperparameters as well as additional experiments are\nprovided in supplementary materials in Section C.\n4.1 Competitor approaches\nWe have compared our approach to HotFlip, FGSM variant for adversarial sequences [12, 25], and\nDeepFool [23]. We have also tried to implement [27], but haven’t managed to ﬁnd hyperparameters\nthat provide performance similar to that reported by the authors.\nHotFlip. The main idea of HotFlip [7] is to select the best token to change, given an approximation\nof partial derivatives for all tokens and all elements of the dictionary. We change multiple tokens in a\ngreedy way or by a beam search of a good sequence of changes.\nFGSM chooses random token in a sequence and uses the Fast Gradient Sign Method to perturb its\nembedding vector. Then, the algorithm ﬁnds the closest vector in an embedding matrix and replaces\nthe chosen token with the one that corresponds to the identiﬁed vector.\n1The code is available at https://github.com/fursovia/dilma\n6\nNLP datasets AG TREC SST-2 MR\nFGSM 0.66 / 0.26 0.62 / 0.02 0.63 / 0.1 0.57 / 0.03\nDeepFool 0.48 / 0.24 0.42 / 0.01 0.59 / 0.08 0.52 / 0.03\nHotFlip 0.78 / 0.39 0.75 / 0.27 0.84 / 0.14 0.62 / 0.21\nSamplingFool (ours) 0.49 / 0.47 0.40 / 0.37 0.44 / 0.41 0.37 / 0.34\nDILMA, β = 0 (ours) 0.38 / 0.32 0.52 / 0.52 0.51 / 0.39 0.36 / 0.34\nDILMA (ours) 0.45 / 0.40 0.50 / 0.44 0.43 / 0.43 0.38 / 0.34\nDILMA w/ sampling (ours) 0.59 / 0.54 0.60 / 0.56 0.52 / 0.51 0.47 / 0.40\nOther datasets EHR Tr.Age Tr.Gender\nFGSM 0.17 / 0.04 0.18 / 0.17 0.92 / 0.59\nDeepFool 0.19 / 0.04 0.42 / 0.4 0.67 / 0.26\nHotFlip 0.21 / 0.03 0.83 / 0.68 0.97 / 0.10\nSamplingFool (ours) 0.01 / 0.01 0.85 / 0.84 0.72 / 0.71\nDILMA, β = 0 (ours) 0.05 / 0.05 0.57 / 0.56 0.45 / 0.46\nDILMA (ours) 0.06 / 0.05 0.60 / 0.59 0.46 / 0.46\nDILMA w/ sampling (ours) 0.06 / 0.06 0.76 / 0.75 0.83 / 0.82\nTable 3: NAD metric (↑) before/after adversarial training on 5000 examples. The best values are in\nbold, the second best values are underscored. DILMA is resistant to adversarial training.\nDeepFool follows the same idea of gradient-based replacement method in an embedded space. In\naddition, by assuming the local linearity of a classiﬁer DeepFool provides a heuristic to select the\nmost promising modiﬁcation.\n4.2 Datasets\nWe have conducted experiments on four open NLP datasets for text classiﬁcation and three non-NLP\ndatasets (bank transactions and electronic health records datasets). The datasets’ statistics are listed\nin Table 2.\nThe AG News corpus (AG) [41] consists of news articles on the web from the AG corpus. It has\nfour classes: World, Sports, Business, and Sci/Tech. Both training and test sets are perfectly balanced.\nThe TREC dataset (TREC) [34] is a dataset for text classiﬁcation consisting of open-domain, fact-\nbased questions divided into broad semantic categories. We use a six-class version (TREC-6). The\nStanford Sentiment Treebank (SST-2) [30] contains phrases with ﬁne-grained sentiment labels in\nthe parse trees of 11,855 sentences from movie reviews. The Movie Review Data (MR) [24] is a\nmovie-review data set of sentences with sentiment labels (positive or negative).\nElectronic Health Records (EHR) [8] dataset contains information on medical treatments. The goal\nis to help an insurance company to detect frauds based on a history of visits of patients to a doctor.\nEach sequence consists of visits with information on a drug code and the amount of money spent at\neach visit. This dataset is imbalanced, as the share of frauds is 1.5%.\nWe have also used two open bank Transaction Datasets (Tr.Age, Tr.Gender). They are aimed at\npredicting age and gender [1, 11]. For each transaction we have the Merchant Category Codes and\nthe decile of transaction amounts. Sequences of transactions provide inputs for predicting a target.\n4.3 Metrics\nTo create an adversarial attack, changes must be applied to the initial sequence. A change can be\ndone either by inserting, deleting, or replacing a token in some position in the original sequence. In\nthe WER calculation, any change to the sequence made by insertion, deletion, or replacement is\ntreated as 1. Therefore, we consider the adversarial sequence to be perfect if WER = 1 and the\ntarget classiﬁer output has changed. For the classiﬁcation task, Normalised Accuracy Drop (NAD) is\ncalculated in the following way:\nNAD(A) = 1\nN\nN∑\ni=1\n1{Ct(xi) ̸= Ct(x′\ni))}\nWER(xi,x′\ni) ,\n7\nNLP datasets AG TREC SST-2 MR\nFGSM 0.96 0.71 0.98 0.94\nDeepFool 0.89 0.71 0.97 0.94\nHotFlip 0.99 0.96 0.99 0.98\nSamplingFool (ours) 0.59 0.60 0.60 0.60\nDILMA, β = 0 (ours) 0.71 0.80 0.68 0.70\nDILMA (ours) 0.70 0.71 0.70 0.68\nDILMA w/ sampling (ours) 0.66 0.69 0.67 0.60\nOther datasets EHR Tr.Age Tr.Gender\nFGSM 0.96 0.61 0.98\nDeepFool 0.95 0.87 0.96\nHotFlip 0.99 0.99 0.99\nSamplingFool (ours) 0.52 0.69 0.69\nDILMA, β = 0 (ours) 0.98 0.71 0.65\nDILMA (ours) 0.98 0.63 0.83\nDILMA w/ sampling (ours) 0.97 0.57 0.83\nTable 4: ROC AUC scores (↓) for Adversarial Discriminator as a countermeasure against adversarial\nattacks: a binary classiﬁcation “adversary vs. non-adversary”. The best values are in bold, the second\nbest values are underscored.\nwhere x′= A(x) is the output of an adversarial generation algorithm for the input sequencex, Ct(x)\nis the label assigned by the target classiﬁcation model, and WER(x′,x) is the Word Error Rate. The\nhighest value of NAD is achieved when WER(x′\ni,xi) = 1 and C(xi) ̸= C(x′\ni) for all i. Here we\nassume that adversaries produce distinct sequences and WER(xi,x′\ni) ≥1.\n4.4 Adversarial attack quality\nResults for the considered methods are presented in Table 3. We demonstrate not only the quality of\nattacks on the initial target classiﬁer, but also the quality of attacks on the target classiﬁer after its\nre-training with additional adversarial samples added to the training set. After re-training the initial\ntarget classiﬁer, HotFlip cannot provide reasonable results, whilst our methods perform only slightly\nworse than before re-training and signiﬁcantly better than HotFlip and other approaches. In case of\nthe Tr.Agedataset, SamplingFool works better than DILMA because of the overall low quality of the\ntarget classiﬁer. We provide additional metrics in supplementary materials.\n4.5 Discriminator defense\nWe have also considered another defense strategy — discriminator training [ 36]. We have trained an\nadditional discriminator on 10,000 samples of the original sequences and adversarial examples. The\ndiscriminator should detect whether an example is normal or adversarial. The discriminator has a\nGRU architecture and has been trained for 5 epochs using the negative log-likelihood loss.\nA high ROC AUC of the discriminator means easy detection of adversarial attacks. The results for\nthe considered methods are presented in Table 4. The discriminator ROC AUC for SamplingFool\nand DILMA is only slightly better than the ROC AUC of a random classiﬁer 0.5. SamplingFool uses\noriginal language model without any parameter tuning, so it is even harder to detect.\n5 Conclusion\nConstructing adversarial attacks for categorical sequences is a challenging problem. Our idea is to\ncombine sampling from a masked language model (MLM) with tuning of its parameters to produce\ntruly adversarial examples. To tune parameters of the MLM we use a loss function based on two\ndifferentiable surrogates — for a distance between sequences and for a classiﬁer. This results in the\nproposed DILMA approach. If we only use sampling from the MLM, we obtain a simple baseline\nSamplingFool.\n8\nTo estimate the efﬁciency of adversarial attacks on categorical sequences we have proposed a metric\ncombining the WER and the accuracy of the target classiﬁer. For considered applications that include\ndiverse datasets on bank transactions, electronic health records, and NLP datasets, our approaches\nshow a good performance. Moreover, in contrast to competing methods, our approaches win over\ncommon strategies used to defend from adversarial attacks.\n6 Broader impact\nOne of the biggest problems with Deep Learning (DL) models is that they lack robustness. An\nexcellent example is provided by adversarial attacks: a small perturbation of a target image fools a DL\nclassiﬁer, which predicts a wrong label for the perturbed image. The original gradient-based approach\nto adversarial attacks is not suitable for categorical sequences (e.g. NLP data), as the gradients are\nnot easy to obtain. Existing approaches to adversarial attacks for categorical sequences often fail to\nkeep the meaning and semantics of the original sequence.\nOur approach is better at this task, as it leverages modern language models like BERT. With the\nhelp of our approach, one can generate meaningful adversarial sequences that are persistent against\nadversarial training and defense strategies based on detection of adversarial examples. This approach\nposes an important question to society: can we delegate the processing of sequential data to AI? An\nexample of a malicious use of such an approach would be an attack on a model that detects Fake\nnews in major social networks, as an adversarial change undetectable to a human eye can render that\nmodel useless.\nWe hope that our work will have a broad impact, as we have made our code and all details of our\nexperiments available to ML community. Moreover, it is easy to use, because it requires only a\nmasked language model to work.\n7 Acknowledgements\nWe thank the Skoltech CDISE HPC Zhores cluster staff for computing cluster provision.\nA Appendix overview\nIn these supplementary notes we provide information and results additional to the main text of the\narticle \"Differentiable language model adversarial attacks on categorical sequence models\". The\nsupplementary notes consist of two sections.\n• More detailed descriptions of models used and their training process are given in Section B.\n• Descriptions of additional experiments are given in Section 4.\nB Models\nIn this section we provide a more detailed description of the considered models and their training,\nso one can reproduce the results in the article and reuse the introduced models. We start with the\ndescription of classiﬁers used in Subsection B.1, then we describe our masked language model in\nSubsection B.2 and the Deep Levenstein model in Subsection B.3.\nAs we aim at adversarial attacks on existing models, we select hyperparameters using values and\nsettings from approaches described in the literature where these models perform well. Metrics\nobtained in performed experiments suggest that it is a reasonable choice and other hyperparameter\nchoices cannot signiﬁcantly improve the quality of considered models. For our attack method we\nselect hyperparameters using grid search. A more detailed description of the used procedure for the\nselection of hyperparameters is given in Section B.4.\nB.1 Sequence classiﬁers\nTo train all models in this work we use Adam optimiser [ 19] with learning rate 0.001 and batch\nsize 64.\n9\nMASK MASK \nMASKER\nTransformer Encoder\nFully connected layer decoder\nTransformer\nFigure 2: Masked Language Model architecture\nArchitecture In all experiments we use two classiﬁers: a target classiﬁer Ct(x) that we attack and\na substitute classiﬁer C(x) that provides differentiable surrogate classiﬁer scores.\nThe target classiﬁer is a Gated Recurrent Unit (GRU) RNN classiﬁer [4]. We apply it in a combina-\ntion with a learned embedding matrix layer Eof shape d×e, where dis the dictionary size and e\nis the embedding dimension. We use e= 100, hidden state size of GRU h= 128, number of GRU\nlayers l= 1, and dropout rate r= 0.1. We use a bi-directional version of GRU, averaging the GRU’s\noutput along the hidden state dimension and passing the resulting vector through one fully-connected\nlayer. As we show in the main paper, these settings provide a reasonable performance for considered\nproblems.\nThe substitute classiﬁer is a Convolutional Neural Network (CNN) for Sentence Classiﬁcation\nproposed in [18]. In our experiments, the CNN consists of multiple convolution layers and max\npooling layers. The CNN has one convolution layer for each of the n-gram ﬁlter sizes. Each\nconvolution operation gives out a vector of size num_filters. We use [3,5] n-gram ﬁlter sizes with\nnum_filters = 8. The size of the learned embedding matrix E remains the same e = 100. The\ndropout ratio during training is set to 0.1. For this classiﬁer the accuracy values provided in the main\npaper are also high.\nTraining As we consider various multiclass classiﬁcation problems, we train both models using\nthe cross-entropy loss for 50 epochs. If no improvement on the validation set occurs during 3 epochs,\nwe stop our training earlier and use the model, which is the most accurate on the validation set. So\nfor most of datasets we run around 8 epochs.\nThe substitute model has access only to 50% of the data, whilst the target model uses the whole\ndataset. We split the dataset into two parts with stratiﬁcation. Results are shown in Subsection 4.\nBoth models demonstrate comparable results for all datasets.\nB.2 Transformer Masked Language Model\nMasked Language Model architecture We use a 4-layer [33] transformer encoder Masked Lan-\nguage Model (MLM) with the embedding of dimension of 64 and 4 attention heads.\nTo train the Masked Language Model, we randomly replace or mask some of the tokens from the\ninput, and the objective is to predict the original dictionary id of the masked sequence. The MLM can\nbe seen as a generative model that captures the distribution of the data and can estimate probabilities\np(x′|x) and p(x) [26].\nIn our experiments, we follow the ideas from [6] and train a BERT-like LM model. The architecture\nof the model is shown in Figure 2.\n1. The initial input to our model is a sequence x = (x1,x2,...,x t).\n2. The masking layer randomly adds noise to the sequence by masking and replacing some\ntokens x →ˆx.\n10\nEncoder\nVector Vector Vector \nFully connected layer\nInitial\nsequences\nEmbeddings\nDeep\nLevenstein\ndistace\nFigure 3: Deep Levenstein Model architecture\n3. We pass ˆx to the multi-layer bidirectional Transformer encoder [33] and receive a hidden\nrepresentation hi for each token in the sequence.\n4. Then we pass the resulting sequence of hidden representations hi, i = 1,...,t through\none fully-connected layer, where the output shape equals the dictionary size d, to get logits\nP = {p1,..., pt}∈ Rd·t for each index 1,...,t , where dis the size of the dictionary of\ntokens.\n5. On top of each logit vector pi we can apply argmax to get for each token i= 1,...,t the\nindex x′\ni with the maximum logit value.\n6. Using logits P we can estimate p(x′|x).\nTraining We train the transformer MLM in the way similar to [6]. For a sequence x, BERT ﬁrst\nconstructs a corrupted version ˆx by randomly replacing tokens with a special symbol [MASK] or\nother tokens. Then the training objective is to reconstruct masked tokens ¯x from ˆx.\nThe masking module changes tokens to [MASK] with a 50% probability and replaces tokens with\nother random tokens with a 10% probability to get ˆx from x. The output probabilities for a token at\ni-th place are:\nqij = exp (pij)\n∑d\nk=1 exp (pik)\n.\nAs the loss function we use the cross-entropy loss for generated tokens, or in other words we maximise\npθ(x|ˆx) →maxθ with respect to the parameters of the Transformer θ. We obtain a single MLM for\nNLP datasets by aggregating all available data and training the MLM for 50 epochs. For non-NLP\ndatasets, we train separate MLMs for the same number of epochs.\nB.3 Deep Levenstein\nArchitecture The Deep Levenstein model DL(x,y) receives two sequences (x,y). It encodes\nthem into a dense representation of ﬁxed length lusing the shared encoder zx = E(x), zy = E(y).\nThen it concatenates the representations and the absolute difference between them in a vector\n(zy,zy,|zy −zy|) of length 3l. At the end, the model uses a fully-connected layer. The architecture\nis similar to the one proposed in [5]. The schematic description of the architecture is presented in\nFigure 3.\nSample generation and training To estimate the parameters of the encoder and the fully connected\nlayer we use the L2 loss between true and model values of the WER distance. We form a training\nsample of size of about two million data points by sampling sequences and their modiﬁcations from\nthe training data. To collect the training data for constructing DL(x,x′), we use sequences randomly\nselected from a training dataset. As a result, we get pairs, where each pair contains a sequence and\n11\na close but different sequence obtained after application of the masking. We also add pairs with\ndissimilar sequences, where each sequence is randomly selected from the training data to get a better\ncoverage. It total we generate, around two million of pairs.\nB.4 Hyperparameters selection\nWe use the following procedure to select hyperparameters for the DILMA attack.\n1. Try each combination of hyperparameters from the grid deﬁned as a Cartesian product for\nvalues in Table 5\n2. Select the vector of hyperparameters with the best NAD metric.\nHyperparameter Considered values\nLearning rate α 0.01,0.05,0.1,0.5,1\nCoefﬁcient βin loss function L 0.5,1,3,5,10\nNumber of Gumbel samples m 1,2,3,..., 10\nSampling temperature τ 1,1.1,..., 2\nSubset of updated parameters θ all (0), only linear (1), linear and 3rd layer (2),\nlinear and 3rd and 2nd layer (3)\nTable 5: Set of possible hyperparameters during grid search\nFor the considered datasets we obtain the hyperparameters presented in Table 6. For DILMA with\nsampling we use the same hyperparameters. An ablation study for the considered hyperparameters is\npresented in Section C.2.\nDataset α β m τ Subset ind.\nAG 0.01 1 1 1 .5 2\nMR 0.01 10 3 1 .8 1\nSST 0.01 5 1 1 .5 2\nTREC 0.01 5 10 1 .8 1\nEHR 0.1 5 3 1 .8 1\nTr.Age 1 1 10 1 .2 2\nTr.Gender 0.1 10 10 2 .0 2\nTable 6: Selected hyperparameters for each dataset\nB.5 Hyperparameters for other attacks\nFor HotFlip there are no hyperparameters. FSGM and DeepFool also have a small number of\nhyperparameters. The number of steps is set to 10 for both of them and the coefﬁcients before the\nstep direction are set to 0.1 for FGSM and 1.05 for DeepFool. We have tried a number of values,\nand have selected the best ones according to NAD, whilst the quality difference has been almost\nnegligible.\nC Experiments\nC.1 Mean probability difference and WER metrics\nThe NAD metric uniﬁes the WER (word error rate) and true class probability difference for a particular\nexample. But these two metrics can also provide additional insight on the quality of considered\napproaches. The WER for each pair of an initial sequence x and an adversarial sequence x′is the\nword error rate between them, WER(x,x′). The value of the WER should be as close as possible to\n1. For the probability difference we consider the probability py(x) of the true class yfor the initial\nsequence x and the adversarial sequence py(x′). The probability difference py(x)−py(x′) should be\nas high as possible, as the adversarial sequence should have a very low score for the true class of the\ninitial sequence. These two metrics are conﬂicting, so we typically can increase the mean probability\ndifference by simultaneously worsening (increasing) the WER. For DILMA, we can adjust the target\n12\nNLP datasets AG TREC SST-2 MR\nFGSM 1.01 0.98 1.24 1.00\nDeepFool 0.97 0.92 1.18 0.98\nHotFlip 1.26 1.29 0.96 1.04\nSamplingFool (ours) 1.37 1.64 1.24 1.96\nDILMA (ours) 2.10 1.53 1.91 2.62\nDILMA w/ sampling (ours) 1.74 1.32 1.54 1.97\nOther datasets EHR Tr.Age Tr.Gender\nFGSM 1.42 0.33 1.00\nDeepFool 1.42 0.70 0.86\nHotFlip 1.91 1.67 1.01\nSamplingFool (ours) 1.28 1.12 1.33\nDILMA (ours) 3.98 2.28 2.70\nDILMA w/ sampling (ours) 4.06 1.51 1.91\nTable 7: The average WER metric (the closer to 1, the better) for FGSM, DeepFool, HotFlip,\nSamplingFool, DILMA and DILMA w/ sampling adversarial algorithms. As WER is the same before\nand after retraining of the main classiﬁer, we provide only one number. The best values are in bold,\nthe second best values are underscored. The best value of WER is one: we change one token, and\nget an adversarial example, so we consider absolute difference between 1 and observed WER for\nhighlighting best approaches.\nNLP datasets AG TREC SST-2 MR\nFGSM 0.46 / 0.43 0.32 / 0.02 0.64 / 0.02 0.23 / 0.01\nDeepFool 0.37 / 0.17 0.27 / 0.00 0.58 / 0.05 0.20 / 0.01\nHotFlip 0.62 / 0.26 0.53 / 0.17 0.69 / -0.05 0.27 / -0.04\nSamplingFool (ours) 0.34 / 0.31 0.28 / 0.25 0.48 / 0.30 0.16 / 0.14\nDILMA (ours) 0.48 / 0.47 0.52 / 0.36 0.60 / 0.46 0.28 / 0.19\nDILMA w/ sampling (ours) 0.52 / 0.49 0.56 / 0.41 0.63 / 0.43 0.27 / 0.20\nOther datasets EHR Tr.Age Tr.Gender\nFGSM 0.34 / 0.22 0.02 / 0.01 0.42 / 0.14\nDeepFool 0.38 / 0.20 0.07 / 0.05 0.30 / 0.01\nHotFlip 0.61 / 0.23 0.20 / -0.17 0.48 / 0.16\nSamplingFool (ours) 0.04 / 0.03 0.15 / 0.15 0.27 / 0.23\nDILMA (ours) 0.28 / 0.34 0.16 / 0.15 0.25 / 0.27\nDILMA w/ sampling (ours) 0.35 / 0.39 0.16 / 0.16 0.29 / 0.30\nTable 8: The mean probability difference metric (↑) before/after adversarial training on 5000 examples.\nThe best values are inbold, the second best values areunderscored. DILMA is resistant to adversarial\ntraining.\n13\nFigure 4: Hyperparameters grid search for the Tr.Gender dataset. Each point corresponds to a\nconﬁguration of hyperparameters. The whole ﬁgure shows the correspondence between NAD,\naverage WER, and average adversarial probability drop metrics.\nWER by modifying hyperparameters (see experiments in Subsection C.2), for other approaches it is\nmuch harder to select an appropriate trade-off.\nIn a way similar to the main text, we measure the quality by averaging the metrics over 10,000\nadversarial examples. In Table 7 we provide the WER for considered approaches before and after\nretraining. In Table 8 we provide the mean probability difference for the target class for considered\napproaches before and after retraining. We see that we can make similar conclusions when comparing\nthe quality of considered approaches.\nC.2 Ablation study\nIn this subsection we provide an ablation study for our metric NAD and for our approach DILMA.\nWe vary the learning rate, the coefﬁcient before the loss related to the classiﬁer α, the number of\nGumbel samples, and the temperature for sampling τ to generate a grid. Each point in Figures 4\nand 5 corresponds to quality metrics for selected four hyperparameters. For each conﬁguration\nwe generated 10000 adversarial examples and evaluated 3 quality metrics: the accuracy drop for a\nclassiﬁer, the word error rate, and our metric NAD. We see that we can vary the WER by changing\nhyperparameters. In general for DILMA the performance depends on the choice of hyperparameters.\nHowever, we observe that we always can select the same hyperparameters setting that provides good\nperformance across diverse datasets.\nReferences\n[1] Sberbank Age transcation dataset. https://onti.ai-academy.ru/competition. Ac-\ncessed: 2020-07-02.\n[2] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer\nvision: A survey. IEEE Access, 6:14410–14430, 2018.\n14\nFigure 5: Hyperparameters grid search for the Tr.Gender dataset. Each point corresponds to a\nconﬁguration of hyperparameters. The whole ﬁgure shows the correspondence between NAD, average\nWER, and average adversarial probability drop metrics for different values of hyperparameters.\n[3] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating\nthe robustness of sequence-to-sequence models with adversarial examples. arXiv preprint\narXiv:1803.01128, 2018.\n[4] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling, 2014.\n[5] Xinyan Dai, Xiao Yan, Kaiwen Zhou, Yuxuan Wang, Han Yang, and James Cheng. Convolu-\ntional embedding for edit distance, 2020.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding, 2018.\n[7] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip: White-box adversarial\nexamples for text classiﬁcation. In 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 31–36, 2018.\n[8] I Fursov, A Zaytsev, R Khasyanov, M Spindler, and E Burnaev. Sequence embeddings help to\nidentify fraudulent cases in healthcare insurance. arXiv preprint arXiv:1910.03072, 2019.\n[9] Ivan Fursov, Alexey Zaytsev, Nikita Kluchnikov, Andrey Kravchenko, and Evgeny Burnaev.\nGradient-based adversarial attacks on categorical sequence models via traversing an embedded\nworld. arXiv preprint arXiv:2003.04173, 2020.\n[10] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial\ntext sequences to evade deep learning classiﬁers. In IEEE Security and Privacy Workshops,\npages 50–56. IEEE, 2018.\n[11] Bank gender transcation dataset. https://www.kaggle.com/c/\npython-and-analyze-data-final-project/data . Accessed: 2020-07-02.\n[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-\nial examples. arXiv preprint arXiv:1412.6572, 2014.\n[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network,\n2015.\n[14] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax,\n2016.\n[15] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumble-softmax. In\nInternational Conference on Learning Representations (ICLR 2017). OpenReview. net, 2017.\n15\n[16] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Pete Szolovits. Is bert really robust? a strong baseline\nfor natural language attack on text classiﬁcation and entailment. In AAAI, 2020.\n[17] Valentin Khrulkov and Ivan Oseledets. Art of singular vectors and universal adversarial\nperturbations. In IEEE CVPR, pages 8562–8570, 2018.\n[18] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages\n1746–1751, Doha, Qatar, October 2014. Association for Computational Linguistics.\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[20] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale.\nhttps://arxiv.org/abs/1611.01236, 2017.\n[21] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text\nclassiﬁcation can be fooled. arXiv preprint arXiv:1704.08006, 2017.\n[22] Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. Multimodal named entity recognition\nfor short social media posts. In Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 852–860, 2018.\n[23] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple\nand accurate method to fool deep neural networks, 2015.\n[24] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up? sentiment classiﬁcation\nusing machine learning techniques. In EMNLP, pages 79–86, 2002.\n[25] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adver-\nsarial input sequences for recurrent neural networks. In MILCOM 2016-2016 IEEE Military\nCommunications Conference, pages 49–54. IEEE, 2016.\n[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. 2018.\n[27] Yankun Ren, Jianbin Lin, Siliang Tang, Jun Zhou, Shuang Yang, Yuan Qi, and Xiang Ren.\nGenerating natural language adversarial examples on a large scale with generative models.\narXiv preprint arXiv:2003.10388, 2020.\n[28] Suranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. arXiv\npreprint arXiv:1707.02812, 2017.\n[29] Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto. Interpretable adversarial\nperturbation in input embedding space for text. In IJCAI, 2018.\n[30] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association\nfor Computational Linguistics.\n[31] Lichao Sun, Ji Wang, Philip S Yu, and Bo Li. Adversarial attack and defense on graph data: A\nsurvey. arXiv preprint arXiv:1812.10528, 2018.\n[32] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna Estrach, Dumitru Erhan, Ian\nGoodfellow, and Robert Fergus. Intriguing properties of neural networks. In ICLR, 2014.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998–6008,\n2017.\n[34] Ellen M. V oorhees and Dawn M. Tice. The trec-8 question answering track evaluation. In\nTREC, 1999.\n[35] Wenqi Wang, Benxiao Tang, Run Wang, Lina Wang, and Aoshuang Ye. A survey on adversarial\nattacks and defenses in text. arXiv preprint arXiv:1902.07285, 2019.\n[36] Han Xu, Yao Ma, Haochen Liu, Debayan Deb, Hui Liu, Jiliang Tang, and Anil Jain. Adversarial\nattacks and defenses in images, graphs and text: A review. arXiv preprint arXiv:1909.08072,\n2019.\n16\n[37] Jincheng Xu and Qingfeng Du. Texttricker: Loss-based and gradient-based adversarial attacks\non text classiﬁcation models. Engineering Applications of Artiﬁcial Intelligence, 92:103641,\n2020.\n[38] Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved\nvariational autoencoders for text modeling using dilated convolutions. In ICML, pages 3881–\n3890. JMLR. org, 2017.\n[39] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses\nfor deep learning. IEEE transactions on neural networks and learning systems, 30(9):2805–\n2824, 2019.\n[40] Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and CHENLIANG LI. Adversarial\nattacks on deep learning models in natural language processing: A survey. arXiv preprint\narXiv:1901.06796, 2019.\n[41] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassiﬁcation, 2015.\n[42] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. Adversarial attacks on neural\nnetworks for graph data. In 24th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 2847–2856, 2018.\n17",
  "topic": "Categorical variable",
  "concepts": [
    {
      "name": "Categorical variable",
      "score": 0.7242496013641357
    },
    {
      "name": "Adversarial system",
      "score": 0.6901088953018188
    },
    {
      "name": "Sequence (biology)",
      "score": 0.66294926404953
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5742653608322144
    },
    {
      "name": "Differentiable function",
      "score": 0.5731390714645386
    },
    {
      "name": "Computer science",
      "score": 0.5367461442947388
    },
    {
      "name": "Natural language processing",
      "score": 0.4670718312263489
    },
    {
      "name": "Machine learning",
      "score": 0.2898235619068146
    },
    {
      "name": "Mathematics",
      "score": 0.24498429894447327
    },
    {
      "name": "Pure mathematics",
      "score": 0.07365363836288452
    },
    {
      "name": "Chemistry",
      "score": 0.06755369901657104
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}