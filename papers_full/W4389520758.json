{
  "title": "Query2doc: Query Expansion with Large Language Models",
  "url": "https://openalex.org/W4389520758",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1983828941",
      "name": "Liang Wang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2087975791",
      "name": "Yang Nan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297899309",
    "https://openalex.org/W3206455169",
    "https://openalex.org/W2740492458",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W2164547069",
    "https://openalex.org/W4385565351",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4306311899",
    "https://openalex.org/W4246858749",
    "https://openalex.org/W3174203100",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W4287780085",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W3175111331",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2937036051",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W1966093341",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3154280800",
    "https://openalex.org/W2470818894",
    "https://openalex.org/W4206121183",
    "https://openalex.org/W4385571915",
    "https://openalex.org/W3203288040",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9414–9423\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nQuery2doc: Query Expansion with Large Language Models\nLiang Wang and Nan Yang and Furu Wei\nMicrosoft Research\n{wangliang,nanya,fuwei}@microsoft.com\nAbstract\nThis paper introduces a simple yet effec-\ntive query expansion approach, denoted as\nquery2doc, to improve both sparse and dense\nretrieval systems. The proposed method\nﬁrst generates pseudo-documents by few-shot\nprompting large language models (LLMs), and\nthen expands the query with generated pseudo-\ndocuments. LLMs are trained on web-scale\ntext corpora and are adept at knowledge mem-\norization. The pseudo-documents from LLMs\noften contain highly relevant information that\ncan aid in query disambiguation and guide\nthe retrievers. Experimental results demon-\nstrate that query2doc boosts the performance\nof BM25 by 3% to 15% on ad-hoc IR datasets,\nsuch as MS-MARCO and TREC DL, with-\nout any model ﬁne-tuning. Furthermore, our\nmethod also beneﬁts state-of-the-art dense re-\ntrievers in terms of both in-domain and out-of-\ndomain results.\n1 Introduction\nInformation retrieval (IR) aims to locate relevant\ndocuments from a large corpus given a user is-\nsued query. It is a core component in modern\nsearch engines and researchers have invested for\ndecades in this ﬁeld. There are two mainstream\nparadigms for IR: lexical-based sparse retrieval,\nsuch as BM25, and embedding-based dense re-\ntrieval (Xiong et al., 2021; Qu et al., 2021). Al-\nthough dense retrievers perform better when large\namounts of labeled data are available (Karpukhin\net al., 2020), BM25 remains competitive on out-of-\ndomain datasets (Thakur et al., 2021).\nQuery expansion (Rocchio, 1971; Lavrenko\nand Croft, 2001) is a long-standing technique\nthat rewrites the query based on pseudo-relevance\nfeedback or external knowledge sources such as\nWordNet. For sparse retrieval, it can help bridge\nthe lexical gap between the query and the docu-\nments. However, query expansion methods like\nRM3 (Lavrenko and Croft, 2001; Lv and Zhai,\n2009) have only shown limited success on popular\ndatasets (Campos et al., 2016), and most state-of-\nthe-art dense retrievers do not adopt this technique.\nIn the meantime, document expansion methods like\ndoc2query (Nogueira et al., 2019) have proven to\nbe effective for sparse retrieval.\nIn this paper, we demonstrate the effectiveness\nof LLMs (Brown et al., 2020) as query expan-\nsion models by generating pseudo-documents con-\nditioned on few-shot prompts. Given that search\nqueries are often short, ambiguous, or lack neces-\nsary background information, LLMs can provide\nrelevant information to guide retrieval systems, as\nthey memorize an enormous amount of knowledge\nand language patterns by pre-training on trillions\nof tokens.\nOur proposed method, called query2doc, gen-\nerates pseudo-documents by few-shot prompting\nLLMs and concatenates them with the original\nquery to form a new query. This method is simple\nto implement and does not require any changes in\ntraining pipelines or model architectures, making it\northogonal to the progress in the ﬁeld of LLMs and\ninformation retrieval. Future methods can easily\nbuild upon our query expansion framework.\nFor in-domain evaluation, we adopt the MS-\nMARCO passage ranking (Campos et al., 2016),\nTREC DL 2019 and 2020 datasets. Pseudo-\ndocuments are generated by prompting an im-\nproved version of GPT-3 text-davinci-003 from\nOpenAI (Brown et al., 2020). Results show\nthat query2doc substantially improves the off-the-\nshelf BM25 algorithm without ﬁne-tuning any\nmodel, particularly for hard queries from the\nTREC DL track. Strong dense retrievers, in-\ncluding DPR (Karpukhin et al., 2020), SimLM\n(Wang et al., 2023), and E5 (Wang et al., 2022)\nalso beneﬁt from query2doc, although the gains\ntend to be diminishing when distilling from a\nstrong cross-encoder based re-ranker. Experi-\nments in zero-shot OOD settings demonstrate that\n9414\nour method outperforms strong baselines on most\ndatasets. Further analysis also reveals the im-\nportance of model scales: query2doc works best\nwhen combined with the most capable LLMs while\nsmall language models only provide marginal im-\nprovements over baselines. To aid reproduction,\nwe release all the generations from text-davinci-\n003 at https://huggingface.co/datasets/\nintfloat/query2doc_msmarco.\n2 Method\nWrite a passage that answers the given query:\nQuery: what state is this zip code 85282\nPassage: Welcome to TEMPE, AZ 85282. \n85282 is a rural zip code in Tempe, Arizona. \nThe population is primarily white…\n…\nQuery: when was pokemon green released\nPassage:\nLLM Prompts\nPokemon Green was released in Japan on \nFebruary 27th, 1996. It was the first in the \nPokemon series of games and served as the \nbasis for Pokemon Red and Blue, which were \nreleased in the US in 1998. The original \nPokemon Green remains a beloved classic \namong fans of the series.\nLLM Output\nFigure 1: Illustration of query2doc few-shot prompting.\nWe omit some in-context examples for space reasons.\nGiven a query q, we employ few-shot prompting\nto generate a pseudo-document d′as depicted in\nFigure 1. The prompt comprises a brief instruction\n“Write a passage that answers the given query:”and\nklabeled pairs randomly sampled from a training\nset. We use k = 4throughout this paper. Subse-\nquently, we rewrite q to a new query q+ by con-\ncatenating with the pseudo-document d′. There are\nslight differences in the concatenation operation\nfor sparse and dense retrievers, which we elaborate\non in the following section.\nSparse Retrieval Since the query q is typically\nmuch shorter than pseudo-documents, to balance\nthe relative weights of the query and the pseudo-\ndocument, we boost the query term weights by\nrepeating the query ntimes before concatenating\nwith the pseudo-document d′:\nq+ = concat({q}×n, d′) (1)\nHere, “concat” denotes the string concatenation\nfunction. q+ is used as the new query for BM25\nretrieval. We ﬁnd that n = 5is a generally good\nvalue and do not tune it on a dataset basis.\nDense Retrieval The new query q+ is a sim-\nple concatenation of the original query qand the\npseudo-document d′separated by [SEP]:\nq+ = concat(q, [SEP], d′) (2)\nFor training dense retrievers, several factors can\ninﬂuence the ﬁnal performance, such as hard nega-\ntive mining (Xiong et al., 2021), intermediate pre-\ntraining (Gao and Callan, 2021), and knowledge\ndistillation from a cross-encoder based re-ranker\n(Qu et al., 2021). In this paper, we investigate two\nsettings to gain a more comprehensive understand-\ning of our method. The ﬁrst setting is training DPR\n(Karpukhin et al., 2020) models initialized from\nBERTbase with BM25 hard negatives only. The op-\ntimization objective is a standard contrastive loss:\nLcont = −log ehq·hd\nehq·hd + ∑\ndi∈N ehq·hdi\n(3)\nwhere hq and hd represent the embeddings for the\nquery and document, respectively. N denotes the\nset of hard negatives.\nThe second setting is to build upon state-of-the-\nart dense retrievers and use KL divergence to distill\nfrom a cross-encoder teacher model.\nmin DKL(pce,pstu) +αLcont (4)\npce and pstu are the probabilities from the cross-\nencoder and our student model, respectively. α\nis a coefﬁcient to balance the distillation loss and\ncontrastive loss.\nComparison with Pseudo-relevance Feedback\nOur proposed method is related to the clas-\nsic method of pseudo-relevance feedback (PRF)\n(Lavrenko and Croft, 2001; Lv and Zhai, 2009). In\nconventional PRF, the feedback signals for query\nexpansion come from the top-k documents ob-\ntained in the initial retrieval step, while our method\nprompts LLMs to generate pseudo-documents. Our\nmethod does not rely on the quality of the initial\nretrieval results, which are often noisy or irrelevant.\nRather, it exploits cutting-edge LLMs to generate\ndocuments that are more likely to contain relevant\nterms.\n9415\nMethod Fine-tuning MS MARCO dev TREC DL 19 TREC DL 20\nMRR@10 R@50 R@1k nDCG@10 nDCG@10\nSparse retrieval\nBM25 \u0017 18.4 58.5 85.7 51.2 ∗ 47.7∗\n+ query2doc \u0017 21.4+3.0 65.3+6.8 91.8+6.1 66.2+15.0 62.9+15.2\nBM25 + RM3 \u0017 15.8 56.7 86.4 52.2 47.4\ndocT5query (Nogueira and Lin) \u0013 27.7 75.6 94.7 64.2 -\nDense retrieval w/o distillation\nANCE (Xiong et al., 2021) \u0013 33.0 - 95.9 64.5 64.6\nHyDE (Gao et al., 2022) \u0017 - - - 61.3 57.9\nDPRbert-base(our impl.) \u0013 33.7 80.5 95.9 64.7 64.1\n+ query2doc \u0013 35.1+1.4 82.6+2.1 97.2+1.3 68.7+4.0 67.1+3.0\nDense retrieval w/ distillation\nRocketQAv2 (Ren et al., 2021) \u0013 38.8 86.2 98.1 - -\nAR2 (Zhang et al., 2022) \u0013 39.5 87.8 98.6 - -\nSimLM (Wang et al., 2023) \u0013 41.1 87.8 98.7 71.4 69.7\n+ query2doc \u0013 41.5+0.4 88.0+0.2 98.8+0.1 72.9+1.5 71.6+1.9\nE5base+ KD (Wang et al., 2022) \u0013 40.7 87.6 98.6 74.3 70.7\n+ query2doc \u0013 41.5+0.8 88.1+0.5 98.7+0.1 74.9+0.6 72.5+1.8\nTable 1: Main results on the MS-MARCO passage ranking and TREC datasets. The “Fine-tuning” column indi-\ncates whether the method requires ﬁne-tuning model on labeled data or not. ∗: our reproduction.\n3 Experiments\n3.1 Setup\nEvaluation Datasets For in-domain evaluation,\nwe utilize the MS-MARCO passage ranking (Cam-\npos et al., 2016), TREC DL 2019 (Craswell et al.,\n2020a) and 2020 (Craswell et al., 2020b) datasets.\nFor zero-shot out-of-domain evaluation, we select\nﬁve low-resource datasets from the BEIR bench-\nmark (Thakur et al., 2021). The evaluation met-\nrics include MRR@10, R@k (k ∈{50,1k}), and\nnDCG@10.\nHyperparameters For sparse retrieval including\nBM25 and RM3, we adopt the default implementa-\ntion from Pyserini (Lin et al., 2021). When training\ndense retrievers, we use mostly the same hyper-\nparameters as SimLM (Wang et al., 2023), with\nthe exception of increasing the maximum query\nlength to 144 to include pseudo-documents. When\nprompting LLMs, we include 4 in-context exam-\nples and use the default temperature of 1 to sample\nat most 128 tokens. For further details, please refer\nto Appendix A.\n3.2 Main Results\nIn Table 1, we list the results on the MS-MARCO\npassage ranking and TREC DL datasets. For sparse\nretrieval, “BM25 + query2doc” beats the BM25\nbaseline with over 15% improvements on TREC\nDL 2019 and 2020 datasets. Our manual inspection\nreveals that most queries from the TREC DL track\nare long-tailed entity-centric queries, which beneﬁt\nmore from the exact lexical match. The traditional\nquery expansion method RM3 only marginally\nimproves the R@1k metric. Although the docu-\nment expansion method docT5query achieves bet-\nter numbers on the MS-MARCO dev set, it requires\ntraining a T5-based query generator with all the\navailable labeled data, while “BM25 + query2doc”\ndoes not require any model ﬁne-tuning.\nFor dense retrieval, the model variants that com-\nbine with query2doc also outperform the corre-\nsponding baselines on all metrics. However, the\ngain brought by query2doc tends to diminish when\nusing intermediate pre-training or knowledge distil-\nlation from cross-encoder re-rankers, as shown by\nthe “SimLM + query2doc” and “E5 + query2doc”\nresults.\nFor zero-shot out-of-domain retrieval, the results\nare mixed as shown in Table 2. Entity-centric\ndatasets like DBpedia see the largest improvements.\nOn the NFCorpus and Scifact datasets, we observe\na minor decrease in ranking quality. This is likely\ndue to the distribution mismatch between training\nand evaluation.\n4 Analysis\nScaling up LLMs is Critical For our proposed\nmethod, a question that naturally arises is: how\ndoes the model scale affect the quality of query\nexpansion? Table 3 shows that the performance\nsteadily improves as we go from the 1.3B model\n9416\nDBpedia NFCorpus Scifact Trec-Covid Touche2020\nBM25 31.3 32.5 66.5 65.6 36.7\n+ query2doc 37.0 +5.7 34.9+2.4 68.6+2.1 72.2+6.6 39.8+3.1\nSimLM (Wang et al., 2023) 34.9 32.7 62.4 55.0 18.9\n+ query2doc 38.3 +3.4 32.1-0.6 59.5-2.9 59.9+4.9 25.6+6.7\nE5base + KD (Wang et al., 2022) 40.7 35.0 70.4 74.1 30.9\n+ query2doc 42.4+1.7 35.2+0.2 67.5-2.9 75.1+1.0 31.7+0.8\nTable 2: Zero-shot out-of-domain results on 5 low-resource datasets from the BEIR benchmark (Thakur et al.,\n2021). The reported numbers are nDCG@10. For a fair comparison, the in-context examples for prompting LLMs\ncome from the MS-MARCO training set.\n# params TREC 19 TREC 20\nBM25 - 51.2 47.7\nw/ babbage 1.3B 52.0 50.2\nw/ curie 6.7B 55.1 50.1\nw/ davinci-001 175B 63.5 58.2\nw/ davinci-003 175B 66.2 62.9\nw/ gpt-4 - 69.2 64.5\nTable 3: Query expansion with different model sizes.\nEven though GPT-4 performs best, we are unable to\napply it in the main experiments due to quota limits.\nto 175B models. Empirically, the texts generated\nby smaller language models tend to be shorter and\ncontain more factual errors. Also, the “davinci-003”\nmodel outperforms its earlier version “davinci-001”\nby using better training data and improved instruc-\ntion tuning. The recently released GPT-4 (OpenAI,\n2023) achieves the best results.\n1 10 30 50 100\n% labeled data for fine-tuning\n20\n22\n24\n26\n28\n30\n32\n34\n36MRR on dev set\n21.4\n27.3\n31.4\n32.8\n33.7\n22.7\n28.5\n32.1\n34.1\n35.1\nDPR w/o query2doc\nDPR w/ query2doc\nFigure 2: MRR on MS-MARCO dev set w.r.t the per-\ncentage of labeled data used for ﬁne-tuning.\nPerformance Gains are Consistent across Data\nScales Figure 2 presents a comparison between\ntwo variants of DPR models, which differ in the\namount of labeled data used. The results show\nthat the “DPR + query2doc” variant consistently\noutperforms the DPR baseline by approximately\n1%, regardless of the amount of data used for ﬁne-\ntuning. This observation highlights that our contri-\nbution is orthogonal to the continual scaling up of\nsupervision signals.\nTREC 19 TREC 20\nBM25 + query2doc 66.2 62.9\nw/ query only 51.2 47.7\nw/ pseudo-doc only 48.7 44.5\nTable 4: Using the concatenation of the original query\nand the generated pseudo-documents perform substan-\ntially better.\nHow to Use Pseudo-documents In this paper,\nwe concatenate the original query and pseudo-\ndocuments as the new query. Alternatively, one can\nsolely use the pseudo-documents, as done in the\napproach of HyDE (Gao et al., 2022). The results\npresented in Table 4 demonstrate that the original\nquery and pseudo-documents are complementary,\nand their combination leads to substantially better\nperformance in sparse retrieval.\nCase Analysis In Table 5, we show two queries\nalong with their respective pseudo-documents and\ngroundtruth. The pseudo-documents, which are\ngenerated by LLMs, offer detailed and mostly ac-\ncurate information, thereby reducing the lexical\nmismatch between the query and documents. In\nsome cases, the pseudo-documents are sufﬁcient to\nmeet the user’s information needs, rendering the\nretrieval step unnecessary. However, it is worth\nnoting that the LLM generations may contain fac-\ntual errors. For instance, in the second query, the\ntheme song \"It’s a Jungle Out There\" was used as\nof season two in 2003, not 2002 1. Although such\nerrors may appear subtle and difﬁcult to verify, they\npose a signiﬁcant challenge to building trustworthy\nsystems using LLMs.\n1Refer to https://en.wikipedia.org/wiki/It’s_a_\nJungle_Out_There_(song)\n9417\nquery who killed nicholas ii of russia\nLLM generationNicholas II of Russia, thelast Tsarof Russia, was executed byBolshevik revolutionarieson\nJuly 17th,1918, along with his wife Alexandra andtheir ﬁve children. The execution was\nordered by VladimirLenin, the leader of the Bolshevik Party. The actual execution was carried\nout by a ﬁring squad of Bolshevik troops, and the bodies were later buried in a mass grave. . .\nGroundtruth (1868-1918). Nicholas II was thelastemperor, ortsar, of Russia, serving from 1894 to\n1917. Nicholas, his wife, andtheir ﬁve childrenwere killed by theBolsheviks, members of a\nrevolutionarygroup (led byLenin) who seized control of the government in Russia during\nthe October Revolution (see Russian Revolution) of 1917.\nquery who sings monk theme song\nLLM generationThe theme song for the television show Monk is entitled \"It’s a Jungle Out There\" and is\nsung by American singer-songwriterRandy Newman. The song was written speciﬁcally for\nthe show, andit has been used as the theme song since the series premiered in 2002. It has\nbeen praised by critics and fans alike and is often regarded as one of the best theme songs in\ntelevision history.\nGroundtruth exists and is an alternate of. The Monk theme song isIt’s a Jungle Out TherebyRandy\nNewman. The Monk theme song isIt’s a Jungle Out TherebyRandy Newman.\nTable 5: Examples from the TREC DL 2020 dataset. Bold texts are the overlapping words between groundtruth\nand pseudo-documents generated from LLMs. The italicized red sentence demonstrates a factual error in language\nmodel generations.\n5 Related Work\nQuery Expansion and Document Expansion\nare two classical techniques to improve retrieval\nquality, particularly for sparse retrieval systems.\nBoth techniques aim to minimize the lexical gap be-\ntween the query and the documents. Query expan-\nsion typically involves rewriting the query based\non relevance feedback (Lavrenko and Croft, 2001;\nRocchio, 1971) or lexical resources such as Word-\nNet (Miller, 1992). In cases where labels are not\navailable, the top-k retrieved documents can serve\nas pseudo-relevance feedback signals (Lv and Zhai,\n2009). Liu et al. ﬁne-tunes an encoder-decoder\nmodel to generate contextual clues.\nIn contrast, document expansion enriches the\ndocument representation by appending additional\nrelevant terms. Doc2query (Nogueira et al., 2019)\ntrains a seq2seq model to predict pseudo-queries\nbased on documents and then adds generated\npseudo-queries to the document index. Learned\nsparse retrieval models such as SPLADE (Formal\net al., 2021) and uniCOIL (Lin and Ma, 2021) also\nlearn document term weighting in an end-to-end\nfashion. However, most state-of-the-art dense re-\ntrievers (Ren et al., 2021; Wang et al., 2023) do not\nadopt any expansion techniques. Our paper demon-\nstrates that strong dense retrievers also beneﬁt from\nquery expansion using LLMs.\nLarge Language Models (LLMs) such as GPT-3\n(Brown et al., 2020), PaLM (Chowdhery et al.,\n2022), and LLaMA (Touvron et al., 2023) are\ntrained on trillions of tokens with billions of param-\neters, exhibiting unparalleled generalization ability\nacross various tasks. LLMs can follow instruc-\ntions in a zero-shot manner or conduct in-context\nlearning through few-shot prompting. Labeling a\nfew high-quality examples only requires minimal\nhuman effort. In this paper, we employ few-shot\nprompting to generate pseudo-documents from a\ngiven query. A closely related recent work HyDE\n(Gao et al., 2022) instead focuses on the zero-\nshot setting and uses embeddings of the pseudo-\ndocuments for similarity search. HyDE implicitly\nassumes that the groundtruth document and pseudo-\ndocuments express the same semantics in different\nwords, which may not hold for some queries. In the\nﬁeld of question answering, RECITE (Sun et al.,\n2022) and GENREAD (Yu et al., 2022) demon-\nstrate that LLMs are powerful context generators\nand can encode abundant factual knowledge. How-\never, as our analysis shows, LLMs can sometimes\ngenerate false claims, hindering their practical ap-\nplication in critical areas.\n6 Conclusion\nThis paper presents a simple method query2doc\nto leverage LLMs for query expansion. It ﬁrst\nprompts LLMs with few-shot examples to gener-\nate pseudo-documents and then integrates with ex-\nisting sparse or dense retrievers by augmenting\nqueries with generated pseudo-documents. The un-\nderlying motivation is to distill the LLMs through\nprompting. Despite its simplicity, empirical evalua-\ntions demonstrate consistent improvements across\nvarious retrieval models and datasets.\n9418\nLimitations\nLLM call Index search\nBM25 - 16ms\n+ query2doc >2000ms 177ms\nTable 6: Latency analysis for retrieval systems with our\nproposed query2doc. We retrieve the top 100 results for\nMS-MARCO dev queries with a single thread and then\naverage over all the queries. The latency for LLM API\ncalls depends on server load and is difﬁcult to precisely\nmeasure.\nAn apparent limitation is the efﬁciency of re-\ntrieval. Our method requires running inference with\nLLMs which can be considerably slower due to the\ntoken-by-token autoregressive decoding. Moreover,\nwith query2doc, searching the inverted index also\nbecomes slower as the number of query terms in-\ncreases after expansion. This is supported by the\nbenchmarking results in Table 6. Real-world de-\nployment of our method should take these factors\ninto consideration.\nReferences\nAlexander Bondarenko, Maik Fröbe, Johannes Kiesel,\nShahbaz Syed, Timon Gurcke, Meriem Beloucif,\nAlexander Panchenko, Chris Biemann, Benno Stein,\nHenning Wachsmuth, et al. 2022. Overview of\ntouché 2022: argument retrieval. In Interna-\ntional Conference of the Cross-Language Evalua-\ntion Forum for European Languages , pages 311–\n336. Springer.\nVera Boteva, Demian Gholipour, Artem Sokolov, and\nStefan Riezler. 2016. A full-text learning to rank\ndataset for medical information retrieval. In Euro-\npean Conference on Information Retrieval , pages\n716–722. Springer.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\nXia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\nmarco: A human generated machine reading com-\nprehension dataset. ArXiv preprint, abs/1611.09268.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben-\nton C. Hutchinson, Reiner Pope, James Bradbury, Ja-\ncob Austin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcía, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Díaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathleen S. Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nArXiv preprint, abs/2204.02311.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M V oorhees. 2020a. Overview\nof the trec 2019 deep learning track. ArXiv preprint,\nabs/2003.07820.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. V oorhees.\n2020b. Overview of the trec 2020 deep learning\ntrack. ArXiv preprint, abs/2003.07820.\nThibault Formal, Benjamin Piwowarski, and Stéphane\nClinchant. 2021. Splade: Sparse lexical and expan-\nsion model for ﬁrst stage ranking. Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing , pages 981–993,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie\nCallan. 2022. Precise zero-shot dense re-\ntrieval without relevance labels. ArXiv preprint ,\nabs/2212.10496.\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong,\nKrisztian Balog, Svein Erik Bratsberg, Alexander\nKotov, and Jamie Callan. 2017. Dbpedia-entity v2:\nA test collection for entity search. In Proceedings\nof the 40th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nShinjuku, Tokyo, Japan, August 7-11, 2017 , pages\n1265–1268. ACM.\n9419\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nVictor Lavrenko and W. Bruce Croft. 2001. Relevance-\nbased language models. ACM SIGIR Forum, 51:260\n– 267.\nJimmy J. Lin and Xueguang Ma. 2021. A few brief\nnotes on deepimpact, coil, and a conceptual frame-\nwork for information retrieval techniques. ArXiv\npreprint, abs/2106.14807.\nJimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, Rodrigo Nogueira, and\nDavid R. Cheriton. 2021. Pyserini: A python toolkit\nfor reproducible information retrieval research with\nsparse and dense representations. Proceedings of the\n44th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval.\nLinqing Liu, Minghan Li, Jimmy Lin, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Query expansion us-\ning contextual clue sampling with language models.\nArXiv preprint, abs/2210.07093.\nYuanhua Lv and ChengXiang Zhai. 2009. A compara-\ntive study of methods for estimating query language\nmodels with pseudo feedback. Proceedings of the\n18th ACM conference on Information and knowl-\nedge management.\nGeorge A. Miller. 1992. WordNet: A lexical database\nfor English. In Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New\nYork, February 23-26, 1992.\nRodrigo Nogueira and Jimmy Lin. From doc2query to\ndoctttttquery.\nRodrigo Nogueira, Wei Yang, Jimmy J. Lin, and\nKyunghyun Cho. 2019. Document expansion by\nquery prediction. ArXiv preprint, abs/1904.08375.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nOﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5835–5847, Online. Association for Computational\nLinguistics.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A joint training method\nfor dense passage retrieval and passage re-ranking.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2825–2835, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJ. J. Rocchio. 1971. Relevance feedback in information\nretrieval.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2022. Recitation-augmented language\nmodels. ArXiv preprint, abs/2210.01296.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogeneous benchmark for zero-shot evalua-\ntion of information retrieval models. In Thirty-ﬁfth\nConference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track (Round 2).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aur’elien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023.\nLlama: Open and efﬁcient foundation language mod-\nels. ArXiv preprint, abs/2302.13971.\nEllen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTrec-covid: constructing a pandemic information re-\ntrieval test collection. In ACM SIGIR Forum, vol-\nume 54, pages 1–12. ACM New York, NY , USA.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verify-\ning scientiﬁc claims. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7534–7550, On-\nline. Association for Computational Linguistics.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. ArXiv preprint,\nabs/2212.03533.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,\nLinjun Yang, Daxin Jiang, Rangan Majumder, and\nFuru Wei. 2023. SimLM: Pre-training with repre-\nsentation bottleneck for dense passage retrieval. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2244–2258, Toronto, Canada.\nAssociation for Computational Linguistics.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\n9420\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nW. Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingx-\nuan Ju, Soumya Sanyal, Chenguang Zhu, Michael\nZeng, and Meng Jiang. 2022. Generate rather than\nretrieve: Large language models are strong context\ngenerators. ArXiv preprint, abs/2209.10063.\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng\nLv, Nan Duan, and Weizhu Chen. 2022. Adversar-\nial retriever-ranker for dense text retrieval. In The\nTenth International Conference on Learning Repre-\nsentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nA Implementation Details\nDPR w/ distillation\nlearning rate 2 ×10−5 3 ×10−5\nPLM BERT base SimLM / E5base-unsup\n# of GPUs 4 4\nwarmup steps 1000 1000\nbatch size 64 64\nepoch 3 6\nα n.a. 0.2\nnegatives depth 1000 200\nquery length 144 144\npassage length 144 144\n# of negatives 15 23\nTable 7: Hyper-parameters for training dense retrievers\non MS-MARCO passage ranking dataset.\nFor dense retrieval experiments in Table\n1, we list the hyperparameters in Table 7.\nWhen training dense retrievers with distillation\nfrom cross-encoder, we use the same teacher\nscore released by Wang et al.. The SimLM\nand E5 checkpoints for initialization are pub-\nlicly available at https://huggingface.\nco/intfloat/simlm-base-msmarco and\nhttps://huggingface.co/intfloat/\ne5-base-unsupervised. To compute the\ntext embeddings, we utilize the [CLS] vector for\nSimLM and mean pooling for E5. This makes sure\nthat the pooling mechanisms remain consistent\nbetween intermediate pre-training and ﬁne-tuning.\nThe training and evaluation of a dense retriever\ntake less than 10 hours to ﬁnish.\nWhen prompting LLMs, we include 4 in-context\nexamples from the MS-MARCO training set. To\nincrease prompt diversity, we randomly select 4\nexamples for each API call. A complete prompt\nis shown in Table 11. On the budget side, we\nmake about 550k API calls to OpenAI’s service,\nwhich costs nearly 5k dollars. Most API calls are\nused to generate pseudo-documents for the training\nqueries.\nFor GPT-4 prompting, we ﬁnd that it has a ten-\ndency to ask for clariﬁcation instead of directly\ngenerating the pseudo-documents. To mitigate this\nissue, we set the system message to “You are asked\nto write a passage that answers the given query.\nDo not ask the user for further clariﬁcation.”.\nRegarding out-of-domain evaluations on DBpe-\ndia (Hasibi et al., 2017), NFCorpus (Boteva et al.,\n2016), Scifact (Wadden et al., 2020), Trec-Covid\n(V oorhees et al., 2021), and Touche2020 (Bon-\ndarenko et al., 2022), SimLM’s results are based\non the released checkpoint by Wang et al..\nFor ablation experiments in Figure 2, we ﬁne-\ntune for 40 epochs or 18k steps, whichever is\nreached ﬁrst.\nB Exploring Other Prompting Strategies\nDL 2019 DL 2020\nQuery2doc 69.2 64.5\n+ iterative prompt 68.6 64.8\nTable 8: GPT-4 with iterative prompting to self-verify\nthe generated pseudo-documents.\nInstead of generating pseudo-documents in one\nround, recent work (Press et al., 2022) proposes\nto iteratively prompt the LLM to improve the gen-\neration quality. We explore this intuition by ask-\ning GPT-4 to rewrite its own generated pseudo-\ndocuments with the following prompt template:\nYou are asked to rewrite the passage that poten-\ntially answers the given query. You should only\ncorrect the factual errors in the passage, do not\nask for clariﬁcation or make unnecessary changes.\nQuery: {{query}}\n# Begin of passage\n{{passage}}\n# End of passage\nEmpirically, we ﬁnd that GPT-4 makes very few\nchanges to the generated pseudo-documents, which\nsuggests that the pseudo-documents are already of\nhigh quality or GPT-4 is not capable of correcting\nits own errors. The results are shown in Table 8.\n9421\nquery who sings hey good looking\nLLM generationHey Good Lookin’ was originallywritten and recordedby the country music singerHank\nWilliamsin 1951. The song has sincebeen coveredby several otherartistsand bands,\nincluding Jimmy Buffett, Conway Twitty, and Willie Nelson.\nGroundtruth Hey, Good Lookin’. (1951) Hey, Good Lookin’ is a1951songwritten and recordedbyHank\nWilliams, and his version was inducted into the Grammy Hall of Fame in 2001. Since its\noriginal 1951 recording it hasbeen coveredby a variety ofartists.\nquery trumbull marriott fax number\nLLM generationThe fax number for the Trumbull Marriott Shelton is203-378-4444.\nGroundtruth Business name: Trumbull Marriott Merritt Parkway; Address: 180 Hawley Lane Trumbull,\nConnecticut 06611; Phone number: 203-378-4958; Fax number:203-378-1400; Business\nhours: 24; Credit cards accepted: Yes; Number of employees: 10-19; Map:\nquery what is hra and hsa\nLLM generationHRA stands for Health Reimbursement Arrangement. It is a type of beneﬁt plan where an\nemployer reimbursesemployees for qualifyingmedical expenses. It also allows employers\nto set aside money tax-free to reimburseemployeesfor medical expenses that are not covered\nunder the employer’shealth insuranceplan. HSA stands for Health Savings Account. It is\na savings account that is used in conjunction with a qualiﬁedHigh Deductible Health Plan\n(HDHP). An HSA enables an individual to pay for currenthealthexpenses and save for future\nhealth expenses on atax-free basis. Contributions to the account are tax\nGroundtruth The key difference between HRA and HSA is that HRA is anemployerfunded health beneﬁt\nplan thatreimbursesformedical expensesincluding personalhealth insurancepolicy pre-\nmiums ofemployeeswhereas HSA is also atax-advantagedhealthbeneﬁt plan exclusively\navailable to taxpayers in the United States who are enrolled in aHigh-Deductible Health Plan\n(HDHP).\nTable 9: More examples of LLM generations. The format is the same as in Table 5.\nDL 2019 DL 2020\nAverage 64.8 60.9\nStd dev. ±1.14 ±1.63\nTable 10: Sparse retrieval results of query2doc across\n3 random runs. The randomness comes from the se-\nlection of few-shot examples and the auto-regressive\nsampling of LLMs.\nC Results Across Multiple Runs\nIn our method, there are two sources of random-\nness: the selection of few-shot examples and the\nauto-regressive top-p sampling of LLMs. To quan-\ntify the variance of our method, we report the av-\nerage and standard deviation of sparse retrieval\nresults across 3 random runs in Table 10. One pos-\nsible improvement is to select few-shot examples\nbased on semantic similarity to the query. We leave\nthis for future work.\n9422\nprompts\nWrite a passage that answers the given query:\nQuery:what state is this zip code 85282\nPassage:Welcome to TEMPE, AZ 85282. 85282 is a rural zip code in Tempe, Arizona. The population\nis primarily white, and mostly single. At $200,200 the average home value here is a bit higher than\naverage for the Phoenix-Mesa-Scottsdale metro area, so this probably isn’t the place to look for housing\nbargains.5282 Zip code is located in the Mountain time zone at 33 degrees latitude (Fun Fact: this is the\nsame latitude as Damascus, Syria!) and -112 degrees longitude.\nQuery:why is gibbs model of reﬂection good\nPassage:In this reﬂection, I am going to use Gibbs (1988) Reﬂective Cycle. This model is a recognised\nframework for my reﬂection. Gibbs (1988) consists of six stages to complete one cycle which is able\nto improve my nursing practice continuously and learning from the experience for better practice in the\nfuture.n conclusion of my reﬂective assignment, I mention the model that I chose, Gibbs (1988) Reﬂective\nCycle as my framework of my reﬂective. I state the reasons why I am choosing the model as well as some\ndiscussion on the important of doing reﬂection in nursing practice.\nQuery:what does a thousand pardons means\nPassage:Oh, that’s all right, that’s all right, give us a rest; never mind about the direction, hang the\ndirection - I beg pardon, I beg a thousand pardons, I am not well to-day; pay no attention when I soliloquize,\nit is an old habit, an old, bad habit, and hard to get rid of when one’s digestion is all disordered with eating\nfood that was raised forever and ever before he was born; good land! a man can’t keep his functions\nregular on spring chickens thirteen hundred years old.\nQuery:what is a macro warning\nPassage:Macro virus warning appears when no macros exist in the ﬁle in Word. When you open\na Microsoft Word 2002 document or template, you may receive the following macro virus warning,\neven though the document or template does not contain macros: C:\\<path>\\<ﬁle name>contains macros.\nMacros may contain viruses.\nQuery:when was pokemon green released\nPassage:\noutput\nPokemon Green was released in Japan on February 27th, 1996. It was the ﬁrst in the Pokemon series of\ngames and served as the basis for Pokemon Red and Blue, which were released in the US in 1998. The\noriginal Pokemon Green remains a beloved classic among fans of the series.\nTable 11: The full prompt used for the example in Figure 1.\n9423",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8359291553497314
    },
    {
      "name": "Query expansion",
      "score": 0.7479391098022461
    },
    {
      "name": "Information retrieval",
      "score": 0.621378481388092
    },
    {
      "name": "Query language",
      "score": 0.591528058052063
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5884337425231934
    },
    {
      "name": "Memorization",
      "score": 0.5481950044631958
    },
    {
      "name": "Language model",
      "score": 0.5032030940055847
    },
    {
      "name": "Natural language processing",
      "score": 0.47483164072036743
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4250551164150238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4205949306488037
    },
    {
      "name": "Web search query",
      "score": 0.4121500253677368
    },
    {
      "name": "Search engine",
      "score": 0.17301607131958008
    },
    {
      "name": "Linguistics",
      "score": 0.0939035415649414
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}