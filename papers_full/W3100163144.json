{
  "title": "Syntax-driven Iterative Expansion Language Models for Controllable Text Generation",
  "url": "https://openalex.org/W3100163144",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2621828843",
      "name": "Noe Casas",
      "affiliations": [
        "Universitat Politècnica de Catalunya",
        "United Language Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2343218668",
      "name": "José A. R. Fonollosa",
      "affiliations": [
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A4227926241",
      "name": "Marta R. Costa‐jussà",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2016589492",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2804228899",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2962935015",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2912937082",
    "https://openalex.org/W4300925921",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2996068536",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2985694911",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2970101155",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2166905217",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3106104873",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W2953345635",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2948197522",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2970530566",
    "https://openalex.org/W2949644922",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2987253016",
    "https://openalex.org/W1607229519",
    "https://openalex.org/W2971167892"
  ],
  "abstract": "The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradigm for introducing a syntactic inductive bias into neural text generation, where the dependency parse tree is used to drive the Transformer model to generate sentences iteratively. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations.",
  "full_text": "Proceedings of 4th Workshop on Structured Prediction for NLP, pages 1–10\nNovember 20, 2020.c⃝2020 Association for Computational Linguistics\n1\nSyntax-driven Iterative Expansion Language Models\nfor Controllable Text Generation\nNoe Casas†∗, Jose A. R. Fonollosa ∗, Marta R. Costa-jussà ∗\n†Lucy Software, United Language Group\n∗TALP Research Center, Universitat Politècnica de Catalunya\n{noe.casas,jose.fonollosa,marta.ruiz}@upc.edu\nAbstract\nThe dominant language modeling paradigm\nhandles text as a sequence of discrete to-\nkens. While that approach can capture the la-\ntent structure of the text, it is inherently con-\nstrained to sequential dynamics for text gener-\nation. We propose a new paradigm for intro-\nducing a syntactic inductive bias into neural\ntext generation, where the dependency parse\ntree is used to drive the Transformer model to\ngenerate sentences iteratively.\nOur experiments show that this paradigm is\neffective at text generation, with quality be-\ntween LSTMs and Transformers, and compa-\nrable diversity, requiring less than half their de-\ncoding steps, and its generation process allows\ndirect control over the syntactic constructions\nof the generated text, enabling the induction of\nstylistic variations.\n1 Introduction\nThe currently dominant text generation paradigm is\nbased on generating a sequence of discrete tokens\nin a left-to-right autoregressive way. Most neural\nlanguage models (LMs) fall into this autoregressive\ngeneration category. Some neural architectures are\nsequential in nature, such as those based on recur-\nrent neural networks (RNNs), lending themselves\nnaturally to the autoregressive approach when used\ntogether with teacher forcing (Williams and Zipser,\n1989). Other architectures, such as Transformer\n(Vaswani et al., 2017), while not intrinsically se-\nquential, have also been targeted for sequential\ngeneration. On the other hand, some recent lines of\nresearch have focused on nonsequential generation.\nIn this work, we propose a new paradigm for text\ngeneration and language modeling called Iterative\nExpansion Language Model, which generates the\nﬁnal sequence following a token ordering deﬁned\nby the sentence dependency parse by iteratively\nexpanding each level of the tree.\n2 Related Work\nIn this section, we provide an overview of\nworks related to ours, including dependency tree-\ndriven LMs (§2.1), syntax-driven generation (§2.2),\ninsertion-based approaches (§2.3) and iterative re-\nﬁnement approaches (§2.4).\n2.1 Dependency LMs\nThe use of dependency parse trees to drive a lan-\nguage model was ﬁrst proposed by Chelba et al.\n(1997), with a similar structure to an n-gram LM,\nbut where the context of a word is its preceding\nbigram plus a list of preceding words whose parent\ndoes not precede it. Shen et al. (2008) make use of\nthe dependency tree in a probabilistic LM, comput-\ning the probability of each word conditioned on its\nparent and the sibling words between both.\nMirowski and Vlachos (2015) propose a de-\npendency LM based on RNNs, where the depen-\ndency tree is decomposed into a collection of un-\nrolls, that is, paths from the root to one of the\nleaves, and where the probability of a word can\nbe predicted from these unrolls. Buys and Blun-\nsom (2018) propose a shift-reduce transition-based\nLSTM (Hochreiter and Schmidhuber, 1997) depen-\ndency LM that can be used for language modeling\nand generation by means of dynamic programming.\n2.2 Syntax-driven Generation\nRecurrent neural network grammars (Dyer et al.,\n2016) are recursive models that operate with a stack\nof symbols that can be populated with terminals or\nnonterminals, or “reduced” to generate a syntactic\nconstituent, obtaining as a result a sentence and its\nassociated constituency parse tree.\nShen et al. (2018) use skip-connections to in-\ntegrate constituent relations with RNNs, learning\nthe underlying dependency structures by leverag-\ning a syntactic distance together with structured\n2\nattention.\nAkoury et al. (2019) use a simpliﬁed con-\nstituency tree as latent variables, modeling it au-\ntoregressively to later use it as input for a non-\nautoregressive transformer that generates the out-\nput sentence.\nOrdered neurons (Shen et al., 2019) are modiﬁed\nLSTMs where the latent sentence tree structure is\nused to control the dependencies between recurrent\nunits with a special “master” input and forget gates.\n2.3 Insertion-based Generation\nStern et al. (2019) propose a conditional generative\nmodel that iteratively generates tokens plus the po-\nsition at which they should be inserted within the\nsequence. Emelianenko et al. (2019) further pro-\npose to optimize the generation order by sampling\nfrom the ordering permutations. Instead, Chan et al.\n(2019) optimize a lower bound of the marginalized\nprobability over every possible ordering.\nGu et al. (2019a) handle the generation order as\na latent variable that is captured as the relative po-\nsition through self-attention, optimizing the ELBO\nto train the model.\nLevenshtein Transformer (Gu et al., 2019b) is a\nnon-autoregressive approach trained with reinforce-\nment learning (RL) to generate token insertion and\ndeletion actions. While it beneﬁts from the same\ngeneration speed-ups over autoregressive models as\nour model, it has the added difﬁculty of learning an\ninsertion/deletion policy using RL without any lin-\nguistically or empirically motivated priors, which\ncan be slow or difﬁcult to obtain convergence in\npractice. By comparison, our approachmakes uses\na linguistically motivated prior for word insertion\nin a fully supervised way, avoiding the optimization\ndifﬁculties of RL.\nWelleck et al. (2019) use cost minimization imi-\ntation learning to learn a policy to generate a binary\ntree that is used to drive the token generation.\n2.4 Iterative Reﬁnement\nLee et al. (2018) propose a latent variable non-\nautoregressive machine translation model where\nﬁrst the target length is predicted by the model, and\nthen, the decoder is iteratively applied to its own\noutput to reﬁne it.\nMask-predict (Ghazvininejad et al., 2019) also\npredicts the target sentence length and then non-\nautoregressively predicts the sentence itself, itera-\ntively reﬁning it a ﬁxed number of times, masking\nout and regenerating the tokens it is least conﬁdent\nabout. Lawrence et al. (2019) follow a similar ap-\nproach and start with a sequence of placeholder\ntokens (all the same) of a speciﬁed length, and\nthey iteratively replace them with normal tokens\nvia masked LM-style inference. As the masking\nstrategy for the training data, the authors propose\ndifferent stochastic processes to randomly select\nwhich placeholders are to be uncovered.\n3 Iterative Expansion LMs\nOur proposal is to train a new kind of language\nmodel where the token generation order is driven\nby the dependency parse tree of the sentence and\nwhere the generation process is iterative.\nMy dog also likes eating sausage\nposs\nnsubj\nadvmod xcomp dobj\nROOT\nFigure 1: Example of dependency parse tree.\nThe input vocabulary contains terminal tokens\nas well as non-terminal special tokens called de-\npendency placeholders, each of which is associated\nwith one of the possible dependency relations to\nthe heads. For the dependency tree in Figure 1, the\ndependency placeholders are [poss], [nsubj],\n[advmod], [xcomp], [dobj] and [ROOT].\nThe input of the ﬁrst iteration is the sequence\nwith the [ROOT] element. At each iteration, the\nmodel receives as input a sequenceItok with tokens\nfrom the input vocabulary and non-autoregressively\ngenerates two new sequences, each with the same\nlength as the input.\nThe ﬁrst output sequence, Otok , contains tokens\nfrom a vocabulary with all possible textual tokens\n(terminal tokens). The second output, Oexp , is\na sequence of tokens called expansion placehold-\ners, which are taken from a separate vocabulary.\nEach expansion placeholder is associated with a\npattern describing the left and right dependencies\nof the token at that position in the Otok sequence.\nAn example of dependency expansion could be\n[nsubj-advmod-HEAD-xcomp] for the word\n“likes” in the dependency parse tree from Figure 1.\nAfter each iteration, the output of the model is ex-\npanded.1 This consists of creating a new sequence\n1The expansion of the output to be fed as input in the next\niteration occurs in the CPU outside of the neural model itself.\n3\nby combining the tokens fromItok , Otok and Oexp .\nThis process is illustrated in Figure 2, making use\nof the dependency tree from Figure 1.\nWhen there is a padding token [pad] in the\noutput (either Otok or Oexp ), this means that the\noutput at that position is ignored when computing\nthe loss function. This occurs when the terminal\ntoken has already been computed in previous iter-\nations and has therefore been received as part of\nItok , and the model does not need to compute it\nagain.\nNote also that an empty dependencies token\n[HEAD] marks the end of a branch and that there\nis no need for an end of sequence token<eos>. As\nshown in the example from Figure 1, the generation\nof different branches occurs in parallel, needing\nonly 3 iterations to generate a 6-token sentence.\nIteration 1\nItok : [ROOT]\nOtok : likes\nOexp : [nsubj-advmod-HEAD-xcomp]\nIteration 2\nItok : [nsubj] [advmod] likes [xcomp]\nOtok : dog also [pad] eating\nOexp : [poss-HEAD] [HEAD] [pad] [HEAD-dobj]\nIteration 3\nItok : [poss] dog also likes eating [dobj]\nOtok : my [pad] [pad] [pad] [pad] sausage\nOexp : [HEAD] [pad] [pad] [pad] [pad] [HEAD]\nFigure 2: Example of iterative text generation.\nThe strategy for composing tree expansion to-\nkens (e.g., [nsubj-advmod-HEAD-xcomp])\nmay not scale well when single words have many\ndirect dependencies. To alleviate this, we introduce\na preprocessing step to modify the dependency tree\nso that every word has at most one dependency to\nthe left and one to the right. For each word with\nmore than one dependency on any of its sides, we\nrearrange the tree to force left-to-right dependen-\ncies. Although this tree binarization reduces the\ndegree of parallelism, it reduces data sparsity and\nallows handling constructions with a number of\ndependencies may otherwise be too large for the\nmodel to properly capture, such as enumerations\n(e.g., “I bought a pair of shoes, an umbrella, a beau-\ntiful jacket and a bracelet”).\nIterative expansion LMs can be naturally ex-\ntended to subword vocabularies, like byte-pair en-\ncoding (BPE; Sennrich et al., 2016): for each word,\nwe decompose its node in the tree into as many\nnodes as subwords in the word, rearranging the tree\nso that the head of the old word is now the head\nof the ﬁrst subword, and each subsequent subword\ndepends on the previous one, while every depen-\ndency of the old word node now depends on the\nlast subword.\n3.1 Neural Architecture\nThe neural architecture proposed is based on a\nTransformer decoder (Vaswani et al., 2017). To\ngenerate the dual output (terminal tokens and ex-\npansion placeholders) we condition the generation\nof terminals on the expansions: the probability dis-\ntribution over the expansion token space is gener-\nated ﬁrst by projecting from one of the intermediate\nlayers’ hidden states. We sample from it and use the\nresulting expansion IDs as an index to a trainable\nexpansion embedding layer; the embedded vectors\nare added to the hidden state used to generate them\nfor use as input to subsequent layers.\nAs described in Section 3, the input and output\ntoken vocabularies are different: the latter only\ncontains terminal tokens (plus some special tokens\nsuch as [PAD]); the former also contains depen-\ndency placeholders. However, for practical pur-\nposes, at the model level, we deﬁne both vocabular-\nies to be the same, both with terminal tokens and\ndependency placeholders, and we mask the entries\nof dependency placeholders in the ﬁnal softmax.\nTo inject the syntactic dependency information\nas input into the model, we add a layer of learned\npositional embeddings containing the position of\nthe head of each token, and we refer to this embed-\nding layer as head position embedding.\nThe self-attention mask used in Transformer to\nforce causality is not used in our proposal. The\ninput is therefore not masked at all, and the token\npredictions have access to the full input sequence.\n3.2 Training\nFor training iterative expansion LMs, the main in-\nput of the model is the tokens at one of the levels of\nthe dependency parse tree (Itok ), while the output\nis the following level tokens (Otok ) and expansion\nplaceholders (Oexp ). A secondary input to the\nmodel are the dependency indexes, which are used\nin the head position embedding.\nThe model is trained with the categorical cross-\nentropy for both tokens and expansion placehold-\ners, then adding both sublosses into the ﬁnal loss\n(with equal weights). Tokens generated in previous\n4\niterations appear as [PAD] tokens in the expected\noutput and are ignored when computing the loss.\nTraining takes place in batches; as the trainable\nunit is a level transition, a training batch is com-\nposed of level transitions from different sentences.\n3.3 Inference and Text Generation\nIn iterative expansion LMs, inference takes place\niteratively. The initial state is a batch of [ROOT]\ntokens, together with the head positions initialized\nto the special value representing the root node and,\nin constrained attention variants, a mask with the\nself-dependency of the single node in each sentence\nin the batch. At each iteration, the model generates\nthe probability distributions for terminal tokens\nand expansion tokens. We use nucleus sampling\n(Holtzman et al., 2020) to sample from them. The\nterminal token sequences are expanded according\nto the expansion tokens (see §3), and these are\nthe inputs for the following iteration if there are\nstill unﬁnished branches. Before sampling from\nthe token and expansion probability distributions,\nwe mask the <unk> token and the dependency\nplaceholders to avoid generating them.\nAlthough iterative expansion LMs could be sub-\nject to beam search across iterations, we have not\ncovered such a possibility as part of this work.\n4 Experimental Setup\n4.1 Unconditional Text Generation\nWe conducted experiments on unconditional text\ngeneration following the methodology used by Cac-\ncia et al. (2020). The goal is to assess both the\nquality and diversity of the text generated by the\nmodel and the baselines. For the quality evalua-\ntion, we use the BLEU score (Papineni et al., 2002)\nover the test set, where each generated sentence is\nevaluated against the whole test set as a reference.\nFor diversity, we used the self-BLEU score (Zhu\net al., 2018), computed using as references the rest\nof the generated sentences. For each model, the\ntemperature of the ﬁnal softmax τ is tuned to gen-\nerate text in the closest quality/diversity regime to\nthe training data.\nIterative expansion LMs are compared against\na standard LM baselines, namely, AWD-LSTM 2\n(Merity et al., 2018) and a Transformer LM\n(Vaswani et al., 2017), both with word (w) and\nBPE subword (sw) vocabularies. The models\n2Abbreviation of ASGD weight-dropped LSTM, where\nASGD stands for averaged stochastic gradient descent.\nwere trained on the EMNLP2017 News dataset,\nwhich contains news in English, enriched with de-\npendency annotations by corenlp, an automatic\nannotation tool that provides pre-trained models.\nSyntax-driven generation baseline models were not\nincluded because the only model with an available\nimplementation that is able to do unsupervised text\ngeneration are RNNGs, but they proved not to scale\neven to medium-sized datasets like EMNLP2017\nNews. When sampling from models, we use nu-\ncleus sampling (Holtzman et al., 2020), a form of\nancestral sampling that constrains the candidate\npool by discarding the distribution tail. Samples\nfrom the training and validation data are included\nfor reference. Full hyperparameters and data pro-\ncessing details are described in Appendices D and\nB.\n4.2 Style Variation\nIterative expansion LMs drive the generation of text\nwith the dependency parse tree. It is possible to\ninﬂuence the generated trees by altering artiﬁcially\nthe probability of the different expansion tokens.\nTo demonstrate this, we modiﬁed the decoding pro-\ncess of iterative expansion LMs to force the proba-\nbility of generating adjectival constructions to be\nhigher than normal, aiming at generating a more\ndescriptive style: during decoding, we multiply the\nprobabilities of the expansion placeholders that ex-\npress adjectival dependencies (i.e. those containing\nadjectival modiﬁer “amod” relations), and renor-\nmalize the probabilities by dividing by the sum.\nWe conducted this experiment with the word-\nlevel models trained on EMNLP2017 News data.\nWe compute the ratio of adjectives per sentence to\nverify the increased presence of adjectives, while\ncontrolling quality and diversity measures over the\ngenerated text for potential degradation.\n5 Results and Analysis\nWe assess the ability of iterative expansion LMs\nto unconditionally generate text in terms quality\n(BLEU-5) vs. diversity (self BLEU-5), comparing\nagainst sequential baselines, each with a softmax\ntemperature τ tuned separately.\nIn order to tune the output softmax termperature\nτ, we generated text with each model at different\ntemperatures and chose the value of τ that was the\nmost similar to a sample from the training data in\nterms of BLEU-5 against a sample from the val-\nidation set (proxy for quality) and self BLEU-5\n5\n-30.0 -27.5 -25.0 -22.5 -20.0 -17.5 -15.0 -12.5 -10.0\nNegative validation BLEU-5\n4\n6\n8\n10\n12\n14\n16\n18\n20self BLEU-5\ntrain sample\nIt. Expansion LM (w)\nAWD-LSTM (w)\nTransformer (w)\nIt. Expansion LM (w)\nAWD-LSTM (w)\nTransformer (w)\n-30.0 -27.5 -25.0 -22.5 -20.0 -17.5 -15.0 -12.5 -10.0\nNegative validation BLEU-5\n4\n6\n8\n10\n12\n14\n16\n18\n20self BLEU-5\ntrain sample\nIt. Expansion LM (sw)\nAWD-LSTM (sw)\nTransformer (sw)\n= 0.7\n= 0.8\n= 0.9\n= 1.0\n= 1.1\n= 1.2\n= 0.7\n= 0.8\n= 0.9\n= 1.0\n= 1.1\n= 1.2\nFigure 3: Quality vs. diversity on EMNLP2017 News (BLEU-5). Models with word-level vocabulary on the left\nand subword-level on the right. The point marker is color-ﬁlled for the chosen value of τ. Each point represents\nthe average over 20 generated text samples, and is surrounded by a small colored ellipse representing the standard\ndeviation.\nτ ITEXP (w) A WD-LSTM (w) Transformer (w)\nvalid ↑ self ↓ valid ↑ self ↓ valid ↑ self ↓\n0.70 30.1 ±0.8 22 .3 ±1.0 39.2 ±0.9 33 .4 ±1.1 40.5 ±0.6 35 .0 ±1.1\n0.80 26.8 ±0.8 16 .0 ±1.0 33.0 ±0.7 23 .2 ±1.0 35.8 ±0.7 26 .3 ±0.8\n0.90 23.5 ±0.7 12 .4 ±0.7 26.0 ±0.6 14 .7 ±0.8 30.4 ±0.7 19 .0 ±0.8\n1.00 20.0 ±0.6 9 .4 ±0.5 19.4 ±0.6 9 .0 ±0.6 25.2 ±0.5 13 .3 ±0.5\n1.10 16.4 ±0.5 6 .8 ±0.5 13.4 ±0.4 5 .0 ±0.4 19.9 ±0.6 9 .0 ±0.6\n1.20 13.4 ±0.6 5 .1 ±0.4 9.0 ±0.5 2 .9 ±0.3 15.8 ±0.5 6 .2 ±0.5\nτ ITEXP (sw) A WD-LSTM (sw) Transformer (sw)\nvalid ↑ self ↓ valid ↑ self ↓ valid ↑ self ↓\n0.70 28.6 ±0.9 20 .3 ±1.1 39.0 ±0.8 33 .5 ±1.1 36.9 ±0.7 30 .6 ±1.2\n0.80 25.5 ±0.5 15 .1 ±0.7 32.3 ±0.7 22 .4 ±0.7 32.5 ±0.7 22 .4 ±1.0\n0.90 22.7 ±0.6 11 .5 ±0.7 25.6 ±0.6 14 .3 ±0.6 27.8 ±0.7 16 .0 ±0.8\n1.00 19.9 ±0.6 9 .2 ±0.5 19.2 ±0.5 8 .9 ±0.5 22.9 ±0.8 11 .0 ±0.7\n1.10 16.9 ±0.8 7 .0 ±0.6 13.9 ±0.5 5 .5 ±0.4 18.4 ±0.7 7 .6 ±0.6\n1.20 14.1 ±0.6 5 .4 ±0.5 9.7 ±0.4 3 .3 ±0.3 14.5 ±0.5 5 .2 ±0.5\nTable 1: Validation and self BLEU-5 scores of the text generated by the word-level (top) and subword-level\n(bottom) models under study at different temperatures τ, showing the average and standard deviation over 20\ndifferent generated text samples. The selected generation regime is highlighted for each model, being the closest\nto the training sample, which has a validation BLEU-5 of 17.8 and a self BLEU-5 of 6.6.\n(proxy for diversity). Each model was used to gen-\nerate 20 samples of 400 sentences, and self-BLEU5\nand validation-BLEU5 were computed over each\nof them, taking the average and the standard devia-\ntion. Figure 3 and Table 1 show these BLEU values,\nhighlighting the chosen τ for each model. Given\nthe low values for the standard deviation, we de-\ncided not to include it in subsequent tables to avoid\nunnecessary clutter. Note that in all BLEU vs. self-\nBLEU ﬁgures, each model is shown as a different\nline (each with its own color and/or dashed pattern)\nand that the data points computed for each temper-\nature value are plotted with a speciﬁc marker shape\n(square, diamond, triangle, or ﬂipped triangle).\nApart from BLEU scores, we also include extra\nquality measures, namely the perplexity obtained\n6\nτ Test BLEU-5 Self BLEU-5 A WD-LSTM Transformer GPT-2\n(quality ↑) (diversity ↓) perplex. ↓ perplex. ↓ perplex. ↓\nAWD-LSTM (w) 1.0 22.9 8.9 37.0 47.9 99.5\nTransformer (w) 1.1 23.8 9.0 33.6 18.6 66.5\nITEXP (w) 1.0 23.7 9.4 40.8 40.7 85.2\nAWD-LSTM (sw) 1.0 22.7 8.9 43.5 56.9 113.5\nTransformer (sw) 1.1 22.1 7.6 37.5 31.6 77.1\nITEXP (sw) 1.0 23.6 9.2 45.2 49.2 97.1\nTrain sample - 21.5 6.6 49.5 29.1 37.7\nValid sample - 21.2 7.2 53.3 44.7 36.7\nTable 2: Quality and diversity on EMNLP2017, with τ generating the closest text to the validation data.\nby other language models: an AWD-LSTM word-\nlevel LM and a Transformer word-level LM, both\ntrained on EMNLP2017 News, plus OpenAI GPT-\n2 (1.5 B parameters) (Radford et al., 2019). The\nresults are shown in Table 2.\nThese results show how the generated text im-\nproves over AWD-LSTM in terms of quality by all\nmeasures, with a comparable level of diversity. In\ncomparison to the Transformer, while the quality\nmeasured with BLEU-5 is better for ITEXP, the\nrest of the quality measures indicate that the text\ngenerated by the Transformer is of better quality.\nAdjective Adjs. per Test Self\nprobability sentence BLEU-5 BLEU-5\n×1 1.2 23.7 9.4\n×10 3.4 21.3 8.4\n×20 4.2 20.6 8.8\n×50 5.2 19.8 8.9\nTable 3: I TEXP (w, τ = 1.0) with increased adjectives.\nThe results of the styled text generation experi-\nments, shown in Table 3, conﬁrm that the style of\nthe resulting text can be successfully modulated to\nthe desired degree and that the quality and diversity\nare only slightly degraded at moderate increases of\nthe probability of adjectival clause generation.\n5.1 Human Evaluation\nIn order to better assess the quality of the generated\ntext, we also include a human evaluation. For this,\nwe took a sample of 60 sentences of each model\nunder study, including also a sample of the same\nsize from the validation data, to serve as reference.\nThe sentences were evaluated by a pool of anno-\ntators, who were requested to rate the sentence in\nan integer scale from 1 to 5, taking into account its\nﬂuency and correctness.\nThe pack of sentences rated by each annotator\ncontained 10 sentences from each of the models un-\nder evaluation. Each sentence under evaluation was\npart of the packs of 3 evaluators; this redundancy\nwas used to measure the discrepancies in the rat-\ning of each sentence among annotators, which was\nquantiﬁed by means of the average per-sentence\nstandard deviation.\nModel Average Per sentence\nrating avg. stddev\nAWD-LSTM (w) 3.08 0.74\nTransformer (w) 3.43 0.78\nITEXP (w) 3.28 0.73\nAWD-LSTM (sw) 2.66 0.68\nTransformer (sw) 3.33 0.83\nITEXP (sw) 3.09 0.70\nValid sample 4.49 0.61\nTable 4: Human evaluation for the different models.\nTable 4 shows the statistics of the obtained rat-\nings, were we can see the average rating of the\nsentences generated by each model, together with\nthe average per-sentence standard deviation, to un-\nderstand how different the ratings for each sentence\nwere among the different evaluator ratings. We can\nsee that the highest human ratings were obtained\nby the Transformer, both with word and subword-\nlevel vocabularies, followed by ITEXP and then\nAWD-LSTM.\nTable 5 shows the human evaluation for the mod-\nels from the style variation experiments presented\nin Table 3. As we can see, there is a small degrada-\ntion in quality as we force high levels of adjectival\npresence.\n7\nAdjective Average Per sentence\nprobability rating avg. stddev\n×1 3.28 0.73\n×10 3.16 0.79\n×20 2.98 0.84\n×50 3.19 0.70\nTable 5: Human evaluation for ITEXP (w) models with\nincreased adjectival construction probability.\n6 Further Comparison with Real Text\nGiven that the generation process in iterative ex-\npansion LMs is not sequential, we studied the dis-\ntribution of the sentence lengths it generates. This\nis shown in Figure 4 for the text generated by\na word-level iterative expansion LM trained on\nEMNLP2017 News, along with the lengths of a\nsample from the training data.\n15 20 25 30 35 40 45\nSentence length in tokens\nItExp LM (w)\ntrain sample\nFigure 4: Distribution of generated text length.\nIterative expansion LMs generate the depen-\ndency parse tree as they generate text. We studied\nthe depths of the dependency trees of generated\ntext in relation to those parsed from the training\ndata, as shown in Figure 5.\n5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0\nDependency parse tree depth\nItExp LM (w)\ntrain sample\nFigure 5: Histogram of generated text tree depth.\nWe also measured the degree to which the gener-\nated trees adhere to the trees obtained by parsing\ntheir lexicalized representation. Speciﬁcally, we\ncomputed the labeled and unlabeled attachment\nscores between both for the text generated at dif-\nferent softmax temperatures τ. Attachment scores\nare the standard performance measure in depen-\ndency parsing and are computed as the percentage\nof words that have been assigned the same head as\nthe reference tree, over a test set. The attachment\nscore is \"labeled\" if the dependency label is taken\ninto account or \"unlabeled\" otherwise. As shown\nin Table 6, the obtained labeled attachment scores\n(LAS) and unlabeled attachment scores (UAS) are\nvery high across the different values of the genera-\ntion temperature τ.\nτ 0.7 0 .8 0 .9 1 .0 1 .2\nLAS 96.4 95.3 94.2 92.3 86.2\nUAS 98.0 97.3 96.5 95.2 90.7\nTable 6: Attachment scores of the generated trees.\n6.1 Quantiﬁcation of the Generation Speedup\nText generation with autoregressive models like\nLSTM or Transformer models offers a linear com-\nputational complexity with respect to the length\nof the generated sequence. In comparison, the de-\npendency tree-driven decoding used by iterative\nexpansion LMs generates text in parallel for each\nbranch in the tree. If the tree was a perfectly bal-\nanced binary tree, then the computational complex-\nity would be logarithmic. However, dependency\ntrees in general are not balanced and, given the tree\nbinarization postprocessing that we introduce, the\nparallelization is slightly reduced.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7\nRatio of tree-based decoding steps with respect to sequential decoding\nBinarized tree\nNon-binarized tree\nIdeal binary tree\nFigure 6: Histogram of the ratio of the decoding steps\nneeded to generate a sentence with tree-based decoding\nwith respect to sequential generation.\nFigure 6 shows the speedup of the needed de-\ncoding steps of tree-based decoding with respect\nof auto-regressive decoding, taking a sample of the\ntraining data and computing the needed steps to\ndecode them should the sentences have an ideal-\nized binary dependency parse tree, a normal parse\ntree, and a binarized parse tree. On average, the\nbinarized parse tree, which is the decoding used by\niterative expansion LMS, needs only 45% of the\ndecoding steps needed by autoregressive decoding.\n8\nAmerican students were 62 percent more likely to die in a heart attack during the ﬁrst week of 2004, according\nto the study.\nFor 150 days, Hillary Clinton will do more to improve access to affordable quality care, support and education\nfunding for millions of Americans, she says.\nFor those on this list, it’s likely that I would rather be able to train them up, she said.\nHe made it clear the SNP repeated on Friday as a response, saying they discussed a contract getting the extra\ncost here.\nHe’ll pay $25, 000 for rent and more buses and bring his collection to The Academy on Channel 31.\nSix years later, at least eight people died as a result of the shooting.\nThe health prime minister told CNN Thursday that he was willing to back up against the US and remove all\nof the relevant items at the end of the transition.\nThen, another man told police that was a friend’s friend, and as a child, he made the decision to call his\nmother.\nThey are 40 - 60 among the top 50, 000 women in the last year in that group since 2014 - 15.\nThey’ve worked hard on Twitter and they think they’ve tried to focus on our sport, she said.\nWe like to think that if you try to get this game done, we can get a lower success rate out of 15.\nTable 7: Samples of text generated by iterative expansion LMs with word vocabulary.\nI feel that they’re going to Syria because we had this explanation, that they have an indication of their advance.\nThe girl’s mother told the group of three she needed treatment and the family said her daughter would still\nbe alive with another child.\nBut she added: \"The data is important to the EU that the UK can attract more businesses.\nThough he also spoke to Mr Wilson on Saturday morning at the Netherlands Police trial, Johnson referred it\nto the No. 1 commission.\nIt’s a collective belief and it’s a statement to us, he said.\nIt’s just the ﬁrst thing we’re feeling now and I don’t like it.\nSo if you want to be sitting in a garden, you have to wait for something to make sure that this does not end.\nSo, for example, we need to argue about what the president did, but I’m just interested in having any talk.\nThe British defence ministry conﬁrmed action had been taken at the hospital but could not conﬁrm the details\nuntil now.\nWe’ll ask for a fair share of Russia to stop border security, particularly for people of color, he added.\nTable 8: Samples of text generated by iterative expansion LMs with subword vocabulary.\n6.2 Generation Examples\nTable 7 shows a selection of text samples gener-\nated by iterative expansion LMs with a word-level\nvocabulary, while Table 8 shows samples gener-\nated with a subword-level vocabulary. We can see\nthat, despite being generated non-sequentially and\neach branch of the dependency parse tree being\ngenerated in parallel, the resulting sentences main-\ntain coherence and syntactic agreement, conﬁrming\nthat conditioning on the token dependencies in the\nparse tree provides enough information to generate\nit while speeding up the decoding process.\n7 Conclusion\nIn this work, we presented iterative expansion LMs,\nwhich are iterative non-autoregressive text genera-\ntion models that rely on syntactic dependency trees\nto generate sentence tokens in parallel. As opposed\nto other syntax-driven generation mechanisms, the\ntraining of iterative expansion LMs can be natu-\nrally computed in batches and they are amenable\nto subword-level vocabularies.\nWe showed that our proposed method generates\ntext with quality between LSTMs and Transform-\ners, with comparable diversity, both regarding auto-\nmatic measurements and human judgement, while\ngenerating text in half of the decoding steps needed\nby sequential LMs, and also allowing direct control\nover the generation process at the syntactic level,\nenabling the induction of stylistic variations in the\ngenerated text.\nOur code is available as open source athttps://\ngithub.com/noe/iterative_expansion_lms .\n9\nAcknowledgments\nThis work is partially supported by Lucy Software\n/ United Language Group (ULG) and the Cata-\nlan Agency for Management of University and\nResearch Grants (AGAUR) through an Industrial\nPh.D. Grant. This work also is supported in part\nby the Spanish Ministerio de Economía y Competi-\ntividad, the European Regional Development Fund\nthrough the postdoctoral senior grant Ramón y Ca-\njal and by the Agencia Estatal de Investigación\nthrough the projects EUR2019-103819, PCIN-\n2017-079 and PID2019-107579RB-I00 / AEI /\n10.13039/501100011033\nReferences\nNader Akoury, Kalpesh Krishna, and Mohit Iyyer.\n2019. Syntactically supervised transformers for\nfaster neural machine translation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1269–1281, Florence,\nItaly. Association for Computational Linguistics.\nJan Buys and Phil Blunsom. 2018. Neural syntactic\ngenerative models with exact marginalization. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 942–952, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2020. Language gans falling short. In International\nConference on Learning Representations.\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604.\nCiprian Chelba, David Engle, Frederick Jelinek, Victor\nJimenez, Sanjeev Khudanpur, Lidia Mangu, Harry\nPrintz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-\ncke, and Dekai Wu. 1997. Structure and perfor-\nmance of a dependency language model. In In Pro-\nceedings of Eurospeech, pages 2775–2778.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209, San Diego, California.\nAssociation for Computational Linguistics.\nDmitrii Emelianenko, Elena V oita, and Pavel\nSerdyukov. 2019. Sequence modeling with un-\nconstrained generation order. In Advances in\nNeural Information Processing Systems 32, pages\n7698–7709. Curran Associates, Inc.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6114–\n6123, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019a.\nInsertion-based decoding with automatically in-\nferred generation order. Transactions of the Asso-\nciation for Computational Linguistics, 7:661–676.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019b.\nLevenshtein transformer. In Advances in Neural\nInformation Processing Systems 32, pages 11179–\n11189. Curran Associates, Inc.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nCarolin Lawrence, Bhushan Kotnis, and Mathias\nNiepert. 2019. Attending to future tokens for bidi-\nrectional sequence generation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1–10, Hong Kong, China.\nAssociation for Computational Linguistics.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1173–\n1182, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nEdward Loper and Steven Bird. 2002. Nltk: The natu-\nral language toolkit. In In Proceedings of the ACL\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Compu-\ntational Linguistics. Philadelphia: Association for\nComputational Linguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nPiotr Mirowski and Andreas Vlachos. 2015. Depen-\ndency recurrent neural language models for sentence\ncompletion. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\n10\non Natural Language Processing (Volume 2: Short\nPapers), pages 511–517, Beijing, China. Associa-\ntion for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nLibin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A\nnew string-to-dependency machine translation algo-\nrithm with a target dependency language model. In\nProceedings of ACL-08: HLT, pages 577–585.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. In Interna-\ntional Conference on Learning Representations.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2019. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\nInternational Conference on Learning Representa-\ntions.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, pages 5976–5985.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nSean Welleck, Kianté Brantley, Hal Daumé III, and\nKyunghyun Cho. 2019. Non-monotonic sequen-\ntial text generation. In Proceedings of the 36th In-\nternational Conference on Machine Learning, vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 6716–6726, Long Beach, California,\nUSA. PMLR.\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation, 1(2):270–\n280.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, pages 1097–1100. ACM.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8342291712760925
    },
    {
      "name": "Transformer",
      "score": 0.6895133256912231
    },
    {
      "name": "Parsing",
      "score": 0.6287734508514404
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6030121445655823
    },
    {
      "name": "Text generation",
      "score": 0.5827359557151794
    },
    {
      "name": "Decoding methods",
      "score": 0.5071139335632324
    },
    {
      "name": "Natural language processing",
      "score": 0.5068503022193909
    },
    {
      "name": "Treebank",
      "score": 0.49819183349609375
    },
    {
      "name": "Language model",
      "score": 0.4930669963359833
    },
    {
      "name": "Syntax",
      "score": 0.46698233485221863
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4124011993408203
    },
    {
      "name": "Algorithm",
      "score": 0.19234031438827515
    },
    {
      "name": "Voltage",
      "score": 0.1647162139415741
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9617848",
      "name": "Universitat Politècnica de Catalunya",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210131992",
      "name": "United Language Group (United States)",
      "country": "US"
    }
  ]
}