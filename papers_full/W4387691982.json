{
  "title": "Identifying Rare Circumstances Preceding Female Firearm Suicides: Validating A Large Language Model Approach",
  "url": "https://openalex.org/W4387691982",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2472033190",
      "name": "Weipeng Zhou",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A3092976990",
      "name": "Laura C Prater",
      "affiliations": [
        "University of Washington",
        "Harborview Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2883577261",
      "name": "Evan V. Goldstein",
      "affiliations": [
        "University of Utah"
      ]
    },
    {
      "id": "https://openalex.org/A2352329143",
      "name": "Stephen J. Mooney",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4327606006",
    "https://openalex.org/W1923457383",
    "https://openalex.org/W4224987885",
    "https://openalex.org/W1977772270",
    "https://openalex.org/W4283751091",
    "https://openalex.org/W2301422081",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4254034712",
    "https://openalex.org/W4327946446"
  ],
  "abstract": "Background Firearm suicide has been more prevalent among males, but age-adjusted female firearm suicide rates increased by 20% from 2010 to 2020, outpacing the rate increase among males by about 8 percentage points, and female firearm suicide may have different contributing circumstances. In the United States, the National Violent Death Reporting System (NVDRS) is a comprehensive source of data on violent deaths and includes unstructured incident narrative reports from coroners or medical examiners and law enforcement. Conventional natural language processing approaches have been used to identify common circumstances preceding female firearm suicide deaths but failed to identify rarer circumstances due to insufficient training data. Objective This study aimed to leverage a large language model approach to identify infrequent circumstances preceding female firearm suicide in the unstructured coroners or medical examiners and law enforcement narrative reports available in the NVDRS. Methods We used the narrative reports of 1462 female firearm suicide decedents in the NVDRS from 2014 to 2018. The reports were written in English. We coded 9 infrequent circumstances preceding female firearm suicides. We experimented with predicting those circumstances by leveraging a large language model approach in a yes/no question-answer format. We measured the prediction accuracy with F1-score (ranging from 0 to 1). F1-score is the harmonic mean of precision (positive predictive value) and recall (true positive rate or sensitivity). Results Our large language model outperformed a conventional support vector machine–supervised machine learning approach by a wide margin. Compared to the support vector machine model, which had F1-scores less than 0.2 for most infrequent circumstances, our large language model approach achieved an F1-score of over 0.6 for 4 circumstances and 0.8 for 2 circumstances. Conclusions The use of a large language model approach shows promise. Researchers interested in using natural language processing to identify infrequent circumstances in narrative report data may benefit from large language models.",
  "full_text": "Short Paper\nIdentifying Rare Circumstances Preceding Female Firearm\nSuicides: Validating A Large Language Model Approach\nWeipeng Zhou1, BA; Laura C Prater2,3, MPH, PhD; Evan V Goldstein4, MPP, PhD; Stephen J Mooney5, MS, PhD\n1Department of Biomedical Informatics and Medical Education, School of Medicine, University of Washington, Seattle, WA, United States\n2Department of Psychiatry and Behavioral Health, University of Washington, Seattle, WA, United States\n3Harborview Medical Center, School of Medicine, University of Washington, Seattle, WA, United States\n4Department of Population Health Sciences, University of Utah, Salt Lake City, UT, United States\n5Department of Epidemiology, School of Public Health, University of Washington, Seattle, WA, United States\nCorresponding Author:\nStephen J Mooney, MS, PhD\nDepartment of Epidemiology\nSchool of Public Health\nUniversity of Washington\nHans Rosling Center for Population Health\n3980 15th Ave NE\nSeattle, WA, 98195\nUnited States\nPhone: 1 206 685 1643\nEmail: sjm2186@uw.edu\nAbstract\nBackground: Firearm suicide has been more prevalent among males, but age-adjusted female firearm suicide rates increased\nby 20% from 2010 to 2020, outpacing the rate increase among males by about 8 percentage points, and female firearm suicide\nmay have different contributing circumstances. In the United States, the National Violent Death Reporting System (NVDRS) is\na comprehensive source of data on violent deaths and includes unstructured incident narrative reports from coroners or medical\nexaminers and law enforcement. Conventional natural language processing approaches have been used to identify common\ncircumstances preceding female firearm suicide deaths but failed to identify rarer circumstances due to insufficient training data.\nObjective: This study aimed to leverage a large language model approach to identify infrequent circumstances preceding female\nfirearm suicide in the unstructured coroners or medical examiners and law enforcement narrative reports available in the NVDRS.\nMethods: We used the narrative reports of 1462 female firearm suicide decedents in the NVDRS from 2014 to 2018. The reports\nwere written in English. We coded 9 infrequent circumstances preceding female firearm suicides. We experimented with predicting\nthose circumstances by leveraging a large language model approach in a yes/no question-answer format. We measured the\nprediction accuracy with F1-score (ranging from 0 to 1). F1-score is the harmonic mean of precision (positive predictive value)\nand recall (true positive rate or sensitivity).\nResults: Our large language model outperformed a conventional support vector machine–supervised machine learning approach\nby a wide margin. Compared to the support vector machine model, which had F1-scores less than 0.2 for most infrequent\ncircumstances, our large language model approach achieved an F1-score of over 0.6 for 4 circumstances and 0.8 for 2 circumstances.\nConclusions: The use of a large language model approach shows promise. Researchers interested in using natural language\nprocessing to identify infrequent circumstances in narrative report data may benefit from large language models.\n(JMIR Ment Health 2023;10:e49359) doi: 10.2196/49359\nKEYWORDS\nfemale firearm suicide; large language model; document classification; suicide prevention; suicide; firearm suicide; machine\nlearning; mental health for women; violent death; mental health; language models; women; female; depression; suicidal\nJMIR Ment Health 2023 | vol. 10 | e49359 | p. 1https://mental.jmir.org/2023/1/e49359\n(page number not for citation purposes)\nZhou et alJMIR MENTAL HEALTH\nXSL•FO\nRenderX\nIntroduction\nSuicide is a leading cause of death in the United States. Suicide\nrisk factors include physical and mental health disorders,\nsubstance use disorders, prior exposure to violence, and having\na firearm at home [1-4]. Firearm suicide has been more prevalent\namong men, but age-adjusted female firearm suicide rates have\nincreased by 20% from 2010 to 2020, outpacing the rate increase\namong males by about 8 percentage points [5]. However,\nrelatively few studies have focused specifically on female\nfirearm suicide instead focusing on samples in which males are\noverrepresented (eg, military veterans) [6,7]. More data are\nneeded to identify circumstances surrounding female firearm\nsystems, and a primary source of these data derives from\nunprocessed narrative reports in the National Violent Death\nReporting System (NVDRS).\nIn a previous study [1], we found that conventional natural\nlanguage processing (NLP) algorithms could successfully\nidentify some relatively common circumstances preceding\nfemale firearm suicide, using coroners’and medical examiners’\n(CMEs’) and law enforcement (LE) narrative reports provided\nby the NVDRS. However, because reliably training a\nconventional NLP pipeline requires a sizeable training data set,\nthe approach worked well only for the most common preceding\ncircumstances.\nRecently, large language models such as ChatGPT were found\nto perform well on tasks such as answering yes/no questions\nand document classification [8-10]. Large language models\nwere developed on the basis of large corpora of data crawled\nfrom the web and can be used to solve machine learning tasks\nin a question-answer format. Moreover, these large language\nmodels do not rely on the task's data set size. In this study, we\nexplored the value of a large language model approach by\nframing our coding task as a binary response for classification.\nSpecifically, we tested a large language model approach to\nidentify infrequent circumstances preceding female firearm\nsuicide and compared this approach’s performance to that of\ntraditional machine learning models.\nMethods\nOverview\nChatGPT is the state-of-the-art large language model, but we\ncould not use it directly in this study. Our data contain protected\ninformation; hence, we could not upload them directly to\nChatGPT. Instead, we used open-source large language model\nalternatives. We ran these models locally to protect potentially\nsensitive information in the data. These models are developed\nsimilarly and are competitive in certain areas compared to\nChatGPT. In a benchmark evaluation of large language models’\nability in problem-solving [11], FLAN-T5 [12] and FLAN-UL2\n[13] were found to be less accurate than ChatGPT for world\nknowledge understanding and programming but competitive in\nfollowing complex instructions, comprehension and arithmetic,\nand causal reasoning. In preliminary studies, we experimented\nwith multiple large language models (FLAN-T5 [12],\nFLAN-UL2 [13], and others) and found that FLAN-UL2\nperformed the best. We hence used FLAN-UL2 for our\nsubsequent experiments. Developed by Google LLC,\nFLAN-UL2 is an open-source, 20 billion–parameter\nencoder-decoder model and is useful for zero-shot learning (ie,\nthe model makes predictions directly without further training).\nData Sets\nWe used the NVDRS Restricted Access Database of female\nfirearm suicides from 2014 to 2018 [1]. The data set contained\nunstructured CME and LE narrative reports describing the\ncircumstances leading up to the suicide deaths of 1462 females.\nThe reports were written in English. We manually coded 9\ninfrequent circumstances (ie, labels) preceding the firearm\nsuicide deaths following the instructions specified by Goldstein\net al [1]: sleep problems, abusive relationships, custody issues,\nsexual violence, isolation or loneliness, substance abuse,\ndementia, bullying, and caregiver issues. All infrequent labels\noccurred in <5% of the cases. We have provided details\nregarding the circumstance distribution in Table S1 in\nMultimedia Appendix 1. We split the data set into training and\ntest sets with a 0.5:0.5 ratio.\nModel Evaluation\nA prompt is the input for the large language model and will\nguide it for generating outputs. For FLAN-UL2, we designed\na prompt as a pair of a narrative report and a question. The\nnarrative report is the text we input into a traditional machine\nlearning model. The question varies depending on the\ncircumstances we want to the model to code. For example, for\nthe circumstance “bullying,” “Answer the following yes/no\nquestion: was the decedent experiencing bullying in-person or\nonline? Answer:” is the question. The model will yield an output\nof “Yes” if “bullying” is mentioned in the narrative report; if\nnot, “No” will be the output. The question was adapted from\nthe definition of each label developed through a previously\nreported manual review process with minimal changes [1]. A\ncomplete list of questions and definitions for each label is\nincluded in Table S2 in Multimedia Appendix 1. As a baseline,\nwe used a series of conventional support vector machine (SVM)\nmodels [14] trained to identify each circumstance. FLAN-UL2\nwas only applied on the test set. SVM models were trained on\nthe training set and applied on the test set. We repeated all\nexperiments 5 times with resampling of the training and test\nsets. We reported the average F1-score, which is the harmonic\nmean of the precision (positive predictive value) and recall (true\npositive rate or sensitivity). The F1-score measures the model’s\naccuracy considering the imbalance in the data set and ranges\nfrom 0 to 1.\nEthical Considerations\nThis study did not require approval from the University of\nWashington institutional research board because these\ndeidentified data on deceased persons were not considered\nhuman subjects research. We received ethical approval from\nthe National Violent Death Reporting System Restricted Access\nDatabase review committee (request number #410).\nJMIR Ment Health 2023 | vol. 10 | e49359 | p. 2https://mental.jmir.org/2023/1/e49359\n(page number not for citation purposes)\nZhou et alJMIR MENTAL HEALTH\nXSL•FO\nRenderX\nResults\nFLAN-UL2 performed better than the SVM for most of the\nfemale firearm suicide circumstances, sometimes substantially\nbetter (Figure 1). The F1-score of FLAN-UL2 was greater than\n0.8 for “sleep problem” (3.4% prevalence) and “sexual violence”\n(2.6% prevalence). “Bullying” (1.6% prevalence) was the only\ncircumstance where the SVM outperformed FLAN-UL2, and\nF1-scores were 0 or nearly 0 for all SVM and FLAN-UL2 runs.\nSee Table S1 in Multimedia Appendix 1 for prevalence in other\ncircumstances.\nFigure 1. F1-scores of the support vector machine (SVM) and FLAN-UL2 model for coding rare circumstances of female firearm suicide from suicide\nreports when averaged over 5 runs. The height of the bars represents the mean F1-score, and the line at the tip of the bars represents the SD across 5\nruns.\nDiscussion\nPrincipal Findings\nWe found that the large language model (FLAN-UL2)\noutperformed the SVM for 8 of the 9 infrequent circumstances\npreceding female firearm suicide deaths. These findings suggest\nthat a large language model approach can address a critical gap\nin identifying infrequent circumstances in unstructured text.\nUnlocking these data efficiently allows for subsequent analyses\nof female firearm suicide risk, including relationships among\nsexual violence, dementia, sleep problems, and firearm suicide.\nThe characterization of circumstances preceding female firearm\nsuicides is an understudied area. In a previous study, Goldstein\net al [1] used traditional NLP methods to predict 5 circumstances\nfrom suicide reports, with F1-scores ranging from 0.6 to 0.8.\nHowever, all these circumstances had a prevalence of at least\n15%. In our study, all 9 circumstances had a prevalence of less\nthan 5%. We complemented the existing work by providing a\nmethod for automatically coding circumstances preceding\nfemale firearm suicides at a larger scope.\nThe failure in identifying the “bullying” circumstance may be\ndue to the fact that bullying is one of the most infrequent\ncircumstances preceding female firearm suicide in the NVDRS.\nThe question we provided to the large language model, “was\nthe decedent experiencing bullying in-person or online?” might\nnot be sufficiently sensitive for the model to understand how\nto identify these circumstances in the narrative reports. More\ndetailed explanation of “bullying,” such as the victim was\ninsulted or hurt at school or at the workplace, might be needed\nfor the model to reason better. Large language models are known\nto be sensitive to prompt text, and designing an appropriate\nprompt (also known as prompt engineering) is an essential part\nof using large language models effectively [10]. Novel\nprompting techniques, such as few-shot learning (provide\nproblem examples as part of the prompt) [10], have been\nproposed and may improve large language models’performance.\nIn this study, we used simple and consistent prompts to provide\na baseline for using large language models to code infrequent\ncircumstances preceding female suicide. Large language models\nare also computationally expensive. The experiments in this\nstudy were carried out on 2 NVIDIA A100 40 GB graphics\nprocessing units. Large language models are also known to be\nsensitive to “hallucination” [15], meaning that they generate\nparagraphs of texts that look reasonable but are factually\nincorrect. In this study, we prompted the model to generate\nyes/no answers, bypassing the risks of hallucination.\nJMIR Ment Health 2023 | vol. 10 | e49359 | p. 3https://mental.jmir.org/2023/1/e49359\n(page number not for citation purposes)\nZhou et alJMIR MENTAL HEALTH\nXSL•FO\nRenderX\nConclusions\nOur large language model successfully identified infrequent\ncircumstances preceding female firearm suicide deaths, having\noutperformed conventional NLP approaches by a wide margin.\nThis finding suggests that large language models can be used\nto unlock textual analysis within public health research. More\nbroadly, the success of our relatively simple queries at\nidentifying infrequent circumstances suggests that large\nlanguage models may be useful in public health surveillance,\npotentially allowing practitioners to track the prevalence of\ninfrequent conditions that are never explicitly coded into\nsurveillance systems. Future studies should explore the\nperformance of different large language models and variations\nin the models’ underlying mechanisms when applied to coding\ninfrequent circumstances.\nAcknowledgments\nThis work was funded by grants from the National Collaborative on Gun Violence Research (PI: Prater/ Goldstein, 2021) and\nthe National Library of Medicine (4R00LM012868). We thank Dr Meliha Yetisgen for providing the computational resources.\nAuthors' Contributions\nWZ conceptualized the study; carried out the formal analysis, investigation, and validation; designed the methodology; visualized\nthe data; and drafted, edited, and reviewed the manuscript. EVG conceptualized the study, carried out the investigation, acquired\nfunding, designed the methodology, supervised the study, and reviewed and edited the manuscript. LCP conceptualized the study,\ncurated the data, acquired funding, designed the methodology, carried out the investigation, and edited and reviewed the manuscript.\nSJM conceptualized and supervised the study, acquired funding, designed the methodology, carried out the investigation, and\nedited and reviewed the manuscript.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nPrevalence, definitions and derived questions of the rare female firearm suicide circumstances.\n[DOCX File , 28 KB-Multimedia Appendix 1]\nReferences\n1. Goldstein EV, Mooney SJ, Takagi-Stewart J, Agnew BF, Morgan ER, Haviland MJ, et al. Characterizing Female Firearm\nSuicide Circumstances: A Natural Language Processing and Machine Learning Approach. Am J Prev Med 2023\nAug;65(2):278-285 [doi: 10.1016/j.amepre.2023.01.030] [Medline: 36931986]\n2. Anestis MD. Prior suicide attempts are less common in suicide decedents who died by firearms relative to those who died\nby other means. J Affect Disord 2016 Jan 01;189:106-109 [doi: 10.1016/j.jad.2015.09.007] [Medline: 26432034]\n3. Miller M, Zhang Y, Prince L, Swanson SA, Wintemute GJ, Holsinger EE, et al. Suicide Deaths Among Women in California\nLiving With Handgun Owners vs Those Living With Other Adults in Handgun-Free Homes, 2004-2016. JAMA Psychiatry\n2022 Jun 01;79(6):582-588 [FREE Full text] [doi: 10.1001/jamapsychiatry.2022.0793] [Medline: 35476016]\n4. Kellermann AL, Rivara FP, Somes G, Reay DT, Francisco J, Banton JG, et al. Suicide in the home in relation to gun\nownership. N Engl J Med 1992 Aug 13;327(7):467-472 [doi: 10.1056/NEJM199208133270705] [Medline: 1308093]\n5. WISQARS™ — Web-based Injury Statistics Query and Reporting System. Centers for Disease Control and Prevention.\nURL: https://www.cdc.gov/injury/wisqars/index.html [accessed 2023-02-03]\n6. Spark TL, Cogan CM, Monteith LL, Simonetti JA. Firearm Lethal Means Counseling Among Women: Clinical and Research\nConsiderations and a Call to Action. Curr Treat Options Psychiatry 2022;9(3):301-311 [FREE Full text] [doi:\n10.1007/s40501-022-00273-3] [Medline: 35791313]\n7. Rowhani-Rahbar A, Simonetti JA, Rivara FP. Effectiveness of Interventions to Promote Safe Firearm Storage. Epidemiol\nRev 2016;38(1):111-124 [doi: 10.1093/epirev/mxv006] [Medline: 26769724]\n8. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. Performance of ChatGPT on USMLE: Potential\nfor AI-assisted medical education using large language models. PLOS Digit Health 2023 Feb;2(2):e0000198 [FREE Full\ntext] [doi: 10.1371/journal.pdig.0000198] [Medline: 36812645]\n9. Xu F, Alon U, Neubig G, Hellendoorn V. A systematic evaluation of large language models of code. 2022 Presented at:\nMAPS '22: 6th ACM SIGPLAN International Symposium on Machine Programming; June 13, 2022; San Diego, CA [doi:\n10.1145/3520312.3534862]\n10. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few-shot learners. arXiv\nPreprint posted online May 28, 2020. [FREE Full text] [doi: 10.5860/choice.189890]\n11. Chia K, Hong P, Bing L, Poria S. INSTRUCTEVAL: towards holistic evaluation of instruction-tuned large language models.\narXiv Preprint posted online June 7, 2023. [FREE Full text]\nJMIR Ment Health 2023 | vol. 10 | e49359 | p. 4https://mental.jmir.org/2023/1/e49359\n(page number not for citation purposes)\nZhou et alJMIR MENTAL HEALTH\nXSL•FO\nRenderX\n12. Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling instruction-finetuned language models. arXiv\nPreprint posted online October 20, 2022. [FREE Full text]\n13. Tay Y. A New Open Source Flan 20B with UL2. Yi Tay. URL: https://www.yitay.net/blog/flan-ul2-20b[accessed 2023-04-03]\n14. Goodfellow I, Bengio Y, Courville A. Deep Learning. Cambridge, MA: MIT Press; 2016.\n15. Sallam M. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives\nand valid concerns. Healthcare (Basel) 2023 Mar 19;11(6) [FREE Full text] [doi: 10.3390/healthcare11060887] [Medline:\n36981544]\nAbbreviations\nCME: coroners or medical examiners\nLE: law enforcement\nNVDRS: National Violent Death Reporting System\nNLP: natural language processing\nSVM: support vector machine\nEdited by J Torous; submitted 30.05.23; peer-reviewed by R Panirselvam, L Souza; comments to author 26.08.23; revised version\nreceived 31.08.23; accepted 02.09.23; published 17.10.23\nPlease cite as:\nZhou W, Prater LC, Goldstein EV, Mooney SJ\nIdentifying Rare Circumstances Preceding Female Firearm Suicides: Validating A Large Language Model Approach\nJMIR Ment Health 2023;10:e49359\nURL: https://mental.jmir.org/2023/1/e49359\ndoi: 10.2196/49359\nPMID: 37847549\n©Weipeng Zhou, Laura C Prater, Evan V Goldstein, Stephen J Mooney. Originally published in JMIR Mental Health\n(https://mental.jmir.org), 17.10.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution\nLicense (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any\nmedium, provided the original work, first published in JMIR Mental Health, is properly cited. The complete bibliographic\ninformation, a link to the original publication on https://mental.jmir.org/, as well as this copyright and license information must\nbe included.\nJMIR Ment Health 2023 | vol. 10 | e49359 | p. 5https://mental.jmir.org/2023/1/e49359\n(page number not for citation purposes)\nZhou et alJMIR MENTAL HEALTH\nXSL•FO\nRenderX",
  "topic": "Injury prevention",
  "concepts": [
    {
      "name": "Injury prevention",
      "score": 0.632893443107605
    },
    {
      "name": "Poison control",
      "score": 0.6200106739997864
    },
    {
      "name": "Suicide prevention",
      "score": 0.6163855195045471
    },
    {
      "name": "Occupational safety and health",
      "score": 0.5472991466522217
    },
    {
      "name": "Medicine",
      "score": 0.5259330868721008
    },
    {
      "name": "Human factors and ergonomics",
      "score": 0.5092088580131531
    },
    {
      "name": "Narrative",
      "score": 0.4906226694583893
    },
    {
      "name": "Law enforcement",
      "score": 0.46552416682243347
    },
    {
      "name": "Demography",
      "score": 0.3810902237892151
    },
    {
      "name": "Psychiatry",
      "score": 0.3785775601863861
    },
    {
      "name": "Psychology",
      "score": 0.3304176330566406
    },
    {
      "name": "Medical emergency",
      "score": 0.3215833902359009
    },
    {
      "name": "Pathology",
      "score": 0.08685523271560669
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802336062",
      "name": "Harborview Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223532165",
      "name": "University of Utah",
      "country": "US"
    }
  ]
}