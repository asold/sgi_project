{
  "title": "Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?",
  "url": "https://openalex.org/W3176793246",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2337383024",
      "name": "Cunxiang Wang",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2131802016",
      "name": "Pai Liu",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Westlake University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "Cunxiang Wang, Pai Liu, Yue Zhang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3241–3251\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3241\nCan Generative Pre-trained Language Models Serve as\nKnowledge Bases for Closed-book QA?\nCunxiang Wang♠♣∗\n, Pai Liu♣∗\nand Yue Zhang♣♥†\n♠Zhejiang University, China\n♣School of Engineering, Westlake University, China\n♥Institute of Advanced Technology, Westlake Institute for Advanced Study, China\n{wangcunxiang, zhangyue, liupai}@westlake.edu.cn\nAbstract\nRecent work has investigated the interesting\nquestion using pre-trained language models\n(PLMs) as knowledge bases for answering\nopen questions. However, existing work is\nlimited in using small benchmarks with high\ntest-train overlaps. We construct a new dataset\nof closed-book QA using SQuAD, and in-\nvestigate the performance of BART. Experi-\nments show that it is challenging for BART\nto remember training facts in high precision,\nand also challenging to answer closed-book\nquestions even if relevant knowledge is re-\ntained. Some promising directions are found,\nincluding decoupling the knowledge memoriz-\ning process and the QA ﬁnetune process, forc-\ning the model to recall relevant knowledge\nwhen question answering.\n1 Introduction\nLarge-scare pre-trained language models (PLMs)\nsuch as BERT (Devlin et al., 2019), GPT (Radford\net al., 2018) have signiﬁcantly improved the perfor-\nmance of NLP tasks (Radford et al., 2019). There\nis increasing evidence showing that PLMs contain\nworld knowledge (Petroni et al., 2019; Zhou et al.,\n2020; Talmor et al., 2020). As a result, recent re-\nsearch considers generative PLMs such as T5 (Raf-\nfel et al., 2020) and BART (Lewis et al., 2020a) for\nClosed-book QA, which has only question-answer\npairs without external knowledge source. For ex-\nample, after being ﬁnetuned on a few QA pairs, a\ngenerative LM can directly output “Florence” af-\nter being given the question “Where was Dante\nborn?”. Roberts et al. (2020) ﬁnd that generative\nPLMs can store and use knowledge as they can\nachieve relatively high performance in closed-book\nQA task on three datasets. However, Lewis et al.\n(2020b) ﬁnd that the excellent results are mainly\n∗Equal contribution\n†The corresponding author\nFigure 1: Process of generative PLMs for closed-book\nQA. (1) BART performs poorly on closed-book QA\nafter QA ﬁnetuning; (2) We LM-ﬁnetune BART with\nrelated passages to feed knowledge and use a recit-\ning task to evaluate how much knowledge the LM-\nﬁnetuned model memorizes; (3) Though memorizing\nmost needed knowledge, BART still faces challenge on\nclosed-book QA after QA ﬁnetuning.\ndue to high question/answer overlap rates between\ntraining and testing data.\nExisting research leaves many open questions\non the potential of generative pre-trained LMs on\nclosed-book QA. For example, the used datasets\nconsist of question-answer pairs only, and there is\nno mechanism to control what factual knowledge\nis already used to train a generative PLM before\ntaking the closed-book questions. In addition, the\nhigh overlapping rates between training and testing\nquestions and answers make it difﬁcult to under-\nstand whether the answer that a model gives comes\nfrom its inherent knowledge or superﬁcial cues in\ntraining data. To address these issues, we make\na new benchmark of question-answer pairs from\nSQuAD (Rajpurkar et al., 2018), where each ques-\n3242\ntion has a corresponding Wikipedia passage as a\ntraceable knowledge source for pre-training. We\nﬁnd that despite giving around 25% accuracy on ex-\nisting test sets (i.e., WebQuestions and TriviaQA),\nBART gives only 1.5% accuracy on the SQuAD\ndataset.\nThis result shows that there is still much chal-\nlenge in using BART for closed-book QA directly.\nWe further investigate the reason by separately\nexamining whether BART can remember factual\nknowledge accurately, and whether it can make\nuse of remembered knowledge to answer questions.\nThe general process of investigating these two is-\nsues is presented in Figure 1.\nFor the ﬁrst issue, we use related passages in\nSQuAD to further extra pre-train BART, which\nwe call as LM-ﬁnetuning, and test the ratio of re-\ntained factual knowledge using a language mod-\neling task, which we call as reciting. Results\nshow that as the number of training passages grows,\nBART demonstrates severe issues of forgetting, los-\ning track of exact facts in the LM task. For example,\nwhen the number of passage is around 500, BART\ncan memorize 66% needed knowledge. But when\nthe number of passage increases to about 5000, the\nratio becomes 4%.\nFor the second issue, we use versions of LM-\nﬁnetuned BART that can retain the majority of\nfactual knowledge for further QA ﬁnetuning, by\nconstraining the number of passages. Although\nall the training and testing questions concern the\npassages in LM-ﬁnetuning, BART still fails to an-\nswer the majority of questions. This demonstrates\ndifﬁculties in making use of internal knowledge\nfor QA. In addition, further experiments show that\nQA ﬁnetuning can negatively inﬂuence the retained\nfactual knowledge as measured using the original\nLM task.\nWhile reporting such challenges, we also ﬁnd\nsome promising directions by using simple data\naugmentation tricks. For example, simply adding\nrelated passages to test outputs can help BART\nretrieve relevant factual knowledge and give the\ncorrect answer. In addition, rather than treating\nQA ﬁnetuning in the same way as LM pre-training\n(Roberts et al., 2020), decoupling the LM pre-\ntraining task and the QA ﬁnetuning tasks can also\nallow a model to better retain factual knowledge\nthrough the QA-ﬁnetuning task. 1\n1We have released the code and dataset at\nhttps://github.com/wangcunxiang/Can_\nPLM_Server_as_KB for future study.\nTrain Set Dev Set Test Set\nWebQuestions 3778 1016 1016\nTriviaQA 961091 4975 4976\nNaturalQuestions 107369 900 900\n(a) The QA pairs of three datasets.\nTrain Set Dev Set Test Set\nSQuAD 86396(19035) 2968(602) 2930(602)\n(b) The QA pairs and passages statistics of SQuAD.\nThe numbers in () are the passage amounts.\nTable 1: Details of each dataset after our processing.\nModels \\ Dataset SQuAD WB TQ NQ\noriginal BART-Large\n→ QA-ﬁnetune 1.5% 30.0% 24.9% 23.0%\noriginal BART-Large\n→ pre-trained with\nall passages\n→ QA-ﬁnetune\n1.8% - - -\nTable 2: Closed-book QA performance of BART on\nfour datasets. For SQuAD, only QA pairs are used in\nthis experiments. WB, TQ and NQ means WebQues-\ntions, TriviaQA and NaturalQuestions, respectively.\n2 Using SQuAD for Closed-book QA\nIn the closed-book QA task (Roberts et al., 2020), a\nmodel needs to answer questions without external\nresources. Formally, the input is a question q, and\nthe output is a sequence of tokenso. For evaluation,\nthe correct golden answer g will be compared with\no. Previous work (Roberts et al., 2020) uses the\nExact Match (EM) metric to score o against g.\nWe conduct closed-book QA by using the\nBART model (Lewis et al., 2020a) on four\ndatasets-WebQuestions (Berant et al., 2013), Triv-\niaQA (Joshi et al., 2017), NaturalQuestions\n(Kwiatkowski et al., 2019) and SQuAD2 (Ra-\njpurkar et al., 2018). BART is a transformer-based\n(Vaswani et al., 2017) sequence-to-sequence gen-\nerative PLM, which we choose because it has\nachieved several state-of-the-art results on genera-\ntive tasks. We use the publicly released checkpoint\nBART-Large in this work.2\nTo use a generative PLM on each dataset,\nthe model is ﬁrst ﬁnetuned using the training\nquestion-answer pairs. We call this process as QA-\nﬁnetuning. While the other three datasets are used\nby following previous work (Roberts et al., 2020),\nwe make a novel adaptation of the SQuAD dataset\nfor closed-book QA. SQuAD (Rajpurkar et al.,\n2018) is a wildly-adopted QA dataset typically for\nextractive QA, where the input is a question to-\n2https://huggingface.co/facebook/\nbart-large/tree/main\n3243\nDataset\\Overlap Type Answer Overlap Question Overlap\nNaturalQuestions 61.5% 32.5%\nTriviaQA 78.7% 33.6%\nWebQuestions 59.3% 27.5%\nSQuAD 24.0% 1.0%\nTable 3: Question and Answer Overlaps on four\ndatasets. Question overlaps data of NaturalQuestions,\nTriviaQA and WebQuestions are from Lewis et al.\n(2020b); Answer overlaps on the three datasets are a\nbit different from Lewis et al. (2020b) because of our\ndataset pre-processing.\nDataset \\ Overlap Type Otest Overlap\nwith Gtrain\nGtest Overlap\nwith Gtrain\nWebQuestions 88.5% 59.3%\nSQuAD 39.8% 24.0%\nTable 4: Overlap analysis between test outputs/golden\nanswers and training answers. We select the top-\nperforming results to analyze.\ngether with a passage containing the answer fact,\nand the answer is a span from the passage. How-\never, no previous work has used SQuAD for closed-\nbook QA yet. Compared to other QA datasets,\nSQuAD is the most suitable for our setting, con-\ntaining corresponding passages, lower test-train\noverlap, and receiving more research attention. To\napply SQuAD on closed-book QA, we only use\nQA pairs for input and output when QA-ﬁnetuning.\nFor TriviaQA and WebQuestions, many questions\nhave multiple answers. In order to align with the\nother two data sets, we split one question with sev-\neral answers into several same questions with one\nanswer when training, and take one test output as\ncorrect if it appears in the answer list when testing.\nAs the test sets of SQuAD, NaturalQuestions and\nTriviaQA are not fully publicly released yet and\nWebQuestions does not have a development set,\nwe split the development set of the three datasets\nand the test set of WebQuestions into two subsets\nto serve as a new development set and a new test\nset. We report performance on the new test sets in\nTable 2 while analyzing the overlaps on the two\nsubsets together in Table 4 and Table 5. The details\nof four datasets after our pre-processing are shown\nin Table 1.\nPrevious work shows that T5 and BART can\nachieve promising results (Roberts et al., 2020;\nLewis et al., 2020b) on WebQuestions, TriviaQA\nand NaturalQuestions. However, recently, Lewis\net al. (2020b) ﬁnd that the high performance is\nmainly because the three datasets have severe test-\ntrain overlap problems. In particular, we use an-\nOverlap Non-Overlap\nCorrect 29.8% (604) 0.2% (5)\nIncorrect 58.7% (1189) 11.3% (228)\n(a) On WebQuestions\nOverlap Non-Overlap\nCorrect 1.3% (77) 0.1% (6)\nIncorrect 38.5% (2272) 60.1% (3530)\n(a) On SQuAD\nTable 5: Overlap analysis of test outputs on WebQues-\ntions and SQuAD by BART. In the result cells, we\npresent both percentages and case numbers. We select\nthe top performing result to analyze.\nswer overlap to denote the situation where the\nanswer a in a test (q, a) pair exists in training an-\nswers, and the term question overlap to denote the\nfact that a training question with similar meaning\ncan be found for q. To analyze whether SQuAD\nhas the same problem, we also compute the overlap\nof it. Answer overlap can be easily calculated. For\nquestion overlap, following Lewis et al. (2020b),\nwe ﬁrst randomly sample 1,000 (q, a) pairs from\nthe SQuAD test set. Then for each test question,\nwe automatically select SQuAD training questions\nwhose answer is a sub-sequence of the test answer.\nThen we ask three human experts to ﬁnd whether\nthe test q overlaps with any training question.\nThe breakdown statistics are given in Table 3.\nSQuAD has much fewer test-train overlapped cases\nthan the other three datasets. For example, only\naround 1% of SQuAD test questions overlap with\ntraining questions while the number is around 30%\nin the other three datasets.\n2.1 Results\nThe overall QA results on the four datasets are\nshown in the ﬁrst row of Table 2. BART\nachieves relatively high results on the three datasets\nWebQuestions, TriviaQA, and NaturalQuestions.\nHowever, it performs poorly on SQuAD in closed-\nbook QA, with only 1.5% accuracy. We also use\nSQuAD passages to further pre-train BART and\nthen conduct QA-ﬁnetuning. The result is shown\nin the second row of Table 2, the performance is\n1.8% a bit better than 1.5% but still extremely low.\nAccording to Lewis et al. (2020b), the results\nare inﬂuenced by test-train overlap rates. For sim-\nplicity, we deﬁne the set of gold standard answers\nin the train set as Gtrain, the set of gold standard\nanswers in the test set asGtest. We deﬁne the set of\noutput answers of BART on the test set asOtest, the\nset of output answers which are correct as Ocorrect.\nTo further investigate how overlap inﬂuences\n3244\nBART’s outputs, we choose WebQuestions as the\nhigh-overlap dataset representative to compare with\nthe low-overlap dataset SQuAD. Results are shown\nin Table 4. The Otest of BART on WebQuestions\nhave an 88.5% overlap with Gtrain, which is a\ndecisive proportion. However, the Gtest have only\n59.3% overlap with Gtrain. For BART on SQuAD,\nthe ratios are 39.8% to 24.9%, which is relatively\nless severe. This indicates that if testing questions\nhave a large overlap with training questions, the\nmodel tends to generate the targets and words in\nthe train set.\nWe further measure the relationship between\nhow correct/incorrect outputs and overlap/non-\noverlap with Gtrain. The results are shown in\nTable 5, 604 of Ocorrect of BART on WebQues-\ntions overlap with Gtrain, and only 5 instances\nof Ocorrect do not exist in Gtrain. However, all\nthe ﬁve non-overlapping Otest on WebQuestions\nare combinations of words of Gtrain and question\nwords, which can be viewed as a mild type of over-\nlap. The situation is similar but sightly better on\nSQuAD. These results indicate that it is much eas-\nier for BART to answer correctly by superﬁcial\ncues than by using its internal knowledge.\n3 Task Design\nThe original purpose of previous research (Petroni\net al., 2019; Roberts et al., 2020) is to use pre-\ntrained language models (PLMs) as knowledge\nbases (KBs) and answer questions according to\ninternal knowledge the model contains. However,\nif the model tends to match test questions with\ntraining questions for retrieving answers, then the\nsource of knowledge is restricted to training ques-\ntions. This deviates from the ultimate goal.\nWe are interested in quantitatively measuring\nthe capability of pre-trained model in closed-book\nQA using its own internal knowledge from pre-\ntraining. This capability can be broken down into\ntwo components. First, the capability of a memoriz-\ning knowledge from pre-training. Second, the abil-\nity of retrieving memorized knowledge for question\nanswering. We show investigations and report the\nresults in the two sections below.\n3.1 Procedure\nAs shown in Figure 2, our design is motivated by\nclassroom teaching. A teacher ﬁrst teaches the\ncontent of a textbook and then asks the student\nto recite the important points of the book in order\nFigure 2: The main task design. The lower right bold\ncontext of each process are names of this process. The\nbold context in the upper middle of each process is the\ncorresponding process in the classroom teaching. The\nmiddle context is the purpose of this process. The left\nicon represent the state of the model.\nModels \\ Dataset ALL SQuAD\n(20279)\nrandom-initialized BART 0.0%\noriginal BART 2.2%\nBART → LM-ﬁnetuning 2.7%\nTable 6: The reciting performance on all SQuAD pas-\nsages. We use the BART-Large checkpoint. LM-\nﬁnetuning and reciting are both conducted on the same\n20279 passages.\nto test how well they know the book. Next, the\nteacher gives the student some exercise questions\nfor practice. Finally, the teacher gives a different\nset of exam questions to test the student. Note that\nthe whole book is taught and recited, rather than\na split of the book, and the exercise questions and\nexam questions are all related to the book.\nSection 4 ( Knowledge Memory) corresponds\nthe teaching and reciting processes in the class-\nroom teaching. Section 5 ( Question Answering)\ncorresponds the practice and exam processes.\n4 Knowledge Memory\nTo investigate whether BART can acquire and store\nknowledge from raw corpus, we use passages from\nSQuAD to ﬁnetune the BART model, which we\ncall LM-ﬁnetuning. This period can be seen as\n3245\nModels \\ Dataset 20 160 547 1094 1641 6020\noriginal BART 1.5% 5.2% 3.6% 3.2% 2.9% 2.2%\nBART → LM-ﬁnetuning 87.3% 72.6% 66.3% 34.3% 14.0% 3.9%\nBART → LM-ﬁnetuning\n(Added Preﬁx/Sufﬁx) 85.5% 79.6% 59.5% 40.4% 15.8% 4.0%\nTable 7: Performance of reciting. We use the BART-Large checkpoint. For the header of each column, the numbers\nstand for passage amounts of the subset. Note that LM-ﬁnetuning and reciting are both conducted on the same\npassages. The last row of this table will be discussed in Section 5.3\nFigure 3: Examples of two types of MASK policies in\ntraining and testing periods of LM-ﬁnetuning. The pas-\nsage masked randomly is for training and the passage\nmasked with answer spans is for testing (reciting).\nfeeding knowledge into BART. Then we test the\nmodel to examine how much knowledge BART\ncan memorize. We also call this testing process as\nreciting.\nTraining of LM-ﬁnetuning. We follow the\noriginal training objective of BART for the MLM-\nﬁnetune step, which is a denoising auto-encoding\nprocess. The original BART training objective in-\nvolves ﬁve operations, namely token masking, sen-\ntence permutation, document rotation, token dele-\ntion and text inﬁlling (Lewis et al., 2020a). We\nonly adopt token inﬁlling in this work because\nit shows beneﬁts on all downstream tasks (Lewis\net al., 2020a). In addition, the sentence permutation\ntask is shown harmful for tasks despite only being\nuseful for text summarization (Lewis et al., 2020a).\nFor each input passage, we randomly mask 30%\ntokens following Lewis et al. (2020a). An example\nis shown in the third row of Figure 3. We ask the\nmodel to recover the passage as the output, and\nuse the output and the original passage to compute\nloss.\nTesting of LM-ﬁnetuning (Reciting). In test-\ning period of LM-ﬁnetuning, we develop a task\ncalled ‘Reciting’to probe how much (speciﬁc)\nknowledge the model has. Inspired by Petroni\net al. (2019) and Talmor et al. (2020), who ask\ndiscriminative PLMs to ﬁll masks of given masked\npassages/sentences, our reciting task is to give a\ngenerative PLM several masked passages and ask\nit to recover them. For each passage, we mask the\ntoken spans which are answers of related questions.\nAn example is shown in the last row of Figure 3.\nIn this way, we can assume that if the BART can\nrecover the speciﬁc-masked passages, it must have\nthe knowledge needed for further QA. Note that\ndoing training for LM-ﬁnetuning, the masked to-\nkens are randomly chosen, following BART (Lewis\net al., 2020a). Besides, because the answer spans\nare mostly entities or independent knowledge seg-\nments, it is relatively less likely for models to re-\ncover them by heuristics or superﬁcial cues. It\nis natural to do reciting to probe the model’s in-\nternal knowledge since it is most related to the\nMasked Language Model process (LM-ﬁnetuning\nand BART’s pre-training task).\nEvaluation Metrics. We use the accuracy of\nmasked spans recovery to measure how much\nknowledge the model memorizes. Because many\nanswer spans appear several times in passages, we\ncannot simply treat the presence of the span as cor-\nrect. In addition, even when the masked token is\ngenerated correctly, if its contextual words change,\nthe meaning of the sentence may be different. Con-\nsidering these, we choose a more strict evaluation\nmetric for the reciting accuracy. We treat a span as\ncorrectly predicted only if subsequent words after\nthe current mask and before the next mask (or the\nsubsequent 10 tokens if the span between masked\ntokens is more than 10) are also correctly predicted.\n4.1 Results\nWe ﬁrst conduct reciting experiments on all\nSQuAD passages using the original BART, a\nrandom-initialized BART and a LM-ﬁnetuned\nBART. The results are shown in Table 6. The\nrandom-initialized BART gives zero accuracy,\ndemonstrating that the task is difﬁcult and there\n3246\nis no possibility of guessing. The original BART\nscores 2.2%, showing that it contains certain but\nlimited knowledge. The LM-ﬁnetuned BART\ngives 2.7% accuracy. This result shows that LM-\nﬁnetuning is useful to a certain extent. However, de-\nspite that 100% knowledge is given, LM-ﬁnetuning\nonly increases the result by 0.5%, demonstrating\nthat BART faces signiﬁcant challenges in memoriz-\ning important knowledge contained in pre-training\nSQuAD texts.\nGiven above observations, we try to reduce the\nchallenge by producing smaller datasets by ex-\ntracting subsets from SQuAD. The subsets include\n20, 160, 547, 1094, 1641, 6020 passages, respec-\ntively, where the three numbers indicate the passage\namounts. For these reciting experiments, we con-\nsider only the original and LM-ﬁnetuned BART.\nThe results are shown in the ﬁrst two rows of\nTable 7. We can ﬁnd that (1) using LM-ﬁnetuning,\nBART can memorize some knowledge. For ex-\nample, when passage subset is 547, the original\nBART can only recover 3.6% masked spans cor-\nrectly while the LM-ﬁnetuned BART can recover\n66.3% masked spans; (2) The memorization ability\nquickly decreases when the passage amount in-\ncreases. For example, when passage subset are 20,\nBART can recover 87.3% masks correctly; when\nit is 1094, the accuracy falls to 34.3%; when it is\n6020, the accuracy is only 3.9%.\nWe conclude that BART has a certain ability\nto store (factual) knowledge, but the capacity is\nrather weak. If we control the number of passages\nfor LM-ﬁnetuning, we can make sure that BART\ncan memorize most needed knowledge. The LM-\nﬁnetuned model trained on smaller subsets gives\na more useful setting for testing QA abilities of\nBART when we are conﬁdent that relevant knowl-\nedge is retained.\n5 Question Answering\nWe employ the settings in the ﬁrst three columns\nin Table 7, where models can memorize at least\n50% of needed knowledge, for further analyzing\nthe relationship between memory and QA ability.\nFor these experiments, all QA pairs come from\npassages that BART has been LM-ﬁnetuned on.\n5.1 Overall Results\nBesides Exact Match (EM) which is commonly\nused in previous closed-book QA work (Roberts\net al., 2020; Lewis et al., 2020b), we also consider\nFigure 4: An intuitive approach to QA-bridge-tuning.\nTo make the model more dependent on the inter-\nnal knowledge to answer the question, the model is\nrequired to generate not only answer but also the\ncorresponding passage. The outputs should be ‘P\n<ANSWER> A’, where ‘P’ stands for the correspond-\ning passage, <ANSWER> is a special marker and the\nA stands for the answer.\nHuman Evaluation (HE) and F1 for two reasons.\nFirst, we observe that EM cannot fully indicate cor-\nrectness. For example, a question is “What century\ndid ... ?” and the golden answer is “10th century”.\nThe model outputs “10th” which is actually cor-\nrect in but taken incorrect by EM. Second, F1 can\nhelp indicate the similarity between the outputs and\ngolden answers.\nThe overall results are presented in the ﬁrst\ntwo rows of Table 8. According to the result\nof ‘original BART-Large→LM-ﬁnetuning→QA-\nﬁnetuning’, compared to Reciting Accuracy (RA)\nof each model, the QA accuracy is much lower\n(87.3% vs 30%, 72.6% vs 6.5%, 66.3% vs 6.7% in\nHE). This result shows that BART’s ability to use\nits internal knowledge to answer questions is weak.\nIn addition, comparison between the ﬁrst row and\nthe second row shows that memorized knowledge\nhelps the models better answer questions, though\nthe help is not much (30% vs 0.0%, 6.5% vs 4.3%,\n6.9% vs 4.9% in HE).\nFor the reciting-QA-accuracy gap, we propose\ntwo possible explanations, the ﬁrst is that the model\ncannot activate related memory for question an-\nswering; the second is that the memorized knowl-\nedge is somehow corrupted during QA-ﬁnetuning.\n5.2 Strengthening Memory Retrieval\nQualitative cases show that, even the model con-\ntains needed knowledge, the model does not neces-\nsarily refer to the most relevant memory for ques-\ntion answering after QA-ﬁnetuning. We list several\nthis kind of examples in the ‘QA-ﬁnetune’ col-\numn of Table 9. For example, in the ﬁrst row of\nTable 9, for the question “What is Southern Cali-\nfornia often abbreviated as?”, despite of the model\n3247\nModels \\ Dataset 20 (16/2/2;125/8/10) 160 (128/16/16;653/107/93) 547 (442/53/52;2334/314/306)\nRA(%) EM(%) HE(%) F1(%) RA(%) EM(%) HE(%) F1(%) RA(%) EM(%) HE(%) F1(%)\nBART → QA-ﬁnetuning 1.5 0.0 0.0 11.0 5.2 2.2 4.3 6.4 3.6 1.9 4.9 7.0\nBART → LM-ﬁnetuning\n→ QA-ﬁnetuning 87.3 10.0 30.0 15.4 72.6 3.2 6.5 9.0 66.3 2.3 6.9 6.7\nBART → LM-ﬁnetuning\n→ QA-ﬁnetuning\n(Added Preﬁx/Sufﬁx)\n85.5 10.0 30.0 21.0 79.6 3.2 10.8 10.1 59.5 2.9 7.8 8.2\nBART → LM-ﬁnetuning\n→ QA-bridge-tuning 87.3 20.0 40.0 27.8 72.6 9.7 20.4 15.3 66.3 4.6 11.8 9.3\nBART → LM-ﬁnetuning\n→ QA-bridge-tuning\n(Added Preﬁx/Sufﬁx)\n85.5 20.0 40.0 31.7 79.6 11.8 22.6 16.3 59.5 5.6 12.7 10.3\nTable 8: QA performance on three subsets of SQuAD. The numbers in headers are the passage and QA pair\namounts, for example, ‘160 (128/16/16;653/107/93)’ indicates this subset has overall 160 passages and 128/16/16\npassages, 653/107/93 QA pairs in train/dev/test set, respectively. The number in RA column stands for reciting\naccuracy, which is the same with Table 7. The RAs in the table can show how much knowledge BART memo-\nrizes before QA-ﬁnetuning, of which values the model should achieve in QA accuracy if it can fully use internal\nknowledge to answer questions. The cells with bold text are our methods. EM, HE indicate Exact Match, Human\nEvaluation, respectively. ‘BART’ denotes the ‘BART-Large’ checkpoint.\nQuestion&Answer Model Output\nQA-ﬁnetune\nQA-bridge-tune\nQ: What is Southern\nCalifornia often\nabbreviated as?\nA: SoCal\nSouthern\nCalifornia\nSouthern California, often\nabbreviated SoCal, is...\n<ANSWER> SoCal\nQ: What century\ndid the Normans\nﬁrst gain their\nseparate identity?\nA:10th century\n20th\ncentury\n... distinct cultural and\nethnic identity of the\nNormans emerged initially\nin the ﬁrst half of the\n10th century ...\n<ANSWER> 10th\nQ: What is the\nlargest stadium\nin Australia?\nA: Melbourne\nCricket ground\nAustralia\nStadium\n... <ANSWER>\nMelbourne Cricket\nground\nQ: When did the\n1973 oil crisis begin?\nA: October 1973\n1973 ... <ANSWER> October\n1973\nTable 9: Four real output examples on QA-ﬁnetuning\nand QA-bridge-tuning by BART.\nis trained with “Southern Californi, often abbrevi-\nated SoCal”, it still answers ‘Southern California’,\nwhich indicates that the model cannot retrieve re-\nlated memory for answering questions.\nWe propose a simple way to strength knowledge\nretrieval, namely QA-bridge-tune, which is a ex-\ntended QA-ﬁnetuning process. The process is il-\nlustrated in Figure 4, for each question input, the\noutput concatenates the related passage with the\nanswer. Thus, the model can explicitly recall the\nmemorized passages when answering questions, by\nwhich QA-bridge-tune builds a bridge between QA\nand memorized knowledge so that the model can\nA > B A = B A < B\nRelevance 30.2% 53.3% 16.6%\nTable 10: Human-evaluated relevance between the re-\nsults using and not using QA-bridge-tune with correct\nanswers. A > B means that A’s outputs are more re-\nlated to correct answers than B’s, etc. A = QA-bridge-\ntune, B = QA-ﬁnetune in this Table.\nModels \\ Dataset 16/2/2 128/16/16 442/53/52\nBART → LM-ﬁnetuning 87.3% 72.6% 66.3%\nBART → LM-ﬁnetuning\n(Added Preﬁx/Sufﬁx) 85.5% 79.6% 59.5%\nBART → LM-ﬁnetuning\n→ QA-ﬁnetuning 2.8% 10.9% 2.4%\nBART → LM-ﬁnetuning\n→ QA-ﬁnetuning\n(Added Preﬁx/Sufﬁx)\n5.7% 51.4% 16.2%\nTable 11: Performance of reciting after QA. The num-\nbers in the header is the passage amount of this subset.\n‘BART’ denotes the ‘BART-Large’ checkpoint.\nanswer questions with learned knowledge. In addi-\ntion, this method can help improve interpretability.\nThe results are shown in Table 8. We can see\nthat QA-bridge-tune can help the model wake up\nthe related memorize knowledge when QA, thus\nimproving EM accuracy and by two or three times\non baselines. In addition to answer correctness,\nwe also consider the relevance between model out-\nputs and golden answers regardless whether the\nanswer is correct. For example, the question is\n“The Amazon rainforest makes up what amount of\nEarth’s rainforests?” and the golden answer is\n“over half”, and two generated answers are “60%”\nand “the Amazon rainforest”. They are both incor-\n3248\nrect but the former is more relevant and therefore\na better answer. We ask human experts to man-\nually compare the results between using and not\nusing QA-bridge-tuning, selecting results by us-\ning ‘BART→LM-ﬁnetuning→QA-ﬁnetuning’ and\n‘oBJ→LM-ﬁnetuning→QA-bridge-tuning’ strate-\ngies on the ‘128/16/16’ subset. The results are\nshown as Table 10. According to human experts,\nin 30.2% cases, the outputs of QA-bridge-tuning\nare more relevant to the golden answer than those\nof QA-ﬁnetuning while only in 16.6% cases, QA-\nﬁnetuning is more relevant. This result shows\nthat QA-bridge-tuning can help BART ﬁnd more\nrelevant knowledge. We also list several exam-\nples showing in Figure 9. As the example in\nthe ﬁrst paragraph of this subsection, for ques-\ntion “What is Southern California often abbrevi-\nated as?” , BART can output the corresponding\npassage along with the correct answer “SoCal” af-\nter QA-bridge-tuning. These results suggests that\nQA-bridge-tuning can effectively help the model\nrecall the remembered knowledge.\n5.3 Inﬂuence of QA on Memory\nTo explore whether QA-ﬁnetune interferes with\nthe memory of LM-ﬁnetuned models, we use QA-\nﬁnetuned models for the reciting task. The results\nare given in Table 11. After QA-ﬁnetuning, the\nmodels’ reciting accuracy declines. We have two\npossible explanations for this phenomenon. First,\nQA-ﬁnetune process disrupts the models’ internal\nmemory with regard to representation; Second,\nthe tasks are different, so model output space is\ndisturbed, but the model still retains knowledge.\nThough we cannot qualitatively understand the in-\nﬂuence of each reason above, isolating the QA func-\ntionality from pre-trained denoising auto-encoding\ncan potentially address interference issues.\nWe experiment with a simple intuitive solution to\nthis issue, namely to decouple the QA-ﬁnetune pro-\ncess and the LM-ﬁnetune process, so that the two\ntask input/output spaces are differentiated to some\nextent. This is done simply in the input and out-\nput level. We add <PASSAGE>/<QUESTION>\npreﬁx tokens and </PASSAGE>/</QUESTION>\nsufﬁx tokens to each input passage/question when\nLM-ﬁnetuning and Reciting/QA-ﬁnetuning, respec-\ntively, and also add</PASSAGE>/</ANSWER>\nsufﬁx tokens to each output passage/answer.\nThe results are shown in the rows with (Added\nPreﬁx/Sufﬁx) in Table 11. The reciting accuracy\nModels \\ Dataset 16/2/2 128/16/16 442/53/52\noriginal GPT-2\n→ LM-ﬁnetuning\n→ QA-ﬁnetuning\n0% 1.1% 1.0%\nTable 12: Performance of GPT2 in the same setting as\nthe second row of 8. The numbers in the header is the\npassage amount of this subset. The score is evaluated\nwith Exact Match (EM).\nwith preﬁx/sufﬁx after LM-ﬁnetuning is not much\ndifferent compared without preﬁx/sufﬁx. How-\never, the QA accuracy signiﬁcantly improves when\nadding preﬁx/sufﬁx (2.8% to 5.7%, 10.9% to\n51.4%, 2.4% to 16.2% in HE). The results show\nthat our decoupled methods can help the model\ndistinguish the input type to ﬁnd the appropriate\nsemantic space, thus alleviating this problem. Be-\nsides, according to the comparison between the\nsecond row and the third row in Table 8, adding\npreﬁx/sufﬁx can help models better answer ques-\ntions. We suppose it is also because this method\ncan help models distinguish the input/output space.\n5.4 GPT-3\nGPT3 has also been shown to have certain capabil-\nities to answer factual closed-book questions. As\nshown in Table 3.3 of Brown et al. (2020), it can\nachieve relatively high performance on TriviaQA\nin closed-book task even in zero-shot learning set-\nting. However, it underperforms T5 (Roberts et al.,\n2020) in the other two datasets WebQuestions and\nNaturalQuestions, which indicates that super large\nscale pre-training is not the ultimate solution to the\nissue we discussed. There is also a possibility that\nGPT-3 has seen most test QA pairs of TriviaQA in\nthe pre-training stage as it crawls extremely large\ndocuments from the internet.\nWe also apply GPT-2 to LM-ﬁnetuning and QA-\nﬁnetuning, which has similar architecture, pre-\ntraining and ﬁnetune process with GPT-3. Thus we\nbelieve that they can have the same fundamental\nproblem. The results are shown in Table 12. LM-\nﬁnetuned GPT-2 has worse performance compared\nto LM-ﬁnetuned BART. This conﬁrms that the ar-\nchitecture and the training process of GPT3/GPT-2\ndo not solve the problems we ﬁnd using BART.\n6 Related Work\nThere are two types of pre-trained language mod-\nels (PLMs), discriminative PLMs such as BERT\n(Devlin et al., 2019), ELMo (Peters et al., 2018)\nand generative PLMs such as GPT (Radford et al.,\n3249\n2018), BART (Lewis et al., 2020a). The key dif-\nference is that generative PLMs are of encoder-\ndecoder architectures so they can generate text\nsequences of any length or token. An increas-\ning number of works have shown that PLMs con-\ntains world knowledge. Petroni et al. (2019) ﬁrst\nsolves that discriminative PLMs such as BERT\n(Devlin et al., 2019) can be used for Cloze-style\nQA using a mask language modeling task with-\nout external resources, such as “Dante was born\nin [MASK]. ”→ “Florence”. Their results show\nthat PLMs have certain factual knowledge. Tal-\nmor et al. (2020) set eight types of Cloze-style\nQA, such as ‘ALW AYS-NEVER’ and ‘AGE COM-\nPARISON’, to test different types of knowledge\nin several discriminative PLMs, including BERT\nand RoBERTa (Liu et al., 2019). They also use\nthe mask language modeling task to do QA with-\nout ﬁnetuning, and results show that the evaluated\nPLMs indeed contain those kinds of knowledge.\nWang et al. (2019); Zhou et al. (2020) adopt some\ndiscriminative PLMs on commonsense reasoning\nQA tasks such as ComVE (Wang et al., 2020) and\nSwag (Zellers et al., 2018) without ﬁnetuning, in-\ndicating the PLMs have commonsense knowledge.\nBosselut et al. (2019) show that pretrained trans-\nformer models can be used to help construct com-\nmonsense knowledge graphs, such as ConceptNet\n(Speer and Havasi, 2012). However, Poerner et al.\n(2019) argue that BERT uses some superﬁcial cues\nsuch as stereotypical characters to solve factual\nquestions. GPT-3 (Brown et al., 2020) seems to\nhave ability to answer factual questions in zero-\nshot setting, but there exists some evidence that\nGPT-3 is limited in storing and using knowledge\n(Bergdahl, 2020).\nRoberts et al. (2020) ﬁrstly use closed-book QA\nto detect how much knowledge is in pre-trained\nlanguage models’ parameters. They perform ex-\nperiments on three datasets WebQuestions (Berant\net al., 2013), TriviaQA (Joshi et al., 2017) and Nat-\nuralQuestions (Kwiatkowski et al., 2019) by T5\nmodel (Raffel et al., 2020). The results are rela-\ntively pleasant. However, Lewis et al. (2020b) ﬁnd\nthat the high performance of Roberts et al. (2020) is\nmainly due to the high test-train overlap of the three\ndatasets rather than the model’s internal knowledge.\nOur ﬁndings conﬁrm the conclusions of Lewis et al.\n(2020b), and we further experiment with a more\ncontrolled SQuAD dataset, and discussed the weak-\nness of BART in both memorization and knowledge\nretrieval. Because T5 (Raffel et al., 2020) is more\nresource demanding, considering the balance of ef-\nfectiveness and experimental feasibility, we choose\nBART rather than the T5 model.\nDifferent from closed-book QA, where no addi-\ntional resource is available when answering ques-\ntions, open-domain QA requires models to generate\na sequence of tokens as the answer to each ques-\ntion by looking up related text from unstructured\ndocuments (Chen et al., 2017). Chen et al. (2017)\nﬁrst try to retrieve related passages from Wikipedia\nfor each question and encode both the question and\npassages into the model, then output the answer.\nGuu et al. (2020) integrate the retrieval process\ninto pre-training process, helping the PLMs bet-\nter retrieve information from external knowledge\nsource when needed, and ﬁnding beneﬁts on open-\ndomain QA task. Retriever-based models have the\nadvantage of relieving the burden of pre-trained\nlanguage models to remember every factual detail.\nThe retrieval QA setting is slightly reminiscent to\nour data augmentation setting in Figure 4, but with\nthe related passage being the input, rather than the\noutput. In contrast, the settings we consider fully\nrely on a neural model for all knowledge.\nSQuAD (Rajpurkar et al., 2016, 2018) is a\nwidely-used dataset for machine reading compre-\nhension, which is also a type of QA task. It asks\nmodels to use a text span from a given referential\npassage to answer questions. It is also used in other\ntype of QA task, for example, Chen et al. (2017)\nadopt it in the open-domain QA task. We ﬁrst ap-\nply it on closed-book QA and analyze why it is\nsuperior than other three commonly used datasets.\n7 Conclusion\nWe investigated by using SQuAD, ﬁnding that\nclosed-book QA is still challenging for generative\npre-trained language models such as BART. The\nchallenge lies both in remembering the knowledge\ndetails and in answering the questions after remem-\nbering the knowledge. Potential solutions include\nexplicitly asking models to recall relevant knowl-\nedge when answering questions and decoupling\nLM-ﬁnetuning process and QA-ﬁnetuning process.\n8 *Acknowledgement\nThe work was supported by NSFC 61976180. We\nthank Yongjing Yin, Chuang Fan, Yuchen Niu, Sara\nGong, Tony Ou, Libo Qin and all reviewers for their\ngenerous help and advice during this research.\n3250\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.\nJacob Bergdahl. 2020. No, gpt-3 is not superintelligent.\nit’s not tricking humans, and it‘s not pretending to be\nstupid. Medium Website.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, A. C ¸ elikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for au-\ntomatic knowledge graph construction. In ACL.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computa-\ntional Linguistics (ACL).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Z. Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv,\nabs/2002.08909.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601–1611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2020b. Question and answer test-train overlap in\nopen-domain question answering datasets.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. of NAACL.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.\n2019. Bert is not a knowledge base (yet): Fac-\ntual knowledge vs. name-based reasoning in unsu-\npervised qa. ArXiv, abs/1911.03681.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers) , pages 784–789.\nAssociation for Computational Linguistics.\n3251\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392. Asso-\nciation for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nRobyn Speer and Catherine Havasi. 2012. Repre-\nsenting general relational knowledge in Concept-\nNet 5. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation\n(LREC’12), pages 3679–3686, Istanbul, Turkey. Eu-\nropean Language Resources Association (ELRA).\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nCunxiang Wang, Shuailong Liang, Y . Jin, Yilong Wang,\nX. Zhu, and Y . Zhang. 2020. Semeval-2020 task 4:\nCommonsense validation and explanation. In SE-\nMEVAL.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiao-\nnan Li, and Tian Gao. 2019. Does it make sense?\nand why? a pilot study for sense making and ex-\nplanation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4020–4026, Florence, Italy. Association for\nComputational Linguistics.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 9733–9740.\nA *Ethics / Impact Statement\nOur used data is from open source datasets, includ-\ning NaturalQuestions3, TriviaQA4, WebQuestion5\nand SQuAD26. We split the development set of the\nNaturalQuestions, TriviaQA and SQuAD2 and the\ntest set of WebQuestions into two subsets to serve\nas a new development set and a new test set. And\nwe extract several subsets from SQuAD2 to serve\nas our new datasets. There is no additional data\ncollection process.\n3https://ai.google.com/research/\nNaturalQuestions\n4http://nlp.cs.washington.edu/\ntriviaqa/\n5https://nlp.stanford.edu/software/\nsempre/\n6https://rajpurkar.github.io/\nSQuAD-explorer/",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7248557806015015
    },
    {
      "name": "Computer science",
      "score": 0.7036552429199219
    },
    {
      "name": "Natural language processing",
      "score": 0.5757487416267395
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5284150838851929
    },
    {
      "name": "Artificial intelligence",
      "score": 0.527472198009491
    },
    {
      "name": "Computational linguistics",
      "score": 0.5198160409927368
    },
    {
      "name": "Zhàng",
      "score": 0.5109833478927612
    },
    {
      "name": "Linguistics",
      "score": 0.49483317136764526
    },
    {
      "name": "Joint (building)",
      "score": 0.46031495928764343
    },
    {
      "name": "Natural language",
      "score": 0.4279751777648926
    },
    {
      "name": "Library science",
      "score": 0.33234256505966187
    },
    {
      "name": "History",
      "score": 0.15140530467033386
    },
    {
      "name": "Engineering",
      "score": 0.14694160223007202
    },
    {
      "name": "Philosophy",
      "score": 0.09917747974395752
    },
    {
      "name": "China",
      "score": 0.07398352026939392
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3133055985",
      "name": "Westlake University",
      "country": "CN"
    }
  ]
}