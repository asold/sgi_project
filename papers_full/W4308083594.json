{
  "title": "Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation",
  "url": "https://openalex.org/W4308083594",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Fazry, Lhuqita",
      "affiliations": [
        "University of Indonesia"
      ]
    },
    {
      "id": "https://openalex.org/A3180739587",
      "name": "Haryono Asep",
      "affiliations": [
        "University of Indonesia"
      ]
    },
    {
      "id": null,
      "name": "Nissa, Nuzulul Khairu",
      "affiliations": [
        "University of Indonesia"
      ]
    },
    {
      "id": "https://openalex.org/A209309480",
      "name": "Sunarno _",
      "affiliations": [
        "University of Indonesia"
      ]
    },
    {
      "id": null,
      "name": "Hirzi, Naufal Muhammad",
      "affiliations": [
        "University of Indonesia"
      ]
    },
    {
      "id": "https://openalex.org/A3208387185",
      "name": "Rachmadi, Muhammad Febrian",
      "affiliations": [
        "University of Indonesia"
      ]
    },
    {
      "id": "https://openalex.org/A2742781569",
      "name": "Jatmiko, Wisnu",
      "affiliations": [
        "University of Indonesia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2268919114",
    "https://openalex.org/W3204334240",
    "https://openalex.org/W2036639507",
    "https://openalex.org/W3013692475",
    "https://openalex.org/W2936316583",
    "https://openalex.org/W2109289860",
    "https://openalex.org/W2124624517",
    "https://openalex.org/W3025793767",
    "https://openalex.org/W3207662977",
    "https://openalex.org/W3002705197",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2990987289",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6794906783",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6797533734",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4312560592"
  ],
  "abstract": "The left ventricular of ejection fraction is one of the most important metric of cardiac function. It is used by cardiologist to identify patients who are eligible for lifeprolonging therapies. However, the assessment of ejection fraction suffers from inter-observer variability. To overcome this challenge, we propose a deep learning approach, based on hierarchical vision Transformers, to estimate the ejection fraction from echocardiogram videos. The proposed method can estimate ejection fraction without the need for left ventrice segmentation first, make it more efficient than other methods. We evaluated our method on EchoNet-Dynamic dataset resulting 5.59, 7.59 and 0.59 for MAE, RMSE and R2 respectivelly. This results are better compared to the state-of-the-art method, Ultrasound Video Transformer (UVT). The source code is available on https://github.com/lhfazry/UltraSwin.",
  "full_text": "Hierarchical Vision Transformers for Cardiac\nEjection Fraction Estimation\nLhuqita Fazry*, Asep Haryono *, Nuzulul Khairu Nissa *, Sunarno*,\nNaufal Muhammad Hirzi *, Muhammad Febrian Rachmadi * +, Wisnu Jatmiko *\n*Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia\n+RIKEN Research, Tokyo, Japan\nlhuqita.fazry@ui.ac.id\nAbstract—The left ventricular of ejection fraction is one\nof the most important metric of cardiac function. It is used\nby cardiologist to identify patients who are eligible for life-\nprolonging therapies. However, the assessment of ejection frac-\ntion suffers from inter-observer variability. To overcome this\nchallenge, we propose a deep learning approach, based on\nhierarchical vision Transformers, to estimate the ejection fraction\nfrom echocardiogram videos. The proposed method can estimate\nejection fraction without the need for left ventrice segmentation\nﬁrst, make it more efﬁcient than other methods. We evaluated\nour method on EchoNet-Dynamic dataset resulting 5.59, 7.59\nand 0.59 for MAE, RMSE and R 2 respectivelly. This results\nare better compared to the state-of-the-art method, Ultrasound\nVideo Transformer (UVT). The source code is available on\nhttps://github.com/lhfazry/UltraSwin.\nIndex Terms—Echocardiography, Cardiac Ejection Fraction,\nUltraSwin, Vision Transformers, EchoNet-Dynamic\nI. I NTRODUCTION\nThe cardiovascular system is the human circulatory system\nconsists of various important organs which have the main\nfunction to circulate oxygen, nutrients, and hormones to all\ncells and tissues of the body [1]. One of the vital organs in the\ncirculatory system is the cardiac which pump blood throughout\nthe body and receive blood ﬂow back. Based on data from the\nWorld Health Organization (WHO), cardiovascular disease is\nstill a deadly disease worldwide. Every year the death rate\nfrom this disease increases and in 2019 around 17.9 million\npeople died or 32% of the world’s mortality rate [2]. Therefore,\na fast and accurate method is needed for cardiac diagnoses so\nit can be handled quickly and properly.\nA common method to diagnose cardiac disease is the\nassesment through echocardiograph video. It is an imaging\ntechnique to assess the cardiac function and structure [3]. The\ninformation that is taken from echocardiograph video can be\nused as the basis for initial screening to diagnoses the cardiac\ndisease. It can also helps for deciding further treatments.\nOne of the most important metric that can be used to deter-\nmine the cardiac function is Left Ventricular Ejection Fraction\n(LVEF) or Ejection Fraction (EF) for short [4]. EF measures\nhow much blood volume that are ejected out of cardiac within\none heart-beat. To calculate EF from echocardiograph video,\na cardiologist need to tracing the left ventricular to estimate\nEnd Systolic V olume (ESV) and End Diastolic V olume (EDV).\nESV is the volume of left ventricular after the ejection process.\nOn the other hand, EDV is the volume of left ventricular before\nthe ejection process. Having the value of ESV and EDV in\nhand, EF is then calculated using the following formula:\nEF = EDV −ESV\nEDV ×100% (1)\nEF can be used to classify the cardiac condition using\ncommon threshold. EF value which is less than 50% can be\nconsidered as cardiomyopathies [5]. Cardiomyopathies are a\nheterogeneous group of heart muscle diseases and an important\ncause of heart failure (HF) [6]. Cardiac with EF less than 50%\nis an indication of heart failure. Heart failure with preserved\nejection fraction (HFpEF) has been deﬁned as having signs\nand symptoms of heart failure with preserved EF and diastolic\nabnormalities [7].\nHowever, manually tracing the left ventricular and calcu-\nlate the EF is very complicated task. It suffer from inter-\nobserver variability. The EF can varies from one heart-beat\nto another. Furthermore, the American Society of Echocardio-\ngraphy (ASE) and the European Association of Cardiovascular\nImaging (EACVI) recommend to observe up to 5 consecutive\nheart-beats, thus making the approach more complicated [8].\nSo the method that can estimate EF faster is needed.\nWith the advance of deep learning, some methods are\ndeveloped to overcome this problem. Jahren et. al. use the\ncombination of Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN) to predict the location of\nend-diastole from electrocardiogram data (ECG) [9]. Ouyang\net. al. use the combination of 3D convolution and atrous\nconvolution to estimate EF [5]. It’s clear from formula 1, ESV\nand EDV are needed to calculate EF. The above methods ﬁrst\nsegment the left ventricular in an echocardiograph video. From\nthe segmentation, they try to detect ESV and EDV and then\nestimate the volume.\nRecently, Reynaud et al. proposed UltraSound Video Trans-\nformers (UVT) [3] to estimate EF from echocardiograph video.\nUVT uses the Transformers, a popular model in Natural\nLanguage Processing (NLP), as a features extraction. Before\nprocessed by the Transformers, the input video is splitted\nframe by frame. Each frame is then encoded by ResNet Auto\nEncoder (ResNetAE) to reduce the dimension into 1, 024 token\nlength. This low dimensional features are then learned by the\nTransformers to produce another feature maps. The feature\narXiv:2304.00177v1  [cs.CV]  31 Mar 2023\nmaps are then processed by head regressor to produce EF\nestimation.\nHowever, the process of learning from low dimensional\nfeatures are not optimal, because most important feature may\nbe loss during the encoding process. In this paper, we propose\na novel method to predict EF by directly process the input\nvideo using hierarchical vision Transformers. Our method also\ncan directly estimate the EF without the need to segment ESV\nand EDV and calculate their respective volume.\nThe focus of this research is to estimate the value of the EF\nthat can be used to diagnose cardiomyopathy (the abnormal-\nities in the heart muscle that can cause heart failure), assess\neligibility for certain chemotherapies, and determine indication\nfor medical devices [10]. The output of the regression task that\nutilize deep learning model in this study is the EF value. The\ncommon threshold value on the EF which is less than 50%\ncan be used to classify cardiomyopathy, this threshold will be\nused as a reference to determine the heart condition [5].\nII. R ELATED WORK\nVideo data is any sequence of time-varying images. In the\nvideo data, the picture information is digitized both spatially\nand temporally. Nowadays, research on video data processing\nis also an emerging ﬁeld of computer vision (CV) [11].\nFurthermore, video processing techniques have begun to be\nused in research in the ﬁeld of medical imaging. One of them is\nGhorbani et al. that uses the CNN method with the architecture\nbased on Inception-Resnet-v1 [12]. Inception-Resnet-v1 has\ngood performance on the Imagenet benchmark dataset and\ncomputationally efﬁcient compared to other architectures [13].\nThis research proved that the use of the deep learning method\napplied to echocardiography was able to identify the cardiac\nlocal anatomy and structure, to estimate metrics for measuring\ncardiac function and to predict the characteristics of the\npatients such as gender, height, and weight that not easily\nobserved by human [12].\nOther research from [14] comparing the use of four CNN\narchitectures which aims to classify 14 classes of echocar-\ndiographic views consisting of single frame classiﬁcation (2D\nCNN), multi-frame classiﬁcation (TD CNN), spatio-temporal\nconvolution (3D CNN) and two stream classiﬁcations. The\nbest-performing model was a ”two-stream” network using both\nspatial and optical ﬂow inputs, with a corresponding error rate\n3.9%.\nOuyang et al. used the model of spatio-temporal convo-\nlutions with residual connections and generates frame-level\nto predict the EF for each cardiac cycle and then generates\nframe-level semantic segmentations of the left ventricle using\nweak supervision from expert human tracings. These outputs\nare combined to create beat-to-beat predictions of the EF\nand to predict the presence of heart failure. This study uses\nechocardiography video dataset from EchoNet-Dynamic [10].\nIn other research [3] which can also perform the task\nof predicting EF values by utilizing the Transformers based\narchitecture that capable to process the videos of arbitrary\nduration. The method uses a Transformers architecture based\non the Residual Auto-Encoder (ResAE) Network and a BERT\nmodel adapted for token classiﬁcation.\nBased on Reynaud et al [3], it can be concluded that\nthe backbone architecture modeling in computer vision (CV)\nhas begun to shift to the use of the Transformers architec-\nture. The trend started with the introduction of ViT (Vision\nTransformers), which globally models non-overlapping spatial\nrelationships in image patches using the standard Transformers\nencoder [15]. For this research, we use Video Swin Trans-\nformers [16], which completely follows the original Swin\nTransformers hierarchical structure [17]. However, we extend\nthe local attention computation scope from the spatial domain\nto the spatio-temporal domain. The adaptation process was\ncarried out in the 3D patch partition section and replaced\nthe local window self-attention module into a 3D shifted\nwindow based on multi-head self-attention (MSA) and shifted\nwindow multi-head self-attention (SW-MSA) in the Trans-\nformers Block section. Video Swin Transformers can do the\nvideo-recognition tasks that contains an inductive bias towards\nspatio-temporal locality.\nIII. M ETHOD\nIn this paper, we propose a novel method to estimate EF\nfrom a cardiac ultrasound video. Our method uses a deep\nlearning model, based on hierarchical vision Transformers. We\nnamed the model as UltraSwin. UltraSwin adopts Transform-\ners [15], a popular deep learning model in Natural Language\nProcessing (NLP) and its derivative work on Computer Vision\n(CV) [18, 17].\nA. Model Architecture\nThe architecture of UltraSwin is described in ﬁgure 1. The\nmodel is received a video as an input, speciﬁcally ultrasound\nvideo containing short time cardiac recording. The output\nof the model is the estimation of EF for the cardiac in the\nultrasound video.\nUltraSwin has 2 main modules, Transformers Encoder (TE)\nand EF Regressor. The TE module acts as feature extractor\nwhile EF Regressor as regressor head. The TE module is used\nto learn representation from input video and then output the\nfeature maps. They are then processed by EF Regressor and\ntransformed into scalar value. This value is then used as an\nEF estimation for the input ultrasound video.\nInstead of treat a frame of the video as input token like\nin UVT [3], UltraSwin uses 3D video patches as input token\nfollowing work of Liu et. al. [16].\nB. Pre-processing\nIn this research, we use ultrasound video from EchoNet-\nDynamic dataset [10]. This dataset contains echocardiography\nvideos with variety of frame length and contains at least one\nheart-beat. Although, it can have more than one heart-beat per\nvideo, ES (End Systolic) and ED (End Dyastolic) ground-truth\nare only for one heart-beat.\nEach frame in the video have spatial dimension of 112×112\npixels. The frame’s width and height must satisfy 2n, so it\nFig. 1. Overall UltraSwin architecture. UltraSwin processes cardiac ultrasound video then output an estimation of ejection fraction for the video. UltraSwin\nconsists of two main modules: Transformers Encoder (TE) and EF Regressor. TE modules acts as a features extractor and EF Regressor as regressor head.\nFig. 2. The illustration of 3D tokens and shifted windows mechanism. At ﬁrst, each frame is splitted into patches. The patches are then grouped into windows.\nIn the two consecutive attention layers, the windows conﬁguration are then shifted. In this way, the attention can happen across windows while keeping the\ncomputation cheap, because the attention are only calculated within window (not calculated globally).\ncan be processed into patches. Each input for Transformers\nmust have same length, so we cut the video into ﬁxed length\nof 128 frames. We choose 128, because it is the closest 2n\nvalue from 112. We select ES and ED frames and any frames\nbetween them then cut them. The Echonet Dynamic dataset\nthat we used in this research, contains varied length (total\nframes), frame rate and image quality [10]. Therefore, if the\ntotal frames in the cut out video are more than 128, we\nsubsample the frames between ES and ED. Otherwise, we\nrepeat or mirroring the frames between ES and ED and place\nthem after ED to get the total 128 frame length. Suppose\nthe sequence of frames F = [mES , mb1 , ···, mbn, mED], we\nrepeat the frames between ED and ES to create new sequences\nˆF = [mES , mb1 , ···, mbn, mED, mb1 , ···]. We choose this\ntechnique based on the research by Reynaud et al., where\nthe mirroring technique gives better results than the random\nsampling technique in terms of ﬁtting the number of total\nframes to 128 [3]. After that, we pad the frames with blank\npixels, so its dimension becomes 128 ×128 pixels.\nWe also tried to augment the video with standard augmen-\ntations like horizontal ﬂip, vertical ﬂip, random rotation and\nothers. Suprisingly, we found that augmentations lead to worse\nperformance. This result indicates that ultrasound video dataset\nare sensitive to augmentation operations.\nC. Transformers Encoder\nThis module contains 4 stages. Unlike Vision Transformers\n(ViT) [18] that has ﬁxed patch size along the stages, UltraSwin\nuse hierarchical architecture following Swin Transformers\n[17] in the spatial spaces. At every stage, the patch size is\ndownsampled into half of the patch size in the previous stages.\nTo make the model learn temporal information, UltraSwin\nfollows Video Swin Transformers [16] to process the video\ninput in the shape the 3D patches.\nTE module contains two main components.\n1) 3D Patch Partition: Suppose an input video has di-\nmension of T ×H ×W ×3, where T, H, Wand 3 represent\nnumber of frames, frame’s height, frame’s width and number\nof channels respectively. The video input is then partitioned\ninto 3D patch with dimension 2×4×4×3. In the Transformers\nworld, this 3D patch is called token. Each token contains\nembedding features with length 96. Actually, we can use any\nnumber other than 96, but greater number can signiﬁcantly\naffects the computation cost. We use 96 following Video Swin\nTransformers [19] as it gives a good performance. This process\nyields T\n2 ×H\n4 ×W\n4 tokens in total. The tokens are then ﬂattened\ninto sequences before processed by the Transformers. Figure\n2 illustrates the 3D tokens.\nThe features of each token are then transformed by a linear\nlayer into an arbritrary C dimensions. So, the dimension of\nthe tokens is now T\n2 ×H\n4 ×W\n4 ×C. This number is hyper-\nparameter and we can use arbitrary number for C.\n2) Block Swin Transformers: Transformers [15] and Vision\nTransformers (ViT) [18] use global self-attention (SA) and\ncompute softmax score between each tokens, thus making the\ncomputation and memory resources grow quadratically with\ntoken length. This approach is efﬁcient enough for single\ninput image. On the other side, the video have multiple image\nframes, so the approach are not suitable for video related\ntasks like video classiﬁcation, video segmentation and others.\nUltraSwin use local window self-attention following Swin\nTransformers [17] that is proven more efﬁcient in video related\ntask than global self-attention [19].\nWhile efﬁcient, local window self-attention lacks of connec-\ntion accross window. This can cause performance degradation\non the model. To solve this issue, UltraSwin shifts the window\npartition in two consecutive Swin Transformers Block as\nillustrated in ﬁgure 2. As Transformers can have multiple\nlayers of blocks, UltraSwin shifted the window conﬁguration\nin every two consecutive blocks. This design is proven to be\neffective in image recognition task [17]. The main reason why\nit is effective is because it enables the connections between\nnon-overlapping windows with their neighbours.\nSuppose a sequence of 3D tokens with size T\n′\n×H\n′\n×W\n′\n×3.\nIn the ﬁrst layer, these tokens are then arranged into regular\nnon-overlapping window of size P ×M ×M, thus resulting\n⌈T\n′\nP ⌉×⌈ H\n′\nM ⌉×⌈ W\n′\nM ⌉non-overlapping 3D windows in total.\nIn the second layer, conﬁguration of every window is shifted\nwithin width, height and temporal axes by (P\n2 ×M\n2 ×M\n2 ).\nThe self-attention mechanism is applied multiple times in\nparallel. This is called heads. In multi-head scenario, the\noutput from each self-attention are concatenated. In ﬁrst\nlayer, we called multi-head self-attention (MSA) and shifted\nwindow multi-head self-attention (SW-MSA) for second layer.\nFormally, we stated MSA as [SA1, SA2, ···, SAn] and SW-\nMSA as [SW-SA1, SW-SA2, ···, SW-SAn], where SA i and\nSW-SAi refer to self-attention in layer- i and shifted-window\nself-attention in layer- i respectivelly. The SA itself can be\nformulated as follow:\nSA(Q, K, V) =SoftMax(QKT\n√\nd\n+ B)V (2)\nwhere K, V, Q∈ RPM 2×d are matrices for key, value and\nquery respectively, while d is query and key dimension, PM 2\nis the number of tokens in 3D window, and B ∈RP2×M2×M2\nis matrix of relative position bias.\nThe self-attention blocks are then followed by feed forward\nnetworks, which is 2 layers MLP with GELU [20] non-\nlinearity in between. Layer Normalization (LN) [21] is applied\nTABLE I\nULTRA SWIN VARIANTS\nParameter UltraSwin-base UltraSwin-small\nembedding dimension 128 96\nnumber of head 4, 8, 16, 32 1 3, 6, 12, 24 1\nlayer depth 2, 2, 18, 2 1 2, 2, 18, 2 1\nTotal parameter 88.2M 49.7M\n1 These values are for stage 1, 2, 3 and 4 respectivelly\nbefore self-attention module and before the MLP. Residual\nconnection [22] is then applied after self-attention and after\nMLP. Two consecutives of Swin Transformers blocks in layer-l\nand layer-l + 1can be formulated as follow:\nˆzl = MSA(LN(zl−1)) +zl−1\nzl = MLP(GELU(LN(ˆzl))) + ˆzl\nˆzl+1 = SW-MSA(LN(zl)) +zl\nzl+1 = MLP(GELU(LN(ˆzl+1))) + ˆzl+1 (3)\nD. EF Regressor\nThe EF regressor take the output of the TE module as input.\nThe input is a features map with dimension T\n2 ×H\n32 ×W\n32 ×8C.\nThe temporal axes are then reduced from the map, resulting\nnew dimension H\n32 ×W\n32 ×8C. A linier layer is then applied to\nreduce the last axes of feature maps from 8C into 4C. A Layer\nNormalization (LN) is then applied followed by linier layer to\nreduce the feature axes into 1 dimension. Spatial reduction is\nthen applied to the map resulting 1 ×1 dimesion scalar. This\nscalar value is then used as EF estimation.\nE. Model Variants\nWe propose two variants of UltraSwin: UltraSwin-base and\nUltraSwin-small. Table I summarizes the two variants. Number\nof head and layer depth values in table I refer to conﬁgura-\ntions on stage 1, 2, 3 and 4 respectivelly. The total parameter\nfor UltraSwin-small is almost half from total parameter for\nUltraSwin-base.\nF . Loss Function\nBoth variants are trained to minimize MSE (Mean Squared\nError). We use MSE because it is commonly used in regression\ntask and gives the best performances. MSE is deﬁned as\nfollow:\nL(y, ˆy) = 1\nN\nN∑\ni=1\n(yi −ˆyi)2 (4)\nwhere yi and ˆyi refer to EF ground-truth and EF prediction\nfrom the model respectivelly.\nIV. E XPERIMENTS\nA. Dataset\nThe dataset used in this research is EchoNet-Dynamic [10]\nwhich is an open dataset and sourced from the Stanford Arti-\nﬁcial Intelligence in Medicine and Imaging (AIMI) Center ob-\ntained from https://stanfordaimi.azurewebsites.net. The dataset\ncontains videos of heart movement and chamber volume from\nechocardiography or cardiac ultrasound. The total video is\n10, 030 in .avi format consists of 7, 465 training data, 1, 288\nvalidation data and 1, 277 test data.\nEach video has varies duration with number of frames\nranging from 28 to 1002. The spatial dimension for each frame\nis 112×112 pixels. Each video has frame per second (FPS) 50.\nThe videos come with EF ground truth which value ranging\nfrom 6.9 to 96.96.\nB. Implementation Details\nThe model architecture was created using the Python\n3.8 programming language and the PyTorch 1.11 frame-\nwork. The Pytorch Lightening 1.6.4 library was\nused to simplify the training process. We also use Tensorboard\nlibrary to records the evaluation metrics. The model was\ntrained using a 1 core NVidia Tesla T4 GPU. To save the\nmemory usage, 16 bit precision is used for gradient calcula-\ntions during training and batch_accumulation = 2 to\nspeed up the training process.\nIn the UltraSwin-base model, the batch_size parameter\nused is 2, while in the UltraSwin-small is 4. To speed up\nthe model convergencies during training, we initialize the TE\nmodule weights using the pre-trained Swin Transformer model\nthat had been trained using the ImageNet 22k dataset [23].\nC. Training Details\nThe UltraSwin models were trained without freezing the TE\nmodule to avoid the problem of different domains in transfer\nlearning. For the UltraSwin-base, the TE module weights\nare initialized using pretrained swin_base_patch4_\nwindow7_224_22k, while for the UltaSwin-small using the\npretrained swin_small_patch4_window7_224_22k.\nBoth pretrained models can be downloaded at the\nhttps://github.com/microsoft/Swin-Transformer page.\nDuring the training process, AdamW [24] optimization was\nused with an initial learning rate of 10−4 and weight decay of\n10−4. Both models were trained for 20 epochs. At each epoch,\nthe learning rate was reduced by 0.15 from the learning rate\nin the previous epoch.\nOn the UltraSwin-base model, the training process takes\napproximately 30 minutes for one epoch, while on the\nUltraSwin-small it takes approximately 15 minutes for one\nepoch. When making predictions using the trained model, the\nsame conﬁguration is used as the conﬁguration in the training.\nHowever, because the inference process only performs forward\npropagation without the need to calculate the gradient (back\npropagation), the batch_size parameter can be increased\nto 8.\nV. R ESULT AND DISCUSSION\nHere we show the result of the experiments for UltraSwin-\nbase and UltraSwin-small. We then compared the results of\nour two variations models with the state-of-the-art method,\nUltrasound Video Transformers (UVT) [3].\nTable II summarizes the result of our experiments and we\ncompare it with the results of UVT model from Reynaud et\nTABLE II\nRESULT COMPARISON OF ULTRA SWIN AND UVT\n.\nModel Total Parameter MAE RMSE R2\nUVT 346.8M 5.95 8.38 0.52\nUltraSwin-small 49.7M 5.72 7.63 0.58\nUltraSwin-base 88.2M 5.59 7.59 0.59\nal [3]. We use three metrics to evaluate the models, MAE\n(Mean Absolute Error), RMSE (Root Mean Squared Error)\nand R2 (Coefﬁcient of Determination). Smaller value of MAE\nand RMSE means better performance. However, higher value\nof R2 means better performance.\nIt can be seen that UltraSwin-small with smaller number of\nparameters than UVT is able to produce a smaller values for\nMAE and RMSE and higher value for R2. This proves that\nUltraSwin-small is superior to UVT. Furthermore, UltraSwin-\nbase is superior to UltraSwin-small. Both variations of Ultra-\nSwin are able to outperform the UVT on the three evaluation\nmetrics.\nDuring training, we log the training and validation losses\nat every epoch. Figure 3 shows the training and validation\nlosses for UltraSwin-small model. The blue and orange line\nrepresent training and validation losses. From the graph, it can\nbe seen that both training and validation losses are reduced\nas the epoch increased. But the validation loss seems to be\nﬂuctuated on early epoch for UltraSwin-small. It’s because\nthe model still in the early learning phase. After 3 epochs, the\nreduction of validation loss is quite stable.\nFig. 3. Graph of training and validation loss for UltraSwin-small. It can be\nseen from this graph that both training and validation loss are reduced as\nepoch increases. On the early epoch, the validation loss is ﬂuctuated. It is\nbecause the model still in early learning phase. After 3 epochs, the validation\nis quite stable.\nSimilar to UltraSwin-small, both training and validation\nlosses for UltraSwin-base are reduced as the epoch increased.\nIt can be seen from Figure 4 that the loss reduction are quite\nstable both for training and validation loss.\nVI. C ONCLUSION\nIn this paper, we propose UltraSwin, a novel method to\nestimate EF from echocardiogram videos. This method uses\nSwin Transformers, a hierarchical vision transformers to ex-\ntract spatio-temporal features. Furthermore, it gives better EF\nFig. 4. Graph of training and validation loss for UltraSwin-base. It can be\nseen from this graph that both training and validation loss are reduced as\nepoch increases. It indicates that the model learn very well\nestimation than UVT. One can futher research to improve Ul-\ntraSwin performance, for example by aggregating the features\nextraction on every stages before processed by EF regressor\nor use combination between 3D tokens and another Vision\nTransformers backbone such as Pyramid Vision Transformer\n(PVTv2) [25].\nACKNOWLEDGMENT\nThis work is supported by Research Laboratory of Faculty\nof Computer Science, Universitas Indonesia. Thank you for\ncontributing to provide some facilities in laboratory and sup-\nporting this research.\nREFERENCES\n[1] M. Tringelov ´a, P. Nardinocchi, L. Teresi, and A. Di Carlo, “The\ncardiovascular system as a smart system,” Topics on Mathematics for\nSmart Systems - Proceedings of the European Conference , no. January,\npp. 253–270, 2007.\n[2] W. H. Organization (WHO), “Cardiovascular diseases (CVDs),” 2021.\n[Online]. Available: https://www.who.int/en/news-room/fact-sheets/\ndetail/cardiovascular-diseases-(cvds)\n[3] H. Reynaud, A. Vlontzos, B. Hou, A. Beqiri, P. Leeson, and B. Kainz,\n“Ultrasound video transformers for cardiac ejection fraction estimation,”\nin Medical Image Computing and Computer Assisted Intervention –\nMICCAI 2021 , M. de Bruijne, P. C. Cattin, S. Cotin, N. Padoy,\nS. Speidel, Y . Zheng, and C. Essert, Eds. Cham: Springer International\nPublishing, 2021, pp. 495–505.\n[4] P. W. Wood, J. B. Choy, N. C. Nanda, and H. Becher, “Left ventricular\nejection fraction and volumes: it depends on the imaging method.”\nEchocardiography, vol. 70, pp. 87–100, 2014.\n[5] D. Ouyang, B. He, A. Ghorbani, N. Yuan, J. Ebinger, C. P. Langlotz,\nP. A. Heidenreich, R. A. Harrington, D. H. Liang, E. A. Ashley,\nand J. Y . Zou, “Video-based ai for beat-to-beat assessment of cardiac\nfunction,” Nature, vol. 580, pp. 252–256, 2020. [Online]. Available:\nhttp://dx.doi.org/10.1038/s41586-020-2145-8\n[6] P. Seferovic, M. Polovina, J. Bauersachs, M. Arad, T. Ben Gal, L. Lund,\nS. Felix, E. Arbustini, A. Caforio, D. Farmakis, G. Filippatos, E. Gi-\nalafos, V . Kanjuh, G. Krljanac, G. Limongelli, A. Linhart, A. Lyon,\nR. Maksimovic, D. Milicic, I. Milinkovic, M. Noutsias, A. Oto, O. Oto,\nS. Pavlovic, M. Piepoli, A. Ristic, G. Rosano, H. Seggewiss, M. Asanin,\nJ. Seferovic, F. Ruschitzka, J. Celutkiene, T. Jaarsma, C. Mueller,\nB. Moura, L. Hill, M. V olterrani, Y . Lopatin, M. Metra, J. Backs,\nW. Mullens, O. Chioncel, R. de Boer, S. Anker, C. Rapezzi, A. Coats,\nand C. Tschoepes, “Heart failure in cardiomyopathies: a position paper\nfrom the heart failure association of the european society of cardiology,”\nEuropean Journal of Heart Failure , vol. 21, no. 5, pp. 553–576, May\n2019.\n[7] C. L. Lekavich, D. J. Barksdale, V . Neelon, and J. R. Wu, “Heart failure\npreserved ejection fraction (HFpEF): an integrated and strategic review,”\nHeart Failure Reviews, vol. 20, no. 6, pp. 643–653, nov 2015. [Online].\nAvailable: https://link.springer.com/article/10.1007/s10741-015-9506-7\n[8] R. M. Lang, L. P. Badano, V . Mor-Avi, J. Aﬁlalo, A. Armstrong,\nL. Ernande, F. A. Flachskampf, E. Foster, S. A. Goldstein,\nT. Kuznetsova, P. Lancellotti, D. Muraru, M. H. Picard, E. R.\nRietzschel, L. Rudski, K. T. Spencer, W. Tsang, and J.-U.\nV oigt, “Recommendations for Cardiac Chamber Quantiﬁcation by\nEchocardiography in Adults: An Update from the American Society\nof Echocardiography and the European Association of Cardiovascular\nImaging,” Journal of the American Society of Echocardiography ,\nvol. 28, no. 1, pp. 1–39.e14, Jan. 2015. [Online]. Available:\nhttps://linkinghub.elsevier.com/retrieve/pii/S0894731714007457\n[9] T. S. Jahren, E. N. Steen, S. A. Aase, and A. H. Solberg, “Estimation\nof end-diastole in cardiac spectral doppler using deep learning,” IEEE\nTransactions on Ultrasonics, Ferroelectrics, and Frequency Control ,\nvol. 67, pp. 2605–2614, 2020.\n[10] D. Ouyang, B. He, A. Ghorbani, M. P. Lungren, E. A. Ashley, D. H.\nLiang, and J. Y . Zou, “Echonet-dynamic: a large new cardiac motion\nvideo data resource for medical machine learning,” 33rd Conference on\nNeural Information Processing Systems (NeurIPS 2019), pp. 1–11, 2019.\n[11] V . Sharma, M. Gupta, A. Kumar, and D. Mishra, “Video processing\nusing deep learning techniques: A systematic literature review,” IEEE\nAccess, vol. 9, pp. 139 489–139 507, 2021.\n[12] A. Ghorbani, D. Ouyang, A. Abid, B. He, J. H. Chen, R. A. Harrington,\nD. H. Liang, E. A. Ashley, and J. Y . Zou, “Deep learning interpretation\nof echocardiograms,” npj Digital Medicine , vol. 3, pp. 1–10, 2020.\n[Online]. Available: http://dx.doi.org/10.1038/s41746-019-0216-8\n[13] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n“Rethinking the inception architecture for computer vision,” in 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\nLos Alamitos, CA, USA: IEEE Computer Society, jun 2016, pp.\n2818–2826. [Online]. Available: https://doi.ieeecomputersociety.org/10.\n1109/CVPR.2016.308\n[14] J. P. Howard, J. Tan, M. J. Shun-Shin, D. Mahdi, A. N. Nowbar,\nA. D. Arnold, Y . Ahmad, P. McCartney, M. Zolgharni, N. W. Linton,\nN. Sutaria, B. Rana, J. Mayet, D. Rueckert, G. D. Cole, and D. P.\nFrancis, “Improving ultrasound video classiﬁcation: An evaluation of\nnovel deep learning methods in echocardiography,” Journal of Medical\nArtiﬁcial Intelligence, vol. 3, 2020.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 6\n2017. [Online]. Available: http://arxiv.org/abs/1706.03762\n[16] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin, and H. Hu,\n“Video swin transformer,” pp. 1–12, 2021. [Online]. Available:\nhttp://arxiv.org/abs/2106.13230\n[17] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” pp. 9992–10 002, 2022.\n[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” 10 2020. [Online].\nAvailable: http://arxiv.org/abs/2010.11929\n[19] Y . Li, K. Zhang, J. Cao, R. Timofte, and L. V . Gool, “Localvit:\nBringing locality to vision transformers,” 4 2021. [Online]. Available:\nhttp://arxiv.org/abs/2104.05707\n[20] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” 6\n2016. [Online]. Available: http://arxiv.org/abs/1606.08415\n[21] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” 2016.\n[Online]. Available: http://arxiv.org/abs/1607.06450\n[22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770–778.\n[23] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database.” Institute of Electrical and\nElectronics Engineers (IEEE), 3 2010, pp. 248–255.\n[24] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\n2017. [Online]. Available: https://arxiv.org/abs/1711.05101\n[25] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu,\nP. Luo, and L. Shao, “Pvtv2: Improved baselines with pyramid vision\ntransformer,” CoRR, vol. abs/2106.13797, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2106.13797",
  "topic": "Ejection fraction",
  "concepts": [
    {
      "name": "Ejection fraction",
      "score": 0.5958251357078552
    },
    {
      "name": "Transformer",
      "score": 0.4986264705657959
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4185114800930023
    },
    {
      "name": "Computer science",
      "score": 0.40673333406448364
    },
    {
      "name": "Cardiology",
      "score": 0.23667678236961365
    },
    {
      "name": "Medicine",
      "score": 0.17890852689743042
    },
    {
      "name": "Engineering",
      "score": 0.16454559564590454
    },
    {
      "name": "Heart failure",
      "score": 0.09565004706382751
    },
    {
      "name": "Electrical engineering",
      "score": 0.0928506851196289
    },
    {
      "name": "Voltage",
      "score": 0.07606339454650879
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I29617571",
      "name": "University of Indonesia",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I4210087238",
      "name": "Kaken Pharmaceutical (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210110652",
      "name": "RIKEN",
      "country": "JP"
    }
  ],
  "cited_by": 26
}