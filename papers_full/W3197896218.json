{
  "title": "Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification",
  "url": "https://openalex.org/W3197896218",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5066433900",
      "name": "Zhongxing Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063066022",
      "name": "Yifan Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059344854",
      "name": "Jia Li",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2931208564",
    "https://openalex.org/W3107308941",
    "https://openalex.org/W2736410039",
    "https://openalex.org/W2799132636",
    "https://openalex.org/W2998792609",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2798775284",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2984145721",
    "https://openalex.org/W2963047834",
    "https://openalex.org/W2585635281",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W2990317318",
    "https://openalex.org/W3034611771",
    "https://openalex.org/W2964304299",
    "https://openalex.org/W2984040540",
    "https://openalex.org/W2979938149",
    "https://openalex.org/W2884366600",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W2963438548",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2890159224",
    "https://openalex.org/W2938145879",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3110536152",
    "https://openalex.org/W2511791013",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2986093954",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W3034580371",
    "https://openalex.org/W3035537506",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3035186652",
    "https://openalex.org/W2962691289",
    "https://openalex.org/W2980073905",
    "https://openalex.org/W2987151592",
    "https://openalex.org/W3097870364",
    "https://openalex.org/W2739031953",
    "https://openalex.org/W3174336354",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W2964201641",
    "https://openalex.org/W2798590501",
    "https://openalex.org/W2981420411",
    "https://openalex.org/W3035539956",
    "https://openalex.org/W2580899942",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3115484111",
    "https://openalex.org/W2983178467",
    "https://openalex.org/W2204750386"
  ],
  "abstract": "Person Re-Identification (Re-Id) in occlusion scenarios is a challenging problem because a pedestrian can be partially occluded. The use of local information for feature extraction and matching is still necessary. Therefore, we propose a Pose-guided inter-and intra-part relational transformer (Pirt) for occluded person Re-Id, which builds part-aware long-term correlations by introducing transformers. In our framework, we firstly develop a pose-guided feature extraction module with regional grouping and mask construction for robust feature representations. The positions of a pedestrian in the image under surveillance scenarios are relatively fixed, hence we propose an intra-part and inter-part relational transformer. The intra-part module creates local relations with mask-guided features, while the inter-part relationship builds correlations with transformers, to develop cross relationships between part nodes. With the collaborative learning inter- and intra-part relationships, experiments reveal that our proposed Pirt model achieves a new state of the art on the public occluded dataset, and further extensions on standard non-occluded person Re-Id datasets also reveal our comparable performances.",
  "full_text": "Pose-guided Inter- and Intra-part Relational Transformer for\nOccluded Person Re-Identification\nZhongxing Ma1, Yifan Zhao1, Jia Li1,2âˆ—\n1State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, Beijing, China\n2Peng Cheng Laboratory, Shenzhen, China\nABSTRACT\nPerson Re-Identification (Re-Id) in occlusion scenarios is a challeng-\ning problem because a pedestrian can be partially occluded. The use\nof local information for feature extraction and matching is still nec-\nessary. Therefore, we propose a Pose-guided inter- and intra-part\nrelational transformer (Pirt) for occluded person Re-Id, which builds\npart-aware long-term correlations by introducing transformer. In\nour framework, we firstly develop a pose-guided feature extraction\nmodule with regional grouping and mask construction for robust\nfeature representations. The positions of a pedestrian in the image\nunder surveillance scenarios are relatively fixed, hence we pro-\npose intra-part and inter-part relational transformer. The intra-part\nmodule creates local relations with mask-guided features, while\nthe inter-part relationship builds correlations with transformers,\nto develop cross relationships between part nodes. With the col-\nlaborative learning inter- and intra-part relationships, experiments\nreveal that our proposed Pirt model achieves a new state of the art\non the public occluded dataset, and further extensions on standard\nnon-occluded person Re-Id datasets also reveal our comparable\nperformances.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Object identification; Object\nrecognition.\nKEYWORDS\nperson re-identification, pose-guided, attention\n1 INTRODUCTION\nPerson Re-Identification, which intends to retrieve the most similar\nperson image to the query person, has made great progress due\nto the wide application of deep learning [39]. However, handling\ncross-camera identification in complex occlusion scenes still faces\ngreat challenges. In the conventional person Re-Id task, the aligned\nperson image usually contains the information of the holistic body\n[37, 47]. Previous works [4, 32, 33, 41] tend to build strong back-\nbone features or add part correlations, thereby the robust global\nfeature expressions of the person can be learned. By contrast, in the\nocclusion scene, there usually exists a large number of occluding\nobjects in aligned person images [52], which makes it difficult to\nproduce reliable and discernible features.\nBy taking a deep investigation into the occluded person Re-Id\ntask, the redundant occlusion does not only lead to the loss of target\ninformation but also introduces additional disturbing information.\nThese occlusions of different characters such as color, size, shape,\nand structural position have affected the overall re-identification\nâˆ—Jia Li is the corresponding author (E-mail: jiali@buaa.edu.cn).\nWebsite: https://cvteam.net/.\nFigure 1: The motivation of our proposed method. We esti-\nmate the keypoints of the pedestrian in image and extract\nlocal information with limited region. Then we aggregate in-\nformation in more abstract part region according to the po-\nsition of each keypoint. We capture the most representative\ninformation and match it with its corresponding semantic\ninformation to get the final retrieval results.\nof the person. Most of the existing methods adopt the pose esti-\nmation [9, 21, 35], the mask guidance [15], or the class activations\n[13, 16, 18, 29, 31, 44] as auxiliary guidance to amplify the discrimi-\nnative regions with higher confidence. Mask-guided information\n[16] helps discover the foreground region but loses the internal\nrelation of keypoints which is provided by the pose estimation like\n[35].\nTo solve this problem, pose-guided person Re-Id models [9, 21,\n35] adopt an auxiliary keypoint detector to detect visible body parts,\nsuch as heads, hands, and legs, which build the strong semantic\nrepresentation of the object. In the occluded scenarios, the overlap\nof other agnostic objects usually leads to the disturbance of the\nright location of body parts. Hence directly matching these regions\nwould produce severe errors in the retrieval phase. To this end, most\nof the existing methods [ 9, 21, 35] adopt local feature matching,\nsuch as feature alignment and graph matching.\nWang et al. [35] propose a Graph Convolutional Network (GCN)\nbased method to generate part features using pose guided key-\npoints and they apply graph matching to align these part features.\nHowever, some keypoints extracted from the image cover the back-\nground and occlusion or they have limited area. Miao et al. [21]\npropose a partial and pose-guided part feature alignment to select\nthe useful information from the heavily occluded person. But the\nsemantic information of different partial features is not always the\nsame, and it neglects the relationship between these partial features.\nDespite their performance differences, current occluded person\nRe-Id methods still show limitations in three aspects: 1) the pose\narXiv:2109.03483v1  [cs.CV]  8 Sep 2021\nestimation network generates unreliable localization, further fea-\nture matching with these parts is easily lead to misalignment; 2)\nthe generation of local part regions usually neglects essential con-\ntextual information, leads to overfitting issues on visual patterns; 3)\nthe structural relationship of keypoints structures is not deeply in-\nvestigated, which makes it difficult to recognize some unreasonable\nmatching results.\nTo solve the aforementioned problems, we propose a pose-guided\ninter- and intra-part relational transformer for the occluded person\nRe-Id task. For constructing reliable part features, we firstly expand\nthe keypoint regions into larger masks and then merge the keypoint\nparts into several groups (3 groups for illustration in Fig. 1). Thus\nfurther feature representation learning is conducted within each\ngroup to form a stable feature. Moreover, we aggregate the key-\npoint heatmaps to form a holistic object to enhance the foreground\nsemantic regions for identification.\nAfter exerting pose-based information from an external pre-\ntrained model, we propose a joint part compositional model of three\ntypes of parts, encoding the contextual regions while strengthening\nthe local features. We adopt the striped slice, patched grid, and\npose-keypoint region as part representations of the image as shown\nin Fig. 2. After introducing contextual information and building\nstrong part representations, we propose our relational transformer\nto construct structural understanding. We adopt the successfully\npracticed transformer architecture in the field of natural language\nprocessing. With part parsing, each part feature is regarded as one\ngraph node to adaptively learn the robust representation, which\nhandles the missing semantics with the occlusion. In Fig. 1, within\neach part group, every regularized keypoint region is aggregated\ninto the holistic representation for this group. During the retrieval\nprocess, we search the nearest neighbor of the query image using\nglobal features and part-based local features. With the collaborative\nconstraints of local features and global features, our proposed Pirt\nmodel achieves a new state of the art on the public occluded per-\nson Re-Id dataset, Occluded-Duke dataset [21] with the supervised\nsetting. We also conduct detailed ablations to verify the effective-\nness of the proposed pose guidance strategies and the inter- and\nintra-part relational architecture. Further extensions on standard\nnon-occluded person Re-Id datasets also reveal the comparable\ngeneralization capability of our model.\nOur contribution can be summarized as three-fold:\n(1) We propose a pose-guided inter- and intra-part relational\ntransformer for occluded person re-identification, which\nbuilds long-term correlations by introducing transformer\narchitectures. Experimental results also verify our proposed\nmethod reaches a new state of the art on public benchmarks.\n(2) We make insightful improvements for pose-guided feature\nextractions, in which detected keypoints are expanded to\nconstruct the holistic object while forming part groups.\n(3) We propose to learn the intra-part relationship with self-\ncorrelations and construct a multi-source inter-part rela-\ntionship learning with relational transformers, providing a\nstructural understanding of different part components.\n2 RELATED WORK\nPerson re-identification. Great improvements have been made in\nsupervised Person Re-Id [39]. One solution to handle the challenge\nis the part-based model, which was proposed according to the fixed\nposition of one person in the image [32, 43]. Sun et al. [32] proposed\na uniform partitioning strategy to output the visual descriptor con-\nsisting of several horizontal part-level features, similar ideas can\nbe founded in other fine-grained tasks [11, 46]. On the other hand,\nto enhance the person feature representation capability of Con-\nvolutional Neural Network (CNN), several methods [4, 17, 24, 50]\ntried to extend modules to get better performance. Hou et al. [17]\nproposed a spatial interaction-and-aggregation module to capture\nrelationships between spatial features. Attention mechanism was\nused for designing CNNs to capture person information in images\n[2, 33, 38, 41, 45]. Zhang et al. [41] utilized the clustering-like infor-\nmation among spatial positions in the feature map and proposed\ntwo relation-aware global attention modules. In some mask-guided\nmodels [3, 27], external knowledge helps to capture the informa-\ntion of the foreground. In some pose-guided models [10, 28], the\npositional and semantic information of the keypoints were used to\nproduce local features and explore the connection between them.\nOccluded person re-identification. Recognizing an occluded\nperson is much difficult because of the confusing information and\nspatial feature misalignment [35]. Most of the papers adopt the local\nfeature matching method [ 8, 13, 15, 16, 18, 21, 29, 31, 35, 44, 51].\nHe et al. [13] proposed a method of spatial feature reconstruction\nthat got robust local feature maps, then the author used the least\nsquare algorithm to solve the coefficient matrix to align correspond-\ning spatial features. Heet al. [16] proposed a method that was based\non the inspiration of [13]. The paper [16] proposed a method aim-\ning to extract probability scores from the foreground probability\nnetwork. The scores are used for the spatial reconstruction by as-\nsigning the body parts with confidence scores. Wang et al. [35]\nproposed a keypoint semantic feature extraction method with pose\nguidance, then feature maps were passing messages using GNN.\nIn addition, the method in the paper[21] used the basic idea of the\npaper [32] with pose estimation methods to generate robust fea-\ntures, while [15] used pose estimation and salience object detection\nsimultaneously, which generated the semantic mask constrained\nby two keypoint detectors of the human body.\nImage retrieval. The local feature matching used in the oc-\ncluded person Re-Id is similar to the local feature matching used\nin image retrieval [ 1, 6, 19, 26, 36, 40]. Keypoints across images\nmust comply some certain physical constraints. For example, one\nkeypoint is associated with a certain keypoint in another image and\nsome keypoints are mismatched due to occlusion. Sarlin et al. [26]\nproposed an attention-based graph neural network to jointly find\ncorresponding keypoints and reject irrelevant keypoints. The paper\n[26] combined with traditional local feature detector and descrip-\ntor which extract sparse keypoints. Cao et al. [1] introduced an\nauto-encoder-based dimensionality reduction technique for local\nfeatures and combined the global feature with local features to\nget better performances. The local features were generated from\nCNN in the paper [1], which were a sort of high-level information\nextracted from the original image. Our method also draws some\ninspiration from the paper [1].\nFigure 2: The overall architecture of our proposed model, which consists of three essential parts: feature extraction from\nthe image, keypoints generation from the image, inter- and intra-part relational transformer to aggregate the information\nbetween different visible or invisible regions.\nTransformer. The transformer was firstly proposed by [34] for\nmachine translation. Similar and improved methods reached the\nstate of the art in many natural language processing tasks. One\nof the representative works [7] is a solid example to explain the\nstrength of self-attention. With the development of the transformer,\na lot of works [23, 42] were looking at how to design the transformer\nfor image recognition. [42] explored several self-attention modules\nwhich were invariant to permutation, then they proposed a patch-\nwise self-attention module with the capacity to uniquely identify\nspecific locations.\n3 METHOD\n3.1 Overview\nThe difficulties of occluded person Re-Id lie in the incomplete in-\nformation and the unstable location of the person in the image. In\nmost cases, a pedestrian in the surveillance scenario has a stand-\ning or walking posture with his head, body, and legs well aligned\nfrom the top to bottom. An intricate occlusion environment would\ncause more negative effects, especially on locality. We use a pose\nestimation network Pto extract keypoints of the person in the\nimage Iâˆˆ[ 0,255]ğ»Ã—ğ‘ŠÃ—3. The keypoints cover the correspond-\ning positions of human body joints in the image and keypoints\nare represented by heatmaps M âˆˆ(0,1)ğ»ğ‘šÃ—ğ‘Šğ‘šÃ—ğ‘ƒ, where ğ‘ƒ is the\nnumber of keypoints. Besides, we use a Global Max Pooling (GMP)\nlayer to expand the original limited coverage of keypoints to seize\nmore essential contextual information. We extract the visible part\ninformation of the pedestrian through these heatmaps M.\nAfter the essential local part information is obtained, we merge\nall the heatmaps M into one human mask C âˆˆ( 0,1)ğ»ğ‘“Ã—ğ‘Šğ‘“ . We\ncombine human mask C together with the corresponding spatial\nfeature map F âˆˆRğ»ğ‘“Ã—ğ‘Šğ‘“Ã—ğ¶ generated by the backbone. F is passed\ninto the Intra-part Relational Module (IRM) which is one of the\ncomponents of our model. The feature mapsFğ‘–ğ‘Ÿğ‘š generated by IRM\nare partitioned by different strategies into three-part feature sets\nas shown in Fig. 2 and Fig. 4. For each feature in each set, we use a\nGlobal Average Pooling (GAP) layer to mix the information of the\npartitioned area. We denote Fğ‘™ğ‘œğ‘ğ‘ğ‘™ âˆˆRğ‘Ã—ğ¶ as our local features\nand ğ‘ is the number of parts. In addition, all the keypoint parts are\ndivided into three groups. We totally have three groups of keypoints\nthat are represented by heatmaps: one group of head keypoints,\none group of upper body keypoints, and the last one of lower body\nkeypoints.\nFinally, each type of local featuresFğ‘™ğ‘œğ‘ğ‘ğ‘™ is sent into the Inter-part\nRelational Transform (IRT) to generate well representative features\nas the final embedding features. Embedding features partitioned\nby the M are received by Confidence Score Module (CSM). The\noutputs are incorporated with max score S from M to produce final\nembedding features.\n3.2 Pose-guided Feature Extraction\nPerson Re-Id is a branch of fine-grained task, therefore severe oc-\nclusion and multiple pedestrians in the image seriously impair the\ninformation of the target person. It is difficult for general models to\nlearn the knowledge from the datasets and to reliably positioning\nthe person in the image. To overcome the problems described above,\npose information is critical to find keypoints that represent visible\nparts of the pedestrian.\nWe firstly usePto generate keypoints and their original heatmaps.\nFor each heatmap, the value in each grid ranged from 0 to 1 repre-\nsents the corresponding confidence score. Each initial pose heatmap\nonly covers a limited region. Because the domain gap between the\ndataset trained for Pand the dataset trained for Re-ID task is large.\nWe use a GMP layer to expand the area and integrate more contex-\ntual information:\nM = ğºğ‘€ğ‘ƒ(Ë†M), (1)\nwhere Ë†M is initial heatmaps generated by P. After gathering all\nheatmaps M containing extra peripheral information, we choose\nthe maximum confidence score S for all heatmaps on the identical\ngrid to construct the human mask C:\nCğ‘” = ğ‘šğ‘ğ‘¥(Mğ‘”), (2)\nwhere ğ‘”represents the ğ‘”th grid in original heatmaps. We use local\nfeatures as complementary information to get better performances.\nFor each local feature generated by each keypoint, we set a threshold\nğœto convert the original heatmapsM into 0-1 masks. Then we apply\na GAP layer to generate features:\nFğ‘ğ‘œğ‘ ğ‘’ = ğºğ´ğ‘ƒ((M > ğœ)Â· Fğ‘–ğ‘Ÿğ‘š), (3)\nwhere Fğ‘ğ‘œğ‘ ğ‘’ is one type of Fğ‘™ğ‘œğ‘ğ‘ğ‘™ in Sect. 3.4. We select the max\nconfidence score in each grouped heatmaps M for subsequent fu-\nsion.\n3.3 Intra-part Relational Module\nWith the guidance of pose information, learning methods for local\npart features from an image still have a large development space.\nBased on the mentioned traits of the image, we assume that each\nhorizontal area probably contains part information of the pedestrian.\nAfter we obtain the spatial features F generated by the backbone,\nwe design a bottleneck-like style module called intra-part relational\nmodule. For each horizontal part of F, our goal is to find the re-\nlationship between features in it. Our overall architecture of IRM\nis shown in Fig. 3. We split our architecture into three stages. At\nthe first stage, we use a convolutional layer with 1 Ã—1 filters to\nencode the channel information. InstanceNorm layer, BatchNorm\n(BN) layer and the activation function ReLU to construct a block\nğœ™(Â·), and for features F:\nFğœ™ = ğœ™(F), (4)\nwhere Fğœ™ âˆˆRğ»ğ‘“Ã—ğ‘Šğ‘“Ã—ğ‘‘ indicates the intermediate features gener-\nated by ğœ™(Â·), and ğ‘‘is their dimension. To capture visible pedestrian\ninformation of each part, an attention mechanism called Multi-Head\nSelf-Attention (MHSA) [34] is used to find the relation between\nfeatures in the horizontal part. This module is used in a standard\ntransformer block as well. We then horizontally divide Fğœ™ into\nstriped features eFğœ™ âˆˆR1Ã—ğ‘Šğ‘“Ã—ğ‘‘ as shown in Fig. 3. The striped fea-\nture eFğœ™ contains multiple individual features with dimensionğ‘‘. We\nuse MHSA to build relationships between these features, then the\nrelational guidance is built to construct different attention scores\nfor each feature eFğœ™. We place all features into original position. To\navoid the over-fitting problem that might be raised by MHSA, we\nFigure 3: Brief illustration of our intra-part relational mod-\nule and transform unit. a) is our proposed intra-part rela-\ntional module and b) is the general component of our inter-\npart relational transformer.\nadopt a Dropout layer to be a stochastic identical mapping:\nFâˆ—\nğœ™ = ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘€ğ»ğ‘†ğ´(ğ‘â„ğ‘¢ğ‘›ğ‘˜(Fğœ™)))+ ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (Fğœ™). (5)\nWe apply BN and ReLU to generate final features Fâˆ—\nğœ™ of stage 2.\nMHSA aggregates and captures important messages between fea-\ntures. The input consists of query features and key features of di-\nmension ğ‘‘ğ‘˜, and value features of dimension ğ‘‘ğ‘£. For self-attention,\nquery features, key features, and value features are identical. We\ncompute the relation matrix as same as [34].\nğ‘ğ‘¡ğ‘¡ğ‘›ğ‘–(X)= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (XWğ‘–ğ‘„(XWğ‘–ğ¾)ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n)XWğ‘–ğ‘‰, (6)\nX = ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘ğ‘¡ğ‘¡ğ‘›ğ‘–(X))Wğ» ğ‘– âˆˆ1,...,â„, (7)\nwhere X indicates abstract input features, and X indicates output\nfeatures, and Wğ‘„,ğ¾,ğ‘‰ represent learnable weights of query, key,\nvalue features, and their projections. At last, we use Wğ» to en-\nhance the features of one head. Applying multiple heads can get\nmore relationships because of the diverse projections, and richer\nexpressiveness of features through MHSA is then obtained. The\nnext procedure of the final generated features Fğ‘–ğ‘Ÿğ‘š is similar to\nfunction ğœ™(Â·). ğœƒ(Â·)contains a convolutional layer and BN layer to\nrestore original dimension as input features F. Instead of applying\nthe input features F as residual connection, we combine these fea-\ntures with human mask C as an auxiliary attention complement.\nFinally, Fğ‘–ğ‘Ÿğ‘š is produced by the following equation:\nFğ‘–ğ‘Ÿğ‘š = ğ‘…ğ‘’ğ¿ğ‘ˆ(ğœƒ(Fâˆ—\nğœ™)+C Â·F). (8)\n3.4 Inter-part Relational Transformer\nIn complex and extreme occlusion scenes, e.g., few regions of the\npedestrian are visible because of the huge size of occlusion or an-\nother person in the image, the pose estimation network Pwould\ngenerate heatmaps M that contain error locations with strong con-\nfidence scores. The different orientations of the body also influence\nthe matching results. When optimizing local features generated by\nour model through the dataset, the above problems often lead to\nsome fatal errors and an unstable training procedure.\nTherefore, we intend to aggregate local part features extracted\nby pose estimation network Pwith the global feature of the whole\nimage to train the model. The main challenge of generating global\nfeatures is obliging the model to focus on visible parts of the pedes-\ntrian as much as possible, while the information of the occlusions\noccupies a small proportion in features. In the IRM model, for any\nhorizontal region, we have paid attention to visible parts of the\npedestrian, but a solitary part hardly represents the whole pedes-\ntrian. The relation between visible parts becomes one essential\nfactor for representing the pedestrian. By constructing the relation-\nship between the different parts, we can pass messages to each part\nwhich is abstracted as a graph node. The relationship provides a\nstructural understanding between these nodes, and useless features\nlike occlusion features are discarded.\nFor tackling these problems, we introduce the inter-part rela-\ntional transformer [34]. In the field of natural language processing,\nthe transformer takes sentences or words in some scenes as the\ninput. We could build the internal relationship of various words\nfrom the global perspective. But in computer vision, the formation\nof image data is not constructed in a sequence style. In our pro-\nposed inter-part relational transformer, we incorporate transformer\nencoder architecture into our model. In each partitioning strategy,\nspatial features Fğ‘–ğ‘Ÿğ‘š are used to generated different part features.\nDifferent partitioned abstract features X in each group are sent into\na weight-sharing transformer unit as shown in Fig. 3 to capture the\nrelationship between them. For the representation of a query part\nğ‘, some value of the feature X is related to the key part ğ‘˜:\nğ›¼ğ‘,ğ‘˜ = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (XWğ‘–ğ‘„(XWğ‘–ğ¾)ğ‘‡\nâˆšğ‘ ), (9)\nwhere the attention score ğ›¼ğ‘,ğ‘˜ is based on similarities over the\nquery and key part features.ğ‘denotes the dimension of the abstract\nfeature X. We can obtain the attention-based features by using a\nlearnable weight Wğ‘… and use multiple projections to generate final\nfeatures:\nX = ğ¹ğ¹ğ‘ (ğ›¼ğ‘,ğ‘˜XWğ‘…), (10)\nwhere FFN denotes the linear projection on the X called Feed For-\nward Network. This module consists of two linear projections with\nthe ReLU activation function and the Dropout function:\nğ¹ğ¹ğ‘ (X)= ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (ğ‘…ğ‘’ğ¿ğ‘ˆ(XW1 +b1))W2 +b2. (11)\nIn this module, partitioned featuresX are firstly calculating relation\nthrough MHSA module, and output features are then combined\nwith residual features and the layer normalization. The subsequent\nFFN module would process these features as the same.\nPartitioning strategy. Based on the intra-part relation mod-\nule, there are three strategies for partitioning as shown in Fig. 4.\nWe firstly apply the stripe pooling which is closely related to the\nregional pooling to split features Fğ‘–ğ‘Ÿğ‘š horizontally. We assume fea-\ntures Fğ‘–ğ‘Ÿğ‘š of each part have already focused on the visible location\nof the pedestrian, and information of each part still well remains.\nWe mark the striped features as Fğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘’ âˆˆRğ»ğ‘“Ã—1Ã—ğ¶.\nFigure 4: Different partitioning strategies and the procedure\nof relation building. Part features learn the self-correlation\nthrough intra-part relational module and understand the\nstructural information in each group through inter-part re-\nlational transformer.\nThe second partitioning strategy divides the spatial featuresFğ‘–ğ‘Ÿğ‘š\ninto patched girds. We assume that the solitary utilization of intra-\npart relation module and horizontal partitioning strategy loses some\ninformation that is incorrectly ignored, resulting in the inability to\nunderlie the thorough relation between their potential information.\nIn the divided part, we aim to find the hidden spatial features and\ntheir relation, as a supplement for the global feature. We adopt a\nGMP layer to select features from Fğ‘–ğ‘Ÿğ‘š without influencing their\npotential capability on representation. We mark patched features\nas Fğ‘ğ‘ğ‘¡ğ‘â„ âˆˆR\nğ»ğ‘“\n4 Ã—\nğ‘Šğ‘“\n2 Ã—ğ¶.\nFor the last partitioning strategy, we incorporate pose heatmaps\nM with features Fğ‘–ğ‘Ÿğ‘š to select the corresponding part feature as\nshown in Fig. 4. In addition, we divide all features generated by\nthis strategy into three disjoint groups. We denote the pose-guided\nfeatures in one of the groups as Fğ‘ğ‘œğ‘ ğ‘’ âˆˆRğ‘€Ã—ğ¶, where ğ‘€ indicates\nthe number of manually divided features.\nWe apply our inter-part relation transformer to build long-term\nrelationships between partitioned areas. To build relations crossing\nthe part features, we individually send features into our transformer.\nFor each set of local features Fğ‘™ğ‘œğ‘ğ‘ğ‘™ âˆˆ{Fğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘’ ,Fğ‘ğ‘ğ‘¡ğ‘â„,Fğ‘ğ‘œğ‘ ğ‘’}as\nmentioned in Sect. 3.2, we average the first two local features and\nregard these features as final embedding features fğ‘™ğ‘œğ‘ğ‘ğ‘™:\nfğ‘™ğ‘œğ‘ğ‘ğ‘™ = ğºğ´ğ‘ƒ(ğ¼ğ‘…ğ‘‡(Fğ‘™ğ‘œğ‘ğ‘ğ‘™)). (12)\nBecause the pose estimation network Pis unstable for confidence\nscore generation due to the domain gap, we propose a confidence\nscore module to generate self-scores Sğ‘ ğ‘’ğ‘™ğ‘“ and combine with the\nscores S mentioned in Sect. 3.2. The CSM is consistent with a Multi-\nLayer Perceptron layer, and the Sğ‘ ğ‘’ğ‘™ğ‘“ is generated as the following:\nSğ‘ ğ‘’ğ‘™ğ‘“ = ğ‘…ğ‘’ğ¿ğ‘ˆ(Ë†Fğ‘ğ‘œğ‘ ğ‘’W1 +b1)W2 +b2, (13)\nwhere Ë†Fğ‘ğ‘œğ‘ ğ‘’ denotes three combined pose-guided features fğ‘ğ‘œğ‘ ğ‘’.\nWe use the ReLU activation function to build a non-linear mapping\nfunction. Sğ‘ ğ‘’ğ‘™ğ‘“ is multiplied by S and a normalization function is\napplied to smooth the value. The eFğ‘ğ‘œğ‘ ğ‘’ is combined with the score\nat last:\neFğ‘ğ‘œğ‘ ğ‘’ = Ë†Fğ‘ğ‘œğ‘ ğ‘’ Â·ğ‘›ğ‘œğ‘Ÿğ‘š(Sğ‘ ğ‘’ğ‘™ğ‘“ Â·S). (14)\nMatching strategy. Image retrieval task usually adopts global\nfeatures and local features to do matching like [ 19]. Due to the\nhuge differences between categories, robust local features could\nwell complete traditional retrieval tasks. Different from the image\nretrieval task, many person re-identification methods use one single\nglobal feature with dimensionğ‘‘to represent the person in the image.\nThe main reason is that the discrepancy between local features is\nnot huge, and contributing to insufficient fine-grained details of\ntraditional local features. Therefore the results of matching only\nbased on local features are relatively poor.\nOn the other hand, occluded person Re-Id often requires local fea-\ntures for precise feature alignment to get better results. Therefore,\nseveral methods are proposed to handle this problem. [16] and [13]\nused spatial feature reconstruction and [35] directly used keypoints\nfeatures. We choose pose-guided local features which contain the\nfine-grained information of the person. We combine global features\nand local features to produce results. An initial coarse rank list is\nestablished using the global feature, and local features are used to\nperform more precisely matching on the rank list. With the first\nranking list obtained by measuring the global feature, we only con-\nduct local ranking on the top-N images (e.g., N=100) that appear in\nthe list. In nearly all the cases, top-N-selected images cover all the\ncorrect results of a query image.\n3.5 Loss\nIn the training process, we use cross entropy loss and triplet loss\nas the supervised classification signals. In every training step, we\nrandomly sample K samples of P pedestrians from the training\ndataset. For triplet loss, we use hard triplet loss [ 16]. The final\nlearning objective of pose-guided local featureseFğ‘ğ‘œğ‘ ğ‘’ is formulated\nas the following:\nLğ‘™ğ‘œğ‘ğ‘ğ‘™ = 1\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\n[Lğ‘ğ‘™ğ‘ (fğ‘–\nğ‘ğ‘œğ‘ ğ‘’)+Lğ‘¡ğ‘Ÿğ‘–(fğ‘–\nğ‘ğ‘œğ‘ ğ‘’)], (15)\nwhere ğ‘is the number of pose-guided groups. Learning objective of\nfeatures fğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘’ and fğ‘ğ‘ğ‘¡ğ‘â„ which contain information in the inter-\nand intra-part manner can be formulated as:\nLğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ = Lğ‘ğ‘™ğ‘ (fğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘’ )+Lğ‘ğ‘™ğ‘ (fğ‘ğ‘ğ‘¡ğ‘â„), (16)\nThe joint loss function is described below:\nL= Lğ‘™ğ‘œğ‘ğ‘ğ‘™ +Lğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™. (17)\nWith the loss function above, our model can better understand the\nstructural relationship of different part components.\n4 EXPERIMENTS\n4.1 Datasets\nOccluded-DukeMTMC dataset [21]. There are 15,618 images for\ntraining, 17,661 gallery images and 2,210 occluded query images\nfor testing. Training images contain about 9% occluded images and\nthe portion of occluded images in the gallery is around 10%. There\nalways exists one occluded image when computing the distance.\nMarket-1501 dataset [47]. The dataset includes 32,668 images\nof 1,501 identities. These pedestrians are captured by six cameras\nfrom different scenes and viewpoints. There are 12,936 training\nimages of 751 identities. The testing set contains the remaining\nimages. There is one label to represent the background.\nDukeMTMC-reID dataset [25, 48]. The dataset includes 36,411\nimages of 1,404 identities. There are eight different cameras to cap-\nture the person. This dataset selects 702 identities for training. The\nremaining images are for testing. In the testing phase, there is only\none query image of each person in each camera, and remaining\nimages are reserved for gallery.\n4.2 Implementation details\nModel architectures. For our feature extraction backbone, we use\nResNet50-ibn [12, 22] pretrained on ImageNet [5]. We drop its final\nGAP layer and fully connected layer to extend our module. For\npose estimation network, we use HR-Net [30] pretrained on COCO\ndataset [20] which follows the identical setting as [35]. Pose estima-\ntion network predicts a total of 17 keypoints, including head, joints\nof arms, and joints of legs. The parameters of the pose estimation\nnetwork are set to unchanged during training. We use three stacked\ntransformer units with 512 hidden dimensions in FFN, 0.1 dropout\nratio, and the ReLU activation layer. We set a thresholdğœ as 0.001\nto filter the low confidence score. We use a BN layer to normalize\nour final embedding features. We use the modified early version of\nthe platform [14] to implement our method.\nTraning details. The input images are resized into 384 Ã—128.\nIn the training stage, batch size is set to 64 by selecting 16 different\nidentities and 4 samples for each identity. We choose some common\ndata augmentation strategies including padding 10 pixels, random\ncropping, horizontal flipping, and random erasing [49] with a prob-\nability of 0.5. During training, Adam optimizer is adopted. We set\nweight decay 5 Ã—10âˆ’4 and we train our model for 60 epochs with\ninitialized learning rate 3.5 Ã—10âˆ’4. The learning rate is warmed up\nfor 10 epochs first and decayed by cosine method from the 30th\nepoch to 1 Ã—10âˆ’6. The model is implemented with the pytorch\nframework and trained with two NVIDIA RTX 2080 Ti.\nEvaluation metrics. For evaluation, cumulative matching char-\nacteristic curve and mean average precision (mAP) are adopted.\n4.3 Experimental Results\nResults on the occluded dataset. Our experiments are conducted\non the occluded Re-Id benchmark dataset with supervised setting,\nOccluded-DukeMTMC dataset [21]. Tab. 1 shows the comparisons\nover the dataset Occluded-DukeMTMC. Part-Aligned [ 44], PCB\n[32] and Adver Occluded [18] are designed for holistic ReID task.\nFD-GAN [9], Part Bilinear [ 29], PGFA [21], and HONet [ 35] use\nFigure 5: Comparison of retrieval results. The leftmost denotes query image which is followed by the ten closest matching\nresults (including the same camera). The green color indicates correct retrieval results and the red color indicates error results.\nTable 1: Performance (%) comparisons to the state-of-the-art\noccluded Re-Id results on the Occluded-DukeMTMC dataset.\nMethods Occluded-DukeMTMC\nmAP Rank-1\nPart-Aligned [44] 20.2 28.8\nPCB [32] 33.7 42.6\nAdver Occluded [18] 32.2 44.5\nFD-GAN [9] - 40.8\nPart Bilinear [29] 36.9 -\nPGFA [21] 37.3 51.4\nHONet [35] 43.8 55.1\nDSR [13] 30.4 40.8\nBaseline (ours) 43.4 51.8\nPirt (ours) 50.9 60.0\nthe extra pose information. DSR [13] does not use extra keypoints\ndetector.\nResults on holistic datasets. If one solution performs well\nin the occluded dataset, it should also perform well on holistic\ndatasets. To verify whether our method works on holistic datasets,\nwe compare our method with other methods for occluded Re-Id on\nholistic datasets Market-1501 [47] and DukeMTMC-reID [25, 48].\nTab. 2 shows that some methods perform well on the Occluded-\nDukeMTMC dataset, but have different performances on holistic\ndatasets. The method in [16] can outperform two other methods\nproposed by [21, 35], which indicates that simply using external\ninformation like keypoints might not achieve the best performance\non holistic datasets. We think it is mainly because holistic datasets\ncontain few occlusions, and the ability of these methods on ex-\ntracting and representing global information from the image to\ngenerate pedestrian features is relatively low. Occluded images in\nthe dataset contain massive noise caused by occlusions, and directly\nTable 2: The comparisons over the Market-1501 dataset and\nthe DukeMTMC-reID dataset.\nMethods Market-1501 DukeMTMC-reID\nmAP Rank-1 mAP Rank-1\nPCB [32] 77.4 92.3 66.1 81.8\nDSR [13] 64.3 83.6 - -\nFPR [16] 86.6 95.4 78.4 88.6\nVPM [31] 80.8 93.0 72.6 83.6\nPGFA [21] 76.8 91.2 65.5 82.6\nHONet [35] 84.9 94.2 75.6 86.9\nPirt (ours) 86.3 94.1 77.6 88.9\nusing the global feature leads to serious confusion. Semantic in-\nformation inside the global feature cannot make a good alignment\nto distinguish corresponding regions in images. In this case, we\nput forward different partition strategies and attention modules.\nOn the basis of ensuring the robustness of feature representation\nability, our method combines local features and global features for\nmore detailed alignment to achieve better results.\nIn Fig. 5 we make a rank list to compare our baseline with our\nproposed method. In the first row, our method finds the holistic\npedestrian according to visible parts in the query image. In the\nsecond and the third row, visible parts are fewer than occlusions\nthus baseline only capture obvious regions without considering\nthe relation between the person and the occlusion in the image.\nThe last row shows that our method more robustly removes the\nnoise generated by the information of another person. Our method\nshows the effectiveness to find occluded people.\nTable 3: Analysis of three different components: P represent\npose generation, Intra represents intra-part relational mod-\nule and Inter represents inter-part relational transformer.\nP Intra Inter mAP Rank-1\nÃ— Ã— Ã— 43.40 51.81\nâœ“ Ã— Ã— 42.23 52.17\nâœ“ âœ“ Ã— 47.20 55.88\nâœ“ âœ“ âœ“ 50.90 60.00\nTable 4: The performances on different number of our trans-\nformer unit N.\nN mAP Rank-1\n1 48.93 57.51\n2 49.93 59.05\n3 50.90 60.00\n4 50.59 59.41\n4.4 Performance Analysis\nAblations studies. We evaluate the effectiveness of our proposed\nmethod by reconstructing our model to verify the influences of\neach module. As shown in Tab. 3, our proposed module improves\nthe performance on the Occluded-DukeMTMC dataset. Baseline\nmethod uses only an individual feature vector extracted from the\nsingle backbone without using triplet loss function. Then we use\nHR-Net [30] to extract keypoints from the image and apply averaged\nlocal features to present each keypoint. We adopt a GMP and a GAP\nlayer to select representative features as local features in Tab. 3.\nFor each local feature, we use all the loss functions as described\nabove to constraint the local part feature. But extracting features\naccording to the keypoints from the backbone features hardly leads\nto a better result. DSR [ 13] shows another type of local features\nwhich consists of multiple max poolings with different scales. The\nfinal amount of the local features [13] is large and the used least\nsquare algorithm between two different local features set leads to\ninefficiency. Our method uses only three robust local features, and\nthe computational complexity is similar to the baseline.\nEffects of inter- and intra-part relational transformer. As\nshown in Tab. 3, we found that different dividing strategies applied\non the spatial features into parts and building the relationship\nbetween grids inside each part can give a positive effect to recognize\nthe person in the image as shown in Tab. 3. Different from [32], we\ndo not restrict each striped part using independent cross entropy\nloss and regard the divided parts as separate local features. With\nthe support of the attention mechanism, merging local features into\nglobal features well represents the pedestrian while eliminating\nlots of parameters. We visualize our model using its weight and\nactivation function as shown in Fig. 6.\nEffects of the number of our transformer units. Tab. 4 shows\nthe influence of the different numbers of transformer units on the\naccuracy. One layer of the transformer unit is insufficient for ex-\ntracting features. However, the person re-identification task heavily\nrelies on pretrained backbone weights, more encoder layers lead to\na tough training procedure while keeping a similar performance.\nFigure 6: Visualization on features generated by our pro-\nposed intra-part relational module.\nTable 5: The effects of different combinations of confidence\nscores on the matching results. Q and G represent confi-\ndence scores of query images and gallery images respec-\ntively.\nCombination Methods mAP Rank-1\nQ & G 50.90 60.00\nQ only 49.91 59.86\nG only 49.89 59.86\nNo confidence score 48.19 57.78\nAs a result, we set the number of transformer units to 3 to balance\nthe accuracy and the efficiency.\nEffects of confidence score combination method. Corre-\nsponding local features are measured by cosine distance and multi-\nplied with confidence scores generated by confidence score module.\nThere are four situations as shown in Tab. 5. The best performance\nis conducted in the first situation by simultaneously uniting two\nconfidence scores. The next two situations indicate that using only\none of the scores impairs the performances. The reason behind\nthis phenomenon mainly occurred in an occluded scene, e.g., differ-\nent degrees of occlusion on the corresponding part can get wrong\nmatching results. Ignoring potential confidence score influenced\nby occlusions may lead to a weak performances.\n5 CONCLUSIONS\nWe propose a novel framework namely Pose-guided inter- and intra-\npart relational transformers for occluded person Re-Id. In the pose\ngeneration stage, our detected keypoints are naturally fused into\nour module to well represent the holistic object with part groups.\nWe combine these local part features with an attention mechanism\nto construct long-term correlations and provide a structural under-\nstanding of different part components by introducing transformer.\nWith these two complementary relationships constructed from dif-\nferent perspectives, our method reaches the new state of the art\non occluded person re-identification task and our experiments also\nshow the effectiveness of our model.\n6 ACKNOWLEDGEMENT\nThis work was supported by grants from National Natural Science\nFoundation of China (No. 61922006) and CAAI-Huawei MindSpore\nOpen Fund.\nREFERENCES\n[1] Bingyi Cao, AndrÃ© Araujo, and Jack Sim. 2020. Unifying deep local and global\nfeatures for image search. In European Conference on Computer Vision . Springer,\n726â€“743.\n[2] Binghui Chen, Weihong Deng, and Jiani Hu. 2019. Mixed high-order attention\nnetwork for person re-identification. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 371â€“381.\n[3] Xuesong Chen, Canmiao Fu, Yong Zhao, Feng Zheng, Jingkuan Song, Rongrong\nJi, and Yi Yang. 2020. Salience-guided cascaded suppression network for person\nre-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition . 3300â€“3310.\n[4] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu, and Ping Tan. 2019.\nBatch dropblock network for person re-identification and beyond. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision . 3691â€“3701.\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:\nA large-scale hierarchical image database. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) . 248â€“255.\n[6] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. 2018. Superpoint:\nSelf-supervised interest point detection and description. In Proceedings of the\nIEEE conference on computer vision and pattern recognition workshops . 224â€“236.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for Language Under-\nstanding.. In NAACL-HLT (1), Jill Burstein, Christy Doran, and Thamar Solorio\n(Eds.). Association for Computational Linguistics, 4171â€“4186. http://dblp.uni-\ntrier.de/db/conf/naacl/naacl2019-1.html#DevlinCLT19\n[8] Shang Gao, Jingya Wang, Huchuan Lu, and Zimo Liu. 2020. Pose-guided visible\npart matching for occluded person ReID. InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition . 11744â€“11752.\n[9] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, and\nHongsheng Li. 2018. FD-GAN: Pose-guided Feature Distilling GAN for Robust\nPerson Re-identification. In Advances in Neural Information Processing Systems .\n1229â€“1240.\n[10] Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jin-Ge Yao, and Kai Han.\n2019. Beyond human parts: Dual part-aligned representations for person re-\nidentification. InProceedings of the IEEE/CVF International Conference on Computer\nVision. 3642â€“3651.\n[11] Bing He, Jia Li, Yifan Zhao, and Yonghong Tian. 2019. Part-regularized near-\nduplicate vehicle re-identification. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 3997â€“4005.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. InIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) . 770â€“778.\n[13] Lingxiao He, Jian Liang, Haiqing Li, and Zhenan Sun. 2018. Deep spatial feature\nreconstruction for partial person re-identification: Alignment-free approach. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\n7073â€“7082.\n[14] Lingxiao He, Xingyu Liao, Wu Liu, Xinchen Liu, Peng Cheng, and Tao Mei.\n2020. FastReID: A Pytorch Toolbox for General Instance Re-identification. arXiv\npreprint arXiv:2006.02631 (2020).\n[15] Lingxiao He and Wu Liu. 2020. Guided Saliency Feature Learning for Person\nRe-identification in Crowded Scenes. In European Conference on Computer Vision .\nSpringer, 357â€“373.\n[16] Lingxiao He, Yinggang Wang, Wu Liu, He Zhao, Zhenan Sun, and Jiashi Feng.\n2019. Foreground-aware pyramid reconstruction for alignment-free occluded\nperson re-identification. In Proceedings of the IEEE/CVF International Conference\non Computer Vision . 8450â€“8459.\n[17] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin\nChen. 2019. Interaction-and-aggregation network for person re-identification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n9317â€“9326.\n[18] Houjing Huang, Dangwei Li, Zhang Zhang, Xiaotang Chen, and Kaiqi Huang.\n2018. Adversarially occluded samples for person re-identification. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition . 5098â€“5107.\n[19] Zhenzhong Lan, Yi Zhu, Alexander G Hauptmann, and Shawn Newsam. 2017.\nDeep local video feature for action recognition. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition workshops . 1â€“7.\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision . Springer, 740â€“755.\n[21] Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi Yang. 2019. Pose-guided\nfeature alignment for occluded person re-identification. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision . 542â€“551.\n[22] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. 2018. Two at Once:\nEnhancing Learning and Generalization Capacities via IBN-Net. In Proceedings\nof the European Conference on Computer Vision (ECCV) .\n[23] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,\nAlexander Ku, and Dustin Tran. 2018. Image transformer. In International Con-\nference on Machine Learning . PMLR, 4055â€“4064.\n[24] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi Yang. 2019. Auto-reid:\nSearching for a part-aware convnet for person re-identification. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision . 3750â€“3759.\n[25] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi.\n2016. Performance measures and a data set for multi-target, multi-camera track-\ning. In European conference on computer vision . Springer, 17â€“35.\n[26] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. 2020. Superglue: Learning feature matching with graph neural networks.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recogni-\ntion. 4938â€“4947.\n[27] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. 2018. Mask-guided\ncontrastive attention model for person re-identification. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition . 1179â€“1188.\n[28] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. 2017.\nPose-driven deep convolutional model for person re-identification. InProceedings\nof the IEEE international conference on computer vision . 3960â€“3969.\n[29] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Kyoung Mu Lee. 2018.\nPart-aligned bilinear representations for person re-identification. In Proceedings\nof the European Conference on Computer Vision (ECCV) . 402â€“419.\n[30] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. 2019. Deep high-resolution\nrepresentation learning for human pose estimation. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 5693â€“5703.\n[31] Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, and Jian Sun.\n2019. Perceive where to focus: Learning visibility-aware part-level features for\npartial person re-identification. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 393â€“402.\n[32] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. 2018. Beyond part\nmodels: Person retrieval with refined part pooling (and a strong convolutional\nbaseline). In Proceedings of the European conference on computer vision (ECCV) .\n480â€“496.\n[33] Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap. 2019. Aanet: Attribute attention\nnetwork for person re-identifications. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition . 7134â€“7143.\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , I. Guyon, U. V.\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[35] Guanâ€™an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang, Shuliang\nWang, Gang Yu, Erjin Zhou, and Jian Sun. 2020. High-order information matters:\nLearning relation and topology for occluded person re-identification. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n6449â€“6458.\n[36] Runzhong Wang, Junchi Yan, and Xiaokang Yang. 2019. Learning combinatorial\nembedding networks for deep graph matching. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision . 3056â€“3065.\n[37] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. 2018. Person transfer\ngan to bridge domain gap for person re-identification. In Proceedings of the IEEE\nconference on computer vision and pattern recognition . 79â€“88.\n[38] Bryan Ning Xia, Yuan Gong, Yizhe Zhang, and Christian Poellabauer. 2019.\nSecond-order non-local attention networks for person re-identification. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision . 3760â€“3769.\n[39] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. H. Hoi. 2021. Deep Learning\nfor Person Re-identification: A Survey and Outlook. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2021), 1â€“1. https://doi.org/10.1109/TPAMI.\n2021.3054775\n[40] Andrei Zanfir and Cristian Sminchisescu. 2018. Deep learning of graph matching.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition .\n2684â€“2693.\n[41] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, and Zhibo Chen. 2020.\nRelation-aware global attention for person re-identification. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition . 3186â€“3195.\n[42] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. 2020. Exploring self-attention\nfor image recognition. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition . 10076â€“10085.\n[43] Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiao-\ngang Wang, and Xiaoou Tang. 2017. Spindle net: Person re-identification with\nhuman body region guided feature decomposition and fusion. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 1077â€“1085.\n[44] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. 2017. Deeply-learned\npart-aligned representations for person re-identification. In Proceedings of the\nIEEE international conference on computer vision . 3219â€“3228.\n[45] Shizhen Zhao, Changxin Gao, Jun Zhang, Hao Cheng, Chuchu Han, Xinyang\nJiang, Xiaowei Guo, Wei-Shi Zheng, Nong Sang, and Xing Sun. 2020. Do Not\nDisturb Me: Person Re-identification Under the Interference of Other Pedestrians.\nIn European Conference on Computer Vision . Springer, 647â€“663.\n[46] Yifan Zhao, Ke Yan, Feiyue Huang, and Jia Li. 2021. Graph-Based High-Order\nRelation Discovery for Fine-Grained Recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 15079â€“15088.\n[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.\n2015. Scalable person re-identification: A benchmark. In Proceedings of the IEEE\ninternational conference on computer vision . 1116â€“1124.\n[48] Zhedong Zheng, Liang Zheng, and Yi Yang. 2017. Unlabeled samples generated\nby gan improve the person re-identification baseline in vitro. In Proceedings of\nthe IEEE International Conference on Computer Vision . 3754â€“3762.\n[49] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. 2020. Random\nerasing data augmentation. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 13001â€“13008.\n[50] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. 2019. Omni-\nscale feature learning for person re-identification. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision . 3702â€“3712.\n[51] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao Wang. 2020. Identity-\nGuided Human Semantic Parsing for Person Re-Identification. European Confer-\nence on Computer Vision (2020).\n[52] Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, and Guangcong Wang. 2018. Occluded\nperson re-identification. In 2018 IEEE International Conference on Multimedia and\nExpo (ICME) . IEEE, 1â€“6.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7460419535636902
    },
    {
      "name": "Computer science",
      "score": 0.7174491882324219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6256141066551208
    },
    {
      "name": "Feature extraction",
      "score": 0.47885820269584656
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45056217908859253
    },
    {
      "name": "Pedestrian",
      "score": 0.4477541148662567
    },
    {
      "name": "Matching (statistics)",
      "score": 0.42708051204681396
    },
    {
      "name": "Computer vision",
      "score": 0.34357893466949463
    },
    {
      "name": "Machine learning",
      "score": 0.34182363748550415
    },
    {
      "name": "Engineering",
      "score": 0.1223059892654419
    },
    {
      "name": "Mathematics",
      "score": 0.12004396319389343
    },
    {
      "name": "Voltage",
      "score": 0.11319386959075928
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Transport engineering",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "cited_by": 10
}