{
  "title": "The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",
  "url": "https://openalex.org/W4385574039",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2782282100",
      "name": "Leonie Weissweiler",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A3021366198",
      "name": "Valentin Hofmann",
      "affiliations": [
        "University of Oxford",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2965293488",
      "name": "Abdullatif Köksal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1558866924",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W1964268059",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2623494909",
    "https://openalex.org/W3200809495",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2963044847",
    "https://openalex.org/W2574887544",
    "https://openalex.org/W3115633976",
    "https://openalex.org/W3129857645",
    "https://openalex.org/W125414735",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3170666909",
    "https://openalex.org/W2122566486",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2094200269",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W1541437793",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4225707828",
    "https://openalex.org/W2902402179",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W4230557881",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W1550933260",
    "https://openalex.org/W2018823791",
    "https://openalex.org/W2162175054",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2130454225",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2171397533",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2126941166",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2104776191",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that combine syntax and semantics. As a first step towards assessing the compatibility of CxG with the syntactic and semantic knowledge demonstrated by state-of-the-art pretrained language models (PLMs), we present an investigation of their capability to classify and understand one of the most commonly studied constructions, the English comparative correlative (CC). We conduct experiments examining the classification accuracy of a syntactic probe on the one hand and the models’ behaviour in a semantic application task on the other, with BERT, RoBERTa, and DeBERTa as the example PLMs. Our results show that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning. While human-like performance of PLMs on many NLP tasks has been alleged, this indicates that PLMs still suffer from substantial shortcomings in central domains of linguistic knowledge.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10859–10882\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nThe Better Your Syntax, the Better Your Semantics? Probing Pretrained\nLanguage Models for the English Comparative Correlative\nLeonie Weissweiler*⋄, Valentin Hofmann†*, Abdullatif Köksal*⋄, Hinrich Schütze*⋄\n*Center for Information and Language Processing, LMU Munich\n⋄Munich Center of Machine Learning\n†Faculty of Linguistics, University of Oxford\n{weissweiler,akoksal}@cis.lmu.de\nvalentin.hofmann@ling-phil.ox.ac.uk\nAbstract\nConstruction Grammar (CxG) is a paradigm\nfrom cognitive linguistics emphasising the con-\nnection between syntax and semantics. Rather\nthan rules that operate on lexical items, it\nposits constructions as the central building\nblocks of language, i.e., linguistic units of dif-\nferent granularity that combine syntax and se-\nmantics. As a ﬁrst step towards assessing the\ncompatibility of CxG with the syntactic and\nsemantic knowledge demonstrated by state-of-\nthe-art pretrained language models (PLMs), we\npresent an investigation of their capability to\nclassify and understand one of the most com-\nmonly studied constructions, the English com-\nparative correlative (CC). We conduct exper-\niments examining the classiﬁcation accuracy\nof a syntactic probe on the one hand and the\nmodels’ behaviour in a semantic application\ntask on the other, with BERT, RoBERTa, and\nDeBERTa as the example PLMs. Our results\nshow that all three investigated PLMs are able\nto recognise the structure of the CC but fail to\nuse its meaning. While human-like perform-\nance of PLMs on many NLP tasks has been al-\nleged, this indicates that PLMs still suffer from\nsubstantial shortcomings in central domains of\nlinguistic knowledge.\n1 Introduction\nThe sentence “The better your syntax, the better\nyour semantics.” contains a construction called\nthe English comparative correlative (CC; Fillmore,\n1986). Paraphrased, it could be read as “If your\nsyntax is better, your semantics will also be better.”\nHumans reading this sentence are capable of doing\ntwo things: (i) recognising that two instances of\n“the” followed by an adjective/adverb in the compar-\native as well as a phrase of the given structure (i.e.,\nthe syntax of the CC) express a speciﬁc meaning\n(i.e., the semantics of the CC); (ii) understanding\nthe semantic meaning conveyed by the CC, i.e.,\nunderstanding that in a sentence of the given struc-\nture, the second half is somehow correlated with\nthe ﬁrst.\nIn this paper, we ask the following question: are\npretrained language models (PLMs) able to achieve\nthese two steps? This question is important for\ntwo reasons. Firstly, we hope that recognising the\nCC and understanding its meaning is challenging\nfor PLMs, helping to set the research agenda for\nfurther improvements. Secondly, the CC is one\nof the most commonly studied constructions in\nconstruction grammar (CxG), a usage-based syntax\nparadigm from cognitive linguistics, thus providing\nan interesting alternative to the currently prevailing\npractice of analysing the syntactic capabilities of\nPLMs with theories from generative grammar (e.g.,\nMarvin and Linzen, 2018).\nWe divide our investigation into two parts. In\nthe ﬁrst part, we examine the CC’s syntactic prop-\nerties and how they are represented by PLMs, with\nthe objective to determine whether PLMs can re-\ncognise an instance of the CC. More speciﬁcally,\nwe construct two syntactic probes with different\nproperties: one is inspired by recent probing meth-\nodology (e.g., Belinkov et al., 2017; Conneau et al.,\n2018) and draws upon minimal pairs to quantify\nthe amount of information contained in each PLM\nlayer; for the other one, we write a context-free\ngrammar (CFG) to construct approximate minimal\npairs in which only the word order determines if\nthe sentences are an instance of the CC or not. We\nﬁnd that starting from the third layer, all invest-\nigated PLMs are able to distinguish positive from\nnegative instances of the CC. However, this method\nonly covers one speciﬁc subtype of comparative\nsentences. To cover the full diversity of instances,\nwe conduct an additional experiment for which we\ncollect and manually label sentences from C4 (Raf-\nfel et al., 2020) that resemble instances of the CC,\nresulting in a diverse set of sentences that either\nare instances of the CC or resemble them closely\nwithout being instances of the CC. Applying the\n10859\nsame methodology to this set of sentences, we ob-\nserve that all examined PLMs are still able to sep-\narate the examples very well.\nIn the second part of the paper, we aim to de-\ntermine if the PLMs are able to understand the\nmeaning of the CC. We generate test scenarios in\nwhich a statement containing the CC is given to the\nPLMs, which they then have to apply in a zero-shot\nmanner. As this way of testing PLMs is prone to a\nvariety of biases, we introduce several mitigating\nmethods in order to determine the full capability\nof the PLMs. We ﬁnd that none of the PLMs we\ninvestigate perform above chance level, indicating\nthat they are not able to understand and apply the\nCC in a measurable way in this context.\nWe make three main contributions:\n– We present the ﬁrst comprehensive study examin-\ning how well PLMs can recognise and understand\na CxG construction, speciﬁcally the English com-\nparative correlative.\n– We develop a way of testing the PLMs’ recog-\nnition of the CC that overcomes the challenge\nof probing for linguistic phenomena not lending\nthemselves to minimal pairs.\n– We adapt methods from zero-shot prompting and\ncalibration to develop a way of testing PLMs for\ntheir understanding of the CC.1\n2 Construction Grammar\n2.1 Overview\nA core assumption of generative grammar (Chom-\nsky, 1988), which can be already found in Bloom-\nﬁeldian structural linguistics (Bloomﬁeld, 1933), is\na strict separation of lexicon and grammar: gram-\nmar is conceptualized as a set of compositional\nand general rules that operate on a list of arbit-\nrary and speciﬁc lexical items in generating syn-\ntactically well-formed sentences. This dichotom-\nous view was increasingly questioned in the 1980s\nwhen several studies drew attention to the fact\nthat linguistic units larger than lexical items (e.g.,\nidioms) can also possess non-compositional mean-\nings (Langacker, 1987; Lakoff, 1987; Fillmore\net al., 1988; Fillmore, 1989). For instance, it is\nnot clear how the effect of the words “let alone”(as\n1In order to foster research at the intersection of NLP\nand construction grammar, we will make our data and code\navailable at https://github.com/LeonieWeissweiler/\nComparativeCorrelative.\nin “she doesn’t eat ﬁsh, let alone meat”) on both the\nsyntax and the semantics of the rest of the sentence\ncould be inferred from general syntactic rules (Fill-\nmore et al., 1988).. This insight about the ubiquity\nof stored form-meaning pairings in language is ad-\nopted as the central tenet of grammatical theory by\nConstruction Grammar (CxG; see Hoffmann and\nTrousdale (2013) for a comprehensive overview).\nRather than a system divided into non-overlapping\nsyntactic rules and lexical items, CxG views lan-\nguage as a structured system of constructions with\nvarying granularities that encapsulate syntactic and\nsemantic components as single linguistic signs—\nranging from individual morphemes up to phrasal\nelements and ﬁxed expressions (Kay and Fillmore,\n1999; Goldberg, 1995). In this framework, syn-\ntactic rules can be seen as emergent abstractions\nover similar stored constructions (Goldberg, 2003,\n2006). A different set of stored constructions can\nresult in different abstractions and thus different\nsyntactic rules, which allows CxG to naturally ac-\ncommodate for the dynamic nature of grammar as\nevidenced, for instance, by inter-speaker variability\nand linguistic change (Hilpert, 2006).\n2.2 Construction Grammar and NLP\nWe see three main motivations for the development\nof a ﬁrst probing approach for CxG:\n– We believe that the active discourse in (cognit-\nive) linguistics about the best description of hu-\nman language capability can be supported and\nenriched through a computational exploration of\na wide array of phenomena and viewpoints. We\nthink that the probing literature in NLP investig-\nating linguistic phenomena with computational\nmethods should be diversiﬁed to include theor-\nies and problems from all points on the broad\nspectrum of linguistic scholarship.\n– We hope that the investigation of large PLMs’ ap-\nparent capabilities to imitate human language and\nthe mechanisms responsible for these capabilit-\nies will be enriched by introducing a usage-based\napproach to grammar. This is especially import-\nant as some of the discourse in recent years has\nfocused on the question of whether PLMs are\nconstructing syntactically acceptable sentences\nfor the correct reasons and with the correct under-\nlying representations (e.g. McCoy et al., 2019).\nWe would like to suggest that considering altern-\native theories of grammar, speciﬁcally CxG with\n10860\nits incorporation of slots in constructions that\nmay be ﬁlled by speciﬁc word types and its focus\non learning without an innate, universal grammar,\nmay be beneﬁcial to understanding the learning\nprocess of PLMs as their capabilities advance\nfurther.\n– Many constructions present an interesting chal-\nlenge for PLMs. In fact, recent work in challenge\ndatasets (Ribeiro et al., 2020) has already started\nusing what could be considered constructions,\nin an attempt to identify types of sentences that\nmodels struggle with, and to point out a potential\ndirection for improvement. One of the central\ntenets of CxG is the relation between the form of\na construction and its meaning, or to put it in NLP\nterms, a model must learn to infer parts of the\nsentence meaning from patterns that are present\nin it, as opposed to words. We believe this to be\nan interesting challenge for future PLMs.\n2.3 The English Comparative Correlative\nThe English comparative correlative (CC) is one\nof the most commonly studied constructions in lin-\nguistics, for several reasons. Firstly, it constitutes\na clear example of a linguistic phenomenon that\nis challenging to explain in the framework of gen-\nerative grammar (Culicover and Jackendoff, 1999;\nAbeillé and Borsley, 2008), even though there have\nbeen approaches following that school of thought\n(Den Dikken, 2005; Iwasaki and Radford, 2009).\nSecondly, it exhibits a range of interesting syntactic\nand semantic features, as detailed below. These\nreasons, we believe, also make the CC an ideal\ntestbed for a ﬁrst study attempting to extend the\ncurrent trend of syntax probing for rules by devel-\noping methods for probing according to CxG.\nThe CC can take many different forms, some of\nwhich are exempliﬁed here:\n(1) The more, the merrier.\n(2) The longer the bake, the browner the colour.\n(3) The more she practiced, the better she became.\nSemantically, the CC consists of two clauses, where\nthe second clause can be seen as the dependent vari-\nable for the independent variable speciﬁed in the\nﬁrst one (Goldberg, 2003). It can be seen on the one\nhand as a statement of a general cause-and-effect\nrelationship, as in a general conditional statement\n(e.g., (2) could be paraphrased as “If the bake is\nlonger, the colour will be more brown”), and on the\nother as a temporal development in a comparative\nsentence (paraphrasing (3) as “She became better\nover time, and she practiced more over time”). Us-\nage of the CC typically implies both readings at the\nsame time. Syntactically, the CC is characterised\nin both clauses by an instance of “the” followed\nby an adverb or an adjective in the comparative,\neither with “-er” for some adjectives and adverbs,\nor with “more” for others, or special forms like\n“better”. Special features of the comparative sen-\ntences following this are the optional omission of\nthe future “will” and of “be”, as in (1). Crucially,\n“the” in this construction does not function as a de-\nterminer of noun phrases (Goldberg, 2003); rather,\nit has a function speciﬁc to the CC and has vari-\nously been called a “degree word” (Den Dikken,\n2005) or “ﬁxed material” (Hoffmann et al., 2019).\n3 Syntax\nOur investigation of PLMs’ knowledge of the CC\nis split into two parts. First, we probe for the PLMs’\nknowledge of the syntactic aspects of the CC, to\ndetermine if they recognise its structure. Then we\ndevise a test of their understanding of its semantic\naspects by investigating their ability to apply, in a\ngiven context, information conveyed by a CC.\n3.1 Probing Methods\nAs the ﬁrst half of our analysis of PLMs’ know-\nledge of the CC, we investigate its syntactic aspects.\nTranslated into probing questions, this means that\nwe ask: can a PLM recognise an instance of the\nCC? Can it distinguish instances of the CC from\nsimilar-looking non-instances? Is it able to go bey-\nond the simple recognition of its ﬁxed parts (“The\nCOMP-ADJ/ADV, the ...”) and group all ways of com-\npleting the sentences that are instances of the CC\nseparately from all those that are not? And to frame\nall of these questions in a syntactic probing frame-\nwork: will we be able to recover, using a logistic\nregression as the probe, this distinguishing inform-\nation from a PLM’s embeddings?\nThe established way of testing a PLM for its\nsyntactic knowledge has in recent years become\nminimal pairs (e.g., Warstadt et al., 2020, Dem-\nszky et al., 2021). This would mean pairs of sen-\ntences which are indistinguishable except for the\nfact that one of them is an instance of the CC and\nthe other is not, allowing us to perfectly separate\na model’s knowledge of the CC from other con-\nfounding factors. While this is indeed possible for\nsimpler syntactic phenomena such as verb-noun\n10861\nnumber agreement, there is no obvious way to con-\nstruct minimal pairs for the CC. We therefore con-\nstruct minimal pairs in two ways: one with artiﬁcial\ndata based on a context-free grammar (CFG), and\none with sentences extracted from C4.\n3.1.1 Synthetic Data\nIn order to ﬁnd a pair of sentences that is as close\nas possible to a minimal pair, we devise a way to\nmodify the words following “The X-er” such that\nthe sentence is no longer an instance of the con-\nstruction. The pattern for a positive instance is\n“The ADV-er the NUM NOUN VERB”, e.g., “The harder\nthe two cats ﬁght”. To create a negative instance,\nwe reorder the pattern to “The ADJ-er NUM VERBthe\nNOUN”, e.g., “The harder two ﬁght the cats”. The\nchange in role of the numeral from the depend-\nent of a head to a head itself, made possible by\nchoosing a verb that can be either transitive or in-\ntransitive, as well as the change from an adverb\nto an adjective, allows us to construct a negative\ninstance that uses the same words as the positive\none, but in a different order.2 In order to generate\na large number of instances, we collect two sets\neach of adverbs, numerals, nouns and verbs that\nare mutually exclusive between training and test\nsets. To investigate if the model is confused by ad-\nditional content in the sentences, we write an CFG\nto insert phrases before the start of the ﬁrst half, in\nbetween the two halves, and after the second half\nof the CC (see Appendix, Algorithms 1 and 2 for\nthe complete CFG).\nWhile this setup is rigourous in the sense\nthat positive and negative sentences are exactly\nmatched, it comes with the drawback of only con-\nsidering one type of CC. To be able to conduct\na more comprehensive investigation, we adopt a\ncomplementary approach and turn to pairs extrac-\nted from C4 (see Appendix, Tables 6 and 7, for\nexamples of training and test data). These cover a\nbroad range of CC patterns, albeit without meeting\nthe criterion that positive and negative samples are\nexactly matched.\n3.1.2 Corpus-based Minimal Pairs\nWhile accepting that positive and negative in-\nstances extracted from a corpus will automatically\nnot be minimal and therefore contain some lexical\n2Note that an alternative reading of this sentence exists:\nthe numeral “two” forms the noun phrase by itself and “The\nharder” is still interpreted as part of the CC. The sentence is\nactually a positive instance on this interpretation. We regard\nthis reading as very improbable.\noverlap and context cues, we attempt to regularise\nour retrieved instances as far as possible. To form\na ﬁrst candidate set, we POS tag C4 using spaCy\n(Honnibal and Montani, 2018) and extract all sen-\ntences that follow the pattern “The” (DET) followed\nby either “more” and an adjective or adverb, or an\nadjective or adverb ending in “-er”, and at any point\nlater in the sentence again the same pattern. We dis-\ncard examples with adverbs or adjectives that were\nfalsely labelled as comparative, such as “other”.\nWe then group these sentences by their sequence of\nPOS tags, and manually classify the sequences as\neither positive or negative instances. We observe\nthat sentences sharing a POS tag pattern tend to be\neither all negative or all positive instances, allowing\nus to save annotation time by working at the POS\ntag pattern level instead of the sentence level. To\nmake the ﬁnal set as diverse as possible, we sort the\npatterns randomly and label as many as possible.\nIn order to further reduce interfering factors in our\nprobe, we separate the POS tag patterns between\ntraining and test sets (see Appendix, Table 8, for\nexamples).\n3.1.3 The Probe\nFor both datasets, we investigate the overall ac-\ncuracy of our probe as well as the impact of sev-\neral factors. The probe consists of training a\nsimple logistic regression model on top of the\nmean-pooled sentence embeddings (Vuli ´c et al.,\n2020). To quantify the impact of the length of\nthe sentence, the start position of the construction,\nthe position of its second half, and the distance\nbetween them, we construct four different subsets\nDtrain\nf and Dtest\nf from both the artiﬁcially construc-\nted and the corpus-based dataset. For each subset,\nwe sample sentences such that both the positive and\nthe negative class is balanced across every value of\nthe feature within a certain range of values. This\nensures that the probes are unable to exploit correla-\ntions between a class and any of the above features.\nWe create the dataset as follows\nDf =\n⋃\nv∈fv\n⋃\nl∗∈L\nS(D, v, l∗, n∗),\nwhere f is the feature, fv is the set of values for\nf, L = {positive, negative}are the labels, and S\nis a function that returns n∗elements from D that\nhave value v and label l∗.\nTo make this task more cognitively realistic,\nwe aim to test if a model is able to general-\nise from shorter sentences, which contain relat-\n10862\n0 2 4 6 8 10 12 14 16 18 20 22 24\nModel Layers\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0T est Accuracy\nBERT-L corpus\nRoBERT a-L corpus\nDeBERT a-L corpus\nBERT-L artificial\nRoBERT a-L artificial\nDeBERT a-L artificial\nFigure 1: Overall accuracy per layer for Dlength. All\nshown models are the large model variants. The models\ncan easily distinguish between positive and negative\nexamples in at least some of their layers.\nively little additional information besides the parts\nrelevant to the classiﬁcation task, to those with\ngreater potential interference due to more addi-\ntional content that is not useful for classiﬁcation.\nThus, we restrict the training set to samples from\nthe lowest quartile of each feature so that fv be-\ncomes [vmin\nf , vmin\nf +1\n4 (vmax\nf −vmin\nf )] for Dtrain\nf and\n[vmin\nf , vmax\nf ] for Dtest\nf . We report the test perform-\nance for every value of a given feature separately to\nrecognise patterns. For the artiﬁcial syntax probing,\nwe generate 1000 data points for each value of each\nfeature for each training and test for each subset\nassociated with a feature. For the corpus syntax\nprobing, we collect 9710 positive and 533 negat-\nive sentences in total, from which we choose 10\ntraining and 5 test sentences for each value of each\nfeature in a similar manner. To improve compar-\nability and make the experiment computationally\nfeasible, we test the “large” size of each of our\nthree models, using the Huggingface Transformers\nlibrary (Wolf et al., 2019). Our logistic regression\nprobes are implemented using Scikitlearn (Pedre-\ngosa et al., 2011).\n3.2 Probing Results\n3.2.1 Artiﬁcial Data\nAs shown in Figure 1, the results of our syntactic\nprobe indicate that all models can easily distin-\nguish between positive and negative examples in\nat least some of their layers, independently of any\nof the sentence properties that we have investig-\nated. We report full results in the Appendix in\nFigures 2, 3, and 4. We ﬁnd a clear trend that De-\nBERTa performs better than RoBERTa, which in\nturn performs better than BERT across the board.\nAs DeBERTa’s performance in all layers is nearly\nperfect, we are unable to observe patterns related to\nthe length of the sentence, the start position of the\nCC, the start position of the second half of the CC,\nand the distance between them. By contrast, we ob-\nserve interesting patterns for BERT and RoBERTa.\nFor Dlength, and to a lesser degreeDdistance (which\ncorrelates with it), we observe that at ﬁrst, perform-\nance goes down with increased length as we would\nexpect—the model struggles to generalise to longer\nsentences with more interference since it was only\ntrained on short ones. However, this trend is re-\nversed in the last few layers. We hypothesize this\nmay be due to an increased focus on semantics in\nthe last layers (Peters et al., 2018; Tenney et al.,\n2019), which could lead to interfering features par-\nticularly in shorter sentences.\n3.2.2 Corpus Data\nIn contrast, the results of our probe on more nat-\nural data from C4 indicate two different trends:\nﬁrst, as the positive and negative instances are not\nidentical on a bag-of-word level, performance is\nnot uniformly at 50% (i.e., chance) level in the ﬁrst\nlayers, indicating that the model can exploit lexical\ncues to some degree. We observe a similar trend as\nwith the artiﬁcial experiment, which showed that\nDeBERTa performs best and BERT worst. The cor-\nresponding graphs can be found in the Appendix in\nFigures 5, 6, and 7.\nGenerally, this additional corpus-based experi-\nment validates our ﬁndings from the experiment\nwith artiﬁcially generated data, as all models per-\nform at 80% or better from the middle layers on,\nindicating that the models are able to classify in-\nstances of the construction even when they are very\ndiverse and use unseen POS tag patterns.\nComparing the average accuracies on Dlength\nfor both data sources in Figure 1, we observe that\nall models perform better on artiﬁcial than on cor-\npus data from the ﬁfth layer on, with the notable\nexception of a dip in performance for BERT large\naround layer 10.\n4 Semantics\n4.1 Probing Methods\n4.1.1 Usage-based Testing\nFor the second half of our investigation, we turn\nto semantics. In order to determine if a model has\nunderstood the meaning of the CC, i.e., if it has\nunderstood that in any sentence, “the COMP .... the\n10863\nNo. Purpose Approach Sentence Schema\nS1 Base The ADJ1-er you are, the ADJ2-er you are. The ANT1-er you are, the ANT2-er you are.\nNAME1 is ADJ1-er than NAME2. Therefore, NAME1 is [MASK] than NAME2.\nS2\nBias Test\nRecency The ANT1-er you are, the ANT2-er you are. The ADJ1-er you are, the ADJ2-er you are.\nNAME1 is ADJ1-er than NAME2. Therefore, NAME1 is [MASK] than NAME2.\nS3 V ocabulary The ADJ1-er you are, the ANT2-er you are. The ANT1-er you are, the ADJ2-er you are.\nNAME2 is ADJ1-er than NAME2. Therefore, NAME1 is [MASK] than NAME2.\nS4 Name The ADJ1-er you are, the ADJ2-er you are. The ANT1-er you are, the ANT2-er you are.\nNAME2 is ADJ1-er than NAME1. Therefore, NAME2 is [MASK] than NAME1.\nS5\nCalibration\nShort NAME1 is ADJ1-er than NAME2. Therefore, NAME1 is [MASK] than NAME2.\nS6 Name The ADJ1-er you are, the ADJ2-er you are. The ANT1-er you are, the ANT2-er you are.\nNAME1 is ADJ1-er than NAME2. Therefore, NAME3 is [MASK] than NAME4.\nS7 Adjective The ADJ1-er you are, the ADJ2-er you are. The ANT1-er you are, the ANT2-er you are.\nNAME1 is ADJ3-er than NAME2. Therefore, NAME1 is [MASK] than NAME2.\nTable 1: Overview of the schemata of all test scenarios used for semantic probing\nCOMP” implies a correlation between the two halves,\nwe adopt a usage-based approach and ask: can the\nmodel, based on the meaning conveyed by the CC,\ndraw a correct inference in a speciﬁc scenario? For\nthis, we construct general test instances of the CC\nthat consist of a desired update of the belief state of\nthe model about the world, which we then expect\nit to be able to apply. More concretely, we gen-\nerate sentences of the form “The ADJ1-er you are,\nthe ADJ2-er you are.”, while picking adjectives at\nrandom. To this general statement, we then add a\nspeciﬁc scenario with two random names: “NAME1\nis ADJ1-er than NAME2.” and ask the model to draw\nan inference from it by predicting a token at the\nmasked position in the following sentence: “There-\nfore, NAME1 is [MASK] than NAME2.” If the model\nhas understood the meaning conveyed by the CC\nand is able to use it in predicting the mask, we ex-\npect the probability of ADJ2 to be high. To provide\nthe model with an alternative, we add a second\nsentence, another instance of the CC, using the\nantonyms of the two adjectives. This sentence is\ncarefully chosen to have no impact on the best ﬁller\nfor [MASK], but also for other reasons explained\nin Section 4.1.2. The full test context is shown in\nTable 1, S1. This enables us to compare the prob-\nability of ADJ2 for the mask token directly with a\nplausible alternative, ANT2. One of our test sen-\ntences might be “The stronger you are, the faster\nyou are. The weaker you are, the slower you are.\nTerry is stronger than John. Therefore, Terry will\nbe [MASK] than John”, where we compare the prob-\nabilities of “faster” and “slower”.\nNote that success in our experiment does not\nnecessarily indicate that the model has fully un-\nderstood the meaning of the CC. The experiment\ncan only provide a lower bound for the underlying\nunderstanding of any model. However, we believe\nthat our task is not unreasonable for a masked lan-\nguage model in a zero-shot setting. It is compar-\nable in difﬁculty and non-reliance on world know-\nledge to the NLU tasks presented in LAMBADA\n(Paperno et al., 2016), on which GPT-2 (117M\nto 1.5B parameters) has achieved high zero-shot\naccuracy (Radford et al., Table 3). While we invest-\nigate masked language models and not GPT-2, our\nlargest models are comparable in size to the sizes\nof GPT-2 that were used (340M for BERTL, 355M\nfor RoBERTaL, and 1.5B parameters for DeBERTa-\nXXLL), and we believe that this part of our task is\nachievable to some degree.\n4.1.2 Biases\nIn this setup, we hypothesise several biases that\nmodels could exhibit and might cloud our assess-\nment of its understanding of the CC, and devise a\nway to test their impact.\nFirstly, we expect that models might prefer to re-\npeat the adjective that is closest to the mask token.\nThis has recently been documented for prompt-\nbased experiments (Zhao et al., 2021). Here, this\nadjective is ANT2, the wrong answer. To test the in-\nﬂuence this has on the prediction probabilities, we\nconstruct an alternative version of our test context\nin which we ﬂip the ﬁrst two sentences so that the\ncorrect answer is now more recent. The result can\nbe found in Table 1, S2.\nSecondly, we expect that models might assign\nhigher probabilities to some adjectives, purely\n10864\nbased on their frequency in the pretraining corpus,\nas for example observed by Holtzman et al. (2021).\nTo test this, we construct a version of the test con-\ntext in whichADJ2/ANT2 are swapped, which means\nthat we can keep both the overall words the same\nas well as the position of the correct answer, while\nchanging which adjective it is. The sentence is now\nS3 in Table 1. If there is a large difference between\nthe prediction probabilities for the two different\nversions, that this means that a model’s prediction\nis inﬂuenced by the lexical identity of the adjective\nin question.\nLastly, a model might have learned to associ-\nate adjectives with names in pretraining, so we\nconstruct a third version, in which we swap the\nnames. This is S4 in Table 1. If any prior associ-\nation between names and adjectives inﬂuences the\nprediction, we expect the scores between S4 and\nS1 to differ.\n4.1.3 Calibration\nAfter quantifying the biases that may prevent us\nfrom seeing a model’s true capability in understand-\ning the CC, we aim to develop methods to mitigate\nit. We turn to calibration, which has recently been\nused in probing with few-shot examples by Zhao\net al. (2021). The aim of calibration is to improve\nthe performance of a model on a classiﬁcation task,\nby ﬁrst assessing the prior probability of a label\n(i.e., its probability if no context is given), and\nthen dividing the probability predicted in the task\ncontext by this prior; this gives us the conditional\nprobability of a label given the context, represent-\ning the true knowledge of the model about this task.\nIn adapting calibration, we want to give a model\nevery possible opportunity to do well so that we do\nnot underestimate its underlying comprehension.\nWe therefore develop three different methods\nof removing the important information from the\ncontext in such a way that we can use the prediction\nprobabilities of the two adjectives in these contexts\nfor calibration. The simplest way of doing this is to\nremove both instances of the CC, resulting in S5 in\nTable 1. If we want to keep the CC in the context,\nthe two options to remove any information are to\nreplace either the names or the adjectives with new\nnames/adjectives. We therefore construct two more\ninstances for calibration: S6 and S7 in Table 1.\nFor each calibration method, we collect ﬁve ex-\namples with different adjectives or names. For a\ngiven base sample Sb, we calculate Pc, the calib-\nAccuracy Decision Flip\nS1 S2 S2 S3 S4\nBERTB 37.65 64.64 26.98 75.69 02.70\nBERTL 36.85 67.21 30.44 73.31 02.32\nRoBERTaB 61.60 52.84 09.91 76.18 02.76\nRoBERTaL 55.71 68.00 14.33 79.47 04.33\nDeBERTaB 49.72 49.80 00.91 99.66 01.07\nDeBERTaL 50.88 51.40 07.04 94.83 02.23\nDeBERTaXL 47.73 49.33 05.46 89.28 02.51\nDeBERTaXXL 47.34 48.72 03.59 82.09 01.13\nTable 2: Selected accuracies and results for the semantic\nprobe. We report the average accuracy on the more dif-\nﬁcult sentences in terms of recency bias (S1) and the\neasier ones (S2), as well as the percentage of decisions\nﬂipped by changing from the base S1 to the sentences\ntesting for recency bias (S2), vocabulary bias (S3), and\nname bias (S4). RoBERTa and DeBERTa perform close\nto chance on S1 and S2 accuracy, indicating that they\ndo not understand the meaning of CC. BERT’s perform-\nance is strongly inﬂuenced by biases (recency, lexical\nidentity), also indicating that it has very limited if any\nunderstanding of CC.\nrated predictions, as follows:\nPc(a|Sb) =P(a|Sb)/[\ni=5∑\ni=1\n(P(a|Ci)/5)]\nwhere Ci is the i-th example of a given calibration\ntechnique, a is the list of adjectives tested for the\nmasked position, and the division is applied ele-\nmentwise. We collect a list of 20 adjectives and\ntheir antonyms manually from the vocabulary of\nthe RoBERTa tokenizer and 33 common names\nand generate 144,800 sentences from them. We\ntest BERT (Devlin et al., 2019) in the sizes base\nand large, RoBERTa (Liu et al., 2019) in the sizes\nbase and large, and DeBERTa (He et al., 2020) in\nthe sizes base, large, xlarge and xxlarge.\n4.2 Results\nIn Table 2, we report the accuracy for all examined\nmodels. Out of the three variations to test biases,\nwe report accuracy only for the sentence testing the\nrecency bias as we expect this bias to occur system-\natically across all sentences: if it is a large effect, it\nwill always lead to the sentence where the correct\nanswer is the more recent one being favoured. To\nassess the inﬂuence of each bias beyond accuracy,\nwe report as decision ﬂip the percentage of sen-\ntences for which the decision (i.e., if the correct\nadjective had a higher probability than the incorrect\none) was changed when considering the alternative\n10865\nsentence that was constructed to test for bias. We\nreport full results in Appendix, Table 4.\nLooking at the accuracies, we see that\nRoBERTa’s and DeBERTa’s scores are close to\n50% (i.e., chance) accuracy for both S1 and S2.\nBERT models differ considerably as they seem to\nsuffer from bias related to the order of the two\nCCs, but we can see that the average between them\nis also very close to chance. When we further\nlook at the decision ﬂips for each of the biases,\nwe ﬁnd that there is next to no bias related to the\nchoice of names (S4). However, we can see a\nlarge bias related to both the recency of the cor-\nrect answer (S2) and the choice of adjectives (S3).\nThe recency bias is strongest in the BERT models,\nwhich also accounts for the difference in accuracies.\nFor RoBERTa and DeBERTa models, the recency\nbias is small, but clearly present. In contrast, they\nexhibit far greater bias towards the choice of ad-\njective, even going as far as 99.66% of decisions\nﬂipped by changing the adjective for DeBERTa\nbase. This suggests that these models’ decisions\nabout which adjective to assign a higher probability\nis almost completely inﬂuenced by the choice of\nadjective, not the presence of the CC. Overall, we\nconclude that without calibration, all models seem\nto be highly susceptible to different combinations\nof bias, which completely obfuscate any underly-\ning knowledge of the CC, leading to an accuracy at\nchance level across the board.\nWe therefore turn to our calibration methods,\nevaluating them ﬁrst on their inﬂuence on the de-\ncision ﬂip scores, which directly show if we were\nable to reduce the impact of the different types of\nbias. We report these only for order and vocabulary\nbias as we found name bias to be inconsequen-\ntial. We report the complete results in Appendix,\nTables 4 and 5. We see that across all models, while\nall three calibration methods work to reduce some\nbias, none does so consistently across all models\nor types of bias. We report the impact of all calib-\nration methods on the ﬁnal accuracies of the three\nlargest models in Table 3. Even in cases where cal-\nibration has clearly reduced the decision ﬂip score,\nwe ﬁnd that the ﬁnal calibrated accuracy is still\nclose to 50%. This indicates that despite the ef-\nfort to retrieve any knowledge that the models have\nabout the CC, they are unable to perform clearly\nabove chance, and we have therefore found no evid-\nence that the investigated models understand and\ncan use the semantics of the CC.\nModel Test - S5 S6 S7\nBERTL\nS1 36.85 31.91 47.21 44.03\nS2 67.13 73.48 54.39 64.45\nS3 36.46 43.43 47.79 44.36\nRoBERTaL\nS1 55.72 58.37 65.08 69.53\nS2 68.01 74.53 62.73 77.76\nS3 55.36 52.02 65.28 69.23\nDeBERTaXXL\nS1 47.35 53.56 54.92 54.12\nS2 48.73 52.85 54.03 53.81\nS3 47.57 49.36 55.25 53.59\nTable 3: Effect of our three calibration methods com-\npared to no calibration, for the three largest models. We\nreport the accuracy scores for the base sentence (S1),\nrecency bias (S2), and vocabulary bias (S3). The results\nindicate that, even if we try to address bias through cal-\nibration, the models are unable to perform clearly above\nchance. We have therefore found no evidence that the\nmodels understand the semantics of the CC.\n4.2.1 Problem Analysis\nDifferent conclusions might be drawn as to why\nnone of these models have learned the semantics\nof the CC. They might not have seen enough ex-\namples of it in their training corpus to have formed\na general understanding. Given the many examples\nthat we were able to ﬁnd in C4, and the overall pos-\nitive results from the syntax section, we ﬁnd this to\nbe unlikely. Alternatively, it could be argued that\nmodels have never had a chance to learn what the\nCC means because they have never seen it applied,\nand do not have the same opportunities as humans\nto either interact with the speaker to clarify the\nmeaning or to make deductions using observations\nin the real world. This is in line with other con-\nsiderations about large PLMs acquiring advanced\nsemantics, even though it has for many phenomena\nbeen shown that pretraining is sufﬁcient (Radford\net al., 2019). Lastly, it might be possible that the\ntype of meaning representation required to solve\nthis task is beyond the current transformer-style ar-\nchitectures. Overall, our ﬁnding that PLMs do not\nlearn the semantics of the CC adds to the growing\nbody of evidence that complex semantics like neg-\nation (Kassner and Schütze, 2020) is still beyond\nstate-of-the-art PLMs.\n5 Related Work\n5.1 Construction Grammar in NLP\nCxG has only recently and very sparsely been\ninvestigated in neural network-based NLP. Tay-\nyar Madabushi et al. (2020) use a probe to show\n10866\nthat while a probe on top of BERT contextual em-\nbeddings is able to mostly correctly classify if two\nsentences contain instances of the same construc-\ntion, injecting this knowledge into the model by\nadding it to pretraining does not improve its per-\nformance. Our work differs from this study in that\nwe delve deeper into what it means to understand\na construction on a semantic level, and take care-\nful precautions to isolate the recognition of the\nconstruction at the syntax level from confounding\nfactors. Li et al. (2022) recreate the experiments\nof Bencini and Goldberg (2000) and Johnson and\nGoldberg (2013) on argument structure construc-\ntions, by creating artiﬁcial sentences with four ma-\njor argument structure types and a random combin-\nation of verbs, to investigate whether PLMs prefer\nsorting by construction or by main verb. Tseng\net al. (2022) choose items from a Chinese construc-\ntion list and investigate PLM’s predictions when\nmasking the open slots, the closed slots, or the en-\ntire construction. They ﬁnd that models ﬁnd closed\nslots easier to predict than open ones. Other com-\nputational studies about CxG have either focused\non automatically annotating constructions (Dunietz\net al., 2017) or on the creation and evaluation of\nautomatically built lists of constructions (Marques\nand Beuls, 2016; Dunn, 2019).\n5.2 Probing\nOur work also bears some similarity to recent work\nin generative grammar-based syntax probing of\nlarge PLMs in that we approximate the minimal\npairs-based probing framework similar to Wei et al.\n(2021), Marvin and Linzen (2018) or Goldberg\n(2019). However, as we are concerned with dif-\nferent phenomena and investigating them from a\ndifferent theoretical standpoint, the syntactic half\nof our work clearly differs.\nThe semantic half of our study is closest to re-\ncent work on designing challenging test cases for\nmodels such as Ribeiro et al. (2020), who design\nsome edge cases for which most PLMs fail. Des-\npite the different motivation, the outcome is very\nsimilar to a list of some particularly challenging\nconstructions.\n6 Conclusion\nWe have made a ﬁrst step towards a thorough in-\nvestigation of the compatibility of the paradigm of\nCxG and the syntactic and semantic capabilities\nexhibited by state-of-the-art large PLMs. For this,\nwe chose the English comparative correlative, one\nof the most well-studied constructions, and invest-\nigated if large PLMs have learned it, both syntactic-\nally and semantically. We found that even though\nthey are able to classify sentences as instances of\nthe construction even in difﬁcult circumstances,\nthey do not seem to be able to extract the mean-\ning it conveys and use it in context, indicating that\nwhile the syntactic aspect of the CC is captured\nin pretraining, the semantic aspect is not. We see\nthis an indication that major future work will be\nneeded to enable neural models to fully understand\nlanguage to the same degree as humans.\nLimitations\nAs our experimental setup requires signiﬁcant cus-\ntomisation with regards to the properties of the\nspeciﬁc construction we investigate, we are unable\nto consider other constructions or other languages\nin this work. We hope to be able to extend our\nexperiments in this direction in the future. Our ana-\nlysis is also limited—as all probing papers are—by\nthe necessary indirectness of the probing tasks: we\ncannot directly assess the model’s internal repres-\nentation of the CC, but only construct tasks that\nmight show it but are imperfect and potentially\naffected by external factors.\nAcknowledgements\nThis work was funded by the European Research\nCouncil (#740516). The second author was also\nsupported by the German Academic Scholarship\nFoundation. The third author was also supported by\nthe German Federal Ministry of Education and Re-\nsearch (BMBF, Grant No. 01IS18036A). We thank\nthe reviewers for their extremely helpful comments.\nWe are also very grateful to David Mortensen and\nLori Levin for helpful discussions and comments.\nReferences\nAnne Abeillé and Robert D Borsley. 2008. Comparative\ncorrelatives and parameters. Lingua, 118(8):1139–\n1157.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neural\nmachine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 861–872, Vancouver, Canada.\nAssociation for Computational Linguistics.\n10867\nGiulia ML Bencini and Adele E Goldberg. 2000. The\ncontribution of argument structure constructions to\nsentence meaning. Journal of Memory and Lan-\nguage, 43(4):640–651.\nLeonard Bloomﬁeld. 1933. Language. Holt, Rinehart\n& Winston, New York, NY .\nNoam Chomsky. 1988. Generative grammar. Studies in\nEnglish linguistics and literature.\nAlexis Conneau, German Kruszewski, Guillaume\nLample, Loïc Barrault, and Marco Baroni. 2018.\nWhat you can cram into a single $&!#* vector: Prob-\ning sentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nPeter W Culicover and Ray Jackendoff. 1999. The\nview from the periphery: The english comparative\ncorrelative. Linguistic inquiry, 30(4):543–571.\nDorottya Demszky, Devyani Sharma, Jonathan H. Clark,\nVinodkumar Prabhakaran, and Jacob Eisenstein.\n2021. Learning to recognize dialect features. In\nAnnual Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL HTL) 2021.\nMarcel Den Dikken. 2005. Comparative correlatives\ncomparatively. Linguistic Inquiry, 36(4):497–532.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dunietz, Lori Levin, and Jaime Carbonell. 2017.\nAutomatically tagging constructions of causation and\ntheir slot-ﬁllers. Transactions of the Association for\nComputational Linguistics, 5:117–133.\nJonathan Dunn. 2019. Frequency vs. association\nfor constraint selection in usage-based construction\ngrammar. In Proceedings of the Workshop on Cognit-\nive Modeling and Computational Linguistics, pages\n117–128, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nCharles J Fillmore. 1986. Varieties of conditional sen-\ntences. In Eastern States Conference on Linguistics,\nvolume 3, pages 163–182.\nCharles J. Fillmore. 1989. Grammatical construction:\nTheory and the familiar dichotomies. In Rainer\nDietrich and Carl F. Graumann, editors, Language\nprocessing in social context , pages 17–38. North-\nHolland, Amsterdam.\nCharles J. Fillmore, Paul Kay, and Mary C. O’Connor.\n1988. Regularity and idiomaticity in grammatical\nconstructions: The case of let alone. Language,\n64(3):501–538.\nAdele Goldberg. 1995. Constructions: A construction\ngrammar approach to argument structure. University\nof Chicago Press, Chicago, IL.\nAdele Goldberg. 2006. Constructions at work: The\nnature of generalization in language. Oxford Univer-\nsity Press, Oxford, UK.\nAdele E Goldberg. 2003. Constructions: A new the-\noretical approach to language. Trends in cognitive\nsciences, 7(5):219–224.\nYoav Goldberg. 2019. Assessing bert’s syntactic abilit-\nies. arXiv preprint arXiv:1901.05287.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nMartin Hilpert. 2006. A synchronic perspective on the\ngrammaticalization of Swedish future constructions.\nNordic Journal of Linguistics, 29(2):151–173.\nThomas Hoffmann, Jakob Horsch, and Thomas Brun-\nner. 2019. The more data, the better: A usage-based\naccount of the english comparative correlative con-\nstruction. Cognitive Linguistics, 30(1):1–36.\nThomas Hoffmann and Graeme Trousdale. 2013. The\nOxford handbook of construction grammar. Oxford\nUniversity Press.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 7038–7051, Online and Punta Cana, Domin-\nican Republic. Association for Computational Lin-\nguistics.\nMatthew Honnibal and Ines Montani. 2018. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nEiichi Iwasaki and Andrew Radford. 2009. Comparat-\nive correlatives in english: A minimalist-cartographic\nanalysis.\nMatt A Johnson and Adele E Goldberg. 2013. Evidence\nfor automatic accessing of constructional meaning:\nJabberwocky sentences prime associated verbs. Lan-\nguage and Cognitive Processes, 28(10):1439–1452.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\n10868\nPaul Kay and Charles J. Fillmore. 1999. Grammat-\nical constructions and linguistic generalizations: The\nWhat’s X doing Y?construction. Language, 75(1):1–\n33.\nGeorge Lakoff. 1987. Women, ﬁre, and dangerous\nthings: What categories reveal about the mind. Uni-\nversity of Chicago Press, Chicago, IL.\nRonald W. Langacker. 1987. Foundations of cognit-\nive grammar: Theoretical prerequisites . Stanford\nUniversity Press, Stanford, CA.\nBai Li, Zining Zhu, Guillaume Thomas, Frank Rudzicz,\nand Yang Xu. 2022. Neural reality of argument struc-\nture constructions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7410–7423,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTânia Marques and Katrien Beuls. 2016. Evaluation\nstrategies for computational construction grammars.\nIn Proceedings of COLING 2016, the 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 1137–1146, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heurist-\nics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nDenis Paperno, Germán Kruszewski, Angeliki Lazar-\nidou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repres-\nentations. In Annual Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies (NAACL\nHLT) 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Beha-\nvioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nHarish Tayyar Madabushi, Laurence Romain, Dagmar\nDivjak, and Petar Milin. 2020. CxGBERT: BERT\nmeets construction grammar. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 4020–4032, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R. Thomas McCoy, Najoung Kim, Benjamin\nvan Durme, Samuel R. Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations (ICLR) 7.\nYu-Hsiang Tseng, Cing-Fang Shih, Pin-Er Chen, Hsin-\nYu Chou, Mao-Chang Ku, and Shu-Kai Hsieh. 2022.\nCxLM: A construction and context-aware language\nmodel. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 6361–\n6369, Marseille, France. European Language Re-\nsources Association.\nIvan Vuli´c, Edoardo M. Ponti, Robert Litschko, Goran\nGlavaš, and Anna Korhonen. 2020. Probing pre-\ntrained language models for lexical semantics. In\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) 2020.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of lin-\nguistic minimal pairs for English. Transactions of\n10869\nthe Association for Computational Linguistics, 8:377–\n392.\nJason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick.\n2021. Frequency effects on syntactic rule learning\nin transformers. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 932–948, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\n10870\nAlgorithm 1 Context-Free Grammar for Artiﬁcial Data Creation Training Set\nS →SPOS | SNEG\nSPOS →POS1 PUNCT POS2 ’.’ | POS1 INSERT PUNCT POS2 ’.’\nSNEG →NEG1 PUNCT NEG2 ’.’ | NEG1 INSERT PUNCT NEG2 ’.’\nPUNCT →’,’ | ’;’ | ”\nCORE_POS →ADV_I ’the’ NUM NOUN VERB\nCORE_NEG →ADV_I NUM VERB ’the’ NOUN\nPOS_UPPER →’0 The’ CORE_POS\nPOS_LOWER →’0 the’ CORE_POS\nNEG_UPPER →’0 The’ CORE_NEG\nNEG_LOWER →’0 the’ CORE_NEG\nPOS1 →POS_UPPER | POS_UPPER ADD | START POS_LOWER | START POS_LOWER ADD\nPOS2 →POS_LOWER | POS_LOWER ADD\nNEG1 →NEG_UPPER | NEG_UPPER ADD | START NEG_LOWER | START NEG_LOWER ADD\nNEG2 →NEG_LOWER | NEG_LOWER ADD\nINSERT →INSERT1 | INSERT2\nINSERT2 →ADDITION BETWEEN_ADD_AND_SENT SENT\nPRON →’we’ | ’they’\nADDITION →’, and by the way ,’ | ’, and I want to add that’ | ’, and’ PRON ’just want to say that’ | ’,\nand then’ PRON ’said that’ | ’, and then’ PRON ’said that’\nSAY →’say’ | ’think’ | ’mean’ | ’believe’\nBETWEEN_ADD_AND_SENT →PRON SAY ’that’ | PRON SAY ’that’ | PRON SAY ’that’ | PRON\nSAY ’that’\nLOC_SENT →PRON ’said this in’ LOC ’too’\nLOC →CITY ’and’ LOC | CITY\nCITY →’Munich’ | ’Washington’ | ’Cologne’ | ’Prague’ | ’Istanbul’\nSENT →’this also holds in other cases’ | ’this is not always true’ | ’this is always true’ | ’this has only\nrecently been the case’ | ’this has not always been the case’ | ’this has always been the case’\nINSERT1 →’without stopping’ | ’without a break’ | ’without a pause’ | ’uninterrupted’ |\nSTART→’Nowadays ,’ | ’Nowadays’ | ’Therefore ,’ | ’Therefore’ | ’We can’ CANWORD ’that’ | ’It is’\nKNOWNWORD ’that’ | ’It follows that’ | ’Sometimes’ | ’Sometimes ,’ | ’It was recently announced that’\n| ’People have told me that’ | ’I recently read in a really interesting book that’ | ’I have recently read in\nan established , well-known newspaper that’ | ’It was reported in a special segment on TV today that’\nCANWORD →’say’ | ’surmise’ | ’accept’ | ’state’\nKNOWNWORD →’clear’ | ’known’ | ’accepted’ | ’obvious’\nADD →TEMP | UNDER1 | TEMP UNDER1 | UNDER1 TEMP\nADV_I →ADV | ADV ’and’ ADV\nTEMP →TEMP1 TEMP2\nTEMP1 →’before’ | ’after’ | ’during’\nTEMP2 →’the morning’ | ’the afternoon’ | ’the night’\nUNDER1 →’under the’ UNDER2\nUNDER2 →’bed’ | ’roof’ | ’sun’\nVERB →’push’ | ’attack’ | ’chase’ | ’beat’ | ’believe’ | ’boil’ | ’box’ | ’burn’ | ’call’ | ’date’\nNOUN →’lions’ | ’pandas’ | ’camels’ | ’pigs’ | ’horses’ | ’sheep’ | ’chickens’ | ’foxes’ | ’cows’ | ’deer’\nADV →’worse’ | ’earlier’ | ’slower’ | ’deeper’ | ’bigger’ | ’smaller’ | ’ﬂatter’ | ’weaker’ | ’stronger’ |\n’louder’\nNUM →’twelve’ | ’thirteen’ | ’fourteen’ | ’ﬁfteen’ | ’sixteen’ | ’seventeen’ | ’eighteen’ | ’nineteen’ |\n’twenty’ | ’twenty-one’\n10871\nAlgorithm 2 Context-Free Grammar for Artiﬁcial Data Creation Test Set\nS →SPOS | SNEG\nSPOS →POS1 PUNCT POS2 ’.’ | POS1 INSERT PUNCT POS2 ’.’\nSNEG →NEG1 PUNCT NEG2 ’.’ | NEG1 INSERT PUNCT NEG2 ’.’\nPUNCT →’,’ | ’;’ | ”\nCORE_POS →ADV_I ’the’ NUMNOUN VERB\nCORE_NEG →ADV_I NUM VERB ’the’NOUN\nPOS_UPPER →’0 The’ CORE_POS\nPOS_LOWER →’0 the’ CORE_POS\nNEG_UPPER →’0 The’ CORE_NEG\nNEG_LOWER →’0 the’ CORE_NEG\nPOS1 →POS_UPPER | POS_UPPER ADD | START POS_LOWER | START POS_LOWER ADD\nPOS2 →POS_LOWER | POS_LOWER ADD\nNEG1 →NEG_UPPER | NEG_UPPER ADD | START NEG_LOWER | START NEG_LOWER ADD\nNEG2 →NEG_LOWER | NEG_LOWER ADD\nINSERT →INSERT1 | INSERT2\nINSERT2 →ADDITION BETWEEN_ADD_AND_SENT SENT\nPRON →’I’ | ’you’\nADDITION →’, and by the way ,’ | ’, and I want to add that’ | ’, and’ PRON ’just want to say that’ | ’,\nand then’ PRON ’said that’ | ’, and then’ PRON ’said that’\nSAY →’say’ | ’think’ | ’mean’ | ’believe’\nBETWEEN_ADD_AND_SENT →PRON SAY ’that’ | PRON SAY ’that’ | PRON SAY ’that’ | PRON\nSAY ’that’\nLOC_SENT →PRON ’said this in’ LOC ’too’\nLOC →CITY ’and’ LOC | CITY\nCITY →’London’ | ’New York’ | ’Berlin’ | ’Madrid’ | ’Paris’\nSENT →’this also holds in other cases’ | ’this is not always true’ | ’this is always true’ | ’this has only\nrecently been the case’ | ’this has not always been the case’ | ’this has always been the case’\nINSERT1 →’without stopping’ | ’without a break’ | ’without a pause’ | ’uninterrupted’ |\nSTART→’Nowadays ,’ | ’Nowadays’ | ’Therefore ,’ | ’Therefore’ | ’We can’ CANWORD ’that’ | ’It is’\nKNOWNWORD ’that’ | ’It follows that’ | ’Sometimes’ | ’Sometimes ,’ | ’It was recently announced that’\n| ’People have told me that’ | ’I recently read in a really interesting book that’ | ’I have recently read in\nan established , well-known newspaper that’ | ’It was reported in a special segment on TV today that’\nCANWORD →’say’ | ’surmise’\nKNOWNWORD →’clear’ | ’known’\nADD →TEMP | UNDER1 | TEMP UNDER1 | UNDER1 TEMP\nADV_I →ADV | ADV ’and’ADV\nTEMP →TEMP1 TEMP2\nTEMP1 →’before’ | ’after’ | ’during’\nTEMP2 →’the day’ | ’the night’ | ’the evening’\nUNDER1 →’under the’ UNDER2\nUNDER2 →’bridge’ | ’stairs’ | ’tree’\nVERB →’slam’ | ’break’ | ’bleed’ | ’shake’ | ’smash’ | ’throw’ | ’strike’ | ’shoot’ | ’swallow’ | ’choke’\nNOUN →’cats’ | ’dogs’ | ’girls’ | ’boys’ | ’men’ | ’women’ | ’people’ | ’humans’ | ’mice’ | ’alligators’\nADV →’faster’ | ’quicker’ | ’harder’ | ’higher’ | ’later’ | ’longer’ | ’shorter’ | ’lower’ | ’wider’ | ’better’\nNUM →’two’ | ’three’ | ’four’ | ’ﬁve’ | ’six’ | ’seven’ | ’eight’ | ’nine’ | ’ten’ | ’eleven’\n10872\n0.0\n0.5\n1.0test acc.\nLength of the sentence Distance between the two start positions Distance between the two start positions Start Position of the CC\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n15 20 25 30 35 40 45 50\nlayers\n0.0\n0.5\n1.0test acc.\n10 15 20 25 30\nlayers\n10 15 20 25 30 35 40\nlayers\n0 1 2 3 4 5\nlayers\nFigure 2: Full results for BERTLARGE on artiﬁcial data. Columns indicate the variable that the training and test set\ncontrols for.\n10873\n0.0\n0.5\n1.0test acc.\nLength of the sentence Distance between the two start positions Distance between the two start positions Start Position of the CC\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n15 20 25 30 35 40 45 50\nlayers\n0.0\n0.5\n1.0test acc.\n10 15 20 25 30\nlayers\n10 15 20 25 30 35 40\nlayers\n0 1 2 3 4 5\nlayers\nFigure 3: Full results for RoBERTaLARGE on artiﬁcial data. Columns indicate the variable that the training and test\nset controls for.\n10874\n0.0\n0.5\n1.0test acc.\nLength of the sentence Distance between the two start positions Distance between the two start positions Start Position of the CC\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n15 20 25 30 35 40 45 50\nlayers\n0.0\n0.5\n1.0test acc.\n10 15 20 25 30\nlayers\n10 15 20 25 30 35 40\nlayers\n0 1 2 3 4 5\nlayers\nFigure 4: Full results for DeBERTaLARGE on artiﬁcial data. Columns indicate the variable that the training and test\nset controls for.\n10875\n0.0\n0.5\n1.0test acc.\nLength of the sentence Start Position of the CC Start Position of the CC Distance between the two start positions\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n15 20 25 30 35\nlayers\n0.0\n0.5\n1.0test acc.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nlayers\n10 15 20 25\nlayers\n4 6 8 10\nlayers\nFigure 5: Full results for BERTLARGE on corpus data. Columns indicate the variable that the training and test set\ncontrols for.\n10876\n0.0\n0.5\n1.0test acc.\nLength of the sentence Start Position of the CC Start Position of the CC Distance between the two start positions\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n15 20 25 30 35\nlayers\n0.0\n0.5\n1.0test acc.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nlayers\n10 15 20 25\nlayers\n4 6 8 10\nlayers\nFigure 6: Full results for RoBERTaLARGE on corpus data. Columns indicate the variable that the training and test set\ncontrols for.\n10877\n0.0\n0.5\n1.0test acc.\nLength of the sentence Start Position of the CC Start Position of the CC Distance between the two start positions\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n0.0\n0.5\n1.0test acc.\n15 20 25 30 35\nlayers\n0.0\n0.5\n1.0test acc.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nlayers\n10 15 20 25\nlayers\n4 6 8 10\nlayers\nFigure 7: Full results for DeBERTaLARGE on corpus data. Columns indicate the variable that the training and test set\ncontrols for.\n10878\nModel Test Scenario - S5 S6 S7\nBERTB\nS1 37.65% 37.62% 44.39% 47.9%\nS2 64.64% 62.79% 56.66% 55.41%\nS3 38.04% 44.78% 44.09% 48.29%\nBERTL\nS1 36.85% 31.91% 47.21% 44.03%\nS2 67.13% 73.48% 54.39% 64.45%\nS3 36.46% 43.43% 47.79% 44.36%\nRoBERTaB\nS1 61.6% 58.76% 42.13% 62.32%\nS2 52.85% 51.35% 71.33% 60.25%\nS3 62.21% 55.17% 43.04% 62.76%\nRoBERTaL\nS1 55.72% 58.37% 65.08% 69.53%\nS2 68.01% 74.53% 62.73% 77.76%\nS3 55.36% 52.02% 65.28% 69.23%\nDeBERTaB\nS1 49.72% 49.72% 49.86% 49.2%\nS2 49.81% 48.67% 49.7% 49.06%\nS3 50.28% 50.19% 49.97% 50.0%\nDeBERTaL\nS1 50.88% 49.86% 50.03% 49.39%\nS2 51.41% 48.09% 47.21% 48.04%\nS3 50.58% 49.94% 50.41% 49.42%\nDeBERTaXL\nS1 47.73% 45.08% 43.31% 43.67%\nS2 49.34% 46.27% 45.58% 41.74%\nS3 47.9% 49.14% 42.68% 45.58%\nDeBERTaXXL\nS1 47.35% 53.56% 54.92% 54.12%\nS2 48.73% 52.85% 54.03% 53.81%\nS3 47.57% 49.36% 55.25% 53.59%\nTable 4: Accuracies for the semantic probe with our three calibration methods compared to no calibration. We\nreport the average accuracy on the more difﬁcult sentences in terms of recency bias (S1),the easier ones (S2), and\nvocabulary bias (S3). Our calibration tecniques are short (S5), name (S6), and adjective (S7).\n10879\nModel Test Scenario - S5 S6 S7\nBERTB\nS2 26.99% 25.22% 14.75% 10.77%\nS3 75.69% 23.51% 86.33% 91.05%\nS4 2.71% - - -\nBERTL\nS2 30.44% 41.8% 13.37% 22.24%\nS3 73.31% 25.94% 88.65% 85.97%\nS4 2.32% - - -\nRoBERTaB\nS2 9.92% 8.67% 31.13% 10.86%\nS3 76.19% 22.04% 79.03% 74.75%\nS4 2.76% - - -\nRoBERTaL\nS2 14.34% 17.82% 15.94% 15.86%\nS3 79.48% 43.54% 64.78% 57.27%\nS4 4.34% - - -\nDeBERTaB\nS2 0.91% 11.77% 7.13% 10.8%\nS3 99.67% 56.44% 96.52% 94.94%\nS4 1.08% - - -\nDeBERTaL\nS2 7.04% 7.85% 14.31% 14.28%\nS3 94.83% 43.18% 85.75% 79.86%\nS4 2.24% - - -\nDeBERTaXL\nS2 5.47% 7.87% 13.48% 18.78%\nS3 89.28% 45.44% 68.48% 65.94%\nS4 2.51% - - -\nDeBERTaXXL\nS2 3.59% 3.09% 17.02% 17.21%\nS3 82.1% 79.06% 63.43% 59.81%\nS4 1.13% - - -\nTable 5: Decision ﬂip scores for the semantic probe with our three calibration methods compared to no calibration.\nWe report the percentage of decisions ﬂipped by changing from the base S1 to the sentences testing for recency bias\n(S2), vocabulary bias (S3), and name bias (S4). Our calibration tecniques are short (S5), name (S6), and adjective\n(S7).\n10880\nSentence Label\nNowadays , the bigger the eighteen sheep date , the louder and bigger the twelve horses beat\nunder the sun .\nPositive\nThe ﬂatter the fourteen lions push , the deeper and smaller the sixteen deer burn under the\nroof .\nPositive\nThe deeper the sixteen cows beat ; the ﬂatter and earlier the twenty cows attack . Positive\nTherefore , the worse the sixteen sheep believe after the morning without a pause , the smaller\nthe thirteen cows box after the morning under the sun .\nPositive\nThe ﬂatter the fourteen lions push , the deeper and smaller the sixteen deer burn under the\nroof .\nPositive\nSometimes , the worse and earlier seventeen believe the deer , and we just want to say that\nthey mean that this has always been the case , the ﬂatter twenty-one attack the foxes before\nthe afternoon under the roof .\nNegative\nNowadays , the smaller sixteen box the camels , and by the way , they mean that this is\nalways true ; the weaker thirteen date the cows .\nNegative\nTherefore the earlier and weaker fourteen chase the deer , the stronger and earlier thirteen\nboil the chickens during the night .\nNegative\nThe weaker and worse ﬁfteen box the lions during the morning under the sun , the worse\ntwenty push the cows .\nNegative\nIt follows that the worse twelve date the pigs without a break the ﬂatter and louder nineteen\ncall the pigs under the sun .\nNegative\nTable 6: Examples of artiﬁcial training data\nSentence Label\nThe harder and longer the three cats throw , the harder and shorter the ten dogs shake . Positive\nI have recently read in an established , well-known newspaper that the later the ten mice\nstrike ; the later and better the seven men smash under the tree during the night .\nPositive\nThe shorter the ten girls break without a pause ; the later the ten boys bleed under the tree . Positive\nIt was recently announced that the better and later the ﬁve women break ; the quicker the six\nmice smash under the tree during the evening .\nPositive\nThe faster the seven humans choke under the stairs after the evening , and I just want to say\nthat I think that this is not always true , the lower and higher the two boys swallow .\nPositive\nThe higher nine strike the women without a pause the shorter ten choke the girls . Negative\nWe can say that the longer and faster four strike the men under the stairs before the evening ,\nthe harder four throw the dogs after the day under the bridge .\nNegative\nThe quicker and higher eight bleed the people , and then I said that you believe that this also\nholds in other cases ; the longer seven break the girls after the night .\nNegative\nThe shorter four smash the people before the night , and by the way , you think that this is\nalways true ; the harder three bleed the people .\nNegative\nThe longer seven shoot the women without stopping , the faster ten strike the mice after the\nnight under the bridge .\nNegative\nTable 7: Examples of artiﬁcial test data\n10881\nSentence Label\n\" The higher up the nicer ! \" Positive\nShe thinks the more water she drinks the better her skin looks . Positive\nIt becomes an obsession lightly because the more ﬁsh you catch the higher your adrenaline\nﬂows .\nPositive\nIt is worth noting , however , that the more speciﬁc you are the better . Positive\nIn other words , the more videos you make the greater your audience reach . Positive\nSubtract the smaller from the larger . \" Negative\nThe way the older guys help out the younger guys is fantastic . Negative\nIn this procedure the lower lip is pulled ventrally to expose the lower incisors . Negative\nThe 5th bedroom is on the lower ﬂoor with easy access to the lower bath . Negative\nNote the distinctive bend of the larger vein adjacent to the smaller vein at the top . Negative\nTable 8: Examples of corpus data\n10882",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8079136610031128
    },
    {
      "name": "Natural language processing",
      "score": 0.7454258799552917
    },
    {
      "name": "Syntax",
      "score": 0.6713228821754456
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6481386423110962
    },
    {
      "name": "Correlative",
      "score": 0.5604255795478821
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5425719022750854
    },
    {
      "name": "Principle of compositionality",
      "score": 0.5022897720336914
    },
    {
      "name": "Grammar",
      "score": 0.4649903178215027
    },
    {
      "name": "Linguistics",
      "score": 0.4140568673610687
    },
    {
      "name": "Programming language",
      "score": 0.1708633303642273
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ],
  "cited_by": 12
}