{
  "title": "Foundation model for cancer imaging biomarkers",
  "url": "https://openalex.org/W4392850630",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Pai, Suraj",
      "affiliations": [
        "Maastricht University",
        "Dana-Farber Cancer Institute",
        "Dana-Farber Brigham Cancer Center",
        "Harvard University",
        "Brigham and Women's Hospital",
        "Mass General Brigham"
      ]
    },
    {
      "id": "https://openalex.org/A4306430592",
      "name": "Bontempi, Dennis",
      "affiliations": [
        "Maastricht University",
        "Harvard University",
        "Mass General Brigham",
        "Dana-Farber Cancer Institute",
        "Dana-Farber Brigham Cancer Center",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": null,
      "name": "Hadzic, Ibrahim",
      "affiliations": [
        "Dana-Farber Brigham Cancer Center",
        "Dana-Farber Cancer Institute",
        "Brigham and Women's Hospital",
        "Harvard University",
        "Maastricht University",
        "Mass General Brigham"
      ]
    },
    {
      "id": null,
      "name": "Prudente, Vasco",
      "affiliations": [
        "Dana-Farber Brigham Cancer Center",
        "Mass General Brigham",
        "Dana-Farber Cancer Institute",
        "Harvard University",
        "Brigham and Women's Hospital",
        "Maastricht University"
      ]
    },
    {
      "id": "https://openalex.org/A5111023280",
      "name": "Sokač Mateo",
      "affiliations": [
        "Aarhus University Hospital",
        "Aarhus University"
      ]
    },
    {
      "id": null,
      "name": "Chaunzwa, Tafadzwa L.",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham",
        "Dana-Farber Cancer Institute",
        "Dana-Farber Brigham Cancer Center",
        "Harvard University"
      ]
    },
    {
      "id": null,
      "name": "Bernatz, Simon",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Mass General Brigham",
        "Dana-Farber Brigham Cancer Center",
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2275639543",
      "name": "Hosny Ahmed",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Brigham and Women's Hospital",
        "Mass General Brigham",
        "Harvard University",
        "Dana-Farber Brigham Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A4361162613",
      "name": "Mak, Raymond H.",
      "affiliations": [
        "Mass General Brigham",
        "Maastricht University",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5101233168",
      "name": "Birkbak Nicolai J",
      "affiliations": [
        "Aarhus University",
        "Aarhus University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4227472630",
      "name": "Aerts Hugo J.W.L.",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Maastricht University",
        "Dana-Farber Cancer Institute",
        "Dana-Farber Brigham Cancer Center",
        "Harvard University",
        "Mass General Brigham"
      ]
    },
    {
      "id": null,
      "name": "Pai, Suraj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4306430592",
      "name": "Bontempi, Dennis",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hadzic, Ibrahim",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Prudente, Vasco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5111023280",
      "name": "Sokač Mateo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Chaunzwa, Tafadzwa L.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Bernatz, Simon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2275639543",
      "name": "Hosny Ahmed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4361162613",
      "name": "Mak, Raymond H.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5101233168",
      "name": "Birkbak Nicolai J",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227472630",
      "name": "Aerts Hugo J.W.L.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4366208220",
    "https://openalex.org/W4366598511",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W3032723274",
    "https://openalex.org/W2903150666",
    "https://openalex.org/W4292318865",
    "https://openalex.org/W6851949647",
    "https://openalex.org/W4367055910",
    "https://openalex.org/W2883683269",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W3158714121",
    "https://openalex.org/W2967302726",
    "https://openalex.org/W2124932867",
    "https://openalex.org/W2103004421",
    "https://openalex.org/W4292809017",
    "https://openalex.org/W2966710588",
    "https://openalex.org/W3120430728",
    "https://openalex.org/W4291023040",
    "https://openalex.org/W4310396525",
    "https://openalex.org/W3046015102",
    "https://openalex.org/W4286487934",
    "https://openalex.org/W2394599079",
    "https://openalex.org/W2888844929",
    "https://openalex.org/W3025872843",
    "https://openalex.org/W6778672394",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W4386697749",
    "https://openalex.org/W4379878309",
    "https://openalex.org/W4388092751",
    "https://openalex.org/W3160314846",
    "https://openalex.org/W6754669440",
    "https://openalex.org/W3202023533",
    "https://openalex.org/W2584017349",
    "https://openalex.org/W6977944434",
    "https://openalex.org/W6978096433",
    "https://openalex.org/W3175593095",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3011008306",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W4308610041",
    "https://openalex.org/W3168115304",
    "https://openalex.org/W6892480207",
    "https://openalex.org/W6969075882",
    "https://openalex.org/W6930213142",
    "https://openalex.org/W2979708377",
    "https://openalex.org/W4392850630",
    "https://openalex.org/W4392697024",
    "https://openalex.org/W4205772728",
    "https://openalex.org/W4377695098",
    "https://openalex.org/W948663339"
  ],
  "abstract": null,
  "full_text": "Nature Machine Intelligence | Volume 6 | March 2024 | 354–367 354\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-024-00807-9\nFoundation model for cancer imaging  \nbiomarkers\nSuraj Pai    1,2,3, Dennis Bontempi    1,2,3, Ibrahim Hadzic    1,2,3, Vasco Prudente    1,2,3, \nMateo Sokač4,5, Tafadzwa L. Chaunzwa1,3, Simon Bernatz    1,3, Ahmed Hosny1,3, \nRaymond H. Mak    1,2, Nicolai J. Birkbak    4,5 & Hugo J. W. L. Aerts    1,2,3,6 \nFoundation models in deep learning are characterized by a single \nlarge-scale model trained on vast amounts of data serving as the \nfoundation for various downstream tasks. Foundation models are \ngenerally trained using self-supervised learning and excel in reducing \nthe demand for training samples in downstream applications. This \nis especially important in medicine, where large labelled datasets \nare often scarce. Here, we developed a foundation model for cancer \nimaging biomarker discovery by training a convolutional encoder \nthrough self-supervised learning using a comprehensive dataset of \n11,467 radiographic lesions. The foundation model was evaluated in \ndistinct and clinically relevant applications of cancer imaging-based \nbiomarkers. We found that it facilitated better and more efficient learning \nof imaging biomarkers and yielded task-specific models that significantly \noutperformed c  on ve nt ional supervised and other state-of-the-art  \np re tr ai ned i mp lementations on downstream tasks, especially when \ntraining dataset sizes were very limited. Furthermore, the foundation \nmodel was more stable to input variations and showed strong associations \nwith underlying biology. Our results demonstrate the tremendous \npotential of foundation models in discovering new imaging biomarkers \nthat may extend to other clinical use cases and can accelerate the \nwidespread translation of imaging biomarkers into clinical settings.\nFoundation models, popularized recently due to their unprecedented \nperformance in language, vision and several other domains1, are large \ndeep-learning models trained on extensive amounts of unannotated \ndata serving as the base for a wide range of downstream tasks. In the \nfield of natural language processing, for example, foundation mod-\nels drive the successes of applications such as ChatGPT2, BERT3 and \nCLIP4. Similarly, foundation models, such as SimCLR5 and DINO6, have \nreported considerable success in computer vision applications.\nMedicine represents a vast potential for foundation models as \nlabelled data are scarce, while multimodal data, such as medical images, \nbiologic and clinical notes, are frequently collected in routine clinical \ncare7. Indeed, different applications of foundation models, such as \naugmented surgical procedures, bedside decision support, interactive \nradiology reports and note-taking, have been reported8.\nWhile many studies investigating imaging-based biomarkers incor-\nporate supervised deep-learning algorithms into their models9–11, they \nReceived: 9 June 2023\nAccepted: 8 February 2024\nPublished online: 15 March 2024\n Check for updates\n1Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard Medical School, Harvard Institutes of Medicine, Boston, MA, USA. \n2Radiology and Nuclear Medicine, CARIM and GROW, Maastricht University, Maastricht, the Netherlands. 3Department of Radiation Oncology, Brigham \nand Women’s Hospital, Dana-Farber Cancer Institute, Harvard Medical School, Boston, MA, USA. 4Department of Molecular Medicine, Aarhus University \nHospital, Aarhus, Denmark. 5Department of Clinical Medicine, Aarhus University, Aarhus, Denmark. 6Department of Radiology, Brigham and Women’s \nHospital, Dana-Farber Cancer Institute, Harvard Medical School, Boston, MA, USA.  e-mail: haerts@bwh.harvard.edu\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 355\nArticle https://doi.org/10.1038/s42256-024-00807-9\nb \nBiomarker\ndiscovery\nUse case 2: nodule \nmalignancy classi/f_ication\nUse case 3: classi/f_ication\nof NSCLC overall survival\nFoundation \nmodel\nDeepLesion\nn = 2,610\nDeepLesion\nn = 1,220\nTrain\nTune Tune\nTrain\nDeepLesion\nn = 1,221Test Test\nTune\nTrain\nTest\nTest\nLUNA16\nn = 169\nLUNA16\nn = 170\nHarvardRT\nn = 203\nHarvardRT\nn = 88\nLUNG1\nn = 420\nRADIO\nn = 133\nLUNA16\nn = 338\nUse case 1: lesion \nanatomical site classi/f_ication\nTechnical validation Diagnostic biomarker Prognostic biomarker\nFoundation\nmodel\nContrastive \nlearning\na \nVolumes \nwith lesions\nRandom volumes \nwithout lesions\nOutcome\nLesion\nvolume\nTrain linear classi/f_ier\non extracted features\nc \nd \nQuantitative analysis\nApproach 1: linear classi/f_ier on extracted features\nComparison against existing baseline approaches\nState-of-the-art pretrained modelsSupervised approaches\nApproach 2: transfer learning\nQualitative analysis Stability analysis Biological analysis\nROCs\nTransfer model weights\nLimited data \nevaluation\nClinical\nperformance\nFeature\nvisualization Test–retest Input\nperturbations\nGenomic\nassociations\nSaliency\nmaps\nOutcome\nTrain task-speci/f_ic\nmodel\nLesion\nvolume\nInitialize weights from\nfoundation model\nEf/f_iciency\nResource\nconstraints\nn = 11,467\nlesions\nOutcome\nRandom \ninitialization\nTransfer learning Med3D\n(MedicalNet)\nModels Genesis\nFig. 1 | General overview of the study. a, Foundation model pretraining: a \nfoundation model, specifically a deep convolutional encoder, was pretrained \nby contrasting volumes with and without lesions. b, Clinical application of the \nfoundation model: the foundation model was used to extract biomarkers and \nsubsequently evaluated on three classification tasks using diverse datasets. \nc, Foundation model implementation approaches: the foundation model was \nimplemented on specific use cases by (1) training a linear classifier on extracted \nfeatures or (2) through transfer learning by fine-tuning all model parameters. \nd, Performance evaluations: we compared the performance of the foundation \nmodel against supervised models, trained from random initialization and \ntransfer-learned, through fine-tuning, from a different task. Publicly available \nstate-of-the-art models, Med3D and Models Genesis, were also compared \nagainst our foundation model using identical implementation approaches. \nThe comparison was made through several criteria for the different use cases, \nincluding quantitative performance, stability, biological and efficiency analysis.\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367 356\nArticle https://doi.org/10.1038/s42256-024-00807-9\nare typically applied in scenarios where large datasets are available for \ntraining and testing. The quantity and quality of annotated data are \nstrongly linked to the robustness of deep-learning models. However, \naccess to large amounts of annotated data for specialized applications \nis often challenging and demands expertise, time and labour. In such \nscenarios, many investigators fall back on traditional handcrafted or \nengineered approaches based on defined mathematical and statisti-\ncal algorithms that analyse attributes such as the shape and texture \nof objects in images, which limit the scope of discovery. This caveat is \ncommonplace in many scenarios where insights from imaging-based \nbiomarkers have great potential in informing clinical care.\nFoundation models are generally pretrained using self-supervised \nlearning (SSL), a set of methods that leverage innate information avail-\nable within data by learning generalized, task-agnostic representations \nfrom large amounts of unannotated samples. Existing literature 12 \nhas suggested several strategies, such as image reconstruction, to \npretrain networks to learn these representations. Following pretrain-\ning, foundation models can be applied to task-specific problems, \nimproving generalization, especially in tasks with small datasets. The \nexpanding literature on SSL in medical imaging13 focuses primarily on \ntwo-dimensional (2D) images (X-ray, whole slide images, dermatology \nimages, fundus images and so on) for diagnostic applications. There is \nstill limited evidence investigating whether SSL can help train founda-\ntion models that learn general, robust and transferrable representa-\ntions that can act as imaging biomarkers, especially prognostic, for \ntasks of clinical relevance.\nIn this study, we investigated whether foundation models can \nimprove the development of deep-learning-based imaging biomark-\ners, especially in limited dataset-size scenarios. The foundation model, \na convolutional encoder, was self-supervised pretrained on 11,467 \ndiverse and annotated lesions identified on computed tomography \n(CT) imaging from 2,312 unique patients14 (Fig. 1a). The model was first \ntechnically validated by classifying lesion anatomical site (use case 1). \nSubsequently, it was applied to two clinically relevant applications: \ndeveloping a diagnostic biomarker that predicts the malignancy of lung \nnodules (use case 2) and a prognostic biomarker for non-small cell lung \ncancer (NSCLC) tumours (use case 3; Fig. 1b). We evaluated two distinct \nimplementation approaches of incorporating a pretrained foundation \nmodel into training pipelines for downstream tasks: using the founda-\ntion model as a feature extractor followed by a linear classifier and \nanother where the foundation model is fine-tuned through transfer \nlearning. The performance of the foundation model approaches was \ncompared to several existing baselines developed using supervised \napproaches and publicly available pretrained models. Our analysis \nexamines effective pretraining techniques, performance in limited \ndata scenarios, consistency in test–retest and inter-reader evaluations \nand the interpretability of findings through deep-learning attribution \nmethods along with their biological relevance to gene expression \ndata. Our results demonstrate the potential of foundation models in \ndiscovering new imaging biomarkers and their particular strength in \napplications with limited dataset sizes. This evidence may extend to \nother clinical use cases and imaging modalities and can accelerate \nthe widespread development and translation of imaging biomarkers \ninto clinical settings.\nResults\nWe developed a deep-learning foundation model using SSL and tested \nthe model’s performance in three distinct use cases. The study design \nand the pretraining process are outlined in Fig. 1. We trained a single \nfoundation model using a dataset with 11,467 annotated CT lesions \nidentified from 2,312 unique patients. Lesion findings were diverse \nand included multiple lesions, such as lung nodules, cysts and breast \nlesions, among numerous others. A task-agnostic contrastive learn-\ning strategy was used to pretrain the model on these lesion findings  \n(Fig. 1a). We showed the applicability of our pretrained foundation \nmodel to several tasks through the evaluation on three diverse clinical \napplications over five distinct datasets (Fig. 1b).\nPretraining strategy selection\nWe compared simple auto-encoder pretraining and several state-  \nof-the-art self-supervised pretraining approaches—namely SimCLR 5, \nSwAV15 and NNCLR16—against the modified version of SimCLR devel -\noped in our study (Methods). We evaluated pretraining strategies \non the technical validation use case of lesion anatomical site clas -\nsification by comparing linear classifiers trained on top of features \nextracted from each of the chosen strategies. We observed that our \nmodified SimCLR pretraining surpassed all others (P  < 0.001) in bal-\nanced accuracy (Fig. 2a) and mean average precision (mAP) (Fig. 2b), \nachieving a balanced accuracy of 0.779 (95% confidence interval (CI) \n0.750–0.810) and mAP = 0.847 (95% CI 0.750–0.810). As expected, \nthe second best-performing approach was SimCLR (balanced accu-\nracy 0.696 (95% CI 0.663–0.728); mAP = 0.779 (95% CI 0.749–0.811)). \nThe auto-encoder approach, previously popular for pretraining, \nperformed the worst compared to state-of-the-art contrastive \n \nSSL approaches.\nWhen limited data (50, 20 and 10%) was used for downstream \ntask training, our method demonstrated consistently improved per-\nformance. More importantly, it remained robust as evidenced by the \nsmallest decline in balanced accuracy and mAP of 9 and 12%, respec -\ntively, when reducing training data from 100 to 10%.\nLesion anatomical site classification (use case 1)\nAs a technical validation of the foundation model, we selected an \nin-distribution task (that is, sourced from the same cohort as the \nfoundation model pretraining) and developed classification models \nto predict anatomical sites on a training and tuning dataset totalling \n3,830 lesions (use case 1, Fig. 1b). On a held-out test set of 1,221 lesions, \nwe evaluated the performance of two different implementations of the \nfoundation model (Fig. 1c).\nWe found that foundation model implementations showed supe-\nriority over compared baseline methods (Fig. 2c,d). The fine-tuned \nfoundation model, denoted Foundation (fine-tuned), with a mAP of \n0.857 (95% CI 0.828–0.886) significantly (P < 0.05) outperformed all \nbaseline methods on mAP. With a balanced accuracy of 0.804 (95% CI \n0.775–0.835), a significant (P < 0.01) improvement in balanced accu-\nracy was also observed in comparison to all baselines except Med3D \n(fine-tuned), where the improvement was borderline (P = 0.059).\nFeatures extracted from the foundation model, Foundation \n(features), when linearly classified, showed significantly improved \nperformance in balanced accuracy and mAP over features extracted \nfrom Med3D (ref. 17) and Models Genesis 18 baseline methods. Mod -\nels fine-tuned using compute-intensive supervised deep-learning \nmethods—Supervised, Med3D (fine-tuned) and Models Genesis \n(fine-tuned)—did not significantly improve in balanced accuracy and \nmAP over the simple linear classification of foundation model features. \nMoreover, when considering only mAP, the simple linear classification \nsignificantly (P < 0.05) outperformed all other implementations. T o \nprovide deeper insight into feature separability that allows for such \nstrong linear classification performance, we attempted to explore \nvisual associations by interpreting projected features (Extended Data \nFig. 1). We observed that features from the pretrained foundation \nmodel provided consistently interpretable and well-separated clusters \nacross different settings. Modelling using features also provided a \ncomputational benefit, with both memory and time, over deep-learning \ntraining (Extended Data Fig. 2).\nThe performance advantage of the foundation model was even \nstronger in limited data scenarios (Fig. 2c,d). When we reduced train-\ning data to 50% (n = 2,526), 20% (n = 1,010) and 10% (n = 505), Founda-\ntion (features) significantly improved balanced accuracy and mAP \nover every baseline method. Foundation (fine-tuned) showed a larger \nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 357\nArticle https://doi.org/10.1038/s42256-024-00807-9\ndrop in performance and failed to improve significantly over baseline \nimplementations as training data were decreased (losing significance \nfrom 20% onward). Individual comparisons between each model can be \nfound in Extended Data Fig. 3. T o show the applicability of our approach \nacross the various anatomical sites, we provide a site-wise breakdown \nof performance in Extended Data Fig. 4.\nNodule malignancy prediction (use case 2)\nT o assess the generalizability of the foundation model, we chose an \nout-of-distribution task (that is, belonging to a cohort different from the \npretraining) and trained classification models to predict the malignancy \nof 507 lung nodules from the LUNA16 dataset (use case 2 in Fig. 1b). \nWe then evaluated performance on a separate test set of 170 nodules.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBA\nb \nc \nAuto-encoder SimCLR Our modi/f_ied SimCLRSwAV\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nd \ne f \nFull training dataset\nFull training dataset\nPercentage of training data\nPercentage of training data\nFull training dataset\nPercentage of training data\nFull training dataset\nPercentage of training data\nFull training dataset\nPercentage of training data\nFull training dataset\nPercentage of training data\na\nBA\n10%20%50%100%\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nmAP\n100%\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n10%20%50%\n10%20%50%100%\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFoundation (features)\nFoundation (fine-tuned)\nSupervised\nMed3D (features)\nMed3D (fine-tuned)\nModels Genesis (features)\nModels Genesis (fine-tuned)\nmAP\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n10%20%50%100%\n0.8\n0.9\n1.0\nAUC\n0.6\n0.7\n10%20%50%100%\n0.4\n0.5\nFoundation (features)\nFoundation (fine-tuned)\nSupervised\nSupervised (fine-tuned)\nMed3D (features)\nMed3D (fine-tuned)\nModels Genesis (features)\nModels Genesis (fine-tuned)\nmAP\n0.8\n0.9\n1.0\n10%20%50%100%\n0.4\n0.5\n0.6\n0.7\nNNCLR Auto-encoder SimCLR Our modi/f_ied SimCLRSwAV NNCLR\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.4\n0.5\n0.6\nFoundation (features)\nFoundation (fine-tuned)\nSupervised\nSupervised (fine-tuned)\nMed3D (features)\nMed3D (fine-tuned)\nModels Genesis (features)\nModels Genesis (fine-tuned)\nFoundation (features)\nFoundation (fine-tuned)\nSupervised\nMed3D (features)\nMed3D (fine-tuned)\nModels Genesis (features)\nModels Genesis (fine-tuned)\nFig. 2 | Comparison of pretraining strategies and performance evaluation for \nlesion anatomical site (use case 1) and nodule malignancy classification (use \ncase 2). We determined the best pretraining approach for our foundation model \non their ability to extract features that can be linearly classified to best predict \nlesion anatomical site. a,b, Different pretraining approaches were evaluated using \nbalanced accuracy (BA) (a) and mAP (b). c,d, After pretraining our foundation \nmodel using the best strategy, we adapted them to use case 1, lesion anatomical \nsite classification, and compared them against baseline methods using balanced \naccuracy (c) and mAP (d). We show performance on these metrics aggregated \nacross eight anatomical sites when trained on the full training set and when the \ntraining data percentage decreased to 50, 20 and 10%. e,f, Similar to use case 1,  \nwe implemented our foundation model on use case 2 and compared it  \nagainst baseline methods using the AUC-ROC (e) and mAP (f). Both metrics  \nwere computed when trained on the full and 50, 20 and 10% of the dataset. In  \ne,f, Models Genesis approaches are shaded and/or dotted as they were trained \non the same data split of LUNA16 and therefore do not present a fair comparison \ndue to overfitting. For use case 2, we also added a supervised model fine-tuned \nthrough transfer learning from use case 1. The error bars for a–f show 95% CIs of \nthe estimates and the bar centre shows the mean estimate of the displayed metric. \nThe estimates were computed by generating a bootstrap distribution with 1,000 \nresamples for datasets with n = 1,221 samples (a–d) and n = 170 samples (e,f).\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367 358\nArticle https://doi.org/10.1038/s42256-024-00807-9\nAUC\na \nc \nb \nd \nLUNG1\nRADIO\nFoundation (features)\nSupervised\nFoundation (fine-tuned)\nSupervised (fine-tuned)\nMed3D (features) Med3D (fine-tuned)\nModels Genesis (features) Models Genesis (fine-tuned)\nFoundation (features)\nSupervised\nFoundation (fine-tuned)\nSupervised (fine-tuned)\nMed3D (features) Med3D (fine-tuned)\nModels Genesis (features) Models Genesis (fine-tuned)\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nHigh risk\nLow risk\n0.4\n0.6\n0.8\n1.0\nSurvival probability\nn = 420\nlog-rank test, P = 0.088\nMed3D (features)\nHigh risk\nLow risk\n0.2\n0 1 2 3 4 5\nTime (years)\n0.4\n0.6\n0.8\n1.0\n0.2 High risk\nLow risk\n0 1 2\nTime (years)\nn = 420\nlog-rank test, P = 0.035\nModels Genesis (features)\n3 4 5\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSupervised (fine-tuned)\nTime (years)\n0.8\n1.0\nSurvival probability\n0 1 2 3 4 5\nTime (years)\n0.2\n0.4\n0.6\nn = 133\nlog-rank test, P = 0.009\nFoundation (features)\nHigh risk\nLow risk\n0.6\n0.8\n1.0\n0.4\n0 1 2 3 4 5\nTime (years)\n0.2\nn = 133\nlog-rank test, P = 0.141\nSupervised (fine-tuned)\nHigh risk\nLow risk\n0.6\n0.8\n1.0\nSurvival probability\nn = 133\nlog-rank test, P = 0.351\nMed3D (fine-tuned)\n0 1 2 3 4 5\nTime (years)\n0.2\n0.4\nHigh risk\nLow risk\n1.0 n = 133\nlog-rank test, P = 0.972\nModels Genesis (fine-tuned)\n0 1 2 3 4 5\nTime (years)\n0.2\n0.4\n0.6\n0.8\nHigh risk\nLow risk\n0 1\n0.2\n0.4\n0.6\n0.8\n1.0\nSurvival probability\nHigh risk\nLow risk\n2 3 4 5\nTime (years)\nn = 420\nlog-rank test, P < 0.001\nFoundation (features)\nAUC\n1 2 3 4 5\nn = 420\nlog-rank test, P = 0.032\nFig. 3 | Performance of the foundation model for prognostication of NSCLC \ntumours (use case 3). We compared the foundation model implementation \napproaches against baseline methods using the AUC. a,c, Each implementation \nwas adapted for 2 year overall survival classification, trained on the HarvardRT \ndataset and evaluated on LUNG1 (a) and RADIO (c) datasets. b,d, Kaplan–Meier \ncurves for groups stratified by model predictions from the best performing \namong implementation approaches are shown for LUNG1 (b) and RADIO (d). T o \nensure a fair comparison, we calculated the threshold to split the risk groups on \nthe HarvardRT tuning set for each implementation. Kaplan–Meier curves for all \napproaches can be found in Extended Data Fig. 6. The 95% CI of the estimates is \nshown by error bars in a,c and error bands in b,d. The measure of centre for the \nerror bars is the mean estimate of AUC and the measure of centre for the error \nbands is the Kaplan–Meier estimate of the survival function. The estimates for \nthe bar plots in a and c have been computed through a bootstrap distribution \nwith 1,000 resamples using dataset sizes of n = 420 and n = 133, respectively.\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 359\nArticle https://doi.org/10.1038/s42256-024-00807-9\nThe approach of fine-tuning the foundation model, Founda -\ntion (fine-tuned), with an area under the curve (AUC) = 0.944 (95% CI \n0.907–0.972) and mAP = 0.953 (95% CI 0.915–0.979) resulted in signifi-\ncant (P < 0.01) superiority over most of the baseline implementations  \n(Fig. 2e,f). The implementation Med3D (fine-tuned), with AUC = 0.917 \n(95% CI 0.871–0.957) and mAP = 0.9307 (95% CI 0.888–0.964), performs \nslightly worse than our model, but this is not significant (P = 0.134). For \nfeatures extracted from our foundation model, similar to use case 1, our \nimplementation surpasses (P < 0.001) baseline feature-based implemen-\ntations. Notably, none of the deep-learning fine-tuned baselines signifi-\ncantly improve over linear classification. The baseline Models Genesis \nimplementation was excluded in this analysis as this model was pretrained \non the same dataset and, therefore, does not indicate a fair comparison.\nAgain, the Foundation (features) approach shows improved per-\nformance in reduced data analyses, dominating all baselines (P < 0.05) \non 50% (n = 254), 20% (n = 101) and 10% (n = 51) training data. Founda-\ntion (fine-tuned) shows superior performance over all baselines at 50% \nbut shows large drops in performance from a 20% reduction onward. \nMed3D (fine-tuned), which performed well on the full dataset, shows \na large drop from 50% data reduction onward. Detailed comparisons \ncan be found in Extended Data Fig. 5a.\nNSCLC prognostication (use case 3)\nNext, we evaluated the efficacy of our foundation model in another \nclinically relevant use case to capture prognostic radiographic pheno-\ntypes of NSCLC tumours. We trained and tuned prognostication models \nusing data from the HarvardRT (n = 291) cohort to predict 2 year overall \nsurvival after treatment and then compared the performance of the \nfoundation model and baseline implementations on two independ-\nent testing cohorts, LUNG1 (NSCLC-Radiomics) (n = 420) and RADIO \n(NSCLC-Radiogenomics) (n = 133) (use case 3 in Fig. 1b).\nIn the LUNG1 cohort, features extracted from the foundation \nmodel followed by a linear classifier, Foundation (features), exceeded \nall baseline performances with an AUC of 0.638 (95% CI 0.584–0.692) \n(Fig. 3a). All comparisons were significant (P < 0.05) except for Med3D \n(fine-tuned), where borderline significance was observed (P = 0.053). \nDeep-learning-based implementations in the baseline comparisons \ndid not perform strongly on this use case. In addition to AUC, we plot-\nted Kaplan–Meier estimates for the top-performing implementations  \n(Fig. 3b). Foundation (features) provided the best stratification \n(P < 0.001), indicating its ability to determine appropriate risk groups \non the basis of mortality. More detailed analyses can be found in \nExtended Data Figs. 5b and 6.\n0\n0 2 4 6 8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTest\nRetest\nc e d \nb \n0.25\n0.30\n0.35\n0.40MSE\n−15 −10 −5 0 5 10 15\n−15\n−10\n−5\n0\n5\n10\n15\n50\n100\n150\n200\nCount\nx axis perturbation\ny axis perturbation\n0.80\n0.85\n0.90\n0.95\n1.00\nFoundation (features)\nSupervised (fine-tuned)\nICC\na\n0 2 4 6\n0\n1\n2\n3\n4\n5\n6\n7\nTest\nRetest\n0.10\n0.15\n0.20\nµ = 0.13, σ = 0.003 µ = 0.22, σ = 0.071\n0.64\n0.58\n0.60\n0.62\nAUC\n0.50\n0.52\n0.54\n0.56\nµ = 0.64, σ = 0.004 µ = 0.57, σ = 0.014\nFig. 4 | Input and test–retest stability of the foundation model. We analysed \ninput stability on the LUNG1 dataset and test–retest robustness on the \nRIDER dataset by comparing between Foundation (features) and Supervised \n(fine-tuned) (best performing, overall for LUNG1 and RADIO use cases). a, We \ncompared ICC between test–retest model predictions on the RIDER dataset \n(n = 26). b, We further visualize the linearity between flattened features \nextracted from test and retest scans on the RIDER dataset. c, We show the \nsampling distribution for input perturbations that are used to simulate inter-\nreader variability. We perturbed across x, y and z axes, although the distribution \nis shown only for x and y perturbations for simplicity. d, We compared the \nstability of the features across models using mean-squared error (MSE) between \nfeature values across all the trials. e, We demonstrated the prognostic stability of \nmodels when the input seed point is perturbed, estimated through calculating \nAUC for 2-year survival from model predictions. The error bars in a represent \nthe 95% CI of the estimates and the bar centre is the mean estimate. For the box \nplots (d,e), the centre line shows the median, the box edges represent first and \nthird quartiles and the whiskers extend to 1.5 times the inter-quartile range. \nThe distribution of the data is shown alongside the box plot. Each AUC and MSE \nmeasure in the box plots (d,e) have been computed on a dataset with n = 422 \nsamples and the distribution of the measures are obtained from 50 independent \nperturbation trials.\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367 360\nArticle https://doi.org/10.1038/s42256-024-00807-9\nFor the RADIO cohort, Foundation (features) shows the best per-\nformance with an AUC of 0.653 (95% CI 0.532–0.771). Similar to the \nLUNG1 cohort, deep-learning implementations did not demonstrate \nsuperior performance (Fig. 3c ). Due to the small sample size, none \nof the models showed significant differences from the rest (P > 0.05) \nexcept for the Foundation (features) improving over the Supervised \nmodel, which had near-random performance (AUC = 0.520). Kaplan–\nMeier analysis showed that the sole model that offered significant \nstratification was the Foundation (features) with P = 0.009 (Fig. 3d).\nStability of the foundation model\nWe evaluated the stability of our foundation model through a test–\nretest scenario and an inter-reader variability analysis. We used scans \nfrom 26 patients from the RIDER dataset19, routinely used for test–\nretest robustness analysis in tumour imaging19–21. We found that predic-\ntions from the overall best-performing models on LUNG1 and RADIO: \nFoundation (features) and Supervised (fine-tuned) had high stability \nwith intraclass correlation coefficient (ICC) values of 0.984 and 0.966, \nrespectively. Furthermore, the test–retest features for both networks \nwere strongly correlated (Fig. 4a,b).\nT o evaluate stability against inter-reader variability, we used \nthe LUNG1 dataset and perturbed the input seed point to extract the \nthree-dimensional (3D) volume, simulating variations among human \nreaders (Fig. 4c). We found that the Foundation (features) had signifi-\ncantly (P < 0.05) higher stability against simulated inter-reader vari -\nations in feature differences and prediction performance (Fig. 4d,e).\nSaliency maps for fine-tuned foundation models\nT o gain insight into regions of the input volumes that contribute to a \ngiven prediction, we used gradient-based saliency maps for Founda-\ntion (fine-tuned) on three selected use cases (as depicted in Fig. 5).\nOur analysis revealed that for each use case, the focus was pri -\nmarily around tissues within or in proximity to the tumour, which is \nconsistent with research demonstrating the tumour microenviron -\nment’s influence on cancer development22 and prognosis. Specifically, \nin use case 1 (Fig. 5a), the focus was mainly on areas surrounding the \nlesions, such as the parenchyma and bone regions in the lung and the \ntrachea in mediastinal lesions. For use case 2 (Fig. 5b ), tissues of the \nnodule were highlighted, avoiding high-density bone regions. Use case \n3 (Fig. 5c) primarily attributed areas surrounding the centre of mass of \nthe tumour, with some contribution from high-density bone regions. \nOverall, these findings indicated that the areas that contribute to the \nnetworks’ predictions varied in accordance with the specific use case, \nwith the tumour and surrounding tissues playing a pivotal role.\nUnderlying biological basis of the foundation model\nFinally, we investigated the biological basis of our foundation model \nby analysing gene expression data associated with model predic -\ntions for 130 participants from the RADIO dataset. T o identify \nrelevant genes, we selected the top 500 genes and performed a cor-\nrelation analysis, comparing Foundation (features) and Supervised \n(fine-tuned) predictions with gene expression profiles. We found that \nabsolute correlation coefficients between gene expression profiles \nand model predictions were significantly higher (P  = 0.008) for the \nfoundation model, indicating a stronger association with underlying \ntumour biology (Fig. 6a).\nAdditionally, we examined the genes associated with these mod -\nels through a gene-set enrichment analysis (genes with a correlation \ncoefficient >0.1). Our analysis revealed that the foundation model \nshowed an enrichment pattern of immune-associated pathways, \nincluding interferon signalling, interferon gamma signalling, major \nhistocompatibility complex class II antigen presentation and PD-1 \nsignalling. Conversely, while the supervised model did show enrich -\nment of individual pathways, no identifiable pattern was observed \n(Fig. 6b).\nDiscussion\nIn this study, we demonstrated that our foundation model, trained \nusing self-supervised contrastive learning, provided robust perfor-\nmance in predicting anatomical site, malignancy and prognosis across \nthree different use cases in four cohorts. Several studies23–25 have dem-\nonstrated the efficacy of SSL in medicine where only limited data might \nbe available for training deep-learning networks. Our findings comple-\nment and extend this for identifying reliable imaging biomarkers for \ncancer-associated use cases. We showed that our foundation model \nprovided superior performance for anatomical lesion site classification \non average and across individual anatomical sites, even when very few \ntraining samples were available for that site. Similarly, for malignancy \nprediction, our model outperformed all other baseline approaches. In \nboth these use cases, the benefit of our model was especially evident \nin limited data scenarios. Modelling using features extracted from the \nfoundation model was the most robust across these use cases when \nsubjected to drops in training data, offering stable performance even \nwhen data sizes were considerably reduced, for example, using only \n51 samples in use case 2. Using these features provided the best perfor-\nmance on small cohorts in predicting prognosis and also demonstrated \nsignificant stratification of patients by their associated risk for each of \nthe LUNG1 and RADIO cohorts (P < 0.01). Feature-based implementa-\ntions were also computationally efficient when considering both time \nand memory. Additionally, features and predictions from the founda-\ntion model features were found to be highly stable against inter-reader \nand test–retest variations. Regarding interpretability, we observed \nthat models focused on varying regions of the tumour and surround-\ning tissue relevant to the associated use case. T o gain insight into the \nunderlying biological associations of these features, RNA sequencing \nanalysis combined with imaging data showed that these features cor-\nrelated with immune-associated pathways.\nImage-biomarker studies for predicting endpoints, such as overall \nsurvival on small cohorts, largely rely on statistical feature extraction \n(engineered radiomics) and classical machine learning-based mod -\nelling. These require precise 3D segmentations for feature extrac -\ntion, increasing the annotation burden of these studies. Moreover, \nthese statistical features are affected by several confounders, such \nas inter-reader variability in segmentations 26 and acquisition set -\ntings of the scanners27, limiting their applicability in diverse settings. \nDeep-learning methods, in comparison, are robust to differences \nin acquisition and segmentation variability and provide improved \nperformance10. Surveying diagnostic biomarker studies, Shen et al.28 \ntrained a simple deep convolutional network to extract features from \nlung nodules followed by malignancy classification using a support \nvector machine, possibly one of the first convolutional approaches \nfor this use case. In a subsequent study, Shen et al. 29 proposed a new \nmulti-crop convolutional neural networks (CNN) architecture and \ndemonstrated improved performance over auto-encoder-based pre-\ntraining and radiomic feature-based training. Kumar et al. 30 identi-\nfied radiomic sequences through deep convolutional encoders to \ndetermine lung nodule malignancy. These developed approaches \nwere specific to nodule malignancy classification, and it is difficult \nto determine their transferability to other use cases. By contrast, \nour approach is generalizable to multiple use cases, and for nodule \nmalignancy, we obtain high performance using significantly lesser \ntraining data, only 338 nodules (due to our more stringent exclusion \ncriteria). Considering prognostic biomarkers, Hosny et al.\n10 trained a \ndeep-learning model for lung cancer prognostication using several \nmulti-institutional cohorts and demonstrated strong performance \nover traditional radiomics. Haarburger et al.31 presented a deep con-\nvolutional network-based approach to predict survival endpoints on \nthe LUNG1 dataset. Mukherjee et al. 32 developed a shallow CNN for \npredicting overall survival by round-robin training on four different \ncohorts and additionally observed that their model transferred well to \npredicting nodule malignancy. A general trend observed across these \nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 361\nArticle https://doi.org/10.1038/s42256-024-00807-9\nstudies was that the performance of deep-learning models was more \nrobust when larger and multi-institutional cohorts were available for \ntraining, and validation was generally performed on smaller cohorts. \nA demonstrated strength of our approach is that training on smaller \ncohorts performs well in larger validation cohorts.\nAdvances in deep learning, such as SSL, have translated well \nto medical imaging use cases, with several studies incorporating \npretraining for improved performance 23,25,33,34. More recently, foun-\ndation models have become popular for their ability to learn general \nconcepts adaptable to various tasks. Zhou et al. 35 proposed a foun -\ndation model where a visual transformer was trained on 1.6 million \nretinal images and validated on ocular disease use cases. Azizi et al.36 \npresented distinct foundation models for five domains trained in a \nmulti-step approach with different amounts of pretraining data for \neach (ranging from 8,000 to 2.2 million images). Azad et al. 37 con-\nducted an extensive review, highlighting the development of diverse \nfoundation models, both generalist and more specific, across several \nmedical imaging domains.\nDeveloping a reliable and reproducible foundation model \nfor a specific domain involves the consideration of several design \nchoices. Cole et al. 38 present empirical observations on the quantity \nof pretraining data, the impact of the pretraining domain, the qual-\nity of data and task difficulty when using contrastive pretraining \nmethods. They show a saturation point associated with pretraining \ndataset size and diminishing returns beyond this point. This point \nlargely depends on the nature and sizes of training data in the down-\nstream task. In our study, we pretrained on 11,467 lesion volumes and \nrandomly sampled volumes, from 5,513 unique CT scans, leveraging \nnot only one of the largest lesion-specific datasets but also one of the \nlargest pretraining 3D CT datasets. The only other study we know \nthat uses more data is by Ghesu et al. 25 where 24,000 CT scans are \nused for pretraining. Cole et al. 38 also showed that pretraining using \nin-domain data, semantically connected to the downstream task, \nhas a huge impact besides scale of the pretraining data. Azizi et al. 36 \nalso observed improvements when incorporating in-domain data, \neven when the number of samples used was smaller. In the context \nof our study, our pretraining process is the closest to the domain of \noncological image biomarkers; as a result, improvements over more \nout-of-domain pretraining methods are seen.\nDespite the strengths outlined in our study, we recognize several \nlimitations that need to be addressed before the clinical applicabil -\nity of our foundation model. First, the retrospective nature of this \nstudy constrains our ability to assess the real-world practicality of \nmodel-based biomarkers. Second, evaluating the model’s reliability \nand reproducibility across diverse demographic groups and various \nbiomarker discovery tasks is crucial to ensure broad applicability. \nThis includes examining how well the model handles distribution \nshifts between the pretraining and application phases. Another key \nconsideration is investigating whether a larger volume of pretraining \ndata could enhance model performance, particularly for complex \ntasks. Additionally, since imaging features alone may not suffice for \ncomprehensive clinical decision making, integrating clinical data as \ncovariates could notably improve the model’s effectiveness. Third, \na significant challenge with deep-learning models, including ours, is \na \nb \nc \nOriginal Contour overlay Saliency heatmap Original Contour overlay Saliency heatmap\nSample 1\nOriginal Contour overlay Saliency heatmap Original Contour overlay Saliency heatmap\nSample 2\nSample 1\nOriginal Contour overlay Saliency heatmap Original Contour overlay Saliency heatmap\nSample 2\nSample 1 Sample 2\nFig. 5 | Saliency maps for fine-tuned foundation models. a–c, We generated \ngradient-based saliency maps for each of the fine-tuned foundation models \nfrom use cases 1 (a), 2 (b) and 3 (c) using smooth guided back-propagation and \nvisualized salient regions on two samples from corresponding test datasets. The \nfirst and fourth columns show the central axial slice (50 × 50 mm) of the volume \nprovided as input to the model. The second and fifth columns show isolines for \nsaliency contours overlayed on the image. Finally, the third and sixth columns \nshow saliency maps highlighting areas of the input volume that contribute the \nmost to a change in the output prediction.\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367 362\nArticle https://doi.org/10.1038/s42256-024-00807-9\ntheir ‘black box’ nature, which limits interpretability and explainability. \nAlthough we used established saliency attribution methods to interpret \nour model’s predictions, the technical limitations39,40 of these methods \nmay restrict the applicability of the insights gained. Furthermore, our \ninitial biological association analysis, aimed at explaining the model’s \ndecisions, is preliminary and requires more rigorous investigation for \na concrete understanding.\nIn conclusion, our foundation model offers a powerful and reliable \nframework for discovering cancer imaging biomarkers, especially \nin small datasets. Furthermore, it surpasses current deep-learning \ntechniques in various tasks while fitting conveniently into existing \nradiomic research methods. This approach can potentially uncover \nnew biomarkers contributing to research and medical practice. We \nshare our foundation model and reproducible workflows so that more \nstudies can investigate our methods, determine their generalizability \nand incorporate them into their research studies.\nMethods\nStudy population\nWe use a total of five distinct datasets: four of which are publicly acces-\nsible and one is an internal dataset. These were acquired from various \ninstitutions as components of separate investigations (Extended \nData Fig. 9).\nDeepLesion14 is a dataset comprising 32,735 lesions from 10,594 \nstudies of 4,427 unique patients collected over two decades from the \nNational Institute of Health Clinical Center PACS server. Various lesions, \nincluding kidney, bone and liver lesions, as well as enlarged lymph \nnodes and lung nodules, are annotated. The lesions are identified \nFoundation (features)\nSupervised (fine-tuned)\nBest performing implementations\nFoundation (features)\nGeneRatio\n0.04 0.05 0.06 0.07 0.08\nInterferon signalling\nInterferon gamma signalling\nMHC class II\nantigen presentation\nPhosphorylation of CD3 and\nTCR zeta chains\nSurfactant metabolism\nTranslocation of ZAP-70 to\nimmunological synapse\nPD-1 signalling\nGeneration of second\nmessenger molecules\nSupervised (fine-tuned)\nAbsolute correlation coef/f_icients\nn = 130,\nWilcoxon, P = 0.0087\na \nb\nCount\nAdjusted P\n0.04\n0.03\n0.02\n0.01\n5 Count\n14.00\n14.25\n14.50\n14.75\n15.00\n6\n7\n8\n9\n10\nAdjusted P\n1.6 × 10\n–10\n1.2 × 10\n–10\n8.0 × 10\n–11\n4.0 × 10\n–11\nGeneRatio\n0.1350 0.1375 0.1400 0.1425\nPeptide chain elongation\nEukaryotic translation\nelongation\nInfluenza viral RNA\ntranscription and replication\nViral mRNA translation\nSelenocysteine synthesis\nEukaryotic translation\ntermination\nNonsense mediated decay\nindependent of the\nexon junction complex\nFormation of a pool of free 40S\nsubunits\nResponse of EIF2AK4 (GCN2)\nto amino acid de/f_iciency\nL13a-mediated translational\nsilencing of ceruloplasmin\nexpression\n0\n0.2\n0.4\nFig. 6 | Underlying biological basis of the foundation model. We compared \nthe Foundation (features) and Supervised (fine-tuned) (best-performing models \non the RADIO dataset) model predictions with gene expression profiles. a, Box \nplot of absolute correlation coefficients (y axis) of selected genes against model \npredictions (x axis) across n = 130 samples. Statistical significance between \nthe two groups is determined through a two-sided Wilcoxon signed rank test. \nb, Gene-set enrichment analysis of genes with correlation coefficient greater \nthan 0.1 revealed for the foundation (left) and supervised model predictions \n(right). Genetic pathways are shown on the y axis, and the gene ratio is shown on \nthe x axis. Gene count and adjusted P values are also shown in the legend. False \ndiscovery rates are used to adjust the P values for multiple comparisons. The box \nplots in a are defined by the median as the centre line, first and third quartiles as \nthe box edges and 1.5 times the inter-quartile range as the whiskers. MHC, major \nhistocompatibility complex.\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 363\nArticle https://doi.org/10.1038/s42256-024-00807-9\nthrough radiologist bookmarked RECIST (Response Evaluation Crite-\nria in Solid Tumors, National Cancer Institute, USA) diameters across \n32,120 CT slices. In our study, we excluded CT scans with a slice thick-\nness exceeding 3 mm, resulting in 16,518 remaining lesions. Subse -\nquently, we divided this into 11,467 unlabelled lesions for contrastive \ntraining and 5,051 labelled lesions for anatomical site classification. The \nunlabelled lesions were sourced from 5,513 unique CT scans across 2,312 \npatients. Labelled lesions chosen for the anatomical site classification \nuse cases were excluded from the pretraining data to avoid potential \ndata leakage between pretraining and evaluation tasks. Despite not \nusing class labels during pretraining, we consciously decided to prevent \noverlapping lesions from being seen at this stage to ensure unbiased \nevaluation. The labelled lesion data were further separated randomly \ninto training, tuning and testing sets, containing 2,610, 1,220 and 1,221 \nlesions, respectively.\nLUNA16 (ref. 41) is a curated version of the LIDC-IDRI dataset of \n888 diagnostic and lung cancer screening thoracic CT scans obtained \nfrom seven academic centres and eight medical imaging companies \ncomprising 1,186 nodules. The nodules are accompanied by annota -\ntions agreed on by at least three out of four radiologists. Alongside \nnodule location annotations, radiologists also noted various observed \nattributes such as internal composition, calcification, malignancy, \nsuspiciousness and more. For our evaluation, we chose nodules with \nat least one indication of malignancy suspicion, totalling 677. We ran-\ndomly picked 338 nodules for training and 169 for tuning the malig -\nnancy prediction networks. The final 170 nodules were used to assess \nthe networks’ performance.\nHarvardRT10 is a cohort of 317 patients with stage I–IIIB NSCLC \ntreated with radiation therapy at the Dana-Farber Cancer Institute and \nBrigham and Women’s Hospital, Boston, MA, USA, between 2001 and \n2015. All CT scans for this cohort were acquired with and without intra-\nvenous contrast on the GE Lightspeed CT scanner. The primary tumour \nsite was contoured by radiation oncologists using soft tissue and lung \nwindows. A subset of 291 patients with a follow-up of 2 years was selected \nfor this study. We used 203 tumour volumes for training the prognostica-\ntion networks and the remaining 88 tumour volumes for tuning.\nLUNG1 (ref. 42) is a cohort of 422 patients with stage I–IIIB NSCLC \ntreated with radiation therapy at MAASTRO Clinic, Maastricht, the \nNetherlands. Fluorodeoxyglucose positron emission tomography \n(PET)-CT scans were acquired with or without contrast on the Siemens \nBiograph Scanner. Radiation oncologists used PET and CT images to \ndelineate the gross tumour volume. For our study, we selected CT scans \nof 420 patients (right-censored for 2-year survival) with annotated \nprimary gross tumour volumes and used these as an independent test \nset for prognostication networks.\nThe RADIO43 dataset is a collection of 211 patients with NSCLC \nstage I–IV recruited between 2008 and 2012 who were referred for \nsurgical treatment and underwent preoperative CT and PET-CT scans. \nThese patients were recruited from the Stanford University School of \nMedicine and the Palo Alto Veterans Affairs Healthcare System. Scans \nwere obtained using various scanners and protocols depending on \nthe institution and physician. A subset of 144 patients in the cohort \nhave available tumour segmentations independently reviewed by \ntwo thoracic radiologists. In addition to imaging data, the dataset \nincludes molecular data from EGFR, KRAS, ALK mutational testing, \ngene expression microarrays and RNA sequencing. For the current \nstudy, we used 133 patients with annotated gross tumour volumes as \nan independent test set for prognostication after right-censoring for \n2 year survival and subsequently investigated the biological basis of \nour networks using this dataset.\nData preprocessing\nCT scans were resampled using linear interpolation to achieve isotropic \nvoxels with a 1 mm 3 resolution to address variations in slice thick -\nness and in-plane resolutions across study populations. We extracted \npatches of 50 × 50 × 50 voxels from the scans centred around a seed \npoint (Extended Data Fig. 7). For the DeepLesion dataset, which pro-\nvided annotations in the form of RECIST diameters, the seed point was \ndetermined by calculating the midpoint of the RECIST diameter. For the \nother datasets (that is, LUNA16, HarvardRT, LUNG1 and RADIO), which \nsupplied annotations as 3D contours, the seed point was obtained by \ncomputing the centre of mass. This approach allows for significantly \nhigher throughput than manual segmentation, which can be more \ntedious. We then normalized the voxel values in the patches by sub -\ntracting −1,024 (lower-bound Hounsfield unit) and dividing by 3,072 \n(upper-bound Hounsfield unit of 2,048), ensuring the intensity values \nin the input data ranged between 0 and 1.\nTask-agnostic pretraining of the foundation model\nWe implemented contrastive pretraining using a modified version of \nthe SimCLR framework5. The SimCLR framework’s general principle \ninvolves transforming a single data sample (for example, a patch taken \nfrom a CT scan) into two correlated and augmented samples (for exam-\nple, the same patch rotated 15° clockwise and flipped horizontally). A \nconvolutional encoder is then used to extract latent representations \nfrom these samples. Through a contrastive loss function44, the model \nlearns to identify similar representations from the same data sample and \ndissimilar representations from different data samples (Extended Data \nFig. 8). The framework emphasizes effective transformation choices, \nconvolutional encoder architectures and contrastive loss functions \nfor optimal SSL performance. T o effectively represent the nature of \nmedical images, we made modifications to each of these components.\nTransformations proposed in the original SimCLR framework for \nnatural world images, such as cutout augmentation, Sobel filtering and \ncolour distortion, are unsuited for 3D medical images due to dynamic \nrange and colour depth differences. Therefore, our study applies dif-\nferent augmentations to replace these transformations. For instance, \nwe substituted the random colour jitter transform with a random \nhistogram intensity shift transform, as they both induce variation in \nintensity distribution.\nT o extract representations from the transformed 3D volumes, we \nselected the 3D ResNet50 (ref. 45) architecture as our deep convolu-\ntional encoder. While the SimCLR authors used a 2D ResNet50 archi-\ntecture, we opted for its 3D counterpart, which has proven effective in \nhandling 3D medical imaging data46.\nRegarding loss functions, we extended normalized \ntemperature-scaled cross-entropy loss (NT-Xent)47 to support contras-\ntive training for lesion volumes. The modifications include: (1) selecting \npositive pairs as 3D patches surrounding the lesion’s seed point, (2) \nchoosing negative pairs by randomly sampling 3D patches from the \nrest of the scan and (3) computing the contrastive loss on these posi-\ntive and negative pairs, with each iteration comprising n positive pairs \nand n × 2(n − 1) negative pairs. We also explored different temperature \nparameters for the NT-Xent loss. However, the original value of 0.1 \nproposed by the original paper was the most effective.\nOur model was pretrained for 100 epochs using an effective batch \nsize of 64 (32 × 2 training nodes) on two NVIDIA Quadro RTX 8,000 \ngraphical processing units (GPUs) taking approximately 5 days. We \nused stochastic gradient descent as the optimizer, with layer-wise adap-\ntive rate control, momentum and weight-decay enabled. T o improve the \noptimization process, we used learning rate schedulers that combined \nlinear and cosine decay strategies and a warmup phase to modify the \nlearning rate at the beginning of training gradually. While most speci-\nfications were consistent with the original SimCLR experiments, we \nexperimented with different batch sizes, patch sizes (50 and 64 mm3), \nlearning rates, transforms and model architectures.\nWe conducted a comparison of our modified SimCLR version with \nits original form along with various well-known and recent pretraining \nmethods. Before the rise of contrastive approaches, auto-encoder \nmethods were commonly used for pretraining and, therefore, we \nNature Machine Intelligence | Volume 6 | March 2024 | 354–367 364\nArticle https://doi.org/10.1038/s42256-024-00807-9\nadded this to the comparison. This was implemented using MONAI’s \nauto-encoder framework, ensuring a parameter count similar to that of \nResNet50 (230 million compared to ResNet50’s 200 million). Despite \nSimCLR’s ongoing popularity 13, recent methodologies have shown \nsuperior results in particular scenarios and tasks. We adapted SwAV15 \nand NNCLR 16 approaches, combining settings from their original \ndesigns with modifications suitable for medical imaging contexts. \nIn our comparative analysis, we maintained uniformity in batch \nsizes and dataset parameters across all methods, while optimizer \nand loss-specific settings were aligned with each method’s original \nconfiguration.\nTask-specific training of the foundation model\nOur foundation model was adapted for a specific task through two \napproaches: (1) extracting features from the frozen encoder and fitting \na linear classifier and (2) transfer learning the pretrained ResNet50 for \nthe given classification task.\nWe extracted 4,096 features from the foundation model for each \ndata point and used them to train a logistic regression model using the \nscikit-learn framework48. A comprehensive parameter search for the \nlogistic regression model was performed using the optuna hyperpa -\nrameter optimization framework49. No performance improvements \nwere observed through feature selection strategies; therefore, all 4,096 \nfeatures were used in accordance with linear evaluation strategies \nprevalent in SSL literature.\nTransfer learning through fine-tuning was carried out with all \nlayers updated during training, using cross-entropy loss. A series of \nrandomly chosen augmentations—random flips, random 90° rotations \nand random translations of ±10 voxels across all axes—were applied \nthroughout the training. Stochastic gradient descent was used for \nnetwork training, with momentum enabled and step-wise learning \nrate decay. Following the original SimCLR experiments, configurations \nand similar parameters (including learning rate, transforms and model \narchitectures) were explored during hyperparameter tuning. Each \nnetwork was trained for 100 epochs using a single NVIDIA Quadro RTX \n8,000 GPU, and the best-performing model checkpoints were chosen \non the basis of the tuning set.\nFor supervised models, we selected four different baselines. First, \nwe randomly initialized the weights of a ResNet50 and trained it using \ntask-specific configurations consistent with fine-tuning the foundation \nmodel. Second, the randomly initialized model trained on use case 1 \nwas fine-tuned through transfer learning for use cases 2 and 3. For the \nthird and fourth baselines, publicly available pretrained models were \ninvestigated to add comparisons against the state of the art. Specifi -\ncally, Med3D and Models Genesis were selected on the basis of their \nrelevance to similar domains and tasks, and their established popularity \nwithin the community. These models were tailored to each task using \nconfigurations that mirrored those of our foundational model, taking \ninto account both their inherent feature representations and transfer \nlearning capabilities.\nTask-specific training was conducted on reduced dataset sizes in \naddition to usic models using these samples with the same configu-\nration as the entire dataset. As the training dataset sizes decreased, \nwe considered training the models for a higher number of epochs; \nhowever, models frequently overfitted during extended training. The \nentire test dataset was used to allow benchmarking across these splits. \nHowever, we do not conduct reduced dataset training for use case 3, as \nit is typical to have inherently small sample sizes in such use cases when \ncompared to task complexity due to study-specific inclusion criteria. \nTherefore, experiments involving further data reduction in this case \ndo not provide any valuable insights.\nPerformance analysis\nValidation of the foundation model was performed using several \nuse case-relevant metrics. Lesion anatomical site classification \nperformance was assessed using balanced accuracy as a multi-label \ncounting metric and mAP as a multi-threshold metric. The multi-label \nmetric, balanced accuracy, adjusts class-wise accuracy on the basis of \nthe class distribution at a chosen threshold (0.5). The multi-threshold \nmetric, mAP, enables the examination of a given class’s performance \nacross a range of prediction thresholds. All classes other than the class \nof interest are considered negatives, and performance is averaged \nacross all possible classes. We avoided using the AUC-receiver oper-\nating curve (AUC-ROC) for this use case due to the high proportion \nof negatives relative to positives, which results in consistently low \nfalse-positive rates and might overestimate the AUC. However, due to \na more balanced class distribution, nodule malignancy prediction was \nevaluated using AUC-ROC. NSCLC prognostication networks also used \nAUC-ROC for evaluation, as it estimates the ranking of participants on \nthe basis of their survival times.\nModels underwent pair-wise comparison using permutation tests. \nn permutations (n  = 1,000) were conducted for each pair, and new \nmodels were computed after permuting class labels. Metrics were recal-\nculated after resampling, and a two-sided P value was calculated to test \nthe null hypothesis of observations from each pair originating from the \nsame underlying distribution. Additionally, 95% CIs were established \nfor each model using a bootstrap sampling with n = 1,000 resamples.\nKaplan–Meier curves were also used to determine the stratifi -\ncation of participants on the basis of their prediction scores for the \nprognostication models. Groups were selected on the basis of predic-\ntion scores on the tuning set, and curves were plotted on the test set \nfor these groups. Multivariate log-rank tests were used to examine the \nsignificance of the stratification. Univariate Cox regression models \nwere built using the model predictions as the categorical variables of \ninterest, grouped similarly to the Kaplan–Meier curve.\nFeature visualization and saliency maps\nWe used the foundation model, top-performing supervised model, \nMed3D and Models Genesis as feature extractors to obtain 4,096 dis-\ntinct features (except for Med3D’s 2,048 features) per data point. T o \nenable visual interpretation of these high-dimensional features, we \nused t-stochastic neighbourhood embeddings50 at different perplexity \nvalues and principal component analysis to reduce their dimensional-\nity to 2D. Points in the 2D visualization were colour-coded according \nto their respective target classes despite dimensionality reduction \nbeing agnostic to these distinctions. Density contours were superim-\nposed over the visualizations to enhance the understanding of group \npatterns, offering a more comprehensive representation of trends \nacross data points.\nT o generate saliency maps for each task, the fine-tuned founda-\ntion model was used to generate predictions on randomly selected \nvolumes from respective datasets. The fine-tuned foundation \nmodel with a single output prediction (corresponding to the pre -\ndicted target class) was chosen in contrast to the feature extractor as \nexpressing saliency maps over 4,096-dimensional outputs remains \nchallenging in practice. We used a combination of (1) smooth gradient \nback-propagation, which averages gradients of the output with respect \nto several noisy inputs, and (2) guided back-propagation, which com-\nbines deconvolution with back-propagation, mainly stopping the flow \nof negative gradients or neurons that decrease the activation signal. \nThe method is termed smooth guided back-propagation 51,52 and is \nimplemented in the MONAI framework53.\nStability testing\nT o test the stability of our models, we performed a test–retest and \ninter-reader variation evaluation. For the test–retest evaluation, we \ncompared model predictions (of outcome) from the best founda -\ntion and supervised models generated on chest CT scans taken in a \n15-minute interval for 26 patients. ICC was computed using the inter-\nrater reliability and agreement package (irr) in R\n54. We also tested the \nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 365\nArticle https://doi.org/10.1038/s42256-024-00807-9\nstability of the flattened features computed by the models by calculat-\ning Spearman correlation and R2.\nFor the inter-reader variation evaluation, we used the LUNG1 \ndataset and generated 50 random perturbations sampled from a 3D \nmultivariate normal distribution with zero mean and diagonal covari-\nance matrix for each seed point. Across each dimension, a variance of \n16 voxels was used for generating samples. We generated predictions \non volumes extracted from perturbed seed points using the best foun-\ndation and supervised model, resulting in 50 different prediction sets \nfor each. The mean and variance of the 50 sets were computed for each \nand compared.\nBiological associations\nThe GSE103584 dataset contains 130 NSCLC samples that consist \nof paired CT scans and gene expression profiles generated by RNA \nsequencing. T o analyse gene expression profiles, we filtered them on \nthe basis of cohort mean expression and standard deviation. First, we \ntook only the genes with a higher expression than the overall dataset \nmean and then picked the top 500 genes on the basis of standard \ndeviation. Next, we performed a correlation analysis comparing the \nbest-supervised and foundation models. T o further evaluate founda-\ntion model features’ association with tumour biology, we computed \nthe absolute value of the correlation coefficients and performed a \ngene-set enrichment analysis with all genes with a correlation coef -\nficient above 0.1.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nMost of the datasets used in this study are openly accessible for both \ntraining and validation purposes and can be obtained from the follow-\ning sources: (1) DeepLesion14, used both for our pretraining and use \ncase 1, (2) LUNA16 (ref. 55) used for developing our diagnostic image \nbiomarker, (3) LUNG1 (ref. 56) and (4) RADIO57 used for the validation of \nour prognostic image-biomarker model. Imaging and clinical data for \nthe LUNG1 and RADIO datasets were obtained from Imaging Data Com-\nmons58 collections. The training dataset for our prognostic biomarker \nmodel, HarvardRT, is internal to Mass General Brigham institutions and \ncontains sensitive protected health information. Due to privacy con-\ncerns and legal restrictions associated with patient data, the complete \ndataset cannot be made publicly available. However, we have shared the \nmodel predictions obtained on this dataset so to ensure that our statis-\ntical analyses can be reproduced. Researchers interested in accessing \nthe dataset can submit a formal request detailing the intended use of \nthe data to R.H.M. (RMAK@partners.org). Each request will be evalu-\nated on a case-by-case basis in compliance with the ethical guidelines \nand agreements under which the data were collected.\nCode availability\nThe complete pipeline used in this study can be accessed \neither from the AIM webpage at https://aim.hms.harvard.edu/\nfoundation-cancer-image-biomarker or directly on https://github.\ncom/AIM-Harvard/foundation-cancer-image-biomarker (ref. 59). This \nincludes the code for (1) data download and preprocessing: starting \nfrom downloading the data to generating train-validation-test splits \nused in our study; (2) replicating the training and inference of founda-\ntion and baseline models across all tasks through easily readable and \ncustomizable YAML files (leveraging project-lighter60) and (3) code for \nreproducing our comprehensive performance validation. In addition \nto sharing reproducible code, we also provide trained model weights, \nextracted features and outcome predictions for all the models used \nin our study. Most importantly, we provide our foundation model \naccessible through a simple pip package install and two lines of code \nto extract features for your dataset. We also provide a detailed docu-\nmentation website that can be accessed at https://aim-harvard.github.\nio/foundation-cancer-image-biomarker/. The final model weights61 are \nmade available through the Zenodo platform. The full model imple -\nmentation is also available through https://mhub.ai/ in a reproducible, \ncontainerized, off-the-shelf executable format, allowing fast applica-\ntion in several academic and clinical environments.\nReferences\n1. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://arxiv.org/abs/2108.07258 (2021).\n2. Ouyang, L. et al. Training language models to follow instructions \nwith human feedback. In Advances in Neural Information \nProcessing Systems (eds Koyejo, S. et al.) 27730–27744 (Curran \nAssociates Inc., 2022).\n3. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics (eds Burstein, J. et al.) \n4171–4186 (ACL, 2019).\n4. Radford, A. et al. Learning transferable visual models from natural \nlanguage supervision. In Proc. 38th International Conference on \nMachine Learning 8748–8763 (PMLR, 2021).\n5. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework \nfor contrastive learning of visual representations. In Proc. 37th \nInternational Conference on Machine Learning (eds III, H.D. & Singh, \nA.) 1597–1607 (PMLR, 2020).\n6. Oquab, M. et al. DINOv2: learning robust visual features without \nsupervision. Transact. Mach. Learn. Res. 1–32 (2024).\n7. Thieme, A. et al. Foundation models in healthcare: opportunities, \nrisks & strategies forward. In Extended Abstracts 2023 CHI \nConference on Human Factors in Computing Systems 1–4 (ACM, \n2023).\n8. Moor, M. et al. Foundation models for generalist medical artificial \nintelligence. Nature 616, 259–265 (2023).\n9. Mahajan, A. et al. Deep learning-based predictive imaging \nbiomarker model for EGFR mutation status in non-small cell lung \ncancer from CT imaging. J. Clin. Orthod. 38, 3106 (2020).\n10. Hosny, A. et al. Deep learning for lung cancer prognostication: \na retrospective multi-cohort radiomics study. PLoS Med. 15, \ne1002711 (2018).\n11. Braghetto, A., Marturano, F., Paiusco, M., Baiesi, M. & Bettinelli, A. \nRadiomics and deep learning methods for the prediction of 2-year \noverall survival in LUNG1 dataset. Sci. Rep. 12, 14132 (2022).\n12. Balestriero, R. et al. A cookbook of self-supervised learning. \nPreprint at https://arxiv.org/abs/2304.12210 (2023).\n13. Huang, S.-C. et al. Self-supervised learning for medical image \nclassification: a systematic review and implementation \nguidelines. NPJ Digit. Med. 6, 74 (2023).\n14. Yan, K., Wang, X., Lu, L. & Summers, R. M. DeepLesion: automated \nmining of large-scale lesion annotations and universal lesion \ndetection with deep learning. J. Med. Imaging 5, 036501 (2018).\n15. Caron, M. et al. Unsupervised learning of visual features by \ncontrasting cluster assignments. Adv. Neural Inf. Process. Syst. 33, \n9912–9924 (2020).\n16. Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P. & Zisserman, A. \nWith a little help from my friends: nearest-neighbor contrastive \nlearning of visual representations. In 2021 IEEE/CVF International \nConference on Computer Vision (ICCV) 9568–9577 (IEEE, 2021).\n17. Chen, S., Ma, K. & Zheng, Y. Med3D: transfer learning for  \n3D medical image analysis. Preprint at https://arxiv.org/abs/  \n1904.00625 (2019).\n18. Zhou, Z. et al. Models Genesis: generic autodidactic models for \n3D medical image analysis. Med. Image Comput. Comput. Assist. \nInterv. 11767, 384–393 (2019).\nNature Machine Intelligence | Volume 6 | March 2024 | 354–367 366\nArticle https://doi.org/10.1038/s42256-024-00807-9\n19. Zhao, B. et al. Evaluating variability in tumor measurements from \nsame-day repeat CT scans of patients with non-small cell lung \ncancer. Radiology 252, 263–272 (2009).\n20. Aerts, H. J. W. L. et al. Decoding tumour phenotype by noninvasive \nimaging using a quantitative radiomics approach. Nat. Commun. \n5, 4006 (2014).\n21. Hosny, A. et al. Clinical validation of deep learning algorithms \nfor radiotherapy targeting of non-small-cell lung cancer: an \nobservational study. Lancet Digit. Health 4, e657–e666 (2022).\n22. Hinshaw, D. C. & Shevde, L. A. The tumor microenvironment \ninnately modulates cancer progression. Cancer Res. 79,  \n4557–4566 (2019).\n23. Azizi, S. et al. Big self-supervised models advance medical image \nclassification. In 2021 IEEE/CVF International Conference on \nComputer Vision (ICCV) 3458–3468 (IEEE, 2021).\n24. Krishnan, R., Rajpurkar, P. & Topol, E. J. Self-supervised learning in \nmedicine and healthcare. Nat. Biomed. Eng. 6, 1346–1352 (2022).\n25. Ghesu, F. C. et al. Contrastive self-supervised learning from 100 \nmillion medical images with optional supervision. J. Med. Imaging \n9, 064503 (2022).\n26. Haarburger, C. et al. Radiomics feature reproducibility under \ninter-rater variability in segmentations of CT images. Sci. Rep. \nhttps://doi.org/10.1038/s41598-020-69534-6 (2020).\n27. Campello, V. M. et al. Minimising multi-centre radiomics variability \nthrough image normalisation: a pilot study. Sci. Rep. 12, 12532 (2022).\n28. Shen, W., Zhou, M., Yang, F., Yang, C. & Tian, J. Multi-scale \nconvolutional neural networks for lung nodule classification.  \nInf. Process. Med. Imaging 24, 588–599 (2015).\n29. Shen, W. et al. Multi-crop convolutional neural networks for \nlung nodule malignancy suspiciousness classification. Pattern \nRecognit. 61, 663–673 (2017).\n30. Kumar, D. et al. in Image Analysis and Recognition (eds Karray, F. \net al.) 54–62 (Springer, 2017).\n31. Haarburger, C., Weitz, P., Rippel, O. & Merhof, D. Image-based \nsurvival prediction for lung cancer patients using CNNS. In 2019 \nIEEE 16th International Symposium on Biomedical Imaging (ISBI \n2019) 1197–1201 (IEEE, 2019).\n32. Mukherjee, P. et al. A shallow convolutional neural network \npredicts prognosis of lung cancer patients in multi-institutional \ncomputed tomography image datasets. Nat.Mach. Intell. 2, \n274–282 (2020).\n33. Taleb, A. et al. 3D self-supervised methods for medical imaging. \nAdv. Neural Inf. Process. Syst. 33, 18158–18172 (2020).\n34. Tiu, E. et al. Expert-level detection of pathologies from unannotated \nchest X-ray images via self-supervised learning. Nat. Biomed. Eng. 6, \n1399–1406 (2022).\n35. Zhou, Y. et al. A foundation model for generalizable disease \ndetection from retinal images. Nature https://doi.org/10.1038/\ns41586-023-06555-x (2023).\n36. Azizi, S. et al. Robust and data-efficient generalization of \nself-supervised machine learning for diagnostic imaging. Nat. \nBiomed. Eng. 7, 756–779 (2023).\n37. Azad, B. et al. Foundational models in medical imaging:  \na comprehensive survey and future vision. Preprint at  \nhttps://arxiv.org/abs/2310.18689 (2023).\n38. Cole, E., Yang, X., Wilber, K., Aodha, O. M. & Belongie, S. When \ndoes contrastive visual representation learning work? In Proc. \n2022 IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR) 14755–14764 (IEEE, 2022).\n39. Adebayo, J. et al. Sanity checks for saliency maps. In Advances \nin Neural Information Processing Systems 9505–9515 (Curran \nAssociates, 2018).\n40. Arun, N. et al. Assessing the trustworthiness of saliency maps for \nlocalizing abnormalities in medical imaging. Radiol. Artif. Intell. 3, \ne200267 (2021).\n41. Setio, A. A. A. et al. Validation, comparison, and combination  \nof algorithms for automatic detection of pulmonary nodules  \nin computed tomography images: the LUNA16 challenge.  \nMed. Image Anal. 42, 1–13 (2017).\n42. Aerts, H. J. W. L. et al. Data from NSCLC-Radiomics (The Cancer \nImaging Archive, 2019); https://doi.org/10.7937/K9/TCIA.2015.\nPF0M9REI\n43. Napel, S. & Plevritis, S. K. NSCLC Radiogenomics: Initial Stanford \nStudy of 26 cases (The Cancer Imaging Archive, 2014);  \nhttps://doi.org/10.7937/K9/TCIA.2014.X7ONY6B1\n44. Wang, F. & Liu, H. Understanding the behaviour of contrastive \nloss. In Proc. IEEE/CVF Conference on Computer Vision and \nPattern Recognition 2495–2504 (IEEE, 2021).\n45. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for \nimage recognition. In Proc. IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR) 770–778 (2016).\n46. Uemura, T., Näppi, J. J., Hironaka, T., Kim, H. & Yoshida, H. \nComparative performance of 3D-DenseNet, 3D-ResNet, and \n3D-VGG models in polyp detection for CT colonography. In Proc. \nMedical Imaging 2020: Computer-Aided Diagnosis Vol. 11314, \n736–741 (SPIE, 2020).\n47. Sohn, K. Improved deep metric learning with multi-class N-pair \nloss objective. In Advances in Neural Information Processing \nSystems (eds Lee, D. et al.) 1857–1865 (Curran Associates, 2016).\n48. Pedregosa, F. et al. Scikit-learn: machine learning in Python.  \nJ. Mach. Learn. Res. 12, 2825–2830 (2011).\n49. Akiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: a \nnext-generation hyperparameter optimization framework. In \nProc. 25th ACM SIGKDD International Conference on Knowledge \nDiscovery & Data Mining 2623–2631 (Association for Computing \nMachinery, 2019).\n50. van der Maaten, L. & Hinton, G. Visualizing data using t-SNE.  \nJ. Mach. Learn. Res. 9, 2579–2605 (2008).\n51. Springenberg, J. T., Dosovitskiy, A., Brox, T. & Riedmiller, M. A.  \nStriving for simplicity: the all convolutional net. In 3rd \nInternational Conference on Learning Representations Workshop \n(ICLR, 2015).\n52. Smilkov, D., Thorat, N., Kim, B., Viégas, F. & Wattenberg, M. \nSmoothGrad: removing noise by adding noise. Preprint at  \nhttps://arxiv.org/abs/1706.03825 (2017).\n53. Jorge Cardoso, M. et al. MONAI: an open-source framework \nfor deep learning in healthcare. Preprint at https://arxiv.org/\nabs/2211.02701 (2022).\n54. Gamer, M. irr: Various Coefficients of Interrater Reliability and \nAgreement (R Foundation for Statistical Computing, 2010); \ncran.r-project.org/web/packages/irr/irr.pdf\n55. The Cancer Imaging Archive. LIDC-IDRI (TCIA, 2023); www.\ncancerimagingarchive.net/collection/lidc-idri/\n56. The Cancer Imaging Archive. NSCLC-RADIOMICS (TCIA, 2023); \nwww.cancerimagingarchive.net/collection/nsclc-radiomics/\n57. The Cancer Imaging Archive. NSCLC-RADIOGENOMICS- \nSTANFORD (TCIA, 2023); www.cancerimagingarchive.net/\nanalysis-result/nsclc-radiogenomics-stanford/\n58. Fedorov, A. et al. NCI imaging data commons. Cancer Res. 81, \n4188–4193 (2021).\n59. Pai, S. AIM-Harvard/foundation-cancer-image-biomarker: v0.0.1. \nZenodo https://doi.org/10.5281/zenodo.10535536 (2024).\n60. Hadzic, I., Pai, S., Bressem, K. & Aerts, H. Lighter. Zenodo  \nhttps://doi.org/10.5281/zenodo.8007711 (2023).\n61. Pai, S. Foundation model for cancer imaging biomarkers. Zenodo \nhttps://doi.org/10.5281/zenodo.10528450 (2024).\nAcknowledgements\nWe acknowledge financial support from the National Institute of \nHealth (NIH) (H.J.W.L.A. grant nos. NIH-USA U24CA194354, NIH-USA \nNature Machine Intelligence | Volume 6 | March 2024 | 354–367\n 367\nArticle https://doi.org/10.1038/s42256-024-00807-9\nU01CA190234, NIH-USA U01CA209414, NIH-USA R35CA22052 \nand NIH-USA U54CA274516-01A1), the European Union, European \nResearch Council (H.J.W.L.A. grant no. 866504) and Deutsche \nForschungsgemeinschaft, the German Research Foundation  \n(S.B. grant no. 502050303).\nAuthor contributions\nThe concept for the study was developed by S.P. and H.J.W.L.A. \nData acquisition, analysis and interpretation were done by S.P., \nD.B., A.H., T.L.C., R.H.M. and H.J.W.L.A. Methodological design and \nimplementation were done by S.P. and D.B. Conceptualization \nof assessment strategies was developed by S.P., D.B., N.J.B. and \nH.J.W.L.A. Statistical analyses was carried out by S.P., M.S., N.J.B. and \nH.J.W.L.A. Code and reproducibility were the responsibility of S.P., \nI.H. and V.P. The paper was written by S.P., D.B., M.S., S.B., R.H.M. and \nH.J.W.L.A. Critical revision of the paper was carried out by S.P., D.B., \nI.H., V.P., M.S., T.L.C., S.B., A.H., R.H.M., N.J.B. and H.J.W.L.A. The study \nwas supervised by H.J.W.L.A.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-024-00807-9.\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s42256-024-00807-9.\nCorrespondence and requests for materials should be addressed to \nHugo J. W. L. Aerts.\nPeer review information Nature Machine Intelligence thanks  \nPaula Jacobs, Pritam Mukherjee and the other, anonymous, reviewer(s) \nfor their contribution to the peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2024\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 1 | Visual exploration of the features generated from the \nfoundation and baseline models. Features from the foundation model and each \nof the baseline models are extracted on the independent test-set for identifying \nlesion anatomical sites and visualized using several different dimensionality \nreduction approaches. Approaches chosen aim to avoid biases from parameter \nselection, therefore, tSNE with different perplexity settings and PCA are used. \nThe x-axis corresponds to dimension 1, and the y-axis to dimension 2 of the \ndimensionality reduction. The density contours of each class are underlaid to \nhighlight separability between classes in the feature space. It is to be noted that \nthe supervised model was trained with lesion anatomical site labels while all the \nother models (Foundation, Med3D, ModelsGenesis) were used merely as feature \nextractors without being trained for the particular label.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 2 | Time and memory efficiency of implementation \napproaches. We compare the two implementation approaches of our \nfoundation model, 1) linear modelling on extracted features, which comprises \na feature extraction step followed by the linear modelling step, and 2) transfer \nlearning through a fine-tuning step. a, Training times (in minutes) for each of \nthe three use cases and the three steps are shown. b, Memory usage (GPU VRAM \nand System RAM) are shown for the feature extraction, linear modelling and \nfine-tuning steps. Memory usage for each step across use-cases remains mostly \nconstant due to batch processing and high feature dimensionality. All analyses \nwere run with six cores on AMD EPYCTM 7402 P Processor 24-core @ 2.80 GHz. \nThe GPU, which was only used for fine-tuning, was the Quadro RTX 8000. For \nboth CPU and GPU runs, where batch processing was used, a batch size of 32 was \nchosen.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 3 | Detailed comparison of the foundation model \nimplementations against baseline methods for lesion anatomical site \nclassification. Comparison of the balanced accuracy and mean average \nprecision of the Foundation (Features) and Foundation (Finetuned) against \nall other methods when using 100%, 50%, 20%, and 10% percent of the training \ndata. For each metric-percentage pair, a p-value heatmap (darker colours show \nnon-significant values) is shown with the foundation models on the y-axis and all \nother models to compare on the x-axis. In each cell, the increase or decrease in \nmetric value is shown along with the corresponding p-value. p-values between \nmodels were compared using the permutation test with N = 1000 permutations \nconducted for each pair-wise comparison.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 4 | Anatomical site-wise breakdown of foundation model and baseline method performance. We compare the foundation model against \nbaseline methods across different training data percentages using average precision scores for each anatomical site in the DeepLesion held-out test dataset. This \nallows us to show the generalizability of approaches across anatomical sites.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 5 | Detailed comparison of the foundation model \nimplementations against baseline methods for nodule malignancy \nclassification and NSCLC prognostication. a, Comparison of the area-under-\nreceiver operating curve (AUC) and mean average precision(mAP) of the \nFoundation (Features) and Foundation (Finetuned) against all other methods \nwhen using 100%, 50%, 20%, and 10% percent of the training data on use case \n2. b, Comparison of the AUC of the Foundation (Features) and Foundation \n(Finetuned) against all other models for the LUNG1 (left) and RADIO (right) \ndataset for use-case 3. For each metric-percentage pair, a p-value heatmap \n(darker colours show non-significant values) is shown with the foundation \nmodels on the y-axis and all other models to compare on the x-axis. In each cell, \nthe increase or decrease in metric value is shown along with the corresponding \np-value. p-values between models were compared using the permutation test \nwith N = 1000 permutations conducted for each pair-wise comparison.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 6 | Survival analysis for all models implemented on \nNSCLC prognostication. a,b, Kaplan Meier curves on the LUNG1 (a) and RADIO \n(b) datasets for both the foundation model implementation approaches as \nwell as the baseline comparisons are shown. c,d, In c and d, Hazard ratios (HR), \ncomputed through univariate Cox regression, for each of the implementation \napproaches on the LUNG1 and RADIO datasets are shown using forest plots. \nFor both these analyses, groups are determined based on respective model \npredictions split on the median of the corresponding HarvardRT tuning set \npredictions. The error bands in (a,b) represent the 95% confidence interval of \nthe Kaplan-Meier estimates of the survival function. The log-rank test is used to \ndetermine significant differences between the groups in the KM analysis. For (c, \nd), the error bars represent the 95% confidence interval of the hazard ratio, and \nthe p-values are calculated using the Wald test. For the LUNG1 dataset, n = 420 \nand RADIO, n = 133 samples are used to compute each of the analyses in the  \nplots above.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 7 | Diameter distribution of DeepLesion. Distribution of diameters in the x and y axes for the DeepLesion training dataset based on RECIST \nbookmarks identified on key slices. Input dimensions of 50x50x50 mm3 were chosen as they covered 93% and 97% of the distribution in the x and y axes, respectively.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 8 | Stages of the implementation pipeline. a, We first \npre-train using a modified version of the SimCLR on 11.467 lesions. The pre-\ntraining process consists of a positive contrastive and a negative contrastive loss \ncomponent. In the positive contrastive loss, augmentations of the same lesion \nare made to learn similar features. At the same time, the negative contrastive loss \nlearns different features for volumes with and without lesions. b, In the second \nstage, for each task, different implementation approaches are followed by \nadapting the pretrained model by either extracting features from a frozen model \nfollowed by linearly predicting a target or by fine-tuning all model weights for \npredicting a target.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00807-9\nExtended Data Fig. 9 | Dataset breakdown. The first table shows the 6 different cohorts used in this study along with eligible scans and patients. A secondary table \nshows the outcome, sex, and age distribution of each of the cohorts.\n\n\n",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7377556562423706
    },
    {
      "name": "Medicine",
      "score": 0.46915900707244873
    },
    {
      "name": "Cancer",
      "score": 0.4592449963092804
    },
    {
      "name": "Medical physics",
      "score": 0.3536572754383087
    },
    {
      "name": "Oncology",
      "score": 0.3292091190814972
    },
    {
      "name": "Internal medicine",
      "score": 0.19657590985298157
    },
    {
      "name": "History",
      "score": 0.08831223845481873
    },
    {
      "name": "Archaeology",
      "score": 0.06148460507392883
    }
  ]
}