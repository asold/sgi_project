{
  "title": "Universal Captioner: Long-Tail Vision-and-Language Model Training through Content-Style Separation",
  "url": "https://openalex.org/W3217561355",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5066519737",
      "name": "Marcella Cornia",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A5048928616",
      "name": "Lorenzo Baraldi",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A5046146028",
      "name": "Giuseppe Fiameni",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A5030948871",
      "name": "Rita Cucchiara",
      "affiliations": [
        "Nvidia (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2575842049",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3182293212",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3112448493",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W3195108980",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2901988662",
    "https://openalex.org/W3088493063",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3193931782",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2250384498",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W2963175879",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2963877622",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W3153469116",
    "https://openalex.org/W3135099046",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3121480429",
    "https://openalex.org/W2984138079",
    "https://openalex.org/W2970569830",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3106859150",
    "https://openalex.org/W2965697393",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2173180041",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3110019360",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W3104279398",
    "https://openalex.org/W3174476431",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3194591991",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3103651098",
    "https://openalex.org/W3170928047"
  ],
  "abstract": "While captioning models have obtained compelling results in describing natural images, they still do not cover the entire long-tail distribution of real-world concepts. In this paper, we address the task of generating human-like descriptions with in-the-wild concepts by training on web-scale automatically collected datasets. To this end, we propose a model which can exploit noisy image-caption pairs while maintaining the descriptive style of traditional human-annotated datasets like COCO. Our model separates content from style through the usage of keywords and stylistic tokens, employing a single objective of prompt language modeling and being simpler than other recent proposals. Experimentally, our model consistently outperforms existing methods in terms of caption quality and capability of describing long-tail concepts, also in zero-shot settings. According to the CIDEr metric, we obtain a new state of the art on both COCO and nocaps when using external data.",
  "full_text": "Generating More Pertinent Captions by Leveraging\nSemantics and Style on Multi-Source Datasets\nMarcella Cornia1†, Lorenzo Baraldi1†, Giuseppe Fiameni2, Rita Cucchiara1,3\n1*University of Modena and Reggio Emilia, Modena, Italy.\n2NVIDIA AI Technology Centre, Bologna, Italy.\n3IIT-CNR, Pisa, Italy.\n*Corresponding author(s). E-mail(s): marcella.cornia@unimore.it;\nContributing authors: lorenzo.baraldi@unimore.it; gfiameni@nvidia.com;\nrita.cucchiara@unimore.it;\n†These authors contributed equally to this work.\nAbstract\nThis paper addresses the task of generating fluent descriptions by training on a non-uniform com-\nbination of data sources, containing both human-annotated and web-collected captions. Large-scale\ndatasets with noisy image-text pairs, indeed, provide a sub-optimal source of supervision because of\ntheir low-quality descriptive style, while human-annotated datasets are cleaner but smaller in scale.\nTo get the best of both worlds, we propose to leverage and separate semantics and descriptive style\nthrough the incorporation of a style token and keywords extracted through a retrieval component.\nThe proposed model avoids the need of object detectors, is trained with a single objective of prompt\nlanguage modeling, and can replicate the style of human-collected captions while training on sources\nwith different input styles. Experimentally, the model shows a strong capability of recognizing real-\nworld concepts and producing high-quality captions. Extensive experiments are performed on different\nimage captioning datasets, including CC3M, nocaps, and the competitive COCO dataset, where our\nmodel consistently outperforms baselines and state-of-the-art approaches.\nKeywords: Image Captioning, Vision and Language, Multimodal Learning\n1 Introduction\nImage captioning, which aims at generating tex-\ntual descriptions from visual inputs, has emerged\nas an attractive research problem in the last\nfew years, as it entails modeling the connections\nbetween the visual and textual modalities (X. Li et\nal., 2020; P. Zhang et al., 2021) and can be seen as\na fundamental step toward machine intelligence.\nThis has led to the development of effective strate-\ngies for feature extraction (Anderson et al., 2018),\ncross-modal modeling (Cornia, Stefanini, Baraldi,\n& Cucchiara, 2020; Pan, Yao, Li, & Mei, 2020)\nand model training (X. Li et al., 2020; Rennie,\nMarcheret, Mroueh, Ross, & Goel, 2017).\nDespite these advances, researchers are still\nworking to endow such models with the ability\nto describe the entire variety of real-world con-\ncepts. The issue boils down to the limitations\nof popularly-used human-annotated datasets like\nCOCO (Gurari, Zhao, Zhang, & Bhattacharya,\n2020; T.-Y. Lin et al., 2014; Young, Lai, Hodosh,\n1\narXiv:2111.12727v3  [cs.CV]  30 Nov 2023\nStandard Captioner:\nA picture of a bridge over a\nbody of water.\nOurs:\nA picture of the Golden Gate\nbridge in San Francisco.\nStandard Captioner:\nA group of people standing in\nfront of a building.\nOurs:\nA line of people standing\noutside of an Apple store.\nStandard Captioner:\nTwo plates of pancakes with\nsyrup on a table.\nOurs:\nA plate of pancakes and a jar\nof Nutella on a table.\nStandard Captioner:\nA group of people riding\nskateboards in a field.\nOurs:\nA group of people riding\nsegways in a field.\nStandard Captioner:\nA tall building sitting in the\nmiddle of a body of water.\nOurs:\nAn aerial view of the Burj Al\nArab in Dubai.\nStandard Captioner:\nA man is talking on the phone\nsitting at a desk.\nOurs:\nPresident Obama talking on\nthe phone in the oval office.\nStandard Captioner:\nA black car parked in front of\na building.\nOurs:\nA black Jaguar parked in front\nof a car dealership.\nStandard Captioner:\nA crowd of people standing in\nfront of a tall tower.\nOurs: A group of people\nstanding near the leaning\ntower of Pisa.\nStandard Captioner:\nA plate of pasta with onions\non a yellow plate.\nOurs:\nA plate of spaghetti and\nclams on a table.\nStandard Captioner:\nA person holding a cellphone in\ntheir hand.\nOurs:\nA person holding a smartphone\nwith the Facebook logo.\nStandard Captioner:\nA woman with blonde hair is\nposing for a picture.\nOurs:\nA picture of Marilyn Monroe\nwith a red lipstick.\nStandard Captioner:\nA group of dogs walking in\nthe snow with a man.\nOurs:\nA group of huskies pulling a\nsled dogs in the snow.\nFig. 1 Sample descriptions generated by our model, in comparison with a Transformer-based captioner trained on COCO.\nOur approach generates high-quality captions by separating content from style.\n& Hockenmaier, 2014) which, while being high-\nquality in terms of descriptive style, are limited\nin terms of semantic variability and size. Recent\nefforts have automatically collected large-scale\ndatasets with noisy image-text pairs from the\nweb, partially solving the semantic scale issue\nat the cost of reducing the quality of the anno-\ntations (Changpinyo, Sharma, Ding, & Soricut,\n2021; Ordonez, Kulkarni, & Berg, 2011; Schuh-\nmann et al., 2022; Sharma, Ding, Goodman, &\nSoricut, 2018; Srinivasan, Raman, Chen, Bender-\nsky, & Najork, 2021).\nIn this work we focus on generating cap-\ntions that can be richer in terms of semantics\nand include proper names and long-tail concepts\n(Fig. 1), thus being more pertinent to the input\nimage. We do this by jointly leveraging web-\ncollected and human-annotated sources and main-\ntaining the style and fluency of human-annotated\ncaptions. The core idea behind our approach is\nthat of separating semantics and descriptive style\nwhile training on non-homogeneous data sources.\nThis is achieved through the introduction of a\nstyle token that can condition the network both at\ntraining and generation time. During training, the\ntoken is employed to distinguish between human-\nannotated and web-crawled sources. At generation\ntime, the style token can be used to generate\ndescriptions which resemble the style of human-\nannotated ones, enriched by the semantics learned\non web-collected datasets (Fig. 2).\nFurther, to better represent semantics, we\nextract textual keywords through a novel\nretrieval-based approach (Radford et al., 2021),\nwhich avoids the need of using tags or descrip-\ntions from object detectors (Anderson et al., 2018;\nP. Zhang et al., 2021). This also allows us to scale\nbeyond a limited set of categories and fully rep-\nresent the semantics of the image regardless of its\nsource. The addition of the style token and of tex-\ntual keywords fosters the transfer of descriptive\nstyle and semantic concepts between data sources.\nFrom the point of view of the architecture, our\nmodel features an encoder-decoder structure that\nclearly separates the visual and textual domain, in\ncontrast to the paradigm of employing BERT-like\narchitectures (X. Li et al., 2020; Zhou et al., 2020).\nTo represent images, we employ a multimodal fea-\nture extractor based on CLIP (Dosovitskiy et al.,\n2021; Radford et al., 2021) which can directly take\nraw pixels as input and avoids the need of using\nobject detectors. Finally, our model is trained\nusing only a language modeling objective and does\nnot require complex pre-training strategies.\nOur model outperforms existing proposals in\nterms of caption quality, sometimes also sur-\npassing models trained on significantly larger\ndatasets (Wang et al., 2022), and shows an\nimproved capability of generating named entities\nto improve the description pertinence. Experi-\nmentally, we assess the performance of the pro-\nposed approach on different image captioning\n2\ndatasets, including COCO (T.-Y. Lin et al., 2014),\nnocaps (Agrawal et al., 2019), VizWiz (Gurari et\nal., 2020), and TextCaps (Sidorov, Hu, Rohrbach,\n& Singh, 2020) that all contain human-annotated\nsentences, and CC3M (Sharma et al., 2018),\nWIT (Srinivasan et al., 2021), and a portion\nof LAION-400M (Schuhmann et al., 2021) that\ninstead are composed of web-collected data. Over-\nall, our work demonstrates that heterogeneous\ndata sources can be properly exploited, together\nwith a selective architecture, to increase the per-\nformance of image captioning systems.\nContributions. To sum up, the contributions of\nthis paper are fourfold:\n• We propose a framework for learning on non-\nuniform collections of caption sources while\nmaintaining a separation between semantic\ncontent and descriptive style. This allows the\ngeneration of fluent descriptions that resem-\nble the quality of human-collected ones while\nlearning from web-scale data.\n• Our approach employs a style token as a\nmeans to separate the descriptive styles of\nhuman-annotated and web-collected sources,\nand textual keywords extracted through a\nretrieval component to represent semantics.\n• In terms of architecture, our model features\na fully-attentive encoder-decoder structure\nthat jointly encodes keywords, style, and\ntext, and is trained with a single objective of\nprompt language modeling.\n• We evaluate our model against carefully-\ndesigned baselines and recent approaches on\nCOCO, nocaps, VizWiz, TextCaps, CC3M,\nWIT, and LAION-400M. On COCO, our\napproach reaches a performance of 149.6\nCIDEr points.\n2 Related Work\nImage Captioning. Research on image cap-\ntioning has jointly focused on modeling the\nvisual encoding pipeline, the language model, and\nthe multi-modal connections between them (Ste-\nfanini et al., 2022). While traditional approaches\nhave focused on training on curated datasets,\nthe recently emerged pre-training paradigm (Y.-\nC. Chen et al., 2020; Hu et al., 2020; Lu,\nBatra, Parikh, & Lee, 2019; Tan & Bansal, 2019;\nZhou et al., 2020) aims at learning from weakly\nDIY Spring Landscaping \n<PERSON> on the TV \n#flowerbed #gardening\n#landscaping #DIY\nhuman-annotated\nA young girl is trying \nto brush her hair with \na pink brush.\nThere is a very large \nwhite building in the \nmiddle of this street.\nweb-collected\nNow We've Really \nFound The World's \nOldest Tesla Model 3 \nOwner.\nDubai, Emirates, \naerial, gulf, coastline\nStandard \nCaptioner\nA tall building \nsitting in the middle \nof a body of water.\nAn aerial view of \nthe Burj Al Arab in \nDubai.\nBurj Al Arab, tower \nand luxury hotel. \nStock photography.\nOurs\nFig. 2 Samples of human-annotated and web-collected\n(image, caption) pairs and overview of our approach.\nlabeled or noisy sources. Most of the approaches\nhave employed BERT-like (Devlin, Chang, Lee,\n& Toutanova, 2018) or encoder-decoder archi-\ntectures in conjunction with self-supervised or\nsequence learning objectives. The OSCAR (X. Li\net al., 2020) model considers triplets of object\ntags, detections, and captions and trains using a\ncombination of masked token loss and contrastive\nloss. VinVL (P. Zhang et al., 2021) employs the\nsame objectives while proposing a better object\ndetector, and trains on 8.85 million text-image\npairs. SimVLM (Wang et al., 2022), instead,\nuses an encoder-decoder architecture and learns\nvisual features from scratch, training on the large\nALIGN dataset (Jia et al., 2021). Recent models\nlike LEMON (Hu et al., 2022) and BLIP (J. Li,\nLi, Xiong, & Hoi, 2022) have further investigated\nthe scaling properties of captioning models, also\nadopting custom architectures.\nRecently, following the latest trends of large-\nscale language models (Brown et al., 2020;\nS. Zhang et al., 2022), several large-scale multi-\nmodal solutions have been proposed (Alayrac et\nal., 2022; J. Li, Li, Savarese, & Hoi, 2023; Yu et\nal., 2022), considerably increasing the number of\nparameters up to few billion ( e.g. the well-known\nFlamingo model (Alayrac et al., 2022) has 10.6B\nparameters) and, consequently, the computational\ncomplexity.\n3\nVisual encoders. In terms of visual encoding,\nafter the emergence of global (Karpathy & Fei-\nFei, 2015; Rennie et al., 2017) and grid descrip-\ntors (K. Xu et al., 2015), the use of object\ndetections (Anderson et al., 2018; P. Zhang et\nal., 2021) has become one of the most popular\napproaches. Indeed, it provides clean visual ele-\nments and a partial bridge between the visual\nand the textual domains. While several works\nhave encoded regions through graph-based (Yang,\nTang, Zhang, & Cai, 2019) or self-attentive struc-\ntures (Cornia et al., 2020; Huang, Wang, Chen,\n& Wei, 2019; Pan et al., 2020), the emergence\nof self-attentive visual encoders (Dosovitskiy et\nal., 2021) and large-scale multi-modal models has\nenabled new strategies, ranging from training bet-\nter detectors to having end-to-end visual models\ntrained from scratch (Kim, Son, & Kim, 2021;\nH. Xu et al., 2021; Yan et al., 2021; P. Zhang et\nal., 2021). Recently, Shen et al. (2022) showed that\nfeatures encoded by large-scale multi-modal archi-\ntectures like CLIP (Radford et al., 2021) perform\nat least on par with detection-based approaches.\nThese findings have been confirmed by subsequent\nmethods (Barraco, Cornia, Cascianelli, Baraldi,\n& Cucchiara, 2022; Mokady, Hertz, & Bermano,\n2021), also augmenting the captioning model with\neither knowledge distillation (Barraco, Stefanini,\net al., 2022) or retrieval components (Y. Li, Pan,\nYao, & Mei, 2022; Sarto, Cornia, Baraldi, &\nCucchiara, 2022).\nSemantic content and style separation. A\nrelated line of research focuses on the generation of\ntextual descriptions conditioned on a specific style\nor sentiment. In this context, some solutions have\nbeen proposed to train a captioning model with\nimage-text pairs composed of images directly asso-\nciated with positive and negative sentences (Math-\news, Xie, & He, 2016), eventually constraining\nthe captioning model to also preserve the abil-\nity to generate factual descriptions ( i.e. captions\nwithout a specific style) (T. Chen et al., 2018).\nTo reduce the dependency on paired data, other\napproaches proposed to leverage unpaired styl-\nized corpus to generate more accurate stylized\ncaptions (Gan, Gan, He, Gao, & Deng, 2017;\nGuo, Liu, Yao, Li, & Lu, 2019; Mathews, Xie,\n& He, 2018; Zhao, Wu, & Zhang, 2020). In\nparticular, Guo et al. (2019) employed a one-\nhot style indicator to condition their captioning\nmodel, which resembles our style token. Differ-\nently, the work introduced by (Klein, Mahajan,\n& Roth, 2021) also considers the stylistic content\nof the input image represented with style-specific\nattributes.\nWhile these solutions can provide a good strat-\negy to personalize the generation of a captioning\nmodel thus potentially meeting the requirements\nof different users, only a few attempts have been\ndone on the separations of semantics and style. For\nexample, recent works on dataset collection like\nConceptual Captions (Changpinyo et al., 2021;\nSharma et al., 2018) have proposed strategies for\nfiltering lower quality captions to ensure mini-\nmum quality levels. BLIP (J. Li et al., 2022) has\nexplicitly addressed quality assurance in web-scale\ntraining by proposing a learnable image-grounded\ncaption filter. To our knowledge, we are the first\nto address the separation of semantics and style\nas a method for dealing with noisy web-collected\ncaptions.\n3 Proposed Method\nThe goal of an image captioning algorithm is that\nof modeling an autoregressive probability distri-\nbution over words conditioned on an input image,\ni.e.\np(wt|wτ<t , V), (1)\nwhere V represents an input image and {wt}t\nis the sequence of words comprising the gener-\nated caption. In previous works, this is usually\nachieved by training a language model condi-\ntioned on visual features to mimic ground-truth\ndescriptions.\nThe relation between images and word\nsequences, though, is far from being bijec-\ntive, and the same semantic content can be\ndescribed in different ways according to intent\nand descriptive style. The variance in descrip-\ntive style is a key element that differenti-\nates human-annotated captions from noisy web-\ncollected datasets. This is well testified in Fig. 2,\nwhich compares web-collected caption sources like\nConceptual Captions (Sharma et al., 2018) or\nYFCC100M (Thomee et al., 2016) and human-\nannotated sources like COCO. The latter feature\na grammatically correct, constant, and generic\ndescriptive style. The former, instead, can have\nheterogeneous descriptive styles depending on\n4\nbuilding\nTransformer DecoderTransformer Encoder\nImage \nEncoder\nkeyword \nvocabulary\nview\n … StateA Empirewith the\nkeywords\nstyle token\nword tokens\ntop-k\nTextual \nEncoder\nbuilding\ncar\nempire\nnight\nskyline\nstreet\n…\nFig. 3 Illustration of the overall structure of our approach, which is composed of an encoder module, a keyword-extraction\nmodule, and a decoder module.\ntheir source and collection procedure. On aver-\nage, though, they are more noisy and can comprise\nhashtags, comments and proper nouns.\nFollowing this insight, we develop a model\nwhich is style-aware and can thus separate\nbetween the descriptive styles of the two aforemen-\ntioned sets at generation time. Further, to enable\nthe transfer of semantics learned from different\nsources, we employ textual keywords as a means to\nrepresent the content of an image regardless of its\ndescriptive style. Formally, our approach considers\na distribution\np(wt|wτ<t , V, θ,K), (2)\nwhere θ is a parameter encoding style and K a set\nof keywords encoding semantics.\n3.1 Leveraging semantics and style\nExtracting textual keywords through\nretrieval. Extracting a condensed textual rep-\nresentation of the visual input aims to promote\nan objective transfer between visual and textual\nfeatures. Previous works have employed tags\ncoming from an object detector (Anderson et al.,\n2018; P. Zhang et al., 2021), which however are\nlimited in terms of the number of classes. Given\nthe semantic breadth of web-scale datasets, it is\ninstead crucial to scale beyond the limitation of\nobject detection classes (Krishna et al., 2017). To\nthis end, we cast the tagging problem as a cross-\nmodal retrieval one, instead of a classification\none.\nGiven a dictionary of keywords Q, the set\nof predicted keywords for an input image V is\nobtained by selecting the k elements in Q with the\nhighest similarity to V, according to the matching\nfunction defined by a cross-modal retrieval model.\nFormally, being ϕ the similarity function defined\nby the cross-modal model, the set of keywords K\nis computed as\nK = argtop-k\nq∈Q\nϕ(V, q), (3)\nwhere argtop-kx∈Sf(x) returns the elements in S\nwhich produce the k largest values in {f(x), x∈\nS}.\nThe keywords dictionary Q must be large\nenough to ensure sufficient coverage with respect\nto the semantic distribution of web-collected\ndatasets. To this aim, we construct Q by min-\ning around 11.5k unigrams from COCO, Visual\nGenome, and the OpenWebText corpus, a pub-\nlic clone of OpenAI’s WebText dataset (Radford\net al., 2019). For computing cross-modal simi-\nlarities, we leverage the multi-modal embedding\nspace of CLIP (Radford et al., 2021), which can\nscale well in terms of the number of concepts.\nEmbeddings for the keywords in Q are obtained\nthrough the CLIP text model and can be pre-\ncomputed in advance. The embedding of an image\nis instead obtained through the CLIP image model\nand acts as a query of a kNN search. Although\nCLIP’s encoder was trained on web-collected sen-\ntences, we found it to work well enough also in\nour case, in which it is fed with only one unigram.\nFor efficiency reasons, we optimize this process\n5\nusing an index for approximate search (Johnson,\nDouze, & J´ egou, 2019). Compared to the tags\nextracted from object regions (Anderson et al.,\n2018; P. Zhang et al., 2021), the keywords we\nextract do not refer to local regions of the input\nimage, but rather correspond to the image as\na whole and have an increased semantic cover-\nage with respect to object detectors trained on\nstandard datasets.\nStyle token. To aid the generation process and\nseparate content from style, we give the model\nawareness of the kind of dataset to which every\ntraining caption belongs. This is done with a “style\ntoken”, which is implemented through a learnable\ntoken embedding and which can be concatenated\nto the representation of the keywords. For sim-\nplicity, we employ a style token with two possible\nvalues, one for encoding human-annotated sources\n(θ = human-annotated) and the second for encod-\ning web-collected sources ( θ = web-collected).\nNotice that, while θ = human-annotated actu-\nally refers to a uniform annotation style, θ =\nweb-collected can be thought as containing a\ncollection of heterogeneous descriptive styles, as\nweb-collected sources might have different styles\n(e.g., news captions and captions extracted from\nsocial media sites).\n3.2 Architecture\nOur approach represents each training image-\ncaption pair as a quadruple (V, W, K, θ) of image,\nground-truth caption, keywords, and style token,\nwhere V is encoded with a set of fixed-length\nvisual descriptors. The text input, including the\ncaption and keywords, are tokenized lower-cased\nByte Pair Encoding (BPE) (Sennrich, Haddow, &\nBirch, 2016).\nFor multimodal fusion, we define an encoder-\ndecoder Transformer architecture (Vaswani et al.,\n2017) where each layer of the encoder com-\nprises multi-head self-attention (MSA) and feed-\nforward layers, and each layer of the decoder\nincludes multi-head self-attention (MSA), multi-\nhead cross-attention (MSCA), and feed-forward\nlayers. To enable text generation, the decoder\nemploys sequence-to-sequence attention masks in\neach self-attention layer. The visual descriptors\nV = {vi}N\ni=1 are encoded via bi-directional atten-\ntion in the encoder, while textual keyword tokens\nK = {ki}M\ni=1, token embeddings of the caption\nW = {wi}L\ni=1, and the style token θ are inputs\nof the decoder, where N, M, and L indicates\nthe number of visual embeddings, keywords, and\ncaption tokens respectively. The overall network\noperates according to the following schema:\nencoder ˜vi = MSA(vi, V), ˜V = {˜ vi}N\ni=1\ndecoder\nOki = MSCA(ki, ˜V, K)\nOθ = MSCA(θ, ˜V, K ∪ θ)\nOwi = MSCA(wi, ˜V, K ∪ θ ∪ {wt}i\nt=1),\nwhere MSA(x, Y) indicates a self-attention with\nx mapped to query and Y mapped to key-values,\nand MSCA(x, Y, Z) is a self-attention with x as\nquery and Z as key-values, followed by a cross-\nattention with x as query and Y as key-values. O\nindicates the network output and ∪ indicates con-\ncatenation. We omit feed-forward layers and the\ndependency between consecutive layers for ease of\nnotation.\nUnlike a traditional decoder, the network is\nonly trained to predict a left-shifted version of\nthe caption tokens W, while the sequence K ∪ θ\nis treated as a prompt. Different from prompt-\ning in pre-trained language models (Gao, Fisch,\n& Chen, 2021; Radford et al., 2019), this prompt-\ning strategy is explicitly employed while training\nthe network. Further, in contrast to previous V&L\npre-training works which adopted a bidirectional\nMasked Language Modeling objective that tends\nto be suboptimal for sequence generation, we train\nour network by following a unidirectional language\nmodeling loss based on cross-entropy, i.e.\nL = −E(V,W)∼D\n LX\nt=1\nlog p(Owt |V, K, θ,wτ<t )\n!\n,\n(4)\nwhere D indicates the training dataset.\nIn the training stage, human-annotated and\nweb-collected image-text pairs are fed through the\nmodel, each with its corresponding style token.\nTo allow content transfer between the two types\nof data sources, it is important to maintain a\nsufficient balance between the two kind of data\nsources. To this end, we randomly select samples\nin each mini-batch to have at least 10% of image-\ncaption pairs with θ = human-annotated. In our\npreliminary experiments, we verified that such a\n6\nTable 1 Statistics on the training corpus.\nSource Type # Images # Words # Words in 0.1% Tail Length (mean ± std)\nCOCO Human annotated 112k 26,815 5,947 10.50 ± 2.42\nFlickr30k Human annotated 29k 17,798 1,793 12.34 ± 5.21\nOpen Images Generated (COCO) 1.7M 7,693 4,050 10.09 ± 1.59\nSBU Flickr desc. 875k 222,393 10,053 12.20 ± 6.10\nWIT Wikipedia desc. 3.1M 905,095 281,32 9.21 ± 8.49\nCC3M Alt-texts 3.1M 47,422 21,100 9.58 ± 4.30\nCC12M Alt-texts 12.2M 450,594 189,792 18.28 ± 13.59\nYFCC100M Alt-texts 14.6M 2,384,078 383,942 26.31 ± 70.48\nOverall 35.7M 3,180,785 666,519 19.40 ± 47.11\npercentage is enough to enable a smooth content\ntransfer between different data sources.\nInference. Once the model is trained, predic-\ntions are conditioned on the style token θ, which\ncan be chosen according to the desired genera-\ntion style ( i.e. that of human-annotated captions\nor that of noisy web-collected ones). Given key-\nwords and style token, at each time step t the\nmodel samples a token ˆwt from the output prob-\nability distribution. This is then concatenated to\npreviously predicted tokens to form a sequence\n{ ˆwτ }t\nτ=1 which is employed as the input for the\nnext iteration. Since the representation of out-\nput tokens does not depend on subsequent tokens,\nthe past intermediate representations are kept\nin memory to avoid repeated computation and\nincrease efficiency at prediction time.\nVisual features. To obtain the set of visual fea-\ntures V for an image, we employ the same visual\nCLIP model employed for keyword retrieval (Rad-\nford et al., 2021). Compared to using features\nextracted from object detectors (Hu et al., 2022;\nP. Zhang et al., 2021), this strategy is benefi-\ncial both in terms of computational efficiency and\nfeature quality. Specifically, we use a ViT-based\nvisual encoder. In the original CLIP model, acti-\nvations from the last layer of the encoder are\ndiscarded, except for those generated by the first\nquery of the input sequence which are used to form\na global descriptor. While global image vectors\ncoming from CLIP have been used in concurrent\ncaptioning works (Mokady et al., 2021), we instead\nemploy the entire grid of features coming from the\nlayer, so to preserve spatial awareness and better\nfeature granularity.\n4 Experimental Evaluation\nWe conduct extensive experiments to validate\nthe architectural choices behind our model and\ncompare its performances with state-of-the-art\nsolutions for image captioning.\n4.1 Training sources\nWe train on a mixture of datasets with image-\ncaption pairs, which are heterogeneous in terms\nof style and semantics, for a total of 35.7M\nimages. Our mixture contains COCO (T.-Y. Lin\net al., 2014), Flickr30k (Young et al., 2014),\nSBU (Ordonez et al., 2011), Conceptual Captions\n3M (Sharma et al., 2018) and 12M (Changpinyo\net al., 2021), WIT (Srinivasan et al., 2021), a sub-\nset of YFCC100M (Thomee et al., 2016), and a\nsubset of Open Images (Kuznetsova et al., 2020).\nThe SBU dataset contains captions automati-\ncally collected from the Flickr website, while Con-\nceptual Captions 3M and 12M have been obtained\nby cleaning image alt-text pairs from the web. The\nWIT dataset, instead, contains images extracted\nfrom Wikipedia together with alt texts. After\nfiltering out all non-English pages, the dataset\ncontains about 3.1M pairs. Finally, we use the\nsubset of YFCC100M (Thomee et al., 2016) con-\ntaining image descriptions (around 14.6M pairs),\nand 1.7M images from Open Images (Kuznetsova\net al., 2020), automatically annotated with cap-\ntions generated from OSCAR, following (P. Zhang\net al., 2021).\nIn Table 1, we report detailed statistics on\nthe data sources employed during training. Over-\nall, the mixture used for training our model has\nthree key features: (i) differently from the datasets\nemployed in concurrent works (X. Li et al., 2020;\nP. Zhang et al., 2021), it contains only data for the\nimage-captioning task, thus neglecting the use of\ndata from ancillary tasks like VQA or GQA; ( ii)\nit is made of publicly available data, thus allowing\nreproducibility, and does not employ proprietary\ndata (Jia et al., 2021; Wang et al., 2022); ( iii)\noverall, it contains 35.7 million images and around\n7\nKeywords: crosswalk, \nraincoat, rainy, taxi, \nraining.\nKeywords: westminster, \nlondon, parliament, uk, \nparliamentary.\nKeywords: flatbread, \nhomemade, pizza, \nmozzarella, gourmet.\ntextual keywords\nKeywords: pooh, bear, \nteddy, warmly, bears.\nKeywords: medieval, \nknight, knights, fencing, \ncontested.\nKeywords: 14th, \nnumbered, markings, \nwoolly, sheep.\nKeywords: horseback, \nriders, galloping, horses, \njockeys.\nKeywords: beagle, \nprisoners, fence, \nimprisoned, enclosure.\nKeywords: airport, \ntravelling, starbucks, \ntravels, passport.\nKeywords: crosswalk, \nraincoat, rainy, taxi, \nraining.\nKeywords: westminster, \nlondon, parliament, uk, \nparliamentary.\nKeywords: airport, \ntravelling, starbucks, \ntravels, passport.\nKeywords: pooh, bear, \nteddy, warmly, bears.\nKeywords: medieval, \nknight, knights, fencing, \ncontested.\nKeywords: 14th, \nnumbered, markings, \nwoolly, sheep.\nFig. 4 Sample textual keywords extracted on COCO\nimages.\n0.6M long-tail words ( i.e. that lie in the 0.1%\nof the distribution tail), making it sufficiently\nlarge and diverse to perform web-scale analyses on\nimage captioning.\nDuring training, we employ the style\ntoken for human-annotated sources ( θ =\nhuman-annotated) when dealing with sam-\nples from COCO, Flickr30k, and Open\nImages, and the one for web-collected sources\n(θ = web-collected) when dealing with sam-\nples from SBU, WIT, CC3M, CC12M, and\nYFCC100M.\n4.2 Implementation details\nArchitectural details. We devise three model\nconfigurations, varying the number of decoding\nlayers L, model dimensionality d, and number\nof attention heads H: Tiny ( L = 3, d = 384,\nH = 6, 52M params), Small ( L = 6, d = 512,\nH = 8, 87M params), and Base ( L = 12, d = 768,\nH = 12, 213M params). For all models, we employ\nCLIP-ViT-L/14 as visual feature and keyword\nextractor, three layers in the visual encoder, and\nfive textual keywords. To represent words, we use\nlower-cased Byte Pair Encoding (BPE) (Sennrich\net al., 2016) with a 49,152 vocab size and linearly\nproject them to the input dimensionality of the\nmodel d. We employ classic sinusoidal positional\nencodings (Vaswani et al., 2017) to represent word\npositions. For efficiency, the maximum sequence\nlength of the decoder is capped at 80.\nFollowing recent literature (Cornia et al.,\n2020), we enrich all layers of our encoder with\nmemory slots. Specifically, we extend the set\nof keys and values of each self-attention opera-\ntion with 40 additional learnable vectors, which\nare independent of the input sequence and can\nencode a priori information retrieved through\nattention. For fair comparison, this also applies to\nall baselines and ablation studies presented in the\nfollowing.\nTraining details. Training is performed using\nthe LAMB optimizer (You et al., 2020) and\nfollowing the learning rate scheduling strategy\nof (Vaswani et al., 2017) with a warmup equal\nto 6,000 iterations and multiplying the resulting\nlearning rate by a factor of 5. We use a minibatch\nsize of 1,080 and employ ZeRo memory offload-\ning (Rajbhandari, Rasley, Ruwase, & He, 2020)\nand mixed-precision (Micikevicius et al., 2018).\nAfter training with cross-entropy, we also fine-tune\nsome of our models on COCO using Reinforce-\nment Learning. During this fine-tuning stage, we\nemploy the SCST variant proposed in (Cornia et\nal., 2020) that sets the baseline reward equal to\nthe mean of rewards of generated captions inside\na beam. In this phase, we use the Adam opti-\nmizer (Kingma & Ba, 2015), a batch size equal to\n80, and a fixed learning rate of 5 × 10−6.\nKeyword extraction details and visualiza-\ntion. As previously mentioned, the keyword dic-\ntionary is composed by extracting around 11.5k\nunigrams from COCO, Visual Genome, and the\nOpenWebText corpus1. During pre-processing, all\nunigrams are converted into lowercase, and proper\nnames that identify persons are removed. Fig. 4\nreports keywords extracted from sample images\nof the COCO dataset. As it can be seen, they\nprovide significant and high-level information on\nthe global content of the image. Compared to\nkeywords extracted from object detectors, CLIP-\nbased keywords tend to include more long-tail\nconcepts.\nWeight initialization. We initialize all weights\nby drawing inspiration from GPT-2 (Radford et\nal., 2019). All linear and embedding weights are\ninitialized according to a uniform distribution and\nusing the approach proposed by (Glorot & Bengio,\n2010). Layer normalization weights are initialized\nto a constant value of 1. All biases are initialized to\n1https://skylion007.github.io/OpenWebTextCorpus\n8\nTable 2 Comparison with the state of the art on the COCO test split in a single model setting.\nFine-tuning\nTF SCST Training Images B-4 M R C S\nBLIPbase (J. Li et al., 2022) ✓ - 129M 39.7 - - 133.3 -\nBLIPlarge (J. Li et al., 2022) ✓ - 129M 40.4 - - 136.7 -\nSimVLMbase (Wang et al., 2022) ✓ - 1.8B 39.0 32.9 - 134.8 24.0\nSimVLMlarge (Wang et al., 2022) ✓ - 1.8B 40.3 33.4 - 142.6 24.7\nSimVLMhuge (Wang et al., 2022) ✓ - 1.8B 40.6 33.7 - 143.3 25.4\nLEMONbase (Hu et al., 2022) ✓ ✓ 200M 41.6 31.0 - 142.7 25.1\nLEMONlarge (Hu et al., 2022) ✓ ✓ 200M 42.3 31.2 - 144.3 25.3\nLEMONhuge (Hu et al., 2022) ✓ ✓ 200M 42.6 31.4 - 145.5 25.5\nOurstiny (θ = human-annotated) - ✓ 35.7M 42.8 31.0 61.2 148.4 24.6\nOurssmall (θ = human-annotated) - ✓ 35.7M 42.5 31.2 61.3 148.6 25.0\nOursbase (θ = human-annotated) - ✓ 35.7M 42.9 31.4 61.5 149.6 25.0\nOSCARbase (X. Li et al., 2020) ✓ ✓ 4.1M 40.5 29.7 - 137.6 22.8\nOSCARlarge (X. Li et al., 2020) ✓ ✓ 4.1M 41.7 30.6 - 140.0 24.5\nVinVLbase (P. Zhang et al., 2021) ✓ ✓ 5.8M 40.9 30.9 - 140.6 25.1\nVinVLlarge (P. Zhang et al., 2021) ✓ ✓ 5.8M 41.0 31.1 - 140.9 25.2\nOurstiny (θ = human-annotated) - ✓ 5.8M (VinVL data) 42.9 31.1 61.3 147.1 24.9\nOurssmall (θ = human-annotated) - ✓ 5.8M (VinVL data) 42.7 31.3 61.3 147.5 25.2\nOursbase (θ = human-annotated) - ✓ 5.8M (VinVL data) 43.2 31.4 61.7 147.8 25.4\n0. We also employ the “Special Scaled Initializa-\ntion” used in GPT-22 when initializing the output\nlinear projection of each Transformer layer. Again,\nthis also applies to all baselines.\n4.3 Captioning performance\nWe firstly assess the performance of our model\non human-annotated datasets, comparing with\nrecent models trained using both web-collected\nand human-annotated data, i.e. OSCAR (X. Li\net al., 2020), VinVL (P. Zhang et al., 2021),\nSimVLM (Wang et al., 2022), BLIP (J. Li et al.,\n2022), and LEMON (Hu et al., 2022)3. In contrast\nto these approaches that employ a teacher-forcing\n(TF) fine-tuning on COCO when comparing on\nCOCO and nocaps, we only employ the RL fine-\ntuning (SCST), which we observed being lighter in\nterms of forgetting impact and avoids forgetting\nconcepts learned on web-collected data. During\ngeneration, in this setting, we set the style token\nθ equal to the token embedding employed for\nhuman-annotated sources.\nEvaluation is reported in terms of the\nclassical captioning metrics: BLEU (Papineni,\n2A reference implementation can be found in\nhttps://github.com/huggingface/transformers/blob/main/\nsrc/transformers/models/gpt2/modeling gpt2.py#L493\n3The number of parameters of these models is as follows:\nVinVLbase (135M), VinVLlarge (370M), LEMONlarge (338M),\nLEMONhuge (675M), BLIPbase (224M), BLIPlarge (446M),\nSimVLMbase (86M), SimVLM large (307M), SimVLM huge\n(632M).\nRoukos, Ward, & Zhu, 2002), METEOR (Baner-\njee & Lavie, 2005), ROUGE (C.-Y. Lin, 2004),\nCIDEr (Vedantam, Lawrence Zitnick, & Parikh,\n2015), and SPICE (Anderson, Fernando, Johnson,\n& Gould, 2016).\nPerformance on COCO. The performances of\nour approach on COCO are reported in the upper\nportion of Table 2, in a single model setting. As\npresented, the proposed method exhibits better\nperformances than the compared methods, with-\nout requiring a teacher-forcing fine-tuning phase,\nusing less training data and fewer parameters.\nThe Tiny version of the proposed approach, for\ninstance, overcomes the performance of SimVLM,\nBLIP, and LEMON in their Base, Large, and Huge\nconfigurations according to the BLEU-4, ROUGE,\nand CIDEr metrics. Increasing model size further\naugment the performances, up to 149.6 CIDEr\npoints achieved by the Base version.\nAs additional comparison, we also report the\nperformance of our models when trained with\na dataset size that is directly comparable to\nthat used for OSCAR and VinVL, which also\nemploy the SCST fine-tuning stage. In particular,\nwe employ the same datasets used for train-\ning VinVL, excluding VQA sources. Results are\nreported in the bottom part of Table 2, and show\nincreased captioning metrics with respect to the\ncompared approaches.\nResults on nocaps. We then evaluate the\ncapabilities of our model on the nocaps\n9\nTable 3 Results on the nocaps dataset.\nValidation Set\nFine-tuning in near out overall\nTF SCST Training Ims C S C S C S C S\nOSCAR (X. Li et al., 2020) ✓ ✓ 112k (COCO) 85.4 11.9 84.0 11.7 80.3 10.0 83.4 11.4\nVIVO (Hu et al., 2020) ✓ ✓ 112k (COCO) 92.2 12.9 87.8 12.6 87.5 11.5 88.3 12.4\nVinVL (P. Zhang et al., 2021) ✓ ✓ 112k (COCO) 103.7 13.7 95.6 13.4 83.8 11.9 94.3 13.1\nBLIPbase (J. Li et al., 2022) ✓ - 129M 111.8 14.9 108.6 14.8 111.5 14.2 109.6 14.7\nBLIPlarge (J. Li et al., 2022) ✓ - 129M 114.9 15.2 112.1 14.9 115.3 14.4 113.2 14.8\nSimVLMhuge (Wang et al., 2022) ✓ - 1.8B 113.7 - 110.9 - 115.2 - 112.2 -\nLEMONlarge (Hu et al., 2022) ✓ - 200M 116.9 15.8 113.3 15.1 111.3 14.0 113.4 15.0\nLEMONhuge (Hu et al., 2022) ✓ - 200M 118.0 15.4 116.3 15.1 120.2 14.5 117.3 15.0\nOurstiny (θ = human-annotated) - ✓ 35.7M 122.3 14.8 115.3 14.6 116.1 13.6 116.5 14.5\nOurssmall (θ = human-annotated) - ✓ 35.7M 123.7 15.0 118.5 15.0 116.2 13.8 118.8 14.8\nOursbase (θ = human-annotated) - ✓ 35.7M 124.8 15.3 119.6 15.2 120.3 14.4 120.5 15.1\nVinVLbase (P. Zhang et al., 2021) ✓ ✓ 5.8M 112.4 14.7 104.2 14.3 93.1 12.7 103.1 14.1\nVinVLlarge (P. Zhang et al., 2021) ✓ ✓ 5.8M 115.3 15.2 105.6 14.7 96.1 13.0 105.1 14.4\nOurstiny (θ = human-annotated) - ✓ 5.8M (VinVL data) 121.4 14.9 115.7 14.8 110.6 13.5 115.5 14.6\nOurssmall (θ = human-annotated) - ✓ 5.8M (VinVL data) 120.0 15.4 117.1 15.2 112.0 13.9 116.5 15.0\nOursbase (θ = human-annotated) - ✓ 5.8M (VinVL data) 122.3 15.6 117.7 15.4 115.6 14.5 118.0 15.2\nTest Set\nSimVLMhuge (Wang et al., 2022) ✓ - 1.8B 109.0 14.6 110.8 14.6 109.5 13.9 110.3 14.5\nLEMONlarge (Hu et al., 2022) ✓ - 200M 111.2 15.6 112.3 15.2 105.0 13.6 110.9 15.0\nLEMONhuge (Hu et al., 2022) ✓ - 200M 112.8 15.2 115.5 15.1 110.1 13.7 114.3 14.9\nOurstiny (θ = human-annotated) - ✓ 35.7M 114.0 14.7 115.3 14.7 107.3 13.2 113.7 14.4\nOurssmall (θ = human-annotated) - ✓ 35.7M 117.6 15.3 117.9 15.0 113.3 13.7 117.1 14.8\nOursbase (θ = human-annotated) - ✓ 35.7M 118.8 15.5 120.4 15.4 114.0 14.1 119.1 15.2\nVinVLbase (P. Zhang et al., 2021) ✓ ✓ 5.8M 104.8 14.8 102.9 14.4 85.8 12.5 100.1 14.1\nVinVLlarge (P. Zhang et al., 2021) ✓ ✓ 5.8M 107.4 14.9 106.2 14.7 91.0 12.9 103.7 14.4\nOurstiny (θ = human-annotated) - ✓ 5.8M (VinVL data) 115.2 15.2 115.2 15.0 106.3 13.8 113.6 14.8\nOurssmall (θ = human-annotated) - ✓ 5.8M (VinVL data) 117.2 15.8 115.3 15.1 106.9 14.0 114.0 15.0\nOursbase (θ = human-annotated) - ✓ 5.8M (VinVL data) 116.0 15.6 117.4 15.4 110.2 14.4 115.9 15.2\ndataset (Agrawal et al., 2019), which contains\nout-of-domain images with respect to COCO.\nTable 3 reports the results for both the val-\nidation and test sets, at the top and bottom\npart respectively. As it can be seen, the pro-\nposed method achieves higher performances with\nrespect to previous approaches when tested on\nnocaps, which confirms its capability of describ-\ning out-of-domain concepts. Also in this case,\nour approach tends to achieve better performance\nusing fewer parameters than competitors. Our\nTiny configuration, for instance, achieves higher\ngeneration quality than SimVLMhuge on both the\nvalidation and test sets, while the Small config-\nuration is superior to LEMON huge. It should be\nnoted, nevertheless, that the performances on the\nout-of-domain portion of nocaps increase as the\ndimensionality of the model increases, from 116.1\nto 120.3 CIDEr points when comparing the Tiny\nconfiguration to the Base configuration of the pro-\nposed approach. Overall, the Base version of our\nVinVL: A small green insect\non a green plant.\nOurs: A damselfly sitting on\ntop of a blade of grass.\nVinVL: A yellow and white van\nparked on the side of a street.\nOurs: An ice cream van parked\nin front of the Big Ben.\nVinVL: A plate of onion rings\non a blue plate.\nOurs: A bunch of pretzels on\na black plate.\nVinVL: A person holding a\ncan of Pepsi in their hand.\nOurs: A person holding a can\nof Emerald Mountain Blend.\nVinVL: A plate of rice and a\ncup of beer on a table.\nOurs: A plate of popcorn and\na glass of beer on a table.\nVinVL: A bee is sitting on a\npurple flower.\nOurs: A bumble bee on a\npurple flower.\nFig. 5 Comparison of captions generated by VinVL and\nthose generated by our approach on sample images from\nnocaps.\nmodel overcomes all the reported competitors, tes-\ntifying the advantage of the proposed strategies\nwhen describing out-of-domain concepts.\n10\nTable 4 Results on the CC3M validation split.\nTF Fine-tuning Training Images B-4 M R C S\nLEMONbase (Hu et al., 2022) - 200M 10.1 11.9 - 108.1 19.8\nLEMONbase (Hu et al., 2022) ✓ 200M 10.1 12.0 - 111.9 20.5\nLEMONlarge (Hu et al., 2022) ✓ 200M 10.8 12.3 - 117.4 21.0\nLEMONhuge (Hu et al., 2022) ✓ 200M 13.0 13.9 - 136.8 23.2\nOurstiny (θ = web-collected) ✓ 35.7M 10.6 13.1 30.0 121.3 23.0\nOurssmall (θ = web-collected) ✓ 35.7M 11.6 13.5 30.5 130.0 23.6\nOursbase (θ = web-collected) - 35.7M 9.2 12.1 27.8 105.7 20.9\nOursbase (θ = web-collected) ✓ 35.7M 13.2 14.2 31.4 144.4 24.7\nTable 5 Ablation study on the COCO test split and nocaps validation split.\nTraining Images: 35.7M Training Images: 5.8M (VinVL data)\nCOCO nocaps COCO nocaps\nB-4 M R C S C S B-4 M R C S C S\nw/o web-collected data 40.6 30.0 59.9 139.4 23.9 88.9 12.6 40.6 30.0 59.9 139.4 23.9 88.9 12.6\nw/o keywords and style token 42.5 30.6 60.8 145.8 24.2 108.8 13.6 42.4 30.9 61.0 145.4 24.8 108.2 14.2\nw/o style token 42.5 30.7 60.9 146.5 24.3 110.1 13.6 42.5 31.0 61.1 146.2 24.8 108.6 14.2\nOurstiny (θ = human-annotated) 42.8 31.0 61.2 148.4 24.6 116.5 14.5 42.9 31.1 61.3 147.1 24.9 115.5 14.6\nw/o web-collected data 40.9 30.4 60.1 141.5 24.5 89.1 12.8 40.9 30.4 60.1 141.5 24.5 89.1 12.8\nw/o keywords and style token 42.3 31.0 60.9 147.5 24.7 112.2 14.2 42.7 31.1 61.1 146.2 25.0 105.6 14.2\nw/o style token 42.1 31.0 61.0 148.1 24.8 113.7 14.4 43.0 31.3 61.1 147.2 25.0 106.5 14.3\nOurssmall (θ = human-annotated) 42.5 31.2 61.3 148.6 25.0 118.8 14.8 42.7 31.3 61.3 147.5 25.2 116.5 15.0\nw/o web-collected data 41.4 30.2 60.2 142.0 24.1 89.2 12.6 41.4 30.2 60.2 142.0 24.1 89.2 12.6\nw/o keywords and style token 42.6 31.3 61.2 147.9 25.1 114.7 14.8 42.6 31.1 61.2 146.8 24.8 109.4 14.3\nw/o style token 42.5 31.4 61.2 149.2 25.0 116.6 14.9 42.7 31.3 61.5 147.0 25.2 109.5 14.5\nOursbase (θ = human-annotated) 42.9 31.4 61.5 149.6 25.0 120.5 15.1 43.2 31.4 61.7 147.8 25.4 118.0 15.2\nWe also investigate the behavior of the model\nwhen trained with the same data of VinVL, and\nthus on a smaller dataset. Compared to VinVL,\nwhich also employs SCST fine-tuning, our model\nis superior according to all metrics. Scaling the\ndataset size from 5.8M to 35.7M images brings,\nhowever, a significant improvement: from 147.8\nto 149.6 CIDEr points on COCO, and from\n118.0 to 120.5 CIDEr points on nocaps. To fur-\nther validate the results on nocaps, we show\nin Fig. 5 sample images and corresponding tex-\ntual descriptions generated by our model (with\nθ = human-annotated) in comparison to those\ngenerated by VinVL.\nResults on CC3M. We also test our approach\non Conceptual Captions 3M, which contains web-\ncollected annotations. During generation, we set\nthe style tokenθ to the token embedding employed\nfor web-collected sources, so to have an appropri-\nate generation style. We also test our model in\na zero-shot setting ( i.e. without any fine-tuning),\nand when fine-tuning with teacher forcing on\nthis dataset. Results are reported in Table 4, in\ncomparison with different configurations of the\nLEMON model. Without fine-tuning, our model\nachieves a better generation quality in terms of\nMETEOR and SPICE, with slightly lower CIDEr\nscores. Table 4 also compares when running a fine-\ntuning stage, where our model overcomes LEMON\naccording to all metrics, which again confirms the\nsuperiority of the proposed approach.\n4.4 Ablation and analysis\nSeparation of semantics and style. In Table 5\nwe investigate the role of using retrieval-based key-\nwords and the style token. Specifically, we report\nthe results for all three versions of our model\nusing different amounts of image-caption pairs\nduring training. We notice that adding keywords\nalone provides an improvement on both COCO\nand nocaps and that the benefit is especially evi-\ndent on out-of-domain captioning and with large\nmodels, confirming that retrieval-based keywords\nhelp to deal with out-of-domain concepts. The\ncombination of keywords and the style token θ\nfor human-annotated style, instead, further boosts\nperformances on all model configurations. For\ninstance, when using the Tiny configuration and\nthe entire training dataset, it increases COCO per-\nformances from 145.8 CIDEr points to 148.4. The\n11\nTable 6 Comparison with different tags sources on the COCO test split and nocaps validation split. All models are\ntrained on the COCO dataset only.\nCOCO nocaps\nTags/Keywords B-4 M R C S C S\nOurstiny None 40.6 30.0 59.9 139.4 23.9 88.9 12.6\nOurstiny Faster R-CNN 41.0 30.2 60.1 141.1 23.9 89.8 12.5\nOurstiny ResNeXt-152 C4 40.8 30.3 60.1 140.3 24.0 89.9 12.7\nOurstiny Retrieval-based 41.1 30.4 60.2 141.9 24.1 90.7 12.7\nOursbase None 41.4 30.2 60.2 142.0 24.1 89.2 12.6\nOursbase Faster R-CNN 40.5 30.3 60.3 140.5 23.7 90.2 13.3\nOursbase ResNeXt-152 C4 40.5 30.2 60.0 141.7 23.8 88.4 12.3\nOursbase Retrieval-based 41.6 30.6 60.7 143.4 24.2 90.5 13.4\nTable 7 Results when conditioning on different styles on the COCO test split, the CC3M and WIT validation splits, and\nsample images from LAION-400M. Models are trained with cross-entropy loss on the entire training corpus.\nCOCO CC3M WIT LAION-400M\nStyle token B-4 C S B-4 C S B-4 C S B-4 C S\nOursbase θ = web-collected 25.7 86.3 15.5 9.2 105.7 20.9 2.0 25.9 6.5 4.1 58.1 9.4\nOursbase θ = human-annotated 39.7 132.3 23.2 5.0 58.8 14.6 0.9 12.3 4.2 2.1 31.2 6.9\nsame improvement is maintained when increasing\nmodel size, e.g. the Base configuration moves from\n147.9 to 149.6 CIDEr points. Also in this case,\nthe improvement is more significant on nocaps,\nwhere the Base configuration is improved from\n114.7 to 120.5 CIDEr points, confirming that the\ntwo proposed strategies, together, help to trans-\nfer semantic concepts across descriptive styles.\nSimilar improvements can also be noticed when\ntraining on a smaller dataset ( i.e. on the same\ndata of VinVL).\nComparison with other keywords strate-\ngies. We compare the proposed retrieval-based\ntextual keywords with existing alternatives: tags\nextracted from Faster R-CNN trained on Visual\nGenome (Krishna et al., 2017) and tags extracted\nfrom a ResNeXt-152 C4, trained on the same\ndata used in VinVL. Results are reported in\nTable 6 using both Tiny and Base versions, in\nwhich we train separate models with different\nkeywords on COCO only, thus in a purely lim-\nited in-domain setting where transfer from web\nsources is not allowed. This choice is motivated by\ncomputational requirements, as the cost of run-\nning object detectors on large-scale data would\nhave been intractable. As semantic transfer from\nweb sources is not allowed, this setting is also\nless favorable for the proposed retrieval-based\nstrategy. Employing the proposed tag approach,\nhowever, improves caption quality on both COCO\nand nocaps according to all metrics, bringing an\nθ = human-annotated: \nA display case filled with \ndifferent types of donuts.\nθ = web-collected: \nThe best donuts I have ever \nhad.\nθ = human-annotated: \nA black and white photo of a \ntrain station.\nθ = web-collected: \nUnion Station, Toronto.\nθ = human-annotated: \nA bathroom sink with a large \nmirror above it.\nθ = web-collected: \nProperty image, #Luxury \nfamily home.\nθ = human-annotated: \nAn elephant is standing in a \ngrassy field.\nθ = web-collected: \nThe African bush elephant or \nAfrican savanna elephant is \nthe larger of the two species \nof African elephant.\nθ = human-annotated: \nA red truck cake sitting on \ntop of a wooden table.\nθ = web-collected: \nI made this cake for a \nfriend's birthday. It was a red \nvelvet cake with raspberry \nbuttercream.\nθ = human-annotated: \nA person holding a cell phone \nwith a picture of a ufo on it.\nθ = web-collected: \nThis is what happens when \nyou take a picture of a \npicture of a cow with a ufo in \nthe background.\nFig. 6 Sample images from the COCO dataset and cap-\ntions predicted by the Base version of our model when\nconditioned on different styles.\nimprovement on the COCO dataset of around 0.8\nand 1.4 CIDEr points respectively for the Tiny\nand Base versions.\nEffect of the style token. We also assess the\nsignificance of the style token value. To this aim,\nwe extract captions generated by our method\nwhen fed with both style token values. For this\nexperiment, we report the results on the stan-\ndard COCO test set and the CC3M and WIT\nvalidation splits. We also include the results on\n12\nTable 8 Performances on the FlickrStyle10k and SentiCap test splits.\nθ = humorous θ = romantic θ = positive θ = negative\nB-1 B-3 M C B-1 B-3 M C B-1 B-3 M C B-1 B-3 M C\nMSCap (Guo et al., 2019) 16.3 1.9 5.3 15.2 17.0 2.0 5.4 10.1 46.9 16.2 16.8 55.3 45.5 15.4 16.2 51.6\nMemCap (Zhao et al., 2020) 19.9 4.3 7.4 19.4 21.2 4.8 8.4 22.4 50.8 17.1 16.6 54.4 48.7 19.6 15.8 60.6\nSAN (G. Li, Zhai, Lin, & Zhang, 2021) 29.5 9.9 12.5 47.2 30.9 10.9 13.0 53.3 53.0 23.4 18.1 72.0 51.2 20.5 17.6 67.0\nOurstiny 32.9 12.9 15.6 59.9 34.4 14.3 15.3 61.6 54.5 21.6 19.1 91.7 51.4 22.3 17.5 83.9\nTable 9 User study results on sample images from the Open Images dataset. We report the percentage of times our full\nmodel (i.e. Oursbase with θ = human-annotated) is preferred against a competitor. Notably, our model is always\npreferred more than 50% of the times.\nRichness Coherency Fluency\nvs. VinVLlarge (P. Zhang et al., 2021) 59.1 58.8 53.2\nvs. Oursbase (θ = human-annotated) – w/o web-collected data 70.7 71.3 58.0\nvs. Oursbase (θ = human-annotated) – w/o keywords and style token 62.7 68.1 54.8\nvs. Oursbase (θ = web-collected) 53.0 63.6 67.2\n30k images randomly extracted from the LAION-\n400M dataset (Schuhmann et al., 2021) to validate\nthe effectiveness of the style token on a famous\nweb-collected dataset not employed during the\ntraining of our model. Results are reported in\nTable 7. As it can be seen, choosing the proper\nstyle token value significantly increases the quality\nof the generation on all considered datasets, high-\nlighting that the model has learned to mimic both\nclean annotations and web-collected ones. As a\ncomplement to this experiment, we show in Fig. 6\nsome qualitative results on sample images from\nthe COCO dataset when varying the style token\nduring inference.\nTo further analyze the effectiveness of our\nmodel in correctly following a given conditioning\nsignal in the form of style tokens, we experiment\nwith the FlickrStyle10k (Gan et al., 2017) and\nSentiCap (Mathews et al., 2016) datasets which\nare commonly used in stylized captioning liter-\nature. While the former contains image-caption\npairs with humorous and romantic styles, the lat-\nter contains images and corresponding captions\nwith positive and negative sentiments. For fair\ncomparison with other methods, we train the Tiny\nversion of our model following the training proto-\ncol described in (G. Li et al., 2021) and without\nemploying web-scale data. In this case, we use a\ndifferent style token for each of the four styles\ncontained in the two datasets plus one for fac-\ntual captions (i.e. captions without a specific style\nor sentiment). Results are shown in Table 8 com-\nparing our model with other previous captioning\napproaches focused on the generation of stylized\ncaptions. Notably, our approach can also handle a\nlarger number of styles and perform competitively\ncompared to methods specifically designed for the\nstylized captioning task.\nUser study. In addition to the standard quan-\ntitative metrics used in the previous analyses, we\nconduct a user study to fully validate the proposed\nmodel. In particular, we recruited 30 different par-\nticipants and asked them to select either our full\nmodel or one of the competitors or baselines, judg-\ning in terms of (1) richness of semantics , (2)\ncoherency with the image , and (3) linguistic flu-\nency. Participants could also state that captions\nwere equivalent on one or more evaluation axes. In\nthis case, 0.5 points are given to both competitors.\nWe perform the user study on a random sub-\nset of Open Images (Kuznetsova et al., 2020)\ncomposed of 1,000 images. As shown in Table 9,\nadding large-scale data, keywords, and the style\ntoken increases the richness of the generated cap-\ntions while maintaining coherency and fluency.\nMoreover, the use of a cleaned style ( i.e. θ =\nhuman-annotated) also increases fluency and\ncoherency with the input image. Even when\ncomparing our model with the Large version of\nVinVL (P. Zhang et al., 2021), the user study\nresults confirm the effectiveness of our approach\nand the improvement over the competitor espe-\ncially in terms of richness of semantics and\ncoherency with the image.\n13\nTable 10 Zero-shot performances on the VizWiz test split and the TextCaps validation split.\nVizWiz TextCaps\nZero-Shot Training Images B-4 M R C S B-4 M R C S\nUp-Down (Anderson et al., 2018) ✓ 112k 11.3 12.6 35.8 18.9 5.8 12.4 13.3 33.7 24.2 8.7\nAoANet (Huang et al., 2019) ✓ 112k 13.2 13.4 37.6 19.4 6.2 18.1 17.7 41.4 32.3 11.2\nUp-Down (Anderson et al., 2018) ✗ 23k/22k 19.8 18.4 43.2 49.7 12.2 20.1 17.8 42.9 41.9 11.7\nAoANet (Huang et al., 2019) ✗ 23k/22k 23.2 20.3 47.1 60.5 14.0 20.4 18.9 42.9 42.7 13.2\nVinVLbase (P. Zhang et al., 2021) ✓ 5.8M 16.9 15.8 41.1 34.7 9.9 17.3 16.5 38.9 41.2 13.1\nVinVLlarge (P. Zhang et al., 2021) ✓ 5.8M 17.4 16.3 41.7 37.7 10.3 17.5 16.6 38.9 41.9 13.1\nOurstiny (θ = human-annotated) ✓ 5.8M (VinVL data) 22.5 18.7 45.7 56.9 14.1 20.5 17.9 41.0 53.0 14.6\nOurssmall (θ = human-annotated) ✓ 5.8M (VinVL data) 22.2 18.9 45.5 58.1 14.3 20.7 18.1 41.1 54.4 14.7\nOursbase (θ = human-annotated) ✓ 5.8M (VinVL data) 22.5 19.2 45.9 59.6 14.9 20.6 18.2 41.2 55.4 15.0\nOurstiny (θ = human-annotated) ✓ 35.7M 23.6 19.3 46.4 65.6 14.8 20.7 18.0 41.1 58.6 14.6\nOurssmall (θ = human-annotated) ✓ 35.7M 24.5 19.9 47.2 70.2 15.3 21.9 18.9 42.3 66.0 15.4\nOursbase (θ = human-annotated) ✓ 35.7M 25.7 20.3 47.9 76.2 16.2 23.6 19.4 42.8 69.9 15.9\nTable 11 Long-tail description performances on the validation splits of Open Images, ImageNet-21K and CC3M.\nOpen Images\nTraining Images Long-tail Words Named Words CLIP-S PAC-S\nVinVLbase (P. Zhang et al., 2021) 5.8M 149 57 0.729 0.822\nVinVLlarge (P. Zhang et al., 2021) 5.8M 186 68 0.736 0.828\nOursbase (θ = human-annotated) 5.8M (VinVL data) 628 176 0.761 0.849\nOursbase (θ = human-annotated) 35.7M 884 254 0.759 0.851\nImageNet-21K\nTraining Images Long-tail Words Named Words CLIP-S PAC-S\nVinVLbase (P. Zhang et al., 2021) 5.8M 149 64 0.713 0.813\nVinVLlarge (P. Zhang et al., 2021) 5.8M 194 72 0.721 0.820\nOursbase (θ = human-annotated) 5.8M (VinVL data) 789 172 0.756 0.850\nOursbase (θ = human-annotated) 35.7M 1152 261 0.759 0.851\nCC3M\nTraining Images Long-tail Words Named Words CLIP-S PAC-S\nVinVLbase (P. Zhang et al., 2021) 5.8M 84 46 0.732 0.824\nVinVLlarge (P. Zhang et al., 2021) 5.8M 95 45 0.735 0.829\nOursbase (θ = human-annotated) 5.8M (VinVL data) 572 112 0.777 0.856\nOursbase (θ = human-annotated) 35.7M 581 162 0.778 0.856\n4.5 Zero-shot and long-tail\ndescription\nTwo of the main benefits of employing web-\ncollected data are zero-shot generalization and\nthe description of long-tail entities. In the follow-\ning, we consider the VizWiz dataset (Gurari et\nal., 2020), which contains images originating from\nblind people, and TextCaps (Sidorov et al., 2020),\nwith images containing text for zero-shot gener-\nalization. Both of them represent distinct visual\nand semantic distributions from the COCO ones.\nFurther, we also investigate the capabilities of the\nproposed approach to deal with long-tail concepts\nand generate named entities.\nZero-shot performances on VizWiz and\nTextCaps. Table 10 shows a comparison when\nusing Up-Down (Anderson et al., 2018) and\nAoANet (Huang et al., 2019) in a zero-shot man-\nner trained on COCO and when fine-tuning them\non the aforementioned datasets. We also com-\npare with the Base and Large configurations of\nVinVL (P. Zhang et al., 2021). As shown, the\nproposed approach consistently outperforms the\nperformances of Up-Down and AoANet when\nevaluated in a zero-shot setting. Interestingly, it\nalso overcomes these approaches when trained\non VizWiz and TextCaps, confirming that the\nmodel is capable of properly transferring semantic\nconcepts learned from web-collected annotations.\nFurther, our approach also beats VinVL in both\nconfigurations by a significant margin, highlight-\ning the appropriateness of the proposed strategies\n14\nwith respect to previous literature. For complete-\nness, we also report the zero-shot performances of\nour approach when training on 5.8M images. Even\nwhen using less training data our approach still\nshowcases good zero-shot prediction capabilities.\nLong-tail and named entities description.\nWe assess the capability of our approach to name\nlong-tail concepts and named entities. In par-\nticular, we consider the validation sets of three\ndatasets with a large variety of concepts: Open\nImages V6 (Kuznetsova et al., 2020) (subset with\nbounding boxes), ImageNet-21K (Ridnik, Ben-\nBaruch, Noy, & Zelnik-Manor, 2021), and CC3M.\nWe count the number of unique words which do\nnot appear in COCO at least 5 times ( i.e. termed\nas long-tail words), and the number of named\nentities extracted using the spaCy NLP toolkit 4.\nIn this setting, we evaluate caption quality using\nthe CLIP-Score (Hessel, Holtzman, Forbes, Bras,\n& Choi, 2021) and PAC-Score (Sarto, Barraco,\nCornia, Baraldi, & Cucchiara, 2023) metrics, that\nboth are based on CLIP features coming from a\nViT-B/32 model, do not require ground-truth cap-\ntions, and have a high correlation with human\njudgments. Table 11 shows the results of both\nversions of our Base model trained on 5.8M and\n35.7M images, in comparison with VinVL. Our\napproach is capable of naming significantly more\nwords that are outside of COCO and also consis-\ntently generates more named entities than VinVL.\nThis improvement is further confirmed by the\nresults in terms of CLIP-S and PAC-S which are\nsignificantly better than those obtained by VinVL.\nReducing the amount of web-collected sources\nimpacts performances, with a significant reduc-\ntion in the number of long-tail words and named\nentities produced during generation. We notice,\nthough, that even when using the same dataset\nsize our approach beats VinVL according to all\nevaluation metrics in both its Base and Large\nconfigurations, further confirming the appropri-\nateness of the proposed approach, and of avoiding\nteacher-forcing fine-tuning.\n4.6 Qualitative results\nFinally, we showcase the capabilities of our\napproach of generating pertinent captions with\nnamed entities ( i.e. recognizable objects, places,\n4https://spacy.io/\nVinVL: A statue of a man\nholding a beer in his hand.\nOurs: The incredible Hulk in\na scene from the movie.\nVinVL: A blue and yellow\nplane flying in the sky.\nOurs: A Goodyear Blimp is\nflying in the sky.\nVinVL: A picture of a statue\non top of a building.\nOurs: Silhouette of the Rio de\nJaneiro skyline with a banner.\nVinVL: A large rock formation\non the side of a desert.\nOurs: The Pyramid of Djoser\nwith a blue sky.\nVinVL: A couple of cars\nparked in front of a building.\nOurs: Two Tesla cars parked\nin front of a building.\nVinVL: A man walking past a\nsoccer ball on the field.\nOurs: The Duke of Cambridge\nis playing with a soccer ball.\nFig. 7 Comparison of captions generated by VinVL and\nthose generated by our approach on sample images from\nCC3M and Open Images.\nand people) through some qualitative examples.\nIn Fig. 1, on the second page, we compare with\nthe baseline Tiny model trained without web-\nscale data and keywords (reported in Table 5).\nWe observe that our approach correctly rec-\nognizes and names famous people, places, and\ntrademarks like the Burj Al Arab , Marilyn Mon-\nroe, or the Facebook logo . The same can be\nobserved in Fig. 5 and 7, where we compare with\nVinVLlarge (P. Zhang et al., 2021) on images taken\nfrom nocaps, OpenImages, and CC3M. Again, our\nmodel can recognize and name long-tail concepts\nbetter than previous approaches, also recogniz-\ning famous people and places as for example the\nDuke of Cambridge and the Pyramid of Djoser .\nAdditional qualitative results are reported in the\nAppendix.\n5 Conclusion\nWe proposed an approach for captioning images\nwith fluent and pertinent descriptions while train-\ning on non-uniform data sources. Our approach\nrelies on the separation of semantics and style\nand the usage of retrieval-based textual keywords,\nand allows to learn from noisy web-collected\nsources while maintaining a fluent descriptive\nstyle. Experimentally, it achieves state-of-the-art\nresults on different datasets, including COCO,\nCC3M and nocaps, demonstrating its effective-\nness in both in-domain and out-of-domain image\ncaptioning.\n15\nAcknowledgments. We thank CINECA for\nproviding computational resources. This work\nhas been supported by the PNRR-M4C2 project\n(PE00000013) “FAIR - Future Artificial Intelli-\ngence Research” funded by the European Com-\nmission and the PRIN “CREATIVE: CRoss-\nmodal understanding and gEnerATIon of Visual\nand tExtual content” co-funded by the Ital-\nian Ministry of University and Research (CUP\nB87G22000460001).\nReferences\nAgrawal, H., Desai, K., Chen, X., Jain, R., Batra,\nD., Parikh, D., . . . Anderson, P. (2019).\nnocaps: novel object captioning at scale.\nProceedings of the IEEE/CVF International\nConference on Computer Vision.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A.,\nBarr, I., Hasson, Y., . . . others (2022).\nFlamingo: a Visual Language Model for\nFew-Shot Learning. Advances in Neural\nInformation Processing Systems.\nAnderson, P., Fernando, B., Johnson, M., Gould,\nS. (2016). SPICE: Semantic Propositional\nImage Caption Evaluation. Proceedings\nof the European Conference on Computer\nVision.\nAnderson, P., He, X., Buehler, C., Teney,\nD., Johnson, M., Gould, S., Zhang, L.\n(2018). Bottom-up and top-down attention\nfor image captioning and visual question\nanswering. Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition.\nBanerjee, S., & Lavie, A. (2005). METEOR:\nAn automatic metric for MT evaluation\nwith improved correlation with human judg-\nments. Proceedings of the Annual Meeting of\nthe Association for Computational Linguis-\ntics Workshops.\nBarraco, M., Cornia, M., Cascianelli, S., Baraldi,\nL., Cucchiara, R. (2022). The Unreasonable\nEffectiveness of CLIP Features for Image\nCaptioning: An Experimental Analysis.Pro-\nceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition\nWorkshops.\nBarraco, M., Stefanini, M., Cornia, M., Cas-\ncianelli, S., Baraldi, L., Cucchiara, R.\n(2022). CaMEL: Mean Teacher Learning\nfor Image Captioning. Proceedings of the\nInternational Conference on Pattern Recog-\nnition.\nBrown, T., Mann, B., Ryder, N., Subbiah,\nM., Kaplan, J.D., Dhariwal, P., . . . oth-\ners (2020). Language models are few-shot\nlearners. Advances in Neural Information\nProcessing Systems.\nChangpinyo, S., Sharma, P., Ding, N., Soricut,\nR. (2021). Conceptual 12M: Pushing Web-\nScale Image-Text Pre-Training To Recognize\nLong-Tail Visual Concepts. Proceedings of\nthe IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nChen, T., Zhang, Z., You, Q., Fang, C., Wang,\nZ., Jin, H., Luo, J. (2018). “Fac-\ntual”or“Emotional”: Stylized Image Cap-\ntioning with Adaptive Learning and Atten-\ntion. Proceedings of the European Confer-\nence on Computer Vision.\nChen, Y.-C., Li, L., Yu, L., El Kholy, A., Ahmed,\nF., Gan, Z., . . . Liu, J. (2020). UNITER:\nLearning UNiversal Image-TExt Represen-\ntations. Proceedings of the European Con-\nference on Computer Vision.\nCornia, M., Stefanini, M., Baraldi, L., Cucchiara,\nR. (2020). Meshed-Memory Transformer\nfor Image Captioning. Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nDevlin, J., Chang, M.-W., Lee, K., Toutanova, K.\n(2018). BERT: Pre-training of deep bidi-\nrectional transformers for language under-\nstanding. Proceedings of the Annual Confer-\nence of the North American Chapter of the\nAssociation for Computational Linguistics.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weis-\nsenborn, D., Zhai, X., Unterthiner, T.,\n. . . Houlsby, N. (2021). An Image is\n16\nWorth 16x16 Words: Transformers for Image\nRecognition at Scale. Proceedings of the\nInternational Conference on Learning Rep-\nresentations.\nGan, C., Gan, Z., He, X., Gao, J., Deng, L.\n(2017). StyleNet: Generating Attractive\nVisual Captions With Styles. Proceedings\nof the IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nGao, T., Fisch, A., Chen, D. (2021). Making Pre-\ntrained Language Models Better Few-shot\nLearners. Proceedings of the Annual Meet-\ning of the Association for Computational\nLinguistics.\nGlorot, X., & Bengio, Y. (2010). Understanding\nthe difficulty of training deep feedforward\nneural networks. Proceedings of the inter-\nnational conference on artificial intelligence\nand statistics.\nGuo, L., Liu, J., Yao, P., Li, J., Lu, H. (2019).\nMSCap: Multi-Style Image Captioning with\nUnpaired Stylized Text. Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nGurari, D., Zhao, Y., Zhang, M., Bhattacharya, N.\n(2020). Captioning Images Taken by People\nWho Are Blind. Proceedings of the European\nConference on Computer Vision.\nHessel, J., Holtzman, A., Forbes, M., Bras,\nR.L., Choi, Y. (2021). CLIPScore: A\nReference-free Evaluation Metric for Image\nCaptioning. Proceedings of the Conference\non Empirical Methods in Natural Language\nProcessing.\nHu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu,\nY., Wang, L. (2022). Scaling Up Vision-\nLanguage Pre-training for Image Caption-\ning. Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern\nRecognition.\nHu, X., Yin, X., Lin, K., Wang, L., Zhang, L., Gao,\nJ., Liu, Z. (2020). VIVO: Visual Vocabu-\nlary Pre-Training for Novel Object Caption-\ning. Proceedings of the AAAI Conference on\nArtificial Intelligence.\nHuang, L., Wang, W., Chen, J., Wei, X.-Y. (2019).\nAttention on Attention for Image Caption-\ning. Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision.\nJia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh,\nZ., Pham, H., . . . Duerig, T. (2021). Scaling\nUp Visual and Vision-Language Representa-\ntion Learning With Noisy Text Supervision.\nProceedings of the International Conference\non Machine Learning.\nJohnson, J., Douze, M., J´ egou, H. (2019). Billion-\nscale similarity search with gpus. IEEE\nTrans. on Big Data , 7(3), 535–547,\nKarpathy, A., & Fei-Fei, L. (2015). Deep visual-\nsemantic alignments for generating image\ndescriptions. Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition.\nKim, W., Son, B., Kim, I. (2021). ViLT:\nVision-and-Language Transformer Without\nConvolution or Region Supervision. Pro-\nceedings of the International Conference on\nMachine Learning.\nKingma, D.P., & Ba, J. (2015). Adam: A Method\nfor Stochastic Optimization. Proceedings of\nthe International Conference on Learning\nRepresentations.\nKlein, F., Mahajan, S., Roth, S. (2021). Diverse\nImage Captioning with Grounded Style.\nProceeding of the DAGM German Confer-\nence on Pattern Recognition.\nKrishna, R., Zhu, Y., Groth, O., Johnson, J.,\nHata, K., Kravitz, J., . . . Fei-Fei, L. (2017).\nVisual Genome: Connecting Language and\nVision Using Crowdsourced Dense Image\nAnnotations. International Journal of Com-\nputer Vision , 123(1), 32–73,\nKuznetsova, A., Rom, H., Alldrin, N., Uijlings,\nJ., Krasin, I., Pont-Tuset, J., . . . Ferrari,\nV. (2020). The Open Images Dataset V4.\n17\nInternational Journal of Computer Vision ,\n128(7), 1956–1981,\nLi, G., Zhai, Y., Lin, Z., Zhang, Y. (2021). Simi-\nlar Scenes arouse Similar Emotions: Parallel\nData Augmentation for Stylized Image Cap-\ntioning. Proceedings of the ACM Interna-\ntional Conference on Multimedia.\nLi, J., Li, D., Savarese, S., Hoi, S. (2023). BLIP-\n2: Bootstrapping Language-Image Pre-\ntraining with Frozen Image Encoders and\nLarge Language Models. Proceedings of\nthe International Conference on Machine\nLearning.\nLi, J., Li, D., Xiong, C., Hoi, S. (2022).\nBLIP: Bootstrapping Language-Image Pre-\ntraining for Unified Vision-Language Under-\nstanding and Generation. Proceedings of\nthe International Conference on Machine\nLearning.\nLi, X., Yin, X., Li, C., Zhang, P., Hu, X.,\nZhang, L., . . . others (2020). Oscar:\nObject-Semantics Aligned Pre-training for\nVision-Language Tasks. Proceedings of the\nEuropean Conference on Computer Vision.\nLi, Y., Pan, Y., Yao, T., Mei, T. (2022). Compre-\nhending and Ordering Semantics for Image\nCaptioning. Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition.\nLin, C.-Y. (2004). Rouge: A package for auto-\nmatic evaluation of summaries. Proceedings\nof the Annual Meeting of the Association for\nComputational Linguistics Workshops.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J.,\nPerona, P., Ramanan, D., . . . Zitnick, C.L.\n(2014). Microsoft COCO: Common Objects\nin Context. Proceedings of the European\nConference on Computer Vision.\nLu, J., Batra, D., Parikh, D., Lee, S. (2019).\nViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-\nLanguage Tasks. Advances in Neural Infor-\nmation Processing Systems.\nMathews, A., Xie, L., He, X. (2016). SentiCap:\nGenerating Image Descriptions with Senti-\nments. Proceedings of the AAAI Conference\non Artificial Intelligence.\nMathews, A., Xie, L., He, X. (2018). Semstyle:\nLearning to generate stylised image captions\nusing unaligned text. Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nMicikevicius, P., Narang, S., Alben, J., Diamos,\nG., Elsen, E., Garcia, D., . . . Wu, H. (2018).\nMixed Precision Training. Proceedings of\nthe International Conference on Learning\nRepresentations.\nMokady, R., Hertz, A., Bermano, A.H. (2021).\nClipCap: CLIP Prefix for Image Captioning.\narXiv preprint arXiv:2111.09734 , ,\nOrdonez, V., Kulkarni, G., Berg, T. (2011).\nIm2Text: Describing Images Using 1 Million\nCaptioned Photographs. Advances in Neural\nInformation Processing Systems.\nPan, Y., Yao, T., Li, Y., Mei, T. (2020). X-Linear\nAttention Networks for Image Captioning.\nProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recogni-\ntion.\nPapineni, K., Roukos, S., Ward, T., Zhu, W.-J.\n(2002). BLEU: a method for automatic eval-\nuation of machine translation. Proceedings\nof the Annual Meeting of the Association for\nComputational Linguistics.\nRadford, A., Kim, J.W., Hallacy, C., Ramesh,\nA., Goh, G., Agarwal, S., . . . Sutskever, I.\n(2021). Learning Transferable Visual Models\nFrom Natural Language Supervision. Pro-\nceedings of the International Conference on\nMachine Learning.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei,\nD., Sutskever, I. (2019). Language Models\nare Unsupervised Multitask Learners.\nRajbhandari, S., Rasley, J., Ruwase, O., He,\nY. (2020). ZeRO: Memory optimizations\n18\nToward Training Trillion Parameter Models.\nProceedings of the International Conference\nfor High Performance Computing, Network-\ning, Storage and Analysis.\nRennie, S.J., Marcheret, E., Mroueh, Y., Ross,\nJ., Goel, V. (2017). Self-Critical Sequence\nTraining for Image Captioning. Proceedings\nof the IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nRidnik, T., Ben-Baruch, E., Noy, A., Zelnik-\nManor, L. (2021). ImageNet-21K Pretrain-\ning for the Masses. Advances in Neural\nInformation Processing Systems.\nSarto, S., Barraco, M., Cornia, M., Baraldi, L.,\nCucchiara, R. (2023). Positive-Augmented\nContrastive Learning for Image and Video\nCaptioning Evaluation. Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nSarto, S., Cornia, M., Baraldi, L., Cucchiara, R.\n(2022). Retrieval-Augmented Transformer\nfor Image Captioning. Proceedings of the\nInternational Conference on Content-Based\nMultimedia Indexing.\nSchuhmann, C., Beaumont, R., Vencu, R., Gor-\ndon, C., Wightman, R., Cherti, M., . . .\nJitsev, J. (2022). LAION-5B: An open\nlarge-scale dataset for training next genera-\ntion image-text models. Advances in Neural\nInformation Processing Systems.\nSchuhmann, C., Vencu, R., Beaumont, R., Kacz-\nmarczyk, R., Mullis, C., Katta, A., . . .\nKomatsuzaki, A. (2021). LAION-400M:\nOpen Dataset of CLIP-Filtered 400 Mil-\nlion Image-Text Pairs. Advances in Neural\nInformation Processing Systems.\nSennrich, R., Haddow, B., Birch, A. (2016). Neu-\nral Machine Translation of Rare Words with\nSubword Units. Proceedings of the Annual\nMeeting of the Association for Computa-\ntional Linguistics.\nSharma, P., Ding, N., Goodman, S., Soricut, R.\n(2018). Conceptual Captions: A Cleaned,\nHypernymed, Image Alt-text Dataset For\nAutomatic Image Captioning. Proceedings\nof the Annual Meeting of the Association for\nComputational Linguistics.\nShen, S., Li, L.H., Tan, H., Bansal, M., Rohrbach,\nA., Chang, K.-W., . . . Keutzer, K. (2022).\nHow Much Can CLIP Benefit Vision-and-\nLanguage Tasks? Proceedings of the Inter-\nnational Conference on Learning Represen-\ntations.\nSidorov, O., Hu, R., Rohrbach, M., Singh, A.\n(2020). TextCaps: A Dataset for Image\nCaptioning with Reading Comprehension.\nProceedings of the European Conference on\nComputer Vision.\nSrinivasan, K., Raman, K., Chen, J., Bender-\nsky, M., Najork, M. (2021). WIT:\nWikipedia-based Image Text Dataset for\nMultimodal Multilingual Machine Learning.\nACM SIGIR Conference on Research and\nDevelopment in Information Retrieval.\nStefanini, M., Cornia, M., Baraldi, L., Cascianelli,\nS., Fiameni, G., Cucchiara, R. (2022). From\nShow to Tell: A Survey on Deep Learning-\nbased Image Captioning. IEEE Trans-\nactions on Pattern Analysis and Machine\nIntelligence, 45(1), 539–559,\nTan, H., & Bansal, M. (2019). LXMERT:\nLearning Cross-Modality Encoder Represen-\ntations from Transformers. Proceedings of\nthe Conference on Empirical Methods in\nNatural Language Processing.\nThomee, B., Shamma, D.A., Friedland, G.,\nElizalde, B., Ni, K., Poland, D., . . . Li, L.-J.\n(2016). YFCC100M: The new data in mul-\ntimedia research. Communications of the\nACM, 59(2), 64–73,\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit,\nJ., Jones, L., Gomez, A.N., . . . Polosukhin,\nI. (2017). Attention is all you need.\nAdvances in Neural Information Processing\nSystems.\n19\nVedantam, R., Lawrence Zitnick, C., Parikh, D.\n(2015). CIDEr: Consensus-based Image\nDescription Evaluation. Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nWang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov,\nY., Cao, Y. (2022). SimVLM: Simple\nVisual Language Model Pretraining with\nWeak Supervision. Proceedings of the Inter-\nnational Conference on Learning Represen-\ntations.\nXu, H., Yan, M., Li, C., Bi, B., Huang, S.,\nXiao, W., Huang, F. (2021). E2E-VLP:\nEnd-to-End Vision-Language Pre-training\nEnhanced by Visual Learning. Proceedings\nof the Annual Meeting of the Association for\nComputational Linguistics.\nXu, K., Ba, J., Kiros, R., Cho, K., Courville, A.,\nSalakhutdinov, R., . . . Bengio, Y. (2015).\nShow, attend and tell: Neural image cap-\ntion generation with visual attention. Pro-\nceedings of the International Conference on\nMachine Learning.\nYan, M., Xu, H., Li, C., Bi, B., Tian, J., Gui,\nM., Wang, W. (2021). Grid-VLP: Revisit-\ning Grid Features for Vision-Language Pre-\ntraining. arXiv preprint arXiv:2108.09479 ,\n,\nYang, X., Tang, K., Zhang, H., Cai, J. (2019).\nAuto-Encoding Scene Graphs for Image\nCaptioning. Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition.\nYou, Y., Li, J., Reddi, S., Hseu, J., Kumar, S.,\nBhojanapalli, S., . . . Hsieh, C.-J. (2020).\nLarge Batch Optimization for Deep Learn-\ning: Training BERT in 76 minutes. Pro-\nceedings of the International Conference on\nLearning Representations.\nYoung, P., Lai, A., Hodosh, M., Hockenmaier,\nJ. (2014). From image descriptions to\nvisual denotations: New similarity metrics\nfor semantic inference over event descrip-\ntions. Transactions of the Association for\nComputational Linguistics , 2, 67–78,\nYu, J., Wang, Z., Vasudevan, V., Yeung,\nL., Seyedhosseini, M., Wu, Y. (2022).\nCoCa: Contrastive Captioners are Image-\nText Foundation Models. arXiv preprint\narXiv:2205.01917, ,\nZhang, P., Li, X., Hu, X., Yang, J., Zhang, L.,\nWang, L., . . . Gao, J. (2021). VinVL:\nRevisiting visual representations in vision-\nlanguage models. Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nZhang, S., Roller, S., Goyal, N., Artetxe, M.,\nChen, M., Chen, S., . . . others (2022). OPT:\nOpen Pre-trained Transformer Language\nModels. arXiv preprint arXiv:2205.01068 , ,\nZhao, W., Wu, X., Zhang, X. (2020). Mem-\nCap:Memorizing Style Knowledge for Image\nCaptioning. Proceedings of the AAAI Con-\nference on Artificial Intelligence.\nZhou, L., Palangi, H., Zhang, L., Hu, H., Corso,\nJ.J., Gao, J. (2020). Unified Vision-\nLanguage Pre-Training for Image Caption-\ning and VQA. Proceedings of the AAAI\nConference on Artificial Intelligence.\nAppendix A Additional\nQualitative\nResults\nWe report different qualitative results obtained on\nimages from nocaps (Fig. A1), VizWiz (Fig. A2),\nTextCaps (Fig. A3), CC3M and Open Images\n(Fig. A4). We observe how our model can describe\nobjects, people, and scenes with a significantly\nincreased level of detail when compared to the cur-\nrent state of the art and regardless of the dataset.\nAlso, our approach qualitatively appears to be less\nprone to hallucination and can constantly generate\nfluent textual descriptions.\n20\nVinVL:\nA woman in a police uniform\nholding a stuffed animal.\nOurs:\nA woman holding a koala in\nher arms.\nVinVL:\nA birthday cake with flowers on\ntop of a table.\nOurs:\nA cake with a congratulations\nwritten on it with flowers.\nVinVL:\nA large purple flower in a field\nof green plants.\nOurs:\nA close up of an artichoke\nplant with purple flowers.\nVinVL:\nA large bird walking across a\npark near a fence.\nOurs:\nAn ostrich walking next to a\nfence.\nVinVL:\nA silver coin with a picture of a\ntree on it.\nOurs:\nA Canadian silver coin with a\nmaple leaf on it.\nVinVL:\nA man riding a skateboard\ndown a street.\nOurs:\nA man riding a unicycle down\na street.\nVinVL:\nA close up of a brown and black\nanimal in the grass.\nOurs:\nA close up of a hedgehog in the\ngrass.\nVinVL:\nA pile of cars that are parked\non the side of the street.\nOurs:\nA wrecked car on the side of a\ncity street.\nVinVL:\nA white golf ball with a black\nline on it.\nOurs:\nA white golf ball with a Nike\nlogo on it.\nVinVL:\nAn orange snake is sitting on\nthe ground.\nOurs:\nAn orange salamander\ncrawling on the ground.\nVinVL:\nA red digital clock with the\ntime on a car.\nOurs:\nA digital clock showing the\ntime of 8 . 3 3.\nVinVL:\nA couple of large animals in the\nwater.\nOurs:\nTwo beluga whales swimming\nin the blue water.\nVinVL:\nA man and a woman\nskateboarding down a street.\nOurs:\nA man and a woman\nrollerblading down the street.\nVinVL:\nA green drink with a lemon on\na table.\nOurs:\nA glass of lemonade with a\nslice of lime in it.\nVinVL:\nA brown bear walking in the\ngrass near a tree.\nOurs:\nA red panda walking in the\ngrass next to a tree.\nnocaps\nVinVL:\nA couple of girls playing with\ngiant balloons.\nOurs:\nTwo pictures of a girl standing\nnext to a giant Angry Bird.\nVinVL:\nA bunch of fruit growing on a\ntree.\nOurs:\nA bunch of figs hanging from a\ntree.\nVinVL:\nA couple of people standing\non a rock near a river.\nOurs:\nTwo scuba divers standing on\na rock next to a river.\nVinVL:\nA large billboard with a skull\non the side of a building.\nOurs:\nA building with a sign with a\nskull and crossbones on it.\nVinVL:\nA cat laying on top of a large\nrock.\nOurs:\nA snow leopard laying on a\nrock.\nVinVL:\nA couple of people in karate\nuniforms on a gym.\nOurs:\nA couple of people are playing\nfencing on a court.\nVinVL:\nA woman with long hair\nstanding on a beach.\nOurs:\nA woman with dreadlocks on a\nbeach.\nVinVL:\nTwo cats are laying next to\neach other on a blanket.\nOurs:\nTwo Siamese cats laying next\nto each other on a bed.\nVinVL:\nA group of different colored\nbottles on a table.\nOurs:\nBottles of nail polish sitting on\na table.\nVinVL:\nA salad with fruits and\nvegetables on a plate.\nOurs:\nA salad with tomatoes and\nfeta cheese on a white plate.\nVinVL:\nA plate of snails on a white\nplate.\nOurs:\nA white bowl of mussels with\na white sauce.\nVinVL:\nA group of boats in the water.\nOurs:\nTwo people in gondolas in the\nwater.\nVinVL:\nA shirtless man is standing in\nfront of a crowd.\nOurs:\nA sumo wrestler standing on\na stage with a crowd.\nVinVL:\nA cake sitting on a counter\nnext to a cup of coffee.\nOurs:\nA blueberry crumble cake\nwith a cup of coffee.\nVinVL:\nA person holding a can of\nPepsi in their hand.\nOurs:\nA person holding a can of Red\nBull energy drink.\nnocaps\nFig. A1 Sample descriptions generated on nocaps images.\n21\nVinVL:\nA can of coffee sitting on top\nof a table.\nOurs:\nA can of chicken noodle soup\nsitting on a table.\nVinVL:\nA colorful block of gummy\nbears on the floor.\nOurs:\nA picture of a colorful\nChristmas tree on the floor.\nVinVL:\nA bottle of cough syrup sitting\non a counter.\nOurs:\nA bottle of tomato ketchup\nsitting on a desk.\nVinVL:\nA blue sky with a half moon\nand a line of light.\nOurs:\nA computer screen with a sad\nface on it.\nVinVL:\nA close up of a book with a\nbuilding in the background.\nOurs:\nA dollar bill sitting on top of a\ntable.\nVinVL:\nA toy car on the pocket of a\nman's jeans.\nOurs:\nA picture of a blue and yellow\nracing car on a person.\nVinVL:\nA orange sign hanging on the\nwall.\nOurs:\nA sign that says 1 1 4 1 on a\nwall.\nVinVL:\nA person holding a green and\nwhite bottle of sauce.\nOurs:\nA person holding a pill bottle\nin their hand.\nVinVL:\nA book sitting on top of a bed.\nOurs:\nA box of Green Mountain\ncoffee sitting on a bed.\nVinVL:\nA package of toilet paper with\na label on it.\nOurs:\nA package of Cascade\ndetergent sitting on a table.\nvizwiz\nFig. A2 Sample descriptions generated on images from the VizWiz dataset.\nVinVL:\nA picture of a movie poster on\na wall.\nOurs:\nA poster of a group of Lego\nfigures with lightsaber.\nVinVL:\nA red stop sign with graffiti on\nit.\nOurs:\nA red stop sign with the word\nquest written on it.\nVinVL:\nA poster on a wall in a room.\nOurs:\nA no butts on the beach poster\nin a window.\nVinVL:\nA busy city street with a lot of\nstreet signs.\nOurs:\nA stop here on red signal sign\non a city street.\nVinVL:\nA red and white airplane is\nflying in the sky.\nOurs:\nA Turkish airlines plane flying\nin the sky.\nVinVL:\nA group of blue jerseys\nhanging on a shelf.\nOurs:\nA group of Chelsea shirts\nhanging in a locker room.\nVinVL:\nA book about the ghost of a\nman.\nOurs:\nA cover of a Star Wars story\nwith a man holding a sword.\nVinVL:\nA large billboard with a crown\non the side of a building.\nOurs:\nA building with a sign that\nsays Drupalcon Copenhagen.\nVinVL:\nA couple of books sitting next\nto each other.\nOurs:\nA blue book with the title half\nhours at the seaside.\nVinVL:\nA motel sign on the side of a\nbuilding.\nOurs:\nA car wash shop with an\nAmerican flag flying in front.\nVinVL:\nA red and white sign on a\npole.\nOurs:\nA big Al's pizza and subs sign\non a pole.\nVinVL:\nA person holding a bottle of\nwine in their hand.\nOurs:\nA person holding a bottle of\nblack beer.\nVinVL:\nA book with a picture of a\nflower on it.\nOurs:\nA sign that says to be happy\nmake other people happy.\nVinVL:\nA group of pictures and a\nbook on a table.\nOurs:\nA display of pictures of\nwomen of the supreme court.\nVinVL:\nA bottle of wine next to a\nglass.\nOurs:\nA bottle of whisky and a glass\non a counter.\ntextcaps\nFig. A3 Sample descriptions generated on images from the TextCaps dataset.\n22\nVinVL:\nA tall tower sitting next to a\nbody of water.\nOurs:\nA view of the Washington\nmonument at sunset.\nVinVL:\nA group of people are doing a\nyoga class.\nOurs:\nA group of people practicing\nmartial arts in a square.\nVinVL:\nA large spider is sitting on a\npurple flower.\nOurs:\nA close up of a dragonfly on a\npurple flower.\nVinVL:\nA shirtless man in a tie\nstanding in the dark.\nOurs:\nA bodybuilder posing for a\npicture.\nVinVL:\nA blue logo with a globe and a\nball.\nOurs:\nA diagram of a solar eclipse\nwith the earth and moon.\nVinVL:\nA man standing in front of a\nrobot statue.\nOurs:\nA man standing between two\nTransformers.\nVinVL:\nA large airplane is parked at an\nairport.\nOurs:\nA Nasa plane parked in front of\na building.\nVinVL:\nA crowd of people standing in\nfront of a building.\nOurs:\nA crowd of people standing in\nfront of the Taj Mahal.\nVinVL:\nA pink design of a skull and\ncrossbones.\nOurs:\nA pink and purple mandala on\na brown background.\nVinVL:\nA stuffed animal is standing in\nfront of a fireworks display.\nOurs:\nMickey Mouse dressed in a\nred suit holding a firecracker.\nVinVL:\nA group of helicopters flying\nover a building.\nOurs:\nTwo military helicopters flying\nover a building with a minaret.\nVinVL:\nA red poster with a cartoon\nelephant on it.\nOurs:\nThe London dungeon logo on a\nblack wall.\nVinVL:\nA slice of banana cut in half on\ntop of other leaves.\nOurs:\nA drawing of a durian fruit on\na white background.\nVinVL:\nA blue sign with a flower on it.\nOurs:\nThe Huawei logo is displayed\non a blue wall.\nVinVL:\nA couple of men walking down\na hallway with a red arrow.\nOurs:\nA Mitsubishi motor company\nlogo is displayed on a building.\ncc3m\nVinVL:\nA large bridge over the water\nwith a boat.\nOurs:\nA view of the Mackinac bridge\nfrom the water.\nVinVL:\nA woman in a bikini standing on\na tennis court.\nOurs:\nA woman is holding a javelin on\na track.\nVinVL:\nA close up of a bunch of cups\nof coffee.\nOurs:\nA close up of a palette of\ndifferent colors of makeup.\nVinVL:\nA colorful bird perched on a\ntree branch.\nOurs:\nA toucan sitting on a branch\nin a tree.\nVinVL:\nA person holding a lego game\ncontroller in their hand.\nOurs:\nA person holding a Rubik cube\nin their hand.\nVinVL:\nA black sports car parked in a\ngarage.\nOurs:\nA black Nissan GT R parked in\na parking lot.\nVinVL:\nA close up of a green plant in\nthe snow.\nOurs:\nA black sea urchin in the middle\nof the ocean.\nVinVL:\nTwo green drinks sitting on a\ntable with limes.\nOurs:\nThree glasses of mojito with\nlime and a straw .\nVinVL:\nA close up of a bunch of\npurple flowers.\nOurs:\nA group of purple jellyfish\nswimming in the water.\nVinVL:\nA red car is parked on display\nat a show.\nOurs:\nA red Cadillac car on display\nat a show.\nopen images\nFig. A4 Sample descriptions generated on images from the CC3M and Open Images datasets.\n23",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.7941367030143738
    },
    {
      "name": "Computer science",
      "score": 0.7935903072357178
    },
    {
      "name": "Metric (unit)",
      "score": 0.6520302295684814
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5774188041687012
    },
    {
      "name": "Exploit",
      "score": 0.5667764544487
    },
    {
      "name": "Task (project management)",
      "score": 0.5638400316238403
    },
    {
      "name": "Style (visual arts)",
      "score": 0.5169562697410583
    },
    {
      "name": "Natural language processing",
      "score": 0.5006928443908691
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4495099186897278
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4339999854564667
    },
    {
      "name": "Image (mathematics)",
      "score": 0.40722405910491943
    },
    {
      "name": "Machine learning",
      "score": 0.3302288055419922
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I122346577",
      "name": "University of Modena and Reggio Emilia",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I1304085615",
      "name": "Nvidia (United Kingdom)",
      "country": "GB"
    }
  ]
}