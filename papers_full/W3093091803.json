{
    "title": "TurboTransformers: An Efficient GPU Serving System For Transformer Models",
    "url": "https://openalex.org/W3093091803",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2393351683",
            "name": "Fang Jiarui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107028462",
            "name": "Yu Yang",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Zhao, Chengduo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1950278301",
            "name": "Zhou Jie",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2402144811",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2964108773",
        "https://openalex.org/W2804032941",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3101543398",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3014367186",
        "https://openalex.org/W3005664618",
        "https://openalex.org/W2982157693",
        "https://openalex.org/W2798291715",
        "https://openalex.org/W2955715866",
        "https://openalex.org/W2999228288",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2970971581"
    ],
    "abstract": "The transformer is the most critical algorithm innovation of the Nature Language Processing (NLP) field in recent years. Unlike the Recurrent Neural Network (RNN) models, Transformers can process on dimensions of sequence lengths in parallel, therefore leading to better accuracy on long sequences. However, efficient deployments of them for online services in data centers equipped with GPUs are not easy. First, more computation introduced by transformer structures makes it more challenging to meet the latency and throughput constraints of serving. Second, NLP tasks take in sentences of variable length. The variability of input dimensions brings a severe problem to efficient memory management and serving optimization. This paper designed a transformer serving system called TurboTransformers, which consists of a computing runtime and a serving framework to solve the above challenges. Three innovative features make it stand out from other similar works. An efficient parallel algorithm is proposed for GPU-based batch reduction operations, like Softmax and LayerNorm, major hot spots besides BLAS routines. A memory allocation algorithm, which better balances the memory footprint and allocation/free efficiency, is designed for variable-length input situations. A serving framework equipped with a new batch scheduler using dynamic programming achieves the optimal throughput on variable-length requests. The system can achieve the state-of-the-art transformer model serving performance on GPU platforms and can be seamlessly integrated into your PyTorch code with a few lines of code.",
    "full_text": "TurboTransformers: An Efficient GPU Serving System\nFor Transformer Models\nJiarui Fang\nPattern Recognition Center, Wechat AI, Tencent Inc\nBeijing, China\njiaruifang@tencent.com\nYang Yu\nPattern Recognition Center, Wechat AI, Tencent Inc\nBeijing, China\njosephyu@tencent.com\nChengduo Zhao\nPattern Recognition Center, Wechat AI, Tencent Inc\nBeijing, China\nflorianzhao@tencent.com\nJie Zhou\nPattern Recognition Center, Wechat AI, Tencent Inc\nBeijing, China\nwithtomzhou@tencent.com\nAbstract\nThe transformer is the most critical algorithm innovation of\nthe Nature Language Processing (NLP) field in recent years.\nUnlike the Recurrent Neural Network (RNN) models, Trans-\nformers can process on dimensions of sequence lengths in par-\nallel, therefore leading to better accuracy on long sequences.\nHowever, efficient deployments of them for online services\nin data centers equipped with GPUs are not easy. First, more\ncomputation introduced by transformer structures makes it\nmore challenging to meet the latency and throughput con-\nstraints of serving. Second, NLP tasks take in sentences of\nvariable length. The variability of input dimensions brings a\nsevere problem to efficient memory management and serving\noptimization.\nThis paper designed a transformer serving system called\nTurboTransformers, which consists of a computing runtime\nand a serving framework to solve the above challenges. Three\ninnovative features make it stand out from other similar works.\nAn efficient parallel algorithm is proposed for GPU-based\nbatch reduction operations, like Softmax and LayerNorm,\nmajor hot spots besides BLAS routines. A memory alloca-\ntion algorithm, which better balances the memory footprint\nand allocation/free efficiency, is designed for variable-length\ninput situations. A serving framework equipped with a new\nbatch scheduler using dynamic programming achieves the\noptimal throughput on variable-length requests. The system\ncan achieve the state-of-the-art transformer model serving per-\nformance on GPU platforms and can be seamlessly integrated\ninto your PyTorch code with a few lines of code.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8294-6/21/02. . . $15.00\nhttps://doi.org/10.1145/3437801.3441578\nCCS Concepts:â€¢ Computing methodologies Concurrent\ncomputing methodologies; Natural language generation;\nParallel algorithms.\nKeywords: Transformers, Deep Learning Runtime, Serving\nSystem, GPU\n1 Introduction\nThe recent success of Nature Language Processing (NLP)\ntechniques is enabled largely by the transformer-based Deep\nNeural Networks (DNNs), such as Seq2seq [30], BERT [7],\nGPT2 [25], and XLNet [ 31], ALBERT [14]. With the sup-\nport of the attention mechanism, the transformer models can\ncapture long-range dependency in long sequences. In data\ncenters, GPU has proven to be the most effective hardware\nfor deploying deep learning (DL) services. The DL service\naccepts network requests and performs inference computation\nby performing a feed-forward pass on the model. Although\nthere are mature solutions for Convolutional Neural Network\n(CNN) and RNN services, deploying transformer services\nwith low latency and high throughput on GPU still faces two\ncritical challenges.\nDespite their success in model accuracy, transformer mod-\nels are notorious for the massive amount of computation. For\nthe inference of a 40 words sequence, the base BERT model\nrequires 6.9 Gflops. To translate a 20 words sentence from\nChinese to English, a typical Seq2seq model requires over\n20 Gflops. In comparison, for the inference of a 3x224x224\nimage, the ResNet50 [11], GoogleNet [29] and AlexNet [13]\nrequire 3.9, 1.6 and 0.7 Gflops, respectively. Generally, trans-\nformer models lead to more computations than previous DNN\nmodels.\nBesides enhanced computation requirement, transformer\nmodels introduced the problem of variable-length input,\nwhere intermediate tensor dimensions have to change ac-\ncording to the input sequence length during a serving process.\nAlthough also facing at variable-length input, RNN-based\nmodels, like LSTM [ 12] and GRU [ 4], split the variable-\nlength input into multiple fixed-length inputs and execute\nthem sequentially. Unlike models with fixed-length input,\narXiv:2010.05680v4  [cs.DC]  20 Feb 2021\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\ntransformer models cannot benefit from pre-optimization of\nmemory space allocated for intermediate tensors with known\nlengths, resulting in memory optimization challenges. In\naddition, due to variable-length input, the optimization of\nthe serving framework becomes more complicated. Conven-\ntional serving frameworks take advantage of batching tech-\nniques to increase GPU execution efficiency. Extra computa-\ntions brought by zero-padding of short requests in a batch of\nvariable-length requests often conflict with the performance\ngains of batching computing.\nExisting online serving solutions are not able to resolve\nboth the large computation demand and the variable-length\ninput issues, especially the latter one. The deficiencies can be\nsummarized from the three following aspects. First, directly\nemploying a training framework, such as TensorFlow [1] or\nPyTorch [23], on inference tasks is not able to fully utilize the\nhardware resources. Inference differs from training in a way\nthat it does not perform backward propagations which require\nextra memory space for intermediate tensors and eliminate the\nopportunity for operator fusion. Second, currently existing DL\ninference frameworks such as onnxruntime [19], TenorFlow\nXLA [10], TVM [3], tensorRT [22] use techniques designed\nfor fixed-length input workloads and have insufficient capa-\nbility in dealing with variable-length input. Most of these\ncurrent frameworks need a time-consuming preprocessing\nstep to tune the computation pattern of the model operators\naccording to their pre-determined input dimension. In or-\nder to work with transformer models, they usually convert\nvariable-length requests into fixed-length requests through\nzero paddings, which not only introduces additional computa-\ntional overhead but also limits the maximum length allowed\nto the pre-determined value. Although onnxruntime [19] re-\ncently provides some patches to support computation for\nvariable-length input, the runtimeâ€™s GPU memory manage-\nment is not efficient. After it serves a long request or a large\nbatch of requests, a huge amount of memory allocated for\nintermediate tensors will not be released, introducing waste\nin terms of the memory footprint. Third, none of the ex-\nisting solutions have investigated serving optimization for\nvariable-length input workloads. The request batching, which\nis the technique most helpful for performance, adopted in\nmodern serving systems, such as TF-serving [9], Clipper [5],\nNexus [28], are suitable for only fixed-length input.\nTo solve the challenges of deploying an efficient trans-\nformer service, we proposed a serving system called Turbo-\nTransformers. The system consists of a light-weight computa-\ntion runtime and a serving framework. The runtime adopts a\nvariable-length-input-friendly design to avoid time-consuming\npreprocessing specified with dimensions of the intermediate\ntensor. After loading a pre-trained model, the runtime rewrites\nthe computation graph by fusing non-GEMM kernels, and\nprovides efficient CUDA implementations for them. Before\nlaunching an inference, it conducts light-weight memory us-\nage optimizations according to the input sequence length. The\nruntime can achieve state-of-the-art speed and a smaller mem-\nory footprint compared with existing DL runtimes. Moreover,\nit is easy to use, It can bring end-to-end speedup by adding a\nfew lines of Python code. Building upon the runtime, the serv-\ning framework improves throughput of the service through a\nvariable-length-aware batching technique.\nThe innovative contributions of the paper are listed as fol-\nlows.\nâ€¢We proposed a new parallel algorithm for Batch Re-\nduction kernels like Softmax and LayerNorm, which\npushes the efficiency boundary of these kernels on\nGPU.\nâ€¢We proposed a sequence-length-aware memory allo-\ncation algorithm. Unlike other popular allocators for\nDNN runtimes, our allocator can utilize the computation-\ngraph of the DNN model to derive efficient memory\nreusing strategies for variable dimension intermediate\ntensors.\nâ€¢We proposed a sequence-length-aware batch scheduler.\nIt utilizes a dynamic programming algorithm to derive\na batching scheme to achieve the optimal throughput.\n2 Backgrounds\n2.1 Transformer Models\nSelf-attention is the key idea behind the transformer model.\nIt has the ability to attend to different positions of the input\nsequence to compute a representation of that sequence. A\ntransformer model handles variable-length input using stacks\nof self-attention layers instead of RNNs or CNNs. An encoder-\ndecoder model architecture is illustrated in Figure 1.\nMulti-head attention consists of four parts, a set of linear\nlayers that are split into heads, a scaled dot-product attention,\na concat, and a final linear layer. Each multi-head attention\nblock gets three tensors as inputs; Q (query), K (key), V\n(value). These are put through the linear layers and split up\ninto multiple heads. The scaled dot-product attention com-\nputes the dot products of the query with all keys, and applies\na Softmax function to obtain the weights on the values. The\nattention output for each head is then concatenated and put\nthrough a final linear layer. In addition to multi-head atten-\ntion, each of the layers in our encoder and decoder contains a\nfully connected feed-forward network to improve the capacity\nof the model. It consists of two linear transformations with\nactivations in between.\nBy introducing parallelism on the sequence length, dimen-\nsions of Q, K, and V tensors change unpredictably during\nserving. Take a real-time translation system as an example.\nAfter a short greeting phrase including a few words as input,\na long paragraph of hundreds of words maybe its next input.\nThe variable-length input feature brings difficulties to batch-\ning. When the input contains a batch of request, in order to\nbe processed together, the short sequence must be filled with\nTurboTransformers PPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nzeros according to the longest sequence before being sent to\nthe transformer models.\nFigure 1. The transformer model [30] - an encoder-decoder\nmodel architecture and its key components.\nFigure 1 shows a transformer architecture with both en-\ncoder and decoder parts. Note that decoder parts are not neces-\nsary in transformer-based model, for example the widely-used\nBERT model only contain the encoder parts.\n2.2 Serving Systems\nLeveraging the power of transformer-based models to NLP\nonline applications requires joint efforts in both training and\nserving stages. Training is the process of building a model\nfrom offline data, which requires iteratively forward and back-\nward passes. Serving, consisting of repeated inferences, is the\nprocess of using the model to extract useful features from user\nonline input through a forward pass. Although inference does\nnot involve complex backward computation and iterative pro-\ncessing, its performance requirements are more demanding.\nThe serving process must run in real-time with low latency\nand sometimes should have the capacity of handling orders\nof magnitude more throughput than the training process. A\nserving system can be divided into two components, a DL\ninference runtime, and a serving framework.\nTraining frameworks have been widely used as inference\nruntimes. For instance, TF-serving [9] is wrapped with Tensor-\nFlow. Considering the poor performance of applying training\nframework in inference, some works have been dedicated to\ninference-specific runtimes, such as onnxruntime [19], Tenor-\nFlow XLA [10], TVM [3], and TensorRT [22]. Due to time-\nconsuming preprocessing specific to the dimension of inputs,\nmost of these runtimes can not be applied in variable-length\ninput tasks. Among them, only the onnxruntime is able to\nbe used in the variable-length input tasks, with dynamic axis\nsupports after version 1.3. Faster Transformers [20] is a trans-\nformer boost software developed by NVIDIA. However, it\nis not a complete runtime because it has no memory man-\nager and has to be used as an operator for TensorFlow. The\ncomparison between this work and them are listed in Table 1.\nTable 1. Comparison of our runtime and existing GPU DL\nInference runtimes.\nRelated Works Speed Preprocess Variable-Len Usage\nTensorflow-XLA [10] Medium Yes No Easy\nPyTorch [23] Medium No Yes Easy\nTensorRT [22] Fastest Yes No Hard\nFaster Transformers [20] Fast Yes No Hard\nONNX-runtime [19] Fast Yes Yes Medium\nTurboTransformers Fastest No Yes Easy\nThe serving framework wraps the runtime into a service\nexposing gRPC/HTTP as endpoints. The advanced function-\nalities of the serving framework include batching, caching,\nmodel version management, and model ensembles. Batch-\ning boosts throughput substantially by combining multiple\ninference requests into a batch to increase GPU usability,\nwhich is the main focus of this paper. TF-serving enforces\na static batch size by concat multiple requests together and\nhas to pad zeros if requests are not enough. Clipper [5] pro-\nposed an adaptive batching scheme to dynamically find and\nadapt the maximum batch size. Nexus [28] further designed\na batch scheduler to serve multiple different models on the\nsame GPU. Ebird [6] is a prototype system designed an elas-\ntic batch scheduler based on an inference engine supporting\nmultiple batches of the same model running concurrently. All\nof the above works are targeted at fixed-length input, which\ndoes not consider performance harm brought by zero-padding\nof short requests in a batch of variable-length requests. To\navoid the zero-padding problem in RNN, BatchMaker [ 8]\nbreaks the computation graph into a graph of cellulars and\ndynamically decides the set of cellulars should be batched\ntogether. It takes advantage of the weight sharing between\nmultiple forward passes of RNN, which is not applicable in\ntransformer models.\n3 Design Overview\nAs shown in the Figure 2, there are two ingredients of Tur-\nboTransformers, an inference runtime and a serving frame-\nwork. The system accepts network requests and responds the\nresults processed by the Transformer models to end users.\nSection 4 will elaborate on the details of the runtime 1, and\nSection 5 will focus on the serving framework.\n4 Inference Runtime\nInference runtime focuses on increasing computation effi-\nciency and optimizing memory allocation.\n1The code of runtime is publicly available at https://github.com/Tencent/\nTurboTransformers.\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\nInference runtimeDatabase\nMessage Queue\nBatch SchedulerreqreqreqvvvvvBatch req\nreq reqreq\nServer\nclients\nrespresp Response Cache\nServing Framework\nFigure 2. The serving system architecture adopted by Turbo-\nTransformers.\n4.1 Computational Optimizations\n4.1.1 Kernel Fusion. Without backward propagations, there\nis a lot of room left for inference customized optimizations.\nWhen using training frameworks, like TensorFlow or PyTorch\nto infer a transformer model on the GPU, a significant amount\nof time is spent on some non-computational intensive kernels.\nTake PyTorch as an example. For the case where batch size\nand sequence length are both relatively large, PyTorch BERT\nbrings low efficiency due to the inefficient implementations of\nnon-GEMM kernels. For a BERT inference on a Tesla V100\nGPU using input with batch size as 20 and sequence length\nas 128, only 61.8% of the time is spent on GEMM kernels,\nand 38.2% is spent on non-GEMM intensive cores, such as\nLayerNorm, Softmax, Add Bias, Transpose, etc. For the case\nwhere batch size and sequence length are relatively small,\nPyTorch leads to poor efficiency due to the launch overhead\nof the CUDA kernels. For a BERT model inference on a Tesla\nV100 GPU with batch size as 1 and sequence length as 40,\nGPU is completely idle 80.64% of the time.\ngemm0\ntransposebias0gemm1bias1gemm2bias2\nbatched stride gemm3softmaxbatched stride gemm4\ngemm6add biasgemm5\nadd biasgemm7add bias\ntransposetranspose\nfromtensor fused gemm0123 addbiastranspose0addbiastranspose1addbiastranspose2batched stride gemm3softmaxbatched stride gemm4\ngemm6add bias + Layer Normgemm5\nadd bias + activationgemm7add bias + Layer Norm\nLayer Norm\nactivation\nLayer Norm(a) Unfused Transformer Encoder Layer(b) Fused Transformer Encoder Layer\nFigure 3. Kernel fusion of a transformer encoder. The part\nin darker color is a multi-head attention. The part in lighter\ncolor is a feed forward network.\nKernel fusion is able to increase computation efficiency by\nreducing the number of memory accesses, increasing cache lo-\ncality, and reducing kernel launch overhead. Similar to many\npopular frameworks, such as TensorFlow, and Theano [ 2],\nour runtime represents the DNN forward propagation by con-\nstructing a computation graph, in which nodes are operators\nand edges are tensors. As is shown in Figure 3, the com-\nputation graph of a transformer can be reorganized into a\nmore compact graph by fusing all the kernels between two\nGEMM kernels into a single one. The fused non-GEMM ker-\nnels are non-standard DNN operators, so it is impossible to\ntake advantage of existing DNN operator libraries, such as\ncuDNN [21]. For example, there is no such API to combine\nmatrix addition and transpose operation in a single CUDA\nkernel.\nTurboTransformers implements all the non-GEMM ker-\nnels using CUDA, which can be categorized into two types.\nOne type of kernel, such as fused activation functions and\nfused transpose operations, are composed of element-wise\noperations. There is no dependency between the processing of\ntwo different tensor elements, so we can process them in em-\nbarrassingly parallel. The second type of kernels, including\nSoftmax and fused LayerNorm, are composed of reduction\noperations, which is notorious for being hard to parallelized.\nThe latter is the focus of our performance improvement.\n4.1.2 GPU-based Batch-Reduction. Both of Softmax and\nLayerNorm based kernels can be viewed as Batch Reduction\noperations. On the lower dimension of a 2D tensor, Softmax\ncalculates summation and maximum and LayerNorm calcu-\nlates the mean and variance. In other words, they both need\nto reduce a batch of 1D arrays in parallel. Table 2 shows the\nproportion of time of the two operators in the attention layer.\nIn the table, the execution time of Softmax and LayerNorm is\nmeasured using PyTorch. Attention time is measured using\nour runtime after replaced Softmax and LayerNorm with Py-\nTorchâ€™s implementations, respectively. We can observe that\nthey are two big hotspots if not carefully optimized.\nTable 2. Proportion of batch reduction operations in attention\nlayer before and after optimizing.\n(batch size, seq len) (1, 10) (1, 100) (1, 500) (20, 10) (20, 100) (20, 500)\nSoftmax/ before 26.23% 24.73% 34.41% 3.04% 29.4% 90.68%\nAttention after 3.44% 3.18% 11.56% 2.46% 5.50% 15.46%\nLayerNorm/ before 29.20% 21.72% 18.96% 10.61% 52.59% 83.38%\nAttention after 4.96% 4.40% 4.08% 5.14% 6.44% 4.24%\nRealizing the inefficiency of PyTorch operators, some ef-\nfort is spent on optimize GPU batch reduction. According\nto the programming model of CUDA, efficient reduction al-\ngorithms need to fully utilize the power of three hardware\nlevels. First, by splitting on the batch dimension, streaming-\nprocessors (SMs) level parallelism is exploited by the pro-\ncess of the workload on different SMs in parallel. Second,\nwarp level parallelism is exploited by taking advantage of the\nTurboTransformers PPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nwarp-level inter-thread communication mechanism provided\nby CUDA beyond the 9.0 version. Third, instruction-level\nparallelism is exploited by overlapping memory access and\ncomputation instructions. A classical implementation adopted\nin Faster Transformers [20] is shown in the top part of the\nFigure 4. It is derived from work [16], which proposed a best\npractice for 1-D array reduction operations on the GPU. Re-\nduction workloads of ğ‘›rows (inside the dotted line on the top\nof the figure) are assigned to a thread block, which is sched-\nuled to be executed on an SM. The thread block sequentially\nperforms ğ‘›times independent 1-D array reduction, which is\nfinished with two-pass. In the first pass, each warp of the SM\nconducts a warpReduce operation on 32 aligned elements,\nand then store reduction results of them inside shared memory.\nIn the second pass, a warp load at most 32 partial results to\nregister and conduct another warpReduce to obtain the final\nresults.\nIn this paper, we push the efficiency boundary of classical\nbatch reduction algorithms on GPU. Note that some space\nis still left for improvement of warp level and instruction\nparallelism in the above algorithm. First, due to the accesses\nof shared memory, synchronizations of warps inside an SM\nintroduce huge overheads. Second, if the input array is not\n32-aligned, warp divergence resulting from boundary process-\ning also introduces extra overhead. Third, warpReduce leads\nto poor efficiency of instruction issuing. As pointed out by\nreference [16], there is a dependency between shuffle and\nadd instructions. In the upper right corner of the Figure 4,\nthe target register R3 in an SHFL DOWN instruction is re-\nquired immediately as a source register in FADD instruction.\nThe FADD instruction can only be issued until the SHFL is\ncompletely finished, whose latency is more than 1 cycle.\nThe above three shortcomings can be overcome by lever-\naging parallelism between multiple 1-D reduction operations.\nAs shown in the bottom part of Figure 4, a new subroutine\nwarpAllReduceSum_XElem (X = 2 in our figure) is intro-\nduced, which combine ğ‘‹ warp as a batch and do their reduc-\ntion together. First, only one synchronization is required for\nğ‘‹ elements reduction in blockReduceSum_XElem, therefore,\nreduces (ğ‘‹ âˆ’1)/ğ‘‹ synchronization cost. Second, ğ‘‹ inde-\npendent boundary processing can be merged into a single\none, therefore reduce warp divergence. Third, the instruction\nissuing is more efficient because we eliminate instructions\ndependency. As shown in the figure, the target register of\nSHFL DOWN is required two cycles later by FADD as a\nsource register. Another SHFL DOWN with no dependency\non the previous one can be issued immediately.\nEspecially for the LayerNorm kernel, TurboTransformers\nfurther derives a mathematical optimization trick to improve\nefficiency. LayerNorm requires the variances of elements of\n1-D arrays. There are two equivalent formulas of variance,\nas shown in the Equation 1. The first one used in [ 20] re-\nquires two separate reductions for ğ‘¥ and ğ‘¥ âˆ’ğ¸(ğ‘¥), during\nwhich one synchronization between. We use the second one\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u0005\u0004\u0002\u0001\u0004)\u0005\u001d\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\n\u0002\u0001\u0015\t\u0002\u0001\u0004)\u0005\u0004\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u000e\f\r\r\u0001\u0015\t\u0002\u0001\u0015\t\u0002\u0001\u0015\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u000b\u0002\u0001\u0004)\u0005\u001d\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\n\u0002\u0001\u0015\t\u0002\u0001\u0004)\u000b\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u000e\f\r\r\u0001\u0015\t\u0002\u0001\u0015\t\u0002\u0001\u0015\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\b\u0002\u0001\u0004)\u0005\u001d\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\n\u0002\u0001\u0015\t\u0002\u0001\u0004)\b\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u000e\f\r\r\u0001\u0015\t\u0002\u0001\u0015\t\u0002\u0001\u0015\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u0006\u0002\u0001\u0004)\u0005\u001d\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\n\u0002\u0001\u0015\t\u0002\u0001\u0004)\u0006\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u000e\f\r\r\u0001\u0015\t\u0002\u0001\u0015\t\u0002\u0001\u0015\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u0005\u0002\u0001\u0004)\u0005\u001d\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\n\u0002\u0001\u0015\t\u0002\u0001\u0004)\u0005\u0002\u0001\u0004)\u0005\u001d\n\u0016&$\u001c\u0019 \u0001\u0014$\"\u001b\u001c%%\"$\u0001\u001f\u001c(\u001c\u001f\n\u0018\u0019$#\u0001\u001f\u001c(\u001c\u001f \u0010!%&$'\u001b&\u001e\"!\u0001\u001f\u001c(\u001c\u001f\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u0005\u0004\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u000b\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\b\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u0006\u0002\u0001\u0004)\u0005\u001d\n\u000e\f\r\r\u0001\u0015\u0007\u0002\u0001\u0015\u0007\u0002\u0001\u0015\b\n\u0016\u000f\u000e\u0011\u0003\u0001\r\u0013\u0018\u0012\u0001\u0014\u0017\u0002\u0001\u0015\b\u0002\u0001\u0015\u0007\u0002\u0001\u0004)\u0005\u0002\u0001\u0004)\u0005\u001d\n\u000f\n\u001a\u0019%\u001c\u001f\u001e!\u001c\n\"'$%\n\f\u000f\u0011\u0015\u0014\u0001\n\u0014\n\u000f\u0013\u0010\u0012\n\u0003\u0010\t\n\u0001\u0013\u000f\f\u0011\u0011\n\u0014\u0001\u0010\u000b\u0001\u0016\u0007\u0012\u0011\u0005\n\t\u0015\b\n\u0003\u0010\t\n\u0001\u0013\u000f\f\u0011\u0011\n\u0014\u0001\u0010\u000b\u0001\u0016\u0007\u0012\u0011\u0005\n\t\u0015\b\n\u0006\u0002\u0004\r\n\u000e\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\u000b \u0004\u0003 \u0005\u0004\u000b\n\u0017\u0006\u0013\u0012\u0004\n\t\u0016\b\n\u0007\u0007\u0003 \u0002\u0001\u0003\u0004\u000b\u000b\n\b\t\n\u0001 \b\t\n\u0002 \b\t\n\u0003 \b\t\n\u0004 \b\t\n\u0003\u0006\b\t\n\u0003\u0007\b\t\n\u0004\u0001\b\t\n\u0004\u0002\u000b\n\u0014\u0018\u0010\b\u0005\u0015\u000b\u0013\n\u0006\t\u0014\n\u0017\u0006\u0013\u0012\u0004\n\t\u0016\b\n\b\t\n\u0017\u0006\u0013\u0012\u0004\n\t\u0016\b\n\u0017\u0006\u0013\u0012\u0004\n\t\u0016\b\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\u000b\u0010\u0015\f\u000f\n\u0014\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\u000b \u0004\u0003 \u0005\u0004\u000b \u0007\u0007\u0003 \u0002\u0001\u0003\u0004\u000b\u000b\n\u0007\u000e\u0011\b\r\u0004\n\t\u0016\b\n\u0014\u0018\u0010\b\u0005\u0015\u000b\u0013\n\u0006\t\u0014\n\u0003\u000e\u000f\u0002\u0017\f\u0014\n\u0001\u0011\u0012\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\f \u0004\u0003 \u0005\u0004\f\n\u0019\b\u0015\u0014\u0006\f\u000b\u0018\n\f\u0007\u0004\u0005\u0010\f\u0011\n\u0007\u0007\u0003 \u0002\u0001\u0003\u0004\f\f\n\u0007\n\u000b\b\u0001\u0007\n\u000b\b\u0002\u0007\n\u000b\b\u0003\u0007\n\u000b\b\u0004 \u0007\n\u000b\b\u0003\u0005\u0007\n\u000b\b\u0003\u0006\u0007\n\u000b\b\u0004\u0001\u0007\n\u000b\b\u0004\u0002\f\n\u0016\u001a\u0012\n\u0007\u0017\r\u0015\f\b\u000b\u0016\n\u0019\b\u0015\u0014\u0006\f\u000b\u0018\n\f\u0007\u0004\u0005\u0010\f\u0011\n\u0007\n\u000b\b\n\u0019\b\u0015\u0014\u0006\f\u000b\u0018\n\f\u0007\u0004\u0005\u0010\f\u0011\u0019\b\u0015\u0014\u0006\f\u000b\u0018\n\f\u0007\u0004\u0005\u0010\f\u0011\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\f\u0012\u0003\u0004\u0017\u000e\u0011\f\u0016\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\f \u0004\u0003 \u0005\u0004\f \u0007\u0007\u0003 \u0002\u0001\u0003\u0004\f\f\n\t\u0010\u0013\n\u000f\u0006\f\u000b\u0018\n\f\u0007\u0004\u0005\u0010\f\u0011\u0016\u001a\u0012\n\u0007\u0017\r\u0015\f\b\u000b\u0016\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\f \u0004\u0003 \u0005\u0004\f \u0007\u0007\u0003 \u0002\u0001\u0003\u0004\f\f\n\u0007\n\u000b\t\u0001\u0007\n\u000b\t\u0002\u0007\n\u000b\t\u0003\u0007\n\u000b\t\u0004 \b\n\u000b\t\u0003\u0006\b\n\u000b\t\u0003\u0007\b\n\u000b\t\u0004\u0001\b\n\u000b\t\u0004\u0002\f\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\f\n\u0007\n\u000b\t\n\u0001 \u0002 \u0003 \u0004 \u0003\u0006\u0003\u0007\u0004\u0001\u0004\u0002\f \u0004\u0003 \u0005\u0004\f \u0007\u0007\u0003 \u0002\u0001\u0003\u0004\f\f\n\u0005\u0010\u0011\u0002\u0019\u000e\u0016\f\u0001\u0013\u0014\nFigure 4. Parallel batch reduction optimizations on three\nhardware levels.\n1.2 1.2 1.2 1.2 1.2 1.2 1.3 1.7 1.4 1.1\n2.6\n3.5\n4.2 4.3 4\n3.4\n2.1 1.8 1.5 1.2\n0.74 0.76 1.1 1.4 1.5 1.7 1.6\n2.1 1.7 1.5\n2.9\n3.9\n4.6\n6\n4.5\n3.7\n2.2 2.1 1.8 1.5\nBaseline\ncuDNN\nSpeedup\n0\n2\n4\n6\n(batch size, seq length)\n1,10 1,20 1,40 1,60 1,80 1,1001,2001,3001,4001,50020,1020,2020,4020,6020,8020,10020,20020,30020,40020,500\nSoftmax Speedup on Tesla V100\n0.984 0.971 0.981 0.981 0.972 0.981\n1.07\n1.14 1.18 1.2\n1.1\n1.18 1.18 1.14 1.12 1.12 1.12\n1.19\n1.13\n1.21\nSpeedup\n0.8\n1.0\n1.2\n(batch size, seq length)\n1,10 1,20 1,40 1,60 1,80 1,1001,2001,3001,4001,50020,1020,2020,4020,6020,8020,10020,20020,30020,40020,500\nLayerNorm Speedup on Tesla V100\nFigure 5. Speedup of batch reduction kernels on Tesla V100.\nto compute variances. The warpAllReduceSum_2Elem can\nsimultaneously reduce ğ‘¥ and ğ‘¥2, which not only increases the\nefficiency of instruction execution but also reduces half of the\nnumber of synchronizations.\nVar(ğ‘¥)= ğ¸(ğ‘¥âˆ’ğ¸(ğ‘¥))2 = ğ¸(ğ‘¥2)âˆ’ğ¸2 (ğ‘¥) (1)\nFigure 5 shows the speedups of Softmax and LayerNorm\nkernels in TurboTransformers compared with the other imple-\nmentations. The baseline of both kernels is used implemen-\ntations from [20]. For the Softmax kernel, we also compare\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\nwith the Softmax routine cuDNNv7.5. In most cases, our op-\ntimization strategy has achieved obvious acceleration. Due\nto the batch dimension is larger for Softmax, its performance\nboost is more significant.\n4.2 Memory Manager\nBesides computing optimizations, memory management is\nalso vital to a DNN runtime. It is evaluated by two metrics.\nThe allocation efficiency, which determined by the number\nof times and the amount of the memory is allocated and re-\nleased, affects the overall execution speed of the runtime.\nThe memory footprint affects the possible size of the model\nas well as the maximum batch size of requests. Three types\nof memory are managed by the runtime, i.e., input tensors,\nintermediate tensors, layer parameters. Dimensions of inter-\nmediate tensors change frequently during inferences in case\nof variable-length input. Adversely, for fixed-length input,\nthe dimensions of intermediate tensors are determined in ad-\nvance and allocation scheme never changes during inference\nprocesses. Therefore, the memory usage and positions of ten-\nsors can be pre-optimized and fixed during serving. However,\nThe optimal memory allocation strategy is different in case\nof the different input lengths. Allocator of variable-length\ninput must efficiently deal with unpredictable memory usage\nrequirements.\nIt is complicated to achieve both a high allocation efficiency\nand a small memory footprint for variable-length input. To\nachieve the best memory footprint, the memory of interme-\ndiate tensors should be allocated when needed, and released\nimmediately when not required. Frequent allocation and re-\nlease of small memory space on GPU will lead to a worse\nruntime efficiency. For example, in this case, 50% of the\ncomputing resources idle wait for memory allocation ready\non Tesla M40 (batch size = 20, sequence length = 128). To\nachieve the best allocation efficiency, the memory should be\nallocated in advance and cached for repeated use in the future\nthe inferences without any extra allocation. However, it is\nchallenging to predict maximum memory usage to meet the\nrequirement of a long-term serving process. Even if we know\nit, occupying a vast amount of memory for a long time will\nlead to extremely poor memory footprints.\nAlthough the allocator for fixed-length input has been well\nstudied, the one for variable-length input is still not perfect.\nFor fixed-length input, by taking advantage of the topology\nof the computation graph, works [24] [15] figure out the op-\ntimal memory usage by reusing the same memory space for\nintermediate tensors that do not coexist. For variable-length\ninput, The existing solutions have to tradeoff allocation effi-\nciency and footprint. PyTorch [23] designed a custom caching\ntensor allocator which incrementally builds up a cache of\nCUDA memory and reassigns it to later allocations. Pad-\ndlePaddle [17] used a similar method, which is all inspired\nby the caching device allocator implemented in the NVlabâ€™s\ncub library [26]. Experiments have shown that these methods\ncannot achieve optimal memory usage, because they do not\nconsider the DNNâ€™s computation graph.\nWe adopted an innovative memory optimization method for\nvariable-dimension intermediate tensors, which evoke a light-\nweight variable-length-aware memory manager after knowing\nthe length of each inference. To achieve more efficiency and\nless footprint, the allocator used in TurboTransformers com-\nbines the idea of memory cache and graph-topology-aware\nspace reuse. First, it organizes memory space in units of the\nchunks, which is a small block, for example, 2MB of memory.\nBy reusing already allocated chunks, allocation efficiency can\nremain at a high level during serving. Second, it utilizes the\ncomputation graph to know the life cycle of each intermedi-\nate tensor in advance, and calculate the offset of each tensor\nwithin a specific chunk as soon as it recognizes the sequence\nlength of the new arrival request. In this way, tensors with\nno overlapping life cycle can reuse the same memory space,\ntherefore reduce memory footprint as much as possible.\nOur sequence-length-aware allocation method is shown\nas Algorithm 1. The input tensor usage record is a list of tu-\nples {ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ _ğ‘œğ‘, ğ‘™ğ‘ğ‘ ğ‘¡_ğ‘œğ‘, ğ‘ ğ‘–ğ‘§ğ‘’}, where ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ _ğ‘œğ‘, ğ‘™ğ‘ğ‘ ğ‘¡_ğ‘œğ‘ are\nindices of the first and last operator that use the tensor. The\nindices are from the topological sorting of the DNNâ€™s compu-\ntation graph. It first sorts the usage records in non-increasing\norder based on the size of the tensor. ğ¹ğ‘–ğ‘›ğ‘‘ğºğ‘ğ‘ğ¹ğ‘Ÿğ‘œğ‘šğ‘â„ğ‘¢ğ‘›ğ‘˜ is\nused to determine if there exists a free gap inside a chunk\nto fit for that tensor. If no such gap is found in all existing\nchunks, we append a new chunk to the end of the chunk\nlist. The size of the new chunk is the maximal one of DE-\nFAULT_chunk_SIZE (2MB in our implementation) and the\nsize of tensor times K_SCALE (1.2 in our implementation).\nWhen a chunk is not used in this inference, in our algorithm,\nits memory is released immediately. Alternatively, we can as-\nsign each chunk a maximum inference idle times, and release\nit after it reaches the time limit.\nğ¹ğ‘–ğ‘›ğ‘‘ğºğ‘ğ‘ğ¹ğ‘Ÿğ‘œğ‘šğ‘â„ğ‘¢ğ‘›ğ‘˜ of Algorithm 1 finds the best gap in\na memory chunk. It is equivalent to a special case of the 2D\nstrip packing problem, which is NP-hard. We slightly modify\nthe Greedy by Size for Offset Calculation Algorithm in [24]\nto solve FindGapFromchunk in ğ‘‚(ğ‘›2)time complexity. The\ninputs are a target tensorğ‘¡ and a target chunk ğ‘â„ğ‘¢ğ‘›ğ‘˜. For each\nrecord ğ‘¥ in the chunk, the algorithm first check whetherğ‘¥ and\nthe target tensor ğ‘¡ overlap in usage time (L5-L7), in order to\nfind the smallest gap between them such that current tensor\nfits into that gap (L8 - L11). If we found such a gap before the\nend of the chunk, we assign the tensorğ‘¡ to the gap. Otherwise,\nthe function return invalid. (L15 - L21).\nFigure 6 shows an example of applying our algorithm on a\nBERT inference application. When the input length changes\nfrom 200 to 240, we allocate one more chunk and adjust the\noffsets.\nTurboTransformers PPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nAlgorithm 1: Sequence-length-aware allocator\n1 def FindGapFromchunk(ğ‘¡ : ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ_ğ‘–ğ‘‘, ğ‘â„ğ‘¢ğ‘›ğ‘˜):\n2 get ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘ ğ‘–ğ‘§ğ‘’ from ğ‘â„ğ‘¢ğ‘›ğ‘˜; ğ‘ ğ‘šğ‘ğ‘™ğ‘™ğ‘’ğ‘ ğ‘¡_ğ‘”ğ‘ğ‘ â†âˆ;\n3 ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â†0; ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â†ğ‘ğ¼ğ¿;\n4 foreach record ğ‘¥ âˆˆğ‘â„ğ‘¢ğ‘›ğ‘˜ do\n5 ğ‘šğ‘ğ‘¥_ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ _ğ‘œğ‘ â†max (ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ _ğ‘œğ‘ğ‘¡ , ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ _ğ‘œğ‘ğ‘¥ ) ;\n6 ğ‘šğ‘–ğ‘›_ğ‘™ğ‘ğ‘ ğ‘¡_ğ‘œğ‘ â†min (ğ‘™ğ‘ğ‘ ğ‘¡_ğ‘œğ‘ğ‘¡ , ğ‘™ğ‘ğ‘ ğ‘¡_ğ‘œğ‘ğ‘¥ );\n7 if ğ‘šğ‘ğ‘¥_ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ _ğ‘œğ‘ â‰¤ğ‘šğ‘–ğ‘›_ğ‘™ğ‘ğ‘ ğ‘¡_ğ‘œğ‘ then\n8 ğ‘”ğ‘ğ‘ â†ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ğ‘¥ âˆ’ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ;\n9 if ğ‘”ğ‘ğ‘ â‰¥ğ‘ ğ‘–ğ‘§ğ‘’ğ‘¡ and ğ‘”ğ‘ğ‘ < ğ‘ ğ‘šğ‘ğ‘™ğ‘™ğ‘’ğ‘ ğ‘¡_ğ‘”ğ‘ğ‘ then\n10 ğ‘ ğ‘šğ‘ğ‘™ğ‘™ğ‘’ğ‘ ğ‘¡_ğ‘”ğ‘ğ‘ â†ğ‘”ğ‘ğ‘;\nğ‘ğ‘’ğ‘ ğ‘¡_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â†ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ;\n11 end\n12 ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â†\nmax(ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡,ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ğ‘¥ +ğ‘ ğ‘–ğ‘§ğ‘’ğ‘¥ );\n13 end\n14 end\n15 if ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ is ğ‘ğ¼ğ¿ and\nğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘ ğ‘–ğ‘§ğ‘’ âˆ’ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â‰¥ğ‘ ğ‘–ğ‘§ğ‘’ğ‘¡ then\n16 ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â†ğ‘ğ‘Ÿğ‘’ğ‘£_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ;\n17 end\n18 if ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ is ğ‘ğ¼ğ¿ then\n19 return ğ¼ğ‘ğ‘‰ğ´ğ¿ğ¼ğ·\n20 end\n21 return ğ‘ğ‘’ğ‘ ğ‘¡_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡\n22 end\n23 def MemAllocate(ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ_ğ‘¢ğ‘ ğ‘ğ‘”ğ‘’_ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘ğ‘  : a list of tuples\n(first_op, last_op, size), ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ : a chunk has size, mem\naddr, list of <tensor_id, offset>):\n24 sort ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ_ğ‘¢ğ‘ ğ‘ğ‘”ğ‘’_ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘ğ‘  in decreasing order of ğ‘ ğ‘–ğ‘§ğ‘’;\n25 foreach record ğ‘¡ âˆˆğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ_ğ‘¢ğ‘ ğ‘ğ‘”ğ‘’_ğ‘Ÿğ‘’ğ‘ğ‘œğ‘Ÿğ‘‘ğ‘  do\n26 ğ‘–ğ‘ _ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘ â†ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ ;\n27 foreach ğ‘â„ğ‘¢ğ‘›ğ‘˜ âˆˆğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡ do\n28 ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ â†FindGapFromchunk(ğ‘¡, ğ‘â„ğ‘¢ğ‘›ğ‘˜);\n29 if ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ is valid then\n30 ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘¡ â†ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘–ğ‘‘;\nğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ğ‘¡ â†ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ;\n31 ğ‘–ğ‘ _ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘ â†ğ‘‡ğ‘Ÿğ‘¢ğ‘’;\n32 break;\n33 end\n34 end\n35 if ğ‘–ğ‘ _ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘ is false then\n36 ğ‘›ğ‘’ğ‘¤_ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘ ğ‘–ğ‘§ğ‘’ â†\nmax(DEFAULT_CHUNK_SIZE,ğ‘¡_ğ‘ ğ‘–ğ‘§ğ‘’Ã—\nK_SCALE);\n37 ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘â„ğ‘¢ğ‘›ğ‘˜ğ‘¡ â†len(ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡);\nğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ ğ‘¡ â†0;\n38 append a new chunk of size ğ‘›ğ‘’ğ‘¤_ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘ ğ‘–ğ‘§ğ‘’ to\nğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡;\n39 end\n40 end\n41 release unused chunk in ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡;\n42 return ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘â„ğ‘¢ğ‘›ğ‘˜, ğ‘ğ‘ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘’ğ‘‘_ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ , ğ‘â„ğ‘¢ğ‘›ğ‘˜_ğ‘™ğ‘–ğ‘ ğ‘¡;\nMemory Allocation of seq len= 200\nMemory Allocation of seq len= 240\nqkv_out1843200Q614400\ntransout160000\nctxlayer614400\nattnscore160000\nfuse_gemm0120splitAddBiasTranspose1batch_gemm32Softmax+Batch_gemm43transposeforscore4gemm55addbiaslayernorm6gemm67gemm7+AddBiasLN8 layer_out614400\nintermediate_out2457600\nK614400V614400\natn_out614400\nvb vblayerout614400\nchunk 0 (2949120 B)chunk 1 (2097152 B)\nqkv_out2211840Q737280\ntransout230400\nctxlayer737280\nattnscore230400\nlayer_out737280\nintermediate_out2949120\nK737280V737280\nattn_out737280\nvb vb layerout737280   \nChunk 0 (2949120 B)Chunk 1 (2097152 B)Chunk 2 (2097152 B)\nfuse_gemm012\nsplitAddBiasTranspose\nBatch_gemm3\nSotfmax+ Batch_gemm4\nAddBiasLayerNorm\nTransposeForScore\ngemm6+AddBiasAct\nGemm7+AddBiasLN\nQ K\nV\nQKV_out\nattn_score\nctx_layer\ntrans_out\nattn_out\nattn_out\nIntermediate_outlayer_out\ncomputation graph\nfuse_gemm0120splitAddBiasTranspose1batch_gemm32Softmax+Batch_gemm43transposeforscore4gemm55addbiaslayernorm6gemm67gemm7+AddBiasLN8\nmemory address offset\ngemm5\nFigure 6. A memory allocation example uses our proposed\nvariable-length-aware allocator.\n5 Serving Framework\nBased on the computation runtime, a serving framework is\nrequired to attain enough serving throughput under latency\nconstraints to satisfy a Service Level Objectives (SLOs). The\nserving framework of TurboTransformers is shown in Fig-\nure 2. The userâ€™s requests first arrive at a Message Queue\n(MQ) and then are sent to runtime for inference computa-\ntion after two serving-level optimizations, i.e. Caching and\nBatching. For caching, similar to Clipper [5], by caching the\ninference results in a database, the Resp Cache component in\nthe figure responses the frequent requests without evaluating\nthe model. For batching, the Batch Scheduler component is\nresponsible for packaging requests that come in a period of\ntime into a batch. In most application scenarios, the input of\nthe user is a single inference request with batch size as 1. It\nhas been known that small batch sizes lead to low GPU hard-\nware utilization. Packaging multiple requests into a relatively\nlarger batch and conducting inference on them together can\nimprove hardware utilization. As shown in Figure 7, serving\nrequests in batch brings significant speedup, especially for\nshort sequences. The batch schedulers adopted by conven-\ntional serving systems, like TF-serving [9] and Nexus [28],\nare designed to packages requests with fixed-length into a\nbatch. Currently, serving systems are lack of critical ability to\nhandle variable-length requests.\nHow to batch variable-length requests to achieve the opti-\nmal throughput is tricky. If we package multiple requests of\ndifferent lengths into a batch, then all requests in the batch\nwill be zero-padded with regards to the maximum length of\nthe request in the batch. Zero paddings will introduce a lot\nof extra computations. An efficient batch scheduler has to\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\nseq_len=10\nseq_len=20\nseq_len=30\nseq_len=50\nseq_len=100\nseq_len=200\nNormalized latency0.2\n0.4\n0.6\n0.8\n1.0\nBtach Size\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nFigure 7. Batching brings performance gain for the base\nBERT serving on RTX 2060 GPU. The y-axis illustrates the\nnormalized latency of inferring a request of batch size 1.\ncarefully balance the overheads of padding and the benefits\nof batching. For example, assume that there are five inference\nrequests to be served, with lengths of 17, 18, 52, 63, and 77,\nrespectively. Packing a single batch with 5 instances is less\nefficient than no batching. The batching scheme achieving\nthe optimal throughput is packing three batches. As shown in\nFigure 8, the response throughput (resp/sec) improved 35%\nby the optimal scheduling scheme.\n17\n18\n52\n63\n77\n1718526377\nBatch 0\nBatch 2Requests in MQBatch Schema of Requests\nBatch 1\n15.24ms(65.62 resp/sec)20.62 ms(48.50 resp/sec)\n2.97ms\n2.97ms\n4.54ms\n4.61ms\n5.53ms\n4.35ms\n5.36ms\n5.53ms\nFigure 8. An example of batch scheduler for variable-length\nrequests.\nFor variable-length request serving, we proposed an in-\nnovative sequence-length-aware batch scheduler in Algo-\nrithm 2. The core idea is to use dynamic programming (DP) to\nsolve an optimization problem which maximizes the response\nthroughput as the objective in ğ‘‚(ğ‘›2)time complexity. The\ninputs of the algorithm consist of two data structures. The\nğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡ is the list of input requests with variable length.\nThe ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘_ğ‘ğ‘œğ‘ ğ‘¡ is a dictionary. It uses two keys as indices,\nnamely the sequence length and batch size. Its value is the\ninference cost with the parameter from the corresponding\nkey. The values of ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘_ğ‘ğ‘œğ‘ ğ‘¡ are collected by a warm-up\nphase after the service first starts on specific hardware, which\nutilizes the runtime to run inferences under all possible batch\nsizes and sequence lengths. They are stored on disk or data-\nbase (database in Figure 2) and reloaded to memory when the\nserving module is restarted. First, the ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡ are sorted\nin increasing order according to the sequence length. Then\nline 3 initializes an array called ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  to store intermediate\ninformation. Specifically, ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’[ğ‘–]records the minimum time\noverhead for processing the sublist ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡[0 : ğ‘–]. The\nalgorithm traverses each request in ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡ and uses a\nDP algorithm to update the ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’[ğ‘–]at the corresponding po-\nsition ğ‘–. The Bellman equation of this DP problem is shown\nin Equation 2.\nğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’[ğ‘–]= ğ‘šğ‘–ğ‘›\n0â‰¤ğ‘— â‰¤ğ‘–\n(ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘_ğ‘ğ‘œğ‘ ğ‘¡[ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡[ğ‘–].ğ‘™ğ‘’ğ‘›][ğ‘–âˆ’ğ‘—+1]\nÃ—(ğ‘–âˆ’ğ‘—+1)+ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  [ğ‘—âˆ’1])\n(2)\nAlgorithm 2: Batch Scheduler With DP\nInput: ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘_ğ‘ğ‘œğ‘ ğ‘¡\n1 sort ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡ in increasing order with regards to\nsequence length;\n2 ğ‘ â†Size(ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡);\n3 Create ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘ ,ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ _ğ‘–ğ‘‘ğ‘¥_ğ‘™ğ‘–ğ‘  as lists of size ğ‘ +1;\n4 ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘  [0]â† 0; ğ‘– â†1;\n5 while ğ‘– â‰¤ğ‘ do\n6 ğ‘— â†ğ‘–âˆ’1; ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥ = ğ‘–âˆ’1;\n7 ğ‘ğ‘¢ğ‘Ÿ_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„ = ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡[ğ‘–âˆ’1].ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„;\n8 ğ‘šğ‘–ğ‘›_ğ‘ğ‘œğ‘ ğ‘¡ = ğ‘ğ‘ğ‘â„ğ‘’ğ‘‘_ğ‘ğ‘œğ‘ ğ‘¡[ğ‘ğ‘¢ğ‘Ÿ_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„][1]+ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘ [ğ‘—];\n9 while ğ‘— > 0 do\n10 ğ‘¡ğ‘šğ‘_ğ‘ğ‘œğ‘ ğ‘¡ = ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘ [ğ‘—âˆ’1]+\nğ‘ğ‘ğ‘â„ğ‘’ğ‘‘_ğ‘ğ‘œğ‘ ğ‘¡[ğ‘ğ‘¢ğ‘Ÿ_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„][ğ‘–âˆ’ğ‘—+1]âˆ—( ğ‘–âˆ’ğ‘—+1);\n11 if ğ‘¡ğ‘šğ‘_ğ‘ğ‘œğ‘ ğ‘¡ < ğ‘šğ‘–ğ‘›_ğ‘ğ‘œğ‘ ğ‘¡ then\n12 ğ‘šğ‘–ğ‘›_ğ‘ğ‘œğ‘ ğ‘¡ = ğ‘¡ğ‘šğ‘_ğ‘ğ‘œğ‘ ğ‘¡; ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥ = ğ‘—âˆ’1;\n13 end\n14 ğ‘— â†ğ‘—âˆ’1;\n15 end\n16 ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘ [ğ‘–]= ğ‘šğ‘–ğ‘›_ğ‘ğ‘œğ‘ ğ‘¡; ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥_ğ‘™ğ‘–ğ‘ ğ‘¡[ğ‘–]= ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥;\n17 ğ‘– â†ğ‘–+1;\n18 end\n19 ğ‘– = ğ‘;\n20 while ğ‘– > 0 do\n21 ğ‘’ğ‘›ğ‘‘_ğ‘–ğ‘‘ğ‘¥ â†ğ‘–; ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥ â†ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥_ğ‘™ğ‘–ğ‘ ğ‘¡[ğ‘–];\n22 pack ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡_ğ‘™ğ‘–ğ‘ ğ‘¡[ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥ : ğ‘’ğ‘›ğ‘‘_ğ‘–ğ‘‘ğ‘¥]into a batch;\n23 ğ‘– = ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡_ğ‘–ğ‘‘ğ‘¥ âˆ’1;\n24 end\nNote that the premise of this algorithm work is that there\nexists a scheduling strategy for requests in MQ that can meet\nSLO on this server. In a multi-server environment, an upper-\nlevel load balancer as the one in Nexus [28] can ensure that\nthe requests assigned to each server will not be overloaded.\nThere are two options to decide when to evoke the batch\nscheduler. The first one is a hungry strategy. When the runtime\nis idle, we immediately start the batch scheduler to batch\nrequests in MQ. This strategy is suitable for the situation\nwhere request throughput is high and the GPU have to run\nat full load. The second one is a lazy strategy. Similar to\ndelayed batching of Clipper, we sets a timeout value and a\nmaximum batch size. Once the number of requests in the\nbatch exceeds the maximum batch size, or the timeout is\nreached, we start the batch scheduler. Due to the reordering\nof requests in Algorithm 2 , requests that arrive early may be\nTurboTransformers PPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nserved late. We check the elapse between the current time and\nthe recorded arrival timestamp of request at the front of the\nMQ, and start the batch scheduler immediately if the elapse\nplus the estimated execution latency of current requests in\nbatch exceeds half of the latency constraints.\n6 Experimental Results\nWe evaluated the performance of both the runtime and the\nserving system on a server equipped with an AMD Ryzen 7\n3700X CPU and one RTX 2060 GPU.\n6.1 Usability\nOur runtime provides C++ and Python APIs. For the Python\none, as shown in the following code snippet, adding 3 lines\nof python code (L3, 12, 13) can bring end-to-end speedup.\n1 import torch\n2 import transformers\n3 import turbo_transformers\n4 model_id = \"bertâˆ’baseâˆ’uncased\"\n5 torch_model =\ntransformers.BertModel.from_pretrained(model_id)\n6 torch_model.eval()\n7 input_ids = torch.tensor(\n8 [12166, 10699, 16752, 4454],\n9 dtype=torch.long, device = torch.device('cuda:0'))\n10 torch_model.to(torch.device('cuda:0'))\n11 torch_res = torch_model(input_ids)\n12 turbo_model =\nturbo_transformers.BertModel.from_torch(torch_model)\n13 turbo_res = turbo_model(input_ids)\n6.2 Performance of the Runtime\nThe runtime is evaluated on four transformer DNNs, includ-\ning Bert [7], Albert [14], DistilBert [27] and a Seq2Seq De-\ncoder [30], the parameters of which are shown in Table 3. The\nformer three DNNs consist of Transformer encoder structures,\nand the latter consists of both encoder and decoder structure\nand is applied in a Neural Machine Translation system. The\nBert model adopts a base configuration, while the Albert\nmodel adopts a large configuration.\nTable 3. Evaluated transformer models & parameters\nModel Parameters\nBert num_layer=12, num_head=12, hidden_size=4096, inter_size=3072\nAlbert num_layer=12, num_head=64, hidden_size=4096, inter_size=16384\nDistilBert num_layer=6, num_head=12, hidden_size=4096, inter_size=3072\nSeq2Seq num_layer=6, num_head=16, hidden_size=3072,\nDecoder beam_size=4, max_target_len=500\n6.2.1 End-to-end Speed on Variable-length Input. The\nability to handle variable-length input is evaluated by se-\nquential execution of the runtime using requests of different\nlengths. For Bert and Albert, the input requests are randomly\ngenerated texts whose lengths are uniformly distributed from\nTurbo\nPyTorch\nonnxrt\nTurbo-TC\nLatency (ms)\n0\n10\n20\nsequence length\n0 50 100 150 200 250 300 350 400 450 500\nLatency of Bert on Variable Length Requests (RTX 2060 GPU)\nTurbo\nPyTorch\nTrubo-TC\nLatency (ms)\n0\n200\n400\nsequence length\n0 50 100 150 200 250 300 350 400 450 500\nLatency of Albert on Variable Length Requests (RTX 2060)\nTurbo\nPyTorch\nonnxrt\nTurbo-TC\nLatency (ms)\n0\n5\n10\nsequence length\n0 50 100 150 200 250 300 350 400 450 500\nLatency of Distilled Bert on Variable Length Requests (RTX 2060 GPU)\nTurbo\nPyTorch\nTurbo-TC\nLatency (ms)\n0\n100\n200\n300\nsource sequence length\n30 40 50 60 70 80 90 100 110 120 130 140\nLatency of Decoder on Variable Length Requests (RTX 2060)\nFigure 9. Benchmarking the latency of runtimes using vari-\nable length requests.\n5 to 500. For Decoder, it is a model adopted from a Chinese-\nEnglish translation task, and its inputs are randomly sampled\nChinese texts whose length ranges from 28 to 137. The per-\nformance results are shown in Figure 9. Note that our gen-\nerating and sampling processes are completely random, and\nthe random seed is the same for different tests, although the\nfigure displays in order of input length from small to large\nfor the sake of clearness. We compare our runtime with the\nPyTorch (v1.5.0) and onnxruntime-gpu (v1.3.0). Turbo is the\nperformance of our proposed runtime implemented by FP32\nGEMM algorithms. The turbo-TC allows GEMM operation\nto use Tensor Core [18] if possible. Because the tensor core\noptimization is not allowed in PyTorch and onnxruntime, we\nlist it here as an additional reference. However, it introduces\nminimal and acceptable precision loss to the FP32 version.\nTurboTransformersâ€™ runtime shows significant performance\nadvantages over PyTorch on short requests. In the case of\nBert inference, Turboâ€™s speedups to PyTorch are ranging\nfrom 0.97x-2.44x, on average 1.25x. In Albert inference,\nTurboâ€™s speedups to PyTorch are ranging from 1.04x-1.45x,\non average 1.17x. In the case of DistilBert inference, Turboâ€™s\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\nspeedups to PyTorch are ranging from 0.85x-2.24x, on aver-\nage 1.13x. In the case of Decoder inference, Turboâ€™s speedups\nto PyTorch are ranging from 1.14x-1.20x, on average 1.16x.\nThe performance improvement is more obvious in test cases\nof short input sequences. The latency of long input sequences\nis mainly limited by GEMM operations, which are not opti-\nmized by our runtime. Since the GEMM operations are con-\nducted on the tensor core, Turbo-TC always leads to much\nlow latency. For the same reason, the decrease in Albertâ€™s\nspeedup to Bert is caused by the larger parameters of the Al-\nbert model, leading to an increase in the proportion of GEMM\noperations.\nTurboTransformers exhibits similar performance with on-\nnxruntime. In the case of Bert, the speedups of Turbo to on-\nnxruntime are ranging from 0.88x-1.05x, on average 1.01x. In\nthe case of DistilBert, the speedups of Turbo to onnxruntime\nare ranging from 0.83x-1.36x, on average 1.03x.\nThe time distribution of different BERT computation ker-\nnels is analyzed in Figure 10. We selected two test cases to\nshow the inference hotspots in a long sequence input as 400\nand a short sequence input as 20. In the sequence length 20,\nthe GEMM kernels account for 70.31% of overall compu-\ntation time. The Softmax kernel (ApplyMaskAndSoftmax)\nonly accounts for 1.85% of time, and LayerNorm kernels (Ad-\ndBiasLayerNorm + BertOutput/AddBiasLayerNorm) account\nfor 2.71% of time. In the sequence length 400, the GEMM\nkernels account for 82.80% of overall computation time. The\nSoftmax accounts for 4.57%, and LayerNorm accounts for\n3.64%. The remaining time is spent in element-wise opera-\ntions, such as activations, biases addition, tensor transposing,\nand reshaping. Our optimization for batch reduction is very\nsuccessful since they are no longer the main hotspots among\nthe non-GEMM kernels.\n0 10 20 30 40 50 60 70 80 90 100\nBertOutput/gemm\nBertIntermediate/gemm\nGemm012fused\nBatchGemm4\nGemm5\nBatchGemm3\nApplyMaskAndSoftmax\nBertIntermediate/AddBiasAct\nSplitAddBiasTransposeForScore\nAddBiasLayerNorm\nBertOutput/AddBiasLayerNorm\nTransposeForScore\nLayerNorm\nOthers\nseqlen=400\nseqlen=20\nPercentage (%)\nFigure 10. Time distribution of different BERT computation\nkernels in the case of long and short sequences.\n6.2.2 Memory Optimization on Variable-length Requests.\nFor memory optimization, we first analyze the memory foot-\nprint of the intermediate tensors. Our proposed model-aware-\nallocator is compared with three other allocators, including\nthe allocators of PyTorch and onnxruntime and an alloca-\ntor implemented by Greedy by Size for Offset Calculation\n(GSOC) algorithm proposed in work [24], which has achieved\nthe near-optimal memory footprint for fixed-length input in-\nference. Memory footprint results are reported in Figure 11\nand the memory allocation results are reported in Figure 12,\nwhich are evaluated using C++ APIs. The memory allocated\nfor intermediate tensors in PyTorch and onnxruntime keep\nincreasing during benchmarking. After processing a long se-\nquence request, 460 in this case, the memory usage reaches\nits peak and no longer drops. The two runtimes incrementally\nbuild up a cache of CUDA memory and reassign it to later\nallocations. When there are no available fragments in the\ncached memory blocks, they allocate a large block of addi-\ntional memory and never release it until the memory usage\nreaches an upper limit. In contrast, our allocator and GCOS\nuse the information of computation-graph to control memory\nusage wisely. As shown in the figure, the maximum memory\nusage of us is 12.15 MB. The memory footprint of Turbo is\nquite close to the GCOS method. However, Turbo allocates\nand frees less memory than GCOS for each inference.\nPyTorch onnxrt Turbo GSOC\nFootprint (MB)0\n20\n40\n60\n80\nSequence Length\n437\n202\n393\n460\n220\n25\n137\n499\n266\n253\n212\n475\n406\n429\n160\n500\n249\n188\n303\n461\n469\n116\n263\n76\n149\n76\n391\n53\n321\n414\n133\n470\n277\n366\n419\n313\n466\n80\n163\n55\n378\n42\n465\n440\n355\n174\n246\n291\n56\n186\n227\n166\n317\n332\n472\n109\n499\n287\n249\n231\n448\n271\n138\n36\n417\n475\n285\n473\n12\n52\n373\n435\n209\n368\n427\nFootprint of BETR Intermediate Tensors\nFigure 11. Footprint of intermediate tensors during BERT\ninferences.\nPyTorch onnxrt Turbo GSOC\nAllocation+Free (MB)\n0\n20\n40\nSequence Length\n437\n202\n393\n460\n220\n25\n137\n499\n266\n253\n212\n475\n406\n429\n160\n500\n249\n188\n303\n461\n469\n116\n263\n76\n149\n76\n391\n53\n321\n414\n133\n470\n277\n366\n419\n313\n466\n80\n163\n55\n378\n42\n465\n440\n355\n174\n246\n291\n56\n186\n227\n166\n317\n332\n472\n109\n499\n287\n249\n231\n448\n271\n138\n36\n417\n475\n285\n473\n12\n52\n373\n435\n209\n368\n427\nDevice memory allocation and free of intermediate tensor during BERT inferences\nFigure 12. Amount of device memory allocation/free for\nintermediate tensors during BERT inferences.\nWe also analyze the overall memory footprint of the run-\ntime. The peak device memory usage is measured by mon-\nitoring the nvidia-smi information in every 1 ms. the peak\nGPU memory used for Turbo is 663 MB while the GPU peak\nmemory used by PyTorch is 1307 MB and 1653 MB for on-\nnxruntime. Since the CUDA contexts of a variety of CUDA\nkernels need to take up a large amount of memory space 2,\nPyTorch and onnxruntime require more GPU memory than\nthe DNN model and intermediate tensors actually used, which\nvalid our efforts to implement a lightweight transformer-target\ninference runtime instead of a training framework or a general\ninference runtime.\nTo prove our proposed model-aware allocatorâ€™s efficiency,\nwe measure the overhead of Algorithm 1 to schedule memory\noffset of each intermediate tensor. Since the time complexity\n2https://github.com/pytorch/pytorch/issues/20532\nTurboTransformers PPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nof the algorithm is O(#ğ‘¡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ2), a trick to reduce the number\nof tensors is that, for the DNN model with repeated structures,\nwe only compute memory addresses once for intermediate\ntensors in one of those structures and reuse them in the other\nones. We measure the cost of Algorithm 1 in each inference\nprocess using a BERT model with input sequence lengths\nrandomly generated from 5 to 500. As shown in Fig. 13., the\naverage cost of offset scheduling (cost of Algorithm 1) is\n1.8% on average (0.07%-5.77%) of DNN inference latency.\nThe overhead of the algorithm is extremely low and its benefit\noutweighs its cost.\nPercentage (%)0\n2\n4\n6\nsequence length\n0 50 100 150 200 250 300 350 400 450 500\nThe cost of offset scheduling to the total inference cost\nFigure 13. The offset scheduling overhead of model-aware\nallocator.\n6.2.3 End-to-End Speed on Fixed-length Requests. We\nalso compared with another three popular runtimes that only\nsupport fixed-length requests. 1) TensorFlow-XLA is imple-\nmented with TensorFlow (version 1.13) and preprocessed\nwith XLA. 2) Faster Transformers (v1) is a transformer boost\nsoftware developed by NVIDIA, which implements a set of\ncustomized fused kernels like us. However, it has no memory\nmanagement and using the memory allocator of the Tensor-\nFlow framework. 3) NVIDIA TensorRT (v5.1.5) is an SDK\nfor high-performance deep learning inference. The above\nthree solutions require a time-consuming pre-tuning process\nbased on the input dimension in advance, so they cannot be\napplied to handling real-time variable-length requests.\nFor the sake of fairness, we chose BERT as the transformer\nmodel to be evaluated on a fixed-length input task, for which\nevery runtime has been specifically optimized for it, and an\nofficial example is provided. We select a parameter space\nconsisting of the Cartesian Product of a set of batch sizes\nincluding 1 and 20 and a set of sequence lengths sampled from\n10 to 500. The speedups of TurboTransformers to the other\nruntimes are shown in Figure 14. Compared with PyTorch, the\nspeedup of Turbo is 1.23x-2.77x, on average 1.54x. Compared\nwith onnruntime-gpu, the speedup of Turbo is 1.01x-1.26x, on\naverage 1.11x. Compared with TensorFlow-XLA, the speedup\nof Turbo is 1.03x-1.31x, on average 1.11x. Compared with\nFaster Transformers, the speedup of Turbo is 0.71x-1.32x,\non average 0.91x. Compared with TensorRT, the speedup\nof Turbo is 0.53x-0.96x, on average 0.87x. On average, our\nruntime is around 10% faster than XLA and onnxruntime and\naround 10% slower than Faster Transformers and TensorRT.\nTensorRT needs an offline tuning process, during which it can\nselect the optimal parameters for GEMM kernels and may\nidentify the optimal CUDA thread block sizes for non-GEMM\nkernels. On the contrary, our runtime does not involve the\nbenefits of these optimizations.\nPyTorch\nonnxrt-gpu\nTensorFlow-XLA\nFasterTransformers\nTensorRT\nTurbo-TC\nSpeedup\n0\n1\n2\n3\n(batch size, seq length)\n1,10 1,20 1,40 1,60 1,80 1,1001,2001,3001,4001,50020,1020,2020,4020,6020,8020,10020,20020,30020,40020,500\nFixed-length input BERT inference on RTX 2060\nFigure 14. Benchmarking the runtimes for fixed-length input\ntasks on RTX 2060. The y-axis indicates normalized speedup\nof TurboTransformers.\n6.3 Performance of Serving Framework\nWe choose a BERT-based service as our target application,\nwhich is used to classify a paragraph of text. The input text\nis first randomly generated with sequence length uniformly\ndistributed between 2-100 and then generated with sequence\nlength uniformly distributed between 5-500. The requests are\nsent to the serving system with Poisson inter-arrival times.\nWe turn off the caching optimization.\nThere are two strategies to build the cached_cost dictio-\nnary in Algorithm 2. First, if the parameter space is small,\na warmup phase that records the latency of executing with\nall possible parameters is required after the service started.\nIt takes tens of minutes in our experiments. Second, if the\nparameter space is large, we sample the parameter space and\nuse the interpolation method to estimate a specific case during\nserving. After you get real data, it can be used to update the\ndictionary in a lazy evaluation way.\nThe serving throughput of input sequence length ranging\n2-100 is illustrated in Figure 15. The figureâ€™s x-axis indicates\nhow many requests arrive at the serving system per second,\nranging from 20 req/sec to 1500 req/sec. The figureâ€™s y-axis\nrepresents how many responses can be obtained per second,\nwhich is usually called serving throughput. The Critical Point\nfor service latency to remain stable is that request throughput,\nand serving throughput are equal. When request throughput is\nhigher than the critical point, the requests will accumulate in\nthe message queue, leading to long delays of latter requests.\nAfter a while, its latency will gradually tend to infinity (+âˆ),\nand the service system has to drop some requests.\nOur proposed batch scheduler achieves the best serving\nthroughput. The baseline is shown as PyTorch-Nobatch, which\nuses PyTorch as the runtime and serve without batching opti-\nmization. Turbo-NoBatch is the service using our proposed\nruntime to replace PyTorch. Turbo-Naive-Batch is imple-\nmented with a naive batch scheduler, which packs the re-\nquests currently inside the message queue into a single batch.\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\nTurbo-DP-Batch is our proposed variable-length-aware batch\nscheduler. The batch scheduler of our system employs the\nhungry strategy, and the maximum batch size is 20. The serv-\ning throughput of PyTorch-Nobatch is saturated at 99 resp/sec.\nThe serving throughput of Turbo-NoBatch is saturated at 237\nresp/sec (2.39x). Turbo-Naive-Batch improves it to 323 re-\nsp/sec (3.26x) and the Turbo-DP-Batch futher improves it to\n402 resp/sec (4.06x).\nThe latency results on four systemsâ€™ critical points are\nshown in Table 4. Since batching brings higher GPU utiliza-\ntion for short requests, the latency of Naive-Batch is smaller\nthan NoBatch. Turbo-DP-Batch sorts the requests in MQ, the\nexecution of long sequences is delayed, thus increasing their\nlatency. Therefore Naive-Batch brings smaller latency than\nTurbo-DP-Batch.\nPyTorch-NoBatch\nTurbo-NoBatch\nTurbo-Naive-Batch\nTurbo-DP-Batch\nServing Throughput (resp/sec)0\n100\n200\n300\n400\nRequest Throughput (req/sec)\n40 60 80 100 120 140 250 500 750 1000 1250 1500\nServing with requests of variable sequence length (2-100)\nFigure 15. Serving throughput under different request\nthroughput (Sequence Length 2-100).\nTable 4.The latency of four serving systems (sequence length\n2-100). Table item contains the avg (min, max) latency in ms.\nRequest PyTorch Turbo\nThrpt (req/sec) NoBatch NoBatch Naive-Batch Turbo-DP-Batch\n99 36.52 7.47 7.51 7.49\n(10.05, 108.72) (3.84, 18.59) ( 3.78, 21.58) (3.82, 20.23)\n237 +âˆ 16.68 10.09 10.89\n(3.68, 45.39) ( 3.67, 28.60) (3.81, 37.00)\n323 +âˆ +âˆ 12.44 15.37\n(3.48, 26.70) (3.58, 40.45)\n402 +âˆ +âˆ +âˆ 24.74\n(4.23, 57.02)\nWhen increasing the length difference between the se-\nquences by changing the input sequence range from 2-100 to\n5-500, our proposed allocator achieves the best throughput\nand leads to the lowest latency. The throughput of input se-\nquence length ranging 5-500 is illustrated in Figure 16 and\nthe latency results are shown in Table 5. In this case, we turn\non the tensor core optimization, which brings no accuracy\nloss and can better satisfy the SLO. The serving through-\nput of PyTorch-Nobatch is saturated at 60 resp/sec, while\nTurbo-TC-NoBatch improves it to 120 resp/sec (2.0x). Turbo-\nTC-DP-Batch further improves it to 144 resp/sec (2.4x). Due\nto the additional zero-padding overhead, Turbo-TC-Naive-\nBatchâ€™s throughput is 98 resp/sec, which is even worse than\nTurbo-NoBatch. As shown in Table 5. Under the same request\nthroughput rate, Turbo-DP-Batch usually brings the lowest\naverage and maximum service latency. This is because of the\nreduction of zero-padding cost and shortening the latency of\nevery single request.\nPyTorch-NoBatch\nTurbo-TC-NoBatch\nTurbo-TC-Naive-Batch\nTurbo-TC-DP-Batch\nServing Throughput (resp/sec)0\n50\n100\n150\nRequest Throughput (req/sec)\n40 60 80 100 120 140 250 500 750 1000 1250 1500\nServing with requests of variable sequence length (5-500)\nFigure 16. Serving throughput under different request\nthroughput (Sequence Length 5-500).\nTable 5.The latency of four serving systems (sequence length\n5-500). Table item contains the avg (min, max) latency in ms.\nRequest PyTorch Turbo\nThrpt (req/sec) NoBatch NaiveBatch NoBatch Turbo-DP-Batch\n60 77.71 17.80 8.05 8.05\n(10.61, 158.06) (3.06, 121.96) (2.76, 20.53) ( 2.70, 27.42)\n98 +âˆ 16.68 24.88 13.79\n(2.96, 65.09) (3.0, 65.09) (2.94, 45.09)\n120 +âˆ +âˆ 32.91 23.18\n(3.14, 127.61) (2.72, 81.83)\n144 +âˆ +âˆ +âˆ 38.51\n(4.44, 106.65)\n7 Conclusion\nTurboTransformers improves latency and throughput for de-\nploying transformer-based DL services in GPU datacenter. It\nsolves two critical problems introduced by the transformer\nmodels, which are unprecedented computation pressure and\nvariable-length input. For these purposes, it proposed three\ninnovations in computing, memory and serving levels, includ-\ning a new parallel batch reduction algorithm for Softmax and\nLayerNorm kernels, a sequence-length-aware memory alloca-\ntor as well as a sequence-length-aware batch scheduler. The\nruntime achieves better speed than PyTorch and similar speed\nas onnxruntime in the variable-length request tests but main-\ntains a smaller memory footprint. It also achieves comparable\nspeed than TensorFlow-XLA, TensorRT, and FasterTrans-\nformers in the fixed-length request tests. While conventional\nbatching is inefficient for variable-length request, the serving\nframework achieves higher throughput using the proposed\nbatch scheduler.\n8 Acknowledgements\nWe would like to thanks Yin Li (UC Davis), Shengqi Chen,\nWentao Han (Tsinghua Univ.) for their proofreading.\nTurboTransformers PPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea\nReferences\n[1] MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,\nJeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,\nMichael Isard, et al. 2016. Tensorflow: A system for large-scale ma-\nchine learning. In12th USENIX symposium on operating systems design\nand implementation (OSDI16). 265â€“283.\n[2] James Bergstra, Olivier Breuleux, FrÃ©dÃ©ric Bastien, Pascal Lamblin,\nRazvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-\nFarley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math\nexpression compiler. In Proceedings of the Python for scientific com-\nputing conference (SciPy), V ol. 4. Austin, TX, 1â€“7.\n[3] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie\nYan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis\nCeze, et al. 2018. TVM: An automated end-to-end optimizing compiler\nfor deep learning. In 13th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 18). 578â€“594.\n[4] Kyunghyun Cho, Bart Van MerriÃ«nboer, Dzmitry Bahdanau, and\nYoshua Bengio. 2014. On the properties of neural machine translation:\nEncoder-decoder approaches. arXiv preprint arXiv:1409.1259 (2014).\n[5] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin,\nJoseph E Gonzalez, and Ion Stoica. 2017. Clipper: A low-latency online\nprediction serving system. In 14th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI 17). 613â€“627.\n[6] Weihao Cui, Mengze Wei, Quan Chen, Xiaoxin Tang, Jingwen Leng,\nLi Li, and Mingyi Guo. 2019. Ebird: Elastic Batch for Improving\nResponsiveness and Throughput of Deep Learning Services. In 2019\nIEEE 37th International Conference on Computer Design (ICCD) .\nIEEE, 497â€“505.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n2018. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n[8] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency\nrnn inference with cellular batching. In Proceedings of the Thirteenth\nEuroSys Conference. 1â€“15.\n[9] Google. 2020. TensorFlow Serving. https://github.com/tensorflow/\nserving. [Online; accessed July-2020].\n[10] Google. 2020. XLA: Optimizing Compiler for Machine Learning.\nhttps://www.tensorflow.org/xla. [Online; accessed July-2020].\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition. 770â€“778.\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term\nmemory. Neural computation 9, 8 (1997), 1735â€“1780.\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Im-\nagenet classification with deep convolutional neural networks. In Ad-\nvances in neural information processing systems. 1097â€“1105.\n[14] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,\nPiyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for\nself-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 (2019).\n[15] Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pisarchyk,\nMogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei Kulik, and\nMatthias Grundmann. 2019. On-device neural net inference with mo-\nbile gpus. arXiv preprint arXiv:1907.01989 (2019).\n[16] Yuan Lin and V Grover. 2019. Using cuda warp-level primitives.\nRetrived from https://devblogs. nvidia. com/using-cuda-warp-level-\nprimitives/. Accessed (2019).\n[17] Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. 2019. PaddlePad-\ndle: An open-source deep learning platform from industrial practice.\nFrontiers of Data and Domputing 1, 1 (2019), 105â€“115.\n[18] Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng,\nand Jeffrey S Vetter. 2018. Nvidia tensor core programmability, per-\nformance & precision. In 2018 IEEE International Parallel and Dis-\ntributed Processing Symposium Workshops (IPDPSW). IEEE, 522â€“531.\n[19] MicroSoft. 2020. ONNX Runtime is a cross-platform inferencing and\ntraining accelerator compatible with many popular ML/DNN frame-\nwork. https://github.com/microsoft/onnxruntime. [Online; accessed\nJuly-2020].\n[20] NVIDIA. 2019. FasterTransformer V1, a highly optimized BERT equiv-\nalent Transformer layer for inference. https://github.com/NVIDIA/\nDeepLearningExamples/tree/master/FasterTransformer. [Online;\naccessed April-2020].\n[21] NVIDIA. 2020. cuDNN. https://developer.nvidia.com/cudnn. [On-\nline; accessed Augest-2020].\n[22] NVIDIA. 2020. NVIDIA TensorRT. https://developer.nvidia.com/\ntensorrt. [Online; accessed July-2020].\n[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\nBradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia\nGimelshein, Luca Antiga, et al . 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances in neural informa-\ntion processing systems. 8026â€“8037.\n[24] Yury Pisarchyk and Juhyun Lee. 2020. Efficient Memory Management\nfor Deep Neural Net Inference. arXiv preprint arXiv:2001.03288\n(2020).\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\nand Ilya Sutskever. 2019. Language models are unsupervised multitask\nlearners. OpenAI Blog 1, 8 (2019), 9.\n[26] NVIDIA Research. 2020. CUB. https://github.com/NVlabs/cub.\n[Online; accessed Augest-2020].\n[27] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.\n2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108 (2019).\n[28] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,\nMatthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019.\nNexus: a GPU cluster engine for accelerating DNN-based video analy-\nsis. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples. 322â€“337.\n[29] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\nDragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew\nRabinovich. 2015. Going deeper with convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition. 1â€“9.\n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need. In Advances in neural information processing\nsystems. 5998â€“6008.\n[31] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R\nSalakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. In Advances in neural\ninformation processing systems. 5753â€“5763.\nA Artifact Appendix\nA.1 Abstract\nThe artifact contains the code for the runtime of TurboTrans-\nformers. We provide instructions for obtaining critical results\nused in this paper and scripts for running the experiments in\nthe paper.\nA.2 Description\nA.2.1 Check-list (artifact meta information).\nâ€¢Algorithms: The artifact includes the runtime compo-\nnent of TurboTransformers. More specifically, it con-\ntains the parallel batch-reduction algorithms proposed\nin Section 4.1.2, as well as the model-aware-allocator\nproposed in Section 4.2.\nPPoPP â€™21, February 27-March 3, 2021, Virtual Event, Republic of Korea Jiarui Fang and Y ang Yu, et al.\nâ€¢Datasets: The inputs used for benchmark scripts of the\nartifact are randomly generated.\nâ€¢Compilation: The experiments in the paper used g++\nversion 7.5.0, nvcc version 10.2. The artifact also pro-\nvided a docker file to build a docker environment to\ncompile the code. The docker version used is 19.03.8.\nâ€¢Runtime environment: The artifact provided a compi-\nlation script to run on the CentOS 7 OS installed with\nCUDA version 10.2.\nâ€¢Hardware: The artifact can run on a server equipped\nwith at least one NVIDIA GPU. The supported GPU\nmicroarchitecture codenames include Pascal, V olta, Tur-\ning.\nâ€¢Output: Running times of the algorithms are output to\nthe console.\nâ€¢Experiment Workflow: Clone the repository and use\nthe provided scripts to run the experiments.\nâ€¢Publicly available: Yes\nA.2.2 How Delivered. Available as open-source under the\nBSD license: https://github.com/Tencent/TurboTransformers.\nThe artifact branch is ppopp21_artifact_centos.\nA.3 Installation\nA.3.1 Docker.\nâ€¢Build a docker image using the docker file.\n1 bash tools/build_docker_gpu.sh $PWD\nâ€¢Run the image as a container\n1 nvidia-docker run --gpus all\n--net=host --rm -it -v\n$PWD:/workspace\n--name=turbo_dev:latest\nâ€¢Inside the container, build the artifact.\n1 cd /workspace\n2 bash\ntools/build_and_run_unittests.sh\n$PWD -DWITH_GPU=ON\nA.3.2 CentOS.\nâ€¢Use the following command to build the artifact on\nCenOS 7.\n1 bash build_centos.sh\nA.4 Experiment Workflow\nYou can run the scripts provided in the benchmark directory\nto compare the speed of turbo runtime with PyTorch. Run the\nfollowing script will reproduce the Bert and Albert results\nshown in Figure 9.\n1 bash gpu_run_variable_benchmark.sh\nRun the following script will reproduce the Bert and Albert\nresult in Figure 14.\n1 bash gpu_run_fixed_benchmark.sh"
}