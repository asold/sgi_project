{
  "title": "Ethical data acquisition for LLMs and AI algorithms in healthcare",
  "url": "https://openalex.org/W4405713523",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5051234274",
      "name": "Marta Williams",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5115615855",
      "name": "Wasie Karim",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5109608216",
      "name": "J. Gelman",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5045670851",
      "name": "Marium Raza",
      "affiliations": [
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384389802",
    "https://openalex.org/W4399317294",
    "https://openalex.org/W4392986561",
    "https://openalex.org/W4400087060",
    "https://openalex.org/W4313313666",
    "https://openalex.org/W4311102276",
    "https://openalex.org/W4282838692",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W4387584778",
    "https://openalex.org/W4315646960",
    "https://openalex.org/W4322616538",
    "https://openalex.org/W4380538551",
    "https://openalex.org/W4320016009",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W3200896515",
    "https://openalex.org/W2607176948"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Perspective\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-024-01399-9\nEthical data acquisition for LLMs and AI\nalgorithms in healthcare\nCheck for updates\nMarta Williams , Wasie Karim, Justin Gelman & Marium Raza\nArtiﬁcial intelligence (AI) algorithms will become increasingly integrated into our healthcare systems in\nthe coming decades. These algorithms require large volumes of data for development andﬁne-tuning.\nPatient data is typically acquired for AI algorithms through an opt-out system in the United States,\nwhile others support an opt-in model. We argue that ethical principles around autonomy, patient\nownership of data, and privacy should be prioritized in the data acquisition paradigm.\nArtiﬁcial intelligence (AI) innovation has permeated nearly every industry,\nincluding healthcare. AI refers to computer technology that reasons and\ncognitively functions in a similar manner to humans. Recent advancements\nin AI include large language models (LLMs) - AI tools trained on large\namounts of data to simulate human conversation. LLMs like ChatGPT and\nBard attract great excitement, with human-like responses to queries across\ndiverse knowledge areas. LLMs can excel at speciﬁc, task-oriented problems\neither through‘ﬁne-tuning’on speciﬁc, relevant datasets or by leveraging\ntechniques such as in-context learning or Chain-of-Thought (CoT)\nprompting. The effectiveness of these techniques can vary depending on the\nmodel architecture, training data, and whether the model is open-source or\nproprietary\n1. Within healthcare, LLMs are already being used to assist with\nadministrative tasks such as clinical note-writing and patient portal\ncommunications2,3. These applications highlight the potential of LLMs to\ntransform healthcare delivery by improving ef ﬁciency and patient\nengagement.\nMoreover, AI has the potential to signiﬁcantly improve patient out-\ncomes as AI tools continue to be developed and integrated into clinical\npractice. For example, recent trial data suggest AI may improve care for\npatients suspected to be having a myocardial infarction\n4.F u r t h e r m o r e ,A I -\nassisted imaging technology is already in use to aid physicians in the real-\ntime identiﬁcation of cancerous polyps during colonoscopies. Without a\ndoubt, the further integration of AI into medical practice is not only\ninevitable but also poised to revolutionize healthcare if done appropriately.\nHowever, it must be acknowledged that AI models are developed\nwithin the conﬁnes of existing structural inequities, and without deliberate\neffort, are at risk of perpetuating them\n5,6. While historical cases like Dr.\nSims’se x p e r i m e n t so ne n s l a v e dw o m e na n dt h eT u s k e g e es y p h i l i ss t u d y\nillustrate a long-standing precedent of medical exploitation, modern\nexamples demonstrate that these issues have the potential to persist in\ncontemporary AI-driven healthcare7. For instance, Obermeyer et al. found\nracial bias in a widely used commercial health algorithm, where Black\npatients assigned the same risk level as White patients were sicker\n8. Simi-\nlarly, convolutional neural networks for skin lesion classiﬁcation show\nsigniﬁcantly reduced diagnostic accuracy for Black patients, as they are often\ntrained on datasets where only 5% to 10% of the images come from Black\nindividuals9. These examples underscore the capacity for AI to exacerbate\nexisting disparities if not developed with equity in mind and reﬂect the\no n g o i n gn e e dt oa d d r e s sb i a si nh e a l t h c a r ea l g o r i t h m st o d a y10.\nThe challenge for future development lies in acquiring data that is\nrepresentative of diverse patient populations, without impeding on patient\nrights or worsening existing population health disparities. Currently, data\nfor algorithm development is acquired in two ways: opt-in and opt-out. Opt-\nin involves patients explicitly providing informed consent to include their\nhealth data in an AI training dataset.Opt-out, the current default in the\nUnited States\n11, involves automatically including patient data in AI training\ndatasets unless patients speciﬁcally choose otherwise. We aim to deﬁne the\nbeneﬁts and pitfalls of each model, and argue that ethics should be prior-\nitized overﬁnancial incentives for future LLM development.\nComparing opt-in and opt-out\nThe opt-out model for collecting patient health data has advantages (Table\n1), nearly guaranteeing sample sizes representing the full spectrum of\ndiversity and thereby ensuring moreaccurate AI models and output. This\nmethod is easily scalable, with higher consent rates than seen with opt-in\nmodels\n3, and provides a wealth of data with minimal expense or paperwork.\nThe disadvantages of the opt-out method, however, are signiﬁcant.\nBypassing an explicit informed consentprocess limits patient autonomy;\npatients may be unaware that their data is utilized, or that they have the\noption to limit that use. If patients arenot explicitly asked about the use of\ntheir data, they are also unlikely to be compensated for proﬁts garnered by\nmodels trained using their data.\nThe opt-in model for data collection has signiﬁcant advantages in\nterms of informed consent and patient autonomy (Table1). The current\ndefault model for most of Europe based on the European Union General\nData Protection Regulation\n12, opting-in requires that patients be informed\nabout, and provide consent for, the use of their data in AI development. The\nopt-in model improves patient trust and prioritizes transparency between\nresearchers, healthcare providers, and patients. The disadvantages for opt-in\ndata collection models are lower consent rates and consent bias\n11.O p t - i n\nHarvard Medical School, Boston, MA, USA. e-mail: martawilliams@hms.harvard.edu\nnpj Digital Medicine|           (2024) 7:377 1\n1234567890():,;\n1234567890():,;\nprocedures tend to be biased towards the inclusion of patients who are male,\nmore highly educated, and of higher socioeconomic status11,t h u sA Im o d e l s\ntrained using that data are likely to be similarly skewed. Opt-in procedures\nare also more labor-intensive and expensive because of the time, money, and\npaperwork required to inform patients and document their consent.\nCall to action\nGiven the rapid integration of AI into healthcare, it is imperative for the\nhealthcare and AI communities to prioritize patient needs as the central\nfocus while advancing the implementation of this technology.\nIdeally, the opt-in model would address concerns of patient\nautonomy by requiring informed consent before collecting patient data.\nYet, this model risks perpetuating existing biases by failing to recruit a\nrepresentative patient population as so many other well-intentioned\nhealthcare efforts currently do. We considered how an opt-in model\ncould more successfully recruit patients across the socioeconomic and\neducational spectrum. Opportunities could involve direct compensation\nfor data collection or discounted healthcare services. Yet even when\ncompensation is offered, studies often struggle to recruit under-\nrepresented populations due to historical injustices and structural health\ninequities, including ﬁnancial and transportation barriers to study\nparticipation\n13,14. For example, between 2015 and 2019, 78% of FDA\nclinical trial participants were non-Hispanic whites, despite them com-\nprising only 61% of the population\n13. While a 2023 study found that a\n$100 incentive was more effective at increasing participation among\nwhite and afﬂuent people than among those from low-income or non-\nwhite households, a larger $500 incentive closed the participation gap\namong different racial, ethnic, and socioeconomic groups, indicating\nthat sufﬁcient ﬁnancial incentives may persuade underrepresented\npatients to opt in\n15. However, ﬁnancial compensation to this degree\nwould be unsustainable considering the prodigious amount of patient\ndata required for a well-functioning LLM.\nGiven theﬁnancial incentives and ease ofdata collection offered by\nopt-out models, it is likely that opt-out models will predominate despite\ntheir potential ethical disadvantages. Advancements in privacy-preserving\nmethodologies, such as differentialprivacy and federated learning, may\nprovide a path forward for the opt-out model while upholding ethical\nprinciples\n16. Differential privacy introducescarefully calibrated noise into\ndatasets, ensuring that individual data points remain unidentiﬁable while\npreserving the overall utility of the data. Federated learning enables\ndecentralized AI training by keeping patient data within local systems and\nsharing only aggregated updates, reducing risks associated with centralized\nrepositories and enhancing data security. By integrating these technologies\ninto data collection frameworks, it will be possible to maintain patient\nprivacy and autonomy while still acquiring the diverse, representative\ndatasets necessary for unbiased AI model development. Alongside tech-\nnological developments, it is imperative that opt-out models be made with\ntransparency in mind to appeal to patients concerned about privacy; these\nactions will not only address immediate ethical concerns but also foster trust\nand inclusivity as AI technologies become more commonplace in\nhealthcare.\nThe following are three speciﬁc actions we recommend:\n1. Patients must be provided with clear and concise terms and conditions\nthat are less than one page and written in patient-centered language\nrather than legal jargon.\nUnlike those of smartphones and social media platforms that are so\nlong that most people skip past them, consent in an equitable opt-out\nsystem must be accessible to patients of all health literacy levels.\nHealthcare providers and administrators should create clear, concise\nterms and conditions, as well as educational materials that help\npatients understand the impact of their data on predictive analytics.\nThese materials can be provided to patients at the outset explaining\nthe use of their data and their right to opt-out, rather than their data\n“silently” being used behind a wall of lengthy terms and conditions.\nAdditionally, securing consent at regular intervals through online\nportals or in-person reminders at clinic visits can ensure that patients\nremain aware of their rights and can exercise autonomy within an\nopt-out system.\n2. Patients must be the ultimate owners of their data, and infrastructure\nmust be built to both protect patient data and allow patients to readily\nextricate their information from databases by request.\nWhether opt-in or opt-out models are pursued, concerns regarding\ndata ownership still stand. When eliciting concerns regarding AI in\nhealthcare, patients consistently voice fears about rising costs to\nincorporate this novel technology\n17. Given the inevitable proﬁt\nmotive underlying LLM implementation in healthcare, it is hard to\nimagine a sustainable system in which patients are asked to provide\ntheir data for free to develop models for which they are subsequently\ncharged. This dilemma will require extensive conversation on patient\ndata ownership, compensation, and reimbursement for the use of AI\ntechnology. The commodiﬁcation of patient data, without adequate\nsafeguards and fair compensation, risks perpetuating the legacy of\nexploitation exempliﬁed by Henrietta Lacks, whose cells were used to\ngenerate enormous proﬁts without any compensation to her or her\nfamily\n7. If healthcare and technology companies do not drawﬁrm\nboundaries on patients’ right to own their data, we may see a\ncontinuation of healthcare ex ploiting our most vulnerable\ncommunities.\n3. Government and healthcare organizations must immediately invest in\ncreating and enforcing regulatorystandards to ensure patient safety\nand trust.\nPlacing the burden of ethical practice entirely on individual organi-\nzations and patients is insufﬁcient. Many patients lack the data and\ntechnology literacy necessary to make truly informed decisions about\ncontributing their data\n18,19. Patients cannot and should not be\nexpected to be capable of interrogating the safety, transparency, and\nreversibility of their data contributions to LLMs. Therefore, respon-\nsibility must also fall on other partners— such as funders, healthcare\nTable 1 | Advantages and Disadvantages of Opt-In and Opt-Out Models\nRelevant factors Opt-in consent Opt-out consent\nAutonomy Respects patient autonomy/choice Offers limited autonomy\nInformed Consent Involves explicit informed consent May bypass the informed consent process\nParticipation Rates Limited participation Greater participation\nAdministrative Efﬁciency May require investment in recruitment, advertisement,\nmessaging\nEasily scalable, ease of collection\nSelection Bias Biases model to skew male, higher education level, and\nhigher SES\nLarger, more diverse samples that may yield more accurate AI models\nTrust Building Enhanced transparency improves trust between\nresearchers, healthcare providers, and patients\nPatients, especially vulnerable patient populations, may not be aware of\ntheir right to opt out if nobody explicitly tells them\nEquity and Fairness/Beneﬁt\nSharing\nPatients can be informed and possibly compensated for\ntheir data contributions\nPatients might not be compensated for proﬁt gained from an algorithm\ntrained on their data\nhttps://doi.org/10.1038/s41746-024-01399-9 Perspective\nnpj Digital Medicine|           (2024) 7:377 2\nsystems, healthcare administrators, data use committees, and others\ninvolved in the AI healthcare enterprise— to contribute to the ethical\nintegration of AI. Adopting frameworks like differential privacy and\nfederated learning will ensure that patient data is utilized ethically and\ninclusively, minimizing risks while maintaining the utility of datasets.\nIt is also imperative to uphold the trust that underpins an opt-out\nsystem by establishing a third-party regulatory board, supported by\nthe government, to develop and enforce transparency and safety\nstandards while ensuring ongoing compliance through rigorous\noversight.\nWith signiﬁcant existing patient skepticism surrounding AI, we must\nanticipate and respond proactively to concerns to ensure LLMs are\nrepresentative, transparent, and respectful of patient autonomy.\nLLMs have already begun to revolutionize science, medicine, and the\nspeed at which we can advance in any givenﬁeld. We have already\nseen the immense beneﬁt AI can provide patients, and the AI com-\nmunity must ensure these beneﬁts are available toall patients by\ncombating the reinforcement of existing biases in emerging\ntechnologies.\nReceived: 3 June 2024; Accepted: 16 December 2024;\nReferences\n1. Naveed, H. et al. A Comprehensive Overview of Large Language\nModels. Preprint athttps://doi.org/10.48550/arXiv.2307.06435\n(2024).\n2. Hudelson, C. et al. Selection and implementation of virtual scribe\nsolutions to reduce documentation burden: a mixed methods pilot.\nAMIA Summits Transl. Sci. Proc.2024, 230–238 (2024).\n3. Garcia, P. et al. Artiﬁcial intelligence–generated draft replies to patient\ninbox messages.JAMA Netw. Open7, e243201 (2024).\n4. Lin, C. et al. Artiﬁcial intelligence–powered rapid identiﬁcation of ST-\nElevation Myocardial Infarction via Electrocardiogram (ARISE)— A\npragmatic randomized controlled trial.NEJM AI1, AIoa2400190\n(2024).\n5. Jindal, A. Misguided artiﬁcial intelligence: how racial bias is built into\nclinical models.J. Brown Hosp. Med.2,1 –6 (2022).\n6. Agarwal, R. et al. Addressing algorithmic bias and the perpetuation of\nhealth inequities: An AI bias aware framework.Health Policy Technol.\n12, 100702 (2023).\n7. Baptiste, D. et al. Henrietta Lacks and America’s dark history of research\ninvolving African Americans.Nurs. Open9,2 2 3 6–2238 (2022).\n8. Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting\nracial bias in an algorithm used to manage the health of populations.\nScience 366, 447–453 (2019).\n9. Kamulegeya, L. et al. Using artiﬁcial intelligence on dermatology\nconditions in Uganda: a case for diversity in training data sets for\nmachine learning.Afr. Health Sci.23, 753–763 (2023).\n10. Raza, M. M., Venkatesh, K. P. & Kvedar, J. C. Promoting racial equity\nin digital health: applying a cross-disciplinary equity framework.Npj\nDigit. Med.6,1 –3 (2023).\n11. de Man, Y. et al. Opt-in and opt-out consent procedures for the reuse\nof routinely recorded health data in scientiﬁc research and their\nconsequences for consent rate and consent bias: systematic review.\nJ. Med. Internet Res.25, e42131 (2023).\n12. What is GDPR, the EU’s new data protection law?GDPR.eu https://\ngdpr.eu/what-is-gdpr/ (2018).\n13. National Academies of Sciences, Engineering, and Medicine; Policy\nand Global Affairs; Committee on Women in Science, Engineering,\nand Medicine; Committee on Improving the Representation of\nWomen and Underrepresented Minorities in Clinical Trials and\nResearch. Improving Representation in Clinical Trials and Research:\nBuilding Research Equity for Women and Underrepresented Groups\n.\n(National Academies Press (US), Washington (DC), 2022).\n14. Fairley, R. et al. Increasing clinical trial participation of black women\ndiagnosed with breast cancer.J. Racial Ethn. Health Disparities11,\n1701–1717 (2024).\n15. Dutz, D. et al. Representation and hesitancy in population health\nresearch: evidence from a COVID-19 Antibody Study. Working Paper\nat https://doi.org/10.3386/w30880 (2023).\n16. Abadi, M. et al. Deep learning with differential privacy. inProceedings\nof the 2016 ACM SIGSAC Conference on Computer and\nCommunications Security308–318 (Association for Computing\nMachinery, New York, NY, USA, 2016).https://doi.org/10.1145/\n2976749.2978318.\n17. Richardson, J. P. et al. Patient apprehensions about the use of artiﬁcial\nintelligence in healthcare.Npj Digit. Med.4,1 –6 (2021).\n18. Nguyen, A., Mosadeghi, S. & Almario, C. V. Persistent digital divide in\naccess to and use of the Internet as a resource for health information:\nResults from a California population-based study.Int. J. Med. Inf.103,\n49–54 (2017).\n19. Cohort bias in predictive risk assessments of future criminal justice\nsystem involvement | PNAS.https://www.pnas.org/doi/10.1073/\npnas.2301990120\nAuthor contributions\nMW, WK, JG, and MR all contributed equally to this manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nMarta Williams.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s41746-024-01399-9 Perspective\nnpj Digital Medicine|           (2024) 7:377 3",
  "topic": "Autonomy",
  "concepts": [
    {
      "name": "Autonomy",
      "score": 0.7096105813980103
    },
    {
      "name": "Health care",
      "score": 0.5749803185462952
    },
    {
      "name": "Computer science",
      "score": 0.5591146945953369
    },
    {
      "name": "Data science",
      "score": 0.4667262136936188
    },
    {
      "name": "Healthcare system",
      "score": 0.42402273416519165
    },
    {
      "name": "Big data",
      "score": 0.4146076440811157
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39843297004699707
    },
    {
      "name": "Algorithm",
      "score": 0.37754741311073303
    },
    {
      "name": "Data mining",
      "score": 0.20116034150123596
    },
    {
      "name": "Political science",
      "score": 0.19148901104927063
    },
    {
      "name": "Law",
      "score": 0.09159055352210999
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}