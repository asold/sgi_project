{
  "title": "MM-Transformer: A Transformer-Based Knowledge Graph Link Prediction Model That Fuses Multimodal Features",
  "url": "https://openalex.org/W4401207414",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104868541",
      "name": "Dongsheng Wang",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5108908084",
      "name": "Kangjie Tang",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096518151",
      "name": "Jun Zeng",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104596435",
      "name": "Yue Pan",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2131980566",
      "name": "Yun Dai",
      "affiliations": [
        "Jiangsu Police Officer College"
      ]
    },
    {
      "id": "https://openalex.org/A2109127218",
      "name": "Huige Li",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1964111531",
      "name": "Bin Han",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908230750",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2798385737",
    "https://openalex.org/W3166051255",
    "https://openalex.org/W2810583043",
    "https://openalex.org/W3045384077",
    "https://openalex.org/W6678830454",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2951105272",
    "https://openalex.org/W2774837955",
    "https://openalex.org/W2963606508",
    "https://openalex.org/W2807480793",
    "https://openalex.org/W2526174222",
    "https://openalex.org/W2946165673",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2979129662",
    "https://openalex.org/W4385573706",
    "https://openalex.org/W3207311065",
    "https://openalex.org/W4404622629",
    "https://openalex.org/W4402703093",
    "https://openalex.org/W4353007065",
    "https://openalex.org/W4229024390",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3099387504"
  ],
  "abstract": "Multimodal knowledge graph completion necessitates the integration of information from multiple modalities (such as images and text) into the structural representation of entities to improve link prediction. However, most existing studies have overlooked the interaction between different modalities and the symmetry in the modal fusion process. To address this issue, this paper proposed a Transformer-based knowledge graph link prediction model (MM-Transformer) that fuses multimodal features. Different modal encoders are employed to extract structural, visual, and textual features, and symmetrical hybrid key-value calculations are performed on features from different modalities based on the Transformer architecture. The similarities of textual tags to structural tags and visual tags are calculated and aggregated, respectively, and multimodal entity representations are modeled and optimized to reduce the heterogeneity of the representations. The experimental results show that compared with the current multimodal SOTA method, MKGformer, MM-Transformer improves the Hits@1 and Hits@10 evaluation indicators by 1.17% and 1.39%, respectively, proving that the proposed method can effectively solve the problem of multimodal feature fusion in the knowledge graph link prediction task.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7389780282974243
    },
    {
      "name": "Computer science",
      "score": 0.721494734287262
    },
    {
      "name": "Modalities",
      "score": 0.5800797939300537
    },
    {
      "name": "Encoder",
      "score": 0.5586905479431152
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5198307633399963
    },
    {
      "name": "Modal",
      "score": 0.4801727831363678
    },
    {
      "name": "Graph",
      "score": 0.46718481183052063
    },
    {
      "name": "Machine learning",
      "score": 0.3887830078601837
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36888760328292847
    },
    {
      "name": "Natural language processing",
      "score": 0.33230680227279663
    },
    {
      "name": "Theoretical computer science",
      "score": 0.21805685758590698
    },
    {
      "name": "Voltage",
      "score": 0.09448152780532837
    },
    {
      "name": "Engineering",
      "score": 0.08251774311065674
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}