{
  "title": "s2s-ft: Fine-Tuning Pretrained Transformer Encoders for Sequence-to-Sequence Learning",
  "url": "https://openalex.org/W3208244077",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222425276",
      "name": "Bao, Hangbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097573093",
      "name": "Dong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1496033914",
      "name": "Wang Wen-hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979826969",
      "name": "Yang Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2890166583",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2962977247",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2495998536"
  ],
  "abstract": "Pretrained bidirectional Transformers, such as BERT, have achieved significant improvements in a wide variety of language understanding tasks, while it is not straightforward to directly apply them for natural language generation. In this paper, we present a sequence-to-sequence fine-tuning toolkit s2s-ft, which adopts pretrained Transformers for conditional generation tasks. Inspired by UniLM, we implement three sequence-to-sequence fine-tuning algorithms, namely, causal fine-tuning, masked fine-tuning, and pseudo-masked fine-tuning. By leveraging the existing pretrained bidirectional Transformers, experimental results show that s2s-ft achieves strong performance on several benchmarks of abstractive summarization, and question generation. Moreover, we demonstrate that the package s2s-ft supports both monolingual and multilingual NLG tasks. The s2s-ft toolkit is available at https://github.com/microsoft/unilm/tree/master/s2s-ft.",
  "full_text": "s2s-ft: Fine-Tuning Pretrained Transformer Encoders\nfor Sequence-to-Sequence Learning\nHangbo Bao, Li Dong, Wenhui Wang, Nan Yang, Furu Wei\nMicrosoft Research\nhttps://github.com/microsoft/unilm/tree/master/s2s-ft\nAbstract\nPretrained bidirectional Transformers, such as\nBERT (Devlin et al., 2019), have achieved\nsigniÔ¨Åcant improvements in a wide variety\nof language understanding tasks, while it is\nnot straightforward to directly apply them for\nnatural language generation. In this paper,\nwe present a sequence-to-sequence Ô¨Åne-tuning\ntoolkit s2s-ft, which adopts pretrained Trans-\nformers for conditional generation tasks. In-\nspired by UniLM (Dong et al., 2019; Bao\net al., 2020), we implement three sequence-\nto-sequence Ô¨Åne-tuning algorithms, namely,\ncausal Ô¨Åne-tuning, masked Ô¨Åne-tuning, and\npseudo-masked Ô¨Åne-tuning. By leveraging the\nexisting pretrained bidirectional Transformers,\nexperimental results show that s2s-ft achieves\nstrong performance on several benchmarks of\nabstractive summarization, and question gener-\nation. Moreover, we demonstrate that the pack-\nage s2s-ft supports both monolingual and mul-\ntilingual NLG tasks. The s2s-ft toolkit is avail-\nable at https://github.com/microsoft/\nunilm/tree/master/s2s-ft.\n1 Introduction\nPretrained bidirectional Transformers (Devlin et al.,\n2019; Yang et al., 2019; Dong et al., 2019; Liu et al.,\n2019; Conneau et al., 2020; Clark et al., 2020; Bao\net al., 2020) have achieved remarkable success on\nvarious NLP tasks, such as text classiÔ¨Åcation, and\nquestion answering. The BERT-like models are\nusually pretrained by the masked language model-\ning task (Taylor, 1953; Devlin et al., 2019), which\nlearns to predict masked tokens based on given\ncontext. However, due to the bidirectionality na-\nture, it is not straightforward to directly apply the\npretrained bidirectional Transformers to language\ngeneration tasks (Wang and Cho, 2019).\nThere have been several attempts to achieve the\nabove goal. Liu and Lapata (2019) use pretrained\nBERT (Devlin et al., 2019) as an encoder, and ran-\ndomly initialize a Transformer-based decoder with\nlarger learning rate. Rothe et al. (2020) initial-\nize the encoder and decoder with different com-\nbinations of BERT, GPT (Radford et al., 2018),\nand RoBERTa (Liu et al., 2019) models. De-\nspite achieving promising results, the performance\nis still far behind the jointly pretrained encoder-\ndecoder models, such as BART (Lewis et al., 2020)\nand T5 (Raffel et al., 2019) on generation tasks.\nWe argue that the capability of pretrained bidirec-\ntional Transformers has not been fully unleashed\non sequence-to-sequence tasks.\nIn this paper, we present a toolkit (named as\ns2s-ft) used to Ô¨Åne-tune pretrained bidirectional\nTransformers on conditional language generation\ntasks, such as abstractive summarizaiton, and ques-\ntion generation. We follow uniÔ¨Åed modeling as\nin (Dong et al., 2019), which shares the same Trans-\nformer parameters for both encoding and decoding.\nSequence-to-sequence modeling is achieved by em-\nploying well-designed self-attention masks in bidi-\nrectional Transformers. In other words, the source\ntokens can attend to each other, while the target\ntokens can only attend to the left-side context.\nWe implement three Ô¨Åne-tuning algorithms in\ns2s-ft. Firstly, causal Ô¨Åne-tuning introduces a posi-\ntion shift for decoding target sequences as in causal\nlanguage modeling, so that all the decoding tokens\ncan be trained with one forward pass. Secondly,\nmasked Ô¨Åne-tuning randomly masks some target\ntokens and learns to recover them. The method\nminimizes the mismatch between pre-training and\nÔ¨Åne-tuning. Thirdly, pseudo-masked Ô¨Åne-tuning\nappends pseudo masks into the original target se-\nquence, which combines the beneÔ¨Åts of the above\ntwo methods.\nWe build the s2s-ft toolkit upon HuggingFace‚Äôs\nTransformers library (Wolf et al., 2019). We con-\nduct extensive experiments on several language\narXiv:2110.13640v1  [cs.CL]  26 Oct 2021\n123456[CLS]ùë†‡¨µ [SEP][M]ùë°‡¨∂ [M]ùë°‡¨µ [SEP]Prediction LayerStackedTransformersPosition EmbeddingToken EmbeddingTransformer Block NTransformer Block 1‚Ä¶123456ùë°‡¨µ [SEP](b) Masked Fine-TuningTransformer Block NTransformer Block 1‚Ä¶123445Transformer Block NTransformer Block 1‚Ä¶[CLS]ùë†‡¨µ [SEP][SOS]ùë°‡¨µ ùë°‡¨∂\nùë°‡¨∂\n[CLS]ùë†‡¨µ [SEP][P]ùë°‡¨µ [P]ùë°‡¨∂ [P](c) Pseudo-Masked Fine-Tuning56 ùë°‡¨µ [SEP]ùë°‡¨∂\n(a) Causal Fine-Tuning\nFigure 1: Overview of different Ô¨Åne-tuning methods. We pack the source and target sequence together to form the\ninput and use speciÔ¨Åc attention masks shown in Figure 2 to perform sequence-to-sequence Ô¨Åne-tuning. [M] and\n[P] denote the masked token, [CLS] and [SOS] the start-of-sequence tokens, and [SEP] the end-of-sequence\ntoken. For causal Ô¨Åne-tuning, each target token is fed into the model in order to predict the next token. For masked\nÔ¨Åne-tuning, we randomly mask some tokens in target sequence and train the model as masked language modeling.\nFor pseudo-masked Ô¨Åne-tuning, we insert a pseudo mask for each target token, and assign them with the same\nposition embeddings.\ngeneration benchmarks, such as XSum and CNN\n/ DailyMail for abstractive summarization, and\nSQuAD question generation. We also compare\noff-the-shelf pretrained bidirectional Transformers\n(i.e., BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), ELECTRA (Clark et al., 2020), and\nUniLM (Dong et al., 2019; Bao et al., 2020)) for\nsequence-to-sequence learning. In addition, we\nshow that s2s-ft can be easily applied to multi-\nlingual language generation tasks by using XLM-\nRoBERTa (Conneau et al., 2020) as the multi-\nlingual pretrained model. Experimental results\ndemonstrate that s2s-ft achieves strong perfor-\nmance across different tasks, and languages.\n2 Sequence-to-Sequence Fine-Tuning\nSequence-to-sequence learning aims at generate a\ntarget sequence t = t1,¬∑¬∑¬∑ ,t|t| by conditioning\non the given source sequence s= s1,¬∑¬∑¬∑ ,s|s|. In\ns2s-ft, all tokens are encoded into hidden vectors\nby Transformer (Vaswani et al., 2017). The target\ntokens are autoregressively generated via:\np(t|s) =\n|t|‚àè\ni=1\np(ti|t<i,s) (1)\nwhere t<i = t1,¬∑¬∑¬∑ ,ti‚àí1.\nFirst, the model is initialized by a pretrained\nTransformer. Then we use a sequence-to-sequence\nlearning objective to Ô¨Åne-tune the network. In-\nspired by UniLM (Dong et al., 2019), we share\nthe same model architecture and parameters for\nboth encoding and decoding, which reduces the\nmodeling discrepancies between pre-training and\nÔ¨Åne-tuning.\nFigure 1 shows an overview of three sequence-\nto-sequence Ô¨Åne-tuning algorithms implemented\nin the s2s-ft toolkit. We employ special tokens to\nindicate the boundary of sequences. For example,\n[CLS] is the Ô¨Årst source token, and [SEP] indi-\ncates the end of sequences. Sequence-to-sequence\nlearning is achieved by using well-designed self-\nattention masks (Dong et al., 2019). As shown in\nFigure 2, all the source tokens can attend to each\nother, while a target token can only attend to the\npreviously generated tokens and source sequence.\nWe encode the source sequences as conventional\nbidirectional Transformers. The main difference\nbetween three Ô¨Åne-tuning methods lies in how to\ndecode target sequences.\n2.1 Causal Fine-Tuning\nThe Ô¨Årst method learns to decode target in a similar\nway as causal language models, such as GPT (Rad-\nford et al., 2018). In the decoding part, the model\ngenerates the current token by feeding the previ-\nous prediction at each time step. As shown in\nFigure 1(a), we feed the start-of-sequence token\n[SOS] into the model and predict t1 by condition-\ning on the hidden state. Similarly, t1 is the input\nat the next time step, which is used to produce t2.\nThe target tokens are completely generated until\nthe end-of-sequence token [SEP] is emitted.\nThe Ô¨Åne-tuning objective is to maximize the\nlikelihood of generating target tokens conditioning\non the source sequence. Unlike masked language\nmodel pre-training, where only a portion of tokens\nare masked and predicted, causal Ô¨Åne-tuning can\ngather supervision signals from every target pre-\ndictions within one forward pass. However, the\n[CLS]ùë†‡¨µ\n[SEP][M]ùë°‡¨∂\n[M]1        2      3       4      5      6Position Embedding123456Allow to attendPrevent from attending[CLS]ùë†‡¨µ [SEP][M]ùë°‡¨∂ [M][CLS]ùë†‡¨µ\n[SEP][SOS]ùë°‡¨µ\nùë°‡¨∂\n1       2       3       4      5      6Position Embedding123456[CLS]ùë†‡¨µ [SEP] [SOS]ùë°‡¨µ ùë°‡¨∂\n[CLS]ùë†‡¨µ\n[SEP][P]ùë°‡¨µ\n[P]1       2      3        4      4      5       5      6  Position Embedding123445[CLS]ùë†‡¨µ [SEP][P]ùë°‡¨µ [P]56ùë°‡¨∂\n[P]ùë°‡¨∂ [P](a) Causal Fine-Tuning(b) Masked Fine-Tuning(c) Pseudo-Masked Fine-Tuning\nFigure 2: Self-attention masks for different Ô¨Åne-tuning methods. Tokens in the target sequence can attend to source\ntokens, left context in the target sequence and itself. For pseudo-masked Ô¨Åne-tuning, the mask token[P] can only\nbe attended by itself.\nmethod involves a position shift between model\ninput and prediction in the decoding part, which re-\nsults in a discrepancy compared with bidirectional\nTransformer pre-training.\n2.2 Masked Fine-Tuning\nFollowing Dong et al. (2019), we randomly mask\na certain percentage of target tokens, and learn\nto recover them. The masked Ô¨Åne-tuning algo-\nrithm is identical to masked language model pre-\ntraining, despite we use a sequence-to-sequence\nself-attention mask as shown in Figure 2(b). The\nmasked position is supposed to predict the current\ntarget token, while other tokens are given as con-\ntext. Notice that the end-of-sequence token [SEP]\ncan also be masked during Ô¨Åne-tuning in order to\nlearn when to terminate the decoding process.\nThe Ô¨Åne-tuning objective is to maximize the\nlikelihood of masked tokens given source and un-\ncorrupt target tokens. The method overcomes the\nposition-shift discrepancy between pre-training and\nÔ¨Åne-tuning described in Section 2.1.\n2.3 Pseudo-Masked Fine-Tuning\nFollowing Bao et al. (2020), we append pseudo-\nmasked tokens [P] for all the target tokens. The\npseudo mask is assigned with the same position\nembedding as the corresponding original token.\nCompared with masked Ô¨Åne-tuning, the original\ntokens are still kept in the input rather than being\nmasked.\nThe self-attention mask used for pseudo-masked\nÔ¨Åne-tuning is illustrated in Figure 2(c). All the\nsource tokens can be accessed by others. The\npseudo masks and target tokens can only attend\nto the previous given tokens and themselves. More-\nover, the original target token instead of its corre-\nsponding pseudo mask is attended by the future\ntime steps.\nAs shown in Figure 1(c), the target tokens are\npredicted at the positions of pseudo masks. The\nÔ¨Åne-tuning objective is to maximize the likelihood\nof target tokens given source sequence. Pseudo-\nmasked Ô¨Åne-tuning gets the best of the above two\nmethods. The algorithm avoids the position-shift\ndiscrepancy compared with causal Ô¨Åne-tuning (Sec-\ntion 2.1). Moreover, all the target tokens can back-\npropagate error signals, rather than only a portion\nof target sequence are masked and predicted as in\nmasked Ô¨Åne-tuning (Section 2.2).\n2.4 Decoding\nGiven input source s, the target sequence is au-\ntoregressively generated via ÀÜt= arg maxt‚Ä≤ p(t‚Ä≤|s),\nwhere p(t‚Ä≤|s) is factorized as in Equation (1). We\napproximately Ô¨Ånd the best decoding results by\ngreedy search or beam search, similar in conven-\ntional encoder-decoder methods.\nIt is worth noting that the hidden states of previ-\nous time steps can be cached without re-computing\nthem during the decoding process. So the decoding\nprocess has the same computation complexity com-\npared with conventional Transformer sequence-to-\nsequence models. Moreover, the implementation\nbecomes more uniÔ¨Åed because s2s-ft uses the same\narchitecture for both encoding and decoding. In\ncontrast, conventional Transformers need to distin-\nguish encoder and decoder (Vaswani et al., 2017),\nwhere different architecture are implemented.\nFor causal Ô¨Åne-tuning, a start-of-sequence to-\nDataset #Train/#Dev/#Test Language\nAbstractive Summarization\nCNN / DailyMail 287k/13k/11k English\nXSum 204k/11k/11k English\nGigawordfr 500k/5k/5k French\nGigawordzh 500k/5k/5k Chinese\nQuestion Generation\nSQuAD 76k/11k/12k English\nWebQAzh 136k/5k/3k Chinese\nTable 1: Summary of the evaluation benchmarks.\nken is fed into the model in order to predict the\nÔ¨Årst target token. Then we in turn append the pre-\ndiction to input and generate the next token. We\nrepeat the process until the end-of-sequence token\nis emitted. In contrast, the other two methods use\na mask [M]/[P] as input to predict the current\ntarget token. The mask will be substituted with its\nprediction in the next time step.\n3 Experiments\ns2s-ft is built upon HuggingFace‚Äôs Transformers\nlibrary (Wolf et al., 2019), so that we can load vari-\nous off-the-shelf pretrained models. We implement\nthe sequence-to-sequence Ô¨Åne-tuning algorithms\ndescribed in Section 2. We conduct experiments\non a set of language generation benchmarks, in-\ncluding abstractive summarizaiton, and question\ngeneration. The hyperparameters are chosen on the\ndevelopment set of each dataset.\n3.1 Benchmarks\nWe summarize all the evaluation benchmarks in Ta-\nble 1. The datasets cover two tasks, and two more\nlanguages. We report ROUGE (Lin, 2004) scores\nas the evaluation metrics for abstractive summa-\nrization. In addition, we include BLEU (Papineni\net al., 2002) and METEOR (Banerjee and Lavie,\n2005) metrics for question generation.\n3.1.1 Monolingual Dataset\nCNN / DailyMail(See et al., 2017) The abstractive\nsummarization dataset aims at generating a concise\nand Ô¨Çuent summary from an English news article\ncrawled from CNN and DailyMail.\nXSum (Narayan et al., 2018) The extreme summa-\nrization dataset compresses a BBC news article to\na one-sentence summary.\nSQuAD (Du and Cardie, 2018) The question gener-\nation dataset aims at generating relevant questions\ngiven a paragraph and an answer span, which is\nbased on SQuAD v1.1 (Rajpurkar et al., 2016).\nMethod XSum SQuAD\nRG-1/RG-2/RG-L BLEU-4/MTR/RG-L\nCausal 40.72/18.44/33.30 23.42/25.07/49.96\nMasked 41.12/18.52/33.51 23.53/25.19/51.00\nPseudo-Masked 41.04/18.69/33.58 23.61 /25.36/51.05\nTable 2: Results of different Ô¨Åne-tuning methods on\nthe XSum and SQuAD development sets. The models\nare initialized with the BERT-base-uncased checkpoint.\nRG is short for ROUGE, MTR for METEOR.\nPretrained Model XSum SQuAD\nRG-1/RG-2/RG-L BLEU-4/MTR/RG-L\nELECTRA 40.65/18.03/33.23 21.24/23.65/49.39\nBERT 41.04/18.69/33.58 23.61/25.36/51.05\nRoBERTa 43.30/20.47/35.53 25.32/26.61/52.62\nUniLMv2 44.45/21.67/36.78 26.30/27.09/53.19\nTable 3: Evaluation results of four pretrained bidirec-\ntional Transformers on the development sets of XSum\nand SQuAD. Pseudo-masked Ô¨Åne-tuning is used. The\nmodels are all base size. Same shorthands apply as in\nTable 2.\n3.1.2 Multilingual Dataset\nGigawordfr/zh (Chi et al., 2020) The headline gen-\neration datasets are built upon French (fr) and Chi-\nnese (zh) article-headline pairs.\nWebQAzh (Chi et al., 2020) The Chinese question\ngeneration dataset is built upon WebQA (Li et al.,\n2016).\n3.2 Comparison of Fine-Tuning Methods\nWe Ô¨Årst compare the three sequence-to-sequence\nÔ¨Åne-tuning algorithms using the BERT-base-\nuncased checkpoint1 as the pretrained model. We\nreport evaluation results on the developments sets\nof XSum and SQuAD in Table 2. The results show\nthat pseudo-masked Ô¨Åne-tuning achieves the best\nperformance on two datasets, except that masked\nÔ¨Åne-tuning obtains the highest ROUGE-1 score\non XSum. Moreover, causal Ô¨Åne-tuning is consis-\ntently worse than the other two algorithms. The\nresults indicate that reducing the discrepancy be-\ntween masked language model pre-training and\nsequence-to-sequence Ô¨Åne-tuning is beneÔ¨Åcial. We\ntherefore use pseudo-masked Ô¨Åne-tuning in the rest\nof the experiments.\n3.3 Comparison of Pretrained Models\nWe compare different pretrained models for ini-\ntialization, including BERT (Devlin et al., 2019),\n1github.com/google-research/bert\nModel #Param Corpus CNN / DailyMail XSum\nRG-1/RG-2/RG-L RG-1/RG-2/RG-L\nWithout pre-training\nPTR NET (See et al., 2017) - - 39.53/17.28/36.38 28.10/8.02/21.72\nFine-tuning base-size pretrained models\nMASS (Song et al., 2019) 123M - 42.12/19.50/39.01 39.75/17.24/31.95\nBERTS UM ABS (Liu, 2019) 156M 16GB 41.72/19.39/38.76 38.76/16.33/31.15\nERNIE-GEN (Xiao et al., 2020) 110M 16GB 42.30/19.92/39.68 -\nT5 (Raffel et al., 2019) 220M 750GB 42.05/20.34/39.40 -\ns2s-ftRoBERTa-base 125M 160GB 42.28/20.21/39.87 43.39/20.55/35.63\ns2s-ftUniLMv2-base 110M 160GB 43.89/21.05/41.02 44.37 /21.54/36.61\nFine-tuning large-size pretrained models\nUniLM (Dong et al., 2019) 340M 16GB 43.08/20.43/40.34 -\nERNIE-GEN (Xiao et al., 2020) 340M 16GB 44.02/21.17/41.26\nBART (Lewis et al., 2020) 400M 160GB 44.16/21.28/40.90 45.14/22.27/37.25\nProphetNet (Yan et al., 2020) 400M 160GB 44.20/21.17/41.30 -\nPEGASUS C4 (Zhang et al., 2020) 568M 750GB 43.90/21.20/40.76 45.20/22.06/36.99\nPEGASUS HUGE NEWS (Zhang et al., 2020) 568M 3800GB 44.17/21.47/41.11 47.21/ 24.56/39.25\nT511B (Raffel et al., 2019) 11B 750GB 43.52/21.55/40.69 -\ns2s-ftRoBERTa-large 355M 160GB 43.92/21.25/41.06 45.63/22.72/37.86\ns2s-ftUniLMv2-large 340M 160GB 44.79/21.98/41.93 47.58 /24.35/ 39.50\nTable 4: Abstractive summarization results on the test set of CNN / DailyMail, and XSum. The evaluation metric\nis the F1 version of ROUGE (RG) scores. We also present the number of parameters (#Param) for the methods\nusing pretrained models.\nELECTRA (Clark et al., 2020), RoBERTa (Liu\net al., 2019) and UniLMv2 (Bao et al., 2020). The\nbase-size checkpoints are used in the comparison.\nAs shown in Table 3, we report the results of\npseudo-masked Ô¨Åne-tuning (Section 2.3) on XSum\nand SQuAD.\nAmong the four pretrained models, UniLMv2\nperforms best in terms of the automatic evaluation\nmetrics, which contains a partially autoregressive\npre-training objective that is similar to sequence-\nto-sequence modeling. The models initialized by\nBERT and RoBERTa obtain better results com-\npared with ELECTRA. The results indicate that\nmasked language model pre-training over the full\nvocabulary are helpful for sequence-to-sequence\ntasks. Although ELECTRA obtains comparable\nperformance on a wide range of language under-\nstanding tasks (e.g., text classiÔ¨Åcation, and question\nanswering), the language modeling ability is not\nfully pretrained (Clark et al., 2020).\n3.4 Comparisons with Previous Work\nWe conduct evaluation by usings2s-ft to Ô¨Åne-tune\nRoBERTa (Liu et al., 2019) and UniLMv2 (Bao\net al., 2020) on abstractive summarization (i.e.,\nCNN / DailyMail, and XSum) and question gener-\nation (i.e., SQuAD). Pseudo-masked Ô¨Åne-tuning is\nused both base-size and large-size models.\nAs shown in Table 4 and Table 5,s2s-ftUniLMv2\nachieves state-of-the-art performance on all three\nbenchmarks compared with the models that use\nmore parameters, larger corpus, or task-speciÔ¨Åc pre-\ntraining. SpeciÔ¨Åcally, T5 11B (Raffel et al., 2019)\nuses 11 billion parameters and 750GB text corpus\nto pretrain a sequence-to-sequence model. PEGA-\nSUS (Zhang et al., 2020) is a task-speciÔ¨Åc pre-\ntrained model designed for abstractive summariza-\ntion. The comparisons indicate that s2s-ft can ob-\ntain strong performance on sequence-to-sequence\ntasks by leveraging the pretrained models.\nIt is notable that RoBERTa obtains very compet-\nitive performance compared with previous work.\nThe comparisons show that the masked language\nmodeling pre-training (Devlin et al., 2019) is help-\nful for language generation tasks. Moreover, s2s-ft\nprovides a uniÔ¨Åed modeling method to employ the\nexisting pretrained Transformers for sequence-to-\nsequence tasks.\n3.5 Results of Multilingual Generation\nApart from monolingual generation tasks, we can\nuse s2s-ft to leverage the multilingual pretrained\nmodels, such as mBERT (Devlin et al., 2019), and\nXLM-RoBERTa (Conneau et al., 2020). We con-\nduct language generation experiments on both ab-\nstractive summarization (French Gigawordfr, and\nChinese Gigawordzh) and Chinese question genera-\ntion (WebQAzh).\nModel #Param Corpus OfÔ¨Åcial Split Reversed Split\nBLEU-4/MTR/RG-L BLEU-4/MTR/RG-L\nWithout pre-training\n(Du and Cardie, 2018) - - 15.16/19.12/ - -\n(Zhao et al., 2018) - - - 16.38/20.25/44.48\n(Zhang and Bansal, 2019) - - 18.37/22.65/46.68 20.76/24.20/48.91\nFine-tuning base-size pretrained models\nERNIE-GEN (Xiao et al., 2020) 110M 16GB 22.28/25.13/50.58 23.52/25.61/51.45\ns2s-ft RoBERTa-BASE 125M 160GB 23.86/25.93/51.68 25.32/26.61/52.62\ns2s-ft UniLMv2-BASE 110M 160GB 24.70 /26.33 /52.13 26.30 /27.09 /53.19\nFine-tuning large-size pretrained models\nUniLM (Dong et al., 2019) 340M 16GB 22.12/25.06/51.07 23.75/25.61/52.04\nERNIE-GEN (Xiao et al., 2020) 340M 16GB 24.03/26.31/52.36 25.57/26.89/53.31\nProphetNet (Yan et al., 2020) 400M 16GB 25.01/26.83/52.57 26.72/27.64/53.79\ns2s-ft RoBERTa-LARGE 400M 160GB 25.30/26.85/52.66 26.82/27.48/53.92\ns2s-ft UniLMv2-LARGE 340M 160GB 25.97 /27.33 /53.43 27.12 /27.95 /54.25\nTable 5: Question generation results on the test set of SQuAD. MTR is short for METEOR, and RG for ROUGE.\nThe ofÔ¨Åcial split is from (Du and Cardie, 2018), while the reversed split is the same as in (Zhao et al., 2018).\nWebQAzh Gigawordzh Gigawordfr\n(Chinese) (Chinese) (French)\nBLEU-4/MTR/RG-L RG-1/RG-2/RG-L RG-1/RG-2/RG-L\nXLM (Chi et al., 2020) 23.41/23.32/47.40 55.30/42.57/52.95 56.27/39.20/52.84\nXNLG (Chi et al., 2020) 24.89/24.53/49.72 57.65/44.93/54.95 57.84/40.81/54.24\ns2s-ftXLM-RoBERTa-BASE 27.45/25.20/49.76 60.29/47.24/57.46 57.95/41.30/54.54\ns2s-ftXLM-RoBERTa-LARGE 28.49/26.48/52.94 60.95 /47.94/58.09 58.48 /41.79/55.04\nTable 6: Evaluation results of Chinese and French abstractive summarization, and Chinese question generation.\nQG is short for question generation, AS for abstractive summarization, BL for BLEU, MTR for METEOR, and\nRG for ROUGE.\nAs shown in Table 6, we employ s2s-ft to Ô¨Åne-\ntune XLM-RoBERTa on the three benchmarks. We\ncompare our results with Ô¨Åne-tuning XNLG (Chi\net al., 2020) and XLM (Conneau and Lample,\n2019) that are pretrained conventional sequence-\nto-sequence Transformers. s2s-ft achieves sig-\nniÔ¨Åcantly better performance than previous work\nacross different languages and tasks. The results\nindicate that s2s-ft can unleash the multilingual-\nity of XLM-RoBERTa on generation tasks. More\nimportantly, the support of multilingual pretrained\nmodels greatly widens the application range of our\ns2s-ft toolkit.\n4 Conclusion\nWe introduce a sequence-to-sequence toolkit s2s-\nft to Ô¨Åne-tune the pretrained bidirectional Trans-\nformers for language generation tasks. The toolkit\nfollows the UniLM (Dong et al., 2019; Bao et al.,\n2020) Ô¨Åne-tuning algorithms, which uniÔ¨Åes encod-\ning and decoding with the same modeling method.\nWe conduct extensive experiments on abstractive\nsummarization and question generation, including\nboth monolingual and multilingual settings. We\nplug in different pretrained models in our toolkit\nand evaluate three Ô¨Åne-tuning approaches. Then\nwe compare s2s-ft with previous work using both\nbase-size and large-size models. In addition, we\nuse s2s-ft to apply off-the-shelf multilingual pre-\ntrained model on Chinese and French sequence-\nto-sequence learning. Experimental results show\nthat the proposed toolkit achieves strong perfor-\nmance across the tasks and languages. We believe\nthe toolkit is important to unleash the abilities of\nBERT-like bidirectional Transformers on sequence-\nto-sequence tasks.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65‚Äì72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang,\nNan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao,\nSonghao Piao, Ming Zhou, and Hsiao-Wuen Hon.\n2020. Unilmv2: Pseudo-masked language models\nfor uniÔ¨Åed language model pre-training. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 642‚Äì652. PMLR.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence, AAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7570‚Äì7577. AAAI Press.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm¬¥an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 8440‚Äì8451. Associa-\ntion for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057‚Äì7067. Curran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171‚Äì4186. Association for Computa-\ntional Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, 8-14 December 2019, Vancouver, BC,\nCanada, pages 13042‚Äì13054.\nXinya Du and Claire Cardie. 2018. Harvest-\ning paragraph-level question-answer pairs from\nwikipedia. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 1: Long Papers, pages 1907‚Äì1917. Associa-\ntion for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871‚Äì7880. Association for Computational\nLinguistics.\nPeng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying\nCao, Jie Zhou, and Wei Xu. 2016. Dataset and\nneural recurrent sequence labeling model for open-\ndomain factoid question answering. arXiv preprint\narXiv:1607.06275.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summa-\nrization Branches Out: Proceedings of the ACL-04\nWorkshop, pages 74‚Äì81, Barcelona, Spain. Associa-\ntion for Computational Linguistics.\nYang Liu. 2019. Fine-tune BERT for extractive sum-\nmarization. CoRR, abs/1903.10318.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3728‚Äì3738, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don‚Äôt give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018 , pages 1797‚Äì1807. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311‚Äì318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. arXiv e-prints.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383‚Äì2392, Austin,\nTexas. Association for Computational Linguistics.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. Trans. Assoc. Comput. Lin-\nguistics, 8:264‚Äì280.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073‚Äì\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA , volume 97 of Pro-\nceedings of Machine Learning Research , pages\n5926‚Äì5936. PMLR.\nWilson L Taylor. 1953. Cloze procedure: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415‚Äì433.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998‚Äì6008. Curran Asso-\nciates, Inc.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a markov ran-\ndom Ô¨Åeld language model. CoRR, abs/1902.04094.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R‚Äôemi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace‚Äôs Trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nDongling Xiao, Han Zhang, Yu-Kun Li, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. ERNIE-\nGEN: An enhanced multi-Ô¨Çow pre-training and Ô¨Åne-\ntuning framework for natural language generation.\nCoRR, abs/2001.11314.\nYu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu,\nNan Duan, Jiusheng Chen, Ruofei Zhang, and Ming\nZhou. 2020. ProphetNet: Predicting future n-\ngram for sequence-to-sequence pre-training. CoRR,\nabs/2001.04063.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754‚Äì5764.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter J. Liu. 2020. PEGASUS: pre-training with\nextracted gap-sentences for abstractive summariza-\ntion. In Proceedings of the 37th International Con-\nference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 ofProceedings\nof Machine Learning Research, pages 11328‚Äì11339.\nPMLR.\nShiyue Zhang and Mohit Bansal. 2019. Address-\ning semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2495‚Äì2509. Association for\nComputational Linguistics.\nYao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa\nKe. 2018. Paragraph-level neural question gener-\nation with maxout pointer and gated self-attention\nnetworks. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3901‚Äì3910, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nA Hyperparameters for Fine-Tuning\nTable 7 reports the most hyperparameters used in\nthis paper.\nBatch size 64\nLabel smoothing 0.1\nAdam œµ 1e-6\nAdam Œ≤ (0.9, 0.999)\nLearning rate schedule Linear\nWarmup steps 1000\nGradient clipping 1.0\nDropout 0.1\nWeight decay 0.01\nTable 7: Hyperparameters for Ô¨Åne-tuning.\nThe optimal hyperparameter values are task-\nspeciÔ¨Åc and we provide a range of possible values\nthat work well for various downstream tasks:\n‚Ä¢ Learning rate for base-sized models: 5e-5,\n7e-5, 1e-4\n‚Ä¢ Learning rate for large-sized models: 1e-5,\n1.5e-5, 2e-5, 3e-5\n‚Ä¢ Number of Ô¨Åne-tuning epochs : 10, 15, 20,\n30\n‚Ä¢ Mask prob for target sequence: 40%, 50%,\n60%, 70%\nMask prob for target sequence denotes the proba-\nbility that each token in target sequence is masked.\nWe conduct grid search on the development sets to\nÔ¨Ånd the best hyperparameters and use for the test\nsets. The other task-speciÔ¨Åc hyperparameters are\nlisted in Table 8.\nTask Max input\ntokens\nMax output\ntokens\nBeam\nsize\nLength\npenalty\nMin output\ntokens\nCNN / DailyMail 608 160 5 0.9 48\nXSum 720 48 8 0.7 1\nSQuAD QG 384 32 8 1.3 5\nWebQAzhQG 384 32 8 1.3 5\nGigawordfr 96 48 5 0.9 1\nTable 8: Task-speciÔ¨Åc hyperparameters for evaluation\nbenchmarks.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8438345193862915
    },
    {
      "name": "Transformer",
      "score": 0.8264683485031128
    },
    {
      "name": "Computer science",
      "score": 0.8026536107063293
    },
    {
      "name": "Encoder",
      "score": 0.7378584146499634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5027849674224854
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5013904571533203
    },
    {
      "name": "Fine-tuning",
      "score": 0.45811596512794495
    },
    {
      "name": "Natural language processing",
      "score": 0.34579187631607056
    },
    {
      "name": "Speech recognition",
      "score": 0.3424169421195984
    },
    {
      "name": "Voltage",
      "score": 0.14090928435325623
    },
    {
      "name": "Engineering",
      "score": 0.11040779948234558
    },
    {
      "name": "Electrical engineering",
      "score": 0.08771425485610962
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}