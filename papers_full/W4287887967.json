{
  "title": "ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence",
  "url": "https://openalex.org/W4287887967",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100760100",
      "name": "Yibo Hu",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A4287892298",
      "name": "MohammadSaleh Hosseini",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A2971082336",
      "name": "Erick Skorupa Parolin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2308051806",
      "name": "Javier Osorio",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2155983610",
      "name": "Latifur Khan",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A2215179617",
      "name": "Patrick Brandt",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A4207522094",
      "name": "Vito D’Orazio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1493526108",
    "https://openalex.org/W2106483804",
    "https://openalex.org/W3088577908",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2523074985",
    "https://openalex.org/W4398265979",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2129104434",
    "https://openalex.org/W2989845849",
    "https://openalex.org/W3208862453",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3088752502",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2561022382",
    "https://openalex.org/W2567097689",
    "https://openalex.org/W4220980558",
    "https://openalex.org/W3107480012",
    "https://openalex.org/W2137742540",
    "https://openalex.org/W2302501749",
    "https://openalex.org/W3194985328",
    "https://openalex.org/W128570927",
    "https://openalex.org/W4297819062",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W3169290870",
    "https://openalex.org/W2563621800",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3088357828",
    "https://openalex.org/W2734990196",
    "https://openalex.org/W4242588950",
    "https://openalex.org/W3023904525",
    "https://openalex.org/W4242901782",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4205821650",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3113568521",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W3174223793",
    "https://openalex.org/W3034824379",
    "https://openalex.org/W2515855575",
    "https://openalex.org/W3088270371",
    "https://openalex.org/W3153169456",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2741212913",
    "https://openalex.org/W2931732047",
    "https://openalex.org/W1853541408",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3024298906",
    "https://openalex.org/W3174688068",
    "https://openalex.org/W3084565080",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2140948436",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2396881363"
  ],
  "abstract": "Yibo Hu, MohammadSaleh Hosseini, Erick Skorupa Parolin, Javier Osorio, Latifur Khan, Patrick Brandt, Vito D'Orazio. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5469 - 5482\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nConfliBERT: A Pre-trained Language Model for Political Conflict and\nViolence\nYibo Hu†, MohammadSaleh Hosseini†, Erick Skorupa Parolin†,\nJavier Osorio§, Latifur Khan†, Patrick T. Brandt†, Vito J. D’Orazio†\n†The University of Texas at Dallas,§The University of Arizona\n{yibo.hu,seyyedmohammadsaleh.hosseini,erick.skorupaparolin,\nlkhan,pbrandt,dorazio}@utdallas.edu,josorio1@email.arizona.edu\nAbstract\nAnalyzing conflicts and political violence\naround the world is a persistent challenge in the\npolitical science and policy communities due\nin large part to the vast volumes of specialized\ntext needed to monitor conflict and violence\non a global scale. To help advance research\nin political science, we introduce ConfliBERT,\na domain-specific pre-trained language model\nfor conflict and political violence. We first\ngather a large domain-specific text corpus for\nlanguage modeling from various sources. We\nthen build ConfliBERT using two approaches:\npre-training from scratch and continual pre-\ntraining. To evaluate ConfliBERT, we collect\n12 datasets and implement 18 tasks to assess\nthe models’ practical application in conflict re-\nsearch. Finally, we evaluate several versions\nof ConfliBERT in multiple experiments. Re-\nsults consistently show that ConfliBERT outper-\nforms BERT when analyzing political violence\nand conflict. Our code is publicly available.1\n1 Introduction\nThe study of political violence is a central concern\nof conflict scholars and security analysts in the\nacademic and policy communities. For decades,\nscholars and governments have devoted incalcu-\nlable resources to monitoring, understanding, and\npredicting the dynamics of social unrest, political\nviolence, and armed conflict worldwide. Conflict\nresearch is a sub-field of political science that ana-\nlyzes a broad scope of interactions between govern-\nment agents, their challengers, and the civilian pop-\nulation, including material and verbal conflict and\ncooperation. Conflict research covers protest, riots,\nrepression, insurgency, civil war, terrorism, human\nrights, genocide, criminal violence, forced displace-\nment, conventional and unconventional warfare, nu-\nclear deterrence, peacekeeping, diplomatic disputes\nand cooperation, among others.\n1https://github.com/eventdata/\nConfliBERT\nTraditionally, researchers used manual coding to\ntrack conflict processes worldwide (Raleigh et al.,\n2010). Unfortunately, the high costs and slow pace\nof domain experts conducting these tasks make\nit extraordinarily difficult and costly to monitor\nhighly complex and rapidly changing conflicts in\nan ever-growing volume of information available\non a global scale. Furthermore, these efforts tend\nto focus on quantifying particular types of conflict\nevents between specific kinds of actors (Sundberg\nand Melander, 2013).\nInitial efforts to address these challenges mo-\ntivated political scientists to develop automated\nsystems to classify or extract structured event data\nfrom news articles (Bond et al., 2003; Boschee\net al., 2016; O’Brien, 2010; Osorio and Reyes,\n2017; Schrodt, 2006, 2009; Alliance, 2015; Norris\net al., 2017; Lu and Roy, 2017; Ward et al., 2013).\nThese systems capture a broader range of event\ntypes, including different conflict and cooperation\nevents, between a larger number of political ac-\ntors. They can also extract volumes of data that are\norders of magnitude greater than manual coding ef-\nforts. Automated event data such as the Integrated\nCrisis Early Warning System have been used for\nconflict forecasting and other kinds of research in\npolitical science (Bagozzi et al., 2021; Beger et al.,\n2016; Brandt et al., 2022).\nHowever, these existing systems rely on dated\npattern matching techniques and large dictionar-\nies, which often yield low-accuracy results and are\ntoo costly to maintain. Recent efforts by politi-\ncal scientists employ traditional machine learning\n(Hanna, 2017; Osorio et al., 2020) and deep learn-\ning (Beieler, 2016; Radford, 2020b; Glavaš et al.,\n2017; Skorupa Parolin et al., 2020) to analyze po-\nlitical conflict and violence. Standard supervised\nlearning requires labeled data, which are expensive\nto obtain due to the expertise required for quality\nannotation. This led conflict scholars to seek alter-\nnative solutions based on the latest developments\n5469\nin natural language processing (NLP).\nRecent progress in NLP has been driven by\npre-trained transformer language models (Vaswani\net al., 2017; Radford et al., 2019; Devlin et al.,\n2018; Yang et al., 2019). Self-supervision using\nlarge-scale unlabeled text can significantly alleviate\nthe annotation bottleneck using transfer learning.\nThe training parallelization of transformers also\nimproves their efficiency on large datasets. As a\nresult, the use of powerful computational devices\nand the advantage of transformer structures make\nlarge-scale language models’ pre-training possi-\nble. Furthermore, the introduction of extensive\nbenchmarks (Wang et al., 2018, 2019; Rajpurkar\net al., 2018; Lai et al., 2017) validates the signifi-\ncant improvement of pre-trained language models\non various downstream tasks.\nWhile many language models are built on gen-\neral domain corpora, such as Wikipedia, Book-\nCorpus (Zhu et al., 2015), and WebText (Radford\net al., 2019), recent works show that pre-training\non domain-specific corpora can boost downstream\nperformance on those domains (Lee et al., 2019;\nGururangan et al., 2020). Domain-specific work\nin bio-medicine focuses not only on developing\npre-trained models (Lee et al., 2019; Beltagy et al.,\n2019; Alsentzer et al., 2019; Lewis et al., 2020;\nGu et al., 2021) but also on proposing domain-\nrelevant evaluation benchmarks (Peng et al., 2019;\nGu et al., 2021). Pre-training models also have\nadvanced research in other domains such as aca-\ndemic papers (Beltagy et al., 2019) and legal stud-\nies (Chalkidis et al., 2020). Despite some efforts\nto apply transformers-based approaches in political\nscience (Büyüköz et al., 2020; Olsson et al., 2020;\nÖrs et al., 2020; Radford, 2020a; Halterman and\nRadford, 2021; Hürriyeto˘glu et al., 2021; Parolin\net al., 2021a, 2022), we are unaware of any stud-\nies that develop and evaluate domain-specific pre-\ntrained language models for political science or\nconflict research.\nBy combining the expertise of conflict schol-\nars and computer scientists, we developed Con-\nfliBERT, a pre-trained language model designed\nfor conflict and political violence. ConfliBERT\nimproves downstream tasks for conflict research\nwhile significantly alleviating the annotation bot-\ntleneck. We expect it to support a broad commu-\nnity of academic and policy researchers, enabling\nthe analysis of conflict processes using a domain-\nspecific NLP tool that yields accurate and valid\nresults at minimum operational cost. Our paper\nprovides the following key contributions: (1) We\ncurate a large domain-specific corpus for language\nmodeling in the domain of political violence, con-\nflict, cooperation, and diplomacy. (2) Based on our\ndomain-specific corpora, we devise a pre-trained\nlanguage model, ConfliBERT, and make it avail-\nable to the general public, which directly benefits\nthe political science and policy communities. (3)\nTo evaluate our model in practical applications, we\ncollect 12 datasets and conduct 18 tasks relevant to\nconflict research. We are the first to carry out such\na comprehensive evaluation of language models for\nconflict studies. (4) We evaluate different versions\nof ConfliBERT and show it outperforms models\ntrained on generic domains. We also perform in-\ndepth analyses of different tasks to investigate the\nfactors affecting the performance.\n2 Preliminaries\nRecent pre-trained transformer language models,\nsuch as the Bidirectional Encoder Representation\nfrom Transformers (BERT) (Devlin et al., 2018),\nfollow a two-steps framework: (1) pre-train on a\nlarge unlabeled corpus; and (2) fine-tune on task-\nspecific labeled data. These models learn seman-\ntics during the pre-training step and require smaller\nlabeled data to significantly improve their perfor-\nmance on downstream tasks. The fine-tuning step\nrequires minor network modifications to create\nstate-of-the-art models for various tasks.\nTechnically, BERT uses the multi-layer, multi-\nhead self-attention mechanism, which provides sub-\nstantial advantages for language modeling, such as\nallowing parallel GPU computation and capturing\nlong-range dependencies. This allows to efficiently\npre-train large language models on large corpora\nusing powerful devices.\nAnother key element of BERT-like models is the\ndesign of self-supervision tasks. Self-supervision\nrefers to generating labels for unlabelled data and\nusing them to train a model in a supervised man-\nner. BERT uses two self-supervision tasks during\npre-training. On the one hand, masked language\nmodel (MLM) is a fill-in-the-blank task based on\nrandomly masking a token and then using the sur-\nrounding words to predict the word hidden behind\nthe mask. On the other hand, next sentence pre-\ndiction (NSP) determines whether one sentence\nfollows another one in the same document.\nRecent works also propose variants of self-\n5470\nsupervision tasks. For example, Joshi et al. (2020)\nmask out contiguous sequences of tokens to im-\nprove span representations. Clark et al. (2020) use\nreplaced token detection, where the model distin-\nguishes real input tokens from plausible but syn-\nthetically generated replacements. However, Liu\net al. (2019) prove that MLM is competitive with\nother recently proposed training objectives with\nmore data and improved training strategies.\nFinally, most BERT-like models focus on a\ngeneric domain, such as Wikipedia, BookCor-\npus (Zhu et al., 2015) and WebText (Radford et al.,\n2019). However, BERT without domain adaptation\ntends to underperform in target domains with dis-\ntinct characteristics such as specialized vocabulary,\nlanguage style, and specific semantics. Our domain\nincludes political violence, armed conflict, interna-\ntional cooperation, and diplomacy—all of which\nhave these characteristics. This performance gap\nis the primary motivation for developing domain-\nspecific language models.\nSpecifically, the language of political actors in-\nvolves strategic and complex semantics. Policy\npositions that show support for one actor while\nalso threatening another are sometimes embedded\nin simple statements. For example, “NATO will not\ntolerate this aggression” mixes a negation, a condi-\ntional, and the action of potential interest. Signals\nare highly context-dependent, adapted for a target\naudience, and vary in strength depending on the\nspecific actor sending the signal (McManus, 2017;\nBlankenship, 2020). Compared to a generic lan-\nguage model, we expect ours to incorporate impor-\ntant contextual information and learn more about\nthe strategic ways that political actors convey in-\nformation. This context and the related political\nbiases in it are exactly a need that ConfliBERT\naims to fulfill. Political conflict and violence text\ngains from this domain knowledge: one political\nactor’s definition of “rebels” is another’s “freedom\nfighters”.\nTable 1 summarizes various recent domain-\nspecific BERT models, including our model, Con-\nfliBERT. These models mainly differ in their cor-\npora and pre-training strategies, including: (1) con-\ntinuing pre-training ( Cont); and (2) pre-training\nfrom scratch (SCR). In the next section, we elabo-\nrate on the strategies and our method of developing\nConfliBERT in the domain of political conflict and\nviolence.\nModel Method Corpora and Text Size\nBERT - Wiki+Books, 3.3B words/16 GB\nBioBERT Cont PubMed, 4.5B words\nSciBERT SCR BIO+CS papers , 3.2B words\nBlueBERT Cont PubMed+MIMIC, 4.5B words\nPubBERT SCR PubMed, 3.1B words/21 GB\nLegalBERT Both legislation, court cases, 12 GB\nConfliBERT Both organization/government reports,\nnews, 7B words/34 GB\nTable 1: Summary of selected BERT models in general\nand specific domains.\n3 Approach\nAs described in Section 2, MLM-based BERT\nachieves competitive performance among other\ntransformer models with different self-supervision\ntasks. Besides, BERT has been validated in vari-\nous domains (Lee et al., 2019; Beltagy et al., 2019;\nPeng et al., 2019; Chalkidis et al., 2020; Gu et al.,\n2021) shown in Table 1. Therefore, we develop\nour domain-specific model based on BERT. The\nkey components of developing and validating our\nmodel, ConfliBERT, include pre-training strategies,\ncorpora, and evaluation tasks.\n3.1 Domain-specific Pretraining\nWe explore both strategies (SCR and Cont) of\nadapting BERT to the political conflict and violence\ndomain. A Cont model initializes with BERT’s\ncheckpoint and vocabulary, and trains for addi-\ntional steps on a domain-specific corpus. Since\nBERT has already been pre-trained about one mil-\nlion steps on the generic domain, Cont usually re-\nquires fewer steps than training a new model from\nscratch. For example, Lee et al. (2019) report that\ncontinual pre-training of BERT on a biomedical\ndataset for 470K steps yields comparable perfor-\nmance to pre-training for one million steps.\nOn the other hand, when pre-training BERT from\nscratch (SCR) on the domain-specific corpora, we\ngenerate a new vocabulary from the target domain\ninstead of using the original BERT’s vocabulary.\nVarious papers (Beltagy et al., 2019; Gu et al.,\n2021) argue that SCR generates substantial gains\nover Cont for domains with sizeable unlabeled text.\nWe refer to the original BERT vocabulary as\nBaseV ocab and our domain vocabulary as Con-\nfliV ocab. We generated both cased and uncased\nversions of ConfliV ocab on our training corpus\nusing the Wordpiece algorithm (Wu et al., 2016).\nWe set the ConfliV ocab size to 30,000 words to\n5471\nWords BERT ConfliBERT\nDaesh Da-esh Daesh\nextremists ex-tre-mist-s extremists\nFARC FA-RC FARC\nIED I-ED IED\nindiscriminately in-dis-c-rim-inate-ly indiscriminately\nmanhunt man-hun-t manhunt\nmutilation m-uti-lation mutilation\nparamilitaries para-mi-lit-aries paramilitaries\nperpetrator per-pet-rator perpetrator\npunitive pu-ni-tive punitive\nracketeering rack-ete-ering racketeering\nseparatists se-par-ati-sts separatists\nsubversive sub-vers-ive subversive\nundemocratic und-em-oc-ratic undemocratic\nxenophobic x-eno-phobic xenophobic\nTable 2: Examples of common terms in conflict domain.\nmatch that of BaseV ocab. The resulting token over-\nlap between BaseV ocab and ConfliV ocab is 58.3%,\nwhich indicates a considerable difference (41.7%)\nin high-frequency words between the general and\nconflict-specific corpora.\nIn particular, we find a substantial advantage of\nusing ConfliV ocab during the tokenization. Table 2\nshows examples of conflict-related terms that ex-\nclusively appear in ConfliV ocab. For example, the\nterm \"separatists\" is not included in BaseV ocab,\nand BERT erroneously splits it into four sub-words\n[\"se\", \"##par\", \"##ati\", \"##sts\"]. This fragmenta-\ntion may hinder learning in downstream tasks. We\nwill validate the advantage of ConfliV ocab in the\ndownstream tasks in our experiments section.\n3.2 Corpora\nThe first step to develop ConfliBERT is to build a\ndomain-specific corpus for pre-training. As illus-\ntrated in Table 1, there exist large-scale publicly\navailable biomedical datasets, such as PubMed and\nMIMIC (Johnson et al., 2016). SciBERT (Beltagy\net al., 2019) is built from a large corpus of academic\npapers (Ammar et al., 2018; Lo et al., 2020). His-\ntory Lab2 provides many government documents,\nbut lacks the breadth we need for the politics and\nconflict domain (Connelly et al., 2021). Thus we\ncurated a domain corpus that consists of 33.7 GB\nof clean, plain text in the BERT required format.\nWe bin the sources into five categories below and\nprovide more details in Appendix.\nExpert Domain Corpora (EDC). We curated\n2,293 MB of plain text from multiple profes-\nsional sources relevant to conflict and diplomacy.\n2http://history-lab.org\nThe sources include United Nations’ websites\nand databases, international humanitarian non-\ngovernmental organizations, think tanks, and gov-\nernment sources such as the Foreign Relations of\nthe United States. These are examples of objective\nrecords of government and diplomatic activity from\nnon-partisan observers.\nMainstream Media Collection (MMC). We\ncrawled 35 worldwide news agencies reporting in\nEnglish and with coverage from 1966 to 2021. We\npre-processed and filtered 20 GB of stories using\nmetadata such as document tags for War and Poli-\ntics. These cover a period during and after the Cold\nWar with global coverage that focuses on primarily\nstate-based conflict.\nGigaword. This corpus provides a distinct cover-\nage of seven international English newswires from\n1994 to 2010 (Parker et al., 2011). We removed the\noverlapping stories (which also existed in MMC)\nand filtered an 8,818 MB domain-specific subset.\nPhoenix Real-Time (PRT). PRT is a developing\nevent dataset crawled from more than 400 news\nagencies worldwide from October 2017 (Salam\net al., 2018). It contains many news agencies in ar-\neas other than Europe and the U.S., thus improving\nthe scope of our coverage. We removed the dupli-\ncated news agencies (which also existed in MMC\nand Gigaword) and filtered a 2,425 MB relevant\nsubset. This allows the capture of post-Cold War\nactors, the Global War on Terrorism Service Medal\n(GWOT), and more recent events.\nWikipedia. Wikipedia has a different language\nstyle for describing political events and can enrich\nthe diversity of our corpus. Based on its category\nlabels, we curated 2,845 MB of relevant articles\nfrom an 18 GB size of the Wikipedia dump released\non March 20, 2021.\n3.3 Evaluation Tasks\nThe introduction of comprehensive benchmarks ac-\ncelerated the development of pre-trained language\nmodels in the general NLP domain (Wang et al.,\n2018, 2019; Rajpurkar et al., 2018; Lai et al., 2017)\nand biomedical applications (Peng et al., 2019; Gu\net al., 2021). However, few comprehensive bench-\nmarks exist for evaluating language models in the\npolitical conflict and violence domain. The focus\nof political science professionals is different from\nthat of general NLP researchers. For example, they\n5472\nare more interested in classifying, tracking, and\npredicting conflict events from the text.\nTo conduct a comprehensive evaluation of Con-\nfliBERT, we collected a broad range of NLP tasks\nrelated to political conflict and violence from\nboth publicly available and our newly-annotated\ndatasets. Table 3 shows the datasets and their corre-\nsponding tasks. Some datasets may contain subsets\nand are related to various tasks. The table also lists\nthe number of examples in the training, develop-\nment, and test datasets as well as the evaluation\nmetrics used for each task. In particular, we use\nF1 scores as performance metrics for binary clas-\nsification tasks. We use example-based F1 metrics\nfor multi-label classification tasks (Sorower, 2010).\nFor all the other tasks, we rely on Macro F1 to\nassess the model’s performance. Next, we describe\nthe datasets and their tasks.\nBinary classification (BC). We collected BBC\nNews (Greene and Cunningham, 2006) and\n20 Newsgroups (Lang, 1995) for identifying\npolitical news, a subset from Gun Violence\nDatabase (Pavlick et al., 2016) for finding arti-\ncles related to gun violence. We also used the\nsamples from Global Contention Politics Dataset\n(GLOCON) (Hürriyeto˘glu et al., 2019) to conduct\none sentence-level and one document-level classifi-\ncation task to predict whether the story is related to\nprotests. These BC tasks are essential for political\nscientists as a first step to classify and filter docu-\nments containing political and conflict events from\nlarge-scale news wires.\nMulti-class classification (MCC). GTD refers\nto Global Terrorism Database which collects ter-\nrorist incidents from 1970 onward (START, 2019).\nWe sampled a subset with description text longer\nthan 40 words and single labels to classify 9 types\nof attacks such as bombing/explosion, armed as-\nsault, and hostage-taking.\nIndia Police Events (Halterman et al., 2021)\nconsists of sentences from English-language Times\nof India articles about police activity events in Gu-\njarat during March 2002 (a relevant period due to\nwidespread Hindu-Muslim violence). The labels\nare available for both document and sentence levels\nand consider five categories of police activity: kill,\narrest, fail to act, force, and any action.\nEvent Status includes English news articles\nabout civil unrest events annotated with temporal\ntags (Huang et al., 2016). Following the original\nsetting, we conduct a temporal status classification\n(TS MCC) to detect the primary temporal distinc-\ntions among past, ongoing, and future. Besides,\nwe also build a BC task of predicting if the story\ncontains civil unrest events.\nMulti-label classification (MLC). SATP stands\nfor South Asia Terrorism Portal3 from which we\nmanually annotated a sample of 7,445 narratives\nbetween 2011 and 2019. We focus on incidents ini-\ntiated by terrorist organizations. 23.6% of the sam-\nple are relevant stories classified into one or more\ncategories: armed assault, bombing/explosion, kid-\nnapping, and others. The rest samples are irrelevant\n(stories not about terrorism attacks such as arrests\nor armed clashes). Based upon this, we built three\ntasks. The first is a BC task to find relevant sto-\nries. The second is an MLC task to predict attack\ntypes on the relevant subset (Rel MLC). The third\nis the same as the second but conducted on the\nmore imbalanced full dataset (All MLC).\nInSight Crime (Parolin et al., 2021b) contains\nannotated stories about organized criminal activity\nin Latin America and the Caribbean from InSight\nCrime.4 We applied an MLC task to predict multi-\nple crime categories expressed in the stories, such\nas drug trafficking, corruption, and law enforce-\nment.\nSequence Labeling or Named Entity Recogni-\ntion (NER). MUC-4 consists of documents re-\nporting terrorism events, annotated with entities\nsuch as Perpetrator Individuals, Perpetrator Orga-\nnizations, Physical targets, Victims, and Weapons\n(MUC-4, 1992). We split the dataset following Du\nand Cardie (2020).\nRe3d stands for Relationship and Entity Extrac-\ntion Evaluation Dataset (DSTL, 2018), comprising\ntask-specific documents focused on the topic of the\nconflict in Syria and Iraq. The data contains annota-\ntions in span format with their corresponding entity\ntypes: Organization, Weapon, Military platform,\nPerson, among others.\nCAMEO (C onflict and Mediation Event\nObservations) is the industry standard for event\nextraction in political science (Gerner et al., 2002).\nAn event classification, known as pentacode, con-\nsists of five event types: 0-Make a Statement, 1-\nVerbal Cooperation, 2-Material Cooperation, 3-\nVerbal Conflict, and 4-Material Conflict, and spans\n3https://satp.org\n4https://insightcrime.org\n5473\nof texts containing sources (who conducted the\naction) and targets (to whom the action was con-\nducted). We formulated two tasks for CAMEO\nevent extraction on our newly-annotated dataset:\nsources and targets labeling (ST NER), and penta-\ncode classification (PC MCC).\n4 Experimental Setup\n4.1 Pre-training Setup\nWe implemented ConfliBERT using two methods,\nCont and SCR. Each approach has an uncased\nand a cased version. The architecture is the same\nas BERT-Base with 12 layers, 768 hidden units,\n12 attention heads, and 110M parameters in total.\nSpecifically, for our Cont models, we ran additional\npre-training steps of the released checkpoints of\nBERT-Base models on our domain-specific corpus.\nThe vocabulary is the same as the original BERT’s\nvocabulary. For our SCR models, we use an in-\ndomain vocabulary, ConfliV ocab (See Section 3.1\nfor more details).\nWe discarded the next sentence prediction (NSP)\ntask. We found that the predicted NSP accuracy\nquickly reached 90% in the middle of our training,\nwhich indicated that NSP might be less challeng-\ning for the model to learn in our domain. How-\never, learning NSP simultaneously affected the\nspeed of optimization of masked language mod-\nels (MLM) loss. Following many recent works\ndiscarding NSP (Lample and Conneau, 2019; Yang\net al., 2019; Joshi et al., 2020; Liu et al., 2019) and\nour observation, we optimized MLM only.\nWe used four V-100 GPUs with 32 GB mem-\nory to train each model. We used Adam opti-\nmizer (Kingma and Ba, 2015). The learning rate\nwas warmed up over the first 10,000 steps to the\npeak value of 5e-4, and then linearly decayed. We\npre-trained each SCR model for about 150K steps\nover the 7 billion word corpus. We followed Devlin\net al. (2018) to train the model with a sequence\nlength of 128 for 80% of the steps. Then, we\ntrained the remaining 20% steps with a sequence of\n512. The overall training time for each SCR model\ntook about eight days. We trained Cont models the\nsame as SCR models but in two fewer days because\nthey were trained from intermediate checkpoints.\nSee Appendix for more details.\n4.2 Fine-Tuning Setup\nArchitecture. We followed the same architecture\nmodification as BERT (Devlin et al., 2018) in the\ndownstream tasks. Our task mainly consists of\nclassification and sequence labeling. The sequence\nlabeling tasks predict the sequence of BIO tags for\neach token in the input sentence. The classification\ntasks require a sequence classification/regression\nhead on top of the pooled output of BERT. We\nused cross-entropy loss for binary/multi-class clas-\nsification. We used mean-square loss and set the\ndiscrimination thresholds as 0.5 in all the multi-\nlabel classification tasks.\nCasing. Devlin et al. (2018) use the cased models\nfor NER and the uncased models for all other tasks.\nHowever, other works report that uncased models\nperform slightly better than cased models in spe-\ncific domains, even on NER tasks (Beltagy et al.,\n2019; Gu et al., 2021). Therefore, we evaluated\nboth cased and uncased versions of all models.\nHyperparameters. Devlin et al. (2018) propose\na hyperparameter tuning strategy relying on a grid-\nsearch on the ranges such as the number of training\nepochs ∈{3, 4}, and batch size ∈{16, 32}. How-\never, this strategy for general domain benchmarks\n(e.g. GLUE (Wang et al., 2018)) has not been\nsufficiently justified in other datasets (Chalkidis\net al., 2020). The optimal hyperparameters are\nhighly dataset- and task-dependent in our tasks.\nFor instance, the models may be underfitting after\nthe suggested maximum of four epochs. Addi-\ntionally, based on our observations from the con-\nflict datasets (e.g., GTD, SATP, MUC-4, InSight\nCrime), ConfliBERT models converge to the best\nresults faster than BERT. Therefore, to compare\nwith BERT fairly, we used early stopping based\nupon the development dataset within a range of\nthe maximum training epochs when all the models\nhave achieved stable results. A more detailed de-\nscription of other hyperparameters can be found in\nthe Appendix. Finally, we repeated all the experi-\nments ten times with different seeds.\n5 Results and Analysis\n5.1 Pre-training Results\nWe use perplexity (ppl) to measure how well the\nlanguage models predict a masked token in an un-\nseen test set. We sampled 0.02% of stories from\neach source during the data preparation, ending\nwith an 8.62 MB held-out dataset representing our\ncorpus’s distribution. Table 4 shows the ppl of our\nmodels on the held-out dataset. We also list the val-\nues reported by the original models (Devlin et al.,\n5474\nDataset Domain Tasks Train/dev/test Metrics BERT Confli.-Cont Confli.-SCR\nuncased cased uncased cased uncased cased\nBBC General BC 1588/315/322 F1 97.24 96.38 97.9 96.95 98.08 98.13\n20 News. General BC 9044/2270/7532 F1 80.30 79.58 80.4 80.51 81.05 80.37\nGun V . Violence BC 3387/423/423 macro F1 84.30 85.24 90.02 90.27 86.35 86.13\nGLOCON Protest Sent BC 1549/193/193 macro F1 84.53 84.92 85.60 85.72 86.57 82.20\nDoc BC 782/130/130 macro F1 88.97 84.61 89.76 89.97 91.13 88.27\nGTD Terrorism MCC 2825/471/471 macro F1 83.55 82.05 81.97 83.23 83.82 83.16\nSATP Terrorism\nBC 5956/744/745 F1 87.78 87.10 87.51 87.49 88.12 88.72\nRel MLC 1085/232/232 example F1 87.81 88.36 88.14 88.37 89.08 88.64\nAll MLC 4794/1192/1489 example F1 63.36 63.32 64.14 63.72 64.47 64.53\nInsight C. Crime MLC 1002/332/319 example F1 68.57 67.83 69.09 69.15 68.68 69.47\nIndia P. Violence Sent MLC 14943/3172/3276 example F1 64.89 64.54 63.03 63.40 67.27 66.22\nDoc MLC 905/165/187 example F1 66.80 63.41 67.09 67.38 69.97 66.71\nEvent S. Protest TS MCC 1818/226/227 macro F1 70.65 67.15 73.32 75.03 72.55 70.94\nBC 4010/500/501 F1 91.72 90.67 92.42 91.85 92.10 92.40\nCAMEO Politics PC MCC 1348/224/225 macro F1 86.44 85.85 87.88 86.12 87.64 87.83\nST NER 1153/224/225 macro F1 72.29 72.25 74.00 74.45 74.35 72.87\nMUC-4 Terrorism NER 1300/200/200 macro F1 62.96 60.33 60.29 60.90 63.97 60.31\nRe3d Defence NER 574/191/200 macro F1 63.44 62.46 64.40 66.20 66.40 64.23\nTable 3: The datasets, tasks and summary results of our evaluation.\nBERT Confli.-SCR Confli.-Cont\nuncased uncased cased uncased cased\nppl 3.99 3.14 3.14 3.40 2.93\nTable 4: Perplexity on held-out training data by model.\n2018). Low ppl scores indicate that our models\nhave been sufficiently pre-trained and have better\ngeneralization on our corpora.\n5.2 Fine-Tuning Results and Analysis\nTable 3 reports the F1 scores for each task using the\nmean of 10 seeds. We have the below observations:\nConfliBERT’s superiority over BERT. Con-\nfliBERT provides additional improvement to the\noriginal BERT in our target domain. In Table 3,\nalthough the performance is task-, dataset- and\ncasing- dependent, our models consistently report\nthe best results (in bold). In Figure 1, we compare\nConfliBERT SCR-uncased with the best results\nfrom both cased and uncased versions of BERT\nin each experiment. We use different colors to\ndenote four p-value thresholds (p <0.01, p<0.05,\np<0.1, and p≥0.1) of statistical significance. SCR-\nuncased demonstrates superior performance across\nall the tasks, and the difference is statistically sig-\nnificant at p<0.1 in all but three. Specifically\nfor GTD, we observed that SCR-uncased slightly\nbeats the best BERT, but it still shows a significant\nBBCNews\n20News.\nGunV.\n        GLO.-Sent\n       GLO.-Doc \nGTD \nSATP-BC \nSATP-Rel\nSATP-All\nInSightC.\nIndiaP.-Sent\nIndiaP.-Doc\nEventS.-TS    \nEventS.-BC \nCAMEO-PC\nCAMEO-ST\nMUC-4\nRe3d\n0 2 3\np<0.01\np<0.05\np<0.1\np≥0.1\nDifference in metric\n1\nFigure 1: Significance test of SCR-uncased vs. the best\nof BERT models in each task, regardless of casing.\nlevel of confidence, as depicted in Figure 1. We\nalso observed that on InSight Crime, ConfliBERT\nachieves the best results in SCR-cased. Yet for\nSCR-uncased, the margin is not significant when\ncompared with the best BERT in Figure 1. How-\never, we conduct certain experiments on GTD and\nInsight Crime showing ConfliBERT’s significant\nsuperiority when tackling limited training data in\nsection 5.2.\nEvaluating differences between the two pre-\n5475\nMacro F1\n 50\n 60\n 70\n 80\n 90\n11/21/41/81/161/32\nSCR-uncased \nSCR-cased\nCont-uncased\nCont-cased \nBERT-uncased\nBERT-cased\nSize Scale\n(a) GTD\nExample F1\n 40\n 50\n 60\n 70\n 80\n 90\n11/21/41/81/161/32\nSize Scale\nSCR-uncased \nSCR-cased\nCont-uncased\nCont-cased \nBERT-uncased\nBERT-cased (b) SATP-relevant\nExample F1\n   0\n 20\n 40\n 60\n 80\n11/21/41/8\nSize Scale\nSCR-uncased \nSCR-cased\nCont-uncased\nCont-cased \nBERT-uncased\nBERT-cased (c) Insight Crime\nFigure 2: Performance vs. varying size of the training data.\ntraining strategies, Cont and SCR, remains to be\nstudied. Table 3 shows that SCR slightly beats\nCont in most cases (13 out of 18 tasks), and SCR-\nuncased provides the most stable improvements\nover BERT among our four models. However, the\nperformance is still dataset- and task-dependent.\nFor example, Cont beats SCR significantly in Gun\nviolence and Event Status. We present an in-depth\nanalysis of these two cases in Appendix.\nEffect of ConfliVocab One major difference be-\ntween SCR and Cont is the use of in-domain vo-\ncabulary. Section 5.2 shows that both Cont and\nSCR outperform BERT significantly while SCR\nslightly beats Cont. We have discussed the sub-\nstantial advantage of using ConfliV ocab during the\ntokenization in Section 3.1. Besides the examples\nin Table 2, in ConfliV ocab we also find terrorist\ngroups and criminal organizations frequently men-\ntioned in the reports of violence and crime. Ex-\namples include Boko Haram, Al Qaeda, Sinaloa\nCartel, PCC, FARC, Mara, among others. On the\nother hand, the range of actor entities in the politics\ndomain is much larger and sparser than terrorist\nand criminal organizations. Given that we have a\nmore distinct in-domain vocabulary in the conflict\ndomain, we expect a more significant benefit from\nConfliV ocab in the conflict domain instead of the\ngeneral politics domain.\nConfliBERT requires less annotated data than\nBERT. ConfliBERT performs well with limited\ndata in various conflict datasets. Figure 2 shows\nthree groups of experiments on GTD, SATP, and\nInsight Crime, where we used varying training data\nsizes but the same valid and testing set as the orig-\ninal experiments respectively. We repeated each\nexperiment with five seeds and plotted the average\nmetric scores.\nFigure 2a shows that ConfliBERT beats BERT\nusing limited size of GTD training data. Especially\nin the case of 1/32 size of GTD training data (88\nexamples), both SCR models still have 69% F1\nscores, while BERT models drop to 55% F1 scores.\nIn Figure 2b, we sampled various subsets of SATP-\nrelevant, the SATP subset related to terrorist attacks.\nResults show that three of our models remain 65%\nto 73% F1 scores when using only 1/32 size of the\ntraining data (34 examples), while BERT drops to\nonly 44% F1 scores. Finally, we also observe that\nboth ConfliBERT SCR models significantly beat\nCont and BERT models with a large margin on\nInsight Crime in Figure 2c.\nThese results show large improvements when us-\ning ConfliBERT with limited training data. Given\nthe resources required to annotate data in conflict\nresearch, this is a particularly encouraging finding.\nThese experiments also show that ConfliBERT out-\nperforms BERT on GTD, SATP, and Insight Crime,\nstrengthening the results in Figure 1.\n6 Conclusion and Future Work\nThis paper presents the development, application,\nevaluation, and further exploration of ConfliBERT,\na pre-trained language model for political conflict\nand violence. The development of ConfliBERT\nrests on an unprecedented effort on three fronts.\nFirst, we collect and curate a large domain-specific\ncorpus to support the pre-training process. Sec-\nond, we conduct a comprehensive evaluation across\nseveral datasets and various NLP tasks of distinct\nnature and varying degrees of complexity.\nThe results show that ConfliBERT consistently\noutperforms BERT in the conflict and political vio-\nlence domain. Furthermore, the biggest improve-\n5476\nments are with limited training data, which conflict\nresearchers often have due to the high costs of data\nannotation. In this way, ConfliBERT constitutes\na valuable development that will contribute to a\nbroad community of researchers in political science\nand policy sectors interested in tracking, analyzing,\nand predicting political violence and conflict on a\nglobal scale.\nDue to limited time and computational resources,\nwe did not conduct more experiments to explore\nvarious hyperparameters that could affect fine-\ntuning results, such as vocabulary size and pre-\ntraining epochs, to name a few. Future work should\nanalyze how to optimize ConfliBERT, expand Con-\nfliBERT to multi-lingual settings, and apply Con-\nfliBERT to more challenging tasks such as un-\nderstanding, inference, question answering, uncer-\ntainty qualification (Hu et al., 2021; Hu and Khan,\n2021), and few-/ zero-shot tasks to speed up the\nstudy of NLP application for the political science\ncommunity.\n7 Ethical Impact\nOur research considers several measures to miti-\ngate concerns of bias in machine learning: (i) we\nimplement standard social science practices to se-\nlect corpora and training data (Barberá et al., 2021);\n(ii) for the pre-training stage, we gather a corpus\nwith unprecedented global coverage to reduce re-\ngional biases; (iii) we move beyond the biases in-\ntroduced from dictionary-based methods by using\nmachine learning, as suggested by Wilkerson and\nCasas (2017); (iv) finally, we use multiple coders\nfor the training data. However, copyright issues\nprevent us from sharing the raw data and hinder\nFAIR data principles (Wilkinson et al., 2016).\nThe broader goal of producing accurate and valid\nconflict data is to prevent or mitigate harm. These\ntypes of data provide a more objective means to\nunderstand and study conflict and armed violence.\nOur effort is an attempt to produce higher-quality\ndata resources to serve this purpose.\nAcknowledgments\nThe research reported herein was supported in part\nby NSF awards DMS-1737978, DGE-2039542,\nOAC-1828467, OAC-1931541, and DGE-1906630,\nONR awards N00014-17-1-2995 and N00014-\n20-1-2738, Army Research Office Contract No.\nW911NF2110032 and IBM faculty award (Re-\nsearch).\nReferences\nOpen Event Data Alliance. 2015. Petrarch:\nPython engine for text resolution and related cod-\ning hierarchy. http://www.github.com/\nopeneventdata/petrarch.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical BERT\nembeddings. In Proceedings of the 2nd Clinical Nat-\nural Language Processing Workshop, pages 72–78,\nMinneapolis, Minnesota, USA. Association for Com-\nputational Linguistics.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feldman,\nVu Ha, et al. 2018. Construction of the literature\ngraph in semantic scholar. NAACL HLT 2018, pages\n84–91.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nBenjamin E Bagozzi, Daniel Berliner, and Ryan M\nWelch. 2021. The diversity of repression: Measuring\nstate repressive repertoires with events data. Journal\nof Peace Research, 58(5):1126–1136.\nPablo Barberá, Amber E Boydstun, Suzanna Linn, Ryan\nMcMahon, and Jonathan Nagler. 2021. Automated\ntext classification of news articles: A practical guide.\nPolitical Analysis, 29(1):19–42.\nAndreas Beger, Cassy L Dorff, and Michael D Ward.\n2016. Irregular leadership changes in 2014: Fore-\ncasts using ensemble, split-population duration mod-\nels. International Journal of Forecasting, 32(1):98–\n111.\nJohn Beieler. 2016. Generating politically-relevant\nevent data. In Proceedings of the First Workshop\non NLP and Computational Social Science , pages\n37–42.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3615–3620.\nBrian Blankenship. 2020. Promises under Pressure:\nStatements of Reassurance in US Alliances. Interna-\ntional Studies Quarterly, 64(4):1017–1030.\nDoug Bond, Joe Bond, Churl Oh, Craig J. Jenkins, and\nCharles L. Taylor. 2003. Integrated Data for Events\nAnalysis (IDEA): An Event Typology for Automated\nEvents Data Development. Journal of Peace Re-\nsearch, 40(6):733–745.\nElizabeth Boschee, Jennifer Lautenschlager, Sean\nO’Brien, Steve Shellman, James Starz, and Michael\nWard. 2016. ICEWS Coded Event Data.\n5477\nPatrick T Brandt, Vito D’Orazio, Latifur Khan, Yi-Fan\nLi, Javier Osorio, and Marcus Sianan. 2022. Conflict\nforecasting with event data and spatio-temporal graph\nconvolutional networks. International Interactions,\npages 1–23.\nBerfu Büyüköz, Ali Hürriyeto˘glu, and Arzucan Özgür.\n2020. Analyzing ELMo and DistilBERT on socio-\npolitical news classification. In Proceedings of the\nWorkshop on Automated Extraction of Socio-political\nEvents from News 2020 , pages 9–18, Marseille,\nFrance. European Language Resources Association\n(ELRA).\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nMatthew J Connelly, Raymond Hicks, Robert Jervis,\nArthur Spirling, and Clara H Suong. 2021. Diplo-\nmatic documents data for international relations: the\nfreedom of information archive database. Conflict\nManagement and Peace Science, 38(6):762–781.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDSTL. 2018. Relationship and entity extraction eval-\nuation dataset. https://github.com/dstl/\nre3d/. Accessed: 2021-07-01.\nX. Du and Claire Cardie. 2020. Document-level event\nrole filler extraction using multi-granularity contex-\ntualized encoding. In ACL.\nDeborah J Gerner, Philip A Schrodt, Omür Yilmaz, and\nRajaa Abu-Jabr. 2002. Conflict and mediation event\nobservations (cameo): A new event data framework\nfor the analysis of foreign policy interactions. Inter-\nnational Studies Association, New Orleans.\nGoran Glavaš, Federico Nanni, and Simone Paolo\nPonzetto. 2017. Cross-lingual classification of top-\nics in political texts. In Proceedings of the Second\nWorkshop on NLP and Computational Social Science,\npages 42–46, Vancouver, Canada. Association for\nComputational Linguistics.\nDerek Greene and Pádraig Cunningham. 2006. Practi-\ncal solutions to the problem of diagonal dominance\nin kernel document clustering. In Proc. 23rd Interna-\ntional Conference on Machine learning (ICML’06),\npages 377–384. ACM Press.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nAndrew Halterman, Katherine Keith, Sheikh Sarwar,\nand Brendan O’Connor. 2021. Corpus-level eval-\nuation for event qa: The indiapoliceevents corpus\ncovering the 2002 gujarat violence. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4240–4253.\nAndrew Halterman and Benjamin J. Radford. 2021.\nFew-shot upsampling for protest size detection. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 3713–3720, On-\nline. Association for Computational Linguistics.\nAlex Hanna. 2017. Mpeds: Automating the gener-\nation of protest event data. Available at https:\n//osf.io/preprints/socarxiv/xuqmv\n(2020/05/22). Unpublished Manuscript.\nYibo Hu and Latifur Khan. 2021. Uncertainty-aware\nreliable text classification. In Proceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery\n& Data Mining, pages 628–636.\nYibo Hu, Yuzhe Ou, Xujiang Zhao, Jin-Hee Cho, and\nFeng Chen. 2021. Multidimensional Uncertainty-\nAware Evidential Neural Networks. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 35, pages 7815–7822.\nRuihong Huang, Ignacio Cases, Dan Jurafsky, Cleo Con-\ndoravdi, and Ellen Riloff. 2016. Distinguishing past,\non-going, and future events: The EventStatus corpus.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n44–54, Austin, Texas. Association for Computational\nLinguistics.\nAli Hürriyeto˘glu, Hristo Tanev, Vanni Zavarella, Jakub\nPiskorski, Reyyan Yeniterzi, Deniz Yuret, and Aline\nVillavicencio. 2021. Challenges and applications of\nautomated extraction of socio-political events from\ntext (case 2021): Workshop and shared task report.\nIn Proceedings of the 4th Workshop on Challenges\nand Applications of Automated Extraction of Socio-\npolitical Events from Text (CASE 2021), pages 1–9.\nAli Hürriyeto ˘glu, Erdem Yörük, Deniz Yüret, Ça ˘grı\nYoltar, Burak Gürel, Fırat Duru¸ san, and Osman\nMutlu. 2019. A task set proposal for automatic\nprotest information collection across multiple coun-\ntries. In European Conference on Information Re-\ntrieval, pages 316–323. Springer.\n5478\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntific data, 3(1):1–9.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale read-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785–\n794.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In NeurIPS.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Proceedings of the Twelfth International\nConference on Machine Learning, pages 331–339.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. Asso-\nciation for Computational Linguistics.\nJ. Lu and Joydeep Roy. 2017. Universal pe-\ntrarch: Language-agnostic political event cod-\ning using universal dependencies. Available\nat https://github.com/openeventdata/\nUniversalPetrarch (2020/05/22).\nRoseanne W McManus. 2017. Statements of Resolve:\nAchieving Coercive Credibility in International Con-\nflict. Cambridge University Press.\nMUC-4. 1992. Fourth message understanding confer-\nence (muc-4). In Fourth Message Uunderstanding\nConference (MUC-4): Proceedings of a Conference\nHeld in McLean, Virginia, June 16-18, 1992.\nClayton Norris, Philip Schrodt, and John Beieler. 2017.\nPetrarch2: Another event coding program. Journal\nof Open Source Software, 2(9):133.\nSean P. O’Brien. 2010. Crisis Early Warning and\nDecision Support: Contemporary Approaches and\nThoughts on Future Research. International Studies\nReview, 12(1):87–104.\nFredrik Olsson, Magnus Sahlgren, Fehmi ben Ab-\ndesslem, Ariel Ekgren, and Kristine Eck. 2020. Text\ncategorization for conflict event annotation. In Pro-\nceedings of the Workshop on Automated Extraction\nof Socio-political Events from News 2020, pages 19–\n25, Marseille, France. European Language Resources\nAssociation (ELRA).\nFaik Kerem Örs, Süveyda Yeniterzi, and Reyyan Yen-\niterzi. 2020. Event clustering within news articles.\nIn Proceedings of the Workshop on Automated Ex-\ntraction of Socio-political Events from News 2020 ,\npages 63–68, Marseille, France. European Language\nResources Association (ELRA).\nJavier Osorio and Alejandro Reyes. 2017. Supervised\nEvent Coding From Text Written in Spanish: Intro-\nducing Eventus ID. Social Science Computer Review,\n35(3):406–416.\nJavier Osorio, Alejandro Reyes, Alejandro Beltrán, and\nAtal Ahmadzai. 2020. Supervised event coding from\ntext written in Arabic: Introducing hadath. In Pro-\nceedings of the Workshop on Automated Extraction\nof Socio-political Events from News 2020, pages 49–\n56, Marseille, France. European Language Resources\nAssociation (ELRA).\nRobert Parker, David Graff, Junbo Kong, Ke Chen, and\nKazuaki Maeda. 2011. English gigaword fifth edition\nldc2011t07. Linguistic Data Consortium, Philadel-\nphia.\nErick Skorupa Parolin, Mohammadsaleh Hosseini, Yibo\nHu, Latifur Khan, Javier Osorio, Patrick T Brandt,\nand Vito D’Orazio. 2022. Multi-CoPED: A Mul-\ntilingual Multi-Task Approach for Coding Political\nEvent Data on Conflict and Mediation Domain. In\nProceedings of the 2022 AAAI/ACM Conference on\nAI, Ethics, and Society.\nErick Skorupa Parolin, Yibo Hu, Latifur Khan, Javier\nOsorio, Patrick T Brandt, and Vito D’Orazio. 2021a.\nCoMe-KE: A New Transformers Based Approach\nfor Knowledge Extraction in Conflict and Mediation\nDomain. In 2021 IEEE International Conference on\nBig Data (Big Data), pages 1449–1459. IEEE.\nErick Skorupa Parolin, Latifur Khan, Javier Osorio,\nPatrick Brandt, Vito D’Orazio, and Jennifer Holmes.\n5479\n2021b. 3M-Transformers for Event Coding on Or-\nganized Crime Domain. IEEE International Con-\nference on Data Science and Advanced Analytics\n(DSAA).\nEllie Pavlick, Heng Ji, Xiaoman Pan, and Chris Callison-\nBurch. 2016. The gun violence database: A new task\nand data set for nlp. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1018–1024.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of bert and elmo on ten bench-\nmarking datasets. In Proceedings of the 2019 Work-\nshop on Biomedical Natural Language Processing\n(BioNLP 2019), pages 58–65.\nAlec Radford, Jeff Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\nBenjamin Radford. 2020a. Seeing the forest and the\ntrees: Detection and cross-document coreference res-\nolution of militarized interstate disputes. In Proceed-\nings of the Workshop on Automated Extraction of\nSocio-political Events from News 2020 , pages 35–\n41, Marseille, France. European Language Resources\nAssociation (ELRA).\nBenjamin J Radford. 2020b. Multitask models for su-\npervised protests detection in texts. arXiv preprint\narXiv:2005.02954.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789.\nClionadh Raleigh, Andrew Linke, Håvard Hegre, and\nJoakim Karlsen. 2010. Introducing acled: an armed\nconflict location and event dataset: special data fea-\nture. Journal of peace research, 47(5):651–660.\nSayeed Salam, Patrick Brandty, Jennifer Holmesy, and\nLatifur Khan. 2018. Distributed framework for polit-\nical event coding in real-time. In 2018 2nd European\nConference on Electrical Engineering and Computer\nScience (EECS), pages 266–273.\nPhilip A. Schrodt. 2006. Twenty Years of the Kansas\nEvent Data System Project. The Political Methodolo-\ngist, 14(1):2–6.\nPhilip A. Schrodt. 2009. TABARI. Textual Analysis by\nAugmented Replacement Instructions.\nErick Skorupa Parolin, Latifur Khan, Javier Osorio, Vito\nD’Orazio, Patrick T. Brandt, and Jennifer Holmes.\n2020. Hanke: Hierarchical attention networks for\nknowledge extraction in political science domain. In\n2020 IEEE 7th International Conference on Data\nScience and Advanced Analytics (DSAA), pages 410–\n419.\nMohammad S Sorower. 2010. A literature survey on\nalgorithms for multi-label learning. Oregon State\nUniversity, Corvallis, 18:1–25.\nSTART. 2019. The global terrorism database (gtd)\n[data file]. Retrieved from https://www.start.\numd.edu/gtd.\nRalph Sundberg and Erik Melander. 2013. Introducing\nthe ucdp georeferenced event dataset. Journal of\nPeace Research, 50(4):523–532.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Neural Information Processing Systems\n(NIPS), pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nMichael Ward, Andreas Beger, Josh Cutler, Matthew\nDickenson, Cassy Dorff, and Ben Radford. 2013.\nComparing GDELT and ICEWS Event Data.\nJohn Wilkerson and Andreu Casas. 2017. Large-scale\ncomputerized text analysis in political science: Op-\nportunities and challenges. Annual Review of Politi-\ncal Science, 20:529–544.\nMark D Wilkinson, Michel Dumontier, IJsbrand Jan\nAalbersberg, Gabrielle Appleton, Myles Axton,\nArie Baak, Niklas Blomberg, Jan-Willem Boiten,\nLuiz Bonino da Silva Santos, Philip E Bourne, et al.\n2016. The fair guiding principles for scientific data\nmanagement and stewardship. Scientific data, 3.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\n5480\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n19–27.\nA Dataset\nTable 5 and Table 6 list the detailed sources in our\nExpert Domain Corpora and Main Stream Media\nCollection described in Section 3.2, respectively.\nType: Sources Size (MB)\nUnited Nations:\nnews.un.org, unodc.org, ohchr.org,\nunhcr.org, Refworld.org\n427\nU.S. Department of State:\nAnnual Country Reports on Human\nRights Practices,\nAnnual Country Reports on Terrorism,\nInternational Religious Freedom Reports,\nTrafficking in Persons Report,\nForeign Relations of the United States*\n1,027\nNon-government organizations:\namnesty.org, hrw.org, rescue.org, phr.org,\nthenewhumanitarian.org, satp.org, cfr.org\n838\n*We filter a subset after World War II (Sep 1945 to 1989)\nTable 5: Sources in Expert Domain Corpora.\nRegion Sources Size (GB)\nAsia\nAljazeera, CNA, IndianTimes,\nJapanTimes, SCMP, TheNewsIntl,\nXinhua\n2.0\nEurope BBC, DW, France24, Guardian,\nReuters, RFI, TASS 3.7\nUS\nABC, AP, CNBC, CNN, LATimes,\nNBC, NPR, NY Post, NYT, PBS,\nPolitico, SFGATE, UPI, USA Today,\nUS News, W ASHPOST, WSJ\n14.3\nOthers AllAfrica, News24, EFE,\nTheConversation 0.8\nTable 6: Sources in Mainstream Media Collection.\nFiltering News Wires. We considered all the sto-\nries in EDC as relevant. However, for the general\nnews in MMC, Gigaword, and PRT, we needed to\nfilter our specific domain of political conflict and\nviolence based on the websites’ metadata informa-\ntion such as URLs, subjects, and tags. For example,\nwe collected the stories with the tags such as Con-\nflicts, Violence, War, Politics, Defense, Crime, et\nal. We also defined a bag-of-words classifier to\nassess unlabeled stories’ relevance to our domain.\nTherefore, we statistically summarized two lists of\nthe most frequent keywords’ regular expressions\nfrom relevant stories and irrelevant stories. There\nare 266 patterns in the relevant list and 246 in the\nnot relevant list. For example, our relevant list con-\ntains patterns such as \"activist\", \"protest\",\n\"counter.?terrorism\", and \" jails?\\b\".\nSports news use bellicose language similar to that\nof conflict stories with words such as attack,\nshoot, and defeat, thus presenting a classifi-\ncation challenge. The not relevant list contains\nfrequent patterns such as \"shot \\w+ goal\" to\nremove sports news. We compared the number\nof unique matching in the two lists and tuned the\nthresholds with the help of conflict experts. Finally,\nwe filtered a small subset from MMC, Gigaword,\nand PRT in the conflict domain.\nFiltering Wikipedia. We modified Wikiextractor\n(Attardi, 2015) to extract 18 GB size of documents\nwith category labels from the Wikipedia dump 5\nreleased on March 20, 2021. We used PetScan 6\nto fetch pages of interest in the category hierarchy\ngraph. We searched all the sub-categories within 0\nto 4 depths under the union of five high-level top-\nics: politics, activism, crime, government, and war.\nAnd we got 5 GB size of stories within 208,008 sub-\ncategories from the query. Then, we summarized\nthe top 300+ frequent keywords from our targeted\ncategories to prune irrelevant or too far-away child\nnodes based on the sub-category labels. We also\nremoved unrelated categories such as fictional char-\nacters, movies, video war games, and historical\nevents or people before the 20th Century, et al.\nB Hyperparameters\nTable 7 and Table 8 describe the detailed hyper-\nparameters used in our pre-training and fine-tuning\nexperiments, respectively. We implement our mod-\nels using Huggingface API (Wolf et al., 2020).\n5https://dumps.wikimedia.org\n6https://petscan.wmflabs.org\n5481\nHyperparamter SCR Cont\nNumber of layers 12 12\nHidden Size 768 768\nFFN inner hidden size 3072 3072\nAttention heads 12 12\nMask percent 15 15\nLearning Rate Decay Linear Linear\nWarmup steps 10000 10000\nLearning Rate LR 5e-4 5e-4\nAdam ϵ 0.9 0.9\nAdam β1 0.98 0.98\nAdam β2 1e-6 1e-6\nAttention Dropout 0.1 0.1\nDropout 0.1 0.1\nWeight Decay 0.01 0.01\nTrain Steps 15,000 8,000\nV ocabulary ConfliV ocab BaseV ocab\nUncased V ocab Size 30,000 30,552\nCased V ocab Size 30,000 28,996\nBatch Size 2048 2048\nTable 7: Hyperparamters for pre-training ConfliBERT\nusing two strategies, pre-training from scratch (SCR)\nand continual pre-training (Cont). BaseV ocab refers\nto the original BERT’s vocabulary, while ConfliV ocab\nrefers to our domain-specific vocabulary.\nDataset - Tasks max\nepochs\nbatch\nsize\nmax\nseq-len\nlearning\nrate\ndrop-\nout\nBBC News-BC 3 16 512 4e-5 0.1\n20 News.-BC 3 16 512 4e-5 0.1\nGun V .-BC 10 8 512 5e-5 0.05\nGLOCON-Sent BC 20 128 128 5e-5 0.05\nGLOCON-Doc BC 5 8 512 5e-5 0.05\nGTD-MCC 10 16 128 4e-5 0.1\nSATP-BC 10 16 256 5e-5 0.05\nSATP-Rel MLC 10 16 256 4e-5 0.1\nSATP-All MLC 10 16 256 4e-5 0.1\nInSight C.-MLC 5 16 512 4e-5 0.1\nIndia P.-Sent MLC 10 16 128 4e-5 0.1\nIndia P. - Doc MLC 10 16 512 4e-5 0.1\nEvent S.-TS MCC 10 192 150 5e-5 0.05\nEvent S.-BC 10 192 150 5e-5 0.05\nCAMEO-PC MCC 40 32 128 5e-5 0.05\nCAMEO-ST NER 60 32 128 5e-5 0.3\nMUC4-NER 20 16 128 4e-5 0.1\nRe3d-NER 25 16 128 4e-5 0.1\nTable 8: Hyperparamters for fine-tuning all the models\nin our evaluation experiments.\nC Other detailed results\nThis section analyzes in a detailed manner the\nmodel’s performance on certain datasets. Specifi-\ncally, we analyze two rare cases where all ConfliB-\nERT models outperform BERTs and where Cont\nmodels significantly outperform SCR models. Ta-\nble 9 indicates how Cont significantly outperforms\nSCR in all performance metrics (p<0.05 for all met-\nrics). Table 10 shows how Cont-cased beats all the\nother counterparts for classifying event status of\npieces of civil unrest. While there may be many\nfactors, we postulate that some words in the origi-\nnal SCR-cased vocabulary are accidentally good at\ntokenizing the out-of-domain text in Gun Violence,\nwhile that vocabulary is also good at classifying\nongoing (OG) and future (FU) events.\nTags BERT Confli.-Cont Confli.-SCR\nuncased cased uncased cased uncased cased\n0-TRUE 85.50 86.53 91.21 91.39 87.53 87.23\n1-FALSE 83.11 83.95 88.84 89.14 85.17 85.02\nMicro F1 84.40 85.36 90.17 90.40 86.47 86.23\nMacro F1 84.30 85.24 90.02 90.27 86.35 86.13\nAUROC 90.13 91.19 94.63 95.45 92.54 92.87\nAUPRC 88.30 89.86 94.95 95.76 92.03 92.07\nTable 9: Gun Violence Binary Classification.\nTags BERT Confli.-Cont Confli.-SCR\nuncased cased uncased cased uncased cased\nPA 88.38 87.28 89.27 89.53 88.92 88.63\nOG 53.13 43.39 52.86 56.17 54.97 53.23\nFU 70.45 70.77 77.82 79.40 73.76 70.94\nMicro F1 79.47 77.49 81.28 82.29 80.48 79.87\nMacro F1 70.65 67.15 73.32 75.03 72.55 70.94\nMCC 56.67 51.45 60.40 62.71 59.74 57.40\nTable 10: Event Status Temporal Status Classification.\n5482",
  "topic": "Politics",
  "concepts": [
    {
      "name": "Politics",
      "score": 0.4567800760269165
    },
    {
      "name": "Computer science",
      "score": 0.45506829023361206
    },
    {
      "name": "Humanities",
      "score": 0.40993964672088623
    },
    {
      "name": "Linguistics",
      "score": 0.4000847041606903
    },
    {
      "name": "Sociology",
      "score": 0.3752972185611725
    },
    {
      "name": "Cognitive science",
      "score": 0.3650056719779968
    },
    {
      "name": "Psychology",
      "score": 0.31118500232696533
    },
    {
      "name": "Political science",
      "score": 0.2203545868396759
    },
    {
      "name": "Art",
      "score": 0.16509971022605896
    },
    {
      "name": "Philosophy",
      "score": 0.1510951817035675
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}