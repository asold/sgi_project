{
  "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning",
  "url": "https://openalex.org/W4388924092",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4307788499",
      "name": "Thwal, Chu Myaet",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227404587",
      "name": "Nguyen, Minh N. H.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4307788498",
      "name": "Tun, Ye Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125203478",
      "name": "Kim Seong Tae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225336532",
      "name": "Thai, My T.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2412176488",
      "name": "Hong Choong Seon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6774302960",
    "https://openalex.org/W6797478244",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W6838405875",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W6759984927",
    "https://openalex.org/W6741263554",
    "https://openalex.org/W2007339694",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W4304080501",
    "https://openalex.org/W6738957339",
    "https://openalex.org/W4386083031",
    "https://openalex.org/W6667932521",
    "https://openalex.org/W6783500014",
    "https://openalex.org/W6771659168",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W3035414587",
    "https://openalex.org/W6682475119",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W4212868693",
    "https://openalex.org/W6773817997",
    "https://openalex.org/W6788556936",
    "https://openalex.org/W2910121883",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6677103964",
    "https://openalex.org/W1912570122",
    "https://openalex.org/W3204445636",
    "https://openalex.org/W3182158470",
    "https://openalex.org/W6775960533",
    "https://openalex.org/W3021654819",
    "https://openalex.org/W6759238902",
    "https://openalex.org/W2962298324",
    "https://openalex.org/W6845186658",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W4285601701",
    "https://openalex.org/W2962851801",
    "https://openalex.org/W6797854001",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6838836003",
    "https://openalex.org/W6728757088",
    "https://openalex.org/W6802648153",
    "https://openalex.org/W6850117044",
    "https://openalex.org/W3170647102",
    "https://openalex.org/W2965862774",
    "https://openalex.org/W3004049187",
    "https://openalex.org/W6838078367",
    "https://openalex.org/W4285042965",
    "https://openalex.org/W3003792425",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W4312352414",
    "https://openalex.org/W6743446608",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W6777017071",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6810156959",
    "https://openalex.org/W4223430324",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W6781318954",
    "https://openalex.org/W6772307254",
    "https://openalex.org/W6810815564",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W6758623387",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W4386065441",
    "https://openalex.org/W4283811336",
    "https://openalex.org/W4386072014",
    "https://openalex.org/W6848185429",
    "https://openalex.org/W1922658220",
    "https://openalex.org/W4287828539",
    "https://openalex.org/W4313069943",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W4309797634",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2750384547",
    "https://openalex.org/W4320036918",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W3208245705",
    "https://openalex.org/W4287025584",
    "https://openalex.org/W4306295203",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W2069429561",
    "https://openalex.org/W3211787299",
    "https://openalex.org/W4281988564",
    "https://openalex.org/W2756417987",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3043723611",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4249502209",
    "https://openalex.org/W4289147229",
    "https://openalex.org/W3006555759",
    "https://openalex.org/W3103802018",
    "https://openalex.org/W3038022836",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W3006017224",
    "https://openalex.org/W2909882940",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3169769781",
    "https://openalex.org/W2792643794",
    "https://openalex.org/W2734358244",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2156150815",
    "https://openalex.org/W3194959296",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2963819344",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4294691337",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3168997536",
    "https://openalex.org/W3100321043",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2972570881",
    "https://openalex.org/W2963452728",
    "https://openalex.org/W4320459518",
    "https://openalex.org/W3087194612",
    "https://openalex.org/W2185062546",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W4225654619",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4256361765",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W4205616158",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W2992144222",
    "https://openalex.org/W4313125078",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2279098554",
    "https://openalex.org/W2707890836",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4319452505",
    "https://openalex.org/W3118608800"
  ],
  "abstract": null,
  "full_text": "OnDev-LCT: On-Device Lightweight Convolutional Transformers\ntowards Federated Learning\nChu Myaet Thwala, Minh N. H. Nguyenb, Ye Lin Tuna, Seong Tae Kima, My T. Thaic, Choong Seon Honga,∗\naDepartment of Computer Science and Engineering, Kyung Hee University, Yongin-si, Gyeonggi-do 17104, South Korea\nbVietnam - Korea University of Information and Communication Technology, Danang, Vietnam\ncDepartment of Computer and Information Science and Engineering, University of Florida, Gainesville, Florida 32611, USA\nAbstract\nFederated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple\nedge devices while preserving privacy. The success of FL hinges on the efficiency of participating models and their ability to handle\nthe unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as\nalternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher compu-\ntational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL.\nSince client devices in FL typically have limited computing resources and communication bandwidth, models intended for such\ndevices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data\ndistributions encountered in FL. To address these challenges, we proposeOnDev-LCT: Lightweight Convolutional Transformers for\nOn-Device vision tasks with limited training data and resources. Our models incorporate image-specific inductive biases through\nthe LCT tokenizer by leveraging e fficient depthwise separable convolutions in residual linear bottleneck blocks to extract local\nfeatures, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global repre-\nsentations of images. Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight\nvision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data\nheterogeneity and communication bottlenecks.\nKeywords: convolutional transformer, lightweight, computer vision, federated learning, data heterogeneity\n1. Introduction\nRecent trends in deep learning enable artificial intelligence\nto make significant strides, even surpassing humans in many\ntasks. When it comes to image understanding, convolutional\nneural networks (CNNs) have become the de facto among nu-\nmerous types of deep neural architectures. Along with the\ntremendous size of publicly available image datasets, i.e., Im-\nageNet (Deng et al., 2009) and Microsoft COCO (Lin et al.,\n2014), deep CNNs like ResNet (He et al., 2016) and its variants\nhave been dominating the field of computer vision for decades.\nEvidently, lightweight CNNs, such as MobileNets (Howard\net al., 2017; Sandler et al., 2018; Howard et al., 2019) and their\nextensions (Han et al., 2020; Li et al., 2021b), have achieved\nstate-of-the-art performance for many on-device vision tasks\nsince image-specific inductive biases (Lenc and Vedaldi, 2015;\nMitchell et al., 2017) allow them to learn visual representations\nand recognize complex patterns with fewer parameters. CNNs,\nhowever, are spatially local (Zuo et al., 2015), making them\n∗Corresponding author\nEmail addresses: chumyaet@khu.ac.kr (Chu Myaet Thwal),\nnhnminh@vku.udn.vn (Minh N. H. Nguyen), yelintun@khu.ac.kr (Ye\nLin Tun), st.kim@khu.ac.kr (Seong Tae Kim), mythai@cise.ufl.edu\n(My T. Thai), cshong@khu.ac.kr (Choong Seon Hong)\n0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\nMACs (×109)\n60\n65\n70\n75\n80\n85\n90Accuracy (%)\n0.21M\n0.31M 0.51M 0.91M\n0.28M\n0.48M\n0.42M 0.45M\n1.21M\n1.23M\n0.27M 0.47M 0.67M\n0.21M\n0.72M\nOnDev-LCT\nCCT\nViT\nResNet\nMobileNetv2\nFigure 1: Performance comparison on CIFAR-10 dataset in the central-\nized scenario. The comparison is performed among small model variants,\ni.e., #Params < 1.3M, while constraining the computational budget within\n0.1 ×109 MACs. Each bubble’s area is proportional to the model size.\nhard to capture global representations, i.e., contextual depen-\ndencies between different image regions.\nParallel to the widespread use of CNNs in computer vision,\ntransformer architectures (Vaswani et al., 2017), which exploit\nthe multi-head self-attention mechanism, have emerged to be-\ncome an integral part of natural language processing (NLP).\nIn response to the dominant success of transformers in large-\nscale language modeling, attention-based architectures have re-\nPreprint submitted to Neural Networks January 23, 2024\narXiv:2401.11652v1  [cs.CV]  22 Jan 2024\ncently piqued a growing interest in computer vision. Attention-\nembedded CNN models (Wang et al., 2017; Hu et al., 2018;\nBello et al., 2019), transformer-based networks (Parmar et al.,\n2018; Dosovitskiy et al., 2021; Touvron et al., 2021), and\nmost recently, hybrid architectures of convolutional transform-\ners (Wu et al., 2021a; Hassani et al., 2021; Mehta and Rastegari,\n2022; Graham et al., 2021; Chen et al., 2022b) have shown their\npotential in a wide range of vision tasks. Vision Transformer\n(ViT) (Dosovitskiy et al., 2021), adapting the encoder design of\nNLP transformer (Vaswani et al., 2017) with minimum mod-\nifications, shows its superior performance over cutting-edge\nCNNs in several benchmark tasks. Hence, ViT becomes more\nprevalent in computer vision for its ability to learn global rep-\nresentations through attention mechanisms.\nConversely, the general trend towards improving the perfor-\nmance of transformers is to increase the number of parame-\nters, which demands a vast amount of computational resources.\nThus, the unprecedented size of ViT hinders its applications\non resource-constrained devices. Moreover, ViT requires pre-\ntraining on large-scale image datasets, i.e., ImageNet-21K or\nJFT-300M, to learn generalizable visual representations since\nit lacks some inductive biases inherent in CNNs (Dosovitskiy\net al., 2021), such as translation equivariance and locality. Re-\ncent studies focus on combining the strengths of CNNs with\ntransformers and have become acknowledged for their high-\nperforming hybrid architectures of convolutional transform-\ners (Wu et al., 2021a; Hassani et al., 2021; Mehta and Raste-\ngari, 2022; Graham et al., 2021; Chen et al., 2022b). However,\nthese hybrid models still have a huge number of parameters\nor require extensive data augmentation techniques, demanding\nhigher computational resources for training to perform at the\nsame level as CNNs in low-data regimes.\nOn the other hand, for many domains including science and\nmedical research (Varoquaux and Cheplygina, 2022; Ghassemi\net al., 2020), it is costly to get a large quantity of annotated data\nwith the size of ImageNet (Deng et al., 2009) for training. Due\nto the privacy-sensitive nature of user data in such domains, col-\nlecting and centralizing the training data also turn out to be chal-\nlenging. Federated learning (FL) is a distributed model training\nstrategy that guarantees user privacy without the need for data\ncollection. FL enables multiple client devices to collaboratively\ntrain their local models without exposing the sensitive data of\neach client. The vanilla FL algorithm called federated averag-\ning (FedAvg) (McMahan et al., 2017) learns a shared global\nmodel by iteratively averaging the local model updates of par-\nticipating clients. A successful FL implementation can yield\npromising results for on-device applications, especially in cer-\ntain domains where data privacy and confidentiality are crucial.\nNonetheless, FL introduces several core challenges, including\ncommunication bottlenecks and data heterogeneity (Li et al.,\n2020a), which need to be considered before it can be applied in\nreal-world distributed systems. Moreover, since client devices\ncan only offer a limited amount of computing resources, models\nintended for those devices should be lightweight and capable of\non-device applications. As a result, it demands an open research\nquestion: “Whether a powerful lightweight vision transformer\ncan be efficiently designed for resource-constrained devices?”.\nIn light of this demand, we propose OnDev-LCT: On-Device\nLightweight Convolutional Transformers for vision tasks, in-\nspired by the idea of employing an early convolutional stem\nin transformers. We introduce image-specific inductive priors\nto our models by incorporating a convolutional tokenizer be-\nfore the transformer encoder. Specifically, we leverage e ffi-\ncient depthwise separable convolutions in our LCT tokenizer\nfor extracting local features from images. The multi-head self-\nattention (MHSA) mechanism in our LCT encoder implicitly\nenables our models to learn global representations of images.\nHence, benefiting from both convolutions and attention mech-\nanisms, we design our OnDev-LCTs for on-device vision tasks\nwith limited training data and resources. We analyze the per-\nformance of our models on various image classification bench-\nmarks in comparison with popular lightweight vision models\nin the centralized scenario. We further explore our OnDev-\nLCT models in FL scenarios to empower client devices for vi-\nsion tasks while coping with data heterogeneity, resource con-\nstraints, and user privacy (Li et al., 2020a). Since data aug-\nmentations during local training can incur additional computa-\ntion overhead for resource-constrained FL devices, we compare\nour models with the baselines in FL scenarios without applying\ndata augmentation techniques. Our empirical analysis indicates\nthat OnDev-LCTs significantly outperform the other baseline\nmodels in both centralized and FL scenarios under various data\nheterogeneity settings. Additionally, we provide detailed dis-\ncussions based on our observations from various performance\nanalyses to demonstrate the e fficiency and effectiveness of our\nOnDev-LCTs. Figure 1 depicts the image classification per-\nformance on the CIFAR-10 (Krizhevsky et al., 2009) dataset\nfor different model families with variants of comparable sizes,\ni.e., #Params < 1.3M (millions) and multiply-accumulate or\nMACs <0.1G (billions), in the centralized scenario. Compared\nto the other baselines, we can observe that our OnDev-LCTs\ncan achieve better accuracy with fewer parameters and lower\ncomputational demands.\n2. Related Works\nIn this section, we highlight the evolution of prior research\nefforts in computer vision. Then, we give an overview of how\nFL can be applied for on-device vision applications.\nConvolutional neural networks. Since AlexNet’s (Krizhevsky\net al., 2012) record-breaking achievement on ImageNet (Deng\net al., 2009), CNN architectures have gained a great deal of\ntraction in computer vision. As the general trend towards better\naccuracy is to develop deeper and wider networks (Li et al.,\n2021c), deep CNNs (He et al., 2016; Simonyan and Zisser-\nman, 2014; Szegedy et al., 2015) that can extract complex fea-\ntures from images have become popular for vision applications.\nThe residual attention networks proposed by Wang et al. (2017)\nbuild attention-aware feature maps to provide di fferent repre-\nsentations of focused image patches as they embed the power of\nattention mechanisms in CNNs for better classification. How-\never, these improvements in accuracy do not always result in\nmore e fficient networks in terms of computation, speed, and\n2\nsize (Khan et al., 2020). Real-world applications, such as med-\nical imaging (Varoquaux and Cheplygina, 2022), self-driving\ncars (Parekh et al., 2022), robotics (Roy et al., 2021; Neuman\net al., 2022), and augmented reality (AR) (Mutis and Ambekar,\n2020) require edge devices to run vision tasks promptly un-\nder limited resources. In response to these challenges, several\nlightweight CNNs (Howard et al., 2017; Sandler et al., 2018;\nHoward et al., 2019; Han et al., 2020; Li et al., 2021b; Hu et al.,\n2018; Iandola et al., 2016; Park et al., 2020) have been devel-\noped as their flexible and simple-to-train nature may easily re-\nplace deep CNN backbones while minimizing the network size\nand improving latency. Yet, CNNs have one major drawback:\nas they are spatially local, it makes them hard to extract com-\nplex global interactions among the spatial features of images on\na broader range (Zuo et al., 2015; LeCun et al., 2015).\nTransformers in vision. Dosovitskiy et al. (2021) introduced\nVision Transformer (ViT) as the first notable example of using\na pure transformer backbone for computer vision tasks. How-\never, lacking some inductive priors makes it di fficult for ViT\nto enhance its generalization ability for out-of-distribution data\nsamples when training data is insu fficient. Hence, transform-\ners are data-hungry, demanding extremely large pre-training\ndatasets, such as ImageNet-21K or JFT-300M, to compete with\ntheir convolutional counterparts (Khan et al., 2021). This has\nled to an explosion in model and dataset sizes, limiting the use\nof ViT among practitioners in low-data regimes. In the con-\ntext of lightweight and e fficient vision transformers, parallel\nlines of research have emerged, exploring di fferent strategies\nto accomplish high-performance on-device vision tasks. Re-\nsearchers have delved into techniques such as knowledge dis-\ntillation (Touvron et al., 2021), quantization (Li et al., 2022;\nLin et al., 2022), pruning (Zhu et al., 2021; Fang et al., 2023),\nand architectural modifications (Mehta and Rastegari, 2022;\nGraham et al., 2021) to enhance the e fficiency of transformers\non vision tasks. In an e ffort to improve upon ViT, the Data-\nefficient image Transformer (DeiT) (Touvron et al., 2021) in-\ntroduced a transformer-specific distillation strategy that does\nnot require a large quantity of training data. For a given pa-\nrameter constraint, DeiT still requires huge computational re-\nsources, with its performance slightly below that of E fficient-\nNets (Tan and Le, 2019) with equivalent sizes. In addition,\nextensive data augmentation techniques and powerful teacher\nmodels are prerequisites for the best performance of DeiT mod-\nels. Nonetheless, models for real-world vision applications\nshould be lightweight and e fficient so that they can be deftly\nrun on resource-constrained devices (Menghani, 2023).\nAs one of the most powerful model compression approaches,\nquantization has gained considerable attention due to its po-\ntential in reducing the computation cost and memory over-\nhead while striving to maintain the performance of vision trans-\nformers. Quantization involves converting the full-precision or\nfloating-point values (i.e., 32 bits) of weights and activations\nto integers with lower bit-width, thus accelerating the process-\ning of vision transformers in resource-constrained settings. To\nthis point, several works have been studied for adapting vi-\nsion transformers to two distinct families of quantization meth-\nods: quantization-aware training (QAT) (Li et al., 2022; Liu\net al., 2023) and post-training quantization (PTQ) (Liu et al.,\n2021; Lin et al., 2022; Yuan et al., 2022; Ding et al., 2022),\ndepending on the trade-o ff between achieving better accuracy\nand simpler deployment. With an intuitive idea of compress-\ning the model without significant performance sacrifice, prun-\ning (Blalock et al., 2020; Frankle et al., 2021) has also proved\nto be highly effective and practical for accelerating vision trans-\nformers to varying degrees. Researchers have explored var-\nious pruning techniques aimed at removing specific compo-\nnents of vision transformers, such as attention heads or indi-\nvidual weights, to create more streamlined variants suitable\nfor resource-constrained devices. For instance, unstructured\npruning (Dong et al., 2017; LeCun et al., 1989; Hassibi et al.,\n1993; Sanh et al., 2020) targets individual weights, providing\nmore flexibility in model compression, but it often requires\nspecial hardware accelerators or software for model accelera-\ntion (Han et al., 2016). Conversely, structured pruning (Zhu\net al., 2021; Yu and Xiang, 2023; Yu et al., 2022; Yang et al.,\n2023; Fang et al., 2023) has gained great interest due to its\nhardware-friendly nature and ability to reduce inference over-\nhead by removing entire components of vision transformers,\nsuch as multi-head self-attention layers, token embeddings, or\nneurons. Nonetheless, the choice of the most suitable pruning\ntechnique for a particular task depends on several factors in-\ncluding the target model size, the desired performance level,\nand the available computational resources, in order to strike a\nbalance between reducing the model size and retaining essential\nrepresentations and prediction capabilities.\nConvolutional transformers. In the context of architec-\ntural modifications, the Convolutional vision Transformer\n(CvT) (Wu et al., 2021a) introduced convolutions into ViT for\nimproving its performance and e fficiency on vision tasks. A\nconvolutional token embedding is added prior to each hierar-\nchical stage of CvT transformers, along with a convolutional\nprojection layer, which replaces the position-wise projection of\nmulti-head self-attention in the transformer encoders. Hassani\net al. (2021) dispelled the myth of “data-hungry vision trans-\nformers” by presenting ViT-Lite with smaller patch sizing for\npatch-based image tokenization of the original ViT. The same\nauthors introduced a sequential pooling method (SeqPool) in\nCompact Vision Transformer (CVT) (Hassani et al., 2021) to\neliminate the need for a classification token from vision trans-\nformers. Coupling the strengths of CNNs and ViT, Compact\nConvolutional Transformer (CCT) (Hassani et al., 2021) and\nMobileViT (Mehta and Rastegari, 2022) have been proposed as\nlightweight hybrid models to improve the flexibility of trans-\nformers in vision tasks. CCT replaced the patch-based tok-\nenization of ViT with convolutional tokenization that preserves\nthe local spatial relationships between convolutional patches.\nMobileViT introduced a lightweight ViT model by replacing\nthe local processing in convolutions with transformers to learn\nglobal representations of images. Thus, in addition to the as-\npect of ViT in capturing long-range feature dependencies pro-\nvided by the attention mechanism, convolution and pooling op-\nerations of CNNs come up with the translation equivariance and\n3\n4\n33\n2\n11\nClient 1\nServer\nClient k\n1. Server distributes a global model.\n2. Client trains a local model.\n3. Client sends model updates back         \n    to server .\n4. Server aggregates model  updates into \n    a new global model.\n2\nGlobal model\nOn-Device\nLocal model\nOn-Device\nLocal model\nFigure 2: The vanilla federated averaging (FedAvg) framework. A typical\nfour-step procedure is iterated until convergence.\nlocality, allowing these hybrid models to work better on smaller\ndatasets. To this end, these models desire high-level data aug-\nmentation techniques to justify their optimal performance in vi-\nsion tasks. Likewise, large computational resources are still es-\nsential for them to beat mobile-friendly CNN models.\nRecently, Graham et al. (2021) proposed a hybrid neural net-\nwork named LeViT for faster inference of image classification\nby designing a multi-stage transformer in ConvNet’s clothing.\nConvolutional blocks of LeViT are built as pyramids for de-\ncreasing the resolution of feature maps with an increasing num-\nber of channels. The authors replaced the ViT’s positional\nembedding with a learned, per-head translation-invariant at-\ntention bias and redesigned attention-MLP blocks to improve\nthe network capacity. LeViT models are trained following\nthe DeiT’s distillation-driven training with two classification\nheads; thus, they require a powerful pre-trained teacher model,\ni.e., RegNetY-16GF, and strong computational resources, i.e.,\n32 GPUs, to perform 1000 training epochs in 3 to 5 days.\nMobile-Former proposed by Chen et al. (2022b) parallelizes\nMobileNet (Howard et al., 2019) and transformer with a bidi-\nrectional cross-attention bridge in between. In contrast to the\ncomputationally efficient parallel design, Mobile-Former intro-\nduces additional parameters due to its parameter-heavy classi-\nfication head. The EdgeNeXt (Maaz et al., 2022), with its in-\nnovative split depthwise transpose attention (SDTA) encoder,\ndemonstrates remarkable potential for mobile devices through\nits stage-wise design of hybrid architecture. EdgeNeXt variants\nsurpass previous hybrid models in both accuracy and model size\nwhile significantly reducing the computational overhead.\nMehta and Rastegari (2023) proposed MobileViTv2, an im-\nproved version of the original MobileViT, to alleviate the main\nefficiency bottleneck caused by the multi-headed self-attention.\nThe authors introduced a separable self-attention method with\nlinear complexity, making MobileViTv2 suitable for deploy-\nment on resource-constrained devices. Jeevan and Sethi (2022)\nadapted linear attention mechanisms to Vision Transformer\n(ViT) (Dosovitskiy et al., 2021), resulting in the creation of\nVision X-formers (ViXs), where X ∈ {Performer (P), Lin-\nformer (L), Nystr ¨omformer (N) }(Choromanski et al., 2020;\nWang et al., 2020c; Xiong et al., 2021). This adaptation led\nto a remarkable reduction of up to seven times in GPU memory\nrequirements. Additionally, the authors introduced inductive\npriors by replacing the initial linear embedding layer with con-\nvolutional layers in ViXs, resulting in a substantial improve-\nment in classification accuracy without increasing the model\nsize. Drawing our inspiration from the key concepts presented\nin those prior works, we aim to develop high-performing,\nlightweight convolutional transformers for vision applications\non devices with limited resources in low-data regimes.\nFederated learning. Federated averaging (FedAvg) (McMahan\net al., 2017) is the earliest and most widely used FL baseline\nthat executes an iterative model averaging process via a typ-\nical four-step procedure, as illustrated in Figure 2. However,\nFedAvg suffers from performance deterioration when dealing\nwith distribution shifts in training data across multiple clients.\nVarious works have been proposed in an attempt to enhance the\nperformance of FedAvg for tackling the data heterogeneity by\nfocusing on either two aspects: improvements in local training,\ni.e., Step 2 of Figure 2 (Li et al., 2020b; Karimireddy et al.,\n2020; Li et al., 2021a) or improvements in server aggregation,\ni.e., Step 4 of Figure 2 (Wang et al., 2020a; Hsu et al., 2019;\nWang et al., 2020b). A recent study on ViT-FL (Qu et al., 2022)\npresents a new perspective on utilizing ViT in federated vision\nmodels. Their empirical analysis demonstrates that “transform-\ners are highly robust to distribution shifts in data”. However,\nthe performance of ViT-FL heavily relies on the pre-training\ndataset, or ViT requires a huge amount of training data to out-\nperform CNNs when trained from scratch. Often, it is hard to\nreadily obtain a well-pre-trained ViT model for various domain-\nspecific tasks in a practical distributed system. Moreover, the\nunprecedented size of ViT (e.g., ≈6M for ViT-Tiny or ≈22M\nfor ViT-S (Qu et al., 2022; Touvron et al., 2022)) may intro-\nduce hurdles to communication bottlenecks when exchanging\nthe model updates for aggregation. Hence, in this study, we\naim to tackle those challenges by deploying our OnDev-LCTs\nin FL scenarios under various data heterogeneity settings.\n3. Preliminaries\nIn this section, we describe the preliminaries and motivations\nbehind the development of our OnDev-LCT models.\nPatch-based tokenization. In the standard ViT model (Doso-\nvitskiy et al., 2021), a 2 D input image x is split into multiple\nnon-overlapping fixed-size patches:\nx ∈RH×W×C ⇒xp ∈RN×(P2C),\nwhere C is the number of channels, N = HW\nP2 is the number of\npatches with ( H,W) and ( P,P) for the dimensions of original\nimage and patches, respectively. Image patches are flattened\ninto a sequence of 1D vectors (aka tokens), which are then lin-\nearly projected into D-dimensional space: [x1\npE; x2\npE; ... ; xN\np E],\nwhere E ∈ R(P2C)×D is a projection vector. For the purpose\n4\nReLU\nV K Q \nBN\nReLU Linear\nClassifier\nClass\nCat\nDog\n...\nLinear\nDepthwise\nSeparable\nConvolution\nLayerNorm\nMulti-Head\nSelf-Attention\nLayerNorm\nDense\nDropout\nMLP  block x 2\nstride = 2\nLinear\nBottleneck\nResidual Linear Bottleneck\nLCT Encoder\nInput Image\nReshaped T okenized Patches\nstride = 1\nLinear\nBottleneck\n3 x 3\nConv2D\nLCT T okenizer\nReshaping\nLCT Encoder\nSequence Pooling\nBN\n BN\nLinear\nDepthwise\nSeparable\nConvolution\nstride = s\n3 x 3\nDwise\nConv2D\nBN\nReLU\nx L \nM  x\nExpand\n1 x 1\nPwise\nConv2D\nShrink\n1 x 1\nPwise\nConv2D\nx R \nFigure 3: An overview of On-Device Lightweight Convolutional Transformer (OnDev-LCT). The middle row depicts the architecture of an LCT tokenizer,\nvisualizing its core components in the bottom row. The structure of an LCT encoder is illustrated on the right.\nof classification, an extra learnable xclass token is prepended to\nthe patch embeddings. Since this ViT’s patch-based tokeniza-\ntion step ignores spatial inductive biases, information along the\nboundary regions of the input image is lost (Khan et al., 2021).\nThus, a positional vector E pos ∈R(N+1)×D is encoded to the se-\nquence, resulting in an input embedding z0 to the transformer\nencoder as:\nz0 = [xclass ; x1\npE; x2\npE; ... ; xN\np E] + Epos.\nAlthough this positional encoding makes it easier to learn com-\nplex features through visual representations, ViT still requires a\nlarge scale of training data to generalize well.\nConvolutional tokenization. A convolutional tokenization\nblock introduced by CCT (Hassani et al., 2021) consists of a\nsingle standard convolution with a ReLU (Agarap, 2018) acti-\nvation followed by a MaxPool operation. An input image x is\ntokenized into convolutional patches and reshaped into an input\nembedding z0 to the transformer encoder as:\nz0 = MaxPool(ReLU(Conv2D(x))).\nThis convolutional tokenization step makes CCT flexible\nenough to remove the positional encoding as well by injecting\ninductive biases into the model. However, as the MaxPool op-\neration reduces the spatial resolution of the input image, all the\nrelevant information cannot be preserved well (Sabour et al.,\n2017; Xinyi and Chen, 2019). Moreover, having multiple con-\nvolutional tokenization blocks may speed up the loss of all spa-\ntial information, which in turn hurts the model performance.\nTransformer encoder. In the original ViT (Dosovitskiy et al.,\n2021), each encoder, i.e., l = 1,2,..., L, is stacked on top of\none another, sending its output zl to the subsequent encoder.\nEach encoder contains two main components, such as a multi-\nhead self-attention (MHSA) block and a 2-layer multi-layer\nperceptron (MLP) block, with a LayerNorm (LN) and residual\nconnections in between. The last encoder outputs a sequencezL\nof image representations.\nSequence pooling. An attention-based sequence pooling (Se-\nqPool) in CVT (Hassani et al., 2021) maps the last encoder’s\noutput sequence to the classifier by pooling the relevant infor-\nmation across different parts of the sequence and eliminates the\nneed for an extra learnable class token.\n4. OnDev-LCT Architecture\nIn this section, we present the architecture of our OnDev-\nLCT by explaining key components in detail. It includes three\nmain parts: LCT tokenizer, LCT encoder, and a classifier. An\noverview of our OnDev-LCT is illustrated in Figure 3.\nLCT tokenizer. Inspired by the idea of leveraging a convolu-\ntional stem for image tokenization, we introduce spatial induc-\ntive priors into our models by replacing the entire patch-based\ntokenization of ViT (Dosovitskiy et al., 2021) with an LCT to-\nkenizer. Given an input image x, our LCT tokenizer encodes\nit into convolutional patches before forwarding it to the LCT\nencoder. Our tokenizer starts with M convolution operations:\nxm = ReLU(BN(Conv2D(x))), m = 1,2,..., M (1)\nwhere Conv2D represents a standard 3×3 convolution followed\nby a BatchNorm (BN) with ReLU (Agarap, 2018) activation.\nAfter the final convolution operation, the output xM is sent to a\nblock of Linear Depthwise Separable Convolution:\nx′\n0 = LinearDWS(xM), (2)\nwhich is composed of a layer of 3 ×3 depthwise convolution\nfollowed by a layer of 1 ×1 pointwise convolution:\nx′\n0,dw = ReLU(BN(DepthwiseConv2D(xM))),\nx′\n0 = Linear(BN(PointwiseConv2D(x′\n0,dw))). (3)\n5\nDepthwiseConv2D extracts features through spatial filtering per\ninput channel, while PointwiseConv2D is used to project chan-\nnels output from the depthwise layer by a linear combination\nof those channels. Each convolution operation is followed by\nBatchNorm with a ReLU for depthwise convolution and a Lin-\near activation for pointwise convolution. The output feature\nmap is forwarded throughR Residual Linear Bottleneckblocks:\nx′\nr = LinearBottleneck(x′\nr−1) + x′\nr−1, r = 1,2,..., R (4)\nEach Bottleneck block first shrinks the feature map via a 1 ×1\nconvolution. DWS convolution is then applied to remap the\noutput to the same dimension as the input feature map:\nx′\nr,shrink = ReLU(BN(PointwiseConv2D(x′\nr−1))),\nx′\nr = LinearDWS(x′\nr,shrink). (5)\nHere, instead of using several MaxPool operations, we im-\nplement DWS convolutions at the heart of our OnDev-LCT to\nmaintain the spatial information of the input image while build-\ning up better representations. The amount of computations can\nalso be drastically reduced as DWS breaks the standard convo-\nlution operation, i.e., channel-wise and spatial-wise computa-\ntions, into two separate layers. Moreover, the Bottleneck struc-\nture reduces the number of parameters and computation costs\nwhile preventing information loss of using non-linear functions,\nsuch as ReLU (Agarap, 2018)). The idea of residual blocks is\nto make our model as thin as possible by increasing the depth\nwith fewer parameters while improving the ability to effectively\ntrain a lightweight vision transformer. At the end of the LCT\ntokenization step, encoded patches are reshaped to be processed\nthrough L blocks of LCT encoder:\nx′\nR ∈RHR×WR×CR ⇒z0 ∈RHW×D, (6)\nwhere CR = D is the dimension of the input embedding z0.\nViT-based LCT encoder. We construct our LCT encoder by\nfollowing the encoder design of the original ViT (Dosovitskiy\net al., 2021), which comprises MHSA and MLP blocks:\nz′\nl = MHSA(LN(zl−1)) + zl−1, l = 1,2,..., L (7)\nzl = MLP(LN(z′\nl )) + z′\nl , l = 1,2,..., L (8)\nThe MHSA block in the encoder receives the input embedding\nz0 in the form of three parameters Query (Q), Key (K), and\nValue (V). Self-attention (SA) begins by computing the dot\nproducts of the Query with all Keys and applying a softmax\nfunction to obtain the attention weights on the Values. Multi-\nhead enables the multiple calculations of self-attention in par-\nallel with h independent attention heads:\nMHSA(Q,K,V) = Concat(SA1,SA2,..., SAh). (9)\nWith the multi-head self-attention (MHSA), our model can\ncapture long-range positional information as each head can pay\nattention to different parts of the input embedding and capture\ndifferent contextual information by calculating the associations\nbetween image patches in a unique way. The MLP block con-\ntains two layers with GELU (Hendrycks and Gimpel, 2016) ac-\ntivation and dropouts. LayerNorm (LN) is applied before each\ncomponent of the encoder to prevent its values from chang-\ning abruptly, improving the training performance. A residual\nconnection is also applied to route the information between the\ncomponents. Then, the final output sequence zL is forwarded to\nthe sequence pooling (SeqPool) layer.\nClassifier. We apply SeqPool (Hassani et al., 2021) over the\noutput sequence zL to eliminate the use of learnable class token:\nz = σ(FC(LN(zL))) ×zL, (10)\nwhere LayerNorm (LN) is applied before passing the sequence\nto the fully connected (FC) layer, then attention weights are\nadded by a softmax function σ. As the output sequence con-\ntains relevant information across di fferent parts of the input\nembedding, preserving this information via attention weights\ncan improve the model performance with no additional param-\neters compared to the class token and thus significantly reduce\nthe computation overhead. Eventually, the output z is sent to a\nsingle-layer classifier:\ny = FC(z), (11)\nwhere y is the final classification output of our OnDev-LCT.\n5. Experiments\nIn this section, we analyze the performance of our OnDev-\nLCTs over existing lightweight vision models by conducting\nextensive experiments on multiple image datasets in both cen-\ntralized and federated learning scenarios.\nDatasets. We conduct extensive experiments on six image\nclassification datasets: CIFAR-10, CIFAR-100 (Krizhevsky\net al., 2009), MNIST (Deng, 2012), Fashion-MNIST (Xiao\net al., 2017), EMNIST-Balanced (Cohen et al., 2017), and\nFEMNIST (Caldas et al., 2018). Since our study focuses on\nedge devices with limited resources for tackling vision tasks\nin low-data regimes, we do not use additional pre-training data\nfor all experiments. CIFAR-10 and CIFAR-100 (Krizhevsky\net al., 2009) datasets consist of 50,000 training samples and\n10,000 test samples of 32 ×32 RGB images in 10 and 100\nclasses, respectively. MNIST (Deng, 2012) and Fashion-\nMNIST (Xiao et al., 2017) datasets contain 60,000 training\nsamples and 10,000 test samples of 28 ×28 single-channel im-\nages in 10 classes. EMNIST-Balanced (Cohen et al., 2017)\ncontains 112,800 training samples and 18,800 test samples of\n28 ×28 single-channel handwritten character images in 47 bal-\nanced classes. We use the original training and test sets of each\ndataset for the centralized scenario while dividing the training\nsets into 10 and 50 client data partitions for the FL scenarios.\nTo control the level of data heterogeneity for each partition,\nwe adjust the concentration parameter β of the Dirichlet dis-\ntribution (Ferguson, 1973) from {0.1,0.5,5}, where smaller β\ncorresponds to a higher degree of data heterogeneity. Using\n6\n0 2 4 6 8 10\nClient ID\n0\n2\n4\n6\n8\n10Class ID\n0\n1000\n2000\n3000\n4000\n(a) For 10 clients (β= 0.1)\n0 2 4 6 8 10\nClient ID\n0\n2\n4\n6\n8\n10Class ID\n0\n500\n1000\n1500\n2000\n2500 (b) For 10 clients (β= 0.5)\n0 2 4 6 8 10\nClient ID\n0\n2\n4\n6\n8\n10Class ID\n200\n400\n600\n800\n1000\n1200 (c) For 10 clients (β= 5)\n0 10 20 30 40 50\nClient ID\n0\n2\n4\n6\n8\n10Class ID\n0\n500\n1000\n1500\n2000 (d) For 50 clients (β= 0.1)\n0 10 20 30 40 50\nClient ID\n0\n2\n4\n6\n8\n10Class ID\n0\n100\n200\n300\n400\n500\n600\n700 (e) For 50 clients (β= 0.5)\n0 10 20 30 40 50\nClient ID\n0\n2\n4\n6\n8\n10Class ID\n50\n100\n150\n200\n250 (f) For 50 clients (β= 5)\nFigure 4: Detailed data partitions on CIFAR-10 dataset using Dirichlet distribution. Smaller βcorresponds to higher data heterogeneity. Best viewed in color.\nModel #Trans #Convs Patch PosEmb SeqPool#ParamsViT-Lite-1/16 1 ✗ 16×16 ✓ ✗ 0.25MViT-Lite-2/16 2 ✗ 16×16 ✓ ✗ 0.27MViT-Lite-1/8 1 ✗ 8×8 ✓ ✗ 0.42MViT-Lite-2/8 2 ✗ 8×8 ✓ ✗ 0.45MViT-Lite-1/4 1 ✗ 4×4 ✓ ✗ 1.21MViT-Lite-2/4 2 ✗ 4×4 ✓ ✗ 1.23MCCT-2/2 2 2 ✗ ✓ ✓ 0.28MCCT-4/2 4 2 ✗ ✓ ✓ 0.48MOnDev-LCT-1/1 1 1 ✗ ✗ ✓ 0.21MOnDev-LCT-2/1 2 1 ✗ ✗ ✓ 0.31MOnDev-LCT-4/1 4 1 ✗ ✗ ✓ 0.51MOnDev-LCT-8/1 8 1 ✗ ✗ ✓ 0.91MOnDev-LCT-1/3 1 3 ✗ ✗ ✓ 0.25MOnDev-LCT-2/3 2 3 ✗ ✗ ✓ 0.35MOnDev-LCT-4/3 4 3 ✗ ✗ ✓ 0.55MOnDev-LCT-8/3 8 3 ✗ ✗ ✓ 0.95M\nTable 1: Detailed architecture of transformer-based model variants. We\nfollow the implementation of Hassani et al. (2021) for ViT-Lite variants.\nthis partitioning strategy, each FL client may have relatively\nfew or no data samples in certain classes. Figure 4 visualizes\nthe data distribution of 10 and 50 FL clients for the CIFAR-\n10 dataset. Additionally, we evaluate the e ffectiveness of our\nOnDev-LCTs in realistic non-IID settings by using the Fed-\nerated Extended MNIST (FEMNIST) dataset provided by the\nLEAF benchmark (Caldas et al., 2018). The FEMNIST dataset\nis commonly used in the FL literature, as it is designed to repli-\ncate practical FL scenarios for multi-class (i.e., 62 classes) im-\nage classification by naturally partitioning the total of 805,263\ndata samples in Extended MNIST (EMNIST) (Cohen et al.,\n2017) dataset into 3,550 clients based on the writer of the digit\nor character. To align with prior FL works (Caldas et al., 2018;\nWu et al., 2021b; Chen et al., 2022a; Shi et al., 2022), we sub-\nsample 5% of the total data (approximately 180 clients) and\nsplit it into 80% for training and 20% for testing per client.\nImplementation details. We implement our experiments using\nthe TensorFlow (Abadi et al., 2015) framework. All experi-\nments are conducted on a single NVIDIA RTX 3080 GPU with\n10 GB memory. We consider two client sampling strategies for\nexperiments in FL scenarios (except for the FEMNIST dataset):\n(1) we sample 10 clients where all the clients participate in ev-\nery FL round, (2) we sample 50 clients and randomly choose\n10 clients to participate in each FL round. For CIFAR-10 and\nCIFAR-100 (Krizhevsky et al., 2009), we train each model for\n200 epochs in centralized experiments and conduct 60 commu-\nnication rounds for the FL scenario with 10 clients and 100\nrounds for the FL scenario with 50 clients, with each client\ntraining for 5 local epochs. For other datasets, except for FEM-\nNIST, we train each model for 50 epochs in centralized exper-\niments and conduct 30 communication rounds for the FL sce-\nModel FamilyCIFAR10 CIFAR100 MNISTFashionMNISTEMNISTBalancedFEMNISTOptimizerTypeResNet 5e−3 5e −3 5e −3 5e −3 5e −3 1e −3 AdamMobileNetv25e−3 5e −3 3e −3 5e −3 5e −3 3e −3 AdamViT-Lite 1e−3 1e −3 1e −3 1e −3 1e −3 1e −3 AdamWCCT 1e−3 1e −3 1e −3 1e −3 1e −3 1e −3 AdamWOnDev-LCT3e−3 5e −3 1e −3 3e −3 3e −3 1e −3 Adam\nTable 2: Learning rate values and optimizer types for each model family.\nModels with AdamW use the default values ofβ1 = 0.9, β2 = 0.999, and weight\ndecay = 1e−4.\nnario with 10 clients and 60 rounds for the FL scenario with 50\nclients, with each client training for 3 local epochs. For FL ex-\nperiments with the FEMNIST dataset, we conduct 120 commu-\nnication rounds, with each client training for 3 local epochs per\nround. During the communication rounds, we randomly select\nC clients to participate in training, considering three di fferent\nsettings of C ∈{5,10,2 ∼10}, which simulate common sce-\nnarios where clients may be offline or have limited connectivity\nto participate in every round. The setting 2 ∼10 represents\nthe number of clients selected in each round, varying from 2\nclients to 10 clients. We conduct 5 separate runs for each cen-\ntralized experiment and report the best accuracy values. For\nFL, reported values are the best median values from the last 5\nFL rounds of 3 separate runs.\nModel variants and baselines. We explore several variants of\nOnDev-LCT by tuning the number of LCT encoders and the\nnumber of standard convolutions in the LCT tokenizer. For\ninstance, “OnDev-LCT-2 /3” specifies a variant with 2 LCT\nencoders and 3 standard convolutions. We compare the per-\nformance of our OnDev-LCTs with existing lightweight vi-\nsion models as baselines. For pure CNN baselines, we imple-\nment small-sized variants of ResNet (He et al., 2016) and Mo-\nbileNetv2 (Sandler et al., 2018), which are specially designed\nfor CIFAR, denoted by “ResNet- r” where r is the number of\nlayers and “MobileNetv2 /α” where α is the width multiplier,\nrespectively. Following (Hassani et al., 2021), we design sev-\neral “ViT-Lite” model variants with pure backbones of the vi-\nsion transformer (Dosovitskiy et al., 2021) by varying numbers\nof transformer layers and patch sizes. We also implement small\n“CCT” variants (Hassani et al., 2021) (only with 3×3 convolu-\ntion kernels), denoted with the number of transformer encoders\nfollowed by the number of convolutional blocks. For all the\ntransformer-based models, we set the embedding dimension as\n128 and use 4 attention heads. Table 1 shows the detailed ar-\nchitecture of transformer-based model variants.\nHyperparameter selection. We perform manual hyperparame-\nter tuning for each model family and apply the best settings.\n7\nModel CIFAR-10 CIFAR-100 MNIST Fashion\nMNIST\nEMNIST\nBalanced #Params MACs\nConvolutional Neural Networks (Designed for CIFAR)\nResNet-20 82.30 42.62 99.49 92.45 88.38 0.27M 0.04G\nResNet-32 82.68 46.75 99.50 92.82 88.18 0.47M 0.07G\nResNet-44 83.53 48.79 99.58 92.46 88.07 0.67M 0.10G\nResNet-56 83.03 48.14 99.50 92.48 88.14 0.86M 0.13G\nMobileNetv2 /0.2 71.79 37.99 99.06 90.16 85.42 0.21M <0.01G\nMobileNetv2 /0.5 77.10 41.71 99.11 89.70 87.66 0.72M <0.01G\nMobileNetv2 /0.75 80.10 44.37 99.16 90.13 86.75 1.39M <0.01G\nMobileNetv2 /1.0 80.50 44.51 99.34 90.53 87.49 2.27M 0.01G\nLite Vision Transformers\nViT-Lite-1/16 53.71 25.24 97.66 87.37 82.02 0.25M <0.01G\nViT-Lite-2/16 53.74 25.21 97.66 87.67 82.23 0.27M <0.01G\nViT-Lite-1/8 61.67 32.51 98.04 87.01 83.35 0.42M 0.01G\nViT-Lite-2/8 62.60 33.46 97.97 87.30 83.35 0.45M 0.02G\nViT-Lite-1/4 68.66 38.43 98.63 90.25 86.22 1.21M 0.04G\nViT-Lite-2/4 71.03 41.98 98.65 90.70 86.05 1.23M 0.07G\nCompact Convolutional Transformers\nCCT-2/2 79.71 50.75 99.17 91.37 88.93 0.28M 0.03G\nCCT-4/2 80.92 53.23 99.20 91.73 89.41 0.48M 0.05G\nOn-Device Lightweight Convolutional Transformers (Ours)\nOnDev-LCT-1 /1 84.55 57.69 99.46 92.73 89.52 0.21M 0.03G\nOnDev-LCT-2 /1 86.27 59.17 99.39 92.50 89.18 0.31M 0.04G\nOnDev-LCT-4 /1 86.61 61.36 99.53 92.70 89.39 0.51M 0.05G\nOnDev-LCT-8 /1 86.64 62.62 99.49 92.59 89.55 0.91M 0.08G\nOnDev-LCT-1 /3 85.73 57.66 99.41 92.72 88.96 0.25M 0.07G\nOnDev-LCT-2 /3 86.04 60.26 99.46 93.31 89.60 0.35M 0.08G\nOnDev-LCT-4 /3 87.03 61.95 99.38 92.60 89.49 0.55M 0.09G\nOnDev-LCT-8 /3 87.65 62.91 99.28 92.82 89.45 0.95M 0.12G\nTable 3: Comparison on image classification performance in the centralized setting.All reported accuracy (%) values are the best of 5 runs. All experiments are\nconducted without applying data augmentation or learning rate schedulers. The best accuracy is marked in bold. The best entry of each model family is underlined.\nIn the centralized scenario, the training batch size is set to\n256 for EMNIST-Balanced (Cohen et al., 2017) and 128 for\nothers. In FL experiments, we reduce the batch size by half,\ni.e., 128 for EMNIST-Balanced (Cohen et al., 2017) and 64\nfor the rest. It is important to note that, we set the training\nbatch size to 16 for all experiments with the FEMNIST dataset.\nWe use Adam (Kingma and Ba, 2014) optimizer for OnDev-\nLCTs and CNN baselines (He et al., 2016; Sandler et al., 2018)\nwhile AdamW (Loshchilov and Hutter, 2017a) with weight de-\ncay = 1e−4 is used for CCT (Hassani et al., 2021) and ViT-\nLite (Dosovitskiy et al., 2021) variants. We manually tune the\nlearning rate and use the best value for each model family. The\nselected learning rate values and optimizers are shown in Ta-\nble 2. Since we are focusing on a fair comparison of the model\nperformance, we train all the models from scratch without pre-\ntraining or applying data augmentation techniques and learning\nrate schedulers for all experiments by default. These techniques\ncould challenge the fairness of evaluation and provide sensitive\nresults for tuning hyperparameters, especially in FL scenarios.\nHowever, label smoothing with a probability of 0.1 is used dur-\ning the training of all model variants.\nComparison in centralized scenario. Table 3 reports image\nclassification performance of our OnDev-LCTs compared to\nexisting lightweight vision models in a centralized scenario.\nThe results show that our OnDev-LCT variants significantly\noutperform the other baselines while having lower computa-\ntional demands. Even our smallest OnDev-LCT-1/1 with 0.21M\nparameters can perform better in most cases compared to other\nmodel variants. Our larger variant OnDev-LCT-8/3 further im-\nproves the performance, especially on CIFAR-10 and CIFAR-\n100 (Krizhevsky et al., 2009), achieving 4 .62% and 14 .77%\nbetter than ResNet-56 (He et al., 2016), which has a compa-\nrable model size and computational demands. Figure 5 shows\nt-SNE (van der Maaten and Hinton, 2008) visualizations of\nfeature embeddings learned by each model on the CIFAR-\n10 (Krizhevsky et al., 2009) test data. Visualizations show that\nfeatures learned by our proposed OnDev-LCT model are more\ndivergent and separable than those learned by other baselines.\nComparison in FL scenarios. We conduct experiments in FL\nscenarios with small variants of each model family, constrain-\ning the number of parameters to 1M (millions) and the com-\n8\nClass ID\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) OnDev-LCT-1/1\n (b) CCT-2/2\n (c) ViT-1/8\n (d) ResNet-20\n (e) MobileNetv2/0.2\nFigure 5: t-SNE visualizations of feature embeddings on CIFAR-10 test set learned by centralized models. Best viewed in color.\nModel\nFedAvg with 10 clients\nCIFAR-10 CIFAR-100\nβ=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5\nResNet-20 67.55 75.43 77.72 27.89 37.08 37.53\nResNet-32 71.55 78.05 80.39 31.34 41.33 42.68\nResNet-44 72.56 77.73 80.55 32.85 43.60 44.81\nMobileNetv2/0.2 56.55 67.07 69.44 21.52 30.57 31.67\nMobileNetv2/0.5 60.50 72.35 74.27 23.40 33.28 32.58\nViT-Lite-1/8 48.53 58.05 59.70 17.85 27.60 31.37\nViT-Lite-2/8 49.03 58.57 60.02 17.99 28.20 32.32\nCCT-2/2 66.45 74.86 78.42 35.21 47.99 50.50\nCCT-4/2 66.38 75.36 78.55 39.44 50.43 52.76\nOnDev-LCT-1/1 76.02 82.25 84.24 42.64 55.23 58.07\nOnDev-LCT-2/1 76.85 82.81 84.95 44.83 56.74 59.53\nOnDev-LCT-4/1 78.10 84.01 86.12 47.51 58.85 61.79\nOnDev-LCT-8/1 79.70 84.74 86.77 49.75 59.71 62.42\nTable 4: Comparison on image classification performance in FL with 10\nclients under different data heterogeneity settings. All reported accuracy (%)\nvalues are the best of 3 runs. Three best accuracy values are marked in bold.\nputation (i.e., MACs) to 0.1G (billions). For ViT-Lite (Has-\nsani et al., 2021) models, instead of choosing the smallest vari-\nants, we choose larger variants with smaller patch sizes that\ncan achieve better accuracy in centralized experiments. Table 4\nand Table 5 provide the image classification performance of\nour OnDev-LCTs on CIFAR-10 and CIFAR-100 (Krizhevsky\net al., 2009) datasets compared to the other baselines under\nvarious data heterogeneity settings of FL. We can observe that\nour OnDev-LCTs significantly outperform other baselines in all\ndata heterogeneity settings. From Table 4 with a 10-client sce-\nnario, most of the transformer-based model variants can achieve\ncomparable or only slightly lower results than the centralized\nperformance under less data heterogeneity settings with βval-\nues of 5 and 0.5. For CNNs, we can observe a significant gap\nbetween their centralized and FL performance, especially for\nthe highest data heterogeneity setting with β = 0.1. Remark-\nably, all the existing lightweight baseline models su ffer from\ndata heterogeneity in FL, especially the ViT-Lite model vari-\nants. Meanwhile, our OnDev-LCT models achieve the best\nperformance and are more robust to data heterogeneity across\nclients than the other baselines. Similarly, in Table 5 with a\n50-client scenario, we can observe that our OnDev-LCTs out-\nperform the other baselines in all degrees of data heterogeneity.\nThe reason behind this capability stems from the innova-\ntive combination of convolutions in our LCT tokenizer and\nthe MHSA mechanism in our LCT encoder, which enables our\nOnDev-LCTs to strike a balance between local feature extrac-\ntion and global context representation. The strength of our LCT\ntokenizer lies in its e fficient depthwise separable convolutions\nModel\nFedAvg with 50 clients\nCIFAR-10 CIFAR-100\nβ=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5\nResNet-20 53.44 69.62 74.84 26.64 33.89 34.93\nResNet-32 55.79 70.50 76.17 26.27 36.89 38.02\nResNet-44 55.78 71.08 76.10 28.91 36.44 39.07\nMobileNetv2/0.2 44.30 55.97 60.84 13.66 16.15 18.06\nMobileNetv2/0.5 46.59 62.36 63.29 12.19 18.52 18.83\nViT-Lite-1/8 36.22 52.94 57.05 14.02 24.68 28.06\nViT-Lite-2/8 36.85 53.13 57.28 14.44 25.39 28.22\nCCT-2/2 55.67 68.87 73.45 33.51 42.17 45.59\nCCT-4/2 56.05 69.82 74.52 35.26 43.68 46.95\nOnDev-LCT-1/1 66.18 76.93 79.71 32.69 49.34 53.83\nOnDev-LCT-2/1 68.25 78.20 80.55 35.66 52.10 54.71\nOnDev-LCT-4/1 67.24 77.68 80.87 38.82 52.64 56.15\nOnDev-LCT-8/1 68.44 78.28 82.42 42.23 54.97 58.18\nTable 5: Comparison on image classification performance in FL with 50\nclients under different data heterogeneity settings. All reported accuracy (%)\nvalues are the best of 3 runs. Three best accuracy values are marked in bold.\nand residual linear bottleneck blocks, as these convolutions cap-\nture relevant spatial information e fficiently while significantly\nreducing the computational burden and model size. By e ffec-\ntively extracting local features, our LCT tokenizer can capture\nessential details in image representations, thus enhancing the\noverall accuracy of the model. Moreover, the MHSA mecha-\nnism in our LCT encoder facilitates learning global representa-\ntions of images, making OnDev-LCTs better at recognizing un-\nseen patterns and more adaptive to new and diverse data. This\nadaptability is crucial in practical applications where the model\nmay encounter images from different sources or domains.\nIn the context of FL, models are trained locally on individual\ndevices with non-IID data distribution, and their updates are\naggregated on the server. This process often introduces chal-\nlenges, such as data heterogeneity and communication bottle-\nnecks. However, the lightweight nature of our OnDev-LCT ar-\nchitecture allows for more e fficient communication of model\nupdates between client devices and the central server, mitigat-\ning the impact of communication constraints. The advantage of\nOnDev-LCT can be attributed to its unique design and the in-\ntegration of image-specific inductive biases through the convo-\nlutional tokenization step before the transformer encoder. The\nincorporation of residual linear bottleneck blocks in the LCT\ntokenizer enhances the model’s ability to adapt to local varia-\ntions in data distribution, making it more resilient to non-IID\ndata in the FL setting. By leveraging these design choices,\nour OnDev-LCTs can learn more generalizable representations\nwith fewer parameters, which is particularly beneficial in FL\nscenarios with limited data and communication constraints. As\n9\nClass ID\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) OnDev-LCT-1/1\n (b) CCT-2/2\n (c) ViT-Lite-1/8\n (d) ResNet-20\n (e) MobileNetv2/0.2\nFigure 6: t-SNE visualizations of feature embeddings on CIFAR-10 by FedAvg global models (10 clients withβ= 0.5). Best viewed in color.\nClass ID\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) OnDev-LCT-1/1\n (b) CCT-2/2\n (c) ViT-1/8\n (d) ResNet-20\n (e) MobileNetv2/0.2\nFigure 7: t-SNE visualizations of feature embeddings on CIFAR-10 by FedAvg global models (50 clients withβ= 0.5). Best viewed in color.\nModel No. of selected clients (C)\n5 10 2 ∼10\nResNet-20 78.94 81.57 78.29\nResNet-32 79.82 82.23 79.27\nResNet-44 80.01 82.45 80.20\nMobileNetv2/0.2 66.72 68.68 64.93\nMobileNetv2/0.5 66.70 74.60 69.16\nViT-Lite-1/8 59.75 62.06 59.57\nViT-Lite-2/8 60.48 63.48 59.58\nCCT-2/2 78.58 80.71 78.78\nCCT-4/2 78.60 81.28 79.95\nOnDev-LCT-1/1 77.86 80.68 76.67\nOnDev-LCT-2/1 79.57 81.43 76.56\nOnDev-LCT-4/1 79.84 81.84 78.76\nOnDev-LCT-8/1 80.33 82.02 80.06\nTable 6: Comparison on image classification performance in realistic non-\nIID FL settings using the FEMNIST dataset from LEAF benchmark. All re-\nported accuracy (%) values are the best of 3 runs. Three best accuracy values\nare marked in bold.\na result, the proposed OnDev-LCT models demonstrate higher\nperformance and robustness compared to other baselines under\nvarious data heterogeneity settings of FL. However, the scope\nand applicability of OnDev-LCT are not confined to FL scenar-\nios and, the flexibility and efficient nature of OnDev-LCT holds\ngreat potential in emerging fields like edge AI and on-device\nAI. It is important to emphasize that the superior performance\nof the proposed OnDev-LCT highlights its potential value and\nbroader applicability in FL and other resource-constrained on-\ndevice vision tasks across various domains. Figure 6 and Fig-\nure 7 show the t-SNE (van der Maaten and Hinton, 2008) visu-\nalizations of feature embeddings learned by each global model\nunder FedAvg setting (β = 0.5) with 10 clients and 50 clients\non the CIFAR-10 (Krizhevsky et al., 2009) test data.\nComparison in realistic FL scenarios. In this section, we con-\nducted a performance comparison of our proposed OnDev-\nLCTs with other baseline models using the FEMNIST (Cal-\ndas et al., 2018) dataset. The results, as presented in Table 6,\ndemonstrate that convolutional transformers, such as OnDev-\nLCTs and CCTs (Hassani et al., 2021), achieve competitive per-\nformance on par with ResNet variants (He et al., 2016). ViT-\nLite (Hassani et al., 2021) models, while being lightweight,\nshow comparatively weaker performance, due to their limited\nability to e fficiently capture both local and global features,\nwhich are crucial for accurate image classification in realis-\ntic FL scenarios like FEMNIST. The accuracy of MobileNetv2\nvariants (Sandler et al., 2018) may not have reached its peak\nduring the conducted training epochs, implying the potential\nfor increased accuracy with further training. Nonetheless, our\nproposed OnDev-LCTs consistently show their impressive per-\nformance across various scenarios with different numbers of se-\nlected clients participating in each training round, thus demon-\nstrating the effectiveness of our model design for on-device vi-\nsion tasks in FL scenarios. The integration of e fficient convo-\nlutions, self-attention mechanisms, and image-specific induc-\ntive biases empowers our models to strike a balance between\nlocal feature extraction and global context representation, con-\ntributing to their superior performance and e fficiency. These\ncompelling findings highlight the potential of our OnDev-LCT\narchitecture as a practical solution for on-device vision tasks in\nFL with limited data and computational resources.\n6. Additional Results\nWe present additional results for the performance analysis of\nour proposed architecture in this section.\nComparative analysis with low-bit vision transformers. As an\northogonal approach to our work, we explore the existing lit-\nerature on popular quantized, aka low-bit vision transformers,\nemphasizing their contributions to the domain of on-device im-\nage classification tasks. Liu et al. (2021) present VT-PTQ, a\nmixed-precision post-training quantization scheme, which in-\ntroduces a ranking loss to the quantization objective to keep\n10\nModel Quantization\nmethod #bit(W-A)\nCIFAR-10 FEMNIST\n(C=10)\nInference\ntime (s) #ParamsCentralized FedAvg with 50 clients\nβ=0.1 β=0.5 β=5\nQ-ViT QAT 4-4 55.91 37.89 43.02 45.73 59.48 3.50 22M\nMinMax PTQ 8-8 55.05 31.04 45.27 49.56 37.99 2.42 22M\nEMA PTQ 8-8 52.86 29.75 45.08 51.19 50.73 2.43 22M\nPercentile PTQ 8-8 53.70 29.82 45.79 50.42 47.75 2.46 22M\nOMSE PTQ 8-8 52.69 30.83 45.66 50.24 49.50 2.44 22M\nFQ-ViT PTQ 8-8 51.72 28.86 44.52 49.19 49.37 2.62 22M\nOnDev-LCT-1/1 None 32-32 84.55 66.18 76.93 79.71 80.68 2.66 0.21M\nOnDev-LCT-2/1 None 32-32 86.27 68.25 78.20 80.55 81.43 2.78 0.31M\nOnDev-LCT-4/1 None 32-32 86.61 67.24 77.68 80.87 81.84 3.13 0.51M\nOnDev-LCT-8/1 None 32-32 86.64 68.44 78.28 82.42 82.02 3.77 0.91M\nTable 7: Comparative analysis with low-bit vision transformers on CIFAR-10 and FEMNIST datasets. Three best accuracy (%) values are marked in bold.\nInference time represents the wall-clock time, measured on a single NVIDIA RTX 3080 GPU for the CIFAR-10 test set with a batch size of 1000.\nModel Pruning\nRate\nCIFAR-10\n#Params Params\nRatioCentralized FedAvg with 50 clients\nβ=0.1 β=0.5 β=5\nViT-Tiny (Baseline) 0.0 55.86 36.25 47.85 50.76 5.49M 1.000×\nL1-norm Importance 0.3 52.91 33.23 44.94 46.88 3.01M 0.548×\nL2-norm Importance 0.3 53.21 33.59 44.86 46.71 3.01M 0.548×\nTaylor Importance 0.3 53.90 32.51 44.97 47.58 3.01M 0.548×\nRandom Importance 0.3 39.88 27.66 37.59 39.08 3.01M 0.548×\nOnDev-LCT-1/1 0.0 84.55 66.18 76.93 79.71 0.21M 0.038×\nOnDev-LCT-2/1 0.0 86.27 68.25 78.20 80.55 0.31M 0.056×\nOnDev-LCT-4/1 0.0 86.61 67.24 77.68 80.87 0.51M 0.093×\nOnDev-LCT-8/1 0.0 86.64 68.44 78.28 82.42 0.91M 0.166×\nTable 8: Comparative analysis with pruned vision transformers on CIFAR-10 dataset. Three best accuracy (%) values are marked in bold.\nthe relative order of the self-attention results after quantiza-\ntion. Yuan et al. (2022) develop the e fficient PTQ4ViT frame-\nwork, employing twin uniform quantization to handle the spe-\ncial distributions of post-softmax and post-GELU activations.\nFQ-ViT (Lin et al., 2022) is then implemented under the PTQ,\na training-free paradigm, by introducing the power-of-two fac-\ntor (PTF) and log-int-softmax (LIS) quantization methods for\nLayerNorm and Softmax modules, which are not quantized in\nPTQ4ViT. In comparison to the adaptation of vision transform-\ners to prior PTQ-based quantization schemes, including Min-\nMax, EMA (Jacob et al., 2018), Percentile (Li et al., 2019),\nOMSE (Choukroun et al., 2019), and PTQ4ViT (Yuan et al.,\n2022), FQ-ViT achieves a nearly lossless quantization perfor-\nmance with full-precision models. On the other hand, Li et al.\n(2022) analyze the quantization robustness of each component\nin the vision transformer to implement a fully quantized ViT\nbaseline under the straightforward QAT pipeline. The Q-ViT is\nthen proposed by introducing an information rectification mod-\nule (IRM) and a distribution-guided distillation (DGD) scheme\ntowards accurate and fully quantized vision transformers. Ex-\nperimental results on the ImageNet (Deng et al., 2009) dataset\nshow that Q-ViTs outperform the baselines and achieve com-\npetitive performance with the full-precision counterparts while\nsignificantly reducing memory and computational resources.\nIn Table 7, we present a comprehensive comparison of our\nproposed OnDev-LCTs with various low-bit transformers (i.e.,\nViT-S backbone) in terms of accuracy, inference time, and num-\nber of parameters for image classification tasks on CIFAR-\n10 (Krizhevsky et al., 2009) and FEMNIST (Caldas et al., 2018)\ndatasets. To ensure a fair comparison, we implement Q-ViT\nwithout the DGD scheme, which relies on the performance of\na powerful teacher model. All models are trained from scratch\nwithout pre-training or applying data augmentation techniques\nand learning rate schedulers. The results clearly show that\nour proposed OnDev-LCTs outperform the low-bit vision trans-\nformers in all cases. Nonetheless, we maintain the perspective\nthat the performance of low-bit vision transformers can also\nbe significantly improved by leveraging a powerful backbone\nmodel along with a proper configuration of data augmentation\nand learning rate scheduler while being smaller and faster than\ntraditional vision transformers. However, it is worth noting that\nlow-bit vision transformers can be more complex to train and\nmay not be as effective for certain tasks. As a result, the choice\nof whether to use our OnDev-LCT or a low-bit vision trans-\nformer may depend on the specific task and application.\nComparative analysis with vision transformer pruning. In this\nsection, we delve into prior research on vision transformer\npruning, which is an emerging study within the broader domain\nof model compression and optimization. This process system-\natically identifies and removes unnecessary parameters, such\nas unimportant weights or neurons, from a pre-trained vision\ntransformer, guided by various criteria, including weight mag-\nnitude (LeCun et al., 1989; Han et al., 2016; Liu et al., 2017),\ngradient magnitude (Molchanov et al., 2019, 2016), activation\nvalues (Chen et al., 2021), or importance scores (Zhu et al.,\n2021). The primary goal is to create a more compact vision\ntransformer that maintains reasonable performance, making it\nsuitable for deployment on resource-constrained devices where\ncomputational e fficiency is crucial. One notable approach,\n11\nModel\nCIFAR-10 FEMNIST\n(C=10) #ParamsCentralized FedAvg with 50 clients\nβ=0.1 β=0.5 β=5\nEdgeNeXt-XXS 58.17 36.22 45.24 53.16 63.65 1.17M\nMobileViT-XXS 52.91 28.30 45.91 49.37 75.37 0.94M\nMobileViTv2-0.5 52.84 28.28 48.95 51.80 75.64 1.13M\nCCT-2/2 79.71 55.67 68.87 73.45 80.71 0.28M\nCCT-4/2 80.92 56.05 69.82 74.52 81.28 0.48M\nHybrid Vision Transformer (ViT) 74.64 45.22 65.70 70.93 80.25 0.75M\nHybrid Vision Performer (ViP) 77.52 43.87 65.53 70.89 80.37 0.76M\nHybrid Vision Linformer (ViL) 50.68 31.83 49.01 57.91 76.65 0.95M\nHybrid Vision Nystr¨oformer (ViN) 77.35 51.25 63.81 68.56 81.31 0.76M\nOnDev-LCT-1/1 84.55 66.18 76.93 79.71 80.68 0.21M\nOnDev-LCT-2/1 86.27 68.25 78.20 80.55 81.43 0.31M\nOnDev-LCT-4/1 86.61 67.24 77.68 80.87 81.84 0.51M\nOnDev-LCT-8/1 86.64 68.44 78.28 82.42 82.02 0.91M\nTable 9: Comparative analysis with convolutional transformers on CIFAR-10 and FEMNIST datasets. Each model variant is trained from scratch without\napplying data augmentation or learning rate schedulers. Three best accuracy (%) values are marked in bold.\nModel Pre-trained\nCIFAR-10 FEMNIST\n(C=10) #ParamsCentralized FedAvg with 50 clients\nβ=0.1 β=0.5 β=5\nMobileNetv2/1.0 ✓ 82.22 49.85 80.74 84.00 79.53 2.30M\nMobileNetv2/1.0 ✗ 36.76 31.34 47.92 50.72 71.81 2.30M\nViT-Tiny ✓ 67.51 48.82 64.09 68.20 61.77 5.49M\nViT-Tiny ✗ 55.86 36.25 47.85 50.76 57.10 5.49M\nMobileViT-XXS ✓ 81.27 56.75 79.12 82.77 81.04 0.94M\nMobileViT-XXS ✗ 52.91 28.30 45.91 49.37 75.37 0.94M\nMobileViTv2-0.5 ✓ 82.69 44.06 80.52 83.13 80.65 1.13M\nMobileViTv2-0.5 ✗ 52.84 28.28 48.95 51.80 75.64 1.13M\nOnDev-LCT-8/1 ✗ 86.64 68.44 78.28 82.42 82.02 0.91M\nOnDev-LCT-8/3 ✗ 87.65 68.56 78.56 82.70 82.75 0.95M\nTable 10: Comparative analysis with ImageNet pre-trained models on CIFAR-10 and FEMNIST datasets. Three best accuracy (%) values are marked in bold.\nVTP (Zhu et al., 2021), extends the principles of network slim-\nming (Liu et al., 2017) to reduce the number of embedding di-\nmensions through the introduction of learnable coefficients that\nevaluate importance scores. Neurons with small coe fficients\nare removed based on a predefined threshold. However, it’s\nworth noting that VTP requires manual tuning of thresholds for\nall layers and subsequent model fine-tuning. Another method,\nWDPruning (Yu et al., 2022), is a structured pruning technique\nfor vision transformers, which employs a 0 /1 mask to di ffer-\nentiate unimportant and important parameters based on their\nmagnitudes. While it uses a di fferentiable threshold, the non-\ndifferentiable mask may introduce gradient bias, potentially re-\nsulting in suboptimal weight retention. X-Pruner (Yu and Xi-\nang, 2023) focuses on removing less contributing units, where\neach prunable unit is assigned an explainability-aware mask to\nquantify its contribution to predicting each class in terms of\nexplainability. Finally, DepGraph (Fang et al., 2023) serves\nas a grouping algorithm used to analyze dependencies in net-\nworks for enabling any structural pruning over various network\narchitectures. Nonetheless, while vision transformer pruning\ncan yield smaller and faster models, the challenge remains in\nfinding the right balance between reducing model size and pre-\nserving task-specific features.\nTable 8 provides a performance comparison between our\nproposed OnDev-LCTs and pruned vision transformers on the\nCIFAR-10 (Krizhevsky et al., 2009) dataset. We focus on key\nmetrics such as accuracy and the reduction in parameters com-\npared to the ViT-Tiny (Dosovitskiy et al., 2021) backbone. We\nfollowed the Torch-Pruning (TP) implementation for structural\npruning, which employs the DepGraph algorithm to physically\nremove parameters (Fang et al., 2023). Several important cri-\nteria are considered, including L-p Norm, Taylor, and Random.\nThe results demonstrate that our proposed OnDev-LCTs con-\nsistently outperform the pruned ViT models in all scenarios.\nFrom one point of view, developing effective pruning strategies\ncan be a complex task, often involving iterative trial and error to\nidentify the optimal model components for pruning. Aggressive\npruning carries the risk of significant model generalization loss,\nas removing too many parameters may lead to underfitting and\nreduced performance. In summary, vision transformer pruning\nis a technique applied to existing large models to reduce their\nsize. On the other hand, OnDev-LCT provides an e fficient and\nlightweight neural network architecture, offering an alternative\nsolution for resource-constrained on-device vision applications.\nComparative analysis with convolutional transformers. In this\nanalysis, we conduct a comparative study between our OnDev-\nLCTs and other convolutional transformers that do not rely on\nheavy data augmentation techniques. All models are trained\nfrom scratch without utilizing data augmentation or learning\nrate schedulers. It is important to note that while CCTs (Hassani\net al., 2021) require high-level data augmentation techniques to\n12\nModel Centralized\nTop-1 Acc\nCentralized\nTop-5 Acc\nFedAvg with 10 clientsFedAvg with 50 clients #Params MACsβ=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5\nResNet-20 26.99 50.38 13.91 22.18 26.13 9.08 16.64 22.09 0.34M 0.04G\nResNet-32 27.92 51.98 14.78 22.82 27.03 10.23 17.89 21.84 0.53M 0.07G\nResNet-44 28.12 52.35 15.16 23.45 27.67 11.32 17.25 23.04 0.73M 0.10G\nResNet-56 28.18 52.57 15.03 23.56 27.93 12.45 18.42 23.06 0.93M 0.13G\nMobileNetv2/0.5 18.44 39.22 11.16 18.87 20.28 8.90 14.09 16.19 1.99M <0.01G\nMobileNetv2/0.75 20.66 42.72 16.27 23.60 25.58 10.63 15.85 18.04 2.66M <0.01G\nMobileNetv2/1.0 29.84 54.60 16.58 24.78 27.76 11.00 16.92 20.14 3.54M 0.01G\nViT-Lite-1/4 11.76 27.39 4.85 8.53 10.61 3.42 7.26 10.18 1.46M 0.04G\nViT-Lite-2/4 12.94 30.31 5.55 9.35 11.73 3.81 8.00 11.08 1.48M 0.07G\nCCT-2/2 26.97 50.04 14.07 22.71 27.43 10.26 19.16 24.14 0.40M 0.03G\nCCT-4/2 30.27 53.89 15.82 25.21 29.99 11.20 20.92 26.46 0.60M 0.05G\nOnDev-LCT-1/1 38.46 62.95 20.91 32.01 36.93 13.87 24.68 31.07 0.34M 0.03G\nOnDev-LCT-2/1 40.59 65.53 21.78 32.93 38.59 14.84 26.23 32.39 0.44M 0.04G\nOnDev-LCT-4/1 42.34 67.33 23.27 34.66 39.98 15.54 27.28 33.77 0.64M 0.05G\nOnDev-LCT-8/1 43.37 68.30 24.87 35.23 41.03 16.32 28.10 35.84 1.04M 0.08G\nTable 11: Performance analysis on ImageNet-32 dataset. Three best accuracy (%) values are marked in bold.\nachieve their best performance, we include them in our com-\nparison to provide a complete evaluation of existing lightweight\nconvolutional transformers. Based on the reported values in Ta-\nble 9, our models outperform the other benchmarks, demon-\nstrating a significant performance gap compared to variants\nwith similar sizes. This highlights the superior performance and\nefficiency of our proposed architecture in terms of parameters\nand computation cost. In contrast to other methods that com-\nbine convolutions and transformers, the distinguishing factor of\nour OnDev-LCT lies in its unique and e fficient LCT tokenizer\ndesign, effectively leveraging depthwise separable convolutions\nand residual linear bottleneck blocks. By integrating the LCT\ntokenizer before the transformer encoder, we introduce image-\nspecific inductive biases to our OnDev-LCT, enabling it to cap-\nture essential local details from image representations. Further-\nmore, the LCT encoder’s MHSA mechanism aids in learning\nglobal representations of images. This combination of comple-\nmentary components allows our model to achieve high accuracy\nwhile maintaining its lightweight nature, making it well-suited\nfor on-device vision tasks with limited resources.\nComparative analysis with pre-trained models. In this analy-\nsis, we evaluate the performance of our OnDev-LCTs in com-\nparison to variants of existing state-of-the-art models that are\npre-trained on the ImageNet (Deng et al., 2009) dataset. We\nimplement all the pre-trained models using the PyTorch (Paszke\net al., 2019) framework, following the o fficial implementation\nprovided by the Py Torch Image Models (timm) (Wightman,\n2019) collection. According to the results shown in Tabel 10,\nour OnDev-LCTs achieve competitive performance even with\nlimited training data, illustrating their robustness and effective-\nness for on-device vision tasks. While pre-trained models have\nshown promising performance across various vision tasks, their\npracticality in resource-constrained on-device scenarios is of-\nten hindered by challenges related to accessing and utilizing\nlarge pre-training datasets. Such datasets may not be readily\navailable in many domains (Varoquaux and Cheplygina, 2022;\nParekh et al., 2022; Mutis and Ambekar, 2020; Ghassemi et al.,\n2020) due to privacy concerns or data distribution heterogene-\nity across devices. Since pre-trained models are designed and\nModel Augment Augment+\nLR SchedulerBaseline #Params\nOnDev-LCT-1/1 90.04 91.05 84.55 0.21M\nOnDev-LCT-8/1 90.36 91.14 86.64 0.91M\nOnDev-LCT-1/3 90.25 91.41 85.73 0.25M\nOnDev-LCT-8/3 90.50 91.99 87.65 0.95M\nTable 12: Performance analysisof our OnDev-LCT architecture on CIFAR-10\ndataset for the impact of simple data augmentation and learning rate scheduler.\noptimized for specific datasets and objectives, which may not\nclosely align with the target domain, fine-tuning a pre-trained\nmodel on downstream tasks may not always be straightfor-\nward for on-device scenarios with limited training data and re-\nsources. Despite not relying on extensive pre-training datasets,\nour OnDev-LCTs leverage depthwise separable convolutions\nand MHSA mechanism to e fficiently capture both local and\nglobal features from images, making them well-suited for on-\ndevice vision tasks with limited data. Thus, the unique design\nof our OnDev-LCT o ffers a practical and e ffective alternative\nto pre-train-and-fine-tune schemes, providing more robust and\nefficient solutions for on-device vision tasks in FL scenarios,\nwhere pre-training datasets may not be readily available.\nAnalysis on ImageNet-32 dataset. Table 11 compares the cen-\ntralized and FL performance of our proposed models for im-\nage classification on ImageNet-32, the down-sampled version\nof the original ImageNet-1K (Deng et al., 2009) dataset, which\ncontains 1,281,167 training samples and 50,000 test samples of\n32×32 RGB images in 1,000 classes. We apply the same hyper-\nparameter settings and training strategy used with the CIFAR-\n100 dataset and train all models from scratch without data aug-\nmentation and learning rate schedulers. According to the re-\nported values, our models outperform the other baselines, and\nthere is a significant gap between their performance and that of\nvariants with comparable sizes. Thus, we can summarize that\nour proposed architecture is better in performance and also ef-\nficient in terms of parameters and computation cost.\nAnalysis on the impact of data augmentation and learning\nrate scheduler. In this experiment, we analyze the improved\n13\nModel FedAvg with 10 clients FedAvg with 50 clients\nMNIST FashionMNIST EMNISTBalanced MNIST FashionMNIST EMNISTBalancedβ=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5 β=0.1 β=0.5 β=5\nResNet-20 98.99 99.50 99.58 81.33 91.83 92.75 81.47 88.46 89.68 97.65 99.46 99.54 70.63 90.71 92.06 82.98 88.64 89.50ResNet-32 99.13 99.49 99.59 82.23 92.04 92.77 83.18 89.23 89.64 97.38 99.42 99.51 72.24 90.75 92.12 83.70 88.78 89.51ResNet-44 98.99 99.51 99.57 82.27 92.15 92.73 83.86 88.99 89.88 97.85 99.44 99.52 71.51 91.01 92.04 83.90 88.74 89.61\nMobileNetv2/0.2 98.31 99.02 99.26 72.01 89.49 89.87 67.78 84.97 83.94 94.16 98.64 98.77 46.82 86.62 87.59 69.66 80.64 80.69MobileNetv2/0.5 98.71 99.24 99.32 75.03 90.15 90.62 73.37 85.70 86.90 96.85 98.97 98.93 50.20 87.41 89.58 75.46 83.87 80.07\nViT-Lite-1/8 93.58 97.95 98.10 73.90 87.82 88.79 57.96 81.02 84.00 83.33 96.89 97.23 57.54 82.33 85.17 58.55 77.72 80.24ViT-Lite-2/8 93.71 97.99 98.11 75.74 87.89 88.82 58.92 81.10 84.10 84.19 97.01 97.31 58.97 82.17 85.42 58.86 77.85 80.26\nCCT-2/2 98.45 99.29 99.41 81.90 90.42 91.33 82.74 87.94 89.39 95.56 99.03 99.18 72.44 88.07 89.75 81.77 86.87 88.45CCT-4/2 98.47 99.34 99.45 82.80 90.74 91.53 83.64 88.32 89.53 96.49 99.07 99.34 73.82 87.30 89.98 82.77 87.47 88.69\nOnDev-LCT-1/1 98.97 99.43 99.53 82.75 92.03 92.85 82.32 88.96 89.95 97.92 99.29 99.38 75.30 90.92 91.88 82.13 87.92 89.34OnDev-LCT-2/1 99.18 99.46 99.52 83.12 92.07 92.97 84.43 89.06 90.06 97.85 99.37 99.37 78.56 90.78 92.21 82.98 88.04 89.58OnDev-LCT-4/1 99.07 99.50 99.54 83.71 92.26 93.13 85.96 89.25 90.32 98.21 99.38 99.36 78.24 90.92 92.03 84.15 88.32 89.64OnDev-LCT-8/1 99.10 99.43 99.59 83.42 92.37 93.15 86.99 89.48 90.35 97.87 99.28 99.43 78.82 90.84 91.94 84.56 88.84 89.88\nTable 13: Comparison on image classification performance in FL under different degrees of data heterogeneity. All reported accuracy (%) values are the best of\n3 runs. All experiments were conducted without applying any data augmentation or learning rate schedulers. Three best accuracy values are marked in bold.\n16 28 32 48 56 64\nImage size\n76.0\n77.5\n79.0\n80.5\n82.0\n83.5\n85.0\n86.5\n88.0Accuracy (%)\nOnDev-LCT-1/1\nOnDev-LCT-1/3\nOnDev-LCT-8/1\nOnDev-LCT-8/3\nFigure 8: Accuracy (%) versus image size on CIFAR-10.\nperformance of our proposed OnDev-LCTs on the CIFAR-\n10 (Krizhevsky et al., 2009) dataset by applying simple data\naugmentation and learning rate schedulers. We conduct exper-\niments in the centralized scenario by selecting the smallest and\nlargest model variants of our OnDev-LCT with di fferent num-\nbers of standard convolutions in the LCT tokenizer. First, we\napply simple data augmentation techniques, including horizon-\ntal flip, random rotation, and zoom. As shown in Table 12, our\nmodels significantly improve accuracy even with simple aug-\nmentations applied to the training data. Then, we set a linear\nlearning rate warm-up for the initial 10 epochs, followed by a\ngradual reduction of the learning rate per epoch using cosine\nannealing (Loshchilov and Hutter, 2017b). The performance is\nfurther enhanced, demonstrating that a proper configuration of\ndata augmentation and learning rate scheduler can e ffectively\nimprove the efficiency of our OnDev-LCT when implementing\nit in real-world applications.\nAnalysis on the impact of input image resolution. Here, we\nanalyze the performance of our OnDev-LCTs on various im-\nage sizes using the CIFAR-10 (Krizhevsky et al., 2009) dataset.\nWe conduct this analysis by downsampling the images to two\nsmaller sizes, i.e., 16×16, 28 ×28 or upsampling the images to\nthree larger sizes, i.e., 48×48, 56 ×56, and 64 ×64. From Fig-\nure 8, we can observe that our models still perform well with\nsmaller images, even without data augmentation. Thus, we can\ninfer that by incorporating inductive biases via the LCT tok-\n1 2 4 8 12 16\nAttention heads\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0Accuracy (%)\nOnDev-LCT-1/1\nOnDev-LCT-1/3\nOnDev-LCT-8/1\nOnDev-LCT-8/3\nFigure 9: Accuracy (%) versus number of attention heads on CIFAR-10.\nenizer, our OnDev-LCTs can maintain the spatial information\nof images, which helps in learning better image representations.\nAnalysis on the impact of multi-head self-attention. In this ex-\nperiment, we investigate the impact of varying the number of\nattention heads in the LCT encoder on the performance of our\nOnDev-LCTs. Multiple attention heads in the MHSA block of\nan LCT encoder help in parallel processing, giving our OnDev-\nLCT greater power to encode multiple relationships between\nimage patches. Figure 9 demonstrates the performance of our\nOnDev-LCTs with different numbers of attention heads. Obvi-\nously, small variants of our models perform better with more\nattention heads, but larger variants gradually drop in accuracy\nwhen using 12 and 16 attention heads. Still, all our models pre-\nserve their performance even with a single attention head.\nAnalysis on various FL settings. Image classification perfor-\nmance of our proposed OnDev-LCTs on the MNIST, Fashion-\nMNIST, and EMNIST-Balanced datasets, compared to the other\nbaselines under various data heterogeneity settings, is shown in\nTable 13. The reported values are the best median values from\nthe last 5 FL rounds of 3 separate runs. Our findings indicate\nthat OnDev-LCTs outperform the competition in most situa-\ntions. Figure 10 shows the accuracy versus the number of FL\nrounds on the CIFAR-10 dataset with varying degrees of data\nheterogeneity for both 10-client and 50-client scenarios; thus,\nour proposed models outperform the other baselines in all cases.\nEspecially for higher β values, OnDev-LCT variants converge\n14\n0 10 20 30 40 50 60\nFL rounds\n10\n20\n30\n40\n50\n60\n70\n80\n90Accuracy (%)\nOnDev-LCT-1/1\nOnDev-LCT-2/1\nOnDev-LCT-4/1\nOnDev-LCT-8/1\nMobileNetv2/0.5\nMobileNetv2/0.2\nResnet-20\nResnet-32\nResnet-44\nViT-Lite-1/8\nViT-Lite-2/8\nCCT-2/2\nCCT-4/2\n(a) For 10 clients (β= 0.1)\n0 10 20 30 40 50 60\nFL rounds\n10\n20\n30\n40\n50\n60\n70\n80\n90Accuracy (%) (b) For 10 clients (β= 0.5)\n0 10 20 30 40 50 60\nFL rounds\n10\n20\n30\n40\n50\n60\n70\n80\n90Accuracy (%) (c) For 10 clients (β= 5)\n0 20 40 60 80 100\nFL rounds\n10\n20\n30\n40\n50\n60\n70\n80\n90Accuracy (%)\n(d) For 50 clients (β= 0.1)\n0 20 40 60 80 100\nFL rounds\n10\n20\n30\n40\n50\n60\n70\n80\n90Accuracy (%) (e) For 50 clients (β= 0.5)\n0 20 40 60 80 100\nFL rounds\n10\n20\n30\n40\n50\n60\n70\n80\n90Accuracy (%) (f) For 50 clients (β= 5)\nFigure 10: Accuracy (%) versus FL rounds on CIFAR-10 test set under different degrees of data heterogeneity. Best viewed in color.\n1 5 10\nLocal epochs\n50\n55\n60\n65\n70\n75\n80Accuracy (%)\nOnDev-LCT-1/1 (10 clients)\nOnDev-LCT-2/1 (10 clients)\nOnDev-LCT-4/1 (10 clients)\nOnDev-LCT-8/1 (10 clients)\nOnDev-LCT-1/1 (50 clients)\nOnDev-LCT-2/1 (50 clients)\nOnDev-LCT-4/1 (50 clients)\nOnDev-LCT-8/1 (50 clients)\n(a) β= 0.1\n1 5 10\nLocal epochs\n70\n72\n74\n76\n78\n80\n82\n84Accuracy (%)\n (b) β= 0.5\n1 5 10\nLocal epochs\n78\n80\n82\n84\n86Accuracy (%)\n (c) β= 5\nFigure 11: Accuracy (%) versus the number of local epochs on CIFAR-10 test set under different degrees of data heterogeneity. Best viewed in color.\nfaster than the other baselines with a significant performance\ngap in between. Figure 11 depicts the impact of local epochs\nin each FL round for the CIFAR-10 dataset. When we increase\nthe local epoch from 1 to 5, the performance of our models dra-\nmatically improves, but just a minor gain when increasing to\n10. As a result, we set the default number of local epochs to 5.\n7. Conclusion\nIn this study, we propose a novel design, namely OnDev-\nLCT, which introduces inductive biases of CNNs to the vi-\nsion transformer by incorporating an early convolutional stem\nbefore the transformer encoder. We leverage e fficient depth-\nwise separable convolutions to build residual linear bottleneck\nblocks of the LCT tokenizer for extracting local features from\nimages. The LCT encoder enables our models to learn global\nrepresentations of images via multi-head self-attention. Com-\nbining the strengths of convolutions and attention mechanisms,\nwe design our OnDev-LCTs for on-device vision tasks with\nlimited training data and resources. We conduct extensive ex-\nperiments to analyze the e fficiency of our proposed OnDev-\nLCTs on various image classification tasks compared to the\npopular lightweight vision models. Centralized experiments\non five benchmark image datasets show the superiority of our\nOnDev-LCTs over the other baselines. Furthermore, extensive\nFL experiments under various settings indicate the e fficacy of\nour OnDev-LCTs in dealing with data heterogeneity and com-\nmunication bottlenecks since our models significantly outper-\nform the other baselines while having fewer parameters and\nlower computational demands. We advocate that the proper\nconfiguration of data augmentations and learning rate sched-\nulers could further improve the performance of our OnDev-\nLCT when implementing it in real-world low-data scenarios.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG.S., Davis, A., Dean, J., Devin, M., et al., 2015. Tensorflow: Large-scale\nmachine learning on heterogeneous systems.\nAgarap, A.F., 2018. Deep learning using rectified linear units (relu). arXiv\npreprint arXiv:1803.08375 .\n15\nBello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V ., 2019. Attention aug-\nmented convolutional networks, in: Proceedings of the IEEE /CVF interna-\ntional conference on computer vision, pp. 3286–3295.\nBlalock, D., Gonzalez Ortiz, J.J., Frankle, J., Guttag, J., 2020. What is the state\nof neural network pruning? Proceedings of machine learning and systems\n2, 129–146.\nCaldas, S., Duddu, S.M.K., Wu, P., Li, T., Koneˇcn`y, J., McMahan, H.B., Smith,\nV ., Talwalkar, A., 2018. Leaf: A benchmark for federated settings. arXiv\npreprint arXiv:1812.01097 .\nChen, D., Gao, D., Kuang, W., Li, Y ., Ding, B., 2022a. pfl-bench: A compre-\nhensive benchmark for personalized federated learning. Advances in Neural\nInformation Processing Systems 35, 9344–9360.\nChen, T., Cheng, Y ., Gan, Z., Yuan, L., Zhang, L., Wang, Z., 2021. Chasing\nsparsity in vision transformers: An end-to-end exploration. Advances in\nNeural Information Processing Systems 34, 19974–19988.\nChen, Y ., Dai, X., Chen, D., Liu, M., Dong, X., Yuan, L., Liu, Z., 2022b.\nMobile-former: Bridging mobilenet and transformer, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n5270–5279.\nChoromanski, K.M., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos,\nT., Hawkins, P., Davis, J.Q., Mohiuddin, A., Kaiser, L., et al., 2020. Re-\nthinking attention with performers, in: International Conference on Learn-\ning Representations.\nChoukroun, Y ., Kravchik, E., Yang, F., Kisilev, P., 2019. Low-bit quantiza-\ntion of neural networks for e fficient inference, in: 2019 IEEE /CVF Inter-\nnational Conference on Computer Vision Workshop (ICCVW), IEEE. pp.\n3009–3018.\nCohen, G., Afshar, S., Tapson, J., Van Schaik, A., 2017. Emnist: Extending\nmnist to handwritten letters, in: 2017 international joint conference on neu-\nral networks (IJCNN), IEEE. pp. 2921–2926.\nDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Imagenet:\nA large-scale hierarchical image database, in: 2009 IEEE conference on\ncomputer vision and pattern recognition, Ieee. pp. 248–255.\nDeng, L., 2012. The mnist database of handwritten digit images for machine\nlearning research [best of the web]. IEEE signal processing magazine 29,\n141–142.\nDing, Y ., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., Liu, X., 2022. Towards\naccurate post-training quantization for vision transformer, in: Proceedings\nof the 30th ACM International Conference on Multimedia, pp. 5380–5388.\nDong, X., Chen, S., Pan, S., 2017. Learning to prune deep neural networks via\nlayer-wise optimal brain surgeon. Advances in neural information process-\ning systems 30.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-\nterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,\nJ., Houlsby, N., 2021. An image is worth 16x16 words: Transformers for\nimage recognition at scale, in: International Conference on Learning Repre-\nsentations. URL: https://openreview.net/forum?id=YicbFdNTTy.\nFang, G., Ma, X., Song, M., Mi, M.B., Wang, X., 2023. Depgraph: Towards\nany structural pruning, in: Proceedings of the IEEE /CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16091–16101.\nFerguson, T.S., 1973. A bayesian analysis of some nonparametric problems.\nThe annals of statistics , 209–230.\nFrankle, J., Dziugaite, G.K., Roy, D.M., Carbin, M., 2021. Pruning neural net-\nworks at initialization: Why are we missing the mark?, in: 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021, OpenReview.net. URL: https://openreview.net/\nforum?id=Ig-VyQc-MLK.\nGhassemi, M., Naumann, T., Schulam, P., Beam, A.L., Chen, I.Y ., Ranganath,\nR., 2020. A review of challenges and opportunities in machine learning for\nhealth. AMIA Summits on Translational Science Proceedings 2020, 191.\nGraham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J´egou, H., Douze,\nM., 2021. Levit: a vision transformer in convnet’s clothing for faster infer-\nence, in: Proceedings of the IEEE /CVF international conference on com-\nputer vision, pp. 12259–12269.\nHan, K., Wang, Y ., Tian, Q., Guo, J., Xu, C., Xu, C., 2020. Ghostnet: More fea-\ntures from cheap operations, in: Proceedings of the IEEE /CVF conference\non computer vision and pattern recognition, pp. 1580–1589.\nHan, S., Mao, H., Dally, W.J., 2016. Deep compression: Compressing deep\nneural networks with pruning, trained quantization and huffman coding. In-\nternational Conference on Learning Representations (ICLR) .\nHassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., Shi, H., 2021. Es-\ncaping the big data paradigm with compact transformers. arXiv preprint\narXiv:2104.05704 .\nHassibi, B., Stork, D.G., Wol ff, G.J., 1993. Optimal brain surgeon and gen-\neral network pruning, in: IEEE international conference on neural networks,\nIEEE. pp. 293–299.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image\nrecognition, in: Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 770–778.\nHendrycks, D., Gimpel, K., 2016. Gaussian error linear units (gelus). arXiv\npreprint arXiv:1606.08415 .\nHoward, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W.,\nZhu, Y ., Pang, R., Vasudevan, V ., et al., 2019. Searching for mobilenetv3,\nin: Proceedings of the IEEE /CVF International Conference on Computer\nVision, pp. 1314–1324.\nHoward, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,\nAndreetto, M., Adam, H., 2017. Mobilenets: E fficient convolutional neural\nnetworks for mobile vision applications. arXiv preprint arXiv:1704.04861 .\nHsu, T.M.H., Qi, H., Brown, M., 2019. Measuring the e ffects of non-\nidentical data distribution for federated visual classification. arXiv preprint\narXiv:1909.06335 .\nHu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks, in: Proceed-\nings of the IEEE conference on computer vision and pattern recognition, pp.\n7132–7141.\nIandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.,\n2016. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡\n0.5 mb model size. arXiv preprint arXiv:1602.07360 .\nJacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,\nKalenichenko, D., 2018. Quantization and training of neural networks for\nefficient integer-arithmetic-only inference, in: Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pp. 2704–2713.\nJeevan, P., Sethi, A., 2022. Resource-e fficient hybrid x-formers for vision,\nin: Proceedings of the IEEE /CVF Winter Conference on Applications of\nComputer Vision, pp. 2982–2990.\nKarimireddy, S.P., Kale, S., Mohri, M., Reddi, S., Stich, S., Suresh, A.T., 2020.\nScaffold: Stochastic controlled averaging for federated learning, in: Interna-\ntional Conference on Machine Learning, PMLR. pp. 5132–5143.\nKhan, A., Sohail, A., Zahoora, U., Qureshi, A.S., 2020. A survey of the recent\narchitectures of deep convolutional neural networks. Artificial intelligence\nreview 53, 5455–5516.\nKhan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021.\nTransformers in vision: A survey. ACM Computing Surveys (CSUR) .\nKingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980 .\nKrizhevsky, A., Hinton, G., et al., 2009. Learning multiple lay-\ners of features from tiny images. Technical report, University\nof Toronto, 2009 URL: https://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf .\nKrizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with\ndeep convolutional neural networks. Advances in neural information pro-\ncessing systems 25.\nLeCun, Y ., Bengio, Y ., Hinton, G., 2015. Deep learning. nature 521, 436–444.\nLeCun, Y ., Denker, J., Solla, S., 1989. Optimal brain damage. Advances in\nneural information processing systems 2.\nLenc, K., Vedaldi, A., 2015. Understanding image representations by measur-\ning their equivariance and equivalence, in: Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pp. 991–999.\nLi, Q., He, B., Song, D., 2021a. Model-contrastive federated learning, in:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 10713–10722.\nLi, R., Wang, Y ., Liang, F., Qin, H., Yan, J., Fan, R., 2019. Fully quantized\nnetwork for object detection, in: Proceedings of the IEEE /CVF conference\non computer vision and pattern recognition, pp. 2810–2819.\nLi, T., Sahu, A.K., Talwalkar, A., Smith, V ., 2020a. Federated learning: Chal-\nlenges, methods, and future directions. IEEE Signal Processing Magazine\n37, 50–60.\nLi, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V ., 2020b.\nFederated optimization in heterogeneous networks. Proceedings of Machine\nLearning and Systems 2, 429–450.\nLi, Y ., Chen, Y ., Dai, X., Chen, D., Liu, M., Yuan, L., Liu, Z., Zhang, L.,\nVasconcelos, N., 2021b. Micronet: Improving image recognition with ex-\ntremely low flops, in: Proceedings of the IEEE /CVF International confer-\n16\nence on computer vision, pp. 468–477.\nLi, Y ., Xu, S., Zhang, B., Cao, X., Gao, P., Guo, G., 2022. Q-vit: Accurate\nand fully quantized low-bit vision transformer, in: Oh, A.H., Agarwal, A.,\nBelgrave, D., Cho, K. (Eds.), Advances in Neural Information Processing\nSystems. URL: https://openreview.net/forum?id=fU-m9kQe0ke.\nLi, Z., Liu, F., Yang, W., Peng, S., Zhou, J., 2021c. A survey of convolutional\nneural networks: analysis, applications, and prospects. IEEE transactions\non neural networks and learning systems .\nLin, T.Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll ´ar,\nP., Zitnick, C.L., 2014. Microsoft coco: Common objects in context, in:\nEuropean conference on computer vision, Springer. pp. 740–755.\nLin, Y ., Zhang, T., Sun, P., Li, Z., Zhou, S., 2022. Fq-vit: Post-training quanti-\nzation for fully quantized vision transformer, in: Proceedings of the Thirty-\nFirst International Joint Conference on Artificial Intelligence, IJCAI-22, pp.\n1173–1179.\nLiu, S.Y ., Liu, Z., Cheng, K.T., 2023. Oscillation-free quantization for low-bit\nvision transformers. arXiv preprint arXiv:2302.02210 .\nLiu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C., 2017. Learning efficient\nconvolutional networks through network slimming, in: Proceedings of the\nIEEE international conference on computer vision, pp. 2736–2744.\nLiu, Z., Wang, Y ., Han, K., Zhang, W., Ma, S., Gao, W., 2021. Post-training\nquantization for vision transformer. Advances in Neural Information Pro-\ncessing Systems 34, 28092–28103.\nLoshchilov, I., Hutter, F., 2017a. Decoupled weight decay regularization. arXiv\npreprint arXiv:1711.05101 .\nLoshchilov, I., Hutter, F., 2017b. SGDR: Stochastic gradient descent with warm\nrestarts, in: International Conference on Learning Representations. URL:\nhttps://openreview.net/forum?id=Skq89Scxx.\nvan der Maaten, L., Hinton, G., 2008. Visualizing data using t-sne. Journal\nof Machine Learning Research 9, 2579–2605. URL: http://jmlr.org/\npapers/v9/vandermaaten08a.html.\nMaaz, M., Shaker, A., Cholakkal, H., Khan, S., Zamir, S.W., Anwer,\nR.M., Shahbaz Khan, F., 2022. Edgenext: e fficiently amalgamated cnn-\ntransformer architecture for mobile vision applications, in: European Con-\nference on Computer Vision, Springer. pp. 3–20.\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A., 2017.\nCommunication-efficient learning of deep networks from decentralized data,\nin: Artificial intelligence and statistics, PMLR. pp. 1273–1282.\nMehta, S., Rastegari, M., 2022. Mobilevit: Light-weight, general-purpose,\nand mobile-friendly vision transformer, in: International Conference on\nLearning Representations. URL: https://openreview.net/forum?id=\nvh-0sUt8HlG.\nMehta, S., Rastegari, M., 2023. Separable self-attention for mobile vision\ntransformers. Transactions on Machine Learning Research URL: https:\n//openreview.net/forum?id=tBl4yBEjKi.\nMenghani, G., 2023. Efficient deep learning: A survey on making deep learning\nmodels smaller, faster, and better. ACM Computing Surveys 55, 1–37.\nMitchell, B.R., et al., 2017. The spatial inductive bias of deep learning. Ph.D.\nthesis. Johns Hopkins University.\nMolchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J., 2019. Importance\nestimation for neural network pruning, in: Proceedings of the IEEE /CVF\nconference on computer vision and pattern recognition, pp. 11264–11272.\nMolchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., 2016. Pruning con-\nvolutional neural networks for resource e fficient inference. arXiv preprint\narXiv:1611.06440 .\nMutis, I., Ambekar, A., 2020. Challenges and enablers of augmented reality\ntechnology for in situ walkthrough applications. J. Inf. Technol. Constr. 25,\n55–71.\nNeuman, S.M., Plancher, B., Duisterhof, B.P., Krishnan, S., Banbury, C.,\nMazumder, M., Prakash, S., Jabbour, J., Faust, A., de Croon, G.C., et al.,\n2022. Tiny robot learning: challenges and directions for machine learning in\nresource-constrained robots, in: 2022 IEEE 4th International Conference on\nArtificial Intelligence Circuits and Systems (AICAS), IEEE. pp. 296–299.\nParekh, D., Poddar, N., Rajpurkar, A., Chahal, M., Kumar, N., Joshi, G.P.,\nCho, W., 2022. A review on autonomous vehicles: Progress, methods and\nchallenges. Electronics 11, 2162.\nPark, J., Woo, S., Lee, J.Y ., Kweon, I.S., 2020. A simple and light-weight\nattention module for convolutional neural networks. International journal of\ncomputer vision 128, 783–798.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.,\n2018. Image transformer, in: International Conference on Machine Learn-\ning, PMLR. pp. 4055–4064.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., et al., 2019. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural informa-\ntion processing systems 32.\nQu, L., Zhou, Y ., Liang, P.P., Xia, Y ., Wang, F., Adeli, E., Fei-Fei, L., Rubin, D.,\n2022. Rethinking architecture design for tackling data heterogeneity in fed-\nerated learning, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 10061–10071.\nRoy, N., Posner, I., Barfoot, T., Beaudoin, P., Bengio, Y ., Bohg, J., Brock, O.,\nDepatie, I., Fox, D., Koditschek, D., et al., 2021. From machine learning\nto robotics: challenges and opportunities for embodied intelligence. arXiv\npreprint arXiv:2110.15245 .\nSabour, S., Frosst, N., Hinton, G.E., 2017. Dynamic routing between capsules.\nAdvances in neural information processing systems 30.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C., 2018. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks, in: Proceedings of the\nIEEE conference on computer vision and pattern recognition, pp. 4510–\n4520.\nSanh, V ., Wolf, T., Rush, A., 2020. Movement pruning: Adaptive sparsity\nby fine-tuning. Advances in Neural Information Processing Systems 33,\n20378–20389.\nShi, M., Zhou, Y ., Ye, Q., Lv, J., 2022. Personalized federated learning with\nhidden information on personalized prior. arXiv preprint arXiv:2211.10684\n.\nSimonyan, K., Zisserman, A., 2014. Very deep convolutional networks for\nlarge-scale image recognition. CoRR abs/1409.1556. URL: https://api.\nsemanticscholar.org/CorpusID:14124313.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V ., Rabinovich, A., 2015. Going deeper with convolutions, in:\nProceedings of the IEEE conference on computer vision and pattern recog-\nnition, pp. 1–9.\nTan, M., Le, Q., 2019. Efficientnet: Rethinking model scaling for convolutional\nneural networks, in: International conference on machine learning, PMLR.\npp. 6105–6114.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J ´egou, H.,\n2021. Training data-e fficient image transformers & distillation through\nattention, in: International Conference on Machine Learning, PMLR. pp.\n10347–10357.\nTouvron, H., Cord, M., El-Nouby, A., Verbeek, J., J ´egou, H., 2022. Three\nthings everyone should know about vision transformers, in: Computer\nVision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XXIV , Springer. pp. 497–515.\nVaroquaux, G., Cheplygina, V ., 2022. Machine learning for medical imaging:\nmethodological failures and recommendations for the future. NPJ digital\nmedicine 5, 48.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Advances in\nneural information processing systems 30.\nWang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.,\n2017. Residual attention network for image classification, in: Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pp. 3156–\n3164.\nWang, H., Yurochkin, M., Sun, Y ., Papailiopoulos, D., Khazaeni, Y ., 2020a.\nFederated learning with matched averaging, in: International Conference on\nLearning Representations. URL: https://openreview.net/forum?id=\nBkluqlSFDS.\nWang, J., Liu, Q., Liang, H., Joshi, G., Poor, H.V ., 2020b. Tackling the objective\ninconsistency problem in heterogeneous federated optimization. Advances\nin neural information processing systems 33, 7611–7623.\nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H., 2020c. Linformer: Self-\nattention with linear complexity. arXiv preprint arXiv:2006.04768 .\nWightman, R., 2019. Pytorch image models. https://github.com/\nrwightman/pytorch-image-models. doi:10.5281/zenodo.4414861.\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L., 2021a.\nCvt: Introducing convolutions to vision transformers, in: Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 22–31.\nWu, Y ., Kang, Y ., Luo, J., He, Y ., Yang, Q., 2021b. Fedcg: Leverage conditional\ngan for protecting privacy and maintaining competitive performance in fed-\nerated learning, in: International Joint Conference on Artificial Intelligence.\nURL: https://api.semanticscholar.org/CorpusID:244130332.\n17\nXiao, H., Rasul, K., V ollgraf, R., 2017. Fashion-mnist: a novel image\ndataset for benchmarking machine learning algorithms. arXiv preprint\narXiv:1708.07747 .\nXinyi, Z., Chen, L., 2019. Capsule graph neural network, in: International\nconference on learning representations.\nXiong, Y ., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y ., Singh, V .,\n2021. Nystr ¨omformer: A nystr ¨om-based algorithm for approximating self-\nattention, in: Proceedings of the AAAI Conference on Artificial Intelli-\ngence, pp. 14138–14148.\nYang, H., Yin, H., Shen, M., Molchanov, P., Li, H., Kautz, J., 2023. Global\nvision transformer pruning with hessian-aware saliency, in: Proceedings of\nthe IEEE /CVF Conference on Computer Vision and Pattern Recognition,\npp. 18547–18557.\nYu, F., Huang, K., Wang, M., Cheng, Y ., Chu, W., Cui, L., 2022. Width & depth\npruning for vision transformers, in: Proceedings of the AAAI Conference on\nArtificial Intelligence, pp. 3143–3151.\nYu, L., Xiang, W., 2023. X-pruner: explainable pruning for vision transform-\ners, in: Proceedings of the IEEE /CVF Conference on Computer Vision and\nPattern Recognition, pp. 24355–24363.\nYuan, Z., Xue, C., Chen, Y ., Wu, Q., Sun, G., 2022. Ptq4vit: Post-training\nquantization for vision transformers with twin uniform quantization, in: Eu-\nropean Conference on Computer Vision, Springer. pp. 191–207.\nZhu, M., Tang, Y ., Han, K., 2021. Vision transformer pruning. arXiv preprint\narXiv:2104.08500 .\nZuo, Z., Shuai, B., Wang, G., Liu, X., Wang, X., Wang, B., Chen, Y ., 2015.\nConvolutional recurrent neural networks: Learning spatial dependencies for\nimage representation, in: Proceedings of the IEEE conference on computer\nvision and pattern recognition workshops, pp. 18–26.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8716428279876709
    },
    {
      "name": "Bottleneck",
      "score": 0.7561997175216675
    },
    {
      "name": "Encoder",
      "score": 0.5586430430412292
    },
    {
      "name": "Edge device",
      "score": 0.5516782402992249
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5507975220680237
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5395772457122803
    },
    {
      "name": "Deep learning",
      "score": 0.5017046928405762
    },
    {
      "name": "Software deployment",
      "score": 0.47824326157569885
    },
    {
      "name": "Transformer",
      "score": 0.423079252243042
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4228832721710205
    },
    {
      "name": "Machine learning",
      "score": 0.41418203711509705
    },
    {
      "name": "Computer engineering",
      "score": 0.40601831674575806
    },
    {
      "name": "Distributed computing",
      "score": 0.32501885294914246
    },
    {
      "name": "Cloud computing",
      "score": 0.17309796810150146
    },
    {
      "name": "Embedded system",
      "score": 0.1551882028579712
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}