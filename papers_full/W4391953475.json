{
  "title": "Computer Vision Based Transfer Learning-Aided Transformer Model for Fall Detection and Prediction",
  "url": "https://openalex.org/W4391953475",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5102949443",
      "name": "Sheldon Mccall",
      "affiliations": [
        "University of Lincoln"
      ]
    },
    {
      "id": "https://openalex.org/A5093960480",
      "name": "Shina Samuel Kolawole",
      "affiliations": [
        "University of Lincoln"
      ]
    },
    {
      "id": "https://openalex.org/A4365375088",
      "name": "Afreen Naz",
      "affiliations": [
        "University of Lincoln"
      ]
    },
    {
      "id": "https://openalex.org/A2105762319",
      "name": "Liyun Gong",
      "affiliations": [
        "University of Lincoln"
      ]
    },
    {
      "id": "https://openalex.org/A2183073300",
      "name": "Syed Waqar Ahmed",
      "affiliations": [
        "National University of Computer and Emerging Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5104246084",
      "name": "Pandey Shourya Prasad",
      "affiliations": [
        "International Institute of Information Technology Bangalore"
      ]
    },
    {
      "id": "https://openalex.org/A2100934371",
      "name": "Miao Yu",
      "affiliations": [
        "University of Lincoln"
      ]
    },
    {
      "id": "https://openalex.org/A2990871758",
      "name": "James Wingate",
      "affiliations": [
        "University of Lincoln"
      ]
    },
    {
      "id": "https://openalex.org/A2321040059",
      "name": "Saeid Pourroostaei Ardakani",
      "affiliations": [
        "University of Lincoln"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6679343360",
    "https://openalex.org/W2809750278",
    "https://openalex.org/W3021765248",
    "https://openalex.org/W4365483659",
    "https://openalex.org/W3126750064",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W4200301490",
    "https://openalex.org/W2034541299",
    "https://openalex.org/W2059810738",
    "https://openalex.org/W6632447487",
    "https://openalex.org/W2118218997",
    "https://openalex.org/W3033610243",
    "https://openalex.org/W2479843705",
    "https://openalex.org/W2570475642",
    "https://openalex.org/W2783960515",
    "https://openalex.org/W2074290069",
    "https://openalex.org/W2767362598",
    "https://openalex.org/W2909645133",
    "https://openalex.org/W4292387559",
    "https://openalex.org/W3131579999",
    "https://openalex.org/W4386960870",
    "https://openalex.org/W3029924594",
    "https://openalex.org/W4306947767",
    "https://openalex.org/W3036337111",
    "https://openalex.org/W3035057651",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W4312245820",
    "https://openalex.org/W2963066927",
    "https://openalex.org/W6852903969",
    "https://openalex.org/W3193432016",
    "https://openalex.org/W3002458900",
    "https://openalex.org/W4387849842",
    "https://openalex.org/W2551508839",
    "https://openalex.org/W4399168337",
    "https://openalex.org/W3127157842"
  ],
  "abstract": "Falls bring about significant risks to individuals&#x2019; well-being and independence, prompting widespread public health concerns. Swift detection and even predicting the risk of falls are crucial for implementing effective measures to alleviate the adverse consequences associated with such incidents. This study presents a new framework for identifying and forecasting fall risks. Our approach utilizes a novel transformer model trained on 2D poses extracted through an off-the-shelf pose extractor, incorporating transfer learning techniques. Initially, the transformer is trained on a large dataset containing 2D poses of general actions. Subsequently, we freeze the majority of its layers and fine-tune only the last few layers using relatively smaller datasets for fall detection and prediction tasks. Experimental results indicate that our proposed method outperforms traditional machine learning (e.g., SVM, Decision Tree, etc.) and deep learning approaches (e.g., LSTM, CNN, ST-GCN, PoseC3D, etc.) in both fall detection and prediction tasks across various datasets.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nComputer Vision based Transfer Learning-\nAided Transformer Model for Fall Detection and \nPrediction  \nSheldon McCall1, Shina Samuel Kolawole1, Afreen Naz1, Liyun Gong1, Syed Waqar Ahmed2, \nPandey Shourya Prasad3, Miao Yu1 , James Wingate1   and Saeid Pourroostaei Ardakani1 \n1School of Computer Science, University of Lincoln, UK,  \n2Department of Sciences and Humanities, FAST-National University of Computer and Emerging Sciences, Pakistan,  \n3International Institute of Information Technology Bangalore  \nCorresponding author: M. Yu (myu@lincoln.ac.uk) \nThis research has received funding from the European Unionâ€™s Horizon 2020 research and innovation programme under the Marie S klodowska-Curie \ngrant agreement No. 778602. \n \nABSTRACT Falls bring about significant risks to individuals' well -being and independence, prompting \nwidespread public health concerns. Swift detection and even predicting the risk of falls are crucial for \nimplementing effective measures to alleviate the adverse con sequences associated with such incidents. This \nstudy presents a new framework for identifying and forecasting fall risks. Our approach utilizes a novel \ntransformer model trained on 2D poses extracted through an off -the-shelf pose extractor, incorporating \ntransfer learning techniques. Initially, the transformer is trained on a large dataset containing 2D poses of \ngeneral actions. Subsequently, we freeze the majority of its layers and fine-tune only the last few layers using \nrelatively smaller datasets for fa ll detection and prediction tasks. Experimental results indicate that our \nproposed method outperforms traditional machine learning (e.g., SVM, Decision Tree, etc.) and deep learning \napproaches (e.g., LSTM, CNN, ST -GCN, PoseC3D, etc.) in both fall detection  and prediction tasks across \nvarious datasets.  \nINDEX TERMS Computer Vision, Deep Learning, Fall Detection, Fall Prediction, Healthcare, Transfer \nLearning, Transformer  \nI. INTRODUCTION \nFall among older adults represents a critical public health \nconcern, posing a significant threat to their well -being and \nindependence. According to [1], approximately one -third of \nindividuals aged 65 and above, and half of those aged 80 and \nolder, experience at least one f all annually. The \nconsequences of falling can result in severe physiological \nand psychological damage to an elderly person's overall \nhealth. Notably, falling ranks among the top three most \ncommon causes of Traumatic Brain Injury (TBI) in the \nUnited States, as reported by [2]. Tragically, around 10% of \nall falls in seniors lead to major injuries, including \nintracranial injuries (ICIs) and fractures, as documented in  \n[3]. The gravity of the issue becomes even more apparent \nwhen considering the World Health Organisation's (WHO) \nreport [4], which identifies falling as the second leading \ncause of accidental or unintentional injury -related deaths  \nworldwide. Even when falls don't result in serious injurie s, \nfallers often struggle to get up without assistance, leading to \nprolonged periods of lying on the floor, known as \"long lies.\" \nThese â€œlong liesâ€ can lead to dehydration, pressure sores, \npneumonia, hypothermia, and even death [5].  Fall detection \nand prediction play a crucial role in mitigating the negative \neffects of falls mentioned earlier. Fall detection aims to \npromptly identify falls as they occur, ensuring timely \nassistance can be provided to prevent prolonged lying on the \nfloor. On the other hand, fall prediction assesses the \nlikelihood that an individual will experience a fall, allowing \nfor proactive interventions to be implemented, thus \npreventing potential falls and associated injuries. \n  In this study, our  emphasis is on addressing the research \nchallenge of employing cutting -edge sensor technologies \nand artificial intelligence techniques for the detection and \nprediction of falls. This particular research problem has \ngarnered significant attention in recent years. Up until 2024, \nresearchers have diligently explored and validated a \nmultitude of approaches, utilizing diverse sensor modalities \nincluding cameras and wearable sensors. These \ninvestigations have seamlessly integrated advanced signal \nprocessing and m achine learning techniques into the realm \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nof fall detection and prediction [6 ,7]. Among different \ntechniques, the computer vision-based approach [8] utilizing \ncameras has gained significant attention in the fall \ndetection/prediction community. The key advantage of this \nmethod lies in its non-intrusive nature, as it does not require \nusers to wear any additional equipment. This makes it a \nconvenient an d user -friendly option for fall detection and \nprediction. \n  This study presents a unified framework that effectively \naddresses fall detection and prediction tasks using state -of-\nthe-art computer vision and machine learning techniques.  \nThe proposed approach comprises several key steps. \nInitially, we extract body key points from video frames by \nemploying an â€œon -the-shelfâ€ key points detector. These \ndetected key points are then pre -processed and input into a \nnovel transformer model, which serves as the foundation for \nfall detection and prediction. Due to the limited availability \nof fall data, which may not be sufficient to train a complex \ntransformer model, we incorporate transfer learning [ 9] into \nour methodolo gy. We train the transformer model on the \nMPOSE dataset [10], a comprehensive collection of 2D pose \nsequences involving various actions like walking, jogging, \nrunning, and kicking. This pre-training step helps the model \nlearn useful representations from abundant data related to \nhuman poses and actions. Following t he pre-training phase, \nwe fine -tune the transformer model for fall detection and \nprediction. Most of the network parameters are frozen, \nretaining the knowledge gained during pre -training, while \nonly the last few layers are fine -tuned to adapt the model \nspecifically to fall -related tasks.  By combining transfer \nlearning with advanced computer vision and machine \nlearning techniques, our proposed framework demonstrates \nenhanced performance in both fall detection and prediction \ntasks, even when working with limited fall-specific data.  \n  In contrast to traditional computer vision -based techniques \nfor fall detection and prediction, our proposed approach \noffers distinct contributions. Firstly, we present pioneering \nwork that leverages a novel transformer model. Secondly, we \nemploy transfer l earning to efficiently train our model with \nlimited data, enhancing the accuracy of fall detection and \nprediction. Furthermore, our technique transcends the \nconfines of a singular task, catering to both effective fall \ndetection and prediction. Extensive evaluation studies \ndemonstrate the superiority of our proposed technique over \nother counterparts in the realm of dual fall detection and \nprediction tasks. \nII. LITERATURE REVIEW  \nLately, numerous research endeavors have focused on the \nrealm of computer vision for fall detection and prediction. \nThe following is a summary of these studies. \n \nA. FALL DETECTION \nIn the domain of fall detection, researchers have explored \nthreshold-based methodologies in several studies ([ 11-14]). \nThese approaches involve the extraction of specific features \nsuch as head velocity ([ 11,12]), the height -to-width ratio \n([13]), and movement amplitude along with shape changes \n([14]) through image processing techniques. These features \nare subsequently compared to predefined threshold values to \nidentify falls. However, it's worth noting that such threshold-\nbased techniques may struggle to accurately distinguish \nbetween falls occurrin g in various dir ections within the \nimages due to their reliance on a single threshold value which \nis not robust enough to distinguish falls in different \ndirections. \nApart from the threshold -based approach, more \nsophisticated machine -learning techniques have gained \nsignificant traction in the field of fall detection. In the study \n[15], researchers employed a combination of OpenPose for \nhuman key point identification and DeepSORT for tracking. \nThese key points were then input into various classifiers \nincluding Gradient-Boosted Trees (GDBT), Decision Trees \n(DT), Random Forest (RF), Support Vector Machine (SVM), \nand k -Nearest Neighbor (KNN) to detect falls. In [ 16], a \nHidden Markov Model (HMM) was utilized, leveraging \nsilhouette data extracted through background subtraction \ntechniques for fall detection. Remarkably, this approach \nachieved an accuracy of 84.72% based on their recorded \ndataset. [ 17] employed a range of machine learning \nalgorithms, including NaÃ¯ve Bayes, k -Nearest Neighbor, \nNeural Networks, and Support Vector Machines. These \nalgorithms were applied to video sequences using features \nextracted from human silhouette regions obtained thr ough \nbackground subtraction algorithms. An evaluation was \nperformed on the FDD and URFD datasets, with Support \nVector Machines yielding the most robust performance. In \n[18], Support Vector Machines were again utilized, this time \nbased on motion history images and histograms of oriented \ngradient features. This approach achieved remarkably high \nrecall rates and precision rates of 98.1% and 96.8%, \nrespectively, in a dataset co nsisting of realistic image \nsequences of both simulated falls and daily activities. [ 19] \nintroduced a Directed Ac yclic Graph Support Vector \nMachine (DAGSVM) approach for posture classification \nand fall detection. Results from self -recorded datasets \ndemonstrated superior accuracy compared to classical \nmachine learning model counterparts.  \nConventional machine learning-based approaches for fall \ndetection heavily rely on manually crafted features. To \nadvance the field and automatically capture crucial aspects \nof fall detection, deep learning methods have been \nintroduced. In [20], Convolutional Neural Networks (CNNs) \nwere employed. They processed silhouettes extracted via \nbackground subtraction to classify various postures, \nincluding falling, standing, sitting, and bending. The results \ndemonstrated that the CNN outperformed SVM m odels in \nfalling classification.[ 21] introduced a Three -Dimensional \nConvolutional Neural Network (3-D CNN) to extract motion \nfeatures from temporal sequences. To enhance spatial \nunderstanding, a Long Short -Term Memory (LSTM) -based \nspatial visual attention mechanism was incorporated. This \ncombined approach effectively captured both temporal and \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nspatial information for precise fall detection. In [2 2], the \nYOLO (You Only Look Once) network was harnessed to \nextract features using the darknet backbone from video \nframes. These features were then processed by the YOLO \nnetwork's heads to detect fall regions in images. \nImpressively, this YOLO -based system achieved over 90% \naccuracy on the UR Fall dataset while being deployed on an \nedge device. [2 3] utilized an ST -GCN (Spatiotemporal \nGraph Convolutional Network) deep neural network model \nto classify fall and non -fall activities. This model operated \non 3D skeleton sequences representing human actions. \nExperimental results revealed that the proposed ST -GCN \nmodel surpassed other counterparts such as Random Forest, \nSVM, and CNN in terms of fall detection accuracy. \n \nB. FALL PREDICTION \nIn the realm of fall risk prediction using computer vision \ntechniques, there has been comparatively less research, but \nnotable strides have been made. In [ 24], a computer vision -\nbased method was employed to predict the Timed -Up-and-\nGo (TUG) score, an indicator of fall risk. This was achieved \nby utilizing regression models, including linear regression \nFIGURE 1. The flowchart of the proposed technique \nPositional encoding \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nand SVM regressors, based on 3D poses derived from video \nrecordings captured by 2D/3D cameras. Experimental results \ndemonstrated that the SVM regressor yielded more accurate \nTUG predictions. [ 25] highlighted the significance of gait \nvariables, such as cadence and step width/time, extracted \nfrom 2D human poses in video frames. These variables were \nfound to be strongly associated with future falls through \nPoisson regression analysis, indicating their potential for fall \nrisk prediction. In [ 26], gait parameter s, alongside clinical \nassessment scores from STRATIFY, were combined to \npredict short-term fall risk for individuals with dementia in a \ndomestic setting. A two -layered MLP network model was \nemployed, achieving sensitivity and specificity rates of \n72.8% and  73.2%, respectively. [ 27] applied Computer \nVision and Machine Learning techniques to differentiate \nnormal gait patterns from those associated with fall risks. \nFour classification methods, including Convolutional Neural \nNetworks (CNN), Support Vector Machi ne (SVM), K -\nNearest Neighbors (KNN), and Long Short -Term Memory \n(LSTM) neural networks, were used. Results indicated that \nSVM and KNN outperformed CNN and LSTM, delivering \nsuperior performance on the collected data. [ 28] utilized a \n3D vision sensor to capture 3D skeletons, from which \nfeatures were extracted to train Random Forest and Support \nVector Machine models for estimating the Berg Balance \nScale (BBS) and assessing fall risk. A pilot test demonstrated \nhigh rates of fall risk prediction and a notable co rrelation \nwith physiotherapists' BBS scores on individual motion \ntasks. \nIt is essential to recognize that both fall detection and \nprediction tasks addressed in this study revolve around the \nvideo sequence classification problem. This involves \ncategorizing video sequences into fall/non-fall and high/low \nfall risk, constituting a typical pattern recognition \nclassification challenge. In this context, our work aligns with \nother image classification studies, exemplified by [2 9], as \nthey share a common focus on classification problems. \nHowever, while image classification, as demonstr ated in \n[29], centers on categorizing individual images, our research \ndelves into the classification of video sequences, comprising \na series of images. This disparity necessitates distinct models \nand methodologies compared to single -image classification, \naiming to fu lly leverage the temporal dependency \ninformation inherent in consecutive frames within a video \nsequence for precise fall detection and prediction. \n  In this study, we employ a transformer model \ncomplemented by transfer learning to address both fall \ndetection and fall risk (high/low risk) prediction tasks, using \n2D body key points extracted from the OpenPose detector \n[30]. In contrast to utilizing complete video frames and other \nvideo features, our approach leverages 2D skeletons, \nproviding a streamlined representation of the human body. \nThis reduction in data dimensionality enhances the \ncomputational efficiency of fall d etection and prediction \ntasks. Additionally, the 2D skeleton representation mitigates \nsensitivity to variations in lighting, background clutter, and \nclothing. Distinguishing our research from prior work in the  \nfield, our study marks the pioneering use of a transformer \nmodel for fall detection and prediction tasks. Furthermore, \nwe incorporate transfer learning by initially training the \ntransformer model on the extensive MPOSE dataset. This \napproach allows the model to glean valuable insights from a \nwealth of motion data pertaining to human poses and actions, \nultimately bolstering its performance in fall detection and \nprediction tasks.  \nIII. METHODOLOGY \nThis study introduces an innovative approach to fall \ndetection and fall risk prediction, leveraging a transformer \nmodel through the application of transfer learning. The \nrelated flowchart is presented in Figure 1, showing a visual \nrepresentation of the methodology. A comprehensive \nexplanation of the components depicted in Fig. 1 will be \ngiven in the next subsections. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nA. Pose Estimation and Pre-Processing \nWe utilize the Openpose algorithm [30] to extract 2D human \nposes from the original video sequences. Openpose allows \nus to extract the (x, y) coordinates of 25 key points from the \nsubject's body for each individual frame in a scene, as \nvisually illustrated in Fig. 2. Following the key point \nextraction by Openpose, we perform several preprocessing \ntasks. Initially, we establish the midpoint of the shoulders \n(point 1 in Fig. 2) as the origin and adjust the position of each \nkey point accordingly to obtain relative key point positions. \nSubsequently, we standardize the key point positions using \nthe length of the trunk, which is defined as the distance \nbetween points 1 and 8, as a normalization factor. Upon \ncompleting the preprocessing steps, we transform an \nindividual key point denoted as ğ’‘ğ‘– via the following formula: \n \n                               ğ’‘Ì‚ğ‘– =\nğ’‘ğ‘–âˆ’ğ’‘1\nğ¿                                (1) \nFIGURE 2.  Illustration of 25 key points on the human body \nextracted by Openpose. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nwhere ğ’‘1  represents the position of the shoulder and L  \nrepresents the trunk length. \n  By employing the aforementioned pre-processing methods, \nwe can ensure that the extracted 2D poses remain invariant \nwith respect to scale and position. This enables the developed \ntechnique to effectively detect and predict fall risks across \nvarious positi ons and distances from the camera when \nprocessing videos. Based on the position of every pre -\nprocessed pose key point denoted as ğ’‘Ì‚ğ‘–, we further calculate \nits velocity between consecutive frames as ğ’—ğ‘– = ğ’‘Ì‚ğ‘– âˆ’ ğ’‘Ì‚ğ‘–âˆ’1 \nto glean more insights into the dynamics of the human body. \nFor the i -th frame, we can obtain a concatenated vector \ndenoted as ğ’„ğ‘– = [ğ’‘Ì‚ğ‘–,1; â€¦ , ğ’‘Ì‚ğ‘–,25; â€¦ , ğ’—ğ‘–,1; â€¦ ğ’—ğ‘–,25]  by \ncombining both position and velocity information of all \nkeypoints, where ğ’‘Ì‚ğ‘–,ğ‘—  and  ğ’—ğ‘–,ğ‘—  represent the position and \nvelocity of the j -th keypoint at this i -th frame. Finally, we \ncompile these concatenated vectors for all frames within a \nvideo sequence, denoted as [ğ’„1, â€¦ , ğ’„ğ‘]  (where ğ‘  is the \nsequence length) as the input for the subsequent transformer \nmodel.  \nB. Transformer for Fall Detection and Prediction \nThe transformer architecture employed in this study draws \ninspiration from the methodology outlined in [ 31]. Unlike \n[31] for a single image classification , our developed model \nis adapted for video processing. It  utilizes 2D skeletons \nextracted from a video sequence as its input . And a 'class \ntoken' [CLS] vector derived from the transformer \nencapsulating information from all skeletons, is taken as the \nfeature for fall detection and prediction. \n  In specific, firstly embedding operation is performed by \nmapping every vector in the skeleton sequence to a sequence \nof higher dimension D tokens (denoted as ğ’™1\nğ¸, â€¦ , ğ’™ğ‘\nğ¸ ) using a \nlinear projection map ğ‘Š âˆˆ ğ‘¹ğ‘ƒÃ—ğ· , where P is the vector \ndimension. Besides, a trainable vector of dimension D \n(denoted as ğ’™ğ‘ğ‘™ğ‘  ) is prepended to the beginning of the \nembedded sequence as shown in Fig. 1 , which is taken as a \nclass token [CLS] leveraging the self -attention to aggregate \ninformation into a compact high-dimensional representation \nfor fall detection/prediction. As the traditional transformer, \nthe positional vectors are added to take into account the \nposition information of vectors in a sequence for our fall \ndetection/prediction tasks. \n  The embedded vectors and class token after being added by \npositional information, are then fed into a Transformer \nencoder. As shown in Fig. 1, the Transformer encoder \ncontains multiple blocks while each block contains multi -\nhead attention, additional&norma lization and feed -forward \nlayers. The pivotal element within the Transformer encoder \nis the multi -head attention layer  [32], which leverages \nmultiple 'heads' to generate outputs. For the i-th head, queries \n(ğ‘„ğ‘– ), keys ( ğ¾ğ‘–) and values ( ğ‘‰) are computed as ğ‘„ğ‘–  = ğ‘‹ğ‘Šğ‘„ğ‘– , \nğ¾ğ‘–  = ğ‘‹ğ‘Šğ¾ğ‘–  and ğ‘‰ğ‘–  = ğ‘‹ğ‘Šğ‘‰ğ‘–  respectively, where ğ‘‹ represents \nthe input of the multi -head attention layer while ğ‘Šğ‘„ğ‘– , ğ‘Šğ¾ğ‘–  \nand ğ‘Šğ‘‰ğ‘–  denote the respective parameter matrices associated \nwith the i -th head. Based on ğ‘„ğ‘– , ğ¾ğ‘–  and ğ‘‰ğ‘– , the attention \nweights ğ´ğ‘– for the i-th head are calculated as: \n \n                   ğ´ğ‘– = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(\nğ‘„ğ‘–ğ¾ğ‘–\nğ‘‡\nâˆšğ·â„ )                                         (2) \n \nwhere ğ·â„ is a scale factor and ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(âˆ™) is an activation \nfunction the as defined in [33] and the output of the i-th head \ndenoted as ğ»ğ‘–  is calculated as: \n \n                   ğ»ğ‘– = ğ´ğ‘–ğ‘‰ğ‘–                                                        (3) \n \nwhich is the weighted summation of ğ‘‰ğ‘– based on the attention \nweights. All head outputs are calculated in the same way and \nfinally concatenated and linearly projected as the final output \nof the multi-head attention layer as: \n \n                  ğ‘€ğ‘†ğ´(ğ‘‹) = [ğ»1; ğ»2; â€¦ ; ğ»ğ‘]ğ‘Šğ‘€ğ‘†ğ´                    (4) \n \nwhere ğ‘€ğ‘†ğ´(ğ‘‹)  represents the multi-head attention layer  \noutput based on the input ğ‘‹, ğ‘Šğ‘€ğ‘†ğ´ is a projection matric and \nğ‘ is the head number in the multi-head attention layer . The \noutput of a multi-head attention layer will then go through a \nseries of add&norm operations and a small feed -forward \nnetwork to generate the output of a block. \n   The final output of the whole transformer encoder is \nobtained through N blocks of multi-head attention operations \nas well as add&norm and feed-forward operations as shown \nin Fig. 1 .  The output of the transformer encoder --ğ‘‹ğ¿  is \nrepresented as: \n \n            ğ‘‹ğ¿ = ğ¹([ğ‘¥ğ‘ğ‘™ğ‘ ; ğ‘‹ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘’ğ‘‘ ] + ğ‘‹ğ‘ğ‘œğ‘ )                          (5) \n \nwhere F( âˆ™ ) represents the operations associated with the \ntransformer encoder. ğ‘¥ğ‘ğ‘™ğ‘   represents the class token, \nğ‘‹ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘’ğ‘‘  represents embedded vectors and ğ‘‹ğ‘ğ‘œğ‘  represents \npositional vectors. Finally, only the first column of ğ‘‹ğ¿ \ncorresponding to the class token is fed into a feed -forward \nnetwork head, for performing the final tasks of classifying \nfalls/non-fall and high/low fall risks. \n \nC. Transfer Learning \nAs falls tend to be fairly uncommon in real scenarios, fall \ndetection/prediction datasets tend to follow this trend of not \nbeing widely available and those that are available typically \nwill be smaller compared to those used for general action \nrecognition. This makes it challenging to train complex \ntransformer models as they will likely suffer from under -\nfitting. In order to address this issue, we decided to \nincorporate a transfer learning strategy into our work. We \nfirst train a transformer model on a large MPOSE dataset   as \nin [34], comprising 15429 samples of 20 actions performed \nby 100 subjects. This pre -training empowers the model to \nlearn useful feature representations from abundant data for \naction classification tasks (e.g., classifying fall or non -fall). \nThen we froze some of the layers of the model, such as the \nembedding and transformer encoder layers, which contain \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nthe majority of the network parameters, while leaving only \nthe last two MLP layers trainable. Finally, we modify the \narchitecture of the network to change the output of 20 actions \nto 2 for our specific fall detection (fall/non -fall) or fall risk \nprediction (high/low risk). In this way, we can significantly \nreduce the number of trainable parameters, allowing the \nmodel to be trained with a smaller amount of fall detection \nand fall risk prediction associated datasets. \nIV. EXPERIMENTS \nA. MPOSE Dataset Evaluations \nThis section unveils the evaluation results of our devised \nmethodology for fall detection and prediction. As delineated \nin Section III.C, our initial step involves the training of our \ntransformer model using a relatively expansive MPOSE \ndataset, consisting of 15,429 samples representing 20 distinct \nactions (i.e., walking, jogging, handshaking, etc.) executed \nby 100 subjects. Each sample comprises 30 frames. We \npartitioned the dataset into 80% for training and the \nremaining 20% for testing. Exploring various  transformer \narchitectures, namely â€˜microâ€™, â€˜smallâ€™, â€˜baseâ€™, and â€˜largeâ€™, \neach progressively increasing in model complexity, as \npresented in Table 1.  The statistical metrics, including mean \nand standard deviation of accuracy and balanced accuracy, \nacquired through multiple rounds of evaluation on the \nMPSOSE dataset, are summarized in Table 2. Notably, our \nanalysis reveals that the 'small' transformer archi tecture \nyields the highest accuracy, and further augmenting model \ncomplexity does not confer any advantages. So, we have  \n \n \n \n \nchosen the 'small' transformer architecture for fall detection \nand prediction in our work. \n \nB. Descriptions on Fall Detection&Prediction Datasets \nWe conducted a n extensive performance evaluation of our \ndeveloped methods for both fall detection and fall risk \nprediction tasks using two distinct datasets: the CAUCAFall \ndataset [35] and a gait dataset [36].  The CAUCAFall dataset  \nis created in conditions of an uncontrolled home \nenvironment, with occlusions, changes in lighting (natural, \nartificial, and night), variety in the clothing of the \nparticipants, movement in the background, different textures \n micro small base large \nNo. of heads 1 2 3 4 \nNo. of blocks 4 5 6 6 \nEmbedded \nDimension \n64 128 192 256 \nDimension of \nMLP Layer \n256 256 256 512 \n micro small base large \nAccuracy 89.55%Â±0.\n52% \n89.90%Â±0\n.32% \n89.21%Â±0.\n57% \n88.87%Â±0.\n64% \nBalanced \nAccuracy \n84.76%Â±0.\n79% \n85.51%Â±0\n.59% \n84.47%Â±0.\n70% \n84.37%Â±1.\n04% \nTABLE 1. Transformer architectures with different \ncomplexities. \n \n \nFIGURE 3. Representative frames and pose extraction results for CAUCAFall dataset (a) for fall (top line) and non -fall (bottom line) activities, gait \ndataset (b) for low-fall risk gait (top line) and high fall risk gait (bottom line) \nTABLE 2. Action recognition comparisons between different \narchitectures on the MPOSE dataset. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \non the floor and room, variety in the  angles of fall, different \ndistances from the camera to the fall, with participants of \ndifferent age, weight, height, and even different dominant \nleg. It contains 10 subjects simulating 5 types of falls \n(forward falls, backward falls, falls to the left, falls to the \nright, and falls from sitting) and 5 types of activities of daily \nliving (ADLs) including walking, hopping, object retrieval,  \nsitting, and kneeling , which are recorded by a normal RGB \ncamera with the size of approx. 8GB. The recorded video \ndata are organized into 10 main directories corresponding to  \nthe subjects, each of which contains 10 folders with the \ndifferent activities performed by the participants, in each \nfolder there is a video of the action in .avi format, and the \nimages of the actions in .png format, and each of the frame  \nFIGURE 4. The evolutions of the loss function value during training for CAUCAFall dataset (a) and gait dataset (b).  \nFIGURE 5. Generated attention maps for samples in the CAUCAFall dataset (top row) and gait dataset (bottom row).  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nsegmentation labels in .txt format. Videos are recorded using \na standard RGB camera at a frame rate of 23 frames per \nsecond, with a 1080 Ã— 960 pixels resolution. The gait dataset \ncomprises 96 subjects which involves 50 Knee Osteoarthritis \n(KOA), 16 Parkinson's Disease (PD) , and 30 \nNormal/Healthy (NM) subjects with different fall risk levels. \nFor each subject, two gait video sequences (left to right and \nright to left) are recorded using a single NIKON DSLR 5300 \ncamera placed 8m away from the walking mat in the hospital \narea, with a video resolution of 1920 Ã— 1080 pixels and a \nframe rate of 23 frames per second.  In total, the video \nrecordings of this dataset have a size of approx. 3GB.  More \ndetailed descriptions of these two datasets can be found in \n[35] and [36].  \n \nC. Model Training Details \nWe divided the CAUCAFall and gait datasets into video \nclips containing 30 frames. We acquired 716 video clips  \ndepicting falling activities and 793 video clips showcasing \nnon-fall activities from the CAUCAFall dataset. For the gait \ndataset, we collected about 2000 video clips representing  \nhigh fall -risk gait patterns and about 1300 video clips \nrepresenting low fall -risk gait patterns. Moreover, these \nobtained video clips were segregated into training and testing \nsubsets, following a distribution of 70% for training and 30% \nfor testing. To extract 2D poses from the original video \nframes, we used Openpose. Fig. 3 presents some examples \nof the extracted 2D poses from the original video frames for \nboth datasets. We extract 2D poses for all the video clips in \nthe training dataset and construct vector sequences of pre -\nprocessed key pointsâ€™ positions and velocities as per Section \nIII.A. The constructed sequences for all video clips in the \ntraining dataset are then used to train the transformer model.   \nFor the model training, as mentioned in Section III.C, we \nfroze the transformer encoder, after training our transformer \nmodel on the MPOSE dataset, while leaving only the MLP  \nlayers embedding layer and trainable. For training, the binary \ncross-entropy loss function is exploited as below: \n \n     \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n     ğ¿(ğ‘¦, ğ‘) = âˆ’(ğ‘¦ âˆ™ log(ğ‘) + (1 âˆ’ ğ‘¦) âˆ™ log(1 âˆ’ ğ‘))          (6) \n \nwhere y is the true class label and p is the predicted \nprobability. The AdamW optimizer [33] is used to minimize \nthe loss function, with a batch size of, weight decay factor of \n0.0001, and an adaptive learning rate strategy with the \nlearning rate being 0.0001 after 80% of the training steps and \ncalculated as in [ 32] before 80% of the training steps.  10% \nof the training dataset is taken as the validation dataset and \nthe transformer model which achieves the best performance \non the validation dataset is saved for testing.  \nFig. 4 shows the loss function value with respect to the \ntraining epoch for both the fall detection and gait datasets, \nfrom which we can see that at around 20 epochs the loss \nfunction almost converges to a minimum. Thus 20 epochs  \nare determined for model training. The training loss values \nare mostly smaller than the validation loss values (as the \nmodel is training on the training dataset, not the validation \none).  \n  The transformer model, being an attention -based model, \nassigns distinct attention weights to elements at various \npositions within a sequence, thus different elements \ncontribute differently to the final transformer output. In \nFigure 5, we visually depict th e generated attention weight \nmaps for several sequences taken from the two datasets. \nThese visual representations vividly highlight the variations \nin attention weights among elements within a sequence, \nindicating how different elements contribute differe ntly to \nthe overall transformer output. \n \nD. Model Evaluations&Comparisons  \nThe trained model is then evaluated on the test datasets. A \nvariety of metrics are applied to evaluate its performance on \nthe test dataset, including sensitivity, specificity, and \naccuracy, which are defined as below: \n \n                  ğ‘ ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ =\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘                                            (6)  \n sensitivity specificity accuracy \nProposed 98.37%Â±0.01% 97.58%Â±0.02% 97.95%Â±0.87% \nSVM 73.68%Â±0% 83.61%Â±0% 79.03%Â±0% \nNN 72.25%Â±0% 88.11%Â±0% 80.79%Â±0% \nNaive Bayesian 34.45%Â±0% 90.98%Â±0% 64.90%Â±0% \nDecision Tree 89.57%Â±1.13% 95.16%Â±0.63% 92.58%Â±0.59% \nMLP 87.80%Â±0.89% 89.63%Â±0.69% 88.79%Â±0.49% \n Trainable \nparam. \nsensitivity specificity accuracy \nLSTM 61,402 97.89%Â±0.89% 96.43%Â±0.95% 97.11%Â±0.65% \nTCN 78,882 94.11%Â±0.40% 91.07%Â±0.01% 92.25%Â±0.02% \nTransformer \n(without transfer learning) \n1,035,778 94.16%Â±2.39% 95.16%Â±1.73% 94.70%Â±1.28% \nProposed 33,538 98.37%Â±1.03% 97.58%Â±1.41% 97.95%Â±0.87% \nTABLE 3. Comparisons of the proposed methodology with traditional classifiers for the fall detection (the best result is bolded)  \nTABLE 4. Comparisons of the proposed methodology with deep learning models for the fall detection(the best result is bolded  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \n           \n \n \n \n \n \n \n \n \n \n \n \n \n \n                  ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦ =\nğ‘‡ğ‘\nğ‘‡ğ‘+ğ¹ğ‘ƒ                                       (7) \n \n             ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =\nğ‘‡ğ‘ƒ+ğ‘‡ğ‘\nğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘                                    (8) \n \nwhere TP, TN, FP, and FN represent true positive (correctly \nclassified positive sample), true negative (correctly classified \nnegative sample), false positive (incorrectly classified \npositive sample) , and false negative (incorrectly classified \nnegative sample) respectively. \n  We compared the performance metrics of our developed \nmethod with those of other machine learning models and \ndeep learning models used in other research works for both \nfall detection and risk prediction. The results are summarized \nin Tables 3, 4, 5, and 6. Note, that multiple evaluations were \nperformed for each model, and the mean and standard \ndeviation were reported. From these four tables, we can  \nobserve that our developed approach achieves much better \nperformance than the traditional machine learning models  \n(SVM, nearest neighbor, Naive Bayesian, decision tree, and \nmulti-layer perception (MLP)) with higher sensitivity, \nspecificity, and accuracy values. Besides, our approach \noutperforms the other deep l earning models (LSTM, \ntemporal convolutional network (TCN), and the Transformer \nwithout transfer learning), with even fewer trainable \nparameters. \n  Moreover, we have also compared our proposed \napproaches with other state -of-the-art ones originally \ndeveloped for action recognitions, including the ST -GCN \n[37], 2s-AGN [38], and PC3D [39], which are trained based \non CAUCAFall and gait datasets for performing fall \ndetection&prediction tasks. The comparison results are \nshown in Table 7, which shows that the proposed approach \nalso achieves the highest accuracies on both fall detection \nand prediction tasks compared to these three approaches. The \nadvantage of t he proposed transformer -based method over \ngraph model-based ST-GCN and 2s -AGN approaches and \nconvolutional model-based PC3D approach can be attributed \nto the capability of the transformer for modeling the global  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ninformation of input data as mentioned in [32], thus to extract \nthe most representative features for the whole input skeleton \nsequence for performing fall detection and prediction tasks. \n \n \n \nV. DISCUSSIONS \nWe present a pioneering transformer model for fall detection \nand prediction in this study, leveraging transfer learning to \nenhance model performance in the presence of limited data. \nThrough extensive evaluations across multiple datasets, \nfocusing on fall detection and prediction tasks, our proposed \nmethod demonstrates superior performance compared to \nboth classical machine learning models and state-of-the-art  \naction recognition models.  While our proposed method \nproves effective, there remains  room for improvement and \npotential benefits from recent advancements in the machine-\nlearning community. One avenue for enhancement is the \nadoption of bilinear pooling [ 40] to fuse diverse features \nextracted from a video clip, thereby achieving more precise \nfall detection and prediction performance.  Additionally, we \ncan explore the application of decentralized federated \nlearning [ 41] to train the machine learning model \ncollaboratively using a cluster of machines. In this approach, \neach client in the cluster trains a model based on its local data \nand then communicates the model parameters with other \nclients, facilitating the aggregation of results from each client \nto obtain a final global model. This strategy capitalizes on \nthe computational and storage resources of multiple  \n sensitivity specificity accuracy \nProposed 94.60%Â±0.96% 92.93%Â±1.41% 93.98%Â±0.70% \nSVM 72.51%Â±0% 47.43%Â±0% 63.17%Â±0% \nNN 81.99%Â±0% 43.90%Â±0% 67.81%Â±0% \nNaive Bayesian 8.20%Â±0% 87.53%Â±0% 37.74%Â±0% \nDecision Tree 86.82%Â±0.31% 73.33%Â±1.14% 81.80%Â±0.52% \nMLP 80.10%Â±3.75% 67.03%Â±9.44% 75.26%Â±4.61% \n Trainable \nparam. \nsensitivity specificity accuracy \nLSTM 61,402 92.81%Â±0.75% 92.17%Â±1.75% 92.57%Â±0.73% \nTCN 78,882 81.95%Â±1.84% 71.44%Â±3.66% 78.03%Â±1.81% \nTransformer \n(without transfer learning) \n1,035,778 83.88%Â±5.75% 59.91%Â±16.67% 75.12%Â±7.62% \nProposed 33,538 94.60%Â±0.96% 92.93%Â±1.41% 93.98%Â±0.70% \n ST-\nGCN[31] \n2s-AGN \n[32] \nPoseC3\nD [33] \nProposed \nFall detection \nAccuracy \n91.22% \nÂ±2.09% \n95.62% \nÂ±1.50% \n92.27% \nÂ±4.86% \n97.95% \nÂ±0.87% \nFall \nprediction \nAccuracy \n85.99% \nÂ±5.18% \n89.99% \nÂ±2.42% \n92.03% \nÂ±3.73% \n93.98% \nÂ±0.70% \nTABLE 5. Comparisons of the proposed methodology with traditional classifiers for the fall risk prediction (the best result is bolded)  \nTABLE 6. Comparisons of the proposed methodology with deep learning models for the fall risk prediction (the best result is bolded)  \nTABLE 7. Comparisons of proposed methodology with state -\nof-the-art ones \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nmachines, leading to more efficient training on large \ndatasets. Importantly, decentralized federated learning \nensures data privacy, as it shares only the model parameters \nrather than the raw data. \n  Concerning the implementation of the developed technique \nfor real-world application, the related algorithms can either \nbe deployed on the video analytics unit on the â€˜edgeâ€™ (the \nplace where the video data is captured) or a remote cloud \nserver, to process the collected videos on a specific site (e.g., \nhome, care home, hospital ward, etc.) for performing fall \ndetection&prediction tasks. Achieving an optimal balance \nbetween algorithm complexity and computational/storage \nresources is crucial for practical real-world applications. For \ninstances where the intention is to deploy the algorithms on \nan edge device with limited computational and storage \nresources, strategic considerations are required. In such \ncases, adopting a lightweight 2D pose detection model, such \nas [ 42], becomes essential. Additionally, simplifying our \ntransformer model further becomes a necessity to align with \nthe constraints of the edge device. This ensures that the \noverall system remains efficient and effective even within \nthe limitations of  the hardware. On the contrary, if the \ndecision is made to deploy the algorithms on a robust cloud \nserver with ample computational and storage capabilities, \nmore complex models can be embraced. The increased \nprocessing power of the cloud server allows for the \nutilization of sophisticated algorithms, enhancing the \nsystem's overall performance and accuracy. \n   Privacy concerns are paramount in any computer vision -\nbased healthcare application. Effectively addressing these \nconcerns necessitates the adoption of encryption techniques. \nOne approach involves employing both software [ 43] and \nhardware [ 44] based encryption methods, which can be \napplied to blur images within the captured videos, thereby \nsafeguarding privacy. Furthermore, encryption techniques \nsuch as SSL (Secure Sockets Layer) and TLS (Transport \nLayer Security) [45] play a pivotal role in enhancing privacy \nduring the data transmission process to the cloud server. In \nscenarios where our developed technique is deployed on a \ncloud server for real -world applications, implementing SSL \nand TLS encryption ensures a secure and private exchange \nof data between t he edge device and the cloud server. This \nadditional layer of encryption fortifies the protection of \nsensitive healthcare information during transit, addressing \nprivacy concerns extensively. \n \nVI. CONCLUSION \nIn this work, we have developed a novel transformer model \napproach aided by transfer learning for fall \ndetection/prediction tasks. In specific, 2D human skeletons  \nfrom a video clip are extracted by the on -the-shelf pose \nextractor, which is then pre -processed and fed into a  \ntransformer model for fall detection/prediction. To further \nimprove the performance, a transfer learning approach is \nadopted to pre-train the transformer model on a larger human \nmotion dataset which step helps the model learn useful \nrepresentations from abundant data related to human poses \nand actions. Following the pre -training phase, we fine -tune \nthe transformer model for fall detection and fall risk \nprediction. The experimental results show that our approach \nachieves better performance than other machine learning or \ndeep learning models in both fall detection and prediction \ntasks. \nIn the current work, we only use the 2D pose features for the \nfall detection/prediction tasks. Not only limited to the pose \nfeatures, in the future we will investigate more video features \n(e.g., optical flow, silhouettes) and investigate the optimal \nfusion of multiple video features together with the \ntransformer model, to further improve the performance of  \nfall detection/prediction. Moreover,  more advanced \ntransformer architecture  and machine learning techniques \n(e.g., bilinear pooling, distributed machine learning)  will \nalso be investigated. \n \nACKNOWLEDGMENT \nThis research has received funding from the European \nUnionâ€™s Horizon 2020 research and innovation programme \nunder the Marie Sklodowska -Curie grant agreement No. \n778602 \n \nREFERENCES \n[1] UK. Department of Health &  Social Care , Falls: applying all our \nhealth. GOV.UK;2022.[Online]. Available at:  \nhttps://www.gov.uk/government/publications/falls-applying-all-our-\nhealth/falls-applying-all-our-health (Accessed: Oct. 8, 2023). \n[2] Centers for Disease Control and Prevention, â€œWISQARS-Web-based \ninjury statistics query and reporting system ,â€ WISQARS Injury Data, \n2023. [Online].  Available at \nhttps://webappa.cdc.gov/sasweb/ncipc/nfilead.html (Accessed Oct. 8, \n2023). \n[3] T. Al -Aama, â€œFalls in the elderly: Spectrum and prevention,â€ Can \nFam Physician, vol. 57, no. 7, pp. 771â€“776, 2011. \n[4] World Health Organization, Falls. 2021. [Online]. Available at: \nhttps://www.who.int/news-room/fact-sheets/detail/falls (Accessed \nOct. 8, 2023). \n[5] T. Broadley, J. Klenk , and M. Granat, â€œMethods for the real -world \nevaluation of fall detection technology: A scoping review,â€ Sensors, \nvol. 18, no. 7, pp. 1â€“28, 2018. \n[6] X. Wang, J. Ellul and G. Azzopardi, â€œElderly Fall Detection Systems: \nA Literature Survey,â€ Frontiers in Robotics and AI, vol. 7, 2020. \n[7] V. Adeli, N. Korhani, A. Sabo, S. Mehdizadeh, A. Mansfield, A. Flint, \nA. Iaboni and B. Taati, â€œAmbient Monitoring of Gait and Machine \nLearning Models for Dynamic and Short-Term Falls Risk Assessment \nin People With Dementia ,â€ IEEE Journal of Biomedical and Health \nInformatics, vol. 27, no. 7, pp. 3599-3609, 2023. \n[8] J. GutiÃ©rrez, V. RodrÃ­guez and S. Martin, â€œComprehensive Review of \nVision-Based Fall Detection Systems,â€ Sensors, vol. 21, no.3, 2021. \n[9] F. Zhuang, Z. Qi, K. Duan, and D. Xi, â€œA comprehensive survey on \ntransfer learning,â€ Proceedings of the IEEE, vol. 99, p. 1, 2020. \n[10] V. Mazzia, S. Angarano, F. Salvetti, F. Angelini, and M. Chiaberge, \nâ€œAction transformer: A self-attention model for short-time pose-based \nhuman action recognition,â€ Pattern Recognition, vol. 124, 2022. \n[11] M. Yu, S. Naqvi , and J. Chambers, â€œFall detection in the elderly by \nhead tracking â€, IEEE/SP 15th Workshop on Statistical Signal \nProcessing, Cardiff, UK, 2009. \n[12] C. Rougier, J. Meunier, A. St -Arnaud and J. Rousseau, â€œ3D head \ntracking for fall detection using a single calibrated camera,â€ Image and \nVision Computing, vol. 31, no. 3, pp. 246-254, 2013. \n[13] S. Miaou, F. Shih, and C. Huang, â€œA smart vision -based human fall \ndetection system for telehealth applications,â€ The Third International \nConference on Telehealth (IASTED), Anaheim, CA, USA, 2007. \n[14] C. Rougier, J. Meunier, A. St-Arnaud, and J. Rousseau, â€œFall detection \nfrom human shape and motion history using video surveillance,â€ 21st \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nInternational Conference on Advanced Information Networking and \nApplications Workshops (AINAW), Niagara Falls, Ont., Canada, 2007. \n[15] B. Wang and J. Yu and K. Wang and X. Bao and K. Mao, â€œFall \ndetection based on dual -channel feature integration,â€ IEEE Access , \nvol. 8, 2020. \n[16] S. Taghvaei and M. Jahanandish and K. Kosuge, â€œAuto regressive \nmoving average hidden markov model for vision -based fall \nprediction-an application for walker robot,â€ Assist. Technol., vol. 29, \np. 19, 2016. \n[17] N. Zerrouki, F. Harrou, A. Houacine, and Y. Sun, â€œFall detection using \nsupervised machine learning algorithms: A comparative study, â€ 8th \nInternational Conference on Modelling, Identification and Control \n(ICMIC), Algiers, Algeria, 2016. \n[18] Q. Feng, C. Gao, L. Wang, M. Zhang, L. Du and S. Qin, â€œFall detection \nbased on motion history image and histogram of oriented gradient \nfeature,â€ International Symposium on Intelligent Signal Processing \nand Communication Systems (ISPACS), Xiamen, China, 2017. \n[19] M. Yu, A. Rhuma, S. Naqvi, L. Wang and J. Chambers, â€œA Posture \nRecognition-Based Fall Detection System for Monitoring an Elderly \nPerson in a Smart Home Environment, â€ IEEE Transactions on \nInformation Technology in Biomedicine, vol. 16, no. 6, pp. 1274-1286, \n2012.  \n[20] M. Yu, L. Gong, and S. Kollias, â€œComputer vision based fall detection \nby a convolutional neural network,â€ In: 19th ACM International \nConference on Multimodal Interaction, Glasgow, Scotland, 2017. \n[21] N. Lu, Y. Wu, L. Feng , and J. Song, â€œDeep Learning for Fall \nDetection: Three-Dimensional CNN Combined With LSTM on Video \nKinematic Data â€, IEEE Journal of Biomedical and Health \nInformatics, vol. 23, no. 1, pp. 314-323, 2019. \n[22] A. Raza, M. Yousaf , and S. Velastin, â€œHuman Fall Detection using \nYOLO: A Real -Time and AI -on-the-Edge Perspective, â€ 12th \nInternational Conference on Pattern Recognition Systems (ICPRS) , \nSaint-Etienne, France, 2022. \n[23] O. Keskes and R. Noumeir, â€œVision-Based Fall Detection Using ST -\nGCN,â€ IEEE Access, pp. 28224-28236, 2021. \n[24] J. Ma, â€œPredicting TUG Score from Gait Characteristics with Video \nAnalysis and Machine Learning, â€ In Proceedings of 2023 Chinese \nIntelligent Automation Conference, 2023. \n[25] K. Ng, S. Mehdizadeh, A. Iaboni, A. Mansfield, A. Flint and B. Taati, \nâ€œMeasuring Gait Variables Using Computer Vision to Assess Mobility \nand Fall Risk in Older Adults With Dementia, â€ IEEE Journal of \nTranslational Engineering in Health and Medicine, vol. 8, 2020. \n[26] V. Adeli, N. Korhani, A. Sabo, S. Mehdizadeh, A. Mansfield, A. Flint, \nA. Iaboni and B. Taati, â€œAmbient Monitoring of Gait and Machine \nLearning Models for Dynamic and Short-Term Falls Risk Assessment \nin People With Dementia, â€ IEEE Journal of Biomedical and Health \nInformatics, vol. 27, no. 7, pp. 3599-3609, 2023. \n[27] B. Chen, C. Chen, J. Hu, Z. Sayeed, J. Qi, H. Darwiche, B. Little, S. \nLou, M. Darwish, C. Foote , and C. Palacio -Lascano, â€œComputer \nVision and Machine Learning-Based Gait Pattern Recognition for Flat \nFall Prediction,â€ Sensors, vol. 22, no. 20, 2022.  \n[28] A. Masalha, N. Eichler, S. Raz, A. Toledano -Shubi, D. Niv, I. \nShimshoni, and H. Hel -Or, â€œPredicting Fall Probability Based on a \nValidated Balance Scale,â€ IEEE/CVF Conference on Computer Vision \nand Pattern Recognition Workshops (CVPRW) , Seattle, WA, USA, \n2020. \n[29] W. Zhu, B. Peng, H. Wu and B. Wang, â€œQuery set centered sparse \nprojection learning for set based image classificationâ€, Applied \nIntelligence, Vol. 50, pp. 3400-3411, 2020. \n[30] Z. Cao, G. Martinez, T. Simon, S. Wei, and Y. Sheikh, â€œ Openpose: \nRealtime multi-person 2d pose estimation using part affinity fields,â€ \nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. \n43, pp. 172-186, 2021. \n[31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. \nUnterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. \nUszkoreit and N. Houlsby, â€œAn Image is Worth 16x16 Words: \nTransformers for Image Recognition at Scale, â€ Ninth International \nConference on Learning Representations, 2021. \n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, \nL. Kaiser and I. Polosukhin, â€œ Attention Is All You Need â€, 31st \nConference on Neural Information Processing Systems (NIPS 2017) , \nLong Beach, CA, USA, 2017. \n[33] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning, MIT \nPress, 2016. \n[34] V. Mazzia, S. Angarano, F. Salvetti, F. Angelini and M. Chiaberge. \n2021,  â€œ MPOSE2021, A Dataset for Short -Time Human Action \nRecognition,â€ [Online]. Available at: \nhttps://github.com/PIC4SeR/MPOSE2021_Dataset,  \n[35] J. Eraso, E. Munoz, M. Munoz and J. Pinto, July 2022, â€œ Dataset \nCAUCAFall,â€ Mendeley Data. [Online]. Available at: \nhttps://data.mendeley.com/datasets/7w7fccy7ky/4  \n[36] N. Kour, S. Gupta and S. Arora, December 2020, â€œ Gait Dataset for \nKnee Osteoarthritis and Parkinson's Disease Analysis With Severity \nLevels,â€ Mendeley Data. [Online]. Available at: \nhttps://data.mendeley.com/datasets/44pfnysy89/1 \n[37] S. Yan, Y. Xiong, and D. Lin, â€œSpatial-temporal graph convolutional \nnetworks for skeleton -based action recognition ,â€ The Thirty-Second \nAAAI Conference on Artificial Intelligence (AAAI-18), 2018. \n[38] L. Shi, Y. Zhang, J. Cheng, and H. Liu, â€œTwo-Stream Adaptive Graph \nConvolutional Networks for Skeleton -Based Action Recognition,â€ \nIEEE/CVF Conference on Computer Vision and Pattern Recognition \n(CVPR), Long Beach, CA, USA, 2019. \n[39] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, â€œRevisiting Skeleton-\nbased Action Recognition,â€ 2022 IEEE/CVF Conference on Computer \nVision and Pattern Recognition (CVPR) , New Orleans, LA, USA, \n2022. \n[40] Y. Gao, O. Beijbom, N. Zhang and T. Darrell, â€œCompact Bilinear \nPooling,â€ 2016 IEEE Conference on Computer Vision and Pattern \nRecognition (CVPR), Las Vegas, NV, USA, 2016. \n[41] L. Yuan, L. Sun, P. Yu and Z. Wang, â€œDecentralized Federated \nLearning: A Survey and Perspective,â€ [Online]. Available at: \nhttps://arxiv.org/abs/2306.01603 \n[42] H. Jeon, Y. Yoon and D. Kim, â€œLightweight 2D Human Pose \nEstimation for Fitness Coaching System,â€ 2021 36 th International \nTechnical Conference on Circuits/Systems, Computers and \nCommunications (ITC-CSCC), Jeju, Korea (South). \n[43] J. Liu, Y. Xia, and Z. Tang, â€œPrivacy -preserving video fall detection \nusing visual shielding information,â€ The Visual Computer, vol. 37, pp. \n359-370,2021. \n[44] L. Gong, S. Mccall , and M. Yu, â€œEnhancing privacy with optical \nelement design for fall detection,â€ Electronic Letters, vol. 59, no. 20, \n2023. \n[45] A. Satapathy and J. Livingston , â€œA Comprehensive Survey on \nSSL/TLS and their Vulnerabilities,â€ International Journal  of \nComputer Applications, vol.153, no. 5, pp. 31-38, 2016. \n        \n \nSheldon Mccall was born in Bedfordshire, \nEngland in 1999. He received his BSc (2020) and \nMSc (2021) degrees in Computer Science from the \nUniversity of Lincoln and is currently studying for \nhis PhD in Computer Science, specializing in \nMachine Learning and Computer Vision .  From \n2022 he has worked as a Research Assistant \ncollaborating on several research projects within \nthe School of Computer Science department at the \nUniversity. These centred around the development \nof software artifacts used in Machine Learning/Computer Vision projects. \n \nShina Samuel Kolawole is a highly \naccomplished professional with a strong academic \nbackground. He obtained a degree in Mechanical \nEngineering from Manchester Metropolitan \nUniversity in 2015, followed by an MSc in \nEngineering Management from Nottingham Trent \nUniversity in 2019. In 2022, he pursued a second \nMaster's program at the University of Lincoln, \nfocusing on Data Science and Applied Analytics, \nand graduated with distinction in 2023. Samuel's expertise spans diverse \nareas of data science, including deep learning, machine learning, statistical \nanalysis, statistical modelling, and computer vision. As a research assistant \nat the University of Lincoln, he led a groundbreaking project on \n\"Developing Machine Learning Models for Fall Risk Prediction with Open-\npose,\" utilizing video recordings of gaits. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \nPandey Shourya Prasad  is a dedicated \nthird-year undergraduate at the International \nInstitute of Information Technology \nBangalore, pursuing a Bachelor's degree in \nElectronics and Communication with \nspecialization in AI/ML. With a robust \nacademic background in machine learning, \nShourya is particularly enthusiastic about the \nintricacies of computer vision within the ML \ndomain. His main research interest lies in computer vision, NLP, and signal \nprocessing. \n \nAfreen Naz is a Research Assistant at the \nschool of Chemistry in the University of \nLincoln. She has a MSc in Computer Science \nand an MS in Applied Mathematics. Her \ncurrent research involves studying the biology \nof obesity for prevention and treatment of age-\nrelated diseases.  \n \n \n \nSyed Waqar Ahmed  is a lecturer in the \nDepartment of Sciences and Humanities at the \nNational University of Computer and \nEngineering Sciences. There, he teaches \nApplied Physics and Digital Logic Design. Mr. \nWaqar completed his BS in Physics \nspecializing in spectroscopy and MS with a \nLow-field Magnetic Resonance Spectroscopy \nthesis. Mr. Waqar is interested in the research \nfield at the junction of physics and computer \nscience. His interests are physics, machine \nlearning artificial intelligence, quantum computing, and data science. \n \nLiyun Gong received the Bachelor degree in \nElectronic and Communication Engineering from \nLiverpool University, Liverpool, U.K., in 2010. \nShe is currently a Ph.D. student at the Department \nof Electrical Engineering and Electronics, \nLiverpool University, Liverpool, U. K. Her \nresearch interests are in the areas of pattern \nrecognition and data mining. Currently, she is a \nPost Doctoral Research Associate in Data \nScience and Machine Learning in the University \nof Lincoln \n \nMiao Yu is a Senior Lecturer at the School of \nComputer Science in University of Lincoln. He \nworked as a research associate for the research \nproject â€œSignal Processing Solutions in a \nNetworked Battlespaceâ€ (funded by the \nEPSRC) in the Aeronautical and Automotive \nEngineering Department, Loughborough \nUniversity from 2013 to 2017. Prior to it, he \nobtained his PhD in the School of Electrical and \nElectronic Engineering, Loughborough \nUniversity with the PhD thesis title â€” â€œComputer vision based techniques \nfor fall detecti on with application towards assisted livingâ€. Dr. Miaoâ€™s \nresearch interests lie in developing algorithms in statistical signal \nprocessing, image/video processing, machine learning and data/knowledge \nmodelling, with applications in objects detection and tra cking, behaviour \nrecognitions and abnormal detection for healthcare. \n \n \n \n \n \n \nJames Wingate is a Lecturer in the School of \nComputer Science and member of the MLearn \nresearch group at the University of Lincoln. His \nresearch interests are in the area of machine \nlearning and artificial intelligence, especially \nlooking at latent space analysis or healthcare \napplications. \n \n \nSaeid POURROOSTAEI ARDAKANI  \ncurrently works as a Senior Lecturer in Computer \nScience at the University of Lincoln, UK. He is an \nacademic member of MLearn research group, an \nassociated member of Lincoln Centre for \nAutonomous Systems (L-CAS), and has formerly \nworked at the University of Nottingham (UNNC) \nas an Assistant Professor in Computer Science, \nmember of the Next Generation Internet of \nEverything Laboratory (NGIoE) and Artificial Intelligent Optimisation \nResearch group. He received his PhD in Computer Science from the \nUniversity of Bath focusing on data aggregation in Sensor Networks. \nSaeidâ€™s research and teaching expertise centres on smart and adaptive \ncomputing and/or communication solutions to build collaborative/federated \n(sensory/feedback)systems in Internet of Things (IoT) a pplications and \ncloud environments. He is also interested in (ML -enabled) Big Data \nprocessing and analysis applications. To date, Saeid has published two \nbooks and more than 100 journal articles, conference papers, book chapters, \nand technical reports. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368065\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.740970253944397
    },
    {
      "name": "Transfer of learning",
      "score": 0.5702126026153564
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5665841698646545
    },
    {
      "name": "Transformer",
      "score": 0.524972140789032
    },
    {
      "name": "Machine learning",
      "score": 0.4276476502418518
    },
    {
      "name": "Computer vision",
      "score": 0.3987159729003906
    },
    {
      "name": "Engineering",
      "score": 0.11976847052574158
    },
    {
      "name": "Voltage",
      "score": 0.07022920250892639
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51532219",
      "name": "University of Lincoln",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I201384688",
      "name": "National University of Computer and Emerging Sciences",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I181514455",
      "name": "International Institute of Information Technology Bangalore",
      "country": "IN"
    }
  ],
  "cited_by": 15
}