{
  "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
  "url": "https://openalex.org/W3126792443",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2370940768",
      "name": "Kim Won-Jae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753983497",
      "name": "Son Bo-Kyung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A750377168",
      "name": "Kim Il-Doo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3120237956",
    "https://openalex.org/W2786209943",
    "https://openalex.org/W2613526370",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W2963921132",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W3018442911",
    "https://openalex.org/W2987741655",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W3121480429",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2774267535",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3104152799",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2949844032",
    "https://openalex.org/W2899335602",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3110570034",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2911925209",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W3001555892",
    "https://openalex.org/W2884093133",
    "https://openalex.org/W3038476992",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2966645965",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2995181141",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3171654275"
  ],
  "abstract": "Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.",
  "full_text": "ViLT: Vision-and-Language Transformer\nWithout Convolution or Region Supervision\nWonjae Kim* 1 † Bokyung Son * 1 Ildoo Kim 2\nAbstract\nVision-and-Language Pre-training (VLP) has im-\nproved performance on various joint vision-and-\nlanguage downstream tasks. Current approaches\nto VLP heavily rely on image feature extraction\nprocesses, most of which involve region super-\nvision (e.g., object detection) and the convolu-\ntional architecture (e.g., ResNet). Although dis-\nregarded in the literature, we ﬁnd it problem-\natic in terms of both (1) efﬁciency/speed, that\nsimply extracting input features requires much\nmore computation than the multimodal interac-\ntion steps; and (2) expressive power, as it is upper\nbounded to the expressive power of the visual\nembedder and its predeﬁned visual vocabulary.\nIn this paper, we present a minimal VLP model,\nVision-and-Language Transformer (ViLT), mono-\nlithic in the sense that the processing of visual\ninputs is drastically simpliﬁed to just the same\nconvolution-free manner that we process textual\ninputs. We show that ViLT is up to tens of times\nfaster than previous VLP models, yet with com-\npetitive or better downstream task performance.\nOur code and pre-trained weights are available at\nhttps://github.com/dandelin/vilt.\n1. Introduction\nThe pre-train-and-ﬁne-tune scheme has been expanded to a\njoint domain of vision and language, giving birth to the cat-\negory of Vision-and-Language Pre-training (VLP)models\n(Lu et al., 2019; Chen et al., 2019; Su et al., 2019; Li et al.,\n2019; Tan & Bansal, 2019; Li et al., 2020a; Lu et al., 2020;\nCho et al., 2020; Qi et al., 2020; Zhou et al., 2020; Huang\n*Equal contribution †Current afﬁliation: NA VER AI Lab, Seong-\nnam, Gyeonggi, Republic of Korea. 1Kakao Enterprise, Seong-\nnam, Gyeonggi, Republic of Korea 2Kakao Brain, Seongnam,\nGyeonggi, Republic of Korea. Correspondence to: Wonjae Kim\n<wonjae.kim@navercorp.com>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nModality \nInteraction\nLinear \nEmbedding\nLinear \nEmbedding\nCNN \nBackbone\nCNN \nBackbone\nRegion \nOperations\nVisual Embedding Schema\nUNITER-Base \n(75.8 / 85.9 / 72.5)\nPixel-BERT-R50 \n(72.4 / 75.7 / 53.4)\nViLT-B/32 (Ours) \n(76.1 / 83.5 / 64.4)\n~75 ms \n(R101)\n~810 ms \n(RPNs, RoI Align, NMS, and RoI Heads)\n~45 ms \n(R50)\n~900 ms\n~60 ms\n~15 ms\n~0.4 ms \n(Linear Embedding)\n~15 ms \n(BERT-base-like)\nRunning Time \n(Performances : NLVR2 test-P Acc. / F30K TR R@1 / F30K IR R@1)\nRegion Feature \n(ViLBERT, UNITER, ...)\nGrid Feature \n(Pixel-BERT)\nPatch Projection \n(Ours)\nImage\nImage\nImage\nText\nFigure 1.Visual comparison of conventional VLP architectures\nand our proposed ViLT. We have entirely removed convolutional\nneural networks from the VLP pipeline without hurting perfor-\nmance on downstream tasks. ViLT is the ﬁrst VLP model of which\nthe modal-speciﬁc components require less computation than the\ntransformer component for multimodal interactions.\net al., 2020; Li et al., 2020b; Gan et al., 2020; Yu et al.,\n2020; Zhang et al., 2021). These models are pre-trained\nwith image text matching and masked language modeling\nobjectives1 on images and their aligned descriptions, and are\nﬁne-tuned on vision-and-language downstream tasks where\nthe inputs involve two modalities.\nTo be fed into VLP models, image pixels need to be ini-\ntially embedded in a dense form alongside language tokens.\nSince the seminal work of Krizhevsky et al. (2012), deep\nconvolutional networks have been regarded as essential for\nthis visual embedding step. Most VLP models employ an\nobject detector pre-trained on the Visual Genome dataset\n(Krishna et al., 2017) annotated with 1,600 object classes\nand 400 attribute classes as in Anderson et al. (2018). Pixel-\n1While some works employ additional objectives and data\nstructures, these two objectives apply to almost every VLP model.\narXiv:2102.03334v2  [stat.ML]  10 Jun 2021\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nVisual \nEmbedTextual \nEmbed\nText Image\nModality \nInteraction\n(a) VE > TE > MI\nTextual \nEmbed\nVisual \nEmbed\nText Image\nModality \nInteraction (b) VE = TE > MI\nTextual \nEmbed\nVisual \nEmbed\nModality \nInteraction\nText Image\n (c) VE > MI > TE\nVisual \nEmbed\nTextual \nEmbed\nModality \nInteraction\nText Image\n (d) MI > VE = TE\nFigure 2.Four categories of vision-and-language models. The height of each rectangle denotes its relative computational size. VE, TE,\nand MI are short for visual embedder, textual embedder, and modality interaction, respectively.\nBERT (Huang et al., 2020) is one exception of this trend,\nas it uses ResNet variants (He et al., 2016; Xie et al., 2017)\npre-trained on ImageNet classiﬁcation (Russakovsky et al.,\n2015) embedding pixels in place of object detection mod-\nules.\nTo this date, most VLP studies have focused on improving\nperformance by increasing the power of visual embedders.\nThe shortcomings of having a heavy visual embedder are\noften disregarded in academic experiments because region\nfeatures are commonly cached in advance at training time\nto ease the burden of feature extraction. However, the lim-\nitations are still evident in real-world applications as the\nqueries in the wild have to undergo a slow extraction pro-\ncess.\nTo this end, we shift our attention to the lightweight and fast\nembedding of visual inputs. Recent work (Dosovitskiy et al.,\n2020; Touvron et al., 2020) demonstrated that using a simple\nlinear projection of a patch is effective enough to embed\npixels before feeding them into transformers. Whereas being\nthe solid mainstream for text (Devlin et al., 2019), it is only\nrecently that transformers (Vaswani et al., 2017) are used for\nimages as well. We presume that the transformer module–\nused for modality interaction in VLP models– can also\nmanage to process visual features in place of a convolutional\nvisual embedder, just as it processes textual features.\nThis paper proposes the Vision-and-Language Transformer\n(ViLT) that handles two modalities in a single uniﬁed man-\nner. It mainly differs from previous VLP models in its\nshallow, convolution-free embedding of pixel-level inputs.\nRemoving deep embedders solely dedicated to visual in-\nputs signiﬁcantly cuts down the model size and running\ntime by design. Figure 1 shows that our parameter-efﬁcient\nmodel is tens of times faster than VLP models with region\nfeatures and at least four times faster than those with grid\nfeatures while exhibiting similar or even better performance\non vision-and-language downstream tasks.\nOur key contributions can be summarized as follows:\n• ViLT is the simplest architecture by far for a vision-\nand-language model as it commissions the transformer\nmodule to extract and process visual features in place\nof a separate deep visual embedder. This design in-\nherently leads to signiﬁcant runtime and parameter\nefﬁciency.\n• For the ﬁrst time, we achieve competent performance\non vision-and-language tasks without using region fea-\ntures or deep convolutional visual embedders in gen-\neral.\n• Also, for the ﬁrst time, we empirically show that whole\nword masking and image augmentations that were un-\nprecedented in VLP training schemes further drive\ndownstream performance.\n2. Background\n2.1. Taxonomy of Vision-and-Language Models\nWe propose a taxonomy of vision-and-language models\nbased on two points: (1) whether the two modalities have an\neven level of expressiveness in terms of dedicated parame-\nters and/or computation; and (2) whether the two modalities\ninteract in a deep network. A combination of these points\nleads to four archetypes in Figure 2.\nThe visual semantic embedding (VSE) models such as\nVSE++ (Faghri et al., 2017) and SCAN (Lee et al., 2018)\nbelong to Figure 2a. They use separate embedders for image\nand text, with the former being much heavier. Then, they\nrepresent the similarity of the embedded features from the\ntwo modalities with simple dot products or shallow attention\nlayers.\nCLIP (Radford et al., 2021) belongs to Figure 2b as it uses\nseparate but equally expensive transformer embedders for\neach modality. Interaction between the pooled image vec-\ntor and text vector is still shallow (dot product). Despite\nCLIP’s remarkable zero-shot performance on image-to-text\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nretrieval, we could not observe the same level of perfor-\nmance on other vision-and-language downstream tasks. For\ninstance, ﬁne-tuning the MLP head on NLVR2 (Suhr et al.,\n2018) with the dot product of pooled visual and textual vec-\ntors from CLIP as the multimodal representation gives a\nlow dev accuracy of 50.99 ±0.38 (ran with three different\nseeds); as chance level accuracy is 0.5, we conclude that the\nrepresentations are incapable of learning this task. It also\nmatches the ﬁndings of Suhr et al. (2018) that all models\nwith simply fused multimodal representation failed to learn\nNLVR2.\nThis result backs up our speculation that simple fusion of\noutputs even from high-performing unimodal embedders\nmay not be sufﬁcient to learn complex vision-and-language\ntasks, bolstering the need for a more rigorous inter-modal\ninteraction scheme.\nUnlike models with shallow interaction, the more recent\nVLP models that fall under Figure 2c use a deep transformer\nto model the interaction of image and text features. Aside\nfrom the interaction module, however, convolutional net-\nworks are still involved in extracting and embedding image\nfeatures, which accounts for most of the computation as de-\npicted in Figure 1. Modulation-based vision-and-language\nmodels (Perez et al., 2018; Nguyen et al., 2020) also fall\nunder Figure 2c, with their visual CNN stems correspond-\ning to visual embedder, RNNs producing the modulation\nparameters to textual embedder, and modulated CNNs to\nmodality interaction.\nOur proposed ViLT is the ﬁrst model of type Figure 2d\nwhere the embedding layers of raw pixels are shallow and\ncomputationally light as of text tokens. This architecture\nthereby concentrates most of the computation on modeling\nmodality interactions.\n2.2. Modality Interaction Schema\nAt the very core of contemporary VLP models lie transform-\ners. They get visual and textual embedding sequences as\ninput, model inter-modal and optionally intra-modal interac-\ntions throughout layers, then output a contextualized feature\nsequence.\nBugliarello et al. (2020) classiﬁes interaction schema into\ntwo categories: (1) single-stream approaches (e.g., Visual-\nBERT (Li et al., 2019), UNITER (Chen et al., 2019)) where\nlayers collectively operate on a concatenation of image and\ntext inputs; and (2) dual-stream approaches (e.g., ViLBERT\n(Lu et al., 2019), LXMERT (Tan & Bansal, 2019)) where the\ntwo modalities are not concatenated at the input level. We\nfollow the single-stream approach for our interaction trans-\nformer module because the dual-stream approach introduces\nadditional parameters.\n2.3. Visual Embedding Schema\nWhereas all performant VLP models share the same textual\nembedder– tokenizer from pre-trained BERT, word and po-\nsition embeddings resembling those of BERT– they differ\non visual embedders. Still, in most (if not all) cases, visual\nembedding is the bottleneck of existing VLP models. We\nfocus on cutting corners on this step by introducing patch\nprojection instead of using region or grid features for which\nheavy extraction modules are used.\nRegion Feature. VLP models dominantly utilize region\nfeatures, also known as bottom-up features (Anderson et al.,\n2018). They are obtained from an off-the-shelf object detec-\ntor like Faster R-CNN (Ren et al., 2016).\nThe general pipeline of generating region features is as fol-\nlows. First, a region proposal network (RPN) proposes re-\ngions of interest (RoI) based on the grid features pooled\nfrom the CNN backbone. Non-maximum suppression\n(NMS) then reduces the number of RoIs to a few thousand.\nAfter being pooled by operations such as RoI Align (He\net al., 2017), the RoIs go through RoI heads and become\nregion features. NMS is again applied to every class, ﬁnally\nreducing the number of features under a hundred.\nThe above process involves several factors that affect the\nperformance and runtime: the backbone, the style of NMS,\nthe RoI heads. Previous works were lenient with controlling\nthese factors, making varying choices from each other as\nlisted in Table 7.2\n• Backbone: ResNet-101 (Lu et al., 2019; Tan & Bansal,\n2019; Su et al., 2019) and ResNext-152 (Li et al., 2019;\n2020a; Zhang et al., 2021) are two commonly used\nbackbones.\n• NMS: NMS is typically done in a per-class fashion.\nApplying NMS to each and every class becomes a ma-\njor runtime bottleneck with a large number of classes,\ne.g. 1.6K in the VG dataset (Jiang et al., 2020). Class-\nagnostic NMS was recently introduced to tackle this\nissue (Zhang et al., 2021).\n• RoI head: C4 heads were initially used (Anderson et al.,\n2018). FPN-MLP heads were introduced later (Jiang\net al., 2018). As heads operate for each and every RoI,\nthey pose a substantial runtime burden.\nHowever lightweight, object detectors are less likely to\nbe faster than the backbone or a single-layer convolution.\nFreezing the visual backbone and caching the region fea-\ntures in advance only helps at training time and not during\n2Bugliarello et al. (2020) showed that a controlled setup bridges\nthe performance gap of various region-feature-based VLP models.\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nTransformer Encoder\nLinear Projection of Flattened PatchesWord Embedding\na stone near an [MASK]\nstatue\n1 0\n **0 0 0 1 0 2 0 3 0 4 0 5 0 6 1 1 1 2 1 3 1 4 1 5 1 6\nPatch position embedding\nToken position embedding\nModal-type embedding\nExtra learnable [class] embedding* *\nMasked Language Modeling\noffice\nTrue\nImage Text Matching\nPooler MLPFC\n OT\nzD|t zD|v\nzD|t zD|v\nWord Patch Alignment\nFigure 3.Model overview. Illustration inspired by Dosovitskiy et al. (2020).\ninference, not to mention that it could hold performance\nback.\nGrid Feature. Besides detector heads, the output feature\ngrid of convolutional neural networks such as ResNets can\nalso be used as visual features for vision-and-language pre-\ntraining. Direct use of grid features was ﬁrst proposed by\nVQA-speciﬁc models (Jiang et al., 2020; Nguyen et al.,\n2020), mainly to avoid using severely slow region selection\noperations.\nX-LXMERT (Cho et al., 2020) revisited grid features by\nﬁxing the region proposals to grids instead of those from\nthe region proposal networks. However, their caching of\nfeatures excluded further tuning of the backbone.\nPixel-BERT is the only VLP model that replaces the VG-\npre-trained object detector with a ResNet variant backbone\npre-trained with ImageNet classiﬁcation. Unlike frozen\ndetectors in region-feature-based VLP models, the backbone\nof Pixel-BERT is tuned during vision-and-language pre-\ntraining. The downstream performance of Pixel-BERT with\nResNet-50 falls below region-feature-based VLP models,\nbut it matches that of other competitors with the use of a\nmuch heavier ResNeXt-152.\nWe claim that grid features are not the go-to option, however,\nsince deep CNNs are still expensive that they account for a\nlarge portion of the whole computation as in Figure 1.\nPatch Projection. To minimize overhead, we adopt the\nsimplest visual embedding scheme: linear projection that\noperates on image patches. The patch projection embedding\nwas introduced by ViT (Dosovitskiy et al., 2020) for image\nclassiﬁcation tasks. Patch projection drastically simpliﬁes\nthe visual embedding step to the level of textual embedding,\nwhich also consists of simple projection (lookup) operations.\nWe use a 32×32 patch projection which only requires 2.4M\nparameters. This is in sharp contrast to complex ResNe(X)t\nbackbones3 and detection components. Its running time is\nalso ignorable as shown in Figure 1. We make a detailed\nruntime analysis in Section 4.6.\n3. Vision-and-Language Transformer\n3.1. Model Overview\nViLT has a succinct architecture as a VLP model with a\nminimal visual embedding pipeline and following the single-\nstream approach.\nWe deviate from the literature that we initialize the inter-\naction transformer weights from pre-trained ViT instead\nof BERT. Such initialization exploits the power of the in-\nteraction layers to process visual features while lacking a\nseparate deep visual embedder. 4\n¯t= [tclass; t1T; ··· ; tLT] + Tpos (1)\n¯v= [vclass; v1V; ··· ; vN V] + Vpos (2)\nz0 = [¯t+ ttype; ¯v+ vtype] (3)\nˆzd = MSA(LN(zd−1)) + zd−1, d = 1 ...D (4)\nzd = MLP(LN(ˆzd)) + ˆzd, d = 1 ...D (5)\np= tanh(zD\n0 Wpool) (6)\nViT consists of stacked blocks that include a multiheaded\nself-attention (MSA) layer and an MLP layer. The posi-\ntion of layer normalization (LN) in ViT is the only differ-\nence from BERT: LN comes after MSA and MLP in BERT\n(“post-norm”) and before in ViT (“pre-norm”). The input\n3Parameters for R50 is 25M, R101 is 44M, and X152 is 60M.\n4We also experimented with initializing the layers from BERT\nweights and using the pre-trained patch projection from ViT, but it\ndid not work.\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\ntext t ∈RL×|V |is embedded to ¯t ∈RL×H with a word\nembedding matrix T ∈R|V |×H and a position embedding\nmatrix Tpos ∈R(L+1)×H.\nThe input image I ∈ RC×H×W is sliced into patches\nand ﬂattened to v ∈ RN×(P2·C) where (P,P ) is the\npatch resolution and N = HW/P2. Followed by lin-\near projection V ∈R(P2·C)×H and position embedding\nVpos ∈R(N+1)×H, vis embedded into ¯v∈RN×H.\nThe text and image embeddings are summed with their cor-\nresponding modal-type embedding vectorsttype,vtype ∈RH,\nthen are concatenated into a combined sequence z0. The\ncontextualized vector z is iteratively updated through D-\ndepth transformer layers up until the ﬁnal contextualized\nsequence zD. pis a pooled representation of the whole mul-\ntimodal input, and is obtained by applying linear projection\nWpool ∈RH×H and hyperbolic tangent upon the ﬁrst index\nof sequence zD.\nFor all experiments, we use weights from ViT-B/32 pre-\ntrained on ImageNet, hence the name ViLT-B/32.5 Hidden\nsize H is 768, layer depth Dis 12, patch size P is 32, MLP\nsize is 3,072, and the number of attention heads is 12.\n3.2. Pre-training Objectives\nWe train ViLT with two objectives commonly used to train\nVLP models: image text matching (ITM) and masked lan-\nguage modeling (MLM).\nImage Text Matching. We randomly replace the aligned\nimage with a different image with the probability of 0.5. A\nsingle linear layer ITM head projects the pooled output fea-\nture pto logits over binary class, and we compute negative\nlog-likelihood loss as our ITM loss.\nPlus, inspired by the word region alignment objective in\nChen et al. (2019), we design word patch alignment (WPA)\nthat computes the alignment score between two subsets of\nzD: zD|t (textual subset) and zD|v (visual subset), using\nthe inexact proximal point method for optimal transports\n(IPOT) (Xie et al., 2020). We set the hyperparameters of\nIPOT following Chen et al. (2019) (β = 0.5,N = 50), and\nadd the approximate wasserstein distance multiplied by 0.1\nto the ITM loss.\nMasked Language Modeling. This objective is to predict\nthe ground truth labels of masked text tokens tmasked from\nits contextualized vector zD\nmasked|t. Following the heuris-\ntics of Devlin et al. (2019), we randomly mask twith the\nprobability of 0.15.\n5ViT-B/32 is pre-trained with ImageNet-21K and ﬁne-tuned\non ImageNet-1K for image classiﬁcation. We expect that weights\npre-trained on larger datasets (e.g., JFT-300M) would yield better\nperformance.\nWe use a two-layer MLP MLM head that inputszD\nmasked|t and\noutputs logits over vocabulary, just as the MLM objective\nof BERT. The MLM loss is then computed as the negative\nlog-likelihood loss for the masked tokens.\n3.3. Whole Word Masking\nWhole word masking is a masking technique that masks all\nconsecutive subword tokens that compose a whole word. It\nis shown to be effective on downstream tasks when applied\nto original and Chinese BERT (Cui et al., 2019).\nWe hypothesize that whole word masking is particularly cru-\ncial for VLP in order to make full use of information from\nthe other modality. For example, the word “giraffe” is to-\nkenized into three wordpiece tokens [\"gi\", \"##raf\",\n\"##fe\"] with the pre-trained bert-base-uncased\ntokenizer. If not all tokens are masked, say, [\"gi\",\n\"[MASK]\", \"##fe\"], the model may solely rely on the\nnearby two language tokens [\"gi\", \"##fe\"] to predict\nthe masked \"##raf\" rather than using the information\nfrom the image.\nWe mask whole words with a mask probability of 0.15\nduring pre-training. We discuss its impact in Section 4.5.\n3.4. Image Augmentation\nImage augmentation reportedly improves the generalization\npower of vision models (Shorten & Khoshgoftaar, 2019).\nDeiT (Touvron et al., 2020) that builds on ViT experimented\nwith various augmentation techniques (Zhang et al., 2017;\nYun et al., 2019; Berman et al., 2019; Hoffer et al., 2020;\nCubuk et al., 2020), and found them beneﬁcial for ViT train-\ning. However, the effects of image augmentation have not\nbeen explored within VLP models. Caching visual features\nrestrains region-feature-based VLP models from using im-\nage augmentation. Notwithstanding its applicability, neither\ndid Pixel-BERT study its effects.\nTo this end, we apply RandAugment (Cubuk et al., 2020)\nduring ﬁne-tuning. We use all the original policies except\ntwo: color inversion, because texts often contain color in-\nformation as well, and cutout, as it may clear out small but\nimportant objects dispersed throughout the whole image.\nWe use N = 2,M = 9 as the hyperparameters. We discuss\nits impact in Section 4.5 and Section 5.\n4. Experiments\n4.1. Overview\nWe use four datasets for pre-training: Microsoft COCO\n(MSCOCO) (Lin et al., 2014), Visual Genome (VG) (Kr-\nishna et al., 2017), SBU Captions (SBU) (Ordonez et al.,\n2011), and Google Conceptual Captions (GCC) (Sharma\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nTable 1.Pre-training dataset statistics. Caption length is the length\nof tokens from pre-trained bert-base-uncased tokenizer. †\nGCC and SBU provide only image urls, so we collect the images\nfrom urls which were still accessible.\nDataset # Images # Captions Caption Length\nMSCOCO 113K 567K 11.81 ±2.81\nVG 108K 5.41M 5.53 ±1.76\nGCC† 3.01M 3.01M 10.66 ±4.93\nSBU† 867K 867K 15.0 ±7.74\net al., 2018). Table 1 reports the dataset statistics.\nWe evaluate ViLT on two widely explored types of vision-\nand-language downstream tasks: for classiﬁcation, we use\nVQAv2 (Goyal et al., 2017) and NLVR2 (Suhr et al., 2018),\nand for retrieval, we use MSCOCO and Flickr30K (F30K)\n(Plummer et al., 2015) re-splited by Karpathy & Fei-Fei\n(2015). For the classiﬁcation tasks, we ﬁne-tune three times\nwith different initialization seeds for the head and data or-\ndering and report the mean scores. We report the standard\ndeviation in Table 5 along with ablation studies. For the\nretrieval tasks, we only ﬁne-tune once.\n4.2. Implementation Details\nFor all experiments, we use AdamW optimizer (Loshchilov\n& Hutter, 2018) with base learning rate of 10−4 and weight\ndecay of 10−2. The learning rate was warmed up for 10% of\nthe total training steps and was decayed linearly to zero for\nthe rest of the training. Note that downstream performance\nmay be further improved if we customize the hyperparame-\nters to each task.\nWe resize the shorter edge of input images to 384 and limit\nthe longer edge to under 640 while preserving the aspect\nratio. This resizing scheme is also used during object de-\ntection in other VLP models, but with a larger size of the\nshorter edge (800). Patch projection of ViLT-B/32 yields 12\n×20 = 240 patches for an image with a resolution of 384 ×\n640. As this is a rarely reached upper limit, we sample 200\npatches at maximum during pre-training. We interpolate\nVpos of ViT-B/32 to ﬁt the size of each image and pad the\npatches for batch training. Note that the resulting image\nresolution is four times smaller than 800 ×1,333, which\nis the size that all other VLP models use for inputs to their\nvisual embedders.\nWe use the bert-base-uncased tokenizer to tokenize\ntext inputs. Instead of ﬁne-tuning from pre-trained BERT,\nwe learn the textual embedding-related parameters tclass, T,\nand Tpos from scratch. Although beneﬁcial prima facie,\nemploying a pre-trained text-only BERT does not guarantee\nperformance gain for vision and language downstream tasks.\nCounterevidence has already been reported by Tan & Bansal\nTable 2.Comparison of ViLT-B/32 with other models on down-\nstream classiﬁcation tasks. We use MCAN (Yu et al., 2019) and\nMaxEnt (Suhr et al., 2018) for VQAv2 and NLVR2 w/o VLP SOTA\nresults. † additionally used GQA, VQAv2, VG-QA for pre-training.\n‡ made additional use of the Open Images (Kuznetsova et al., 2020)\ndataset. a⃝indicates RandAugment is applied during ﬁne-tuning.\n+⃝indicates model trained for a longer 200K pre-training steps.\nVisual\nEmbed Model Time\n(ms)\nVQAv2 NLVR2\ntest-dev dev test-P\nRegion\nw/o VLP SOTA ~900 70.63 54.80 53.50\nViLBERT ~920 70.55 - -\nVisualBERT ~925 70.80 67.40 67.00\nLXMERT ~900 72.42 74.90 74.50\nUNITER-Base ~900 72.70 75.85 75.80\nOSCAR-Base† ~900 73.16 78.07 78.36\nVinVL-Base†‡ ~650 75.95 82.05 83.08\nGrid Pixel-BERT-X152 ~160 74.45 76.50 77.20\nPixel-BERT-R50 ~60 71.35 71.70 72.40\nLinear\nViLT-B/32 ~15 70.33 74.41 74.57\nViLT-B/32a⃝ ~15 70.85 74.91 75.57\nViLT-B/32a⃝ +⃝ ~15 71.26 75.70 76.13\n(2019), where initializing with pre-trained BERT parameters\nled to weaker performance than pre-training from scratch.\nWe pre-train ViLT-B/32 for 100K or 200K steps on 64\nNVIDIA V100 GPUs with a batch size of 4,096. For all\ndownstream tasks, we train for ten epochs with a batch size\nof 256 for VQAv2/retrieval tasks and 128 for NLVR2.\n4.3. Classiﬁcation Tasks\nWe evaluate ViLT-B/32 on two commonly used datasets:\nVQAv2 and NLVR2. We use a two-layer MLP of hidden\nsize 1,536 as the ﬁne-tuned downstream head.\nVisual Question Answering. The VQAv2 task asks for\nanswers given pairs of an image and a question in natural\nlanguage. The annotated answers are originally in free-form\nnatural language, but it is a common practice to convert the\ntask to a classiﬁcation task with 3,129 answer classes. Fol-\nlowing this practice, we ﬁne-tune ViLT-B/32 on the VQAv2\ntrain and validation sets while reserving 1,000 validation\nimages and their related questions for internal validation.\nWe report the test-dev score results6 from the submission to\nthe evaluation server. ViLT falls short of VQA score com-\npared to other VLP models with a heavy visual embedder.\nWe suspect a detached object representation generated by\nthe object detector eases the training of VQA since questions\nin VQA typically ask about objects.\n6VQA score is calculated by comparing the inferred answer\nto 10 ground-truth answers: see https://visualqa.org/\nevaluation.html for details.\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nTable 3.Comparison of ViLT-B/32 with other VLP models on downstream zero-shot retrieval tasks. We exclude the models of which\nzero-shot retrieval performances were not reported in their original papers. † is pre-trained with a 10M proprietary vision-and-language\ndataset in addition to the 4M dataset of GCC+SBU. +⃝indicates model trained for a longer 200K pre-training steps.\nVisual\nEmbed Model Time\n(ms)\nZero-Shot Text Retrieval Zero-Shot Image Retrieval\nFlickr30k (1K) MSCOCO (5K) Flickr30k (1K) MSCOCO (5K)\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nRegion\nViLBERT ~900 - - - - - - 31.9 61.1 72.8 - - -\nUnicoder-VL ~925 64.3 85.8 92.3 - - - 48.4 76.0 85.2 - - -\nUNITER-Base ~900 80.7 95.7 98.0 - - - 66.2 88.4 92.9 - - -\nImageBERT† ~925 70.7 90.2 94.0 44.0 71.2 80.4 54.3 79.6 87.5 32.3 59.0 70.2\nLinear ViLT-B/32 ~15 69.7 91.0 96.0 53.4 80.7 88.8 51.3 79.9 87.9 37.3 67.4 79.0\nViLT-B/32+⃝ ~15 73.2 93.6 96.5 56.5 82.6 89.6 55.0 82.5 89.8 40.4 70.0 81.1\nTable 4.Comparison of ViLT-B/32 with other models on downstream retrieval tasks. We use SCAN for w/o VLP SOTA results. †\nadditionally used GQA, VQAv2, VG-QA for pre-training. ‡ additionally used the Open Images dataset. a⃝indicates RandAugment is\napplied during ﬁne-tuning. +⃝indicates model trained for a longer 200K pre-training steps.\nVisual\nEmbed Model Time\n(ms)\nText Retrieval Image Retrieval\nFlickr30k (1K) MSCOCO (5K) Flickr30k (1K) MSCOCO (5K)\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nRegion\nw/o VLP SOTA ~900 67.4 90.3 95.8 50.4 82.2 90.0 48.6 77.7 85.2 38.6 69.3 80.4\nViLBERT-Base ~920 - - - - - - 58.2 84.9 91.5 - - -\nUnicoder-VL ~925 86.2 96.3 99.0 62.3 87.1 92.8 71.5 91.2 95.2 48.4 76.7 85.9\nUNITER-Base ~900 85.9 97.1 98.8 64.4 87.4 93.1 72.5 92.4 96.1 50.3 78.5 87.2\nOSCAR-Base† ~900 - - - 70.0 91.1 95.5 - - - 54.0 80.8 88.5\nVinVL-Base†‡ ~650 - - - 74.6 92.6 96.3 - - - 58.1 83.2 90.1\nGrid Pixel-BERT-X152 ~160 87.0 98.9 99.5 63.6 87.5 93.6 71.5 92.1 95.8 50.1 77.6 86.2\nPixel-BERT-R50 ~60 75.7 94.7 97.1 59.8 85.5 91.6 53.4 80.4 88.5 41.1 69.7 80.5\nLinear\nViLT-B/32 ~15 81.4 95.6 97.6 61.8 86.2 92.6 61.9 86.8 92.8 41.3 72.0 82.5\nViLT-B/32a⃝ ~15 83.7 97.2 98.1 62.9 87.1 92.7 62.2 87.6 93.2 42.6 72.8 83.4\nViLT-B/32a⃝ +⃝ ~15 83.5 96.7 98.6 61.5 86.3 92.7 64.4 88.7 93.8 42.7 72.9 83.1\nNatural Language for Visual Reasoning. The NLVR2\ntask is a binary classiﬁcation task given triplets of two im-\nages and a question in natural language. As there are two\ninput images unlike the pre-training setup, multiple strate-\ngies exist7. Following OSCAR (Li et al., 2020b) and VinVL\n(Zhang et al., 2021), we use the pair method. Here, the\ntriplet input is reformulated into two pairs (question, im-\nage1) and (question, image2), and each pair goes through\nthe ViLT. The head takes the concatenation of two pooled\nrepresentations (p) as input and outputs the binary predic-\ntion.\nTable 2 shows the results. ViLT-B/32 maintains competitive\nperformance on both datasets considering its remarkable\ninference speed.\n4.4. Retrieval Tasks\nWe ﬁne-tune ViLT-B/32 on the Karpathy & Fei-Fei (2015)\nsplit of MSCOCO and F30K. For image-to-text and text-to-\nimage retrieval, we measure both zero-shot and ﬁne-tuned\nperformance8. We initialize the similarity score head from\n7UNITER proposed three downstream head setups: pair, triplet,\nand pair-biattn.\n8R@K corresponds to whether the ground truth is included\namong top K results from the validation set.\nthe pre-trained ITM head, particularly the part that computes\nthe true-pair logits. We sample 15 random texts as negative\nsamples and tune the model with cross-entropy loss that\nmaximizes the scores on positive pairs.\nWe report the zero shot retrieval results in Table 3 and ﬁne-\ntuned results in Table 4. At zero-shot retrieval, ViLT-B/32\nperforms better in general than ImageBERT despite Image-\nBERT’s pre-training on a larger (14M) dataset. At ﬁne-tuned\nretrieval, recalls for ViLT-B/32 are higher by a large margin\nthan the second fastest model (Pixel-BERT-R50).\n4.5. Ablation Study\nIn Table 5, we perform various ablations. More training\nsteps, whole word masking, and image augmentation come\nto be beneﬁcial, whereas an additional training objective\ndoes not help.\nIt has been reported that the number of training iterations\naffects the performance of self-supervised models (Devlin\net al., 2019; Chen et al., 2020a;b). As VLP is also a form\nof self-supervised training, we examine the effects of train-\ning durations. As expected, the performance constantly\nincreases as we train the model for longer training steps\n(rows 1~3). Masking whole words for the MLM objective\n(rows 3~4) and ﬁne-tuning with augmentation (row 6) also\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nTable 5.Ablation study of ViLT-B/32. w⃝ denotes whether whole word masking is used for pre-training. m⃝ denotes whether MPP\nobjective is used for pre-training. a⃝denotes whether RandAugment is used during ﬁne-tuning.\nTraining\nSteps\nAblation VQAv2 NLVR2 Flickr30k R@1 (1K) MSCOCO R@1 (5K)\nw⃝ m⃝ a⃝ test-dev dev test-P TR (ZS) IR (ZS) TR (ZS) IR (ZS)\n25K X X X 68.96 ± 0.07 70.83 ± 0.19 70.83 ± 0.23 75.39 (45.12) 52.52 (31.80) 53.72 (31.55) 34.88 (21.58)\n50K X X X 69.80 ± 0.01 71.93 ± 0.27 72.92 ± 0.82 78.13 (55.57) 57.36 (40.94) 57.00 (39.56) 37.47 (27.51)\n100K X X X 70.16 ± 0.01 73.54 ± 0.02 74.15 ± 0.27 79.39 (66.99) 60.50 (47.62) 60.15 (51.25) 40.45 (34.59)\n100K O X X 70.33 ± 0.01 74.41 ± 0.21 74.57 ± 0.09 81.35 (69.73) 61.86 (51.28) 61.79 (53.40) 41.25 (37.26)\n100K O O X 70.21 ± 0.05 72.76 ± 0.50 73.54 ± 0.47 78.91 (63.67) 58.76 (46.96) 59.53 (47.75) 40.08 (32.28)\n100K O X O 70.85 ± 0.13 74.91 ± 0.29 75.57 ± 0.61 83.69 (69.73) 62.22 (51.28) 62.88 (53.40) 42.62 (37.26)\n200K O X O 71.26 ± 0.06 75.70 ± 0.32 76.13 ± 0.39 83.50 (73.24) 64.36 (54.96) 61.49 (56.51) 42.70 (40.42)\nTable 6.Comparison of VLP models in terms of parameter size,\nFLOPs, and inference latency. Since FLOPs are proportional to\ninput size, we denote the number of input tokens (image+text) as\nsuperscripts (\"?\" when text length is unreported; we arbitrarily use\nlength 40). Although not captured in FLOPs count nor parameter\nsize (because it is not a tensor operation), note that per-class NMS\nfor 1,600 classes amounts to more than 500 ms in latency. NMS\nlatency varies a lot according to the number of detected classes.\nVisual\nEmbed Model #Params\n(M)\n#FLOPs\n(G)\nTime\n(ms)\nRegion\nViLBERT36+36 274.3 958.1 ~900\nVisualBERT36+128 170.3 425.0 ~925\nLXMERT36+20 239.8 952.0 ~900\nUNITER-Base36+60 154.7 949.9 ~900\nOSCAR-Base50+35 154.7 956.4 ~900\nVinVL-Base50+35 157.3 1023.3 ~650\nUnicoder-VL100+? 170.3 419.7 ~925\nImageBERT100+44 170.3 420.6 ~925\nGrid Pixel-BERT-X152146+? 144.3 185.8 ~160\nPixel-BERT-R50260+? 94.9 136.8 ~60\nLinear ViLT-B/32 200+40 87.4 55.9 ~15\ndrive performance. Further increase in training iterations\nto 200K improved performance on VQAv2, NLVR2, and\nzero-shot retrieval. We stop increasing the number of itera-\ntions over 200K as the ﬁne-tuned text retrieval performance\ndecreases afterward.\nAn additional masked region modeling (MRM) objective\nhas been the key for performance boost in VLP models\nsuch as Chen et al. (2019). We experiment with masked\npatch prediction (MPP) (Dosovitskiy et al., 2020) which\nmimics the effect of MRM in a form compatible with patch\nprojections. The patch vis masked with the probability of\n0.15, and the model predicts the mean RGB value of the\nmasked patch from its contextualized vector zD\nmasked|v. How-\never, MPP turns out not to be contributing to downstream\nperformance (rows 4~5). This result is in sharp contrast\nto the MRM objective on supervision signals from object\ndetection.\nTable 7.VLP model components. \"PC\" is for per-class manner\nNMS and \"CA\" is for class-agnostic. Following Tan & Bansal\n(2019), one single-modality layer is counted as 0.5 multi-modality\nlayer.\nVisual\nEmbed Model CNN\nBackbone\nRoI\nHead NMS Trans.\nLayers\nRegion\nViLBERT R101 C4 PC ~15\nVisualBERT X152 FPN PC 12\nLXMERT R101 C4 PC ~12\nUNITER-Base R101 C4 PC 12\nOSCAR-Base R101 C4 PC 12\nVinVL-Base X152 C4 CA 12\nUnicoder-VL X152 FPN PC 12\nImageBERT X152 FPN PC 12\nGrid Pixel-BERT-X152 X152 - - 12\nPixel-BERT-R50 R50 - - 12\nLinear ViLT-B/32 - - - 12\n4.6. Complexity Analysis of VLP Models\nWe analyze the complexity of VLP models in various terms.\nIn Table 6, we report the number of parameters, the number\nof ﬂoating-point operations (FLOPs), and the inference la-\ntency of the visual embedder and transformer. We exclude\nthe textual embedder because it is shared by all VLP mod-\nels9. The latency is averaged over 10K times on a Xeon\nE5-2650 CPU and an NVIDIA P40 GPU.\nThe input size in terms of image resolution and the length of\nconcatenated multimodal input sequence affects the number\nof FLOPs. We co-note the sequence lengths. The image\nresolution is 800 ×1,333 for region-based VLP models and\nPixel-BERT-R50, 600×1,000 for Pixel-BERT-X152, and\n384 ×640 for ViLT-B/32.\nIn Pixel-BERT and ViLT, visual tokens are sampled during\npre-training and used in full during ﬁne-tuning. We report\nthe maximum number of visual tokens.\nWe observe that the runtime of BERT-base-like transformers\nvaries only by <1 ms for input sequences of length under\n300. Since patch projection of ViLT-B/32 generates at most\n9FLOPs and time are neglectable because the operation is\nan embedding lookup. The 30K embedding dictionary used by\nbert-base-uncased has 23.47 M parameters\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nflowers wall cottages cloudy\na display of flowers growing out and over the retaining wall in front of cottages on a cloudy day.\nrug chair painting plant\na room with a rug, a chair, a painting, and a plant.\nFigure 4.Visualizations of transportation plan of word patch alignment. Best viewed zoomed in.\n240 image tokens, our model can still be efﬁcient even\nthough it receives a combination of image and text tokens.\n4.7. Visualization\nFigure 4 is an example of a cross-modal alignment. The\ntransportation plan of WPA expresses a heatmap for a text\ntoken highlighted in pink color. Each square tile represents\na patch, and its opacity indicates how much mass is trans-\nported from the highlighted word token.\nMore IPOT iterations– more than over 50 as in the training\nphase– help the visualization heatmap converge; empirically,\n1,000 iterations are sufﬁcient to get a clearly identiﬁable\nheatmap. We z-normalize the plan for each token and clamp\nthe values to [1.0, 3.0].\n5. Conclusion and Future Work\nIn this paper, we present a minimal VLP architecture,\nVision-and-Langauge Transformer (ViLT). ViLT is com-\npetent to competitors which are heavily equipped with con-\nvolutional visual embedding networks (e.g., Faster R-CNN\nand ResNets). We ask for future work on VLP to focus more\non the modality interactions inside the transformer module\nrather than engaging in an arms race that merely powers up\nunimodal embedders.\nAlthough remarkable as it is, ViLT-B/32 is more of a proof\nof concept that efﬁcient VLP models free of convolution\nand region supervision can still be competent. We wrap\nup by pointing out a few factors that may add to the ViLT\nfamily.\nScalability. As shown in papers on large-scale transform-\ners (Devlin et al., 2019; Dosovitskiy et al., 2020), the per-\nformance of pre-trained transformers scale well given an\nappropriate amount of data. This observation paves the\nway for even better performing ViLT variants (e.g., ViLT-L\n(large) and ViLT-H (huge)). We leave training larger mod-\nels for future work because aligned vision-and-language\ndatasets are yet scarce.\nMasked Modeling for Visual Inputs. Considering the\nsuccess of MRM, we speculate that the masked modeling\nobjective for the visual modality helps by preserving the\ninformation up until the last layer of the transformer. How-\never, as observed in Table 5, a naive variant of MRM on\nimage patches (MPP) fails.\nCho et al. (2020) proposed to train their grid RoIs on masked\nobject classiﬁcation (MOC) tasks. However, the visual vo-\ncabulary cluster in this work was ﬁxed during the vision\nand language pre-training together with the visual back-\nbone. For trainable visual embedders, one-time clustering\nis not a viable option. We believe that alternating clustering\n(Caron et al., 2018; 2019) or simultaneous clustering (Asano\net al., 2019; Caron et al., 2020) methods studied in visual\nunsupervised learning research could be applied.\nWe encourage future work that does not use region super-\nvision to devise a more sophisticated masking objective for\nthe visual modality.\nAugmentation Strategies. Previous work on contrastive\nvisual representation learning (Chen et al., 2020a;b) showed\nthat gaussian blur, not employed by RandAugment, brings\nnoticeable gains to downstream performance compared with\na simpler augmentation strategy (He et al., 2020). Explo-\nration of appropriate augmentation strategies for textual and\nvisual inputs would be a valuable addition.\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nReferences\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\nGould, S., and Zhang, L. Bottom-up and top-down atten-\ntion for image captioning and visual question answering.\nIn Proceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pp. 6077–6086, 2018.\nAsano, Y ., Rupprecht, C., and Vedaldi, A. Self-labelling via\nsimultaneous clustering and representation learning. In\nInternational Conference on Learning Representations,\n2019.\nBerman, M., Jégou, H., Vedaldi, A., Kokkinos, I., and\nDouze, M. Multigrain: a uniﬁed image embedding for\nclasses and instances. arXiv preprint arXiv:1902.05509,\n2019.\nBugliarello, E., Cotterell, R., Okazaki, N., and Elliott, D.\nMultimodal pretraining unmasked: Unifying the vision\nand language berts. arXiv preprint arXiv:2011.15124,\n2020.\nCaron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep\nclustering for unsupervised learning of visual features. In\nProceedings of the European Conference on Computer\nVision (ECCV), pp. 132–149, 2018.\nCaron, M., Bojanowski, P., Mairal, J., and Joulin, A. Un-\nsupervised pre-training of image features on non-curated\ndata. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pp. 2959–2968, 2019.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski,\nP., and Joulin, A. Unsupervised learning of visual fea-\ntures by contrasting cluster assignments. arXiv preprint\narXiv:2006.09882, 2020.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In International conference on machine\nlearning, pp. 1597–1607. PMLR, 2020a.\nChen, X., Fan, H., Girshick, R., and He, K. Improved\nbaselines with momentum contrastive learning. arXiv\npreprint arXiv:2003.04297, 2020b.\nChen, Y .-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,\nCheng, Y ., and Liu, J. Uniter: Learning universal image-\ntext representations. arXiv preprint arXiv:1909.11740,\n2019.\nCho, J., Lu, J., Schwenk, D., Hajishirzi, H., and Kembhavi,\nA. X-lxmert: Paint, caption and answer questions with\nmulti-modal transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 8785–8805, 2020.\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V . Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\nWorkshops, pp. 702–703, 2020.\nCui, Y ., Che, W., Liu, T., Qin, B., Yang, Z., Wang, S., and\nHu, G. Pre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 4171–4186,\n2019.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929, 2020.\nFaghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. Vse++: Im-\nproving visual-semantic embeddings with hard negatives.\narXiv preprint arXiv:1707.05612, 2017.\nGan, Z., Chen, Y .-C., Li, L., Zhu, C., Cheng, Y ., and Liu, J.\nLarge-scale adversarial training for vision-and-language\nrepresentation learning. arXiv preprint arXiv:2006.06195,\n2020.\nGoyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answer-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 6904–6913, 2017.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016.\nHe, K., Gkioxari, G., Dollár, P., and Girshick, R. Mask r-\ncnn. In Proceedings of the IEEE international conference\non computer vision, pp. 2961–2969, 2017.\nHe, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 9729–\n9738, 2020.\nHoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoeﬂer,\nT., and Soudry, D. Augment your batch: Improving\ngeneralization through instance repetition. InProceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 8129–8138, 2020.\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nHuang, Z., Zeng, Z., Liu, B., Fu, D., and Fu, J. Pixel-bert:\nAligning image pixels with text by deep multi-modal\ntransformers. arXiv preprint arXiv:2004.00849, 2020.\nJiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., and\nChen, X. In defense of grid features for visual question\nanswering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 10267–\n10276, 2020.\nJiang, Y ., Natarajan, V ., Chen, X., Rohrbach, M., Batra, D.,\nand Parikh, D. Pythia v0. 1: the winning entry to the vqa\nchallenge 2018. arXiv preprint arXiv:1807.09956, 2018.\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic align-\nments for generating image descriptions. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 3128–3137, 2015.\nKrishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K.,\nKravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma,\nD. A., et al. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. In-\nternational journal of computer vision , 123(1):32–73,\n2017.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NIPS, 2012.\nKuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin,\nI., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M.,\nKolesnikov, A., et al. The open images dataset v4. Inter-\nnational Journal of Computer Vision, pp. 1–26, 2020.\nLee, K.-H., Chen, X., Hua, G., Hu, H., and He, X. Stacked\ncross attention for image-text matching. In Proceedings\nof the European Conference on Computer Vision (ECCV),\npp. 201–216, 2018.\nLi, G., Duan, N., Fang, Y ., Gong, M., Jiang, D., and Zhou,\nM. Unicoder-vl: A universal encoder for vision and lan-\nguage by cross-modal pre-training. In AAAI, pp. 11336–\n11344, 2020a.\nLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang,\nK.-W. Visualbert: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557,\n2019.\nLi, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,\nL., Hu, H., Dong, L., Wei, F., et al. Oscar: Object-\nsemantics aligned pre-training for vision-language tasks.\nIn European Conference on Computer Vision, pp. 121–\n137. Springer, 2020b.\nLin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-\nmanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco:\nCommon objects in context. In European conference on\ncomputer vision, pp. 740–755. Springer, 2014.\nLoshchilov, I. and Hutter, F. Decoupled weight decay reg-\nularization. In International Conference on Learning\nRepresentations, 2018.\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-\nand-language tasks. In Advances in Neural Information\nProcessing Systems, pp. 13–23, 2019.\nLu, J., Goswami, V ., Rohrbach, M., Parikh, D., and Lee, S.\n12-in-1: Multi-task vision and language representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 10437–\n10446, 2020.\nNguyen, D.-K., Goswami, V ., and Chen, X. Revisiting\nmodulated convolutions for visual counting and beyond.\narXiv preprint arXiv:2004.11883, 2020.\nOrdonez, V ., Kulkarni, G., and Berg, T. Im2text: Describing\nimages using 1 million captioned photographs. Advances\nin neural information processing systems, 24:1143–1151,\n2011.\nPerez, E., Strub, F., De Vries, H., Dumoulin, V ., and\nCourville, A. Film: Visual reasoning with a general con-\nditioning layer. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 32, 2018.\nPlummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,\nHockenmaier, J., and Lazebnik, S. Flickr30k entities:\nCollecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision, pp. 2641–\n2649, 2015.\nQi, D., Su, L., Song, J., Cui, E., Bharti, T., and Sacheti,\nA. Imagebert: Cross-modal pre-training with large-\nscale weak-supervised image-text data. arXiv preprint\narXiv:2001.07966, 2020.\nRadford, A., Sutskever, I., Kim, J., Krueger, G., and Agar-\nwal, S. Learning transferable visual models from natural\nlanguage supervision, 2021.\nRen, S., He, K., Girshick, R., and Sun, J. Faster r-cnn:\nTowards real-time object detection with region proposal\nnetworks. IEEE transactions on pattern analysis and\nmachine intelligence, 39(6):1137–1149, 2016.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., et al. Imagenet large scale visual recognition chal-\nlenge. International journal of computer vision, 115(3):\n211–252, 2015.\nViLT: Vision-and-Language Transformer Without Convolution or Region Supervision\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\nceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 2556–\n2565, 2018.\nShorten, C. and Khoshgoftaar, T. M. A survey on image\ndata augmentation for deep learning. Journal of Big Data,\n6(1):1–48, 2019.\nSu, W., Zhu, X., Cao, Y ., Li, B., Lu, L., Wei, F., and Dai, J.\nVl-bert: Pre-training of generic visual-linguistic represen-\ntations. arXiv preprint arXiv:1908.08530, 2019.\nSuhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H.,\nand Artzi, Y . A corpus for reasoning about natural\nlanguage grounded in photographs. arXiv preprint\narXiv:1811.00491, 2018.\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers.arXiv preprint\narXiv:1908.07490, 2019.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,\nA., and Jégou, H. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30:5998–6008, 2017.\nXie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggre-\ngated residual transformations for deep neural networks.\nIn Proceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pp. 1492–1500, 2017.\nXie, Y ., Wang, X., Wang, R., and Zha, H. A fast proximal\npoint method for computing exact wasserstein distance.\nIn Uncertainty in Artiﬁcial Intelligence , pp. 433–453.\nPMLR, 2020.\nYu, F., Tang, J., Yin, W., Sun, Y ., Tian, H., Wu, H.,\nand Wang, H. Ernie-vil: Knowledge enhanced vision-\nlanguage representations through scene graph. arXiv\npreprint arXiv:2006.16934, 2020.\nYu, Z., Yu, J., Cui, Y ., Tao, D., and Tian, Q. Deep modular\nco-attention networks for visual question answering. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 6281–6290, 2019.\nYun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo,\nY . Cutmix: Regularization strategy to train strong clas-\nsiﬁers with localizable features. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npp. 6023–6032, 2019.\nZhang, H., Cisse, M., Dauphin, Y . N., and Lopez-Paz,\nD. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412, 2017.\nZhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.,\nChoi, Y ., and Gao, J. Vinvl: Making visual representa-\ntions matter in vision-language models. arXiv preprint\narXiv:2101.00529, 2021.\nZhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J. J., and\nGao, J. Uniﬁed vision-language pre-training for image\ncaptioning and vqa. In AAAI, pp. 13041–13049, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8009465932846069
    },
    {
      "name": "Transformer",
      "score": 0.7211116552352905
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5195765495300293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5010011196136475
    },
    {
      "name": "Computation",
      "score": 0.49449607729911804
    },
    {
      "name": "Vocabulary",
      "score": 0.48609232902526855
    },
    {
      "name": "Architecture",
      "score": 0.4690832197666168
    },
    {
      "name": "Natural language processing",
      "score": 0.40464070439338684
    },
    {
      "name": "Speech recognition",
      "score": 0.3513191342353821
    },
    {
      "name": "Programming language",
      "score": 0.2679139971733093
    },
    {
      "name": "Linguistics",
      "score": 0.10643535852432251
    },
    {
      "name": "Artificial neural network",
      "score": 0.1014329195022583
    },
    {
      "name": "Voltage",
      "score": 0.08345732092857361
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}