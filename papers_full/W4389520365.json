{
    "title": "Conceptor-Aided Debiasing of Large Language Models",
    "url": "https://openalex.org/W4389520365",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2341116025",
            "name": "Li Yifei",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2139503071",
            "name": "Lyle Ungar",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2554339368",
            "name": "João Sedoc",
            "affiliations": [
                "New York University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963526187",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3035591180",
        "https://openalex.org/W4288029087",
        "https://openalex.org/W4287890645",
        "https://openalex.org/W3035102548",
        "https://openalex.org/W3099624838",
        "https://openalex.org/W4385571122",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W3177141404",
        "https://openalex.org/W3035241006",
        "https://openalex.org/W2962783425",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3093211917",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2902784465",
        "https://openalex.org/W3197876970",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2926555354",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W3177468621",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963578173",
        "https://openalex.org/W2972697496",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2786672974"
    ],
    "abstract": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use *conceptors*–a soft projection method–to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs’ performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT’s training takes all layers’ bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10703–10727\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nConceptor-Aided Debiasing of Large Language Models\nLi S. Yifei1, Lyle Ungar1, João Sedoc2,\n1University of Pennsylvania, 2New York University\n{liyifei, ungar}@upenn.edu, jsedoc@stern.nyu.edu\nAbstract\nPre-trained large language models (LLMs) re-\nflect the inherent social biases of their training\ncorpus. Many methods have been proposed to\nmitigate this issue, but they often fail to de-\nbias or they sacrifice model accuracy. We use\nconceptors–a soft projection method–to iden-\ntify and remove the bias subspace in LLMs\nsuch as BERT and GPT. We propose two meth-\nods of applying conceptors (1) bias subspace\nprojection by post-processing by the concep-\ntor NOT operation; and (2) a new architecture,\nconceptor-intervened BERT (CI-BERT), which\nexplicitly incorporates the conceptor projection\ninto all layers during training. We find that con-\nceptor post-processing achieves state-of-the-\nart (SoTA) debiasing results while maintaining\nLLMs’ performance on the GLUE benchmark.\nFurther, it is robust in various scenarios and\ncan mitigate intersectional bias efficiently by\nits AND operation on the existing bias sub-\nspaces. Although CI-BERT’s training takes all\nlayers’ bias into account and can beat its post-\nprocessing counterpart in bias mitigation, CI-\nBERT reduces the language model accuracy.\nWe also show the importance of carefully con-\nstructing the bias subspace. The best results\nare obtained by removing outliers from the list\nof biased words, combining them (via the OR\noperation), and computing their embeddings\nusing the sentences from a cleaner corpus.1\n1 Introduction\nLLMs such as BERT (Devlin et al., 2019) and\nGPT (Radford et al., 2019; Brown et al., 2020)\nare extremely successful in most natural language\nprocessing (NLP) tasks. However, since they are\ntrained on texts written by humans, the social bias\nis inherited and represented in the parameters of\nLLMs (Bolukbasi et al., 2016; Caliskan et al.,\n2022). For example, gender bias has been found in\ncontextualized embeddings (May et al., 2019; Zhao\n1Code link: https://github.com/realliyifei/\nconceptor-debias-llm.\nFigure 1: The pipeline of the conceptor-aided debias-\ning paradigm. We first use different settings (wordlists\nwith outlier filter and corpora) to generate the best bias\nsubspace (conceptor matrix), then apply them to two\nconceptor-aided debiasing methods and measure the\ndebiasing performance by two evaluation metrics. The\nexperiment is conducted on two LLMs: BERT and GPT.\net al., 2019). Therefore, many researchers have de-\nveloped debiasing techniques to improve the social\nfairness of NLP. However, such debiasing often\nfails to debias effectively and reduces language\nmodel performance in downstream tasks (Meade\net al., 2022). Furthermore, most debiasing methods\nneither follow Bommasani et al. (2020)’s sugges-\ntion to reduce bias in all layers nor tackle intersec-\ntional bias in an efficient way (Lalor et al., 2022).\nIn this paper, we challenge Karve et al. (2019)’s\nconclusion that conceptor negation fails to debias\nBERT stably. Instead, we are the first ones to empir-\nically find that as a soft shrinkage of the principal\ncomponents of the subspace defined by the list of\nbiased words (Liu et al., 2018), conceptors is a pow-\nerful tool to debias LLMs such as BERT and GPT\nusing either post-processing or continued-training.\nIn this process, we demonstrate the effect on debi-\nasing performance of choosing different corpora,\nsubspace removal methods, and criteria for select-\ning the list of bias attribute words that are used to\n10703\nconstruct the bias subspace. Further, we unprece-\ndentedly show that the conceptor can tackle varied\ntypes of biases (e.g. gender, race) intersectionally\nand efficiently by its unique logical operation.\nSpecifically, the attribute wordlists at the core\nof our method, and the methods we build on, are\nsets of attribute words related to bias. These typi-\ncally come in opposing pairs (e.g. ‘man’/‘woman’,\n‘prince’/‘princess’). Bolukbasi et al. (2016), Liang\net al. (2020) and others use the first principal com-\nponent (PC) to define the bias subspace–which can\nbe later subtracted entirely to debias. We similarly\nconstruct such subspaces, but use conceptors as\na ‘soft’ way to remove them–downscale the PC\nadjusted by a regularized identity map. When gen-\nerating such wordlists, it may be more representa-\ntive of bias by removing outliers in the embedding\nspace. Considering the embeddings are contextual-\nized, we select the contextualized token-level word\nembeddings using sentences from a specificcorpus.\nThen we stack them to generate a bias subspace in\na form of a conceptor matrix for the debiasing in\nthe next step. The pipeline is shown in Figure 1.\nThis work contributes the following:\n• Employs conceptor negation post-processing\nto debias LLMs such as BERT and GPT, beat-\ning most SoTA while retaining useful seman-\ntics and robustness in multiple scenarios\n• Explores conceptor-intervened BERT (CI-\nBERT)–a novel model architecture that con-\ntinues training BERT after incorporating con-\nceptors within all of BERT’s layers\n• Illustrates how different corpora, bias attribute\nwordlists, and outlier removal criteria impact\ndebiasing performance\n• Demonstrates conceptor-aided methods can\nbe generalized to different layers of LLMs and\nvarious types of biases and can mitigate them\nintersectionally by its unique logical operation\n2 Related Work\n2.1 Bias Manifestation\nMultiple demographic biases are common in so-\nciety. Among them, gender bias is the most well-\nstudied in academia, given its omnipresence and\nbi-polarity (Bolukbasi et al., 2016; May et al., 2019;\nKurita et al., 2019). Other social biases (e.g. racial,\nreligious) are also widespread in LLMs and at-\ntracting increasing attention (Nangia et al., 2020;\nNadeem et al., 2021; Meade et al., 2022).\nSuch social bias manifests itself in all layers of\nthe contextualized representations of LLMs like\nBERT and GPT (Bommasani et al., 2020); and\nKaneko and Bollegala (2021) show that debias-\ning all layers is more effective. Moreover, Lalor\net al. (2022) indicates the importance of addressing\nvaried biases in different dimensions. Thus, a new\nchallenge is raised on how to adapt current methods\nor develop novel paradigms to mitigate the bias in\neach layer and across multiple social dimensions.\n2.2 Debiasing Techniques and Challenges\nWe collect the mainstream SoTA debiasing meth-\nods (Overview: Meade et al. (2022); Xie and\nLukasiewicz (2023)), each with typical examples:\n(1) Bias Subspace Projection (BSP): the clas-\nsic method of bias subspace subtraction is to first\ncapture the bias subspace determined by attribute\nwords in the corpora and then project the bias direc-\ntion out from the language embeddings. This can be\ndone by post-processing as either hard projection\n(Bolukbasi et al., 2016;SENTENCE DEBIAS , Liang\net al., 2020) or soft projection (Karve et al., 2019).\nSome variants attain a similar goal by training a\nlinear classifier (INLP, Ravfogel et al., 2020) or\nfine-tuning LLMs (Kaneko and Bollegala, 2021).\n(2) Counterfactual Data Augmentation (CDA):\nswapping the bi-polar bias attribute words (e.g.\nher/him) to rebalance the training dataset and there-\nfore decrease the gender bias (Webster et al., 2020;\nBarikeri et al., 2021).\n(3) Dropout Regularization ( DROPOUT ): in\ncombination with an additional pre-training, in-\ncreasing the dropout components inside the\ntransformer-based language models can lead to\nlower bias (Webster et al., 2020).\n(4) SELF -DEBIAS : by using specific templates to\nencourage LLMs to generate toxic output and then\nmodifying the original output distribution of the\nmodel by a decoding algorithm, Schick et al. (2021)\nmakes use of the internal knowledge of language\nmodel to debias in a post-hoc manner.\nFurther, it is common to combine multiple such\nmethods. For instance, Zhao et al. (2019) and Liang\net al. (2020) combine the techniques of data aug-\nmentation and hard debiasing. However, per the\ndiscussion in Meade et al. (2022), the methods of-\nten neither debias as well as they claim (e.g. CDA,\nDROPOUT , SENTENCE DEBIAS ), nor do they main-\ntain the model’s capability for downstream tasks\n(e.g. CDA, DROPOUT , INLP). Worse, some tech-\nniques like CDA and DROPOUT increase the bias\n10704\nmeasured on SEAT–a test of language bias which\nwe will describe in Section 5. This dilemma chal-\nlenges us to develop new methods to further reduce\nbias while retaining meaningful semantics. Last,\nthe majority of debiasing methods ground the bias\nby word list; different lists can lead to different\ndebias performance (Antoniak and Mimno, 2021).\n2.3 Conceptors in NLP\nConceptors–a soft projection method support-\ning conceptual abstraction and logical opera-\ntions (Jaeger, 2014)–has been adapted to NLP do-\nmains such as debiasing (Liu et al., 2018; Sedoc\nand Ungar, 2019; Karve et al., 2019), continual\nlearning (Liu et al., 2019a), and semantic infor-\nmation enrichment (Liu et al., 2019b). Conceptor\nnegation is a soft shrinkage of the PCs of a sub-\nspace such as stop words or, in our case, of the\ntarget words defining the bias directions (Liu et al.,\n2018). Therefore it has the potential to debias better\nthan hard projection (e.g., Bolukbasi et al., 2016)\nwhile retaining enough semantics. Mathematically,\nit can capture, conjoin, and negate the bias con-\ncepts by logical operation, and thus can deal with\nintersectional bias efficiently.\nAlthough Karve et al. (2019) showed that debias-\ning conceptors can successfully debias both static\nembeddings such as Glove, Word2vec, and Fasttext,\nand contextual embeddings such as ELMo (Peters\net al., 2018), they state that the performance in\nBERT is far less consistent and effective than other\nword representations. We discover that this is the\nresult of their having selected the wrong set of at-\ntribute words, which leads to a poor bias subspace.2\nAnother difference is that the BERT tokens of at-\ntribute words should be averaged if they contain\nmultiple subwords after tokenization (Liang et al.,\n2020; Kaneko and Bollegala, 2021).\n3 The Mechanism of Conceptors\nLet us take a closer look at the mathematics of con-\nceptors: considering a set of vectors {x1,··· ,xn},\nxi ∈RN for all i ∈{1,··· ,n}, a conceptor ma-\ntrix Cis a regularized identity map that minimizes\n1\nn\nn∑\ni=1\n∥xi −Cxi∥2\n2 + α−2∥C∥2\nF (1)\n2We fixed the coding issues.\nwhere ∥·∥ F is the Frobenius norm and α−2 is a\nscalar hyper-parameter called aperture 3. It can be\nshown that Chas a closed-form solution:\nC = 1\nnXX⊤\n(1\nnXX⊤+ α−2I\n)−1\n(2)\nwhere X = [xi]i∈{1,···,n}is a data collection ma-\ntrix whose i-th column is xi. Intuitively, C is a\nsoft projection matrix on the linear subspace where\nthe typical components of xi samples lie so that\nit can capture the components that all representa-\ntions roughly share. Therefore, different from PCA\nprojection which removes the first several princi-\npal components (PCs) completely, the conceptors\nmethod softly downscales the PCs adjusted by a\nregularized identity map (Figure 2).\nFigure 2: Geometry of three conceptors in the shape of\nellipsoids (Jaeger, 2014).\nConceptors support Boolean operations such as\nNOT (¬), AND (∧) and OR (∨). For two arbitrary\nconceptors C1 and C2, we have\n¬C1 = I−C1 (3)\nC1 ∧C2 = (C−1\n1 + C−1\n2 −I)−1 (4)\nC1 ∨C2 = ¬(¬C1 ∧¬C2) (5)\n= I−((I−C1)−1 + (I−C2)−1 −I)−1\nThese logical operations are feasible if C1 and C2\nare created by the sets of equal sizes (Jaeger, 2014),\nas shown in Figure 3. This reveals the potential\nfor debiasing by combining different conceptors\nlearned from different bias subspaces. This is help-\nful both in combining different wordlists for the\nsame bias (e.g. gender) or different wordlists for\ndifferent protected classes (e.g. gender and race).\n4 Debiasing Sentence Representations\n4.1 Bias Subspace Setting\nWe explore the impact of different choices of at-\ntribute wordlists, the corpora used to find their em-\nbeddings, and how the wordlists are combined and\nfiltered to remove outliers, on the quality of bias\n3The default value of αis 1; we empirically find that grid-\nsearching is not helpful for debiasing so keep it as default\n10705\nFigure 3: Visualizing the boolean operations on two\nconceptor matrices. The OR (AND) operator leads to\na conceptor matrix (in pink color) with the smallest\n(largest) ellipsoid (He and Jaeger, 2018). In our case, it\nis then negated by the NOT operator to debias.\nsubspace, and hence the debiasing (Fig 1). Differ-\nent procedures of bias subspace construction yield\nsignificantly different debiasing performances.\nCorpora We compare three corpora: (1) the\nBrown Corpus (Francis and Kucera, 1979), a col-\nlection of text samples of mixed genres; (2) the\nStanford Sentiment Treebank (SST; Socher et al.,\n2013), a polarized dataset of 10,662 movie reviews;\nand (3) a Reddit Corpus (Liang et al., 2020), a\ndataset collected from discussion forums about re-\nlationships, electronics, and politics. The reason is\nto see how the language formality and topic breadth\nof texts impact the debiasing, the Brown corpus is\nformal and contains 15 genres, the Reddit corpus\nis informal with 3 domains and the SST corpus is\ninformal, has only one domain. They are used to\nprovide embeddings for the attribute words.\nCombining and Filtering Wordlists We com-\npare five ways of using three different wordlists to\ncreate conceptor bias subspaces.\nThe three wordlists are gender words originating\nfrom different sources: the pronouns wordlist is a\nset of common terms that are specific to particular\ngenders, such as ‘daughter’ or ‘son’; the extended\nwordlist, an extension of the former, contains less\nfrequent words such as ‘cowgirls’ or ‘fiancees’; and\npropernouns wordlist is comprised of proper nouns\nlike ‘Tawsha’, ‘Emylee’, and so on.\nThere are five methods of using these three\nwordlists to generate a bias subspace. We can\nuse each of them individually (their subspaces\nare named the same as themselves: pronouns, ex-\ntended, and propernouns, respectively). We can\nalso combine them in two ways: either by con-\ncatenating them as a single list generating a cor-\nresponding subspace (named all); or by running\nthe conceptor OR operation–a Boolean operation\nof conceptors described in subsection 2.3–on the\nthree corresponding conceptor matrices to generate\nwhat can be viewed as a union of the three bias\nsubspaces (named or).\nUnlike Karve et al. (2019), to study the effects\nof removing outliers from the wordlists, we first\nproject the LLM’s embeddings of the words in\nthe wordlist to a 2-dimensional UMAP cluster-\ning (McInnes et al., 2018) space, shown in Fig-\nure 4, and then filter the outliers by percentile on\ntheir (x,y)-coordinate. The outliers are defined as\nthe points that fall outside of 1.5 times the inter-\nrange (IR), which is the difference between p-th\nand (1−p)-th percentile. We iterate pfrom 0.1 to\n1.0 with step size 0.1 to generate different wordlists\nand then test how well each debiases. Our goals are\nto detect the negative effect of outliers on debiasing\nperformance and to explore which percentile here\nis optimal for debiasing.\n(a) Wordlist\n (b) Gender\nFigure 4: 2D UMAP BERT Embeddings of Words.\n4.2 Debiasing Methods\nWe propose and explore two kinds of conceptor-\naided debiasing: conceptor post-processing, and\nconceptor-intervened continued training. They are\nabbreviated as P.P. and C.T. respectively in tables.\nConceptor Bias Subspace Construction We\nconstruct the conceptor negation matrix ¬C as\ndemonstrated in Algorithm 1, where matrix X is\na stack of the within-sentence contextualized em-\nbeddings of the words. The words are determined\nby attribute wordlists and the sentences are from\nthe specified corpus as mentioned in Section 4.1.\nNote that we do not need the “difference space” of\nbipolar bias as the conceptor projection matrix is\napplied to the original space–in this way the con-\nceptor method is different from the so-called hard-\ndebiasing (Bolukbasi et al., 2016). To ensure con-\ntextualization we remove the less-than-four-word\nsentences. Also, following Kaneko and Bollegala\n(2021)’s idea, if a word crosses multiple sub-tokens,\n10706\nthen its contextualized embedding is computed by\naveraging the contextualized embeddings of its con-\nstituent sub-tokens, which is different than the pre-\nvious conceptor works.\nConceptor Negation and Post-Processing Next,\nwe post-process the sentence embeddings twhich\ncontain attribute words and target words, by tak-\ning the matrix product of ¬C to subtract the bias\nsubspace, rendering debiased embeddings t∗, as\ndemonstrated in the last part of Algorithm 1. Each\nBERT layer manifests different levels of bias (Bom-\nmasani et al., 2020). To maximize the effectiveness\nof ¬C, we want ¬Cto be generated from the corre-\nsponding layer. Therefore, we are the first ones to\ntest the debiasing performance by using different\nconceptor matrices generated by different layers of\nthe language model and to explore whether concep-\ntor post-processing generalizes well on each layer\nof LLMs and on different LLMs (BERT and GPT).\nIntersectioanl Debiasing Importantly, not only\ncan conceptors mitigate different types of biases\nsuch as gender and race respectively, but it can also\nconjoin and negate these biased concepts together\ndue to its magical logical operations. It is natural\nthat societal biases co-exist in multi-dimensions:\nsuch as “African Male” rather than “African” and\n“Male”. Therefore, it is efficient that conceptors\ncan tackle them intersectionally by utilizing the\npreviously constructed bias subspaces via its OR\noperation to construct the new mixed conceptors.\nConceptor Intervention and Continued Training\nThe varying levels of bias across BERT layers sug-\ngest the possible utility of an alternate approach\nto mitigate the bias. Accordingly, we construct a\nnew architecture, Conceptor-Intervened BERT (CI-\nBERT), by placing the corresponding conceptor\nmatrix after each layer of BERT (Figure 5). We\nthen continue training the entire model to incor-\nporate the model weights with the bias negation\ncaptured by the conceptors in each layer. Thus we\ncan take the biases in all layers into account so that\nwe can mitigate the layerwise bias simultaneously.\nCI-BERT architecture can be used in three ways.\nWe can load the original pre-trained weights to\nCI-BERT and directly render the language embed-\ndings (Type I; CI-BERT ×original weights). Alter-\nnatively, we can continue training the model using\nCI-BERT to get newly trained weights; then we\ncan load these weights back to either the original\noff-the-shelf BERT architecture (Type II; BERT\n×trained weights) or to the new architecture CI-\nBERT (Type III; CI-BERT ×trained weights).\n5 Quantifying Bias\n5.1 Sentence Encoder Association Test\nThe Sentence Encoder Association Test (SEAT)\n(May et al., 2019) is an extension of the Word\nEncoder Association Test (WEAT) (Caliskan et al.,\n2017). It can measure the bias at the sentence level\nin different kinds of bias (Meade et al., 2022).\nSEAT uses two types of words:attribute words\nWa (e.g. he/she) and target words Wt (e.g. occupa-\ntions), which we expect to be gender-neutral. That\nis, the associations between wa/w′\na ∈ Wa and\nwt ∈Wt should be no difference in the sentence-\ntemplate representations of LLMs.\nDenote the sentence sets of attribute words as A\nand A′, and of target words as T and T′, we have:\nc(A,A′,T,T ′) =\n∑\nt∈T\nc(t,A,A′) −\n∑\nt′∈T′\nc(t′,A,A′)\nwhere for each sentence s, we have c(s,A,A ′),\nthe difference of the mean cosine similarity of s\nconcerning sentences from between Aand A′; as\nc(s,A,A′) = 1\n|A|\n∑\na∈A\ncos(s,a) − 1\n|A′|\n∑\na′∈A′\ncos(s,a′)\nThe amount of bias is given by the effect size\nd= µ({c(t,A,A′)}t∈T) −µ({c(t′,A,A′)}t′∈T′)\nσ({c(a,T,T′)}a∈A∪A′)\nwhere µand σdenote the mean and standard devia-\ntion, respectively. The smaller the absolute value of\ndis, the less bias has been detected. The one-sided\np-value measures the likelihood that a random re-\nsampling of the sentence set that contains attribute\nwords would generate the observed test statistic.\n5.2 Gender Co-Reference Resolution\nAs described by Gonen and Goldberg (2019), SEAT\ncan detect only the presence but not the absence\nof bias. To further understand how the conceptor-\naided methods work on debiasing, we adopt an\nend-task: gender co-reference resolution.\nWinoBias (Zhao et al., 2018) provides gender-\nbalanced co-reference tests to evaluate LLMs’ neu-\ntrality towards pronouns referring to occupations.\nThe tests include pro-stereotypical (PRO) scenarios,\nwhere gender pronouns match gender-conforming\noccupations (e.g., her/nurse), and anti-stereotypical\n(ANTI) scenarios, where gender pronouns apply to\ndisfavored occupations. The bias is measured by\nthe average and absolute difference in F1 scores\nbetween the PRO and ANTI subsets.\n10707\nAlgorithm 1 CONCEPTOR -DEBIAS : a conceptor-aided post-process algorithm for debiasing LLMs.\nRequire: large language model M θ (with parameters θ), bias attribute wordlist W, and corpus S.\n1: X←[ ]\n2: for each word w∈W do\n3: for each sentence s∈S do\n4: if winside sthen\n5: wc ←the embedding of winside M θ(s) // get contextualized word embedding\n6: X ←X+ wc // stack as a matrix\n7: end if\n8: end for\n9: end for\n10: C ←XX⊤(XX⊤+ I)−1 // construct conceptor bias subspace\n// note that different Xi yields different Ci for arbitrary i\n11: C ←(C−1\n1 + C−1\n2 −I)−1 // cross bias subspaces by AND operator (if intersectional debias)\n12: C ←I−((I−C1)−1 + (I−C2)−1 −I)−1 // unite bias subspaces by OR operator (for robust debias)\n13: ¬C ←I−C // make negation conceptor matrix by NOT operator\n14: for each new sentence tdo\n15: t∗←¬C·M θ(t) // debias sentence by projection\n16: end for\nFigure 5: Conceptor-Intervened BERT (CI-BERT). Each\nmodel’s layer takes the matrix product (blue circle) of\nthe conceptor-X generated from the corresponding layer\nX. It can be used directly or continually trained.\nBased on this, de Vassimon Manela et al. (2021)\ndevelop two intuitive metrics, skew and stereotype,\nto better probe model fairness. In formula,\nµSkew ≜\n⏐⏐F1Mpro −F1Fpro\n⏐⏐+⏐⏐F1M\nanti −F1F\nanti\n⏐⏐\n2\nµStereo ≜\n⏐⏐F1Mpro −F1M\nanti\n⏐⏐+⏐⏐F1Fpro −F1F\nanti\n⏐⏐\n2\nwhere superscripts M and F denote male and fe-\nmale respectively and F1 stands for the F1-score.\nIt is shown that there is an approximate trade-off\nbetween these two biases. The authors argue that\nthe T2 test set of WinoBias is better than the T1 test\nset at revealing bias, as the latter is less ambiguous\nto LLMs. Therefore, we only report T2 here.\n6 Debiasing Results\nThis section aims to answer these questions:\n• What is the best setting for bias subspace gen-\neration within conceptor-aided debiasing?\n• Given the best setting, can the conceptor post-\nprocessing mitigate bias and beat SoTA?\n• Does embedding conceptors into LLMs via\ncontinued training beat post-processing?\n• What roles can conceptors operators–NOT,\nOR, AND–play in the debiasing pipeline?\nTo help comparison, the SoTA debiasing results\nfrom Meade et al. (2022) is included in the tables.\n6.1 Models\nTo investigate the generalization of conceptor de-\nbiasing, we explored different scales of typical\nLLM families, which are: BERT-T ( bert-tiny),\nBERT (bert-base-uncased), BERT-L (bert-large-\nuncased), GPT2 (gpt2), GPT-L (gpt2-large), and\nGPT-J (gpt-j). We did not test on GPT3 and Chat-\nGPT since their embedding models (e.g. text-\nembedding-ada-002) do not support the contextual-\nized embedding on token level. However, due to the\nsimilar modeling, once we have such embedding,\nconceptor techniques can be transferred.\n6.2 Bias Subspace Construction with\nRobustness Boosted via OR Operator\nWe construct the conceptor bias subspaces upon\nthe different combinations of corpora, wordlist se-\nlections, and outlier removal.\nTo evaluate corpora, by testing on the last layer\nof the BERT, we compare the debiasing result of\nthree different corpora: Brown, SST, and Reddit on\nSEAT. Table 8 shows that Brown stably provides\nthe best debiasing result even if using different\nwordlist subspaces. The SST corpus is a close sec-\nond, while Reddit is by far the worst. The style of\nthe Reddit corpus is most likely least similar to that\nof the SEAT evaluations.\n10708\nTo evaluate alternate methods of constructing\nthe bias wordlist subspace, we use the five sub-\nspaces described in Section 4.1. Among them, the\nor subspace is the most robust; see Table 9, 10\nand 11. Combining the pronouns, extended and\npropernouns subspaces with or represents the dis-\ntinct yet union concepts (and hence subspaces) of\neach of the wordlists, thus both outperforming in-\ndividual wordlists and outperforming the all sub-\nspace, which simply concatenates all the wordlists,\ngiving a less precise subspace definition.\nTo evaluate wordlist outlier removal, we define\nthe outliers by the UMAP filter as discussed in sec-\ntion 4.1 and generate different percentages of the\nwords that are used to capture bias. For example,\nthe all subspace has 2071 words within 0.5−1.0\npercentile, 2061 in the 0.4 percentile, 1601 in the\n0.3 percentile, 430 in the 0.2 percentile, and 82\nin the 0.1 percentile (Table 6). We observe that\nincluding fewer words often leads to higher debias-\ning performance, presumably due to the removal of\noutliers. However, an extremely small percentile,\nsay 0.1, would harm the effectiveness of debiasing\nbecause of the inadequate loss being left (Table 9,\n10 and 10). Similar results are obtained if using\nT-SNE (Van der Maaten and Hinton, 2008).\nIn conclusion, the optimal setting for BERT-T\nis “sst-0.5-or” (SST; percentile 0.5; or subspace);\nsimilarly, for BERT is “brown-0.4-or” (Brown; per-\ncentile 0.4; or subspace). For other models, if not\nmentioned, it is default as “brown-1.0-or”. Hence-\nforth, these settings are held for the conceptor de-\nbiasing on the models respectively.\n6.3 Post-Processing Debias via NOT Operator\nFor general debiasing via conceptor negation post-\nprocessing, the performance is excellent. The SEAT\nscore of BERT decreases from 0.620 to around\n0.350−0.400 in Brown Corpus (Table 9), and can\nbe as low as 0.311 if using the setting “brown-0.4-\nor”, outperforming the debiasing result of CDA,\nDROPOUT and SENTENCE DEBIASE (Table 1). The\nsuccess of debiasing is further verified by Wino-\nBias (Table 2), where the skew bias drops from\n38.3 to 22.3 without any additional fine-tuning.\nAlthough the stereotype bias increases, it is not\nonly expected since these two biases are trade-offs\nbut also acceptable, as they now reach a good bal-\nance (de Vassimon Manela et al., 2021).\nThe debiasing conceptors are robust and general-\nizable, as shown in Table 1, the debias performance\nis consistent in different scales of BERT and GPT\nmodels. Note that the settings of BERT-L, GPT2-L\nand GPT-J are nottuned (i.e. default setting), which\nmeans that they can likely reach much lower SEAT\nscores. Moreover, conceptors can mitigate the bias\nin almost all scenarios, no matter using which cor-\npus, bias subspace, or wordlist threshold (Table 9,\n11 and 10); no matter which LLMs (Table 1, 15,\n17, 16 and ,19) ; no matter in which layer (Table 12,\n13 and 18); and no matter which type of biases\n(Table 3, Table 21 and 22).\nModel F1 Male F1 Female Bias\nPro Anti Pro Anti Stereo Skew\nBERT 66.4 58.9 31.8 17.0 11.2 38.3\n+ Conceptor P.P. 69.5 48.1 52.8 20.1 27.0 22.3\n+ Conceptor C.T. 41.0 39.3 57.6 56.6 4.1 17.0\nTable 2: F1 of skew and stereotype biases in WinoBias.\n6.4 Intersectional Debias via AND Operator\nTable 3 empirically shows that conceptors not only\ncan mitigate the different type of biases, but also\ncan intersect the existing bias subspaces (e.g. gen-\nder and race) to create a mixed conceptor matrix in\nan efficient way and to debias gender and race re-\nspectively. Furthermore, for assessing the intersec-\ntional debiasing, we employ the I1-I5 intersectional\nbias test introduced by Tan and Celis (2019). They\nadapt the SEAT to examine the privilege associated\nwith the combination of being African/European\nAmerican and being male or female. The results\ndemonstrate that such intersected conceptor formed\nvia the AND operator can effectively reduce multi-\ndimensional bias, lowering the SEAT score from\n0.673 to 0.434, while its conceptor counterparts\nfocused solely on single-dimensional bias can only\nreduce the score to 0.613 and 0.635 respectively.\n6.5 Conceptor-Intervention Debias\nWe use CI-BERT architecture to continue to train\nthe models to get the new weights. Then we\ndemonstrate the combinations of architectures and\nweights as an ablation study (Type I, II, and III).\nAmong them, Type III can outperform conceptor\npost-processing (Table 1), and Type I and II (Ta-\nble 4). Compared to the SEAT score after post-\nprocessing, Type I can outperform it at each layer\nof BERT-T but underperform it at most layers of\nBERT (Table 13 and 18).\nIn short, using the CI-BERT with the newly\ntrained weights could receive the lowest bias in\nthe model and is promising to beat post-processing.\nFor example, when using the setting “brown-0.4-\n10709\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Gender (AAvg.)\nBERT 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n+ Conceptor P.P. (tuned) 0.388 -0.078 -0.292 0.179 0.594 ∗ 0.335 ↓0.309 0.311\n+ Conceptor C.T. 0.227 0.426 -0.341 -0.253 -0.344 -0.088 ↓0.340 0.280\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nBERT-L 0.370 -0.015 0.418 ∗ 0.221 -0.258 0.711 ∗ 0.332\n+ Conceptor P.P. (default) 0.197 -0.206 0.064 0.065 -0.371 0.337 ↓0.125 0.207\nGPT2 -0.510 0.057 -0.274 -0.186 -0.369 -0.313 0.285\n+ Conceptor P.P. (tuned) 0.092 0.316 -0.001 0.064 -0.035 -0.062 ↓0.190 0.095\nGPT2-L 1.093 ∗ 0.192 0.214 1.354 ∗ 0.861∗ 1.157∗ 0.812\n+ Conceptor P.P. (default) 1.055∗ 0.008 -0.089 1.406∗ 0.282 0.992 ∗ ↓0.173 0.639\nGPT-J 1.299 ∗ 0.300 0.962 ∗ 1.434∗ 0.617∗ 1.031∗ 0.940\n+ Conceptor P.P. (default) 1.184∗ 0.285 0.661 ∗ 1.284∗ 0.558∗ 1.024∗ ↓0.107 0.833\nTable 1: SEAT effect size of gender debiased BERT and GPT model. Effect sizes closer to 0 indicate less biased\nsentence representations (bolded value). Statistically significant effect sizes at p <0.01 are denoted by *. The\nfinal column is the average absolute SEAT score of the first six columns.Default means using the default setting:\nbrown corpus, no wordlist filtering, and OR subspace; while tuned means using the optimal combination of corpus,\nwordlist percentile, and conceptor bias subspace. P.P. stands for post-processing, while C.T. stands for continued\ntraining. The full version is in Appendixes E and G.\nModel Gender (AAvg.) Race (AAvg.) SEAT-I1 SEAT-I2 SEAT-I3 SEAT-I4 SEAT-I5 Intersect (AAvg.)\nBERT 0.620 0.620 0.389 ∗ -0.424 1.195 ∗ 0.525∗ 0.834∗ 0.673\n+ Gender Conceptor ↓0.309 0.311 N/A 0.394 ∗ -0.456 1.156∗ 0.413∗ 0.755∗ ↓0.060 0.613\n+ Race Conceptor N/A ↓0.043 0.577 0.394∗ -0.456 1.156∗ 0.413∗ 0.755∗ ↓0.062 0.635\n+ Intersect Conceptor§ ↓0.029 0.591 ↓0.016 0.604 0.214 -0.474 0.872∗ 0.207 0.403 ∗ ↓0.239 0.434\nTable 3: SEAT effect size of race, gender, and intersectionally debiased BERT model, where the absolute average\nSEAT score of gender, race, and intersect are across 6, 7, 5 tests, respectively. The full version is in Appendix I.\n§It indicates the conceptor matrix generated by its negated AND operation of gender conceptor matrix and race conceptor matrix\nType CI-BERT (Arch.) Trained Weights SEAT\n(Orig.) 0.620\nI ✓ 0.336\nII ✓ 0.592\nIII ✓ ✓ 0.280\nTable 4: The ablation study of architecture and weights\nof CI-BERT evaluated by SEAT (the same as Table 1).\nor”, the lowest SEAT score is 0.280, beating the\npost-processing result of 0.311 and more than half\nof the SoTA methods. This is verified again by gen-\nder co-reference resolution in Table 2–in compar-\nison to its post-processing counterpart, CI-BERT\ncontinued training lowers both stereotype bias by\n22.9 and skew bias by 5.3 from Test Set 2 of Wino-\nBias. This is non-trivial since these two biases are\na tradeoff and thus generally hard to decrease si-\nmultaneously (de Vassimon Manela et al., 2021).\nTo further study the feasibility and robustness\nof CI-BERT continued training concerning the\nmodel property. We experiment on both BERT-T\nand BERT and plot the average SEAT curve along\nwith training steps (Figure 6). Both can beat their\npost-processing counterparts in some steps during\nthe early training stage, and then the bias fluctu-\nates and increases again, perhaps due to the model\nrelearning the bias during continued training, or\noversaturating the conceptor bias projections into\nits weights.\nIn comparison, the continual-trained CI-BERT\ncan more stably lower the bias in smaller Bert\nmodel. We suspect this is related to the model com-\nplexity. The debiasing projection of the last layer’s\nconceptor matrix is upon the last hidden state and\nthus generated transitively from all the prior layers.\nCurrently, we are embedding all layers’ conceptor\nmatrices, which may lead to overlapping and re-\ndundant debiasing projection from the prior layers.\n6.6 Maintaining Meaningful Semantics\nTo understand how conceptor debiasing impacts\nthe downstream natural language understanding\n(NLU) tasks, the GLUE benchmark (Wang et al.,\n2018)–comprised of nine different tasks–is used to\nevaluate the model after debiasing (Table 5). While\nthere seems to be no consensus about the quanti-\n10710\nFigure 6: SEAT score curve of CI-BERT continued train-\ning. We compare the results with the original embed-\ndings and post-processed embeddings. We test on the\nlast layer of BERT-T (top) and BERT (bottom).\ntative threshold of the trade-off between language\nmodeling capability and debiasing performance, a\nsmall decrease may be acceptable depending on\nthe downstream tasks. We believe that, in an ideal\nscenario, the performance on the GLUE benchmark\nshould not significantly decline after debiasing.\nThe conceptor post-processing of BERT can re-\ntain and even improve the useful semantics (in-\ncrease the average GLUE score by 1.77) for down-\nstream tasks without damage to the model’s abil-\nity, outperforming any other listed SoTA debiasing\nmethods. Even if scaling to BERT-L, the GLUE\nis still slightly higher. In comparison, the average\nGLUE score of conceptor continued-training BERT\nis relatively low, although it is not the worst among\nall the methods. This indicates that the continued-\ntraining method, while still capable of outperform-\ning its post-processing counterpart under the same\nsetting, may sacrifice NLU abilities.\nSince GPT is an autoregressive model, we\nadopt the SequenceClassification counterpart on\nthe GLUE benchmark, following the method of\nMeade et al. (2022). The score of GPT2 and GPT-J\nare decreased slightly by 0.11-0.14, which is an\naffordable cost, while GPT2-L increases slightly\nby 0.05.\nModel Average\nBERT 77.74\n+ Conceptor P.P. ↑1.77 79.51\n+ Conceptor C.T. ↓1.03 76.71\n+ CDA ↓0.22 77.52\n+ DROPOUT ↓1.46 76.28\n+ INLP ↓0.99 76.76\n+ SENTENCE DEBIAS ↑0.07 77.81\nBERT-L 78.81\n+ Conceptor P.P. ↑0.05 78.86\nGPT2 73.01\n+ Conceptor P.P. ↓0.11 72.90\nGPT2-L 75.84\n+ Conceptor P.P. ↑0.05 75.89\nGPT-J 78.22\n+ Conceptor P.P. ↓0.14 78.06\nTable 5: GLUE validation set results for gender debi-\nased BERT and GPT model. The full version is in Ap-\npendixes E and H.\nNotice that even when trained on the original\nBERT architecture, the average GLUE score still\ndrops about 0.3 point. Thus, the lower GLUE score\nhere is not completely caused by CI-BERT, though\nthe actual reason is hard to determine due to train-\ning randomness (McCoy et al., 2019).\n7 Conclusion and Future Work\nWe have shown that conceptor-aided debiasing\ncan successfully mitigate bias from LLMs (e.g.,\nBERT, GPT) by its NOT operation. Specifically,\nconceptor post-processing outperforms many state-\nof-the-art debiasing methods in both debiasing ef-\nfectiveness and semantic retention. We also tested a\nnew architecture, conceptor-intervened BERT (CI-\nBERT), which in combination with continued train-\ning, takes all layers’ biases into account and shows\nthe promise to outperform its post-processing coun-\nterpart. However, it might be at the cost of in-\ncreased instability and worse semantic retention.\nIn all cases, the best conceptor matrices are gener-\nally obtained when the bias subspace is constructed\nusing (1) a cleaner corpus, (2) the union of different\nrelated wordlists (e.g. pronouns, roles, and names)\nby the conceptor OR operation, and (3) removal\nof outliers from the wordlists. We further show\nthat cocneptor-aided debiasing is robust in differ-\nent LLMs, various layers of models, and varied\ntypes of biases. Moreover, conceptors can utilize\nthe current conceptor matrices to construct a new\nconceptor matrix to mitigate the intersectional bias\nin an efficient manner by AND operation.\nIn future research we plan to make CI-BERT and\nintersectional conceptors more robust and effective.\n10711\nLimitations\nWe list several limitations of our work below.\n1) We only test the binary bias. We only test\nthe bias in pairs via SEAT and WinoBias, for ex-\nample, ‘male’/‘female’ or ‘young’/‘old’. However,\nit is widely recognized that terms in gender, race,\netc. can be multi-polar.\n2) Our result is limited to English, and both\ncorpora and wordlist tend towards North Amer-\nican social biases. The whole of our experiment\nis conducted in English. In addition, Brown and\nSST Corpora are collected entirely in the North\nAmerican environment. So are the wordlists. There-\nfore, it is expected that they skew towards North\nAmerican social biases. When such models are de-\nbiased under the North American environment, it\nis necessary to understand how effective they are\nwhen transferred to other cultures.\n3) The generalization of conceptor-aided debias-\ning techniques can be tested more exhaustively.\nThis work has tested it on gender and race, but it\ncan also be tested on other types of bias such as\nreligious bias and hate speech.\nEthical Considerations\nThe definition and recognition of bias are subtle.\nFor example, we have used simple traditional bi-\nnary definitions of male and female to examine\ngender bias. This, of course, ignores a much wider\nvariety of gender identities, thus introducing an\nimplicit bias to the analysis. Similarly, studies on\nracial bias rely on possibly problematic definitions\nof race. Core to our debiasing method is the selec-\ntion of the wordlists. Each wordlist carries its own\nimplicit definitions of gender, race, and other im-\nportant dimensions. Care should be used to ensure\nthat they represent the desired categories. To this\nend, it is often useful to involve people from the\ncommunities whose language is being debiased to\nbetter represent their values and belief systems.\nOne should also be careful in the use of debi-\nasing. Removing signals about race or gender is\noften beneficial in reducing discrimination or in\nproducing better translations. It may also remove\nkey features of models needed for analyses. For ex-\nample, removing gender or race ‘signal’ from the\nmodel may severely hamper the use of that model\nin gender studies or work on critical race theory.\n“White-washing” models are not always a benefit;\nsometimes one wants to see the bias inherent in a\ncorpus.\nAcknowledgements\nFirst, we would like to thank the reviewers for their\nfruitful discussion with us. We would also like to\nthank Claire Daniele for her editorial support. Last\nbut not least, we appreciate PennNLP members for\ntheir helpful comments.\nReferences\nMaria Antoniak and David Mimno. 2021. Bad seeds:\nEvaluating lexical methods for bias measurement.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1889–1904, Online. Association for Computational\nLinguistics.\nSoumya Barikeri, Anne Lauscher, Ivan Vuli´c, and Goran\nGlavaš. 2021. RedditBias: A real-world resource for\nbias evaluation and debiasing of conversational lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1941–1955, Online. Association for\nComputational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nneural information processing systems, 29.\nRishi Bommasani, Kelly Davis, and Claire Cardie. 2020.\nInterpreting pretrained contextualized representations\nvia reductions to static embeddings. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4758–4781.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAylin Caliskan, Pimparkar Parth Ajay, Tessa E. S.\nCharlesworth, Robert Wolfe, and Mahzarin R. Banaji.\n2022. Gender bias in word embeddings: A compre-\nhensive analysis of frequency, syntax, and semantics.\nArXiv, abs/2206.03390.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\n10712\nDaniel de Vassimon Manela, David Errington, Thomas\nFisher, Boris van Breugel, and Pasquale Minervini.\n2021. Stereotype and skew: Quantifying gender bias\nin pre-trained and fine-tuned language models. In\nEACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nW Nelson Francis and Henry Kucera. 1979. Brown\ncorpus manual. Letters to the Editor, 5(2):7.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 609–614,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nXu He and Herbert Jaeger. 2018. Overcoming catas-\ntrophic interference using conceptor-aided backprop-\nagation. In International Conference on Learning\nRepresentations.\nHerbert Jaeger. 2014. Controlling recurrent neu-\nral networks by conceptors. arXiv preprint\narXiv:1403.3369.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1256–1266.\nSaket Karve, Lyle Ungar, and João Sedoc. 2019. Con-\nceptor debiasing of word representations evaluated\non weat. In Proceedings of the First Workshop on\nGender Bias in Natural Language Processing, pages\n40–48.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nJohn Lalor, Yi Yang, Kendall Smith, Nicole Forsgren,\nand Ahmed Abbasi. 2022. Benchmarking intersec-\ntional biases in NLP. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3598–3609, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nTianlin Liu, João Sedoc, and Lyle Ungar. 2018. Cor-\nrecting the common discourse bias in linear represen-\ntation of sentences using conceptors. arXiv preprint\narXiv:1811.11002.\nTianlin Liu, Lyle Ungar, and João Sedoc. 2019a. Con-\ntinual learning for sentence representations using con-\nceptors. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n3274–3279.\nTianlin Liu, Lyle Ungar, and Joao Sedoc. 2019b. Unsu-\npervised post-processing of word vectors via concep-\ntor negation. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 33, pages 6778–\n6785.\nChandler May, Alex Wang, Shikha Bordia, Samuel Bow-\nman, and Rachel Rudinger. 2019. On measuring so-\ncial biases in sentence encoders. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628.\nR Thomas McCoy, Junghyun Min, and Tal Linzen.\n2019. Berts of a feather do not generalize together:\nLarge variability in generalization across models\nwith similar test set performance. arXiv preprint\narXiv:1911.02969.\nLeland McInnes, John Healy, and James Melville. 2018.\nUmap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint\narXiv:1802.03426.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\n10713\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256, Online. Association for Computational\nLinguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in nlp. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nJoão Sedoc and Lyle Ungar. 2019. The role of protected\nclass word lists in bias identification of contextual-\nized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 55–61.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nYi Chern Tan and L Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. Advances in neural information pro-\ncessing systems, 32.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. ArXiv\npreprint 1804.07461.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models. arXiv preprint\narXiv:2010.06032.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nZhongbin Xie and Thomas Lukasiewicz. 2023. An em-\npirical analysis of parameter-efficient methods for de-\nbiasing pre-trained language models. arXiv preprint\narXiv:2306.04067.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,\nVicente Ordonez, and Kai-Wei Chang. 2019. Gender\nbias in contextualized word embeddings. In NAACL\n(short).\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias\nin coreference resolution: Evaluation and debiasing\nmethods. In NAACL (short).\n10714\nA Attribute Wordlist\nThe examples and sources of the attribute wordlists\nare given below. Due to space limitations, we would\nonly provide up to 50 words for each list.\nA.1 Pronouns Wordlist\nWords (in total 22): son, mother, daughter, him,\nbrother, girl, uncle, hers, grandfather, his, boy, her,\nfather, she, sister, man, female, aunt, woman, grand-\nmother, he, male.\nThey are the concatenation of W7_terms and\nW8_terms from WEAT list4.\nA.2 Extended Wordlist\nWords (randomly 50 of 388): paramour, abbesses,\nheadmistress, stepson, gods, congressman, gents,\nuncle, hers, wizard, cowgirls, fiancees, adultress,\nsororal, ladies, sons, uncles, actors, beards, heiress,\nfellas, salesman, princess, empress, masters, chair-\nwomen, miss, horsewomen, actor, mr., strong-\nwoman, barons, andrology, busboy, prince, hens,\nwomb, masseuse, lady, testosterone, daughter, girl,\nstateswoman, businessmen, women, fraternities,\naunts, boys, abbot, heroine, . . .\nThey are the concatenation of lists: WinoBias\nextra gendered words5, GN-GloVe male’s name6,\nand female’s name7.\nA.3 Propernouns Wordlist\nWords (randomly 50 of 7578): Broddie, Tony, Taw-\nsha, Emylee, Orelle, Gerrilee, Katusha, Georges,\nReine, Hayley, Deloria, Richmond, Wilfrid, Neille,\nFlorie, Riva, Sandro, Cooper, Thom, Pate, Niko-\nletta, Rodrique, Pat, Chuck, Theressa, Brett, Kas-\npar, Elric, Storm, Yule, Bubba, Thomasina, An-\nson, Margery, Abra, Benedict, Cy, Gertrud, Morly,\nJulina, Melly, Quinta, Paolo, Brynne, Maurene,\nAlexis, Ramsey, Sianna, Phebe, Alfred, . . .\nThey are the concatenation of lists: CMU male’s\nname8 and female’s name9.\n4https://github.com/jsedoc/ConceptorDebias/\nblob/master/lists/WEAT_lists.py\n5https://github.com/uclanlp/corefBias/blob/\nmaster/_site/WinoBias/wino/extra_gendered_words.\ntxt\n6https://github.com/uclanlp/gn_glove/blob/\nmaster/wordlist/female_word_file.txt\n7https://github.com/uclanlp/gn_glove/blob/\nmaster/wordlist/male_word_file.txt\n8https://www.cs.cmu.edu/Groups/AI/areas/nlp/\ncorpora/names/male.txt\n9https://www.cs.cmu.edu/Groups/AI/areas/nlp/\ncorpora/names/female.txt\nA.4 Race Wordlist\nWords (in total 8): africa, african, america, asia,\nasian, caucasian, china, europe\nA.5 Outlier Filtering\nTable 6 shows the number of remaining gender\nwords per percentile after being filtered on UMAP-\nclustering space.\nPercentile Bias Subspace Type\npronouns extended propernouns all\n0.5-1.0 22 388 7578 7988\n0.4 22 388 5443 6902\n0.3 11 372 4942 5140\n0.2 7 364 3194 3289\n0.1 4 67 1067 1087\nTable 6: The number of remaining words per percentile\nafter filtering on UMAP-clustering space. The model is\n“bert-base-uncased”.\nB Model Checkpoints\nWe use the Hugging Face Transformers pack-\nage (Wolf et al., 2019) in our experiments. The\nmodels and checkpoint names are given in Table 7.\nModel Checkpoint\nBERT-T prajjwal1/bert-tiny\nBERT bert-base-uncased\nBERT-L bert-large-uncased\nGPT2 gpt2\nGPT2-L gpt2-large\nGPT-J EleutherAI/gpt-j-6B\nTable 7: The package’s model and checkpoint name in\nour experiment.\nC Continued Training Details\nThe conceptor-intervened model is trained for one\nepoch by setting prediction_loss_only as true\nand per_device_train_batch_size as 8. Fol-\nlowing the training procedure in Devlin et al.\n(2019), we train by tasks Masked Language Model\n(MLM) and Next Sentence Prediction (NSP) si-\nmultaneously. The training corpus is the Wikipedia\ndump from datasets library (Lhoest et al., 2021).\nD GLUE Details\nBefore being evaluated on GLUE, each model is\ntrained for three epochs with the following settings:\nbatch_size 32, maximum_sequence_length 128,\nand learning_rate 2e−5; the same as Meade\net al. (2022).\n10715\nE Full Bert-Base-Uncased Model Results\n• Table 8 shows the gender debiasing result by\ndifferent types of the corpus, using the last\nlayer of “bert-base-uncased” as a benchmark.\n• Table 9, and 10, 11 show the post-processing\ngender debiasing result of different percentiles\nof wordlist on three different corpora: Brown,\nSST, and Reddit, respectively.\n• Table 12 and 13 show the post-processing and\nconceptor-intervened gender debiasing result\nof each layer on two different corpora: Brown\nand SST, respectively.\n• Table 14 contains GLUE results for the gender\ndebiased model.\nF Full Bert-Tiny Model Results\n• Table 15, 16, and 17 show the post-processing\ngender debiasing result of different percentiles\nof wordlist on three different corpora: Brown,\nSST, and Reddit, respectively.\n• Table 18 shows the post-processing and\nconceptor-intervened gender debiasing result\nof each layer on the SST corpus.\nG Full GPT2 Model Debiasing Results\n• Table 19 shows the post-processing gender\ndebiasing result of different percentiles of\nwordlist on Brown corpus.\nH Full Other LLMs’ GLUE Results\n• Table 20 contains GLUE results for gender\ndebiased model.\nI Full Intersectional Debiasing Results\n• Table 21 and 22 show the post-processing in-\ntersectional debiasing results.\n10716\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT (“bert-base-uncased”) 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n(Brown Corpus)\n+ Conceptor-12 (pronouns) 0.488∗ -0.091 -0.331 0.471∗ 0.783∗ 0.621∗ ↓0.156 0.464\n+ Conceptor-12 (extended) 0.509∗ -0.109 -0.406 0.240 0.606 ∗ 0.449∗ ↓0.234 0.386\n+ Conceptor-12 (propernouns) 0.581∗ -0.053 -0.258 0.187 0.585 ∗ 0.659∗ ↓0.233 0.387\n+ Conceptor-12 (all) 0.452∗ -0.123 -0.277 0.258 0.662 ∗ 0.607∗ ↓0.224 0.396\n+ Conceptor-12 (or) 0.440∗ -0.063 -0.136 0.251 0.640 ∗ 0.617∗ ↓0.262 0.358\n(SST Corpus)\n+ Conceptor-12 (pronouns) 0.627∗ -0.104 -0.416 0.520∗ 0.636∗ 0.628∗ ↓0.132 0.488\n+ Conceptor-12 (extended) 0.626∗ -0.068 -0.365 0.280 0.556 ∗ 0.429 ↓0.233 0.387\n+ Conceptor-12 (propernouns) 0.680∗ -0.050 -0.405 0.614∗ 0.585∗ 0.790∗ ↓0.099 0.521\n+ Conceptor-12 (all) 0.624∗ -0.093 -0.480 0.442∗ 0.538∗ 0.663∗ ↓0.147 0.473\n+ Conceptor-12 (or) 0.606∗ -0.100 -0.447 0.337 0.428 0.575 ∗ ↓0.204 0.416\n(Reddit Corpus)\n+ Conceptor-12 (pronouns) 0.619∗ -0.092 -0.235 0.816∗ 0.756∗ 0.962∗ ↓0.040 0.580\n+ Conceptor-12 (extended) 0.630∗ -0.061 -0.157 0.676∗ 0.711∗ 0.806∗ ↓0.113 0.507\n+ Conceptor-12 (propernouns) 0.792∗ 0.151 0.068 0.964∗ 0.765∗ 0.934∗ ↓0.008 0.612\n+ Conceptor-12 (all) 0.613∗ -0.010 0.004 0.803 ∗ 0.735∗ 0.917∗ ↓0.106 0.514\n+ Conceptor-12 (or) 0.593∗ 0.004 0.092 0.838 ∗ 0.652∗ 0.961∗ ↓0.097 0.523\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nTable 8: SEAT effect size of gender debising. The impact of different corpora on bert-base-uncased models. Effect\nsizes closer to 0 are indicative of less biased sentence representations (bolded value). Statistically significant effect\nsizes at p< 0.01 are denoted by *. Note that the “conceptor-X (subspace)” indicates the conceptor negation matrix\nis generated by the X-layer of the language model in combinations with the subspace of the specific attribute\nwordlist. The top-3 best performance is colored in orange.\n10717\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT (“bert-base-uncased”) 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n(Wordlist Percentile 0.5-1.0)\n+ Conceptor-12 (pronouns) 0.488∗ -0.091 -0.331 0.471∗ 0.783∗ 0.621∗ ↓0.156 0.464\n+ Conceptor-12 (extended) 0.509∗ -0.109 -0.406 0.240 0.606 ∗ 0.449∗ ↓0.234 0.386\n+ Conceptor-12 (propernouns) 0.581∗ -0.053 -0.258 0.187 0.585 ∗ 0.659∗ ↓0.233 0.387\n+ Conceptor-12 (all) 0.452∗ -0.123 -0.277 0.258 0.662 ∗ 0.607∗ ↓0.224 0.396\n+ Conceptor-12 (or) 0.440∗ -0.063 -0.136 0.251 0.640 ∗ 0.617∗ ↓0.262 0.358\n(Wordlist Percentile 0.4)\n+ Conceptor-12 (pronouns) 0.483∗ -0.095 -0.385 0.435∗ 0.776∗ 0.609∗ ↓0.156 0.464\n+ Conceptor-12 (extended) 0.509∗ -0.110 -0.407 0.239 0.603 ∗ 0.447∗ ↓0.234 0.386\n+ Conceptor-12 (propernouns) 0.451∗ -0.188 -0.505 -0.122 0.399 0.264 ↓0.298 0.322\n+ Conceptor-12 (all) 0.466∗ -0.112 -0.260 0.267 0.697 ∗ 0.617∗ ↓0.217 0.403\n+ Conceptor-12 (or) 0.388 -0.078 -0.292 0.179 0.594 ∗ 0.335 ↓0.309 0.311\n(Wordlist Percentile 0.3)\n+ Conceptor-12 (pronouns) 0.487∗ -0.016 -0.351 0.398∗ 0.807∗ 0.776∗ ↓0.148 0.472\n+ Conceptor-12 (extended) 0.509∗ -0.109 -0.410 0.228 0.604 ∗ 0.453∗ ↓0.235 0.385\n+ Conceptor-12 (propernouns) 0.495∗ -0.168 -0.478 -0.083 0.456 ∗ 0.315 ↓0.288 0.332\n+ Conceptor-12 (all) 0.348 -0.236 -0.520 -0.019 0.506 ∗ 0.361 ↓0.288 0.332\n+ Conceptor-12 (or) 0.407 -0.022 -0.247 0.331 0.677 ∗ 0.483∗ ↓0.259 0.361\n(Wordlist Percentile 0.2)\n+ Conceptor-12 (pronouns) 0.570∗ 0.035 -0.378 0.334 0.708 ∗ 0.768∗ ↓0.154 0.466\n+ Conceptor-12 (extended) 0.508∗ -0.109 -0.416 0.219 0.602 ∗ 0.450∗ ↓0.236 0.384\n+ Conceptor-12 (propernouns) 0.548∗ -0.157 -0.397 0.270 0.483 ∗ 0.366 ↓0.250 0.370\n+ Conceptor-12 (all) 0.357 -0.235 -0.598 0.110 0.455 ∗ 0.383 ↓0.264 0.356\n+ Conceptor-12 (or) 0.476∗ -0.063 -0.385 0.296 0.558 ∗ 0.500∗ ↓0.240 0.380\n(Wordlist Percentile 0.1)\n+ Conceptor-12 (pronouns) 0.800∗ 0.204 -0.314 0.273 0.764 ∗ 0.965∗ ↓0.067 0.553\n+ Conceptor-12 (extended) 0.869∗ 0.162 -0.265 0.266 0.861∗ 0.635∗ ↓0.110 0.510\n+ Conceptor-12 (propernouns) 0.613∗ -0.084 -0.582 0.190 0.579 ∗ 0.740∗ ↓0.155 0.465\n+ Conceptor-12 (all) 0.603∗ -0.102 -0.612 0.182 0.566 ∗ 0.712∗ ↓0.157 0.463\n+ Conceptor-12 (or) 0.614∗ 0.197 -0.401 -0.132 0.624 ∗ 0.699∗ ↓0.176 0.444\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nTable 9: SEAT effect size of gender debising. The impact ofdifferent percentiles of wordlist(using UMAP clustering)\non Brown Corpus, bert-base-uncased models. The top-3 best performance is colored in orange.\n10718\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT (“bert-base-uncased”) 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n(Wordlist Percentile 0.5-1.0)\n+ Conceptor-12 (pronouns) 0.627∗ -0.104 -0.416 0.520∗ 0.636∗ 0.628∗ ↓0.132 0.488\n+ Conceptor-12 (extended) 0.688∗ 0.024 -0.293 0.138 0.559 ∗ 0.375 ↓0.274 0.346\n+ Conceptor-12 (propernouns) 0.680∗ -0.050 -0.405 0.614∗ 0.585∗ 0.790∗ ↓0.099 0.521\n+ Conceptor-12 (all) 0.624∗ -0.093 -0.480 0.442∗ 0.538∗ 0.663∗ ↓0.147 0.473\n+ Conceptor-12 (or) 0.619∗ -0.069 -0.428 0.280 0.414 0.539 ∗ ↓0.229 0.391\n(Wordlist Percentile 0.4)\n+ Conceptor-12 (pronouns) 0.619∗ -0.113 -0.526 0.449∗ 0.606∗ 0.584∗ ↓0.137 0.483\n+ Conceptor-12 (extended) 0.688∗ 0.024 -0.293 0.138 0.559 ∗ 0.375 ↓0.274 0.346\n+ Conceptor-12 (propernouns) 0.704∗ -0.086 -0.227 0.590∗ 0.682∗ 0.716∗ ↓0.119 0.501\n+ Conceptor-12 (all) 0.622∗ -0.087 -0.508 0.277 0.519 ∗ 0.578∗ ↓0.188 0.432\n+ Conceptor-12 (or) 0.646∗ -0.034 -0.427 0.200 0.438 ∗ 0.401 ↓0.262 0.358\n(Wordlist Percentile 0.3)\n+ Conceptor-12 (pronouns) 0.550∗ -0.035 -0.396 0.344 0.682 ∗ 0.744∗ ↓0.161 0.459\n+ Conceptor-12 (extended) 0.687∗ 0.023 -0.299 0.129 0.559 ∗ 0.382 ↓0.273 0.347\n+ Conceptor-12 (propernouns) 0.706∗ -0.088 -0.230 0.602∗ 0.683∗ 0.720∗ ↓0.115 0.505\n+ Conceptor-12 (all) 0.652∗ -0.117 -0.378 0.504∗ 0.536∗ 0.657∗ ↓0.146 0.474\n+ Conceptor-12 (or) 0.595∗ 0.027 -0.375 0.171 0.519 ∗ 0.600∗ ↓0.239 0.381\n(Wordlist Percentile 0.2)\n+ Conceptor-12 (pronouns) 0.730∗ 0.090 -0.110 0.523 ∗ 0.714∗ 0.758∗ ↓0.132 0.488\n+ Conceptor-12 (extended) 0.687∗ 0.023 -0.299 0.129 0.559 ∗ 0.382 ↓0.273 0.347\n+ Conceptor-12 (propernouns) 0.755∗ -0.057 -0.346 0.584∗ 0.659∗ 0.733∗ ↓0.098 0.522\n+ Conceptor-12 (all) 0.699∗ -0.094 -0.492 0.456∗ 0.620∗ 0.688∗ ↓0.112 0.508\n+ Conceptor-12 (or) 0.579∗ -0.004 -0.261 0.270 0.503 ∗ 0.634∗ ↓0.245 0.375\n(Wordlist Percentile 0.1)\n+ Conceptor-12 (pronouns) 0.903∗ 0.198 -0.464 0.030 0.434 0.492 ∗ ↓0.200 0.420\n+ Conceptor-12 (extended) 0.631∗ -0.107 -0.012 1.008∗ 0.615∗ 0.794∗ ↓0.092 0.528\n+ Conceptor-12 (propernouns) 0.655∗ -0.130 -0.166 0.895∗ 0.641∗ 0.877∗ ↓0.059 0.561\n+ Conceptor-12 (all) 0.597∗ -0.164 -0.223 0.791∗ 0.698∗ 0.909∗ ↓0.056 0.564\n+ Conceptor-12 (or) 0.542∗ -0.039 -0.184 0.589∗ 0.457∗ 0.929∗ ↓0.163 0.457\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nTable 10: SEAT effect size of gender debising. The impact of different percentiles of wordlist (using UMAP\nclustering) on SST Corpus, bert-base-uncased models. The top-3 best performance is colored in orange.\n10719\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT (“bert-base-uncased”) 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n(Wordlist Percentile 0.5-1.0)\n+ Conceptor-12 (pronouns) 0.619∗ -0.092 -0.235 0.816∗ 0.756∗ 0.962∗ ↓0.040 0.580\n+ Conceptor-12 (extended) 0.630∗ -0.061 -0.157 0.676∗ 0.711∗ 0.806∗ ↓0.113 0.507\n+ Conceptor-12 (propernouns) 0.792∗ 0.151 0.068 0.964∗ 0.765∗ 0.934∗ ↓0.008 0.612\n+ Conceptor-12 (all) 0.613∗ -0.010 0.004 0.803 ∗ 0.735∗ 0.917∗ ↓0.106 0.514\n+ Conceptor-12 (or) 0.593∗ 0.004 0.092 0.838 ∗ 0.652∗ 0.961∗ ↓0.097 0.523\n(Wordlist Percentile 0.4)\n+ Conceptor-12 (pronouns) 0.618∗ -0.095 -0.297 0.769∗ 0.728∗ 0.944∗ ↓0.045 0.575\n+ Conceptor-12 (extended) 0.732∗ 0.047 -0.121 0.601 ∗ 0.748∗ 0.719∗ ↓0.125 0.495\n+ Conceptor-12 (propernouns) 0.691∗ -0.063 0.315 1.028 ∗ 0.667∗ 0.635∗ ↓0.054 0.566\n+ Conceptor-12 (all) 0.651∗ -0.010 -0.004 0.786 ∗ 0.732∗ 0.940∗ ↓0.100 0.520\n+ Conceptor-12 (or) 0.526∗ -0.057 0.054 0.596 ∗ 0.655∗ 0.724∗ ↓0.185 0.435\n(Wordlist Percentile 0.3)\n+ Conceptor-12 (pronouns) 0.596∗ -0.068 -0.286 0.772∗ 0.790∗ 0.983∗ ↓0.038 0.582\n+ Conceptor-12 (extended) 0.740∗ 0.043 -0.119 0.593 ∗ 0.758∗ 0.725∗ ↓0.124 0.496\n+ Conceptor-12 (propernouns) 0.825∗ 0.007 0.512∗ 1.180∗ 0.761∗ 0.705∗ ↑0.045 0.665\n+ Conceptor-12 (all) 0.689∗ 0.007 0.180 0.924∗ 0.627∗ 0.653∗ ↓0.107 0.513\n+ Conceptor-12 (or) 0.612∗ -0.018 0.046 0.745 ∗ 0.778∗ 0.916∗ ↓0.101 0.519\n(Wordlist Percentile 0.2)\n+ Conceptor-12 (pronouns) 0.801∗ 0.104 -0.072 0.757 ∗ 0.873∗ 0.997∗ ↓0.019 0.601\n+ Conceptor-12 (extended) 0.740∗ 0.043 -0.119 0.593 ∗ 0.758∗ 0.725∗ ↓0.124 0.496\n+ Conceptor-12 (propernouns) 0.837∗ 0.037 0.532∗ 1.170∗ 0.785∗ 0.722∗ ↑0.060 0.680\n+ Conceptor-12 (all) 0.694∗ -0.042 0.356 1.044 ∗ 0.608∗ 0.554∗ ↓0.070 0.550\n+ Conceptor-12 (or) 0.665∗ 0.096 0.130 0.607∗ 0.852∗ 0.842∗ ↓0.088 0.532\n(Wordlist Percentile 0.1)\n+ Conceptor-12 (pronouns) 0.949 ∗ 0.121 -0.472 0.102 0.568 ∗ 0.707∗ ↓0.134 0.486\n+ Conceptor-12 (extended) 0.736∗ -0.035 -0.291 0.780∗ 0.812∗ 1.095∗ ↑0.005 0.625\n+ Conceptor-12 (propernouns) 0.948 ∗ 0.107 -0.094 0.949∗ 0.783∗ 0.842∗ – 0.620\n+ Conceptor-12 (all) 0.949 ∗ 0.105 -0.061 0.899 ∗ 0.782∗ 0.846∗ ↓0.013 0.607\n+ Conceptor-12 (or) 0.936 ∗ 0.130 -0.579 0.098 0.591 ∗ 0.914∗ ↓0.079 0.541\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nTable 11: SEAT effect size of gender debising. The impact of different percentiles of wordlist (using UMAP\nclustering) on Reddit Corpus, bert-base-uncased models. The top-3 best performance is colored in orange.\n10720\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\n(Layer 0)\nBERT (“bert-base-uncased”) 0.921 ∗ 0.194 0.251 -0.172 -0.110 0.366 0.336\n+ Conceptor-0 (or) 0.147 -0.087 -0.266 -0.653 -0.405 -0.324 ↓0.022 0.314\n+ Conceptor-Intervened 0.147 -0.087 -0.266 -0.653 -0.405 -0.324 ↓0.022 0.314\n(Layer 1)\nBERT (“bert-base-uncased”) 1.245 ∗ 0.292 0.469 ∗ 1.101∗ 0.110 1.261 ∗ 0.746\n+ Conceptor-1 (or) 0.473∗ 0.205 -0.210 -0.093 -0.095 0.396 ↓0.501 0.245\n+ Conceptor-Intervened 0.241 -0.038 -0.274 0.291 -0.751 -0.107 ↓0.462 0.284\n(Layer 2)\nBERT (“bert-base-uncased”) 1.149 ∗ 0.216 0.431 ∗ 1.021∗ 0.474∗ 1.231∗ 0.754\n+ Conceptor-2 (or) 0.180 -0.047 -0.450 -0.105 0.133 0.133 ↓0.579 0.175\n+ Conceptor-Intervened -0.108 0.115 -0.965 1.388 ∗ -1.146 0.329 ↓0.079 0.675\n(Layer 3)\nBERT (“bert-base-uncased”) 1.186 ∗ 0.214 0.152 0.770 ∗ 0.262 1.049 ∗ 0.606\n+ Conceptor-3 (or) 0.404 -0.046 -0.675 0.102 -0.325 -0.024 ↓0.343 0.263\n+ Conceptor-Intervened 0.158 0.081 -0.959 1.348 ∗ -1.093 0.409 ↑0.069 0.675\n(Layer 4)\nBERT (“bert-base-uncased”) 0.975 ∗ 0.106 0.552 ∗ 0.890∗ 0.542∗ 0.724∗ 0.632\n+ Conceptor-4 (or) 0.597∗ 0.068 -0.249 0.251 -0.016 -0.315 ↓0.383 0.249\n+ Conceptor-Intervened 0.060 0.121 -0.986 1.676 ∗ -0.840 0.943 ∗ ↑0.139 0.771\n(Layer 5)\nBERT (“bert-base-uncased”) 1.002 ∗ 0.184 0.628 ∗ 0.914∗ 0.376 1.053 ∗ 0.693\n+ Conceptor-5 (or) 0.634∗ 0.064 0.118 0.225 -0.160 0.429 ↓0.421 0.272\n+ Conceptor-Intervened -0.046 0.043 -1.038 1.378 ∗ -0.790 0.659∗ ↓0.034 0.659\n(Layer 6)\nBERT (“bert-base-uncased”) 0.753 ∗ 0.118 0.539 ∗ 1.048∗ 0.597∗ 1.042∗ 0.683\n+ Conceptor-6 (or) 0.327 0.041 0.176 0.104 0.150 0.174 ↓0.521 0.162\n+ Conceptor-Intervened -0.210 0.004 -0.965 1.242 ∗ -0.739 0.475∗ ↓0.077 0.606\n(Layer 7)\nBERT (“bert-base-uncased”) 0.719 ∗ 0.155 0.341 0.935 ∗ 0.562∗ 0.721∗ 0.572\n+ Conceptor-7 (or) 0.235 -0.064 -0.038 0.206 0.173 0.223 ↓0.416 0.156\n+ Conceptor-Intervened -0.246 -0.082 -0.821 1.112 ∗ -0.671 0.248 ↓0.042 0.530\n(Layer 8)\nBERT (“bert-base-uncased”) 0.983 ∗ 0.163 0.313 1.157 ∗ 0.766∗ 0.789∗ 0.695\n+ Conceptor-8 (or) 0.235 0.005 -0.136 0.389 0.379 0.135 ↓0.482 0.213\n+ Conceptor-Intervened -0.125 -0.193 -0.940 0.796∗ -0.606 0.084 ↓0.238 0.457\n(Layer 9)\nBERT (“bert-base-uncased”) 0.922 ∗ 0.224 0.503 ∗ 1.293∗ 0.780∗ 0.996∗ 0.786\n+ Conceptor-9 (or) 0.234 0.019 -0.005 0.485 ∗ 0.694∗ 0.686∗ ↓0.432 0.354\n+ Conceptor-Intervened -0.151 -0.246 -0.599 0.836∗ -0.455 -0.095 ↓0.389 0.397\n(Layer 10)\nBERT (“bert-base-uncased”) 0.686 ∗ 0.082 0.226 0.894 ∗ 0.904∗ 0.965∗ 0.626\n+ Conceptor-10 (or) 0.294 -0.091 -0.153 0.078 0.703 ∗ 0.545∗ ↓0.315 0.311\n+ Conceptor-Intervened -0.253 -0.298 -0.569 0.753∗ -0.462 -0.099 ↓0.221 0.405\n(Layer 11)\nBERT (“bert-base-uncased”) 0.665 ∗ -0.015 -0.344 0.602 ∗ 0.919∗ 0.891∗ 0.573\n+ Conceptor-11 (or) 0.197 -0.114 -0.399 -0.157 0.557 ∗ 0.277 ↓0.289 0.284\n+ Conceptor-Intervened -0.314 -0.269 -0.635 0.769 ∗ -0.430 0.096 ↓0.154 0.419\n(Layer 12)\nBERT (“bert-base-uncased”) 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n+ Conceptor-12 (or) 0.388 -0.078 -0.292 0.179 0.594 ∗ 0.335 ↓0.309 0.311\n+ Conceptor-Intervened -0.334 -0.117 -0.698 0.459∗ -0.230 0.178 ↓0.284 0.336\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nTable 12: SEAT fffect size of gender debising from CI-BERT, Type I. The conceptor-intervened performance of\ndifferent layer’s conceptors on SST Corpus, bert-base-uncased models. The setting is “brown-0.4-or”. The layer(s)\nof CI-BERT that outperform the conceptor post-processing of the same layer(s) are colored in orange.\n10721\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\n(Layer 0)\nBERT (“bert-base-uncased”) 0.921 ∗ 0.194 0.251 -0.172 -0.110 0.366 0.336\n+ Conceptor-0 (extended) 0.497∗ -0.095 -0.412 -0.760 -0.001 -0.276 ↑0.004 0.340\n+ Conceptor-Intervened 0.497∗ -0.095 -0.412 -0.760 -0.001 -0.276 ↑0.004 0.340\n(Layer 1)\nBERT (“bert-base-uncased”) 1.245 ∗ 0.292 0.469 ∗ 1.101∗ 0.110 1.261 ∗ 0.746\n+ Conceptor-1 (extended) 0.897∗ 0.156 -0.084 0.208 0.099 0.558 ∗ ↓0.412 0.334\n+ Conceptor-Intervened 0.813∗ 0.029 -0.961 -0.513 -0.211 -0.292 ↓0.276 0.470\n(Layer 2)\nBERT (“bert-base-uncased”) 1.149 ∗ 0.216 0.431 ∗ 1.021∗ 0.474∗ 1.231∗ 0.754\n+ Conceptor-2 (extended) 0.542∗ 0.059 -0.146 0.112 0.515∗ 0.428 ↓0.454 0.300\n+ Conceptor-Intervened 0.366 0.088 -1.342 -0.249 -0.408 -0.634 ↓0.239 0.515\n(Layer 3)\nBERT (“bert-base-uncased”) 1.186 ∗ 0.214 0.152 0.770 ∗ 0.262 1.049 ∗ 0.606\n+ Conceptor-3 (extended) 0.849∗ -0.034 -0.516 0.154 0.205 0.356 ↓0.254 0.352\n+ Conceptor-Intervened 0.329 0.048 -1.240 -0.520 -0.429 -0.499 ↓0.095 0.511\n(Layer 4)\nBERT (“bert-base-uncased”) 0.975 ∗ 0.106 0.552 ∗ 0.890∗ 0.542∗ 0.724∗ 0.632\n+ Conceptor-4 (extended) 0.789∗ 0.109 -0.014 0.254 0.515 ∗ 0.009 ↓0.350 0.282\n+ Conceptor-Intervened 0.248 0.068 -1.367 -0.040 -0.401 -0.100 ↓0.261 0.371\n(Layer 5)\nBERT (“bert-base-uncased”) 1.002 ∗ 0.184 0.628 ∗ 0.914∗ 0.376 1.053 ∗ 0.693\n+ Conceptor-5 (extended) 0.695∗ 0.122 -0.007 0.075 0.158 0.472 ∗ ↓0.438 0.255\n+ Conceptor-Intervened 0.105 0.066 -1.096 -0.170 -0.173 0.036 ↓0.419 0.274\n(Layer 6)\nBERT (“bert-base-uncased”) 0.753 ∗ 0.118 0.539 ∗ 1.048∗ 0.597∗ 1.042∗ 0.683\n+ Conceptor-6 (extended) 0.372 0.084 0.150 0.033 0.467 ∗ 0.209 ↓0.464 0.219\n+ Conceptor-Intervened 0.004 -0.023 -0.866 -0.312 -0.234 -0.106 ↓0.425 0.258\n(Layer 7)\nBERT (“bert-base-uncased”) 0.719 ∗ 0.155 0.341 0.935 ∗ 0.562∗ 0.721∗ 0.572\n+ Conceptor-7 (extended) 0.451∗ 0.082 0.051 0.196 0.326 0.185 ↓0.357 0.215\n+ Conceptor-Intervened 0.041 -0.066 -0.697 -0.509 -0.381 -0.015 ↓0.287 0.285\n(Layer 8)\nBERT (“bert-base-uncased”) 0.983 ∗ 0.163 0.313 1.157 ∗ 0.766∗ 0.789∗ 0.695\n+ Conceptor-8 (extended) 0.597∗ 0.051 -0.023 0.639 ∗ 0.503∗ 0.200 ↓0.359 0.336\n+ Conceptor-Intervened 0.110 -0.095 -0.392 -0.702 -0.287 0.190 ↓0.399 0.296\n(Layer 9)\nBERT (“bert-base-uncased”) 0.922 ∗ 0.224 0.503 ∗ 1.293∗ 0.780∗ 0.996∗ 0.786\n+ Conceptor-9 (extended) 0.597∗ 0.146 0.333 0.903 ∗ 0.764∗ 0.722∗ ↓0.208 0.578\n+ Conceptor-Intervened 0.148 -0.024 0.162 -0.669 0.224 0.487 ∗ ↓0.500 0.286\n(Layer 10)\nBERT (“bert-base-uncased”) 0.686 ∗ 0.082 0.226 0.894 ∗ 0.904∗ 0.965∗ 0.626\n+ Conceptor-10 (extended) 0.639∗ 0.099 -0.034 0.044 0.605 ∗ 0.322 ↓0.335 0.291\n+ Conceptor-Intervened 0.557∗ -0.165 -0.149 -1.046 0.142 0.522 ∗ ↓0.196 0.430\n(Layer 11)\nBERT (“bert-base-uncased”) 0.665 ∗ -0.015 -0.344 0.602 ∗ 0.919∗ 0.891∗ 0.573\n+ Conceptor-11 (extended) 0.565∗ -0.045 -0.511 -0.406 0.523 ∗ 0.198 ↓0.198 0.375\n+ Conceptor-Intervened 0.602∗ -0.189 0.143 -1.219 -0.006 0.205 ↓0.179 0.394\n(Layer 12)\nBERT (“bert-base-uncased”) 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n+ Conceptor-12 (extended) 0.688∗ 0.024 -0.293 0.138 0.559 ∗ 0.375 ↓0.274 0.346\n+ Conceptor-Intervened 0.384 -0.261 0.144 -1.256 -0.148 0.398 ↓0.188 0.432\n+ CDA 0.846∗ 0.186 -0.278 1.342 ∗ 0.831∗ 0.849∗ ↑0.120 0.722\n+ DROPOUT 1.136∗ 0.317 0.138 1.179 ∗ 0.879∗ 0.939∗ ↑0.144 0.765\n+ INLP 0.317 -0.354 -0.258 0.105 0.187 -0.004 ↓0.416 0.204\n+ SENTENCE DEBIAS 0.350 -0.298 -0.626 0.458∗ 0.413 0.462 ∗ ↓0.186 0.434\nTable 13: SEAT effect size of gender debising from CI-BERT, Type I. The conceptor-intervened performance\nof different layer’s conceptors on SST Corpus, bert-base-uncased models. The setting is “sst-0.9-extended”. The\nlayer(s) of CI-BERT that outperform the conceptor post-processing of the same layer(s) are colored in orange.\n10722\nModel CoLA MNLI MRPC QNLI QQP RTE SST STS-B WNLI Average\nBERT 55.89 84.50 88.59 91.38 91.03 63.54 92.58 88.51 43.66 77.74\n+ Conceptor P.P 57.54 84.66 89.30 91.03 91.05 65.34 92.66 89.07 54.93 ↑1.77 79.51\n+ Conceptor C.T. 47.06 83.46 87.20 90.73 90.97 58.98 91.67 88.21 52.11 ↓1.03 76.71\n+ CDA 55.90 84.73 88.76 91.36 91.01 66.31 92.43 89.14 38.03 ↓0.22 77.52\n+ DROPOUT 49.83 84.67 88.20 91.27 90.36 64.02 92.58 88.47 37.09 ↓1.46 76.28\n+ INLP 56.06 84.81 88.61 91.34 90.92 64.98 92.51 88.70 32.86 ↓0.99 76.76\n+ SENTENCE DEBIAS 56.41 84.80 88.70 91.48 90.98 63.06 92.32 88.45 44.13 ↑0.07 77.81\nTable 14: GLUE validation set results for gender debiased BERT model. We use the F1 score for MRPC, the\nSpearman correlation for STS-B, and Matthew’s correlation for CoLA. For all other tasks, we report accuracy. All\nscores are averaged among three runs. The model is “bert-base-uncased”. The top-3 best performance is colored in\norange.\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT-T 1.735 ∗ 0.797∗ 1.294∗ 1.243∗ 0.837∗ 1.293∗ 1.200\n(Wordlist Percentile 1.0)\n+ Conceptor-2 (pronouns) 1.657∗ 0.449∗ 1.185∗ 0.936∗ 0.453∗ 0.833∗ ↓0.281 0.919\n+ Conceptor-2 (extended) 1.570∗ 0.353 1.094 ∗ 0.991∗ 0.176 0.775 ∗ ↓0.373 0.827\n+ Conceptor-2 (propernouns) 1.641∗ 0.655∗ 1.142∗ 1.121∗ 0.203 0.781 ∗ ↓0.276 0.924\n+ Conceptor-2 (all) 1.587∗ 0.377∗ 1.188∗ 1.077∗ 0.128 0.735 ∗ ↓0.351 0.849\n+ Conceptor-2 (or) 1.464∗ 0.257 1.005 ∗ 0.944∗ -0.114 0.503 ∗ ↓0.486 0.714\n(Wordlist Percentile 0.5-0.9)\n+ Conceptor-2 (pronouns) 1.657∗ 0.449∗ 1.185∗ 0.936∗ 0.453∗ 0.833∗ ↓0.281 0.919\n+ Conceptor-2 (extended) 1.296∗ 0.255 1.014 ∗ 1.194∗ -0.274 0.502 ∗ ↓0.444 0.756\n+ Conceptor-2 (propernouns) 1.641∗ 0.655∗ 1.142∗ 1.121∗ 0.203 0.781 ∗ ↓0.276 0.924\n+ Conceptor-2 (all) 1.587∗ 0.377∗ 1.188∗ 1.077∗ 0.128 0.735 ∗ ↓0.351 0.849\n+ Conceptor-2 (or) 1.323∗ 0.186 0.947 ∗ 1.027∗ -0.289 0.403 ↓0.504 0.696\n(Wordlist Percentile 0.4)\n+ Conceptor-2 (pronouns) 1.657∗ 0.443∗ 1.181∗ 0.937∗ 0.448∗ 0.821∗ ↓0.286 0.914\n+ Conceptor-2 (extended) 1.294∗ 0.254 1.014 ∗ 1.194∗ -0.274 0.502 ∗ ↓0.445 0.755\n+ Conceptor-2 (propernouns) 1.589∗ 0.722∗ 1.130∗ 1.084∗ 0.494∗ 0.991∗ ↓0.198 1.002\n+ Conceptor-2 (all) 1.585∗ 0.396∗ 1.192∗ 1.067∗ 0.159 0.726 ∗ ↓0.346 0.854\n+ Conceptor-2 (or) 1.278∗ 0.233 0.852 ∗ 0.910∗ -0.265 0.346 ↓0.553 0.647\n(Wordlist Percentile 0.3)\n+ Conceptor-2 (pronouns) 1.691∗ 0.573∗ 1.227∗ 1.158∗ 0.573∗ 1.009∗ ↓0.161 1.039\n+ Conceptor-2 (extended) 1.295∗ 0.260 1.010 ∗ 1.192∗ -0.286 0.490 ∗ ↓0.444 0.756\n+ Conceptor-2 (propernouns) 1.597∗ 0.746∗ 1.162∗ 1.147∗ 0.551∗ 1.034∗ ↓0.160 1.040\n+ Conceptor-2 (all) 1.536∗ 0.436∗ 1.143∗ 1.181∗ 0.140 0.849 ∗ ↓0.319 0.881\n+ Conceptor-2 (or) 1.277∗ 0.235 1.055 ∗ 1.168∗ -0.090 0.542 ∗ ↓0.472 0.728\n(Wordlist Percentile 0.2)\n+ Conceptor-2 (pronouns) 1.656∗ 0.543∗ 1.253∗ 1.175∗ 0.569∗ 1.038∗ ↓0.161 1.039\n+ Conceptor-2 (extended) 1.296∗ 0.260 1.011 ∗ 1.189∗ -0.290 0.478 ∗ ↓0.446 0.754\n+ Conceptor-2 (propernouns) 1.577∗ 0.723∗ 1.231∗ 1.193∗ 0.541∗ 1.067∗ ↓0.145 1.055\n+ Conceptor-2 (all) 1.490∗ 0.341 1.116 ∗ 1.182∗ -0.018 0.849 ∗ ↓0.367 0.833\n+ Conceptor-2 (or) 1.178∗ 0.141 1.062 ∗ 1.087∗ -0.252 0.502 ∗ ↓0.496 0.704\n(Wordlist Percentile 0.1)\n+ Conceptor-2 (pronouns) 1.677∗ 0.643∗ 1.317∗ 1.342∗ 0.696∗ 1.191∗ ↓0.056 1.144\n+ Conceptor-2 (extended) 1.547∗ 0.700∗ 1.305∗ 1.333∗ 0.464∗ 0.997∗ ↓0.142 1.058\n+ Conceptor-2 (propernouns) 1.722∗ 0.836∗ 1.256∗ 1.213∗ 0.956∗ 1.316∗ ↑0.017 1.217\n+ Conceptor-2 (all) 1.771 ∗ 0.882∗ 1.189∗ 1.160∗ 0.996∗ 1.277∗ ↑0.012 1.212\n+ Conceptor-2 (or) 1.579∗ 0.560∗ 1.278∗ 1.301∗ 0.422 0.881 ∗ ↓0.196 1.004\nTable 15: SEAT effect size of gender debising. The impact of different percentiles of wordlist (using UMAP\nclustering) on Brown Corpus, bert-tiny models. The top-3 best performance is colored in orange.\n10723\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT-T 1.735 ∗ 0.797∗ 1.294∗ 1.243∗ 0.837∗ 1.293∗ 1.200\n(Wordlist Percentile 1.0)\n+ Conceptor-2 (pronouns) 1.703∗ 0.403∗ 0.958∗ 0.706∗ 0.254 0.679 ∗ ↓0.416 0.784\n+ Conceptor-2 (extended) 1.608∗ 0.473∗ 0.870∗ 1.118∗ -0.209 0.732 ∗ ↓0.365 0.835\n+ Conceptor-2 (propernouns) 1.704∗ 0.582∗ 1.012∗ 1.111∗ -0.069 0.730 ∗ ↓0.332 0.868\n+ Conceptor-2 (all) 1.680∗ 0.377∗ 1.028∗ 1.047∗ -0.175 0.669 ∗ ↓0.371 0.829\n+ Conceptor-2 (or) 1.489∗ 0.163 0.539 ∗ 0.937∗ -0.612 0.314 ↓0.524 0.676\n(Wordlist Percentile 0.5-0.9)\n+ Conceptor-2 (pronouns) 1.703∗ 0.403∗ 0.958∗ 0.706∗ 0.254 0.679 ∗ ↓0.416 0.784\n+ Conceptor-2 (extended) 1.647∗ 0.391∗ 0.806∗ 0.815∗ -0.136 0.637 ∗ ↓0.461 0.739\n+ Conceptor-2 (propernouns) 1.704∗ 0.582∗ 1.012∗ 1.111∗ -0.069 0.730 ∗ ↓0.332 0.868\n+ Conceptor-2 (all) 1.680∗ 0.377∗ 1.028∗ 1.047∗ -0.175 0.669 ∗ ↓0.371 0.829\n+ Conceptor-2 (or) 1.542∗ 0.148 0.486 ∗ 0.806∗ -0.549 0.245 ↓0.571 0.629\n(Wordlist Percentile 0.4)\n+ Conceptor-2 (pronouns) 1.703∗ 0.375∗ 0.999∗ 0.720∗ 0.235 0.669 ∗ ↓0.416 0.784\n+ Conceptor-2 (extended) 1.608∗ 0.473∗ 0.870∗ 1.118∗ -0.209 0.732 ∗ ↓0.365 0.835\n+ Conceptor-2 (propernouns) 1.765 ∗ 0.954∗ 1.027∗ 1.036∗ 0.457∗ 1.028∗ ↓0.156 1.044\n+ Conceptor-2 (all) 1.694∗ 0.444∗ 1.057∗ 1.054∗ -0.090 0.709 ∗ ↓0.359 0.841\n+ Conceptor-2 (or) 1.587∗ 0.344 0.512 ∗ 0.881∗ -0.407 0.489 ∗ ↓0.497 0.703\n(Wordlist Percentile 0.3)\n+ Conceptor-2 (pronouns) 1.786 ∗ 0.843∗ 1.121∗ 1.028∗ 0.617∗ 1.064∗ ↓0.124 1.076\n+ Conceptor-2 (extended) 1.610∗ 0.479∗ 0.866∗ 1.119∗ -0.215 0.727 ∗ ↓0.364 0.836\n+ Conceptor-2 (propernouns) 1.749 ∗ 0.945∗ 1.039∗ 1.067∗ 0.474∗ 1.041∗ ↓0.148 1.052\n+ Conceptor-2 (all) 1.751 ∗ 0.813∗ 1.102∗ 1.063∗ 0.522∗ 1.092∗ ↓0.143 1.057\n+ Conceptor-2 (or) 1.652∗ 0.414∗ 0.784∗ 1.047∗ -0.170 0.725 ∗ ↓0.401 0.799\n(Wordlist Percentile 0.2)\n+ Conceptor-2 (pronouns) 1.862 ∗ 0.983∗ 1.178∗ 0.982∗ 0.737∗ 1.080∗ ↓0.063 1.137\n+ Conceptor-2 (extended) 1.610∗ 0.479∗ 0.866∗ 1.119∗ -0.215 0.727 ∗ ↓0.364 0.836\n+ Conceptor-2 (propernouns) 1.751 ∗ 0.893∗ 1.082∗ 1.125∗ 0.581∗ 1.167∗ ↓0.100 1.100\n+ Conceptor-2 (all) 1.773 ∗ 0.862∗ 1.101∗ 1.120∗ 0.628∗ 1.191∗ ↓0.088 1.112\n+ Conceptor-2 (or) 1.736 ∗ 0.582∗ 0.702∗ 0.946∗ -0.103 0.763 ∗ ↓0.395 0.805\n(Wordlist Percentile 0.1)\n+ Conceptor-2 (pronouns) 1.828 ∗ 0.971∗ 1.185∗ 1.065∗ 0.755∗ 1.123∗ ↓0.046 1.154\n+ Conceptor-2 (extended) 1.638∗ 0.511∗ 1.167∗ 1.114∗ 0.265 1.017 ∗ ↓0.248 0.952\n+ Conceptor-2 (propernouns) 1.777 ∗ 0.941∗ 1.121∗ 1.169∗ 0.885∗ 1.332∗ ↑0.004 1.204\n+ Conceptor-2 (all) 1.795 ∗ 0.952∗ 1.070∗ 1.129∗ 0.785∗ 1.269∗ ↓0.033 1.167\n+ Conceptor-2 (or) 1.706∗ 0.726∗ 0.990∗ 0.972∗ 0.455∗ 0.978∗ ↓0.229 0.971\nTable 16: SEAT effect size of gender debising. The impact of different percentiles of wordlist (using UMAP\nclustering) on SST Corpus, bert-tiny models. The top-3 best performance is colored in orange.\n10724\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT-T 1.735 ∗ 0.797∗ 1.294∗ 1.243∗ 0.837∗ 1.293∗ 1.200\n(Wordlist Percentile 1.0)\n+ Conceptor-2 (pronouns) 1.676∗ 0.389∗ 1.218∗ 1.095∗ 0.557∗ 1.008∗ ↓0.210 0.990\n+ Conceptor-2 (extended) 1.578∗ 0.507∗ 1.248∗ 1.220∗ 0.656∗ 1.351∗ ↓0.107 1.093\n+ Conceptor-2 (propernouns) 1.713∗ 0.776∗ 1.184∗ 1.315∗ 0.538∗ 1.193∗ ↓0.080 1.120\n+ Conceptor-2 (all) 1.660∗ 0.379∗ 1.248∗ 1.185∗ 0.486∗ 1.125∗ ↓0.186 1.014\n+ Conceptor-2 (or) 1.550∗ 0.180 1.010 ∗ 1.146∗ 0.197 1.088 ∗ ↓0.338 0.862\n(Wordlist Percentile 0.5-0.9)\n+ Conceptor-2 (pronouns) 1.676∗ 0.389∗ 1.218∗ 1.095∗ 0.557∗ 1.008∗ ↓0.210 0.990\n+ Conceptor-2 (extended) 1.684∗ 0.374 1.204 ∗ 1.065∗ 0.616∗ 1.144∗ ↓0.186 1.014\n+ Conceptor-2 (propernouns) 1.713∗ 0.776∗ 1.184∗ 1.315∗ 0.538∗ 1.193∗ ↓0.080 1.120\n+ Conceptor-2 (all) 1.660∗ 0.379∗ 1.248∗ 1.185∗ 0.486∗ 1.125∗ ↓0.186 1.014\n+ Conceptor-2 (or) 1.573∗ 0.179 0.963 ∗ 1.117∗ 0.103 0.963 ∗ ↓0.384 0.816\n(Wordlist Percentile 0.4)\n+ Conceptor-2 (pronouns) 1.677∗ 0.382∗ 1.218∗ 1.095∗ 0.561∗ 1.008∗ ↓0.210 0.990\n+ Conceptor-2 (extended) 1.578∗ 0.507∗ 1.248∗ 1.220∗ 0.656∗ 1.351∗ ↓0.107 1.093\n+ Conceptor-2 (propernouns) 1.622∗ 0.745∗ 0.946∗ 0.937∗ 0.717∗ 1.104∗ ↓0.188 1.012\n+ Conceptor-2 (all) 1.656∗ 0.388∗ 1.198∗ 1.114∗ 0.490∗ 1.083∗ ↓0.212 0.988\n+ Conceptor-2 (or) 1.544∗ 0.270 0.899 ∗ 1.034∗ 0.497∗ 1.063∗ ↓0.316 0.884\n(Wordlist Percentile 0.3)\n+ Conceptor-2 (pronouns) 1.554∗ 0.447∗ 1.116∗ 1.198∗ 0.973∗ 1.429∗ ↓0.080 1.120\n+ Conceptor-2 (extended) 1.584∗ 0.523∗ 1.248∗ 1.223∗ 0.636∗ 1.345∗ ↓0.107 1.093\n+ Conceptor-2 (propernouns) 1.648∗ 0.764∗ 1.087∗ 1.134∗ 1.084∗ 1.349∗ ↓0.022 1.178\n+ Conceptor-2 (all) 1.588∗ 0.783∗ 1.125∗ 1.132∗ 0.989∗ 1.325∗ ↓0.043 1.157\n+ Conceptor-2 (or) 1.549∗ 0.402∗ 1.236∗ 1.186∗ 1.024∗ 1.430∗ ↓0.062 1.138\n(Wordlist Percentile 0.2)\n+ Conceptor-2 (pronouns) 1.653∗ 0.588∗ 1.124∗ 1.084∗ 0.863∗ 1.257∗ ↓0.105 1.095\n+ Conceptor-2 (extended) 1.584∗ 0.523∗ 1.248∗ 1.223∗ 0.636∗ 1.345∗ ↓0.107 1.093\n+ Conceptor-2 (propernouns) 1.623∗ 0.753∗ 1.184∗ 1.224∗ 1.001∗ 1.336∗ ↓0.013 1.187\n+ Conceptor-2 (all) 1.600∗ 0.747∗ 1.190∗ 1.195∗ 0.986∗ 1.314∗ ↓0.028 1.172\n+ Conceptor-2 (or) 1.476∗ 0.364 1.089 ∗ 1.026∗ 0.407 1.162 ∗ ↓0.279 0.921\n(Wordlist Percentile 0.1)\n+ Conceptor-2 (pronouns) 1.668∗ 0.775∗ 1.154∗ 0.989∗ 0.813∗ 1.171∗ ↓0.105 1.095\n+ Conceptor-2 (extended) 1.689∗ 0.762∗ 1.345∗ 1.206∗ 0.991∗ 1.260∗ ↑0.009 1.209\n+ Conceptor-2 (propernouns) 1.705∗ 0.863∗ 1.257∗ 1.214∗ 0.812∗ 1.282∗ ↓0.011 1.189\n+ Conceptor-2 (all) 1.709∗ 0.870∗ 1.224∗ 1.203∗ 0.858∗ 1.287∗ ↓0.008 1.192\n+ Conceptor-2 (or) 1.630∗ 0.739∗ 1.069∗ 0.928∗ 0.753∗ 1.152∗ ↓0.155 1.045\nTable 17: SEAT effect size of gender debising. The impact of different percentiles of wordlist (using UMAP\nclustering) on Reddit Corpus, bert-tiny models. The top-3 best performance is colored in orange.\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\n(Layer 0)\nBERT-T 1.536 ∗ 0.640∗ 0.959∗ 1.307∗ 0.263 0.814 ∗ 0.920\n+ Conceptor-0 (or) 0.803∗ 0.103 0.249 0.825 ∗ 0.039 0.568 ∗ ↓0.489 0.431\n+ Conceptor-Intervened 0.803∗ 0.103 0.249 0.825 ∗ 0.039 0.568 ∗ ↓0.489 0.431\n(Layer 1)\nBERT-T 1.702 ∗ 1.019∗ 1.102∗ 1.250∗ 0.313 1.094 ∗ 1.080\n+ Conceptor-1 (or) 1.241∗ -0.067 0.588 ∗ 0.939∗ -0.340 0.477∗ ↓0.471 0.609\n+ Conceptor-Intervened 0.928∗ 0.022 -0.427 0.708 ∗ -0.753 0.542∗ ↓0.517 0.563\n(Layer 2)\nBERT-T 1.735 ∗ 0.797∗ 1.294∗ 1.243∗ 0.837∗ 1.293∗ 1.200\n+ Conceptor-2 (or) 1.542∗ 0.148 0.486 ∗ 0.806∗ -0.549 0.245 ↓0.571 0.629\n+ Conceptor-Intervened 1.026∗ -0.079 -0.264 0.862 ∗ -0.500 0.239 ↓0.705 0.495\nTable 18: SEAT effect size of gender debising from CI-BERT, Type I. The conceptor-intervened performance of\ndifferent layer’s conceptor matrix on SST Corpus, bert-tiny models. The layer(s) of CI-BERT that outperform the\nconceptor post-processing of the same layer(s) are colored in orange.\n10725\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nGPT2 -0.510 0.057 -0.274 -0.186 -0.369 -0.313 0.285\n(Wordlist Percentile 1.0)\n+ Conceptor-12 (pronouns) 0.030 0.269 -0.137 0.044 -0.129 -0.076 ↓0.171 0.114\n+ Conceptor-12 (extended) 0.053 0.293 -0.163 0.057 -0.114 -0.054 ↓0.163 0.122\n+ Conceptor-12 (propernouns) 0.713 ∗ 0.430∗ 0.029 0.222 0.045 0.196 ↓0.013 0.272\n+ Conceptor-12 (all) 0.443∗ 0.333 -0.107 0.102 -0.088 0.065 ↓0.095 0.190\n+ Conceptor-12 (or) 0.494∗ 0.269 0.065 0.236 -0.044 0.308 ↓0.049 0.236\n(Wordlist Percentile 0.9)\n+ Conceptor-12 (pronouns) 0.030 0.269 -0.137 0.044 -0.129 -0.076 ↓0.171 0.114\n+ Conceptor-12 (extended) 0.312 0.448∗ -0.352 0.032 -0.067 0.048 ↓0.075 0.210\n+ Conceptor-12 (propernouns) 0.713 ∗ 0.430∗ 0.029 0.222 0.045 0.196 ↓0.013 0.272\n+ Conceptor-12 (all) 0.443∗ 0.333 -0.107 0.102 -0.088 0.065 ↓0.095 0.190\n+ Conceptor-12 (or) 0.537 ∗ 0.268 0.051 0.260 -0.043 0.361 ↓0.032 0.253\n(Wordlist Percentile 0.8)\n+ Conceptor-12 (pronouns) 0.030 0.269 -0.137 0.044 -0.129 -0.076 ↓0.171 0.114\n+ Conceptor-12 (extended) 0.312 0.448∗ -0.352 0.032 -0.067 0.048 ↓0.075 0.210\n+ Conceptor-12 (propernouns) 0.713 ∗ 0.430∗ 0.029 0.222 0.045 0.196 ↓0.013 0.272\n+ Conceptor-12 (all) 0.443∗ 0.333 -0.107 0.102 -0.088 0.065 ↓0.095 0.190\n+ Conceptor-12 (or) 0.537 ∗ 0.268 0.051 0.260 -0.043 0.361 ↓0.032 0.253\n(Wordlist Percentile 0.7)\n+ Conceptor-12 (pronouns) 0.031 0.269 -0.136 0.045 -0.128 -0.076 ↓0.171 0.114\n+ Conceptor-12 (extended) 0.312 0.448∗ -0.352 0.032 -0.067 0.048 ↓0.075 0.210\n+ Conceptor-12 (propernouns) 0.713 ∗ 0.430∗ 0.029 0.222 0.045 0.196 ↓0.013 0.272\n+ Conceptor-12 (all) 0.443∗ 0.333 -0.107 0.102 -0.088 0.065 ↓0.095 0.190\n+ Conceptor-12 (or) 0.538 ∗ 0.269 0.051 0.260 -0.043 0.361 ↓0.031 0.254\n(Wordlist Percentile 0.6)\n+ Conceptor-12 (pronouns) 0.031 0.269 -0.136 0.045 -0.128 -0.076 ↓0.171 0.114\n+ Conceptor-12 (extended) 0.304 0.449∗ -0.381 -0.002 -0.106 0.016 ↓0.075 0.210\n+ Conceptor-12 (propernouns) 0.713 ∗ 0.430∗ 0.029 0.222 0.045 0.196 ↓0.013 0.272\n+ Conceptor-12 (all) 0.443∗ 0.333 -0.107 0.102 -0.088 0.065 ↓0.095 0.190\n+ Conceptor-12 (or) 0.519 ∗ 0.255 0.036 0.244 -0.054 0.348 ↓0.042 0.243\n(Wordlist Percentile 0.5)\n+ Conceptor-12 (pronouns) 0.031 0.269 -0.136 0.045 -0.128 -0.076 ↓0.171 0.114\n+ Conceptor-12 (extended) 0.269 0.411∗ -0.258 -0.038 0.114 0.027 ↓0.099 0.186\n+ Conceptor-12 (propernouns) 0.713 ∗ 0.430∗ 0.029 0.222 0.045 0.196 ↓0.013 0.272\n+ Conceptor-12 (all) 0.565 ∗ 0.445∗ -0.110 0.125 -0.063 0.127 ↓0.046 0.239\n+ Conceptor-12 (or) 0.478∗ 0.248 0.029 0.188 -0.016 0.317 ↓0.072 0.213\n(Wordlist Percentile 0.4)\n+ Conceptor-12 (pronouns) 0.060 0.271 -0.130 0.045 -0.131 -0.074 ↓0.167 0.118\n+ Conceptor-12 (extended) 0.276 0.410∗ -0.255 -0.038 0.110 0.027 ↓0.099 0.186\n+ Conceptor-12 (propernouns) 0.730 ∗ 0.393∗ 0.033 0.190 0.028 0.127 ↓0.035 0.250\n+ Conceptor-12 (all) 0.648 ∗ 0.374∗ -0.036 0.154 0.014 0.134 ↓0.058 0.227\n+ Conceptor-12 (or) 0.564 ∗ 0.258 0.057 0.196 0.014 0.287 ↓0.056 0.229\n(Wordlist Percentile 0.3)\n+ Conceptor-12 (pronouns) 0.092 0.316 -0.001 0.064 -0.035 -0.062 ↓0.190 0.095\n+ Conceptor-12 (extended) 0.264 0.369 -0.261 -0.043 0.115 0.015 ↓0.107 0.178\n+ Conceptor-12 (propernouns) 0.729 ∗ 0.391∗ 0.030 0.187 0.028 0.125 ↓0.037 0.248\n+ Conceptor-12 (all) 0.682 ∗ 0.397∗ -0.021 0.162 0.018 0.140 ↓0.048 0.237\n+ Conceptor-12 (or) 0.545 ∗ 0.214 0.072 0.198 0.006 0.279 ↓0.066 0.219\n(Wordlist Percentile 0.2)\n+ Conceptor-12 (pronouns) 0.094 0.316 -0.001 0.064 -0.033 -0.062 ↓0.190 0.095\n+ Conceptor-12 (extended) 0.249 0.356 -0.238 -0.048 0.106 0.016 ↓0.116 0.169\n+ Conceptor-12 (propernouns) 0.699 ∗ 0.403∗ 0.042 0.176 0.017 0.092 ↓0.047 0.238\n+ Conceptor-12 (all) 0.699 ∗ 0.437∗ 0.056 0.164 0.063 0.098 ↓0.032 0.253\n+ Conceptor-12 (or) 0.519 ∗ 0.223 0.086 0.191 -0.004 0.255 ↓0.072 0.213\n(Wordlist Percentile 0.1)\n+ Conceptor-12 (pronouns) 0.446∗ 0.599∗ 0.035 0.064 0.064 -0.037 ↓0.077 0.208\n+ Conceptor-12 (extended) 0.497∗ 0.544∗ 0.019 0.097 0.171 0.074 ↓0.051 0.234\n+ Conceptor-12 (propernouns) 0.753 ∗ 0.495∗ -0.038 0.095 -0.010 0.062 ↓0.043 0.242\n+ Conceptor-12 (all) 0.730 ∗ 0.508∗ -0.009 0.086 0.102 0.070 ↓0.034 0.251\n+ Conceptor-12 (or) 0.914 ∗ 0.568∗ 0.258 0.247 0.021 0.264 ↑0.094 0.379\nTable 19: SEAT effect size of gender debising. The impact of different percentiles of wordlist (using UMAP\nclustering) on Brown Corpus, gpt-2 models. The top-3 best performance is colored in orange.\n10726\nModel CoLA MNLI MRPC QNLI QQP RTE SST STS-B WNLI Average\nBERT-L 62.82 86.13 88.32 92.15 91.56 69.31 93.81 90.00 35.21 78.81\n+ Conceptor P.P. 62.34 86.16 89.44 91.71 91.59 74.73 93.58 90.06 30.17 ↑0.05 78.86\nGPT2 29.10 82.43 84.51 87.71 89.18 64.74 91.97 84.26 43.19 73.01\n+ Conceptor P.P. 35.47 82.39 84.08 88.30 89.12 67.15 92.09 83.67 33.80 ↓0.11 72.90\nGPT2-L 12.48 88.80 82.98 80.30 87.61 51.26 81.77 78.87 46.48 75.84\n+ Conceptor P.P. 20.03 88.92 82.78 79.64 87.65 50.54 82.34 78.26 40.85 ↑0.04 75.89\nGPT-J 59.73 82.49 87.95 87.93 87.54 75.81 94.50 88.60 39.44 78.22\n+ Conceptor 59.48 82.89 87.69 91.56 89.90 76.17 95.07 88.81 30.99 ↓0.14 78.06\nTable 20: GLUE validation set results for other LLMs. We use the F1 score for MRPC, the Spearman correlation for\nSTS-B,and Matthew’s correlation for CoLA. For all other tasks,we report the accuracy.\nModel SEAT-6 SEAT-6b SEAT-7 SEAT-7b SEAT-8 SEAT-8b Avg. Abs.\nBERT 0.931 ∗ 0.090 -0.124 0.937 ∗ 0.783∗ 0.858∗ 0.620\n+ Gender Conceptor 0.388 -0.078 -0.292 0.179 0.594 ∗ 0.335 ↓0.309 0.311\n+ Intersected Conceptor 0.916∗ 0.026 -0.180 0.840∗ 0.749∗ 0.832∗ ↓0.029 0.591\nTable 21: BERT intersectional gender debiasing, where intersected conceptor indicates the conceptor matrix\ngenerated by its negated AND operation of gender conceptor matrix and race conceptor matrix\nModel ABW-1 ABW-2 SEAT-3 SEAT-3b SEAT-4 SEAT-5 SEAT-5b Avg. Abs.\nBERT -0.079 0.690 ∗ 0.778∗ 0.469∗ 0.901∗ 0.887∗ 0.539∗ 0.620\n+ Race Conceptor -0.063 0.682 ∗ 0.803∗ 0.209 0.949∗ 0.946∗ 0.390∗ ↓0.043 0.577\n+ Intersected Conceptor -0.045 0.685 ∗ 0.799∗ 0.361∗ 0.926∗ 0.931∗ 0.484∗ ↓0.016 0.604\nTable 22: BERT intersectional race debiasing, where intersected conceptor indicates the conceptor matrix generated\nby its negated AND operation of gender conceptor matrix and race conceptor matrix\n10727"
}