{
  "title": "CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare",
  "url": "https://openalex.org/W4393146352",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2441484206",
      "name": "Akash Ghosh",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": null,
      "name": "Arkadeep Acharya",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2791890896",
      "name": "Raghav Jain",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2130936750",
      "name": "Sriparna Saha",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2118470473",
      "name": "Aman Chadha",
      "affiliations": [
        "Stanford University",
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2516204347",
      "name": "Setu Sinha",
      "affiliations": [
        "Indira Gandhi Institute of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2441484206",
      "name": "Akash Ghosh",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": null,
      "name": "Arkadeep Acharya",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2791890896",
      "name": "Raghav Jain",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2130936750",
      "name": "Sriparna Saha",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2118470473",
      "name": "Aman Chadha",
      "affiliations": [
        "Stanford University",
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2516204347",
      "name": "Setu Sinha",
      "affiliations": [
        "Indira Gandhi Institute of Medical Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6763472932",
    "https://openalex.org/W3166358520",
    "https://openalex.org/W3167252243",
    "https://openalex.org/W6853163053",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3167691769",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4306317214",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4366850928",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W4362598512",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W3189279073",
    "https://openalex.org/W3177314227",
    "https://openalex.org/W4378770578",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W2950161719",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4360600216",
    "https://openalex.org/W4380715596"
  ],
  "abstract": "In the era of modern healthcare, swiftly generating medical question summaries is crucial for informed and timely patient care. Despite the increasing complexity and volume of medical data, existing studies have focused solely on text-based summarization, neglecting the integration of visual information. Recognizing the untapped potential of combining textual queries with visual representations of medical conditions, we introduce the Multimodal Medical Question Summarization (MMQS) Dataset. This dataset, a major contribution of our work, pairs medical queries with visual aids, facilitating a richer and more nuanced understanding of patient needs. We also propose a framework, utilizing the power of Contrastive Language Image Pretraining(CLIP) and Large Language Models(LLMs), consisting of four modules that identify medical disorders, generate relevant context, filter medical concepts, and craft visually aware summaries. Our comprehensive framework harnesses the power of CLIP, a multimodal foundation model, and various general-purpose LLMs, comprising four main modules: the medical disorder identification module, the relevant context generation module, the context filtration module for distilling relevant medical concepts and knowledge, and finally, a general-purpose LLM to generate visually aware medical question summaries. Leveraging our MMQS dataset, we showcase how visual cues from images enhance the generation of medically nuanced summaries. This multimodal approach not only enhances the decision-making process in healthcare but also fosters a more nuanced understanding of patient queries, laying the groundwork for future research in personalized and responsive medical care. Disclaimer: The article features graphic medical imagery, a result of the subject's inherent requirements.",
  "full_text": "CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in\nHealthcare\nAkash Ghosh1*, Arkadeep Acharya1*, Raghav Jain1, Sriparna Saha1, Aman Chadha2,3† ,\nSetu Sinha4\n1Department of Computer Science And Engineering, Indian Institute of Technology Patna, India\n2Stanford University\n3Amazon AI\n4Indira Gandhi Institute of Medical Sciences\n{akash 2321cs19, arkadeep 2101ai41, sriparna}@iitp.ac.in, raghavjain106@gmail.com, hi@aman.ai, drsinhasetu@gmail.com\nAbstract\nIn the era of modern healthcare, swiftly generating medical\nquestion summaries is crucial for informed and timely patient\ncare. Despite the increasing complexity and volume of med-\nical data, existing studies have focused solely on text-based\nsummarization, neglecting the integration of visual informa-\ntion. Recognizing the untapped potential of combining textual\nqueries with visual representations of medical conditions, we\nintroduce the Multimodal Medical Question Summarization\n(MMQS) Dataset. This dataset, a major contribution of our\nwork, pairs medical queries with visual aids, facilitating a\nricher and more nuanced understanding of patient needs. We\nalso propose a framework, utilizing the power of Contrastive\nLanguage Image Pretraining(CLIP) and Large Language Mod-\nels(LLMs), consisting of four modules that identify medical\ndisorders, generate relevant context, filter medical concepts,\nand craft visually aware summaries. Our comprehensive frame-\nwork harnesses the power of CLIP, a multimodal foundation\nmodel, and various general-purpose LLMs, comprising four\nmain modules: the medical disorder identification module, the\nrelevant context generation module, the context filtration mod-\nule for distilling relevant medical concepts and knowledge,\nand finally, a general-purpose LLM to generate visually aware\nmedical question summaries. Leveraging our MMQS dataset,\nwe showcase how visual cues from images enhance the gen-\neration of medically nuanced summaries. This multimodal\napproach not only enhances the decision-making process in\nhealthcare but also fosters a more nuanced understanding of\npatient queries, laying the groundwork for future research in\npersonalized and responsive medical care.\nDisclaimer: The article features graphic medical imagery, a\nresult of the subject’s inherent requirements.\nIntroduction\nIn the midst of the global COVID-19 pandemic, healthcare\nsystems have been inundated with an unprecedented volume\nof inquiries, concerns, and uncertainties. Individuals world-\nwide are posing health-related inquiries on online platforms,\nseeking clarity and guidance on symptoms, prevention, treat-\n*These authors contributed equally.\n†Work does not relate to position at Amazon.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nments, and vaccinations. These questions often employ every-\nday language and encompass both pertinent and extraneous\ndetails that may not directly pertain to the sought-after solu-\ntions. The complexity and urgency of the situation, coupled\nwith a considerable imbalance in the doctor-to-patient ratio\nacross many countries, have made the ability to swiftly and\ncomprehensively comprehend a patient’s query paramount\n(Abacha and Demner-Fushman 2019; Yadav et al. 2021). In\nthis context, medical question summarization has emerged as\na vital tool to distill information from consumer health ques-\ntions, ensuring the provision of accurate and timely responses.\nHowever, existing works have overlooked the untapped po-\ntential of integrating visual data, particularly images, with\ntextual information. The motivations for focusing on visual\naids within medical question summarization (MQS) are man-\nifold. A significant portion of the population lacks familiarity\nwith medical terms needed to accurately describe various\nsymptoms, and some symptoms are inherently challenging\nto articulate through text alone. Patients may also be con-\nfused between closely related symptoms, such as distinguish-\ning between skin dryness and skin rash. The combination\nof text and images in medical question summarization can\noffer enhanced accuracy and efficiency, providing a richer\ncontext that textual analysis alone may miss. This approach\nrecognizes the complex nature of patient queries, where pho-\ntographs of symptoms, medical reports, or other visual aids\ncould provide crucial insights. By focusing on the integration\nof images, researchers and healthcare providers can respond\nto the evolving challenges of modern healthcare communica-\ntion.\nLarge Language Models (LLMs) (Kojima et al. 2023) and\nVision Language Models (VLMs) (Zhang et al. 2023) have\nexhibited remarkable capacities in generating human-like\ntext and multimedia content. This prowess has driven their\ndeployment across the medical domain, predominantly for\ndomain-specific tasks such as chest radiograph summariza-\ntion (Thawkar et al. 2023) and COVID-19 CT report gener-\nation (Liu et al. 2021). Yet, their application in multimodal\nmedical question summarization remains uncharted territory.\nLeveraging zero-shot and few-shot learning capabilities of\nthese models (Dong et al. 2022) offers a compelling advan-\ntage, especially for tasks like multimodal medical question\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22031\nFigure 1: Broad categorization of medical disorders in the MMQS Dataset (MMQSD). The number of data points corresponding\nto each category has been provided under each category.\n200 400 600 800 1000 1200 1400 1600\nLength of Question\n0\n200\n400\n600\n800\n1000Frequency\nDistribution of Question Lengths\n(a) Distribution of Question Lengths.\n75 100 125 150 175 200 225 250 275\nLength of Question Summaries\n0\n200\n400\n600\n800\n1000\n1200\nFrequency\nDistribution of Question Summary Length (b) Distribution of Summary Lengths.\nFigure 2: Distribution of question lengths and their summaries in the MMQS dataset.\nsummarization, characterized by inherent data scarcity. How-\never, while the potential of LLMs and VLMs in this domain\nis undeniable, they aren’t without constraints. Predominantly,\ngeneric LLMs and VLMs often lack a solid grounding in task-\nspecific knowledge, risking the generation of summaries that\nmight miss intricate details like symptoms, diagnostic tests,\nand medical complexities. On the visual front, even as VLMs\nhave excelled in typical visual-linguistic tasks, medical imag-\ning presents unique challenges. Efforts like SkinGPT4 (Zhou\net al. 2023), which fine-tunes MiniGPT4 (Zhu et al. 2023)\non skin disease images and clinical notes, are still notably\ndomain-specific. Medical images are inherently complex, de-\nmanding a profound understanding of medical terminology\nand visual conventions, often necessitating an expert medical\npractitioner’s perspective for accurate interpretation. This\ncomplexity, combined with potential gaps in contextual un-\nderstanding, can result in models producing misleading or\nirrelevant summaries.\nTo address the limitations of LLMs and VLMs in multi-\nmodal medical question summarization, we’ve conceived the\nCLIPSyntel framework. The first stage, the Medical Disor-\nder Identification Module, combines Large Language Model\n(LLM) with Contrastive Language Image Pretraining (CLIP)\n(Radford et al. 2021), forming a novel zero-shot classifi-\ncation approach to identify disorders from visual images,\nutilizing the unique benefits of zero or few-shot learning.\nSubsequently, during the Context Generation Phase, LLM\nadds context to the medical query, focusing on vital com-\nponents like symptoms, tests, and procedures, mindful of\nthe potential pitfalls of extraneous content. Addressing this\nchallenge, the Context Filtration Module filters the content us-\ning a multimodal knowledge selection technique, preserving\nonly the most relevant information. The Summary Genera-\ntion Phase follows, where an LLM crafts the final medically\naccurate summaries based on the distilled knowledge. Finally,\nto validate CLIPSyntel, we created the MMQS dataset, rich\nin visual and textual representations of patients’ symptoms\nand queries1. This careful, step-by-step structuring of CLIP-\nSyntel allows it to bridge the divide between general-purpose\nmodels and the niche requirements of medical question sum-\nmarization, effectively integrating textual and visual data to\ncreate precise and context-aware medical summaries. To sum-\nmarize we make the following main contributions:\nA novel task of Multimodal Medical Question Summariza-\ntion for generating medically nuanced summaries.\n1https://github.com/AkashGhosh/CLIPSyntel-AAAI2024\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22032\nA novel dataset, MMQS Dataset, to advance the research in\nthis area.\nA novel metric MMFCM to quantify how well the model\ncaptures the multimodal information in the generated sum-\nmary.\nA novel framework, ClipSyntel that harnesses the power of\nCLIP and LLMs to augment the patient question with an ad-\nditional rich context from visual symptoms for the generation\nof final summaries.\nRelated Works\nMedical Question Summarization: The task of Medical\nQuestion Summarization (MQS) was initially introduced in\n2019 with the development of the MeQSum dataset (Abacha\nand Demner-Fushman 2019) specifically for this purpose.\nEarly research on MQS employed vanilla seq2seq models and\npointer generator networks to generate summaries. In 2021,\na contest was held focusing on the generation of medical\ndomain summaries (Abacha et al. 2021). Participants utilized\nvarious pre-trained models such as PEGASUS (Zhang et al.\n2020), ProphetNet (Qi et al. 2020), and BART (Lewis et al.\n2019). Techniques like multi-task learning were employed,\nutilizing BART to jointly optimize question summarization\nand entailment tasks (Mrini et al. 2021). Another approach in-\nvolved reinforcement learning with question-aware semantic\nrewards derived from two subtasks: question focus recogni-\ntion (QFR) and question type identification (QTR) (Yadav\net al. 2021).\nRole of Multimodality: In order to receive accurate treat-\nment and guidance from a medical expert, our responsibil-\nity is to effectively and efficiently communicate the medical\nsymptoms which can be done with additional visual cues. Pre-\nviously, many studies also showed that adding multimodality\nthrough visual cues improves the performance of various\nmedical tasks. Tiwari et al. (2022) shows how multimodal in-\nformation helps in building better Disease Diagnosis Virtual\nAssistants. Delbrouck, Zhang, and Rubin (2021) also showed\nhow incorporating images helps in better summarization of\nradiology reports. Gupta, Attal, and Demner-Fushman (2022)\nalso showed the benefit of incorporating videos for medical\nQA task The current work is motivated by this idea of how in-\ntegrating the patient’s provision of an image of their medical\ncondition alongside the text, improves the generation of more\nmedically rich summaries. We curate a multimodal dataset\nbased on an existing MQS dataset, focusing on a predefined\nset of symptoms, and propose a novel framework that com-\nbines cutting-edge Multimodal Foundation Models like CLIP\nwith Large Language Models (LLMs). To our best under-\nstanding, this work is the first to tackle the task of question\nsummarization in the medical domain, particularly within a\nmultimodal setting.\nMMQS (Multimodal Medical Question\nSummarization) Dataset\nTo the best of our knowledge, a freely available multimodal\nquestion summarization dataset that includes both textual\nquestions and corresponding medical images of patients’ con-\nditions does not currently exist. To construct such a dataset,\nwe utilized the preexisting HealthCareMagic Dataset, which\nis derived from the MedDialog data introduced by (Mrini\net al. 2021). The initial dataset comprised 226,395 samples,\nof which 523 were found to be duplicates. To ensure data in-\ntegrity and fairness, we eliminated these duplicate entries. In\norder to ascertain the medical symptoms or signs that could\neffectively be conveyed through visual means, we consulted\na medical professional who also happens to be a co-author of\nthis paper. Following a series of brainstorming sessions and\ncarefully analyzing the dataset, we identified 18 symptoms\nthat are hard to specify only through text. These 18 symptoms\nare broadly divided into four broad groups of multimodal\nsymptoms to incorporate into our dataset. These groups (refer\nto figure 1) are categorized as ENT (Ear, Nose, and Throat),\nEYE (Eye-related), LIMB (Limbs-related), and SKIN (Skin-\nrelated). From the ENT category, we selected symptoms in-\ncluding lip swelling, mouth ulcers, and swollen tonsils. From\nthe EYE category, we choose swollen eyes, eye redness, and\nitchy eyelids. For the LIMB category, we included symptoms\nsuch as edema, foot swelling, knee swelling, hand lumps, and\nneck swelling. Lastly, from the SKIN category, we included\nsymptoms of skin rash, skin irritation, and skin growth. To\nextract images for the corresponding symptoms, Bing Im-\nage Search API\n2 was used. Then the images extracted are\nverified by a group of medical students who are led by a med-\nical expert. Therefore, in curating our ultimate multimodal\ndataset, we selectively included instances from the Health-\nCareMagic dataset that featured textual mentions of body\nparts such as skin, eyes, ears, and others, both within the\nquestions and their corresponding summaries. We refrained\nfrom directly searching for symptom names, as our intention\nwas to encompass instances from the dataset where uncer-\ntainty regarding the medical condition exists. Consequently,\npatients implicitly allude to symptoms rather than explicitly\nstating their names. To identify these relevant samples, we\nemployed the Python library FlashText\n3 for efficient term\nmatching in the aforementioned dataset. The Python library\nTextblob4 was used to correct the spelling of many improp-\nerly spelled words. To address the misspelling of numerous\nwords, the Textblob44 Python library was employed. Follow-\ning an initial exploration using FlashText, we initially col-\nlected approximately 5000 samples. After meticulous manual\nvalidation of each sample, our refinement process led us to a\nfinal dataset comprising 3015 samples where multimodality\ncould be incorporated for the final dataset creation.\nData Annotation\nWe extracted 100 random samples from the filtered dataset,\neach comprising a patient’s inquiry alongside its correspond-\ning summary, and gave them to the medical expert for anno-\ntation. The methodology through which the medical expert\nannotated these samples is explained with the given example.\nFor example, suppose the patient is complaining regarding\nsomething that has happened near their tonsils, but they are\n2https://www.microsoft.com/en-us/bing/apis/bing-image-\nsearch-api\n3https://pypi.org/project/flashtext/1.0/\n4https://pypi.org/project/textblob/0.9.0/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22033\n(a) Sample instances from the MMQS-dataset.\n (b) Medical Disorder Identification Module\nFigure 3: Comprehensive illustration of Sample Instances from the MMQS-dataset and Medical Disorder Identification Module.\nFigure 4: Flowchart depicting the workflow of the proposed model CLIPSyntel. The diagram illustrates the step-by-step process\nof ClipSyntel’s functioning.\nnot able to name the exact medical disorder they are suffer-\ning from. But their textual reference gave enough context\nto understand that the patient is suffering from the disorder\nnamed swollen-tonsils. After understanding this, the medical\nexpert adds statements like Please see what happened to my\ntonsils in the image belowin the context and accordingly\nadds a visual image representation of the disorder. In this\nway, visual medical signs are incorporated into the textual\nmedical question. Additionally, the medical expert deduced\nthat the conventional golden summaries did not align well\nwith the multimodal queries, prompting the need for cor-\nresponding updates. To address these issues, the medical\nexpert himself modified these 100 samples by revising the\nquestions, incorporating multimodal information, and adapt-\ning the golden summaries accordingly. Subsequently, we\nengaged with medical students and graduates to annotate\nthe remaining samples based on the guidelines given by the\nmedical expert5. To quantify the level of agreement among\nannotators, we calculated the kappa coefficient (k), which\nyielded a value of 0.78. This coefficient indicates a notewor-\nthy level of consistent annotation. Following this rigorous\n5The students were compensated through gift vouchers and hon-\norarium.\nprocess, the Multimodal Medical Question Summarization\n(MMQS) dataset comprised a total of 3,015 samples. The\nmedian length of the question in the final dataset is 442 words\nand the median length of the question summary is 121 words,\nand example data points are also shown in Table-3a. The\ndistributions of question length and question summary length\nare shown in Figure 2a and Figure 2b, respectively.\nMethodology\nProblem Formulation: Each data point consists of a patient\nquery in textual form T and an image I of the medical disor-\nder that the patient is suffering from or trying to address in\nhis textual query. The final output is a concise summary\nS\nof the query which incorporates information from both the\nmodalities. This section presents the novel architecture of\nCLIPSyntel shown in Figure-4, a multimodal, knowledge-\ngrounded framework designed to generate medically nuanced\nsummaries.For a comprehensive understanding, we partition\nour approach into four distinct modules: (i) Medical Disor-\nder Identification Module, (ii) Contextual Information\nGeneration Module, (iii) Context Filtration Module, and\n(iv) Summary Generation Module\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22034\nMedical Disorder Identification\nFor the process of generating pertinent medical knowledge\nit is necessary to accurately identifying the medical disorder\nthat the patient is experiencing. To achieve this, we employed\nthe following structured approach: (1) Utilization of CLIP:\nCLIP, designed to establish connections between images and\ntext, is initially applied. Despite its applicability to various\nvisual classification tasks, CLIP’s effectiveness in our Medi-\ncal Disorder Identification Task using only the names of the\ndisorder is limited due to the complex nature of medical im-\nages. (2) Enhanced Contextualization through Prompts:\nTo provide better context for a particular medical disorder,\nwe prompted GPT 3.5 using the prompt: Describe the image\nof the disease {DISEASE } in a single sentence stating\nall the necessary details. This approach yields improved\ncontextual information for the specific medical condition.\n(3) Identification of Probable Disorders: When presented\nwith an image of a medical disorder, we provided the con-\ntextual information for all 18 medical disorders, along with\nthe corresponding image, to the CLIP model. Subsequently,\nwe selected the top 3 most likely medical disorders based on\nCLIP’s analysis. (4) Final Prediction with GPT-3.5: These\nthree probable diseases, along with the medical query, is\nthen subsequently passed to LLM(GPT-3.5) using the prompt\nwhich is shown in Figure-3b to predict the final medical disor-\nder. Considering only the most probable disorder prediction\nfrom CLIP yields an accuracy of 84 %. However, when in-\ncorporating the top 3 most probable disorders alongside the\ncontext after passing through the LLM, the accuracy is en-\nhanced to 87% in a zero-shot setup. The entire pipeline for\nthis module, displaying its logical flow and connections, is\ndepicted in Figure 3b.\nContextual Medical Knowledge Generation\nIn cases where patients may lack awareness of their medical\ncondition, and textual inquiries are insufficient, acquiring\nadditional knowledge about their specific symptoms and the\nnecessary tests and procedures(as advised by medical ex-\nperts) proves essential for creating a meaningful summary,\nespecially in a multimodal setup. Formally, after identifi-\ncation of the medical disorder M, we formulate a set of\nprompts P = {p0, p1, . . . , pn} with the aim of generat-\ning diverse contextual information about that disorder M,\nKS = {ks0, ks1, . . . , ksn} by promting GPT-3.5 as follows.\nThe Prompts used to generate contextual information are as\nbelow:\n(1) What are the symptoms of the medical condition\n〈medical disorder 〉?\n(2) What tests and procedures need to be done for the\nmedical condition〈medical disorder〉?\nMultimodal Medical Knowledge Filtration\nWhile we anticipate that GPT-3.5 will generate useful and\npertinent information based on the given prompt, it occasion-\nally engages in hallucinations and produces irrelevant content\nin relation to the primary inquiry. This hallucinated text has\nthe potential to divert Large Language Models (LLMs) from\ngenerating high-quality summaries. It may also generate con-\ntent that is not pertinent for doctors to investigate the case\nswiftly. Particularly in fields such as medicine, one must ex-\nercise heightened caution to minimize misinformation, given\nthat errors could have serious consequences for patients. To\ntackle this issue, we present a filtering strategy called Mul-\ntimodal Medical Knowledge Filtration, which operates as\nfollows. Formally, for each ksi, which corresponds to a spe-\ncific text field in the KS, it is further broken down into a\nset of k sentences ksi = {s1, s2, . . . , sm} using sentence\ntokenization6. To achieve this, we employ a pre-trained multi-\nmodal model, ImageBind (Girdhar et al. 2023), which serves\nas an off-the-shelf encoder-based multimodal model enc(·)\nthat maps a token sequence (text) and an image to their re-\nspective feature vectors embedded in a unified vector space.\nCosine similarity sim(·, ·) is then used to measure the rele-\nvance of each sentence to the image. Formally, a sentence\nsj is retained if sim(enc(sj), enc(I)) > Th, where I is the\nimage associated with the medical disorder, and Th is the\npredefined similarity threshold. Building upon this, we define\nthe subset of retained knowledge sentences, having under-\ngone the Multimodal Medical Knowledge Filtration module\n(MMKFS), as follows:\nMs′\ni = {sj |sj ∈ ksi and\nsim(enc(sj), enc(I)) > Th} (1)\nWith this approach, the filtered knowledge sentences\nMS ′ = {ms′\n0, . . . , ms′\nn} not only hold a high degree of\nvisual-textual alignment with the corresponding medical dis-\norder image but are also contextually relevant.\nSummary Generation Module\nThis component is crafted to leverage the refined knowledge\nsentences, denoted as MS ′ in order to craft informative and\ncontextually relevant summaries for patients’ healthcare in-\nquiries. The Summary Generation Module functions with\na pre-trained Large Language Model (LLM), referred to as\nLM. For summary creation, LM is provided a specially\ncrafted prompt Pin that integrates both the actual patient\nquery X and the generated medical disorder knowledgeMS ′.\nThe outcome of this process is the generated summary text,\ndenoted as S. In a more formal representation, this process\ncan be articulated as: S = LM(Pin(X, MS′)). The prompt\nused to generate the final summary is presented below:\nWrite a very short and concise one line summary of the\nfollowing dialogue as a question in a healthcare forum\nincorporating the relevant medical symptoms in the addi-\ntional context given below. Dialogue:\n〈Patient Query〉and\nAdditional Context:〈Filtered Medical Context〉\nExperimental Results and Discussion\nExperimental Setup: We leveraged the following general\npurpose LLMs for summary generation module: RedPajama7,\nFLAN-T5 (Chung et al. 2022), Vicuna (Zheng et al. 2023)\nand GPT-3.5. We tested these LLMs in different settings:\nWith only the patient’s question(text) in the prompt, With\nthe patient’s question combined with the knowledge obtained\n6https://www.nltk.org/api/nltk.tokenize.html\n7https://huggingface.co/togethercomputer/RedPajama-\nINCITE-Chat-3B-v1\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22035\nModel ROUGE BLEU BERTScoreR1 R2 RL B1 B2 B3 B4\nLLAMA-2 LLM(only texual query) 0.188 0.051 0.159 0.144 0.064 0.032 0.016 0.822\nClip+GPT-3.5 0.189 0.052 0.161 0.156 0.070 0.036 0.018 0.819\nCLIPSyntel 0.249 0.081 0.211 0.195 0.102 0.059 0.034 0.864\nRedPajama LLM(only texual query) 0.146 0.023 0.125 0.086 0.030 0.011 0.004 0.828\nClip+GPT-3.5 0.148 0.025 0.132 0.087 0.0314 0.012 0.004 0.831\nCLIPSyntel 0.157 0.0279 0.140 0.088 0.032 0.014 0.006 0.838\nVicuna-7B LLM(only textual query) 0.372 0.160 0.318 0.385 0.243 0.161 0.102 0.905\nClip+GPT-3.5 0.384 0.164 0.321 0.391 0.245 0.167 0.105 0.906\nCLIPSyntel 0.391 0.167 0.33 0.40 0.25 0.171 0.108 0.910\nFLAN-T5 LLM(only textual query) 0.220 0.042 0.205 0.129 0.052 0.026 0.012 0.88\nClip+ GPT-3.5 0.220 0.042 0.205 0.136 0.056 0.026 0.012 0.88\nCLIPSyntel 0.243 0.068 0.215 0.182 0.096 0.052 0.0256 0.891\nGPT-3.5 LLM(only textual query) 0.451 0.227 0.396 0.471 0.330 0.243 0.170 0.920\nClip + GPT-3.5 0.452 0.226 0.399 0.471 0.331 0.242 0.171 0.920\nCLIPSyntel 0.463 0.241 0.409 0.478 0.342 0.257 0.186 0.921\nTable 1: Performance of various CLIPSyntel models and corresponding baselines, evaluated using automatic metrics with\ndifferent LLMs.\nModel(GPT-3.5) Clinical-EvalScore Factual Recall Omission Rate MMFCM Score\nLLM(Patient Query) 3.4 0.748 0.235 0.9\nClip + GPT-3.5 3.51 0.792 0.2215 1.52\nCLIPSyntel 3.62 0.818 0.2217 1.6\nAnnotated Summary 4.1 0.889 0.144 1.86\nTable 2: Human evaluation scores of the bestCLIPSyntel model and their corresponding baselines across different metrics.\nfrom (CLIP+GPT3.5), and then our proposed framework\n((CLIPSyntel)). We have set the similarity threshold parame-\nter Th to 0.5 and temperature to 0.5 across all settings based\non a thorough investigation. We utilize ROUGE (Lin 2004),\nBLEU (Papineni et al. 2002), and BERTScore (Zhang et al.\n2019) as automatic evaluation metrics. For the purpose of\nhuman evaluation, we collaborate with a medical expert and\na few medical students. We have identified four distinct and\nmedically nuanced metrics for this evaluation: clinical eval-\nuation score, factual recall (Abacha et al. 2023), omission\nrate(Abacha et al. 2023), and our newly introduced MMFCM\nscore.\nAutomated Evaluation: The results presented in Table\n1 provide valuable insights into the performance of various\nmodels, highlighting the effectiveness of different approaches\nin the context of the task. Below, we discuss some key ob-\nservations and trends: (1) CLIPSyntel Performance: Across\nall LLMs, CLIPSyntel consistently performed the best. This\nindicates the robustness and versatility ofCLIPSyntel, demon-\nstrating its ability to outshine other base models. The results\nsuggest the capability of CLIPSyntel’s design to effectively\nleverage both textual and visual information. These results\nalso show that adding contextual information does help in\nour task. (2) GPT-3.5 based Models’ Superiority: Among\nthe various models evaluated, the GPT-3.5 based models\nstood out as the top performers compared to open-source\nmodels. The high scores across multiple metrics reflect the\nsophisticated design and capability of GPT-3.5. (3) Best Per-\nformance Among Open-Source LLMs:Vicuna was the best\nperformer among open-source LLMs. Its scores were notably\nhigher compared to other open-source counterparts, signaling\nits potential as an effective alternative for specialized tasks.\nHuman Evaluation: The human evaluation was done by a\nteam of medical students led by a doctor. The team was given\n10 % of the dataset (selected at random) for evaluation pur-\npose and was asked to rate the summaries generated, which\ntakes only patient question (text) as context, patient query in\naddition to the medical disorder context (CLIP + GPT-3.5)\nand finally the summary generated by our proposed pipeline\n(CLIPSyntel). The following metrics are used for the eval-\nuation: (1) Clinical Evaluation Score: The doctor and his\nteam were asked to rate the summaries between 1 (poor) and\n5 (good) based on their overall relevance, consistency, flu-\nency, and coherence.(2) Multi-modal fact capturing metric\n(MMFCM): We propose a new metric to evaluate how well\na model incorporates relevant medical facts and identifies the\ncorrect disorder in a multimodal setup. MMFCM is calcu-\nlated by considering both the facts extracted from the medical\nquery and image and assessing whether they are correctly\nincorporated in the generated summary. The metric accounts\nfor (1) The ratio of correct facts in the summary to the total\nnumber of relevant medical facts. (2) Additional scores based\non the accuracy of the disorder’s detection, with values ranging\nfrom +2 for fully correct to -1 for incorrect identification. See\nAlgorithm 1 for the detailed algorithm.\n(3) Medical Fact Based Metrics: We employed the Fac-\ntual Recall and Omission Recall metrics (Abacha et al. 2023)\nto assess the extent to which the generated summary cap-\ntures medical facts compared to the gold standard annotated\nsummary.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22036\nFigure 5: Sample Summaries generated by two variations\nGPT-3.5 model: only question and CLIPSyntel.\nTable 2 offers a comprehensive comparative analysis, un-\nderscoring ClipSyntel’s distinct advantages over various base-\nline methods across multiple human evaluation performance\nmetrics. Notably, our proposed architecture excels the sum-\nmaries that incorporate only the patient’s textual information,\nhighlighting the valuable impact of fusing visual symptom\ncues with textual patient queries, resulting in more clinically\nnuanced summaries. Moreover, the augmentation of medical\nknowledge is affirmed by substantial enhancements in Factual\nRecall and Omission Rate metrics, further corroborated by\nour proposed Multimodal fact Capturing Metric(MMFCM)\nscore. Overall, ClipSyntel emerges as a clear frontrunner in\nhuman evaluation metrics, outperforming competing base-\nlines by a considerable margin.\nQualitative analysis: The Figure 5 provides an insightful\ncomparison of the summaries generated by two variations of\nthe GPT-3.5 model, namely the ”Only Question” and ”CLIP-\nSyntel” configurations, for medical inquiries. The analysis\nmainly focuses on two aspects: (1) CLIPSyntel Captures\nMedical Disorders Correctly: The CLIPSyntel variant demon-\nstrates a noteworthy ability to identify and represent medical\nconditions with precision. In both cases presented in the table,\nCLIPSyntel has shown an understanding of the underlying\nmedical issues. (2) CLIPSyntel Does Less Hallucinations:\nCLIPSyntel’s summaries stick to the information explicitly\nprovided in the medical questions. Unlike the ”Only Ques-\ntion” variant that might introduce slight changes or uncertain-\nties, CLIPSyntel maintains a strict adherence to the facts. This\nability to avoid unnecessary extrapolations or inaccuracies\nenhances CLIPSyntel’s reliability.\nRisk Analysis\nIt’s important to note several limitations in our approach. We\nrestricted our study to a zero-shot prompting strategy, not\nfully examining how prompt variations might affect results.\nSecond, although CLIP performs well in low-resource envi-\nronments, we must be cautious of potential misclassifications\nof medical images, as these could lead to serious or even\nAlgorithm 1: MMFCM Method\nRequire: Fm = {factm,1, factm,2, . . . ,factm,n−1, factm,n}\n{Relevant Medical facts from query ’m’.}\nRequire: Sfm ={Summfact m,1, . . . , Summfactm,n}\n{ Relevant Medical facts of summary of query ’m’.}\n#CorrectFacts m = |Fm ∩ Sfm|\n{Number of correct medical facts in each summary}\nif (Correct Medical Disorder phrase ∈ {Fm ∩ Sfm})\nthen\n#CorrectFacts m+ = 2\nelse if (Partially correct disorder phrase ∈ {Fm ∩ Sfm})\nthen\n#CorrectFacts m+ = 1\nelse if (Incorrect disorder phrase ∈ {Fm ∩ Sfm}) then\n#CorrectFacts m+ =−1\nelse\n#CorrectFacts m+ = 0\nend if\nreturn MMFCM = #CorrectFactsm\n|Fm∩Sfm|\nfatal misinformation in a summary generation. Despite the\npromising performance of CLIPSyntel in various scenarios,\nthe risk of misinformation in healthcare is significant. There-\nfore, involving a medical expert is vital to ensure that our\nAI model serves as an aid, not a replacement, for medical\nprofessionals.\nConclusion and Future Work\nIn this study, we delve into the impact of incorporating multi-\nmodal cues, particularly visual information, on question sum-\nmarization within the realm of healthcare. We present the\nMMQS dataset, comprising 3015 multimodal medical queries\nwith golden summaries that merge visual and textual data.\nThis novel collection fosters new assessment techniques\nin healthcare question summarization. We also introduce\nthe CLIPSyntel framework, leveraging LLMs and the CLIP\nmodel, to enhance summaries with visual symptom details.\nCLIP enables symptom classification, and an ImageBind-\nfiltering module mitigates content hallucination. In our future\nendeavors, we aspire to develop a Vision-Language model\ncapable of extracting the intensity and duration details of\nsymptoms and integrating them into the patient query’s fi-\nnal summary generation. Furthermore, our expansion plans\nencompass incorporating medical videos and addressing sce-\nnarios involving code-mixed patient queries.\nEthical Statement\nSummarization in healthcare necessitates strong ethical con-\nsiderations, particularly regarding safety, privacy, and poten-\ntial bias. To address these concerns in our project with the\nMMQS dataset, we implemented several proactive measures.\nWe collaborated closely with medical professionals and also\nobtained IRB approval to ensure ethical rigor and patient\nprivacy. We rigorously followed legal and ethical guidelines8\n8https://www.wma.net/what-we-do/medical-ethics/\ndeclaration-of-helsinki/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22037\nduring dataset validation, integration of images, and annota-\ntion of summaries. Medical experts were engaged throughout\nthe process, providing validation and correction of the dataset\nand also validating the outputs of the models. The proposed\ndataset is based on original HealthCareMagic Dataset; the\nmedical questions/samples are taken from this dataset. The\nincorporation of multimodality into the task is done under\nthe full supervision of a medical professional. Additionally,\nwe ensured user privacy by not disclosing identities.\nAcknowledgements\nAkash Ghosh and Sriparna Saha express their heartfelt grati-\ntude to the SERB (Science and Engineering Research Board)\nPOWER scheme (SPG/2021/003801) of the Department of\nScience and Engineering, Govt. of India, for providing the\nfunding for carrying out this research.\nReferences\nAbacha, A. B.; and Demner-Fushman, D. 2019. On the sum-\nmarization of consumer health questions. In Proceedings\nof the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 2228–2234.\nAbacha, A. B.; M’rabet, Y .; Zhang, Y .; Shivade, C.; Lan-\nglotz, C.; and Demner-Fushman, D. 2021. Overview of the\nMEDIQA 2021 shared task on summarization in the medical\ndomain. In Proceedings of the 20th Workshop on Biomedical\nLanguage Processing, 74–85.\nAbacha, A. B.; Yim, W.-w.; Michalopoulos, G.; and Lin, T.\n2023. An Investigation of Evaluation Metrics for Automated\nMedical Note Generation. arXiv preprint arXiv:2305.17364.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nDelbrouck, J.-B.; Zhang, C.; and Rubin, D. 2021. QIAI\nat MEDIQA 2021: Multimodal Radiology Report Summa-\nrization. In Proceedings of the 20th Workshop on Biomedi-\ncal Language Processing, 285–290. Online: Association for\nComputational Linguistics.\nDong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun,\nX.; Xu, J.; and Sui, Z. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nGirdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V .;\nJoulin, A.; and Misra, I. 2023. Imagebind: One embedding\nspace to bind them all. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n15180–15190.\nGupta, D.; Attal, K.; and Demner-Fushman, D. 2022. A\nDataset for Medical Instructional Video Classification and\nQuestion Answering. arXiv:2201.12888.\nKojima, T.; Gu, S.; Reid, M.; Matsuo, Y .; and Iwasawa, Y .\n2023. Large language models are zero-shot reasoners. arXiv.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mohamed,\nA.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L. 2019. Bart:\nDenoising sequence-to-sequence pre-training for natural lan-\nguage generation, translation, and comprehension. arXiv\npreprint arXiv:1910.13461.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, G.; Liao, Y .; Wang, F.; Zhang, B.; Zhang, L.; Liang,\nX.; Wan, X.; Li, S.; Li, Z.; Zhang, S.; et al. 2021. Medical-\nvlbert: Medical visual language bert for covid-19 ct report\ngeneration with alternate learning. IEEE Transactions on\nNeural Networks and Learning Systems, 32(9): 3786–3797.\nMrini, K.; Dernoncourt, F.; Chang, W.; Farcas, E.; and Nakas-\nhole, N. 2021. Joint summarization-entailment optimization\nfor consumer health question understanding. In Proceedings\nof the Second Workshop on Natural Language Processing for\nMedical Conversations, 58–65.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nQi, W.; Yan, Y .; Gong, Y .; Liu, D.; Duan, N.; Chen, J.; Zhang,\nR.; and Zhou, M. 2020. Prophetnet: Predicting future n-\ngram for sequence-to-sequence pre-training. arXiv preprint\narXiv:2001.04063.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal, H.;\nAnwer, R. M.; Khan, S.; Laaksonen, J.; and Khan, F. S. 2023.\nXraygpt: Chest radiographs summarization using medical\nvision-language models. arXiv preprint arXiv:2306.07971.\nTiwari, A.; Manthena, M.; Saha, S.; Bhattacharyya, P.; Dhar,\nM.; and Tiwari, S. 2022. Dr. can see: towards a multi-modal\ndisease diagnosis virtual assistant. In Proceedings of the 31st\nACM international conference on information & knowledge\nmanagement, 1935–1944.\nYadav, S.; Gupta, D.; Abacha, A. B.; and Demner-Fushman,\nD. 2021. Reinforcement learning for abstractive question\nsummarization with question-aware semantic rewards. arXiv\npreprint arXiv:2107.00176.\nZhang, J.; Huang, J.; Jin, S.; and Lu, S. 2023. Vision-\nlanguage models for vision tasks: A survey. arXiv preprint\narXiv:2304.00685.\nZhang, J.; Zhao, Y .; Saleh, M.; and Liu, P. 2020. Pega-\nsus: Pre-training with extracted gap-sentences for abstractive\nsummarization. In International Conference on Machine\nLearning, 11328–11339. PMLR.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2019. Bertscore: Evaluating text generation with bert.\narXiv preprint arXiv:1904.09675.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023.\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv preprint arXiv:2306.05685.\nZhou, J.; He, X.; Sun, L.; Xu, J.; Chen, X.; Chu, Y .; Zhou,\nL.; Liao, X.; Zhang, B.; and Gao, X. 2023. SkinGPT-4:\nAn Interactive Dermatology Diagnostic System with Visual\nLarge Language Model.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22038\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22039",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8436422348022461
    },
    {
      "name": "Health care",
      "score": 0.5703338980674744
    },
    {
      "name": "Computer science",
      "score": 0.40786027908325195
    },
    {
      "name": "Artificial intelligence",
      "score": 0.22038114070892334
    },
    {
      "name": "Political science",
      "score": 0.17812702059745789
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}