{
  "title": "Development and prospective implementation of a large language model based system for early sepsis prediction",
  "url": "https://openalex.org/W4410462063",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2659836236",
      "name": "Supreeth P. Shashikumar",
      "affiliations": [
        "University of California, San Diego",
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2160595478",
      "name": "Sina Mohammadi",
      "affiliations": [
        "University of California, San Diego",
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A3097222891",
      "name": "Rishivardhan Krishnamoorthy",
      "affiliations": [
        "University of California, San Diego",
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2101701729",
      "name": "Avi Patel",
      "affiliations": [
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2289747926",
      "name": "Gabriel Wardi",
      "affiliations": [
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2985771278",
      "name": "Joseph C. Ahn",
      "affiliations": [
        "University of California, San Diego",
        "Mayo Clinic",
        "UC San Diego Health System",
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2116478967",
      "name": "Karandeep Singh",
      "affiliations": [
        "University of California, San Diego",
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2750988365",
      "name": "Eliah Aronoff-Spencer",
      "affiliations": [
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2136241170",
      "name": "Shamim Nemati",
      "affiliations": [
        "University of California, San Diego",
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2659836236",
      "name": "Supreeth P. Shashikumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160595478",
      "name": "Sina Mohammadi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3097222891",
      "name": "Rishivardhan Krishnamoorthy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101701729",
      "name": "Avi Patel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2289747926",
      "name": "Gabriel Wardi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2985771278",
      "name": "Joseph C. Ahn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116478967",
      "name": "Karandeep Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750988365",
      "name": "Eliah Aronoff-Spencer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136241170",
      "name": "Shamim Nemati",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998853022",
    "https://openalex.org/W2280404143",
    "https://openalex.org/W2755626276",
    "https://openalex.org/W1993397663",
    "https://openalex.org/W2073316020",
    "https://openalex.org/W2599245417",
    "https://openalex.org/W2912904466",
    "https://openalex.org/W3016555942",
    "https://openalex.org/W2137187166",
    "https://openalex.org/W2553610958",
    "https://openalex.org/W4386307515",
    "https://openalex.org/W4286489697",
    "https://openalex.org/W2766207659",
    "https://openalex.org/W4391168223",
    "https://openalex.org/W2980131624",
    "https://openalex.org/W4391690583",
    "https://openalex.org/W3124926562",
    "https://openalex.org/W3133887173",
    "https://openalex.org/W4200334578",
    "https://openalex.org/W2774099121",
    "https://openalex.org/W3185017517",
    "https://openalex.org/W2604972438",
    "https://openalex.org/W3138309625",
    "https://openalex.org/W2963669292",
    "https://openalex.org/W3021738738",
    "https://openalex.org/W4286567174",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4390745503",
    "https://openalex.org/W4391098193",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4384561181",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4394807113",
    "https://openalex.org/W4367047646",
    "https://openalex.org/W3174786846",
    "https://openalex.org/W4391598924",
    "https://openalex.org/W4286491015",
    "https://openalex.org/W4245062343",
    "https://openalex.org/W1983113859",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4392193191",
    "https://openalex.org/W4403229099",
    "https://openalex.org/W2099376575",
    "https://openalex.org/W4284964177",
    "https://openalex.org/W3004443627",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W6764214684",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W2945443402",
    "https://openalex.org/W2282181907",
    "https://openalex.org/W3046761449",
    "https://openalex.org/W4226066902",
    "https://openalex.org/W3196930108",
    "https://openalex.org/W6860710830",
    "https://openalex.org/W4385570594"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01689-w\nDevelopment and prospective\nimplementation of a large language model\nbased system for early sepsis prediction\nCheck for updates\nSupreeth P. Shashikumar1,S i n aM o h a m m a d i1, Rishivardhan Krishnamoorthy1, Avi Patel 2,\nGabriel Wardi 2,3,J o s e p hC .A h n1,4, Karandeep Singh1,5, Eliah Aronoff-Spencer6,7 &\nShamim Nemati 1,7\nSepsis is a dysregulated host response to infection with high mortality and morbidity. Early detection\nand intervention have been shown to improve patient outcomes, but existing computational models\nrelying on structured electronic health record data often miss contextual information from\nunstructured clinical notes. This study introduces COMPOSER-LLM, an open-source large language\nmodel (LLM) integrated with the COMPOSER model to enhance early sepsis prediction. For high-\nuncertainty predictions, the LLM extracts additional context to assess sepsis-mimics, improving\naccuracy. Evaluated on 2500 patient encounters, COMPOSER-LLM achieved a sensitivity of 72.1%,\npositive predictive value of 52.9%, F-1 score of 61.0%, and 0.0087 false alarms per patient hour,\noutperforming the standalone COMPOSER model. Prospective validation yielded similar results.\nManual chart review found 62% of false positives had bacterial infections, demonstrating potential\nclinical utility. Ourﬁndings suggest that integrating LLMs with traditional models can enhance\npredictive performance by leveraging unstructured data, representing a signiﬁcant advance in\nhealthcare analytics.\nSepsis is a dysregulated host response toinfection that is estimated to affect\nover 48 million adults worldwide every year, resulting in approximately 11\nmillion deaths\n1,2. In the United States alone, one in three hospital deaths is\nattributable to sepsis3. Studies have shown that early recognition of sepsis\nfollowed by timely interventions such as appropriate antibiotic adminis-\ntration,ﬂuid resuscitation, and source control can lead to improved patient\noutcomes4– 10. The high mortality rate and substantial economic burden of\nsepsis, estimated at over $32,000 per patient in high-income countries11,\nunderscore the urgent need for effective early detection and intervention\nstrategies. Consequently, extensive efforts have been made to develop tools\nfor the early prediction of sepsis using electronic health record (EHR) data\n12.\nAdditionally, a limited number of studies have demonstrated improvements\nin patient outcomes following the real-world deployment of sepsis predic-\ntion models13– 15.\nAlthough EHRs contain both structured data (vital signs, laboratory\nresults, comorbidities, etc.) and unstructured data (triage notes, progress notes,\nradiology reports, etc.), most sepsis prediction models have primarily utilized\nstructured data12. Unstructured clinical data, including triage notes, provider\nnotes, and progress notes, often contain valuable contextual information that\nstructured data cannot capture. For instance, summaries of clinical signs and\nsymptoms at hospital presentation, initial physical exams, and interpretations\nof radiological exams can be crucial fordiagnosing or ruling out sepsis. Hence,\nincorporating unstructured data could enhance the performance of sepsis\nprediction models\n16– 20. A common approach to using unstructured data has\nbeen obtaining document-level representations (i.e., embeddings) and using\nthem as inputs to the sepsis prediction model alongside structured data. These\ndocument-level representations can beobtained using pre-trained language\nmodels (such as ClinicalBERT)\n16,17,19,21,22,t o p i cm o d e l i n gt echniques (such as\nLDA)18,23, or term frequency-inverse document frequency (tf-idf) embeddings\nbased on clinical entities extracted from clinical notes18,19,22,24. However, these\napproaches have several limitations25,26: 1) they capture information at a\ndocument level (or global level) and may not effectively capture local con-\ntextual information relevant to sepsis; 2) they offer limited interpretability; 3)\nmay overly emphasize repeated information due to redundant text from copy-\n1Division of Biomedical Informatics, UC San Diego, San Diego, CA, USA.2Department of Emergency Medicine, UC San Diego, San Diego, CA, USA.3Division of\nPulmonary, Critical Care and Sleep Medicine, UC San Diego, San Diego, CA, USA.4Division of Gastroenterology and Hepatology, Mayo Clinic, Rochester, NY,\nUSA. 5Jacobs Center for Health Innovation, UC San Diego Health, San Diego, CA, USA.6Division of Infectious Diseases and Global Public Health, UC San Diego,\nSan Diego, CA, USA.7These authors jointly supervised this work: Eliah Aronoff-Spencer, Shamim Nemati.e-mail: snemati@health.ucsd.edu\nnpj Digital Medicine|           (2025) 8:290 1\n1234567890():,;\n1234567890():,;\nand-paste practices27; and 4) they are prone to domain shift, as such\napproaches may capture hospital-speciﬁc data entry patterns. These limita-\ntions could potentially be addressed with the use of large language models\n(LLMs) and techniques such as retrieval augmented generation (RAG)28.\nLLMs are foundation models built with billions or trillions of para-\nmeters, trained on extensive text data to recognize complex language\npatterns\n29. Recently, LLMs have garnered signiﬁcant attention in theﬁeld of\nmedicine for their capability to use natural language to generate coherent,\ncontextually relevant responses. Initial investigations have demonstrated\nsuccessful applications of LLMs in identifying social determinants of\nhealth30, assigning diagnosis-related groups 31, answering medical\nquestions32, drafting clinical notes33, and drafting clinician responses to\npatient inquiries34,35. Of note, LLMs have proven effective in retrieving and\nprocessing relevant information from multiple documents36.\nThe goal of this study was to develop and prospectively validate a mul-\ntimodal system (COMPOSER-LLM) that combines LLM-based processing of\nunstructured clinical notes with structured data from the EHR to enhance\nmodel accuracy in challenging diagnostic scenarios. We hypothesized that\nCOMPOSER-LLM’s incorporation of clinical notes for differential diagnosis\nof sepsis-mimics would outperform our previous binary sepsis prediction\nm o d e l( C O M P O S E R )t h a tr e l i e ds o l e l yo ns t r u c t u r e dE H Rd a t a .P l e a s es e et h e\nsubsection“Differential diagnosis tool” in Methods for a list of all the sepsis-\nmimics. The entire pipeline was hosted on a cloud-based healthcare analytics\nplatform that used Fast Healthcare Interoperability Resources (FHIR) and\nHealth Level Seven International Version 2 (HL7v2) standards to provide real-\ntime access to data elements. This allowed us to evaluate the model’sp e r -\nformance in a real-world setting, whereclinical notes may be incomplete. The\ns c h e m a t i cd i a g r a mo ft h ee n t i r eC O M P O S E R - L L Mp i p e l i n ei ss h o w ni nF i g .1.\nPlease note that throughout this paper, COMPOSER-LLM and COMPOSER-\nLLM\nDDx refer to the same pipeline, unless otherwise noted.\nResults\nStudy population\nDuring the study period for retrospective model development (September 1,\n2023 through December 31, 2023), a total of 1746 emergency department\n(ED) encounters (16.6% septic) met the inclusion criteria (retrospective\ncohort) for this study. Separately, the study period for prospective validation\nof the model included May 1, 2024 through June 15, 2024. During this\nFig. 1 | Schematic Diagram of the COMPOSER-\nLLM pipeline. aThe entire COMPOSER-LLM\npipeline was hosted on a cloud-based healthcare\nanalytics platform that used FHIR and HL7v2\nstandards to provide real-time access to data ele-\nments. The input feature set (including laboratory\nmeasurements, vitals measurements, comorbidities,\nmedications and demographics) was queried using\nFHIR APIs and passed to the COMPOSER model.\nFor samples with a sepsis risk score greater than a\nprimary decision threshold (θ\n1), an alarm wasﬁred\nbased on the standalone composer risk score.\nHowever, for samples with a sepsis risk score falling\nwithin a high uncertainty score region (θ2≤ sepsis\nrisk scores <θ1), the LLM-based differential diag-\nnosis tool was utilized to improve diagnostic cer-\ntainty. The high uncertainty score region is shown as\nthe shaded region in the insetﬁgure. Finally, if\n‘Severe Sepsis’ was present within the top-5 sorted\ndifferentials and the LLM identiﬁed a‘suspicion of\nbacterial infection’, an alarm wasﬁred. b The dif-\nferential diagnosis tool employed an LLM to extract\nsevere sepsis and sepsis-mimic (or differentials)\nrelated clinical signs and symptoms from the clinical\nnotes. These identiﬁed symptoms were then fed into\nlikelihood calculators to determine the likelihood of\nvarious differentials. The resulting differential\ndiagnosis list was sorted by the likelihood scores.\nc The LLM-based clinical sign or symptom extractor\nwas designed to take a prompt (containing the\nclinical sign or symptom to be queried) and clinical\nnotes (all notes generated from admission to the\nprediction time) as input and produce output in a\nJSON format.\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 2\nperiod, a total of 754 ED encounters (18.4% septic) met the inclusion criteria\nfor this study (prospective cohort).T a b l e1 shows baseline characteristics and\nsummary characteristics for theretrospectiveand prospectivecohorts.\nPerformance of composer-LLMDDx pipeline\nThe standalone COMPOSER model achieved a sensitivity of 72.9%, a\npositive predictive value (PPV) of 22.6%, F-1 score of 34.5%, and false\nalarms per patient hour (FAPH) of 0.037 (e.g., 1.48 false alarms every 2 h in a\n20-bed care unit) on the retrospective validation cohort. When the LLM-\nbased differential diagnosis tool was applied to COMPOSER risk scores in\nthe range of 0.5– 0.75 (θ\n2 =0 . 5 ,θ1 = 0.75), COMPOSER-LLMDDx demon-\nstrated improved performance witha sensitivity of 72.1%, a PPV of 52.9%,\nF-1 score of 61.0%, and FAPH of 0.0087 (e.g., 0.348 false alarms every 2 h in\na 20-bed care unit) on the same cohort. In comparison, when the LLM-\nbased sepsis likelihood tool alone was applied to COMPOSER risk scores in\nthe range of 0.5– 0.75 (θ\n2 =0 . 5 ,θ1 = 0.75), COMPOSER-LLMSLT demon-\nstrated a lower performance with a sensitivity of 72.1%, a PPV of 31.9%, F-1\nscore of 44.2%, and FAPH of 0.021 (e.g., 0.84 false alarms every 2 h in a 20-\nbed care unit) on the same cohort. Performance metrics of all models on the\nretrospective development cohort are detailed in Supplementary Table 1.\nNote that, for COMPOSER risk scores greater than the primary decision\nthreshold (θ\n1), an alert wasﬁred irrespective of the output from LLM. The\nperformance of the COMPOSER-LLMDDx pipeline for running the LLM-\nbased sepsis likelihood tool for various risk score intervals is shown in\nSupplementary Table 3. Additionally, outputs generated by the LLM (for\nclinical signs or symptoms related to severe sepsis) for a sample patient are\ns h o w ni nS u p p l e m e n t a r yT a b l e2 .\nFinally, COMPOSER-LLM\nDDx demonstrated superior performance\ncompared to the COMPOSER-LLMbaseline model (detailed in the Methods\nsection under“Baseline model comparison”) which achieved a sensitivity of\n34.4%, PPV of 35.1% and F1-score of 34.7% on the retrospective validation\ncohort. A comparison of performance of all the models considered in this\nstudy is shown in Table2.\nIn an additional exploratory analysis, we measured the proportion of\nthe false positives (produced by COMPOSER-LLMDDx) that contained the\nactual patient diagnosis within the Top-5 differentials produced by the\ndifferential diagnosis tool. It was observed that for 83.1% of the false positive\npatients, the actual patient diagnosis was contained in the predicted dif-\nferential diagnosis list.\nProspective performance of COMPOSER-LLM\nThe COMPOSER-LLM pipeline was prospectively deployed across two EDs\nwithin the UCSD Health system starting on May 1, 2024 (see Fig.2 for an\noverview of the real-time deployment pipeline). Theprospective cohort\nanalyzed in this study included patients admitted to the ED between May 1,\n2024, and June 15, 2024. In theprospective cohort, the COMPOSER-\nLLM\nDDx pipeline (withθ2 =0 . 5 ,θ1 = 0.75) achieved a sensitivity of 70.8%, a\nPPV of 58.2%, F-1 score of 63.9% and FAPH of 0.0086 (e.g., 0.344 false\nTable 1 | Patient characteristics of theretrospective and prospective cohorts\nProspective cohort Retrospective cohort\nSeptic Non-Septic Septic Non-Septic\n# Encounters (%) 290 (16.6%) 1456 139 (18.4%) 615\nAge (in years), median [IQR] 64.5 [51.4 –75.1] 59.1 [44.2 –71.1]* 65.1 [50.1–75.8] 58.1 [43.3 –69.6]*\nGender (Male), % 54.10% 51.50% 58.20% 47.70%\nRace\nWhite, % 45.10% 44.40% 50.40% 45.60%\nAfrican American, % 10% 9.40% 7.20% 7.90%\nAsian, % 7.20% 6.80% 4.30% 6.40%\nED Length of Stay (in hours), median [IQR] 24.9 [11.2 –50.6] 11.3 [6.5 –28.3]* 22.1 [10.5–45.8] 11.9 [6.9 –30.8]*\nCCI, median [IQR] 2 [0 – 4] 1 [0 – 3] 2 [0 –5] 1 [0 –2]\nSOFA, median [IQR] 3 [1 –5] 1 [0 –2]* 3[ 1–5] 1 [0 –2]*\nIn-hospital mortality, % 8.90% 1.5% * 8.10% 1.2% *\nTime from ED triage to onset of sepsis (in hours), median [IQR] 3.2 [1.6 –7.5] N/A 2.3 [1.2 –10.2] N/A\nCCI Charlson Comorbidity Index,SOFA Sequential Organ Failure Assessment score.\n*p-value <0.05.\nTable 2 | Comparison of model performance\nComposera Composer-LLMSLT\nb\n(with sepsis likelihood tool)\nComposer-LLMDDx\nc\n(with differential diagnosis tool)\nRetrospective validation cohort Sensitivity 72.9% 72.1% 72.1%\nPPV 22.6% 31.9% 52.9%\nF1-Score 34.5% 44.2% 61.0%\nFAPH 0.037 0.021 0.0087\nProspective cohort Sensitivity 70.8% 70.5% 70.8%\nPPV 25.1% 36.3% 58.2%\nF1-Score 37.1% 47.9% 63.9%\nFAPH 0.034 0.020 0.0086\nPPV positive predictive value,FAPH false alarms per patient hour.\naDecision threshold of COMPOSER was set to 0.6 as previously described in boussina et al.\nbLLM-based sepsis likelihood tool was applied to COMPOSER risk scores in the range of 0.5 to 0.75 (θ2 = 0.5, θ1 = 0.75).\ncLLM-based differential diagnosis tool was applied to COMPOSER risk scores in the range of 0.5 to 0.75 (θ2 = 0.5, θ1 = 0.75).\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 3\nalarms every 2 h in a 20-bed care unit). The performance in the prospective\ncohort was found to be similar to that of the retrospective validation cohort,\nwhich had a sensitivity of 72.1%, a PPV of 52.9%, F-1 score of 61.0%, and\nFAPH of 0.0087 (e.g., 0.348 false alarms every 2 h in a 20-bed care unit).\nIn an additional exploratory analysis, we calculated the proportion of\nthe false positives (produced by COMPOSER-LLM\nDDx) that contained the\nactual patient diagnosis within the Top-5 differentials produced by the\ndifferential diagnosis tool. It was observed that for 73.2% of the false positive\npatients, the actual patient diagnosis was contained in the predicted dif-\nferential diagnosis list.\nClinician chart review of false-positives\nChart review of 50 false-positive patients were performed by two clinicians.\nThey were asked to examine each patient’s EHR record and determine if\nthere was a suspicion of bacterial infection at the time the COMPOSER-\nLLM alarm was triggered. Clinician 1 identiﬁed 31 out of 50 patients (62%)\nas having a suspicion of bacterial infection, while Clinician 2 identiﬁed 32\nout of 50 patients (64%). Both clinicians agreed on 31 out of the 50 false-\npositive patients (62%) having a suspicion of bacterial infection. The chart\nreview indicated that the majority ofpatients in the false-positive group\nwould beneﬁt from the COMPOSER-LLM alert.\nPerformance of COMPOSER-LLM for patients with clinical sus-\npicion of infection\nThe goal of this sub-analysis was toexplore the potential of utilizing\nCOMPOSER-LLMDDx as a digital sepsis biomarker. Much of the current\nsepsis biomarkers37 are designed to be used for patients with a suspicion of\nclinical infection. Replicating a similar condition for use for COMPOSER-\nLLMDDx, we evaluated its performance for patients with a clinical suspicion\nof infection (deﬁned by the clinical decision to order blood culture and\nantibiotics within a 6-hour window) at the time of prediction. Under the\nabove condition for use, the standalone COMPOSER achieved a sensitivity\nof 75%, a PPV of 55.4% and F-1 score of 63.7% whereas COMPOSER-\nLLM\nDDx achieved a higher performance with a sensitivity of 74.5%, a PPV of\n80.1% and F-1 score of 77.2% on the retrospective validation cohort.\nA similar pattern was observed on the prospective evaluation cohort,\nwith the standalone COMPOSER achieving a sensitivity of 70.9%, a PPV of\n58.4% and F-1 score of 64.1% and COMPOSER-LLMDDx achieving a higher\nperformance with a sensitivity of 74.4%, a PPV of 81.3% and F-1 score of\n77.7% on the retrospective validation cohort.\nDiscussion\nWe introduce a novel approach for accurate early prediction of sepsis,\nleveraging a locally-deployed LLM to analyze unstructured clinical notes in\nreal-time to assess the likelihood of sepsis and sepsis-mimics. Ourﬁndings\nare important because they demonstrate LLMs can enhance the perfor-\nmance of existing clinical predictive models, particularly in high-uncertainty\nprediction scenarios, by incorporating and analyzing the rich clinical con-\ntext found within unstructured medical notes. The entire pipeline was\ndeployed on a cloud-based healthcare analytics platform to ensure scal-\nability and portability, utilizing FHIR and HL7v2 standards for interoper-\nability. The LLM-based differential diagnosis tool was employed only for\npatients with COMPOSER risk scores within a high uncertainty interval\n(near the model decision threshold). This approach enhanced prediction\nFig. 2 | Schematic diagram of the COMPOSER-LLM real-time deployment\npipeline. The real-time platform extracts data at an hourly resolution of all active\npatients using FHIR APIs, and passes the input feature set (consisting of laboratory\nmeasurements, vitals measurements, comorbidities, medications, clinical notes, and\ndemographics) to the COMPOSER-LLM inference engine. The sepsis risk scores\ngenerated by the COMPOSER-LLM pipeline are then written back to the EHR (as a\nﬂowsheet item) through an HL7 device data interface. Theﬂowsheet then triggers a\nnurse-facing Best Practice Advisory that alerts the caregiver that the patient is at risk\nof developing severe sepsis. HL7 health level 7, AWS Amazon Web Services, FHIR\nFast Healthcare Interoperability Resources, RDS relational database service, EC2\nElastic Compute Cloud.\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 4\ncertainty by reducing false alarms while avoiding the continuous compu-\ntational cost of running an LLM. When prospectively evaluated, the\nCOMPOSER-LLM pipeline showed similar performance to the retro-\nspective validation cohort; demonstrating the utility of the LLM-augmented\nsystem in the context of potentially incomplete clinical notes.\nRecent work has highlighted the potential of artiﬁcial intelligence (AI)\nmodels for predicting sepsis using EHR data. These models, including\nneural networks, can process complex, high-dimensional data and capture\nnonlinear interactions between variables, demonstrating promising results\nin early sepsis detection. A systematic review by Islam et al. identiﬁed at least\n42 studies that developed machine learning (ML) models using vital signs,\nlaboratory data, and demographic information\n12. Many of these studies have\nprimarily relied on structured EHR data, overlooking the rich information\ncontained in unstructured clinical notes which may augment predictive\nabilities\n38. Importantly, recent data have highlighted suboptimal perfor-\nmance of commercially available sepsis predictive models that rely on\nstructured data, such as the Epic Sepsis score39,40 (sensitivity of 33% and PPV\nof 12%), and TREWS41 (sensitivity of 71% and PPV of 16.7% for patients not\nalready on antibiotics). In comparison, COMPOSER-LLM demonstrated\nsuperior performance with a sensitivity of 72.1% and PPV of 52.9%.\nIn this study, we examined the effectiveness of LLMs as information\nretrieval tools rather than diagnostictools, followed by probabilistic rea-\nsoning to aid in sepsis prediction under high-uncertainty scenarios. This\nintegrated strategy aligns with the historical evolution of AI in medicine,\nwhere early rule-based sy stems like INTERNIST-I\n42 and MYCIN43\nattempted to combine knowledge representation and reasoning for differ-\nential diagnosis. When the LLM was used to identify sepsis based solely on\nclinical notes (referred to as COMPOSER-LLMbaseline), its performance was\nlower compared to the COMPOSER-LLM pipeline. This aligns with pre-\nvious studies that have also noted the difﬁculties of using standalone LLMs\nfor diagnostic purposes\n44– 46, and highlights the importance of combining the\nstrengths of predictive modeling (for initial risk stratiﬁcation) and differ-\nential diagnosis (for addressing prediction uncertainty). Furthermore,\nincorporating differential diagnosis into predictive modeling can enhance\npatient safety by mitigating anchoring bias associated with false alarms\nfocused on a single diagnosis. By systematically considering alternative\ndiagnoses, clinicians can avoid premature closure and improve diagnostic\naccuracy. Similarly, in scenarios with high prediction uncertainty, differ-\nential diagnosis can mitigate the potential harm of automation bias (missed\ndetections) by improving model sensitivity via a ranked list of potential\ndiagnoses (top-k).\nFalse alarms can often lead to alertfatigue, increase of physician’s\ncognitive burden, and can result in exposure of patients to unnecessary\ntreatments\n47. Although in recent years adoption of Sepsis-3 consensus cri-\nteria for tagging patients for sepsis has resulted in signiﬁcant progress in\nsepsis epidemiological studies and algorithm development48,s u c hs i l v e r -\nstandard methods have limited sensitivity and speciﬁcity49 and often further\nchart-review is warranted. In this study, two study authors performed an in\ndepth analysis of a sample of 50 false-positive patients and were asked to\nidentify if there was a suspicion of bacterial infection at the time of the alert.\nThe review, which had signiﬁcant agreement, revealed that 62% of the false\npositives had bacterial infections whenthe alert was triggered, indicating the\npotential clinical utility of the alerteven for patients who did not progress to\nsepsis. Thus, even if sepsis was not present, a concern for a bacterial infection\nwould likely have beneﬁted from provider evaluation and intravenous\nantibiotics, which may have prevented development of sepsis.\nSome of the notable features of the COMPOSER-LLM system\ninclude: 1) The use of the open-source Mixtral 8x7B Large Language\nModel in this study allowed us to deploy the entire COMPOSER-LLM\npipeline within UCSD’s Health Insurance Portability and Account-\nability Act (HIPAA) compliant cloud environment. This deployment\nensured that patient data remained securely within the hospital ’s\nﬁrewall, maintaining the protection of UCSD patient information; 2)\nThe cloud-based real-time platform for COMPOSER-LLM was\ndesigned to be both scalable andﬂexible, enabling it to support real-\ntime inference for models of any size. Consequently, the Mixtral 8x7B\nmodel, comprising 45 billion parameters, was used for real-time\ninference without encountering any computational bottlenecks; 3)\nThe LLM-based clinical sign or symptom extractor was designed to\nprovide explanation for each of t he clinical signs or symptoms\nextracted, enhancing transparency and explainability for the clinical\nend-user. This provided an advantage over traditional NLP-based\ninformation retrieval techniques, which typically lack interpretability;\n4) The decision to run the LLM-based differential diagnosis tool only\nfor samples with scores between 0.5 and 0.75 enabled us to sig-\nniﬁcantly save on computational costs as the multi-GPU instance\nhosting the LLM was intermittently switched on/off as and when\nrequired (on average 10 calls to the LLM microservice/day).\nThis work has several limitations. The differential diagnosis tool is\ncurrently triggered only after the availability of certain clinical notes (“ED\nprovider note”, “History and Physical note”, “Diagnostic image report” or\n“Progress note”), potentially delaying alert generation among patients in the\nuncertainty interval of 0.5– 0.75. However, when COMPOSER-LLM was\ndeployed prospectively, the differential diagnosis tool was triggered even if a\nnote was incomplete, as the contextual information within the notes could\nstill be useful. Future research could explore using LLM-based queries for\nkey patient and provider information (e.g., suspicion of infection) and real-\ntime capture of provider notes through speech recognition and transcription\nto address missing or incomplete notes. The current implementation of\nCOMPOSER-LLM leverages a pre-trained open-source LLM for informa-\ntion retrieval followed by a Bayesianinference approach to differential\ndiagnosis. Future work includes modelﬁne-tuning, addressing model\ngeneralizability\n50– 52, and exploring the use of more powerful and potentially\ndomain-speciﬁc LLMs for end-to-end differential diagnosis.\nIn conclusion, ourﬁndings underscore the potential of LLMs in aug-\nmenting traditional prediction models for sepsis, particularly in high-\nuncertainty prediction scenarios. Byeffectively processing and analyzing\nunstructured clinical narratives,COMPOSER-LLM offers a promising\nadvancement in predictive healthcare analytics. Future prospective studies\nare needed to assess how this approach impacts patient care and outcomes.\nFurthermore, LLM-based disease likelihood tools could help derive differ-\nential diagnoses\n53, improving the recognition of other life-threatening\nconditions that resemble sepsis.\nMethods\nStudy cohort\nDeidentiﬁed data from the EHR (using FHIR and HL7v2 standards) of\npatient encounters admitted to the Emergency Department (ED) within two\nUniversity of California San Diego (UCSD) Health hospitals was used in this\nstudy. Patients aged 18 years or olderwere monitored throughout their stay\nuntil they experienced theirﬁrst episode of sepsis, transitioned to comfort\ncare measures, or were transferred out of the Emergency Department,\nwhichever occurred earlier. To ensure a sufﬁcient quantity of predictor data,\nwe focused on sequential hourly predictions of sepsis starting two hours\nafter ED triage. While the decision threshold of the previously described\nCOMPOSER model\n15 was set to 0.6, the COMPOSER-LLM adopted a lower\ndecision threshold of 0.5 to improve sensitivity. To address the potential\nincrease in false alarm rate, the proposed model leveraged additional con-\ntextual information from clinical notes for the high-uncertainty predictions\nwithin the 0.5– 0.75 range. To evaluate the effects of adjusting the prediction\nmodel to be more sensitive by loweringits decision threshold, and to explore\nthe beneﬁts of using an LLM to handle uncertain predictions, all patients\nwith at least one COMPOSER risk score exceeding 0.5 were included for\nfurther analysis. Patients identiﬁed as having sepsis before the prediction\nstart-time or those without heart rate or blood pressure measurements prior\nto this time were excluded. Predictionsw e r em a d ei ft h ef o l l o w i n gc r i t e r i a\nwere met: 1) At least one vital and lab measurement in the past 24 h; 2) No\nantibiotics received; and 3) Availability of“ED provider note”, “History and\nPhysical note”, “Diagnostic Image report” or “Progress note”.T h eH i s t o r y\nand Physical (H&P) note is usually completed during a patient’s initial visit,\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 5\nand includes the patient’s medical history, family history, social history, and\na review of systems. It also documents the physical examﬁndings and an\ninitial assessment and plan. The ED provider note is created during an\nemergency room visit, and typically includes detailed documentation to\ncapture the patient’s initial presentation, evaluation, treatment, and dis-\ncharge or admission plan. Progress notes are daily or periodic updates\ndocumenting a patient’s progress, often using the SOAP format (Subjective,\nObjective, Assessment, Plan). They track changes in the patient’s condition,\nresponses to treatment, and any newﬁndings. A Diagnostic Image report\ncontains summaries of imaging studies, like X-rays, CT scans, and MRIs,\ndetailing theﬁndings and the radiologist’s interpretation.\nT h eo v e r a l lr e t r o s p e c t i v edataset was divided into adevelopment cohort\nand a validation cohort.T h eretrospective development cohortincluded\npatient encounters from September 2023, while theretrospective validation\ncohortcomprised encounters from October to December 2023. Additionally,\nthe COMPOSER-LLM pipeline was prospectively deployed in silent mode\nfor real-time sepsis prediction in the two EDs within the UCSD Health\nsystem starting May 1, 2024. The prospective data collected during the time\nperiod of May 1 - June 15 2024 will be referred to asprospective cohort.\nPatients were identiﬁed as having sepsis according to the Sepsis-3\ninternational consensus deﬁnition for sepsis\n2,54. The onset time of sepsis was\nestablished by following previously published methodology, using evidence\nof organ dysfunction and suspicion of clinical infection15,55,56.C l i n i c a ls u s -\npicion of infection was deﬁned by a blood culture draw and at least 4 days of\nnon-prophylactic intravenous antibiotic therapy satisfying either of the\nfollowing conditions: (1) if a blood culture draw was orderedﬁrst, then an\nantibiotic order had to occur within the following 72 h, or (2) if an antibiotic\norder occurredﬁrst, then a blood culture draw had to occur within the next\n24 h. Evidence of organ dysfunction was deﬁned as an increase in the\nSequential Organ Failure Assessment (SOFA) score by two or more points.\nIn particular, evidence of organ dysfunction occurring 48 h before to 24 h\nafter the time of suspected infection was considered, as suggested in Sey-\nmour et al.\n54. This investigation was conducted according to University of\nCalifornia San Diego IRB approved protocol #805726 with a waiver of\ninformed consent.\nCOMPOSER model\nCOMPOSER is a previously published model that was trained to predict the\nonset of sepsis up to four hours in advance57.I tw a ss h o w nt h a tt h e\ndeployment of COMPOSER at two EDs within the UCSD Health system\nwas associated with a 17% relative reduction in in-hospital sepsis mortality\nas part of a quality improvement initiative in our EDs\n15. This deep neural\nnetwork model integrates routinely collected laboratory and vital signs,\nalong with patient demographics (age and sex), comorbidities, and medi-\ncations, to generate a risk score for sepsis onset within the next four hours.\nAdditionally, the model utilizes a conformal prediction module to reject out-\nof-distribution samples that may arise due to data entry errors or\nunfamiliar cases.\nDifferential diagnosis tool\nFor samples with COMPOSER risk scores within a high uncertainty\ninterval, an LLM-based differential diagnosis tool was employed to enhance\ndiagnostic certainty. This high uncertainty interval was characterized by a\nlow and nearly constant positive predictive value (see Supplementary Fig. 1).\nIn this study, differential diagnoses were conducted for a total of 19 con-\nditions, including severe sepsis and sepsis-mimics (i.e., cardiogenic shock,\npulmonary embolism, hypovolemia, hypovolemic shock, cirrhosis, heart\nfailure exacerbation, alcohol/drug withdrawal, gastrointestinal hemorrhage,\nanaphylaxis, acute respiratory distress syndrome, inﬂuenza, covid-19,\nmyocardial infarction, coronary artery disease, malignancy, pancreatitis,\nheat stroke and diabetic ketoacidosis). The tool aimed to increase diagnostic\ncertainty by conﬁrming the presence of clinical signs and symptoms related\nto severe sepsis and its mimics in the clinical notes. It used an LLM toﬁrst\nextract these relevant signs or symptoms, which were then analyzed using\nlikelihood calculators to estimate the probability of each differential. The\ngenerated list, ranked by likelihood, was then used downstream to deter-\nmine if an alert was to beﬁred or not.\nLLM-based clinical sign or symptom extractor\nWe utilized the instructionﬁne-tuned Mixtral 8x7B LLM58 to extract clinical\nsigns and symptoms from unstructured clinical notes. The LLM-based data\nextraction pipeline was designed to accept a prompt and clinical notes (all\nnotes generated from admission to the time of prediction) as input and\nproduce a text output in JSON format. The schematic diagram of the LLM-\nbased data extraction pipeline is shown in Fig.1.\nBecause the length of clinical notes can exceed the predeﬁned input\nlength (context size) of the LLM, we employed the retrieval augmented\ngeneration (RAG) technique to extract smaller, relevant text chunks (con-\ntext) for the queried clinical sign or symptom. These extracted text chunks\nwere then appended to the input prompt of the LLM\n28. The RAG pipeline\nwas implemented as follows: 1) Clinical notes generated from the time of\nadmission until the prediction were segmented into smaller text chunks; 2)\nThese segmented text chunks were passed through an embedding model to\nobtain embedding vectors; 3) The embedding vectors were stored in a vector\ndatabase; 4) The embedding vector corresponding to the query text was\nextracted (query embedding); and 5) The top‘K’ embedding vectors, most\nsimilar to the query embedding vector, were retrieved from the vector store,\nand the corresponding text chunks were appended to the input prompt of\nthe LLM.\nThe RAG pipeline used the Chroma vector database for storing and\nretrieving embedding vectors and HuggingFace’s Instructor model\n59 for\nconverting text to embedding vectors. The text chunk size was set to 1000\ntokens with a 300 token overlap. The total number of embedding vectors to\nbe retrieved from the vector store was set toﬁve (K =5 ) .\nThe input prompt was designed to instruct the LLM to review the\npatient’s medical note and identify the presence of the queried clinical sign\nor symptom. The prompt also provided instructions to the LLM to generate\nthe output in a JSON format. The prompt used in our analysis was as\nfollows:\n“You are an ED doctor. Your task is to identify the following abnormal\nclinical signs and symptoms: {clinical sign or symptom}. Think step-by-step\nand provide your response in the following JSON format: {<clinical sign or\nsymptom> :[ “Yes or No”, “Concise justiﬁcation?”]} Medical note: {RAG\ncontext}.”\nTo minimize hallucinations and to maintain consistency in text gen-\neration, the temperature parameter of the LLM was set to 0.3 (See Supple-\nmentary Note 1 for more details). Additionally, for each clinical sign or\nsymptom, the LLM pipeline was run three times and the majority outcome\n(clinical sign or symptom present or not) across the multiple runs was used\nfor downstream tasks. Finally, the entire LLM-based clinical sign or\nsymptom extraction pipeline was implemented using the LangChain fra-\nmework in Python.\nClinical sign or symptom-based disease likelihood calculator\nA Bayesian likelihood calculator was used to compute the likelihood of a\ndisease based on the clinical signs or symptoms identiﬁed by the LLM. The\nposterior probability of a disease given a set of clinical signs or symptoms\nCS\ni\n/C8/C9\n; i 2 1 ... :M\n/C0/C1\n, PðDjCSÞ was calculated as follows:\nPD jCSðÞ ¼ PDðÞ P ðCSjDÞ\nPðCSÞ ð1Þ\nPC SjDðÞ ¼\nYM\ni¼1\nPðCSi ¼ 1jDÞ ð2Þ\nWhere,CSi ¼ 1 corresponded to the scenario under which the clinical signs\nor symptom CSi was identiﬁed to be present by the LLM pipeline.M\ncorresponded to the total number of clinical signs or symptoms deﬁned for a\ngiven disease. D refers to the various sepsis-mimics considered in this study.\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 6\nPlease see subsection“ Differential diagnosis tool” for list of all the sepsis-\nmimics.\nThe clinical signs or symptoms for severe sepsis and the sepsis-mimics\nwere derived with the help of two of the study authors (G.W and J.C.A). The\nﬁn a ls e to fc l i n i c a ls i g n so rs y m p t o m su s e di nt h i ss t u d yw e r e :e l e v a t e d\ninﬂammatory markers, positive inﬂuenza test, altered mental status, dehy-\ndration+sweating, metabolic acidosis, abdominal pain, ketonuria, elevated\nliver enzymes, ascites, risk factors for lung injury, urticaria+angioedema,\nseizures, history of substance abuse, hypoxemia, jaundice,positive biopsy or\nimaging, risk factors for thrombosis, tachypnea, cough+sore throat,\nexposure to heat, elevated BNP, elevated tryptase+histamine, myalgia+\nheadache, pulmonary edema, elevated lactate, elevated glucose, fever, ele-\nvated cardiac enzymes, positive covid-19 test, hematemesis or melena, dry\nmucous membranes, bilateral inﬁltrates on chest x-ray, suspicion of bac-\nterial infection, elevated d-dimer, low urine output, tachycardia, positive\ntoxicology screen, nausea+vomiting, cough+dyspnea, history ofﬂuid\nloss, anemia, elevated lipase, positiveblood culture, exposure to allergens,\nloss of taste or smell, ECG changes, hypotension, organ dysfunction, chest\npain, weight loss+ fatigue, and dyspnea.\nThe sorted list of differentials (based on their likelihood) was then used\ndownstream to determine if an alert was to beﬁred or not. Additionally, the\nlikelihood values of clinical signs or symptoms conditioned on each of the\ndiseases (PðCS\ni ¼ 1jDÞ) were optimized using Bayesian optimization on the\nretrospective development cohort.\nCOMPOSER-LLM\nThe schematic diagram of the entireCOMPOSER-LLM pipeline is shown in\nFig. 1. Starting from the time of ED admission, COMPOSER generated a\nsepsis risk score at an hourly resolution. If the risk score was greater than a\nprimary decision threshold (θ\n1 = 0.75) a notiﬁcation was sent via a nurse-\nfacing best practice advisory. Risk scores closer to a secondary decision\nthreshold (θ2 = 0.50) were often associated with false alarms. Therefore, for\nrisk scores within the high-uncertainty region (θ2 > = risk score>θ1), the\nLLM-based differential diagnosis tool was utilized to improve diagnostic\ncertainty for sepsis. Speciﬁcally, if‘Severe Sepsis’was present within the top-\n5 sorted differentials and the LLM identiﬁed a ‘suspicion of bacterial\ninfection’,an o t iﬁcation was sent via a nurse-facing Best Practice Advisory.\nPlease note that throughout this paper, COMPOSER-LLM and\nCOMPOSER-LLM\nDDx refer to the same pipeline, unless otherwise noted.\nBaseline model comparison\nInstead of extracting the clinical signs and symptoms followed by per-\nforming differential diagnosis (COMPOSER-LLM\nDDx), we explored the\npossibility of directly asking the LLM toidentify the presence of severe sepsis\nin a patient based on clinical notes (which we’ll refer to as COMPOSER-\nLLMbaseline). In this experiment, the LLM generated a binary output for\n“Sepsis”. The prompt used was as follows:\n“You are an ED doctor. Your task is to identify if sepsis is present in the\ncurrent admission. Think step-by-step and provide your response in the fol-\nlowing JSON format: {“ Sepsis” :[ “Yes or No”, “Concise justiﬁcation?”]}.\nNote that“Sepsis” is deﬁned as suspicion or documentation of infection\nwith evidence of organ dysfunction.\nMedical note: {RAG context}.”\nFor samples with COMPOSER risk scores between 0.5 and 0.75\n(θ2 =0 . 5 ,θ1 = 0.75), an alert wasﬁred if the output from the LLM for\n“Sepsis” was “Yes”. Note that, for COMPOSER risk scores greater than the\nprimary decision threshold (θ1), an alert wasﬁred irrespective of the output\nfrom LLM.\nAn additional baseline model considered in this study was the\nCOMPOSER-LLMSLT pipeline. COMPOSER-LLMSLT utilized a sepsis\nlikelihood tool to calculate the likelihood of severe sepsis based on the\npresence of severe sepsis related clinical signs or symptoms. For samples\nwith COMPOSER risk scores between 0.5 and 0.75 (θ2 =0 . 5 ,θ1 = 0.75), an\nalert was ﬁred if the sepsis likelihood score exceeded a predetermined\nlikelihood-based decision threshold (α) and the LLM identiﬁed a‘suspicion\nof bacterial infection’.N o t et h a t ,f o rC O M P O S E Rr i s ks c o r e sg r e a t e rt h a n\nthe primary decision threshold (θ1), an alert wasﬁred irrespective of the\noutput from LLM.\nExperimental setup and evaluation\nFor all continuous variables, we have reported medians ([25th– 75th per-\ncentile]). For binary variables, we have reported percentages. Differences\nbetween the septic and non-septic cohort were assessed with Wilcoxon rank\nsum tests on continuous variables and Pearson’sc h i - s q u a r e dt e s t so n\ncategorical variables and signiﬁcance was assessed at ap-value of 0.05. For\nbinary variables, we have reported percentages. Sensitivity, positive pre-\ndictive value (PPV), and F-1 score at aﬁxed decision threshold have been\nreported at the encounter level. Additionally, the following policy was uti-\nlized for COMPOSER-LLM alerts: 1) an alert was considered a true positive\nif ‘Severe Sepsis’ was present within the Top-5 differentials for a septic\npatient; 2) an alert was considered a false positive if‘Severe Sepsis’ was\npresent within the Top-1 differentials for a non-septic patient.\nCOMPOSER-LLM was designed as a notiﬁcation-only tool to predict\nonset time of sepsis four hours in advance, no earlier than 48 h in advance\nand under a silencing policy of six hours. Speciﬁcally, Sensitivity, PPV and\nF-1 score were reported under an end-user clinical response policy in which\nalarms ﬁred up to 48 h prior to onset of sepsis were considered as true\nalarms, and the model was silenced for six hours after an alarm wasﬁred.\nThe end-user clinical response policyis identical to the policy described\npreviously in our published work\n15,57.A d d i t i o n a l l y ,w eh a v er e p o r t e df a l s e\nalarms per patient hour (FAPH) which can be used to calculate the expected\nnumber of false alarms per unit of time in a typical care unit (e.g., a FAPH of\n0.025 translates to roughly 1 alarm every 2 h in a 20-bed care unit). The\nFAPH was calculated by dividing the total number of false alarms by the\ntotal number of data points (sum of hourly time points across all patients) in\na given cohort.\nAll hyperparameters of the LLM pipeline including text chunk size\n(1000 tokens), number of retrieval vectors from vector database (‘K’ = 5),\ntemperature of the LLM (0.3), likelihood values of clinical signs or symp-\ntoms conditioned on sepsis were optimized using Bayesian hyperparameter\noptimization on the retrospective development cohort.\nThe COMPOSER model was implemented in TensorFlow (version\n2.13.0). For the LLM pipeline, we utilized the instructionﬁne-tuned Mixtral\n8x7B LLM model (version 0.1, GPTQ quantized)(HuggingFace link:https://\nhuggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ). The LLM-\nbased clinical signs or symptom extraction pipeline was implemented using\nthe LangChain framework (version 0.1.5) in Python (version 3.11.5). The\nL L Mp i p e l i n ew a sr u no na nA W Sm u l t i - G P UE C 2i n s t a n c ew i t hN V I D I A\nA10G GPUs (ec2 instance type: g5.12xlarge).\nClinician chart review\nThe goal of utilizing the LLM-based differential diagnosis tool was to reduce\nthe false alarm rate of the standalone COMPOSER pipeline while main-\ntaining similar sensitivity levels. To further investigate the false positives\ngenerated by the COMPOSER-LLM model, a clinician chart review was\nconducted. Chart review of the false positives was performed by two phy-\nsicians and study authors (J.C.A and A.P). Each physician was asked to\nreview the chart of a given patient and identify if there was a‘suspicion of\nbacterial infection’at the time of alert. In cases of disagreement between the\ntwo clinicians, a third physician and study author (G.W) provided theﬁnal\nevaluation.\nCOMPOSER-LLM prospective deployment\nThe COMPOSER-LLM model was prospectively deployed in silent-mode\non a cloud-based platform, as previously described by Boussina et al.15.\nProspective validation studies are essential in clinical applications of LLMs\nas retrospective performance may not accurately reﬂect real-world perfor-\nmance due to factors such as incomplete or missing clinical notes. The real-\ntime platform extracted data at an hourly resolution of all the active patients\n(across the two Emergency Departments within UCSD Health system)\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 7\nusing FHIR APIs with OAuth 2.0 authentication, and passed the input\nfeature set (consisting of laboratory measurements, vitals measurements,\ncomorbidities, medications, clinical notes, and demographics) to the\nCOMPOSER-LLM inference engine. The inference engine consisted of\nCOMPOSER microservice and differential diagnosis tool microservice\nhosted within separate EC2 instances. The sepsis risk scores generated by the\nCOMPOSER-LLM pipeline were then written to aﬂowsheet within the\nEHR using an HL7v2 outbound message. Theﬂowsheet then triggered a\nnurse-facing Best Practice Advisory (BPA) that alerted the caregiver that the\npatient was at risk of developing severe sepsis. As COMPOSER-LLM was\ndeployed in silent mode, the BPA was not shown to the end-user. The\nschematic diagram of the real-time deployment pipeline is shown in Fig.2.\nThe COMPOSER-LLM pipeline was deployed for real-time prediction of\nsepsis across the two EDs within the UCSD Health system starting from\nMay 1, 2024.\nData availability\nAccess to the de-identiﬁed UCSD cohort can be made available by contacting\nthe corresponding author and via approval from the UCSD Institutional\nReview Boards (IRB) and Health Data Oversight Committee (HDOC).\nCode availability\nThe code for computing performance metrics is available athttps://github.\ncom/NematiLab/COMPOSER-LLM.\nR e c e i v e d :2 5J u l y2 0 2 4 ;A c c e p t e d :2 7A p r i l2 0 2 5 ;\nReferences\n1. Rudd, K. E. et al. Global, regional, and national sepsis incidence and\nmortality, 1990–2017: analysis for the Global Burden of Disease\nStudy. Lancet 395, 200–211 (2020).\n2. Singer, M. et al. The third international consensus deﬁnitions for sepsis\nand septic shock (Sepsis-3).J. Am. Med. Assoc.315,8 0 1–810 (2016).\n3. Rhee, C. et al. Incidence and trends of sepsis in US hospitals using\nclinical vs claims data, 2009-2014.J. Am. Med. Assoc.318,\n1241–1249 (2017).\n4. Kumar, A. et al. Duration of hypotension before initiation of effective\nantimicrobial therapy is the critical determinant of survival in human\nseptic shock*. Crit. Care Med.34, 1589–1596 (2006).\n5. Ferrer, R. et al. Empiric antibiotic treatment reduces mortality in severe\nsepsis and septic shock from theﬁrst hour: results from a guideline-\nbased performance improvement program.Crit. Care Med.42,\n1749–1755 (2014).\n6. Liu, V. X. et al. The Timing of Early Antibiotics and Hospital Mortality in\nSepsis. Am. J. Respir. Crit. Care Med.196, 856–863 (2017).\n7. Peltan, I. D. et al. ED door-to-antibiotic time and long-term mortality in\nsepsis. Chest 155, 938–946 (2019).\n8. Rhodes, A. et al. Surviving Sepsis Campaign: International Guidelines\nfor Management of Sepsis and Septic Shock: 2016.Intensive Care\nMed. 43, 304–377 (2017).\n9. Chamberlain, D. J., Willis, E. M. & Bersten, A. B. The severe sepsis\nbundles as processes of care: a meta-analysis.Aust. Crit. Care24,\n229–243 (2011).\n10. Centers for Medicare & Medicaid Services. QualityNet— inpatient\nhospitals speciﬁcations manual Version 5.13.https://qualitynet.cms.\ngov/inpatient/speciﬁcations-manuals (2023).\n11. Are ﬁan, H. et al. Hospital-related cost of sepsis: a systematic review.\nJ. Infect.74, 107–117 (2017).\n12. Islam, K. R. et al. Machine learning-based early prediction of sepsis\nusing electronic health records: a systematic review.J. Clin. Med.12,\n5658 (2023).\n13. Adams, R. et al. Prospective, multi-site study of patient outcomes\nafter implementation of the TREWS machine learning-based early\nwarning system for sepsis.Nat. Med.28, 1455–1460 (2022).\n14. McCoy, A. & Das, R. Reducing patient mortality, length of stay and\nreadmissions through machine learning-based sepsis prediction in\nthe emergency department, intensive care unit and hospitalﬂoor\nunits. BMJ Open Qual.6, e000158 (2017).\n15. Boussina, A. et al. Impact of a deep learning sepsis prediction model\non quality of care and survival.Npj Digit. Med.7, 14 (2024).\n16. Liu, R., Greenstein, J. L., Sarma, S. V. & Winslow, R. L. Natural\nlanguage processing of clinical notes for improved early prediction of\nseptic shock in the ICU. In2019 41st annual international conference\nof the IEEE engineering in medicine and biology society (EMBC)\n6103–6108 (IEEE, 2019).\n17. Li, Q. et al. Early prediction of sepsis using chatGPT-generated\nsummaries and structured data.Multimed. Tools Appl. 83,\n89521–89543 (2024).\n18. Goh, K. H. et al. Artiﬁcial intelligence in sepsis early prediction and\ndiagnosis using unstructured data in healthcare.Nat. Commun.12,\n711 (2021).\n19. Amrollahi, F., Shashikumar, S. P., Razmi, F. & Nemati, S. Contextual\nembeddings from clinical notes improves prediction of sepsis. In:\nAMIA annual symposium proceedingsvol. 2020, 197 (American\nMedical Informatics Association, 2020).\n20. Yan, M. Y., Gustad, L. T. & Nytrø, Ø. Sepsis prediction, early detection,\nand identiﬁcation using clinical text for machine learning: a systematic\nreview. J. Am. Med. Inform. Assoc.29, 559–575 (2022).\n21. Culliton, P. et al. Predicting severe sepsis using text from the electronic\nhealth record. Preprint athttp://arxiv.org/abs/1711.11536(2017).\n22. Qin, F. et al. Improving early sepsis prediction with multi modal\nlearning. Preprint athttp://arxiv.org/abs/2107.11094 (2021).\n23. Horng, S. et al. Creating an automated trigger for sepsis clinical\ndecision support at emergency department triage using machine\nlearning. PloS One12, e0174708 (2017).\n24. Hammoud, I., Ramakrishnan, I. V., Henry, M. & Morley, E. Multimodal\nearly septic shock prediction model using lasso regression with\ndecaying response. In2020 IEEE International Conference on\nHealthcare Informatics (ICHI)1–3 (IEEE, 2020).\n25. Holderness, E., Cawkwell, P., Bolton, K., Pustejovsky, J. & Hall, M.-H.\nDistinguishing clinical sentiment: the importance of domain\nadaptation in psychiatric patient health records. InProc. of the 2nd\nClinical Natural Language Processing Workshop117–123\n(Association for Computational Linguistics, Minneapolis, Minnesota,\nUSA, 2019).\n26. Laparra, E., Bethard, S. & Miller, T. A. Rethinking domain adaptation\nfor machine learning over clinical language.JAMIA Open3, 146–150\n(2020).\n27. Liu, J., Capurro, D., Nguyen, A. & Verspoor, K. Note Bloat” impacts\ndeep learning-based NLP models for clinical prediction tasks.J.\nBiomed. Inform.\n133, 104149 (2022).\n28. Lewis, P. et al. Retrieval-augmented generation for knowledge-intensive\nnlp tasks.Adv. Neural Inf. Process. Syst.33, 9459–9474 (2020).\n29. Thirunavukarasu, A. J. et al. Large language models in medicine.Nat.\nMed. 29, 1930–1940 (2023).\n30. Guevara, M. et al. Large language models to identify social\ndeterminants of health in electronic health records.NPJ Digit. Med.7,\n6 (2024).\n31. Wang, H., Gao, C., Dantona, C., Hull, B. & Sun, J. DRG-LLaMA: tuning\nLLaMA model to predict diagnosis-related group for hospitalized\npatients. Npj Digit. Med.7, 16 (2024).\n32. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n33. Nayak, A. et al. Comparison of history of present illness summaries\ngenerated by a chatbot and senior internal medicine residents.JAMA\nIntern. Med.183, 1026–1027 (2023).\n34. Ayers, J. W. et al. Comparing physician and artiﬁcial intelligence\nchatbot responses to patient questions posted to a public social\nmedia forum.JAMA Intern. Med.183, 589–596 (2023).\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 8\n35. Tai-Seale, M. et al. AI-Generated Draft Replies Integrated Into Health\nRecords and Physicians’Electronic Communication.JAMA Netw.\nOpen 7, e246565–e246565 (2024).\n36. Chen, J., Lin, H., Han, X. & Sun, L. Benchmarking large language\nmodels in retrieval-augmented generation.Proc. AAAI Conf. Artif.\nIntell. 38, 17754–17762 (2024).\n37. Bhargava, A. et al. Development and validation of theﬁrst FDA\nauthorized artiﬁcial intelligence/machine learning diagnostic tool for\nthe prediction of sepsis risk.medRxiv http://www.medrxiv.org/\ncontent/10.1101/2024.05.06.24306954v2 (2024).\n38. Wardi, G. et al. Bringing the promise of artiﬁcial intelligence to critical\ncare: what the experience with sepsis analytics can teach us.Crit.\nCare Med.51, 985–991 (2023).\n39. Wong, A. et al. External validation of a widely implemented proprietary\nsepsis prediction model in hospitalized patients.JAMA Intern. Med.\n181, 1065–1070 (2021).\n40. Kamran, F. et al. Evaluation of sepsis prediction models before onset\nof treatment.NJEM AI. 1, 3 (2024).\n41. Henry, K. E. et al. Factors driving provider adoption of the TREWS\nmachine learning-based early warning system and its effects on\nsepsis treatment timing.Nat. Med.28, 1447–1454 (2022).\n42. Miller, R. A., Pople, H. E. & Myers, J. D. INTERNIST-I, An experimental\ncomputer-based diagnostic consultant for general internal medicine.\nIn: Computer-assisted medical decision making(eds. Reggia, J. A. &\nTuhrim, S.) 139–158 (Springer, 1985).https://doi.org/10.1007/978-1-\n4612-5108-8_8.\n43. Shortliffe, E. H. A rule-based computer program for advising physicians\nregarding antimicrobial therapy selection. In:Proceedings of the 1974\nannual conference on XX - ACM’74 vol. 2 739 (ACM Press, 1974).\n44. Hager, P. et al. Evaluation and mitigation of the limitations of large\nlanguage models in clinical decision-making.Nat. Med. 30,\n2613–2622 (2024).\n45. Ullah, E., Parwani, A., Baig, M. M. & Singh, R. Challenges and barriers\nof using large language models (LLM) such as ChatGPT for diagnostic\nmedicine with a focus on digital pathology– a recent scoping review.\nDiagn. Pathol.19, 43 (2024).\n46. Williams, C. Y., Miao, B. Y., Kornblith, A. E. & Butte, A. J. Evaluating the\nuse of large language models to provide clinical recommendations in\nthe Emergency Department.Nat. Commun.15, 8236 (2024).\n47. Mitka, M. Joint commission warns of alarm fatigue: multitude of alarms\nfrom monitoring devices problematic.JAMA\n309, 2315–2316 (2013).\n48. Reyna, M. A., Nsoesie, E. O. & Clifford, G. D. Rethinking algorithm\nperformance metrics for artiﬁcial intelligence in diagnostic medicine.\nJAMA 328, 329–330 (2022).\n49. Valik, J. K. et al. Validation of automated sepsis surveillance based on\nthe Sepsis-3 clinical criteria against physician record review in a\ngeneral hospital population: observational study using electronic\nhealth records data.BMJ Qual. Saf.29, 735–745 (2020).\n50. Rafailov, R. et al. Direct preference optimization: your language model\nis secretly a reward model. In:Proceedings of the 37th International\nConference on Neural Information Processing Systems(NeurIPS,\n2023).\n51. Blundell, C. et al. Weight uncertainty in neural networks. In:\nProceedings of the 32nd International Conference on International\nConference on Machine Learning(ACM, 2015).\n52. Hu, E. J. et al. LoRA: Low-rank Adaptation of Large Language Models.\nIn The Tenth International Conference on Learning Representations\n(ICLR) Vol. 1, 3 (International Conference on Learning\nRepresentations (ICLR), 2022).\n53. Müller, L. et al. An open access medical knowledge base for\ncommunity driven diagnostic decision support system development.\nBMC Med. Inform. Decis. Mak.19, 93 (2019).\n54. Seymour, C. W. et al. Assessment of clinical criteria for sepsis: for the\nthird international consensus deﬁnitions for sepsis and septic shock\n(Sepsis-3). J. Am. Med. Assoc.315, 762–774 (2016).\n55. Lauritsen, S. M. et al. Explainable artiﬁcial intelligence model to predict\nacute critical illness from electronic health records.Nat. Commun.11,\n1–11 (2020).\n56. Amrollahi, F. et al. Inclusion of social determinants of health improves\nsepsis readmission prediction models.J. Am. Med. Inform. Assoc.29,\n1263–1270 (2022).\n57. Shashikumar, S. P., Wardi, G., Malhotra, A. & Nemati, S. Artiﬁcial\nintelligence sepsis prediction algorithm learns to say“I don’t know”.\nNPJ Digit. Med.4, 134 (2021).\n58. Jiang, A. Q. et al. Mixtral of experts. Preprint athttp://arxiv.org/abs/\n2401.04088 (2024).\n59. Su, H. et al. One Embedder, Any Task: Instruction-Finetuned Text\nEmbeddings. InFindings of the Association for Computational\nLinguistics 1102–1121 (Association for Computational Linguistics,\nToronto, Canada, 2023).\nAcknowledgements\nS.N. is funded by the National Institutes of Health (#R01LM013998,\n#R35GM143121, #R42AI177108). G.W. has been supported by the\nNational Institutes of Health (#K23GM146092). This work has been\nsupported by the National Institutes of Health (#4R42AI177108-01).\nK.S.’s institution is currently supported by funding from the National\nInstitutes of Health for unrelated work; his institution previously received\ngrant funding from Teva Pharmaceuticals for unrelated work; and he\npreviously consulted for Flatiron Health for unrelated work. The opinions\nor assertions contained herein are the private ones of the author and are\nnot to be construed as ofﬁcial or reﬂecting the views of the NIH or any\nother agency of the US Government.\nAuthor contributions\nS.P.S., E.A.S and S.N. were involved in the original conception and design of\nthe work. S.P.S and S.N. developed the network architectures, conducted\nthe experiments, and analyzed the data. S.M assisted with analyzing the\ndata. R.K assisted with setting up the computational environment and\nconducted experiments. A.P, G.W., J.C.A, K.S and E.A.S. provided clinical\nexpertise, reviewed patient data, and contributed to interpretation of results.\nAll authors contributed to manuscript preparation, critical revisions, and\nhave read and approved the manuscript.\nCompeting interests\nS.N. and S.P.S. are co-founders of a UCSD start-up, Clairyon Inc. (for-\nmerly Healcisio Inc.), which is focused on commercialization of advanced\nanalytical decision support tools, and formed in compliance with UCSD\nconﬂict of interest policies. The remaining authors declare no competing\ninterests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01689-w\n.\nCorrespondenceand requests for materials should be addressed to\nShamim Nemati.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 9\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01689-w Article\nnpj Digital Medicine|           (2025) 8:290 10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5004258155822754
    },
    {
      "name": "Sepsis",
      "score": 0.49686339497566223
    },
    {
      "name": "Intensive care medicine",
      "score": 0.3319905400276184
    },
    {
      "name": "Medicine",
      "score": 0.3100516200065613
    },
    {
      "name": "Internal medicine",
      "score": 0.22692111134529114
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800935791",
      "name": "UC San Diego Health System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210125099",
      "name": "Mayo Clinic in Arizona",
      "country": "US"
    }
  ],
  "cited_by": 7
}