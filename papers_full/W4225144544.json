{
    "title": "<i>CLIN-X</i>: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain",
    "url": "https://openalex.org/W4225144544",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2145220972",
            "name": "Lukas Lange",
            "affiliations": [
                "Robert Bosch (Germany)",
                "Saarland University"
            ]
        },
        {
            "id": "https://openalex.org/A1997977529",
            "name": "Heike Adel",
            "affiliations": [
                "Robert Bosch (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A1484506931",
            "name": "Jannik Strötgen",
            "affiliations": [
                "Robert Bosch (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A1237914314",
            "name": "Dietrich Klakow",
            "affiliations": [
                "Saarland University"
            ]
        },
        {
            "id": "https://openalex.org/A2145220972",
            "name": "Lukas Lange",
            "affiliations": [
                "Robert Bosch (Germany)",
                "Saarland University"
            ]
        },
        {
            "id": "https://openalex.org/A1997977529",
            "name": "Heike Adel",
            "affiliations": [
                "Robert Bosch (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A1484506931",
            "name": "Jannik Strötgen",
            "affiliations": [
                "Robert Bosch (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A1237914314",
            "name": "Dietrich Klakow",
            "affiliations": [
                "Saarland University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6683738474",
        "https://openalex.org/W4229668175",
        "https://openalex.org/W2156235098",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3193158708",
        "https://openalex.org/W1034374084",
        "https://openalex.org/W2137407193",
        "https://openalex.org/W2160987310",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W2768488789",
        "https://openalex.org/W2954075147",
        "https://openalex.org/W3125468681",
        "https://openalex.org/W2901331771",
        "https://openalex.org/W3153617151",
        "https://openalex.org/W2615487675",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2970511757",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2985294119",
        "https://openalex.org/W3034949140",
        "https://openalex.org/W3198146398",
        "https://openalex.org/W3121010779",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W3199422761",
        "https://openalex.org/W3204440036",
        "https://openalex.org/W3096590546",
        "https://openalex.org/W2951243568",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3002832564",
        "https://openalex.org/W2880875857",
        "https://openalex.org/W2949845972",
        "https://openalex.org/W4287728555",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W2970019270",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4295887562",
        "https://openalex.org/W3115095335",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4287634168"
    ],
    "abstract": "Abstract Motivation The field of natural language processing (NLP) has recently seen a large change toward using pre-trained language models for solving almost any task. Despite showing great improvements in benchmark datasets for various tasks, these models often perform sub-optimal in non-standard domains like the clinical domain where a large gap between pre-training documents and target documents is observed. In this article, we aim at closing this gap with domain-specific training of the language model and we investigate its effect on a diverse set of downstream tasks and settings. Results We introduce the pre-trained CLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other pre-trained transformer models by a large margin for 10 clinical concept extraction tasks from two languages. In addition, we demonstrate how the transformer model can be further improved with our proposed task- and language-agnostic model architecture based on ensembles over random splits and cross-sentence context. Our studies in low-resource and transfer settings reveal stable model performance despite a lack of annotated data with improvements of up to 47 F1 points when only 250 labeled sentences are available. Our results highlight the importance of specialized language models, such as CLIN-X, for concept extraction in non-standard domains, but also show that our task-agnostic model architecture is robust across the tested tasks and languages so that domain- or task-specific adaptations are not required. Availability and implementation The CLIN-X language models and source code for fine-tuning and transferring the model are publicly available at https://github.com/boschresearch/clin_x/ and the huggingface model hub.",
    "full_text": "Data and T ext Mining\nCLIN-X : pre-trained language models and a study\non cross-task transfer for concept extraction in the\nclinical domain\nLukas Lange1,2,∗, Heike Adel1, Jannik Strötgen1 and Dietrich Klakow2\n1Bosch Center for Artiﬁcial Intelligence, Robert Bosch GmbH, Renningen, 71272, Germany and\n2Spoken Language Systems, Saarland University, Saarbrücken, 66111, Germany.\n∗T o whom correspondence should be addressed.\nAbstract\nMotivation: The ﬁeld of natural language processing (NLP) has recently seen a large change towards\nusing pre-trained language models for solving almost any task. Despite showing great improvements in\nbenchmark datasets for various tasks, these models often perform sub-optimal in non-standard domains\nlike the clinical domain where a large gap between pre-training documents and target documents is\nobserved. In this paper, we aim at closing this gap with domain-speciﬁc training of the language model\nand we investigate its effect on a diverse set of downstream tasks and settings.\nResults: We introduce the pre-trained CLIN-X (Clinical XLM-R) language models and show howCLIN-X\noutperforms other pre-trained transformer models by a large margin for ten clinical concept extraction\ntasks from two languages. In addition, we demonstrate how the transformer model can be further improved\nwith our proposed task- and language-agnostic model architecture based on ensembles over random\nsplits and cross-sentence context. Our studies in low-resource and transfer settings reveal stable model\nperformance despite a lack of annotated data with improvements of up to 47 F1 points when only 250\nlabeled sentences are available. Our results highlight the importance of specialized language models\nas CLIN-X for concept extraction in non-standard domains, but also show that our task-agnostic model\narchitecture is robust across the tested tasks and languages so that domain- or task-speciﬁc adaptations\nare not required.\nAvailability: The CLIN-X language models and source code for ﬁne-tuning and transferring the model\nare publicly available at https://github.com/boschresearch/clin_x/ and the huggingface model hub.\nContact: Lukas.Lange@de.bosch.com\n1 Introduction\nCollecting and understanding key clinical information, such as disorders,\nsymptoms, drugs, etc., from electronic health records (EHRs) has wide-\nranging applications within clinical practice and research (Leaman et al.,\n2015; Wang et al. , 2018). A better understanding of this information\ncan, on the one hand, facilitate novel clinical studies, and, on the other\nhand, help practitioners to optimize clinical workﬂows. However, free\ntext is ubiquitous in EHRs. This leads to great difﬁculties in harvesting\nknowledge from EHRs. Therefore, natural language processing (NLP)\nsystems, especially information extraction components, play a critical role\nin extracting and encoding information of interest from clinical narratives,\nas this information can then be fed into downstream applications. For\nexample, the extraction of structured information from clinical narratives\ncan help in decision making or drug repurposing (Marimon et al., 2019).\nHowever, information extraction in non-standard domains like the\nclinical domain is a challenging problem due to the large number of\ncomplex terms and unusual document structures (Lee et al. , 2020). In\naddition, pre-trained language models (PLM) such as BERT (Devlinet al.,\n2019) that demonstrated superior performance for many NLP tasks are\ntypically trained on standard domains, such as web texts, news articles\nor Wikipedia. Despite showing some robustness across languages and\ndomains (Conneau et al. , 2020) these models still achieve their best\nperformance when applied to targets similar to their pre-training corpora\nwhich can limit their applicability in many situations (Gururangan et al.,\n2020). One way to overcome this domain-gap is the adaptation of existing\nlanguage models to the new target domain or training a new domain-\nspeciﬁc model from scratch (Beltagyet al., 2019; Leeet al., 2020). Several\n© The Author 2021. All rights reserved. 1\narXiv:2112.08754v3  [cs.CL]  20 May 2022\n2 Lange et al.\nrecent works have shown that this kind of adaptation boosts performance\nfor downstream tasks in non-standard domains by, e.g., pre-training with\nmasked language modeling (MLM) objectives on documents from the\ntarget domain (Weber et al., 2019; Naseem et al., 2021).\nWhile all the previous methods help to build high-performing model\narchitectures, often there is also a lack of annotated data in the clinical\ndomain which is usually needed for all deep-learning-based models. On\nthe one hand, this domain has high requirements regarding the removal\nor masking of protected health information (PHI) of individuals (Uzuner\net al., 2007; Stubbset al., 2015) which is particularly worthy of protection\nand can prevent data publication. On the other hand, information extraction\ntasks are often speciﬁc to their target domain and clinical concepts are only\nfound very infrequently outside EHRs which limits reusability of existing\nresources. Possible solutions for the low-resource problem can be multi-\ntask learning (Khan et al., 2020; Mulyar et al., 2021) or transfer Learning\n(Lee et al., 2018; Penget al., 2019) across similar corpora from the clinical\ndomain. However, transferring knowledge is particularly challenging in the\nclinical domain as biomedical NLP models have problems generalizing to\nnew entities (Kim and Kang, 2021). Therefore, one has to carefully select\nthe transfer sources (Lange et al., 2021b).\nOver the last years, we have participated in a series of shared tasks\non information extraction in the Spanish clinical domain (Marimon et al.,\n2019; Miranda-Escalada et al., 2020; Lima-López et al., 2021). With our\nsystems, we were able to outperform the other participants and won the\ncompetitions twice. The winning systems were task-agnostic and utilized\ndomain-adapted language models and word embeddings (Lange et al. ,\n2019), as well as improved training routines for transformer models\n(Lange et al., 2021a). Based on our ﬁndings and lessons learned during\nthe competitions, we propose in this paper a robust model architecture\nand training procedure for concept extraction in the clinical domain that\nis task- and language-agnostic. We introduce a new Spanish clinical\nlanguage model CLIN-XES (Clinical XLM-R) that outperforms existing\ntransformer models on Spanish corpora and exempliﬁes the beneﬁts of\ncross-language domain adaptation for English tasks as well. For this, we\nperform a broad evaluation of ten clinical information extraction tasks from\ntwo languages (English and Spanish), including low-resource settings.\nFinally, we perform cross-task transfer experiments and show that this\ncan boost performance by more than 47 F1 points for few-shot training.\nOur results demonstrate great and consistent improvements compared to\nstandard transformer models across all tasks in both languages. We release\nboth, CLIN-XES as well as its English counterpart CLIN-XEN .\n2 Approach\nIn this paper, we introduce new pre-trained language models and propose\na robust model architecture to perform concept extraction in the clinical\ndomain for English and Spanish. The overall model architecture is shown\nin Figure 1 and our proposed model components are highlighted. First,\nthe input is computed on subword-level instead of the usual word-level,\nwhich eliminates the need for external tokenization. In addition, the input\nis enriched with its cross-sentence context to capture a wider document\ncontext. Second, the input is processed by a transformer model that\nis adapted to our target domain. Third, the model output is computed\nusing a conditional random ﬁeld (CRF) output layer to address long\nannotations. Then, an ensemble over models trained on different training\nsplits is computed that reduces variance and captures the complementary\nknowledge from all models. Finally, we experiment with cross-task model\ntransfer to further improve the model in few-shot settings.\nIn summary, the contributions of this paper are as follows:\nCLIN-X\n: The p ##t was begun on he ##par ##in g ##tt\n.: It was noted to have plate ##let drop > 50%  ...\n: He ##pa ##in was d / c ' d .\nCross-sentence \nContext\nCRF\nPredictions\nEnsemble\nSubword-level\n: O  O ... O  O  B-treat  I-treat  E-treat  O  O ...\nBIOSE \nLabels\n...     plate    ##let     drop     ...\nModel\nTransfer\nClinical \nPre-training\nFig. 1: Overview of the concept extraction pipeline based onCLIN-X and\nour model components for subword-based extraction with cross-sentence\ncontext, BIOSE labels, CRFs and model transfer.\n• We study the impact of domain-adaptive pre-training for clinical\nconcept extraction for different embedding types and publish new\nlanguage models that are adapted to the clinical domain. We show\nthat this PLM outperforms other publicly available embeddings and\nmodels in our settings and we also show that cross-language domain\nadaptations works for English tasks as well.\n• We perform a broad evaluation of ten clinical sequence labeling tasks\nacross two languages, including low-resource and transfer settings. By\nthis, we demonstrate how our methods can further boost already high-\nperforming transformer models by using advanced training methods\nand effective changes in the architecture.\n• Our models outperform the state-of-the-art methods for clinical and\nbiomedical concept extraction, as well as various other transformer\nmodels for all ten tasks.\n• We make our new domain-adapted CLIN-X language models and the\nsource code for ﬁne-tuning the concept extraction models using our\nmethods publicly available.\n3 Materials and Methods\nIn this section, we start with a brief description of the input representations.\nThen, we discuss our proposed architectural choices as well as the\nadvanced training methods.\n3.1 Input Representations for the Clinical Domain\nState-of-the-art methods for concept extraction typically rely on word\nembeddings or language models as input representations. The standard\napproach is the pre-training of these models on large-scale unannotated\ndatasets once and their reuse as powerful representations for many\ndownstream applications (Collobert et al. , 2011). Phan et al. (2019)\nhave shown that contextual information helps in particular in the medical\ndomain, e.g., due to the high number of synonyms. Thus, we focus on the\nusage of contextualized embeddings in this work, which are most often\nretrieved from transformer language models nowadays. This is either done\nwith auto-regressive language modeling (Peters et al., 2018) or masked\nlanguage modeling (Devlin et al., 2019), which we use in this paper.\nDomain-speciﬁc embeddings. A popular way to approach the challenges\nof NLP in non-standard domains is the inclusion of domain knowledge\nvia domain-speciﬁc embeddings (Friedrich et al., 2020). For this, word\nembeddings or language models are pre-trained or further specialized\non documents of the target domain. These embeddings can be used in\nCLIN-X 3\ndownstream applications. This kind of domain adaptation has shown great\nbeneﬁts in practice (Gururangan et al., 2020), thus, we explore domain-\nand language-adaptive pre-training of transformer models in this paper.\nThe CLIN-X pre-trained language model. At the time of writing, there\nis no Spanish clinical transformer publicly available. Thus, we train\nand publish the CLIN-XES language model. The model is based on the\nmultilingual XLM-R transformer, which was trained on 100 languages\nand showed superior performance in many different tasks across languages\nand can even outperform monolingual models in certain settings (Conneau\net al., 2020). Even though XLM-R was pre-trained on 53GB of Spanish\ndocuments, this was only 2% of the overall training data. To steer this\nmodel towards the Spanish clinical domain, we sample documents from\nthe Scielo archive and the MeSpEn resources (Villegas et al., 2018). The\nresulting corpus has a size of 790MB and is highly speciﬁc for our target\nsetting. We initialize CLIN-X using the pre-trained XLM-R weights and\ntrain masked language modeling (MLM) on the clinical corpus for 3 epochs\nwhich roughly corresponds to 32k steps. Nonetheless, this model is still\nmultilingual and we demonstrate the positive impact of cross-language\ndomain adaptation by applying this model to English tasks. 1\n3.2 Concept Extraction Model\nIn the following, we describe the architectural choices we made compared\nto the standard transformer model for sequence labeling as proposed by\nDevlin et al. (2019).\nSubword-level inputs. Information extraction tasks are typically performed\non the token level, while most transformers work on ﬁner subwords instead.\nThus, the input representations from transformers for tokens are either\nretrieved from the ﬁrst subword or the average (Devlin et al., 2019). In\ncontrasts, we perform concept extraction directly on the subword level.\nBy doing this, there is no need for external tokenization besides the\nsubword segmentation of the transformer. Note that the usage of domain-\nspeciﬁc subwords is still often beneﬁcial compared to the general domain\nsegmentation (Beltagy et al., 2019; Lee et al., 2020).\nCross-sentence context. Transformers are suited to incorporate information\nfrom a larger context. Luoma and Pyysalo (2020) showed that context\ninformation from neighboring sentences has positive effects for named\nentity recognition on the general domain. Finkelet al. (2004) also showed\nthe positive impact of context for clinical concept extraction. We follow\nthese approaches and add context information to the input similar to\nSchweter and Akbik (2020). We incorporate the context of 100 subwords\nto the left and right and use the document boundaries to set the context\nlimits as all corpora are clearly separated in documents.\nConditional Random Field Output. As Kim and Kang (2021) have shown,\nentity recognition models in the biomedical domain tend to memorize\ntraining instances and their labels. This can result in incorrect label\nencodings as the model fails to generalize. A conditional random ﬁeld\n(CRF, Laffertyet al., 2001) can constrain these incorrect sequences as the\nViterbi algorithm is used for decoding. In addition, the CRF has advantages\nwhen it comes to long entities covering multiple tokens (Lima-Lópezet al.,\n2021) that appear frequently in the clinical domain.\n1 In addition to the Spanish CLIN-XES model, we release an English\nversion CLIN-XEN trained on clinical Pubmed abstracts (850MB) ﬁltered\nfollowing Haynes et al. (2005) for a direct comparison of our methods in a\nmonolingual setting. This allows researchers and practitioners to address\nthe English clinical domain with an out-of-the-box tailored model so that\nour transfer methods do not have to be applied. Pubmed is used with the\ncourtesy of the U.S. National Library of Medicine.\nModel 1 Model 2 Model 3 Model 4 Model 5\nEnsemble\nValidation docs. Training docs.\nFig. 2: Ensembles over different training splits splits.\n3.3 Training on Data Splits.\nHaving a robust model architecture is a good starting point for NLP in\nthe clinical domain. However, even more important might be the actual\ntraining procedure of the model. Thus, we discuss standard and random\nsplits, as well as ensemble models over these splits in the following.\nStandard splits. Typically, each dataset is divided into training,\ndevelopment and test splits. The training split is used each epoch to train\nthe model parameters and the best training epoch is selected based on the\nevaluation score on the development set. Finally, the held-out test set is\nused by the selected model to compute the ﬁnal score. These data splits\nare helpful to compare performances of different models on standardized\ndata, however, using the standard training split without modiﬁcations may\nnot result in optimal performance (Gorman and Bedrick, 2019).\nFurther random splits. The training and development parts can be further\nrandomly divided into n separate parts. Then, n −1 parts can be used for\ntraining and one part as the validation set for early stopping similar to cross-\nfold validation. An ensemble based on models trained on the different data\nsplits should be more powerful than the single models as each of them\nencodes complementary knowledge which helps to reduce variance and\nbiases (Clark et al., 2019). In our experiments, we use n = 5so that we\nget 5 different settings with unique training sets and we train one model\nfor each setting. Note that we do not change or use the test set at all to\nensure comparability to previous results.\nTraining on all available instances. Recent works sometimes ﬁnds that\nthere is no need for a held-out development set and that these labeled\ninstances might be better used during the training. For example, Luoma\nand Pyysalo (2020) have shown that training on the combined training\nand development sets boosts performance for named entity recognition\nremarkably. By this, the model has access to the most data during training\nand model selection is based on the training loss. However, the training\nloss is not as meaningful as a stopping criterion and its hard to pick the\nbest model checkpoint. We will compare to this method as an alternative\nto our split-based experiments.\n3.4 Transfer Learning\nMany NLP tasks suffer from a lack of labeled data. This includes non-\nstandard domains like the clinical domain in particular. One solution to\nimprove performance in these domains is the usage of resources from a\nrelated task in a transfer process. For example, Hofer et al. (2018) have\nshown that few-shot NER in the biomedical domain can be improved by\ntransferring trained weights from a similar task. We perform a similar kind\nof model transfer by transferring the transformer to the new target.\nHowever, not all transfer sources are actually useful as many can lead\nto negative transfer (Lange et al., 2021b). Thus, we ﬁrst have to predict\na suitable transfer source. We follow Lange et al. (2021b) and compute\n4 Lange et al.\nTable 1. Statistics of the Spanish datasets.\nCorpus Size (#Sentences)\nTrain Dev Test\nMeddocan (Marimon et al., 2019) 15,858 8,283 8,009\nPharmaconer (Gonzalez-Agirre et al., 2019) 8,582 4,016 4,184\nCantemist (Miranda-Escalada et al., 2020) 19,426 18,172 11,196\nMeddoprof (Lima-López et al., 2021) 51,350 - 10,008\nTable 2. Statistics of the English datasets.\nCorpus Size (#Sentences)\nTrain Dev Test\ni2b2 2006 (Uzuner et al., 2007) 51,429 - 18,770\ni2b2 2010 (Uzuner et al., 2011) 16,487 - 27,882\ni2b2 2012 (Sun et al., 2013) 7,636 - 5,785\ni2b2 2014 (Stubbs et al., 2015) 52,026 - 33,317\nsimilarities between our datasets using their proposed model similarity\nmeasure. This has been shown to work well across different tasks and\ndomains. The similarity between two models is computed based on the\nneural feature representations for the target datasets between two task-\nspeciﬁc trained models. In our experiments, we study the effect of transfer\nfrom different sources in comparison to standard single-task training.\nFurther, we will investigate this kind of transfer in low-resource settings,\nwhen the target task has only limited training resources.\nEnsembles over models. In addition to the other methods, ensembling can\nbe used to combine multiple model predictions into one. This ensemble\nis usually better than a single model – in particular if the models or their\ntraining data differ to some degree. We either create ensembles by majority\nvoting (Clark et al., 2019) of training runs that vary by their random seed\n(standard splits) or their training data (random splits).\n4 Results\nThis section describes the experimental setup starting with tasks, datasets\nand implementation details, and discusses the results for our experiments.\n4.1 Tasks and Datasets\nMany datasets for natural language processing in specialized domains are\npublished in the context of shared tasks – competitions to evaluate different\nsystems and approaches. Besides English, the clinical domain is well\naddressed for Spanish, and there exists an active community of researchers\nfor natural language processing of Spanish clinical texts. Thus, in the\ncontext of the IberLEF workshop series (Iberian Language Evaluation\nForum), several shared tasks have been proposed by the Barcelona\nSupercomputing Center concerning concept extraction in the clinical\ndomain (Marimon et al., 2019; Gonzalez-Agirre et al., 2019; Miranda-\nEscalada et al., 2020; Lima-López et al., 2021). In addition to datasets of\nthese shared tasks for Spanish, we consider four English datasets published\nduring a series of shared tasks of the i2b2 project (Uzuneret al., 2007, 2011;\nSun et al., 2013; Stubbs et al., 2015). Information on the dataset sizes are\ngiven in Table 1 and 2 for Spanish and English, respectively. Note that\nthe Meddoprof and i2b2 2012 corpora consist of two different extraction\ntasks each. Thus, we consider both tracks as separated tasks in this work\nresulting in a total of ten tasks. Following the evaluations in the shared\ntasks, we use the strict micro F1 for all datasets as evaluation metric.\nTable 3. Overview of different models averaged for the two languages ( F1).\nWord embeddings are used in a RNN model similar to Akbik et al. (2018).\nTransformers are used with a classiﬁcation layer similar to Devlin et al. (2019).\nPre-training Domain Model English Spanish\nGeneral\n(e.g., Web, News,\nWikipedia, ...)\nword2vec 80.26 78.20\nﬂair 85.15 80.28\nBERT (En) 85.34 77.78\nBETO (Es) 83.57 83.92\nXLM-R 87.13 83.87\nClinical\nword2vec 80.98 79.72\nﬂair 86.43 80.72\nClinicalBERT (En) 85.76 76.94\nCLIN-XEN 87.67 84.57\nCLIN-XES 87.48 85.37\n4.2 Experimental Setup and Implementation Details\nMasked Language Modeling. We use eight NVIDIA V100 (32GB) GPUs\nfor pre-training theCLIN-X models. The training takes less than 1 day with\na batch size of 4 per device and a sequence length of up to 512 subwords.\nThe models were trained with the huggingface trainer for MLM.\nSequence Labeling. The sequence labeling models were trained on single\nNVIDIA V100 GPUs up to 20 hours depending on the dataset size. The\nmodels were trained using the ﬂair framework with the AdamW optimizer\nwith an initial learning rate of2.0e−5 and a batch size of 16 for 20 epochs.\nThe model selection was performed on the development score if trained\non standard or random splits or the training loss otherwise.\nTransfer and Low-Resource Experiments. The median model according\nto the development score on the source dataset was taken for transfer and\nused for the initialization of the target model. Except for the initialization,\nthe training was identical to the single task training. The low-resource\nsettings were created by limiting the data splits to the ﬁrst n sentences\nwithout shufﬂing. The test set is not changed and remains identical.\n4.3 Evaluation of Embeddings\nThe choice of input embeddings has a large impact on downstream\nperformance and may be the most important factor. Table 3 shows the\naverage performance of several different embeddings and transformer\nmodels for the two languages. As expected, the monolingual transformers\n(BERT, BETO) excel at their target language, but cannot compete\nwith multilingual models (mBERT, XLM-R) when applied to an unseen\nlanguage. The lower part of Table 3 lists domain-speciﬁc variants of the\nembeddings which are generally more powerful in our domain-speciﬁc\nsetting. We see that our CLIN-X models perform best for their respective\nlanguages. Furthermore, the CLIN-XES performs almost as well as the\nCLIN-XEN model on the English datasets, for which it was not explicitly\ntrained. This shows, that the domain adaptation of multilingual models\ncan also help for texts from other languages of the same domain. Due to\nCLIN-XES stable performance across all tasks and languages, we will use\nthis model for the following ablations and transfer experiments.\n4.4 Evaluation of Training Methods\nThe foundation for all following concept extraction models is the\nCLIN-XES transformer, as it has shown robust results across all tasks.\nFor comparison to ﬁxed standard splits, we train the models on different\nrandom splits. We see in Table 4 that in particular ensembles over random\nCLIN-X 5\nTable 4. Comparison of training splits with our model architecture and ablation\nstudy of the model components averaged for each language (F1).\nMethod English Spanish\nAll 87.83 86.46\nStandard\nSplits\nMedian model 87.63 85.16\nBest model 87.85 85.99\nEnsemble 87.95 86.06\nRandom\nSplits\nMedian model 87.69 86.17\nBest model 88.31 86.85\nEnsemble 88.78 88.15\nAblation\nStudy\n– BIOSE Labels 88.52 87.13\n– CRF 88.38 85.95\n– Context 87.83 86.84\n– Subword NER 87.38 86.81\nTable 5. Cross-task transfer results for few-shot settings for the English corpora\n(F1). The predicted transfer source and the best models are highlighted.\n# training sentences\nTgt. Src. / Setting 250 500 1000 2500 7500 All\ni2b2 2006\nNo Transfer 71.24 81.06 84.15 95.49 96.89 98.34\ni2b2 2010 81.55 90.38 89.09 95.61 97.47 96.88\ni2b2 2012-C 79.28 86.5 88.71 96.75 97.92 98.23\ni2b2 2012-T 71.58 80.31 83.29 95.87 97.97 97.41\ni2b2 2014 87.52 90.86 91.87 97.11 97.95 98.50\ni2b2 2010\nNo Transfer 65.38 74.96 82.59 85.54 88.48 89.10\ni2b2 2006 68.90 78.32 82.07 85.70 87.95 88.69\ni2b2 2012-C 83.99 86.25 86.88 88.46 89.34 89.74\ni2b2 2012-T 69.49 74.92 81.31 85.35 88.25 88.65\ni2b2 2014 72.05 79.11 82.49 85.54 87.69 88.80\ni2b2 2012-C\nNo Transfer 69.09 73.21 75.70 78.03 80.36 80.42\ni2b2 2006 68.83 72.14 75.34 77.86 79.25 80.15\ni2b2 2010 76.39 77.98 79.44 80.90 81.65 80.93\ni2b2 2012-T 65.30 69.61 73.30 75.88 80.25 80.12\ni2b2 2014 68.67 72.56 75.39 77.96 79.98 79.83\ni2b2 2012-T\nNo Transfer 67.49 72.67 75.44 78.00 78.33 78.48\ni2b2 2006 68.57 72.49 74.34 77.73 78.43 78.34\ni2b2 2010 68.10 74.04 78.01 78.98 79.29 79.60\ni2b2 2012-C 70.17 75.04 76.36 78.12 78.54 80.03\ni2b2 2014 69.44 72.66 75.04 77.88 78.86 79.36\ni2b2 2014\nNo Transfer 64.96 81.61 85.74 92.70 96.08 97.62\ni2b2 2006 81.50 85.76 88.96 93.51 96.04 97.46\ni2b2 2010 71.72 83.55 87.81 93.18 96.14 97.17\ni2b2 2012-C 71.24 82.97 87.09 93.15 96.13 97.33\ni2b2 2012-T 69.12 81.25 85.08 91.35 96.02 97.00\nsplits are a lot better than the standard splits and also all training instances.\nWhile the median performance is roughly similar for all methods, the\nrandom splits offer a lot more variety in training instances and allow\nfor better maximum performance models. Thus, the ensemble based on\nrandom splits achieves also much higher numbers.\nTable 6. Cross-task transfer results for few-shot settings for the Spanish corpora\n(F1). The predicted transfer source and the best models are highlighted.\n# training sentences\nTgt. Src. / Setting 250 500 1000 2500 7500 All\nCantemist\nNo Transfer 51.68 59.00 67.35 77.15 84.10 88.24\nMeddocan 56.48 59.51 69.33 76.57 83.43 88.00\nMeddoprof-N 52.06 59.26 67.18 77.27 83.05 87.74\nMeddoprof-C 53.94 55.41 65.71 76.65 83.20 88.00\nPharmaconer 55.53 59.14 66.78 76.44 83.39 87.95\nMeddocan\nNo Transfer 84.00 92.01 95.28 96.48 97.20 98.00\nCantemist 83.61 89.36 95.35 96.75 97.43 97.57\nMeddoprof-N 86.99 92.77 93.55 96.15 97.01 97.66\nMeddoprof-C 88.70 93.76 95.03 96.32 97.35 97.73\nPharmaconer 92.74 94.30 96.16 96.84 97.49 97.65\nMeddoprof-N\nNo Transfer 13.99 44.28 51.24 58.95 72.54 81.68\nCantemist 10.01 38.41 50.64 62.66 71.74 79.77\nMeddocan 16.39 45.30 52.89 62.25 73.30 81.38\nMeddoprof-C 61.29 68.37 72.83 72.88 78.04 81.88\nPharmaconer 23.72 44.91 52.90 60.53 73.35 81.07\nMeddoprof-C\nNo Transfer 16.46 24.28 47.67 54.66 68.68 80.54\nCantemist 10.99 29.73 49.20 52.75 66.57 78.76\nMeddocan 31.83 38.01 53.80 56.46 69.98 79.33\nMeddoprof-N 57.46 57.70 61.56 64.92 72.37 79.38\nPharmaconer 22.61 35.15 50.50 53.49 69.59 79.08\nPharmaconer\nSinlge-Task 67.71 76.38 81.32 87.68 91.31 92.27\nCantemist 60.34 71.77 79.45 86.77 90.61 92.35\nMeddocan 74.48 76.02 82.79 88.39 91.49 92.27\nMeddoprof-N 69.48 76.44 78.73 88.60 92.02 91.98\nMeddoprof-C 69.25 74.15 80.13 88.27 91.80 92.29\n4.5 Evaluation of Concept Extraction Models\nThe lower part of Table 4 lists an ablation study of our individual\nmodel components. For example, adding cross-sentence context to the\ntransformers boosts performance across all tasks by 0.5 F1 on average.\nPerforming concept extraction on the subword level helps even further.\nThis is particularly helpful considering that no external tokenization is\nneeded, which can be challenging in the clinical domain (Lange et al.,\n2020). The CRF helps for both languages, though the differences are\nlarger for Spanish, as the two MEDDOPROF tasks have particularly long\nannotations (2.53 tokens per annotation on average). The same holds\nfor the BIOSE labels, that have the smallest impact of all components,\nbut consistently improve upon the standard BIO labels. As each of our\nproposed methods improves the transformer even further, we use the\ncombination of all methods in the following as our model architecture.\n4.6 Evaluation of Transfer Learning\nIn addition to the training based on random splits, we explore the effects\nof transfer learning. For this, we simulate low-resource settings where we\nlimit the annotated data of the target dataset between 250 labeled sentences\nup to 7500 sentences, roughly the size of the smallest corpus. The results\nare given in Table 5 and Table 6 for English and Spanish, respectively.\nLarge positive transfer happens in most settings, particularly for the\nlow-resource settings with up to (+47.3 F1 points) for Meddoprof when\nonly 250 labeled sentences are available. The improvements in the full\ndata scenario are below 1 F1. However, there is also negative transfer, in\nparticular using i2b2 2012-T and Cantemist datasets as transfer sources\n6 Lange et al.\nTable 7. Comparison to baseline systems and state-of-the-art results (F1). We highlight statistically signiﬁcant differences between CLIN-XES +OurArchitecture\nwith and without transfer following the signiﬁcant codes of R: ***p-value ≤ 0.001; ** p-value < 0.01; * p-value < 0.05; †highlights our ClinicalBERT results.\nEnglish (i2b2) Spanish\nModel 2006 2010 2012-C 2012-T 2014 Cantemist Meddocan M.prof-N M.prof-C Pharma.\nBERT/BETO (monolingual) 94.80 85.25 76.51 75.28 94.86 81.30 96.81 79.19 74.59 87.70\nBERT (multilingual) 94.79 84.91 76.01 76.56 95.34 80.94 96.30 76.39 71.84 86.98\nXLM-R (multilingual) 96.72 87.54 79.63 75.36 96.39 82.17 96.76 77.44 74.05 88.92\nHunFlair (monolingual) 93.48 86.70 78.52 77.16 95.90 83.80 96.50 75.16 70.01 88.40\nClinicalBERT 94.8 87.8 78.9 76.58 † 93.0 77.18† 94.63† 65.74† 62.85† 84.32†\nNLNDE - - - - - 85.3 96.96 81.8 79.3 88.6\nCLIN-XEN 96.25 88.10 79.58 77.70 96.73 82.80 97.08 78.62 75.05 89.33\nCLIN-XES 95.49 87.94 79.58 77.57 96.80 83.22 97.08 79.54 76.95 90.05\nCLIN-XEN +OurArchitecture 98.49 89.23 80.62 78.50 97.60 87.72 97.57 81.36 78.53 92.36\nCLIN-XES +OurArchitecture 98.30 89.10 80.42 78.48 97.62* 88.24 98.00 81.68 80.54 92.27\nCLIN-XES +OurArchitecture +Transfer 98.50* 89.74*** 80.93** 79.60* 97.46 88.00 97.65 81.88 79.38 92.27\noften result in negative transfer. The source selection is also crucial in\nlow-resource scenarios, as not every source is equally beneﬁcial. Using the\nmodel similarity measure from Langeet al. (2021b) we are able to predict\ngood transfer sources in all settings; often the best source is selected.\n4.7 Comparison to State-of-the-Art Models\nAs our results demonstrate, we have proposed a robust model for the\nclinical domain that works well across the different tasks in both languages.\nFinally, we compareCLIN-X to various transformer models as introduced\nearlier. We also compare to HunFlair (Weber et al., 2021), the current\nstate-of-the-art for concept extraction in the biomedical domain. We use\ntheir model architecture based on clinical ﬂair and fasttext embeddings and\ntrain models accordingly on our datasets. In addition, we compare to our\nNLNDE submissions for the Spanish shared tasks and the ClinicalBERT\nby Alsentzer et al. (2019) for the English datasets.\nThe results for each task are shown in Table 7. The CLIN-X language\nmodels in combination with our model architecture outperform the other\ntransformers and HunFlair by a large margin.CLIN-X is able to utilize the\ndomain knowledge obtained from the additional pre-training with further\nimprovements from the ensembling over random splits. Even though\nCLIN-X works best in combination with our model architecture, CLIN-X\nbased on the standard transformer architecture with a single classiﬁcation\nlayer already outperforms the existing models on 8 out of 10 tasks.\nWe tested statistical signiﬁcance betweenCLIN-XES with and without\ntransfer learning – highlighted with asterisks in Table 7. We ﬁnd that\nall differences for English are signiﬁcant, while only one difference for\nSpanish is signiﬁcant. This might indicate the complementary relationship\nof domain adaptation and model transfer learning. As CLIN-X was\nexplicitly adapted to Spanish, additional transfer is not necessary in high-\nresource settings. In contrast, the cross-language domain adaptation for\nEnglish can still be improved with transfer from related sources, where\nCLIN-XES +Transfer has also notably higher performances in 3 out of 5\nsettings compared to CLIN-XEN which is adapted to English.\n5 Conclusion\nIn this paper, we described the newly pre-trainedCLIN-X language models\nfor the clinical domain. We have shown thatCLIN-X sets the new state the\nof the art results for ten clinical concept extraction tasks in two languages.\nWe demonstrated the positive impact of other model components, such\nas ensembles over random splits and cross-sentence context and we have\nstudied the effects of cross-task transfer learning from different clinical\ncorpora. Using a model similarity measure, we found good transfer\nsources for almost all datasets in general and for low-resource scenarios\nin particular. We are convinced that the new CLIN-X language models\nwill help boosting performance for various Spanish and English clinical\ninformation extraction tasks with our or other model architectures.\nReferences\nAkbik, A. et al. (2018). Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International Conference on\nComputational Linguistics, pages 1638–1649, Santa Fe, New Mexico,\nUSA. ACL.\nAlsentzer, E. et al. (2019). Publicly available clinical BERT embeddings.\nIn Proceedings of the 2nd Clinical Natural Language Processing\nWorkshop (Clin-NLP), pages 72–78, Minneapolis, Minnesota, USA.\nACL.\nBeltagy, I. et al. (2019). SciBERT: A pretrained language model for\nscientiﬁc text. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages\n3615–3620, Hong Kong, China. ACL.\nClark, C. et al. (2019). Don’t take the easy way out: Ensemble based\nmethods for avoiding known dataset biases. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4069–4082, Hong Kong, China. ACL.\nCollobert, R. et al. (2011). Natural language processing (almost) from\nscratch. J. Mach. Learn. Res., 12, 2493–2537.\nConneau, A. et al. (2020). Unsupervised cross-lingual representation\nlearning at scale. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics (ACL) , pages 8440–8451,\nOnline. ACL.\nDevlin, J. et al. (2019). BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (NAACL-\nHLT), pages 4171–4186, Minneapolis, Minnesota. ACL.\nFinkel, J. et al. (2004). Exploiting context for biomedical entity\nrecognition: From syntax to the web. InProceedings of the International\nJoint Workshop on Natural Language Processing in Biomedicine and\nits Applications (NLPBA/BioNLP), pages 91–94, Geneva, Switzerland.\nCLIN-X 7\nInternational Committee on Computational Linguistics.\nFriedrich, A.et al.(2020). The SOFC-exp corpus and neural approaches to\ninformation extraction in the materials science domain. In Proceedings\nof the 58th Annual Meeting of the Association for Computational\nLinguistics (ACL), pages 1255–1268, Online. ACL.\nGonzalez-Agirre, A. et al. (2019). PharmaCoNER: Pharmacological\nsubstances, compounds and proteins named entity recognition track.\nIn Proceedings of The 5th Workshop on BioNLP Open Shared Tasks\n(BioNLP-OST), pages 1–10, Hong Kong, China. ACL.\nGorman, K. and Bedrick, S. (2019). We need to talk about standard\nsplits. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL) , pages 2786–2791, Florence, Italy.\nACL.\nGururangan, S. et al. (2020). Don’t stop pretraining: Adapt language\nmodels to domains and tasks. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics (ACL) , pages 8342–\n8360, Online. ACL.\nHaynes, R. B. et al. (2005). Optimal search strategies for retrieving\nscientiﬁcally strong studies of treatment from medline: analytical survey.\nBmj, 330, 1179.\nHofer, M. et al. (2018). Few-shot learning for named entity recognition in\nmedical text. arXiv preprint arXiv:1811.05468.\nKhan, M. R. et al. (2020). Mt-bioner: Multi-task learning for biomedical\nnamed entity recognition using deep bidirectional transformers. arXiv\npreprint arXiv:2001.08904.\nKim, H. and Kang, J. (2021). How do your biomedical named entity\nmodels generalize to novel entities? arXiv preprint arXiv:2101.00160.\nLafferty, J. D. et al. (2001). Conditional random ﬁelds: Probabilistic\nmodels for segmenting and labeling sequence data. In Proceedings of\nthe Eighteenth International Conference on Machine Learning , ICML\n’01, pages 282–289, San Francisco, CA, USA.\nLange, L. et al. (2019). NLNDE: The neither-language-nor-domain-\nexperts’ way of spanish medical document de-identiﬁcation. In\nProceedings of The Iberian Languages Evaluation Forum (IberLEF) ,\nCEUR Workshop Proceedings.\nLange, L. et al. (2020). NLNDE at CANTEMIST: neural sequence\nlabeling and parsing approaches for clinical concept extraction. In\nProceedings of The Iberian Languages Evaluation Forum (IberLEF) ,\nCEUR Workshop Proceedings.\nLange, L. et al. (2021a). Boosting transformers for job expression\nextraction and classiﬁcation in a low-resource setting. InProceedings of\nThe Iberian Languages Evaluation Forum (IberLEF), CEUR Workshop\nProceedings.\nLange, L. et al. (2021b). To share or not to share: Predicting sets of sources\nfor model transfer learning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pages\n8744–8753, Online and Punta Cana, Dominican Republic. ACL.\nLeaman, R. et al. (2015). Challenges in clinical natural language\nprocessing for automated disorder normalization. J. Biomed. Inform. ,\n57, 28–37.\nLee, J. et al. (2020). Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics, 36,\n1234—1240.\nLee, J. Y . et al. (2018). Transfer learning for named-entity recognition\nwith neural networks. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation (LREC), Miyazaki,\nJapan. European Language Resources Association.\nLima-López, S. et al. (2021). Nlp applied to occupational health:\nMeddoprof shared task at iberlef 2021 on automatic recognition,\nclassiﬁcation and normalization of professions and occupations from\nmedical texts. In Proceedings of the Iberian Languages Evaluation\nForum (IberLEF), CEUR Workshop Proceedings.\nLuoma, J. and Pyysalo, S. (2020). Exploring cross-sentence contexts\nfor named entity recognition with BERT. In Proceedings of the 28th\nInternational Conference on Computational Linguistics (COLING) ,\npages 904–914, Barcelona, Spain (Online). International Committee on\nComputational Linguistics.\nMarimon, M. et al. (2019). Automatic de-identiﬁcation of medical texts in\nspanish: the meddocan track, corpus, guidelines, methods and evaluation\nof results. In Proceedings of the Iberian Languages Evaluation Forum\n(IberLEF). CEUR Workshop Proceedings.\nMiranda-Escalada, A. et al. (2020). Named entity recognition, concept\nnormalization and clinical coding: Overview of the cantemist track for\ncancer text mining in spanish, corpus, guidelines, methods and results.\nIn Proceedings of the Iberian Languages Evaluation Forum (IberLEF),\nCEUR Workshop Proceedings.\nMulyar, A. et al. (2021). Mt-clinical bert: scaling clinical information\nextraction with multitask learning. J. Am. Med. Inform. Assoc. , 28,\n2108–2115.\nNaseem, U. et al. (2021). Bioalbert: A simple and effective pre-trained\nlanguage model for biomedical named entity recognition. In 2021\nInternational Joint Conference on Neural Networks (IJCNN), pages 1–7.\nIEEE.\nPeng, Y .et al. (2019). Transfer learning in biomedical natural language\nprocessing: An evaluation of BERT and ELMo on ten benchmarking\ndatasets. In Proceedings of the 18th BioNLP Workshop and Shared Task\n(BioNLP), pages 58–65, Florence, Italy. ACL.\nPeters, M. E. et al. (2018). Deep contextualized word representations.\nIn Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT), pages 2227–2237, New Orleans,\nLouisiana. ACL.\nPhan, M. C. et al. (2019). Robust representation learning of biomedical\nnames. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL), pages 3275–3285, Florence, Italy.\nACL.\nSchweter, S. and Akbik, A. (2020). Flert: Document-level features for\nnamed entity recognition. arXiv preprint arXiv:2011.06993.\nStubbs, A. et al. (2015). Automated systems for the de-identiﬁcation of\nlongitudinal clinical narratives: Overview of 2014 i2b2/uthealth shared\ntask track 1. J. Biomed. Inform., 58, 11–19.\nSun, W. et al. (2013). Evaluating temporal relations in clinical text: 2012\ni2b2 challenge. J. Am. Med. Inform. Assoc., 20, 806–813.\nUzuner, Ö. et al. (2007). Evaluating the state-of-the-art in automatic\nde-identiﬁcation. J. Am. Med. Inform. Assoc., 14, 550–563.\nUzuner, Ö. et al. (2011). 2010 i2b2/va challenge on concepts, assertions,\nand relations in clinical text. J. Am. Med. Inform. Assoc., 18, 552–556.\nVillegas, M. et al. (2018). The mespen resource for english-spanish\nmedical machine translation and terminologies: census of parallel\ncorpora, glossaries and term translations. LREC MultilingualBIO.\nWang, Y .et al. (2018). Clinical information extraction applications: a\nliterature review. J. Biomed. Inform., 77, 34–49.\nWeber, L. et al. (2019). HUNER: improving biomedical NER with\npretraining. Bioinformatics, 36, 295–302.\nWeber, L. et al. (2021). HunFlair: an easy-to-use tool for state-of-the-art\nbiomedical named entity recognition. Bioinformatics, 37, 2792–2794."
}