{
  "title": "One billion word benchmark for measuring progress in statistical language modeling",
  "url": "https://openalex.org/W1539309091",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2198452902",
      "name": "Ciprian Chelba",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A292626543",
      "name": "Tomas Mikolov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2568636091",
      "name": "Mike Schuster",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136972167",
      "name": "Qi Ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A251388693",
      "name": "Thorsten Brants",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2501826298",
      "name": "Phillipp Koehn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133654089",
      "name": "Tony Robinson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2091812280",
    "https://openalex.org/W36903255",
    "https://openalex.org/W10704533",
    "https://openalex.org/W2397435450",
    "https://openalex.org/W2026383756",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2020382207",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W161283806",
    "https://openalex.org/W2071315630",
    "https://openalex.org/W62305045",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2158148237",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W115367774",
    "https://openalex.org/W2072223048",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2050971845"
  ],
  "abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.",
  "full_text": "arXiv:1312.3005v3  [cs.CL]  4 Mar 2014\nOne Billion Word Benchmark for Measuring Progress in\nStatistical Language Modeling\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants\nGoogle\n1600 Amphitheatre Parkway\nMountain View, CA 94043, USA\nPhillipp Koehn\nUniversity of Edinburgh\n10 Crichton Street, Room 4.19\nEdinburgh, EH8 9AB, UK\nTony Robinson\nCantab Research Ltd\nSt Johns Innovation Centre\nCowley Road, Cambridge, CB4 0WS, UK\nAbstract\nWe propose a new benchmark corpus to be\nused for measuring progress in statistical lan-\nguage modeling. With almost one billion\nwords of training data, we hope this bench-\nmark will be useful to quickly evaluate novel\nlanguage modeling techniques, and to com-\npare their contribution when combined with\nother advanced techniques. We show perfor-\nmance of several well-known types of lan-\nguage models, with the best results achieved\nwith a recurrent neural network based lan-\nguage model. The baseline unpruned Kneser-\nNey 5-gram model achieves perplexity 67.6.\nA combination of techniques leads to 35%\nreduction in perplexity, or 10% reduction in\ncross-entropy (bits), over that baseline.\nThe benchmark is available as a\ncode.google.com project; besides the\nscripts needed to rebuild the training/held-out\ndata, it also makes available log-probability\nvalues for each word in each of ten held-out\ndata sets, for each of the baseline n-gram\nmodels.\n1 Introduction\nStatistical language modeling has been applied to a\nwide range of applications and domains with great\nsuccess. To name a few, automatic speech recogni-\ntion, machine translation, spelling correction, touch-\nscreen “soft” keyboards and many natural language\nprocessing applications depend on the quality of lan-\nguage models (LMs).\nThe performance of LMs is determined mostly by\nseveral factors: the amount of training data, quality\nand match of the training data to the test data, and\nchoice of modeling technique for estimation from\nthe data. It is widely accepted that the amount of\ndata, and the ability of a given estimation algorithm\nto accomodate large amounts of training are very im-\nportant in providing a solution that competes suc-\ncessfully with the entrenched n-gram LMs. At the\nsame time, scaling up a novel algorithm to a large\namount of data involves a large amount of work, and\nprovides a signiﬁcant barrier to entry for new mod-\neling techniques. By choosing one billion words as\nthe amount of training data we hope to strike a bal-\nance between the relevance of the benchmark in the\nworld of abundant data, and the ease with which any\nresearcher can evaluate a given modeling approach.\nThis follows the work of Goodman (2001a), who\nexplored performance of various language modeling\ntechniques when applied to large data sets. One of\nthe key contributions of our work is that the experi-\nments presented in this paper can be reproduced by\nvirtually anybody with an interest in LM, as we use\na data set that is freely available on the web.\nAnother contribution is that we provide strong\nbaseline results with the currently very popular neu-\nral network LM (Bengio et al., 2003). This should\nallow researchers who work on competitive tech-\nniques to quickly compare their results to the current\nstate of the art.\nThe paper is organized as follows: Section 2 de-\nscribes how the training data was obtained; Section\n3 provides a short overview of the language model-\ning techniques evaluated; ﬁnally, Section 4 presents\nresults obtained and Section 5 concludes the paper.\nModel Num. Params Training Time Perplexity\n[billions] [hours] [CPUs]\nInterpolated KN 5-gram, 1.1B n-grams (KN) 1.76 3 100 67.6\nKatz 5-gram, 1.1B n-grams 1.74 2 100 79.9\nStupid Backoff 5-gram (SBO) 1.13 0.4 200 87.9\nInterpolated KN 5-gram, 15M n-grams 0.03 3 100 243.2\nKatz 5-gram, 15M n-grams 0.03 2 100 127.5\nBinary MaxEnt 5-gram (n-gram features) 1.13 1 5000 115.4\nBinary MaxEnt 5-gram (n-gram + skip-1 features)1.8 1.25 5000 107.1\nHierarchical Softmax MaxEnt 4-gram (HME) 6 3 1 101.3\nRecurrent NN-256 + MaxEnt 9-gram 20 60 24 58.3\nRecurrent NN-512 + MaxEnt 9-gram 20 120 24 54.5\nRecurrent NN-1024 + MaxEnt 9-gram 20 240 24 51.3\nTable 1: Results on the 1B Word Benchmark test set with various types of language models.\n2 Description of the Benchmark Data\nIn the following experiments, we used text data ob-\ntained from the WMT11 website1. The data prepa-\nration process was performed as follows:\n• All training monolingual/English corpora were\nselected\n• Normalization and tokenization was performed\nusing scripts distributed from the WMT11 site,\nslightly augmented to normalize various UTF-8\nvariants for common punctuation, e.g.'\n• Duplicate sentences were removed, dropping\nthe number of words from about 2.9 bil-\nlion to about 0.8 billion (829250940, more\nexactly, counting sentence boundary markers\n<S>,<\\S>)\n• V ocabulary (793471 words including sentence\nboundary markers <S>, <\\S>) was con-\nstructed by discarding all words with count be-\nlow 3\n• Words outside of the vocabulary were mapped\nto<UNK>token, also part of the vocabulary\n• Sentence order was randomized, and the data\nwas split into 100 disjoint partitions\n• One such partition (1%) of the data was chosen\nas the held-out set\n• The held-out set was then randomly shufﬂed\nand split again into 50 disjoint partitions to be\nused as development/test data\n1http://statmt.org/wmt11/training-monolingual.tgz\n• One such resulting partition (2%, amounting to\n159658 words without counting the sentence\nbeginning marker<S>which is never predicted\nby the language model) of the held-out data\nwere used as test data in our experiments; the\nremaining partitions are reserved for future ex-\nperiments\n• The out-of-vocabulary (OoV) rate on the test\nset was 0.28%.\nThe benchmark is available\nas a code.google.com project:\nhttps://code.google.com/p/1-billion-word-langua\nBesides the scripts needed to rebuild the\ntraining/held-out data, it also makes available\nlog-probability values for each word in each of ten\nheld-out data sets, for each of the baseline n-gram\nmodels.\nBecause the original data had already randomized\nsentence order, the benchmark is not useful for ex-\nperiments with models that capture long context de-\npendencies across sentence boundaries.\n3 Baseline Language Models\nAs baselines we chose to use (Katz, 1995), and\nInterpolated (Kneser and Ney, 1995) (KN) 5-gram\nLMs, as they are the most prevalent. Since in\npractice these models are pruned, often quite ag-\ngressivley, we also illustrate the negative effect of\n(Stolcke, 1998) entropy pruning on both models,\nsimilar to (Chelba et al., 2010). In particular KN\nsmoothing degrades much more rapidly than Katz,\ncalling for a discerning choice in a given applica-\ntion.\n4 Advanced Language Modeling\nTechniques\nThe number of advanced techniques for statistical\nlanguage modeling is very large. It is out of scope\nof this paper to provide their detailed description,\nbut we mention some of the most popular ones:\n• N-grams with Modiﬁed Kneser-Ney smooth-\ning (Chen and Goodman, 1996)\n• Cache (Jelinek et al., 1991)\n• Class-based (Brown et al., 1992)\n• Maximum entropy (Rosenfeld, 1994)\n• Structured (Chelba and Jelinek, 2000)\n• Neural net based (Bengio et al., 2003)\n• Discriminative (Roark et al., 2004)\n• Random forest (Xu, 2005)\n• Bayesian (Teh, 2006)\nBelow, we provide a short description of models\nthat we used in our comparison using the benchmark\ndata.\n4.1 Normalized Stupid Backoff\nThe Stupid Backoff LM was proposed\nin (Brants et al., 2007) as a simpliﬁed version\nof backoff LM, suited to client-server architectures\nin a distributed computing environment.It does not\napply any discounting to relative frequencies, and\nit uses a single backoff weight instead of context-\ndependent backoff weights. As a result, the Stupid\nBackoff model does not generate normalized prob-\nabilities. For the purpose of computing perplexity\nas reported in Table 1, values output by the model\nwere normalized over the entire LM vocabulary.\n4.2 Binary Maximum Entropy Language\nModel\nThe Binary MaxEnt model was proposed\nin (Xu et al., 2011) and aims to avoid the ex-\npensive probability normalization during training by\nusing independent binary predictors. Each predictor\nis trained using all the positive examples, but the\nnegative examples are dramatically down-sampled.\nThis type of model is attractive for parallel training,\nthus we explored its performance further.\nWe trained two models with a sampling rate of\n0.001 for negative examples, one uses n-gram fea-\ntures only and the other uses n-gram and skip-1 n-\ngram features. We separated the phases of generat-\ning negative examples and tuning model parameters\nsuch that the output of the ﬁrst phase can be shared\nby two models. The generation of the negative ex-\namples took 7.25 hours using 500 machines, while\ntuning the parameters using 5000 machines took 50\nminutes, and 70 minutes for the two models, respec-\ntively.\n4.3 Maximum Entropy Language Model with\nHierarchical Softmax\nAnother option to reduce training complexity of\nthe MaxEnt models is to use a hierarchical soft-\nmax (Goodman, 2001b; Morin and Bengio, 2005).\nThe idea is to estimate probabilities of groups of\nwords, like in a class based model – only the\nclasses that contain the positive examples need to\nbe evaluated. In our case, we explored a bi-\nnary Huffman tree representation of the vocabu-\nlary, such that evaluation of frequent words takes\nless time. The idea of using frequencies of words\nfor a hierarchical softmax was presented previously\nin (Mikolov et al., 2011a).\n4.4 Recurrent Neural Network Language\nModel\nThe Recurrent Neural Network (RNN) based LM\nhave recently achieved outstanding performance\non a number of tasks (Mikolov, 2012). It was\nshown that RNN LM signiﬁcantly outperforms\nmany other language modeling techniques on the\nPenn Treebank data set (Mikolov et al., 2011b). It\nwas also shown that RNN models scale very\nwell to data sets with hundreds of millions of\nwords (Mikolov et al., 2011c), although the reported\ntraining times for the largest models were in the or-\nder of weeks.\nWe cut down training times by a factor of 20-50\nfor large problems using a number of techniques,\nwhich allow RNN training in typically 1-10 days\nwith billions of words,> 1M vocabularies and up to\n20B parameters on a single standard machine with-\nout GPUs.\nModel Perplexity\nInterpolated KN 5-gram, 1.1B n-grams67.6\nAll models 43.8\nTable 2: Model combination on the 1B Word Benchmark test set.The weights were tuned to minimize perplexity on\nheld-out data. The optimal interpolation weights for the KN, rnn1024, rnn512, rnn256, SBO, HME were, respectively:\n0.06, 0.61, 0.13, 0.00, 0.20, 0.00.\nThese techniques were in order of importance:\na) Parallelization of training across available CPU\nthreads, b) Making use of SIMD instructions where\npossible, c) Reducing number of output parameters\nby 90%, d) Running a Maximum Entropy model in\nparallel to the RNN. Because of space limitations\nwe cannot describe the exact details of the speed-ups\nhere – they will be reported in an upcoming paper.\nWe trained several models with varying number\nof neurons (Table 1) using regular SGD with a\nlearning rate of 0.05 to 0.001 using 10 iterations over\nthe data. The MaxEnt models running in parallel\nto the RNN capture a history of 9 previous words,\nand the models use as additional features the previ-\nous 15 words independently of order. While train-\ning times approach 2 weeks for the most complex\nmodel, slightly worse models can be trained in a few\ndays. Note that we didn’t optimize for model size\nnor training speed, only test performance.\n5 Results\n5.1 Performance of Individual Models\nResults achieved on the benchmark data with vari-\nous types of LM are reported in Table 1. We fo-\ncused on minimizing the perplexity when choosing\nhyper-parameters, however we also report the time\nrequired to train the models. Training times are not\nnecessarily comparable as they depend on the under-\nlying implementation. Mapreduces can potentially\nprocess larger data sets than single-machine imple-\nmentations, but come with a large overhead of com-\nmunication and ﬁle I/O. Discussing details of the im-\nplementations is outside the scope as this paper.\n5.2 Model Combination\nThe best perplexity results were achieved by linearly\ninterpolating together probabilities from all models.\nHowever, only some models had signiﬁcant weight\nin the combination; the weights were tuned on the\nheld-out data. As can be seen in Table 2, the best\nperplexity is about 35% lower than the baseline - the\nmodiﬁed Kneser-Ney smoothed 5-gram model with\nno count cutoffs. This corresponds to about 10%\nreduction of cross-entropy (bits).\nSomewhat surprisingly the SBO model receives\na relatively high weight in the linear combination\nof models, despite its poor performance in perplex-\nity, whereas the KN baseline receives a fairly small\nweight relative to the other models in the combina-\ntion.\n6 Conclusion\nWe introduced a new data set for measuring re-\nsearch progress in statistical language modeling.\nThe benchmark data set is based on resources that\nare freely available on the web, thus fair comparison\nof various techniques is easily possible. The impor-\ntance of such effort is unquestionable: it has been\nseen many times in the history of research that sig-\nniﬁcant progress can be achieved when various ap-\nproaches are measurable, reproducible, and the bar-\nrier to entry is low.\nThe choice of approximately one billion words\nmight seem somewhat restrictive. Indeed, it can be\nhardly expected that new techniques will be immedi-\nately competitive on a large data set. Computation-\nally expensive techniques can still be compared us-\ning for example just the ﬁrst or the ﬁrst 10 partitions\nof this new dataset, corresponding to approx. 10 mil-\nlion and 100 million words. However, to achieve\nimpactful results in domains such as speech recogni-\ntion and machine translation, the language modeling\ntechniques need to be scaled up to large data sets.\nAnother contribution of this paper is the com-\nparison of a few novel modeling approaches when\ntrained on a large data set. As far as we know, we\nwere able to train the largest recurrent neural net-\nwork language model ever reported. The perfor-\nmance gain is very promising; the perplexity reduc-\ntion of 35% is large enough to let us hope for signif-\nicant improvements in various applications.\nIn the future, we would like to encourage other\nresearchers to participate in our efforts to make lan-\nguage modeling research more transparent. This\nwould greatly help to transfer the latest discover-\nies into real-world applications. In the spirit of a\nbenchmark our ﬁrst goal was to achieve the best pos-\nsible test perplexities regardless of model sizes or\ntraining time. However, this was a relatively lim-\nited collaborative effort, and some well known tech-\nniques are still missing. We invite other researchers\nto complete the picture by evaluating new, and well-\nknown techniques on this corpus. Ideally the bench-\nmark would also contain ASR or SMT lattices/N-\nbest lists, such that one can evaluate application spe-\nciﬁc performance as well.\nReferences\n[Bengio et al.2003] Y . Bengio, R. Ducharme, and P. Vin-\ncent. 2003.A neural probabilistic language model.\nJournal of Machine Learning Research, 3:1137-1155.\n[Brants et al.2007] T. Brants, A. C. Popat, P. Xu, F. J. Och,\nand J. Dean. 2007.Large language models in machine\ntranslation. In Proceedings of EMNLP.\n[Brown et al.1992] P. F. Brown, P. V . deSouza, R. L. Mer-\ncer, V . J. Della Pietra, and J. C. Lai. 1992.Class-\nBased n-gram Models of Natural Language. Compu-\ntational Linguistics, 18, 467-479.\n[Elman1990] J. Elman. 1990.Finding Structure in Time.\nCognitive Science, 14, 179-211.\n[Emami2006] A. Emami. 2006.A Neural Syntactic Lan-\nguage Model. Ph.D. thesis, Johns Hopkins University.\n[Goodman2001a] J. T. Goodman. 2001a. A bit of\nprogress in language modeling, extended version.\nTechnical report MSR-TR-2001-72.\n[Goodman2001b] J. T. Goodman. 2001b. Classes for\nfast maximum entropy training. In Proceedings of\nICASSP.\n[Jelinek et al.1991] F. Jelinek, B. Merialdo, S. Roukos,\nand M. Strauss. 1991.A Dynamic Language Model\nfor Speech Recognition. In Proceedings of the DARPA\nWorkshop on Speech and Natural Language.\n[Chelba and Jelinek2000] C. Chelba and F. Jelinek. 2000.\nStructured language modeling. Computer Speech &\nLanguage.\n[Chelba et al.2010] C. Chelba, T. Brants, W. Neveitt, and\nP. Xu. 2010. Study on Interaction between Entropy\nPruning and Kneser-Ney Smoothing. In Proceedings\nof Interspeech.\n[Chen and Goodman1996] S. F. Chen and J. T. Goodman.\n1996. An empirical study of smoothing techniques for\nlanguage modeling. In Proceedings of ACL.\n[Chen2009] S. F. Chen. 2009.Shrinking exponential lan-\nguage models. In Proceedings of NAACL-HLT.\n[Katz1995] S. Katz. 1987.Estimation of probabilities\nfrom sparse data for the language model component of\na speech recognizer. In IEEE Transactions on Acous-\ntics, Speech and Signal Processing.\n[Kneser and Ney1995] R. Kneser and H. Ney. 1995.Im-\nproved Backing-Off For M-Gram Language Modeling.\nIn Proceedings of ICASSP.\n[Mikolov et al.2010] T. Mikolov, M. Karaﬁ´ at, L. Burget,\nJ.ˇCernock´ y, and S. Khudanpur. 2010.Recurrent neu-\nral network based language model. In Proceedings of\nInterspeech.\n[Mikolov et al.2011a] T. Mikolov, S. Kombrink, L. Bur-\nget, J.ˇCernock´ y, and S. Khudanpur. 2011.Exten-\nsions oﬂ Recurrent Neural Network Language Model.\nIn Proceedings of ICASSP.\n[Mikolov et al.2011b] T. Mikolov, A. Deoras, S. Kom-\nbrink, L. Burget, and J.ˇCernock´ y. 2011a.Empirical\nEvaluation and Combination of Advanced Language\nModeling Techniques. In Proceedings of Interspeech.\n[Mikolov et al.2011c] T. Mikolov, A. Deoras, D. Povey,\nL. Burget, and J.ˇCernock´ y. 2011b.Strategies for\nTraining Large Scale Neural Network Language Mod-\nels. In Proceedings of ASRU.\n[Mikolov2012] T. Mikolov. 2012.Statistical Language\nModels based on Neural Networks. Ph.D. thesis, Brno\nUniversity of Technology.\n[Mnih and Hinton2007] A. Mnih and G. Hinton. 2007.\nThree new graphical models for statistical language\nmodelling. In Proceedings of ICML.\n[Morin and Bengio2005] F. Morin and Y . Bengio. 2005.\nHierarchical Probabilistic Neural Network Language\nModel. In Proceedings of AISTATS.\n[Roark et al.2004] B. Roark, M. Saralar, M. Collins, and\nM. Johnson. 2004.Discriminative language model-\ning with conditional random ﬁelds and the perceptron\nalgorithm. In Proceedings of ACL.\n[Rosenfeld1994] R. Rosenfeld. 1994.Adaptive Statis-\ntical Language Modeling: A Maximum Entropy Ap-\nproach. Ph.D. thesis, Carnegie Mellon University.\n[Rumelhart et al.1986] D. E. Rumelhart, G. E. Hinton,\nand R. J. Williams. 1986.Learning internal represen-\ntations by back-propagating errors. Nature, 323:533-\n536.\n[Schwenk2007] H. Schwenk. 2007. Continuous space\nlanguage models. Computer Speech and Language,\nvol. 21.\n[Stolcke1998] A. Stolcke. 1998.Entropy-based Pruning\nof Back-off Language Models. In Proceedings of News\nTranscription and Understanding Workshop.\n[Sundermeyer et al.2012] M. Sundermeyer, R. Schluter,\nand H. Ney. 2012.LSTM Neural Networks for Lan-\nguage Modeling. In Proceedings of Interspeech.\n[Teh2006] Y . W. Teh. 2006.A hierarchical Bayesian lan-\nguage model based on PitmanYor processes. In Pro-\nceedings of Coling/ACL.\n[Wu et al.2012] Y . Wu, H. Yamamoto, X. Lu, S. Matsuda,\nC. Hori, and H. Kashioka. 2012.Factored Recur-\nrent Neural Network Language Model in TED Lecture\nTranscription. In Proceedings of IWSLT.\n[Xu2005] Peng Xu. 2005.Random forests and the data\nsparseness problem in language modeling. Ph.D. the-\nsis, Johns Hopkins University.\n[Xu et al.2011] Puyang Xu, A. Gunawardana, and S.\nKhudanpur. 2011. Efﬁcient Subsampling for Train-\ning Complex Language Models. In Proceedings of\nEMNLP.\n[Zweig and Makarychev2013] G. Zweig and K.\nMakarychev. 2013. Speed Regularization and\nOptimality in Word Classing. In Proceedings of\nICASSP.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8484348058700562
    },
    {
      "name": "Language model",
      "score": 0.8420278429985046
    },
    {
      "name": "Computer science",
      "score": 0.737186074256897
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7082158923149109
    },
    {
      "name": "Baseline (sea)",
      "score": 0.663783848285675
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5262899994850159
    },
    {
      "name": "Natural language processing",
      "score": 0.5145217776298523
    },
    {
      "name": "Word (group theory)",
      "score": 0.4974868595600128
    },
    {
      "name": "Scripting language",
      "score": 0.47049757838249207
    },
    {
      "name": "Statistical model",
      "score": 0.42457112669944763
    },
    {
      "name": "Machine learning",
      "score": 0.40362584590911865
    },
    {
      "name": "Programming language",
      "score": 0.09696301817893982
    },
    {
      "name": "Mathematics",
      "score": 0.07797285914421082
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}