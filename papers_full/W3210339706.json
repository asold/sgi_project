{
    "title": "Detecting Gender Bias in Transformer-based Models: A Case Study on BERT",
    "url": "https://openalex.org/W3210339706",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1623828274",
            "name": "Li, Bingbing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4284263665",
            "name": "Peng, Hongwu",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Sainju, Rajat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2390107103",
            "name": "Yang, Junhuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2037538644",
            "name": "Yang Lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2577040351",
            "name": "Liang Yueying",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2354500975",
            "name": "Jiang, Weiwen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2339238570",
            "name": "Wang Binghui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2076313181",
            "name": "Liu Hang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744413473",
            "name": "Ding, Caiwen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W3174685870",
        "https://openalex.org/W3154435685",
        "https://openalex.org/W3128232076",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2483215953",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3181414820"
    ],
    "abstract": "In this paper, we propose a novel gender bias detection method by utilizing attention map for transformer-based models. We 1) give an intuitive gender bias judgement method by comparing the different relation degree between the genders and the occupation according to the attention scores, 2) design a gender bias detector by modifying the attention module, 3) insert the gender bias detector into different positions of the model to present the internal gender bias flow, and 4) draw the consistent gender bias conclusion by scanning the entire Wikipedia, a BERT pretraining dataset. We observe that 1) the attention matrices, Wq and Wk introduce much more gender bias than other modules (including the embedding layer) and 2) the bias degree changes periodically inside of the model (attention matrix Q, K, V, and the remaining part of the attention layer (including the fully-connected layer, the residual connection, and the layer normalization module) enhance the gender bias while the averaged attentions reduces the bias).",
    "full_text": "Detecting Gender Bias in Transformer-based\nModels: A Case Study on BERT\nBingbing Li[1], Hongwu Peng [1], Rajat Sainju [1], Yueying Liang[1], Junhuan Yang[2], Lei Yang[2],\nWeiwen Jiang[3], Binghui Wang[4], Hang Liu [5], and Caiwen Ding [1]\n[1]University of Connecticut, Storrs, CT, USA. [2]University of New Mexico, NM, USA\n[3]George Mason University, V A, USA. [4]Illinois Institute of Technology, IL, USA.\n[5]Stevens Institute of Technology, Hoboken, NJ, USA.\n[1]{bingbing.li, hongwu.peng, rajat.sainju, yueying.liang, caiwen.ding }@uconn.edu,\n[2]{yangjh1993, leiyang}@unm.edu, [3]wjiang8@gmu.edu, [4]bwang70@iit.edu, [5]hliu77@stevens.edu\nAbstract—In this paper, we propose a novel gender bias\ndetection method by utilizing attention map for transformer-\nbased models. We 1) give an intuitive gender bias judgement\nmethod by comparing the different relation degree between the\ngenders and the occupation according to the attention scores, 2)\ndesign a gender bias detector by modifying the attention module,\n3) insert the gender bias detector into different positions of the\nmodel to present the internal gender bias ﬂow, and 4) draw\nthe consistent gender bias conclusion by scanning the entire\nWikipedia, a BERT pretraining dataset. We observe that 1) the\nattention matrices, Wq and Wk introduce much more gender\nbias than other modules (including the embedding layer) and 2)\nthe bias degree changes periodically inside of the model (attention\nmatrix Q, K, V, and the remaining part of the attention layer\n(including the fully-connected layer, the residual connection, and\nthe layer normalization module) enhance the gender bias while\nthe averaged attentions reduces the bias).\nIndex Terms—gender bias, transformer, attention, detection,\nanalysis\nI. I NTRODUCTION\nGreat success has been witnessed in computer vision (CV)\nand natural language processing (NLP), by utilizing attention-\nbased transformer structure. For instance, Transformer-based\nmodels have advanced the state-of-the-arts of image classi-\nﬁcation, object detection, and semantic segmentation in CV\n(e.g., ViT [1], DeiT [2], DETR [3], Deformable DETR [4],\nSwin Transformer [5]) and text classiﬁcation, natural language\ninference, and question answering in NLP (e.g., BERT [6],\nXLNet [7], RoBERTa [8], MT-DNN [9], ALBERT [10], GPT\nv1-3 [11]–[13], and T5 [14]). However, the inexplainability\nand bias introduced by the transformer-based models could\nbecome a main barrier for their real-world deployment [15],\n[16]. Previous researches focus on the gender bias of the\nembedding layer or the output of the whole model [17], [18].\nIn this paper, we take the ﬁrst step to study whether gender\nbias associated with occupations exists inside of the pretrained\nBERT. We observe that 1) the attention matrices, Wq and\nWk, introduce much more gender bias than other module\n(including the embedding layer) and 2) the bias degree changes\nperiodically inside of the model ( Q, K, V, and the remaining\npart of the attention layer (including the fully-connected layer,\nthe residual connection, and the layer normalization) enhance\nthe gender bias while the averaged attentions reduce the bias).\n(a) Original sentence (b) Gender-swapped sentence\n(c) Normalized attention \nrelation of original sentence\n(d) Normalized attention relation \nof gender-swapped sentence\n0.976\n0.218 0.982\n0.187\nFig. 1. To analysis the gender bias, we ﬁrst swap the genders in the sentence\nto obtain the gender-swapped sentences (in (a) and (b)); and then obtain the\ncorresponding attention connections between different gender pronouns and\nthe occupation (in (c) and (d))\nII. A TTENTION MODULE VISUALIZATION\nSelf-attention module plays an essential role in transformer-\nbased language models, e.g. BERT. In this module, trainable\nmatrices, Wq, Wk, and Wv, are utilized to obtain attention\nmatrices, Q, K, and V, and then the attention score, AS\nand the averaged attentions, AvgAttention, are derived as\nfollows\nQ = input ∗Wq (1)\nK = input ∗Wk (2)\nV = input ∗Wv (3)\narXiv:2110.15733v1  [cs.CL]  15 Oct 2021\nFig. 2. Attention score matrix extraction to derive the gender bias\nAS(Q, K) =Softmax ( Q ×KT\n√Dk\n) (4)\nAvgAttention(Q, K, V) =AS(Q, K) ∗V (5)\nwhere input is the output of the embedding layer for the ﬁrst\nattention layer or the output of the previous attention layer for\nthe remaining attention layers, Dk represents the dimension\nof matrix K.\nFor each attention head (for BERT model, there are 12\nheads for each layer and 12 layers, thus totally 144 heads),\nthe dimension of the AS is m ∗m, which m is the length of\nthe input sentence and each element of the AS corresponds to\nthe attention connection degree between two different words.\nFig. 1 (a) shows the attention connection between different\nwords of the input sentence with the pretrained BERT model\nusing bertviz toolbox [19] and Fig. 1 (c) shows the attention\nconnection between the occupation (e.g., ”nurse”) and the\nwhole sentence. We also extract the elements corresponding\nto different genders and the occupation for bias analysis as\nshown in Fig. 2.\nOriginal sentence:\nShe accompanied him on stage and on several recordings \nbefore becoming a nurse in 1939.\nGender-swapped sentence:\nHe accompanied her on stage and on several recordings \nbefore becoming a nurse in 1939.\nFig. 3. Gender swapping\nIII. G ENDER BIAS ANALYSIS BASED ON THE ATTENTION\nMAP\nA. Gender bias analysis based on gender-swapped sentences\nFor each sentence, we 1) ﬁnd the gender pronouns and the\noccupation (in our test, we choose the last occupation if there\nare more than one), 2) calculate the gender tendency value by\nsumming all the AS elements associated with each gender,\n3) swap the gender of the sentence according to [20], [21] to\navoid the position effect on the gender bias judgement.\nFor input sentence ST , we obtain the index of the word\nassociated with male and female, and calculate the gender\ntendency by summing all the AS elements associate with each\ngender as follows\n[i1, i2, ··· , ip] = IDmale (ST)) (6)\n[j1, j2, ··· , jq] = IDfemale (ST)) (7)\nk = IDoccupation (ST)) (8)\nwhere p and q are the number of male and female pronouns\nin ST , IDmale(∗), IDfemale (∗), and IDoccupation(∗) return\nthe index of male and female pronouns and occupation,\nrespectively.\nThen we extract the AS elements associated with genders\nas shown in Fig. 1 and Fig. 2 (a). Speciﬁcally, we choose the\nattention score between the male pronoun (e.g., “him”) and the\noccupation (e.g., “nurse”) as the male tendency,Tmale, and the\nattention score between the female pronoun (e.g., “she”) and\nthe occupation (e.g., “nurse”) as the female tendency, Tfemale ,\nas follows\nTmale =\n∑\n∀i∈IDmale\nASk,i (9)\nTfemale =\n∑\n∀j∈IDfemale\nASk,j (10)\nThen we derive the bias of the sentence, biasST by normal-\nizing [Tmale, Tfemale ] and calculate the difference as follows\nVmale = Tmale√\nT2\nmale + T2\nfemale\n(11)\nVfemale = Tfemale√\nT2\nmale + T2\nfemale\n(12)\nbiasST = Vmale −Vfemale (13)\nFinally, we swap the gender pronouns as shown in Fig. 3,\nobtain the bias of the gender-swapped sentence, biasSTswap ,\nand derive the ﬁnal degreebiased to determine the gender bias\nexistence and degree (if degreebiased is larger than 0, then\ngender bias is detected) as follows\ndegreebiased = biasST ∗biasSTswap (14)\nThe whole procedure is shown in Fig. 4.\nnurse<->he nurse<->her\nsentence1: \"She accompanied him on stage and on several recordings before \nbecoming a nurse in 1939.\"\nsentence2: \"He accompanied her on stage and on several recordings before \nbecoming a nurse in 1939.\"\nnurse<->him nurse<->she\nTmale\n[0.081,    0.362]\n[0.092,     0.483]\n[0.218, 0.976]\n[0.187, 0.982]\n[Vmale,Vfemale]\ndegreebiased \n= biasST*biasSTswap \ny=xmale\nfemale0\nbiasST = Vmale-Vfemale\nNormalization\nIf: degreebiased>0\nThen: gender bias detected!\nThe relative position of genders \ndoesn’t change the conclusion!\ngender swapping\nfemale0\ny=xmale\nbiasSTswap = V’male-V’female\nTfemale\nT’male T’female\n[V’male,V’female]\nFig. 4. Gender bias judgement method\nB. Gender bias detectors in different positions of the BERT\nmodel\nFor the BERT model, the data ﬂow inside of the BERT\ncan be described as follows: First, each word of the input\nsentence is converted into numbers using the tokenizer; then,\nword vectors are obtained after looking up the dictionary\naccording to trained embedding layer matrix (by default, we\nuse a 768-dimensional vector to represent each word) and we\ninsert the ﬁrst bias detector at the output of the embedding\nlayer; then the output is connected to the attention layer (for\nBERT-base model, there are 12 attention layers) and we insert\n3 bias detector to detect the bias of the 3 attention matrices,\nQ, K, and V; then the averaged attention, AvgAttention\nare calculated, in which we insert the third bias detector;\nﬁnally, we insert the forth bias detector at the output of the\nattention layer to detect the bias change between the averaged\nattention and the remaining part (including the fully-connected\nlayer, the residual connection, and the layer normalization) of\nthe attention layer, which we refer to collectively as residual\nattention part.\nFig. 5 shows the positions for gender bias detection inside\nof the BERT model and the structure of our bias detector. In\nthe gender bias detector, we set the attention matrices, Wq,\nWk, and Wv, as identity matrices ( I) and use multi-head\nattention strategy to obtain attention scores and the gender\nbias judgement to detect the bias.\nIV. E XPERIMENT\nA. Dataset and sentences ﬁltering\nWe test our gender bias detection method on the BERT pre-\ntraining dataset, Wikipedia. In this dataset, there are more than\ntwo million sentences. We extract the sentences by designing\na ﬁlter that the expected sentence should include two opposite\ngender pronouns (e.g., ”he” and ”she”) and one occupations\n(e.g., ”nurse”). Finally,we obtain 60,548 sentences for gender\nbias detection. Fig. 6 shows the occupation distribution of the\nﬁltered dataset.\nB. Detection results\nFor each test sentence, we calculate the degreebiased by\nswapping the gender pronouns of the input sentence and doing\nbias detection in different positions of the pretrained BERT\nmodel with the original and the gender-swapped sentences.\nThen we count the distribution characteristics among the\nwhole test dataset of 60,548 sentences.\nFig. 7 shows the mean value degreebiased across the whole\ntest dataset in different positions as described in Fig. 5. Curves\nbetween green dashed lines correspond to different detection\npositions in one attention layer. We plot the biases introduced\nby Wq, Wk, Wv separately since they work in parallel.\nFrom the results, we observe that 1) Wq and Wk show larger\nbias tendency than other positions while Wv introduce much\nsmaller bias; 2) the averaged attention, AvgAttention, have\nnegative bias values and thus do not show any bias tendency;\n3) the layer outputs show larger bias tendency and help us\nto analysis the bias enhancement of the remaining part of the\nattention layer.\nTo make the relative gender bias change more clearly be-\ntween adjacent detection positions to check the bias enhance-\nment, we calculate the difference between adjacent detection\npositions and also the percentage of bias enhancement (the\nbias of the current position is larger than the previous one) as\nshown in Fig. 8. We conclude that 1) attention matrices, Wq,\nWk, and Wv, have higher probability than other modules\nto enhance the bias; 2) averaged attention usually does not\nenhance the bias; and 3) the remaining part of the attention\nlayer enhance the bias again.\nAdditionally, we show the gender bias distribution in dif-\nferent positions inside of the model. We use box plot to show\nthe gender bias value distributions as Fig. 9 to Fig. 11. We\nobserve that 1) Q, K, and V show signiﬁcant bias at each\nlayer; 2) Q and K have larger bias than other positions of the\nmodel, including Wv.\nFurthermore, we show the percentage of the biased head\n(corresponding to the positive degreebiased) in Fig. 12. We\nconclude that 1) Wq and Wk increase the percentage of\nbiased heads, which is much larger than the percentage of\nbias heads introduced by other modules; 2) Wv leads to\nMulti-head attention 12\nMulti-head attention 2\nEmbedding layer\nInput: “I am a boy”\nEncoder 1\nEncoder 2\n•••\nEncoder 12\n12 \nEncoder\nPooler\nLinear Linear Linear\nMulti-head attention 1\nConcat\nEncoder\nEncoder input\nEncoder output\nQ K V\n…\nFeedforward\ntokenizer\n[1045, 2572, 1037, 2879]\nOutput\nG. bias det. × 3\nG. bias det. × 1\nG. bias det. × 1\n…\nDetector\nGender Bias detector input\nGender Bias judgement\nQI KI VI\nMulti-head attention 1\nQ1 K1\nT\nMatrix mul\nSoftmax\n…\nbias\nLinear: WQ = WK = WV = I\nAS\nFig. 5. Gender bias detectors in different position of the BERT model\nbutcher\neconomist\nartists\nwriter\nsoldiers\nbanker\njudges\neditor\nsecretary\nsinger\nphysicians\ndoctor\nofficer\ncommander\ntherapist\npainter\ndevelopers\narchitect\nteachers\nnurse\ndeveloper\neconomists\nprogrammer\ncaptains\nscholars\nofficers\njudge\nscientists\nscholar\nlawyer\ndentist\ncaptain\nartist\nteacher\nengineer\ncarpenter\ncoach\nsingers\nmusicians\ncontroller\npilot\nfilmmaker\nfarmers\nmusician\ndoctors\nphysician\nscientist\nphotographer\nreceptionist\nattorney\nsecretaries\nprofessor\ndesigner\npsychologist\naccountant\nfarmer\nfilmmakers\npolitician\nbarber\ndesigners\ntranslator\narchitects\npainters\nengineers\nnurses\npilots\npoliticians\npsychologists\nsurgeon\nprofessors\neditors\nchef\nbuilder\nbartender\nlawyers\nmechanic\ncommanders\nattorneys\nchefs\nundertaker\naccountants\ncoaches\nmechanics\nsurgeons\nprogrammers\nphotographers\nbuilders\ncontrollers\nbankers\n[UNK]\ntranslators\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nOccupation Distribution\nFig. 6. Occupation distribution of our ﬁltered dataset (totally 60,548 sen-\ntences)\nEmbedding\nQ1,K1,V1\nAvgAttention1\nLayer1_output\nQ2,K2,V2\nAvgAttention2\nLayer2_output\nQ3,K3,V3\nAvgAttention3\nLayer3_output\nQ4,K4,V4\nAvgAttention4\nLayer4_output\nQ5,K5,V5\nAvgAttention5\nLayer5_output\nQ6,K6,V6\nAvgAttention6\nLayer6_output\nQ7,K7,V7\nAvgAttention7\nLayer7_output\nQ8,K8,V8\nAvgAttention8\nLayer8_output\nQ9,K9,V9\nAvgAttention9\nLayer9_output\nQ10,K10,V10\nAvgAttention10\nLayer10_output\nQ11,K11,V11\nAvgAttention11\nLayer11_output\nQ12,K12,V12\nAvgAttention12\nLayer12_output\nMean Value of Gender Bias Degree of 60,548 Sentences\nQ\nK\nV\nFig. 7. Mean values of gender biases in different positions (embedding layer\noutput, Q, K, V, AvgAttention, and attention layer output) inside of\nthe BERT model for 60,548 test sentences. Curves between red dashed lines\ncorrespond to different detection positions in one attention layer.\nQ, AvgAttention, LayerOutput\nK\nV\nProbability of Gender Bias Enhancement\nEmbedding\nAvgAttention1\nQ1,K1,V1\nLayer1_output\nQ2,K2,V2\nAvgAttention2\nLayer2_output\nQ3,K3,V3\nAvgAttention3\nLayer3_output\nQ4,K4,V4\nAvgAttention4\nLayer4_output\nQ5,K5,V5\nAvgAttention5\nLayer5_output\nQ6,K6,V6\nAvgAttention6\nLayer6_output\nQ7,K7,V7\nAvgAttention7\nLayer7_output\nQ8,K8,V8\nAvgAttention8\nLayer8_output\nQ9,K9,V9\nAvgAttention9\nLayer9_output\nQ10,K10,V10\nAvgAttention10\nLayer10_output\nQ11,K11,V11\nAvgAttention11\nLayer11_output\nQ12,K12,V12\nAvgAttention12\nLayer12_output\nFig. 8. Probability of gender bias enhancement between adjacent detection\npositions. Curves between red dashed lines correspond to different detection\npositions (embedding layer output, Q, K, V, AvgAttention, and attention\nlayer output) in one attention layer.\nGender Bias Distribution\nEmbedding\nAvgAttention1\nQ1,K1,V1\nLayer1_output\nQ2,K2,V2\nAvgAttention2\nLayer2_output\nQ3,K3,V3\nAvgAttention3\nLayer3_output\nQ4,K4,V4\nAvgAttention4\nLayer4_output\nQ5,K5,V5\nAvgAttention5\nLayer5_output\nQ6,K6,V6\nAvgAttention6\nLayer6_output\nQ7,K7,V7\nAvgAttention7\nLayer7_output\nQ8,K8,V8\nAvgAttention8\nLayer8_output\nQ9,K9,V9\nAvgAttention9\nLayer9_output\nQ10,K10,V10\nAvgAttention10\nLayer10_output\nQ11,K11,V11\nAvgAttention11\nLayer11_output\nQ12,K12,V12\nAvgAttention12\nLayer12_output\nQ1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Q12\nFig. 9. Gender bias degree distribution in different positions of the pretrained\nBERT model (embedding layer output, Q, AvgAttention, and attention\nlayer output). Curves between red dashed lines correspond to different\ndetection positions in one attention layer.\nGender Bias Distribution\nK1 K2 K3 K4 K5 K6 K7 K8 K9 K10 K11 K12\nEmbedding\nAvgAttention1\nQ1,K1,V1\nLayer1_output\nQ2,K2,V2\nAvgAttention2\nLayer2_output\nQ3,K3,V3\nAvgAttention3\nLayer3_output\nQ4,K4,V4\nAvgAttention4\nLayer4_output\nQ5,K5,V5\nAvgAttention5\nLayer5_output\nQ6,K6,V6\nAvgAttention6\nLayer6_output\nQ7,K7,V7\nAvgAttention7\nLayer7_output\nQ8,K8,V8\nAvgAttention8\nLayer8_output\nQ9,K9,V9\nAvgAttention9\nLayer9_output\nQ10,K10,V10\nAvgAttention10\nLayer10_output\nQ11,K11,V11\nAvgAttention11\nLayer11_output\nQ12,K12,V12\nAvgAttention12\nLayer12_output\nFig. 10. Gender bias degree distribution in different positions of the pretrained\nBERT model (embedding layer output, K, AvgAttention, and attention\nlayer output). Curves between red dashed lines correspond to different\ndetection positions in one attention layer.\nV1 V2 V3 V4 V5 V6 V7 V9 V10 V11 V12V8\nGender Bias Distribution\nEmbedding\nAvgAttention1\nQ1,K1,V1\nLayer1_output\nQ2,K2,V2\nAvgAttention2\nLayer2_output\nQ3,K3,V3\nAvgAttention3\nLayer3_output\nQ4,K4,V4\nAvgAttention4\nLayer4_output\nQ5,K5,V5\nAvgAttention5\nLayer5_output\nQ6,K6,V6\nAvgAttention6\nLayer6_output\nQ7,K7,V7\nAvgAttention7\nLayer7_output\nQ8,K8,V8\nAvgAttention8\nLayer8_output\nQ9,K9,V9\nAvgAttention9\nLayer9_output\nQ10,K10,V10\nAvgAttention10\nLayer10_output\nQ11,K11,V11\nAvgAttention11\nLayer11_output\nQ12,K12,V12\nAvgAttention12\nLayer12_output\nFig. 11. Gender bias degree distribution in different positions of the pretrained\nBERT model (embedding layer output, V, AvgAttention, and attention\nlayer output). Curves between red dashed lines correspond to different\ndetection positions in one attention layer.\nfewer biased heads and the residual part of the attention layer\nincrease the percentage of the biased heads. This coincides the\ndistribution of the mean value of bias in different position of\nthe BERT model.\nV. C ONCLUSION\nIn this paper, we propose a novel gender bias detection\nmethod based on attention map for transformer-based models.\nWe extract the attention scores of the corresponding gender\npronouns and occupation, swap the gender pronouns to avoid\nposition effect on bias judgement, and check the consistency\nof the gender bias associated with the occupation. The gender\nbias distribution conclusions are drawn by scanning the whole\nﬁlter dataset obtained from Wikipedia, a BERT pretraining\ndataset. We take the ﬁrst attempt to study the gender bias inside\nof the transformer-based models (BERT as the example) and\nobserve that 1) the attention matrices, Wq and Wk introduce\nmuch more gender bias than other modules (including the\nEmb\nL1Q L1K L1V L1A L1Z\nL2Q L2K L2V L2A L2Z\nL3Q L3K L3V L3A L3Z\nL4Q L4K L4V L4A L4Z\nL5Q L5K L5V L5A L5Z\nL6Q L6K L6V L6A L6Z\nL7Q L7K L7V L7A L7Z\nL8Q L8K L8V L8A L8Z\nL9Q L9K L9V L9A L9Z\nL10Q L10K L10V L10A L10Z\nL11Q L11K L11V L11A L11Z\nL12Q L12K L12V L12A L12Z\nPie chart of the all gender bias (12 heads * 12 sentences) for each layer.\nFig. 12. Percentage of unbiased (in blue color) and biased (in orange color)\nheads for 60,548 sentences. Emb: Embedding output. LiQ/Lik/LiV/LiA/LiZ:\nQ, K, V, AvgAttention, ﬁnal output of the ith layer.\nembedding layer) and 2) the bias degree changes periodically\ninside of the model (attention matrix Q, K, V, and the remain-\ning part of the attention layer (including the fully-connected\nlayer, the residual connection, and the layer normalization\nmodule) enhance the gender bias while the averaged attentions\nreduces the bias). We hope our work will shine some lights\non explainable and fairness AI.\nREFERENCES\n[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[2] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distillation\nthrough attention,” in International Conference on Machine Learning .\nPMLR, 2021, pp. 10 347–10 357.\n[3] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[4] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,”arXiv preprint\narXiv:2010.04159, 2020.\n[5] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030 , 2021.\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[7] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” in Advances in neural information processing systems ,\n2019, pp. 5754–5764.\n[8] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.\n[9] X. Liu, P. He, W. Chen, and J. Gao, “Multi-task deep neural networks\nfor natural language understanding,” in Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , 2019, pp.\n4487–4496.\n[10] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language representa-\ntions,” arXiv preprint arXiv:1909.11942 , 2019.\n[11] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding with unsupervised learning,” Technical report,\nOpenAI, Tech. Rep., 2018.\n[12] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners.”\n[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askellet al., “Language models\nare few-shot learners,” arXiv preprint arXiv:2005.14165 , 2020.\n[14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer,” Journal of Machine Learning\nResearch, vol. 21, no. 140, pp. 1–67, 2020.\n[15] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\n“A survey on bias and fairness in machine learning,” ACM Computing\nSurveys (CSUR), vol. 54, no. 6, pp. 1–35, 2021.\n[16] F.-L. Fan, J. Xiong, M. Li, and G. Wang, “On interpretability of artiﬁcial\nneural networks: A survey,”IEEE Transactions on Radiation and Plasma\nMedical Sciences, 2021.\n[17] T. Bolukbasi, K.-W. Chang, J. Y . Zou, V . Saligrama, and A. T. Kalai,\n“Man is to computer programmer as woman is to homemaker? debiasing\nword embeddings,” Advances in neural information processing systems ,\nvol. 29, pp. 4349–4357, 2016.\n[18] E. Sheng, K.-W. Chang, P. Natarajan, and N. Peng, “Societal bi-\nases in language generation: Progress and challenges,” arXiv preprint\narXiv:2105.04054, 2021.\n[19] J. Vig, “A multiscale visualization of attention in the transformer model,”\nin Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations . Florence, Italy:\nAssociation for Computational Linguistics, Jul. 2019, pp. 37–42.\n[20] J. Zhao, T. Wang, M. Yatskar, V . Ordonez, and K.-W. Chang, “Gender\nbias in coreference resolution: Evaluation and debiasing methods,” in\nProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, vol. 2, 2018.\n[21] K. Lu, P. Mardziel, F. Wu, P. Amancharla, and A. Datta, “Gender bias in\nneural natural language processing,” in Logic, Language, and Security .\nSpringer, 2020, pp. 189–202."
}