{
  "title": "Node-adaptive graph Transformer with structural encoding for accurate and robust lncRNA-disease association prediction",
  "url": "https://openalex.org/W4390978706",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5028229932",
      "name": "Guanghui Li",
      "affiliations": [
        "East China Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5101293486",
      "name": "Peihao Bai",
      "affiliations": [
        "East China Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5052382857",
      "name": "Liang Cheng",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5026469278",
      "name": "Jiawei Luo",
      "affiliations": [
        "Hunan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2141831115",
    "https://openalex.org/W1975847255",
    "https://openalex.org/W2071271349",
    "https://openalex.org/W2039816251",
    "https://openalex.org/W2462108616",
    "https://openalex.org/W1999283025",
    "https://openalex.org/W2013900075",
    "https://openalex.org/W2000833392",
    "https://openalex.org/W3116084499",
    "https://openalex.org/W4283398772",
    "https://openalex.org/W2166339213",
    "https://openalex.org/W3162538759",
    "https://openalex.org/W2792132368",
    "https://openalex.org/W4297891375",
    "https://openalex.org/W2113868616",
    "https://openalex.org/W2128983843",
    "https://openalex.org/W4313406530",
    "https://openalex.org/W3116906693",
    "https://openalex.org/W2014162182",
    "https://openalex.org/W1999008082",
    "https://openalex.org/W2512308096",
    "https://openalex.org/W2801501532",
    "https://openalex.org/W2943254651",
    "https://openalex.org/W2757497299",
    "https://openalex.org/W4205450773",
    "https://openalex.org/W2967521176",
    "https://openalex.org/W2611747160",
    "https://openalex.org/W2942942195",
    "https://openalex.org/W3175544068",
    "https://openalex.org/W3185225911",
    "https://openalex.org/W4220690199",
    "https://openalex.org/W3027711990",
    "https://openalex.org/W4205843699",
    "https://openalex.org/W4223419232",
    "https://openalex.org/W3201260316",
    "https://openalex.org/W4205614329",
    "https://openalex.org/W1550222077",
    "https://openalex.org/W3187582009",
    "https://openalex.org/W4281706128",
    "https://openalex.org/W3090999459",
    "https://openalex.org/W4283221132",
    "https://openalex.org/W2393319904",
    "https://openalex.org/W2772618228",
    "https://openalex.org/W2023940858",
    "https://openalex.org/W611910056",
    "https://openalex.org/W2141222510",
    "https://openalex.org/W1967087625",
    "https://openalex.org/W2106029302",
    "https://openalex.org/W2058789641",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3202551657",
    "https://openalex.org/W4293476626",
    "https://openalex.org/W3136042383",
    "https://openalex.org/W3093030756",
    "https://openalex.org/W3081960698",
    "https://openalex.org/W2895679848",
    "https://openalex.org/W3099929518",
    "https://openalex.org/W2117977572",
    "https://openalex.org/W4246271082",
    "https://openalex.org/W4309040000",
    "https://openalex.org/W3080501980",
    "https://openalex.org/W2789078890",
    "https://openalex.org/W2761249484",
    "https://openalex.org/W4292432035",
    "https://openalex.org/W2904953089",
    "https://openalex.org/W4281491483",
    "https://openalex.org/W2059795285",
    "https://openalex.org/W2961823581",
    "https://openalex.org/W2965467089",
    "https://openalex.org/W3090253986",
    "https://openalex.org/W3192520166",
    "https://openalex.org/W2981064378",
    "https://openalex.org/W2952143487",
    "https://openalex.org/W2913268581",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3025456873",
    "https://openalex.org/W4205567020",
    "https://openalex.org/W2984302209",
    "https://openalex.org/W2158135353"
  ],
  "abstract": "Abstract Background Long noncoding RNAs (lncRNAs) are integral to a plethora of critical cellular biological processes, including the regulation of gene expression, cell differentiation, and the development of tumors and cancers. Predicting the relationships between lncRNAs and diseases can contribute to a better understanding of the pathogenic mechanisms of disease and provide strong support for the development of advanced treatment methods. Results Therefore, we present an innovative Node-Adaptive Graph Transformer model for predicting unknown LncRNA-Disease Associations, named NAGTLDA. First, we utilize the node-adaptive feature smoothing (NAFS) method to learn the local feature information of nodes and encode the structural information of the fusion similarity network of diseases and lncRNAs using Structural Deep Network Embedding (SDNE). Next, the Transformer module is used to capture potential association information between the network nodes. Finally, we employ a Transformer module with two multi-headed attention layers for learning global-level embedding fusion. Network structure coding is added as the structural inductive bias of the network to compensate for the missing message-passing mechanism in Transformer. NAGTLDA achieved an average AUC of 0.9531 and AUPR of 0.9537 significantly higher than state-of-the-art methods in 5-fold cross validation. We perform case studies on 4 diseases; 55 out of 60 associations between lncRNAs and diseases have been validated in the literatures. The results demonstrate the enormous potential of the graph Transformer structure to incorporate graph structural information for uncovering lncRNA-disease unknown correlations. Conclusions Our proposed NAGTLDA model can serve as a highly efficient computational method for predicting biological information associations.",
  "full_text": "Li et al. BMC Genomics           (2024) 25:73  \nhttps://doi.org/10.1186/s12864-024-09998-2\nRESEARCH Open Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecom-\nmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nBMC Genomics\nNode-adaptive graph Transformer \nwith structural encoding for accurate and robust \nlncRNA-disease association prediction\nGuanghui Li1*, Peihao Bai1, Cheng Liang2 and Jiawei Luo3* \nAbstract \nBackground Long noncoding RNAs (lncRNAs) are integral to a plethora of critical cellular biological processes, \nincluding the regulation of gene expression, cell differentiation, and the development of tumors and cancers. Predict-\ning the relationships between lncRNAs and diseases can contribute to a better understanding of the pathogenic \nmechanisms of disease and provide strong support for the development of advanced treatment methods.\nResults Therefore, we present an innovative Node-Adaptive Graph Transformer model for predicting unknown \nLncRNA-Disease Associations, named NAGTLDA. First, we utilize the node-adaptive feature smoothing (NAFS) method \nto learn the local feature information of nodes and encode the structural information of the fusion similarity network \nof diseases and lncRNAs using Structural Deep Network Embedding (SDNE). Next, the Transformer module is used \nto capture potential association information between the network nodes. Finally, we employ a Transformer mod-\nule with two multi-headed attention layers for learning global-level embedding fusion. Network structure coding \nis added as the structural inductive bias of the network to compensate for the missing message-passing mechanism \nin Transformer. NAGTLDA achieved an average AUC of 0.9531 and AUPR of 0.9537 significantly higher than state-\nof-the-art methods in 5-fold cross validation. We perform case studies on 4 diseases; 55 out of 60 associations \nbetween lncRNAs and diseases have been validated in the literatures. The results demonstrate the enormous poten-\ntial of the graph Transformer structure to incorporate graph structural information for uncovering lncRNA-disease \nunknown correlations.\nConclusions Our proposed NAGTLDA model can serve as a highly efficient computational method for predicting \nbiological information associations.\nKeywords lncRNA-disease associations, Transformer, Structural deep network embedding, Node-adaptive feature \nsmoothing\n*Correspondence:\nGuanghui Li\nghli16@hnu.edu.cn\nJiawei Luo\nluojiawei@hnu.edu.cn\nFull list of author information is available at the end of the article\nPage 2 of 26Li et al. BMC Genomics           (2024) 25:73 \nBackground\nAccording to a large number of cell biology experi -\nments, lncRNA are RNA molecule that are not involved \nin protein coding and exceed approximately 200 nucleo -\ntides in length [1–4]. At the beginning of the study, most \nresearchers thought that lncRNAs were just an unim -\nportant product in the transcription process. However, \nas biological experimental results continue to accu -\nmulate, researchers are slowly discovering that lncR -\nNAs are assumed to have very important roles in many \nimportant cell biological processes. They are involved in \nmanaging the cell cycle, managing embryonic develop -\nment, the spatial and temporal control of gene expres -\nsion, determining cell fates [5]. Moreover, researchers \nin ongoing clinical experiments on human diseases have \nperceived that lncRNAs are inextricably linked to many \nhuman cancers [6, 7] and have a decisive role in human \ncardiovascular physiological activity and its pathology \n[8]. Therefore, researchers have regarded lncRNAs as a \ncrucial factor in the study of human diseases and have \nexplored the relationships between diseases and lncR -\nNAs as a new research direction to overcome the barriers \nof human diseases. Exploring the relationships between \ndiseases and lncRNAs will lead us to deepen our under -\nstanding of disease mechanisms [9] and find the causa -\ntive factors and sources of diseases from the genetic \nroots. At the same time, understanding the interactions \nbetween lncRNAs and diseases will allow us to intervene \nand regulate the expression of disease-related genes, and \nfind new targets and strategies [10] for the treatment \nof diseases. Researchers have found that the expression \nlevels of some lncRNAs are very prominent in certain \ndiseases, so lncRNAs can be used as potential biomark -\ners and play a very important role in the early detection \nand treatment of diseases. In drug discovery, by exploring \nthe relationship between diseases and lncRNAs, this can \nhelp us to investigate new and optimized drugs that are \nmore effective. In addition, human genetic diseases [11] \nexhibit a close association with lncRNAs. Investigating \nlncRNAs allows for the elucidation of certain genetic dis -\neases stemming from gene mutations, thereby expediting \nresearchers’ investigations into genetic disorders. How -\never, it requires considerable time to study the linkage \nin real clinical experiments, requires significant material \nresources and is challenging to apply on a large scale. \nTherefore, the design of a novel computational model to \ncompute the association between diseases and lncRNAs \nis of great importance in advancing the development of \nbioinformatics. There are some challenges in the actual \nstudy, namely: (1) Large datasets exhibit a low percent -\nage of positive samples, resulting in significant sparsity \nthat reduces the model’s ability to predict positive sam -\nples effectively. (2) The availability of disease and lncRNA \nassociation data is limited, lacking a cohesive fusion of \nbiological association data, and similarity calculations \nheavily rely on association matrices.\nMany methods for calculating lncRNA-disease asso -\nciations have been developed and their accuracy and \nreliability have been verified by biological experiments. \nThus, to propose better calculation methods, researchers \nhave collected a large quantity of data to create relevant \nbenchmark databases. Gene Reference Into Function \n(GRIF) [12], DisGeNET [13], and Disease Ontology \n(DO) [14] are three standard databases related to dis -\neases. RNADisease v4.0 [15], Lnc2Cancer [16] and LncR -\nNADisease [17] are three standard databases related to \nlncRNA-disease association. These standard databases \nwere also created to break away from the previous way \nof thinking that one lncRNA corresponds to one disease \nand to perform global calculations and experiments on \nthe benchmark dataset in the database by the proposed \ncomputational method.\nNumerous computational techniques for exploring dis-\nease-lncRNA interactions have emerged with the contin -\nual advancement of diverse technology. We can classify \nthe available computational methods into bioinformatics \nnetwork-based methods [18] and deep learning-based \nmethods [19].\nBioinformatics network-based models take known \nassociations and their respective similarities to recon -\nstitute heterogeneous networks and use a variety of dif -\nferent messaging mechanisms and random walks for \nthe computation of potential associations on top of the \nconstructed heterogeneity. For example, the KRWRH \nmodel [20] utilized the restarted random walks to com -\npute associations between lncRNAs and diseases on \ntop of integrating similarities between diseases, simi -\nlarities between lncRNAs, and known associations into \na new heterogeneous network. The RWRHLD model \n[21] combined all three of them into a heterogeneous \nnetwork: observed relationships between lncRNAs and \ndiseases, known associations between crosstalk network \nbetween lncRNAs and lncRNAs, and integrating simi -\nlarity between diseases, based on which links between \ndiseases and lncRNAs are inferred using a restart ran -\ndom walk approach. The IRWRLDA model [22] is a \nnovel algorithm that improves upon traditional random \nwalks by considering both lncRNA similarity and dis -\nease similarity for initialization probabilities. It can be \nused to infer new associations, even when the disease \nhas no known association with any lncRNAs. The SIM -\nCLDA model [23] applied matrix completion and prin -\ncipal component analysis to infer potential associations. \nThe NCPLDA model [24] capitalized on the networks \nconsistency projection to obtain a new computational \nmodel for calculating new associations between lncRNAs \nPage 3 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nand diseases. The GrwLDA model [25] generated a \nglobal network by combining identified lncRNA-disease \ninteraction information, disease fusion similarity, and \nlncRNA fusion similarity and utilized this network to \nexplore novel associations between diseases and lncR -\nNAs. The LRWRHLDA model [26] integrated multiple \nheterogeneous and homogeneous networks to construct \na three-layer bioinformatics network using RWR to mine \ninteractions. The LRWHLDA model [27] is designed to \nexcavate the relationships between diseases and lncRNAs \nwith a new idea based on localized random walk that \ntakes full advantage of the topology of the network. The \nLncRDNetFlow model [28] integrated three interaction \nnetworks, disease interaction network, lncRNA interac -\ntion network and protein interaction network, to con -\nstruct a three-layered heterogeneous network to obtain \ndisease and lncRNA feature data. Nevertheless, none of \nthese methods can perform comprehensive learning and \nfusion of local and global information, nor can they per -\nform deeper network feature learning.\nThe deep learning-based lncRNA-disease association \nprediction models have shown significant improvements \nin performance compared to previous shallow models. \nThe CNNLDA model [29] reorganized multiple sources \nof similarity and introduced miRNA datasets to enable \nthe neural network model to learn more information. \nIt utilized convolutional neural networks to learn node \nembeddings and inferred the associations between dis -\neases and lncRNAs. The BiGAN model [30] employed \ngenerative adversarial networks for lncRNA-disease \ninteraction calculations. It combined the similarity of \nlncRNAs and diseases and adopted a bidirectional gen -\nerative adversarial network to infer their associations. \nThe MCA-Net model [31] utilized embedded learning \nfor multiple feature sources, ensuring that each node has \na unique vector representation. It used attention-based \nconvolutional neural networks to excavate direct interac-\ntions between lncRNAs and diseases. The ACLDA model \n[32] constructed a network based on metapaths using \nlncRNAs, miRNAs, and diseases. It introduced a novel \napproach that combines CNN and autoencoders for asso-\nciation prediction. The VADLP model [33] constructed \nmultilayer graphs to integrate multiple similarities and \nemployed variance autoencoders and CNN for lncRNA-\ndisease interaction inference. The gGATLDA model [34] \nutilized attention mechanisms at the graph level. During \nthe graph construction process, each disease-lncRNA \npair is extracted to form a subgraph for lncRNA-disease \nrelationship calculation. The MLMKDNN model [35] \nproposed a deep multi-kernel learning method, which \nincluded feature matrix construction, kernel space map -\nping, and deep neural network fusion. The kernel space \nmapping technique was applied to transform the feature \nmatrix, enabling effective integration using deep neu -\nral networks for fusion. The MLGCNET model [36] \nemployed multilayer graph autoencoder to obtain a rep -\nresentation vector of disease and lncRNA. The MGATE \nmodel [37] applied a multi-channel self-attentive encoder \nto learn latent embeddings of diseases and lncRNAs from \nmultiple angles of the graph. The GANLDA model [38] \nincorporated multi-source data as initial features. GAT is \nadopted to get feature information about nodes and their \nneighbors and finally a multilayer perceptron is leveraged \nto screen the association. However, when building deep \nnetworks in graph neural networks, deep learning tends \nto cause over-smoothing during the node learning pro -\ncess, resulting in minimal differences between the vector \nrepresentations of nodes.\nA new trend of combining Transformers and graph \nneural networks to process graph data. This approach \ncombines the parallelizability of Transformers, the \nadvantages of their multi-head attention mechanism, and \ngraph neural network methods to design new neural net -\nwork models for graph data processing. Microsoft intro -\nduced the Graphormer [39], which, for the first time, \nutilized Transformers for graph-level tasks. It effectively \nintegrated intermediate encoding, spatial encoding, and \nedge encoding into Transformers, successfully incorpo -\nrating graph structural information. This integration has \nshown improved performance in widely used benchmark \ndatasets for graph representation learning. Following \nthis trend, a classic neural network model framework \ncalled GraphGPS emerged, which combines graph neu -\nral networks and Transformers [40]. It used MLP to learn \ngraph information, feeding it into both the graph neural \nnetwork and the Transformer for graph representation \nlearning. The fusion of the results obtained from both \nmodels leads to highly competitive outcomes.\nAlthough these methods have achieved relatively good \nresults in the task of lncRNA-disease association predic -\ntion, they still have limitations and shortcomings as fol -\nlows: (1) Graph-based methods do not maintain good \nperformance and robustness in the face of sparse large \ndatasets and the problem of over-smoothing of node fea -\ntures can occur [41]. Their learning ability is limited when \nconfronted with complex heterogeneous graphs compris-\ning different nodes and edges [42, 43]. (2) Traditional \ndeep learning-based and bioinformatics network-based \napproaches do not capture both local and global informa-\ntion, and do not learn the features of nodes by fusing the \ninformation encoded in the graph structure. (3) In these \nexisting methods, a simple linear fusion is also used for \nthe fusion of features [23, 24, 26, 38]. The incorporation \nof adaptive and efficient fusion approach holds the poten-\ntial for significant improvements in model performance \nand robustness.\nPage 4 of 26Li et al. BMC Genomics           (2024) 25:73 \nBased on the aforementioned limitations of the existing \nmethods and the inherent advantages of the Transformer \nmodel, we propose an innovative lncRNA-disease asso -\nciation prediction model named NAGTLDA. First, we \nconstruct a heterogeneous network by utilizing observed \nassociations and compute the integrated similarity of dis-\neases and lncRNAs to create their respective integrated \nsimilarity networks. Next, we employ node-adaptive fea -\nture smoothing (NAFS) [ 44] to perform local-level node \nembedding on the heterogeneous network and integrated \nsimilarity networks. Simultaneously, we utilize Structural \nDeep Network Embedding (SDNE) [ 45] to encode the \nstructural information of the integrated similarity net -\nworks. Furthermore, we utilize the Transformer model \nfor global-level embedding learning, allowing it to lever -\nage its inherent global perspective to unearth potential \nassociation information. Finally, we employ the Trans -\nformer model to perform global-level fusion of all learned \nembeddings and incorporate the structural inductive bias \nof the network. This fusion approach effectively and sig -\nnificantly enhances the utilization of all captured infor -\nmation, thereby greatly improving the performance of \ninferring the associations between diseases and lncRNAs. \nOur proposed model outperforms these models that exist \nnow in terms of performance and scalability.\nIn summary, our research makes the following key \ncontributions:\n• We employ the NAFS method for feature embedding \nlearning without the need for explicit training, and \nwe utilize SDNE to encode the network structure.\n• We employ both local-level and global-level \napproaches for feature embedding, enabling the \nmodel to effectively uncover potential association \ninformation.\n• To improve the Transformer model for learning \ngraph node information, we learn the network’s \nstructural information as an inductive bias.\n• We propose a Transformer fusion mechanism, which \nintroduces the Transformer model for node embed -\nding and fusion of multiple features and topology \ninformation, enriching the representation of lncR -\nNAs and diseases.\nMethods\nKnown human lncRNA‑disease associations\nIn our experiment, we used a benchmark dataset to assess \nthe effectiveness of our model. This dataset was obtained \nfrom previous research by Fu et  al. [ 46] on lncRNA-dis -\nease association prediction, which includes 240 lncRNAs, \n412 diseases, and 2697 experimentally validated lncRNA-\ndisease interactions from the Lnc2Cancer [ 16], LncRNA-\nDisease [17], and GeneRIF [47] databases. We denoted the \nquantity of diseases and lncRNAs as N l and Nd , respec-\ntively. We constructed an adjacency matrix A based on the \nobserved interactions between lncRNAs and diseases, and \nA ∈ RN l×N d , where A (l(i),d (j)) = 1 if there exists an iden-\ntified relationship between lncRNA l(i) and disease d (j) ; \notherwise A (l(i),d (j)) = 0 .\nLncRNA functional similarity\nThere are multiple methods for expressing the simi -\nlarity between lncRNAs, and one common method is \nbased on their association with related diseases. By com -\nparing the similarity of different lncRNAs with their \nassociated diseases, their functional similarity can be \nassessed. In this experiment, we adopted the lncRNA \nfunctional similarity calculation method proposed by \nChen et  al. [48], which assumes that there are two lncR -\nNAs l1 and l2 , respectively, l1 is linked to disease category \nD (i) = di1 ,di2 ,di3 ,··· ,din  , and l2 is linked to disease \ncategory D (j) =\n{\ndj1 ,dj2 ,dj3 ,··· ,djm\n}\n . The formula for \ncalculating the similarity score between disease dk ∈ D (i) \nand disease category D (j) provided here is:\nwhere DS (dk, d) represents the semantic similarity \nbetween diseases dk and d. Based on the semantic simi -\nlarity between the diseases and the associations between \nthe lncRNAs and disease category, the formula for calcu -\nlating the functional similarity of lncRNAs is as follows:\nwhere n and m denote the quantity of diseases in disease \ncategory D (i) and category D (j) , which can be repre -\nsented as |D (i)|= n, | D(j)|= m  , respectively.\nDisease semantic similarity\nTo compute the semantic similarity between diseases, their \nMedical Subject Headings (MeSH) descriptors can be used \n[49], and they can be denoted as a Directed Acyclic Graph \n(DAG) [50]. Specifically, the hierarchical relationship of a \ndisease can be represented as DAG (di) = (T (di), E(di)) , \nwhere T (di) represents d i and all its ancestor nodes, and \nE (di) is a set of edges from ancestral nodes to descend -\nant nodes. Computing disease semantic similarity can be \ndivided into three steps. For the first stage, for any disease \nd j in DAG (di) , its contribution towards the semantic simi-\nlarity of disease d i can be computed using the following \nformula:\n(1)S(dk, D (j)) = max (DS d∈D (j)(dk, d))\n(2)\nLF (li, lj) =\n∑\nd∈D (i) S(d, D (j) + ∑\nd∈D (j) S(d, D (i))\nn + m\n(3)\nSd i(d j) =\n{\n1 if d j = d i\nmax\n{\nγ ∗ Sd i\n(\nd ′\nj\n)\n|d ′\nj ǫ children of d i\n}\nif d j �=d i\nPage 5 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nwhere parameter γ represents a hyperparameter set to \n0.5 in the formula for disease semantic contribution. The \nsecond stage is to compute the total semantic value of the \ndisease, which is computed using the following formula \nfor DV di:\nThe third stage is to compute the semantic similarity \nbetween diseases d i and d j using the following formula:\nGaussian interaction profile (GIP) kernel similarity \nfor lncRNAs and diseases\nGaussian kernel similarity is a common similarity meas -\nurement method that can map data to a multidimen -\nsional space and compute the similarity between data \npoints. The calculated lncRNA functional similarity and \ndisease semantic similarity are both relatively sparse, so it \nis necessary to introduce other similarities to compensate \nfor this deficiency. Therefore, we decided to introduce \nGIP similarity, which can make the similarity between \ndata nodes more obvious and facilitate the prediction of \nassociations between nodes. The calculation formulas for \nGIP kernel similarity LK (li, li) between lncRNA li and lj \nand DK ( d i , d j ) between disease d i and d j are as follows:\nwhere comparable to reference [51], IP (li) and IP (lj) rep-\nresent the i-row and j-row corresponding to the lncRNA \nin the known lncRNA-disease interaction matrix A,IP(d i) \nand IP(d i) represent the i-column and j-column corre -\nsponding to the disease in the known lncRNA-disease \ninteraction matrix A. rl and rd are the kernel bandwidth \ncontrol parameters and are defined by the following \nformula:\nIntegrated similarity networks for lncRNAs and diseases\nPreviously, we introduced GIP kernel similarity to com -\npensate for the sparsity of lncRNA functional simi -\nlarity and disease semantic similarity. Based on these \n(4)DV di =\n∑\nd∈T (di) Sdi(d)\n(5)DS (di, dj) =\n∑\nd∈T (di)∩T (dj) (Sdi(d) + Sdj(d))\nDV di + DV dj\n(6)LK (li,lj) = exp (−rlP IP(li) − IP(lj)P 2 )\n(7)DK (di,dj) = exp(−rd PIP (di) − IP(dj)P 2 )\n(8)rl = r′\nl/( 1\nN l\n∑N l\ni=1 PIP (li)P2)\n(9)rd = r′\nd /( 1\nN d\n∑N d\ni=1 PIP(di)P2)\nsimilarities, we calculate the integrated similarity matrix \nbetween diseases and lncRNAs using the following \nformula:\nwhere IL (li, lj) represents the integrated similarity matrix \nbetween lncRNAs, and ID (d i, d j) represents the similarity \nmatrix between diseases. To better utilize the integrated \nsimilarity matrices of lncRNAs and diseases, we use them \nto obtain their corresponding integrated similarity net -\nworks. We set two thresholds α and β to calculate the \nsimilarity network, and their formulas are expressed as \nfollows:\nwhere Inet  represents the network obtained from the \nintegrated similarity matrix of lncRNAs. If the similarity \nvalue between li and lj is not less than or equal to thresh -\nold α , then Inet (li, lj) = 1. Otherwise, Inet (li, lj) = 0. D net \ndenotes the network obtained from the integrated simi -\nlarity matrix of diseases. If the similarity value between \nd i and d j is not less than or equal to threshold β , then \nD net(di, dj)=1. Otherwise, D net(di, dj)=0.\nLncRNA‑disease heterogeneous network\nWe constructed a lncRNA-disease heterogeneous net -\nwork that includes the lncRNA similarity matrix, disease \nsimilarity matrix, and the known lncRNA-disease asso -\nciation matrix A:\nwhere AT represents the transpose of the lncRNA-dis -\nease interaction matrix.\nNAGTLDA\nThis section provides a detailed introduction to our pro -\nposed model, NAGTLDA, which accurately excavates the \nlncRNA-disease associations. The NAGTLDA process \nis shown in Fig.  1, which depicts the workflow and the \nsequence of steps involved in the NAGTLDA framework. \n(10)\nIL(li, lj) =\n{ LF (li, lj) if li and l j have functional similarity\nLK (li, lj) otherwise\n(11)\nID (li, lj) =\n{ DS (di, dj) if di and d j have semantic similarity\nDK (di, dj) otherwise\n(12)Inet (li,lj) =\n{\n1 if IL(li,lj) ⩾ α\n0 otherwise\n(13)D net (di,dj) =\n{\n1 if ID (di,dj) ⩾ β\n0 otherwise\n(14)G net =\n[ IL A\nAT ID\n]\n∈ R(N l+N d)×(N l+N d)\nPage 6 of 26Li et al. BMC Genomics           (2024) 25:73 \nThe model framework comprises the following parts: (1) \nusing NAFS to learn local-level node feature embedding, \n(2) using SDNE to encode the structure of networks, \n(3) using a Transformer model with a multi-head atten -\ntion layer to learn global-level node feature embedding, \n(4) using a Transformer model with two multi-head \nFig. 1 The NAGTLDA workflow. Step1: Construct the integrated similarity network, extract the local features of the heterogeneous network \nand the integrated similarity network adopting NAFS, and encode the structural information of the integrated similarity network applying SDNE. \nStep2: Learn global information of heterogeneous network nodes by Transformer architecture. Step3: Adaptively fusing local information of nodes, \nglobal information and structural coding of the network by Transformer architecture. Step4: Predict associations using bilinear encoder\nPage 7 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nattention layers to learn embedding fusion at the global-\nlevel, (5) predicting the association score between dis -\neases and lncRNAs.\nLocal‑level node feature embedding (node‑adaptive \nfeature smoothing)\nIn recent years, GCN [52] has become very popular in \ngraph neural networks (GNNs). This is because GCN can \nlearn the features of all nodes in a graph based on both \nnode features and graph structure. Using GCN to aggre -\ngate multi-order neighbour information in large graph \nnetworks leads to over-smoothing problems and requires \na high computational cost and large memory consump -\ntion. To address this issue, Zhang et al. [44] proposed a \nmodel called NAFS, which aggregates and updates the \nfeatures of nodes in a graph. Compared with GCN, NAFS \nnot only solves the limitations of GCN but also signifi -\ncantly simplifies the model training intricacy and miti -\ngates the occurrence of gradient vanishing and gradient \nexplosion during backpropagation without the need for \nadditional training.\nSince our model uses NAFS for node feature embed -\nding for all three graphs ( Inet, D netand G net ), we use G net \nas an example for illustration. The abbreviation for G net \nis G. We denote the quantity of nodes in G as n and the \nquantity of edges as m. Computing of NAFS consists of \nfour steps. The initial step entails computing the over-\nsmoothing distance, and the calculation is performed in \nthe following manner:\nwhere [ ˆG kX ]i represents the i-th row in the matrix, which \nindicates the smoothed node representation of the ith \nnode. Dis(•) represents a distance formula, which can \nbe implemented using the Euclidean distance formula. \nˆG = ˜Dr−1 ˜G ˜D−r,˜D denotes the degree matrix of graph. r \nis a hyperparameter in the model. ˜G represents the adja -\ncency matrix of the undirected graph with self-loops \nadded. The calculation formula for ˆG∞ is as follows:\nwhere d i represents the degree of node i. The smooth -\ning weight calculated in the second step is computed as \nfollows:\nwhere K represents the maximum number of smoothing \nsteps. The third step is to calculate the smoothing weight \nmatrix, which is computed as follows:\n(15)D i(k) = Dis([ ˆG K X ]i, [ ˆG ∞X ]i)\n(16)ˆG ∞\ni,j = (di + 1)r(dj + 1)1−r\n2m + n\n(17)ωi(k) = eD i(k) /\n∑k\nl=0 eD i(l)\nwhere ϕ(k) ∈ Rn and Diag(·) represents a diagonal \nmatrix. We denote the initial input feature represen -\ntation as X(0) . After l rounds of smoothing, the node \nfeature matrix X (l) = ˆGX (l−1) contains the feature of \nthe previous round of smoothing. After K rounds of \nmaximum smoothing, X (k) will contain more informa -\ntion, and we can obtain a collection of feature matrices {\nX (0), X (1) , X (2) , ··· , X (k)}\n . Finally, the formula for \nsmoothing feature ˆX is as follows:\nThe definition of X(0) is as follows:\nIn GCN, a symmetric normalized adjacency matrix \nˆG = ˜Dr−1 ˜G ˜D−r is used. Setting r = 0.5 yields the symmet-\nric normalized adjacency matrix ˜D−1/2 ˜G˜D−1/2 [52] as \nthe feature extractor. However, in NAFS, \n{r1 , r2 , r3 , ··· , rU } results in a more diverse set of feature \nembeddings. The value of r controls the normalization \nweight of each edge, so different r values lead to distinct \nnode feature embeddings for the same graph. We obtain \na set of smoothed features \n{\nˆX(0),ˆX(1) ,ˆX(2) ,··· ,ˆX(U)\n}\n \nbased on this set of different r values, and we combine \ndifferent smoothed features into \nˆZG = (ˆX(0) ⊗ ˆX(1) ···⊗ ˆX(U )) ∈ R(Nl+Nd)×(Nl+Nd) . Here, \n⊗ represents a type of combination method, which can \nbe replaced with the max function, concatenation, and \nmean function.\nFirst, we input the heterogeneous network \nG net ∈ R(N l+N d )×(N l+N d ) and the initial features \nX (0) ∈ R(N l+N d)×(N l+N d) of the network nodes, which \nconsists of nodes corresponding to lncRNAs and dis -\nease entities. We will compute a smoothing weight \nmatrix W (k) for each k-step according to Eq.  (18), then \nwe use a list {r1 , r2 , r3 , ··· , rU } . For each r-value in the \nlist, we derive a new feature node embedding represen -\ntation of the network structure from Eq.  (19), denoted \nas ˆX (u) ∈ R(N l+N d)×(N l+N d) . The feature embeddings \nobtained from all the r-value are fused to obtain the final \nfeature embedding ˆZG ∈ R(Nl+Nd)×(Nl+Nd) . The final \nNAFS is expressed as follows:\nwhere U denotes the length of the r-list and ⊗ represents \nthe fusion mode of the features (Mean).\n(18)\nW (k) = Diag(ϕ(k)), ϕ(k)[i]= ωi(k), ∀1 ≤ i≤ n\n(19)ˆX =\n∑K\nl=0 W (l)X(l)\n(20)X(0) =\n[ 0 A\nAT 0\n]\n(21)NAFS = ( ˆX(0) ⊗ ˆX(1) ⊗···⊗ ˆX(U ))\nPage 8 of 26Li et al. BMC Genomics           (2024) 25:73 \nSimilarly, we use NAFS to process and obtain the cor -\nresponding lncRNA-integrated similarity network node \nfeatures ˆZL ∈ RN l×N l and disease-integrated similarity \nnetwork node features ˆZD ∈ RNd×Nd . We perform the \nnode features in ˆZL affine, converting ˆZL and ˆZD to the \nsame dimension:\nwhere W LD ∈ RNd×Nl and bLD ∈ RNd are trainable param-\neters. We splice ˆZ′\nL and ˆZD to form a new node feature \nˆZLD =\n[ ˆZ′\nL\nˆZD\n]\n∈ R(N l+N d)×N d.\nNetwork structure encoding\nWe learn the structural encoding of the network as the \nstructural inductive bias and transfer it to the down -\nstream Transformer module for processing. Here, we \nencode the network structure using the SDNE approach \nprovided by Wang et  al. [45] to conduct additional \nresearch on the information in the network.\nIn the model we encode the structure of the network \nwith Inet and D net . Here we use Inet  as an example to \nillustrate the process of SDNE. SDNE is composed of a \ndecoder part and an encoder, where the decoder maps \nthe input network with multiple nonlinear functions \nand the decoder applies multiple nonlinear functions to \nreconstruct the network. In Inet = (V , E ) , the adjacency \nmatrix of the network is denoted by M , V denotes the \ncollection of lncRNA nodes within the network, where \n|V |= N l . Then, the mapping and reconstruction of the \nnetwork is performed as follows:\nwhere M i denotes the initial feature of the ith lncRNA \nin the network, σ(·) denotes the activation function, \nW (1 )\nl ∈ Rn1 ×Nl,b(1) ∈ Rn1,W (k)\nl ∈ Rnk×nk−1 and b(k) ∈ Rnk \nare the trainable parameters, and K is the number of lay -\ners of the decoder and encoder hidden layers. When y(k )\ni  \nis obtained, the encoder will be reused to map to obtain \nthe output ˆM i . To make SDNE capture a more accurate \nnetwork structure, second-order similarity and first-\norder similarity are used here to construct the loss func -\ntion of SDNE so that the error between the reconstructed \nnetwork and the original network is smaller, and the \nSDNE loss function Lsdne is calculated as follows:\n(22)ˆZ′\nL(i) = W LD ˆZL(i) + bLD\n(23)y(1)\ni = σ( W (1)\nl M i+ b(1))\ny(k)\ni = σ( W (k)\nl y(k−1)\ni + b(k)),k = 2,L,K\n(24)\nL2nd =\n∑N l\ni=1 P ( ˆM i− M i)e biP\n2\n2\n= P ( ˆM − M )e BP 2\nF\n.\nHere, ⊙ represents the Hadamard product. \nbi ={ bi,j}N l\nj=1 , if M (i, j)=0, b i,j=1; otherwise, b i,j = β> 1 . \nM represents the adjacency matrix of the network, \nM (i, j) represents the value of the ith row and jth col -\numn of the association matrix, and α is the hyperpa -\nrameter. Lreg is a regularization term proposed to avoid \noverfitting, which is calculated as follows:\nWe input a network G = (V , E) , where V denotes the \nset of nodes and E  denotes the set of edges. Encode \nthe network structure following the formulation in \nEq.  (23). Subsequently, decode the network structure \nby passing it through a decoding module, utilizing \nEq.  (26). Employ Eq.  (24) for the first-order loss func -\ntion, Eq.  (25) for the second-order loss function, and \nEq.  (27) for the regularization function to compute \nthe loss of the reconstructed network structure. This \ncomprehensive approach aims to enhance the accu -\nracy of the encoded network structure. Finally, output \nthe result y(k )\ni  obtained from the encoder. Inet and D net \ndenote lncRNA-integrated similarity network and dis -\nease-integrated similarity network. The final expression \nof the SDNE is as follows:\nwhere ˆM ∈ RNl×np and ˆD ∈ RNd×np , np = K /2 , and K \ndenotes the number of hidden layers in the decoder and \nencoder. We combine ˆM and ˆD into a new network struc-\nture coding SF =\n[ ˆM\nˆD\n]\n∈ R(Nl+Nd)×np.\nGlobal‑level embedding\nIn our model, we account for the limitations of the infor -\nmation contained in the local-level nodes. Therefore, we \nintroduce a Transformer [53] module to learn global-\nlevel node features and deeply explore the unknown \nassociations between diseases and lncRNAs from a global \nperspective. The Transformer is utilized in the domain of \ngraph neural networks and has significant implications \nfor the future development of graph neural networks. In \n(25)\nL1st =\n∑N l\ni,j=1 M (i,j)P y(k)\ni − y(k)\nj P 2\n2\n= M (i,j)P yi − yjP 2\n2\n(26)Lsdne = L2nd + αL1st + Lreg\n(27)Lreg = 1\n2\n∑K\nk=1 (PW (k)\nl P2\nF + P ˆW (k)\nl P2\nF )\n(28)ˆM ={ SDNE K\nL (M i)}N l\ni=1\n(29)ˆD ={ SDNE K\nD (D j)}N d\nj=1\nPage 9 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nNAGTLDA, we only need the Transformer encoder to \nlearn the feature embedding of the global-level nodes.\nWe take the node features ˆZG of the heterogeneous \nnetwork as input to the Transformer, which is first pro -\ncessed through the multi-head attention layer as follows:\nwhere W q\ni,  W k\ni,  W v\ni ∈ R(N l+N d )×((N l+N d )/nhead) are the \nparameters to be trained in the model and nhead repre -\nsents the quantity of multi-head attention heads. We \nobtain a set H i ={ H 1 ,H 2 ,··· ,H nhead } , and finally, we \nobtain the output H from the multi-head attention:\nwhere, W H ∈ R(Nl+Nd)×nh is the training parameter and \n⊕ represents the splicing operation. Then we feedforward \npropagate the output of the multi-head attention, and the \nfeedforward network is defined as follows:\nwhere σ( ·) represents a nonlinear activation function \n(LeakyReLU) and i denotes the quantity of hidden layers \nin the feedforward network. Here, given the initial input \nH , we can proceed to obtain the output X of the feedfor -\nward network:\nwhere W F1 ∈ Rnh×nd, W F2 ∈ Rnd×nh, bF1 ∈ Rnd and bF2 ∈ Rnh are \nthe training parameters.\nGlobal‑level embedding fusion\nWe have acquired local-level and global-level embeddings, \nand as it would be inefficient to combine these various \nembeddings using straightforward splicing or summing \noperations to produce the desired result, we continue to \nemploy Transformer’s decoder to carry out global-level \nnode embedding fusion representation. Transformer does \nnot employ the graph information transfer mechanism for \ngraph computation; as a result, the structural inductive bias \nof the network is introduced to Transformer to compensate \nfor the missing information transfer mechanism, result -\ning in excellent results for the model. Here, we employ two \nmulti-headed attention layers, the first of which handles \nnode embedding and the second of which incorporates \n(30)Qi= ˆZG W q\ni , Ki= ˆZG W k\ni , Vi= ˆZG W v\ni\n(31)H i = ( exp(Q iK T\ni /\n√\nd)\n∑nhead\nj=1 exp((Q jK T\nj )/\n√\nd)\n)Vi\n(32)H = (H1 ⊕ H2 ⊕···⊕ Hnhead) · W H\n(33)FFNi(T) = σ( TW Fi+ bFi)\n(34)Xm = LeakyRelu(HWF1 + bF1)\n(35)X = LeakyRelu(Xm W F2 + bF2)\nstructural inductive bias of the network for developing the \nfinal node embedding representation learning.\nFirst, we use the first multi-head attention layer to pro -\ncess the concatenation of the global-level embedding \nX and the local-level embedding ˆZLD . By applying the \nmulti-head attention Eqs. (30), (31), and (32) along with \nthe feedforward network Eq. (33) we obtain a new node \nembedding X F ∈ R(N l+N d)×n\n′\nh.\nThen, we use the second layer of multi-head attention \nto address the structural induction bias of the network. \nAfter concatenating the structural induction bias SF and \nnode embedding X F , we similarly utilize Eqs. (30), (31), \n(32) for multi-head attention and Eq. (33) for the feedfor-\nward network to obtain a new representation of the node \nembedding X S.\nWe utilized the rich information of the heterogene -\nous network and the topological structure of integrated \nsimilarities networks for lncRNAs and diseases to per -\nform node feature embedding learning at both local-level \nand global level. Simultaneously, we learned the struc -\ntural information of the network. Finally, we fuse them \nusing the Transformer structure to obtain the final node \nembedding representation X S ∈ R(N l+N d )×f.\nPredicting the association score between lncRNAs \nand diseases\nWe expressed the final node embedding expression as \nXS =\n[\nXS\nL\nXS\nD\n]\n , where X S\nL ∈ RN l×f indicates the ultimate \nnode feature embedding of lncRNAs and X S\nD ∈ RN d×f \nindicates the ultimate node feature embedding of dis -\neases. The reconstruction of the lncRNA-disease interac -\ntion matrix ˆA was performed using a bilinear decoder. \nThe bilinear decoder formula is defined as follows:\nwhere W B represents the trainable parameter matrix. We \ncan consider the lncRNA-disease link prediction task as \na simple binary classification problem, so binary cross-\nentropy loss is selected as the loss function for associa -\ntion prediction, which is calculated as follows:\nwhere (i, j) denotes the lncRNA and disease pairs, and \nthe sets of data that are negative and positive data are \nrepresented by I − and I + , respectively. Our model’s over-\nall loss function can be described as follows:\nwhere Ll_p stands for the loss function of the recon -\nstructed association matrix, whereas L1\nsdne and L2\nsdne \n(36)ˆA = sigmoid(X S\nLW BX S\nD )\n(37)\nL l_p =−\n∑\n(i,j)∈I+ ∪I− {A(i,j) ln ˆA( i,j) +[ 1 − ˆA( i,j)]ln[1 − ˆA( i,j)]}\n(38)Lm = Ll_p + L1\nsdne + L2\nsdne\nPage 10 of 26Li et al. BMC Genomics           (2024) 25:73 \nreflect, the loss functions represented by the structures of \nthe disease-integrated similarity and lncRNA-integrated \nsimilarity networks, respectively. In the overall optimi -\nzation of our model, we added the Adam optimizer [54]. \nTo achieve an equal distribution of negative and posi -\ntive samples during the training phase of our model, an \nequivalent quantity of negative data is randomly chosen \nto enter the training. The training process of NAGTLDA \nis shown in Algorithm 1.\nAlgorithm 1. Algorithm of our proposed method\nResults\nExperimental setting\nDuring our experimental process, we employed 5-fold \ncross-validation (5-CV) to test the performance of our \nproposed model. We partitioned the disease-lncRNA \npairs into five equal subsets, employing a four-to-one \nratio for training and testing, which facilitated five \ncross-validation iterations. In each round, we removed \nall known associations from the test set and evaluated \nthe performance of the trained model on the test sam -\nples. For selecting performance evaluation metrics, we \nadopted AUPR (area under precision-recall curve) and \nAUC (area under the receiver operating characteristic \ncurve) as the major markers. Additionally, we considered \nfive auxiliary reference metrics: recall, accuracy (ACC), \nF1-score, precision (Prec.), and specificity (Spec.). After \nconducting our 5-CV experiment, detailed results are \npresented in Table  1. Our model achieved an average \naccuracy of 0.8785 and average recall of 0.9088 on the \nexperimental dataset. The average specificity and preci -\nsion reached 0.8483 and 0.8578, respectively, while the \naverage F1-score reached 0.882. In particular, the AUC \nand AUPR for our model are shown in Fig.  2. The aver -\nage AUC and AUPR were 0.9531 and 0.9537, respectively. \nThe results of the 5-CV experiment demonstrate the \nexcellent performance of our proposed model in disease-\nlncRNA interaction prediction tasks.\nSeveral hyperparameters are included in the model, \nincluding the final embedding dimension (dim ), maxi -\nmum smoothing steps (k), learning rate (lr ), encoding \ndimension for SDNE (nhid), number of Transformer \nlayers (L1 and L2), number of attention heads for multi-\nhead attention (Head 1 and Head 2), r-value for NAFS, \nand weight decay for the optimizer. The best settings of \nhyperparameter optimization are presented in Table  2. \nThe optimal parameter values are bolded, and these opti -\nmal parameters were chosen based on the model AUC.\nParameter analysis\nDuring the process of setting hyperparameters, we \nfound that certain parameter values have a noticeable \nimpact on the model performance. For instance, we \nanalyzed the dimensions of the final node features, as \nshown in Fig.  3. We compared different dimension val -\nues ( dim ∈{ 32, 64, 128, 256, 512} ) and found that when \ndim = 64, the AUC and AUPR values are highest. Select -\ning an appropriate dimension to represent node features \nis crucial. If the dimension is too small, the distinguish -\nability between nodes may not be clear. However, if \nthe dimension is too large, it can result in a significant \namount of redundant information. Therefore, the choice \nof embedding dimension as a hyperparameter is also vital \nfor the model.\nThen, we analyzed the maximum number of smoothing \nsteps in NAFS, as shown in Fig.  4. The maximum num -\nber of smoothing steps indicates the number of neigh -\nbours aggregated in the process of aggregating neighbour \nTable 1 Results of NAGTLDA 5-CV\nFold AUC AUPR F1‑score ACC Recall Spec. Prec.\n1 0.9523 0.9543 0.8806 0.8803 0.8831 0.8775 0.8782\n2 0.9556 0.9559 0.8834 0.8756 0.9424 0.8089 0.8314\n3 0.9538 0.9547 0.8855 0.8794 0.9332 0.8256 0.8425\n4 0.9526 0.9523 0.8784 0.8756 0.8979 0.8534 0.8596\n5 0.9509 0.9510 0.8823 0.8817 0.8872 0.8761 0.8775\nAverage 0.9531 0.9537 0.8820 0.8785 0.9088 0.8483 0.8578\nPage 11 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nnodes, which is equivalent to aggregating multi-order \nneighbours. We found that when hops = 7, the values of \nAUC and AUPR are the highest. When hops are greater \nthan 7, they show a decreasing trend, and when they are \nless than 7, they show an increasing trend. After each \nsmoothing, the following node features will contain all \nthe previous smoothing information, so the number of \nsmoothing steps is also very important for the learning of \nfeature embedding.\nIn our model, we introduced the Transformer mod -\nule, which includes a multi-head attention mechanism \nthat provides us with a global perspective, enabling \nus to perform global-level embedding learning. We \nused two instances of the Transformer module in our \nmodel, and we found that different combinations of \nlayer numbers (L 1 and L 2) have a significant impact \non the model’s performance. As shown in Fig.  5a, dif -\nferent layer numbers affect the model’s AUC, while \nFig.  5b illustrates the impact of different values of L 1 \nand L2 on AUPR. The highest AUC value is achieved \nwhen the combination of (L 1, L2) is set to (10, 20), \nwhile the highest AUPR value is achieved when it is \nFig. 2 ROC curves and PR curves of NAGTLDA in 5-CV\nTable 2 Hyperparameter setting of NAGTLDA\nHyperparameter Setting\nNAFS Threshold of lncRNA network ɑ [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nThreshold of disease network β [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nMaximum smoothing steps k [2, 3, 4, 5, 6, 7, 8, 9]\nList of r value [{0,0.1,0.2,0.3,0.4,0.5}, {0.3,0.4,0.5}]\nSDNE First-order loss parameter alpha 1e-6\nCoding dimension nhid1 [32, 64, 128, 256]\nRegularization term parameters nu1 1e-5\nRegularization term parameters nu2 1e-4\nNAGTLDA Learning rate lr 0.001\nRandom seed 50\nDropout 0.4\nAdam optimizer weight-decay 5e-3\nNumber of layers of global-level embedding L1 [1, 5, 10, 15]\nNumber of layers of global-level embedding fusion L2 [15, 10, 20, 25]\nNumber of heads of global-level embedding H1 [4, 8, 16, 32]\nNumber of heads of global-level embedding fusion H2 [16, 32, 64, 128]\nFeature embedding size out-dim [32, 64, 128, 256, 512]\nEpoch 150\nPage 12 of 26Li et al. BMC Genomics           (2024) 25:73 \nset to (15, 10). Additionally, different combinations of \nthe quantity for the attention heads, Head 1 and Head 2, \nalso affect the prediction efficiency of the model. As \ndepicted in Fig.  6a, the varying combinations of Head 1 \nand Head2 influence the AUC values, with the highest \nvalue observed when it is set to (8, 64). In Fig.  6b, we \ncan observe that the highest AUPR value is achieved \nwhen the combination of Head 1 and Head 2 is (8, 64).\nPerformance comparison with different ratios\nThe different proportions of negative and positive sam -\nples in each fold of cross-validation can also impact the \nmodel’s performance. Therefore, we set the proportions \nbetween positive samples and negative samples in each \nfold as follows: positive samples: negative samples = {1:1, \n1:5, 1:10, random}, for experimental purposes. The \ndetailed outcomes of the studies are presented in Fig.  7. \nFig. 3 The effect of different embedding dimensions on the AUC and AUPR of NAGTLDA\nFig. 4 The effect of different maximal smoothing steps on the AUC and AUPR of NAGTLDA\nPage 13 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nWe can observe that when the ratio = 1:1, indicating a \nbalanced ratio of positive and negative samples, the AUC \nand AUPR values are the highest at 0.9531 and 0.9537, \nrespectively, but the corresponding accuracy is the low -\nest. When the ratio = 1:5, the AUC and AUPR values are \nslightly lower than those of the ratio = 1:1, but the accu -\nracy is slightly higher. When the ratio = 1:10, the AUC \nvalue is the lowest, but the accuracy is higher than the \nprevious ratios. When the ratio is set to random, the \nAUC value is ranked third, and the AUPR value is the \nlowest, but the accuracy is the highest at 0.9783.\nWe speculate that the reason for these results may \nbe due to the low proportion of positive samples in the \nexperimental dataset. If we balance the positive and \nnegative samples in each fold, it leads to the smallest \nquantity of training data in each fold, resulting in the \nlowest model accuracy. As the proportions between \npositive and negative samples decrease, the quantity of \ntraining data in each fold also decreases, leading to a \ndecrease in accuracy.\nPerformance comparison with other methods\nIn our experiments, we compared our model with six \nstate-of-the-art computational methods on a bench -\nmark dataset D1 using a 5-CV approach, which are as \nfollows:\nFig. 5 NAGTLDA performance under various Transformer layers\nFig. 6 NAGTLDA performance under various heads of multi-head attention\nPage 14 of 26Li et al. BMC Genomics           (2024) 25:73 \n• HGATLDA (2022) [55]: A meta-path-based hetero -\ngeneous graph attention network framework was \nused to perform interaction prediction between \ndiseases and lncRNAs by constructing disease, \nlncRNA, and gene heterogeneity networks.\n• SFGAE (2022) [56]: A graph self-encoder was uti -\nlized for feature learning of nodes and self-featured \nrepresentations of miRNAs and diseases were con -\nstructed for association prediction between miR -\nNAs and diseases.\n• VGAELDA (2021) [57]: An end-to-end computa -\ntional model based on a variational self-encoder \nand graph self-encoder was adopted to predict the \nrelationships between diseases and lncRNAs.\n• LAGCN (2020) [58]: A layer-attentive graph convo -\nlution network was used to synthesize multisource \nsimilarity to construct heterogeneous network for \nassociation prediction between drugs and diseases.\n• LDA-LNSUBRW (2020) [59]: A computational \nmethod based on unbalanced double random wan -\ndering and linear neighborhood similarity for asso -\nciation prediction between diseases and lncRNAs.\n• CNNLDA (2019) [29]: A dual convolutional neural \nnetwork model based on an attention mechanism that \nintegrates multiple sources of data was used to excavate \nthe associations between diseases and lncRNAs.\nFor benchmark dataset, the D1 downloaded from the \nLnc2Cancer [16], LncRNADisease [17] and GeneRIF [47]. \nThe dataset utilized in this study was sourced from the \nprevious research conducted by Fu et al. [46] on lncRNA-\ndisease association prediction. The dataset comprises 240 \nlncRNAs, 412 diseases, and 2,697 experimentally validated \nlncRNA-disease interactions. The semantic similarity data \nfor all diseases is obtained from MeSH.\nIn the benchmark dataset D1 experiments, we com -\npared different models using two evaluation metrics, \nnamely, AUC and AUPR, to facilitate better com -\nparison between models. The experimental results \nare presented in Table  3, where we highlight the \nFig. 7 The effect of different ratios of positive and negative samples on the performance of NAGTLDA\nTable 3 Performance comparison between our proposed method \nand six baselines under 5-CV settings\nModels AUC AUPR\nNAGTLDA 0.9531 0.9537\nHGATLDA 0.9421 0.9487\nCNNLDA 0.9402 0.9433\nSFGAE 0.9321 0.9183\nVGAELDA 0.9195 0.9347\nLAGCN 0.9099 0.8891\nLDA-LNSUBRW 0.8750 0.8868\nPage 15 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nhighest results. It can be observed that our proposed \nNAGTLDA model achieves the highest AUC and \nAUPR values. This improvement can be attributed to \nthe utilization of a Transformer for global learning dur -\ning the process of learning node features. NAGTLDA \noutperforms LDA-LNSUBRW by 8.92% in AUC and \n5.51% in AUPR. Figure  8 shows the AUC and AUPR \ncurves of all models obtained through 5-CV experi -\nments. It is evident from the figure that NAGTLDA \noutperforms other models in terms of performance. To \nvisually highlight the performance disparity between \nNAGTLDA and existing state-of-the-art methods, we \nconducted a significance analysis of their AUC values, \nrepresented in Fig.  9 (* denotes P  < 0.05, ** denotes \nFig. 8 ROC curve and PR curve of the proposed method and six baselines under the 5-CV settings\nFig. 9 Significance analysis of other models with NAGTLDA on the D1 dataset\nPage 16 of 26Li et al. BMC Genomics           (2024) 25:73 \nP < 0.01, *** denotes P < 0.001). Notably, the signifi -\ncance levels of NAGTLDA compared to other meth -\nods are consistently high, ranging from a minimum \nsignificance of P  < 0.05 to a maximum significance of \nP < 0.001. The improvement in the performance of our \nmodel has a significant enhancement for uncovering \nunknown lncRNA-disease associations. Hence, we can \ninfer that our proposed model demonstrates excellent \nperformance and serves as an effective computational \napproach for predicting disease-lncRNA associations.\nCompared with these state-of-the-art methods, our \nmodel exhibits a significant performance advantage, \nas confirmed in the experiments above. The enhance -\nment in performance can be attributed to the following \nunique contributions: NAFS is utilized to learn local \nfeatures of nodes, simplifying the model training pro -\ncess and enhancing effectiveness. Moreover, the incor -\nporation of network structure encoding enhances the \nefficiency of graph node information learning. Lastly, \nthe application of the Transformer architecture allows \nfor the learning of global information of nodes in the \ngraph. The global and local features are then adap -\ntively and efficiently fused using a multi-head attention \napproach, resulting in comprehensive feature informa -\ntion for diseases and lncRNAs.\nPerformance on other datasets\nTo further validate the performance and generalization \nability of the NAGTLDA model, we performed experi -\nments on a larger lncRNA-disease association dataset D2 \nand a miRNA-disease association dataset D3, as shown \nin Table 4.\n• D2: We screened the data from the databases of \nknown lncRNA-disease associations, including \nLncRNADisease v2.0 [60] and Lnc2Cancer v3.0 \n[61], known lncRNA-miRNA associations from \nEncori [62] and NPInter V4.0 [63], and known \nmiRNA-disease associations from HMDD v3.2 \n[64]. All disease names were converted to stand -\nard MeSH disease terms to facilitate the calculation \nof semantic similarity between the diseases. After \nremoving redundant data, the final merger yielded \n861 lncRNAs, 432 diseases, and 4516 known \nlncRNA-disease associations. The features used to \nmake semantic similarity of diseases in the model \nare obtained from MeSH.\n• D3: The known miRNA-disease association data \nwere downloaded from the HMDD v3.2 database \n[64], and we obtained 788 miRNAs, 374 diseases, \nand 8968 corresponding known associations from \nthe screening. The features used to make semantic \nsimilarity of diseases in the model are obtained from \nMeSH.\nWe conducted 5-fold cross-validation experiments on \nthe D2 and D3 datasets, and the results are presented in \nTable  5. Comparing the experimental outcomes of the \noriginal dataset with the D2 dataset, we observed that the \nmodel performs better on D2. This improved performance \ncan be attributed to the incorporation of the Transformer \nstructure into the NAGTLDA model, enhancing its per -\nformance on larger datasets. The Transformer, originally \ndesigned for large-scale natural language processing tasks, \nbrings notable advantages to our model, allowing it to excel \non larger datasets.\nOn the D3 dataset, we achieved remarkable results with \nAUC and AUPR values exceeding 0.94, while the F1-score \nreached 0.8746. These outcomes indicate that our model \npossesses strong generalization capabilities. It not only \nperforms well in predicting lncRNA-disease associations, \nwhich is the primary focus of our study, but also dem -\nonstrates high performance on other non-coding RNA \ndatasets.\nWe established independent validation sets to assess \nthe performance of our model, following the methodol -\nogy outlined by Fu et al. [65]. For the D1 dataset, which \ncontains 2697 positive samples, we initially selected \n20% of the positive samples and the same number of \nnegative samples to construct an independent balanced \nvalidation set (B-validation set). The remaining sam -\nples were utilized for training. Subsequently, we ran -\ndomly extracted 20% samples from the D1 dataset to \nTable 4 Details about datasets\nDatasets ncRNA Types ncRNAs Diseases Associations Sparsity\nD1 lncRNA 240 412 2697 2.728%\nD2 lncRNA 861 432 4516 1.214%\nD3 miRNA 788 374 8968 3.043%\nTable 5 NAGTLDA performance under D1 and D2 datasets\nDatasets AUC AUPR F1‑score ACC Recall Spec. Prec.\nD2 0.9630 0.9624 0.9177 0.9170 0.9258 0.9083 0.9103\nD3 0.9419 0.9437 0.8746 0.8724 0.8899 0.8548 0.8601\nPage 17 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \ncreate an unbalanced independent validation set (Unb-\nvalidation set), while the remaining samples served as \nthe training set. The experimental results on these two \nindependent validation sets are summarized in Table  6. \nWe assessed the model’s performance on the two inde -\npendent validation sets in comparison to its perfor -\nmance on the benchmark dataset. Notably, there was a \ndecrease in performance on the independent validation \nsets, specifically in terms of the two primary metrics, \nAUC and AUPR. Despite this decrease, the model still \ndemonstrated relatively good results. Furthermore, the \nAUC and AUPR on the unbalanced independent vali -\ndation set were slightly lower than those on the bal -\nanced validation set. This trend was observed in both \nbalanced and unbalanced datasets, suggesting the need \nto explore strategies for choosing an optimal ratio of \npositive and negative samples to enhance the com -\nprehensiveness of model comprehensiveness during \ntraining.\nAfter comparing NAGTLDA with other state-of-the-\nart models in previous experiments on the D1 dataset, \nwe extended our evaluation to two larger datasets, D2 \nand D3. We analyzed the significance of their AUC val -\nues, as illustrated in Figs.  10 and 11, to assess computa -\ntional efficiency and scalability across models. Notably, \nNAGTLDA exhibited remarkable significance compared \nto other models on both datasets, with particularly note -\nworthy results on the D2 dataset, where the significance \ncompared to other state-of-the-art models reached \nP < 0.001.\nThe reason for the strong scalability of our model \nis as follows: (1) Our model applied SDNE to learn the \nTable 6 Performance of NAGTLDA on D1 dataset and independent validation set\nDatasets AUC AUPR F1‑score ACC Recall Spec. Prec.\nD1 0.9531 0.9537 0.8820 0.8785 0.9088 0.8483 0.8578\nB-validation set 0.9509 0.9510 0.8823 0.8817 0.8872 0.8761 0.8775\nUnb-validation set 0.9505 0.5839 0.5437 0.9763 0.5250 0.9889 0.5717\nFig. 10 Significance analysis of other models with NAGTLDA on the D2 dataset\nPage 18 of 26Li et al. BMC Genomics           (2024) 25:73 \nstructure coding based on the specific network. (2) We \nleveraged the graph transformer structure to learn global \nlevel features, which can adaptively learn the features of \nnodes and has a very powerful learning capability. (3) We \nadded NAFS to learn local features to make the model \nmore scalable by flexibly learning the information of dif -\nferent nodes.\nHowever, there are some limitations of our proposed \nmodel on large dataset. Large datasets are commonly \nimbalanced in positive and negative samples, which \nrequires to introduce multi-source features to com -\npensate for the shortcomings of sparse positive sam -\nples. Moreover, there are many hyperparameters in \nthe model, and the model application on large data -\nsets may cause overfitting phenomenon for too many \nparameters.\nFeature visualization\nTo display the effectiveness of our proposed model \nmore specifically and graphically, we visualize the \nlncRNA-disease pair features learned by the model \nfor comparison. We used t-SNE [66] to downscale the \nlncRNA-disease pair features and plot them in the two-\ndimensional plane to compare the learned pair features \nwith the original pair features. As shown in Fig.  12, we \nvisualize the original pair features (left) and the learned \npair features (right). In the visualization, we distin -\nguish the negative samples from the positive samples \nwith different color dots, and we can observe that the \nlncRNA-disease pairs learned by NAGTLDA are more \nconcentrated and distinguishable than the original pos -\nitive and negative samples respectively. This also indi -\ncates that our model is meaningful and interpretable \nfor disease and lncRNA feature learning.\nAblation experiments\nTo assess the influence of each module on the model \nperformance and its importance, three sets of ablation \nexperiments were performed for validation.\nThe first set of ablation experiments is to remove \na module from the initial model to construct a com -\nparison model, and each new comparison model is \ndescribed as follows:\nFig. 11 Significance analysis of other models with NAGTLDA on the D3 dataset\nPage 19 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \n• Remove T1: Remove the Transformer module that \nperforms global-level embedding of heterogeneous \nnetworks.\n• Remove lncRNA-NAFS: Remove the NAFS module \nthat performs local-level embedding of the lncRNA-\nintegrated similarity network.\n• Remove disease-NAFS: Remove the NAFS module \nthat performs local-level embedding of the disease-\nintegrated similarity network.\n• Remove lncRNA-SDNE: Remove the SDNE module \nthat encodes the structure of the lncRNA-integrated \nsimilarity network.\n• Remove disease-SDNE: Remove the SDNE module \nthat encodes the disease-integrated similarity net -\nwork structure.\nThe results obtained from the experiments are pre -\nsented in Fig. 13 and Table 7, and the original NAGTLDA \nFig. 12 Comparison of visualization features of lncRNA-disease pairs obtained by NAGTLDA and the original\nFig. 13 Comparison between NAGTLDA and multiple variant models\nPage 20 of 26Li et al. BMC Genomics           (2024) 25:73 \nmodel has excellent results compared to other compara -\nble models. For example, on both the AUC and AUPR, \nNAGTLDA outperforms remove disease-SDNE by val -\nues of 0.0181 and 0.0133, respectively. We observe that \nencoding the network structure information exerts \nthe most significant impact on the overall model per -\nformance. Consequently, the acquisition of node-level \ninformation within the network holds great importance. \nHowever, a comprehensive understanding of the net -\nwork’s structural information also emerges as a vital \ncomponent. The overall performance of the new model \nformed by removing a module is lower than that of the \noriginal model, thus proving the effectiveness of our use \nof Transformer layer for global-level embedding, NAFS \nfor local-level embedding, and SNDE for network struc -\nture encoding.\nThe second set of ablation experiments was con -\nducted by replacing the method used for local-level \nembedding in the model with the classical GCN and \nGAT in graph neural networks to construct the com -\nparison models: NAGTLDA_gcn and NAGTLDA_gat. \nAs shown in Table  8 and Fig.  14, NAGTLDA performs \nbetter than the variant model. Specifically, NAGTLDA \nis 0.0106 higher than NAGTLDA_gcn in terms of AUC \nvalue, 0.0079 higher than NAGTLDA_gat in terms \nof AUPR, and 0.0158 higher than NAGTLDA_gcn in \nTable 7 Performance between NAGTLDA and multiple variant models\nModels AUC AUPR F1‑score ACC Recall Spec. Prec.\nNAGTLDA 0.9531 0.9537 0.8821 0.8786 0.9088 0.8483 0.8579\nRemove T1 0.9462 0.9479 0.8736 0.8720 0.8850 0.8591 0.8628\nRemove lncRNA-NAFS 0.9510 0.9517 0.8777 0.8728 0.9139 0.8316 0.8451\nRemove disease-NAFS 0.9520 0.9528 0.8809 0.8767 0.9113 0.8420 0.8523\nRemove lncRNA-SDNE 0.9394 0.9438 0.8683 0.8672 0.8754 0.8590 0.8617\nRemove disease-SDNE 0.9350 0.9404 0.8617 0.8611 0.8646 0.8576 0.8601\nTable 8 Performance of NAGTLDA based on different local-level embeddings methods\nModels AUC AUPR F1‑score ACC Recall Spec. Prec.\nNAGTLDA 0.9531 0.9537 0.8821 0.8786 0.9088 0.8483 0.8579\nNAGTLDA_gcn 0.9425 0.9458 0.8658 0.8628 0.8857 0.8398 0.8470\nNAGTLDA_gat 0.9493 0.9507 0.8757 0.8713 0.9065 0.8363 0.8470\nFig. 14 Comparison results of NAGTLDA, NAGTLDA_gcn and NAGTLDA_gat\nPage 21 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \naccuracy. NAGTLDA compared to NAGTLDA_gcn \nand NAGTLDA_gat in F1- score is the highest, and the \nF1-score is a benchmark indicator for the comprehen -\nsive ability of the model, so the original model is a better \nchoice. Combining the outcomes of the first set of abla -\ntion experiments and the present set of experiments, it \ncan be concluded that using NAFS for embedding learn -\ning of node features is an efficient learning method, and \nit also proves the effectiveness and efficiency of using \nNAFS in the whole model.\nThe third set of ablation experiments is conducted for \nNAFS. We input a set of r values to obtain a set of dif -\nferent node feature representations, and we can use \ndifferent ways to process this set of node feature repre -\nsentations. NAGTLDA_concat, NAGTLDA_max and \nNAGTLDA_simple represent the use of concatenate, \nmax and simple operations, respectively. The simple \noperation means inputting only one r value to one exper -\nimental result. The detailed experimental outcomes are \npresented in Fig.  15 and Table  9. Six of the seven evalu -\nation metrics in the experimental results are the highest \nwhen the mean operation is used.\nCase study\nIn the previous sections, we tested and confirmed the \neffectiveness of NAGTLDA. Now, we evaluate NAGTL -\nDA’s ability to excavate unknown relationships between \ndiseases and lncRNAs. We chose four common diseases, \nwhich are prostate cancer, colon cancer, breast cancer, \nand colorectal cancer, as case studies from the dataset. \nWe trained the model with 2797 observed lncRNA-dis -\nease relationships as instances for training and then made \npredictions for unknown potential associations. We \nextracted the top 15 candidate lncRNAs for each disease \nand validated the results using three benchmark data -\nbases: LncRNADisease v2.0 [60], Lnc2Cancer 3.0 [61], \nand MNDR v3.1 [67].\nThe exact cause of colon cancer is still unknown, but \nstudies and research have shown that the risk of develop -\ning the disease increases with age, obesity, and cancer in \nother parts of the body. As research continued, research -\ners found that colon cancer is closely linked to several \nlncRNAs. For example, CYTOR and the corresponding \nprotein binding can contribute to the metastasis of colon \ncancer [68], and HOXB-AS3 expression can inhibit the \ngrowth of colon cancer [69]. The experimental outcomes \nare presented in Table  10, where 14 of the top 15 candi -\ndate lncRNAs have been confirmed.\nThe most prevalent malignancy is prostate cancer \nin the male urological system, which is highly preva -\nlent in older men, but its etiology has not yet been fully \nidentified. Researchers have found that prostate can -\ncer is closely related to the expression of lncRNAs. For \nexample, the expression of MAGI2-AS3 and MEG3 in \nFig. 15 Comparison results of NAGTLDA, NAGTLDA_concat, NAGTLDA_max and NAGTLDA_simple\nTable 9 Performance of NAFS based on different fusion methods\nModels AUC AUPR F1‑score ACC Recall Spec. Prec.\nNAGTLDA_mean 0.9531 0.9537 0.8821 0.8786 0.9088 0.8483 0.8579\nNAGTLDA_concat 0.9498 0.9506 0.8778 0.8739 0.9054 0.8424 0.8521\nNAGTLDA_max 0.9515 0.9521 0.8792 0.8739 0.9177 0.8301 0.8450\nNAGTLDA_simple 0.9525 0.9535 0.8821 0.8785 0.9087 0.8483 0.8573\nPage 22 of 26Li et al. BMC Genomics           (2024) 25:73 \nlncRNAs inhibits the development of prostate cancer \n[70, 71], and MNX1-AS1 indirectly promotes the devel -\nopment of prostate cancer through expression [72]. We \nused it as the second disease in the case study, and the \nexperimental outcomes are presented in Table  11. Thir-\nteen of the top 15 candidate lncRNA species we identi -\nfied have been confirmed by the relevant literature.\nBreast cancer is the most common cancer among \nwomen. According to research, obesity, excessive alco -\nhol consumption, and overnutrition all increase the inci -\ndence of breast cancer, but thus far, medical researchers \nhave not found the exact cause of cancer. With the per -\nsistent expansion of bioclinical technology, growing \nnumber of lncRNAs related to breast cancer have been \ndiscovered. For example, the distant metastasis-free sur -\nvival, overall survival, and progression-free survival of \nbreast cancer patients are strongly associated with high \nexpression of BCAR4, LUCAT1, and TINCR [73–75]. \nLINC00511 binds to the MMP13 protein to promote \nbreast cancer cell migration and proliferation [76]. We \nused breast cancer as the third type of disease in the case \nstudy, and the experimental outcomes are presented in \nTable 12. All of top 15 candidate lncRNAs have been vali-\ndated by the relevant literature.\nColorectal cancer is the third most common malig -\nnancy in the world, and its incidence is relatively \nTable 10 The top 15 predicted lncRNAs associated with colon cancer\nLncRNA name Evidence Rank LncRNA name Evidence Rank\nGAS5 PMID:28722800 1 DANCR PMID:30127873, 32423468 9\nPVT1 PMID:25043044, 29552759 2 KCNQ1OT1 PMID:31040703 10\nUCA1 PMID:17416635, 26885155 3 HULC PMID:27496341, 30551459 11\nCDKN2B-AS1 PMID:23416462, 33529508 4 XIST PMID:29679755 12\nNEAT1 PMID:26164760, 31173354 5 AFAP1-AS1 PMID:30588252 13\nTUG1 PMID:27634385, 31697952 6 BCYRN1 PMID:29625226 14\nHOTTIP Unknown 7 MIR155HG PMID:27821766 15\nMIR17HG PMID:35249533 8\nTable 11 The top 15 predicted lncRNAs associated with prostate cancer\nLncRNA name Evidence Rank LncRNA name Evidence Rank\nMIR17HG PMID:27556357 1 CCAT1 PMID:28945760, 29863242 9\nXIST PMID:16261845, 27507663 2 WT1-AS Unknown 10\nHCP5 PMID:31746434, 34285549 3 CCAT2 PMID:27558961, 28244168 11\nBCYRN1 PMID:32705287 4 SOX2-OT PMID:31623830, 32407168 12\nGHET1 PMID:30609158 5 LINC00675 PMID:30963639 13\nBANCR Unknown 6 CASC2 PMID:29373811 14\nAFAP1-AS1 PMID:31081081, 31669642 7 SPRY4-IT1 PMID:25307116, 26503110 15\nTP53COR1 PMID:25999983, 27976428 8\nTable 12 The top 15 predicted lncRNAs associated with breast cancer\nLncRNA name Evidence Rank LncRNA name Evidence Rank\nTUG1 PMID:27791993, 27848085 1 MIR155HG PMID:23246696, 32165090 9\nHULC PMID:27986124, 30957286 2 HNF1A-AS1 PMID:31837323, 32319789 10\nMIR17HG PMID:36943627 3 TP53COR1 PMID:22487937, 26656491 11\nBANCR PMID:29565494, 29805676 4 HCP5 PMID:32165090 12\nIGF2-AS PMID:31319040, 33175607 5 PCAT1 PMID:28989584, 31319040 13\nDANCR PMID:27716745, 28978036 6 GHET1 PMID:29843220, 30787968 14\nWT1-AS PMID:18708366 7 CASC2 PMID:29523222, 30106139 15\nNPTN-IT1 PMID:30280783 8\nPage 23 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \nsimilar in men and women. The majority of the popu -\nlation suffers from the disease due to lifestyle habits, \nand a very small percentage is due to genetic factors. \nColorectal cancer ranks second in the number of \ndeaths caused by malignant tumors. Researchers have \nfound through numerous clinical trials that ITGB8-\nAS1 combined with the corresponding signals can \ncontribute to the growth and metastasis of colorectal \ncancer [77] and that GAS5 and YAP phosphorylation \nand degradation interact to inhibit the development of \ncolorectal cancer [78]. We used it as the fourth disease \nin our case study, and the experimental outcomes are \npresented in Table  13, where 13 of the top 15 candi -\ndate lncRNAs we selected have been validated by the \nrelevant literature.\nDiscussion\nIn the present paper, we designed a NAGTLDA compu -\ntational model to make inferences about unknown inter -\nactions between lncRNAs and diseases. Based on the \nexperimental results, our model demonstrates promis -\ning performance, particularly in handling large datasets. \nThe high scalability across varying sizes of datasets can \nbe ascribed to the utilization of the graph Transformer \narchitecture for extracting feature representations. This \narchitecture possesses a highly expressive and adaptive \nlearning capability, enabling it to learn diverse networks \neffectively.\nHowever, our proposed model and the current study \nhave some limitations. The limitations of our model are \nas follows: (1) The main framework of our model is built \nupon the Transformer architecture, requiring consider -\nable computational power during the training process, \nparticularly in practical applications involving large data -\nsets. (2) The existence of numerous hyperparameters \nnecessitates meticulous optimization and tuning, thereby \naugmenting the complexity of the training process. (3) \nOur model also relies on the initial similarity features of \nthe nodes, which are calculated based on the association \nmatrix. There are some limitations in the present field \nof lncRNA-disease association prediction as follows:(1) \nThere are no true negative samples in the experimen -\ntal data, and all the biological data are looking for true \npositive samples and not paying much attention to nega -\ntive samples. Negative samples may be correct or they \nmay be undetected false negatives. (2) The experimental \nresults of computational modeling do not correlate very \nwell with biological experiments, and better integration \nof computational modeling and biological experiments \nmakes the results better interpretable. In future research, \nwe can start by studying the dataset and exploring how to \nbetter represent the correlations between entities, which \nwill result in a more accurate discovery of unknown asso-\nciations. In addition, as medical science and technology \ncontinue to advance, the discovery of more unknown \nlncRNAs, represented as isolated nodes, is anticipated. \nMoving forward, there is a pressing need to develop more \ncomprehensive models that can accurately predict the \nassociations between these isolated nodes and experi -\nmentally verified disease nodes.\nConclusions\nIn the model, we first framed a heterogeneous network \nconsisting of diseases and lncRNAs, an integrated sim -\nilarity network for diseases and an integrated similar -\nity network for lncRNAs, and used NAFS to perform \nnode-level embedding for each of the three networks. \nWe also adopted SDNE to encode the structural infor -\nmation of the networks with the goal of utilizing the \nconstructed networks more effectively. We then intro -\nduce the Transformer module for global-level embed -\nding to explore potential unknown associations in the \ndataset and utilize the Transformer fusion mechanism \nwith two levels of attention to perform global-level \nembedding fusion on the learned embeddings and \nnetwork topology. We performed embedding learn -\ning on the network information from both local and \nTable 13 The top 15 predicted lncRNAs associated with colorectal cancer\nLncRNA name Evidence Rank LncRNA name Evidence Rank\nSPRY4-IT1 PMID:27391336, 27621655 1 LINC00687 Unknown 9\nMIR17HG PMID:31409641 2 IGF2-AS PMID:32853944 10\nCDKN2B-AS1 PMID:26708220, 27286457 3 TRERNA1 PMID:31933996, 33833618 11\nZEB1-AS1 PMID:28618933, 28967064 4 MIR194-2HG Unknown 12\nPANDAR PMID:27629879, 28106228 5 LINC00974 PMID:35907803 13\nHNF1A-AS1 PMID:28791380, 29145164 6 CYTOR PMID:27633443, 28078002 14\nWT1-AS PMID:30714675 7 PCAT1 PMID:23640607, 27591862 15\nDBH-AS1 PMID:33549042 8\nPage 24 of 26Li et al. BMC Genomics           (2024) 25:73 \nglobal perspectives so that some potential associations \ncan be better identified. Finally, a bilinear decoder is \nemployed to fuse the node embedding representa -\ntions of diseases and lncRNAs as input for lncRNA \nand disease association prediction. We also con -\nducted experiments on the performance of our model, \nand the outcomes of the 5-CV and contrast to other \nbaseline models confirm the excellent performance \nof our model. In the case study, NAGTLDA success -\nfully predicted associations, such as NEAT1-colon \ncancer, SOX2-OT-prostate cancer, and WT1-AS-\ncolorectal cancer, which were previously unknown in \nthe dataset. He et al. [79] investigated the function of \nNEAT1 in colon cancer, and found that the expression \nof NEAT1 was significantly elevated in colon cancer \ncells in their experiments, which proved that NEAT1 \nindirectly promotes the occurrence of colon cancer. \nSong et  al. [80] demonstrated that SOX2-OT inhib -\nits the proliferation and metastasis of prostate cancer \ncells by interacting with other non-coding RNAs. This \ndiscovery provides a new therapeutic approach for the \ntreatment of prostate cancer. Zhang et al. [81] experi -\nmentally demonstrated experimentally that WT1-AS \nwas closely associated with overall survival in colorec -\ntal cancer. The correlation between WT1-AS and colo -\nrectal cancer was demonstrated on clinicopathological \nfeatures and data modeling analysis, and WT1-AS can \nbe used as a biomarker and therapeutic target for colo -\nrectal cancer prognosis. This proves that our proposed \nmodel performs very well in finding new therapeutic \nstrategies for diseases and provides a solid foundation \nfor biological experiments and clinical practice.\nAbbreviations\nlncRNA  Long non-coding RNA\nNAFS  Node-adaptive feature smoothing\nSDNE  Structural Deep Network Embedding\nGCN  Graph Convolutional Network\nDAG  Directed acyclic graph\nAUPR  Area under precision-recall curve\nAUC   Area under the receiver operating characteristic curve\n5-CV  5-Fold cross validation\nDO  Disease Ontology\nCNN  Convolution Neural Network\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nGL: conceived the study, analyzed the results, drafted the article. PB: collected \nthe data, designed and performed the experiments, drafted the article. CL: \nrevised the article. JL: supervised the study, revised the article. All authors read \nand approved the final manuscript.\nFunding\nThis work is supported by the National Natural Science Foundation of China \n[grant numbers 62362034, 61862025] and the Natural Science Foundation of \nJiangxi Province of China [grant numbers 20232ACB202010, 20212BAB202009, \n20181BAB211016].\nAvailability of data and materials\nFor lncRNA-disease, the D1 dataset downloaded from the Lnc2Cancer [13]: \nhttp:// www. bio- bigda ta. net/ lnc2c ancer, LncRNADisease [14]: http:// cmbi. \nbjmu. edu. cn/ lncrn adise ase and GeneRIF [38]: https:// ftp. ncbi. nlm. nih. gov/ \ngene/ GeneR IF/, the D2 dataset screened from the databases of known \nlncRNA-disease associations, including LncRNADisease v2.0 [51]: http:// www. \nrnanut. net/ lncrn adise ase, and Lnc2Cancer v3.0 [52]: http:// www. bio- bigda ta. \nnet/ lnc2c ancer, known lncRNA-miRNA associations from Encori [53]: http:// \nstarb ase. sysu. edu. cn/. and NPInter V4.0 [54]: http:// bigda ta. ibp. ac. cn/ npint er, \nand known miRNA-disease associations from HMDD v3.2 [55]: http:// cuilab. \ncn/ hmdd.\nThe miRNA-disease associations D3 are downloaded from the HMDD v3.2 \ndatabase [55]: http:// cuilab. cn/ hmdd.\nThe semantic similarity data for all diseases is obtained from MeSH at http:// \nwww. nlm. nih. gov.\nThe code of NAGTLDA is provided on GitHub (https:// github. com/ ghli16/ \nNAGTL DA).\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1 School of Information Engineering, East China Jiaotong University, Nanchang, \nChina. 2 School of Information Science and Engineering, Shandong Normal \nUniversity, Jinan, China. 3 College of Computer Science and Electronic Engi-\nneering, Hunan University, Changsha, China. \nReceived: 31 July 2023   Accepted: 9 January 2024\nReferences\n 1. Derrien T, Johnson R, Bussotti G, et al. The GENCODE v7 catalog of human \nlong noncoding RNAs: analysis of their gene structure, evolution, and \nexpression. Genome Res. 2012;22:1775–89.\n 2. Guttman M, Rinn JL. Modular regulatory principles of large non-coding \nRNAs. Nature. 2012;482:339–46.\n 3. Wang Kevin C, Chang HY. Molecular mechanisms of long noncoding \nRNAs. Mol Cell. 2011;43:904–14.\n 4. Wapinski O, Chang HY. Long noncoding RNAs and human disease. Trends \nCell Biol. 2011;21:354–61.\n 5. Chen X, Yan CC, Zhang X, et al. Long non-coding RNAs and complex \ndiseases: from experimental results to computational models. Brief Bioin-\nform. 2016;22:558–76.\n 6. Vincent-Salomon A, Ganem-Elbaz C, Manié E, et al. X inactive-specific \ntranscript RNA coating and genetic instability of the X chromosome in \nBRCA1 breast tumors. Cancer Res. 2007;67:5134–40.\n 7. Chen W, Böcker W, Brosius J, et al. Expression of neural BC200 RNA in \nhuman tumours. J Pathol. 1997;183:345–51.\n 8. Congrains A, Kamide K, Oguro R, et al. Genetic variants at the 9p21 \nlocus contribute to atherosclerosis through modulation of ANRIL and \nCDKN2A/B. Atherosclerosis. 2012;220:449–55.\n 9. Spagnolo P , Kropski JA, Jones MG, Lee JS, Rossi G, Karampitsakos T, et al. \nIdiopathic pulmonary fibrosis: disease mechanisms and drug develop-\nment. Pharmacol Ther. 2021;222:107798.\n 10. Gavrilov K, Mark Saltzman W. Therapeutic siRNA: principles, challenges, and \nstrategies. The Yale journal of biology and medicine. 2012;85:187–200.\n 11. Markowitz RHG, LaBella AL, Shi M, Rokas A, Capra JA, Ferguson JF, et al. \nMicrobiome-associated human genetic variants impact phenome-wide \ndisease risk. In: Proceedings of the National Academy of Sciences. 2022. p. 119.\nPage 25 of 26\nLi et al. BMC Genomics           (2024) 25:73 \n \n 12. Jimeno-Yepes AJ, Sticco JC, Mork JG, et al. GeneRIF indexing: sentence \nselection based on machine learning. BMC Bioinformatics. 2013;14:171.\n 13. Piñero J, Saüch J, Sanz F, et al. The DisGeNET cytoscape app: exploring \nand visualizing disease genomics data. Comput Struct Biotechnol J. \n2021;19:2960–7.\n 14. Bello SM, Shimoyama M, Mitraka E, et al. Augmenting the disease ontol-\nogy improves and unifies disease annotations across species. Dis Model \nMech. 2018. https:// doi. org/ 10. 1242/ dmm. 032839.\n 15. Chen J, Lin J, Hu Y, et al. RNADisease v4. 0: an updated resource of RNA-\nassociated diseases, providing RNA-disease analysis, enrichment and \nprediction. Nucleic Acids Res. 2023;51:D1397–404.\n 16. Ning S, Zhang J, Wang P , et al. Lnc2Cancer: a manually curated database \nof experimentally supported lncRNAs associated with various human \ncancers. Nucleic Acids Res. 2015;44:D980–5.\n 17. Chen G, Wang Z, Wang D, et al. LncRNADisease: a database for long-non-\ncoding RNA-associated diseases. Nucleic Acids Res. 2012;41:D983–6.\n 18. Sheng N, Huang L, Lu Y, et al. Data resources and computational \nmethods for lncRNA-disease association prediction. Comput Biol Med. \n2023;153:106527–37.\n 19. Lei X, Mudiyanselage TB, Zhang Y-C. A comprehensive survey on compu-\ntational methods of non-coding RNA and disease association prediction. \nBrief Bioinformatics. 2021;22(4):bbaa350.\n 20. Ganegoda GU, Li M, Wang W, et al. Heterogeneous network model to \ninfer human disease-long intergenic non-coding RNA associations. IEEE \nTrans Nanobiosci. 2015;14:175–83.\n 21. Zhou M, Wang X, Li J, et al. Prioritizing candidate disease-related long \nnon-coding RNAs by walking on the heterogeneous lncRNA and disease \nnetwork. Mol BioSyst. 2015;11:760–9.\n 22. Chen X, You Z-H, Yan G-Y, et al. IRWRLDA: improved random walk \nwith restart for lncRNA-disease association prediction. Oncotarget. \n2016;7:57919–31.\n 23. Lu C, Yang M, Luo F, et al. Prediction of lncRNA–disease associations \nbased on inductive matrix completion. Bioinformatics. 2018;34:3357–64.\n 24. Li G, Luo J, Liang C, et al. Prediction of LncRNA-disease associations based \non network consistency projection. Ieee Access. 2019;7:58849–56.\n 25. Gu C, Liao B, Li X, et al. Global network random walk for predicting poten-\ntial human lncRNA-disease associations. Sci Rep. 2017;7:12442.\n 26. Wang L, Shang M, Dai Q, He P . Prediction of lncRNA-disease association \nbased on a Laplace normalized random walk with restart algorithm on \nheterogeneous networks. BMC Bioinformatics. 2022;23(1):1–20.\n 27. Li J, Zhao H, Xuan Z, Yu JZ, Yang C, Liao B, et al. A novel approach for \npotential human LncRNA-disease association prediction based on local \nrandom walk. IEEE ACM Trans Comput Biol Bioinf. 2021;18:1049–59.\n 28. Zhang J-P , Zhang Z, Chen Z, Deng L. Integrating multiple heterogeneous \nnetworks for novel LncRNA-disease association inference. IEEE/ACM Trans \nComput Biol Bioinform. 2019;16:396–406.\n 29. Xuan P , Cao Y, Zhang T, et al. Dual convolutional neural networks with \nattention mechanisms based method for predicting disease-related \nlncRNA genes. Front Genet. 2019;10:416.\n 30. Yang Q, Li X. BiGAN: LncRNA-disease association prediction based \non bidirectional generative adversarial network. BMC Bioinformatics. \n2021;22(1):357.\n 31. Zhang Y, Ye F, Gao X. MCA-Net: multi-feature coding and attention convo-\nlutional neural network for predicting lncRNA-disease association. IEEE/\nACM Trans Comput Bio Bioinform. 2022;19:2907–19.\n 32. Xuan P , Gong Z, Cui H, et al. Fully connected autoencoder and convolu-\ntional neural network with attention-based method for inferring disease-\nrelated lncRNAs. Brief Bioinform. 2022;23(3):bbac089.\n 33. Sheng N, Cui H, Zhang T, et al. Attentional multi-level representa-\ntion encoding based on convolutional and variance autoencod-\ners for lncRNA–disease association prediction. Brief Bioinformatics. \n2021;22:bbaa067.\n 34. Wang L, Zhong C. gGATLDA: lncRNA-disease association prediction \nbased on graph-level graph attention network. BMC Bioinformatics. \n2022;23(1):11.\n 35. Ai C, Yang H, Guo F, et al. A multi-layer multi-kernel neural network for \ndetermining associations between non-coding RNAs and diseases. \nNeurocomputing. 2022;493:91–105.\n 36. Wu Q, Cao R, Xia J, Ni J, Zheng C-H, Su Y. Extra trees method for predicting \nLncRNA-disease association based on multi-layer graph embedding \naggregation. IEEE/ACM Trans Comput Biol Bioinform. 2022;19:3171–8.\n 37. Sheng N, Huang L, Wang Y, Zhao J, Xuan P , Gao L, et al. Multi-channel \ngraph attention autoencoders for disease-related lncRNAs prediction. \nBrief Bioinform. 2022;23(2):bbab604.\n 38. Lan W, Wu X, Chen Q, Peng W, Wang J, Chen YP . GANLDA: Graph attention \nnetwork for lncRNA-disease associations prediction. Neurocomputing. \n2022;469:384–93.\n 39. Ying C, Cai T, Luo S, et al. Do transformers really perform bad for graph \nrepresentation? Arxiv preprint. 2021;arXiv:2106.05234.\n 40. Rampášek L, Galkin M, Dwivedi VP , et al. Recipe for a general, pow-\nerful, scalable graph transformer. Adv Neural Inf Process Syst. \n2022;35:14501–15.\n 41. Oono K, Suzuki T. Graph neural networks exponentially lose expressive \npower for node classification. In: International conference on learning \nrepresentations. 2020.\n 42. Zhu J, Rossi RA, Rao A, et al. Graph neural networks with heterophily. \nAAAI. 2021;35:11168–76.\n 43. Chen D, O’bray L, Borgwardt K. Structure-aware transformer for graph \nrepresentation learning. In: Proceedings of the 39th International Confer-\nence on Machine Learning, PMLR. Vol. 162. 2022. p. 3469–89.\n 44. Zhang W, Sheng Z, Yang M, et al. NAFS: a simple yet tough-to-beat \nbaseline for graph representation learning. In: Proceedings of the 39th \nInternational Conference on Machine Learning (ICML). Vol. 162. 2022. p. \n26467–26483.\n 45. Wang D, Cui P , Zhu W. Structural Deep Network Embedding. In: Proceedings \nof the 22nd ACM SIGKDD international conference on knowledge discovery \nand data mining. 2016. https:// doi. org/ 10. 1145/ 29396 72. 29397 53.\n 46. Fu G, Wang J, Domeniconi C, et al. Matrix factorization-based data \nfusion for the prediction of lncRNA–disease associations. Bioinformatics. \n2017;34:1529–37.\n 47. Lu Z, Bretonnel Cohen K, Hunter L. GeneRIF quality assurance as sum-\nmary revision. Pac Symp Biocompute. 2006. https:// doi. org/ 10. 1142/ \n97898 12772 435_ 0026.\n 48. Chen X, Clarence Yan C, Luo C, et al. Constructing lncRNA functional \nsimilarity network based on lncRNA-disease associations and disease \nsemantic similarity. Sci Rep. 2015;5:11338.\n 49. Wang D, Wang J, Lu M, et al. Inferring the human microRNA functional \nsimilarity and functional network based on microRNA-associated dis-\neases. Bioinformatics. 2010;26:1644–50.\n 50. Xuan P , Han K, Guo M. Prediction of microRNAs associated with human \ndiseases based on weighted k most similar neighbors. PLoS One. \n2013;8:e70204.\n 51. van Laarhoven T, Nabuurs SB, Marchiori E. Gaussian interaction \nprofile kernels for predicting drug–target interaction. Bioinformatics. \n2011;27:3036–43.\n 52. Davies H, Jones B. Attention all surveyors: our schools need you. Struct \nSurv. 1994;12:31–4.\n 53. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. 2017.\n 54. Kingma D, Ba J. Adam: a method for stochastic optimization. Comput Sci. \n2014. https:// doi. org/ 10. 48550/ arXiv. 1412. 6980.\n 55. Zhao X, Zhao X, Yin M. Heterogeneous graph attention network based on \nmeta-paths for lncRNA–disease association prediction. Brief Bioinform. \n2022;23:bbab407.\n 56. Ma M, Na S, Zhang X, et al. SFGAE: a self-feature-based graph autoen-\ncoder model for miRNA–disease associations prediction. Brief Bioinform. \n2022;23(5):bbac340.\n 57. Shi Z, Zhang H, Jin C, et al. A representation learning model based on \nvariational inference and graph autoencoder for predicting lncRNA-\ndisease associations. BMC Bioinformatics. 2021;22(1):136.\n 58. Yu Z, Huang F, Zhao X, et al. Predicting drug–disease associations \nthrough layer attention graph convolutional network. Brief Bioinform. \n2021;22:bbaa243.\n 59. Xie G, Jiang J, Sun Y. LDA-LNSUBRW: lncRNA-disease association predic-\ntion based on linear neighborhood similarity and unbalanced bi-random \nwalk. IEEE/ACM Trans Comput Biol Bioinf. 2020;22:1–1.\n 60. Bao Z, Yang Z, Huang Z, et al. LncRNADisease 2.0: an updated data-\nbase of long non-coding RNA-associated diseases. Nucleic Acids Res. \n2018;47:D1034-7.\n 61. Gao Y, Shang S, Guo S, et al. Lnc2Cancer 3.0: an updated resource for \nexperimentally supported lncRNA/circRNA cancer associations and \nweb tools based on RNA-seq and scRNA-seq data. Nucleic Acids Res. \n2021;49:D1251-8.\nPage 26 of 26Li et al. BMC Genomics           (2024) 25:73 \n 62. Li J-H, Liu S, Zhou H, et al. starBase v2.0: decoding miRNA-ceRNA, miRNA-\nncRNA and protein–RNA interaction networks from large-scale CLIP-Seq \ndata. Nucleic Acids Res. 2014;42:D92-7.\n 63. Teng X, Chen X, Xue H, et al. NPInter v4.0: an integrated database of \nncRNA interactions. Nucleic Acids Res. 2019;48:D160-5.\n 64. Huang Z, Shi J, Gao Y, et al. HMDD v3.0: a database for experimentally \nsupported human microRNA–disease associations. Nucleic Acids Res. \n2019;47:D1013-7.\n 65. Fu Y, Yang R, Zhang L. Association prediction of CircRNAs and diseases \nusing multi-homogeneous graphs and variational graph auto-encoder. \nComput Biol Med. 2022;151:106289.\n 66. van der Laurens M, Hinton G. Visualizing data using t-SNE Laurens van der \nMaaten. J Mach Learn Res. 2008;9:2579–605.\n 67. Ning L, Cui T, Zheng B, et al. MNDR v3.0: mammal ncRNA–disease \nrepository with increased coverage and annotation. Nucleic Acids Res. \n2021;49:D160–4.\n 68. Yue B, Liu C, Sun H, et al. A positive feed-forward loop between LncRNA-\nCYTOR and Wnt/β-catenin signaling promotes metastasis of colon \ncancer. Mol Ther. 2018;26:1287–98.\n 69. Huang J-Z, Chen M, Chen D, et al. A peptide encoded by a puta-\ntive lncRNA HOXB-AS3 suppresses colon cancer growth. Mol Cell. \n2017;68:171-184.e6.\n 70. Hu R, Wu P , Liu J. LncRNA MAGI2-AS3 inhibits prostate cancer progression \nby targeting the miR-142-3p. Horm Metab Res. 2022;54:754–9.\n 71. Wu M, Huang Y, Chen T, et al. LncRNA MEG3 inhibits the progression \nof prostate cancer by modulating miR-9-5p/QKI-5axis. J Cell Mol Med. \n2018;23:29–38.\n 72. Liang D, Tian C, Zhang X. lncRNA MNX1-AS1 promotes prostate cancer \nprogression through regulating miR-2113/MDM2 axis. Mol Med Rep. \n2022;26(1):231.\n 73. Godinho MFE, Sieuwerts AM, Look MP , et al. Relevance of BCAR4 in \ntamoxifen resistance and tumour aggressiveness of human breast cancer. \nBr J Cancer. 2010;103:1284–91.\n 74. Zheng A, Song X, Zhang L, et al. Long non-coding RNA LUCAT1/miR-\n5582–3p/TCF7L2 axis regulates breast cancer stemness via Wnt/β-catenin \npathway. J Exp Clin Cancer Res. 2019;38(1):305.\n 75. Hou A, Zhang Y, Zheng Y, et al. LncRNA terminal differentiation-induced \nncRNA (TINCR) sponges miR-302 to upregulate cyclin D1 in cervical squa-\nmous cell carcinoma (CSCC). Hum Cell. 2019;32:515–21.\n 76. Shi G, Cheng Y, Zhang Y, et al. Long non-coding RNA LINC00511/miR-\n150/MMP13 axis promotes breast cancer proliferation, migration and \ninvasion. Biochim Biophys Acta Mol Basis Dis. 2021;1867:165957.\n 77. Lin X, Zhuang S, Chen X, et al. lncRNA ITGB8-AS1 functions as a ceRNA \nto promote colorectal cancer growth and migration through integrin-\nmediated focal adhesion signaling. Mol Ther. 2021;30:688–702.\n 78. Ni W, Yao S, Zhou Y, et al. Long noncoding RNA GAS5 inhibits progression \nof colorectal cancer by interacting with and triggering YAP phosphoryla-\ntion and degradation and is negatively regulated by the m6A reader \nYTHDF3. Mol Cancer. 2019;18(1):143.\n 79. He Z, Deng J, Song A, Cui X, Ma Z, Zhang Z. NEAT1 promotes colon \ncancer progression through sponging miR-495-3p and activating CDK6 \nin vitro and in vivo. J Cell Physiol. 2019;234:19582–91.\n 80. Song X, Wang H, Wu J, Sun Y. Long noncoding RNA SOX2-OT knockdown \ninhibits proliferation and metastasis of prostate cancer cells through \nmodulating the miR-452-5p/HMGB3 axis and inactivating Wnt/β-catenin \npathway. Cancer Biother Radiopharm. 2020;35:682–95.\n 81. Zhang H, Wang Z, Wu J, Ma R, Feng J. Long noncoding RNAs predict the \nsurvival of patients with colorectal cancer as revealed by constructing an \nendogenous RNA network using bioinformation analysis. Cancer Med. \n2019;8:863–73.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6426479816436768
    },
    {
      "name": "Graph embedding",
      "score": 0.6281755566596985
    },
    {
      "name": "ENCODE",
      "score": 0.5709220170974731
    },
    {
      "name": "Transformer",
      "score": 0.47245338559150696
    },
    {
      "name": "Computational biology",
      "score": 0.4529328942298889
    },
    {
      "name": "Graph",
      "score": 0.42500966787338257
    },
    {
      "name": "DNA microarray",
      "score": 0.4199671149253845
    },
    {
      "name": "Gene regulatory network",
      "score": 0.4192708134651184
    },
    {
      "name": "Embedding",
      "score": 0.36467528343200684
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3439980447292328
    },
    {
      "name": "Data mining",
      "score": 0.33596205711364746
    },
    {
      "name": "Machine learning",
      "score": 0.32626786828041077
    },
    {
      "name": "Biology",
      "score": 0.3110378384590149
    },
    {
      "name": "Theoretical computer science",
      "score": 0.22561180591583252
    },
    {
      "name": "Gene",
      "score": 0.22144702076911926
    },
    {
      "name": "Gene expression",
      "score": 0.1736546754837036
    },
    {
      "name": "Genetics",
      "score": 0.09257233142852783
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}