{
  "title": "FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining",
  "url": "https://openalex.org/W3035101152",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2133063663",
      "name": "Zhuang Liu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2675334888",
      "name": "Degen Huang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2162239119",
      "name": "Kai-Yu Huang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098887313",
      "name": "Zhuang Li",
      "affiliations": [
        "China Mobile (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2028683064",
      "name": "Jun Zhao",
      "affiliations": [
        "China Mobile (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2938830017",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3123756285",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2888160375",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2798882116",
    "https://openalex.org/W3029462071",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2970636124",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4301239768",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "There is growing interest in the tasks of financial text mining. Over the past few years, the progress of Natural Language Processing (NLP) based on deep learning advanced rapidly. Significant progress has been made with deep learning showing promising results on financial text mining models. However, as NLP models require large amounts of labeled training data, applying deep learning to financial text mining is often unsuccessful due to the lack of labeled training data in financial fields. To address this issue, we present FinBERT (BERT for Financial Text Mining) that is a domain specific language model pre-trained on large-scale financial corpora. In FinBERT, different from BERT, we construct six pre-training tasks covering more knowledge, simultaneously trained on general corpora and financial domain corpora, which can enable FinBERT model better to capture language knowledge and semantic information. The results show that our FinBERT outperforms all current state-of-the-art models. Extensive experimental results demonstrate the effectiveness and robustness of FinBERT. The source code and pre-trained models of FinBERT are available online.",
  "full_text": "FinBERT: A Pre-trained Financial Language Representation Model\nfor Financial Text Mining\nZhuang Liu1;\u0003 , Degen Huang1 , Kaiyu Huang1 , Zhuang Li2 and Jun Zhao2\n1Dalian University of Technology, Dalian, China\n2Union Mobile Financial Technology Co., Ltd., Beijing, China\nAbstract\nThere is growing interest in the tasks of ﬁnan-\ncial text mining. Over the past few years, the\nprogress of Natural Language Processing (NLP)\nbased on deep learning advanced rapidly. Signif-\nicant progress has been made with deep learning\nshowing promising results on ﬁnancial text mining\nmodels. However, as NLP models require large\namounts of labeled training data, applying deep\nlearning to ﬁnancial text mining is often unsuccess-\nful due to the lack of labeled training data in ﬁ-\nnancial ﬁelds. To address this issue, we present\nFinBERT (BERT for Financial Text Mining) that\nis a domain speciﬁc language model pre-trained on\nlarge-scale ﬁnancial corpora. In FinBERT, differ-\nent from BERT, we construct six pre-training tasks\ncovering more knowledge, simultaneously trained\non general corpora and ﬁnancial domain corpora,\nwhich can enable FinBERT model better to cap-\nture language knowledge and semantic informa-\ntion. The results show that our FinBERT outper-\nforms all current state-of-the-art models. Extensive\nexperimental results demonstrate the effectiveness\nand robustness of FinBERT. The source code and\npre-trained models of FinBERT are available on-\nline.\n1 Introduction\nIn ﬁnance and economics, various ﬁnancial text data are used\nto analyze and predict future market trends. Whether for an-\nalyst reports or ofﬁcial company announcements, ﬁnancial\ntexts mining play a crucial role in ﬁnancial technology. The\nvolume of ﬁnancial text data continues to rapidly increase.\nAn unprecedented number of such texts are created every day,\nso for any single entity, manually analyzing these texts and\ngaining actionable insights from them is almost an extremely\ndifﬁcult task. Advances in machine learning technology have\nmade ﬁnancial text mining models in FinTech possible. How-\never, in ﬁnancial text mining tasks, constructing supervised\ntraining data is prohibitively expensive as this requires the\nuse of expert knowledge in ﬁnance ﬁelds. Therefore, due to\n\u0003Corresponding author: liuzhuang.dlut@gmail.com\nthe small amount of labeled training data that can be used for\nﬁnancial text mining tasks, most ﬁnancial text mining models\ncannot directly utilize deep learning technology.\nIn this paper, we propose FinBERT model addressing\nthe issue by leveraging unsupervised Transfer Learning and\nMulti-task Learning. Word embedding, such as word2vec\n[Mikolov et al., 2013 ] is a method of extracting knowledge\nfrom an unsupervised data and has become one of the major\nadvancements in natural language processing (NLP). How-\never, because of the special language used in the ﬁnancial\nﬁeld, these simple word embedding approaches are not ef-\nfective enough. Pre-trained Language Models (PLMs), such\nas BERT [Devlin et al., 2019 ], pre-trained on a large-scale\nunsupervised data (such as Wikipedia) to improve contextu-\nalized representations more effectively. However, in the task\nof ﬁnancial text mining, due to the large differences in vo-\ncabulary and expression between the ﬁnancial corpus and the\ngeneral domain corpus, they still cannot be effectively ap-\nplied to ﬁnancial data. Furthermore, the pre-training of PLMs\nusually focuses on training the model through a few simple\ntasks. For example, BERT uses MaskLM and NSP as pre-\ntraining objectives. However, in fact, vocabulary, semantics,\nsentence order and proximity between sentences, all of which\ncan enable the PLMs to learn more language knowledge and\nsemantic information in the training corpus. Especially for ﬁ-\nnancial text data, for example, named entities like stock, bond\ntype and ﬁnancial institution names, contain unique vocabu-\nlary information. Therefore, in order to efﬁciently capture\nlanguage knowledge and semantic information in large-scale\ntraining corpora, we construct six pre-training tasks cover-\ning more knowledge, and train FinBERT through multi-task\nlearning on training data. Speciﬁcally, proposed FinBERT d-\niffers from standard PLMs pre-training methods. It constructs\nsix pre-training tasks, simultaneously trained on general cor-\npora and ﬁnancial domain corpora, to help FinBERT better\ncapture language knowledge and semantic information.\nIn summary, the main contributions of our paper are the\nfollowing:\n\u000f FinBERT is the ﬁrst domain speciﬁc BERT that is pre-\ntrained through multi-task learning on ﬁnancial corpora,\nto transfer knowledge from ﬁnancial domain corpora.\n\u000f Our FinBERT model differs from standard BERT in the\ntraining objectives. We construct six self-supervised\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4513\nFigure 1: An illustration of the architecture for FinBERT, where the model can be simultaneously trained on a general corpus and a ﬁnancial\ndomain corpus, and six pre-training tasks can be pre-trained through multi-task self-supervised pre-training learning, and last the pre-trained\nmodel is ﬁne-tuned using task-speciﬁc supervised data to adapt to various language understanding tasks.\npre-training tasks (subsection 2.2), which can be learned\nthrough multi-task self-supervised learning, capable of\nefﬁciently capturing language knowledge and semantic\ninformation in large-scale pre-training corpora.\n\u000f We conduct extensive experiments on several ﬁnancial\nbenchmark datasets. Experimental results show that pro-\nposed FinBERT outperforms all previous state-of-the-art\nmodels in ﬁnancial Sentence Boundary Detection, ﬁnan-\ncial Sentiment Analysis, and ﬁnancial Question Answer-\ning, respectively. (subsection 3.3).\n\u000f We implemented our FinBERT on Horovod frame-\nwork using mixed precision training methodology (sec-\ntion 3.1). We make the source code and pre-trained mod-\nels publicly available. With minimal task-speciﬁc archi-\ntecture modiﬁcations, FinBERT can be used for various\nother downstream ﬁnancial text mining tasks to help sig-\nniﬁcantly boost overall performance.\n2 Proposed Model: FinBERT\nAs shown in Figure 1, our proposed FinBERT model is\nbuilt based on the standard BERT architecture [Devlin et\nal., 2019] based on the two-stage ‘Pre-training’-then-‘Fine-\ntuning’ pre-training language model approach, which recent-\nly become enormously popular in NLP. During pre-training\nphase, the FinBERT model differs from standard BERT archi-\ntecture pre-training methods in that, instead of training with\nMaskLM and Next Sentence Prediction (NSP) pre-training\nobjectives, it constructs a large variety of pre-training ob-\njectives to help the FinBERT model better capture language\nknowledge and semantic information. On top of that, Fin-\nBERT keeps updating the pre-trained model through multi-\ntask self-supervised pre-training learning. Meantime, com-\npared with traditional pre-training models, FinBERT is simul-\ntaneously trained on a general corpus and a ﬁnancial domain\ncorpus. During ﬁne-tuning phase, FinBERT is ﬁrst initial-\nized with the pre-trained parameters, and is later ﬁne-tuned\non task-speciﬁc supervised data.\nIn this section, we will brieﬂy introduce FinBERT in our\nproposed framework.\n2.1 Transformer Encoder\nFirst, the input embedding module is responsible for convert-\ning each word into an embedding representation that can be\nfed into the Transformer Encoder. The Transformer[Vaswani\net al., 2017 ] encoder is based on self-attention mechanism.\nSelf-attention mechanism can capture global context infor-\nmation by pairwise correlation. The multi-layer Transformer\ncaptures the context information of each token through stack-\ning the self-attentions. Followed by BERT and ERNIE2[Sun\net al., 2019b ], the embedding representation is the sum of\nfour parts: token embedding, segment embedding, position\nembedding, and task embedding. We use different task IDs\nfor different tasks. Each task ID is assigned to a unique task\nembedding, ranging from 0 to 5. Then, FinBERT uses the\nmulti-layer Transformer architecture as the encoder.\n2.2 Multi-task Self-Supervised Pre-training\nThe choice of unsupervised pre-training objective plays an\nimportant role in applying to pre-training stage by con-\ntinuously gains general knowledge. We will modify and\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4514\ncombine multiple common unsupervised pre-training tasks\n[Joshi et al., 2019; Dong et al., 2019; Liu et al., 2019a;\nLan et al., 2020; Devlin et al., 2019; Liu et al., 2019b; Raffel\net al., 2019; Sun et al., 2019b] ﬁt our framework. Speciﬁcal-\nly, in this layer, we construct six unsupervised pre-training\ntasks to learn different level knowledge from the training\ncorpora. As shown in Figure 1, i) Basic-level Pre-training\ntasks: Span Replace Prediction pre-training task, Capitaliza-\ntion Prediction pre-training task and Token-Passage Predic-\ntion pre-training task. ii) High-level Pre-training tasks: Sen-\ntence Deshufﬂing pre-training task, Sentence Distance pre-\ntraining task and Dialogue Relation pre-training task. Next,\nwe introduce these six self-supervised pre-training tasks in\ndetail.\nSpan Replace Prediction pre-training task. Inspired by\nspanBERT [Joshi et al., 2019 ] and T5 [Raffel et al., 2019 ],\nwe constructed a self-supervised pre-training objective that\nrandomly samples and drop out 15% in the input text. Instead\nof replacing each token with a masked token, we use a unique\nmasked token to replace all of each successive boundary, i.e.,\nthe continuous span of all consecutively discarded tokens will\nbe replaced with a mask token. Last, we predict the masked\nspan by the tokens observed at the span boundary.\nCapitalization Prediction pre-training task. In ﬁnance\nand economics, capital words usually have speciﬁc semantic\nvalue in a sentence. The cased model exhibits certain advan-\ntages in tasks like ﬁnancial named entity recognition, such as\nstock, bond type and ﬁnancial institution name. Similar to\nERNIE2 [Sun et al., 2019b], we do so by introducing a capi-\ntalization words prediction objective that involves predicting\nwhether the word is capitalized or not.\nToken-Passage Prediction pre-training task. We con-\nstructed a pre-training task to identify the key words of a pas-\nsage appearing in the segment, which can enable FinBERT\nto capture the topics of a passage. Empirically, in ﬁnancial\nnews, the words appearing in the passage many times are usu-\nally used words commonly and usually relevant with the main\ntopics of the passage. Similar to ERNIE2 [Sun et al., 2019b],\nwe also do so by introducing a token-passage prediction pre-\ntraining objective to predict whether the token appears in seg-\nments of the original passage.\nSentence Deshufﬂing pre-training task. We also con-\nstructed a sentence reordering pre-training objective which is\nused e.g. in ERNIE2 [Sun et al., 2019b] and T5 [Raffel et al.,\n2019], to learn the relationships among sentences. We split a\ngiven paragraph into 1 to n segments randomly shufﬂe it by\na random permuted order, last we use the original deshufﬂed\nsequence as a training target, and pre-train the model to reor-\nganize permuted segments which are modeled with a multi-\nclass (Pn!) classiﬁcation task.\nSentence Distance pre-training task. Standard BERT\n[Devlin et al., 2019] uses Next Sentence Prediction (NSP) as\na training target, which is a binary classiﬁcation pre-training\ntask. We also constructed a self-supervised training target to\npredict sentence distance, inspired by BERT [Devlin et al.,\n2019]. But differs in that we create a three-class classiﬁca-\ntion task, rather than a binary classiﬁcation. Speciﬁcally, we\ndeﬁne a three-class classiﬁcation problem of two sentences\nto classify as “00”, “01” or “11”. “00” means that they are\nin the same passage and adjacent, “01” means that they are in\nthe same passage and not adjacent, and “11” means that they\nare not in the same passages.\nDialogue Relation pre-training task.We also constructed\na self-supervised pre-training task to learn the semantic rele-\nvance among sentences using Question answering (QA) data.\nEmpirically, QA data is important for semantic relation, since\nthe corresponding question semantics of the same answer are\nusually very similar. Therefore, similar to ERNIE [Sun et\nal., 2019a], we constructed the QA Relation pre-training task,\nwhich enables the model to learn implicit relationships and\nlearn semantic relevance.\n3 Experiments\n3.1 Pre-training FinBERT\nPre-training Data\nSimilar to that of BERT, training data in the English cor-\npus are from BookCorpus and English Wikipedia. In order\nto apply the text mining model to ﬁnancial texts, we further\ncollected various ﬁnancial data that are crawled on ﬁnancial\nwebsites, such as ﬁnancial news and dialogue. Speciﬁcally,\nwe consider ﬁve English ﬁnancial corpora of varying domains\nand sizes, totaling over 61 GB text:\n\u000f English Wikipedia1 and BooksCorpus (Zhu et al., 2015),\nwhich are the original training data used to train BERT\n(totaling 13GB, 3.31B words);\n\u000f FinancialWeb (totaling 24GB, 6.38B words), which we\ncollect from the CommonCrawl News dataset2 between\nJuly 2013 and December 2019, containing 13 million\nﬁnancial news (15GB after ﬁltering), together with web-\ncrawled ﬁnancial articles from FINWEB3 (9GB after ﬁl-\ntering);\n\u000f YahooFinance (totaling 19GB, 4.71B words), a dataset\ncrawled from Yahoo Finance4. We crawled ﬁnancial ar-\nticles (published in the last four years) from Yahoo Fi-\nnance, and performed data cleaning (removing markup,\nremoving non-textual content and ﬁltering out redundant\ndata);\n\u000f RedditFinanceQA (totaling 5GB, 1.62B words), a cor-\npus that contains automatically collected question-\nanswer pairs about ﬁnancial issues from Reddit5 website\nwith at least four up votes.\nThe statistics for all pre-training data are reported in Table 1.\nWe have built and maintained an open repository of ﬁnancial\ncorpora6 that can be accessed and analyzed by anyone.\nPre-training Settings\nOur models, FinBERT LARGE and FinBERTBASE, have the\nsame model settings of transformer and pre-training hyper-\nparameters as BERT. Like BERT, proposed multi-task self-\n1https://dumps.wikimedia.org/enwiki/\n2http://commoncrawl.org/\n3https://www.ﬁnweb.com/\n4https://ﬁnance.yahoo.com/\n5https://www.reddit.com/\n6https://drive.google.com/ﬁnbert/data/\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4515\nCorpus size(G) # of words(B) Domain\nEnglish Wikipedia 9 2.50 General\nBooksCorpus 4 0.81 General\nFinancialWeb 24 6.38 Financial\nYahooFinance 19 4.71 Financial\nRedditFinanceQA 5 1.62 Financial\nNotes: We pre-train FinBERT on ﬁve English corpora, total-\ning over 61GB text (16.02 billion words): general domain cor-\npora (Wikipedia + BooksCorpus, totaling 3.31 billion words)\nand ﬁnancial domain corpora (FinancialWeb + YahooFinance\n+ RedditFinanceQA, totaling 12.71 billion words).\nTable 1: List of text corpora used for FinBERT Corpus.\nsupervised pre-training also has great requirements on com-\nputing power. In our work, we use the architecture we de-\nveloped for FinBERT pre-training. Our architecture is based\non the ﬂexible scheduling of hundreds of GPUs implement-\ned by Apache Hadoop Y ARN, while providing a distributed\ntraining solution based on Horovod framework [Sergeev and\nBalso, 2018], using parallelized training approach. Horovod\nis an open source library 7 developed by Uber [9], which is\nmainly based on [Goyal et al., 2017 ] and baidu-allreduce8.\nOur architecture (based on Horovod) essentially utilizes a\ndistributed optimizer strategy that is an optimizer wrapping\ntf.Optimizer, and before applying the gradient to the mod-\nel weights it uses an allreduce operation to average gradi-\nent values. Empirically, with the increase in the number of\nGPUs, Our architecture’s performance loss is much smaller\nthan that based on standard distributed TensorFlow [Abadi\net al., 2016 ], and the training speed can reach much three\ntimes. Moreover, compared to the distributed Tensorﬂow\nframework, Our architecture can still guarantee a very sta-\nble acceleration ratio on the scale of hundreds of GPU cards,\nand has good scalability.\n[Micikevicius et al., 2018] proposed mixed precision train-\ning methodology for training deep neural networks (DNN),\nwhich can reduce the memory consumption and time spent\nin memory and arithmetic operations of DNN. In our work,\nwe use mixed precision training approach for multi-task self-\nsupervised pre-training. Speciﬁcally, we train our FinBERT\nusing half-precision format FP16, used for storage and arith-\nmetic. We store activations, gradients and weights using in F-\nP16, and update the copy of weights using in FP32. We main-\ntain a single-precision copy of weights, after each optimizer\nstep which accumulates the gradients. Followed by [Micike-\nvicius et al., 2018], we use loss-scaling to preserve gradient\nvalues with small magnitudes. We use FP16 arithmetic that\naccumulates into single-precision outputs, converted to FP16\nbefore storing to memory.\n3.2 Fine-tuning FinBERT\nTypically, BERT [Devlin et al., 2019 ] has two successive\nsteps, one during the pre-training phase and one during the\n7https://github.com/horovod/horovod/\n8https://github.com/baidu-research/baidu-allreduce/\nﬁne-tuning phase. It ﬁrsts conducts unsupervised pre-training\non the large corpus during the pre-training phase, and then\nconducts supervised ﬁne-tuning on down-stream NLP tasks\nduring the ﬁne-tuning phase. Similar to BERT, we pre-train\nour FinBERT model from scratch on these large unsuper-\nvised corpus (subsection 3.1), and ﬁne-tune/apply it to vari-\nous down-stream supervised ﬁnancial text mining tasks. Nex-\nt, we brieﬂy describe three ﬁnancial text mining tasks.\nFinancial S\nentence Boundary Detection (SBD)\nFinancial SBD is a basic task for ﬁnancial text mining, whose\naim is to extracting well segmented sentences from ﬁnancial\nprospectuses by disambiguating/detecting sentence bound-\naries of texts, i.e., beginning boundary and ending bound-\nary. In our work, ﬁnancial SBD dataset used is FinSBD\nShared Task9, which is a dataset that was created for IJCAI19\nFinNLP challenge. FinSBD-2019 provides training data with\nboundary labels (beginning boundary vs. ending boundary)\nfor each token.\nFinancial S\nentiment Analysis (SA)\nFinancial SA is one of the most fundamental ﬁnancial text\nmining tasks. Given a text in the ﬁnancial domain, SA aims\nto detect the target aspects which are mentioned in the ﬁnan-\ncial text and predict the sentiment score for each of the men-\ntioned targets. Sentiment scores will be deﬁned using contin-\nuous numeric values ranged from -1(negative) to 1(positive).\nIn this work, ﬁnancial SA datasets used are Financial Phrase-\nBank10 [Malo et al., 2014 ] and FiQA11 Task 1 [FiQ, 2018].\nFiQA is a dataset that was created for WWW18 conference\nﬁnancial opinion mining and question answering challenge.\nHere we use the data of Task 1, “Aspect-based ﬁnancial sen-\ntiment analysis”.\nFinancial Q\nuestion Answering (QA)\nQuestion Answering (QA) of natural language text is a fun-\ndamental challenge in natural language processing (NLP),\nwhose aims is to automatically provide answers to questions\nrelated to a given short text or passage. Financial QA involves\nanswering client questions such as “ Why are big companies\nlike Apple or Google not included in the Dow Jones Indus-\ntrial Average (DJIA) index?”. Financial QA dataset used in\n9The FinSBD-2019 dataset contains ﬁnancial text that had\nbeen pre-segmented automatically. There are 953 distinct be-\nginning tokens and 207 distinct ending tokens in the train-\ning and dev sets of FinSBD-2019 data. It’s available at http-\ns://sites.google.com/nlg.csie.ntu.edu.tw/ﬁnnlp/.\n10Financial Phrasebank dataset consists of 4845 English sen-\ntences selected randomly from ﬁnancial news found on LexisNex-\nis database. These sentences then were annotated by 16 people\nwith background in ﬁnance and business. It’s available at http-\ns://www.researchgate.net/publication/251231364\nFinancialPhrase\nBank-v10/.\n11FiQA SA dataset includes two types of discourse: ﬁnancial\nnews headlines and ﬁnancial microblogs, with manually annotat-\ned target entities, sentiment scores and aspects. The ﬁnancial news\nheadlines dataset contains a total 529 annotated headlines samples\n(436 samples for the training set and 93 samples for the test set)\nwhile the ﬁnancial microblogs contains a total 774 annotated posts\nsamples (675 samples for the training set and 99 samples for the test\nset). It’s available at https://sites.google.com/view/ﬁqa/home/.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4516\nthis paper is FiQA Task 2 [FiQ, 2018]. FiQA is a dataset that\nwas created for WWW18 conference ﬁnancial opinion min-\ning and question answering challenge12. Here we use the data\nof Task 2, “Opinion-based QA over ﬁnancial data”.\n3.3 Experimental Results\nFinancial Sentence Boundary Detection (SBD)\nTable 2 shows our results on ﬁnancial SBD. The beginning\n(BS) and ending (ES) tokens of the sentence are separately\nevaluated, the ofﬁcial evaluation metrics of FinSBD-2019 in-\nclude: i) F1 scores, separately used to predict BS and ES ; ii)\nthe mean of F1 scores. As shown in Table 2, proposed Fin-\nBERT pre-trained on both general domain dataset and ﬁnan-\ncial domain dataset is highly effective. Both FinBERT BASE\nand FinBERTLARGE outperformed the current state-of-the-\nart model. FinBERT LARGE outperformed BERT-S by 0.085\nin terms of MEAN score.\nModel ES BS MEAN\nRule-based [Fatima et al., 2019] 0.80 0.86 0.830\nBiLSTM-CRF [Du et al., 2019] 0.83 0.88 0.855\nDeep-Att [Tian and Peng, 2019] 0.83 0.91 0.875\nBERT-S[Du et al., 2019] 0.88 0.89 0.885\nFinBERTBASE (ours) 0.91 0.92 0.915\nFinBERTLARGE (ours) 0.96 0.98 0.970\nTable 2: Experimental results on test set for FinSBD English task.\nFinancial Sentiment Analysis (SA)\nThe results of PhraseBank sentiment dataset are shown in Ta-\nble 3. The results of FiQA sentiment dataset are shown in Ta-\nble 4. From the data in Table 3 and Table 4, it is apparent that\nour FinBERTBASE and FinBERTLARGE consistently signiﬁ-\ncantly outperforms all baseline models on PhraseBank senti-\nment dataset and FiQA sentiment dataset, achieving state-of-\nthe-art results. Overall, experimental results highlight the im-\nportance of the ﬁnancial domain-speciﬁc corpora pre-trained\ndesign.\nModel Accuracy F1\nLPS [Malo et al., 2014] 0.71 0.71\nHSC [Krishnamoorthy, 2018] 0.71 0.76\nULMFit [Raaci, 2019] 0.83 0.79\nFB-SA [Raaci, 2019] 0.86 0.84\nFinBERTBASE (ours) 0.91 0.89\nFinBERTLARGE (ours) 0.94 0.93\nTable 3: Experimental results on the test set for the Financial Phrase-\nBank sentiment dataset.\n12Financial QA dataset is built by crawling Stack exchange posts\nunder the Investment topic in the period between 2009 and 2017.\nThe ﬁnal dataset contains a KB of 57,640 answer posts with 17,110\nquestion-answer pairs for training and 531 question-answer pairs for\ntesting. It’s available at https://sites.google.com/view/ﬁqa/home/.\nheadline post\nModel MSE R 2 MSE R 2\nCUKG [FiQ, 2018] 0.13 0.46 0.10 0.09\nIIT-Dehi \\ 0.20 0.18 0.10 0.08\nInf-UFG \\ 0.21 0.17 0.10 0.16\nNLP301 [FiQ, 2018] - - 0.31 -1.67\nSC-V [Yang et al., 2018] 0.08 0.40 - -\nRCNN [Piao et al., 2018] 0.09 0.41 - -\nFB-SA [Raaci, 2019] 0.07 0.55 - -\nFinBERTBASE (ours) 0.29 0.67 0.28 0.26\nFinBERTLARGE (ours) 0.38 0.77 0.37 0.36\nNotes: FiQA sentiment dataset includes two types of dis-\ncourse: ﬁnancial news headlines and ﬁnancial microblogs.\nIn order to evaluate the sentiment scores models, regres-\nsion model evaluation measures were used during the exper-\niments, Mean Squared Error (MSE), R Square (R 2). \\ indi-\ncates results are taken from WWW18 Shared Task.\nTable 4: Experimental results on the test set for the FiQA sentiment\ndataset.\nFinancial Question Answering (QA)\nTable 5 shows our results on ﬁnancial QA. From the ex-\nperiments, we can clearly see that both FinBERT BASE and\nFinBERTLARGE are much better than the previous models\nincluding the state-of-the-art model. As shown in Table 5,\nFinBERTLARGE signiﬁcantly outperformed BERT and the\nstate-of-the-art model, and achieved a nDCG of 0.76 and a\nmean reciprocal rank (MRR) score of 0.68. Considering that\nit is prohibitively expensive to collect labeled training data for\nﬁnancial Question Answering (QA) task. Very often, we only\nhave very small training data (only few hundreds of samples)\nor even no training data. Even FinBERT BASE outperformed\nthe state-of-the-art model by 2 in terms of both nDCG and\nMRR score. The experimental results are highly effective and\nencouraging.\nOn all the ﬁnancial datasets, such as ﬁnancial SBD, SA and\nQA, FinBERT achieved state-of-the-art results, which proves\nthe effectiveness of proposed FinBERT.\nModel nDCG MRR\nCUKG-TongJi [Champin et al., 2018] 0.17 0.10\neLabour [Champin et al., 2018] 0.31 0.19\nFinBERTBASE (ours) 0.63 0.51\nFinBERTLARGE (ours) 0.76 0.68\nNotes: To evaluate ﬁnancial question answering scores mod-\nels, ranking evaluation measures were used during the exper-\niments: Normalized Discounted Cumulative Gain (nDCG)\nand Mean reciprocal rank (MRR). Bold face indicates best\nresult in the corresponding metric.\nTable 5: Experimental results on the test set for the FiQA question\nanswering dataset.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4517\n4 Ablation Study and Analyses\nIn this section, We conduct ablation studies and further com-\npare proposed FinBERT with existing PLMs models and\npresent the results along with experimental details.\n4.1 Effect of Pre-training on the Performance\nWe further measure the effect of pre-training on the perfor-\nmance of the classiﬁer. We compare four models: i) No\nfurther pre-training (denoted by Vanilla BERT, Vanilla Fin-\nBERT), ii) Further pre-training on classiﬁcation training set\n(denoted by BERT-task, FinBERT-task). As shown in Ta-\nble 6, FinBERT-task achieves better performance than that\nof all other models. Specially, although BERT-task is further\npre-trained on the ﬁnancial classiﬁcation training set (FiQA\nAspect category classiﬁcation task), Vanilla FinBERT out-\nperforms two Vanilla BERT based models, Vanilla BERT\nand BERT-task, 0.09% higher than Vanilla BERT and 0.02%\nhigher than BERT-task, in terms of Accuracy score. Overall,\nthis experimental result shows the strength of proposed Fin-\nBERT, and also shows that FinBERT learned domain-speciﬁc\nknowledge from a large number of ﬁnancial domain corpora\nduring the pre-training phase.\nModel Accuracy Precision Recall\nVanilla BERT 0.78 0.33 0.30\nBERT-Task 0.85 0.51 0.45\nVanilla FinBERT 0.87 0.56 0.51\nFinBERT-Task 0.91 0.72 0.69\nTable 6: Experimental results on the test set for the ﬁnancial classi-\nﬁcation dataset (FiQA Aspect category classiﬁcation task).\nSBD SA QA\nModel MEAN Acc. F1 nDCG MRR\nBERT 0.86 0.84 0.82 0.39 0.27\nFinBERT 0.94 0.89 0.90 0.73 0.64\nTable 7: The performance of BERT and FinBERT on three ﬁnancial\ntasks (SBD, SA and QA) when they are trained on a small corpus.\n4.2 Pre-training with Small Training Data\nDeep learning models require a lot of pre-training data. How-\never, due to the lack of training data in the ﬁnancial ﬁeld, in\nmany applications in ﬁnancial ﬁelds, large corpora may not be\navailable. To further demonstrate the advantages of our Fin-\nBERT, we conducted another experiment, which was based\non a simulated small corpus to pre-train BERT and FinBERT\nrespectively. Speciﬁcally, we randomly select 1/5 size of the\nentire ﬁnancial data set as a simulated small corpus. Then,\nwe pre-train both models based on this corpus, and use the\nprevious experiment to test the same tasks. In Table 7, we\nreport the results of our FinBERT and BERT based on the Fi-\nnancial SBD, Financial SA, and Financial QA, respectively.\nAs shown in Table 7, FinBERT constantly outperform BERT\non all three ﬁnancial tasks. Considering that both models are\ntrained on small corpus, the experimental results are highly\nencouraging, which shows that proposed FinBERT can pro-\nvide stable and clear enhancement when trained on ﬁnancial\ncorpora of different sizes. Overall, this experiment simulates\nthat our FinBERT model can better encode ﬁnancial text in\nthe case of limited data, proving that FinBERT still has great\nadvantages under the small training data scenarios in ﬁnancial\ndomain.\n5 Related Work\n5.1 Unsupervised Transfer Learning for PLMs\nPre-trained Language Models (PLMs) has attracted exten-\nsive attention. Fine-tuning of PLMs has shown that it can\neffectively improve downstream NLP tasks. PLMs such as\nword2vec [Mikolov et al., 2013 ] and ELMo [Peters et al.,\n2018] is an approach of extracting knowledge from a large-\nscale unlabeled data and has become one of the major ad-\nvances in NLP. [Du et al., 2019; Tian and Peng, 2019;\nLiu et al., 2020; Joshi et al., 2019; Dong et al., 2019;\nLiu et al., 2019a] presented models to tackle ﬁnancial domain\ntasks. However, because of the special language used in the\nﬁnancial ﬁeld and the large differences in vocabulary and ex-\npression between the ﬁnancial corpus and the general domain\ncorpus, these PLMs approaches are not effective enough.\n5.2 Multi-task Learning\n[Raaci, 2019; Devlin et al., 2019] usually focuses on training\nthe model through a few simple tasks. For example, BERT\nuses MaskLM and NSP as pre-training objectives. [Du et\nal., 2019 ] used word embeddings as input to the BiLSTM-\nCRF model for sentence boundary detection task. [Yang et\nal., 2018] used a word and its surrounding context as input\nto neural network. However, all of these models are based\non a single-task method and cannot use the information that\nis contained in multiple data as in Multi-Task Learning. Al-\nthough models based on multi-task learning has been widely\nused in NLP research, the application of multi-task learning\nin ﬁnancial text mining has not yet seen promising results.\n6 Conclusion\nIn this paper, we proposed FinBERT that is a pre-trained lan-\nguage model for ﬁnancial text mining. By constructing six\npre-training tasks covering more knowledge, we simultane-\nously trained FinBERT on general corpora and ﬁnancial do-\nmain corpora, which enabled FinBERT model effectively to\ncapture language knowledge and semantic information. Also,\nwe implemented our FinBERT on Horovod framework using\nmixed precision training methodology. Extensive experimen-\ntal results demonstrated the effectiveness and robustness of\nFinBERT.\nAcknowledgments\nWe would like to thank the reviewers for their helpful com-\nments and suggestions to improve the quality of the paper.\nThe authors gratefully acknowledge the ﬁnancial support pro-\nvided by the National Natural Science Foundation of China\nunder (No.61672127, U1916109).\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4518\nReferences\n[Abadi et al., 2016] Mart´ın Abadi, Ashish Agarwal, and\nPaul Barham. Tensorﬂow: Large-scale machine learn-\ning on heterogeneous distributed systems. volume ab-\ns/1603.04467, 2016.\n[Champin et al., 2018] Pierre-Antoine Champin, Fabien L.\nGandon, Mounia Lalmas, and Panagiotis G. Ipeirotis, edi-\ntors. In Proceedings of WWW. ACM, 2018.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understand-\ning. In Proceedings of NAACL, pages 4171–4186, 2019.\n[Dong et al., 2019] Li Dong, Nan Yang, and Wenhui Wang.\nUniﬁed language model pre-training for natural language\nunderstanding and generation. In Proceedings of NeurIPS,\npages 13042–13054, 2019.\n[Du et al., 2019] Jinhua Du, Yan Huang, and Karo Moila-\nnen. Sentence boundary detection through sequence la-\nbelling and BERT ﬁne-tuning. In FinNLP IJCAI 2019,\npages 81–87, Macao, China, August 2019.\n[Fatima et al., 2019] Mehwish Fatima, Mark-Christoph\nMueller, and Christoph Mark. Machine learning vs.\nrule-based sentence boundary detection. In FinNLP IJCAI\n2019, pages 115–121, Macao, China, August 2019.\n[FiQ, 2018] In Macedo Maia, Siegfried Handschuh, An-\ndre Freitas, Brian Davis, Ross Mcdermott, and Manel\nZarrouk, editors, In Proceedings of WWW, 2018.\n[Goyal et al., 2017] Priya Goyal, Piotr Doll ´ar, Ross B. Gir-\nshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyro-\nla, Andrew Tulloch, Yangqing Jia, and Kaiming He. Ac-\ncurate, large minibatch SGD: training imagenet in 1 hour.\nCoRR, abs/1706.02677, 2017.\n[Joshi et al., 2019] Mandar Joshi, Danqi Chen, Yinhan Liu,\nDaniel S. Weld, Luke Zettlemoyer, and Omer Levy. Span-\nbert: Improving pre-training by representing and predict-\ning spans. CoRR, abs/1907.10529, 2019.\n[Krishnamoorthy, 2018] Srikumar Krishnamoorthy. Senti-\nment analysis of ﬁnancial news articles using performance\nindicators. Knowl. Inf. Syst., 56(2):373–394, 2018.\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian\nGoodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A lite BERT for self-supervised learning of\nlanguage representations. 2020.\n[Liu et al., 2019a] Xiaodong Liu, Pengcheng He, Weizhu\nChen, and Jianfeng Gao. Multi-task deep neural network-\ns for natural language understanding. In Proceedings of\nACL, pages 4487–4496, 2019.\n[Liu et al., 2019b] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, and Danqi Chen. Roberta: A\nrobustly optimized BERT pretraining approach.CoRR, ab-\ns/1907.11692, 2019.\n[Liu et al., 2020] Zhuang Liu, Keli Xiao, Bo Jin, Kaiyu\nHuang, and Yunxia Zhang. Uniﬁed generative adversar-\nial networks for multiple-choice oriented machine com-\nprehension. ACM Transactions on Intelligent Systems and\nTechnology, 11(3):1–20, 2020.\n[Malo et al., 2014] Pekka Malo, Ankur Sinha, Pekka J. Ko-\nrhonen, Jyrki Wallenius, and Pyry Takala. Good debt or\nbad debt: Detecting semantic orientations in economic\ntexts. JASIST, 65(4):782–796, 2014.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan\nNarang, Jonah Alben, Gregory F. Diamos, Erich Elsen,\nDavid Garc´ıa, Boris Ginsburg, Michael Houston, Oleksii\nKuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In Proceedings of ICLR, 2018.\n[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. Efﬁcient estimation of word rep-\nresentations in vector space. Computer Science, 2013.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann,\nMohit Iyyer, Matt Gardner, and Christopher Clark. Deep\ncontextualized word representations. In Proceedings of\nNAACL-HLT, pages 2227–2237, 2018.\n[Piao et al., 2018] Guangyuan Piao, John G. Breslin, and\nRoss Mc-Dermott. Financial aspect and sentiment predic-\ntions with deep neural networks: An ensemble approach.\nIn Proceedings of WWW, pages 1973–1977, 2018.\n[Raaci, 2019] Dogu Raaci. Finbert: Financial sentimen-\nt analysis with pre-trained language models. CoRR, ab-\ns/1908.10063, 2019.\n[Raffel et al., 2019] Colin Raffel, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits\nof transfer learning with a uniﬁed text-to-text transformer.\nCoRR, abs/1910.10683, 2019.\n[Sergeev and Balso, 2018] Alexander Sergeev and Mike Del\nBalso. Horovod: fast and easy distributed deep learning in\ntensorﬂow. CoRR, abs/1802.05799, 2018.\n[Sun et al., 2019a] Yu Sun, Shuohuan Wang, Yu-Kun Li,\nShikun Feng, Xuyi Chen, Han Zhang, Xin Tian, and\nDanxiang Zhu. ERNIE: enhanced representation through\nknowledge integration. CoRR, abs/1904.09223, 2019.\n[Sun et al., 2019b] Yu Sun, Shuohuan Wang, Yu-Kun Li,\nShikun Feng, Hao Tian, Hua Wu, and Haifeng Wang.\nERNIE 2.0: A continual pre-training framework for lan-\nguage understanding. CoRR, abs/1907.12412, 2019.\n[Tian and Peng, 2019] Ke Tian and Zi Jun Peng. Sentence\nboundary detection in noisy texts from ﬁnancial docu-\nments using deep attention model. In FinNLP IJCAI 2019,\npages 88–92, Macao, China, August 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Proceedings of NeurIPS, pages 5998–6008, 2017.\n[Yang et al., 2018] Steve Yang, Jason Rosenfeld, and\nJacques Makutonin. Financial aspect-based senti-\nment analysis using deep representations. CoRR,\nabs/1808.07931, 2018.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\nSpecial Track on AI in FinTech\n4519",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7678673267364502
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7188037633895874
    },
    {
      "name": "Natural language processing",
      "score": 0.6028296947479248
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5683853626251221
    },
    {
      "name": "Machine learning",
      "score": 0.5389803647994995
    },
    {
      "name": "Language model",
      "score": 0.4948102831840515
    },
    {
      "name": "Deep learning",
      "score": 0.45839983224868774
    },
    {
      "name": "Finance",
      "score": 0.4199085831642151
    },
    {
      "name": "Construct (python library)",
      "score": 0.4182214140892029
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I180662265",
      "name": "China Mobile (China)",
      "country": "CN"
    }
  ],
  "cited_by": 235
}