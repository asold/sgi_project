{
    "title": "Natural Language Identification using Corpus-Based Models",
    "url": "https://openalex.org/W157108407",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5100098606",
            "name": "Clive Souter et al.",
            "affiliations": [
                "University of Leeds"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2331302844",
        "https://openalex.org/W1930895546",
        "https://openalex.org/W2039585592"
    ],
    "abstract": "This paper describes three approaches to the task of automatically identifying the language a text is written in. We conducted experiments to compare the success of each approach in identifying languages from a set of texts in Dutch/Friesian, English, French, Gaelic (Irish), German, Italian, Portuguese, Serbo-Croat and Spanish.....",
    "full_text": "Clive Souter, Gavin Churcher, Judith Hayes, \nJohn Hughes & Stephen Johnson*\nNatural Language Identification using Corpus-\nBased Models\nAbstract\nThis paper describes three approaches to the task of automatically identifying the lan-\nguage a text is written in. We conducted experiments to compare the success of each\napproach in identifying languages from a set of texts in Dutch/Friesian, English,\nFrench, Gaelic (Irish), German, Italian, Portuguese, Serbo-Croat and Spanish.\nThe three techniques we chose to investigate are:\ni) Unique character string identification.\nThis involved finding (empirically or using linguistic ‘competence’) short strings of\ncharacters which are unique to each language.\nii)Frequent word recognition.\nAnother method we explored was to extract frequency ordered wordlists, and\nchoose, say, the top 100 words for each language. Unseen text would then be ana-\nlysed word by word, looking up each candidate in the list for each language, and\nadding to a running total or likelihood for each. At any time, or at the end of the text,\nwe can return the most likely language.\niii)Bigraph/trigraph based recognition.\nAll possible two- and three-letter combinations are extracted from the training texts,\nalong with their frequencies in each language. Unseen text is then analysed by simi-\nlarly splitting up the text into ordered bi/trigraphs, and a running total probability for\neach language maintained. As in method ii), we can return the most likely language\nat any stage.\nEach method was implemented (using the POP11 language), by training the model on\nroughly 100 kilobytes of text and tested on text samples which had been set aside at the\noutset. We varied the length of the text samples, to see how performance was affected.\nThe results showed the unique character string identification to be very poor, since\nthe test samples weren't long enough to contain the unique strings (many of which are\nquite rare). The bigraph recognition was 88% successful, being surpassed by the ‘most-\n183\nHermes, Journal of Linguistics no. 13 – 1994\n* Clive Souter, Gavin Churcher, Judith Hayes, John Hughes & Stephen Johnson\nSchool of Computer Studies\nUniversity of Leeds\nLeeds LS2 9JT (UK)\ncommon-word ’ approach, which correctly identified the language in 91% of the test\nsamples. However, the most successful approach was trigraph recognition, with 94%.\nTest length results showed an erratic improvement in success rate as the test sample got\nlonger, with some instances of decreased success with increasing length of sample. For\nthe bigraph model, optimal success (100%) was reached on text samples of 200 charac-\nters or more, whereas for the trigraph model, 100% success could be gained on samples\nof more than 175 characters.\nWe then went on to monitor the effect of convergence on the bi/trigraph models, to\nsee if a fully converged model performed better than one in which only a part of the\npossible graphs had been learned. Perhaps surprisingly, the two models differed in this\nrespect: The bigraph model performed best having learned only 75% of the possible\nbigraphs in the languages. Whereas, for a trigraph model, only 25-50% of the graphs\nneed to be learned to achieve optimal recognition. It appears that for both models, learn-\ning the very rare graphs only serves to decrease performance (by making the models for\neach language more similar). Some applications of these very simple language models\nare in language classification, enhanced performance for speech and handwriting\nrecognisers, and perhaps in spelling checking. We explored the correlation between the\nfrequencies of both bi- and tri-graphs for each language, to see whether the spelling\nconventions of different languages reflect the accepted historical links between lan-\nguages of different families. Results can be displayed graphically using dendrograms,\nand generally back up the family trees of proto-Indo-European that historical linguists\nhave proposed. English and French appear to be less tied to their supposed Germanic\nand Latin roots than is traditionally accepted.\n1. Introduction\nBefore any processing of a language such as lexical look-up or parsing\ncan begin, it is first necessary to know which language we are dealing\nwith. In many cases, this is obvious because there is a human expert\npresent to identify the text. However, we may wish to identify the ori-\ngin of a text and not have access to such an expert. This paper describes\nan experiment in the development and use of bigraph and trigraph\nmodels for automatically recognising written natural languages. The\nmodels are extracted from corpora of different languages, and then\nemployed to identify new texts probabilistically. The models could be\nused in other applications, such as optical character or handwriting\nrecognition, and spelling checking. In this study, nine languages are\nused; Friesian, English, French, Gaelic, German, Italian, Portuguese,\nSerbo-Croat, and Spanish. Machine-readable samples of each of these\nlanguages were obtained from the Oxford Text Archive at Oxford Uni-\nversity. Approximately one tenth of the data was reserved for testing\n184\npurposes, and the remainder used for training the language models. The\noriginal training sample sizes in characters are given in Table 1 below:\nTable 1. Preliminary text sizes (in bytes)\nIt was not known at the start of the experiment whether these samples\nare of sufficient size to extract an adequate probabilistic model. Indeed,\npart of the experiment is to determine (for each language) when the\nlearning of new bigraphs and trigraphs converges, so an adequate\nsample size can be determined. We selected files for training the lan-\nguage models which were each approximately 100 kb (with the obvious\nexception of Friesian). It so happened that these samples were large\nenough for a bigraph model to converge for all but the Friesian data.\nFurther text needed to be collected to train the trigraph model.\nIt is also intended when testing the language recogniser to ascertain\nafter how many characters the identification process converges, and no\nfurther text need be read.\nThe training samples had to be automatically and manually edited to\n‘comment out’ non-alphabetic characters and introductory description\nwhich was not part of the text. We decided to strip out any notation for\naccents from the texts, as such notation was inconsistent, even within\none language. This was rather unfortunate, as accents are features\nwhich do distinguish languages.\n2. Methods for identifying languages\nLanguage identification can be achieved using several approaches:\n2.1. Unique character strings\nThe simplest identification technique might be to find a string of char-\nacters in the Latin alphabet which are unique to a particular language.\nFor instance, we might suppose that word initial LL is unique to Welsh,\nor that CZY is unique to Polish. The task is to find strings of characters\n185\nwhich are unique to each of the languages we wish to identify. This\ncould be done by soliciting the opinion of expert linguists, who may be\nable to suggest candidates to be tested against the real data, or by\nsearching through text samples iteratively to find a string of characters\nwhich only occurred in one language. Set out in Table 2 are some pairs\nof characters (bigraphs) which were found empirically to be unique to\none language in the original text samples used.\nTable 2. Bigraphs unique to the languages shown\nClearly, there are problems with just using two-letter combinations,\nsince native English speakers know that nx occurs in the words anxious\nand anxiety. These words must simply have not been found in the Eng-\nlish training material. There were no two-letter combinations unique to\nPortuguese found in the training data, so it would be impossible to iden-\ntify Portuguese using this method. Furthermore, there is no guarantee\nthat a string which is unique to one of the nine languages used here\nwould remain so were other languages to be included. Longer se-\nquences of characters are needed to uniquely identify a language, but\nthe problem with such longer sequences is that they tend to occur more\nrarely. Consequently, several hundreds or even thousands of sentences\nmay have to be read before the language can be identified uniquely\nusing this technique.\n2.2. Frequent word recognition\nAn alternative method is to extract a frequency ordered list of the words\nin each language from the training material. Then, the most frequent\n186\nwords in each language can be used as a test list against which the\nwords in a new, unknown sentence can be matched. Some of the words\nin the unknown text will hopefully be found in the test list. For instance,\nthe word the would be found to belong to at least English and French,\nbut would be more frequent in English. Having read in the whole of the\nunknown text, a profile of possible languages can be constructed, and\nprobability values for each language calculated. This technique should\nprovide a solution even with small quantities of the language to be iden-\ntified, such as just one sentence. However, its accuracy is strongly\ndependent on the size of the test list, and may still require several sen-\ntences of unknown material before reaching a reliable solution.\n2.3. Bigraph/Trigraph based recognition\nA compromise between these first two methods is to extract all the pos-\nsible two- and three-letter combinations from the training material,\nalong with frequencies of these combinations for each language. We\nwill use the terms bigraph and trigraph to refer to such combinations, to\navoid confusion with the terms bigram and trigram, which are used for\nword (and often grammatical tag) combinations. Part of the table of\ntwo-letter combinations is shown below (Table 3), with the frequencies\nfor each language represented as a percentage of the total number of\nbigraphs read in the training sample of that language. Note that the\nblank space character has been used as a ‘letter’ in this experiment:\nTable 3. Sample bigraphs and their percentage frequencies\n187\nA similar table for trigraphs has been extracted from the training ma-\nterial. These tables can then be used to identify the language of an\nunknown sentence of input. We anticipated that these will prove to be a\nmore accurate method than those mentioned in sections 1 and 2. In the\nunknown sentence, every two (or three) letter combination will provide\na probability profile for each of the nine languages, and after only a few\nletters, a potential solution can be obtained. The accuracy of the solu-\ntion will increase as more unknown words are read in. Using a trigraph\nrather than a bigraph model should also improve the accuracy of the\nlanguage identifier. The success of each of the identification methods\ndescribed here depends directly on the ‘representativeness’ of the train-\ning language samples, or at least on how similar the genre of the test\nmaterial is to that of the training material. All the original training ma-\nterial came from written media or literary texts.\n3. Designing the recognisers\nThe process of recognition using unique strings is straightforward, so  it\nwill not be described. \nFor the ‘most-common-words’ approach, the recogniser read input a\nword at a time, looked up the word in the table of frequent words, and\nif the word was found to belong to any of the languages, simply in-\ncreased a counter for that language by one. At any point during the\nrecognition process, it would be possible to return the current most like-\nly language, but the recogniser proceeded to the end of the test sample\nbefore delivering its result. \nFor the bigraph and trigraph-based recognisers, quite a naive statis-\ntical approach was adopted. After each graph was read in, the table of\npercentages for each language (which had been extracted from the trai-\nning data) was consulted, and the percentages simply added to a run-\nning total for each language. The running total is itself converted to a\npercentage of the grand total for all languages when returning a result.\nAgain, at any point during the recognition process, it would be possible\nto return the current most likely language, but the recogniser proceeded\nto the end of the test sample before delivering its result. We are aware\nthat this statistical approach is very basic, but are interested to see how\nwell a simple model can perform.\n188\n4. Recognition Results\nEach of the four methods of identification described above have been\nimplemented using POP11. The test data consisted of 4 files each con-\ntaining 45 samples (five from each language). Two files contained short\ntext samples and two longer samples (on average 10 and 70 UNIX\n‘words’ respectively). Similarly, two files contained unseen data and\ntwo contained data from which the models had been trained (seen data).\nThe results of the tests for each method of recognition over all lan-\nguages are summarised in Table 4 (percentage success rates).\nTable 4. Recognition results for preliminary data\nAs we suspected, the unique strings method proved unsuccessful, only\nachieving 24% success overall, because in many cases the test material\ndid not contain any of the unique letter sequences. The bigraph method\nand the most- common-word method were both quite successful on the\ntest data, and each strangely performed better on unseen data than that\non which it had been trained. We can only surmise that the unseen test\ndata by chance happened to be more representative of the training data\nas a whole than the test samples chosen from the training data. The tri-\ngraph model performed best, achieving the highest results in each cate-\ngory. In fact, for the long unseen test file, the trigraph model was 100%\nsuccessful in identifying the 45 language samples. Some languages\nwere more easily recognised than others, as can be seen in the Tables 5\nand 6, showing bigraph and trigraph recognition only, for which the\nrecogniser was set to give the rank order of the target languages, most\nlikely first. For instance, the bigraph result for Friesian shows that 19\ntimes out of 20 Friesian was correctly recognised, but on one occasion,\nit came second behind some other language.\n189\nTable 5. Bigraph results per language\nTable 6. Trigraph results per language\nThe recognition of Portuguese using the bigraph model was very poor.\nWe have already noted that there are no bigraphs which are unique to\nPortuguese in the training data. In most cases the recogniser offered\nSpanish instead as the most likely language. However, the recognition\nof Portuguese improves markedly using the trigraph model, at the\nexpense of the Spanish. In general, the effect of using a trigraph recog-\nniser is to improve accuracy overall, with the exception of Spanish, and\na very slight downturn in the recognition of French and Serbo-Croat.\n190\n5. Studying the effect of convergence in the training\nmodels\nIt was clear that not all the bigraphs had been learned using training\nfiles of about 100 kilobytes, since we could identify bigraphs which we\nknew existed in a particular language but had not been captured. So we\nset about monitoring the convergence of both the bigraph and the tri-\ngraph model. We arbitrarily defined convergence as follows: we moni-\ntor in training how many graphs are learned for every 1,000 graphs\nread. For a chosen number of 1,000 graph samples of text, we calculate\na moving average of the total new graphs learnt. The chosen number is\nusually set at 3 or 4. If the moving average is zero for 3 or 4 samples of\n1,000 graphs, we declare the model to have converged.\nFor example, if we observe the following totals for a series of 1,000\ngraph samples:\n100 80 85 60 20 10 0 0\nThen if we set the moving average window to be only 2, the resulting\nmoving average totals would be:\n90 82.5 72.5 40 15 5 0\nUsing moving averages rather than raw totals has the effect of\nsmoothing out peaks in the data. We can vary the moving average level\nto more than 3 if we wish to allow a very long tail in the convergence,\nor set it to less than 3 if we wish to have a smaller model (for instance\nin the case where the text genre contains many acronyms, which would\ncause the model to converge slowly). Note that this definition does not\npreclude an incomplete graph model, since a sudden increase in new\ngraphs could occur, for example when beginning training on text from\na new genre. \nWe began monitoring convergence with the English text, to give us\nan estimate of how much text would be needed for the bigraph and the\ntrigraph model. Using the definition of convergence given above, the\nEnglish trigraph model converged after reading 346,000 trigraph\ntokens. As a consequence, it was clearly necessary for us to supplement\nour training data for seven of the nine languages. We had no shortage of\nEnglish or French material, but could not be sure whether a particular\nmodel would be the same size for each language, or whether the model\nwould converge at different rates for different languages. We obtained\nfurther on-line texts for each language (except Friesian) primarily by\n191\ncollecting ‘ftp’-able material from archives and from the USENET bul-\nletin boards for each language. Material collected from bulletin boards\nhad to be edited to remove headers and verify that it contained only the\ntarget language. It was not possible to expand our collection of Friesian,\nso we began a new collection of modern Dutch. (At one point we in-\ncluded Dutch and Friesian in the Dutch training data, but discovered\nthat in testing, all Friesian examples were being classified as English,\nso we decided to separate them out from the modern Dutch, and ignore\nthem for the rest of the experiment).\nTable 7 shows the total text available for the remaining experiments,\nonce we had collected supplementary material:\nTable 7. Total amount of training text available (in characters/\nwords)\nTables 8 and 9 give details of training convergence of the bigraph and\ntrigraph models for the nine languages.\n192\nTable 8. Convergence of bigraph models for each language\nTable 9. Convergence of trigraph models for each language\n193\nThe bigraph models converge to varying totals of bigraph types for each\nlanguage, ranging from only 269 (Italian) to 513 (Dutch). A language\nmodel which used all the possible bigraph combinations would contain\n729 bigraph types: (26 + 1) squared, including the blank space as a let-\nter. The largest possible trigraph model is 19,683 graphs. Figures are\nalso given to show the quartile ranges, and how many bigraph tokens\nhad to be read before each model converged. The number of bigraphs\nread cannot be converted directly into characters in the source file,\nbecause of the fact that accent markers and other non-alphabetic text\ncharacters are ignored by the training (and the testing) programs.\nNote that the number of bigraphs needing to be read for convergence\nvaries from 20000 (Spanish) to 94000 (English). It is not the case that a\nlarger graph model will necessarily require a greater number of graph\ntokens to be read before reaching convergence. For example, the Dutch\nbigraph model converged at a total of 513 bigraph types, after reading\n60000 tokens. Whereas smaller models such as English (509 types from\n94000 tokens) and French (386 types from 61000 tokens) required\nmore material to be read before convergence. Figures 1 and 2 below\nshow the learning of bi/trigraph types (on the y axis) for English\naccording to the number of tokens read (on the x axis multiplied by a\nfactor of 1,000).\nFigure 1. Bigraph learning for English\n194\nFigure 2. Trigraph learning for English\n6. How does convergence affect language recognition?\nApart from studying convergence in the training models, we also want-\ned to observe the effect of a converging model on recognition success.\nWould a fully converged model result in improved recognition of\nunknown texts? A further variable to be taken into account is the length\nof the test material. The test material for the preliminary experiments\nwas divided into short (approximately 60 character) texts and longer\n(approximately 420 character) texts. Preliminary results showed that\nthe longer texts were more successfully identified.\nWe constructed some new test material from the additional training\nmaterial, (and in the case of Dutch, exclusively from the modern Dutch)\nand divided the material into samples which were 50, 75, 100, 125, 150,\n175 or 200 characters long. Each sample contained 36 texts, four from\neach language. Both the bigraph and trigraph recognition programs\nwere run on this new test material, with results shown in Tables 10 and\n11 below:\n195\nWe can conclude from these figures that a fully or 75% converged\nbigraph model is slightly more accurate than one which has not yet\nreached this stage of convergence. However, the trigraph model appears\nto be more successful for language recognition when it has reached\nbetween 25-50% of its fully converged size. Learning the rarer tri-\ngraphs only adds noise to the model, and reduces its ability to distin-\nguish between languages. The trigraph model is more successful than\nthe bigraph model at each stage of convergence.\nThe effect of longer test samples improving recognition success is\nfairly clear in the bigraph model, being shown most strongly in the 75%\nand fully converged models.\nHowever, no clear pattern emerges from the trigraph recognition.\nThe overall success rate for longer test samples is lowered by the poor\nperformance of the fully and 75% converged models. The picture is\nclearer when considering only the 25% converged model, which does\ntend to improve performance on longer test samples. The bigraph and\ntrigraph models appear therefore to perform differently with respect to\nlength of test input and convergence. Of the approaches discussed here,\nthe ideal model to use for language recognition would seem to be a part-\nly (25-50%) converged trigraph model, which begins to achieve 100%\naccuracy on texts of at least 175 characters (approximately 30 UNIX\nwords) in length.\nTable 10. Bigraph results for different length test samples\n196\nTable 11. Trigraph results for different length test samples\n7. Investigating the effects of convergence in identification\nUsing the definition of convergence for identifying texts (as given in\nsection 5) a number of tests were carried out to observe the behaviour\nof the identification process using texts of arbitrary length. For each\nlanguage, ten samples were chosen pseudo-randomly from texts\navailable. Choosing the best performing bigraph and trigraph models,\nthe number of characters required before the system converged on the\ncorrect solution was noted. The bigraph model used was 75% of the\nconverged model and the trigraph model used was 25% of the con-\nverged model.\nThe purpose of the tests is to establish limits to the identification pro-\ncess. Hence, lower bounds on the number of characters read before con-\nvergence should be declared can be determined.\n7.1. Test results\nTables 12 and 13 show the maximum, minimum and average number of\ncharacters read for each language, by when convergence in recognition\nhad been reached.\n197\nTable 12. Number of characters read before correct convergence\nfor bigraph model\nTable 13. Number of characters read before correct convergence\nfor trigraph model\n198\nThe tables show mixed results with a trend of fewer characters required\nfor the trigraph model. There were two notable exceptions to this how-\never; Italian and Serbo-croat required the same or fewer characters for\nthe bigraph recogniser to succeed.\nCalculating the average for the bigraph and trigraph results yields\napproximately 20 and 15 characters respectively. The lower bounds on\nthe number of characters read before convergence is tested for can be\nset to these values. This will tend to ensure that the recogniser reaches\nthe likeliest answer. For example, if convergence is tested for after\nevery 5 characters then the first point after which a trigraph model\ncould converge would be after 30 characters read. Since the purpose of\nthis method is to reduce the amount of time the process takes to come\nto a correct solution for a large text, then thirty characters (7-8 UNIX\nwords) would considerably reduce the identification time.\n8. Using the bi/trigraph models in other applications\nApart from their obvious uses in language identification, the bi/trigraph\nmodels may be utilised in other application areas, including language\nclassification, optical character recognition, handwriting recognition\nand spelling checkers. We will consider here only language classifica-\ntion. We can observe how closely related the writing systems of the dif-\nferent languages are by obtaining correlation coefficients between each\nlanguage. We entered the two models (part of one of these was shown\nabove in Table 3) into the Minitab statistical package and generated cor-\nrelation values for the bigraph and trigraph models extracted from the\npreliminary training data. Tables 14 and 15 show correlation coeffi-\ncients for the preliminary training data, but similar tables have been\nextracted for the fully converged models derived from the extended\ntraining data.\n199\nTable 14. Bigraph frequency correlations ( at 75% of convergence).\nTable 15. Trigraph frequency correlations (at 25% of converged\nmodel)\n200\nThe correlation data can be represented more graphically using a clus-\ntering algorithm (here we used Ward's method) and displaying the\nclustering in the form of a dendrogram. Figures 3 and 4 show the\nchanges in the clustering as the bigraph and trigraph models pass\nthough 25%, 50% and 75% to full convergence.\nFigure 3. Dendrograms for bigraph models\n201\nFigure 4. Dendrograms for trigraph models\nThe nearer the node joining two branches is to the right side of the dia-\ngram, the stronger the relationship between the letter combinations in\nthose two languages. For the bigraph model, the clustering becomes\nstable at 75% of the size of the converged model, with Spanish and Por-\ntuguese being most closely paired. Interestingly, French and English\nform a group of their own related to Gaelic, and French is some distance\nfrom the other Romance languages. In the trigraph model, we still see\n202\nthe strong relationship between Spanish and Portuguese, but this time\nthey are linked to Italian and French, as one might hope from a histori-\ncal linguistic viewpoint. German and Dutch are again strongly paired,\nwith Serbo-Croat standing largely on its own at each stage.\nThe clusterings do tend to adhere to Indo-European family tree\nwhich have been proposed by historical linguists (e.g. Yule 1985; 168),\nbut the looseness of the relationship between English and the other Ger-\nmanic languages, and between French and the other Romance lan-\nguages comes as a surprise. They also agree largely with the findings of\nBatagelj et al (1992) who used Ward's algorithm to cluster many more\nlanguages on the basis of only 16 chosen words!\n9. Conclusions\nWe have explored four approaches to language identification, and\nfound that a trigraph model is the most successful for recognising the\nlanguages included in the study. A fully converged bigraph model is\nmore successful than one which has yet to converge, but it still just out-\nstripped by a simple ‘100 most-common-words’ approach. However, a\nfar from converged trigraph model was the most successful of all.\nUsing a trigraph model which had reached only 25-50% of its con-\nverged size on texts of at least 175 characters resulted in faultless recog-\nnition, even with a relatively simple statistical model for combining the\ngraph frequencies. The effect of increasing the length of the test mate-\nrial did tend to improve recognition success, but selecting a model at\nthe right level of convergence was the most important factor in a-\nchieving a high recognition rate.\nBigraph and trigraph models can be used to classify languages along\nthe lines of a historical linguistic family tree for Indo-European lan-\nguages, and generally support the links expressed in such trees, with\nsome exceptions in the classification of French and English.\nReferences\nBatagelj, V ., T. Pisanski and D. Kerzic (1992): Automatic clustering of languages. In:\nComputational LinguisticsV ol. 18, No. 3.\nYule, G. (1985): The study of language. Cambridge University Press.\n203\n204"
}