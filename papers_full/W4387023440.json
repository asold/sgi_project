{
  "title": "Probabilistic generative transformer language models for generative design of molecules",
  "url": "https://openalex.org/W4387023440",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098780580",
      "name": "Lai Wei",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A3059554662",
      "name": "Nihang Fu",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2316852318",
      "name": "Yuqi Song",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2011747184",
      "name": "Qian Wang",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2135881724",
      "name": "Jianjun Hu",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2098780580",
      "name": "Lai Wei",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A3059554662",
      "name": "Nihang Fu",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2316852318",
      "name": "Yuqi Song",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2011747184",
      "name": "Qian Wang",
      "affiliations": [
        "University of South Carolina"
      ]
    },
    {
      "id": "https://openalex.org/A2135881724",
      "name": "Jianjun Hu",
      "affiliations": [
        "University of South Carolina"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3165171933",
    "https://openalex.org/W3121311390",
    "https://openalex.org/W6603707631",
    "https://openalex.org/W3011847211",
    "https://openalex.org/W4213349615",
    "https://openalex.org/W6610423178",
    "https://openalex.org/W6609581451",
    "https://openalex.org/W3036527662",
    "https://openalex.org/W4319310661",
    "https://openalex.org/W4361198756",
    "https://openalex.org/W4281619372",
    "https://openalex.org/W3195604886",
    "https://openalex.org/W3094686696",
    "https://openalex.org/W2956961449",
    "https://openalex.org/W2763220183",
    "https://openalex.org/W2900694120",
    "https://openalex.org/W6601013545",
    "https://openalex.org/W2909240409",
    "https://openalex.org/W6764431594",
    "https://openalex.org/W4229590462",
    "https://openalex.org/W3103753836",
    "https://openalex.org/W6601022194",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4313485929",
    "https://openalex.org/W3138781613",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W2794994220",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W4380225176",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W2037825667",
    "https://openalex.org/W4283071220",
    "https://openalex.org/W2076809861",
    "https://openalex.org/W4308510017",
    "https://openalex.org/W3104088487",
    "https://openalex.org/W4401339550",
    "https://openalex.org/W2953128081",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W3104956673",
    "https://openalex.org/W3099414221",
    "https://openalex.org/W2022476850"
  ],
  "abstract": "Abstract Self-supervised neural language models have recently found wide applications in the generative design of organic molecules and protein sequences as well as representation learning for downstream structure classification and functional prediction. However, most of the existing deep learning models for molecule design usually require a big dataset and have a black-box architecture, which makes it difficult to interpret their design logic. Here we propose the Generative Molecular Transformer (GMTransformer), a probabilistic neural network model for generative design of molecules. Our model is built on the blank filling language model originally developed for text processing, which has demonstrated unique advantages in learning the “molecules grammars” with high-quality generation, interpretability, and data efficiency. Benchmarked on the MOSES datasets, our models achieve high novelty and Scaf compared to other baselines. The probabilistic generation steps have the potential in tinkering with molecule design due to their capability of recommending how to modify existing molecules with explanation, guided by the learned implicit molecule chemistry. The source code and datasets can be accessed freely at https://github.com/usccolumbia/GMTransformer",
  "full_text": "Wei et al. Journal of Cheminformatics           (2023) 15:88  \nhttps://doi.org/10.1186/s13321-023-00759-z\nRESEARCH Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of Cheminformatics\nProbabilistic generative transformer \nlanguage models for generative design \nof molecules\nLai Wei1, Nihang Fu1, Yuqi Song1, Qian Wang2 and Jianjun Hu1* \nAbstract \nSelf-supervised neural language models have recently found wide applications in the generative design of organic \nmolecules and protein sequences as well as representation learning for downstream structure classification \nand functional prediction. However, most of the existing deep learning models for molecule design usually require \na big dataset and have a black-box architecture, which makes it difficult to interpret their design logic. Here we \npropose the Generative Molecular Transformer (GMTransformer), a probabilistic neural network model for generative \ndesign of molecules. Our model is built on the blank filling language model originally developed for text processing, \nwhich has demonstrated unique advantages in learning the “molecules grammars” with high-quality generation, \ninterpretability, and data efficiency. Benchmarked on the MOSES datasets, our models achieve high novelty and Scaf \ncompared to other baselines. The probabilistic generation steps have the potential in tinkering with molecule design \ndue to their capability of recommending how to modify existing molecules with explanation, guided by the learned \nimplicit molecule chemistry. The source code and datasets can be accessed freely at https:// github. com/ uscco \nlumbia/ GMTra nsfor mer\nKeywords Deep learning, Language models, Molecules generator, Molecules discovery, Blank filling\nIntroduction\nThe discovery of novel organic molecules has wide \napplications in many fields, such as drug design \nand catalysis development [1]. However, due to the \nsophisticated structure–property relationships, \ntraditional rational design approaches have only covered \nan extremely limited chemical design space [2]. Recently, \na large number of generative machine learning algorithms \nand models have been proposed for molecule design, \nas systematically reviewed in [1, 3, 4]. The first category \nof these methods is deep generative models (DGMs), \nDeep generative models (DGMs) typically leverage deep \nnetworks to learn from an input dataset and synthesize \nnew designs [5]. Recently, DGMs such as feed forward \nneural networks (NNs), generative adversarial networks \n(GANs) [6], variational autoencoders (VAEs) [7], certain \ndeep reinforcement learning (DRL) frameworks and \nnormalizing flow-based models [8] have shown promising \nresults in design applications like structural optimization, \nmaterials design, and shape synthesis. Two of the major \nlimitations of these models include their black-box \nnature and the challenge of dealing with modularity in \nmolecule design. In [9], Westermayr et  al. proposed an \napproach that combines an autoregressive generative \nmodel that predicts three-dimensional conformations \nof molecules with a supervised deep network model \nthat predicts their properties. The  generation of \n*Correspondence:\nJianjun Hu\njianjunh@cse.sc.edu\n1 Department of Computer Science and Engineering, University of South \nCarolina, Columbia, SC 29201, USA\n2 Department of Chemistry and Biochemistry, University of South \nCarolina, Columbia, SC 29201, USA\nPage 2 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \nmolecules with (multiple) specific properties is achieved \nby screening newly generated molecules for desirable \nproperties and reusing hit molecules to retrain the \ngenerative model with a bias. Despite its efficiency in \nproperty-oriented sampling, it lacks interpretability and \ncannot use modular motifs. Another trend in molecule \ngeneration is that explicit 3D molecular generative \nmodels have recently emerged [10], aiming to generate \nmolecules directly in 3D, outputting both the atom \ntypes and spatial coordinates, either in a one-shot or \nincrementally adding atoms or fragments. One such \nmodel is GeoDiff [11], which is inspired by denoising \ndiffusion generative models for image generation. This \nmodel can generate molecular conformations by treating \neach atom as a particle and learning to directly reverse \nthe diffusion process that transforms from a noise \ndistribution to stable conformations. Flam-Shepherd [12] \nshowed that language models based on LSTM can learn \nthe distributional properties of target datasets using both \nSMILES and SELFIE representations. It can generate \nlarger, more complex molecules or generate from \nchemical spaces with large ranges in size and structure, \nwhich shows advantages over graph generative models. \nORGAN [6] is a GAN based black-box generative model \nfor molecule generation, whose data generation can be \nsubject to a domain-specific reward function. However, \nits black-box nature makes it difficult to interpret learned \nlogic in terms of the chemical knowledge they learn \nand how they exploit the learned implicit knowledge \nfor generation. In addition, due to the lack of syntactic \nand semantic formalization as a limitation of specific \nstructured data, unsuitable generic string generation \nmodels often lead to invalid model outputs. We need \nto prepare a large number of valid combinations of \nstructures in advance to train a reasonable model, which \nis time-consuming. Although the grammar variational \nautoencoder (GVAE) [13] directly encodes from and \ndecodes grammar parse trees, aiming to ensure the \ngenerated outputs are always syntactically valid, it is still \nincapable of regularizing the models so that they only \ngenerate semantically valid objects.\nThe second category of molecule generative design \nmethods includes several key combinatorial optimization \nalgorithms such as genetic algorithms [14], reinforcement \nlearning [15], Bayesian optimization [16], Monte Carlo \nTree Search (MCTS) [17], Markov Chain Monte Carlo \n(MCMC) [3 ]. While GAs have demonstrated superior \nperformance in several molecule design benchmark \nstudies [18, 19], the genetic operators of mutation \nand cross-over lack the learning capability to achieve \nintelligent and efficient chemical space exploration. \nThis also applies to MCTS, which locally and randomly \nsearches each branch of intermediates and selects the \nmost promising ones during each generation’s iteration \n[20]. Bayesian optimization is usually applied together \nwith VAEs and searches the chemical space in the latent \nspace, Jin et  al. use Bayesian optimization to optimize \nmolecules generated by a variational autoencoder based \non molecular graphs, it generates a tree-structured \nscaffold over chemical substructures first, and then \ncombines them into a molecule with a graph message \npassing network [21]. However, the computational \ncomplexity of the dimensional space of its search space \nis relatively high, and its computational complexity \nincreases exponentially with the increase of the dimension \nof the optimization space, which also makes it difficult to \nhandle the modularity in molecule design. The chemical \nconstraints explicitly [16] are also difficult to achieve. \nReinforcement learning has been applied to generative \nmodels with both SMILES and 2D graph representations, \nwhich learns a policy network to determine the optimal \nactions that maximize a global reward such as a given \nproperty [15, 22]. However, RL is rarely used in de novo \nmolecule generation partially due to the difficulty to \nachieve long-range credit assignment and to obtain \ndifferentiable validity checks as the reward signal.\nA pivotal consideration in designing generative models \nfor molecules revolves around the representation level of \nthese molecules, encompassing atom-based, fragment-\nbased, and reaction-based approaches. While the \nmajority of existing models have leaned towards atom-\nbased representations like SMILES, more sophisticated \nalternatives such as SELFIES [23] and DeepSMILES [24] \nhave emerged for molecule property prediction. The \nimpact of choosing a specific molecule representation on \ngenerative design performance remains an unresolved \nquery. Notably, it has been observed that fundamental \natom representations, such as SMILES, pose challenges \nwhen attempting to harness the modules, motifs, or \nskeletons present in known molecules. On the contrary, \nfragment and reaction-based generative models offer the \npotential to exploit these larger building blocks; however, \nthey also grapple with the intricacies of expressive power.\nAnother major limitation of existing deep generative \nmodels for molecule design is that most of them cannot \nbe used for tinkering design: a specified part of an \nexisting molecule is masked for replacement of other \nmodules to gain specific function property, despite \nthat this is one of the most widely used approaches to \nexplore new molecules [2] due to many constraints \nimposed on the possible options. During these processes, \nchemists or molecular scientists usually resort to their \nintuition, chemical knowledge, and expertise to select \nsubstitution or doping elements and proportions to tune \nthe properties of the molecule by considering a variety \nof factors such as chemical compatibility, poison level, \nPage 3 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \ngeometric compatibility, synthesizability, and other \nheuristic knowledge.\nHere we propose a self-supervised probabilistic \nlanguage model, the Generative Molecular Transformer \n(GMTransformer) for molecular design and generation. \nThe model is based on transformers and the self-\nsupervised blank-filling language model BLM [25]. \nThe model interpretably calculates its probabilities \nand derives different actions depending on the token \nfrequency shown by its vocabulary. We use SMILES, \nSELFIES and DeepSMILES representations to train \ndifferent models, and found that each of them has its \nown advantage. The easy interpretation, data efficiency, \nand tinkering design potentials have been demonstrated \nin our recent work on inorganic materials composition \ndesign [26], which inspires us to explore its potential \nin molecule design in this work. We use MOSES \nbenchmarking metrics to evaluate the performance of \nour GMTransformer models. The results of our extensive \nexperiments show strong performance compared to \nstate-of-the-art baselines. Our GMTransformer model \nwith SMILES representation achieves 96.83% novelty and \n87.01% of IntDiv, which demonstrates that our model is \ncapable of generating a wide variety of novel molecules. \nWe also train generative models for maximizing different \nproperties: logP , tPSA, and QED, and find that our \nmodels can learn to generate molecules with specific \nproperties as demonstrated by the distribution of \ngenerated molecular properties.1\nMethods\nGenerative and tinkering molecular design \nas a blank‑filling process\nSMILES (Simplified Molecular Input Line Entry Sys -\ntem) uses a string of characters to describe describe \nthe connectivity of chemical compounds, focusing on \ntheir atomic arrangement and bonding patterns. While \nSMILES notation effectively captures the structural rela -\ntionships between atoms and bonds, it’s essential to note \nthat these representations do not convey information \nabout the three-dimensional arrangement of atoms in \nspace. Within SMILES, atoms, bonds, and branches com-\nbine to form the strings that represent molecules. The \natoms are represented by their element symbols, e.g. C, \nN, O, S, F. The atoms in aromatic rings are represented \nby lowercase letters, such as the lowercase c for aromatic \ncarbon. There are three types of bonds in SMILES: sin -\ngle bonds, double bonds, and triple bonds, and they are \ndenoted by -, =, # respectively. Branches are specified by \nenclosures in parentheses.\nAs shown in Table  1, the following canvas rewriting \nprocess shows how the GMTransformer generates the \nCC(= O)C sequence of the SMILES strings step by step. \nAt the beginning, there is only an initial blank token  of \n$1 on the canvas, then different candidate tokens \nand rewriting actions (E, _E, E_, _E_) are selected by \nGMTransformer. (1) action E: replace a blank with the \nelement E; (2) action _E: replace a blank with element E \nand insert a new blank on its left side, allowing further \nelement insertion; (3) action E_: replace a blank with \nelement E and insert a new blank on its right side, \nallowing further element insertion; (4) action _E_: replace \nthe blank with element E and insert new blanks on both \nsides [26]. Finally, a string without any blank symbols \nis generated on the canvas. In Table  1, there is only one \ninitial blank on the canvas in step 0, and it selects action \n_E_ with the element C to get $1 C $2. Then it replaces \nthe blank of $1 with the element C by taking action \nE in the first step. In the second step, the operation \nreplaces $1 blank with a branch (_. Then it chooses action \n_E_ and replaces the blank with element O. In the next \ntwo steps, it replaces the blank with bond = and branch \n)_ respectively to get canvas C C ( = O ) $1. Finally, it \nreplaces $1 with element C.\nGMTransformer is different from BERT [27] and \nXL-Net [28] as it relies on pre-existing content to learn \nand generate sequences. Instead of using the context \nof a pre-masked word to predict the probability of the \nmasked word, GMTransformer directly chooses the \naction and then inserts the word that best matches the \ncontent it learns at the appropriate position based on the \nprobabilistic dependencies in the generated vocabulary.\nTable 1 Strings of SMILES generated as a canvas rewriting \nprocess\n Canvas rewriting with 4 actions: (E, _E, E_, _E_)\nStep t Action operation\n0. $1 _E_ Replace $1 blank with _C_\n1. $1 C $2 E Replace $1 blank with C\n2. C C $1 E_ Replace $1 blank with (_\n3. C C ( $1 _E_ Replace $1 blank with _O_\n4. C C ( $1 O $2 _E Replace $1 blank with =\n5. C C ( = O $1 E_ Replace $1 blank with )_\n6. C C ( = O ) $1 E Replace $1 blank with C\n7. C C ( = O ) C\n1 Citation: L.W...J.H. Generative transformer for generative molecules \ndesign. DOI:000000/11111.\nPage 4 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \nGenerative molecular transformer: blank filling language \nmodel for molecule generation\nGMTransformer is not like the black-box models such \nas Variational Autoencoders (VAEs) [7], Generative \nAdversarial Networks (GANs) [6], and normalizing flow-\nbased models [8], it is a process interpretable model \ndesigned based on a blank language model (BLM) [25]. \nGMTransformer directly models the probability of the \ntokens in the vocabulary. It relies on the content of the \nexisting canvas to calculate the probability distribution \nfor selecting actions and tokens to generate a new canvas. \nIt can intelligently control the intermediate process \nof generating the string, and each step can give an \nexplanation of why it is doing it.\nString-based assembly strategies represent molecules \nas strings and explore chemical space by modifying \nstrings directly: character-by-character, token-by-\ntoken, or through more complex transformations based \non a specific grammar [29]. SMILES is well-known \nfor its simplicity in denoting molecules as strings by \nfollowing rules like adjacent atoms are assumed to be \nconnected by a single or aromatic bond and branches \nare specified in parentheses, etc. In particular, learning \nvalid molecules is substantially more difficult with the \nSMILES grammar, as there are many more characters to \ngenerate for these molecules and a higher probability that \nthe model will make a mistake and produce an invalid \nstring [12]. GMTransformer uses SMILES, SELFIES and \nDeepSMILES representations of atom-level tokenization. \nSMILES defines a character string representation of a \nmolecule by performing a depth-first pre-order spanning \ntree traversal of the molecular graph, generating symbols \nfor each atom, bond, tree-traversal decision, and broken \ncycles [30]. The SMILES representation of atom-level \ntokenization has 21 tokens in SMILES strings and 7 \nspecial tokens as the vocabulary during the training \nprocess. The vocabulary contains 13 atom tokens \n< C >, < c>, < O >, < o >, < N >, < n >, < F >,\n< S >, < s >, < Cl >,, < [nH ] > , and < [H] > , \n3 bond tokens < − >,<=>,< # > , 6 ring tokens \n< 1 >,< 2 >,< 3 >,< 4 >,< 5 >,< 6 > and 7 special \ntokens < PAD >, < UNK >, < FIRST>, < LAST >,\n< EOS >,< BLANK >,< BLANK _0 > . SELFIES and \nDeepSMILES also contain the same 7 special tokens as \nSMILES.\nThe SmilesPE tokenization has a mean length \nof approximately 6 tokens, while the atom-level \ntokenization has a mean length of approximately 40. \nSMILES Pair Encoding contains the special tokens and \nunique tokens from the frequent SMILES substrings. \ne.g, < CCC(C)(C)>  , < CCCC(C)>  , < NC(=O)C> . \nBoth SMILES and DEEP SMILES use the SmilesPE \ntokenization, which does not apply to SELFIES. More \ndetails can be found in [31].\nFigure  1 shows the architecture of our Generative \nMolecular Transformer (GMTransformer). The \nmodel utilizes four networks in three iterative stages. \nThe first stage includes the transformer network \nand linear and softmax layers. The second and third \nstages include linear and softmax layers, multi-layer \nperceptron network, respectively. In the first stage, the \ntransformer network encodes the canvas into a sequence \nof representations. Then which location of the blank \nshould be filled is selected by computing probabilities \nfrom linear and softmax layers. In the second stage, it \npicks an appropriate token and inserts it into the blank \nFig. 1 Neural network architecture of the blank filling language model for molecules tinkering using SMILES string \nO = C1CC(c2ccccc2)Oc2cc(O )cc(O )c21 as an example\nPage 5 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \nwith linear and softmax layers. In the final stage, the \naction of whether or not to create blanks to the left and \nright is determined by feeding the concatenation of the \nrepresentations of the selected blank and the token into \nthe multi-layer perceptron network. The model updates \nthe canvas and repeats the process until there are no \nblank positions on the canvas.\nDuring the training process, first of all, it initializes \nthe model parameter θ and then randomly samples a \ntraining example x = (x1 ,···,xn) with the length of n. \nNext, it samples the step length t from 0 to n − 1 and \nthe generation order with n-permutation σ of the given \nexample. It constructs a canvas c that remains the first t \ntokens x σj ( j = 1, ...,t ) and collapses the remaining n − t \ntokens as blanks. Then it takes n − t target actions a j−t \nfor filling x σj ( j = t + 1, ...,n) into canvas and calculates \nloss as Eq.  1. Finally it updates parameter θ by gradient \ndescent and repeats the whole process until convergence. \nMore details can be found in [25].\nwhere the θ is the the model parameter; cx,σ\nt  is the tth \ncanvas with the given training example x and the deter -\nmined generation order (permutation σ ); ax,σ\nt  represents \nthe action whether or not to create blanks to the left and \nright of the predicted token at step t with the order per -\nmutation σ and the selected blank.\n(1)− log(n!) − n\nn − t\n∑\nσt+1\nlog p\n(\nax,σ\nt |cx,σ\nt ;θ\n)\nDatasets\nWe use the dataset from the benchmarking platform \nMolecular Sets (MOSES) at https:// github. com/ molec  \nulars ets/ moses [32]. It contains 1,936,962 molecular \nstructures totally and splits them into three datasets \nfor experiments. Each of them consists of training \nsamples (around 1.6  M), test samples (176 k), and \nscaffold test samples (176 k) and we use the training \nand test sets in our experiments. We use the SMILES, \nSELFIES sets with the basic Atom-level and SmilesPE \ntokenizers.\nEvaluation criteria\nWe use the MOSES benchmarking score metrics to \nevaluate the overall quality of the generated samples. \nSeveral models with different tokens are used for \nGMTransformer training and each model generates 30,000 \nsamples that are evaluated by the MOSES benchmarking \nmetrics in Table  2. The ratios of valid and unique \n(unique@1k and unique@10k ) report the validity and \nuniqueness of the generated SMILES string respectively. \nNovelty is the proportion of molecules in the generated \nsamples that is not in the training set. Filter refers to the \nproportion of generated molecules that passed the filter \nduring dataset construction. The MOSES metrics also \nmeasure the internal diversity (IntDiv) [33], the similarity \nto the nearest neighbor (SNN) [32], Frechet ChemNet \ndistance (FCD) [34], fragment similarity (Frag) [32], and \nscaffold similarity (Scaf) [32].\nThe Internal diversity (IntDiv) is calculated via eq (2), it \nevaluates the chemical diversity in the generated set G  of \nTable 2 Performance comparison of generators using the MOSES Benchmark\n Bold value indicates the best performance of samples generated by different models under the same evaluation metric\nGMT MOSES reference models\nGMT‑ SMILES GMT‑PE‑ SMILES GMT‑ SELFIES GCT ‑SGDR VAE AAE char RNN\nValidity ↑ 0.8587 0.8288 1.000 0.9916 0.9767± 0.0012 0.9368± 0.0341 0.9748± 0.0264\nUnique@1k ↑ 1.0000 1.0000 1.0000 0.998 1.0±0.0 1.0±0.0 1.0±0.0\nUnique@10k ↑ 0.9998 0.9995 1.0000 0.9797 0.9984± 0.0005 0.9973± 0.002 0.9994 ± 0.0003\nIntDiv ↑ 0.8569 0.8558 0.8701 0.8458 0.8558± 0.0004 0.8557± 0.0031 0.8562 ± 0.0005\nFilters ↑ 0.9766 0.9797 0.7961 0.9982 0.6949± 0.0069 0.9960± 0.0006 0.9943 ± 0.0034\nNovelty ↑ 0.9531 0.8829 0.9683 0.6756 0.6949± 0.0069 0.7931± 0.0285 0.8419 ± 0.0509\nTest 0.5381 0.5778 0.4673 0.6513 0.6257± 0.0005 0.6081± 0.0043 0.6015 ± 0.0206\nSNN ↑ TestSF 0.5143 0.5460 0.4485 0.5990 0.5783± 0.0008 0.5677± 0.0045 0.5649 ± 0.0142\nTest 0.7294 0.1986 3.7750 0.7980 0.0990± 0.0125 0.5555± 0.2033 0.0732 ± 0.0247\nFCD ↓ TestSF 1.2607 0.7595 4.5698 0.9949 0.5670± 0.0338 1.0572± 0.2375 0.5204 ± 0.0379\nTest 0.9879 0.9982 0.9869 0.9922 0.9994± 0.0001 0.9910± 0.0051 0.9998 ± 0.0002\nFrag ↑ TestSF 0.9850 0.9958 0.9831 0.8562 0.9984± 0.0003 0.9905± 0.0039 0.9983 ± 0.0003\nTest 0.8661 0.9125 0.8431 0.8562 0.9386± 0.0021 0.9022± 0.0375 0.9242 ± 0.0058\nScaf ↑ TestSF 0.1650 0.1087 0.1096 0.0551 0.0588± 0.0095 0.0789± 0.009 0.1101 ± 0.0081\nPage 6 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \nmolecules and detects if the generative model has model \ncollapse.\nWhere G is the generated set, ma and mb are their \nMorgan fingerprints [35] for two molecules a and b. T is \nthe Tanimoto-distance [36] molecules of generated set G.\nThe Similarity to a nearest neighbor (SNN) is calculated \nvia eq (3).\nWhere m is the Morgan fingerprints of a molecule. \nT(mG, mR ) is an average Tanimoto similarity between mG \nin generated set G and its nearest neighbor molecule mR \nin the reference dataset R.\nThe Fréchet ChemNet distance (FCD) is computed \nfrom the activation of the penultimate layer of the deep \nneural network ChemNet, which was trained to predict \nthe biological activity of drugs. These activations can \ncapture chemical and biological properties of compounds \nfor two sets G and R. It is defined as Eq. 4):\nWhere µG , µR are mean vectors for sets G and R \nrespectively, ∑G , ∑R are full covariance matrices of \nactivations. Tr stands for the trace operator.\nThe Fragment similarity (Frag) is calculated via eq (5), \nwhich compares distributions of BRICS fragments [37] in \nthe generated set G and reference set R.\nWhere F is the set of BRICS fragments. cf(X ) stands for \nthe frequency of occurrences of a substructure fragment f \nin the molecules of set X.\nThe Scaffold similarity (Scaff) is similar with Frag but \nit computes the frequencies of Bemis-Murcko scaffolds \n[38]. It is calculated as eq (6):\n(2)IntDivp(G) = 1 − p\n√ 1\n|G|2\n∑\nm1 ,m2 ∈G\nT (m1,m2)p\n(3)SNN (G,R) = 1\n|G|\n∑\nmG∈G\nmax\nmR∈R\nT(mG,mR)\n(4)\nFCD (G,R) = ||µG − µ R||2 + Tr\n\n�\nG\n+\n�\nR\n−2\n��\nG\n�\nR\n�1/2\n\n(5)Frag (G ,R) =\n∑\nf∈F\n(\ncf(G ) ·cf(R)\n)\n√ ∑\nf∈F c2\nf(G )\n√ ∑\nf∈F c2\nf(R)\n(6)Scaf(G ,R) =\n∑\ns∈S (cs(G ) ·cs(R))√∑\ns∈S c2s(G )\n√∑\ns∈S c2s(R)\nWhere S is the set of Bemis-Murcko scaffolds, sS(X ) \nstands for the frequency of occurrences of a substructure \nscaffold s in the molecules of set X.\nResults and discussion\nDe novo generative design of molecules composition\nTraining of GMTransformer for hypothetical molecule \ngeneration\n We use the MOSES dataset as our benchmark dataset, \nwhich is widely used in the generative molecular design \ncommunity. The performance evaluation criteria is \nderived from the MOSES package, which is also a \nstandard in generator performance evaluation.\nThe GMTransformer model was trained and evaluated \nusing the database of the MOSES benchmarking platform. \nMOSES is a benchmarking platform to standardize the \ntraining results of molecule generation models. Its initial \ndataset, ZINK Clean Leads, contains about 4.6 million \nmolecules. The final dataset was obtained by filtering \nmolecules containing charged atoms (except C, N, S, O, \nF, Cl, Br, H); macrocyclic molecules with more than 8 \nmolecules in the ring; medical chemistry filters (MCFs) \nand PAINS filters. MOSES provides both training and \ntest sets and a set of metrics for assessing the quality and \ndiversity of the generated molecules. We also evaluate \nthe generated samples of three additional properties: the \noctanol-water partition coefficient (logP), the topological \nPolar Surface Area (tPSA), and the Quantitative Estimate \nof Drug-likeness (QED) [39] computed from RDKit [40] \nare used for training the conditional GMTransformer \ngenerator.\nEvaluation of GMT’s molecular generation performance\nWe evaluate the performance of our GMTransformer \ngenerators and compare it with that of the reference \nmodels using ten evaluation criteria with MOSES \nmetrics including validity, uniqueness (unique@1k and \nunique@10k), internal diversity (IntDiv), filters, novelty, \nthe similarity to a nearest neighbor (SNN), Frechet \nChemNet distance (FCD), fragment similarity (Frag), \nand scaffold similarity (Scaf). As shown in Table 2, GMT-\nSMILES, GMT-PE-SMILES and GMT-SELFIES generate \n85.87%, 82.88% and 100% valid samples, respectively. \nThe uniqueness of all models is almost 100%. Especially, \nthe novelty of GMT-SMILES, GMT-PE-SMILES and \nGMT-SELFIES is as high as 95.31%, 88.29% and 96.83% \nrespectively. At the same time, GMT-SMILES, GMT-\nPE-SMILES, GMT-SELFIES have the highest values with \n85.69%, 85.58%, and 87.01% of IntDiv respectively among \nall reference models. These high values mean that they \ncan generate samples with higher diversity, which may \nPage 7 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \naccelerate the discovery of new chemical structures. For \nFCD/Test, GMT-PE-SMILES performs best among all \nmodels with 19.86%, while GMT-SMILES and GMT-\nSELFIES have values with72.94% and 377.5%. GMT-\nSMILES, GMT-PE-SMILES and GMT-SELFIES also \nachieve high values with 16.50%, 10.87% and 10.96%, \nrespectively.\nWe also compared our model performance to two recent \ngenerative models. In a recent work, Gnaneshwar et al. [41] \ntrained a transformer-based score function (a diffusion \nmodel) on SELFIES representations of 1.5 million samples \nfrom the ZINC dataset and used the Moses benchmark -\ning framework to evaluate the generated samples on a suite \nof metrics. The evaluation metrics of validity, Unique@1k \nand Unique@10k are 100%, 88% and 82%. The perfor -\nmance for the filters, novelty, IntDiv, FCD/Test and FCD/\nTestSF metrics are 37%, 100%, 90%, 398.4% and 409.2%. We \nfind out that except for IntDiv and novelty, our GMT-PE-\nSMILES model performs better in terms of all other met -\nrics than this diffusion model with filters of 97.97%, FCD/\nTest of 75.95% and FCD/TestSF of 19.86%. In the work of \nWang et  al. [42], the cTransformer method is proposed, \nwhich is capable of generating both drug-like compounds \n(without specified targets) and target-specific compounds. \nThe metrics Valid, Unique@1k and Unique@10k are 98.8%, \n100% and 99.9%. For the Frag/Test, Frag/TestSF, SNN/Test \nand SNN/TestSF, the results are 100%, 99.8%, 61.9% and \n57.8%. We find that compared to the cTransformer, our \nGMT-SELFIES model performance are similar in terms \nof validity, unique@1k, unique@10k, Frag/Test and Frag/\nTestSF with 100%, 100%, 100%, 98.69% and 98.31%. Our \nGMT-PE-SMILES model has relatively lower performance \nin SNN/Test (57.78%) and SNN/TestSF (54.6%) but it has \nbetter interpretability and can be used for tinkering design \nas described in the Discussion section below.\nData efficiency of our GMT model\n We further checked how the amount of training \nsamples affects the generator’s performance. We trained \ntwo additional GMT models using 20% and 50% of the \nSELFIES represented samples and used them to generate \n30,000 hypothetical molecules, respectively. We then \ncompared these molecule qualities with those generated \nby the GMT trained with the whole dataset. The results \nare shown in Table 3. We find that when we reduced the \nsample size by 50%, most of the performance metrics \nonly changed slightly. For example, the Frag/Test, Frag/\nTestSF, IntDiv, IntDiv2, Filters all dropped by less than \n0.01, which indicates that we can achieve almost twice \nthe data-efficiency using our model. When we further \nreduced the training set size to 20%, we found several \nmeasures related to diversity and novelty increased while \nthe other performance measures deteriorate, but not too \nsignificantly.\nInterpretability of our GMT model and tinkering design\n First, we demonstrate the interpretability of our \nGMTransformer models. We selected the SMILES string \nC C n 1 n n n c 1 S C C ( = O ) N 1 C C c 2 c c c c c 2 1 \nfrom the dataset and pre-masked the first token to get a \ntemplate string <mask> C n 1 n n n c 1 S C C ( = O ) N 1 \nC C c 2 c c c c c 2 1, then we fed the template string to our \nmodel to check the possible substitutions for the masked \nposition. Table 4 shows the predicted substitution tokens \nsorted by their probabilities. We found that our model \ncorrectly predicted the masked token to be carbon with a \nTable 3 Performance comparison of the GMT models trained \nwith 20%, 50%, and 100% training samples\n Bold value indicates the best performance of samples generated by different \nmodels under the same evaluation metric\nTraining samples 20% 50% 100%\nValid ↑ 1.0000 1.0000 1.0000\nUnique@1000 ↑ 1.0000 1.0000 1.0000\nUnique@10000 ↑ 1.0000 0.9998 1.0000\nFCD/Test ↓ 4.3961 3.9164 3.7750\nSNN/Test ↑ 0.4526 0.4573 0.4673\nFrag/Test ↑ 0.9840 0.9850 0.9869\nScaf/Test ↑ 0.8225 0.8049 0.8431\nFCD/TestSF ↓ 5.2401 4.7000 4.5698\nSNN/TestSF ↑ 0.4362 0.4395 0.4485\nFrag/TestSF ↑ 0.9792 0.9802 0.9831\nScaf/TestSF ↑ 0.1340 0.1461 0.1096\nIntDiv ↑ 0.8707 0.8704 0.8701\nIntDiv2 ↑ 0.8653 0.8650 0.8646\nFilters ↑ 0.7858 0.7913 0.7961\nNovelty ↑ 0.9790 0.9751 0.9683\nTable 4 Blank-filling substitution suggestions by GMTransformer \nexplain how its working logic\nThe masked template is “<mask> C n 1 n n n c 1 S C C ( = O ) N 1 C C c 2 c c c c c \n2 1”\nSubstitution token Probability Action\nC 0.895 0\nC 0.048 1\nC 0.046 2\n= 0.007 3\nO 0.002 3\nC 0.001 3\n= 0 2\nO 0 1\nN 0 1\n( 0 3\nPage 8 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \nhigh probability of 0.895. The next two suggested actions \nare filling the blank with carbon and adding an additional \ntoken on the left or right, both with much lower prob -\nabilities (0.048 and 0.046). After inspecting the training \nsamples, we found that there are more than 13 training \nsamples that start with C C n 1 n n n c 1 S C C ( = O ), \nwhich establishes a context for making correct probabil -\nistic predictions of the substitutions at the masked posi -\ntions. This shows that our model successfully learns the \nstatistical dependencies among the different positions of \nthe molecule sequences, which explains their blank-fill -\ning suggestions.\nTo investigate the capability of our model for tinkering \ndesign, we picked a molecule’s SMILES string O =  C ( \nN C c 1 c c c s 1 ) N c 1 c c c ( F ) c c 1 and pre-masked \nit at 22nd position (F) to get a template string O =  C ( N \nC c 1 c c c s 1 ) N c 1 c c c ( <mask> ) c c 1 and fed it to \nour GMTransformer model for predicting substitutions \nat this position. Table  5 shows the probabilities over \nthe candidate tokens and corresponding actions \nto guide the blank-filling process. We found that \nour model suggested three substitutions with high \nprobabilities. Besides the masked element fluorine (F) \nwith the probability of 0.297, there are two alternative \nsubstitutions with bromine (Br) and chlorine (Cl) for \ntinkering with this position with the probability of \n0.427 and 0.25, respectively. All these three elements \nbelong to the same element group, sharing common \nchemical properties. which demonstrates that our \nmodel has learned to make meaningful tinkering design \nsuggestions based on the learned chemical knowledge.\nProcess of GMT’s learning of chemical rules\n To illustrate the chemical order/rules emerge during the \ntraining process of our GMT models, we save the inter -\nmediate models at the end of 1/5/10/15/20/25/30/50/100\n/150/200 epochs of training using the SMILES and SELF -\nIES dataset, respectively. Then we use 30,000 generated \nTable 5 Tinkering design based on GMTransformer suggests Br \nand Cl as replacements for F element\nThe template string is “O = C ( N C c 1 c c c s 1 ) N c 1 c c c ( <mask> ) c c 1”\nSubstitution token Probability Action\nBr 0.427 0\nF 0.297 0\nCl 0.25 0\nO 0.019 0\nC 0.002 1\nS 0.001 1\nO 0.001 2\nN 0 0\n# 0 3\n- 0 1\nepoch\nPercentage\n0.0000\n0.2500\n0.5000\n0.7500\n1.0000\n15 10 15 20 25 30 50 100 150 200\nvalid unique@10000 IntDiv Scaf/TestSF Novelty\nFig. 2 Percentages of valid, unique@10000, intDiv and Scaf/TestSF samples generated by the SMILES atom tokenizer models saved \nover the training process. The models generate few valid SMILES strings in the beginning. As the training goes on, the models gradually gain \nthe capability to generate chemically valid SMILES molecules compositions\nPage 9 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \nsamples to evaluate validity, unique@10k, IntDiv, Scaf/\nTestSF and Novelty with MOSES benchmarking metrics. \nAs shown in Fig.  2, the validity of the model using the \nSMILES representation is only about 50% of the maxi -\nmum value when the epoch of the model training is less \nthan 30, and its validity exceeds 80% at the 100 epochs. \nThis growing process shows that the model is learning \nthe valence rules and the syntax of the SMILES language. \nFor the model using the SELFIES representation, the \nresults are shown in Fig. 3. Because every SELFIES syntax \nis guaranteed to correspond to a valid molecule [23], the \nvalidity is always 100% throughout training epochs from \n1 to 200. The increase in Scaf/TestSF value also indicates \nthat the model has learned the Bemis–Murcko scaffold \n[38], which contains all molecule’s ring structures and \nlinker fragments connecting rings.\nComparison of different molecule representations: SMILES, \nSELFIES, and DeepSMILES\nDifferent representations make the model more capable \nof generating new potential molecules. We use three \ntypes of string-based molecular representations: The \nsimplified molecular input line entry system (SMILES) \n[43], SELF-referencIng Embedded Strings (SELFIES) \n[44], DeepSMILES [24] and two kinds of tokenizers: \nAtom-level and SmilesPE [31]. Table  7 shows exam -\nples of the different molecule representations with \ntwo types of tokenizers. SELFIES only has atom-level \ntokenizers. We first use SMILES, which is the most \nwidely used representation in computational chem -\nistry. SMILES has some weaknesses such as multiple \ndifferent SMILES strings can represent the same mol -\necule and it is not robust because it is possible for gen -\nerative models to create strings that do not represent \nvalid molecular graphs. DeepSMILES is a modifica -\ntion of SMILES which obviates most syntactic errors, \nwhile semantic mistakes were still possible [24]. There -\nfore, we also use the representation of SELFIES, which \ncan generate a 100% effective molecular graph to defi -\nnitely avoid the problem of model robustness. SELF -\nIES is like an automaton or derivation grammar, which \nis designed to eliminate syntactic and semantic invalid \nstrings. Atomic-level tokenization is a method com -\nmonly used in deep learning, which simply breaks the \nSMILES string character-by-character, with each char -\nacter serving as a token. We use not only an atom-level \ntokenizer  but also the SmilesPE representation, which \nhas shorter input sequences and can save the compu -\ntational cost of model training and inference. Smile -\nsPE identifies and retains frequent SMILES substrings \nas unique tokens, where each token is represented as \na chemically meaningful substructure. We utilize bold \nand thin strings with spaces between them to distin -\nguish different substrings that are combined into one \nsingle tokenizer of SmilesPE in Table 7 .\nWe also train five GMT models using different \nrepresentations and tokens, generate 30,000 hypo -\nthetical molecules and evaluate them using MOSES \nepoch\nPercentage\n0.0000\n0.2500\n0.5000\n0.7500\n1.0000\n15 10 15 20 25 30 50 100 150 200\nvalid unique@10000 IntDiv Scaf/TestSF Novelty\nFig. 3 Percentages of valid, unique@10000, intDiv and Scaf/TestSF samples generated by the SELFIES atom tokenizer models saved \nover the training process. The models generate almost one hundred percent valid SMILES strings from the beginning to the end and the Scaf/\nTestSF value has also been growing with epoch from 1 to 200\nPage 10 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \nbenchmarking metrics. Table  6 shows the perfor -\nmance of the comparison of the MOSES Benchmark -\ning Results. All models perform very well in terms of \nuniqueness, in the range of 99.5%-100%. In terms of \nthe novelty of the hypothetical molecules, GMT-PE-\nSMILES achieves 88.92%, while all other models exceed \n90%. GMT-PE-SMILES outperforms the other models \nby a wide margin on FCD/Test at 19.86%.\nWe also evaluate our GMT model performance by \nusing the QM9 as the training dataset with the atom-\nlevel SMILES representation (Table  7). The validity, \nunique@1k, unique@10k are 89.37%, 100%, and 96.89% \nrespectively, which are very close to the performance \nof the GMT trained with MOSES SMILES dataset. The \nGMT-QM9 is also slightly better in terms of IntDiv and \nnovelty while its filters score is much lower with 0.6549 \ncompared to 0.8569 of GMT-MOSES. Other metrics \nsuch as SNN, FCD, Frag, Scaf, they are all evaluated \nusing the reference sets of MOSES, the GMT-QM9 has \nmuch lower performance indicating that the training \nsample distribution strongly affects the properties of \ngenerated samples (Table 8 ).\nConditional training of generative models for molecule \ndesign\nOne desirable generation capability of molecular \ngenerators is to design molecules that optimize one or \nmore specific properties. Here we evaluate whether our \nmodels have such capability by conditionally training \nthree generators aiming at generating samples with \na desired property. This is in contrast to conditional \ngenerative models [5] which take the conditions as input. \nBasically, we prepare three different training sets from \nthe MOSES training dataset by picking samples whose \ncorresponding property values are within the top 50% \nof the whole MOSES training dataset, where the three \nproperties include the octanol-water partition coefficient \n(logP), the topological Polar Surface Area (tPSA), and \nTable 6 Performance comparison of GMT models with different representations\n Bold value indicates the best performance of samples generated by different models under the same evaluation metric\nGMT models\nGMT‑ SMILES GMT‑QM9‑ SMILES GMT‑PE‑ SMILES GMT‑ SELFIES GMT‑ DEEP GMT‑PE‑ DEEP\nValidity ↑ 0.8586 0.8937 0.8288 1.0000 0.8168 0.7954\nUnique@1k ↑ 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000\nUnique@10k ↑ 0.9998 0.9689 0.9995 1.0000 1.0000 0.9997\nIntDiv ↑ 0.8569 0.9182 0.8558 0.8701 0.8570 0.8519\nFilters ↑ 0.9765 0.6549 0.9797 0.7961 0.9844 0.9847\nNovelty ↑ 0.9532 1.0000 0.8829 0.9683 0.9367 0.9149\nTest 0.5381 0.2575 0.5778 0.4673 0.5509 0.5722\nSNN ↑ TestSF 0.5143 0.2510 0.5460 0.4485 0.5246 0.5405\nTest 0.7294 30.5280 0.1986 3.7750 0.3604 0.4366\nFCD ↓ TestSF 1.2607 31.3022 0.7595 4.5698 0.9563 1.0736\nTest 0.9879 0.3945 0.9982 0.9869 0.9981 0.9967\nFrag ↑ TestSF 0.9850 0.3909 0.9958 0.9831 0.9964 0.9934\nTest 0.8661 0.0007 0.9125 0.8431 0.8880 0.8903\nScaf ↑ TestSF 0.1649 0.0000 0.1087 0.1096 0.1511 0.1170\nTable 7 Comparison of the different molecule representations: \nSMILES, SELFIES, and DeepSMILE\n Bold value indicates the best performance of samples generated by different \nmodels under the same evaluation metric\nTokenizer Atom‑level\nSMILES C O c 1 c c c c c 1 O C ( = O ) O c 1 c c c c c 1 O C\nDeepSMILES C O c c c c c c 6 O C = O ) O c c c c c c 6 O C\nSELFIES [C] [N] [C] [Branch1] [C] [P] [C] [C] [Ring1] [=Branch1]\nTokenizer SmilesPE\nSMILES COc1ccccc1 O C(=O)O c1ccccc1 OC\nDeepSMILES CO cccc cc 6 OC =O) O cccc cc 6 OC\nTable 8 Datasets for conditional generation\nWhole set Training set (Top \n50%)\nGenerated \nsamples\nLogP 1,584,662 792,331 16,748\ntPSA 1,584,662 792,331 16,643\nQED 1,584,662 792,331 17,082\nPage 11 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \nthe Quantitative Estimate of Drug-likeness (QED), \nwhich are computed using the RDKit. We then train \nthe three generators with these high-property training \nmolecules and use them to generate 20,000 candidate \nsamples, which are then fed to the RDkit for the property \ncalculation. It is found that RDKit cannot calculate the \nproperties for some of these generated samples. After \nfiltering these generated samples, we finally obtain \n16,748, 16,643, and 17,082 samples for LogP , tPSA, and \nQED respectively. The distributions of these properties \nvalues of the whole dataset, the biased (top 50%) training \nset, and the generated candidate sets are shown in Fig.  4. \nIt is found that for all three properties, the distributions \nof our generated molecules are much closer to those \nof the top 50% training sets compared to the property \ndistributions of the whole MOSES training dataset, \nwhich indicates that the GMTransformer models have \nlearned the implicit rules to generate high-property \nmolecules. It indicates that our models can learn the \nintrinsic bias of the molecules that are shared among a \ngroup of molecules with a desired common property. We \ncan also approach fine-tuning to improve the model’s \nperformance when dealing with a smaller, more focused \ndataset.\nIn Fig.  5, we present sample molecular structures gen -\nerated by conditional training models based on proper -\nties such as logP , tPSA, and QED. For each property, we \nselected some of them including the highest score, the \nlowest score, and an intermediary score for each prop -\nerty to provide a comprehensive view. Figure  5a, b, and \nc showcase molecular structures generated by the model \ntrained using the top 50% of logP property values. Mean -\nwhile, Fig.  5d, e, and f exhibit molecular structures gen -\nerated from the model trained with the top 50% of QED \nproperty values. Lastly, the structures in Fig.  5g, h, and i \nare shown from the model trained using the top 50% of \ntPSA property values.\nOne possible issue with our conditional training \nstrategy is that we may only have access to a limited \nnumber of samples with labeled properties of interest. In \nthat case, we can take the transfer learning strategy: we \nFig. 4 Comparison of property distribution of three different datasets: the whole MOSES training set, the top 50% properties set used for training \nthe conditional generator models, and the generated samples set for logP , tPSA, QED\nPage 12 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \nfirst train a pre-trained model using datasets with a large \namount of labels of related properties. The pre-trained \nmodel can then be fine-tuned over the small dataset with \nthe target property labels.\nDiscussion\nThe ability to generate new potential molecular structures \nhas broad implications for a variety of fields, including \ndrug discovery, materials science, and renewable energy. \nIt has the potential to revolutionize the development of \nnew drugs, lead to the creation of new materials with \ndesirable properties, and help advance the development \nof renewable energy technologies. This ability has \nthe potential to drive innovation and advance our \nunderstanding of the world around us, with significant \nimplications for the future of science and technology. \nIt is of benefit to incorporate synthesis knowledge into \ncomputational approaches such as small molecule de \nnovo design in order to enhance the practical relevance \nof the results and achieve better acceptance by medicinal \nchemists [45].\nWhile uniqueness, validity, and novelty are evaluated \nmainly based on the molecule structure, the relevance \nof generated samples to druggability and biological pro -\ncesses is not clear. To address this issue, we evaluate \nour models using the FCD criterion [34], which is com -\nputed using the activation of the penultimate layer of \nChemNet. This criterion can capture both the chemical \nFig. 5 Sample molecular structures generated by conditional training models of logP , QED, and tPSA\nPage 13 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \nand biological property of the generated molecules. We \nfind that out of the six language models (in Tables  2 and \n6), our GMT-PE-SMILES achieves the best performance \nin terms of the FCD/Test measure with 19.86%, while \nGMT-SMILES shows the performance with 72.94% and \nthe baseline GCT-SGDR shows 79.80% of the FCD/\nTest. However, the FCD/Test performance of the GMT-\nSELFIES model is relatively low without a clear reason. \nWe also find the FCD performance is also relatively low \nin other relevant models [41, 46] that also use SELFIES \nrepresentation.\nTo evaluate how the hyper-parameters may affect the \nmodel performance, we utilize SMILES representations \nwith Atom-level tokenizer for hyper-parameter tuning. \nWe use 5, 10, 15, and 20 transformer layers to train \nthe model, then generate 30,000 samples and evaluate \nthe criteria with the MOSES benchmarking metrics. \nAs shown in Table  9, the overall performance of the \nmetrics is similar for each model, the best of which is \nwhen the number of layers is 15. The values of Validity, \nUnique@10000, Filters, and Novelty at this point are \n86.46%, 99.99%, 98.06%, 94.13% respectively. The values \nof FCD/Test and Scaf/TestSF are 32.08% and 15.41% \nrespectively. We use the default number of layers for the \nmodel of 6 instead of 15 because hyper-parameter studies \nshow that the number of layers has little effect on the \noverall performance of the model, and the model with \nthe default number of layers has higher efficiency.\nIn addition to the ability to generate molecular \nstructures from scratch, our model has a potential \napplication: molecular optimization. This feature \nconsists of shielding specific portions of a given \nmolecular structure and then utilizing our model to \nintelligently complete those shielded portions. Unlike \nthe traditional method of regenerating molecules, our \nmodel optimally adapts the existing structure. We have \napplied this method in materials [26] with good results. \nWith this approach, we can extend the utility of our \nmodel to a wider range of molecular design tasks. This \nimprovement makes our model an important tool for a \nvariety of applications in the molecular design process. \nFrom generating new structures to refining existing ones, \nour model demonstrates its multifaceted potential in \nimproving the efficacy and efficiency of molecular design \nstrategies.\nConclusion\nWe propose the Generative Molecular Transformer \n(GMT), a probabilistic generative language model based \non neural networks and transformers for the generation \nand design of molecules. Our model is based on the blank \nfilling language model, which has a unique advantage \nand potential for tinkering molecule design as we \nshowed in both in this study (Tables  4 and 5 ) ans well as \nin our previous work for tinkering design of materials \ncompositions [26]. Since there are many design constraints \nin real-world molecule and drug design, most of the \ntime, the tinkering design is a preferred approach which \nstarts from an exciting molecule and then finetunes its \nstructures. Our GMT model thus conveniently provides a \nway for such tinkering design. The advantages of the GMT \nmodel also include its interpretability and data efficiency \nas shown in this study (Table 3) as well as in our previous \nwork on generating hypothetical inorganic materials [26]. \nOverall, we have shown that our probabilistic transformer \nmodel can efficiently learn the grammar rules of molecules \nand exploit them for generating high-quality hypothetical \nmolecules.\nAnother advantage of our GMTransformer for molecule \ngeneration is that it allows the use functional groups \nof molecules as tokens to train models that generate \nmolecules with specific functions. The advantage over a \nsimple substructure search for the respective functional \ngroup for linking is the ability to directly construct the \nvirtual product. Changes introduced by replacing a part \nof the structure can thus be scored in the context of the \ncomplete molecule [45]. While fragment-based models \nhave been proposed before, the blank filling model we use \nhere can be used to discover those function groups as highly \ndependent subsequences. The discovery and usage of these \nspecial functional groups of molecules may have great \npotential for molecule design for specific functions suitable \nfor real-life scenarios [47]. For example, fragment-based \ndesign has unique advantages in drug design [48]. We also \nfind that the molecule sequence rewriting probabilities and \nTable 9 Hyper-parameter tuning of GMTransform molecules \ngenerator\nNumber of layers 5 10 15 20\nValid 0.8582 0.8488 0.8646 0.8549\nUnique@1000 1.0000 1.0000 1.0000 1.0000\nUnique@10000 1.0000 0.9997 0.9999 0.9998\nIntDiv 0.8529 0.8536 0.8541 0.8540\nFilters 0.9802 0.9838 0.9806 0.9812\nNovelty 0.9351 0.9389 0.9413 0.9362\nSNN Test 0.5559 0.5556 0.5509 0.5554\nTestSF 0.5277 0.5279 0.5252 0.5304\nFCD Test 0.5404 0.3243 0.3108 0.3903\nTestSF 1.1415 0.8461 0.7978 0.8609\nFrag Test 0.9939 0.9950 0.9965 0.9966\nTestSF 0.9904 0.9913 0.9933 0.9950\nScaf Test 0.8954 0.8902 0.8955 0.8868\nTestSF 0.1425 0.1482 0.1541 0.1285\nPage 14 of 15Wei et al. Journal of Cheminformatics           (2023) 15:88 \ninterpretability of the GMT model provide more control \nover the molecular generation process, which brings more \npotential for generating molecules with specific properties. \nThis has been demonstrated in our materials composition \ndesign using the BLM model [26]. We believe that data \nefficiency, interpretability, and modularity are three key \nfeatures that are required for next-generation generative \nmolecule design algorithms.\nAuther contributions\nConceptualization, JH; methodology,JH, LW, NF, YS,QW; software, JH, LW,NF; \nresources, JH; writing–original draft preparation, JH, LW, NF; writing–review \nand editing, JH, LW; visualization, LW, YS, JH; supervision, JH; funding \nacquisition, JH and QW.\nFunding\nThe research reported in this work was supported in part by National Science \nFoundation under the Grant 2110033. The views, perspectives, and content \ndo not necessarily represent the official views of the NSF. QW would like to \nacknowledge the seed funding support from the Big Data Health Science \nCenter (BDHSC) of the University of South Carolina.\nAvailability of data and materials\nhe raw molecules QM9 dataset is downloaded from http:// quant um- machi ne. \norg/ datas ets/. The modified code can be found at http:// github. com/ uscco \nlumbia/ GMTra nsfor mer\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests\nReceived: 20 September 2022   Accepted: 10 September 2023\nReferences\n 1. Meyers Joshua, Fabian Benedek, Brown Nathan (2021) De novo molecular \ndesign and generative models. Drug Discov Today 26(11):2707–2715\n 2. Alex Zunger, Malyi Oleksandr I (2021) Understanding doping of quantum \nmaterials. Chem Rev 121(5):3031–3060\n 3. Du Y, Fu T, Sun J, Liu S (2022) Molgensurvey: a systematic survey in \nmachine learning models for molecule design. arXiv preprint. arXiv: 2203. \n14500\n 4. Fergus Imrie, Bradley Anthony R, Mihaela Schaar, van der, Deane Char-\nlotte M, (2020) Deep generative models for 3d linker design. J Chem \nInform Model 60(4):1983–1995\n 5. Lyle Regenwetter, Heyrani Nobari Amin, Faez Ahmed (2022) Deep gener-\native models in engineering design: a review. J Mech Des 144(7):071704\n 6. Guimaraes GL, Sanchez-Lengeling B, Outeiral C, Farias PLC, Aspuru-Guzik \nA (2017) Objective-reinforced generative adversarial networks (organ) for \nsequence generation models. arXiv preprint. arXiv: 1705. 10843\n 7. Dai H, Tian Y, Dai B, Skiena S, Song L (2018) Syntax-directed variational \nautoencoder for structured data. arXiv preprint. arXiv: 1802. 08786\n 8. Zang C, Wang F (2020) Moflow: an invertible flow model for generating \nmolecular graphs. In Proceedings of the 26th ACM SIGKDD International \nConference on Knowledge Discovery & Data Mining, pages 617–626\n 9. Julia Westermayr, Joe Gilkes, Rhyan Barrett, Maurer Reinhard J (2023) \nHigh-throughput property-driven generative design of func-\ntional organic molecules. Nat Comput Sci. https:// doi. org/ 10. 1038/ \ns43588- 022- 00391-1\n 10. Baillif Benoit, Cole Jason, McCabe Patrick, Bender Andreas (2023) Deep \ngenerative models for 3d molecular structure. Curr Opin Struct Biol \n80:102566\n 11. Xu M, Yu L, Song Y, Shi C, Ermon S, Tang J (2022) Geodiff: a geometric \ndiffusion model for molecular conformation generation. In International \nConference on Learning Representations\n 12. Flam-Shepherd Daniel, Zhu Kevin, Aspuru-Guzik Alán (2022) Language \nmodels can learn complex molecular distributions. Nat Commun \n13(1):3293\n 13. Kusner MJ, Paige B, Hernández-Lobato JM (2017) Grammar vari-\national autoencoder. In International conference on machine learning, \n1945–1954. PMLR\n 14. Kwon Youngchun, Kang Seokho, Choi Youn-Suk, Kim Inkoo (2021) \nEvolutionary design of molecules based on deep learning and a genetic \nalgorithm. Sci Rep 11(1):1–11\n 15. Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Mar-\ngreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, Atanas \nPatronov (2020) Reinvent 2.0: an ai tool for de novo drug design. J Chem \nInform Model 60(12):5918–5922\n 16. Winter Robin, Montanari Floriane, Steffen Andreas, Briem Hans, Noé \nFrank, Clevert Djork-Arné (2019) Efficient multi-objective molecular \noptimization in a continuous latent space. Chem Sci 10(34):8016–8024\n 17. Yang Xiufeng, Zhang Jinzhe, Yoshizoe Kazuki, Terayama Kei, Tsuda Koji \n(2017) Chemts: an efficient python library for de novo molecular genera-\ntion. Sci Technol Adv Mater 18(1):972–976\n 18. Huang K, Fu T, Gao W, Zhao Y, Roohani Y, Leskovec J, Coley CW, Xiao C, \nSun J, Zitnik M (2021) Therapeutics data commons: machine learning \ndatasets and tasks for therapeutics. arXiv e-prints, pages arXiv–2102\n 19. Nathan Brown, Marco Fiscato, Segler Marwin HS, Vaucher Alain C (2019) \nGuacamol: benchmarking models for de novo molecular design. J Chem \nInform Model 59(3):1096–1108\n 20. Yang X, Aasawat TK, Yoshizoe K (2020) Practical massively parallel monte-\ncarlo tree search applied to molecular design. arXiv preprint arXiv: 2006. \n10504\n 21. Jin W, Barzilay R, Jaakkola T (2018) Junction tree variational autoencoder \nfor molecular graph generation. In International conference on machine \nlearning, 2323–2332. PMLR\n 22. Zhenpeng Zhou, Steven Kearnes, Li Li, Zare Richard N, Patrick Riley (2019) \nOptimization of molecules via deep reinforcement learning. Sci Rep \n9(1):1–10\n 23. Krenn M, Häse F, Nigam A, Friederich P , Aspuru-Guzik A (2019) Selfies: a \nrobust representation of semantically constrained graphs with an exam-\nple application in chemistry. arXiv preprint arXiv: 1905. 13741\n 24. O’Boyle N, Dalke A (2018) Deepsmiles: an adaptation of smiles for use in \nmachine-learning of chemical structures\n 25. Shen T, Quach V, Barzilay R, Jaakkola T (2020) Blank language models. In \nProceedings of the 2020 Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP), 5186–5198\n 26. Wei L, Li Q, Song Y, Stefanov S, Siriwardane E, Chen F, Hu J (2022) Crystal \ntransformer: Self-learning neural language model for generative and \ntinkering design of materials. arXiv preprint arXiv: 2204. 11953\n 27. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: pre-training of deep \nbidirectional transformers for language understanding. arXiv preprint \narXiv: 1810. 04805\n 28. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov RR, Le QV (2019) Xlnet: \ngeneralized autoregressive pretraining for language understanding. \nAdvances in neural information processing systems, 32\n 29. Gao Wenhao, Tianfan Fu, Sun Jimeng, Coley Connor (2022) Sample \nefficiency matters: a benchmark for practical molecular optimization. Adv \nNeural Inform Process Syst 35:21342–21357\n 30. Ross Jerret, Belgodere Brian, Chenthamarakshan Vijil, Padhi Inkit, Mroueh \nYoussef, Das Payel (2022) Large-scale chemical language representa-\ntions capture molecular structure and properties. Nat Mach Intell \n4(12):1256–1264\n 31. Li Xinhao, Fourches Denis (2021) Smiles pair encoding: a data-driven \nsubstructure tokenization algorithm for deep learning. J Chem Inform \nModel 61(4):1560–1569\n 32. Polykovskiy Daniil, Zhebrak Alexander, Sanchez-Lengeling Benjamin, \nGolovanov Sergey, Tatanov Oktai, Belyaev Stanislav, Kurbanov Rauf, \nPage 15 of 15\nWei et al. Journal of Cheminformatics           (2023) 15:88 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nArtamonov Aleksey, Aladinskiy Vladimir, Veselov Mark et al (2020) Molecu-\nlar sets (moses): a benchmarking platform for molecular generation \nmodels. Front Pharmacol 11:1931\n 33. Mostapha Benhenda (2018) Can ai reproduce observed chemical diver-\nsity? bioRxiv. https:// doi. org/ 10. 1101/ 292177\n 34. Preuer K, Renz P , Unterthiner T, Hochreiter S, Klambauer G (2018) Fréchet \nchemblnet distance: A metric for generative models for molecules. arXiv \npreprint arXiv: 1803. 09518\n 35. Rogers David, Hahn Mathew (2010) Extended-connectivity fingerprints. J \nChem Inform Model 50(5):742–754\n 36. Tanimoto, Taffee T (1958) Elementary mathematical theory of classifica-\ntion and prediction, International Business Machines Corp.\n 37. Jörg Degen, Christof Wegscheid-Gerlach, Andrea Zaliani, Matthias Rarey \n(2008) On the art of compiling and using’drug-like’chemical fragment \nspaces. ChemMedChem Chem Enabling Drug Discov 3(10):1503–1507\n 38. Bemis Guy W, Murcko Mark A (1996) The properties of known drugs. 1. \nmolecular frameworks. J Med Chem 39(15):2887–2893\n 39. Richard Bickerton G, Paolini Gaia V, Jérémy Besnard, Sorel Muresan, \nHopkins Andrew L (2012) Quantifying the chemical beauty of drugs. Nat \nChem 4(2):90–98\n 40. Landrum Greg (2019) Rdkit: Open-source cheminformatics, v. 2019. \nGitHub (https:// github. com/ rdkit/ rdkit). Accessed 15 Aug 2022\n 41. Gnaneshwar D, Ramsundar B, Gandhi D, Kurchin R, Viswanathan V (2022) \nScore-based generative models for molecule generation. arXiv preprint \narXiv: 2203. 04698\n 42. Wang W, Wang Y, Zhao H, Sciabola S (2022) A pre-trained conditional \ntransformer for target-specific de novo molecular generation. arXiv \npreprint arXiv: 2210. 08749\n 43. David Weininger (1988) Smiles, a chemical language and information sys-\ntem. 1. introduction to methodology and encoding rules. J Chem Inform \nComput Sci 28(1):31–36\n 44. Krenn Mario, Häse Florian, Nigam AkshatKumar, Friederich Pascal, \nAspuru-Guzik Alan (2020) Self-referencing embedded strings (selfies): a \n100% robust molecular string representation. Mach Learn Sci Technol \n1(4):045024\n 45. Hartenfeller Markus, Eberle Martin, Meier Peter, Nieto-Oberhuber Cristina, \nAltmann Karl-Heinz, Schneider Gisbert, Jacoby Edgar, Renner Steffen \n(2011) A collection of robust organic synthesis reactions for in silico \nmolecule design. J Chem Inform Model 51(12):3093–3098\n 46. Yang Yuwei Wu, Zhenxing Yao Xiaojun, Kang Yu, Tingjun Hou, Chang-Yu \nHsieh, Huanxiang Liu (2022) Exploring low-toxicity chemical space with \ndeep learning for molecular generation. J Chem Inform Model. https:// \ndoi. org/ 10. 1021/ acs. jcim. 2c006 71\n 47. Mowbray DJ, Glenn Jones, Sommer Thygesen Kristian (2008) Influence \nof functional groups on charge transport in molecular junctions. J Chem \nPhys 128(11):111103\n 48. McAulay Kirsten, Bilsland Alan, Bon Marta (2022) Reactivity of covalent \nfragments and their role in fragment based drug discovery. Pharmaceuti-\ncals 15(11):1366\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7870946526527405
    },
    {
      "name": "Computer science",
      "score": 0.7227433919906616
    },
    {
      "name": "Transformer",
      "score": 0.6889809370040894
    },
    {
      "name": "Probabilistic logic",
      "score": 0.6101680994033813
    },
    {
      "name": "Generative Design",
      "score": 0.5783739686012268
    },
    {
      "name": "Generative model",
      "score": 0.5337461233139038
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42511457204818726
    },
    {
      "name": "Language model",
      "score": 0.42000800371170044
    },
    {
      "name": "Machine learning",
      "score": 0.361247718334198
    },
    {
      "name": "Engineering",
      "score": 0.12547940015792847
    },
    {
      "name": "Electrical engineering",
      "score": 0.07605531811714172
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Metric (unit)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}