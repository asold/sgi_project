{
    "title": "HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression",
    "url": "https://openalex.org/W3205580107",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5020518694",
            "name": "Chenhe Dong",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5046576694",
            "name": "Yaliang Li",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5074799043",
            "name": "Ying Shen",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A5101851065",
            "name": "Minghui Qiu",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3173256823",
        "https://openalex.org/W2604763608",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W2472819217",
        "https://openalex.org/W2163302275",
        "https://openalex.org/W2753160622",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W4288346515",
        "https://openalex.org/W1986614398",
        "https://openalex.org/W2963804140",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2948974578",
        "https://openalex.org/W3104763958",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W4288256350",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W2994928925",
        "https://openalex.org/W4288087680",
        "https://openalex.org/W2963777311",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W2996834012",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W2970505118",
        "https://openalex.org/W2982455176",
        "https://openalex.org/W3035451444",
        "https://openalex.org/W2770645414",
        "https://openalex.org/W3021805648",
        "https://openalex.org/W2937297214",
        "https://openalex.org/W2997892440",
        "https://openalex.org/W2962897020",
        "https://openalex.org/W4295292688",
        "https://openalex.org/W2951221758",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2329970264",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4255712638",
        "https://openalex.org/W2996923239",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W2965491249",
        "https://openalex.org/W2964105864"
    ],
    "abstract": "On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https://github.com/cheneydon/hrkd.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3126–3136\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3126\nHRKD: Hierarchical Relational Knowledge Distillation for Cross-domain\nLanguage Model Compression\nChenhe Dong1, Y aliang Li2, Ying Shen1∗, Minghui Qiu2∗\n1 Sun Y at-sen University 2 Alibaba Group\ndongchh@mail2.sysu.edu.cn, sheny76@mail.sysu.edu.cn\n{yaliang.li, minghui.qmh}@alibaba-inc.com\nAbstract\nOn many natural language processing tasks,\nlarge pre-trained language models (PLMs)\nhave shown overwhelming performances com-\npared with traditional neural network meth-\nods. Nevertheless, their huge model size and\nlow inference speed have hindered the deploy-\nment on resource-limited devices in practice.\nIn this paper, we target to compress PLMs\nwith knowledge distillation, and propose a\nhierarchical relational knowledge distillation\n(HRKD) method to capture both hierarchical\nand domain relational information. Specif-\nically , to enhance the model capability and\ntransferability , we leverage the idea of meta-\nlearning and set up domain-relational graphs\nto capture the relational information across dif-\nferent domains. And to dynamically select\nthe most representative prototypes for each\ndomain, we propose a hierarchical compare-\naggregate mechanism to capture hierarchical\nrelationships. Extensive experiments on pub-\nlic multi-domain datasets demonstrate the su-\nperior performance of our HRKD method as\nwell as its strong few-shot learning ability . For\nreproducibility , we release the code at\nhttps:\n//github.com/cheneydon/hrkd.\n1 Introduction\nLarge pre-trained language models (PLMs) (e.g.,\nBERT (\nDevlin et al. , 2019)) have demonstrated\ntheir outperforming performances on a wide range\nof NLP tasks, such as machine translation ( CON-\nNEAU and Lample , 2019; Zhu et al. , 2020), sum-\nmarization ( Zhang et al. , 2019; Liu and Lapata ,\n2019), and dialogue generation ( Bao et al. , 2020;\nZheng et al. , 2020). However, their large size\nand slow inference speed have hindered practi-\ncal deployments, such as deploying on resource-\nconstrained devices.\nT o solve the above problem, many compression\ntechniques for PLMs have been proposed, such as\n∗Corresponding author.\nquantization ( Shen et al. , 2020), weight pruning\n(Michel et al. , 2019), and knowledge distillation\n(KD) ( Sun et al. , 2019; Jiao et al. , 2020). Due to\nthe plug-and-play feasibility of KD, it is the most\ncommonly used method in practice, and we focus\non it in this work. The purpose of KD is to trans-\nfer knowledge from a larger teacher model to a\nsmaller student model (\nHinton et al. , 2015). Tra-\nditional KD methods only leverage single-domain\nknowledge, i.e., transferring the knowledge of the\nteacher model to the student model domain by do-\nmain. However, as stated in the purpose of trans-\nfer learning, the model performance on target do-\nmains can be improved by transferring the knowl-\nedge from different but related source domains ( Lu\net al. , 2015), thus the cross-domain knowledge also\nplays an important role. In addition, several recent\nworks have also proved the advantage of cross-\ndomain knowledge, and many multi-domain KD\nmethods have been proposed. For example,\nPeng\net al. (2020); Y ang et al. (2020) demonstrate the\neffectiveness of distilling knowledge from multiple\nteachers in different domains; Liu et al. (2019a,b)\nshow that jointly distilling the student models of\ndifferent domains can enhance the performance.\nNevertheless, these methods fail to capture the\nrelational information across different domains and\nmight have poor generalization ability . T o enhance\nthe transferability of the multi-domain KD frame-\nwork, some researchers have recently adopted the\nidea of meta-learning. Some studies have pointed\nout that meta-learning can improve the transfer-\nability of models between different domains ( Finn\net al. , 2017; Javed and White , 2019). For example,\nMeta-KD ( Pan et al. , 2020) introduces an instance-\nspeciﬁc domain-expertise weighting technique to\ndistill the knowledge from a meta-teacher trained\nacross multiple domains to the student model. How-\never, the Meta-KD framework trains student mod-\nels in different domains separately , which is incon-\nvenient in real-world applications and might not\n3127\nhave enough capability to capture multi-domain\ncorrelations.\nIn this paper, we aim to simultaneously capture\nthe relational information across different domains\nto make our framework more convenient and ef-\nfective. Speciﬁcally , we set up several domain-\nrelational graphs to adequately learn the relations\nof different domains and generate a set of domain-\nrelational ratios to re-weight each domain during\nthe KD process. Moreover, since different domains\nmight have different preferences of layer proto-\ntypes, motivated by the Riesz representation the-\norem (\nHartig, 1983), we ﬁrst construct a set of\nreference prototypes for each domain, which is cal-\nculated by a self-attention mechanism to integrate\nthe information of different domains. Then we\nintroduce a hierarchical compare-aggregate mech-\nanism to compare each layer prototype with the\ncorresponding reference prototype and make an\naggregation based on their similarities. The aggre-\ngated prototypes are ﬁnally sent to the correspond-\ning domain-relational graphs. Our framework is\nreferred to as hierarchical relational knowledge dis-\ntillation (HRKD).\nW e evaluate the HRKD framework on two multi-\ndomain NLP datasets, including the MNLI dataset\n(Williams et al. , 2018) and the Amazon Reviews\ndataset ( Blitzer et al. , 2007). Experiments show\nthat our HRKD method can achieve better perfor-\nmance compared with several multi-domain KD\nmethods. W e also evaluate our approach under the\nfew-shot learning setting, and it can still achieve\nbetter results than the competing baselines.\n2 Method\nIn this section, we detailedly describe the proposed\nHRKD framework. Our HRKD aims to simulta-\nneously capture the relational information across\ndifferent domains with both hierarchical and do-\nmain meta-knowledges. T o achieve this goal, we\nintroduce a hierarchical compare-aggregate mecha-\nnism to dynamically identify more representative\nprototypes for each domain, and construct a set of\ndomain-relational graphs to generate re-weighting\nKD ratios. The overview of HRKD is shown in\nFigure 1. W e ﬁrst introduce the basic multi-domain\nKD method in Section 2.1, which is a naive frame-\nwork lacking the ability of capturing cross-domain\nrelations. Then we describe the domain-relational\ngraph and compare-aggregate mechanism in Sec-\ntion 2.2 and 2.3, respectively , which are the primary\nmodules of our HRKD method to discover the rela-\ntional information.\n2.1 Multi-domain Knowledge Distillation\nSimilar to ( Jiao et al. , 2020), we jointly distill\nthe embeddings, attention matrices, transformer\nlayer outputs, and predicted logits between the\nteacher and student models. Inspired by ( Liu et al. ,\n2019c), we use a multi-task training strategy to per-\nform multi-domain KD. Speciﬁcally , we share the\nweights of the embedding and transformer layers\nfor all domains while assigning different prediction\nlayers to different domains. Innovatively , we opti-\nmize models in different domains simultaneously\nrather than sequentially .\nIn detail, the embedding loss Ld\nembd\nand predic-\ntion loss Ld\npred of d-th domain are formulated as:\nLd\nembd = MSE(ESWembd, ET\nd ), (1)\nLd\npred = CE(zS\nd /t, zT\nd /t ), (2)\nwhere MSE and CE represent the mean square loss\nand cross-entropy loss, respectively . ES and ET\nd\nrepresent the embeddings of student model and\nteacher model of\nd-th domain, respectively . zS\nd\nand zT\nd\nrepresent the predicted logits of student\nmodel and teacher model of d-th domain, respec-\ntively . Wembd is a learnable transformation matrix\nto align the student embedding dimension that mis-\nmatches with the teacher embedding dimension,\nand t is the temperature factor.\nThe attention loss Lm,d\nattn\nand transformer layer\noutput loss Lm,d\nhidn\nat m-th student layer and d-th\ndomain are formulated as:\nLm,d\nattn = 1\nh\nh∑\ni=1\nMSE(AS\ni,m, AT\ni,n,d), (3)\nLm,d\nhidn = MSE(HS\nmWhidn\nm , HT\nn,d), (4)\nwhere h is the number of attention heads, AS\ni,m\nand\nAT\ni,n,d\nare the i-th head of attention matrices at m-\nth student layer and its matching n-th teacher layer\nof d-th domain, respectively . HS\nm\nand HT\nn,d\nare\nthe transformer layer outputs at m-th student layer\nand n-th teacher layer of d-th domain, respectively .\nWhidn\nm is a transformation matrix to align the m-th\nlayer of student output dimension that mismatches\nwith the n-th layer of teacher output dimension. W e\nuse uniform strategy to match the layers between\nthe student and teacher models.\n3128\nMHA\nFFN\nEmbedding\nMHA\nFFN\nMHA\nFFN\nEmbedding\nClassifierClassifier\nTeacher model Student model\nKD\nRe -weighting\nRe -weighting\nSelf-attention\nComp-Agg\nComp-Agg\nComp-Agg\n0,1\nr\n0,2\nr\n0,3\nr\n1,1\nr\n1,2\nr\n1,3\nr\n… …\nDomain-relational \nGraph\nDomain-relational \nRatio\nPrototype\nDomain-relational \nGraph\nDomain-relational \nRatio\nPrototype\nReference Prototype\nHierarchical Compare-aggregate\nFigure 1: An overview of the proposed HRKD method. W e use knowledge distillation (KD) to transfer the knowl-\nedge from the teacher model to the student model. During KD, we set up several domain-relational graphs to gen-\nerate domain-relational ratios for re-weighting each domain. W e then introduce a hierarchical compare-aggregate\nmechanism. The prototypes of different layers are dynamically aggregated based on the similarity ratios compared\nwith the corresponding reference prototypes, which are then fed into the domain-relational graphs.\nFinally , the overall KD loss is formulated as:\nLtotal =\nD∑\nd=1\n(\nLd\nembd +\nM∑\nm=1\n(Lm,d\nattn + Lm,d\nhidn)\n+γLd\npred\n)\n,\n(5)\nwhere D is the total domain number, M is the\nnumber of transformer layers in the student model,\nγ is used to control the weight of the prediction\nloss Lpred.\n2.2 Prototype-based Domain-relational\nGraph\nAlthough the basic multi-domain KD method de-\nscribed in Section 2.1 can distill the student models\nacross different domains, the relational informa-\ntion between different domains is neglected, which\nis important for enhancing the model transferabil-\nity as pointed out by previous studies ( Finn et al. ,\n2017; Javed and White , 2019). T o solve the prob-\nlem, we attempt to leverage meta-learning to en-\nhance the performance and transferability of our\nstudent model. Inspired by the metric-based meth-\nods of meta-learning ( Snell et al. , 2017; Sung et al. ,\n2018), we use prototype representations rather than\nraw samples to reﬂect the characteristics of each\ndomain data. This helps to alleviate the negative im-\npact of abnormal samples when there are few train-\ning samples (e.g., overﬁtting) and make the meta-\nlearner easier to learn transferable cross-domain\nknowledge. Moreover, since we conduct KD over\nall of the student layers, we calculate different pro-\ntotypes for different student layers to explicitly dis-\ntinguish their characteristics. Speciﬁcally , the pro-\ntotype hm,d of m-th layer of the student model at\nd-th domain is calculated by:\nhm,d =\n{ 1\n|Dd|L\n∑ |Dd|\ni=1\n∑ L\nl=1 ES\ni,l, m = 0\n1\n|Dd|L\n∑ |Dd|\ni=1\n∑ L\nl=1 HS\nm,i,l, 1 ≤ m ≤ M\n(6)\nwhere Dd refers to the training set of d-th domain,\nL refers to the sentence length (i.e., number of\ntokens), ES\ni,l\nrepresents l-th token of i-th sampled\nstudent embedding in Dd, and HS\nm,i,l\nrepresents\nthe l-th token output by the i-th sampled student\ntransformer layer of the m-th student layer in Dd.\nIn practice, we calculate different prototypes for\n3129\ndifferent batches of training samples.\nAfterward, these domain prototypes are lever-\naged to probe the relations across different do-\nmains. Although many multi-domain text mining\nmethods have been proposed recently ( W ang et al. ,\n2020; Pan et al. , 2020), they capture the relations\nseparately for each given domain, which might\nbe inconvenient and time-consuming in practice.\nMeanwhile, the learning process is not effective\nenough since the other domains cannot learn from\neach other when optimizing a speciﬁc domain. T o\nsolve this problem, we aim to simultaneously dis-\ncover the cross-domain relations to make our frame-\nwork more convenient and effective. T o achieve the\ngoal, we propose to use the graph attention network\n(GA T) (V eliˇckovi´c et al. , 2018) to process the pro-\ntotypes of all domains at the same time. T o utilize\nGA T , each node in the graph represents a domain\nprototype, and each edge weight represents the\nsimilarity of the connected two prototypes. In this\nway , the relations across different domains can be\ncaptured simultaneously . In detail, we set up a two-\nlayer domain-relational graph for each layer of the\nstudent model (except for the prediction layer). The\ninput\nhm of the m-th graph is a set of node features\ncontaining all of the domain prototypes at m-th stu-\ndent layer, i.e., hm = {hm,1, ..., hm,D} ∈ R D×F ,\nwhere D is the total domain number, F is the chan-\nnel number of each prototype.\nIn the ﬁrst-layer domain-relational graph of the\nm-th student layer, a shared weight matrix Wm ∈\nRF ′×F is ﬁrst applied to each node followed by\na self-attention mechanism, where F ′ is the inter-\nmediate channel number. Then a multi-head con-\ncatenation mechanism with K heads is employed\nto stabilize the training process. Speciﬁcally , each\ninput prototype hm,d is ﬁrst transformed by the\nweight matrix Wm, then the attention coefﬁcient\nα i,j,mbetween two nodes i, j is calculated by ap-\nplying a weight vector am ∈ R 2F ′×1 to the con-\ncatenation of their transformed features followed\nby the LeakyReLU nonlinearity and softmax func-\ntion, which can be formulated as:\nsi,j,m= a⊤\nm [Wmhm,i ⊕ Wmhm,j] , (7)\nα i,j,m= softmax (LeakyReLU(si,j,m))\n= exp (LeakyReLU(si,j,m))\n∑\nk∈Ni exp (LeakyReLU(si,k,m)),\n(8)\nwhere ⊕ represents the concatenation operation and\nNi is all the ﬁrst-order neighbors of node i (includ-\ning node i). Then the ﬁnal output h′\nm,i ∈ R KF ′\nof\nnode i can be obtained by the weighted sum of the\ntransformed features of node i and its neighbors\nbased on their attention coefﬁcients followed by the\nELU nonlinearity and a multi-head concatenation\nmechanism:\nh′\nm,i = ⊕ K\nk=1ELU(\n∑\nj∈Ni\nα k\ni,j,mWk\nmhm,j), (9)\nwhere k represents the head index.\nIn the second-layer domain-relational graph\nof the m-th student layer, targeting at obtaining\ndomain-relational ratios, we reformulate the pa-\nrameters\nWm, am used in the ﬁrst-layer graph as\nW′\nm ∈ R 1×KF ′\n, a′\nm ∈ R 2×1\nrespectively and do\nnot apply the multi-head mechanism. W e use the\nsoftmax operation to normalize the output and ﬁ-\nnally derive the domain-relational ratios rm ∈ R D,\nformulated as below:\nrm = softmax(ELU(\n∑\nj∈Ni\nα ′\ni,j,mW′\nmh′\nm,j)),\n(10)\nwhere α ′\ni,j,mis calculated by:\ns′\ni,j,m= a′⊤\nm\n[\nW′\nmh′\nm,i ⊕ W′\nmh′\nm,j\n]\n, (11)\nα ′\ni,j,m= softmax\n(\nLeakyReLU(s′\ni,j,m)\n)\n. (12)\n2.3 Hierarchical Compare-aggregate\nMechanism\nAs different domains might have different prefer-\nences towards different layer prototypes, we pro-\npose a hierarchical compare-aggregate mechanism\nto dynamically select the most representative pro-\ntotype for each domain. Our compare-aggregate\nmechanism is motivated by the Riesz representa-\ntion theorem (\nHartig, 1983), which indicates that\nan element can be evaluated by comparing it with\na speciﬁc reference element and the quality of the\nelement is the same as that of the selected reference\nelement. Based on this, we establish a set of refer-\nence prototypes for each domain and hierarchically\naggregate the current and previous layer prototypes\nbased on their similarities with the corresponding\nreference prototypes.\nReference prototype.\nFor each student layer, a\nsimple way is to use the original domain prototypes\nof current layer as the reference prototypes for the\ncurrent and previous layer prototypes. However,\nthe information of other domains is not integrated,\n3130\nwhich plays an important role to enhance the model\ntransferability across different domains. T o handle\nthis, we introduce a self-attention mechanism over\nall of the domain prototypes in the same layer to\ninject the information of different domains. Specif-\nically , the reference prototype RPm ∈ R D×F of\nm-th student layer is calculated by:\nRPm = α D\nm ·hm, (13)\nα D\nm = softmax(hm ·WD\nm ·h⊤\nm), (14)\nwhere α D\nm ∈ R D×D\nrefers to the attention matrix\nof m-th layer, hm ∈ R D×F refers to the prototypes\nof all domains at m-th layer, WD\nm ∈ R F ×F refers\nto a learnable parameter matrix at m-th layer, and\nthe softmax operation is performed over the last\nvector dimension.\nCompare-aggregate mechanism. After obtain-\ning the reference prototypes, we propose a\ncompare-aggregate mechanism to hierarchically\naggregate the layer prototypes by comparing them\nwith the corresponding reference prototypes, which\nmakes the model be aware to more representative\nlayer prototypes for each domain. In detail, the\naggregated prototype APm,d ∈ R F of m-th layer\nand d-th domain is formulated as:\nAPm,d = α H\nm,d ·h≤m,d, (15)\nα H\nm,d = softmax(h≤m,d ·WH\nm,d ·RPm,d), (16)\nwhere α H\nm,d ∈ R m+1\nrepresents the similarity\nratios of m-th layer and d-th domain, h≤m,d ∈\nR(m+1)×F represents the prototypes of m-th layer\nand its previous layers at d-th domain, WH\nm,d ∈\nRF ×F is a learnable parameter matrix of m-th\nlayer and d-th domain, and RPm,d ∈ R F is the\nreference prototype of m-th layer and d-th domain.\nThen the aggregated prototype AP is sent to the\ndomain-relational graphs to obtain the domain-\nrelational ratios\nr ∈ R (M+1)×D, as formulated\nby Equation ( 7)-(11).\nFinally , the overall loss of our HRKD can be\nrepresented as:\nLtotal =\nD∑\nd=1\n(\nr0,dLd\nembd +\nM∑\nm=1\nrm,d(Lm,d\nattn + Lm,d\nhidn)\n+ γ\nD Ld\npred\n)\n,\n(17)\nwhere rm,d is the domain-relational ratio at m-th\nstudent layer and d-th domain.\nT able 1: Statistics of the MNLI and Reviews datasets.\nDataset Domain #T rain #Dev #T est\nMNLI\nFiction 69,613 7,735 1,973\nGovernment 69,615 7,735 1,945\nSlate 69,575 7,731 1,955\nT elephone 75,013 8,335 1,966\nTravel 69,615 7,735 1,976\nAmazon\nReviews\nBooks 1,631 170 199\nDVD 1,621 194 185\nElectronics 1,615 172 213\nKitchen 1,613 184 203\n3 Experiment\nIn this section, we conduct extensive experiments\non two multi-domain datasets, namely MNLI and\nAmazon Reviews, to demonstrate the effectiveness\nof our HRKD method.\n3.1 Datasets and Model Settings\nW e evaluate our method on two multi-domain\ndatasets, including the multi-genre natural lan-\nguage inference (MNLI) dataset (\nWilliams et al. ,\n2018) and the Amazon Reviews dataset ( Blitzer\net al. , 2007). In detail, MNLI is a natural language\ninference dataset with ﬁve domains for the task\nof entailment relation prediction between two sen-\ntences. In our setting, we randomly sample 10%\nof the original training data as our development set\nand use the original development set as our test set.\nAmazon Reviews is a sentiment analysis dataset\nwith four domains for predicting whether the re-\nviews are positive or negative. Following Pan et al.\n(2020), we randomly split the original data into\ntrain, development, and test sets. The statistics of\nthese two datasets are listed in T able 1.\nW e use BER TB (the number of layers N=12, the\nhidden size d′=768, the FFN intermediate hidden\nsize d′\ni\n=3072, the number of attention heads h=12,\nthe number of parameters #params=109M) as the\narchitecture of our teacher model, and BER TS\n(M=4, d′=312, d′\ni\n=1200, h=12, #params=14.5M)\nas our student model. Our teacher model\nHRKD-teacher is trained in a multi-domain man-\nner as described in Section 2.1, and our student\nmodel BER T S is initialized with the general distil-\nlation weights of TinyBER T 1.\n1 W e use the 2nd version from https://github.com/\nhuawei-noah/Pretrained-Language-Model/\ntree/master/TinyBERT\n3131\nT able 2: Results on MNLI in terms of accuracy (%) with standard deviations. X\nA\n− → Y denotes using teacher X\nto distill student Y with KD method of A. The bold and underlined numbers indicate the best and the second-best\nperformance, respectively .\nMethod Fiction Government Slate T elephone T ravel A verage\nBER TB-single 82.2 84.2 76.7 82.4 84.2 81.9\nBER TB-mix 84.8 87.2 80.5 83.8 85.5 84.4\nBER TB-mtl 83.7 87.1 80.6 83.9 85.8 84.2\nMeta-teacher 85.1 86.5 81.0 83.9 85.5 84.4\nHRKD-teacher 83.8 87.6 80.4 83.5 85.4 84.2\nBER TB-single\nTinyBER T -KD\n− − − − − − − →BER TS 78.8 83.2 73.6 78.8 81.9 79.3\nBER TB-mix\nTinyBER T -KD\n− − − − − − − →BER TS 79.6 83.3 74.8 79.0 81.5 79.6\nBER TB-mtl\nTinyBER T -KD\n− − − − − − − →BER TS 79.7 83.1 74.2 79.3 82.0 79.7\nMeta-teacher\nMeta-distillation\n− − − − − − − → BER TS 80.5 83.7 75.0 80.5 82.1 80.4\nHRKD-teacher\nTinyBER T -KD\n− − − − − − − →BER TS 80.1± 0.22 84.2± 0.20 75.7± 0.27 80.0 ± 0.23 81.9 ± 0.17 80.4\nHRKD-teacher\nHRKD\n− − − → BER TS 80.4± 0.33 84.3± 0.30 76.1± 0.32 81.4± 0.29 82.2± 0.26 80.9\nT able 3: Results on Amazon Reviews in terms of accuracy (%) with standard deviations.\nMethod Books DVD Electronics Kitchen A verage\nBER TB-single 87.9 83.8 89.2 90.6 87.9\nBER TB-mix 89.9 85.9 90.1 92.1 89.5\nBER TB-mtl 90.5 86.5 91.1 91.1 89.8\nMeta-teacher 92.5 87.0 91.1 89.2 89.9\nHRKD-teacher 88.4 89.2 92.5 91.1 90.3\nBER TB-single\nTinyBER T -KD\n− − − − − − − →BER TS 83.4 83.2 89.2 91.1 86.7\nBER TB-mix\nTinyBER T -KD\n− − − − − − − →BER TS 88.4 81.6 89.7 89.7 87.3\nBER TB-mtl\nTinyBER T -KD\n− − − − − − − →BER TS 90.5 81.6 88.7 90.1 87.7\nMeta-teacher\nMeta-distillation\n− − − − − − − → BER TS 91.5 86.5 90.1 89.7 89.4\nHRKD-teacher\nTinyBER T -KD\n− − − − − − − →BER TS 84.6± 0.93 87.8± 0.55 91.3± 0.23 88.1 ± 2.98 87.9\nHRKD-teacher\nHRKD\n− − − → BER TS 87.4± 0.90 90.5± 1.76 91.8± 1.25 92.2± 0.48 90.5\n3.2 Baselines\nW e mainly compare our KD method with several\nKD baseline methods distilled from four teacher\nmodels, including\nBER TB-single, BER TB-mix,\nBER TB-mtl, and Meta-teacher in Meta-KD ( Pan\net al. , 2020). Speciﬁcally , BER TB-single trains\nthe teacher model of each domain separately with\nthe single-domain dataset; BER TB-mix trains a\nsingle teacher model with the combined dataset\nof all domains;\nBER TB-mtl adopts the multi-task\ntraining method proposed by Liu et al. (2019c) to\ntrain the teacher model; Meta-teacher trains the\nteacher model with several meta-learning strategies\nincluding prototype-based instance weighting and\ndomain corruption.\n3.3 Implementation Details\nFor the teacher model, we train the HRKD-teacher\nfor three epochs with a learning rate of 5e-5. For\nthe student model, we train it for ten epochs with a\nlearning rate of 1e-3 and 5e-4 on MNLI and Ama-\nzon Reviews, respectively . γ is set to 1, and t is 1.\nFor few-shot learning, the learning rate for the stu-\ndent model is 5e-5, while other hyper-parameters\nare kept the same. The few-shot training data is\nselected from the front of our original training set\nwith different sample ratios, while the dev and test\ndata are the same as our original dev and test sets\nwithout sampling to make a fair comparison. In\nall the experiments, the sequence length is set to\n128, and the batch size is 32. The hyper-parameters\nare tuned on the development set, and the results\nare averaged over ﬁve runs. Our experiments are\nconducted on 4 GeForce R TX 3090 GPUs.\n3.4 General Experimental Results\nThe experimental results of our method are shown\nin T able 2 and 3. On the MNLI dataset, our\n3132\n0.02\n 0.05\n 0.1\n 0.2\n 1.0\nSample rate\n0\n5\n10\nImprovement rate (%)\n5.2\n2.4\n2.5\n0.9\n1.1\n10.1\n3.9\n2.3\n1.1\n1.6\nMeta-KD\nHRKD\nFigure 2: Comparison results of few-shot learning be-\ntween Meta-KD and HRKD.\nteacher model HRKD-teacher has similar perfor-\nmances with other baseline teacher models, but\nthe performance of the student model distilled with\nthe HRKD method (\nHRKD-teacher\nHRKD\n− − − → BER TS)\nis signiﬁcantly better than the base TinyBERT -\nKD method (\nHRKD-teacher\nTinyBER T -KD\n− − − − − − − →BER TS)\nas well as its counterpart Meta-KD ( Meta-teacher\nMeta-distillation\n− − − − − − − → BER TS), which demonstrate the su-\nperior performance of our method. Speciﬁcally ,\nwith the HRKD method, the average score of the\nstudent model is both 0.5% higher than that of the\nmodel with the base TinyBERT -KD method and\nits counterpart Meta-KD method (see T able\n2). It\ncan also be observed that the improvement of our\nHRKD method on the T elephone domain is the\nmost signiﬁcant, which is probably caused by the\namount of training data. From T able 1, we can see\nthat the T elephone domain has much more train-\ning data than other domains, indicating that the\nT elephone domain can derive more relationship in-\nformation from other domains and lead to higher\nimprovement. Meanwhile, as shown in the results\non the Amazon Reviews dataset in T able 3, the per-\nformance of the HRKD-teacher model is slightly\nbetter than that of other teacher models, but the stu-\ndent model distilled by the HRKD method largely\noutperforms the models distilled by the TinyBER T -\nKD and Meta-KD methods with average gains of\n2.6% and 1.1% respectively , which prove the ex-\ncellent performance of our method again. Note\nthat our HRKD method signiﬁcantly outperforms\nthe base TinyBERT -KD method on both MNLI\nand Amazon Reviews datasets (t-test with p < 0.1).\nAnd since the performances of the Meta-teacher\nand our HRKD-teacher are similar on both datasets,\nthe impact of the teacher is negligible, making the\ncomparison between our HRKD and its counterpart\nMeta-KD relatively fair.\nT able 4: Ablation studies on Amazon Reviews dataset.\nThe accuracy values (%) with standard deviations are\nreported.\nKD Method Books DVD Elec. Kitchen A verage\nHRKD 87.4 90.5 91.8 92.2 90.5\n- Self-attention 87.3 89.9 92.5 91.6 90.3\n- Comp-Agg 86.3 90.6 90.9 91.8 89.9\n- Hierarchical Rel. 85.9 89.6 91.1 91.6 89.5\n- Domain Rel. 84.6 87.8 91.3 88.1 87.9\n3.5 Few-shot Learning Results\nAs a large amount of training data is hard to col-\nlect in reality , the few-shot learning ability of our\nmethod is worth being evaluated, where both the\nteacher and student models are trained with few\ntraining data in each domain. W e randomly sam-\nple a part of the training data in the MNLI dataset\nto make an evaluation, where the chosen sample\nrates are 2%, 5%, 10%, and 20%. W e mainly com-\npare the performance improvements between two\nmethods: distilling from BER TB-single to BER TS\nwith TinyBERT -KD ( BER TB-single\nTinyBER T -KD\n− − − − − − − →\nBER TS) and our HRKD method ( HRKD-teacher\nHRKD\n− − − → BER TS). From the results in Figure 2, we\ncan observe that the improvement gets more promi-\nnent when the training data gets fewer, and the\naverage improvement rate is the largest of 10.1%\nwhen there is only 2% MNLI training data. In addi-\ntion, we can see that the improvement rates of our\nmethod are higher than those of Meta-KD under\nmost of the sample rates, especially when there are\nonly 2% training data. These results demonstrate\nthe strong learning ability of our HRKD method\nunder the few-shot setting.\n3.6 Ablation Studies\nIn this section, we progressively remove each mod-\nule of our KD method to evaluate the effect of each\nmodule.\nThe results are shown in T able 4. W e ﬁrst re-\nmove the self-attention mechanism across different\ndomain prototypes (- Self-attention), and the av-\nerage score on Amazon Reviews drops by 0.2%,\nwhich proves its effectiveness. Next, we replace the\nhierarchical compare-aggregate mechanism with a\nsimple average operation (- Comp-Agg), and the\naverage score drops by 0.4%, which demonstrates\nthe effectiveness of the compare-aggregate mech-\nanism. Then we remove the hierarchical graph\nstructure (- Hierarchical Rel.), where the input of\neach domain-relational graph comes from a single\n3133\nT able 5: Case study on Amazon Reviews across four domains with three positive samples and one negative sample.\nPositive samples are colored in gray .\nDomain Label Review T ext\nBooks POS ...leading, or molding young people today would beneﬁt from reading this book...\nDVD POS ...The plot wasn’t horrible, it was actually pretty good for a fright ﬂick...\nElectronics NEG ...I returned the camera and bought a Panasonic and never looked back!\nKitchen POS This is great for making poached eggs on toast. My family has enjoyed using it...\nDomain-relational Ratio Hierarchical Similarity Ratio\n[0.24, 0.29, 0.25, 0.23, 0.24] [0.42, 0.58], [0.38, 0.30, 0.32], [0.24, 0.27, 0.25, 0.24], [0.16, 0.19, 0.21, 0.20, 0.24]\n[0.24, 0.29, 0.25, 0.23, 0.24] [0.52, 0.48], [0.32, 0.38, 0.30], [0.27, 0.24, 0.27, 0.22], [0.16, 0.17, 0.19, 0.24, 0.24]\n[0.27, 0.17, 0.25, 0.25, 0.26] [0.62, 0.38], [0.22, 0.21, 0.58], [0.19, 0.22, 0.30, 0.29], [0.14, 0.15, 0.24, 0.21, 0.26]\n[0.24, 0.25, 0.25, 0.28, 0.27] [0.46, 0.54], [0.30, 0.43, 0.28], [0.30, 0.25, 0.26, 0.20], [0.29, 0.19, 0.15, 0.12, 0.26]\nT able 6: Case study on Amazon Reviews across four domains with two positive samples and two negative samples.\nPositive samples are colored in gray .\nDomain Label Review T ext\nBooks NEG ...In this book, his \"hard-evidence\" is ﬂimsey and suspicious...\nDVD POS ...If you have a child who loves John Deere, then this is a perfect DVD for them.\nElectronics POS ...Movies are amazing! My music collection never sounded so good...\nKitchen NEG This is the worst blender I’ve ever used...It’s also loud and it moves a lot...\nDomain-relational Ratio Hierarchical Similarity Ratio\n[0.26, 0.25, 0.25, 0.24, 0.27] [0.43, 0.57], [0.40, 0.31, 0.29], [0.25, 0.26, 0.25, 0.23], [0.13, 0.17, 0.20, 0.21, 0.30]\n[0.24, 0.25, 0.25, 0.26, 0.22] [0.52, 0.48], [0.35, 0.32, 0.33], [0.22, 0.25, 0.27, 0.25], [0.18, 0.22, 0.18, 0.20, 0.22]\n[0.24, 0.25, 0.25, 0.26, 0.25] [0.54, 0.46], [0.31, 0.33, 0.36], [0.22, 0.24, 0.29, 0.25], [0.21, 0.20, 0.18, 0.18, 0.23]\n[0.26, 0.25, 0.25, 0.24, 0.27] [0.49, 0.51], [0.36, 0.33, 0.31], [0.26, 0.25, 0.26, 0.23], [0.22, 0.20, 0.19, 0.18, 0.21]\nstudent layer. As can be seen, the average score\ndrops by 0.4%, which proves the importance of the\nhierarchical relationship. Finally , we remove the\ndomain-relational graph in each layer (- Domain\nRel.), and the performance signiﬁcantly drops by\n1.6%, which strongly demonstrates the advantage\nof the domain relationship.\n3.7 Case Studies\nW e further provide some case studies to intuitively\nexplain the effectiveness of the domain-relational\nratios and hierarchical similarity ratios calculated\nby our HRKD method (see T able 5 and 6).\nIn T able 5 and 6, we use the label to denote the\ncategories of sampled domain examples, and we\nassume that if the learned domain-relational ratios\nand hierarchical similarity ratios are similar for do-\nmain examples with same category while different\nfor those with different categories, then the model\nhas relatively correctly captured the cross-domain\nand hierarchical relational information. W e select\ntwo typical types of cases from Amazon Reviews\nacross four domains, in which we adjust the num-\nber of domains in each category under two settings:\n(i) three same categories (i.e., POS) with another\none category (i.e., NEG) as in T able 5, and (ii) two\nsame categories (i.e., POS) with another two same\ncategories (i.e., NEG) as in T able 6.\nW e ﬁnd the results are intuitive, as we observe\nthat the review texts with the same labels have\nsimilar domain-relational ratios and hierarchical\nsimilarity ratios, while different layers indeed have\ndifferent domain weighting preferences and differ-\nent preferences of layer prototypes for graph input.\nFor example, in T able 5 and 6, positive samples\ntend to have higher domain-relational ratios in the\nmiddle layers (i.e., 2-4), while negative samples\nhave higher ratios in the marginal layers (i.e., 1,\n5). Meanwhile, in the second and third layers of\nT able\n5 as well as the ﬁrst layer of T able 6, lower\npositive layer prototypes tend to have higher sim-\nilarity ratios, and the higher positive layer proto-\ntypes in the third layer of T able 6 also tend to have\nhigher similarity ratios; while those of the nega-\ntive layer prototypes are just the opposite. The\nresults show that HRKD method has distinctively\nand correctly captured the hierarchical and domain\nmeta-knowledges, leading to better performance.\n3134\n4 Related W ork\nPre-trained Language Model (PLM) Compres-\nsion.\nDue to the large size and slow inference\nspeed, PLMs are hard to be deployed on edge de-\nvices for practical usage. T o solve this problem,\nmany PLM compression methods have been pro-\nposed, including quantization (\nShen et al. , 2020),\nweight pruning ( Michel et al. , 2019), and knowl-\nedge distillation (KD) ( Sun et al. , 2019; Jiao et al. ,\n2020). Among them, KD ( Hinton et al. , 2015)\nhas been widely adopted due to its plug-and-play\nfeasibility , aiming to distill the knowledge from a\nlarger teacher model to a smaller student model\nwithout decreasing too much performance. For\nexample, BERT -PKD ( Sun et al. , 2019) distills\nboth intermediate and output layers on ﬁne-tuning.\nTinyBERT ( Jiao et al. , 2020) additionally distills\nthe embedding layer and attention matrices during\npre-training and ﬁne-tuning. Meta-KD ( Pan et al. ,\n2020) proposes to distill knowledge from a cross-\ndomain meta-teacher through an instance-speciﬁc\ndomain-expertise weighting technique.\nIn this paper, we propose a novel cross-domain\nKD framework that captures the relational informa-\ntion across different domains with both domain and\nhierarchical meta-knowledges, which has a better\ncapability for capturing multi-domain correlations.\nT ransfer Learning and Meta-learning. Trans-\nfer learning focuses on transferring the knowledge\nfrom source domains to boost the model perfor-\nmance on the target domain. Among the meth-\nods in transfer learning, the shared-private archi-\ntecture ( Liu et al. , 2017, 2019c) is most commonly\napplied in NLP tasks, which consists of a shared\nnetwork to store domain-invariant knowledge and\na private network to capture domain-speciﬁc in-\nformation. There are also many works applying\nadversarial training strategies (\nShen et al. , 2018;\nLi et al. , 2019; Zhou et al. , 2019), which intro-\nduce domain adversarial classiﬁers to learn the\ndomain-invariant features. Besides, the research of\nmulti-domain learning has gained more and more\nattention recently , which is a particular case of\ntransfer learning targeting transferring knowledge\nacross different domains to comprehensively en-\nhance the model performance ( Cai and W an , 2019;\nW ang et al. , 2020). Unlike transfer learning, the\ngoal of meta-learning is to train a meta-learner that\ncan easily adapt to a new task with a few training\ndata and iterations ( Finn et al. , 2017). Traditional\nmeta-learning typically contains three categories\nof methods: metric-based ( Snell et al. , 2017; Sung\net al. , 2018), model-based ( Santoro et al. , 2016;\nMunkhdalai and Y u , 2017), and optimization-based\n(Ravi and Larochelle , 2017; Finn et al. , 2017). In\naddition, the meta-learning technique can beneﬁt\nthe multi-domain learning task by learning the re-\nlationship information among different domains\n(Franceschi et al. , 2017).\nIn this paper, we leverage meta-learning to solve\nthe multi-domain learning task, where we consider\ncross-domain KD to simultaneously capture the\ncorrelation between different domains, aiming to\ntrain a better student meta-learner.\n5 Conclusion\nIn this paper, we present a hierarchical relational\nknowledge distillation (HRKD) framework to si-\nmultaneously capture the cross-domain relational\ninformation. W e build several domain-relational\ngraphs to capture domain meta-knowledge and in-\ntroduce a hierarchical compare-aggregate mecha-\nnism to capture hierarchical meta-knowledge. The\nlearnt domain-relational ratios are leveraged to\nmeasure domain importance during the KD pro-\ncess. Extensive experiments on public datasets\ndemonstrate the superior performance and solid\nfew-shot learning ability of our HRKD method.\nAcknowledgements\nThis work was ﬁnancially supported by the\nNational Natural Science Foundation of China\n(No.61602013).\nReferences\nSiqi Bao, Huang He, Fan W ang, Hua Wu, and Haifeng\nW ang. 2020. PLA TO: Pre-trained dialogue genera-\ntion model with discrete latent variable. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 85–96.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiﬁcation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation of Computational Linguistics , pages 440–\n447.\nYitao Cai and Xiaojun W an. 2019. Multi-domain sen-\ntiment classiﬁcation based on domain-aware embed-\nding and attention. In Proceedings of the T wenty-\nEighth International Joint Conference on Artiﬁcial\nIntelligence, pages 4904–4910.\n3135\nAlexis CONNEAU and Guillaume Lample. 2019.\nCross-lingual language model pretraining. In Ad-\nvances in Neural Information Processing Systems .\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language T ech-\nnologies, pages 4171–4186.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning , pages\n1126–1135.\nLuca Franceschi, Michele Donini, Paolo Frasconi, and\nMassimiliano Pontil. 2017. Forward and reverse\ngradient-based hyperparameter optimization. In\nProceedings of the 34th International Conference on\nMachine Learning , pages 1165–1173.\nDonald G. Hartig. 1983. The riesz representation theo-\nrem revisited. The American Mathematical Monthly ,\npages 277–280.\nGeoffrey Hinton, Oriol V inyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn NIPS Deep Learning and Representation Learn-\ning W orkshop.\nKhurram Javed and Martha White. 2019. Meta-\nlearning representations for continual learning. In\nAdvances in Neural Information Processing Sys-\ntems.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang W ang, and Qun Liu.\n2020. TinyBER T: Distilling BER T for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n4163–4174.\nZheng Li, Xin Li, Ying W ei, Lidong Bing, Y u Zhang,\nand Qiang Y ang. 2019. Transferable end-to-end\naspect-based sentiment analysis with selective adver-\nsarial learning. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing , pages 4590–\n4600.\nLinqing Liu, Huan W ang, Jimmy Lin, Richard Socher,\nand Caiming Xiong. 2019a. Mkd: a multi-task\nknowledge distillation approach for pretrained lan-\nguage models. arXiv preprint arXiv:1911.03588 .\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.\nAdversarial multi-task learning for text classiﬁca-\ntion. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1–10.\nXiaodong Liu, Pengcheng He, W eizhu Chen, and\nJianfeng Gao. 2019b. Improving multi-task deep\nneural networks via knowledge distillation for\nnatural language understanding. arXiv preprint\narXiv:1904.09482.\nXiaodong Liu, Pengcheng He, W eizhu Chen, and Jian-\nfeng Gao. 2019c. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4487–4496.\nY ang Liu and Mirella Lapata. 2019. T ext summariza-\ntion with pretrained encoders. In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint\nConference on Natural Language Processing , pages\n3730–3740.\nJie Lu, V ahid Behbood, Peng Hao, Hua Zuo, Shan\nXue, and Guangquan Zhang. 2015. Transfer learn-\ning using computational intelligence: A survey .\nKnowledge-Based Systems , pages 14–23.\nPaul Michel, Omer Levy , and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems ,\npages 14014–14024.\nTsendsuren Munkhdalai and Hong Y u. 2017. Meta\nnetworks. In Proceedings of the 34th International\nConference on Machine Learning , pages 2554–\n2563.\nHaojie Pan, Chengyu W ang, Minghui Qiu, Yichang\nZhang, Y aliang Li, and Jun Huang. 2020. Meta-\nkd: A meta knowledge distillation framework for\nlanguage model compression across domains. arXiv\npreprint arXiv:2012.01266 .\nShuke Peng, Feng Ji, Zehao Lin, Shaobo Cui, Haiqing\nChen, and Yin Zhang. 2020. Mtss: Learn from mul-\ntiple domain teachers and become a multi-domain\ndialogue expert. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence , pages 8608–8615.\nSachin Ravi and Hugo Larochelle. 2017. Optimization\nas a model for few-shot learning. In International\nConference on Learning Representations .\nAdam Santoro, Sergey Bartunov , Matthew Botvinick,\nDaan Wierstra, and Timothy Lillicrap. 2016. Meta-\nlearning with memory-augmented neural networks.\nIn Proceedings of The 33rd International Confer-\nence on Machine Learning , pages 1842–1850.\nJian Shen, Y anru Qu, W einan Zhang, and Y ong Y u.\n2018. W asserstein distance guided representation\nlearning for domain adaptation. In Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial In-\ntelligence, pages 4058–4065.\nSheng Shen, Zhen Dong, Jiayu Y e, Linjian Ma, Zhewei\nY ao, Amir Gholami, Michael W . Mahoney , and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low pre-\ncision quantization of bert. Proceedings of the AAAI\n3136\nConference on Artiﬁcial Intelligence , pages 8815–\n8821.\nJake Snell, Kevin Swersky , and Richard Zemel. 2017.\nPrototypical networks for few-shot learning. In Ad-\nvances in Neural Information Processing Systems .\nSiqi Sun, Y u Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BER T model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing , pages 4323–4332.\nFlood Sung, Y ongxin Y ang, Li Zhang, T ao Xiang,\nPhilip H.S. T orr, and Timothy M. Hospedales. 2018.\nLearning to compare: Relation network for few-shot\nlearning. In IEEE/CVF Conference on Computer V i-\nsion and P attern Recognition , pages 1199–1208.\nPetar V eliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Y oshua Bengio.\n2018. Graph attention networks. In International\nConference on Learning Representations .\nChengyu W ang, Minghui Qiu, Jun Huang, and Xi-\naofeng He. 2020. Meta ﬁne-tuning neural language\nmodels for multi-domain text mining. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n3094–3104.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language T echnologies , pages\n1112–1122.\nZe Y ang, Linjun Shou, Ming Gong, Wutao Lin, and\nDaxin Jiang. 2020. Model compression with two-\nstage multi-teacher knowledge distillation for web\nquestion answering system. In Proceedings of the\n13th International Conference on W eb Search and\nData Mining , page 690–698.\nXingxing Zhang, Furu W ei, and Ming Zhou. 2019. HI-\nBER T: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 5059–5069.\nYinhe Zheng, Rongsheng Zhang, Minlie Huang, and\nXiaoxi Mao. 2020. A pre-training based personal-\nized dialogue generation model with persona-sparse\ndata. Proceedings of the AAAI Conference on Artiﬁ-\ncial Intelligence , pages 9693–9700.\nJoey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu,\nMeng Fang, Rick Siow Mong Goh, and Kenneth\nKwok. 2019. Dual adversarial neural transfer for\nlow-resource named entity recognition. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3461–3471.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, T ao Qin,\nW engang Zhou, Houqiang Li, and Tieyan Liu. 2020.\nIncorporating bert into neural machine translation.\nIn International Conference on Learning Represen-\ntations."
}