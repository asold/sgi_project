{
  "title": "Explainable differential diagnosis with dual-inference large language models",
  "url": "https://openalex.org/W4409767213",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2130719219",
      "name": "Zhou Shuang",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A4222700404",
      "name": "Lin, Mingquan",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A3169485310",
      "name": "Ding, Sirui",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A4297435459",
      "name": "Wang Jiashuo",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2795475843",
      "name": "Chen, Canyu",
      "affiliations": [
        "Illinois Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4302413338",
      "name": "Melton Genevieve B.",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A4221438609",
      "name": "Zou, James",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A1956316200",
      "name": "Zhang Rui",
      "affiliations": [
        "University of Minnesota"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4200109803",
    "https://openalex.org/W4306313008",
    "https://openalex.org/W4367057203",
    "https://openalex.org/W4385783820",
    "https://openalex.org/W2945807221",
    "https://openalex.org/W4391221150",
    "https://openalex.org/W4390943000",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4388759569",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W4393160078",
    "https://openalex.org/W4400872202",
    "https://openalex.org/W4403746865",
    "https://openalex.org/W4391170193",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4397003497",
    "https://openalex.org/W4404188625",
    "https://openalex.org/W4402670989",
    "https://openalex.org/W4378509375",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4403229099",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4396605652",
    "https://openalex.org/W4403813762",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4402670894",
    "https://openalex.org/W4402672090",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4393335480",
    "https://openalex.org/W4406152279"
  ],
  "abstract": "Abstract Automatic differential diagnosis (DDx) involves identifying potential conditions that could explain a patient’s symptoms and its accurate interpretation is of substantial significance. While large language models (LLMs) have demonstrated remarkable diagnostic accuracy, their capability to generate high-quality DDx explanations remains underexplored, largely due to the absence of specialized evaluation datasets and the inherent challenges of complex reasoning in LLMs. Therefore, building a tailored dataset and developing novel methods to elicit LLMs for generating precise DDx explanations are worth exploring. We developed the first publicly available DDx dataset, comprising expert-derived explanations for 570 clinical notes, to evaluate DDx explanations. Meanwhile, we proposed a novel framework, Dual-Inf, that could effectively harness LLMs to generate high-quality DDx explanations. To the best of our knowledge, it is the first study to tailor LLMs for DDx explanation and comprehensively evaluate their explainability. Overall, our study bridges a critical gap in DDx explanation, enhancing clinical decision-making.",
  "full_text": "npj |health systems Article\nhttps://doi.org/10.1038/s44401-025-00015-6\nExplainable differential diagnosis with\ndual-inference large language models\nCheck for updates\nShuang Zhou1,M i n g q u a nL i n1, Sirui Ding2, Jiashuo Wang3, Canyu Chen4,G e n e v i e v eB .M e l t o n5,\nJames Zou6 & Rui Zhang1\nAutomatic differential diagnosis (DDx) involves identifying potential conditions that could explain a\npatient’s symptoms and its accurate interpretation is of substantial signiﬁcance. While large language\nmodels (LLMs) have demonstrated remarkable diagnostic accuracy, their capability to generate high-\nquality DDx explanations remains underexplored, largely due to the absence of specialized evaluation\ndatasets and the inherent challenges of complex reasoning in LLMs. Therefore, building a tailored\ndataset and developing novel methods to elicit LLMs for generating precise DDx explanations are\nworth exploring. We developed theﬁrst publicly available DDx dataset, comprising expert-derived\nexplanations for 570 clinical notes, to evaluate DDx explanations. Meanwhile, we proposed a novel\nframework, Dual-Inf, that could effectively harness LLMs to generate high-quality DDx explanations.\nTo the best of our knowledge, it is theﬁrst study to tailor LLMs for DDx explanation and\ncomprehensively evaluate their explainability. Overall, our study bridges a critical gap in DDx\nexplanation, enhancing clinical decision-making.\nDifferential diagnosis (DDx), a critical component of clinical care, involves\ngenerating a list of potential conditions that could explain a patient’s\nsymptoms1. It facilitates comprehensive case evaluation, identiﬁes critical\nbut subtle conditions, guides diagnostic testing, and optimizes resource\nutilization. Additionally, DDx fosters patient involvement and trust through\nimproved communication. While numerous automatic DDx systems\n2,3\nhave been developed to support decision-making, their black-box nature,\nparticularly in deep learning models, often undermines trust4. To address\nthis, providing interpretative insights alongside diagnostic predictions is\nessential5. Explainable DDx, which takes patient symptom descriptions as\ninput, generates differential diagnoses, and offers accompanying explana-\ntions, is thus highly desirable in clinical practice.\nIn recent years, large language mo d e l s( L L M s ) ,s u c ha sC h a t G P T ,\ntrained on extensive corpora, have exhibited remarkable capabilities in\nvarious clinical scenarios, including medical question answering (QA)\n6– 9,\nclinical text summarization10, and disease diagnosis11– 16. Motivated by these\nadvancements, some studies have explored LLMs to improve diagnostic\naccuracy17. For instance, Daniel et al.18 ﬁne-tuned PaLM 2 on medical data\nand developed an interactive interface to assist clinicians with DDx gen-\neration, while Savage et al.19 reﬁned Chain-of-Thought (CoT) prompting20\nto harness LLMs’reasoning capabilities.\nDespite these efforts, the potential of LLMs to generate reliable DDx\nexplanations remains largely unexplored, leaving their role in supporting\nclinical decision-making uncertain. Two key challenges impede progress in\nthis domain. First, the absence of DDxdatasets annotated with diagnostic\nexplanations limits model development and evaluation 21,22. Second,\nnumerous studies have highlighted LLMs’ inherent difﬁculties with com-\nplex reasoning tasks23,24, such as multi-step logical reasoning25,26 and clinical\ndecision-making27,28. Thus, creating tailored datasets and developing novel\nmethodologies to enable LLMs to synthesize high-quality DDx explanations\nare worthy of exploration.\nIn this study, we addressed these challenges by investigating prompting\nstrategies for generating trustworthyDDx explanations. Our contributions\nare threefold. First, we curated a new dataset of 570 clinical notes across nine\nspecialties, sourced from publicly available medical corpora and annotated\nby domain experts with differential diagnoses and explanations. To our\nknowledge, this is theﬁrst publicly available structured dataset with DDx\nexplanation annotation\n21,29, which facilitates automated evaluation and\nholds substantial potential to advance theﬁeld. Second, we proposed Dual-\nInf, a customized framework to optimize LLMs’ explanation generation\ncapabilities. The core design lies in enabling LLMs to perform bidirectional\ninference (i.e., from symptoms to diagnoses and vice versa), leveraging\n1Division of Computational Health Sciences, Department of Surgery, University of Minnesota, Minneapolis, MN, USA.2Bakar Computational Health Sciences\nInstitute, University of California San Francisco, San Francisco, CA, USA.3Department of Computer Science, University of Chicago, Chicago, IL, USA.4Department\nof Computer Science, Illinois Institute of Technology, Chicago, IL, USA.5Institute for Health Informatics and Division of Colon and Rectal Surgery, Department of\nSurgery, University of Minnesota, Minneapolis, MN, USA.6Department of Biomedical Data Science, Stanford University, Stanford, CA, USA.\ne-mail: zhan1386@umn.edu\nnpj Health Systems|            (2025) 2:12 1\n1234567890():,;\n1234567890():,;\nbackward veriﬁcation to boost prediction correctness. Third, we compre-\nhensively evaluated Dual-Inf for explainable DDx, including model\nexplainability and error analysis. The results demonstrated that Dual-Inf\nachieved superior diagnostic performance while delivering reliable inter-\npretations across various base LLMs (i.e., GPT-4, GPT-4o, Llama3-70B, and\nBioLlama3-70B). Overall, ourﬁndings highlight the effectiveness of Dual-\nInf as a promising tool for improving clinical decision-making.\nResults\nDataset\nWe developed Open-XDDx, a well-annotated dataset for explainable\nDDx, consisting of 570 clinical notes from publicly available medical\nexercises across nine specialties: cardiovascular, digestive, respiratory,\nendocrine, nervous, reproductive, circulatory, skin, and orthopedic dis-\neases. Each note includes patient symptoms, differential diagnoses, and\nexpert-derived explanations from the University of Minnesota (Supple-\nmentary Appendix 1). The dataset statistics are detailed in Table1 and\nTable 2.\nDifferential diagnosis performance\nWe evaluated differential diagnosis accuracy (Eq.1) by comparing model\npredictions to ground-truth diagnoses with prompts (Supplementary\nAppendix 3). The results with GPT-4 and GPT-4o are depicted in\nFig. 1(b), and the results with Llama3-70B and BioLlama3-70B are pre-\nsented in Supplementary Appendix 4. It showed that Dual-Inf con-\nsistently outperformed baselines across nine specialties. Speciﬁcally, when\nbuilt on GPT-4, the overall performance of SC-CoT signiﬁcantly exceeded\nCoT (difference of 0.032, 95% CI 0.021– 0.043, p = 0.001) and Diagnosis-\nCoT (difference of 0.019, 95% CI 0.001– 0.028, p = 0.004). Dual-Inf further\nsurpassed SC-CoT (0.533 vs. 0.472, difference of 0.061, 95% CI\n0.055– 0.062, p < 0.001). Concretely, the performance improvement of\nDual-Inf over SC-CoT exceeded 16% on cardiovascular and digestive\ndiseases. Similarly, using GPT-4o, Dual-Inf achieved over 0.55 accuracy\non nervous, skin, and orthopedic diseases, exceeding the baselines by over\n9%. With Llama3-70B and BioLlama3-70B, Dual-Inf outperformed SC-\nCoT by over 10% in cardiovascular, digestive, and respiratory diseases.\nThe overall performance improvement of Dual-Inf over SC-CoT across\nthe three base LLMs (difference of 0.059, 0.048, and 0.049) was statistically\nsigniﬁcant (p < 0.001).\nInterpretation performance\nModel explainability was examined through automatic and human\nassessments. For automatic evaluation, GPT-4o was employed to measure\nthe consistency between ground-truthand predicted interpretations, uti-\nlizing prompts detailed in Supplementary Appendix 3. We tested four base\nLLMs (GPT-4, GPT-4o, Llama3-70B, and BioLlama3-70B). Partial results\non GPT-4 are shown in Fig.2(a), with additional results in Supplementary\nAppendix 5. In Fig.2(a), the interpretation accuracy (Eq.2) of Diagnosis-\nCoT and SC-CoT was 0.305 and 0.334, surpassing CoT by 0.011 (95% CI\n0.004– 0.019, p = 0.012) and 0.04 (95% CI 0.038– 0.043, p < 0.001), respec-\ntively. Dual-Inf achieved even higher accuracy at 0.446, with a 0.112\nimprovement over SC-CoT (95% CI 0.105– 0.118, p < 0.001). Concretely,\nthe performance improvement of Dual-inf over the baselines surpassed 26%\nin cardiovascular and respiratory diseases. For BERTScore, SentenceBert,\nand METEOR, Dual-Inf outperformed SC-CoT with comparisons of 0.345\nvs. 0.258 (difference of 0.087, 95% CI 0.083– 0.090,p < 0.001), 0.427 vs. 0.356\n(difference of 0.071, 95% CI 0.067– 0.076) and 0.333 vs. 0.251 (difference of\n0.082, 95% CI 0.076– 0.088). When taking GPT-4o as the base LLM, the\ninterpretation accuracy of Dual-Inf reached 0.488, outperforming CoT and\nSC-CoT, which scored 0.366 and 0.408, respectively. On the other metrics,\nDual-Inf consistently surpassed SC-CoT, with differences of 0.083, 0.064,\nand 0.08. Similarly, with Llama3-70B and BioLlama3-70B, Dual-Inf\nexceeded SC-CoT by over 17% across all metrics. In detail, the perfor-\nmance improvement on digestive, respiratory, and endocrine diseases\nexceeded 25% over the baselines w.r.tinterpretation accuracy (Supple-\nmentary Appendix 5).\nThe interpretations were also manually examined by clinicians on\nthree qualitative metrics: Correctness, Completeness, and Usefulness\n(Supplementary Appendix 2). Figure2b presents the results for 100 ran-\ndomly selected notes, using GPT-4 as the base LLM. We observed that the\nCorrectness score of Dual-Inf predominantly ranged from 3 to 4, whereas\nSC-CoT scores mainly fell between 2 and 3. In terms of Completeness score,\nDual-Inf achieved 38 scores of 3 and 21 scores of 4, compared to SC-CoT’s\n19 and 3, respectively. Regarding the Usefulness score, Dual-Inf had\n33 scores of 3 and 25 scores of 4, while SC-CoT had 26 and 10, respectively.\nCase study\nWe further provided case studies to demonstrate the superior explainability\nof Dual-Inf over the baselines. The example in Fig.3 showcased that SC-CoT\nonly provided three correct explanations for a differential, i.e.,Pneu-\nmothorax, while Dual-Inf generated more accurate explanations. Besides,\nDual-Inf had one more correct differential, i.e.,Hemothorax,w i t ht h r e e\ncorrect explanations than the baselines. See more examples and detailed\nillustrations in Supplementary Appendices 7 and 8.\nError analysis on explanation\nWe analyzed error types in generated explanations by comparing Dual-Inf\nwith the baselines on 100 randomly selected samples with incorrect outputs.\nErrors were categorized as missing content (missing at least two pieces of\nevidence), factual errors (medically incorrect), or low relevance (evidence\nnot highly pertinent) based on prior studies\n30,31. Using GPT-4 as the base\nLLM (Fig.2(c)), SC-CoT had 89 cases of missing content versus 76 for Dual-\nInf (difference 13.4, 95% CI 11.5– 15.2). For factual errors, the baselines\nachieved similar performance, and the count number comparison between\nDual-Inf and SC-CoT was 17 vs. 8.2 (difference 8.8, 95% CI 7.8– 9.8). As for\nlow-relevance, Self-Contrast andSC-CoT had fewer errors than the CoT\nTable 1 | The data characteristics of our annotated explainable\nDDx dataset Open-XDDx\nStatistic Value\nTotal number of notes 570\nMean note length (words) 113.6\nStandard deviation of note length (words) 60.4\nMean number of diagnoses per note 4.6\nStandard deviation of diagnoses per note 1.0\nMean number of explanations per patient 14.5\nStandard deviation of explanations per patient 5.8\nMean number of explanations per diagnosis 3.1\nStandard deviation of explanations per diagnosis 1.5\nTable 2 | Breakdown of the notes in the DDx dataset Open-\nXDDx across the nine clinical specialties\nClinical Specialty Number of Notes (%)\nCardiovascular disease 26 (4.6%)\nDigestive system disease 105 (18.4%)\nRespiratory disease 58 (10.2%)\nEndocrine disorder 43 (7.5%)\nNervous system disease 137 (24.0%)\nReproductive system disease 54 (9.5%)\nCirculatory system disease 66 (11.6%)\nSkin disease 30 (5.3%)\nOrthopedic disease 51 (8.9%)\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 2\nand Diagnosis-CoT, while the comparison between SC-CoT and Dual-Inf\nwas 15.4 vs. 10.8 (difference 4.6, 95% CI 3.9– 5.3). All differences were\nstatistically signiﬁcant (p < 0.001). We further presented the count of errors\nin each clinical specialty in Supplementary Appendix 6. The results\ndemonstrated that the errors fell intoall the specialties, while the nervous\nand digestive diseases specialty had more errors.\nAblation study\nWe evaluated the contribution of each component in Dual-Inf through\nfour variants: (1) forward-inference only (FI), (2) FI with excluded\nbackward-inference (FI-EM), (3) FI-EM without self-reﬂection (FI-EM*),\nand (4) Dual-Inf without self-reﬂection (Dual-Inf*). Speciﬁcally, we\nadopted automatic metrics for the evaluation. The results in Supple-\nmentary Appendix 12 conﬁrmed that Dual-Inf achieved superior diag-\nnostic accuracy and explainability, highlighting the necessity of all\ncomponents.\nDiscussion\nOur study demonstrated that Dual-Inf signiﬁcantly enhanced diagnostic\naccuracy byﬁltering low-conﬁdence diagnoses through quality assessment.\nSpeciﬁcally, the examination module consolidated outputs from other\ncomponents to verify correctness, while the self-reﬂection mechanism\nenabled the forward-inference module to reﬁne predictions iteratively. To\nevaluate iterative reﬂection, we tracked the iteration count for each note in\nDual-Inf (Fig.4a), revealing that most predictions were iteratively revised.\nFor randomly selected ten notes withﬁve iterations, the number of correct\ndiagnoses improved progressively (Fig.4c), conﬁrming the effectiveness of\nthe iterative reﬂection mechanism. Besides, we observed that most of the\nnotes’prediction correctness was boosted or remained stable in the fourth or\nﬁfth iteration, demonstrating the necessity of setting the maximum iteration\nλ to a relatively large value (e.g., 5). Additionally, the distribution of diag-\nnostic accuracy across cases, visualized in Fig.4b, showed that the median\nand upper quartile for Dual-Inf (0.495 and 0.746) outperformed SC-CoT\nFig. 1 | Overview of the proposed framework and differential diagnosis perfor-\nmance. aAn overview of the Dual-Inference Large Language Model framework\n(Dual-Inf) for explainable DDx. Dual-Inf consists of four components: (1) a\nforward-inference module, which is an LLM to generate initial diagnoses from\npatient symptoms, (2) a backward-inference module, which is an LLM for con-\nducting inverse inference via recalling all the representative symptoms associated\nwith the initial diagnoses, i.e., from diagnoses to symptoms, (3) an examination\nmodule, which is another LLM to receive patients’notes and the output from the two\nmodules for prediction assessment (e.g., completeness examination) and decision\nmaking (e.g.,ﬁltering out low-conﬁdence diagnoses), and (4) an iterative self-\nreﬂection mechanism, which iteratively takes the low-conﬁdence diagnoses as\nfeedback for the forward-inference module to“think twice”. b Differential diagnosis\nperformance built on two base LLMs (GPT-4 and GPT-4o) over nine specialties. The\nresults are averaged overﬁve runs. Standard deviations are also shown.\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 3\n(0.434 and 0.652) and Diagnosis-CoT (0.421 and 0.628), with statistically\nsigniﬁcant improvements (p < 0.001). Theseﬁndings highlight the efﬁcacy\nof Dual-Inf in enhancing diagnostic accuracy.\nSecond, Dual-Inf produced superior DDx explanations. Manual eva-\nluation of 100 cases (Fig.2b) showed higher scores across all metrics,\nattributed to bidirectional inferences and iterative prediction reﬁnement.\nIterative reﬂection effectiveness was conﬁrmed through ten notes withﬁve\niterations (Fig.4c) and a case study of intermediate predictions (Supple-\nmentary Appendix 7), both demonstrating improved explanations over\niterations. Distribution analysis (Fig.4d) revealed higher median and\nquartile scores (e.g., BERTScore, METEOR) for Dual-Inf compared to SC-\nCoT, conﬁrming its ability to generate better explanations across most cases.\nNotably, although the note snippets were publicly available, the ground-\ntruth of DDx and the corresponding explanations were manually generated\nby our domain experts. Therefore, the LLMs have not been exposed to the\nground-truth, and the evaluation on our dataset is trustworthy.\nThird, this study demonstrated that leveraging multiple LLMs miti-\ngates explanation errors in DDx. As shown in Fig.2c, Self-Contrast and SC-\nCoT reduced low-relevance errors compared to CoT and Diagnosis-CoT,\nhighlighting the beneﬁt of integrating multiple LLM interpretations to\naddress hallucinations. Dual-Inf further minimized errors across all the\ntypes, attributed to its dual-inference scheme: the forward-inference module\nFig. 2 | Interpretation performance and error analysis. aInterpretation perfor-\nmance w.r.t interpretation accuracy (see Eq.2) and BERTScore across nine clinical\nspecialties. We implemented the methods with GPT-4. The results were averaged\nover ﬁve runs. Standard deviations were also shown.b Human evaluation results on\ninterpretation. It assessed three aspects: correctness, completeness, and usefulness,\nwith scores ranging from 1 to 5.c Error type analysis on interpretation. We manually\nexamined 100 cases and recorded the count of the error type. Diag-CoT denotes\nDiagnosis-CoT, and Self-Cont means Self-Contrast. The results were averaged over\nﬁve runs. The methods are implemented with GPT-4.\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 4\ngenerated diagnoses, the backward-inference module recalled medical\nknowledge, and the examination module reﬁned predictions. The self-\nreﬂection mechanism further improved explanation quality and reduced\nhallucinations through iterative reﬁnement. Additionally, the higher error\nrates observed in the nervous and digestive disease specialties were attrib-\nuted to their larger sample sizes in the dataset. However, normalizing error\ncounts by sample size revealed comparable error rates across the specialties.\nOne limitation of this study is that our dataset, encompassing nine\nclinical specialties, does not fully capture the breadth of real-world scenarios.\nBesides, the dataset lacks annotations on the priority of each diagnosis\nwithin the DDx, as ranking the likelihood of possible diseases presents\nsigniﬁcant challenges. Furthermore, the backward-inference module’s\nreliance on internal medical knowledge to generate reference signs and\nsymptoms makes it vulnerable to severe hallucinations or erroneous\nknowledge, which could impact performance. This issue can be mitigated by\nimplementing Dual-Inf with advanced LLMs\n32.\nIn summary, this study established amanually annotated dataset for\nexplainable DDx and designed a tailored framework that effectively har-\nnessed LLMs to generate high-quality explanations. Theﬁndings revealed\nthat existing prompting methods exhibited suboptimal performance in\nFig. 3 | Case study of SC-CoT and Dual-Inf.The methods are implemented by taking GPT-4 as the base LLM. Correct predictions are highlighted in blue.\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 5\ngenerating DDx and explanations, limiting their practical utility in clinical\nscenarios. Our experiments veriﬁed the effectiveness of Dual-Inf for pro-\nviding accurate DDx, delivering comprehensive explanations, and reducing\nprediction errors. Furthermore, the released dataset with ground-truth DDx\nand explanations could facilitate the researchﬁeld. Future work could\nexpand the dataset to a broader range of clinical specialties or integrate\ndomain knowledge from external databases for superior performance.\nMethods\nData acquisition and processing\nThe data source is publicly available medical exercises collected from\nmedical books33,34 and MedQA USMLE dataset35. There are two key criteria\nfor selecting the clinical notes: (1) the notes must originate from disease\ndiagnosis exercises; (2) they mustpertain to one of the nine clinical\nspecialties. We transformed the exercises into free text by preserving the\nsymptom descriptions and removing the multiple-choice options, where\napplicable. The texts were further preprocessed, including (1) removing\nduplicate notes, (2) unifying all characters into UTF-8 encoding and\nremoving illegal UTF-8 strings, (3) correcting or removing special char-\nacters, and (4)ﬁltering out notes with fewer than 130 characters. Lastly, we\ncollected 570 clinical notes, among which 10 notes were used for prompt\ndevelopment, and 560 notes were preserved for evaluation. The full dataset\ncan be found in Supplementary Appendix 13.\nData annotation\nThe raw data generally did not have the annotation of DDx, explanation,\nand clinical specialty. To build a well-annotated dataset, we employed three\nclinical physicians to curate the dataset manually. Two independent\nFig. 4 | In-depth analysis of Dual-Inf. aData statistics of the iteration number for\neach note in Dual-Inf.b Distribution visualization of diagnostic accuracy on each\nnote. Diag-CoT denotes Diagnosis-CoT, and Self-Cont means Self-Contrast.\nc Performance change on Dual-Inf w.r.t diagnosis and explanation after each\niteration. We randomly selected ten notes withﬁve iterations. In thisﬁgure, the base\nLLM of the methods is GPT-4.d Distribution visualization of interpretation per-\nformance on each note. SC-CoT and Dual-Inf were implemented with GPT-4. The\ncircular points shown as outliers mean that some scores are deviated from the vast\nmajority.\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 6\nphysicians annotated each exercise. When disagreement existed in the\nannotation, a third physicianexamined the case and made theﬁnal anno-\ntation. We checked the inter-annotator agreement (IAA) on DDx, inter-\npretation, and specialty (Supplementary Appendix 1). Additionally, our\ndataset is well-structured in a standardized format, facilitating automated\nevaluation.\nModel development\nH o wt oe f f e c t i v e l ye l i c i tL L M s’ capability for accurate DDx explanation is\nchallenging. Inspired by the fact that humans usually conduct backward\nreasoning to validate the correctness of answers when solving reasoning\nproblems\n36, we proposed performing backward veriﬁcation (i.e., from\ndiagnosis to symptoms) to examine the predicted diagnoses and elicit\ncorrect answers via self-reﬂection. Accordingly, we developed a customized\nframework called Dual-Inference Large Language Model (Dual-Inf), shown\nin Fig.1a. Speciﬁcally, Dual-Inf consisted of four components: (1) a forward-\ninference module, which was an LLM for initial diagnoses, i.e., from\npatients’symptoms to diagnoses, (2) a backward-inference module, which\nwas an LLM for inverse inference via recalling all the representative\nsymptoms of the initial diagnoses, i.e., from diagnoses to symptoms, (3) an\nexamination module, which was another LLM that received patients’notes\nand the output from the two modules for prediction assessment and deci-\nsion making, and (4) an iterative self-reﬂection mechanism, which itera-\ntively took low-conﬁdence diagnoses as feedback for the forward-inference\nmodule to“think twice”.\nThe pipeline was as follows. First, the forward-inference module\nanalyzed clinical notes to infer initial diagnoses and provide interpreta-\ntions. Next, the backward-inference module received the initial diagnoses\nas input and recalled the representative symptoms that the diagnoses\ngenerally present, including medical examination and laboratory test\nresults. Given that the recalled symptoms were derived from the LLM’s\ninternal knowledge, they are generally reliable in advanced LLMs\n30,32 and\ncould serve as a reference for measuring the correctness of the predicted\nexplanations. Afterward, the examination module veriﬁed and reﬁned the\nabove results. Speciﬁcally, it (i) checked the forward-inference module’s\nexplanations against the recalled knowledge and discarded erroneous\nones, (ii) supplemented the interpretations by integrating patient notes\nwith recalled knowledge, (iii) decided whether to accept orﬁlter predic-\ntions based on their quality. The underlying idea was that a diagnosis\nsupported by fewer interpretations was deemed less trustworthy. To assess\ndiagnostic conﬁdence, a thresholdβ was applied: diagnoses with fewer\nthan β supporting interpretations wereﬂagged as low-conﬁdence. Later,\nthe self-reﬂection mechanism took the low-conﬁdence diagnoses as\nfeedback to prompt the forward-inference module to“think twice.” This\niterative process continued up to a maximum limitλ, balancing accuracy\nwith efﬁciency. Upon reaching this limit, the framework outputted the\nﬁnal results. The prompts for the three modules are detailed in Supple-\nmentary Appendix 9. Importantly, the prompts for the forward-inference\nmodule were carefully designed to ensure objectivity toward feedback\nfrom the examination module, reducing the risk of false negatives\nundermining correct predictions.\nImplementation details\nWe adopted four baselines: (1) CoT20, a popular prompting method; (2)\nDiagnosis-CoT19, a customized prompting method for disease diagnosis; (3)\nSelf-Contrast37, an advanced method with multiple prompts and a re-\nexamination mechanism to enhance reasoning; (4) Self-consistency CoT\n(SC-CoT)\n38, which assembled multiple reasoning paths to enhance per-\nformance. We followed the original papers in the implementation. Speci-\nﬁcally, SC-CoT generatedﬁve reasoning paths for each note and then\nselected the most consistent diagnoses and interpretations. The prompts of\nbaselines were shown in Supplementary Appendix 10. As for Dual-Inf, we\nincorporated CoT into the three LLM-based modules. The maximum\niteration numberλ was assigned to 5, considering the trade-off between\neffectiveness and efﬁciency; the thresholdβ w a ss e tt o3 .W ef u r t h e ra n a l y z e d\nthe impact of the hyper-parameterβ on the performance and presented the\nresults in Supplementary Appendix11. For a fair comparison, all the\nm e t h o d sw e r ei m p l e m e n t e dw i t ht h es a m eb a s eL L M ,i n c l u d i n gG P T - 4 ,\nGPT-4o, Llama3-70B (https://huggingface.co/meta-llama/Meta-Llama-3-\n70B), and BioLlama3-70B ( https://huggingface.co/aaditya/Llama3-\nOpenBioLLM-70B). For the former two LLMs, we used the API from the\nOpenAI company (https://platform.openai.com/docs/models), which were\n“gpt-4-turbo-preview” and “gpt-4o”; for the latter two, we downloaded the\nmodels from Huggingface for inference. The temperature parameter was set\nas 0.1.\nPerformance evaluation\nWe conducted automatic evaluation by comparing the ground-truths with\nthe predicted ones. Following related papers\n27, we used accuracy as the\nprimary metric for assessing diagnostic performance, i.e.,\nDiagnostic Accuracy¼ Cumulative number of correct diagnoses\nTotal number of diagnoses ð1Þ\nFor interpretation performance, we employed metrics designed to\nassess the semantic alignment between the reference text and the predicted\ntext, rather than relying solely on string matching. The metrics, including\naccuracy, BERTScore\n39, SentenceBert40, and METEOR41, have been widely\nused in related tasks42,43. Concretely, interpretation accuracy was computed\nas:\nInterpretation Accuracy¼ Cumulative number of correct interpretations\nTotal number of interpretations ð2Þ\nBERTScore39 employs the BERT model44 to determine the semantic\nsimilarity between reference and generated text, offering a context-\naware evaluation of model performance. SentenceBert\n40 measures\nsentence similarity using a BERT model that generates dense vector\nrepresentations, facilitating efﬁcient and accurate semantic compar-\nisons. METEOR\n41 assesses the harmonic mean of unigram precision\nand recall, utilizing stemmed forms and synonym equivalence. The\ndetails of automatic and human evaluation are shown in Supplementary\nAppendix 2.\nData availability\nData is provided in the supplementary informationﬁles.\nCode availability\nThe code used for this study is available athttps://github.com/betterzhou/\nDual-Inf.\nReceived: 28 December 2024; Accepted: 1 March 2025;\nReferences\n1. Adler-Milstein, J., Chen, J. H. & Dhaliwal, G. Next-generation artiﬁcial\nintelligence for diagnosis: from predicting diagnostic labels to\n“wayﬁnding. Jama 326, 2467–2468 (2021).\n2. Fansi Tchango, A. et al. Towards trustworthy automatic diagnosis\nsystems by emulating doctors’reasoning with deep reinforcement\nlearning. Adv. Neural Inf. Process. Syst.35, 24502–24515 (2022).\n3. Wu, L. et al. Differential diagnosis of secondary hypertension based\non deep learning.Artif. Intell. Med.141, 13 (2023).\n4. Levy, J., Álvarez, D., Del Campo, F. & Behar, J. A. Deep learning for\nobstructive sleep apnea diagnosis based on single channel oximetry.\nNat. Commun.14, 4881 (2023).\n5. Zhang, Z. et al. Pathologist-level interpretable whole-slide cancer\ndiagnosis with deep learning.Nat. Mach. Intell.1, 236–245 (2019).\n6. Zakka, C. et al. Almanac— retrieval-augmented language models for\nclinical medicine.NEJM AI1, AIoa2300068 (2024).\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 7\n7. Wu, S. et al. Benchmarking open-source large language models,\nGPT-4 and Claude 2 on multiple-choice questions in nephrology.\nNEJM AI1, AIdbp2300092 (2024).\n8. Tu, T. et al. Towards generalist biomedical AI.NEJM AI1,\nAIoa2300138 (2024).\n9. Singhal, K. et al. Towards Expert-Level Medical Question Answering\nwith Large Language Models.ArXiv, vol. abs/2305.09617, (2023).\n10. Tang, L. et al. Evaluating large language models on medical evidence\nsummarization. NPJ Dig. Med.6, 158 (2023).\n11. Benary, M. et al. Leveraging Large Language Models for Decision\nSupport in Personalized Oncology.JAMA Netw. Open6,\ne2343689–e2343689 (2023).\n12. Kanjee, Z, Crowe, B & Rodman, A Accuracy of a generative artiﬁcial\nintelligence model in a complex diagnostic challenge.Jama 330,\n78–80 (2023).\n13. Kwon, T. et al. Large language models are clinical reasoners:\nReasoning-aware diagnosis framework with prompt-generated\nrationales. inProceedings of the AAAI Conference on Artiﬁcial\nIntelligence. 18417–18425 (2024).\n14. Hua, R. et al. Lingdan: enhancing encoding of traditional Chinese\nmedicine knowledge for clinical reasoning tasks with large language\nmodels. J. Am. Med. Informat. Asso.31, 2019–2029 (2024).\n15. Chen, J. et al. CoD, Towards an Interpretable Medical Agent using\nChain of Diagnosis.arXiv preprint arXiv:2407.13301, (2024).\n16. Zhou, S. et al. Large Language Models for Disease Diagnosis: A\nScoping Review.ArXiv, vol. abs/2409.00097, (2024).\n17. Tu, T. et al. Towards conversational diagnostic AI.arXiv preprint\narXiv:2401.05654, 2024.\n18. McDuff, D. et al. (2023, November 01, 2023). Towards Accurate\nDifferential Diagnosis with Large Language Models.\narXiv:2312.00164. Available:https://ui.adsabs.harvard.edu/abs/\n2023arXiv231200164M.\n19. Savage, T., Nayak, A., Gallo, R., Rangan, E. S. & Chen, J. H. Diagnostic\nreasoning prompts reveal the potential for large language model\ninterpretability in medicine.NPJ Digit. Med7, 20 (2023).\n20. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Adv. neural Inf. Process. Syst.35, 24824–24837\n(2022).\n21. Tchango, A. F., Goel, R., Wen, Z., Martel, J. & Ghosn, J. DDXPlus: A\nNew Dataset For Automatic Medical Diagnosis.in Neural Information\nProcessing Systems, (2022).\n22. Wu, J. et al. Clinical Text Datasets for Medical Artiﬁcial Intelligence and\nLarge Language Models— A Systematic Review.NEJM AI1,\nAIra2400012 (2024).\n23. Ashwani, S. et al. Cause and effect: Can large language models truly\nunderstand causality? inProceedings of the AAAI Symposium Series,\n2–9 (2024).\n24. Chi, H. et al. Unveiling Causal Reasoning in Large Language Models:\nReality or Mirage? inThe Thirty-eighth Annual Conference on Neural\nInformation Processing Systems.\n25. M. Parmar, et al.“LogicBench: Towards systematic evaluation of\nlogical reasoning ability of large language models. inProceedings of\nthe 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2024, 13679-13707.\n26. Chen, A. et al. Two Failures of Self-Consistency in the Multi-Step\nReasoning of LLMs.Transac. Machine Learning Res.(2024).\n27. Hager, P. et al. Evaluation and mitigation of the limitations of large\nlanguage models in clinical decision-making.Nat. Med.30,\n2613–2622 (2024).\n28. Williams, C. Y., Miao, B. Y., Kornblith, A. E. & Butte, A. J. Evaluating the\nuse of large language models to provide clinical recommendations in\nthe Emergency Department.Nat. Commun.15, 8236 (2024).\n29. Macherla, S., Luo, M., Parmar, M. & Baral, C. MDDial: A Multi-turn\nDifferential Diagnosis Dialogue Dataset with Reliability Evaluation.\narXiv preprint arXiv:2308.08147, (2023)\n30. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n31. Chen, X. et al. FFA-GPT: an automated pipeline for fundusﬂuorescein\nangiography interpretation and question-answer.npj Digital Med.7,\n111 (2024).\n32. Goh, E. et al. Large language model inﬂuence on diagnostic\nreasoning: a randomized clinical trial.JAMA Netw. Open7,\ne2440969–e2440969 (2024).\n33. Le, T., Bhushan, V., Sheikh-Ali, M. & Shahin F. A.First Aid for the\nUSMLE Step 2 CS, Third Edition: McGraw-Hill Education, (2009).\n34. Le, T. & Bechis, S.K.First Aid Q&A for the USMLE Step 1, Second\nEdition: McGraw-Hill Education, (2009).\n35. Jin, D. et al. What Disease Does This Patient Have? A Large-Scale\nOpen Domain Question Answering Dataset from Medical Exams.\nAppl. Sci.11, 6421 (2021).\n36. Jiang, W. et al. Forward-backward reasoning in large language\nmodels for mathematical veriﬁcation. inFindings of the Association for\nComputational Linguistics ACL 2024, 6647–6661 (2024).\n37. Zhang, W. et al. Self-Contrast: Better Reﬂection Through Inconsistent\nSolving Perspectives.Annual Meeting of the Association for\nComputational Linguistics. (2024).\n38. Wang, X. et al. Self-Consistency Improves Chain of Thought\nReasoning in Language Models. inThe Eleventh International\nConference on Learning Representations.\n39. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y.\nBERTScore: Evaluating Text Generation with BERT.ArXiv, vol. abs/\n1904.09675, 2019.\n40. Reimers, N. & Gurevych, I. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks.Conference on Empirical Methods in\nNatural Language Processing, (2019).\n41. Banerjee, S. & Lavie, A. METEOR: An automatic metric for MT\nevaluation with improved correlation with human judgments. in\nProceedings of the acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summarization,6 5–72\n(2005).\n42. Abbasian, M. et al. Foundation metrics for evaluating effectiveness of\nhealthcare conversations powered by generative AI.NPJ Digital Med.\n7, 82 (2024).\n43. Celikyilmaz, A., Clark, E. & Gao, J. Evaluation of Text Generation: A\nSurvey. ArXiv, vol. abs/2006.14799, (2020).\n44. JDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. in\nNorth American Chapter of the Association for Computational\nLinguistics, (2019).\nAcknowledgements\nThis work was supported by the National Institutes of Health’sN a t i o n a l\nCenter for Complementary and Integrative Health under grant number\nR01AT009457, National Institute on Aging under grant number\nR01AG078154, and National Cancer Institute under grant number\nR01CA287413. The content is solely the responsibility of the authors and\ndoes not represent the ofﬁcial views of the National Institutes of Health. We\nalso acknowledge support from the Center for Learning Health System\nSciences, a partnership between the University of Minnesota Medical\nSchool and School of Public Health.\nAuthor contributions\nS.Z. and R.Z. conceptualized the study. S.Z. contributed to the literature\nsearch and model evaluation. S.Z. and S.D. performed the data\ncollection, prompt design, and model construction. S.Z., R.Z., and G.M.\ndiscussed and arranged data annotation and human evaluation. S.Z.,\nM.L., and R.Z. conducted experimental design. S.Z. performed\nmanuscript drafting. R.Z. supervised the study. All authors contributed to\nthe research discussion, manuscript revision, and approval of the\nmanuscript for submission.\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 8\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s44401-025-00015-6\n.\nCorrespondenceand requests for materials should be addressed to\nRui Zhang.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44401-025-00015-6 Article\nnpj Health Systems|            (2025) 2:12 9",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.6190779805183411
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.5697224140167236
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5324856638908386
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.5124664902687073
    },
    {
      "name": "Differential (mechanical device)",
      "score": 0.48195192217826843
    },
    {
      "name": "Computer science",
      "score": 0.4375336170196533
    },
    {
      "name": "Medicine",
      "score": 0.3276313543319702
    },
    {
      "name": "Artificial intelligence",
      "score": 0.24905768036842346
    },
    {
      "name": "Engineering",
      "score": 0.18358245491981506
    },
    {
      "name": "Linguistics",
      "score": 0.10730302333831787
    },
    {
      "name": "Epistemology",
      "score": 0.08862870931625366
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130238516",
      "name": "University of Minnesota",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180670191",
      "name": "University of California, San Francisco",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180949307",
      "name": "Illinois Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}