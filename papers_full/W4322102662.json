{
  "title": "ABT-MPNN: an atom-bond transformer-based message-passing neural network for molecular property prediction",
  "url": "https://openalex.org/W4322102662",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2112448817",
      "name": "Chengyou Liu",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2098039240",
      "name": "Yan Sun",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2099741433",
      "name": "Rebecca Davis",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2256516116",
      "name": "Silvia T. Cardona",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2116121793",
      "name": "Pingzhao Hu",
      "affiliations": [
        "University of Manitoba",
        "Western University"
      ]
    },
    {
      "id": "https://openalex.org/A2112448817",
      "name": "Chengyou Liu",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2098039240",
      "name": "Yan Sun",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2099741433",
      "name": "Rebecca Davis",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2256516116",
      "name": "Silvia T. Cardona",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A2116121793",
      "name": "Pingzhao Hu",
      "affiliations": [
        "Western University",
        "University of Manitoba"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2885020292",
    "https://openalex.org/W2901411193",
    "https://openalex.org/W3007309629",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W3018495986",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W2168480393",
    "https://openalex.org/W3087318293",
    "https://openalex.org/W2104489082",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2949218973",
    "https://openalex.org/W4220847490",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W4214656930",
    "https://openalex.org/W4213077304",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W1780364926",
    "https://openalex.org/W2076498053",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W2008505552",
    "https://openalex.org/W2276859037",
    "https://openalex.org/W2461620095",
    "https://openalex.org/W2473190403",
    "https://openalex.org/W2138077583",
    "https://openalex.org/W2883860328",
    "https://openalex.org/W2511758086",
    "https://openalex.org/W3100157108",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3102797483"
  ],
  "abstract": null,
  "full_text": "Liu et al. Journal of Cheminformatics           (2023) 15:29  \nhttps://doi.org/10.1186/s13321-023-00698-9\nRESEARCH\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\nJournal of Cheminformatics\nABT-MPNN: an atom-bond \ntransformer-based message-passing neural \nnetwork for molecular property prediction\nChengyou Liu1, Yan Sun2, Rebecca Davis3, Silvia T. Cardona4,5 and Pingzhao Hu1,2,6,7* \nAbstract \nGraph convolutional neural networks (GCNs) have been repeatedly shown to have robust capacities for modeling \ngraph data such as small molecules. Message-passing neural networks (MPNNs), a group of GCN variants that can \nlearn and aggregate local information of molecules through iterative message-passing iterations, have exhibited \nadvancements in molecular modeling and property prediction. Moreover, given the merits of Transformers in multiple \nartificial intelligence domains, it is desirable to combine the self-attention mechanism with MPNNs for better molecu-\nlar representation. We propose an atom-bond transformer-based message-passing neural network (ABT-MPNN), to \nimprove the molecular representation embedding process for molecular property predictions. By designing corre-\nsponding attention mechanisms in the message-passing and readout phases of the MPNN, our method provides a \nnovel architecture that integrates molecular representations at the bond, atom and molecule levels in an end-to-end \nway. The experimental results across nine datasets show that the proposed ABT-MPNN outperforms or is compa-\nrable to the state-of-the-art baseline models in quantitative structure–property relationship tasks. We provide case \nexamples of Mycobacterium tuberculosis growth inhibitors and demonstrate that our model’s visualization modal-\nity of attention at the atomic level could be an insightful way to investigate molecular atoms or functional groups \nassociated with desired biological properties. The new model provides an innovative way to investigate the effect of \nself-attention on chemical substructures and functional groups in molecular representation learning, which increases \nthe interpretability of the traditional MPNN and can serve as a valuable way to investigate the mechanism of action of \ndrugs.\nKeywords Message-passing neural networks, Attention mechanism, Molecular representations, Atom-bond \nTransformer message-passing neural network, Molecular property prediction, Biological activity prediction\n*Correspondence:\nPingzhao Hu\nphu49@uwo.ca\n1 Department of Electrical and Computer Engineering, University \nof Manitoba, Winnipeg, MB, Canada\n2 Department of Computer Science, University of Manitoba, Winnipeg, \nMB, Canada\n3 Department of Chemistry, University of Manitoba, Winnipeg, MB, \nCanada\n4 Department of Microbiology, University of Manitoba, Winnipeg, MB, \nCanada\n5 Department of Medical Microbiology & Infectious Disease, University \nof Manitoba, Winnipeg, MB, Canada\n6 Department of Biochemistry and Medical Genetics, University \nof Manitoba, Winnipeg, MB, Canada\n7 Department of Biochemistry, Western University, Building Rm. 362, \nLondon, ON N6A 5C1, Canada\nPage 2 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \nIntroduction\nWith the rapid development and expanding applica -\ntions of artificial intelligence (AI) in academia and \nindustry, molecular property prediction has played a \nfundamental role in the early stage of drug discovery. \nBy training effective computational models and deliv -\nering accurate prediction of molecular properties, \npotential drug candidates were identified from virtual \nscreening libraries of small molecules, thus address -\ning the intensive monetary investment and time-con -\nsuming nature of early stage drug discovery process \n[1–3]. In this context, expressive molecular representa -\ntion modeling performed by a high-precision machine \nlearning (ML) model is indispensable and has garnered \nsignificant attention from researchers.\nSimilar to convolutional neural networks (CNNs) that \nlearn latent featurization of structural data by conduct -\ning convolutional operations, graph convolutional neural \nnetworks (GCNs) can generalize the convolutional oper -\nation to non-structural data and aggregate global infor -\nmation from local features. As small molecules can be \nnaturally considered as graph data in the computational \ncontext, GCNs have been wildly applied to molecular \nproperty prediction tasks and have achieved remark -\nable success [4, 5]. The essence of graph convolution in \nthe spatial domain is the process of designing node-level \nfeature aggregation functions, so that information from \nthe local neighborhoods of the nodes can be transmit -\nted and aggregated throughout the graph [6, 7]. Among \nthe variants of spatial-based GCNs, message-passing \nneural network (MPNN) [8] is a classic approach and \noutlines general frameworks for utilizing spatial graph \nconvolutions.\nThe integration of the self-attention [9] mechanism \ninto the message-passing neural network is of great inter-\nest as it can learn a better representation from molecu -\nlar graphs. While the local information of molecules can \nbe transmitted and aggregated within graphs without \ndistance restrictions, every atom or bond has the same \nweight of impact on the predicted outcomes due to the \naveraging effect in graph convolution or message-pass -\ning schemes. However, in reality, each molecule forms \na particular conformation in the 3D space to reach the \nminimum energy states. The molecular properties and \nmechanism of action (MOA) of specific molecules are \ncritically governed by their conformations. Topologi -\ncally adjacent or close atoms that are connected by bonds \ncan potentially form functional groups or fragments that \ndetermine the properties of molecules, such as toxicity. \nBy integrating attention mechanisms with MPNNs, the \nmodels can focus more on substructures critical to the \ndesired chemical properties in the learning process, thus \nyielding more informative molecular representations.\nGiven the strengths and various successful practices \nof the Transformer models, several previous works have \naugmented self-attention to GCNs, whereas the major -\nity of them employed the self-attention mechanism dur -\ning the node (atom) embedding [10–12]. For example, \nAttentive FP proposed by Xiong et  al. [13] extended a \ngraph-based neural network with the self-attention on \nboth atom and molecule embedding, where they treated \nthe entire molecule as a super-virtual node connecting \nto all atoms. Although these models can learn expressive \nencodings of molecules by applying graph attention to \natoms, none of them modeled the interactions of atomic \nbonds during the message-passing. In cheminformatics, \nbesides the attention to the local environment of mol -\necules, some studies have explored the self-attention \nmechanism in other aspects during representation learn -\ning. For instance, Chuang et al. [14] developed the atten -\ntion mechanism on top of a GCN to aggregate results \nover molecular conformers. In their network, the atten -\ntion coefficients are assigned to individual encodings of \nconformers, whereas the modeling of attention inside the \nmolecular graphs is omitted.\nIn this work, we propose an Atom-Bond Transformer-\nbased Message-passing Neural Network (ABT-MPNN), \nin which we adopted additive attention and scaled dot-\nproduct attention to the MPNN framework at both bond \nand atom levels, respectively. The additive attention [15] \nis an attention mechanism that is performed by calculat -\ning the attention alignment score of the hidden states of \nthe encoder and decoder in the form of feed-forward lay -\ners. The scaled dot-product attention [9] is achieved by \nmodeling the interaction between query and key through \ndot-product, followed by a scaling factor to scale down \nthe results of dot-products. At the atom level attention, \nwe further incorporate three types of inter-atomic feature \nmatrices (atom and bond feature matrix, adjacency and \ndistance matrix and coulomb matrix) into the model to \nprovide structural and electrostatic information about \nmolecules. Finally, we enable our model with the atten -\ntion-based visualization modality on atoms using similar-\nity maps [16], where topography-like molecular maps are \ncolored based on the atomic contribution (weight) to the \ndesired properties.\nThe novelty of this model can be summarized as fol -\nlows: i) our work integrates the additive attention and the \nscaled dot-product attention into graph-based models \nand highlights the effect of self-attention on both atoms \nand bonds of molecules; ii) we introduce the Coulomb \nmatrix to the network and design a feature-engineering \nscheme in which each attention head only comprises one \ntype of scaled feature matrix in addition to the trained \nattention weights. This improvement is inspired by the \nMolecule Attention Transformer (MAT) proposed by \nPage 3 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:29 \n \nMaziarka et al. [11], where adjacency and distance matri -\nces were combined and added to every attention head.\nMaterials and methods\nPreliminaries\nWe conduct a brief description of the preliminar -\nies related to this work, including several graph-\nbased molecular representations, message-passing \nneural networks, as well as the attention mechanism and \nTransformer.\nGraph‑based molecular representation\nA graph G is a data structure defined by a pair of sets \n(V , E) , where V and E represent the collections of verti -\nces and edges, respectively. A directed graph has ordered \npairs of vertices, where edges are directed from one ver -\ntex to another. In contrast, an undirected graph can be \nseen as a special case of directed graph in which ele -\nments of E are unordered pairs of elements in V , mean-\ning the edges between nodes have no direction associated \nwith them. In modeling, the presence of a pair in E (i.e., \neij = (vi, vj) ∈ E  ) signifies a specific connection between \ntwo vertices (i.e., v i, v j ) in V . While one may associate \nfeature vectors to the elements in V and/or those in E , \nthese feature vectors are not strictly part of the graph \ndata structure. Accordingly, a molecular graph comprises \na set of atoms and a set of chemical bonds or interac -\ntions between each pair of adjacent atoms. Instead of \ncharacterizing the complete molecular information into \na one-dimensional array such as molecular fingerprints, \nthe graph structure permits association of a feature vec -\ntor with each atom and with each bond. The graph-based \nrepresentations can thus encode the properties or rela -\ntionships of atoms and bonds locally with a collection of \natom and bond feature vectors.\nAtom and bond feature matrices Various chemical prop-\nerties can be calculated for atoms and bonds of molecules. \nThe extracted atom and bond features are usually mapped \ninto two-dimensional data arrays that can be easily han -\ndled by computers [17]. Specifically, an atom feature \nmatrix can be generated by filling each row (represent -\ning each atom in the molecule) with atomic properties, \nsuch as atomic number, formal charge, and chirality. For a \nbond feature matrix, the values in each row correspond to \nattributes calculated for each bond in a molecule, which \nmay include bond type, conjugation, ring membership, \netc. In practice, categorical properties are commonly \nencoded in a one-hot manner to be more expressive.\nAdjacency and  distance matrices Adjacency and dis -\ntance matrices are two graph representations of mol -\necules that contain the information of connectivity and \ndistance for each pair of atoms, respectively. For an adja -\ncency matrix, entries are set to 1 if chemical bonds exist \nbetween the corresponding atom pairs while nonbonded \natom pairs are denoted with 0. In contrast to this binary \ndefinition of bonding, a distance matrix depicts the topo -\nlogical distances of atoms. For each molecule, a distance \nmatrix is based on the molecular conformation and is cal-\nculated according to the 3D coordinates of atom pairs.\nCoulomb matrix The Coulomb matrix proposed by \nRupp et al. [18] is a molecular featurization method that \ndepicts the electrostatic interaction between atoms, which \nis specified by a set of nuclear charges { Z i } and the corre-\nsponding Cartesian coordinates { R i }. For each molecule, \na Coulomb matrix is encoded by atomic energies and the \ninter-nuclear Coulomb repulsion operator as follows:\nThe elements on diagonal (i = j) represent the inter -\naction of atoms with themselves and are assigned with \na polynomial fit of atomic energy. The rest of the entries \n(i  = j) are calculated by the Coulomb repulsion operator.\nMessage‑passing neural networks\nThe MPNN proposed by Gilmer et al. [8] is another type \nof spatial-based approach that operates on undirected \ngraphs with both node and edge features. The MPNN \nabstracts the commonalities of spatial convolutions and \ncan be used as a general framework for spatial-based \nGCNs. The MPNN framework generally comprises two \nphases to obtain global graph features: a message-pass -\ning phase and a readout phase. Specifically, the mes -\nsage-passing phase consists of T iterations to aggregate \ninformation for each node. A graph is first initialized by \nnode features xv and edge features evw . In each message-\npassing step t ( 1 ≤ t ≤ T  ), the hidden representation \n( ht\nv ) and the message m t\nv associated with each node v are \nupdated at t + 1 according to\nwhere M t is a message function and U t is a vertex update \nfunction. After T iterations, the readout phase, with a \nreadout function R, is used to aggregate a global repre -\nsentation for the entire graph from all hidden representa-\ntions of nodes as follows:\n(1)M ij=\n{\n0.5Z2.4\ni (i= j)\nZiZj\n|Ri−Rj| (i�=j)\n(2)m t+1\nv =\n∑\nw∈N (v)\nM t(ht\nv,ht\nw ,evw)\n(3)ht+1\nv = U t(ht\nv,m t+1\nv )\n(4)ˆy = R({hT\nv |v ∈ G })\nPage 4 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \nWith different definitions of M t , U t , and R , multiple \nspatial-based GCNs can be generalized into the MPNN \nframework. The MPNN framework has been extensively \nused in computational chemistry and biology fields for \nmodeling molecular structures due to the flexible and \ncustomizable message/update functions. For instance, \na robust and powerful architecture called directed mes -\nsage-passing neural network (D-MPNN) [19] engineers \nmessage aggregation schemes associated with directed \nbonds rather than atoms. Using such a design, D-MPNN \ncan avoid unnecessary loops and redundancies in the \nmessage-passing iterations, thus allowing effective aggre -\ngation of local information to the molecular level.\nAttention mechanism and transformer\nThe Transformer [9], a new deep learning approach \nthat uses the self-attention mechanism to differentially \nweigh the significance of each part of the input data \nand its variants, has emerged as one of the most potent \narchitectures for modeling sequence data in natural lan -\nguage processing. Unlike the convolutional operation in \nthe traditional convolutional neural network, the self-\nattention mechanism, which serves as the Transformer’s \ncore, can efficiently model the sequence data by captur -\ning the interactions between each pair of input tokens. \nTransformer-like architectures have been applied and \nshow great promise in multiple AI domains, such as \nvision Transformer, [20] developed for computer vision \ntasks, and AlphaFold2, [21] designed for protein folding \nproblems.\nThe Transformer network [9] is built upon the self-\nattention mechanism, where a scaled dot-product scor -\ning function is applied to model the context by capturing \nthe correspondence between each pair of the position \nof the input. Specifically, a self-attention layer takes an \ninput hidden matrix H ∈ RN ×d , where N is the number \nof entries and d is their hidden dimension. The input is \nprojected to a query matrix ( Q = HW Q ), a key matrix \n( K = HW K ) and a value matrix ( V = HW V ), where WQ , \nWK and WV are the parameter matrices. The self-atten -\ntion in the Transformer is computed as:\nInstead of calculating a single attention function to the \nqueries, keys, and values, the Transformer uses multi-\nhead self-attention, where multiple attention functions \nare performed in parallel and then projected to form \nthe overall output. Specifically, for each attention head \n( head i ), the learned representation is formulated as:\n(5)Attention(Q, K , V ) = softmax( QK T\n√\nd\n)V\nwhere W Qi , W Ki , W Vi are learnable weight matrices for \nhead i . Next, the outputs of attention heads are concat -\nenated and projected by a parameter matrix WO to pro -\nduce the final output:\nAtom‑bond transformer‑based message‑passing neural \nnetwork\nModel architecture\nThe architecture of the proposed atom-bond Trans -\nformer-based message-passing neural network (ABT-\nMPNN) is shown in Fig.  1. As previously defined, the \nMPNN framework consists of a message-passing phase \nand a readout phase to aggregate local features to a \nglobal representation for each molecule. According \nto this paradigm, D-MPNN defines a novel message-\npassing phase through directed bonds. Here, we fur -\nther extend D-MPNN by integrating the self-attention \nmechanism at the bond and atom levels with two \nTransformer-like architectures and design a feature \nengineering scheme at the atom attention step.\nMore concretely, molecules represented by the simpli -\nfied molecular-input line-entry system (SMILES) are first \nentered into the featurization step, and node features ( xv ) \nand bond features ( evw ) are generated, most of which are \none-hot encoded (Additional file 1: Table S1). In addition, \nthree inter-atomic (adjacency, distance, coulomb) matri -\nces and a feature vector containing molecular descrip -\ntors ( h f ) are also generated (Additional file  1: Table S2). \nSince hidden states are transmitted in a directed manner \nin the message-passing (bond embedding) phase, each \nbond is initialized with two feature vectors, represent -\ning the bond messages in two opposite directions. Before \nthe bond embedding stage, the hidden states for chemical \nbonds ( h0\nvw ) are initialized, where W i is the first learnable \nweight matrix of the model (Table 1: Initialization).\nAt each message-passing iteration t , each bond mes -\nsage ( m t\nvw ) is first updated by summing all the incoming \nneighboring hidden states ( ht−1\nkv ,k ∈ Neighbor (v) ) from \nthe previous iteration, except the one that represents \nthe opposite direction of its own ( ht−1\nwv  ). Next, we aug -\nment a multi-head self-attention to the bond messages \nand add the input bond messages to the bond atten -\ntion output through a skip connection. Specifically, to \n(6)\nheadi = Attention\n(\nQW Q i, KW Ki, VW Vi\n)\n= softmax( QW Q i(KW Ki)T\n√\nd\n)VW Vi\n(7)\nMultiHead(Q ,K ,V ) = Concat(head1 ,... ,headh)W O\nPage 5 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:29 \n \nproduce the attention for each bond, the bond attention \nblock takes in all the bond messages from the previous \nmessage-passing iteration as input. The obtained bond \nattention message ( bt\nvw  ) is projected by a hidden weight \nmatrix ( W h ), concatenated with the original bond hid -\nden state ( h0\nvw ), then fed into an activation function to \ngenerate the hidden state ( ht\nvw  ) that is used for the fol -\nlowing message-passing iteration. Compared with the \nFig. 1 Illustration of our proposed ABT-MPNN. The given network takes the SMILES as input and generates atom features, bond features, three \ninter-atomic matrices and molecular descriptors as local and global encodings of the molecule. The bond feature matrix is first learned via bond \nattention blocks and bond update functions in the message-passing layers. After the message-passing phase, the atomic representations are \nobtained by summing the incoming bond hidden states, followed by the concatenation of the atom feature matrix and a multi-head atom \nattention block. In the atom attention block, three scaled inter-atomic matrices are individually added to each attention head’s weights as a bias \nterm. Finally, the learned atomic hidden states are aggregated to a molecular vector, concatenated with the molecular descriptors, then entered \ninto feed-forward layers for property prediction\nPage 6 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \ngeneric message-passing scheme described in the pre -\nvious section, the employment of bond attention has an \nadditional step of updating the hidden representation \n( ht\nvw  ) (Table 1: Bond Embedding Phase).\nAfter iterating through all the message-passing layers, \nthe message of each atom ( m v ) is obtained by aggregat -\ning all the adjacent bond hidden states that originated \nfrom it ( hT\nvw , w ∈ Neighbour(v) ) and concatenating them \nwith atom features, which are then transformed by a \nweight matrix ( W o ) and a ReLu activation. Here, we fur -\nther implement an atom-level Transformer block assisted \nwith three atom-wised matrices and a skip connection \nfrom the input to generate the hidden states for atoms \n(Table  1: Atom Embedding Phase). At the molecule \nembedding phase, all the learned atomic hidden states of \na molecule are summed together as a single representa -\ntion ( h ). The final output of the model is returned by a \ntwo-layer feed-forward neural (FFN) network that is fed \nwith the concatenation of the learned representation and \nthe calculated molecular descriptors (Table  1: Molecule \nEmbedding Phase).\nBond attention\nPrior to the scaled dot-product attention used in the \nTransformer network, the additive attention proposed \nby Bahdanau et al. [15] is known as the earliest attempt \nto use the attention mechanism in deep learning. Based \non the additive attention, Wu et  al. [22] proposed an \nefficient Transformer architecture, namely Fastformer, \nto mitigate the quadratic computational complexity in \nthe Transformer network. In general, instead of mod -\neling the interactions between each pair of units by dot-\nproduct of matrices, Fastformer uses additive attention \nto model global contexts and transform each token rep -\nresentation by its interaction with the global contexts. \nSince the MPNN framework contains T message-passing \niterations, adding the Transformer architecture in each \nmessage-passing layer is computationally expensive, \nespecially for architectures containing numerous layers \nto train large molecules. To this end, we adopt Fastformer \nas the building block for bond attention in our model.\nThe pseudo-code of the bond attention is shown \nin Additional file  1: Table  S3. Specifically, the bond \nTable 1 Algorithm of ABT-MPNN\nInitialization\n1) Given a molecular graph G , generate atom features xv and \nbond features evw\nwhere v ∈ Atom(G) and w ∈ Neighbor(v) ; three inter-atomic \nmatrices M Adjacency, M Distance, M Coulomb  ; molecular descriptors \nh f\n2) for each atom v in molecule G:\n3)     for each atom w in molecule Neighbor(v):\n4) h0\nvw ← ReLU(W iConcat(xv,evw ))\nBond Embedding Phase\n1) Message-passing iteration:t = 1, 2,... ,T\n2) while 1 ≤ t ≤ T :\n3)     for each atom v in molecule G:\n4)         for each atom w in molecule Neighbor(v):\n5) m t\nvw ← ∑\nk∈Neighbor(v) ht−1\nkv −ht−1\nwv\n6) bt\nvw ← BondAttention\n(\nm t\nvw\n)\n+m t\nvw\n7) ht\nvw ← ReLU(h0\nvw +W hbt\nvw )\nAtom Embedding Phase\n1) for each atom v in molecule G:\n2) m v ← ReLU(W oConcat(xv, ∑\nw ∈Neighbor(v) hT\nvw ))\n3) hv ← AtomAttention\n(\nm v, M Adjacency, M Distance, M Coulomb\n)\n+m v\nMolecule Embedding Phase\n1) h ← ∑\nv∈G hv\n2) ˆy ← FFN (Concat(h, hf))\nPage 7 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:29 \n \nattention block contains 6 attention heads and takes \nthe bond messages as input. Given a molecule with N \nbonds, the query, key, and value matrices are set equal \nto the input bond message matrix Hb ∈ R2N ×d , where \nd is the hidden dimension. Firstly, a global bond query \n( qb ) is obtained via the additive attention, in which \nan additive attention weight ( αb i ) of each bond vec -\ntor is calculated, multiplied by its corresponding bond \nquery vector ( qbi ) and summarized together. Next, the \ninteraction between the global bond query and the \nbond key vectors ( kbi ) is carried out by element-wise \nproducts. Similarly, a global bond key ( kb ) is obtained \nby conducting additive attention and is employed to \ntransform the bond value vectors by element-wise \nproducts. Lastly, the resulting key-value interaction \nvectors are projected, added with the bond queries \n( qbi ) through a skip connection, then normalized by \na layer normalization [23] to generate the final bond \nattention output Ob ∈ R2N ×d.\nAtom attention\nAt the atom embedding phase, we further construct a \nmulti-head self-attention layer on the aggregated atom \nvectors, allowing the model to focus more on atoms \nor local environments that are most relevant to the \ntarget properties. Instead of using additive attention, \nwe select the original Transformer network that uses \nscaled dot-product attention as our building block \nfor atom attention. The motivation for this choice \nis mainly due to the encapsulation of additional fea -\ntures. Concretely, due to the architectural constraints, \nmost graph-based networks only operate on molecu -\nlar graphs where atoms or bonds are embedded with \nfeature vectors containing corresponding chemical \nproperties. With the inclusion of scaled dot-product \nattention on atoms, our model can incorporate addi -\ntional graph-level features that contain information \non the spatial and electrostatic relationships between \npairs of atoms, thus providing a more comprehensive \nperspective from molecular topology during modeling.\nAs defined in Additional file  1: Table  S4, the atom \nattention layer with 6 attention heads takes the aggre -\ngated atom messages ( Ha ∈ RM ×d ) as input, where M \nis the number of atoms and d is the hidden dimension. \nFor each attention head, one type of additional inter-\natomic feature matrix is added to the query-key inter -\naction matrix as a bias term. Specifically, the head1 \nand head2 take the adjacency matrices of molecules \nas inputs, which incorporates the connectivity infor -\nmation of molecules into the model. The head3 and \nhead4 include the topological distances of atom pairs \nfrom the RDKit generated conformers to the attention \nweights. The head5 and head6 encapsulate the Cou -\nlomb matrix, which depicts the electrostatic interac -\ntion between atoms in the model. Before importing \nthem to the model, the feature matrices are normalized \nby Z-score normalization and scaled by /afii9838 , a hyperpa -\nrameter used in this architecture.\nExperimental settings\nBenchmark datasets and evaluation metrics\nAs an extension of our previous framework for mod -\neling large-scale chemical-genetic datasets, we con -\nducted the performance evaluation of the proposed \nABT-MPNN on the chemical-genetic interaction pro -\nfiles of drugs from Johnson et  al. [24], which include \n47,217 small molecules against hundreds of Mycobacte -\nrium tuberculosis mutant strains (named by the down -\nregulated gene). The growth inhibition property of a \nmolecule on each M. tuberculosis mutant strain was \ngauged by the statistical test (Z-score) obtained from \nthe experimental results [24]. The smaller the Z-score, \nthe more pronounced the growth inhibitory effect \nof the small molecule on the M. tuberculosis mutant \nstrain. We later clustered the chemical-genetic inter -\naction profiles in gene clusters by first identifying M. \ntuberculosis H37Rv homologs in Escherichia coli K12 \naccording to their gene products. Then, the semantic \ngene similarity of biological process for the homologs \nwere calculated and hierarchical clustering was per -\nformed [25]. After the gene-level clustering, 13 dis -\ntinct M. tuberculosis gene groups were formed, and \nthe target value for each gene cluster was obtained by \nfinding the median Z-score of the genes in that cluster. \nBesides training regression models with continuous \nZ-scores, we built binary classification tasks for each of \nthe 13 gene clusters with a class criterion equal to -4, \nwhere Z-score < − 4 was considered growth inhibitory \nor active (1), or otherwise inactive (0). For this data -\nset (Table  2), we employed a random split to divide the \ndata into subsets (training set, validation set, and test \nset) by the ratio of 80:10:10. Root mean squared error \n(RMSE) was used as the metric for regression and the \narea under the precision-recall curve (AUPRC) was \nused for classification since the binarized dataset is \nhighly imbalanced (the average percentage of positive \nlabels across clusters is 4%).\nIn addition, we conducted prediction of molecu -\nlar properties using 4 classification and 4 regres -\nsion molecular benchmarks from MoleculeNet [26] \n(Table  2). We followed the recommendations of Mol -\neculeNet [26] for selecting data split strategies and \nevaluation metrics, which were based on the content \nof each dataset and previous works. The Scaffold split \nwas employed on the HIV dataset, while the rest used \nPage 8 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \nrandom split as default. The area under the receiver \noperating characteristic curve (AUROC) was applied to \nthe 4 classification datasets. RMSE was calculated for \nregression tasks on ESOL, FreeSolv, and Lipophilicity, \nwhile mean absolute error (MAE) was applied to QM8.\nBaseline models\nWe performed comparative evaluations of ABT-MPNN \nagainst 6 baseline methods covering shallow and deep \nML architectures. These include (1) Random forest \n(RF) [27] with binary Morgan fingerprints as inputs; \n(2) feed-forward network (FFN) trained with normal -\nized chemical descriptors. As our model was derived \nfrom the MPNN framework, we also reported the per -\nformance of (3) the message-passing neural network \n(MPNN) [8 ] and (4) the directed message-passing neu -\nral network (D-MPNN) [19] in the results. Additinoally, \nwe compared our model with two other state-of-the-art \ngraph neural networks: (5) DeeperGCN [28] and (6) \ngeometry-enhanced molecular representation learning \nmethod (GEM) [29], to demonstrate the power of our \nproposed approach.\nImplementation details\nThe RF was implemented with 500 trees based on binary \nMorgan fingerprints ( r = 2 ; bits = 2048 ). The FFN con -\ntained a dense layer with 1400 neurons before the output \nlayer and was fed with 200 normalized chemical descrip -\ntors. To improve models’ performance, the hyperparam -\neters of models were optimized by Bayesian optimization \n[30] with the same optimization budget (30 epochs in \n20 iterations) on the same data split. For our proposed \nmodel, we optimized the four hyper-parameters listed in \nTable 3.\nThe models were optimized with the Adam opti -\nmizer, and the optimum parameters were determined \nas the ones with the highest performance score on the \nvalidation set during training. We employed a fivefold \ncross-validation (CV) on the partitioned data splits and \nreported the mean and standard deviation of the metrics. \nThe ABT-MPNN used PyTorch [31] as the deep learning \nframework and was developed based on the Chemprop \npackage by Yang et al. [32].\nResults and discussion\nPerformance comparison with baselines\nWe compared our proposed ABT-MPNN with 6 base -\nline models on 10 classification and regression tasks, \ncovering chemical-genetic interaction profiles (Johnson \net al. [24, 25]) and a wide range of molecular properties \nin the field of quantum mechanics (QM8 [33]), physical \nchemistry (ESOL [34], lipophilicity [35], hydration free \nenergies (Freesolv [36]), biophysics (HIV [26]), and physi-\nology (Tox21 [37], Clintox [38], ToxCast [39]). The over -\nall performance of a model on each dataset is represented \nTable 2 The summary of the selected molecular datasets\nTask type Dataset No. tasks No. compounds Data split Metric\nClassification Johnson et. al 13 47,217 Random AUPRC\nTox21 12 7,831 Random AUROC\nClinTox 2 1,478 Random AUROC\nToxCast 617 8,576 Random AUROC\nHIV 1 41,127 Scaffold AUROC\nRegression Johnson et. al 13 47,217 Random RMSE\nQM8 12 21,786 Random MAE\nESOL 1 1128 Random RMSE\nFreeSolv 1 642 Random RMSE\nLipophilicity 1 4,200 Random RMSE\nTable 3 Bayesian Optimization for Hyperparameters in ABT-\nMPNN\nHyperparameters Values\nMessage-passing iteration (T) 2, 3, 4, 5, 6\nInter-atomic feature scaler (λ) [0, 0.5] (Interval: 0.05)\nHidden dimension (d) [300, 2400] (Interval: 100)\nDropout probability (p) [0, 0.4] (Interval: 0.05)\nPage 9 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:29 \n \nas the mean ± standard deviation of the evaluation met -\nrics across a fivefold CV, as shown in Table  4. From the \nresults, ABT-MPNN achieved the best performance on \nall classification datasets, except on Tox21 where GEM \nprovided the leading performance. Specifically, the John -\nson et  al. (classification) dataset achieved 4.98% perfor -\nmance increase compared to the second-best model \nD-MPNN. According to Clintox, ToxCast, and HIV, \nABT-MPNN obtained 1.01%, 0.40% and 0.75% relative \nimprovements compared to the second-ranked model, \nrespectively. For classification, the result of RF on the \nToxCast dataset is omitted due to high computational \ncosts with 617 individual tasks.\nRegarding regression tasks, we observed that the \nABT-MPNN model achieved substantial improve -\nments over classification, as it consistently outper -\nformed all baseline models according to the results of \nthe fivefold CV. The outstanding performance of ABT-\nMPNN on regression datasets could be associated with \nthe modeling of inter-atomic attention with topologi -\ncal and electrostatic features, as regression tasks focus \non linking quantum chemical properties to molecular \nstructures, in which such information is of high rele -\nvance. In regression tasks of the Johnson et al. dataset, \nthe ABT-MPNN model improved upon D-MPNN by a \nmodest margin of 0.15%, and it boosted the results of \nQM8 with a 10% relative MAE optimization compared \nto MPNN, D-MPNN and GEM. Moreover, our model \nyielded superior results in RMSE compared to the sec -\nond-best baselines on ESOL (1.57%), Freesolv (1.42%), \nand Lipophilicity (0.72%), respectively.\nOverall, ABT-MPNN achieved state-of-the-art \nresults on 9 out of 10 classification and regression \ntasks according to the fivefold CV, showing the robust -\nness of the molecular representation learned by our \nmodel. The superior performances across multiple \ndatasets compared to D-MPNN further support the \neffectiveness of complementing the directed mes -\nsage-passing scheme with the bond and atomic level \nattention.\nAblation study\nTo validate the impact and contribution of each compo -\nnent to the performance of the proposed ABT-MPNN, \nwe conducted a series of ablation studies on both clas -\nsification (ClinTox) and regression (ESOL) datasets from \nour benchmarks. For each run, we kept the same hyper-\nparameter settings, and the performance was evaluated \non the same fivefold CV, as is shown in Table 5. To better \nevaluate the results, Additional file  1: Fig. S1 shows the \nscore for each ablation experiment on individual fold. \nFollowing the architecture design of the ABT-MPNN, \nwe focused on investigating two key components of our \nmodel: bond attention and atom attention.\nTable 4 The performance comparison for classification and regression tasks\na The evaluation metrics are represented as averaged values ± standard deviation from fivefold CV. The best performance values are highlighted in bold\nb The results of RF on ToxCast are not presented because of the substantial computational cost\nClassification (the higher the better)a\nJohnson et al Tox21 Clintox ToxCast HIV\nRF 0.252 ± 0.014 0.818 ± 0.005 0.721 ± 0.088 _b 0.798 ± 0.040\nFFN 0.258 ± 0.015 0.837 ± 0.010 0.837 ± 0.062 0.738 ± 0.009 0.803 ± 0.045\nMPNN 0.258 ± 0.013 0.859 ± 0.011 0.873 ± 0.051 0.752 ± 0.010 0.788 ± 0.050\nD-MPNN 0.281 ± 0.028 0.855 ± 0.015 0.895 ± 0.037 0.749 ± 0.013 0.788 ± 0.039\nDeeper GCN 0.272 ± 0.022 0.853 ± 0.013 0.870 ± 0.042 0.751 ± 0.010 0.789 ± 0.031\nGEM 0.280 ± 0.018 0.864 ± 0.010 0.825 ± 0.091 0.757 ± 0.013 0.769 ± 0.038\nABT-MPNN 0.295 ± 0.021 0.857 ± 0.010 0.904 ± 0.034 0.760 ± 0.013 0.809 ± 0.036\nRegression (the lower the better)a\nJohnson et al ESOL Lipophilicity Freesolv QM8\nRF 1.315 ± 0.021 1.230 ± 0.066 0.846 ± 0.039 2.467 ± 0.570 0.014 ± 0.000\nFFN 1.321 ± 0.016 0.614 ± 0.109 0.674 ± 0.043 1.275 ± 0.352 0.016 ± 0.000\nMPNN 1.309 ± 0.017 0.575 ± 0.086 0.585 ± 0.044 1.042 ± 0.220 0.010 ± 0.000\nD-MPNN 1.307 ± 0.024 0.594 ± 0.066 0.558 ± 0.044 0.915 ± 0.142 0.010 ± 0.000\nDeeper GCN 1.325 ± 0.015 0.601 ± 0.056 0.580 ± 0.035 0.970 ± 0.368 0.012 ± 0.000\nGEM 1.315 ± 0.021 0.632 ± 0.062 0.599 ± 0.035 0.962 ± 0.257 0.010 ± 0.000\nABT-MPNN 1.305 ± 0.017 0.566 ± 0.075 0.554 ± 0.041 0.902 ± 0.157 0.009 ± 0.000\nPage 10 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \nEffect of bond attention in the message‑passing phase\nOne of the most important distinctions between ABT-\nMPNN and previous works is the integration of bond-\nlevel attention during the message-passing phase. In \nABT-MPNN, we chose Fastformer [22] as the building \nblock of the bond attention, given that it uses additive \nattention to model the global bond context, enabling \neffective representational modeling while mitigating high \ncomputational complexity. To verify the expressive power \nof the Fastformer approach, we also implemented Trans -\nformer in the message-passing phase as the bond atten -\ntion block and conducted experiments #1, #2 and #3 for \ncomparison (Table  5). From the experiments, the bond \nattention scheme improved the performance of base -\nline #1, which does not apply bond attention except for \nthe inclusion of Transformer, which slightly reduced the \nperformance of the classification. Regarding individual \nfolds of the ClinTox dataset (Additional file  1: Fig. S1), \nthe employment of bond attention improved or achieved \non-par performance compared to baseline #1, except for \nfold 2 and fold 3. In comparison between two types of \nattention mechanism, Fastformer exceeded Transformer \non three folds but Transformer got the highest AUROC \nscore among all the experiments on fold 1. Regarding \nregression, both Transformer and Fastformer considera -\nbly enhanced the performance in general. Specifically, the \nbond-level attention, regardless of the architecture of the \nattention block, consistently improved the baseline on \nfour data folds. Between the two attention architectures, \nTransformer achieved a modestly better performance \nthan Fastformer. Possibly, the scaled dot product atten -\ntion models a better bond-level representation in specific \nregression tasks than the additive attention developed in \nFastformer. However, considering the superior perfor -\nmance of Fastformer on classification tasks and linear \ncomplexity of computing attention, we chose Fastformer \nas the building block of bond attention in ABT-MPNN.\nContribution of atom attention and inter‑atomic features\nAs introduced in the methods section, we constructed \natom-level self-attention with Transformer and incor -\nporated additional adjacency, distance, and coulomb \nmatrices into each attention head as auxiliaries. We ana -\nlyzed the choices and optimizations at the atom embed -\nding phase (Table  5) in experiments #4, #5 and baseline \n#1. Notably, the model showed marked improvements \nafter employing atom attention, regardless of whether \nthe additional inter-atomic matrices were included. This \nfinding implies that the atomic-level self-attention facili -\ntates representation learning by assigning more attention \n(weights) to the atoms or molecular functional groups \nthat contribute to the property of interest. We will fur -\nther examine this in our discussion section. Interest -\ningly, through comparison of #4 and #5, the inter-atomic \nmatrices increased the AUROC score of Clintox while \non ESOL the model with this setting was insensitive to \nthe topological and electrostatic information as they \nachieved identical RMSE scores. From the observation in \nAdditional file 1: Fig. S1, the differences in the exclusion/\ninclusion of inter-atomic matrices on the regression task \nare marginal, in which two folds (3, 4) obtained smaller \nRMSE scores with the three inter-atomic matrices while \nthree folds (1, 2, 5) did not.\nCombination of bond attention and atom attention\nFinally, we evaluated the effect of using bond attention \nand atom attention to justify our architecture design. \nSpecifically, model #7 (Table  5) is the complete ABT-\nMPNN where it incorporates Fastformer-based bond \nattention and Transformer-based atom attention with \ninter-atomic features. In contrast to #7, the inter-atomic \nfeatures in experiment #6 were excluded. Comparing #6 \nwith experiments #3 and #4, in which bond and atomic \nlevel attentions were employed separately, we observed \nthat combining attentions at the atomic and bond \nTable 5 Ablation study results on classification (ClinTox) and regression (ESOL) tasks\nThe evaluationmetrics are represented as averaged values ± standard deviation from fivefold CV. The best performance valuesare highlighted in bold\nNo Bond attention Atom attention Classification\n(ClinTox)\nRegression\n(ESOL)\nTransformer Fastformer No inter‑atomic \nmatrices\nWith inter‑atomic \nmatrices\n1 0.890 ± 0.040 0.582 ± 0.070\n2 √ 0.887 ± 0.044 0.570 ± 0.070\n3 √ 0.894 ± 0.042 0.573 ± 0.066\n4 √ 0.896 ± 0.035 0.569 ± 0.065\n5 √ 0.905 ± 0.041 0.569 ± 0.065\n6 √ √ 0.905 ± 0.028 0.567 ± 0.066\n7 √ √ 0.904 ± 0.034 0.566 ± 0.075\nPage 11 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:29 \n \nlevels boosted the performance in both classification and \nregression. With respect to each fold, the combination of \nbond attention and atom attention (#6) resulted in a sub -\nstantial increase in AUROC scores for folds 1, 2, and 4. \nFor regression, although experiment #6 exhibited better \nmean RMSE than #3 and #4, the advantage of adopting \natom and bond level attention together was not as pro -\nnounced as for classification due to the large deviation \nof results on the CV folds. From experiments #6 and \n#7, although the inclusion of the inter-atomic matrices \nmarginally reduced the performance of classification \nby 0.11%, it further optimized the results of regression \nFig. 2 Visualization of the multi-head atom attention weights of the three M. tuberculosis growth inhibitors. In the predicted probability maps, \natoms with positive contributions are colored in green, while red indicates that the corresponding attention weight is negative. The larger the \nabsolute value, the darker the color shown on the map\nPage 12 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \nand achieved the best results among all the ablation \nexperiments.\nInterpretability and visualization\nBesides assessing the model’s performance, it is often \nbeneficial to look into the “black box” of the trained \nmodel and have a deeper understanding of which sub -\nstructures of molecules contribute more to the com -\npound activities/properties. With the interpretability of \nattention weights of atoms, it is possible to investigate the \nlatent linkage between the molecular substructure and \nthe predicted outcomes. Here, we visualize the atomic \nattention weights using the similarity map [16] (or pre -\ndicted probability map in this case) implemented in \nRDKit.\nFigure  2 visualizes the attention weights of different \nheads of three examples of anti-M. tuberculosis investiga-\ntional drugs (Octoclothepin [40]; Amsacrine [41]; Com -\npound 14_palencia [42]) curated from the study [25]. The \ncompounds were chosen on the basis of 1) whole-cell \ninhibitory activity against wild-type M. tuberculosis or M. \nsmegmatis and 2) biochemical validation of the molecular \ntargets. Specifically, Nisa et  al. [40] reported that Octo -\nclothepin, an antipsychotic of the tricyclic group, exhib -\nited inhibition of the in  vitro ATPase activity of ParA \nfrom M. tuberculosis. Amsacrine is an antineoplastic \nagent that has been shown to inhibit mycobacterial TopA, \nthe essential topoisomerase I involved in mycobacterial \ncell viability [41]. Compound 14, a potent M. tuberculo -\nsis protein synthesis inhibitor [42], can form adducts with \nAMP and together bind the ATPase pocket to inhibit \nthe LeuS gene. Since we added an adjacency matrix to \nthe head1 and head2 , a distance matrix to the head3 and \nhead4 , and a Coulomb matrix to the head5 and head6 , \nwe followed this paradigm and visualized their averaged \nattention weights on rows 2–4 of the Fig.  2. The overall \nattention weights of the 6 attention heads are displayed in \nthe last row.\nFirst of all, we observe that the atom attention layer \nonly focuses on a few atoms or substructures of the \nmolecule and different attention branches have dif -\nferent “views” of the input. For instance, the weights \nin the head1& head2 of Octoclothepin focus more on \nthe chlorine ( Cl : #18 ) atom, while one of the nitrogen \natoms ( N : #4 ) is assigned more attention weights in the \nhead5& head6 . This observation demonstrates that the \nmulti-head attention can give the architecture multiple \nsubspaces to model the molecular representation regard -\nless of training with the same input molecule. In addi -\ntion, it is notable that most carbon ( C ) atoms of the three \ninhibitors gain attention values near zero, while green \nareas usually appear on the halogens or chalcogens that \nthe inhibitors uniquely have. Furthermore, we observe \nthat the attention mechanism of the ABT-MPNN facili -\ntates the representation learning to the molecular func -\ntional groups. For instance, the results of Amsacrine \ndemonstrate that all attention heads have emphasized the \nsulfonamide to varying degrees. Therefore, it is reasona -\nble to speculate that the inhibitory capacity of Amsacrine \nagainst M. tuberculosis might be associated with its sul -\nfonamide functional group, in agreement with the sug -\ngested interaction between the sulfonamide moiety and \nthe mycobacterial topoisomerase I TopA [41].\nConclusion\nIn this study, we proposed a novel message-passing \nframework called ABT-MPNN that incorporates both \nadditive attention and scaled dot-product attention at \nthe bond and atomic levels, respectively. To incorpo -\nrate the topological and electrostatic information of \nmolecules into the model, we further designed a fea -\nture engineering scheme that embedded adjacency, \ndistance and Coulomb matrices derived from molecu -\nlar conformations with each atom attention head.\nOverall, our proposed model consistently outperformed \nor is comparable with the state-of-the-art baseline mod -\nels on a wide range of molecular datasets. By introduc -\ning the attention schemes at the atomic level, we realized \nthe visualization modality of the model via the predicted \nprobability map. Through the demonstration of the three \nM. tuberculosis inhibitors, we highlighted the effect of self-\nattention on chemical substructures and functional groups \nduring molecular representation learning, which not only \nincreases the interpretability of the MPNN but also serves \nas a valuable way to investigate the mechanism of action.\nAbbreviations\nABT-MPNN   Atom-bond Transformer-based message-passing \nneural network\nAUPRC   Area under the curve of the precision-recall cure\nAUROC   Area under the curve of the receiver operating char-\nacteristic curve\nCV   Cross-validation\nD-MPNN   Directed message-passing neural network\nFFN   Feed-forward network\nGCN   Graph convolutional neural network\nML   Machine learning\nGEM   Geometry-enhanced molecular representation learn-\ning method\nMAE   Mean absolute error\nMOA   Mechanism of action\nMPNN   Message-passing neural networks\nM. tuberculosis  Mycobacterium tuberculosis\nQSAR   Quantitative structure–activity relationship\nQSPR   Quantitative structure–property relationship\nRF   Random forest\nRMSE   Root mean squared error\nSMILES   Simplified molecular input line entry system\nPage 13 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:29 \n \nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13321- 023- 00698-9.\nAdditional file 1: Table S1. Atom and bond features. Table S2. 200 \nMolecular descriptors generated by RDKit. Table S3. Algorithm of Bond \nAttention. Table S4. Algorithm of Atom Attention. Fig. S1. Comparison of \nablation experiments using 5-fold cross-validation (A) Performance evalua-\ntion of each fold for the classification task (ClinTox) measured with AUROC. \nExperiments settings: #1: baseline; #2: use bond attention (Transformer); \n#3: use bond attention (Fastformer); #4 use atom attention; #5 use atom \nattention with inter-atomic matrices #6 use bond attention (Fastformer) \nand atom attention; #7 use bond attention (Fastformer) and atom atten-\ntion with inter-atomic matrices (B) Performance evaluation of each fold for \nthe regression task (ESOL) measured by RMSE. The settings of each experi-\nment in the regression task are identical to those in the classification one.\nAcknowledgements\nNot Applicable.\nAuthor contributions\nConceptualization: CL, PH. Data curation: CL, YS. Methodology: CL, PH, SC, RD. \nData analysis: CL, YS. Validation: CL, YS. Software: CL. Supervision: PH, SC, RD. \nFunding acquisition: SC, RD and PH. Initial draft: CL, YS. Final manuscript: CL, \nPH, YS, SC, RD. All authors read and approved the final manuscript.\nFunding\nSC, RD and PH are supported by a CIHR project grant and a Cystic Fibrosis \nCanada Research Grant. PH is supported by the Canada Research Chairs Tier II \nProgram. PH is the holder of a Manitoba Medical Services Foundation (MMSF) \nAllen Rouse Basic Science Career Development Research Award.\nAvailability of data and materials\nThe raw data from the Johnson et al. study is publicly accessible on the \nwebsite: https:// www. chemi calge nomic softb. com/. The scripts, datasets, \nand results supporting the conclusions of this article are available in the sup-\nplementary materials and our GitHub repository: https:// github. com/ LCY02/ \nABT- MPNN.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 8 November 2022   Accepted: 10 February 2023\nReferences\n 1. Zhong F, Xing J, Li X et al (2018) Artificial intelligence in drug \ndesign. Sci China Life Sci 61:1191–1204. https:// doi. org/ 10. 1007/ \ns11427- 018- 9342-2\n 2. Mak K-K, Pichika MR (2019) Artificial intelligence in drug development: \npresent status and future prospects. Drug Discov Today 24:773–780. \nhttps:// doi. org/ 10. 1016/j. drudis. 2018. 11. 014\n 3. Stokes JM, Yang K, Swanson K et al (2020) A deep learning approach \nto antibiotic discovery. Cell 180:688-702.e13. https:// doi. org/ 10. 1016/j. \ncell. 2020. 01. 021\n 4. Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, et al (2015) Con-\nvolutional Networks on Graphs for Learning Molecular Fingerprints. \narXiv:150909292 [cs, stat]\n 5. Kearnes S, McCloskey K, Berndl M et al (2016) Molecular graph \nconvolutions: moving beyond fingerprints. J Comput Aided Mol Des \n30:595–608. https:// doi. org/ 10. 1007/ s10822- 016- 9938-8\n 6. Zhou J, Cui G, Zhang Z, et al. (2019). Graph Neural Networks: A Review \nof Methods and Applications. arXiv:181208434 [cs, stat]\n 7. Wu Z, Pan S, Chen F et al (2021) A comprehensive survey on graph \nneural networks. IEEE Trans Neural Netw Learning Syst 32:4–24. https://  \ndoi. org/ 10. 1109/ TNNLS. 2020. 29783 86\n 8. Gilmer J, Schoenholz SS, Riley PF, et al (2017) Neural Message Passing \nfor Quantum Chemistry. arXiv:170401212 [cs]\n 9. Vaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. \nIn: Advances in neural information processing systems. pp 5998–6008\n 10. Tang B, Kramer ST, Fang M et al (2020) A self-attention based message \npassing neural network for predicting molecular lipophilicity and \naqueous solubility. J Cheminform 12:1–9\n 11. Maziarka Ł, Danel T, Mucha S, et al (2020) Molecule attention trans-\nformer. arXiv preprint arXiv:200208264\n 12. Ying C, Cai T, Luo S, et al (2021) Do Transformers Really Perform Bad for \nGraph Representation? arXiv preprint arXiv:210605234\n 13. Xiong Z, Wang D, Liu X et al (2019) Pushing the boundaries of \nmolecular representation for drug discovery with the graph attention \nmechanism. J Med Chem 63:8749–8760\n 14. Chuang KV, Keiser MJ (2020) Attention-Based Learning on Molecular \nEnsembles. arXiv preprint arXiv:201112820\n 15. Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation by \njointly learning to align and translate. arXiv preprint arXiv:14090473\n 16. Riniker S, Landrum GA (2013) Similarity maps—a visualization strategy \nfor molecular fingerprints and machine-learning methods. J Chemin-\nform 5:43. https:// doi. org/ 10. 1186/ 1758- 2946-5- 43\n 17. David L, Thakkar A, Mercado R, Engkvist O (2020) Molecular represen-\ntations in AI-driven drug discovery: a review and practical guide. J \nCheminform 12:56. https:// doi. org/ 10. 1186/ s13321- 020- 00460-5\n 18. Rupp M, Tkatchenko A, Müller K-R, Von Lilienfeld OA (2012) Fast and \naccurate modeling of molecular atomization energies with machine \nlearning. Phys Rev Lett 108:058301\n 19. Yang K, Swanson K, Jin W et al (2019) Analyzing learned molecular rep -\nresentations for property prediction. J Chem Inf Model 59:3370–3388. \nhttps:// doi. org/ 10. 1021/ acs. jcim. 9b002 37\n 20. Dosovitskiy A, Beyer L, Kolesnikov A, et al (2020) An image is worth \n16x16 words: Transformers for image recognition at scale. arXiv pre -\nprint arXiv:201011929\n 21. Jumper J, Evans R, Pritzel A et al (2021) Highly accurate protein struc-\nture prediction with AlphaFold. Nature 596:583–589. https:// doi. org/  \n10. 1038/ s41586- 021- 03819-2\n 22. Wu C, Wu F, Qi T, et al (2021) Fastformer: Additive Attention Can Be All \nYou Need. arXiv preprint arXiv:210809084\n 23. Ba JL, Kiros JR, Hinton GE (2016) Layer normalization. arXiv preprint \narXiv:160706450\n 24. Johnson EO, LaVerriere E, Office E et al (2019) Large-scale chemical–\ngenetics yields new M. tuberculosis inhibitor classes. Nature 571:72–78. \nhttps:// doi. org/ 10. 1038/ s41586- 019- 1315-z\n 25. Liu C, Hogan AM, Sturm H et al (2022) Deep learning-driven predic-\ntion of drug mechanism of action from large-scale chemical-genetic \ninteraction profiles. J Cheminform 14:12. https:// doi. org/ 10. 1186/ \ns13321- 022- 00596-6\n 26. Wu Z, Ramsundar B, Feinberg EN et al (2018) MoleculeNet: a bench-\nmark for molecular machine learning. Chem Sci 9:513–530\n 27. Ho TK (1995) Random decision forests. In: Proceedings of 3rd inter -\nnational conference on document analysis and recognition. IEEE, pp \n278–282\n 28. Li G, Xiong C, Thabet A, Ghanem B (2020) Deepergcn: All you need to \ntrain deeper gcns. arXiv preprint arXiv:200607739\n 29. Fang X, Liu L, Lei J et al (2022) Geometry-enhanced molecular repre -\nsentation learning for property prediction. Nat Mach Intell 4:127–134\n 30. Snoek J, Larochelle H, Adams RP (2012) Practical bayesian optimiza-\ntion of machine learning algorithms. Advances in neural information \nprocessing systems 25:\n 31. Paszke A, Gross S, Chintala S, et al (2017) Automatic differentiation in \nPyTorch\n 32. Yang K, Swanson K, Jin W, et al (2019) chemprop: Message Passing \nNeural Networks for Molecule Property Prediction\n 33. Ramakrishnan R, Hartmann M, Tapavicza E, Von Lilienfeld OA (2015) \nElectronic spectra from TDDFT and machine learning in chemical \nspace. J Chem Phys 143:084111\n 34. Delaney JS (2004) ESOL: estimating aqueous solubility directly from \nmolecular structure. J Chem Inf Comput Sci 44:1000–1005\nPage 14 of 14Liu et al. Journal of Cheminformatics           (2023) 15:29 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 35. Gaulton A, Bellis LJ, Bento AP et al (2012) ChEMBL: a large-scale bioac-\ntivity database for drug discovery. Nucleic Acids Res 40:D1100–D1107\n 36. Mobley DL, Guthrie JP (2014) FreeSolv: a database of experimental and \ncalculated hydration free energies, with input files. J Comput Aided \nMol Des 28:711–720\n 37. Huang R, Xia M, Nguyen D-T et al (2016) Tox21Challenge to build \npredictive models of nuclear receptor and stress response pathways \nas mediated by exposure to environmental chemicals and drugs. Front \nEnviron Sci 3:85\n 38. Gayvert KM, Madhukar NS, Elemento O (2016) A data-driven approach \nto predicting successes and failures of clinical trials. Cell Chem Biol \n23:1294–1301\n 39. Richard AM, Judson RS, Houck KA et al (2016) ToxCast chemical land-\nscape: paving the road to 21st century toxicology. Chem Res Toxicol \n29:1225–1251\n 40. Nisa S, Blokpoel MCJ, Robertson BD et al (2010) Targeting the chromo -\nsome partitioning protein ParA in tuberculosis drug discovery. J Anti-\nmicrob Chemother 65:2347–2358. https:// doi. org/ 10. 1093/ jac/ dkq311\n 41. Szafran MJ, Kołodziej M, Skut P et al (2018) Amsacrine derivatives selec-\ntively inhibit mycobacterial topoisomerase I (TopA). impair M. smegma-\ntis growth and disturb chromosome replication. Front Microbiol 9:1592\n 42. Palencia A, Li X, Bu W et al (2016) Discovery of novel oral protein synthesis \ninhibitors of Mycobacterium tuberculosis that target leucyl-tRNA syn-\nthetase. Antimicrob Agents Chemother 60:6271–6280. https:// doi. org/ 10. \n1128/ AAC. 01339- 16\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7559223175048828
    },
    {
      "name": "Interpretability",
      "score": 0.727885365486145
    },
    {
      "name": "Message passing",
      "score": 0.6765735745429993
    },
    {
      "name": "Molecular graph",
      "score": 0.5652033686637878
    },
    {
      "name": "Theoretical computer science",
      "score": 0.48150333762168884
    },
    {
      "name": "Embedding",
      "score": 0.45648548007011414
    },
    {
      "name": "Artificial neural network",
      "score": 0.4425588548183441
    },
    {
      "name": "Graph embedding",
      "score": 0.42267870903015137
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4210798144340515
    },
    {
      "name": "Graph",
      "score": 0.3993963599205017
    },
    {
      "name": "Distributed computing",
      "score": 0.16689661145210266
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I46247651",
      "name": "University of Manitoba",
      "country": "CA"
    }
  ],
  "cited_by": 45
}