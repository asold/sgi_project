{
  "title": "Glance-and-Gaze Vision Transformer",
  "url": "https://openalex.org/W3167597877",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4283440896",
      "name": "Yu, Qihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3043001498",
      "name": "Xia Yingda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162282092",
      "name": "Bai, Yutong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753263455",
      "name": "Lu Yongyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893462076",
      "name": "Yuille, Alan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115343774",
      "name": "Shen Wei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2023053047",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2884822772"
  ],
  "abstract": "Recently, there emerges a series of vision Transformers, which show superior performance with a more compact model size than conventional convolutional neural networks, thanks to the strong ability of Transformers to model long-range dependencies. However, the advantages of vision Transformers also come with a price: Self-attention, the core part of Transformer, has a quadratic complexity to the input sequence length. This leads to a dramatic increase of computation and memory cost with the increase of sequence length, thus introducing difficulties when applying Transformers to the vision tasks that require dense predictions based on high-resolution feature maps. In this paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer (GG-Transformer), to address the aforementioned issues. It is motivated by the Glance and Gaze behavior of human beings when recognizing objects in natural scenes, with the ability to efficiently model both long-range dependencies and local context. In GG-Transformer, the Glance and Gaze behavior is realized by two parallel branches: The Glance branch is achieved by performing self-attention on the adaptively-dilated partitions of the input, which leads to a linear complexity while still enjoying a global receptive field; The Gaze branch is implemented by a simple depth-wise convolutional layer, which compensates local image context to the features obtained by the Glance mechanism. We empirically demonstrate our method achieves consistently superior performance over previous state-of-the-art Transformers on various vision tasks and benchmarks. The codes and models will be made available at https://github.com/yucornetto/GG-Transformer.",
  "full_text": "Glance-and-Gaze Vision Transformer\nQihang Yu1, Yingda Xia1, Yutong Bai1, Yongyi Lu1, Alan Yuille1, Wei Shen2\n1 The Johns Hopkins University 2 Shanghai Jiaotong University\nAbstract\nRecently, there emerges a series of vision Transformers, which show superior\nperformance with a more compact model size than conventional convolutional\nneural networks, thanks to the strong ability of Transformers to model long-range\ndependencies. However, the advantages of vision Transformers also come with a\nprice: Self-attention, the core part of Transformer, has a quadratic complexity to\nthe input sequence length. This leads to a dramatic increase of computation and\nmemory cost with the increase of sequence length, thus introducing difﬁculties\nwhen applying Transformers to the vision tasks that require dense predictions based\non high-resolution feature maps.\nIn this paper, we propose a new vision Transformer, named Glance-and-Gaze Trans-\nformer (GG-Transformer), to address the aforementioned issues. It is motivated\nby the Glance and Gaze behavior of human beings when recognizing objects in\nnatural scenes, with the ability to efﬁciently model both long-range dependencies\nand local context. In GG-Transformer, the Glance and Gaze behavior is realized by\ntwo parallel branches: The Glance branch is achieved by performing self-attention\non the adaptively-dilated partitions of the input, which leads to a linear complexity\nwhile still enjoying a global receptive ﬁeld; The Gaze branch is implemented by a\nsimple depth-wise convolutional layer, which compensates local image context to\nthe features obtained by the Glance mechanism. We empirically demonstrate our\nmethod achieves consistently superior performance over previous state-of-the-art\nTransformers on various vision tasks and benchmarks. The codes and models will\nbe made available at https://github.com/yucornetto/GG-Transformer.\n1 Introduction\nConvolution Neural Networks (CNNs) have been dominating the ﬁeld of computer vision, which\nhave been a de-facto standard and achieved tremendous success in various tasks, e.g., image clas-\nsiﬁcation [16], object detection [15], semantic segmentation [5], etc. CNNs model images from a\nlocal-to-global perspective, starting with extracting local features such as edges and textures, and\nforming high-level semantic concepts gradually. Although CNNs prove to be successful for various\nvision tasks, they lack the ability to globally represent long-range dependencies. To compensate\na global view to CNN, researchers explored different methods such as non-local operation [ 36],\nself-attention [33], Atrous Spatial Pyramid Pooling (ASPP) [5].\nRecently, another type of networks with stacked Transformer blocks emerged. Unlike CNNs,\nTransformers naturally learn global features in a parameter-free manner, which makes them stronger\nalternatives and raises questions about the necessity of CNNs in vision systems. Since the advent\nof Vision Transformer (ViT) [12], which applied Transformers to vision tasks by projecting and\ntokenizing natural images into sequences, various improvements have been introduced rapidly,e.g.,\nbetter training and distillation strategies [32], tokenization [41], position encoding [7], local feature\nlearning [14]. Moreover, besides Transformers’ success on image classiﬁcation, many efforts have\nbeen made to explore Transformers for various down-stream vision tasks [35, 24, 13, 3, 46].\nPreprint. Under review.\narXiv:2106.02277v1  [cs.CV]  4 Jun 2021\nNevertheless, the advantages of Transformers come at a price. Since self-attention operates on the\nwhole sequences, it incurs much more memory and computation costs than convolution, especially\nwhen it comes to natural images, whose lengths are usually much longer than word sequences, if\ntreating each pixel as a token . Therefore, most existing works have to adopt a compromised strategy\nto embed a large image patch for each token, although treating smaller patches for tokens leads\nto a better performance (e.g., ViT-32 compared to ViT-16 [12]). To address this dilemma, various\nstrategies have been proposed. For instance, Pyramid Vision Transformer (PVT) [35] introduced a\nprogressive shrinking pyramid to reduce the sequence length of the Transformer with the increase of\nnetwork depth, and adopted spatial-reduction attention, where key and value in the attention module\nare down-sampled to a lower resolution. Swin-Transformer [24] also adopted the pyramid structure,\nand further proposed to divide input feature maps into different ﬁx-sized local windows, so that\nself-attention is computed within each window, which reduces the computation cost and makes it\nscalable to large image scales with linear complexity.\nNonetheless, we notice that these strategies have some limitations: Spatial-reduction attention can\nreduce memory and computation costs to learn high-resolution feature maps, yet with a price of losing\ndetails which are expected from the high-resolution feature maps. Adopting self-attention within\nlocal windows is efﬁcient with linear complexity, but it sacriﬁces the most signiﬁcant advantage of\nTransformers in modeling long-range dependencies.\nTo address these limitations, we propose Glance-and-Gaze Transformer (GG-Transformer), in-\nspired by the Glance-and-Gaze human behavior when recognizing objects in natural scenes [11],\nwhich takes advantage of both the long-range dependency modeling ability of Transformers and\nlocality of convolutions in a complementary manner. A GG-Transformer block consists of two\nparallel branches: A Glance branch performs self-attention within adaptively-dilated partitions of\ninput images or feature maps, which preserves the global receptive ﬁeld of the self-attention operation,\nmeanwhile reduces its computation cost to a linear complexity as local window attention [24] does;\nA Gaze branch compensates locality to the features obtained by the Glance branch, which is imple-\nmented by a light-weight depth-wise convolutional layer. A merging operation ﬁnally re-arranges the\npoints in each partition to their original locations, ensuring that the output of the GG-Transformer\nblock has the same size as the input. We evaluate GG-Transformer on several vision tasks and\nbenchmarks including image classiﬁcation on ImageNet [10], object detection on COCO [23], and\nsemantic segmentation on ADE20K [48], and show its efﬁciency and superior performance, compared\nto previous state-of-the-art Transformers.\n2 Related Work\nCNN and self-attention. Convolution has been the basic unit in deep neural networks for computer\nvision problems. Since standard CNN blocks were proposed in [22], researchers have been working\non designing stronger and more efﬁcient network architectures, e.g., VGG [30], ResNet [16], Mo-\nbileNet [29], and EfﬁcientNet [31]. In addition to studying how to organize convolutional blocks into\na network, several variants of the convolution layer have also been proposed, e.g., group convolu-\ntion [21], depth-wise convolution [6], and dilated convolution [40]. With the development of CNN\narchitectures, researchers also seeked to improve contextual representation of CNNs. Representative\nworks, such as ASPP [5] and PPM [45] enhance CNNs with multi-scale context, and NLNet [36]\nand CCNet [20] provided a non-local mechanism to CNNs. Moreover, instead of just using them\nas an add-on to CNNs, some works explored to use attention modules to replace convolutional\nblocks [18, 28, 34, 44].\nVision Transformer. Recently, ViT [12] was proposed to adapt the Transformer [ 33] for image\nrecognition by tokenizing and ﬂattening 2D images into sequence of tokens. Since then, many works\nhave been done to improve Transformers, making them more suitable for vision tasks. These works\ncan be roughly categorized into three types: (1) Type I made efforts to improve the ViT design\nitself. For example, DeiT [32] introduced a training scheme to get rid of large-scale pre-training and\ndistillation method to further improve the performance. T2T-ViT [41] presented a token-to-token\noperation as alternatives to patch embedding, which keeps better local details. (2) Type II tried to\nintroduce convolution back into the ViT design. E.g., Chu et al. [7] proposed to use convolution\nfor position encoding. Wu et al. [37] used convolution to replace the linear projection layers in\nTransformers. (3) Type III tried to replace CNNs by building hierarchical Transformers as a plug-in\nbackbone in many downstream tasks. Wang et al. [35] proposed a pyramid vision Transformer,\n2\n(a) (b) (c)\nFigure 1: Toy examples illustrating different methods to reduce computation and memory cost\nof self-attention. (a) Spatial reduction [ 35, 13] spatially downsamples the feature map; (b) Local\nwindow [24] restricts self-attention inside local windows; (c) Glance attention (ours) applies self-\nattention to adaptively-dilated partitions.\nwhich gradually downsamples the feature map and extract multi-scale features as common CNN\nbackbones do. However, applying self-attention on high-resolution features is not affordable in\nterms of both memory and computation cost, thus they used spatial-reduction attention, which\ndownsamples key and value in self-attention as a trade-off between efﬁciency and accuracy. Later,\nLiu et al. [24] proposed a new hierarchical Transformer architecture, named Swin-Transformer. To\nhandle the expensive computation burden incurred with self-attention, they divided feature maps into\nseveral non-overlapped windows, and limited the self-attention operation to be performed within\neach window. By doing so, Swin-Transformer is more efﬁcient and also scalable to large resolution\ninput. Besides, to compensate the missing global information, a shifted window strategy is proposed\nto exchange information between different windows.\nOur method differs from aforementioned works in the following aspects: Type I, II methods usually\nutilize a large patch size and thus incompatible to work with high-resolution feature map. Type III\nmethods proposed new attention mechanism to handle the extreme memory and computation burden\nwith long sequences, but they sacriﬁces accuracy as a trade-off with efﬁciency. In contrast, GG-\nTransformer proposes a more efﬁcient Transformer block with a novel Glance-and-Gaze mechanism,\nwhich not only enables it to handle long sequences and scalable to high-resolution feature maps, but\nalso leads to a better performance than other efﬁcient alternatives.\n3 Method\nThe design of GG-Transformer draws inspiration from how human beings observe the world, which\nfollows the Glance and Gaze mechanism. Speciﬁcally, humans will glance at the global view, and\nmeanwhile gaze into local details to obtain a comprehensive understanding to the environment. We\nnote that these behaviors surprisingly match the property of self-attention and convolution, which\nmodels long-range dependencies and local context, respectively. Inspired from this, we propose\nGG-Transformer, whose Transformer block consists of two parallel branches: A Glance branch,\nwhere self-attention is performed to adaptively-dilated partitions of the input, and a Gaze branch,\nwhere a depth-wise convolutional layer is adopted to capture the local patterns.\n3.1 Revisit Vision Transformer\nWe start with revisiting the formulation of vision Transformer block, which consists of multi-head\nself-attention (MSA), layer normalization (LN), and multi-layer perceptron (MLP). A Transformer\nblock processes input features as follows:\nz′\nℓ = MSA(LN(zℓ−1)) + zℓ−1, (1)\nzℓ = MLP(LN(z′\nℓ)) + z′\nℓ, (2)\n3\nQ\nK\nV\nSoftmax\nDepthwise\nConv\nMLP\nMerging\nAdaptively \nDilated \nSplitting\nGlance Branch\nGaze Branch\nMerging\nAdaptively \nDilated \nSplitting\nFigure 2: A visual illustration of GG Transformer block, where the Glance and Gaze branches\nparallely extract complementary information.\nwhere zℓ is the encoded image representation at the ℓ-th block. MSA gives Transformers the\nadvantages of modeling a global relationship in a parameter-free manner, which is formulated as:\nMSA(X) = Softmax(QKT\n√\nC\n)V, (3)\nwhere Q, K, V ∈RN×C are the query, key, and value matrices which are linear mappings of input\nX ∈RN×C, Cis the channel dimension of the input, and N is the length of input sequence. Note\nthat for simpliﬁed derivation, we assume the number of heads is 1 in the multi-head self attention,\nwhich will not affect the following complexity analysis and can be easily generalize to more complex\ncases.\nFor vision tasks, N is often related to the input height H and width W. In practice, a 2D image\nis often ﬁrst tokenized based on non-overlapping image patch grids, which maps a 2D input with\nsize H×W into a sequence of token embeddings with length N = H×W\nP2 , where (P,P ) is the grid\nsize. In MSA, the relationships between a token and all tokens are computed. Such designs, though\neffectively capturing long-range features, incur a computation complexity quadratic to N:\nΩ(MSA) = 4NC2 + 2N2C. (4)\nFor ViTs that only work on 16×down-sampled feature maps (i.e., P = 16), the computation cost\nis affordable, since in this scenario N = 14 ×14 = 196 (for a typical ImageNet setting with\ninput size 224 ×224). However, when it comes to a more general vision scenario with the need of\ndense prediction based on high-resolution feature maps (such as semantic segmentation), where the\ncost dramatically increases by thousands of times or even more. Naively applying MSA to such\nhigh-resolution feature maps can easily lead to the problem of out-of-memory (OOM), and extremely\nhigh computational cost. Although some efﬁcient alternatives [ 35, 24] were brought up recently,\naccuracy is often sacriﬁced as a trade-off of efﬁciency. To address this issue, we propose a new vision\nTransformer that can be applied to longer sequence while keeping high accuracy, inspired by the\nGlance-and-Gaze human behavior when recognizing objects in natural scenes [11].\n3.2 Glance: Efﬁcient Global Modeling with Adaptively-dilated Splitting\nTo address the efﬁciency problem of Transformers, existing solutions often adapt Transformers to\nhigh-resolution feature maps by down-sampling the key and value during the attention process [35], or\nlimit self-attention to be computed in a local region then exchange information through shifting these\nlocal regions to mimic a global view [24]. But limitations exist in these methods. For down-sampling\nmethods, although the output feature maps keep to be high-resolution, they lose some details during\nthe down-sampling processes. Besides, they still has a quadratic complexity and thus may not scale up\nto a larger input size. For local-region based methods, though they successfully reduce the complexity\n4\nto a linear level, they cannot directly model a long-range dependency but instead are stuck within\nlocal context, which counters the design intuition of Transformer and self-attention for long-range\ndependency modeling. Besides, two consecutive blocks need to work together to mimic a global\nreceptive ﬁeld, which may not achieve as good performance as MSA (see Table. 4b).\nThus, we propose Glance attention, which performs self-attention efﬁciently with a global receptive\nﬁeld. It shares same time complexity as [24], but directly models long-range dependencies, as shown\nin Fig. 1. Speciﬁcally, we ﬁrst splits an input feature map to several dilated partitions, i.e., the points\nin a partition are not from a local region but from the whole input feature map with a dilation rate\nadaptive to the feature map size and the token size. We name this operation Adaptively-dilated\nSplitting. For example, a partition contains M×M tokens and it is obtained with an adaptive dilation\nrate = ( h\nM , w\nM ), where h, wis the height and width of current feature map respectively, and hw= N.\nHere we assume all divisions have no remainder for simplicity. These partitions are easily to be split\nfrom the input feature map or merged back. Speciﬁcally, we formulate this process as follows:\nzℓ−1 = [z1,1\nℓ−1,z1,2\nℓ−1,..., zh,w\nℓ−1], (5)\nwhere zi,j\nℓ−1 is the feature token at position (i,j) if reshaping the sequence of token embedding zℓ−1\nback into a 2D feature map and use the 2D coordinates accordingly. To reduce the memory and\ncomputation burden, while keeping a global receptive ﬁeld, zℓ−1 is split into several partitions:\nz†\nℓ−1 = AdaptivelyDilatedSplitting(zℓ−1) (6)\n= [z†,1,1\nℓ−1 ,z†,1,2\nℓ−1 ,..., z\n†, h\nM , w\nM\nℓ−1 ], (7)\nwhere z†,i,j\nℓ−1 = [zi,j\nℓ−1,z\ni,j+ h\nM\nℓ−1 ,..., z\ni+ (M−1)h\nM ,j+ (M−1)w\nM\nℓ−1 ], (8)\nwhere z†\nℓ−1 ∈RN×C, z†,i,j\nℓ−1 ∈RM2×C. Afterwards, MSA is applied to each partition, which\nsubstantially reduces the computation and memory cost yet does not lose the global feature represen-\ntation. And then all partitions are merged back into one feature map and go through the remaining\nmodules:\nz′,i,j\nℓ = MSA(LN(z†,i,j\nℓ−1 )) + z†,i,j\nℓ−1 , (9)\nz′\nℓ = Merging(z′,1,1\nℓ ,..., z\n′, h\nM , w\nM\nℓ ), (10)\nzℓ = MLP(LN(z′\nℓ)) + z′\nℓ, (11)\nwhere Merging is an inverse operation of AdaptivelyDilatedSplitting which re-arranges points in\neach partition back in their original orders.\nThis new self-attention module (formulated in Eq. 6 to 10), namely Glance multi-head self attention\nmodule (G-MSA), enables a global feature learning with linear complexity:\nΩ(G-MSA) = 4hwC2 + 2M2hwC = 4NC2 + 2M2NC. (12)\n3.3 Gaze: Compensating Local Relationship with Depthwise Convolution\nAlthough the Glance branch can effectively capture long-range representations, it misses the local\nconnections across partitions, which can be crucial for vision tasks relying on local cues. To this end,\nwe propose a Gaze branch to compensate the missing relationship and enhance the modeling power\nat a negligible cost.\nSpeciﬁcally, to compensate the local patterns missed in the Glance branch, we propose to apply an\nadditional depthwise convolution on the value in G-MSA:\nGaze(X) = DepthwiseConv2d(Merging(V)), (13)\nwhich has a neglectable computational cost and thus the overall cost is still signiﬁcantly reduced:\nΩ(GG-MSA) = 4NC2 + 2M2NC + k2NC, (14)\nwhere kis the Gaze branch kernel size, and M is the partition size set in Glance branch, both kand\nM are constants that are much smaller than N. We note that in this way, long-range and short-range\nfeatures are naturally and effectively learned. Besides, unlike [ 24], GG-MSA does not require\n5\ntwo consecutive blocks (e.g., W-MSA and SW-MSA) to be always used together, instead, it is a\nstandalone module as the original MSA [12].\nWe propose two ways to determine the kernel size kfor better compensating the local features:\nFixed Gazing. A straightforward way is to adopt the same kernel size ( e.g., 3 ×3) for all Gaze\nbranches, which can ensure same local feature learning regardless of the dilation rate.\nAdaptive Gazing. Another way is implementing Gazing branch with adaptive kernels, where the\nkernel size should be the same as dilation rate (h/M,w/M). In this way, GG-MSA still enjoys a\ncomplete view of the input.\nBy combining Glance and Gaze branches together, GG-MSA can achieve superior performance to\nother counterparts while remaining a low cost.\n3.4 Network Instantiation\nWe build a hierarchical GG-Transformer with the proposed Glance-and-Gaze branches as shown\nin Fig. 2. For fair comparison, we follow the same settings as Swin-Transformer [ 24] in terms of\nnetwork depth and width, with only difference in the attention methods used in Transformer blocks.\nFurthermore, we set M to be same as the window size in [24], so that the model size and computation\ncost are also directly comparable. Note that GG-Transformer has not been speciﬁcally tuned by\nscaling depth and width for a better accuracy-cost trade-off.\nWe build GG-T and GG-S, which share the same model size and computation costs as Swin-T and\nSwin-S, respectively. For all GG-Transformers, we set the ﬁxed patch size M = 7 , expansion\nratio of MLP α= 4. All GG-Transformer consists of 4 hierarchical stages, which corresponds to\nfeature maps with down-sampling ratio 4, 8, 16, 32, respectively. The ﬁrst patch embedding layer\nprojects input to a feature map with channel C = 96. When transitioning from one stage to the next\none, we follow CNN design principles [ 16] to expand the channel by 2×when the spatial size is\ndown-sampled by 4×.\n4 Experiments\nIn the following parts, we report results on ImageNet [10] classiﬁcation, COCO [23] object detection,\nand ADE20K [48] semantic segmentation to compare GG-Transformer with those state-of-the-art\nCNNs and ViTs. Afterwards, we conduct ablation studies to verify the design of Glance and Gaze\nbranches and also compare effectiveness of different alternative self-attention designs.\n4.1 ImageNet Classiﬁcation\nWe validate the performance of GG-Transformer on ImageNet-1K [10] classiﬁcation task, which\ncontains 1.28M training images and 50K validation images for 1000 classes. We report top-1 accuracy\nwith a single 224 ×224 crop.\nImplementation Details. To ensure a fair comparison, we follow the same training settings of [24].\nSpeciﬁcally, we use AdamW [25] optimizer for 300 epochs with cosine learning rate decay including\n20 epochs for linear warm-up. The training batch size is 1024 with 8 GPUs. Initial learning rate starts\nat 0.001, and weight decay is 0.05. Augmentations and regularizations setting follows [32] including\nrand-augment [9], mixup [43], cutmix [42], random erasing [47], stochastic depth [19], but excluding\nrepeated repeated augmentation [17] and EMA [26].\nResults. A summary of results in Table 1, where we compare GG-Transformer with various\nCNNs and ViTs. It is shown that GG-Transformer achieve better accuracy-cost trade-off com-\npared to other models. Moreover, GG-T, a light-weight model (28M/4.5G/82.0%), can achieve\ncomparable performance to those even much large models such as DeiT-B (86M/17.5G/81.8%),\nT2T-ViT-24 (64M/14.1G/82.3%), and PVT-Large (61M/9.8G/81.7%). Furthermore, compared to\nSwin-Transformer, which we follows the architecture and ensures the same model size and computa-\ntion costs to ensure a fair comparison, our model consistently brings an improvement to baseline,\nwith a consistent improvement of 0.8% and 0.2% for T and S models respectively.\n6\nTable 1: Comparison of different models on ImageNet-1K classiﬁcation.\nmethod image\nsize #param. FLOPs ImageNet\ntop-1 acc.\nRegNetY-4G [27] 2242 21M 4.0G 80.0\nRegNetY-8G [27] 2242 39M 8.0G 81.7\nRegNetY-16G [27] 2242 84M 16.0G 82.9\nEffNet-B3 [31] 3002 12M 1.8G 81.6\nEffNet-B4 [31] 3802 19M 4.2G 82.9\nEffNet-B5 [31] 4562 30M 9.9G 83.6\nDeiT-T [32] 2242 5M 1.3G 72.2\nDeiT-S [32] 2242 22M 4.6G 79.8\nDeiT-B [32] 2242 86M 17.5G 81.8\nTNT-S [14] 2242 24M 5.2G 81.3\nTNS-B [14] 2242 66M 14.1G 82.8\nT2T-ViT-7 [41] 2242 4M 1.2G 71.7\nT2T-ViT-14 [41] 2242 22M 5.2G 81.5\nT2T-ViT-24 [41] 2242 64M 14.1G 82.3\nPVT-Tiny [35] 2242 13M 1.9G 75.1\nPVT-Small [35] 2242 25M 3.8G 79.8\nPVT-Medium [35] 2242 44M 6.7G 81.2\nPVT-Large [35] 2242 61M 9.8G 81.7\nSwin-T [24] 2242 28M 4.5G 81.2\nSwin-S [24] 2242 50M 8.7G 83.2\nGG-T 2242 28M 4.5G 82.0\nGG-S 2242 50M 8.7G 83.4\n4.2 ADE20K Semantic Segmentation\nADE20K [48] is a challenging semantic segmentation dataset, containing 20K images for training\nand 2K images for validation. We follow common practices to use the training set for training and\nreport mIoU results on the validation sets. We use UperNet [38] as the segmentation framework and\nreplace the backbone with GG-Transformer.\nImplementation Details. We follow [24] and use MMSegmentation [ 8] to implement all related\nexperiments. We use AdamW [ 25] with a learning rate starting at 6 ×10−5, weight decay of\n0.01, batch size of 16, crop size of 512 ×512. The learning rate schedule contains a warmup\nof 1500 iterations and linear learning rate decay. The training is conducted with 8 GPUs and the\ntraining procedure lasts for 160K iterations in total. The augmentations follows the default setting\nof MMSegmentation, including random horizontal ﬂipping, random re-scaling within ratio range\n[0.5, 2.0] and random photometric distortion. For testing, we follow [46] to utilize a sliding window\nmanner with crop size 512 and stride 341. ImageNet-1K pretrained weights are used for initialization.\nResults. We show results in Table 2, where results both w/ and w/o test-time augmentation are\nreported. Noticeably, GG-Transformer not only achieves better results to baselines, but also obtain a\ncomparable single-scale testing performance to those with multi-scale testing results. Speciﬁcally,\nGG-T achieves 46.4% mIoU with single-scale testing, which surpasses ResNet50, PVT-Small, Swin-\nT’s multi-scale testing results by 3.6%, 1.6%, 0.6%, respectively. Moreover, our tiny model even can\nbe comparable to those much larger models (e.g., 47.2% of GG-T compared to 47.6% of Swin-S).\n4.3 COCO Object Detection\nWe further verify the performance of GG-Transformer when used as a plug-in backbone to object\ndetection task on COCO dataset [23], which contains 118K, 5K, 20K images for training, validation\nand test respectively. We use Mask-RCNN [ 15] and Cascaded Mask R-CNN [ 1] as the detection\nframeworks, and compare GG-Transformer to various CNN and ViT backbones.\nImplementation Details. We follow the setting of [24] and use MMDetection [4] to conduct all the\nexperiments. We adopt multi-scale training [2], AdamW optimizer [25] with initial learning rate of\n0.0001, weight decay of 0.05, batch size of 16. The training is conducted with 8 GPUs and a 1×\nschedule. All models are initialized with ImageNet-1K pretrained weights.\n7\nTable 2: Performance comparisons with different backbones on ADE20K validation dataset. FLOPs\nis tested on 1024×1024 resolution. All backbones are pretrained on ImageNet-1k.\nBackbone\nUperNet\nPrams (M) FLOPs (G) mIoU (%) mIoU(ms+ﬂip) (%)\nResNet50 [16] 67 952 42.1 42.8\nPVT-Small [35] 55 919 43.9 44.8\nSwin-T [24] 60 941 44.5 45.8\nGG- (ours) 60 942 46.4 47.2\nResNet101 [16] 86 1029 43.8 44.9\nPVT-Medium [35] 74 977 44.9 45.3\nSwin-S [24] 81 1034 47.6 49.5\nGG-S (ours) 81 1035 48.4 49.6\nTable 3: Object detection and instance segmentation performance on the COCO val2017 dataset\nusing the Mask R-CNN framework. Params/FLOPs is evaluated with Mask R-CNN architecture on a\n1280×800 image.\nBackbone\nParams FLOPs Mask R-CNN Cascaded Mask R-CNN\n(M) (G) APb APb\n50 APb\n75 APm APm\n50 APm\n75 APb APb\n50 APb\n75 APm APm\n50 APm\n75\nResNet50 [16] 44 260 38.2 58.8 41.4 34.7 55.7 37.2 41.2 59.4 45.0 35.9 56.6 38.4\nPVT-Small [35] 44 245 40.4 62.9 43.8 37.8 60.1 40.3 - - - - - -\nSwin-T [24] 48 264 43.7 66.6 47.7 39.8 63.3 42.7 48.1 67.1 52.2 41.7 64.4 45.0\nGG-T (ours) 48 265 44.1 66.7 48.3 39.9 63.3 42.4 48.4 67.4 52.3 41.9 64.5 45.0\nResNet101 [16] 63 336 40.0 60.6 44.0 36.1 57.5 38.6 42.9 61.0 46.6 37.3 58.2 40.1\nResNeXt101-32×4d [39] 63 340 41.9 62.5 45.9 37.5 59.4 40.2 44.3 62.8 48.4 38.3 59.7 41.2\nPVT-Medium [35] 64 302 42.0 64.4 45.6 39.0 61.6 42.1 - - - - - -\nSwin-S [24] 69 354 45.4 67.9 49.6 41.4 65.1 44.6 49.7 68.8 53.8 42.8 66.0 46.4\nGG-S (ours) 69 355 45.7 68.3 49.9 41.3 65.3 44.0 49.9 69.0 54.0 43.1 66.2 46.4\nResults. As shown in Table 3, GG-Transformer achieves superior performance to other backbones in\nthe two widely-used detection frameworks. Speciﬁcally, GG-T achieves 44.1 box AP and 39.9 mask\nAP, which surpasses both CNNs and other ViTs with a similar model size and computation costs.\nCompared with the state-of-the-art Swin-Transformer, GG-Transformer achieves better performance\nwhile keeping the same model size and computation costs for both T and S models.\n4.4 Ablation Studies\nIn this part, we conduct ablation studies regarding to the designs of GG-Transformer. Meanwhile, we\nalso compare among different efﬁcient alternatives toMSA. Besides, we verify GG-MSA on another\nViT architecture [32] to compare its capacity to MSA directly. We conduct all these experiments\nbased on Swin-T [24] with 100 epochs training and DeiT [32] architectures with 300 epochs training.\nKernel Choice of Gaze Branch. We study the choice of Gaze branch in terms of ﬁxed or adaptive\nmechanism. The kernel sizes for each stage and results are summarized in Table 4a, where we observe\nthat both mechanisms work well. Using a larger kernel leads to a non-signiﬁcant improvement. In\ncontrast, adaptive manner leads to a slightly better performance. Considering the adaptive manner\nprovides a complete view as the original MSA has, we choose it in our ﬁnal design.\nGlance/Gaze Branch. We study the necessity of both Glance and Gaze branches. Meanwhile, a\ncomparison between different ways to conduct self-attention is also studied. Results are in Table 4b.\nSwin-T [ 24] serves as the baseline for all variants, which achieves 78.50% top-1 accuracy on\nImageNet validation set. Firstly, we note that the local window attention and shifted window attention\n(W&SW-MSA) in [24] although can signiﬁcantly reduce the computation complexity and makes\nTransformer easier to scale-up, it sacriﬁces the accuracy and the combination of W&SW-MSA to\nmimic a global view is not as good as the original MSA. We replace the W&SW-MSA with MSA for\nall blocks in stage 3 and 4 (i.e., stages with down-sampling rate 16 and 32), which leads to a 1.29%\nperformance improvement, indicating there exists a signiﬁcant performance gap between MSA and\nits efﬁcient alternative. Notably, when adopting the proposed Glance and Gaze mechanism instead,\n8\nTable 4: Ablation studies regarding GG-Transformer design and comparison among different self-\nattention mechanisms.\n(a) Choices of Gaze Kernels.\nGaze Kernel Top-1\nFixed-(3,3,3,3) 80.28%\nFixed-(5,5,5,5) 80.31%\nAdaptive-(9,5,3,3) 80.38%\n(b) Comparison among different\nself-attentions.\nTop-1\nW& SW-MSA [24] 78.50%\nMSA 79.79%\nGlance Only 77.21%\nGaze Only 76.76%\nGlance+Gaze (Attn) 79.07%\nGlance+Gaze (Conv) 80.28%\n(c) Applying GG-MSA to DeiT\nbackbone.\nTop-1\nDeiT-T 72.2%\nGG-DeiT-T 73.8%\nDeiT-S 79.9%\nGG-DeiT-S 80.5%\nwhich shares a same complexity of W& SW-MSA, can achieves much better performance, where the\nGlance+Gaze (Attn) improves the performance by 0.57%, and Glance+Gaze (Conv) (i.e., GG-T) by\n1.78%, which is even higher than MSA by 0.49%.\nBesides using depthwise convolution, another natural choice is to also adopt self-attention for imple-\nmenting the Gaze branch. Therefore, we conduct experiments by using local window attention [24] as\nthe Gaze branch. Note that, unlike depthwise convolution, a self-attention variant of the Gaze branch\ncannot be integrated with the Glance branch into the same Transformer block while keeping the\noverall model size and computation cost at the same level. To ensure a fair comparison, we use two\nconsecutive Transformer blocks where one is Glance attention and another is Gaze attention. Using\neither convolution or self-attention to implement the Gaze branch can both improve the performance\ncompared to [24], illustrating the effectiveness of the Glance and Gaze designs. However, using\nself-attention is inferior to depth-wise convolution with a degrade of 1.21%, which may indicate\nthat convolution is still a better choice when it comes to learning local relationships. Besides, using\ndepth-wise convolution as Gaze branch can also naturally be integrated into the Transformer block\nwith Glance attention, thus makes it more ﬂexible in terms of network designs.\nWe also note that Glance or Gaze branch alone is far from enough, while only a combination of both\ncan lead to a performance gain, which matches the behavior that we human beings can not rely on\nGlance or Gaze alone. For instance, using Glance alone can only lead to an inferior performance\nwith accuracy of 77.21%, and Gaze alone 76.76%, which is signiﬁcantly lower than baseline with a\ndegrade of 1.29% and 1.74%, respectively. Nevertheless, we note that this is because Glance and\nGaze branches miss important local or global cues which can be compensated by each other. As a\nresult, a combination of both Glance and Gaze gives a high accuracy of 80.28%, which improves the\nGlance alone and Gaze alone by 3.07% and 3.52% respectively.\nApply to other backbone. We verify the effectiveness of GG-Transformer on another popular\nViT architecture DeiT [32], as shown in Table 4c. We replace MSA with GG-MSA for two DeiT\nvariants [32], DeiT-T and DeiT-S. We show that, although GG-MSA is an efﬁcient alternative to\nMSA, it can also lead to a performance gain. Compared to DeiT-T and DeiT-S, GG-DeiT-T and\nGG-DeiT-S bring the performance up by 1.6% and 0.6% respectively, illustrating that it is not only\nefﬁcient but also effectively even compared to a fully self-attention.\n5 Limitation\nAlthough GG-Transformer provides a powerful and efﬁcient solution to make Transformers scalable\nto large inputs, some limitations still exist and worth further exploring.\nFirstly, over-ﬁtting is a common problem [ 12] in Vision Transformers and can be alleviated by\nlarge-scale pretraining [12] or strong augmentations and regularization [32]. This problem is more\nserious for stronger models (GG-Transformer) and in the tasks with relatively small dataset (e.g.\nsemantic segmentation). Secondly, Transformers suffer from performance degradation in modeling\nlonger-range dependencies, when there exists large discrepancy in the training-testing image size.\nThe limitations can come from position encoding, which has ﬁxed size and need to be interpolated\nto different input sizes, or self-attention itself, which may not adapt well when signiﬁcant changes\nhappen in input size. Lastly, there is a long-lasting debate on the impacts of AI on human world. As a\n9\nmethod improving the fundamental ability of deep learning, our work also advances the development\nof AI, which means there could be both beneﬁcial and harmful inﬂuences depending on the users.\n6 Conclusion\nIn this paper, we present GG-Transformer, which offers an efﬁcient and effective solution to adapting\nTransformers for vision tasks. GG-Transformer, inspired by how human beings learn from the world,\nis equipped with parallel and complementary Glance branch and Gaze branch, which offer long-range\nrelationship and short-range modeling, respectively. The two branches can specialize in their tasks\nand collaborate with each other, which leads to a much more efﬁcient ViT design for vision tasks.\nExperiments on various architectures and benchmarks validate the advantages of GG-Transformer.\nReferences\n[1] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 6154–6162, 2018.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\npages 213–229. Springer, 2020.\n[3] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and\nYuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv\npreprint arXiv:2102.04306, 2021.\n[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n[5] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\n[6] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.\n[7] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nConditional positional encodings for vision transformers. Arxiv preprint 2102.10882, 2021.\n[8] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and\nbenchmark. https://github.com/open-mmlab/mmsegmentation, 2020.\n[9] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pages 702–703, 2020.\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[11] Heiner Deubel and Werner X. Schneider. Saccade target selection and object recognition: Evidence for a\ncommon attentional mechanism. Vision Research, 36(12):1827–1837, 1996.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[13] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\nFeichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021.\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\n[15] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2961–2969, 2017.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[17] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your\nbatch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8129–8138, 2020.\n10\n[18] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 3464–3473, 2019.\n[19] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision, pages 646–661. Springer, 2016.\n[20] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 603–612, 2019.\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[22] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on\ncomputer vision, pages 740–755. Springer, 2014.\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[26] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM\njournal on control and optimization, 30(4):838–855, 1992.\n[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network\ndesign spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10428–10436, 2020.\n[28] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019.\n[29] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:\nInverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4510–4520, 2018.\n[30] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556, 2014.\n[31] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\n[32] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[34] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on Computer\nVision, pages 108–126. Springer, 2020.\n[35] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[36] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[37] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[38] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for scene\nunderstanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418–434,\n2018.\n[39] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\n[40] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint\narXiv:1511.07122, 2015.\n11\n[41] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[42] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 6023–6032, 2019.\n[43] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[44] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10076–\n10085, 2020.\n[45] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n2881–2890, 2017.\n[46] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[47] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 13001–13008, 2020.\n[48] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 633–641, 2017.\n12",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7674511671066284
    },
    {
      "name": "Computer science",
      "score": 0.6899253129959106
    },
    {
      "name": "Gaze",
      "score": 0.6869162321090698
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5342902541160583
    },
    {
      "name": "Computation",
      "score": 0.49706223607063293
    },
    {
      "name": "Computer vision",
      "score": 0.39995765686035156
    },
    {
      "name": "Algorithm",
      "score": 0.2765270173549652
    },
    {
      "name": "Engineering",
      "score": 0.14977845549583435
    },
    {
      "name": "Voltage",
      "score": 0.11785027384757996
    },
    {
      "name": "Electrical engineering",
      "score": 0.10568675398826599
    }
  ]
}