{
  "title": "Correlation Matching Transformation Transformers for UHD Image Restoration",
  "url": "https://openalex.org/W4393178551",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104339954",
      "name": "Wang Cong",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2100349836",
      "name": "Jinshan Pan",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2094419500",
      "name": "Gang Fu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2115198658",
      "name": "Siyuan Liang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2590391874",
      "name": "Mengzhu Wang",
      "affiliations": [
        "Hebei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2196274784",
      "name": "Xiao-Ming Wu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2097653880",
      "name": "Jun Liu",
      "affiliations": [
        "Singapore University of Technology and Design"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3189755165",
    "https://openalex.org/W4285505226",
    "https://openalex.org/W3201772641",
    "https://openalex.org/W3191278083",
    "https://openalex.org/W3022619948",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W2114770744",
    "https://openalex.org/W4385963586",
    "https://openalex.org/W4303648905",
    "https://openalex.org/W4286750614",
    "https://openalex.org/W4309873613",
    "https://openalex.org/W6750330310",
    "https://openalex.org/W4322100000",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W2968878340",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W2560533888",
    "https://openalex.org/W6721025879",
    "https://openalex.org/W2979983945",
    "https://openalex.org/W6810083443",
    "https://openalex.org/W4223934059",
    "https://openalex.org/W3046976312",
    "https://openalex.org/W3046849953",
    "https://openalex.org/W6801802051",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W6754146604",
    "https://openalex.org/W4312725970",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W6790137093",
    "https://openalex.org/W2935315509",
    "https://openalex.org/W3204598753",
    "https://openalex.org/W3177127346",
    "https://openalex.org/W6775333597",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W3093101131",
    "https://openalex.org/W3202040256",
    "https://openalex.org/W3092703224",
    "https://openalex.org/W4387968529",
    "https://openalex.org/W2990007814",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3035505140",
    "https://openalex.org/W3201770677",
    "https://openalex.org/W4311398044",
    "https://openalex.org/W3034278302",
    "https://openalex.org/W4322746835",
    "https://openalex.org/W2474628748",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4312914040",
    "https://openalex.org/W2949187370",
    "https://openalex.org/W2963928582",
    "https://openalex.org/W4386075642",
    "https://openalex.org/W2965217508",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W4223444776",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4309845474"
  ],
  "abstract": "This paper proposes UHDformer, a general Transformer for Ultra-High-Definition (UHD) image restoration. UHDformer contains two learning spaces: (a) learning in high-resolution space and (b) learning in low-resolution space. The former learns multi-level high-resolution features and fuses low-high features and reconstructs the residual images, while the latter explores more representative features learning from the high-resolution ones to facilitate better restoration. To better improve feature representation in low-resolution space, we propose to build feature transformation from the high-resolution space to the low-resolution one. To that end, we propose two new modules: Dual-path Correlation Matching Transformation module (DualCMT) and Adaptive Channel Modulator (ACM). The DualCMT selects top C/r (r is greater or equal to 1 which controls the squeezing level) correlation channels from the max-pooling/mean-pooling high-resolution features to replace low-resolution ones in Transformers, which can effectively squeeze useless content to improve the feature representation in low-resolution space to facilitate better recovery. The ACM is exploited to adaptively modulate multi-level high-resolution features, enabling to provide more useful features to low-resolution space for better learning. Experimental results show that our UHDformer reduces about ninety-seven percent model sizes compared with most state-of-the-art methods while significantly improving performance under different training sets on 3 UHD image restoration tasks, including low-light image enhancement, image dehazing, and image deblurring. The source codes will be made available at https://github.com/supersupercong/UHDformer.",
  "full_text": "Correlation Matching Transformation Transformers for UHD Image Restoration\nCong Wang1*, Jinshan Pan2, Wei Wang3, Gang Fu1,\nSiyuan Liang4, Mengzhu Wang5, Xiao-Ming Wu1, Jun Liu6\n1The Hong Kong Polytechnic University\n2Nanjing University of Science and Technology\n3Dalian University of Technology\n4National University of Singapore\n5Hebei University of Technology\n6Singapore University of Technology and Design\nAbstract\nThis paper proposes UHDformer, a general Transformer for\nUltra-High-Definition (UHD) image restoration. UHDformer\ncontains two learning spaces: (a) learning in high-resolution\nspace and (b) learning in low-resolution space. The former\nlearns multi-level high-resolution features and fuses low-high\nfeatures and reconstructs the residual images, while the latter\nexplores more representative features learning from the high-\nresolution ones to facilitate better restoration. To better im-\nprove feature representation in low-resolution space, we pro-\npose to build feature transformation from the high-resolution\nspace to the low-resolution one. To that end, we propose two\nnew modules: Dual-path Correlation Matching Transforma-\ntion module (DualCMT) and Adaptive Channel Modulator\n(ACM). The DualCMT selects top C/r (r is greater or equal\nto 1 which controls the squeezing level) correlation channels\nfrom the max-pooling/mean-pooling high-resolution features\nto replace low-resolution ones in Transformers, which can ef-\nfectively squeeze useless content to improve the feature rep-\nresentation in low-resolution space to facilitate better recov-\nery. The ACM is exploited to adaptively modulate multi-level\nhigh-resolution features, enabling to provide more useful fea-\ntures to low-resolution space for better learning. Experimen-\ntal results show that our UHDformer reduces about ninety-\nseven percent model sizes compared with most state-of-the-\nart methods while significantly improving performance under\ndifferent training sets on 3 UHD image restoration tasks, in-\ncluding low-light image enhancement, image dehazing, and\nimage deblurring. The source codes will be made available at\nhttps://github.com/supersupercong/UHDformer.\nIntroduction\nIn recent years, the rapid development of advanced imaging\nsensors and displays has greatly contributed to the progress\nof Ultra-High-Definition (UHD) imaging. However, UHD\nimages captured under low-light, hazy, or high-speed move-\nment conditions often suffer from undesirable degradation,\nresulting in visually low quality and hindering high-level vi-\nsion tasks. This paper presents a unified framework to ad-\ndress the challenging UHD image restoration problem.\nWith the emergence of convolutional neural networks\n(CNNs) and Transformers, learning-based methods have\n*supercong94@gmail.com.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n LRZ-OLghW GURXQd TUXWh UHDFRXU UHDfRUmeU\nHa]e GURXQd TUXWh Deha]eFRUPeU UHDfRUmeU\nBOXU GURXQd TUXWh FFTfRUPeU UHDfRUmeU\nFigure 1: Challenging examples. Our UHDformer with only\n0.3393M parameters outperforms SOTAs with about 50×\nmore parameters than ours (see Tabs. 2-4).\nachieved impressive performance on general image restora-\ntion (Wang et al. 2020a,b; Zhu et al. 2020; Tsai et al. 2022;\nJin et al. 2022, 2023; Jin, Yang, and Tan 2022; Cui et al.\n2022; Wang et al. 2023). Unfortunately, these methods are\ntypically based on general image restoration tasks, which\nare not capable of handling UHD image sizes or effectively\nrecovering high-quality images. This limitation restricts the\npotential applications of UHD imaging systems.\nRecently, with the demand for handling UHD degraded\nimages, several approaches have been developed for UHD\nrestoration (Zheng et al. 2021; Li et al. 2023). Zheng et al.\n(2021) propose a multi-guided bilateral upsampling model\nfor UHD image dehazing. Different from the above meth-\nods which handle restoration in the spatial domain, Li et al.\n(2023) incorporate Fourier transform into low-light image\nenhancement by leveraging the amplitude and phase within\na cascaded network. However, these methods usually fail to\nexplore the valuable content for low-resolution space from\nthe high-resolution one, which could contain useful infor-\nmation that significantly affects restoration quality. Further-\nmore, existing methods often rely on large-capacity models\nto achieve optimal performance. For instance, UHDFour (Li\net al. 2023) and UHD (Zheng et al. 2021) methods have\n17.7M and 34.5M parameters, respectively, making them\nunsuitable for deployment on small-capacity devices.\nTo solve the above problems, we propose the UHD-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5336\nformer, a general Transformer for UHD image restoration.\nOur UHDformer contains two learning spaces: (a) learning\nin high-resolution space and (b) learning in low-resolution\nspace. The former learns multi-level high-resolution fea-\ntures, fuses low-high features, and reconstructs the resid-\nual images, while the latter explores more representative\nfeatures from the high-resolution space to facilitate bet-\nter restoration. To learn more useful features from high-\nresolution space for the low-resolution one, we propose\nto build feature transformation from high-resolution space\nto low-resolution one in Transformers to improve low-\nresolution feature representations for better restoration.\nTo that end, we propose two new modules: Dual-path\nCorrelation Matching Transformation module (DualCMT)\nand Adaptive Channel Modulator (ACM). Specifically, Du-\nalCMT selects the top C/r correlation channels (C denotes\nthe number of channels; r ≥ 1 controls the squeezing\nlevel) from the max-pooling/mean-pooling high-resolution\nfeatures for low-resolution ones. These selected channels\nsubsequently replace the low-resolution features within the\nQuery vector of attention and forward networks within\nTransformers. This process effectively squeezes redundant\nfeatures, improving feature representation within the low-\nresolution space for better recovery. To furnish the low-\nresolution space with more representative features, we pro-\npose the ACM that facilitates the adaptive modulation of\nmulti-level high-resolution features by acting upon their\nchannel dimensions. With the above designs, our UHD-\nformer achieves superior performance (see Fig. 1) while\nsignificantly reducing the model sizes, achieving the best\nparameters-performance trade-off on low-light image en-\nhancement, dehazing, and deblurring.\nOur main contributions are summarized below:\n• We propose UHDformer, the first general UHD image\nrestoration Transformer to the best of our knowledge,\nby building feature transformation from a high-resolution\nspace to a low-resolution one.\n• We propose a dual-path correlation matching transforma-\ntion module that can better transform the features from\nhigh- and low-resolution space to squeeze useless con-\ntent, enabling it to improve feature representations in\nlow-resolution space for better UHD image recovery.\n• We propose an adaptive channel modulator to adaptively\nmodulate multi-level high-resolution features to provide\nmore representative content for low-resolution space.\nRelated Works\nIn this section, we review Transformers for image restora-\ntion and Ultra-High-Definition restoration approaches.\nTransformers for Image Restoration. CNN-based archi-\ntectures (Liu et al. 2019; Cho et al. 2021; Zamir et al. 2021;\nWang et al. 2022a) have been shown to outperform conven-\ntional restoration approaches (Pan et al. 2016) due to im-\nplicitly learning the priors from large-scale data. Recently,\nTransformer-based models (Liang et al. 2021; Wang et al.\n2022b; Zamir et al. 2022; Kong et al. 2023) have domi-\nnated image restoration due to modeling long-range pixel\ndependencies which overcoming the shortage of performing\ncomputation in local windows of CNN-based algorithms.\nAlthough these Transformer-based methods have achieved\npromising performance for general image restoration, they\nusually cannot handle UHD images, which limits further po-\ntential applications on UHD imaging devices.\nUltra-High-Definition Restoration. With the demand for\nprocessing UHD images on imaging systems, a few meth-\nods have been proposed to recover clear UHD images via\nvarious network designs, including bilateral learning for im-\nage dehazing (Zheng et al. 2021), multi-scale separable-\npatch integration networks for video deblurring (Deng et al.\n2021a), Fourier embedding network for low-light image en-\nhancement (Li et al. 2023). However, all methods do not ex-\nplore useful content in high-resolution space for the contri-\nbutions of low-resolution space. In this paper, we propose to\nbuild the feature transformation from high- to low-resolution\nspace to provide more representative features to conduct ef-\nfective Transformer computation in low-resolution space.\nMethodology with UHDformer\nOverall Pipeline\nFig. 2 shows the overall framework of our UHDformer,\nwhich contains two learning spaces: (a) learning in high-\nresolution space and (b) learning in low-resolution space.\nThe former learns multi-level high-resolution features, fuses\nlow-high features, and reconstructs the residual images,\nwhile the latter explores more representative low-resolution\nfeatures by learning to match from the high-resolution space.\nLearning in High-Resolution Space. Given a UHD in-\nput image I ∈ RH×W×3, we first applies a 3×3 convo-\nlution to obtain low-level embeddings X0 ∈ RH×W×C;\nwhere H × W denotes the spatial dimension and C\nis the number of channels. Next, the shallow features\nX0 are hierarchically encoded into multi-level features\n{X1, X2, X3} ∈RH×W×C via 3 ConvNeXt blocks (Liu\net al. 2022). Then, the multi-level features are modulated\nvia an Adaptive Channel Modulator, and the modulated fea-\nture Xacm ∈ RH×W×3C is sent to low-resolution space to\nparticipate in conducting DualCMT in Transformers. Fi-\nnally, a low-high fusion and reconstruction layer containing\na concatenation operation and two ConvNeXt blocks, and\na 3 × 3 convolution receives X3 and shuffled-up features\nlearned from low-resolution space and generates residual\nimage S ∈ RH×W×3 to which UHD input image is added\nto obtain the restored image: ˆH = I+S. Fig. 2(a) shows the\nlearning in high-resolution space.\nLearning in Low-Resolution Space. The low-resolution\nspace, as shown in Fig. 2(b), first receives the shuffled-down\nfeatures Xin\nlow ∈ R\nH\n8 ×W\n8 ×C from X0 of the high-resolution\nspace. Then, Xin\nlow is sent to several Correlation Matching\nTransformation Transformer Blocks (CMT-TB), shown in\nFig. 2(c), to learn the low-resolution features. Meanwhile,\neach CMT-TB also matches the correlation between the\nmodulated feature Xacm in ACM and attention/forward net-\nworks in CMT-TBs to generate more representative features\nto facilitate better restoration. Finally, the learned feature\nXout\nlow ∈ R\nH\n8 ×W\n8 ×C is sent to high-resolution space to par-\nticipate in reconstructing the final recovery images.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5337\nIQSXW IPage (I)\n3h3 CRQY\nCRQYNeXW\n3h3\n DcRQY\nGELU\n1h1\nReVWRUed IPage (   )\nC\nMXOWL-LeYeO HLgK-ReVROXWLRQ FeaWXUeV LRZ-HLgK FXVLRQ & \nRecRQVWUXcWLRQ \nLa\\eU NRUP\nCMTA\nLa\\eU NRUP\nCMTN\n4\nK\n9\nMSA\n CRUUeOaWLRQ MaWcKLQg TUaQVfRUPaWLRQ TUaQVfRUPeU BORcN\n LeaUQLQg LQ LRZ-ReVROXWLRQ SSace \n8hSKXffOe DRZQ\nCRQcaW\nC SSOLW\n0h?h6C\n0h?h3C\n0h?h3C 0h?h3C\n0h?h3C\n0h?hC\n0h?hC\n0h?hC\nMXOWLSO\\\nSRfWPa[ \naW cKaQQeO\nReVLdXaO\n LeaUQLQg LQ HLgK-ReVROXWLRQ SSace \n8hSKXffOe US\n( )\n AdaSWLYe CKaQQeO MRdXOaWRU\nAdaSWLYe CKaQQeO MRdXOaWRU\nCMT-TB\nCMT-TB\n3h3 CRQY\nCRQYNeXW\n1h1\nCRQYNeXW\nCRQYNeXW\nCRQYNeXW\n«\n1h1\nDXaO\nCMT\nDXaO\nCMT\nFigure 2: Overall framework of our UHDformer.\nCorrelation Matching Transformation\nTransformer\nTo better improve low-resolution feature representation, we\npropose constructing the feature transformation from the\nhigh-resolution space to the low-resolution one since we\nobserve that multi-level high-resolution features implicitly\ncontain better content. This transformation aims to substi-\ntute low-resolution features with more representative high-\nresolution counterparts by filtering out redundant high-\nresolution features. To achieve this objective, we intro-\nduce the Correlation Matching Transformation Transformer\nBlock (CMT-TB), depicted in Fig. 2(c). The CMT-TB is\nto endow the low-resolution space with more representa-\ntive features, acquired from multi-level high-resolution fea-\ntures. These features are intended to replace the existing\nlow-resolution features via Dual-path Correlation Matching\nTransformation (DualCMT), illustrated in Fig. 3(a). Each\nCMT-TB contains Correlation Matching Transformation At-\ntention (CMTA) and Correlation Matching Transformation\nForward Network (CMTN) to respectively explore the cor-\nrelation matching transformation in both attention and for-\nward networks:\nX\n′\n= CMTA\n\u0010\nLN(Xl\nlow), Xacm\n\u0011\n+ Xl\nlow,\nXl+1\nlow = CMTN\n\u0010\nLN(X\n′\n), Xacm\n\u0011\n+ X\n′\n,\n(1)\nwhere Xl\nlow means the output features oflth(l = 1, 2, ··· , L)\nCMT-TB; Especially, X0\nlow is the Xin\nlow and XL\nlow is the Xout\nlow;\nCMTA(·, ·) and CMTN(·, ·) respectively denote the opera-\ntions of CMTA and CMTN, which are respectively defined\nin Eq. (2) and Eq. (3); LN(·) means the operation of layer\nnormalization (Ba, Kiros, and Hinton 2016).\nCorrelation Matching Transformation Attention. The\nquery typically describes its relationship with others in the\nattention (Ding et al. 2021). Hence, one more powerful\nquery may significantly influence the results. In this paper,\nwe empower the query with more representative content by\nreplacing it with adjusted multi-level high-resolution fea-\ntures Xacm. To that end, we propose the CMTA to improve\nthe query representation to better conduct attention. Specif-\nically, the CMTA first generates the query (Q), key (K), and\nvalue (V) projections from the normalized low-resolution\nfeatures Xl\nlow via 1×1 convolution Wp and 3×3 depth-wise\nconvolution Wd, and then conducts the DualCMT between\nQ and Xacm. Next, the CMTA conducts attention (Zamir\net al. 2022):\nCMTA(T1, T2) =A\n\u0010\nDualCMT(Q, T2), K, V\n\u0011\n,\nwhere Q, K, V = Split\n\u0010\nWdWp(T1)\n\u0011\n,\n(2)\nwhere T1 is the Xl\nlow in Eq. (1) and T2 is the Xacm in Eq. (1);\nDualCMT(·, ·) means the DualCMT; A\n\u0010\nˆQ, ˆK, ˆV\n\u0011\n= ˆV ·\nSoftmax\n\u0010\nˆK · ˆQ/α\n\u0011\n; Here, α is a learnable scaling parame-\nter to control the magnitude of the dot product of ˆK and ˆQ;\nSplit(·) denotes the split operation;\nCorrelation Matching Transformation Forward Net-\nwork. Similarly to CMTA, we advocate improving feature\nrepresentation in forward networks would help better restore\nimages. To this end, we introduce CMTN, which leverages\nDualCMT to learn more representative features from high-\nresolution space, thereby improving recovery quality. Ini-\ntially, CMTN generates features from the output of CMTA\nthrough layer normalization, 1×1 convolution, and 3×3\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5338\nMa[\nPRROLQg\nMeaQ\nPRROLQg CMT\nCMT\nC\n CRPSXWLQg \nMaWUL[ SLPLOaULW\\\nH/8hW/8hC\nFOaWWeQ\nHW/64hC\nFOaWWeQ\nHW/64hC\nChC\nTRS 1\nSeOecWLRQ Ch1\nSRUWed\nIQdLceV\nCh1\nSeOecWLQg TRS C/U \nMaSV b\\ IQdLceV\nH/8hW/8hC/U\n(a) DXaO-SaWh CRUUeOaWLRQ MaWchLQg TUaQVfRUPaWLRQ (b) CRUUeOaWLRQ MaWchLQg TUaQVfRUPaWLRQ (CMT)\nGFR\nHh\nWh\n3C\nHhWhC\n0hWhC\nH/8hW/8hC\nH/8hW/8hC\n+/8h:/8hC\nH/8hW/8hC\nH/8hW/8hC\nU: STXee]LQg FacWRU\nH/8hW/8hC\nLRZ-ReVROXWLRQ\nPRROed HLgh-ReVROXWLRQ\nTUaQVfRUPaWLRQ \nReVXOWV\n1h1 CRQY\n3h3 DCRQY\nCRQcaW\nMXOWLSO\\\nFigure 3: (a) Dual-path Correlation Matching Transformation and (b) Correlation Matching Transformation.\ndepth-wise convolution. It subsequently employs DualCMT\nbetween the generated features and ACM features to yield\nmore representative forward features:\nCMTN\n\u0010\nT1, T2\n\u0011\n= DualCMT\n\u0010\nWdWp(T1), T2\n\u0011\n, (3)\nwhere T1 is the X\n′\nin Eq. (1) and T2 is the Xacm in Eq. (1).\nDual-path Correlation Matching Transformation\nThe DualCMT shown in Fig. 3(a) aims to transform\nthe features from pooled high-resolution features to low-\nresolution ones to provide the low-resolution space with\nmore representative features via a correlation matching\nscheme for better restoration. Given the learned features\nXacm ∈ RH×W×3C from ACM in high-resolution space, we\nfirst exploit 1×1 convolution to generate channel-reduced\nfeatures, and then utilize dual-branch pooling including\nmax-pooling and mean-pooling to reduce the spatial di-\nmension to Ymax and Ymean ∈ R\nH\n8 ×W\n8 ×C. Then, the Ymax\nand Ymean are matched with the low-resolution features\nY ∈ R\nH\n8 ×W\n8 ×C to squeeze useless features to select more\nrepresentative features to replace the low-resolution features\nvia CMT (M (·, ·)) which is computed as the process in\nEq. (5):\nˆY = Wp(Xacm),\nYmax = Max-Pool(ˆY); Ymean = Mean-Pool(ˆY),\nYselected\nmax = M(Ymax, Y); Yselected\nmean = M(Ymean, Y).\n(4)\nThe CMT shown in Fig. 3(b) first computes the matrix\nsimilarity (MatSim(·, ·)) at the channel dimension between\ntwo tensors R1 ∈ R\nH\n8 ×W\n8 ×C and R2 ∈ R\nH\n8 ×W\n8 ×C after\nflattening to ˜R1 ∈ R\nHW\n64 ×C and ˜R2 ∈ R\nHW\n64 ×C to gener-\nate similarity matrix M ∈ RC×C. Then, we select the Top-1\n(Top1(·)) vector D ∈ RC×1 for each row tensor in M and\nsort the values ofD to produce the sorted indicesS ∈ RC×1.\nFinally, we select (Select C/r(·|·)) top C/r (r ≥ 1 means\nthe squeezing factor which controls the squeezing level) fea-\ntures Yselected ∈ R\nH\n8 ×W\n8 ×C\nr from R1 by the sorted indices\nS:\nM = MatSim(˜R1, ˜R2), # matrix similarity\nD = Top1(M); S = Sort(D), # sorted indices\nYselected = SelectC/r(R1|S). # select Top C/r maps\n(5)\nHere R1 and R2 can be respectively regarded as the pooled\nhigh-resolution and low-resolution features in Fig. 3(b).\nFinally, with the selected features from dual-path match-\ning results Yselected\nmax ∈ R\nH\n8 ×W\n8 ×C\nr and Yselected\nmean ∈ R\nH\n8 ×W\n8 ×C\nr ,\nwe concatenate (Concat[·]) them and then use Gated Feature\nRefinement (GFR(·)) to refine the features:\nYselected\nconcat = Concat\n\u0002\nYselected\nmax , Yselected\nmean\n\u0003\n,\nGFR(Yselected\nconcat ) =Wp\n\u0010\nWpWdWp\n\u0000\nYselected\nconcat\n\u0001\n⊙Yselected\nconcat\n\u0011\n.\n(6)\nAdaptive Channel Modulator\nThe ACM, shown in Fig. 2(d), aims to adaptively modu-\nlate the high-resolution features to better balance the im-\nportance of channel-wise features, enabling to provide more\nrepresentative features for low-resolution space for better\nrestoration. Given the multi-level high-resolution tensors\n{X1, X2, X3} ∈RH×W×C, we first concatenate them to\ngenerate a wider tensor Xconcat ∈ RH×W×3C. Then, we\nuse 1×1 convolution and 3×3 depth-wise convolution to\nexpand the concatenated features Xconcat to wider features\nand split the features into two tensors: Z1 ∈ RH×W×3C and\nZ2 ∈ RH×W×3C. Next, we conduct the softmax Schannel at\nchannel dimension on Z1 to obtain channel-wise weights\nand finally conduct element-wise addition and element-wise\nmultiplication among Xconcat, Z1, and Z2:\nACM\n\u0000\nX1, X2, X3\n\u0001\n= Xconcat⊙Schannel(Z1) +Z2,\nwhere Xconcat = Concat\n\u0002\nX1, X2, X3\n\u0003\n,\nZ1, Z2 = Split\n\u0010\nWdWp\n\u0000\nXconcat\n\u0001\u0011\n.\n(7)\nExperiments\nWe evaluate UHDformer on benchmarks for 3 UHD image\nrestoration tasks: (a) low-light image enhancement, (b) im-\nage dehazing, and (c) image deblurring.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5339\nImplementation Details. The number of CMT-TBs, i.e., L,\nis 15, where we use residual learning (He et al. 2016) to\nconnect every 3 CMT-TB. The number of attention heads is\n8, and the number of channels C is 16. The matching fac-\ntor, i.e., r, is set as 4. We train models using AdamW opti-\nmizer with the initial learning rate 5e−4 gradually reduced\nto 1e−7 with the cosine annealing (Loshchilov and Hutter\n2017). The patch size is set as 512×512. To constrain the\ntraining of UHDformer, we use the same loss function (Kong\net al. 2023) with default parameters. All experiments are\nconducted on two NVIDIA 3090 GPUs.\nDatasets. We use UHD-LL (Li et al. 2023) to conduct UHD\nlow-light image enhancement. For image dehazing and de-\nblurring, we respectively re-collect the samples from the\ndatasets of (Zheng et al. 2021) and (Deng et al. 2021b)\nto form new benchmarks, named as UHD-Haze and UHD-\nBlur. The statistics of these 3 datasets are summarised in\nTab. 1. Besides using UHD images to train the models, we\nalso use commonly-used general image restoration datasets\nto train and then test on UHD images. Here, we respectively\nuse well-known LOL (Wei et al. 2018), SOTS-ITS (Li et al.\n2019), and GoPro (Nah, Hyun Kim, and Mu Lee 2017) as\ngeneral low-light image enhancement, dehazing, and deblur-\nring datasets, where their training samples are used to train\ndeep models.\nEvaluation. Following (Li et al. 2023), we adopt\ncommonly-used IQA PyTorch Toolbox 1 to compute the\nPSNR (Huynh-Thu and Ghanbari 2008) and SSIM (Wang\net al. 2004) scores of all compared methods and also report\nthe trainable parameters (Param). Since some methods can-\nnot directly process full-resolution UHD images, we have\nto adopt an additional manner to conduct the experiments.\nAccording to UHDFour (Li et al. 2023), resizing (RS) the\ninput to the largest size that the model can handle produces\nbetter results than splitting the input into four patches and\nthen stitching the result. Hence, we adopt the resizing strat-\negy for these methods and report whether models need to\nresize or not.\nMain Results\nLow-Light Image Enhancement Results. We evaluate\nUHD low-light image enhancement results on UHD-LL\nwith two training dataset sets, including LOL and UHD-\nLL. Tab. 2 shows that our UHDformer advances state-of-\nthe-art approaches in both these two training sets. Compared\nwith recent state-of-the-art UHDFour (Li et al. 2023), UHD-\nformer saves at least 98% training parameters while consis-\ntently advancing it under different training sets. We note that\nUHDFour trained on LOL (Wei et al. 2018) performs infe-\nrior results on UHD images. In contrast, our UHDformer\nstill keeps excellent enhancement performance on this set-\nting, indicating the strong robustness of our UHDformer.\nMoreover, UHDformer outperforms the methods which can\ndirectly handle full-resolution UHD images, e.g., Zhao et al.\nand URetinex (Wu et al. 2022), which further demonstrates\nthe superiorness of our UHDformer. Fig. 4 presents visual\n1https://github.com/chaofengc/IQA-PyTorch\ncomparisons on UHD-LL, where UHDformer is able to gen-\nerate results with more natural colors.\nImage Dehazing Results. Tab. 3 summarises the quanti-\ntative dehazing results on UHD-Haze with SOTS-ITS and\nUHD-Haze training sets. Compared to recent work Dehaze-\nFormer (Song et al. 2023) which cannot directly handle the\nUHD images, our UHDformer reduces 86% model sizes\nwhile achieving 0.774dB and 7.241dB PSNR gains on both\nSOTS-ITS and UHD-Haze training sets, respectively. We\nnotice that although GridNet (Liu et al. 2019), UHD (Zheng\net al. 2021), and MSBDN (Dong et al. 2020) do not need to\nresize the UHD input images, they are less effective to han-\ndle the UHD images on the SOTS-ITS training set, while\nour UHDformer still keeps competitive performance. Fig. 5\nshows that UHDformer is capable of producing clearer re-\nsults, while other methods always hand down extensive\nhaze.\nImage Deblurring Results. We evaluate UHD image de-\nblurring with GoPro (Nah, Hyun Kim, and Mu Lee 2017)\nand UHD-Blur training sets. Tab. 4 summarises the results,\nwhere UHDformer significantly advances current state-of-\nthe-art approaches on different training sets. Compared\nwith the recent state-of-the-art deblurring approach FFT-\nformer (Kong et al. 2023), UHDformer respectively ob-\ntains 2.811dB and 3.412dB PSNR gains on GoPro and\nUHD-Blur training sets. It is worth noticing that although\nDMPHN (Zhang et al. 2019), MIMO-Unet++ (Cho et al.\n2021), and MPRNet (Zamir et al. 2021) can handle the full-\nresolution UHD images, they consume at least 97.8% more\ntraining parameters compared UHDformer on the GoPro\ntraining set while dropping at least 0.946dB PSNR. Fig. 6\nprovides several visual UHD deblurring examples, where\nour UHDformer is able to produce sharper results while ex-\nisting state-of-the-art approaches cannot handle the UHD\nimages well.\nAblation Study\nWe use the UHD-LL dataset to conduct the ablation study\non the main designs of UHDformer.\nEffect on Dual-path Correlation Matching Transforma-\ntion. Since one of the core designs of our UHDformer is\nthe DualCMT, it is of great interest to analyze its effect\non restoration. To understand the impact of DualCMT, we\ndisable it in the CMTA or CMTN in Transformers or dis-\nable the Max-Pooling or Mean-Pooling operations in Du-\nalCMT to compare with the full model. Tab. 5 shows that\nour full model with DualCMT (Tab. 5(g)) gets the 1.682dB\nPSNR gains compared with the model without transforma-\ntion via DualCMT (Tab. 5(a)). Note that the DualCMT in\nboth CMTA (Tab. 5(b)) and CMTN (Tab. 5(c)) plays a pos-\nitive effect on image restoration, while Max-Pooling and\nDataset Training samples Testing samples Resolution\nUHD-LL 2,000 150 3,840 ×2, 160\nUHD-Haze 2,290 231 3,840 ×2, 160\nUHD-Blur 1,964 300 3,840 ×2, 160\nTable 1: Datasets Statistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5340\n(a) IQSXW (c) ReVWRUPeU (d) UIRUPeU (e) LLIRUPeU(b) GT (I) UHDIRXU (J) UHDfRUPeU\nFigure 4: Low-light image enhancement on UHD-LL. UHDformer is able to generate cleaner results with finer details.\n(a) IQSXW (c) UHD (d) ReVWRUPeU (e) UIRUPeU(b) GT (I) DeKa]eFRUPeU(J) UHDfRUPeU\nFigure 5: Image dehazing on UHD-Haze. UHDformer is able to generate much clearer dehazing results with finer structures.\nMethod Venue PSNR ↑ SSIM ↑ Param ↓ RS\nTraining Set on LOL\nSwinIR ICCVW’21 17.900 0.7379 11.5M (-97%) \"\nZhao et al. ICCV’21 18.604 0.6940 11.6M (-97%) %\nRestormer CVPR’22 19.728 0.7703 26.1M (-99%) \"\nUformer CVPR’22 18.168 0.7201 20.6M (-98%) \"\nLLFlow AAAI’22 19.596 0.7333 17.4M (-98%) %\nLLformer AAAI’23 21.440 0.7763 13.2M (-97%) \"\nUHDFour ICLR’23 14.771 0.3760 17.5M (-98%) %\nUHDformer - 22.615 0.7754 0.3393M %\nTraining Set on UHD-LL\nRestormer CVPR’22 21.536 0.8437 26.1M (-99%) \"\nUformer CVPR’22 21.303 0.8233 20.6M (-98%) \"\nLLformer AAAI’23 24.065 0.8580 13.2M (-97%) \"\nUHDFour ICLR’23 26.226 0.9000 17.5M (-98%) %\nUHDformer - 27.113 0.9271 0.3393M %\nTable 2: Low-light image enhancement. UHDformer with at\nleast 97% fewer parameters achieves the SOTA.\nMean-Pooling are also useful for further improving recov-\nery quality. Moreover, we also compare the model with di-\nrect max-pooling and mean-pooling transformation features\nfrom high-resolution space to low-resolution one without\nDualCMT (Tab. 5(f)), where our DualCMT achieves0.91dB\nPSNR gains, further demonstrating the effectiveness of our\nDualCMT. Fig. 7 shows that the DualCMT is able to help\nbetter enhance the visual quality towards more natural col-\nors.\nThe DualCMT involves exploiting the squeezing factor r\nMethod Venue PSNR ↑ SSIM ↑ Param ↓ RS\nTraining Set on SOTS-ITS\nGridNet ICCV’19 14.783 0.8466 0.96M (-64%) %\nMSBDN CVPR’20 15.043 0.8570 31.4M (-99%) %\nUHD ICCV’21 11.708 0.6569 34.5M (-99%) %\nRestormer CVPR’22 13.875 0.6405 26.1M (-99%) \"\nUformer CVPR’22 15.264 0.6724 20.6M (-98%) \"\nD4 CVPR’22 13.656 0.8290 22.9M (-99%) %\nDehazeFormer TIP’23 14.551 0.6710 2.5M (-86%) \"\nUHDformer - 15.325 0.8560 0.3393M %\nTraining Set on UHD-Haze\nUHD ICCV’21 18.043 0.8113 34.5M (-99%) %\nRestormer CVPR’22 12.718 0.6930 26.1M (-99%) \"\nUformer CVPR’22 19.828 0.7374 20.6M (-98%) \"\nDehazeFormer TIP’23 15.372 0.7245 2.5M (-86%) \"\nUHDformer - 22.586 0.9427 0.3393M %\nTable 3: Image dehazing. UHDformer with the fewest pa-\nrameters significantly advances state-of-the-art methods.\nto control the matching number of features, one may wonder\nto know the effect of r. Tab. 6 shows that the PSNR reaches\nthe best when r is 4, revealing that squeezing to fewer fea-\ntures which may reduce useless content to keep more repre-\nsentative features for post-learning is better than more.\nEffect on Adaptive Channel Modulator. The ACM adap-\ntively modulates the multi-level high-resolution features\nfrom the channel-wise perspective. Hence, it is necessary\nto analyze the impact of ACM on restoration quality by\ndisabling the component or replacing it with other exist-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5341\n(a) IQSXW (c) ReVWRUPeU (d) UfRUPeU (e) SWULSfRUPeU (b) GT (f) FFTfRUPeU (J) UHDfRUPeU\nFigure 6: Image deblurring on UHD-Blur. UHDformer is able to generate deblurring results with sharper structures.\nMethod Venue PSNR ↑ SSIM ↑ Param ↓ RS\nTraining Set on GoPro\nDMPHN CVPR’19 26.490 0.7985 21.7M (-98%) %\nMIMO-Unet++ ICCV’21 24.290 0.7354 16.1M (-98%) %\nMPRNet CVPR’21 24.571 0.7426 20.1M (-98%) %\nRestormer CVPR’22 24.872 0.7484 26.1M (-99%) \"\nUformer CVPR’22 24.382 0.7209 20.6M (-98%) \"\nStripformer ECCV’22 24.915 0.7463 19.7M (-98%) \"\nFFTformer CVPR’23 24.625 0.7396 16.6M (-98%) \"\nUHDformer - 27.436 0.8231 0.3393M %\nTraining Set on UHD-Blur\nMIMO-Unet++ ICCV’21 25.025 0.7517 16.1M (-98%) %\nRestormer CVPR’22 25.210 0.7522 26.1M (-99%) \"\nUformer CVPR’22 25.267 0.7515 20.6M (-98%) \"\nStripformer ECCV’22 25.052 0.7501 19.7M (-98%) \"\nFFTformer CVPR’23 25.409 0.7571 16.6M (-98%) \"\nUHDformer - 28.821 0.8440 0.3393M %\nTable 4: Image deblurring. UHDformer with at least 98%\nfewer parameters significantly advances state-of-the-arts.\nExperiment PSNR ↑ SSIM ↑\n(a) w/o DualCMT in CMTA & CMTN 25.431 0.9203\n(b) w/o DualCMT in CMTA 26.740 0.9264\n(c) w/o DualCMT in CMTN 26.616 0.9262\n(d) w/o Max-Pooling in DualCMT 25.642 0.9217\n(e) w/o Mean-Pooling in DualCMT 26.291 0.9263\n(f) Max- & Mean-Pooling Transformation 26.203 0.9252\n(g) Full Model (Ours) 27.113 0.9271\nTable 5: Ablation study on DualCMT.\n(a) Input (b) w/o\nDualCMT (c) Ours (d) GT\nFigure 7:\nVisual effect on DualCMT.\ning channel attention modules. Tab. 7 shows that our ACM\nis more effective than well-known ECA (Qilong Wang and\nHu 2020) and SE (Hu, Shen, and Sun 2018) modules. Espe-\nr 1 2 3 4 5\nPSNR ↑ 26.634 26.742 26.826 27.113 26.874\nSSIM ↑ 0.9269 0.9249 0.9252 0.9271 0.9275\nTable 6: Effect on squeezing factor r in DualCMT.\nExperiment PSNR ↑ SSIM ↑\n(a) w/o ACM 26.485 0.9252\n(b) w/ ECA (Qilong Wang and Hu 2020) 26.727 0.9240\n(c) w/ SE (Hu, Shen, and Sun 2018) 26.116 0.9255\n(d) w/ ACM (Ours) 27.113 0.9271\nTable 7: Ablation study on ACM.\n(a) Input (b) w/o ACM (c) Ours (d) GT\nFigure 8: Visual effect on ACM.\ncially, our ACM produces about1dB PSNR gains compared\nwith the SE channel attention, which adequately demon-\nstrates the effectiveness of our ACM. Fig. 8 shows that our\nmodel with ACM generates more natural results (Fig. 8(c)),\nwhile the model without ACM tends to underestimate the\nresults (Fig. 8(b)).\nConcluding Remarks\nWe have proposed a general UHDformer to solve UHD im-\nage restoration problems. To generate more useful and repre-\nsentative features for low-resolution space to facilitate better\nrestoration, we have proposed to build the feature transfor-\nmation from the high-resolution space to the low-resolution\none by the proposed DualCMT and ACM. Extensive experi-\nments have demonstrated that our UHDformer significantly\nreduces model sizes while favoring against state-of-the-art\napproaches under different training sets on 3 UHD image\nrestoration tasks, including low-light image enhancement,\nimage dehazing, and image deblurring.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5342\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv:1607.06450.\nCho, S.-J.; Ji, S.-W.; Hong, J.-P.; Jung, S.-W.; and Ko, S.-J.\n2021. Rethinking Coarse-to-Fine Approach in Single Image\nDeblurring. In ICCV.\nCui, X.; Wang, C.; Ren, D.; Chen, Y .; and Zhu, P. 2022.\nSemi-Supervised Image Deraining Using Knowledge Dis-\ntillation. IEEE TCSVT, 32(12): 8327–8341.\nDeng, S.; Ren, W.; Yan, Y .; Wang, T.; Song, F.; and Cao,\nX. 2021a. Multi-Scale Separable Network for Ultra-High-\nDefinition Video Deblurring. In ICCV, 14010–14019.\nDeng, S.; Ren, W.; Yan, Y .; Wang, T.; Song, F.; and Cao,\nX. 2021b. Multi-Scale Separable Network for Ultra-High-\nDefinition Video Deblurring. In ICCV, 14030–14039.\nDing, H.; Liu, C.; Wang, S.; and Jiang, X. 2021. Vision-\nLanguage Transformer and Query Generation for Referring\nSegmentation. In ICCV, 16301–16310.\nDong, H.; Pan, J.; Xiang, L.; Hu, Z.; Zhang, X.; Wang, F.;\nand Yang, M. 2020. Multi-Scale Boosted Dehazing Network\nWith Dense Feature Fusion. In CVPR, 2154–2164.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In CVPR, 770–778.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-Excitation\nNetworks. In CVPR, 7132–7141.\nHuynh-Thu, Q.; and Ghanbari, M. 2008. Scope of valid-\nity of PSNR in image/video quality assessment. Electronics\nLetters, 44(13): 800–801.\nJin, Y .; Lin, B.; Yan, W.; Yuan, Y .; Ye, W.; and Tan, R. T.\n2023. Enhancing visibility in nighttime haze images using\nguided apsf and gradient adaptive convolution. InACM MM,\n2446–2457.\nJin, Y .; Yan, W.; Yang, W.; and Tan, R. T. 2022. Structure\nRepresentation Network and Uncertainty Feedback Learn-\ning for Dense Non-Uniform Fog Removal. In ACCV, 2041–\n2058.\nJin, Y .; Yang, W.; and Tan, R. T. 2022. Unsupervised night\nimage enhancement: When layer decomposition meets light-\neffects suppression. In ECCV, 404–421.\nKong, L.; Dong, J.; Ge, J.; Li, M.; and Pan, J. 2023. Efficient\nFrequency Domain-Based Transformers for High-Quality\nImage Deblurring. In CVPR, 5886–5895.\nLi, B.; Ren, W.; Fu, D.; Tao, D.; Feng, D.; Zeng, W.; and\nWang, Z. 2019. Benchmarking Single-Image Dehazing and\nBeyond. TIP, 28(1): 492–505.\nLi, C.; Guo, C.-L.; Zhou, M.; Liang, Z.; Zhou, S.; Feng, R.;\nand Loy, C. C. 2023. Embedding Fourier for Ultra-High-\nDefinition Low-Light Image Enhancement. In ICLR.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021. SwinIR: Image restoration using swin\ntransformer. In ICCV Workshops.\nLiu, X.; Ma, Y .; Shi, Z.; and Chen, J. 2019. GridDehazeNet:\nAttention-Based Multi-Scale Network for Image Dehazing.\nIn ICCV, 7313–7322.\nLiu, Z.; Mao, H.; Wu, C.-Y .; Feichtenhofer, C.; Darrell, T.;\nand Xie, S. 2022. A ConvNet for the 2020s. CVPR.\nLoshchilov, I.; and Hutter, F. 2017. SGDR: Stochastic gra-\ndient descent with warm restarts. In ICLR.\nNah, S.; Hyun Kim, T.; and Mu Lee, K. 2017. Deep multi-\nscale convolutional neural network for dynamic scene de-\nblurring. In CVPR.\nPan, J.; Sun, D.; Pfister, H.; and Yang, M.-H. 2016. Blind\nimage deblurring using dark channel prior. In CVPR.\nQilong Wang, P. Z. P. L. W. Z., Banggu Wu; and Hu, Q.\n2020. ECA-Net: Efficient Channel Attention for Deep Con-\nvolutional Neural Networks. In CVPR.\nSong, Y .; He, Z.; Qian, H.; and Du, X. 2023. Vision Trans-\nformers for Single Image Dehazing. IEEE TIP, 32: 1927–\n1941.\nTsai, F.-J.; Peng, Y .-T.; Lin, Y .-Y .; Tsai, C.-C.; and Lin, C.-\nW. 2022. Stripformer: Strip Transformer for Fast Image De-\nblurring. In ECCV.\nWang, C.; Pan, J.; Wang, W.; Dong, J.; Wang, M.; Ju, Y .;\nand Chen, J. 2023. PromptRestorer: A Prompting Im-\nage Restoration Method with Degradation Perception. In\nNeurIPS.\nWang, C.; Wu, Y .; Su, Z.; and Chen, J. 2020a. Joint Self-\nAttention and Scale-Aggregation for Self-Calibrated De-\nraining Network. In Chen, C. W.; Cucchiara, R.; Hua, X.; Qi,\nG.; Ricci, E.; Zhang, Z.; and Zimmermann, R., eds., ACM\nMM, 2517–2525. ACM.\nWang, C.; Xing, X.; Wu, Y .; Su, Z.; and Chen, J. 2020b.\nDCSFN: Deep Cross-scale Fusion Network for Single Im-\nage Rain Removal. In ACM MM, 1643–1651. ACM.\nWang, Y .; Wan, R.; Yang, W.; Li, H.; Chau, L.-P.; and Kot,\nA. C. 2022a. Low-Light Image Enhancement with Normal-\nizing Flow. In AAAI.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE TIP, 13(4): 600–612.\nWang, Z.; Cun, X.; Bao, J.; and Liu, J. 2022b. Uformer: A\nGeneral U-Shaped Transformer for Image Restoration. In\nCVPR.\nWei, C.; Wang, W.; Yang, W.; and Liu, J. 2018. Deep\nRetinex Decomposition for Low-Light Enhancement. In\nBMVC, 155.\nWu, W.; Weng, J.; Zhang, P.; Wang, X.; Yang, W.; and Jiang,\nJ. 2022. URetinex-Net: Retinex-Based Deep Unfolding Net-\nwork for Low-Light Image Enhancement. In CVPR, 5901–\n5910.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient Transformer for\nHigh-Resolution Image Restoration. In CVPR, 5718–5729.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nYang, M.-H.; and Shao, L. 2021. Multi-Stage Progressive\nImage Restoration. In CVPR.\nZhang, H.; Dai, Y .; Li, H.; and Koniusz, P. 2019. Deep\nStacked Hierarchical Multi-Patch Network for Image De-\nblurring. In CVPR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5343\nZhao, L.; Lu, S.; Chen, T.; Yang, Z.; and Shamir, A. 2021.\nDeep Symmetric Network for Underexposed Image En-\nhancement with Recurrent Attentional Learning. In ICCV,\n12055–12064.\nZheng, Z.; Ren, W.; Cao, X.; Hu, X.; Wang, T.; Song, F.;\nand Jia, X. 2021. Ultra-High-Definition Image Dehazing via\nMulti-Guided Bilateral Learning. In CVPR, 16185–16194.\nZhu, H.; Wang, C.; Zhang, Y .; Su, Z.; and Zhao, G. 2020.\nPhysical Model Guided Deep Image Deraining. In ICME.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5344",
  "topic": "Transformation (genetics)",
  "concepts": [
    {
      "name": "Transformation (genetics)",
      "score": 0.6142803430557251
    },
    {
      "name": "Transformer",
      "score": 0.5376002192497253
    },
    {
      "name": "Digital image correlation",
      "score": 0.48457348346710205
    },
    {
      "name": "Matching (statistics)",
      "score": 0.46253374218940735
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46068504452705383
    },
    {
      "name": "Image restoration",
      "score": 0.4407910704612732
    },
    {
      "name": "Correlation",
      "score": 0.4360286295413971
    },
    {
      "name": "Computer science",
      "score": 0.38910984992980957
    },
    {
      "name": "Computer vision",
      "score": 0.3488665223121643
    },
    {
      "name": "Mathematics",
      "score": 0.27188050746917725
    },
    {
      "name": "Image (mathematics)",
      "score": 0.25720423460006714
    },
    {
      "name": "Engineering",
      "score": 0.19081470370292664
    },
    {
      "name": "Image processing",
      "score": 0.15678399801254272
    },
    {
      "name": "Statistics",
      "score": 0.1546642780303955
    },
    {
      "name": "Physics",
      "score": 0.08144640922546387
    },
    {
      "name": "Electrical engineering",
      "score": 0.07376191020011902
    },
    {
      "name": "Optics",
      "score": 0.07371097803115845
    },
    {
      "name": "Geometry",
      "score": 0.06074416637420654
    },
    {
      "name": "Chemistry",
      "score": 0.05526083707809448
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I184843921",
      "name": "Hebei University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I152815399",
      "name": "Singapore University of Technology and Design",
      "country": "SG"
    }
  ]
}