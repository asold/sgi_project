{
  "title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks",
  "url": "https://openalex.org/W4385573010",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2111010806",
      "name": "Yuxiang Wu",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2098329557",
      "name": "Yu Zhao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2671856145",
      "name": "Baotian Hu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A681779131",
      "name": "Pasquale Minervini",
      "affiliations": [
        "University College London",
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A43645529",
      "name": "Pontus Stenetorp",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A1976791985",
      "name": "Sebastian Riedel",
      "affiliations": [
        "University College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169283738",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4288284003",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3176627646",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W4296878971",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W3175627818",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3207095490",
    "https://openalex.org/W3134891661",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4288479587",
    "https://openalex.org/W4307653829",
    "https://openalex.org/W4285227756",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2970168256",
    "https://openalex.org/W2953212265",
    "https://openalex.org/W3118423943",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W4386566764",
    "https://openalex.org/W4286903770",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3156789018"
  ],
  "abstract": "Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) – it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 → 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5184–5196\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nAn Efficient Memory-Augmented Transformer for\nKnowledge-Intensive NLP Tasks\nYuxiang Wu† Yu Zhao ‡ Baotian Hu ‡∗ Pasquale Minervini §†\nPontus Stenetorp † Sebastian Riedel †\n† University College London, London, UK ‡ Harbin Institute of Technology, Shenzhen, PRC\n§ University of Edinburgh, Edinburgh, UK\n{yuxiang.wu, p.stenetorp, s.riedel}@cs.ucl.ac.uk p.minervini@ed.ac.uk\n20s151163@stu.hit.edu.cn hubaotian@hit.edu.cn\nAbstract\nAccess to external knowledge is essential for\nmany natural language processing tasks, such\nas question answering and dialogue. Exist-\ning methods often rely on a parametric model\nthat stores knowledge in its parameters, or use\na retrieval-augmented model that has access\nto an external knowledge source. Parametric\nand retrieval-augmented models have comple-\nmentary strengths in terms of computational\nefficiency and predictive accuracy. To com-\nbine the strength of both approaches, we pro-\npose the Efficient Memory-Augmented Trans-\nformer (EMAT) – it encodes external knowl-\nedge into a key-value memory and exploits the\nfast maximum inner product search for mem-\nory querying. We also introduce pre-training\ntasks that allow EMAT to encode informa-\ntive key-value representations, and to learn an\nimplicit strategy to integrate multiple mem-\nory slots into the transformer. Experiments\non various knowledge-intensive tasks such as\nquestion answering and dialogue datasets show\nthat, simply augmenting parametric models\n(T5-base) using our method produces more\naccurate results (e.g., 25.8 → 44.3 EM on\nNQ) while retaining a high throughput (e.g.,\n1000 queries/s on NQ). Compared to retrieval-\naugmented models, EMAT runs substantially\nfaster across the board and produces more ac-\ncurate results on WoW and ELI5.1\n1 Introduction\nNLP tasks often require knowledge that is not\nexplicitly provided with the input. For example,\nOpen-Domain Question Answering (ODQA) re-\nquires answering an open-domain question without\ngiven context passages (Chen et al., 2017), and like-\nwise for open-domain dialogue (Dinan et al., 2019).\nTo handle such tasks, one key challenge is storing\n*Corresponding author.\n1Our code and datasets are available at https://github.\ncom/uclnlp/EMAT.\nand accessing potentially large amounts of knowl-\nedge. One approach is a parametric method that\ntrains a sequence-to-sequence generator to repre-\nsent knowledge within model parameters. Petroni\net al. (2019) find that pre-trained Language Mod-\nels (PLMs) learn a partial knowledge base in their\nparameters, but its coverage is limited. Increasing\nmodel size can improve this issue (Raffel et al.,\n2020; Roberts et al., 2020; Brown et al., 2020);\nhowever, larger language models require signifi-\ncant computational resources.\nRetrieval-augmented models(Guu et al., 2020;\nLewis et al., 2020b; Izacard and Grave, 2021; Das\net al., 2022), on the other hand, retrieve relevant\npassages from an external knowledge source (e.g.,\nWikipedia), and use the retrieved passages to in-\nform generation. Despite being more accurate,\nretrieval-augmented models are often significantly\nmore costly computation-wise than their paramet-\nric counterparts, since they require retrieving, en-\ncoding, and integrating the external knowledge at\ninference time.\nTo combine the strengths of both parametric and\nretrieval-augmented models, we propose Efficient\nMemory-Augmented Transformers (EMATs) – an\nextension to Transformer-based models augmented\nwith an efficient key-value memory module. EMAT\nfirst encodes the external knowledge source into\nkey embeddings and value embeddings, to con-\nstruct the key-value memory (Section 3.1). We\nchoose PAQ (Lewis et al., 2021b), a large collection\nof question-answering generated from Wikipedia,\nas our knowledge source; and we encode the ques-\ntions as keys and answers as values. The trans-\nformer model produces dense query vector, re-\ntrieves from the key-value memory (Section 3.2),\nand integrates the returned dense key-value vec-\ntors at different encoder layers to enhance gener-\nation (Section 3.3). Different from previous ap-\nproaches (Lample et al., 2019; Fan et al., 2021;\nChen et al., 2022), our query representation is com-\n5184\nputed at an early transformer layer, whereas re-\ntrieved key and value embeddings are incorporated\ninto the model at a later layer. This design only\nrequires one forward pass through the transformer\nmodel, and allows memory retrieval to run concur-\nrently with the transformer forward pass, and hence\nreduces the computational overhead (see Fig. 1 for\nour architecture).\nWith this architecture, it is important that the\nkey-value memory accurately represent the knowl-\nedge source, and the transformer learns a strategy\nto incorporate the retrieved key-value representa-\ntions into the model. Therefore, we introduce pre-\ntraining tasks (Section 4.1), which include auto-\nencoding objectives to represent the questions and\nanswers, and a question answering task to learn an\nimplicit strategy to incorporate multiple key-value\nmemory slots. Our ablation study (Section 6.1)\nshows that our pre-training tasks are crucial for the\nperformance, and removing any of them could lead\nto more than 10pp drop in EM score on ODQA\ndatasets.\nOur contribution can be summarised as follows:\ni) we introduce EMAT, an efficient memory ac-\ncess module to augment the transformer architec-\nture; ii) we exploit PAQ as our knowledge source,\nand propose pre-training objectives to encode QA-\npairs as key-value memory and to learn integra-\ntion strategy to incorporate multiple memory slots\ninto transformers; iii) experimental results on vari-\nous knowledge-intensive tasks show that our pro-\nposed method significantly outperforms vanilla\ntransformer baselines, while retaining a similar in-\nference speed.\n2 Related Work\nRetrieve-and-Read Models for ODQA Open-\ndomain question answering is a task that aims to\nanswer a open-domain question without given con-\ntext passages. Many ODQA systems follow a two-\nsteps retrieve-and-read architecture (Chen et al.,\n2017) where, in the first step, a retriever model col-\nlects a set of relevant passages, and then a reader\nmodel processes the retrieved passages and pro-\nduces the answer (Min et al., 2019; Yang et al.,\n2019; Wang et al., 2019; Karpukhin et al., 2020;\nGuu et al., 2020; Lewis et al., 2020b; Izacard and\nGrave, 2021). Despite their high accuracy, retrieve-\nand-read systems have a high computational foot-\nprint, since they need to process a potentially large\nnumber of passages (Wu et al., 2020, 2021).\nEfficient OQDA Systems One simple approach\nto accelerate ODQA is Closed-Book QA(CBQA)\n– a sequence-to-sequence model (Sutskever et al.,\n2014; Kalchbrenner et al., 2014) such as T5 (Raf-\nfel et al., 2020) or BART (Lewis et al., 2020a) is\nfine-tuned on ODQA data, by training it to pro-\nduce the answer given the question. CBQA models\nare substantially faster than retrieve-and-read ap-\nproaches. However, since they solely rely on their\nparameters to store factual knowledge, their capac-\nity is limited by the model size, and hence they\noften produce less accurate results than retrieve-\nand-read methods (Lewis et al., 2021a; Liu et al.,\n2021). Another efficient approach is retrieving\nsemantically similar questions from a large collec-\ntion of QA pair and returning the corresponding\nanswers. Lewis et al. (2021b) propose PAQ, a 65\nmillion QA dataset that is constructed with the\nobjective to cover the most probably-asked ques-\ntions in Wikipedia. RePAQ (Lewis et al., 2021b), a\nretrieval-based QA system built on PAQ, won the\nEfficientQA competition (Min et al., 2020) in 2020,\noutperforming CBQA models by a large margin.\nIn this work, we choose PAQ as our knowledge\nsource, but different from RePAQ, we develop a\ngenerative model. Our results show that EMAT\noutperforms RePAQ while matching its efficiency.\nMemory-Augmented Transformers Geva et al.\n(2021) show that the Feed-Forward Network (FFN)\nlayers in Transformer-based language models be-\nhave similarly to like key-value memories, where\nkeys capture input patterns, and values map to the\noutput vocabulary. Based on this finding, Yao et al.\n(2022) propose to extend the FFN layers by con-\ncatenating a dense representation of the corpus to\nthe layer weights. Fan et al. (2021) introduce a neu-\nral module to access a fixed external memory, show-\ning that it can lead to significant improvements on\ndownstream generative dialogue modelling tasks.\nConcurrently to our work, Chen et al. (2022) pro-\npose QAMAT, a method to augment Transformer\nlayers with a key-value memory network encoding\nquestion-answer pairs. QAMAT requires two in-\nference steps through the encoder: one to retrieve\nmemory values, and another for concatenating the\nretrieved values to the input. In contrast, our pro-\nposed method only requires a single inference steps,\nresulting in a significantly smaller computational\nfootprint. Empirically, we show that our method is\n≈5 times faster than QAMAT, even when using\nfewer hardware resources.\n5185\n… …\n… …\n… …\n… …\nconcatenate\nconcat layer\nQuery\nConvLayer\nKey-Value Memory\nkey layerReturn Key-Value Pairs\nadd\nvalue layer\n“answer: Barack Obama”PREFIX\n“ques�on: who is … ?”PREFIX\nKey\nConvLayer\nValue\nDecoder\nDecoder\n“who is … ?”\n“Barack Obama”\nkey layer\nvalue layer\n… …\n… …\nDecoder\nOutput\nPREFIX Input\nFigure 1: Architecture of the proposed Efficient Key-Value Memory Augmented Transformers (EMAT): factual\nknowledge is stored in a key-value memory (Section 3.1) where keys and values correspond to questions and\nanswers, respectively; during inference, the model retrieves information from the memory via MIPS (Section 3.2)\nand uses it to condition the generation process.\n3 Efficient Memory-Augmented\nTransformer\nIn this work we propose Efficient Memory-\nAugmented Transformer (EMAT), a model archi-\ntecture that uses a key-value memory to store mil-\nlions of dense question-answer representations to\ninform its predictions (see Fig. 1). Given an input\nsequence X = (x1, ··· , x|X|), EMAT’s encoder\nfirst produces a dense query q to retrieve from the\nmemory M. The returned key-value representa-\ntions corresponding to the retrieved k key-value\npairs are Z = (z1, ··· , zk). Finally, the decoder\ngenerates the target sequence Y = (y1, ··· , y|Y|)\nconditioned on the input X and retrieved key-value\npairs Z.\n3.1 Key-Value Memory\nThe key-value memory M = (K, V) contains rep-\nresentations of keys K and values V, with each\nkey ki mapping to one value vi. Since we use\nPAQ (Lewis et al., 2021b) as our knowledge source,\neach key represents a question, and its value repre-\nsents the corresponding answer. We use EMAT’s\nencoder to encode the question and the answer sep-\narately, and it produces key and value embeddings\nfrom lk-th and lv-th layer of encoder respectively.\nFig. 1 (left) shows details regarding how key\n(question) and value (answer) are encoded in\nEMAT. To encode the key embeddings, we first\nconcatenate a prefix PREFIX of length P with the\nquestion q as input, and then obtain the hidden\nstates at thelk-th layer hlk = [hlk\n1 , ··· , hlk\nn], where\nn is the length of the question q prepended with\nPREFIX . Then, hlk is passed through a convolu-\ntional neural network layer to produce[c1, ··· , cn],\nand we use the prefix part as our final key repre-\nsentation k = [ c1, ··· , cP] ∈RP×h . For value\nembeddings, we prepend a prefix to the answer,\nfeed [PREFIX ; a] into the model, and use the pre-\nfix’s representation at the lv-th layer of encoder\nv = [hlv\n1 , ··· , hlv\nP ] ∈RP×h as our value represen-\ntation, where h is the size of hidden representa-\ntions.\n3.2 Memory Retrieval\nThe goal of the retriever is to retrieve relevant en-\ntries from the key-value memory M to inform the\ndownstream generation tasks. EMAT’s encoder\nembeds the question into a query q using the same\nprocedure as the key embeddings, described in Sec-\ntion 3.1. We conduct an extra step of flattening\nfor both q and k by averaging: ¯k = flatten(k) =\n5186\n1\nP\n∑P\nj=1 kj. The key-value encoder shares the pa-\nrameters with the question encoder, and we define\nthe query-key similarity by the inner product be-\ntween the flattened query representation and key\nrepresentation sim(q, k) = ⟨¯q, ¯k⟩. At inference\ntime, this operation can be efficiently computed us-\ning Maximum Inner Product Search (MIPS) to re-\ntrieve the top-k key-value pairs Z = {(ki, vi)}k\ni=1\nbased on the similarity. MIPS implementations\nsuch as faiss (Johnson et al., 2019) enable search-\ning across millions of vectors in milliseconds on\na CPU. The retrieved key-value pairs Z are then\nintegrated in later layers of EMAT’s encoder.\n3.3 Key-Value Integration\nOnce we have retrieves the top-k key-value pairs\nZ, they need to be incorporated into the model.\nMore specifically, in the lc-th layer, all the key em-\nbeddings in Z are ordered by their corresponding\nsimilarity scores, and concatenated into a matrix\nK′= [ki, ··· , kk] ∈RPk×h. Then it is prepended\nto the lc-th layer’s hidden states. To distinguish\nthe different keys, we additionally add relative po-\nsitional encodings to K′. In the lv-th layer, the\nvalue embedding in Z are concatenated in the same\nway to produce V′, and it is added to the posi-\ntions where their corresponding key embeddings\nare prepended to. The updated hidden states con-\ntinue the forward pass of the remaining transformer\nencoder layers. Finally, the decoder generates the\nanswer condition on the output of the encoder,\nwhich already integrates the retrieved key-value\nrepresentations.\n4 Training Pipeline of EMAT\n4.1 Pre-Training\nAuto-encoding Tasks We use T5-base’s pre-\ntrained parameters to initialise EMAT, but the pre-\nfix embeddings and key encoder’s convolutional\nlayer are trained from scratch. To obtain better rep-\nresentation of key and value, we pre-train EMAT\nwith auto-encoding training objectives. We use\nPAQ-L1, a simplified version of PAQ that consists\nof 14M QA pairs, as the pre-training corpus. The\nmodel is trained to recover the input question x\ngiven the key embeddings k, and the answer y\ngiven the value embeddings v, as shown in Fig. 1\n(left). The two tasks key auto-encoding (KAE) and\nvalue auto-encoding (V AE) can be formalised as:\nLKAE = −\n|X|∑\ni=1\nlog P(xi |k, x<i),\nLV AE= −\n|Y|∑\ni=1\nlog P(yi |v, y<i).\nGeneration Task Besides the problem of repre-\nsenting questions and answers in key-value mem-\nory M, we also need the model to make use of M\nfor downstream tasks. Thus, it is also critical to pre-\ntrain the model to learn the key-value integration\nmodule defined in Section 3.3. Since PAQ provides\na large number of QA pairs, we consider a genera-\ntion task built on PAQ to pre-train the model. More\nconcretely, for each QA pair (x, y) in PAQ, we use\nthe RePAQ model (Lewis et al., 2021b) to retrieve\n10 other relevant QA pairs from PAQ, and retrieve\ntheir corresponding keys K′\nx = [k1, ··· , k10] and\nvalues V′\nx = [v1, ··· , v10] from the memory M.\nThen, the model is trained to generate the answer\ny given the question x and the key-value embed-\ndings corresponding to the retrieved QA pairs. The\nobjective can be defined as follows:\nLGen = −\n|Y|∑\ni=1\nlog P(yi |x, K′\nx, V′\nx, y<i).\nWe adopt a multi-task pre-training objective to min-\nimise LKAE + LV AE+ LGen.\n4.2 Fine-Tuning on Downstream Tasks\nAfter pre-training, we fine-tune both the memory\nretrieval module and the generation of EMAT on\nthe downstream tasks.\nRetrieval Objective Learning to retrieve rele-\nvant key-value pairs that provide useful evidence\nto solve a given task can be challenging due to the\nlack of labelled data. To solve this problem, we\npropose a weakly-supervised method to optimise\nthe retriever. Specifically, we first rank all retrieved\nkey-value pairs retrieved from the memory M by\ntheir inner product scores. We consider the top\nretrieved key-value pairs: for each retrieved key-\nvalue pair, if its corresponding answer is lexically\nmatched with the target output, then the key-value\npair is selected as positive sample to optimise the\nretriever. For short output generation tasks such\nas ODQA, we match the answer corresponding to\nthe retrieved value with the target answer. For long\n5187\nsequence generation tasks such as open-domain\ndialogue and long-form QA, we normalise the tar-\nget sequence (i.e., lower-casing and removing stop\nwords), and check whether if the retrieved value\n(answer) is contained in the normalised target se-\nquence. Since these key-value pairs are more likely\nto lead to the correct answer, they can be used to\nprovide a weakly-supervised training signal to the\nretrieval component of EMAT.\nWe denote the selected positive key-value pairs\nas Z+ = ( z+\n1 , ··· , z+\nr ), where each pair z+\ni =\n(k+\ni , v+\ni ) is composed by a key component k+\ni and\na value component v+\ni . We sample a key-value pair\nz+\ni from Z+ based on the similarity between the\ncorresponding key k+\ni and the query q:\nPη(z+\ni |q) = exp(sim(q, k+\ni ))∑r\nj=1 exp(sim(q, k+\nj )),\nz+ ∼Pη(·| q, Z+).\nWe then select m negative pairs {z−\nj }m\nj=1 that do\nnot match the target sequence. Finally, the positive\npairs z+ and the negative pairs z−are used to train\nthe retriever, by optimising the following objective:\nLRet =\n−log exp(sim(q, k+\ni ))\nexp(sim(q, k+\ni )) + ∑m\nj=1 exp(sim(q, k−\nj )).\nMemory Caching for More Efficient Training\nAs described above, EMAT uses MIPS for retriev-\ning the key-value pairs that are the most relevant\nto solve the current task. However, updating the\nmemory M after each training update may not be\nfeasible when the number of entries in M is very\nlarge. To alleviate this problem, we design a mem-\nory cachingmechanism. At the beginning of each\ntraining epoch, we freeze the memory M and, for\neach training example, we retrieve the top-n key-\nvalue pairs. The memory M is updated only at the\nend of the epoch by re-encoding all entries in the\nknowledge source.\nOverall Fine-Tuning Objective The generator\nis optimised to generate the target y given the input\nx and the top-n retrieved key-value pairs Z:\nLGen = −\n|Y|∑\ni=1\nlog P(yi |x, Z, y<i),\nso the overall fine-tuning objective is LRet + LGen.\n4.3 Inference\nDuring inference, we use a fast Hierarchical Navi-\ngable Small World (HNSW, Malkov and Yashunin,\n2020) graph index, generated by faiss, to search\nand retrieve from the key-value memory M. If the\nlk < lc, the search process can run in parallel with\nthe evaluation of the layers lk + 1, ··· , lc −1 in\nEMAT. Since the search process can be efficiently\nexecuted on CPU, it does not increase the GPU\nmemory requirements of the model.\n5 Experiments\n5.1 Experimental Setup\nDatasets We consider several knowledge-\nintensive NLP tasks, including Open-Domain\nQuestion Answering (ODQA), Open-Domain\nDialogue (ODD), and Long-Form Question\nAnswering (LFQA). In ODQA, the aim is to\nanswer factual questions using a large collection of\ndocuments of diversified topics. We choose three\ncommonly used datasets – NaturalQuestions (NQ,\nKwiatkowski et al., 2019), TriviaQA (TQA, Joshi\net al., 2017), and WebQuestions (Berant et al.,\n2013). In addition, we consider two generation\ntasks from the Knowledge Intensive Language\nTasks (KILT, Petroni et al., 2021) benchmark\nto test whether our method generalises to tasks\nbeyond ODQA. Specifically, we consider Wizard-\nof-Wikipedia (WoW, Dinan et al., 2019) for ODD.\nThis task requires modelling long dialogue history\nand acquire relevant Wikipedia knowledge to\nproduce a response utterance. Furthermore, we\nconsider the Explain Like I’m Five (ELI5, Fan\net al., 2019) dataset for LFQA. In ELI5, answers\nare often more diverse and open-ended compared\nto ODQA, and they tend to be significantly longer –\nthey can be composed by several sentences.\nKnowledge Source We use PAQ (Lewis et al.,\n2021b) as our knowledge source, and encode\nquestion-answer pairs in the model’s key-value\nmemory. Since the generative model used to gen-\nerate the QA pairs in PAQ was trained on Natu-\nralQuestions and TriviaQA, PAQ has a high cov-\nerage for these two ODQA datasets. In this work,\nwe also evaluate on tasks beyond ODQA, where it\nis not clear how PAQ can be used. Therefore, our\nevaluation on ODD and LFQA aims to demonstrate\nthat EMAT generalises to different knowledge-\nintensive generation tasks using PAQ as the un-\nderlying knowledge source.\n5188\nBaselines We compare our method with three\ntypes of baselines: parametric models, retrieval-\nonly approaches, and retrieval-augmented mod-\nels. Parametric models fine-tune sequence-to-\nsequence PLMs such as T5 (Raffel et al., 2020) or\nBART (Lewis et al., 2020a) on a datasets, by cast-\ning each task as a sequence generation problem con-\nditioned on the input. In our experiments, we con-\nsider parametric models of multiple sizes, includ-\ning T5-base, T5-large, T5-3B, T5-11B (Roberts\net al., 2020), and BART-large (Lewis et al., 2020a).\nRetrieval-only approaches retrieve the most rele-\nvant information from the knowledge source (PAQ),\nand return the top answer as output. In ODQA\nbenchmark we use the RePAQ model proposed\nby Lewis et al. (2021b); in ODD and LFQA, we use\nthe EMAT key retrieval module described in Sec-\ntion 3.2 as the retriever. Retrieval-augmented mod-\nels such as RAG (Lewis et al., 2020b) or FiD (Izac-\nard and Grave, 2021) retrieve relevant passages\nfrom Wikipedia using a dense retriever such as\nDPR (Karpukhin et al., 2020), and then use the\nretrieved passages and the input sequence to condi-\ntion the generation process.\nPre-Training and Fine-Tuning Configurations\nWe base our EMAT on T5 (Raffel et al., 2020),\nand initialise our model with the pre-trained pa-\nrameters from T5-base. 2 To evaluate the speed\nand accuracy of our proposed method under dif-\nferent computation environments, we pre-train and\nfine-tune EMAT using two settings. In the former\nsetting, we set lk = 3, lc = 3, lv = 7, which em-\nulates an environment where key embeddings has\nfast access, but there is delay in acquiring value\nembeddings; we refer to this setting as Fast Key,\nSlow Value(FKSV). In the latter setting, lk = 3,\nlc = 10, lv = 11, which corresponds to a scenario\nwhere both key querying and value reading can\nhave significant delays. We refer to this setting as\nSlow Key, Slow Value(SKSV). All details on the\ntraining hyperparameters the hardware used in our\nexperiments are available in Appendix B.\n5.2 Open-Domain Question Answering\nTable 1 shows the experimental results on three\nODQA datasets: NQ (Kwiatkowski et al., 2019),\nTQA (Joshi et al., 2017), and WQ (Berant et al.,\n2013). We report the Exact Match (EM) scores and\nthe average inference speed measured by queries\n2We use the original version of T5 without SSM to ensure\nthat our results are comparable with the baselines.\nModel NQ TQA WQ\nEM Q/s EM EM\nParametric models\nT5-base (Roberts et al., 2020) 25.8 1600 24.4 26.6\nT5-large (Roberts et al., 2020) 27.6 570 29.5 27.7\nT5-3B (Roberts et al., 2020) 30.4 55 35.1 33.6\nT5-11B (Roberts et al., 2020) 32.6 - 42.3 37.2\nBART-large (Lewis et al., 2020a) 26.5 570 26.7 27.4\nRetrieval-only models\nDense Retriever (Lewis et al., 2021a) 26.7 - 28.9 -\nDensePhrases (Lee et al., 2021) 40.9 18 50.7 -\nRePAQ-base (Lewis et al., 2021b) 40.9 1400 39.7 29.4\nRePAQ-large (Lewis et al., 2021b) 41.2 1100 - -\nRePAQ-xlarge (Lewis et al., 2021b) 41.5 800 41.3 -\nRetrieval-augmented models\nREALM (Guu et al., 2020) 40.4 - 55.8 40.7\nDPR (Karpukhin et al., 2020) 41.5 2.7 57.9 42.4\nQAMAT (Chen et al., 2022) 44.7 240* 48.0 39.4\nRePAQ rerank (Lewis et al., 2021b) 45.7 55 48.9 37.6\nRAG (Lewis et al., 2020b) 44.5 9.6 56.8 45.2\nFiD-base (Izacard and Grave, 2021) 48.2 3.7 65.0 32.4\nFiD-large (Izacard and Grave, 2021) 51.4 1.4 67.6 -\nOurs\nEMAT-FKSV 44.3 1000 44.4 36.7\nEMAT-SKSV 43.3 1200 43.7 33.2\nTable 1: Exact Match (EM) results for EMAT in compar-\nison to recent state-of-the-art systems. ∗QAMAT runs\non 32 TPU-v3 with 1024GB TPU memory, whereas\nours run on A100 GPU with 40GB GPU memory.\nper second (Q/s). Compared with parametric\nmodels, our proposed method yields substantially\nhigher EM scores across three datasets. EMAT-\nFKSV outperforms T5-base, which share the same\nbackbone model, by 18.5, 20.0, 10.1 percentage\npoints on NQ, TQA and WQ respectively. These\nresults indicate that our method of augmenting\ntransformer with key-value memory effectively\nextends model’s knowledge capacity. Compared\nwith retrieval-only models, our method also demon-\nstrates strong performance. RePAQ baselines also\nexploit PAQ as knowledge source, and hence is\ncomparable with our method. Our EMAT-FKSV\noutperforms the best RePAQ retriever (RePAQ-\nlarge) by 2.8 and 3.1 percentage points on NQ\nand TQA respectively. Speed-wise, EMAT can\nanswer 1000-1200 Q/s, which is a high throughput\nin ODQA and is comparable to some of the fastest\nparametric models and retrieval-only models.\nIn ODQA, retrieval-augmented models are\nknown to be highly accurate, but are also com-\nputationally inefficient (Min et al., 2020). EMAT\nis significantly faster than these models. For ex-\nample, FiD-base uses the same backbone T5-base\nmodel as EMAT, but retrieves and concatenates 20\nto 100 passages from Wikipedia. Despite being\nless accurate on NQ and TQA, EMAT is two or-\n5189\nModel F1 R-L U/s\nParametric models\nTrans MemNet (Dinan et al., 2019) 11.85 10.11 -\nBART-large (Lewis et al., 2020a) 12.86 11.77 55\nT5-base (Raffel et al., 2020) 13.53 12.40 160\nRetrieval-augmented models\nBART + DPR (Petroni et al., 2021) 15.19 13.23 0.7\nRAG (Lewis et al., 2020b) 13.11 11.57 3.4\nRetrieval-only models\nRePAQ w/ EMAT key encoder 1.84 1.48 -\nOurs\nEMAT-FKSV 15.78 14.73 141\nEMAT-SKSV 15.35 14.68 150\nTable 2: Results on the Wizard-of-Wikipedia dataset\nfrom the KILT benchmark.\nders of magnitude faster than FiD-base and more\naccurate on WQ. On NaturalQuestions in terms\nof EM score, our method outperforms REALM\nand DPR, and is comparable with QAMAT and\nRAG. QAMAT (Chen et al., 2022) is a concur-\nrent work to ours and is the fastest among the\nretrieval-augmented models. But QAMAT runs\non 32 TPU-v3 (Jouppi et al., 2017), which have\nroughly 1024GB TPU memory, and the MIPS\nsearch is conducted on TPU. In contrast, EMAT\nruns on a single A100 GPU with 40GB GPU mem-\nory, and the MIPS search is executed on CPU. De-\nspite using substantially fewer resources, EMAT-\nFKSV is roughly 4.2 times faster than QAMAT,\nand EMAT-SKSV is 5 times faster.\n5.3 Generalisation to Open-Domain Dialogue\nand Long-Form QA\nOpen-Domain Dialogue Open-domain dialogue\nis a dialogue task that requires accessing knowl-\nedge from Wikipedia to produce dialogue response.\nTable 2 shows the results on the open-domain di-\nalogue dataset Wizard-of-Wikipedia (Dinan et al.,\n2019) from the KILT (Petroni et al., 2021) bench-\nmark. The utterances from dialogue history are\nconcatenated into a input sequence, and the output\nsequence is the corresponding response utterance.\nWe follow Petroni et al. (2021) and evaluate the\nmodels with F1 and ROUGE-L metrics, and we\nalso report the average number of utterances gener-\nated per second (U/s) for speed evaluation.\nThe results show that, our proposed EMAT out-\nperforms parametric models while retaining a sim-\nilar inference speed. EMAT-FKSV outperforms\nT5-base by 2.25 F1 and 2.28 ROUGE-L points,\nModel F1 R-L Q/s\nParametric models\nBART-large (Lewis et al., 2020a) 19.23 20.55 30\nT5-base (Raffel et al., 2020) 16.01 19.08 76\nRetrieval-augmented models\nBART + DPR (Petroni et al., 2021) 17.88 17.41 0.2\nRAG (Lewis et al., 2020b) 14.51 14.05 0.4\nRetrieval-only models\nRePAQ w/ EMAT key encoder 1.40 1.65 -\nOurs\nEMAT-FKSV 18.42 20.61 67\nEMAT-SKSV 19.03 20.91 71\nTable 3: Results on the ELI5 dataset from the KILT\nbenchmark.\nwhile generating 141 utterances per second. Sur-\nprisingly, EMAT models also outperform retrieval-\naugmented models such as RAG and BART+DPR,\nwhich exploits Wikipedia as knowledge source. It\nindicates that our method that encodes PAQ as key-\nvalue memory is capable of represent crucial in-\nformation in Wikipedia, and generalises well to\ntasks beyond ODQA. We also implement a RePAQ-\nequivalent retrieval-only model using EMAT’s key\nencoder. Since PAQ is a collection of QA pairs,\nsimply retrieving relevant QA pairs for dialogue\ndoes not work well. The large gap between EMAT\nand RePAQ with EMAT key encoder, together with\nthe qualitative analysis in Section 6.2, demonstrates\nthat EMAT decoder does not simply copy informa-\ntion from the key-value memory, but exploits the\nkey-value embeddings to generate novel response.\nLong-Form Question Answering Results on the\nLFQA task ELI5 (Fan et al., 2019) (shown in Ta-\nble 3) reveals similar conclusions as in WoW. Both\nEMAT-FKSV and EMAT-SKSV outperform the\nT5-base baseline by a large margin, 2.41pp and\n3.01pp in F1, while retaining an inference speed\nto 67 Q/s and 71 Q/s, respectively. EMAT is both\nfaster and more accurate than retrieval-augmented\nmodels on ELI5 too. Compared to RAG, EMAT-\nSKSV is 4.52pp better in F1, 6.86pp better in\nROUGE-L, and more than 160 times faster in in-\nference speed.\n6 Analysis\n6.1 Ablation Study\nWe conduct ablation study on the pre-training steps\nand the results are shown in Table 4. Without fine-\ntuning, the pre-trained EMAT outperforms fine-\n5190\nModel NQ TQA WQ\nEMAT-FKSV 44.3 44.4 36.7\n−fine-tune 30.6 32.4 25.6\n−auto-encoding tasks 28.5 34.6 12.9\n−generation task 28.7 24.7 31.4\n−all pre-training tasks 27.1 17.7 6.0\nTable 4: Ablation on the pre-training steps used by\nEMAT, described in Section 4.1, measured using EM on\nNQ, TQA, and WQ: we analyse the impact of removing\nauto-encoding, generation, and all pre-training tasks\nfrom EMAT’s pre-training phase.\ntuned T5-large on NQ and TQA, and has a com-\npetitive result on WQ. When we remove the auto-\nencoding (KAE and V AE) tasks, the performance\non NQ and WQ drops significantly (36.7 →12.9\non WQ). Ablating the generation task results in\nsubstantially worse EM on NQ and TQA (44.4 →\n24.7 on TQA) The ablation results demonstrate\nthat both auto-encoding task and generation task\nare crucial to EMAT’s performance. Without all\nthe pre-training tasks, EMAT perform very poorly,\nand even worse than T5-base baseline. This may\nbe due to the fact that the key-value memory is\nnot well learned and hence incorporating them will\nintroduce noise to the model, thus leads to poor\npredictions.\n6.2 Qualitative Analysis\nTable 5 shows some examples from NQ and WoW.\nThe presented QA pairs correspond to the top-5\nretrieved dense key-value pairs. In NQ, we can see\nthat EMAT retrieves useful key-value and generates\ncorrect answer from the first example. Different\nfrom retrieval-only models that only output the\ntop-1 retrieved QA, EMAT conducted some sort\nof reranking, and the decoder manages to use the\nright key-value to generate the answer. In another\nexample presented in Table 6, it demonstrates that\nEMAT’s output is not always from retrieved values.\nIt will ignore the irrelevant key-value pairs, also\nuses evidences from keys, which are impossible for\nretrieval-only models.\nIn the example from WoW, it requires using the\nfine-grained knowledge 19th centuryto generate\nresponse. We can see that EMAT retrieves context-\nrelated key-value pairs, and mainly uses the two\nunderlined evidences to generate response. In con-\ntrast, T5-base generates hallucinated response, pro-\nducing the wrong time “18th century”. This shows\nthat, with memory augmentation, EMAT generates\nNatural Question\nQ: who plays the judge in drop dead diva?A:Lex Medlin\nEMAT:Lex Medlin\nRetrieved Key-Values\nQ: who plays jane on drop dead diva?A: Brooke Elliott\nQ: who played fred in drop dead diva?A: Beverly Hills\nQ: who played empress katia on drop dead diva?A: Tobin\nQ: who plays judge french in drop dead divorce season 4?A:Lex Medlin\nQ: who played ian holt on drop dead diva?A: Jeffrey Pierce\nWizard-of-Wikipedia\nDialogue History\nApprentice: I like jazz.\nWizard: That’s great! Jazz ... is originated in theafrican-american communitie.\nApprentice: When did it originate?\nResponse\nTarget: Jazz originated in thelate 19th century\nT5-base: It was first recorded in the late 18th century\nEMAT: It originated inlate 19th centuryinnew orleans\nRetrieved Key-Values\nQ: where did the genre of jazz originate?A:New Orleans, US\nQ: when did jazz music start in the united states?A: 1920s\nQ: what type of music was jazz originally?A: dance music\nQ: what genre of music does rock come from?A: blues\nQ: what genre of music is hip hop?A: rap\nTable 5: Examples from NQ and WoW. Noting that\nEMAT only retrieves and integrates dense key-value\npairs, but not accesses the presented text-based QAs.\na more faithful and informative response than T5-\nbase.\nMore examples can be found in Table 6 in the\nappendix. We find that EMAT retrieves useful key-\nvalue pairs and makes full use of them to generate\nanswers. This analysis also demonstrates the inter-\npretability of EMAT, and the feasibility of only us-\ning dense key-value embeddings to provide knowl-\nedge.\n7 Conclusions\nIn this work, we propose the Efficient Memory-\nAugmented Transformer (EMAT) that combines\nthe strength of parametric model and retrieval-\naugmented model. It encodes external knowledge\ninto a key-value memory and exploits the fast MIPS\nsearch for memory querying. We introduce pre-\ntraining tasks to learn better key-value representa-\ntions and integration of multiple memory slots into\ntransformer. Experiments on knowledge intensive\ntasks, including open-domain question answering,\ndialogue and long-form question answering, show\nboth the accuracy and speediness of EMAT. In the\nfuture, we will seek to improve integrate more di-\nverse knowledge into the memory and generalise\nour method to more downstream tasks.\n5191\nLimitations\nOne limitation is that the memory retrieval module\nrequires weak supervision to train with. This may\nmean that we define different weak supervision\nlabels when apply to different downstream tasks.\nOne could use end-to-end training techniques such\nas the ones proposed by Paranjape et al. (2021);\nLewis et al. (2020b), to train the memory retrieval\nmodule with gradients from the decoder, and we\nleave this as future work. Another potential limita-\ntion is that, we need to store the dense key-value\nmemory M, which requires around 300GB CPU\nRAM. But since it is relatively easy to get machine\nwith more CPU RAM than GPU memory, and the\nfact that most deep learning workstations can reach\nthis requirement, we believe this is not too much a\nconstraint. Besides, we can use LRU cache to save\nRAM in low memory resource situations.\nAcknowledgements\nPasquale and Pontus were partially funded by\nthe European Union’s Horizon 2020 research\nand innovation programme under grant agree-\nment no. 875160, and by an industry grant\nfrom Cisco. Baotian Hu and Yu Zhao were\nfunded by grants: Natural Science Founda-\ntion of China (No.62006061), Stable Support\nProgram for Higher Education Institutions of\nShenzhen (No. GXWD20201230155427003-\n20200824155011001). The authors would also like\nto thank Patrick Lewis, Wenhu Chen, and Zetian\nSun for their help and feedback.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nWenhu Chen, Pat Verga, Michiel de Jong, John Wieting,\nand William Cohen. 2022. Augmenting pre-trained\nlanguage models with qa-memory for open-domain\nquestion answering. CoRR, abs/2204.04581.\nRajarshi Das, Patrick Lewis, Sewon Min, June Thai,\nand Manzil Zaheer, editors. 2022. Proceedings of the\n1st Workshop on Semiparametric Methods in NLP:\nDecoupling Logic from Knowledge. Association for\nComputational Linguistics, Dublin, Ireland and On-\nline.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net.\nAngela Fan, Claire Gardent, Chloé Braud, and Antoine\nBordes. 2021. Augmenting transformers with KNN-\nbased composite memory for dialog. Transactions of\nthe Association for Computational Linguistics, 9:82–\n99.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558–3567, Florence,\nItaly. Association for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\n5192\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nNorman P. Jouppi, Cliff Young, Nishant Patil, David A.\nPatterson, Gaurav Agrawal, Raminder Bajwa, Sarah\nBates, Suresh Bhatia, Nan Boden, Al Borchers,\nRick Boyle, Pierre-luc Cantin, Clifford Chao, Chris\nClark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey\nDean, Ben Gelb, Tara Vazir Ghaemmaghami, Ra-\njendra Gottipati, William Gulland, Robert Hagmann,\nC. Richard Ho, Doug Hogberg, John Hu, Robert\nHundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek\nJaworski, Alexander Kaplan, Harshit Khaitan, Daniel\nKillebrew, Andy Koch, Naveen Kumar, Steve Lacy,\nJames Laudon, James Law, Diemthu Le, Chris Leary,\nZhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon\nMacKean, Adriana Maggiore, Maire Mahony, Kieran\nMiller, Rahul Nagarajan, Ravi Narayanaswami, Ray\nNi, Kathy Nix, Thomas Norrie, Mark Omernick,\nNarayana Penukonda, Andy Phelps, Jonathan Ross,\nMatt Ross, Amir Salek, Emad Samadiani, Chris Sev-\nern, Gregory Sizikov, Matthew Snelham, Jed Souter,\nDan Steinberg, Andy Swing, Mercedes Tan, Gre-\ngory Thorson, Bo Tian, Horia Toma, Erick Tuttle,\nVijay Vasudevan, Richard Walter, Walter Wang, Eric\nWilcox, and Doe Hyun Yoon. 2017. In-datacenter\nperformance analysis of a tensor processing unit. In\nISCA, pages 1–12. ACM.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blun-\nsom. 2014. A convolutional neural network for mod-\nelling sentences. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 655–665,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHervé Jégou. 2019. Large memory layers with\nproduct keys. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 8546–8557.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021. Learning dense representations of\nphrases at scale. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6634–6647, Online. Association for\nComputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021a. Question and answer test-train overlap in\nopen-domain question answering datasets. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1000–1008, Online.\nAssociation for Computational Linguistics.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Min-\nervini, Heinrich Küttler, Aleksandra Piktus, Pontus\nStenetorp, and Sebastian Riedel. 2021b. PAQ: 65\nmillion probably-asked questions and what you can\ndo with them. Transactions of the Association for\nComputational Linguistics, 9:1098–1115.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nLinqing Liu, Patrick S. H. Lewis, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Challenges in gener-\nalization in open domain question answering. CoRR,\nabs/2109.01156.\nYury A. Malkov and Dmitry A. Yashunin. 2020. Effi-\ncient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs.\nIEEE Trans. Pattern Anal. Mach. Intell., 42(4):824–\n836.\nSewon Min, Jordan L. Boyd-Graber, Chris Alberti,\nDanqi Chen, Eunsol Choi, Michael Collins, Kelvin\n5193\nGuu, Hannaneh Hajishirzi, Kenton Lee, Jenni-\nmaria Palomaki, Colin Raffel, Adam Roberts, Tom\nKwiatkowski, Patrick S. H. Lewis, Yuxiang Wu,\nHeinrich Küttler, Linqing Liu, Pasquale Minervini,\nPontus Stenetorp, Sebastian Riedel, Sohee Yang,\nMinjoon Seo, Gautier Izacard, Fabio Petroni, Lu-\ncas Hosseini, Nicola De Cao, Edouard Grave,\nIkuya Yamada, Sonse Shimaoka, Masatoshi Suzuki,\nShumpei Miyawaki, Shun Sato, Ryo Takahashi, Jun\nSuzuki, Martin Fajcik, Martin Docekal, Karel On-\ndrej, Pavel Smrz, Hao Cheng, Yelong Shen, Xi-\naodong Liu, Pengcheng He, Weizhu Chen, Jianfeng\nGao, Barlas Oguz, Xilun Chen, Vladimir Karpukhin,\nStan Peshterliev, Dmytro Okhonko, Michael Sejr\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\nWen-tau Yih. 2020. Neurips 2020 efficientqa com-\npetition: Systems, analyses and lessons learned. In\nNeurIPS (Competition and Demos), volume 133 of\nProceedings of Machine Learning Research, pages\n86–111. PMLR.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019. A discrete hard EM ap-\nproach for weakly supervised question answering. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2851–\n2864, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D. Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation. ArXiv preprint,\nabs/2110.07752.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Mon-\ntreal, Quebec, Canada, pages 3104–3112.\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallap-\nati, and Bing Xiang. 2019. Multi-passage BERT: A\nglobally normalized BERT model for open-domain\nquestion answering. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5878–5882, Hong Kong, China. As-\nsociation for Computational Linguistics.\nYuxiang Wu, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2021. Training adaptive com-\nputation for open-domain question answering with\ncomputational constraints. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 447–453, Online. Association\nfor Computational Linguistics.\nYuxiang Wu, Sebastian Riedel, Pasquale Minervini, and\nPontus Stenetorp. 2020. Don’t read too much into\nit: Adaptive computation for open-domain question\nanswering. In EMNLP (1), pages 3029–3039. Asso-\nciation for Computational Linguistics.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 72–77, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nYunzhi Yao, Shaohan Huang, Ningyu Zhang, Li Dong,\nFuru Wei, and Huajun Chen. 2022. Kformer: Knowl-\nedge injection in transformer feed-forward layers.\nCoRR, abs/2201.05742.\n5194\nA Data Efficiency\nFig. 2 shows how the number of retrieved key-value\npairs from PAQ-L1 influences the downstream EM\nscore on Natural Questions, TriviaQA, and We-\nbQuestions. The results show that, as the number\nof retrieved memory entries increases, EMAT’s EM\nscore monotonically increases, which indicates that\nthe model can handle noise in the retrieved memory\nentries and benefit from larger number of retrieved\nmemory entries. In Fig. 3 we analyse the scaling\neffects induced by using larger subsets of PAQ for\ncreating the key-value memory M. The results\ndemonstrate that EMAT’s predictive accuracy in-\ncreases with the number of PAQ questions across\nall considered ODQA datasets.\n/uni000003ee/uni000003ec/uni000003ef/uni000003ec/uni000003f0/uni000003ec/uni000003f1/uni000003ec/uni000003f2/uni000003ec\n/uni0000005e/uni0000015d/uni000001cc/uni0000011e/uni00000003/uni0000017d/uni00000128/uni00000003/uni00000057/uni00000004/uni00000059/uni00000003/uni0000036c/uni00000175/uni0000015d/uni0000016f/uni0000016f/uni0000015d/uni0000017d/uni00000176\n/uni000003ef/uni000003ee\n/uni000003ef/uni000003f0\n/uni000003ef/uni000003f2\n/uni000003ef/uni000003f4\n/uni000003f0/uni000003ec\n/uni000003f0/uni000003ee\n/uni000003f0/uni000003f0/uni0000002c/uni0000015d/uni0000019a/uni0000039b/uni000003ed/uni00000003/uni0000005e/uni00000110/uni0000017d/uni0000018c/uni0000011e\n/uni00000045/uni00000059/uni00000003/uni0000037e/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni00000102/uni0000016f/uni0000037f\n/uni00000064/uni00000059/uni00000004/uni00000003/uni0000037e/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni00000102/uni0000016f/uni0000037f\n/uni00000074/uni00000059/uni00000003/uni0000037e/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni00000102/uni0000016f/uni0000037f\na) EMAT-retriever Hit@1\n/uni000003ee/uni000003ec/uni000003ef/uni000003ec/uni000003f0/uni000003ec/uni000003f1/uni000003ec/uni000003f2/uni000003ec\n/uni0000005e/uni0000015d/uni000001cc/uni0000011e/uni00000003/uni0000017d/uni00000128/uni00000003/uni00000057/uni00000004/uni00000059/uni00000003/uni0000036c/uni00000175/uni0000015d/uni0000016f/uni0000016f/uni0000015d/uni0000017d/uni00000176\n/uni000003ef/uni000003f2\n/uni000003ef/uni000003f4\n/uni000003f0/uni000003ec\n/uni000003f0/uni000003ee\n/uni000003f0/uni000003f0/uni0000001c/uni000001c6/uni00000102/uni00000110/uni0000019a/uni00000003/uni00000044/uni00000102/uni0000019a/uni00000110/uni0000015a\n/uni00000045/uni00000059/uni00000003/uni0000037e/uni00000150/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni0000037f\n/uni00000064/uni00000059/uni00000004/uni00000003/uni0000037e/uni00000150/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni0000037f\n/uni00000074/uni00000059/uni00000003/uni0000037e/uni00000150/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni0000037f\nb) EMAT-generator EM\nFigure 3: Analysis of how the number of PAQ entries\nused to populate the memory M influences the down-\nstream predictive accuracy on several ODQA datasets.\nB Hyperparameters\nModel Settings The length of PREFIX is 2 in\nEMAT. EMAT contains 225M parameters, and\nT5-base contains 221M parameters. The memory\ncache size is set to384 in all downstream tasks. The\nretrieval loss weight and generation loss weight are\nboth set to 1.\nPre-Training We pre-train for 5 epochs on PAQ-\nL1, using learning rate warm-ups for the first 5000\ntraining steps to 10−4, and linear rate decay in the\nremaining steps. For each QA in PAQ-L1, we use\nRePAQ to retrieve10 relevant QAs from PAQ-L1.\nTo force the model use relevant QAs’ information,\nwe sample 10% examples to retain itself in the\nrelevant QA set. The weights of auto-encoding loss\nand generation loss is set to 0.5 and 1.0.\nODQA For NQ and TQA, the learning rate warm-\nups for the first 1000 steps to 5 ×10−5, and linear\nrate decay in the remaining steps. For WQ, the\nlearning rate is fixed to 4 ×10−5 during training.\nWe fine-tune 30 epochs on ODQA tasks, using\nearly stop with patients of 8 epochs. We use greedy\ndecoding algorithm to generate answers.\nWoW We fine-tune 20 epochs on WoW with\n8 ×10−5 learning rate. The scheduler is same\nto ODQA. We use greedy decoding algorithm to\ngenerate responses.\nELI5 We fine-tune 8 epochs on ELI5 with 5 ×\n10−5 learning rate. The scheduler is same to\nODQA. We use beam-sample decoding algorithm\nto generate answers, where beam-size is 5, top-k\nis 64. We force the model do not generate repeat\nphrases by setting no_repeat_n_gram to 8.\nHardware The machine used to measure the\nspeed is a machine learning workstation with In-\ntel(R) Xeon(R) Platinum 8358 CPU, 512GB of\nCPU RAM and one 40GB NVIDIA A100 GPU.\n/uni000003ee/uni000003f0/uni000003f2/uni000003f4/uni000003ed/uni000003ec\n/uni00000045/uni000001b5/uni00000175/uni0000010f/uni0000011e/uni0000018c/uni00000003/uni0000017d/uni00000128/uni00000003/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni0000011e/uni0000011a/uni00000003/uni0000016c/uni0000011e/uni000001c7/uni00000372/uni000001c0/uni00000102/uni0000016f/uni000001b5/uni0000011e/uni00000003/uni00000189/uni00000102/uni0000015d/uni0000018c/uni00000190\n/uni000003ef/uni000003f5/uni00000358/uni000003f1\n/uni000003f0/uni000003ec/uni00000358/uni000003ec\n/uni000003f0/uni000003ec/uni00000358/uni000003f1\n/uni000003f0/uni000003ed/uni00000358/uni000003ec\n/uni000003f0/uni000003ed/uni00000358/uni000003f1/uni0000001c/uni000001c6/uni00000102/uni00000110/uni0000019a/uni00000003/uni00000044/uni00000102/uni0000019a/uni00000110/uni0000015a\n/uni00000045/uni00000059/uni00000003/uni0000037e/uni00000150/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni0000037f\n/uni00000045/uni00000059/uni00000003/uni0000037e/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni00000102/uni0000016f/uni0000037f\na) Natural Questions\n/uni000003ee/uni000003f0/uni000003f2/uni000003f4/uni000003ed/uni000003ec\n/uni00000045/uni000001b5/uni00000175/uni0000010f/uni0000011e/uni0000018c/uni00000003/uni0000017d/uni00000128/uni00000003/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni0000011e/uni0000011a/uni00000003/uni0000016c/uni0000011e/uni000001c7/uni00000372/uni000001c0/uni00000102/uni0000016f/uni000001b5/uni0000011e/uni00000003/uni00000189/uni00000102/uni0000015d/uni0000018c/uni00000190\n/uni000003f0/uni000003ec/uni00000358/uni000003ec\n/uni000003f0/uni000003ec/uni00000358/uni000003f1\n/uni000003f0/uni000003ed/uni00000358/uni000003ec\n/uni000003f0/uni000003ed/uni00000358/uni000003f1\n/uni000003f0/uni000003ee/uni00000358/uni000003ec/uni0000001c/uni000001c6/uni00000102/uni00000110/uni0000019a/uni00000003/uni00000044/uni00000102/uni0000019a/uni00000110/uni0000015a\n/uni00000064/uni00000059/uni00000004/uni00000003/uni0000037e/uni00000150/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni0000037f\n/uni00000064/uni00000059/uni00000004/uni00000003/uni0000037e/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni00000102/uni0000016f/uni0000037f b) TriviaQA\n/uni000003ee/uni000003f0/uni000003f2/uni000003f4/uni000003ed/uni000003ec\n/uni00000045/uni000001b5/uni00000175/uni0000010f/uni0000011e/uni0000018c/uni00000003/uni0000017d/uni00000128/uni00000003/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni0000011e/uni0000011a/uni00000003/uni0000016c/uni0000011e/uni000001c7/uni00000372/uni000001c0/uni00000102/uni0000016f/uni000001b5/uni0000011e/uni00000003/uni00000189/uni00000102/uni0000015d/uni0000018c/uni00000190\n/uni000003ef/uni000003ee\n/uni000003ef/uni000003f0\n/uni000003ef/uni000003f2\n/uni000003ef/uni000003f4\n/uni000003f0/uni000003ec\n/uni000003f0/uni000003ee/uni0000001c/uni000001c6/uni00000102/uni00000110/uni0000019a/uni00000003/uni00000044/uni00000102/uni0000019a/uni00000110/uni0000015a\n/uni00000074/uni00000059/uni00000003/uni0000037e/uni00000150/uni0000011e/uni00000176/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni0000037f\n/uni00000074/uni00000059/uni00000003/uni0000037e/uni0000018c/uni0000011e/uni0000019a/uni0000018c/uni0000015d/uni0000011e/uni000001c0/uni00000102/uni0000016f/uni0000037f c) WebQuestions\nFigure 2: Analysis of how changing the number of retrieved key-value pairs influences the downstream Exact Match\naccuracy on several ODQA datasets.\n5195\nNatural Questions\nQuestion who plays the judge in drop dead diva\nAnswer [Lex Medlin]\nEMAT Predict : Lex Medlin\nRetrieved question: who plays jane on drop dead diva? answer: Brooke Elliott\nquestion: who plays judge french in drop dead divorce season 4? answer: Lex Medlin\nquestion: who played fred in drop dead diva? answer: Beverly Hills, California\nQuestion how long did the menendez brothers get in prison for killing their parents\nAnswer [life imprisonment , life]\nEMAT Predict : life\nRetrieved question: when did the menendez brothers kill their parents? answer: 1989\nquestion: where did the menendez brothers kill their parents? answer: Beverly Hills, California\nquestion: who sentenced the menendez brothers to life in prison? answer: Judge Weisberg\nQuestion how long is a whale shark in meters\nAnswer [12.65m, estimated at 9.7m, 9.7m]\nPredict : few meters\nRetrieved question: how long does a whale shark live? answer: 70 to 100 years\nquestion: how long does it take a whale shark to mature? answer: around 30 years\nquestion: how long does it take a blue whale to dive? answer: 10 minutes\nWizard-of-Wikipedia\nDialogue history Wizard: Red the color at the end of the visible light spectrum looks good on everyone.\nApprentice: I am more of a fan of green. That would leave us only one primary color: Blue.\nGround Truth But the dominant wavelength of red is approximately 625–740. That’s impressive!\nT5 Predict I agree. It is the color between green and red.\nEMAT Predict it is color between violet and greenon spectrum of visible light\nRetrieved question: what is the next color in this series: green, white, red, green, ? answer: Blue\nquestion: what is the color of light between violet and green? answer: Blue\nquestion: what color looks more blue as it brightens? answer: Violet\nDialogue history\nApprentice: I like jazz.\nWizard: That’s great! Jazz is a music genre that originated in the african-american communities.\nApprentice: When did it originate?\nGround Truth Jazz originated in the late 19th century\nT5 Predict It was first recorded in the late 18th century\nEMAT Predict It originated in late 19th centuryin new orleans\nRetrieved question: where did the genre of jazz originate? answer: New Orleans, United States\nquestion: when did jazz music start in the united states? answer: 1920s\nquestion: what genre of music does rock come from? answer: blues\nTable 6: More examples of EMAT’s prediction on NQ and WoW.\n5196",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8406171798706055
    },
    {
      "name": "Transformer",
      "score": 0.7877049446105957
    },
    {
      "name": "Parametric statistics",
      "score": 0.6032319664955139
    },
    {
      "name": "Question answering",
      "score": 0.522984504699707
    },
    {
      "name": "Knowledge base",
      "score": 0.5140934586524963
    },
    {
      "name": "Exploit",
      "score": 0.5032784342765808
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4987757205963135
    },
    {
      "name": "Natural language",
      "score": 0.4189601540565491
    },
    {
      "name": "Machine learning",
      "score": 0.3602527379989624
    },
    {
      "name": "Natural language processing",
      "score": 0.32234638929367065
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 26
}