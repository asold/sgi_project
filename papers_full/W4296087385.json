{
  "title": "HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein Language Model as an Alternative",
  "url": "https://openalex.org/W4296087385",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2232556478",
      "name": "Xiaomin Fang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2080195193",
      "name": "Fan Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2717411603",
      "name": "Lihang Liu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2399894338",
      "name": "Jingzhou He",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2167700071",
      "name": "Dayong Lin",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2769616627",
      "name": "Yingfei Xiang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120652065",
      "name": "Xiaonan Zhang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2108555996",
      "name": "Hua Wu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1977052956",
      "name": "Hui Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105801009",
      "name": "Le Song",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W3104537585",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W3171848268",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W2161151688",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W2983571096",
    "https://openalex.org/W1987134040",
    "https://openalex.org/W3212854871",
    "https://openalex.org/W2152811165",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3193589100",
    "https://openalex.org/W4285483966",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4313430582",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3191761521",
    "https://openalex.org/W2073758233",
    "https://openalex.org/W4296060337"
  ],
  "abstract": "<title>Abstract</title> AI-based protein structure prediction pipelines, such as AlphaFold2, have achieved near-experimental accuracy. These advanced pipelines mainly rely on Multiple Sequence Alignments (MSAs) as inputs to learn the co-evolution information from the homologous sequences. Nonetheless, searching MSAs from protein databases is time-consuming, usually taking dozens of minutes. Consequently, we attempt to explore the limits of fast protein structure prediction by using only primary sequences of proteins. HelixFold-Single is proposed to combine a large-scale protein language model with the superior geometric learning capability of AlphaFold2. Our proposed method, HelixFold-Single, first pre-trains a large-scale protein language model (PLM) with thousands of millions of primary sequences utilizing the self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the co-evolution information. Then, by combining the pre-trained PLM and the essential components of AlphaFold2, we obtain an end-to-end differentiable model to predict the 3D coordinates of atoms from only the primary sequence. HelixFold-Single is validated in datasets CASP14 and CAMEO, achieving competitive accuracy with the MSA-based methods on the targets with large homologous families. Furthermore, HelixFold-Single consumes much less time than the mainstream pipelines for protein structure prediction, demonstrating its potential in tasks requiring many predictions. The code of HelixFold-Single is available at https://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single, and we also provide stable web services on https://paddlehelix.baidu.com/app/drug/protein-single/forecast.",
  "full_text": "HelixFold-Single: MSA-free Protein Structure\nPrediction by Using Protein Language Model as an\nAlternative\nXiaomin Fang¬†\nBaidu Inc.\nFan Wang¬†\nBaidu Inc.\nLihang Liu¬† ( ÔÉ† liulihang@baidu.com )\nBaidu Inc.\nJingzhou He¬†\nBaidu Inc.\nDayong Lin¬†\nBaidu Inc.\nYingfei Xiang¬†\nBaidu Inc. https://orcid.org/0000-0002-4505-7735\nXiaonan Zhang¬†\nBaidu Inc.\nHua Wu¬†\nBaidu\nHui Li¬†\nBioMap\nLe Song¬†\nBioMap\nArticle\nKeywords: Protein structure prediction, Primary sequence, Protein language model, Large-scale\nPosted Date: September 15th, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-1969991/v1\nLicense: Ôâû Ôìß This work is licensed under a Creative Commons Attribution 4.0 International License. ¬†\nRead Full License\nHE L I XFO L D-S I N G L E:\nMSA- F R E E PROT E I N ST RU C T U R E PR E D I C T I O N\nB Y US I N G PROT E I N LA N G UAG E MO D E L A S A N ALT E R NAT I V E\nXiaomin Fang1‚àó\n, Fan W ang1‚àó ‚Ä†, Lihang Liu1‚àó\n, Jingzhou He1,\nDayong Lin1, Yingfei Xiang1, Xiaonan Zhang1, Hua Wu1, Hui Li2, Le Song2‚Ä†\n,\n1Baidu Inc., 2BioMap,\nAB S T R AC T\nAI-based protein structure prediction pipelines, such as AlphaFold2, have achieved near-experimental1\naccuracy . These advanced pipelines mainly rely on Multiple Sequence Alignments (MSAs) as inputs2\nto learn the co-evolution information from the homologous sequences. Nonetheless, searching MSAs3\nfrom protein databases is time-consuming, usually taking dozens of minutes. Consequently , we4\nattempt to explore the limits of fast protein structure prediction by using only primary sequences of pro-5\nteins. HelixFold-Single is proposed to combine a large-scale protein language model with the superior6\ngeometric learning capability of AlphaFold2. Our proposed method, HelixFold-Single, Ô¨Årst pre-trains7\na large-scale protein language model (PLM) with thousands of millions of primary sequences utilizing8\nthe self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the9\nco-evolution information. Then, by combining the pre-trained PLM and the essential components of10\nAlphaFold2, we obtain an end-to-end differentiable model to predict the 3D coordinates of atoms11\nfrom only the primary sequence. HelixFold-Single is validated in datasets CASP14 and CAMEO,12\nachieving competitive accuracy with the MSA-based methods on the targets with large homologous13\nfamilies. Furthermore, HelixFold-Single consumes much less time than the mainstream pipelines for14\nprotein structure prediction, demonstrating its potential in tasks requiring many predictions. The code15\nof HelixFold-Single is available at https://github.com/PaddlePaddle/PaddleHelix/tree/16\ndev/apps/protein_folding/helixfold-single, and we also provide stable web services on17\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast.18\nKeywords Protein structure prediction ¬∑Primary sequence ¬∑Protein language model ¬∑Large-scale19\n1 Introduction20\nProteins participate in essentially all biological processes and play critical roles for an organism. The structures of21\nproteins are highly correlated to their functions in the biological processes. Determining the protein structures to22\nunderstand their functions can bring considerable contributions to life science.23\nIn recent years, AI-based protein structure prediction technologies have made signiÔ¨Åcant progress in prediction accuracy ,24\ndemonstrating great prospects for the drug and vaccine industry . Particularly , AlphaFold2 [1] pushes the performance25\nto a new frontier in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14 [ 2]), approaching26\nthe accuracy of experimental determination methods. Mainstream protein structure prediction pipelines heavily rely on27\nco-evolution information extracted from Multiple Sequence Alignments (MSAs). MSAs can be simply regarded as28\nprotein chains similar to the target protein chain in sequence. MSA is related to the co-evolution information of protein29\nsequences, which is crucial to predicting its structure. However, over-reliance on MSAs becomes the bottleneck of30\nvarious protein-related tasks.31\n‚àó Equal contribution.\n‚Ä† Corresponding to wang.fan@baidu.com and songle@biomap.com.\nHelixFold-Single\nFirst, compared with the time (usually several seconds) required for model inference in the structure prediction pipeline,32\nsearching MSAs is time-consuming, costing dozens of minutes for a protein. The time-consuming searching is33\ndevastating in the tasks demanding high-throughput requests, such as protein design. Second, the primary structures34\n(single sequence), rather than the MSAs, drive the folding of the proteins. The MSA extracting methods are also not35\ndesigned speciÔ¨Åcally for protein folding. Thus, the MSA-based pipelines only memorize the determined structures of36\nsimilar proteins for prediction but do not entirely understand the mechanism of protein folding.37\nConsequently , designing an accurate MSA-free protein structure prediction method to address the mentioned issues38\nis likely to beneÔ¨Åt and accelerate the development of protein studies. W e argue that a large-scale protein language39\nmodel (PLM) can be served as an alternative to the MSAs to learn the co-evolution knowledge for MSA-free prediction.40\nW e speculate that a PLM with billions of parameters can effectively memorize the MSAs and infer the co-evolution41\ninformation. The past few years have seen the tremendous success of large-scale language models [ 3, 4, 5] in42\nNatural Language Processing, a Ô¨Åeld that shares a lot of characters with protein studying. With the increase of the43\nmodel parameters, the capacity for learning language knowledge grows substantially . Using self-supervised learning44\non large-scale unlabeled proteins, PLMs can reveal the long-range relation along protein sequences and improve45\ndownstream protein-related tasks. Advanced works have attempted to adopt PLMs to enhance the performance of46\nmultiple downstream tasks, such as estimating the secondary structures and the functions [ 6, 7, 8, 9]. Particularly ,47\nseveral studies [ 10, 11, 12] attempted to apply PLMs to protein structure prediction. Most works Ô¨Årst predict the48\ninter-residue 2D geometry by neural networks and then reconstruct the 3D structure based on energy minimization,49\nwhich can not provide end-to-end 3D structure prediction. Besides, compared with the geometric learning capability50\nof EvoFormer and Structure Module proposed by AlphaFold, the capacities of the geometric models used by these51\nmethods, such as recursive models and ResNets, are also unsatisfactory in understanding the co-evolution and spatial52\nrelations between the residues in a single sequence.53\nInspired by the progress of PLMs and AlphaFold2, we propose an end-to-end MSA-free protein structure prediction54\npipeline, HelixFold-Single. The model used in HelixFold-Single consists of two major components: a large-scale PLM55\nas the foundation and the essential components from AlphaFold2 for folding. The PLM can encode the primary structure56\ninto single representation and pair representation to learn the domain knowledge. The EvoFormer and Structure Module57\nfrom AlphaFold2 are then integrated to process the representation, learn the geometric knowledge, and then predict the58\ncoordinates of atoms. The two components are wired up to give an end-to-end differentiable model. HelixFold-Single59\ncontains two training stages. In the Ô¨Årst stage, the large-scale PLM is trained with thousands of millions of unlabeled60\nsingle sequences by the task of masked language prediction. In the second stage, we train the whole model with the61\nprotein structures composed of experimental ground-truth and augmentation structures generated by AlphaFold2.62\nW e compare HelixFold-Single with AlphaFold2 and RoseTT AFold on datasets CASP14 and CAMEO. HelixFold-63\nSingle achieves competitive accuracy with those methods on proteins with sufÔ¨Åcient homologous sequences. W e also64\nanalyze the performance of HelixFold-Single on targets with various homologous sequences, and HelixFold-Single65\nis capable of providing accurate structure predictions on most targets, especially the targets with large homologous66\nfamilies. The ablation study comparing the PLMs of different sizes demonstrates the importance of the size of PLM67\nfor structure prediction. Furthermore, HelixFold-Single shows great superiority in prediction efÔ¨Åciency compared68\nwith the MSA-based methods and could be applied to protein-related tasks demanding a great number of predictions.69\nThe code of HelixFold-Single is publicly released at GitHub https://github.com/PaddlePaddle/PaddleHelix/70\ntree/dev/apps/protein_folding/helixfold-single. W eb service of HelixFold-Single is also available at71\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast to provide efÔ¨Åcient protein structure72\npredictions.73\n2 HelixFold-Single74\nHelixFold-Single aims to take advantage of both the protein language model (PLM) and the main modules used in75\nAlphaFold2 for single sequence-based protein structure prediction. As exhibited in Figure 1, HelixFold-Single consists76\nof three components: PLM Base, Adaptor, and Geometric Modeling. A large-scale PLM Base is employed to encode77\nthe co-evolution information in the parameters, which is used as an alternative to MSAs. Then, in Geometric Modeling,78\nfollowing AlphaFold2, we use modiÔ¨Åed EvoFormer and Structure Module to sufÔ¨Åciently exchange the information79\nbetween the single representations and pair representations to capture the geometric information and recover the 3D80\ncoordinates of the atoms. W e adopt an Adaptor layer to extract the co-evolution information from PLM to effectively81\ngenerate the sequence and pair representations required as inputs to the Geometric modeling. The whole differentiable82\npipeline is trained by both self-supervised pre-training with bulks of unlabeled single sequences and supervised learning83\nwith geometric labels.84\n2\nHelixFold-Single\nEvoformer\n(nEvoFormer blocks)\nStructure \nModule\n(nStructure blocks)\npair repr.\nsingle repr.\nProtein \nLanguage \nModel\n(nPLM blocks)\nAdaptor\nsingle repr. single repr.\npair repr.\nattention\nweights\nrecycle\nGeometric Modeling\n~300M primary \nsequences\n~120K determined \nstructures\n~1M estimated \nstructures\nPre-train with\nself-supervised learning\nHelixFold-Single\nGWEIPEPYVWDESFR\nPLM Base\nInput primary sequence\nTrain with\nsupervised learning\nFigure 1: The framework of HelixFold-Single with a protein language model as PLM Base, the compose of EvoFormer\nand Structure Module of AlphaFold2 as Geometric Modeling, and Adaptor to connect PLM Base and Geometric\nModeling.\n2.1 Large-Scale PLM Base85\nInspired by large-scale pre-trained language models, we follow previous works on pre-training a protein language\nmodel (PLM). The PLM processes the primary protein sequences (i.e., the amino acid sequences) and extracts the\nknowledge needed for further geometric modeling. A protein of length L can be uniquely represented by a sequence of\ntypes of amino acids denoted by x = (x1, x 2, ..., x L). An embedding layer E(xl) maps the type id to dPLM -dimension\nembedding vectors:\nx(0) = (E(x1), E (x2), ..., E (xL)).\nNotice that x(k) ‚àà RL√ó dPLM is the representation of the amino acid sequence.86\nW e then apply the widely used Transformer-style blocks ([3] to process the embedding vectors, denoted by\nx(k+1) = DisentangledAttentionTransformer(x(k)). (1)\nAccurately predicting the contacts between the residues, especially the long-rage contacts, is critical for protein structure87\nprediction. Considering the contact between the residues is more dependent on the relative positions rather than88\nthe absolute positions (counted from the start of the sequence), we employ DisentangledAttentionTransformer from89\nDeBerT a [13] to focus on the modeling of interactions between the residue representations and the relative positions.90\nDisentangledAttentionTransformer adopts the attention mechanism to learn the interactions between the residues as91\nwell as the interactions of the interaction-position pairs.92\nBesides, we take advantage of multi-head self-attention weights in DisentangledAttentionTransformer to construct the93\ninitial pair representation. The attention weights of the k-th block is denoted by z(k) ‚àà RL√ó L√ó hPLM , where hPLM is the94\nnumber of heads of self-attention.95\nW e add an additional Adaptor to map the output of PLM Base to the input of Geometric Modeling module.\nÀúx(0) = Linear(x(nPLM )),\nÀúz(0) = Linear([z(1), z(2), ¬∑ ¬∑ ¬∑, z(nPLM )]), (2)\nwhere nPLM is the number of blocks in PLM Base, and operator [] refers to concatenation. Àúx(0) ‚àà RL√ó dSingle and96\nÀúz(0) ‚àà RL√ó L√ó dP air are the initial single representations and pair representations of the Geometric Modeling module,97\nrespectively .98\n2.2 Geometric Modeling99\nW e employ the EvoF ormerand Structure Module proposed by AlphaFold2 [ 1] to model the relations between the100\nresidues and then estimate the 3D coordinates of the atoms in the proteins. W e slightly modify the original EvoFormer101\n3\nHelixFold-Single\nand Structure Module‚Äôs architecture to match our settings. First, the original EvoFormer takes the MSA representation102\nand pair representation, encoded from the searched MSAs, as input. As an alternative, we take the output of the Adaptor103\n(including the single representations ( Àúx(0)) and pair representations ( Àúz(0))). Second, Evoformer adopts various attention104\nmechanisms to exchange the information within the single and pair representations to learn the spatial relationships.105\nNote that, compared with the original version of Evoformer proposed by AlphaFold2, we remove the column-wise106\ngated self-attention because HelixFold-Single focuses on MSA-free protein structure prediction and is no need to107\nexchange the messages within the MSAs. W e follow the other geometric components of AlphaFold2, including the108\nStructure Module that takes the single representation and pair representation yielded by the EvoFormer, and exploits109\nInvariant Point Attention and other geometric transformation operators to end-to-end predict the 3D coordinates of110\nthe atoms. Also, following AlpahFold2, we recycle the whole Geometric Modeling module to reÔ¨Åne the predicted111\nstructures iteratively .112\n2.3 Model Optimization113\nFor the sake of leveraging the domain knowledge from the protein database, we operate two-stage parameter optimization114\non HelixFold-Single.115\nIn the Ô¨Årst stage, the PLM is pre-trained to capture the co-evolution information. The PLM is trained with about 300116\nmillion of single sequences recorded in a protein database. T o encourage PLM to observe the diverse single sequences117\nas soon as possible, we cluster the proteins by the similarity of single sequences and sample the proteins to balance the118\ndistributions of different clusters in our training data. W e apply the self-supervised technique masked language model119\n(MLM) to optimize the parameters of the PLM, by randomly masking 15% of residues in the single sequences and then120\nreconstructing those masked residues. More concretely , MLM attempts to predict p(xl|x1, ..., x l‚àí 1, x M , x l+1, ..., x L)121\ngiven the residue in the l-th position xl being masked by xM . A crucial proposal of this work is that PLM can learn122\nthe dependency between the masked residue and the other residues, and thus represent the co-evolution information.123\nPrevious works [ 6] have already veriÔ¨Åed that PLMs can reveal secondary structures of the proteins, but little has been124\ndiscussed on the relation between PLM and co-evolution. Co-evolution is the phenomenon that two residues in contact125\ntend to evolve at the same time to preserve the structure and thus the function of the protein. In PLM, if a residue at126\nanother position s has a profound impact (the residue at position s is changed, the masked residue will also change) on127\nthe masked residue, then those two residues are likely to evolve at the same time.128\nIn the second stage, since merely relying on PLM to predict the structure is inadequate to capture the geometric129\ninformation, PLM Base and Geometric Modeling modules in HelixFold-Single are jointly optimized. W e utilize130\n100 thousand experimentally determined protein structures. W e also use additional one million estimated protein131\nstructures for training in this stage (distilled from AlphaFold2). Following AlphaFold2, we end-to-end train the network132\nwith the main losses, including Frame Aligned Point Error (F APE) loss and other auxiliary losses. By combining133\nthe computational efÔ¨Åcient PLM Base module (compared with MSA search) and the Geometric Modeling module,134\nHelixFols-Single is capable of providing efÔ¨Åcient and precise protein structure prediction.135\n3 Results136\n3.1 Datasets137\nW e used UniRef30 (2021-03) [ 14] to pre-train the PLM, which clusters UniProtKB [ 15] sequences at the level of138\n30% pairwise sequence identity . Then, three datasets are used to train the whole network, including the proteins in139\nRCSB PDB [ 16, 17] released before 2020-05-14 and two self-distillation datasets constructed from Uniclust30 (version140\n2018-08) and AlphaFold Protein Structure Database [18].141\n3.2 Overall Comparison142\nCASP14 [1, 19, 20] with 87 domain targets and CAMEO [21] with 371 targets collected from 2021-09-04 to 2022-02-19143\nare used to compare the overall accuracy of HelixFold-Single with the several baseline structure prediction pipelines,144\nincluding the MSA-based and MSA-free methods. AlphaFold2 [ 1] and RoseTT AFold [ 22] are currently the most145\nadvanced methods for protein structure prediction, relying on MSAs to provide predictions. W e test the accuracy of146\nAlphaFold2 and RossTT AFold with and without homologous sequences, respectively . A commonly used metric, i.e.,147\nTM-score [23], is exploited to evaluate the accuracy of HelixFold-Single and other methods.148\nFigure 2 exhibits the test results of our proposed HelixFold-Single and the compared methods on CASP14 and CAMEO.149\nFrom the results, we have the following observations:150\n4\nHelixFold-Single\nFM FM/TBM TBM-easy TBM-hard All\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTM-score\nAlphaFold2 (input:MSA) RoseTTAFold (input:MSA) AlphaFold2 (input:single)\nRoseTTAFold (input:single) HelixFold-Single\n(a) CASP14 (87 targets classiÔ¨Åed into FM and TBM based on\ntheir relatedness to existing structures.)\ndepth<=100 100<depth<=1000 1000<depth<=1000 0 depth>10000 All\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTM-score\nAlphaFold2 (input:MSA) RoseTTAFold (input:MSA) AlphaFold2 (input:single)\nRoseTTAFold (input:single) HelixFold-Single\n(b) CAMEO (371 targets classiÔ¨Åed into four categories de-\npending on different MSA depths.)\nFigure 2: Overall comparison of HelixFold-Single and other methods on CASP14 and CAMEO. AlphaF old2 (input:MSA)\nand RoseTTAF old (input:MSA)are MSA-based methods, while the remaining use the primary structures as input.\n(1) In general, HelixFold-Single signiÔ¨Åcantly surpasses all the MSA-free methods on CASP14 and CAMEO and is151\ncompetitive with the MSA-based methods in some cases. Notably , the accuracy of HelixFold-Single on CAMEO152\nis comparable to that of AlphaF old2 (input:MSA)and outshines another strong baseline, RoseTTAF old (input:MSA).153\nHelixFold-Single demonstrates the great potential of incorporating PLM into geometric modeling for protein structure154\nprediction.155\n(2) HelixFold-Single can be par with the MSA-based methods on the targets with large homologous families, e.g., TBM-156\neasy domain targets in CASP14 with a median of seven homologous sequences and targets with more than a thousand157\nhomologous sequences (MSA depth > 1000) in CAMEO. These results indicate that the accuracy of HelixFold-Single is158\ncorrelated to the richness of homologous sequences, revealing that the large-scale PLM adopted by HelixFold-Single is159\ncapable of embedding the information, e.g., co-evolution knowledge, of MSAs used by the MSA-based methods.160\n(3) Compared HelidFold-Single with other MSA-free methods, HelixFold-Single exhibits its great superiority on all the161\ncategories of CASP14 and CAMEO. Since AlphaFold2 and RoseTT AFold rely on MSAs as input during the training162\nprocess, it is challenging for those methods to provide accurate predicts when taking only the single sequences as input.163\n3.3 Effect of Number of Homologous Sequences164\nThe results on CASP14 and CAMEO indicate that the accuracy of HelixFold-Single is related to the number of165\nhomologous sequences. W e further compare the performance of HelixFold-Single and other methods on the targets166\nwith variant MSA depths. W e collected the targets released between 2020-05 and 2021-10 from PDB, from which we167\npicked the targets with relatively sparse homologous sequences. W e blended those targets with the data of CASP14 and168\nCAMEO as a new evaluation set. Figure 3a compares the TM-scores of HelixFold-Single and the baseline methods on169\nthe evaluation set, grouped by the number of homologous sequences (MSA depths). Figure 3b shows the distribution of170\nthe proteins in different groups in this evaluation set. W e can see that as the available homologous sequences grow ,171\nthe average TM-score of both HelixFold-Single and the MSA-based methods increases, while the scores of the other172\nMSA-free methods decrease. For the proteins with sparse homologous sequences, the TM-scores of all the compared173\nmethods are unsatisfactory . For the proteins with larger homologous families, especially those with more than thousands,174\nHelixFold-Single can compete with the MSA-based methods. Given that 90% of the targets in PDB have more than175\n1024 homologous sequences, we can reasonably extrapolate that HelixFold-Single can achieve satisfying accuracy on176\nthe most frequently investigated proteins.177\nIn order to further investigate the relationship between the capacity of the PLM, the accuracy of protein structure178\nprediction, and the size of the homologous family , we utilized the targets in CASP14 and CAMEO datasets to exhibit179\ntheir relations, as shown in Figure 3c, Figure 3d, and Figure 3e. As we expected, from Figure 3c, a protein‚Äôs structure180\naccuracy (TM-score) is correlated to the size of its homologous family (MSA depth), and the results are consistent181\nwith those in Figure 3b. Besides, we use a probability metric, Perplexity [ 24], to indicate the capacity of the protein182\nlanguage model. If the PLM can predict or reconstruct a protein sequence well, the Perplexity is low in predicting that183\ntarget. From Figure 3d and Figure 3e, we can observe that the Perplexity of the PLM and the MSA depths are negatively184\ncorrelated. The Perplexity of the PLM and the TM-scores of HelixFold-Single are also negatively correlated. The185\nresults indicate that if the PLM Base module can well predict (model) a protein sequence, there is a high probability that186\nthe PLM module can learn the co-evolution information of this protein and serves as an alternative to MSAs. Thus, the187\n5\nHelixFold-Single\n[1,4] (4,16] (16,64] (64,256] (256,1024] (1024,4096] (4096,16384] (163 84,+‚àû)\nMSA depth\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTM-score\nAlphaFold2(input:MSA) RoseTTAFold (input:MSA) AlphaFold2 (input:single) RoseTTAFold (input:single) HelixFold-Single\n(a) Comparison between HelixFold-Single and the baseline methods on protein\ntargets with a various numbers of homologous sequences (MSA depths).\n0.54% 0.74% 1.66% 2.88% 4.76%\n21.66%\n66.84%\n0.92%\n0.00%\n10.00%\n20.00%\n30.00%\n40.00%\n50.00%\n60.00%\n70.00%\n80.00%\n90.00%\n100.00%\n[1,4] (4,16] (16,64](64,256](256,1024](1024,4096](4096,16384](16384,+‚àû)\nProportion\nMSA depth\n(b) Distribution of proteins with different\nhomologous sequences in PDB.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1 10 10 0 10 00 10 00 0 10 00 00 10 00 00 0\nTM-score\nMSA depth\nCASP14 CAMEO\n(c) Relations between proteins‚Äô MSA\ndepths and TM-scores\n1\n10\n100\n1000\n10000\n100000\n1000000\n0 10 20 30 40\nMSA depth\nPerplexity\nCASP14 CAMEO\n(d) Relations between Perplexity of PLM\nand MSA depths\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 5 10 15 20 25 30 35 40\nTM-score\nPerplexity\nCASP14 CAMEO\n(e) Relation between Perplexity of PLM\nand TM-scores of HelixFold-Single\nFigure 3: Analysis of the impact of homologous sequences (MSA depths) and investigation of the relations between\nMSA depths, TM-scores, and perplexity of the PLM.\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35 40\nPLM-100M\nPLM-1B\nCASP14 CAMEO\n0\n5\n10\n15\n20\n25\n30\nPerplexity\n1B_ppl 100M_ppl Á≥ªÂàó3 Á≥ªÂàó4\nÁ≥ªÂàó5 1B_ppl 100M_ppl\nCASP14 CAMEO\n(a) Perplexity of PLM-1B and PLM-100M\n0\n0.2\n0.4\n0.6\n0.8\n1\n0 0.2 0.4 0.6 0.8 1\nPLM-100M\nPLM-1B\nCASP14 CAMEO\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nP@L/5\n1 1 Á≥ªÂàó 3 Á≥ªÂàó 4\nÁ≥ªÂàó 5 1B_p@L/5 1\nCASP14 CAMEO\n(b) Contact prediction of PLM-1B and PLM-100M\nFigure 4: Comparison of PLMs of different sizes.\nGeometric Modeling module can leverage the co-evolution embedded in the PLM to provide a more accurate structure188\nfor that protein.189\n3.4 Effect of the Sizes of the PLMs190\nT o comprehensively study the ability of the PLMs of different sizes to learn the co-evolution information, we compare a191\npre-trained PLM of 1B parameters (denoted by PLM-1B) and another pre-trained PLM of 100M (denoted by PLM-192\n100M). T able 4a exhibits the Perplexity of PLM-1B and PLM-100M of the targets from datasets CASP14 and CAMEO.193\nIn general, the smaller the perplexity is, the stronger the capacity of the PLM is. Thus, PLM-1B with more model194\nparameters performs better than PLM-100M with fewer parameters on both datasets CASP14 and CAMEO. In addition,195\nwe apply the PLM-1B and PLM-100M on the task of protein residue contact prediction to compare their performance on196\nthe downstream tasks. W e simply Ô¨Åt a logistic regression that takes the attention weights, i.e., [z(1), z(2), ¬∑ ¬∑ ¬∑, z(nPLM )],197\n6\nHelixFold-Single\nfrom the PLMs as input and predict the contact of residues on the targets in datasets CASP14 and CAMEO. Following198\n[6, 25], we use top L/5 long-range contact precision, denoted by P@L/5, as the evaluation metric, and the results are199\nshown in Figure 4b. As we can see, PLM-1B is signiÔ¨Åcantly superior to PLM-100M on the contact prediction task. The200\nresults from Figure 4a and Figure 4b both support the hypothesis that the larger the size of the PLM is, the stronger its201\ncapacity is. Therefore, it can be reasonably inferred that the performance of the PLM will continue to improve as the202\nsize of the PLM increases to a larger size.203\n3.5 Prediction Speed Comparison204\n1.0\n10.0\n100.0\n1000.0\n[1,100] (100,200] (200,400] (400,800] (800,+‚àû)\nMedian time (sec)\nProtein length\nSea rch MSA AlphaFold2 HelixFold-Single\nFigure 5: Median times of MSA search, AlphaFold2, and HelixFold-Single on proteins with various lengths.\nMassive time consumption for searching MSAs is one of the bottlenecks of the MSA-based folding, and accelerating205\nthe speed of protein structure prediction can considerably broader its applications. The MSA-free HelixFold-Single has206\na tremendous advantage for inference efÔ¨Åciency for exempting MSA searching. Figure 5 exhibits the computation time207\ncost of 1. MSA searching; 2. Whole inference pipeline of AlphaFold2; 3. Inference of HelixFold-Single. All the tests208\nare executed in a single NVIDIA A100(40G) GPU. In general, Helixfold-Single consumes much less time than the209\nAlphafold2, while AlphaFold2 pipeline spends most of its time in MSA searching. For proteins less than 100 in length,210\nHelixFold-Single‚Äôs prediction time is only about one-thousandth of that of AlphaFold2. Even for the proteins with more211\nthan 800 amino acids, HelixFold-Single still has great efÔ¨Åciency superiority . The high efÔ¨Åciency of HelixFold-Single212\ndemonstrates the potential of its application in tasks with a great demand for structural prediction.213\n3.6 Case Study214\nR66\nR29\nE36\nR66\nE36\nR29\n(a) 7KWT:B, AlphaFold2,\nTM-score=0.2425\nR66\nR29\nE36\nR66\nR29\nE36\n(b) 7KWT:B, HelixFold-\nSingle, TM-score=0.8066\nR56\nR106\nR121\nR123 R56\nR106\nR121\nR123\n(c) 7BCJ:A, AlphaFold2, TM-\nscore=0.2914\nR56\nR106\nR121\nR123\nR106\nR56\nR121\nR123\n(d) 7BCJ:A, HelixFold-Single,\nTM-score=0.6420\nFigure 6: HelixFold-Single predicts PlyC and RoxP structure more accurately than AlphaFold2. PlyC structures\npredicted by (a) AlphaFold2 and (b) HelixFold-Single is aligned with the reference structure (PDB ID: 7KWT , chain B);\nRoxP structure predicted by (c) AlphaFold2 and (d) HelixFold-Single is aligned with the reference structure (PDB ID:\n7BCJ, chain A). A-D) Green: structure predicted by AlphaFold2. Magentas: structure predicted by HelixFold-Single.\nCyan: reference crystal structure measured by X-RA Y diffraction approach (resolution<1.8A). Key residues related to\nprotein function are shown as sticks.\nMost proteins exert their functions by interacting with other molecules. Changes in the structure of a protein, especially215\nthose in the key interacting residues, can signiÔ¨Åcantly affect its biological function. As a result, a protein‚Äôs function is216\nclosely associated with its structure, and accurately predicting the structure would facilitate our understanding of its217\n7\nHelixFold-Single\nbiological role. While AlphaFold2 achieves outstanding accuracy in most of the protein structure prediction tasks, its218\nperformance can still be poor in some situations. Here, we demonstrate that HelixFold-Single complements AlphaFold2219\nin several of these cases. Endolysin enzymes from bacteriophages cause bacterial lysis by degrading the peptidoglycan220\ncell wall. The streptococcal C1 phage endolysin PlyC is the most potent endolysin and can rapidly lyse group A, C, and221\nE streptococci. Study on PlyC structure revealed that the key residues, including R66, E36, R29, etc, are important for222\nthe binding of PlyC to its target and hence are critical to its function [26]. However, AlphaFold2 failed to produce the223\nreliable structure of the protein (Figure 6(a)). This is probably due to insufÔ¨Åcient co-evolution information extracted224\nfrom MSAs. In contrast, the structure predicted by HelixFold-Single (Figure 6(b)) more closely resembles the one225\nmeasured by the experiment, likely attributed to its little dependence on the information from MSAs. A similar result226\nis observed for another protein RoxP . This protein is produced by Cutibacterium acnes, a predominant bacterium on227\nhuman skin, and was shown to alleviate radical-induced cell damage. The key residues R56, R106, R121, R123 on228\nRoxP form a positively charged groove, which acts as the binding site for substrate and cofactors [27]. HelixFold-Single229\naccurately predicts the formation of the positively charged groove(Figure 6(d)), which is not observed in the structure230\npredicted by AlphaFold2 (Figure 6(c)). Furthermore, the TM-score of HelixFold-Single for RoxP is much higher231\nthan that of AlphaFold2, suggesting an overall better performance of HelixFold-Single in predicting RoxP structure.232\nAltogether, our case studies indicate that HelixFold-Single outperforms AlphaFold2 in some situations and can be used233\nas a reliable tool to analyze the function of proteins without known X-RA Y structures.234\n4 Related W orks235\n4.1 Protein Language Models236\nLarge-scale language models [ 3] with the self-supervised learning (SSL) paradigm, such as masked language model237\n(MLM) [ 4] and auto-regression [ 26], have achieved extraordinary success in Natural Language Processing (NLP) tasks.238\nRecent progress has revealed that their capabilities are deeply related to the scale of the model parameters: the larger239\nthe scale of the parameters, the better the performance [ 5]. The community has not yet seen a sign of stopping growth240\nby moving from billions to hundreds of billions of parameters. Those language models are capable of memorizing241\nand generalizing massive common-sense knowledge and professional expertise implicitly included in the large-scale242\nunlabeled data. Inspired by those achievements, Protein Language Models (PLMs) tried to transfer language models243\nand SSL tasks to protein modeling. A protein can be represented by an amino acid sequence, similar to the sequences of244\nwords or tokens in NLP . Previous works [ 6, 7, 8, 9] have shown that by pre-training with only single sequences without245\nmuch supervision, protein language models can reveal the protein classiÔ¨Åcation, stability , and lower-level structure246\ninformation (including secondary , tertiary structures and 2D contact maps). However, the accuracy of these models247\nin structure prediction is still far from that of the mainstream folding models supervised by the ground-truth protein248\nstructure.249\n4.2 Protein Structure Prediction250\nMainstream pipelines [ 27, 28, 29, 30] rely on extracting the co-evolution information from Multiple Sequence Align-251\nments (MSAs) to predict the protein structures. Earlier works manually designed the features derived from MSAs, such252\nas inverse covariance matrices of MSAs. Then, deep neural networks (DNNs), e.g., convolutional networks, are utilized253\nto model the relations between the residues. Advanced studies [ 1, 29], directly take the MSAs as input and apply DNNs254\nto predict the 3D coordinates of the proteins. Particularly , the appearance of AlphaFold2 [ 1] has dramatically narrowed255\nthe accuracy gap between the experimentally determined structures and model estimated structures, employing the256\nEvoFormer module to enhance the interaction between MSA sequences and pairwise geometric information and the257\nStructure module to directly predict the atoms‚Äô coordinates. However, the reliance on MSA inevitably impedes the258\ncomputation efÔ¨Åciency and accurate prediction of orphan proteins and designed proteins, as well as downstream tasks259\nsuch as protein design.260\nAlthough the structure of a protein is dependent on its primary structure, it is incredibly challenging to train an accurate261\nmodel that can infer the protein structures with only the primary structures. Only a small number of samples, i.e.,262\nexperimentally determined structures recorded in the PDB database, are available for model training. Several works263\nattempt to incorporate the protein language models (PLMs) for MSA-free protein structure prediction. RGN2 [ 10]264\nemploys a protein language model (AminoBER T) with a recurrent geometric network that utilizes Frenet-Serret frames265\nto generate the backbone structure. Besides, advanced studies [ 11, 12] combine pre-trained PLMs, such as ProT5 [ 7]266\nand ESM-1b [ 31], with ResNets to predict 2D structures, e.g., contact map of a protein, yielding superior performance267\nin orphan proteins. Nonetheless, the overall accuracy of those works is still unsatisfactory due to the limited capacity of268\nthe used model architectures.269\n8\nHelixFold-Single\n5 Conclusion and Future W ork270\nOn the one hand, mainstream protein structure prediction methods, such as AlphaFold2 and RoseTT AFold, rely on the271\nMSAs to extract the homologous information. However, searching MSAs is time-consuming, limiting the application272\nof those methods to broader protein-related tasks. On the other hand, the large-scale protein language model learns273\nthe protein correlations from a great number of unlabeled proteins through self-supervised learning tasks. By utilizing274\nlarge-scale parameters to embed the homologous information, we prove it can be used as an alternative to MSAs to275\nreduce the time consumption required by the protein structure prediction methods. HelixFold-Single attempts to take276\nadvantage of both the protein language model and the geometric modeling, end-to-end predicting the protein structures277\nwith only the primary structures. HelixFold-Single can be par with the MSA-based methods on targets with large278\nhomologous families and is much more efÔ¨Åcient than the MSA-based methods, demonstrating its application prospect279\nfor protein study .280\nIn the future, as the experimental results indicate that the larger size of the PLM can achieve superior performance,281\nwe will continue investigating the PLM with a larger size for protein structure prediction. In addition, the accuracy of282\nthe targets with only a few homologous sequences is still unsatisfactory . Thus we will try to introduce more diverse283\ntraining data to alleviate this problem.284\n9\nHelixFold-Single\nAppendix A: T raining and Evaluation Data285\nT raining Data286\nUniRef30 (2021-03) [ 14], containing about 260 millions protein sequences is utilized to pre-train the PLM, clustering287\nUniProtKB [15] sequences at the level of 30% pairwise sequence identity .288\nThree datasets are utilized to train HelixFold-Single for MSA-free protein strcture prediction.289\n‚Ä¢ RCSB PDB [16, 17]: The targets released before 2020-05-14 in PDB are used to train HelixFold-Single. W e290\nÔ¨Ålter out the targets with resolution larger than 3√Ö and whose number of amino acids less than 10. The targets291\nare clustered at 40% sequence identity cutoff.292\n‚Ä¢ Distillation-Uniclust30: W e inference the structures of the targets in Uniclust30 (version 2018-08) by293\nAlphaFold2 for self-distillation. W e follow the data-prepossess procedure reported in AlphaFold2. Further,294\nthe target structures with average pLDDT less than 0.5 are Ô¨Åltered out. Then, the targets are clustered at 30%295\nsequence identity cutoff.296\n‚Ä¢ Distillation-EBI: About one million protein structures are extracted from AlphaFold Protein Structure297\nDatabase [ 18]. W e removed the protein structures with average pLDDT less than 0.5. The remaining targets298\nare clustered at 50% sequence identity cutoff.299\nEvaluation Data300\nW e exploit three datasets to evaluate the accuracy and efÔ¨Åciency of HelixFold-Single and the baseline methods.301\n‚Ä¢ CASP14: 61 targets are collected from CASP14 [ 19, 20] for overall evaluation, which includes 87 domains302\nwith classiÔ¨Åcation of FM (free modeling), TBM-easy (easy template-based modeling), TBM-hard (hard303\ntemplate-based modeling) and FM/TBM (modeling with only remote structural homologies).304\n‚Ä¢ CAMEO: W e collect 371 targets from CAMEO [ 21] between 2021-09-04 and 2022-02-19, which consists of305\nvarious target difÔ¨Åculties including Easy , Medium, and Hard.306\n‚Ä¢ MSA Depth T est:W e create a test set obtained from RCSB PDB, including 793 targets with a wide range of307\ndifferent MSA depths from 2020-05 to 2021-10, especially the targets with only a few homologous sequences.308\nThis test set is combined with datasets CASP14 and CAMEO to investigate the effect of the number of309\nhomologous sequences.310\nAppendix B: Detailed Settings of HelixFold-Single311\nT raining Settings312\nThe implementation of HelixFold-Single is based on our previous work, HelixFold [ 32], and we use 128 NVIDIA A100313\nGPUs to train HelixFold-Single. T able 1 exhibits the architecture setting of HelixFold-Single. W e train two version314\nof PLMs for ablation study . T o balance the computation costs of multiple GPUs for pre-training, the batch size used315\nin each GPU is dynamically adjusted according to the lengths of protein sequences. W e use AdamW optimizer [ 33]316\nwith learning rate of 5e-4, Œ≤1 = 0.9, Œ≤2 = 0.999, weight decay of 0.01, learning rate warm-up over the Ô¨Årst 30,000317\nsteps. When training the whole network for protein structure estimation, we use Adam optimizer [ 34] to optimize the318\nparameters. W e apply two stages of training: initial training stage and Ô¨Åne-tuning stage. In the initial training stage, the319\nlearning rate is set to be 1e-3 and the lengths of the input protein sequences are cropped to be 256. In the Ô¨Åne-tuning320\nstage, we use learning rate of 2e-4 and the lengths of the input protein sequences are cropped to be 384. Gradient321\nclipping by the global norm [35] is adopted on the parameters with a clipping value of 1.0.322\nModel Architecture323\nPLM Base324\nAs shown in Figure 7, PLM Base is mainly based on DeBerT a [ 13]. W e make two slight modiÔ¨Åcations: (1) T o325\nstabilize the pre-training of PLM, instead of using Post-Norm in DeBerT a, Pre-Norm [ 36] is applied in PLM Base of326\nHelixFold-Single. (2) W e Ô¨Ånd that using residue-to-position and residue-to-residue (Equation 3) is enough, while the327\nperformance gain by adding position-to-residue is trivial. Thus, we left out the position-to-residue term in DeBerT a. As328\na result, we have the DisentangledAttention layer denoted by329\n10\nHelixFold-Single\nT able 1: Architecture setting of HelixFold-Single.\nComponents Model size Layer num Hidden size Intermediate size Head num\nPLM-1B 1.09B nPLM = 20 dPLM = 2048 8192 hPLM = 16\nPLM-100M 100M nPLM = 12 dPLM = 768 3072 hPLM = 12\nEvoFormer 87M nEvoF ormer = 24 dSingle = 512\ndP air = 64\nStructure Module 1.7M nStructure = 8 dStructure = 384\nDisentangledAttention\nLayerNorm\nFeedForward\nLayerNorm\nùë•(\")\nùë•$%\nùë•&'(\nùë•(\")*)\nFigure 7: Architecture of DisentangledAttentionTransformer (superscript k denotes the layer id).\nq = xinWq, k = xinWk, v = xinWv, p = epWp\nAi,j = qikT\njÓ¥ôÓ¥òÓ¥óÓ¥ö\n(a) residue-to-residue\n+ qipT\nŒ¥(i,j )\nÓ¥ô\nÓ¥òÓ¥ó Ó¥ö\n(b) residue-to-position\nxout = softmax( A‚àö 2dPLM\n)v\n(3)\nW‚àó are trainable parameters; ep is the trainable position embedding; Œ¥(i, j ) denotes the relative distance between330\nposition i and j.331\nGeometric Modeling332\nSince HelixFold-Single takes only the single sequence as input, we slightly modify the architecture of Evoformer,333\nremoving the columnwise attention. The architecture of the revised Evoformer is shown in Figure 8.334\nRevised Evoformer block\nRow-wise gated \nattention with \npair bias\nTransition\nTriangle update \nusing ‚Äúoutgoing‚Äù \nedges\nTriangle self-\nattention around \nstarting node\nOuter\nproduct\nPair\nrepresentation\n(L, L, dPair)\n√ó\tnEvoformer\nTriangle update \nusing ‚Äúingoing‚Äù \nedges\nTriangle self-\nattention around \nending node\nPair\nrepresentation\n(L, L, dPair)\nSingle\nrepresentation\n(L, dSingle)\nSingle\nrepresentation\n(L, dSingle)\nFigure 8: Architecture of revised Evoformer.\n11\nHelixFold-Single\nReferences335\n[1]\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov , Olaf Ronneberger, Kathryn336\nTunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, et al. Highly accurate protein structure337\nprediction with alphafold. Nature, 596(7873):583‚Äì589, 2021.338\n[2] John Moult. A decade of casp: progress, bottlenecks and prognosis in protein structure prediction. Current339\nopinion in structural biology, 15(3):285‚Äì289, 2005.340\n[3] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and341\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.342\n[4] Jacob Devlin Ming-W ei Chang Kenton and Lee Kristina T outanova. Bert: Pre-training of deep bidirectional343\ntransformers for language understanding. In Proceedings of NAACL-HLT, pages 4171‚Äì4186, 2019.344\n[5] T om Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-345\nlakantan, Pranav Shyam, Girish Sastry , Amanda Askell, et al. Language models are few-shot learners. Advances346\nin neural information processing systems, 33:1877‚Äì1901, 2020.347\n[6] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Y an Duan, Peter Chen, John Canny , Pieter Abbeel, and Y un348\nSong. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32,349\n2019.350\n[7] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Y u W ang, Llion Jones, T om Gibbs,351\nT amas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: towards cracking the language of life‚Äôs code352\nthrough self-supervised deep learning and high performance computing. arXiv preprint arXiv:2007.06225, 2020.353\n[8] Roshan Rao, Joshua Meier, T om Sercu, Sergey Ovchinnikov , and Alexander Rives. Transformer protein language354\nmodels are unsupervised structure learners. Biorxiv, 2020.355\n[9] Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Y u Hsieh, and Jie T ang. Modeling protein using large-scale pretrain356\nlanguage model. arXiv preprint arXiv:2108.07435, 2021.357\n[10] Ratul Chowdhury , Nazim Bouatta, Surojit Biswas, Charlotte Rochereau, George M Church, Peter Karl Sorger,358\nand Mohammed N AlQuraishi. Single-sequence protein structure prediction using language models from deep359\nlearning. bioRxiv, 2021.360\n[11] Konstantin W ei√üenow , Michael Heinzinger, and Burkhard Rost. Protein language-model embeddings for fast,361\naccurate, and alignment-free protein structure prediction. Structure, 2022.362\n[12] W enkai W ang, Zhenling Peng, and Jianyi Y ang. Single-sequence protein structure prediction using supervised363\ntransformer protein language models. bioRxiv, 2022.364\n[13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and W eizhu Chen. Deberta: Decoding-enhanced bert with disentan-365\ngled attention. In International Conference on Learning Representations, 2020.366\n[14] Milot Mirdita, Lars V on Den Driesch, Clovis Galiez, Maria J Martin, Johannes S√∂ding, and Martin Steinegger.367\nUniclust databases of clustered and deeply annotated protein sequences and alignments. Nucleic acids research,368\n45(D1):D170‚ÄìD176, 2017.369\n[15] Baris E. Suzek, Y uqi W ang, Hongzhan Huang, Peter B. McGarvey , Cathy H. Wu, and the UniProt Consor-370\ntium. UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches.371\nBioinformatics, 31(6):926‚Äì932, 11 2014.372\n[16] Helen M. Berman, John W estbrook, Zukang Feng, Gary Gilliland, T . N. Bhat, Helge W eissig, Ilya N. Shindyalov ,373\nand Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235‚Äì242, 01 2000.374\n[17] Stephen K Burley , Charmi Bhikadiya, Chunxiao Bi, Sebastian Bittrich, Li Chen, Gregg V Crichlow , Cole H375\nChristie, Kenneth Dalenberg, Luigi Di Costanzo, Jose M Duarte, Shuchismita Dutta, Zukang Feng, Sai Ganesan,376\nDavid S Goodsell, Sutapa Ghosh, Rachel Kramer Green, Vladimir Guranovi ¬¥c, Dmytro Guzenko, Brian P Hudson,377\nCatherine L Lawson, Y uhe Liang, Robert Lowe, Harry Namkoong, Ezra Peisach, Irina Persikova, Chris Randle,378\nAlexander Rose, Y ana Rose, Andrej Sali, Joan Segura, Monica Sekharan, Chenghua Shao, Yi-Ping T ao, Maria379\nV oigt, John D W estbrook, Jasmine Y Y oung, Christine Zardecki, and Marina Zhuravleva. RCSB Protein Data380\nBank: powerful new tools for exploring 3D structures of biological macromolecules for basic and applied research381\nand education in fundamental biology , biomedicine, biotechnology , bioengineering and energy sciences. Nucleic382\nAcids Research, 49(D1):D437‚ÄìD451, 11 2020.383\n[18] Mihaly V aradi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Y ordanova,384\nDavid Y uan, Oana Stroe, Gemma W ood, Agata Laydon, Augustin ≈Ω√≠dek, Tim Green, Kathryn Tunyasuvunakool,385\nStig Petersen, John Jumper, Ellen Clancy , Richard Green, Ankur V ora, Mira LutÔ¨Å, Michael Figurnov , Andrew386\n12\nHelixFold-Single\nCowie, Nicole Hobbs, Pushmeet Kohli, Gerard Kleywegt, Ewan Birney , Demis Hassabis, and Sameer V elankar.387\nAlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space388\nwith high-accuracy models. Nucleic Acids Research, 50(D1):D439‚ÄìD444, 11 2021.389\n[19] Lisa N. Kinch, R. Dustin Schaeffer, Andriy Kryshtafovych, and Nick V . Grishin. T arget classiÔ¨Åcation in the390\n14th round of the critical assessment of protein structure prediction (casp14). Proteins: Structure, Function, and391\nBioinformatics, 89(12):1618‚Äì1632, 2021.392\n[20] Andriy Kryshtafovych, T orsten Schwede, Maya T opf, Krzysztof Fidelis, and John Moult. Critical assessment of393\nmethods of protein structure prediction (casp)‚Äîround xiv . Proteins: Structure, Function, and Bioinformatics,394\n89(12):1607‚Äì1617, 2021.395\n[21] Xavier Robin, Juergen Haas, Rafal Gumienny , Anna Smolinski, Gerardo T auriello, and T orsten Schwede. Contin-396\nuous automated model evaluation (cameo)‚Äîperspectives on the future of fully automated evaluation of structure397\nprediction methods. Proteins: Structure, Function, and Bioinformatics, 89(12):1977‚Äì1986, 2021.398\n[22] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov , Gyu Rie Lee, Jue399\nW ang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia Mill√°n, Hahnbeom Park, Carson Adams, Caleb R.400\nGlassman, Andy DeGiovanni, Jose H. Pereira, Andria V . Rodrigues, Alberdina A. van Dijk, Ana C. Ebrecht,401\nDiederik J. Opperman, Theo Sagmeister, Christoph Buhlheller, T ea Pavkov-Keller, Manoj K. Rathinaswamy , Udit402\nDalwadi, Calvin K. Yip, John E. Burke, K. Christopher Garcia, Nick V . Grishin, Paul D. Adams, Randy J. Read,403\nand David Baker. Accurate prediction of protein structures and interactions using a three-track neural network.404\nScience, 373(6557):871‚Äì876, 2021.405\n[23] Y ang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality .406\nProteins: Structure, Function, and Bioinformatics, 57(4):702‚Äì710, 2004.407\n[24] Peter F Brown, Stephen A Della Pietra, V incent J Della Pietra, Jennifer C Lai, and Robert L Mercer. An estimate408\nof an upper bound for the entropy of english. Computational Linguistics, 18(1):31‚Äì40, 1992.409\n[25] Roshan M Rao, Jason Liu, Robert V erkuil, Joshua Meier, John Canny , Pieter Abbeel, T om Sercu, and Alexander410\nRives. Msa transformer. In International Conference on Machine Learning, pages 8844‚Äì8856. PMLR, 2021.411\n[26] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by412\ngenerative pre-training. 2018.413\n[27] Jianyi Y ang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov , and David Baker. Improved414\nprotein structure prediction using predicted interresidue orientations. Proceedings of the National Academy of415\nSciences, 117(3):1496‚Äì1503, 2020.416\n[28] Jianyi Y ang, Renxiang Y an, Ambrish Roy , Dong Xu, Jonathan Poisson, and Y ang Zhang. The i-tasser suite:417\nprotein structure and function prediction. Nature methods, 12(1):7‚Äì8, 2015.418\n[29] Zongyang Du, Hong Su, W enkai W ang, Lisha Y e, Hong W ei, Zhenling Peng, Ivan Anishchenko, David Baker,419\nand Jianyi Y ang. The trrosetta server for fast and accurate protein structure prediction. Nature protocols,420\n16(12):5634‚Äì5651, 2021.421\n[30] Jian Peng and Jinbo Xu. Raptorx: exploiting structure information for protein alignment by statistical inference.422\nProteins: Structure, Function, and Bioinformatics, 79(S10):161‚Äì171, 2011.423\n[31] Alexander Rives, Joshua Meier, T om Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott,424\nC Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning425\nto 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.426\n[32] Guoxia W ang, Xiaomin Fang, Zhihua Wu, Yiqun Liu, Y ang Xue, Yingfei Xiang, Dianhai Y u, Fan W ang,427\nand Y anjun Ma. Helixfold: An efÔ¨Åcient implementation of alphafold2 using paddlepaddle. arXiv preprint428\narXiv:2207.05477, 2022.429\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on430\nLearning Representations, 2018.431\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,432\n2014.433\n[35] Razvan Pascanu, T omas Mikolov , and Y oshua Bengio. On the difÔ¨Åculty of training recurrent neural networks. In434\nInternational conference on machine learning, pages 1310‚Äì1318. PMLR, 2013.435\n[36] Ruibin Xiong, Y unchang Y ang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Y anyan Lan,436\nLiwei W ang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference437\non Machine Learning, pages 10524‚Äì10533. PMLR, 2020.438\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5164056420326233
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38121822476387024
    },
    {
      "name": "Natural language processing",
      "score": 0.33393794298171997
    }
  ]
}