{
  "title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing",
  "url": "https://openalex.org/W4385573402",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1988150995",
      "name": "Peng Shi",
      "affiliations": [
        "University of Waterloo",
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Pennsylvania State University",
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2100002096",
      "name": "He Bai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163619555",
      "name": "Jimmy Lin",
      "affiliations": [
        "University of Waterloo",
        "Pennsylvania State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4317879253",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3206547074",
    "https://openalex.org/W3104237552",
    "https://openalex.org/W3134665270",
    "https://openalex.org/W3154267298",
    "https://openalex.org/W3101642036",
    "https://openalex.org/W3104917146",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W3102020135",
    "https://openalex.org/W3170978252",
    "https://openalex.org/W4226053975",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W3214364360",
    "https://openalex.org/W4220818323",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3113574536",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W3169937871",
    "https://openalex.org/W2910577570",
    "https://openalex.org/W4310001798",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4226226396",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4285254250",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W2953052971",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3199377785",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4254129368",
    "https://openalex.org/W2897767292",
    "https://openalex.org/W2970605129",
    "https://openalex.org/W3119822474",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W4287027440",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4226293366",
    "https://openalex.org/W1539162551",
    "https://openalex.org/W4281490030",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W3101279614",
    "https://openalex.org/W3214600982",
    "https://openalex.org/W4287550997"
  ],
  "abstract": "In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation.Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models.However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages.To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema.We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English).This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts.We also include global translation exemplars for a target language to facilitate the translation process for large language models.To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi.Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines.Data and code are publicly available at https://github.com/Impavidity/XRICL.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5248–5259\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nXRICL: Cross-lingual Retrieval-Augmented In-Context Learning for\nCross-lingual Text-to-SQL Semantic Parsing\nPeng Shi♠, Rui Zhang♣, He Bai♠, and Jimmy Lin♠\n♠University of Waterloo ♣Penn State University\n{peng.shi,he.bai,jimmylin}@uwaterloo.ca, rmz5227@psu.edu\nAbstract\nIn-context learning using large language mod-\nels has recently shown surprising results for\nsemantic parsing tasks such as Text-to-SQL\ntranslation. Prompting GPT-3 or Codex us-\ning several examples of question-SQL pairs\ncan produce excellent results, comparable to\nstate-of-the-art finetuning-based models. How-\never, existing work primarily focuses on En-\nglish datasets, and it is unknown whether large\nlanguage models can serve as competitive se-\nmantic parsers for other languages. To bridge\nthis gap, our work focuses on cross-lingual\nText-to-SQL semantic parsing for translating\nnon-English utterances into SQL queries based\non an English schema. We consider a zero-shot\ntransfer learning setting with the assumption\nthat we do not have any labeled examples in the\ntarget language (but have annotated examples\nin English). This work introduces the XRICL\nframework, which learns to retrieve relevant En-\nglish exemplars for a given query to construct\nprompts. We also include global translation\nexemplars for a target language to facilitate\nthe translation process for large language mod-\nels. To systematically evaluate our model, we\nconstruct two new benchmark datasets, XSPI-\nDER and XKAGGLE -DBQA , which include\nquestions in Chinese, Vietnamese, Farsi, and\nHindi. Our experiments show that XRICL ef-\nfectively leverages large pre-trained language\nmodels to outperform existing baselines. Data\nand code are publicly available at https://\ngithub.com/Impavidity/XRICL.\n1 Introduction\nSemantic parsing is the task of translating natu-\nral language questions into meaning representa-\ntions such as Lambda CDS (Liang, 2013), Python\ncode (Yin et al., 2018), and SQL (Yu et al., 2018).\nMore recently, Text-to-SQL semantic parsing has\nattracted attention from academia and industry due\nto its challenging setup and practical applications.\nCross-lingual Text-to-SQL semantic parsing (Sher-\nborne and Lapata, 2022b; Min et al., 2019; Sher-\nborne et al., 2020) aims to translate non-English\nutterances into SQL queries based on an English\nschema (assuming we have an internationalized\ndatabase), enabling users to query databases in non-\nEnglish languages. For example, such a system\ncould help people from around the world access\nthe US government’s open data1 with natural lan-\nguage questions in different languages.\nState-of-the-art approaches for Text-to-SQL se-\nmantic parsing have been greatly improved by fine-\ntuning pre-trained language models as a sequence-\nto-sequence problem (Scholak et al., 2021; Yin\net al., 2020; Herzig et al., 2020; Yu et al., 2021a,b;\nShi et al., 2021a). More recently, in-context learn-\ning with large language models (LLMs), such as\nGPT-3 (Brown et al., 2020) and Codex (Chen et al.,\n2021), has emerged as a new learning paradigm.\nThis paradigm enables effective few-shot learning\nwithout model finetuning, showing its practical and\nscientific value (Beltagy et al., 2022). Recent pa-\npers also have shown promising results applying\nin-context learning to the Text-to-SQL task. Ra-\njkumar et al. (2022) studied if LLMs are already\ncompetitive Text-to-SQL semantic parsers with-\nout further finetuning on task-specific training data.\nAdditionally, Poesia et al. (2022) and Rubin et al.\n(2022) investigated the exemplar retrieval problem\nfor the semantic parsing task.\nHowever, previous work mostly focused on En-\nglish utterances, leaving other languages behind.\nIt is unclear if LLMs are competitive for cross-\nlingual Text-to-SQL with English exemplars using\nin-context learning. Even in the mono-lingual set-\nting (where the exemplars and the query are in the\nsame language), many approaches are not practi-\ncal beyond English due to the paucity of target\nlanguage query-SQL exemplars.\nTo bridge this gap, we propose XRICL , a novel\nframework based on LLMs with in-context learn-\n1https://data.gov\n5248\n大多数犯罪发生在什么时间？\nRetrieverReranker\nzhvi Generator\nWhen do most crimes take place?SQL: SELECT CrimeTSFROM GreaterManchesterCrimeGROUP BY CrimeTSORDER BY count(*) DESC LIMIT 1\nLabeledEnglishCandidates\nTranslation-basedCandidatesRetrievedexemplars\n1 2\n34\nNon-Englishutterance\nGeneratedSQL\nSQLitetables:…Q:What is the most common birth placeof people?SQL: SELECT Birth_PlaceFROM people GROUP BY Birth_PlaceORDER BY COUNT(*) DESC LIMIT 1\nSQLitetables:…Q:部门中有多少人年龄大于56岁？Translate into English: How many heads of the departments are older than 56 ?SQL: SELECT count(*) FROM head WHERE age  >  56\nSQLite tables:GreaterManchesterCrime(CrimeID, CrimeTS, Location, LSOA, Type, Outcome)Q: 大多数犯罪发生在什么时间？Translate into English:\n…\nTranslation-P\nIndex keyExampleTranslation-basedExample\nFigure 1: Overview of our proposed XRICL framework. Given a labeled English question-SQL candidate pool and\nthe non-English question as input, our framework uses in-context learning with a large pre-trained language model\n(e.g., Codex) to generate SQL queries in four steps: (1) Cross-lingual Exemplar Retrieval, (2) Exemplar Reranking,\n(3) Prompt Construction with Translation as Chain-of-Thought, and (4) Inference.\ning for cross-lingual Text-to-SQL semantic parsing.\nSpecifically, the task is to generate SQL queries for\nnon-English queries based on an English schema\nand an English query-SQL candidate pool. Our\nframework first constructs the context prompt by\nretrieving the most relevant English query-SQL ex-\nemplars for each target language query. Since we\ndo not have any training data in the target language,\nwe cannot train a retriever for target queries di-\nrectly. Our solution is to train an English exemplar\nretriever with mT5 (Xue et al., 2021) and adopt\na model-based cross-lingual transfer method for\ncross-lingual retrieval. The English exemplar re-\ntriever is trained with feedback from the LLM itself\nby distilling soft labels (likelihood).\nOur framework introduces an additional exem-\nplar into the LLM’s input context, to instruct the\nmodel to translate the target query into English\nand then to translate the English query into SQL;\nthis approach is inspired by recent work on chain-\nof-thought prompting (Wei et al., 2022; Shi et al.,\n2022). However, in our framework, this additional\nexemplar is identical for all test queries, which\nmeans that we only need a single pair of transla-\ntions for any English-target language pair, requiring\nminimal translation effort.\nDuring the inference process, the language\nmodel is expected to generate the English trans-\nlation first and then the SQL query. In our exper-\niments, we find that our proposed retriever and\nreranker can improve the LLMs’ cross-lingual few-\nshot in-context learning performance by a large\nmargin, and further improvements can be observed\nby adding an additional translation exemplar.\nWe further construct two benchmarks, XSPI-\nDER and XKAGGLE -DBQA , to systematically eval-\nuate the proposed framework in many languages.\nFor XSPIDER , besides adopting existing work, in-\ncluding CSPIDER (Min et al., 2019) and VSPI-\nDER (Tuan Nguyen et al., 2020), we further trans-\nlate the SPIDER dataset into Farsi and Hindi for\nevaluation. For XKAGGLE -DBQA , we translate\nthe English KAGGLE -DBQA dataset into Chinese,\nFarsi, and Hindi. Experimental results show that\nour proposed framework improves effectiveness\ncompared to baseline systems.\nOur contributions are summarized as follows:\n(1) We propose a novel retrieve-rerank framework\nto improve the exemplar selection process for in-\ncontext learning for cross-lingual Text-to-SQL se-\nmantic parsing. To the best of our knowledge, we\nare the first to explore the effectiveness of large\npre-trained language models for cross-lingual Text-\nto-SQL semantic parsing. (2) We propose to use\ntranslation as a chain-of-thought prompt in the in-\nference process, bridging the cross-lingual gap for\nlarge language models. (3) Last, we construct two\nnew benchmarks, XSPIDER and XKAGGLE -DBQA ,\nto facilitate evaluation of cross-lingual Text-to-SQL\nsemantic parsing.\n5249\n2 Task Formulation\nGiven a database where the schema s is in En-\nglish (denoted as the source language), our task\nis to translate a non-English (denoted the target\nlanguage) example x (x includes utterance u and\nschema s) into a SQL query a. In this work, we\nexplore large pre-trained language models such as\nCodex for this Text-to-SQL task with in-context\nlearning. To support in-context learning, labeled\ncandidates of (utterance, schema, SQL) triples are\nrequired. Since more annotated resources are avail-\nable in English, we assume that the labeled can-\ndidate set D is in English. Overall, in-context\nlearning is an efficient method to leverage large\npre-trained language models without expensive pa-\nrameter fine-tuning. Furthermore, the candidate\npool can be easily expanded for better generaliza-\ntion to new domains.\n3 The XRICL Framework\nOur XRICL framework is shown in Figure 1, con-\nsisting of four steps:\n(1) Cross-lingual Exemplar Retrieval: Retrieve a\nlist of N English exemplars that are relevant to the\ninput non-English example x.\n(2) Exemplar Reranking: Rerank the retrieved N\nexemplars and use the topK exemplars to construct\nprompts.\n(3) Prompt Construction with Translation as Chain\nof Thought: Construct a prompt consisting of the\ntranslation exemplar as a chain of thought, the se-\nlected K exemplars, and the input example.\n(4) Inference: Feed the prompt into a pre-trained\nlanguage model to generate SQL.\n3.1 Cross-lingual Exemplar Retriever\nGiven a non-English question, the goal of the cross-\nlingual exemplar retriever is to find relevant exem-\nplars from the English candidate pool efficiently\nthat can improve the predictions of the generators.\nConsidering that we use labeled examples in En-\nglish (a high-resource language) as candidates, we\nformulate this step as a cross-lingual retrieval prob-\nlem, where the test question is in a non-English lan-\nguage. In this case, traditional term matching meth-\nods such as BM25 (Robertson and Zaragoza, 2009)\nor BM25 + RM3 query expansion (Lin, 2018) can-\nnot be applied due to token mismatch. Instead, we\npropose to use a bi-encoder for cross-lingual seman-\ntic retrieval with model-based zero-shot transfer.\nWe further improve the retriever with distillation-\nbased training.\nModel. Here, we leverage the popular bi-encoder\narchitecture known as dense passage retriever\n(DPR) (Karpukhin et al., 2020), where the query\nand candidates are mapped into representation vec-\ntors independently. The retriever uses a dense en-\ncoder Eu(·) that converts an utterance into a d-\ndimensional vector and builds an index over the\ncandidate pool that is used for retrieval.\nFor a test instance x, we use the same dense en-\ncoder to map the utterance into a d-dimensional\nvector (denoted the query vector). Based on the\nquery vector, the closest top N exemplars are re-\ntrieved from the pre-built index based on the pre-\ndefined distance function. Following Karpukhin\net al. (2020), we define the distance function as\nsim(x, z) = Eu(x)⊤Eu(z) (1)\nwhere Z is the set of candidate exemplars and z ∈\nZ. We use a transformer as the dense encoder, and\nthe average of the contextual embeddings of the\nutterance tokens is taken as the representation of\nthe encoded text.\nModel-based Cross-lingual Transfer. Consid-\nering that we do not have training data in tar-\nget languages, we adopt a model-based cross-\nlingual transfer method, where we leverage the\nzero-shot cross-lingual transfer ability of multilin-\ngual pre-trained transformers such as mBERT (De-\nvlin et al., 2019), XLM-Roberta (Conneau et al.,\n2020), mBART (Liu et al., 2020), and mT5 (Xue\net al., 2021). Specifically, we train the dense re-\ntriever in the source language, where both the query\nutterance and candidate utterances are in English\n(in our case), and apply inference directly on query\nutterances in the target language and retrieve En-\nglish exemplars in a cross-lingual manner.\nDistillation-based Training.One common prac-\ntice for bi-encoder training is contrastive learning.\nGiven a query, positive examples and negative ex-\namples are required. The model is optimized such\nthat examples from the positive class have simi-\nlar representations and examples from the negative\nclass have different representations.\nThe key here is how to define positive and neg-\native examples for the semantic parsing task. Re-\ncently, Hu et al. (2022) used the similarity of tar-\nget meaning representations to first rank the can-\ndidates and choose the top-k as positive examples\nand the bottom- k as negative examples. Instead\n5250\nRetriever\nGeneratorExemplarCandidatesContribution Distribution\nRelevance Distribution\nKL divergence\nFigure 2: Illustration of distillation-based training. The\ncontribution distribution is the likelihood distribution\nof the top- N exemplars produced by the LLM. The\nrelevance distribution is the ranking score distribution\nproduced by the retriever.\nof using human-designed relevance metrics, Rubin\net al. (2022) proposed to use a language model to\nlabel positive and negative examples for contrastive\nlearning; similar to Hu et al. (2022), hard labels\nare used. Another way to train the bi-encoder is to\nuse a regression-based loss function. Poesia et al.\n(2022) proposed to retrieve exemplars that have\nrelevant program structures (tree edit distance of\nSQL abstract syntax trees is used as the relevance\nmetric) for the test utterances and the model is opti-\nmized with mean-squared error loss for predicting\nthe similarity score.\nAs an alternative to these above approaches, we\ntrain our retriever by distilling the LLM’s scor-\ning function. This scoring function calculates the\nground-truth SQL query’s likelihood given an En-\nglish exemplar zk and the input utterance x, which\nestimates the importance of this exemplar for pars-\ning the given input utterance. Hence, we score\nthe retrieved English exemplars with an LLM and\noptimize the KL divergence between the LLM’s\nranking scores and the retriever’s ranking scores to\nupdate the retriever, as shown in Figure 2. This re-\ntriever is denoted DE-Retriever (Distillation-based\nExemplar Retriever). Intuitively, with the KL diver-\ngence loss function, the model tries to match the\nprobability of retrieving an exemplar zk with the\ncontribution of that exemplar to the generated SQL\nquery a.\nWe first obtain N exemplars with the highest\nscores based on Equation (1), denoted as Ztop−N .\nOur loss function is defined as:\nLdistill = KL( SG(p(zn |x, a, Ztop−N ; G))\n∥p(zn |x, Z; E)), (2)\nwhere SG denotes the stop gradient operation, G de-\nnotes the generator, and E denotes the retriever en-\ncoder. We further compute p(zn |x, a, Ztop−N ; G)\nas follows:\np(zn |x, a, Ztop−N ) ∝\np(a |x, zn, Ztop−N ; G) p(zn |x, Ztop−N ) (3)\nWe approximate the posterior under the assump-\ntion that we have a uniform prior over the\nset of retrieved exemplars, so p(zn |x, Ztop−N )\nis approximated as 1\nN . We further compute\np(a |x, zn, Ztop−N ; G) as:\nexp(p(a |x, zn))∑N\nj=1 exp(p(a |x, zj))\n(4)\nwhere p(a |x, zj) is computed with the generator.\nMore specifically, we use example zj as the\nprompt and concatenate it with test instance u\nand target SQL a. Then we feed it to the gen-\nerator to compute the log probability of each token\nlog(p(ai)) in the target SQL query a; p(a |x, zj)\ncan be computed as exp(∑log(p(ai))).\n3.2 Exemplar Reranking\nFor tasks such as information retrieval and open-\ndomain question answering, reranking is widely\nadopted to further improve retrieval results by in-\ncorporating a reranker. Such a two-stage procedure\nis also useful in a variety of natural language pro-\ncessing tasks. In this work, following the retrieve-\nand-rerank idea, we propose to incorporate an ex-\nemplar reranker in our framework. This reranker\ncan leverage token-level interactions between the\nutterances to better rank the exemplars.\nMore specifically, the query utterance u and the\ncandidate utterance uz are concatenated together\nwith special tokens: [CLS] u [SEP] uz [SEP].\nThe tokenized input is fed into a transformer model.\nAn MLP with sigmoid activation is applied on top\nof the contextual embedding of the [CLS] token\nto obtain the relevance score of the candidate ex-\nample (Lin et al., 2021). Sigmoid cross-entropy\nloss is used and the model is optimized to pro-\nduce a relevance score as p(a|x, zn, Ztop−N ; G).\nThis reranker is denoted DE-Reranker (Distillation-\nbased Exemplar Reranker).\n3.3 Prompt Construction with Translation as\nChain of Thought\nFrom the input instance x and the list of retrieved-\nand-reranked exemplars Z, we construct the aug-\nmented query by concatenating exemplars with the\ninput instance following previous work (Hu et al.,\n2022; Rubin et al., 2022; Poesia et al., 2022; Liu\n5251\net al., 2022; Brown et al., 2020; Pasupat et al.,\n2021). For the exemplar, we linearize the table\nschema, the question, and the SQL query. The ex-\nemplars are sorted by relevance score in descending\norder. For the test instance, only the table schema\nand the question are linearized. We denote this\nprompting approach Vanilla-P.\nTranslation as Chain of Thought: Recent work\non chain-of-thought prompting is designed to solve\nthe multi-step reasoning problem by providing in-\ntermediate reasoning steps before the final answer\nin the prompt (Wei et al., 2022). Inspired by this,\nwe use the translation pair (from non-English to\nEnglish in our case) as an intermediate step for\ncross-lingual semantic parsing inference.\nSpecifically, a translation-based exemplar is in-\nserted in front of Z. For example, in the right\npart of Figure 1, the grey box contains the Chi-\nnese version of the translation as a chain-of-thought\nprompt. The question in the prompt is in the target\nlanguage, followed by an instruction Translate\ninto English and the English translation of the\nquestion. Note that this translation-based exemplar\nis shared among all the test instances in that lan-\nguage, as shown in the left part of Figure 1. The\ntranslation-based examples are indexed by the lan-\nguage code, such as zh and vi. In this way, it\nonly requires minimal translation effort to build the\nglobal translation-based exemplar. We denote this\nprompting approach Translation-P.\n3.4 Inference\nFor inference, we feed the constructed prompt to\na large pre-trained language model to generate\nthe target SQL query with greedy decoding. In\nthis work, we consider Codex (Codex-Davinci-\n001) (Chen et al., 2021) because it has shown su-\nperior performance for the English Text-to-SQL\ntask (Poesia et al., 2022).\n4 Experimental Settings\nIn this section, we describe the datasets, implemen-\ntation details, and baselines for our experiments.\n4.1 Datasets\nWe create two benchmarks, XSPIDER and\nXKAGGLE -DBQA , by translating existing English\nText-to-SQL datasets into other languages and eval-\nuate our methods on these two benchmarks.\nXSPIDER : CSPIDER (Min et al., 2019) and VSPI-\nDER (Tuan Nguyen et al., 2020) are Chinese (zh)\nand Vietnamese (vi) cross-domain Text-to-SQL\ndatasets translated from SPIDER (Yu et al., 2018).\nMore specifically, we use the EnglishSPIDER train-\ning set as the candidate pool and training data for\nretriever-reranker models. We use the development\nsets of CSPIDER and VSPIDER for cross-lingual\nevaluation. We further translate the SPIDER devel-\nopment set into Farsi (fa) and Hindi (hi) for a more\ncomprehensive evaluation.\nXKAGGLE -DBQA : This is a recently constructed\ndataset for more realistic and challenging Text-\nto-SQL evaluation. The dataset is based on 8\ndatabases from Kaggle. We translate the questions\ninto Chinese (zh), Farsi (fa), and Hindi (hi) for\ncross-lingual evaluation. We use the English SPI-\nDER training set as the candidate pool.\n4.2 Experimental Details\nFor the exemplar retriever, we use 24-layer trans-\nformers initialized with the parameters of the mT5\nencoder that is then fine-tuned on the English SPI-\nDER dataset for the Text-to-SQL task. For the exem-\nplar reranker, we use InfoXLM (Chi et al., 2021)\nas the starting point. We train the retriever and\nreranker on the English SPIDER dataset and then\napply both models to cross-lingual retrieval and\nreranking in a zero-shot fashion. For the Codex\nconfiguration, we use greedy decoding by setting\nthe temperature to zero. We use N = 16 and\nK = 8for all experiments, which means that the\nDE-Retriever first retrieves 16 exemplars from the\ncandidate pool and the DE-Reranker produces the\ntop 8 exemplars for prompt construction.\nIn terms of evaluation metrics, we use Exact\nMatch (EM) accuracy for both theXSPIDER bench-\nmark and the XKAGGLE -DBQA benchmark. Fol-\nlowing Zhong et al. (2020), we report the Test-\nsuite (TS) accuracy. Only the datasets that are\naligned with the SPIDER dev set can be evaluated\nwith TS accuracy, so the XKAGGLE -DBQA bench-\nmark is not applicable. Because the CSPIDER dev\nset is only partially aligned to the SPIDER dev set,\nthe full CSPIDER (zh-full) dev set can be only eval-\nuated with EM accuracy. We collect a subset of the\nCSPIDER dev set (zh) whose queries are aligned\nwith the English SPIDER dev set, and further evalu-\nate these using TS accuracy.\n4.3 Baselines\nmT5 zero-shot transferis a baseline model that\nis trained with the English SPIDER training set.\n5252\nModel zh-full zh vi fa hi\nEM EM TS EM TS EM TS EM TS\n(1) mT5 zero-shot 39.7 47.9 48.4 42.1 40.1 41.3 39.5 41.2 39.7\n(2) mUSE 38.4 43.0 46.8 31.8 33.4 28.9 31.1 22.2 23.7\n(3) mSBERT 37.9 41.3 47.1 34.6 33.5 29.3 31.8 22.0 22.3\n(4) mT5-encoder 44.4 48.1 51.4 41.3 39.5 38.4 38.5 28.6 27.0\n(5) DE-Retriever 46.0 50.4 53.9 42.2 40.7 38.2 40.0 29.9 27.9\n(6) DE-R2 46.4 52.1 55.3 44.4 41.9 40.0 40.6 30.0 28.2\n(7) + Translation-P 47.4 52.7 55.7 43.7 43.6 43.2 45.1 32.6 32.4\nTable 1: Results on the XSPIDER dev set. “zh-full” and “zh” are two different splits from CSPIDER (Min et al.,\n2019). EM and TS are exact match accuracy and test suite accuracy, respectively. Entry (5) is based on the\nDE-Retriever with Vanilla-P. Entry (6) is based on the DE-Retriever and DE-Reranker (denoted as DE-R2) with\nVanilla-P. Entry (7) is based on DE-R2 with Translation-P.\nThe model is based on the pre-trained sequence-\nto-sequence multilingual language model mT5-\nlarge (Xue et al., 2021). This model has zero-shot\ncross-lingual transfer ability, with which the model\ncan directly handle non-English utterances.\nmUSE and mSBERTare baselines that use un-\nsupervised retrievers to obtain exemplars: multi-\nlingual Universal Sentence Encoder (Yang et al.,\n2020) and multilingual Sentence-BERT (Reimers\nand Gurevych, 2019). Prompts are then constructed\nfor in-context learning with Codex.\n5 Results\n5.1 Results on XSPIDER\nResults on XSPIDER are shown in Table 1. We\nreport the EM and TS accuracy. For the full CSPI-\nDER dataset (zh-full), since TS Accuracy is not\nsupported, we only report EM accuracy. We report\nboth TS and EM accuracy on the subset of CSPI-\nDER . Entry (1) reports the zero-shot performance\nof the mT5 model that is trained on the EnglishSPI-\nDER dataset. On zh-full, vi, fa, and hi, the mT5\nzero-shot method obtains on average 41.1 EM ac-\ncuracy and 39.8 TS accuracy (average TS accuracy\nis computed without zh-full because the metric\ncannot be computed on the full CSPIDER ).\nFrom entry (2) to entry (7), the methods are\nbased on in-context few-shot learning. For entries\n(2–6), the prompting method is Vanilla-P. For entry\n(7), prompting with Translation-P is applied.\nWith unsupervised exemplar retrievers such as\nmUSE and mSBERT, shown in entries (2) and (3),\nCodex performs worse than mT5 zero-shot transfer,\nespecially for Farsi (39.5→31.1/31.8 on TS accu-\nracy) and Hindi (39.7→23.7/22.3 on TS accuracy).\nBy switching the unsupervised exemplar retriever\nto the mT5-encoder, which is the encoder compo-\nnent of the fine-tuned mT5 model, the effectiveness\nof Codex improves by a large margin. For exam-\nple, on the CSPIDER subset, TS accuracy improves\nto 51.4 from 47.1, outperforming mT5 zero-shot\nperformance by 3 points. This indicates that the\nexemplar retrieval component is essential to take\nadvantage of the competitive performance of LLMs\nsuch as Codex. For languages such as Vietnamese\nand Farsi, Codex is comparable to mT5 zero-shot\ntransfer, while for Hindi, there is still a large gap\n(39.7 vs. 27.0 on TS accuracy).\nBy applying our proposed distillation based\nretriever-reranker pipeline (denoted as DE-R2) for\nretrieving exemplars, impressive improvements can\nbe observed in all four languages by comparing en-\ntry (6) with entry (4). Our end-to-end results are\nshown in entry (7), where we see that our proposed\nframework achieves the best results for most of the\nlanguages (except Vietnamese EM accuracy) in the\nin-context learning setting.\nComparing the best results of in-context learn-\ning with mT5 zero-shot results, we can see that\nCodex can achieve better performance in Chinese,\nVietnamese, and Farsi. For example, XRICL out-\nperforms mT5 zero-shot by 7.7 EM accuracy on\nthe full dev set of CSPIDER . One exception is\nHindi, where the best in-context learning perfor-\nmance cannot match mT5 zero-shot transfer. One\npossible explanation is that Codex has weaker mod-\neling ability in Hindi because less Hindi data were\naccessible during the training.\n5.2 Results on XKAGGLE -DBQA\nThere is agreement by researchers today that\nXKAGGLE -DBQA is a more realistic evaluation\nfor the Text-to-SQL parsing task. The databases\nare real-world databases with abbreviated column\n5253\nModel zh fa hi\n(1) mT5 zero-shot 9.7 8.1 7.6\n(2) mUSE 20.7 12.4 16.2\n(3) mSBERT 14.7 13.0 11.9\n(4) mT5-Encoder 22.2 16.8 16.2\n(5) DE-Retriever 26.5 18.4 16.8\n(6) DE-R2 27.0 18.4 17.8\n(7) + Translation-P 28.1 20.0 19.5\nTable 2: Results on the XKAGGLE -DBQA test set. We\nreport exact match (EM) accuracy.\nnames. We use the training set of English SPIDER\nas the candidate pool. In this case, both the model’s\ngeneralization ability and its cross-lingual transfer\ncapability can be tested.\nThe XKAGGLE -DBQA results are shown in Ta-\nble 2. Entry (1) shows the zero-shot cross-lingual\ncross-domain transfer performance of the mT5\nmodel trained on the English SPIDER dataset. For\nexample, on Chinese KAGGLE -DBQA , mT5 only\nobtains 9.7 EM accuracy. For comparison, mT5\nreach 20.0 EM accuracy on the English test set\nin a zero-shot fashion, outperforming the previous\nstate of the art obtained by RAT-SQL (Wang et al.,\n2020) with 18.4 EM accuracy (Lee et al., 2021)\nusing column descriptions and model adaptation.\nThis indicates that the mT5 model is more robust\nthan RAT-SQL on domain transfer. However, the\neffectiveness degrades drastically when mT5 is ap-\nplied to non-English languages. The mT5 zero-shot\nmethod on average obtains only 8.5 EM accuracy\nin the three languages.\nFor the Codex-based in-context learning meth-\nods, the results are shown in entries (2–7). With un-\nsupervised retrieval methods such as mUSE, Codex\ncan reach 20.7 EM accuracy in Chinese, improving\nover the zero-shot mT5 baseline. Comparing en-\ntries (2) and (3), there is no clear winner for these\ntwo unsupervised retrieval methods. Our end-to-\nend results are shown in entry (7), which achieves\nstate-of-the-art performance on the XKAGGLE -\nDBQA benchmark, with 22.5 EM accuracy on aver-\nage, which is better than the mT5 zero-shot method.\nFor example, on Chinese KAGGLE -DBQA , our\nframework obtains an 18.4 point improvement over\nmT5 zero-shot transfer.\n6 Analysis\n6.1 Effectiveness on English Text-to-SQL\nWe show that our model is comparable to other\nin-context learning methods for English semantic\nModel EM EX TS\nRubin et al. (2022) (our impl.) 48.5 53.5 50.3\nPoesia et al. (2022) - 60.0 -\nRajkumar et al. (2022) - 67.0 55.1\nDE-Retriever (Ours) 53.5 60.3 56.3\nTable 3: Results on the EnglishSPIDER development set.\nOur system achieves results comparable to other state-\nof-the-art in-context learning methods for English Text-\nto-SQL. EM: Exact Match Accuracy. EX: Execution\nAccuracy. TS: Test-suite Accuracy (Zhong et al., 2020).\nparsing. Through this comparison, we show that\nour framework is built on a competitive backbone\nfor Text-to-SQL. We use the DE-Retriever as the\nbackbone model in the ablation study and compare\nwith three recent methods, described as follows:\nRubin et al. (2022) used hard labels obtained from\nthe generator to train the retriever. Poesia et al.\n(2022) used the tree edit distance of SQL queries\nas a similarity function: a smaller distance means\nbetter exemplar quality for the specific test instance.\nThe ranking model is optimized to predict the target\nSQL pair tree edit distance based on the utterance\npair. Rajkumar et al. (2022) designed an efficient\nprompt that leverages table contents for zero-shot\nText-to-SQL. We refer the reader to the original\npapers for more details.\nTable 3 shows the results on the SPIDER devel-\nopment set. Our backbone system (DE-Retriever +\nCodex Generator) obtains 53.5 EM accuracy and\n60.3 EX accuracy, which is comparable to the 60.0\nEX accuracy reported by Poesia et al. (2022). Com-\nparing to Rajkumar et al. (2022), our system ob-\ntains comparable TS accuracy (56.3 vs. 55.1).\n6.2 Effectiveness of DE-R2\nWe analyze the effectiveness of DE-R 2 on the\nXSPIDER benchmark and the XKAGGLE -DBQA\nbenchmark. By comparing entries (5) and (4) in\nTable 1 and Table 2, we can observe that the DE-\nRetriever can improve over the mT5-encoder base-\nline in most of the languages (except EM accuracy\nin Farsi). Comparing entries (6) and (5), we find\nthat the reranker can further improve the EM accu-\nracy and the TS accuracy. This indicates that our\nXRICL framework is effective in selecting good\nexemplars as prompts.\n6.3 Effectiveness of Chain-of-Thought Prompt\nBy comparing entries (7) and (6) in Table 1 and\nTable 2, we find that Translation-P can further im-\n5254\nModel zh-full zh\nEM EM TS\n(1) DE-R2 + Translation-P 47.4 52.7 55.7\n(2) T-Oracle 46.3 52.6 57.6\n(3) TG-Oracle 52.5 58.0 62.2\nTable 4: Results with oracles: T-Oracle is the Template\nOracle and TG-Oracle is the Template+Generator Ora-\ncle. EM accuracy and TS accuracy are reported.\nprove the semantic parsing ability of Codex on top\nof DE-R2, except EM accuracy for Vietnamese.\n6.4 Oracle Performance\nIt is interesting to investigate the upper bound of\nCodex on cross-lingual Text-to-SQL semantic pars-\ning. We design two pipelines to experiment with\nthe capabilities of Codex when an oracle is avail-\nable (i.e., the target SQL query is accessible to help\nthe retrieval and reranking). We experiment with\ntwo different oracles:\nTemplate Oracle: We retrieve exemplars using\nthe gold parse. The template is extracted from\nthe target SQL query and only exemplars with the\nsame SQL template are retrieved. This is based\non the assumption that utterances with the same\nSQL templates share the same query intent and the\ngenerator can benefit from these exemplars.\nTemplate Oracle + Codex LM oracle: Here we\nintroduce an oracle from the generator (Codex) into\nthe pipeline. More specifically, we replicate the\ntraining process in the testing phase. The exemplars\nwith the same SQL templates are first retrieved. For\neach retrieved exemplar, we use Codex to compute\nits contribution to the test instance as the reranking\nscore. We then use the top-k as the exemplars.\nThe experimental results are shown in Table 4.\nComparing entries (1) and (2), we can observe that\nour XRICL framework can outperform the Tem-\nplate Oracle in terms of EM accuracy on the full\ndataset and is comparable on the subset. Template\nOracle + Codex LM Oracle reaches 52.5 on the full\ndataset and 58.0 on the subset in terms of EM ac-\ncuracy. This suggests that signals from the Codex\nLM are useful and that there is additional room for\nimprovement in our framework.\n7 Related Work\nIn-context Learning: In-context learning is a rel-\natively new paradigm for zero-shot and few-shot\nlearning with large-scale pre-trained language mod-\nels, first proposed in GPT-3 (Brown et al., 2020).\nIn-context learning for semantic parsing has been\nintensively investigated recently (Pasupat et al.,\n2021; Rubin et al., 2022; Shin and Van Durme,\n2022; Rajkumar et al., 2022; Hu et al., 2022; Xie\net al., 2022; Chen et al., 2021; Poesia et al., 2022).\nHowever, most of the work considers only English,\nwithout examining the cross-lingual ability of the\nproposed methods. Winata et al. (2021) evaluated\nthe multilinguality of pre-trained language models\non non-English multi-class classification with in-\ncontext learning. However, their task is simpler\nthan semantic parsing tasks such as ours. To the\nbest of our knowledge, we are the first to explore\ncross-lingual Text-to-SQL semantic parsing under\nthe in-context learning setting.\nCross-lingual Semantic Parsing: Cross-lingual\nsemantic parsing aims to handle user utterances\nfrom multiple languages and translate them into\nformal representations. Recent advances can be\ncategorized into two threads: multilingual dataset\ncreation and model development.\nFor example, Bai et al. (2018) adapted a Chinese\ndialogue parsing dataset into English. Min et al.\n(2019) and Tuan Nguyen et al. (2020) adapted the\nEnglish Text-to-SQL dataset SPIDER (Yu et al.,\n2018) into Chinese and Vietnamese, which are used\nin this work for evaluation. Some multilingual\ndatasets with different formal representations have\nalso been created, such as SPARQL (Cui et al.,\n2022) and TOP (Li et al., 2021).\nIn terms of model development, Shao et al.\n(2020) is the most relevant to our work, which\nleveraged bilingual input for the semantic pars-\ning task. However, they used RNN models and\nfocused on multilingual representation alignment\nwith pre-training. Instead, our work focuses on\nrepresentation mixup with large multilingual pre-\ntrained models. Improving cross-lingual zero-shot\ntransfer is another direction (Sherborne et al., 2020;\nSherborne and Lapata, 2022b,a).\nMultilingual and Cross-lingual Retrieval: In\nmultilingual retrieval, the task is to retrieve rel-\nevant documents where the user queries and the\ncorpora are in the same language. Recent work\ntakes advantage of cross-language transfer using\npre-trained multilingual models (Shi et al., 2020,\n2021b; Zhang et al., 2022b, 2021). For example,\nShi et al. (2021b) used DPR to retrieve documents\nbased on ad-hoc queries in six languages. On the\n5255\nother hand, cross-lingual retrievers help users find\nrelevant documents in languages that are different\nfrom that of the queries. This task has a long his-\ntory that goes back several decades (Nie, 2010),\nbut recent work includes Zhang et al. (2022a);\nLitschko et al. (2022); Sun and Duh (2020). For\ninstance, Asai et al. (2021) created a cross-lingual\nopen-domain question answering dataset where the\nsystem is required to retrieve passages from differ-\nent languages to answer user questions.\n8 Conclusion\nIn this work, we proposed the XRICL frame-\nwork that improves in-context learning for cross-\nlingual Text-to-SQL semantic parsing. The retrieve-\nand-rerank models that we propose can learn sig-\nnals from large pre-trained models (Codex) to im-\nprove the quality of selected exemplars, which\ncan further benefit the generator. By integrating\nprompts inspired by chain of thought, our pro-\nposed Translation-P method can bridge the cross-\nlingual gap for the generator. Extensive experi-\nments on XSPIDER and XKAGGLE -DBQA demon-\nstrate the effectiveness of our framework, which\nobtains state-of-the-art performance on few-shot\nin-context learning in most of the datasets, thus\nunlocking the potential of Codex.\n9 Limitations\nOur work is based on the large language model\nCodex, which is not open-sourced. To replicate our\nexperiments, an application to OpenAI for Codex\nAPI access is required. Due to annotation costs, we\nwere unable to evaluate on more languages than\nthose described in this paper. In the future, we plan\nto collect more data to investigate Codex perfor-\nmance on different language families.\nAcknowledgements\nThis research was supported in part by the Nat-\nural Sciences and Engineering Research Council\n(NSERC) of Canada, Compute Ontario, and Com-\npute Canada.\nReferences\nAkari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee,\nEunsol Choi, and Hannaneh Hajishirzi. 2021. XOR\nQA: Cross-lingual open-retrieval question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 547–564, Online.\nHe Bai, Yu Zhou, Jiajun Zhang, Liang Zhao, Mei-Yuh\nHwang, and Chengqing Zong. 2018. Source criti-\ncal reinforcement learning for transferring spoken\nlanguage understanding to a new language. In Pro-\nceedings of the 27th International Conference on\nComputational Linguistics, pages 3597–3607, Santa\nFe, New Mexico, USA.\nIz Beltagy, Arman Cohan, Robert Logan IV , Sewon Min,\nand Sameer Singh. 2022. Zero- and few-shot NLP\nwith pretrained language models. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics: Tutorial Abstracts, pages\n32–37, Dublin, Ireland.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online.\nRuixiang Cui, Rahul Aralikatte, Heather Lent, and\nDaniel Hershcovich. 2022. Compositional gener-\nalization in multilingual semantic parsing over Wiki-\ndata. Transactions of the Association for Computa-\ntional Linguistics, 10:937–955.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\nMinneapolis, Minnesota.\n5256\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022. In-\ncontext learning for few-shot dialogue state tracking.\narXiv preprint arXiv:2203.08568.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline.\nChia-Hsuan Lee, Oleksandr Polozov, and Matthew\nRichardson. 2021. KaggleDBQA: Realistic evalu-\nation of text-to-SQL parsers. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), Online.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950–2962, Online.\nPercy Liang. 2013. Lambda dependency-based compo-\nsitional semantics. arXiv preprint arXiv:1309.4408.\nJimmy Lin. 2018. The neural hype and comparisons\nagainst weak baselines. SIGIR Forum, 52(2):40–51.\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021.\nPretrained Transformers for Text Ranking: BERT\nand Beyond. Morgan & Claypool Publishers.\nRobert Litschko, Ivan Vuli ´c, Simone Paolo Ponzetto,\nand Goran Glavaš. 2022. On cross-lingual retrieval\nwith multilingual text encoders. Information Re-\ntrieval Journal, 25(2):149–183.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nQingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A\npilot study for Chinese SQL semantic parsing. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3652–\n3658, Hong Kong, China.\nJian-Yun Nie. 2010. Cross-Language Information Re-\ntrieval. Morgan & Claypool Publishers.\nPanupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.\nControllable semantic parsing via retrieval augmen-\ntation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7683–7698, Online and Punta Cana, Domini-\ncan Republic.\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\nGustavo Soares, Christopher Meek, and Sumit Gul-\nwani. 2022. Synchromesh: Reliable code generation\nfrom pre-trained language models. In International\nConference on Learning Representations.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-SQL capa-\nbilities of large language models. arXiv preprint\narXiv:2204.00498.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China.\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333–389.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic.\nBo Shao, Yeyun Gong, Weizhen Qi, Nan Duan, and\nXiaola Lin. 2020. Multi-level alignment pretraining\nfor multi-lingual semantic parsing. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 3246–3256.\nTom Sherborne and Mirella Lapata. 2022a. Meta-\nlearning a cross-lingual manifold for semantic pars-\ning. arXiv preprint arXiv:2209.12577.\n5257\nTom Sherborne and Mirella Lapata. 2022b. Zero-shot\ncross-lingual semantic parsing. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n4134–4153, Dublin, Ireland.\nTom Sherborne, Yumo Xu, and Mirella Lapata. 2020.\nBootstrapping a crosslingual semantic parser. arXiv\npreprint arXiv:2004.02585.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nPeng Shi, He Bai, and Jimmy Lin. 2020. Cross-lingual\ntraining of neural models for document ranking. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 2768–2773.\nPeng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu,\nAlexander Hanbo Li, Jun Wang, Cicero Nogueira dos\nSantos, and Bing Xiang. 2021a. Learning contextual\nrepresentations for semantic parsing with generation-\naugmented pre-training. Proceedings of the AAAI\nConference on Artificial Intelligence, 35(15):13806–\n13814.\nPeng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2021b.\nCross-lingual training of dense retrievers for docu-\nment retrieval. In Proceedings of the 1st Workshop\non Multilingual Representation Learning, pages 251–\n253, Punta Cana, Dominican Republic.\nRichard Shin and Benjamin Van Durme. 2022. Few-\nshot semantic parsing with language models trained\non code. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5417–5425, Seattle, United States.\nShuo Sun and Kevin Duh. 2020. CLIRMatrix: A mas-\nsively large collection of bilingual and multilingual\ndatasets for cross-lingual information retrieval. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4160–4170.\nAnh Tuan Nguyen, Mai Hoang Dao, and Dat Quoc\nNguyen. 2020. A pilot study of text-to-SQL semantic\nparsing for Vietnamese. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 4079–4085, Online.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nRelation-aware schema encoding and linking for text-\nto-SQL parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567–7578, Online.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung. 2021.\nLanguage models are few-shot multilingual learners.\nIn Proceedings of the 1st Workshop on Multilingual\nRepresentation Learning, pages 1–15, Punta Cana,\nDominican Republic.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang,\net al. 2022. UnifiedSKG: Unifying and multi-tasking\nstructured knowledge grounding with text-to-text lan-\nguage models. arXiv preprint arXiv:2201.05966.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hernandez Abrego,\nSteve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope,\nand Ray Kurzweil. 2020. Multilingual universal sen-\ntence encoder for semantic retrieval. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\npages 87–94, Online.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline.\nPengcheng Yin, Chunting Zhou, Junxian He, and Gra-\nham Neubig. 2018. StructV AE: Tree-structured la-\ntent variable models for semi-supervised semantic\nparsing. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), Melbourne, Australia.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\nRichard Socher, and Caiming Xiong. 2021a. GraPPa:\nGrammar-augmented pre-training for table semantic\nparsing. In International Conference on Learning\nRepresentations.\nTao Yu, Rui Zhang, Alex Polozov, Christopher Meek,\nand Ahmed Hassan Awadallah. 2021b. SCoRe: Pre-\ntraining for context representation in conversational\nsemantic parsing. In International Conference on\nLearning Representations.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\n5258\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium.\nFuwei Zhang, Zhao Zhang, Xiang Ao, Dehong Gao,\nFuzhen Zhuang, Yi Wei, and Qing He. 2022a. Mind\nthe gap: Cross-lingual information retrieval with hi-\nerarchical knowledge enhancement. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n36(4):4345–4353.\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.\n2021. Mr. TyDi: A multi-lingual benchmark for\ndense retrieval. In Proceedings of the 1st Workshop\non Multilingual Representation Learning, pages 127–\n137, Punta Cana, Dominican Republic.\nXinyu Zhang, Kelechi Ogueji, Xueguang Ma, and\nJimmy Lin. 2022b. Towards best practices for\ntraining multilingual dense retrieval models. arXiv\npreprint arXiv:2204.02363.\nRuiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic\nevaluation for text-to-SQL with distilled test suites.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 396–411, Online.\n5259",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9210760593414307
    },
    {
      "name": "Natural language processing",
      "score": 0.759564995765686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6927065253257751
    },
    {
      "name": "Parsing",
      "score": 0.6182752847671509
    },
    {
      "name": "SQL",
      "score": 0.5587980151176453
    },
    {
      "name": "Information retrieval",
      "score": 0.3469262719154358
    },
    {
      "name": "Programming language",
      "score": 0.2666022479534149
    }
  ]
}