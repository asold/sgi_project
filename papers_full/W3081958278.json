{
  "title": "End-to-End Speech Endpoint Detection Utilizing Acoustic and Language Modeling Knowledge for Online Low-Latency Speech Recognition",
  "url": "https://openalex.org/W3081958278",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5101030491",
      "name": "Inyoung Hwang",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A5002418613",
      "name": "Joon‐Hyuk Chang",
      "affiliations": [
        "Hanyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891367150",
    "https://openalex.org/W2748934350",
    "https://openalex.org/W6602761387",
    "https://openalex.org/W2892300106",
    "https://openalex.org/W2742061524",
    "https://openalex.org/W6638205174",
    "https://openalex.org/W6712807204",
    "https://openalex.org/W2018731989",
    "https://openalex.org/W1998677696",
    "https://openalex.org/W1903951673",
    "https://openalex.org/W2117678320",
    "https://openalex.org/W2242685705",
    "https://openalex.org/W2513345070",
    "https://openalex.org/W1600744878",
    "https://openalex.org/W2475988411",
    "https://openalex.org/W2100844198",
    "https://openalex.org/W2403004229",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2748406667",
    "https://openalex.org/W1999454387",
    "https://openalex.org/W2048497537",
    "https://openalex.org/W2625979394",
    "https://openalex.org/W2408468399",
    "https://openalex.org/W2147768505",
    "https://openalex.org/W2080213370",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2114016253",
    "https://openalex.org/W2116064496",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W2148613904",
    "https://openalex.org/W6712580084",
    "https://openalex.org/W2402871917",
    "https://openalex.org/W2046151017",
    "https://openalex.org/W2935756939",
    "https://openalex.org/W1985242443",
    "https://openalex.org/W2109000787",
    "https://openalex.org/W2017854128",
    "https://openalex.org/W6712635302",
    "https://openalex.org/W2194940824",
    "https://openalex.org/W2197404611",
    "https://openalex.org/W2147794814",
    "https://openalex.org/W1536583098",
    "https://openalex.org/W36345725",
    "https://openalex.org/W1837709900",
    "https://openalex.org/W59508977",
    "https://openalex.org/W1538605008",
    "https://openalex.org/W1932883564",
    "https://openalex.org/W3007482166",
    "https://openalex.org/W2023582935",
    "https://openalex.org/W2129120544",
    "https://openalex.org/W2972584841",
    "https://openalex.org/W2289394825",
    "https://openalex.org/W1482149378",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2962760690",
    "https://openalex.org/W2620947159",
    "https://openalex.org/W6627649047",
    "https://openalex.org/W3007528493",
    "https://openalex.org/W2397132693",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1147368348",
    "https://openalex.org/W2729906263",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2962965465",
    "https://openalex.org/W2402316076",
    "https://openalex.org/W24933430",
    "https://openalex.org/W1508665521",
    "https://openalex.org/W68208136",
    "https://openalex.org/W2395271510"
  ],
  "abstract": "Speech endpoint detection (EPD) benefits from the decoder state features (DSFs) of online automatic speech recognition (ASR) system. However, the DSFs are obtained via the ASR decoding process, which can become prohibitively expensive especially in limited-resource scenarios such as the embedded devices. To address this problem, this paper proposes a language model (LM)-based end-of-utterance (EOU) predictor, which is trained to determine the framewise probabilities of the EOU token conditioned on the previous word history obtained from the 1-best decoding hypothesis of the ASR system in an end-to-end manner without an actual decoding process in the test step. Further, a novel end-to-end EPD strategy is presented to incorporate a phonetic embedding (PE)-based acoustic modeling knowledge and the proposed EOU predictor-based language modeling knowledge into an acoustic feature embedding (AFE)-based EPD approach within the recurrent neural networks (RNN)-based EPD framework. The proposed EPD algorithm is built upon the ensemble RNNs, which are independently trained for the three parts, which are the proposed LM-based EOU predictor, AFE-based EPD, and PE-based acoustic model (AM) in accordance with each target. The ensemble RNNs are concatenated at the level of the last hidden layers and then attached into the fully-connected deep neural networks (DNN)-based EPD classifier, which is trained in accordance with the ultimate EPD target. Thereafter, they are jointly retrained at the second step of the DNN training to yield the lower endpoint error. The proposed EPD framework was evaluated in terms of the endpoint accuracy and word error rate for the CHiME-3 and large-scale ASR tasks. The experimental results turn out that the proposed EPD algorithm efficiently outperforms the conventional EPD approaches.",
  "full_text": "Received July 7, 2020, accepted August 25, 2020, date of publication August 31, 2020, date of current version September 15, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3020696\nEnd-to-End Speech Endpoint Detection Utilizing\nAcoustic and Language Modeling Knowledge for\nOnline Low-Latency Speech Recognition\nINYOUNG HWANG AND JOON-HYUK CHANG\n, (Senior Member, IEEE)\nDepartment of Electronics and Computer Engineering, Hanyang University, Seoul 04763, South Korea\nCorresponding author: Joon-Hyuk Chang (jchang@hanyang.ac.kr)\nThis work was supported by the Institute of Information and Communications Technology Planning and Evaluation (IITP) funded by the\nKorea Government (MSIT), through Deep Learning Multi-Speaker Prosody and Emotion Cloning Technology Based on a High Quality\nEnd-to-End Model Using Small Amount of Data, under Grant 2020-0-00059.\nABSTRACT Speech endpoint detection (EPD) beneﬁts from the decoder state features (DSFs) of online\nautomatic speech recognition (ASR) system. However, the DSFs are obtained via the ASR decoding process,\nwhich can become prohibitively expensive especially in limited-resource scenarios such as the embedded\ndevices. To address this problem, this paper proposes a language model (LM)-based end-of-utterance (EOU)\npredictor, which is trained to determine the framewise probabilities of the EOU token conditioned on the\nprevious word history obtained from the 1-best decoding hypothesis of the ASR system in an end-to-end\nmanner without an actual decoding process in the test step. Further, a novel end-to-end EPD strategy is\npresented to incorporate a phonetic embedding (PE)-based acoustic modeling knowledge and the proposed\nEOU predictor-based language modeling knowledge into an acoustic feature embedding (AFE)-based EPD\napproach within the recurrent neural networks (RNN)-based EPD framework. The proposed EPD algorithm\nis built upon the ensemble RNNs, which are independently trained for the three parts, which are the proposed\nLM-based EOU predictor, AFE-based EPD, and PE-based acoustic model (AM) in accordance with each\ntarget. The ensemble RNNs are concatenated at the level of the last hidden layers and then attached into the\nfully-connected deep neural networks (DNN)-based EPD classiﬁer, which is trained in accordance with the\nultimate EPD target. Thereafter, they are jointly retrained at the second step of the DNN training to yield\nthe lower endpoint error. The proposed EPD framework was evaluated in terms of the endpoint accuracy\nand word error rate for the CHiME-3 and large-scale ASR tasks. The experimental results turn out that the\nproposed EPD algorithm efﬁciently outperforms the conventional EPD approaches.\nINDEX TERMS Acoustic model (AM), end-of-turn detection, end-of-utterance (EOU) detection, feature\nembedding, language model (LM), online speech recognition, pause hesitation, speech endpoint detection\n(EPD), spoken dialogue system.\nI. INTRODUCTION\nSpoken dialogue systems make it possible to control con-\ntemporary devices, such as smartphones, navigation systems,\nand AI speakers through natural voice interaction. Usually,\nthe interaction with such devices is user-initiated by uttering\nthe wake-up-word. Then, an automatic speech recognition\n(ASR) technique is performed in an online manner until\nan end-of-utterance (EOU) is automatically detected by a\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Victor Sanchez\n .\nspeech endpoint detection (EPD) algorithm. The EPD is a\nchallenging task since the utterance can be endpointed late\ndue to the ambient noise and early due to long pause hes-\nitation. Since an early endpoint undesirably cuts off the\nspeech region, the performance of speech recognition is\noften degraded seriously; on the other hand, a late endpoint\nincreases the response latency of the online ASR system.\nConsequently, degraded endpoint performance often causes\nthe user dissatisfaction [1], [2].\nThe traditional EPD approaches consist of two cascaded\ndecision processes. First, input speech is classiﬁed into\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 161109\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nspeech and non-speech on a frame basis using a speech\nactivity detection (SAD) algorithm designed with engineered\nfeatures [3]–[7]. Then, the EOU is ﬁnally detected when the\nduration of non-speech obtained using the SAD algorithm\nreaches the pre-deﬁned threshold value, i.e., 500 ms or 1 s [8].\nChung et al.proposed an EPD algorithm that classiﬁes speech\nand non-speech states using the SAD technique based on a\nlog-likelihood ratio (LLR) test proposed in [9], and then ﬁnds\nthe endpoint with the online decoder designed based on a\nweighted ﬁnite-state transducer (wFST) [10]. Since it is dif-\nﬁcult to optimize the LLR test-based SAD and wFST jointly,\nthis EPD scheme was further improved by adopting the quan-\ntized LLR states as the wFST input instead of the binary\nspeech/non-speech state [11]. The performance of these EPD\nstructures is dramatically enhanced with the help of the SAD\nalgorithms based on deep neural networks (DNN), which\nyield the state-of-the-art SAD performance via deep nonlin-\near hidden layers [12]–[17]. Especially, it was observed that\nthe bottleneck features of the DNN-based acoustic model\n(AM), called phonetic embedding (PE), which is trained to\npredict senones (tied triphone states) [18], lead to improved\nSAD and EPD performances [19]–[21].\nAnother way is to directly ﬁnd the EOU from the sequen-\ntial input features by employing a long short-term memory\n(LSTM) [22], whereas the traditional EPD schemes consist of\nthe separate SAD and online decoder. The LSTM can model\nthe complex relations between the input feature sequence and\nthe corresponding framewise EPD targets with the memory\ncell as the temporal state of the network can be successfully\ncontrolled by the input, forget, and output gates [23]–[26].\nNotably, a uniﬁed architecture comprising the convolutional\nneural networks (CNN), LSTM, and fully-connected DNN,\ncalled CLDNN, was proposed to exploit their complementary\nadvantages for the ASR [27] and SAD tasks [28]. However,\nit was observed that the capability of the convolution layer\nis diminished when extracting features in adverse noisy con-\nditions [29]. To address this problem, an alternative method\ncalled grid-LSTM [30] was presented to model the time and\nfrequency variations of input sequential features properly\nwithin separate time and frequency LSTM cells, respectively.\nFurthermore, a grid-LSTM DNN (GLDNN) [31] was intro-\nduced by employing the grid-LSTM in the ﬁrst layer instead\nof the convolutional layer of the CLDNN to improve the EPD\nperformance. However, these feature mapping-based EPD\napproaches often prematurely abandon the speech region due\nto a pause hesitation or cause a higher detection latency since\nthey cannot adequately consider the context of input feature\nsequences such as phone or word alignments. In addition,\nthe performance of the EPD approach using the LSTM can\nbe degraded for a long utterance since the LSTM suffers\nfrom a state saturation problem due to its degradation of gate\ncontrols [32].\nIn addition, the EPD approaches designed to use the\ndecoder state features (DSFs) of the ASR module as\nthe auxiliary features have been introduced to distin-\nguish the EOU from a short or long pause under the\nnoisy environments. First, Ferrer et al.developed a prosodic\nfeature-based EPD method that yields the EOU decision\nwhen a pause of any length is detected, where the deci-\nsion statistic is determined by using the non-speech dura-\ntion, prosodic feature, and language model (LM) knowledge\n[33]–[35]. Besides, Stuker et al. proposed a simple EPD\napproach, which is similar to the aforementioned approaches,\nto segment continuously recorded speech by triggering the\nEOU when the pause duration reaches the maximum pause\nthreshold. Here, the pause duration can be obtained from\nphone alignment corresponding to the 1-best ASR decoding\nhypothesis [36]. However, this approach cannot be applied to\nthe online ASR systems since the 1-best decoding hypothe-\nsis frequently changes during the online decoding process,\nwhich makes the EPD system unstable. To overcome this\ndisadvantage, the expected pause duration is introduced\nas the stable feature for the online EPD task since it is\nobtained by interpolating the pause durations within all active\nhypotheses [37], [38]. Furthermore, it was observed that\nthe word embedding (WE), which is obtained from the\nword LSTM [39] trained with the 1-best ASR decoding\nhypothesis to detect the turn-taking word, can yield the\nsigniﬁcant performance improvement of acoustic feature\nembedding (AFE)-based EPD without an actual decod-\ning process, whereas the combination of the AFE, WE,\nand expected pause durations achieves the state-of-the-art\nEPD performance. Also, [40] incorporated the EOU symbol\ninto the output within uniﬁed recurrent neural networks\n(RNN) transducer-based ASR system. Although various\nframeworks for on-device speech recognition have been\nproposed [41]–[43], the speech recognition accuracy is still\nlimited due to the heavy computational cost since there is\nthe trade-off between the word error rate (WER) and com-\nputational cost. Indeed, it is difﬁcult to assess the advantages\nof the superior ASR system, which requires high complexity\nin limited-resource scenarios. Moreover, the WE cannot be\nconsidered as a reliable feature for the online EPD task since\nthe context-dependent EPD approaches including the WE-\nbased EPD suffer from the ambiguity of the turn-taking word\ndue to the ﬂexibility of a natural language [44].\nIn order to address the aforementioned disadvantages of\nconventional EPD approaches, this paper proposes an end-\nto-end EPD algorithm by incorporating both the acoustic\nand language modeling knowledge into the AFE-based EPD\nalgorithm. First, the LM-based EOU predictor, which is\ntrained to determine the framewise probabilities of the EOU\ntoken conditioned on the previous word history of the 1-best\nhypothesis obtained using the ASR decoding process, is pre-\nsented. Once the proposed EOU-predictor is trained, it can\nderive the framewise probabilities of the EOU token given\nthe input acoustic features without the actual ASR decoding\nprocess. Further, we introduce a novel EPD framework, con-\nsisting of the proposed LM-based EOU predictor, PE-based\nAM, and AFE-based EPD. When training the EOU predictor,\nthe 1-best ASR decoding hypothesis with the N-gram LM\nis used to obtain the probabilities of the EOU token, which\n161110 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\ncorrespond to the training target of RNN. And, the AFE-\nbased EPD algorithm is designed with RNN to classify the\nspeech frame into four labels, namely speech, initial silence,\nﬁnal silence, and intermediate silence, on a frame-by-frame\nbasis. Also, the AM for extracting the acoustic modeling\nknowledge with the use of the PE is trained with RNN by\nincorporating the sequential input features as the input along\nwith senone targets. Then, the last hidden layers of the three\nensemble RNNs are concatenated to train the fully-connected\nDNN-based classiﬁer according to the hand-made EPD label.\nFinally, all the designed EPD networks are jointly retrained,\nthereby leading to a lower endpoint error. The proposed\nEPD algorithm was evaluated in terms of the early endpoint\ntime, late endpoint time, and WER for the CHiME-3 ASR\ntask [45], which includes various simulated and real acoustic\nconditions and a large-scale ASR task. Overall, the proposed\nEPD algorithm without the decoding process was observed to\nachieve a lower endpoint error, which leads to a lower WER\nand lower latency.\nThe rest of this paper is organized as follows. In the next\nsection, we review the recently proposed EPD algorithms.\nIn Section III, we describe the design of the proposed EPD\napproach. An extensive evaluation of the proposed algorithm\nis discussed in Section IV , and the conclusions are presented\nin Section V .\nII. REVIEW OF PREVIOUS WORKS\nThis section brieﬂy describes the conventional\nEPD algorithms which will be compared with the proposed\nEPD algorithm later.\nA. EPD USING A GLDNN\nThe CLDNN-based architecture was previously introduced\nto exploit the complementary modeling advantages of the\nCNN, LSTM, and fully-connected DNN [27]. First, the CNN\ncan extract the time and frequency-invariant features from\nsequential input features such as the Mel-frequency cep-\nstral coefﬁcients (MFCC) and log-Mel ﬁlterbank energies.\nIn addition, the LSTM can model the short- and long-term\ntemporal contexts of input features and the fully-connected\nDNN can model the complex relation between the features,\nwhich is represented via the CNN and LSTM, and the EPD\ntarget through multiple nonlinear hidden layers. As discussed\nin [29], the convolution layer for feature extraction is dete-\nriorated in highly noisy conditions; hence, the alternative\narchitecture called GLDNN [31] was introduced to replace\nthe convolution layer with the grid-LSTM layer [30]. The\ngrid-LSTM models the variations of successive features in the\ntime and frequency axes through separate grid time LSTM\n(gT-LSTM) and grid frequency LSTM (gF-LSTM), respec-\ntively. Here, grid-LSTM is similar to the convolution layer in\nthat both models are used to represent the input features over a\nrestricted local time-frequency block and they use the shared\nmodel parameters. However, the grid-LSTM differs from\nthe convolution layer in that it models frequency variations\nthrough a recurrent state that is passed along the frequency\naxis, whereas the convolution layer independently extracts\nthe locally invariant features via the convolution and pooling\noperations.\nThe GLDNN-based EPD technique consists of the\nstacked grid-LSTM layers, standard LSTM layers, and fully-\nconnected DNN layers. Once the time and frequency invari-\nant features are extracted by the grid-LSTM layer, their short-\nand long-term temporal contexts are modeled by the stan-\ndard LSTM layers. The EOU predictor ﬁnally classiﬁes each\nframe into four distinct classes, namely speech, initial silence,\nﬁnal silence, and intermediate silence, to distinguish the ﬁnal\nsilence from the different silence states in the utterance.\nIn the test step, the posterior probability of the ﬁnal silence is\ncomputed and the EPD is triggered when it exceeds the given\nthreshold value.\nB. EPD BASED ON COMBINING AFE, WE, AND DSFs\nThe combined feature-based EPD algorithm [39] consists\nof three parts to detect the EOU exactly by fusing multiple\nfeatures. They are an acoustic LSTM trained on the acoustic\nfeatures, the word LSTM trained on the 1-best ASR decoding\nhypothesis, and the DSFs composed of three types of pause\ndurations, which are described as follows. First, the acoustic\nLSTM trains the AFE in accordance with the framewise\nendpoint target. The corresponding SAD target is also trained\nin a multi-task fashion to distinguish the ﬁnal silence from the\ninitial silence and intermediate silence. Unlike the acoustic\nLSTM, the word LSTM is trained from the acoustic feature\nsequence to detect the turn-taking word. Hence, the word\nLSTM is triggered when alignments corresponding to the\nturn-taking word are observed instead of the ﬁnal silence\nregion, where the alignment is obtained from the 1-best\nASR decoding hypothesis. To consider the decoder state,\nthree types of expected pause durations extracted from the\nactive ASR decoding hypotheses are utilized as the DSFs.\nSpeciﬁcally, the DSFs consist of the best path pause duration,\nexpected pause duration, and end pause duration, which are\nexplained as follows. For this, Letting Xt ={x1,x2,..., xt }\nand si\nt = {si\n1,si\n2,..., si\nt }be the input feature sequence\nuntil the t-th frame and the state sequence of the i-th active\nhypothesis until the t-th frame, respectively, the posterior\nprobability of the i-th hypothesis is denoted by P(si\nt |Xt ). First,\nthe best path pause duration is determined by Limax\nt with\nimax =argmaxi P(si\nt |Xt ) where Li\nt denotes the pause dura-\ntion according to the i-th hypothesis. Second, the expected\npause duration D(Lt ) is obtained by interpolating the active\nhypotheses as follows:\nD(Lt ) =\nNt∑\ni=1\nLi\nt P(si\nt |Xt ) (1)\nwhere Nt denotes the number of active hypotheses at the\nt-th frame. The expected ﬁnal pause duration Dend(Li\nt ) can\nVOLUME 8, 2020 161111\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nFIGURE 1. Overall block diagram of the proposed EPD algorithm. (The black solid and dotted lines indicate the\nfeed-forward paths for training and test stages while dotted lines are used only in the training stage, not in the\ntest stage. The red dotted lines indicate the error back-propagation paths for separate training of each RNN in\naccordance with each target and DNN for the classifier. The blue dotted lines indicate the error back-propagation\npaths for the final joint-retraining process. The gray blocks are used in the training stage only not in the test\nstage.)\nbe determined as follows:\nDend(Lt ) =\nNt∑\ni=1,sit ∈Send\nLi\nt P(si\nt |Xt ) (2)\nwhere Send denotes the end state of the LM. At each frame,\nthe feature vectors for the EPD are combined with the last hid-\nden layer of both the acoustic LSTM and word LSTM along\nwith the DSFs. The fully-connected DNN-based classiﬁer is\nﬁnally trained with the combined feature vector in accordance\nwith the framewise endpoint target.\nIn the inference step, the EPD is triggered when the pos-\nterior likelihood of the endpoint exceeds a given threshold.\nTo safeguard the lower and upper latency (pause duration)\nbounds, the SAD decision of the acoustic LSTM is addition-\nally used as follows. If the pause duration obtained by the\ntrained SAD does not reach the minimum pause duration,\nTmin, the endpoint is not triggered. Furthermore, the endpoint\nis enforced to be triggered if the pause duration obtained by\nthe SAD is longer than the maximum pause duration, Tmax.\nIII. PROPOSED END-TO-END EPD ALGORITHM BASED\nON ENSEMBLE RNNs\nAs shown in Fig. 1, the novel EPD algorithm is proposed to\nexploit the ensemble of the AFE-based EPD, PE-based AM,\nand decoder embedding (DE) derived from the LM-based\nEOU predictor that is the main contribution of this study.\nThe LM-based EOU predictor directly yields the framewise\nprobability of the EOU token conditioned on the previous\nword history of the 1-best ASR decoding hypothesis with\nthe N-gram LM. Accordingly, the LM-based EOU predictor,\nAFE-based EPD, and PE-based AM are separately trained,\nand then the fully-connected DNN-based EOU predictor is\ntrained with the combined feature vector, which is com-\nposed of the last hidden layers of the three ensemble RNNs,\nin accordance with the framewise hand-labeled EPD targets\nas described in the following subsections.\nA. PROPOSED LM-BASED EOU PREDICTOR\nAs shown in [39], the combination of the AFE and WE can\nyield superior EPD performance without the actual decod-\ning process, closely matching the performance of the EPD\nsystem based on the AFE, WE, and DSFs, which can be\nobtained by performing the online ASR decoding process.\nHowever, in natural language processing (NLP), it is difﬁcult\nto detect the turn-taking word in an online fashion due to\nthe ﬂexibility of the natural language. The natural language\ncan express the user’s intentions variously according to gram-\nmatical rules [44]. For instance, the user’s intention tends\nto be expressed with the action and object information only\nsuch as ‘‘turn the lights on’’. Also, the user’s intention is\noften expressed by including the speciﬁc location information\nby attaching an additional phrase such as ‘‘in the kitchen’’.\nIn other words, from the expression ‘‘turn the lights on’’,\nit cannot be clearly identiﬁed whether ‘‘on’’ is the turn-taking\nword or not, whereas ‘‘on’’ is not the turn-taking word if the\nphrase ‘‘in the kitchen’’ follows the above expression. Thus,\nthe WE, which is extracted from the word LSTM, cannot\nbe considered as a reliable feature for the online EPD task\nsince it is trained to detect the turn-taking word as depicted\nin Fig. 2(a). This ﬁgure shows the example pairs of the\nsentence and label from [46], which are used to train the word\nLSTM. It can be observed that different labels are given for\nthe same word sequence, depending on whether the additional\nphrase follows or not.\n161112 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nFIGURE 2. Example pairs of the sentence and target used to train the\nembedding depending on the 1-best ASR decoding hypothesis for the EPD\ntask: (a) word LSTM [39] and (b) the proposed LM-based EOU predictor.\nIn order to address these problems, this paper proposes the\nLM-based EOU predictor, which is similar to the word LSTM\nin that they are trained depending on the 1-best ASR decod-\ning hypothesis. However, it differs from the word LSTM\nin that the proposed EOU predictor is trained to determine\nthe framewise probabilities of the EOU token conditioned\non the previous word history instead of binary classiﬁcation\nfor ﬁnding the turn-taking word. As depicted in Fig. 2(b),\nthe same probabilities of the EOU token for training the\nEOU predictor are given without a reference to whether each\nword is the turn-taking word or not, where each probability\nof the EOU token is obtained from the 4-gram LM. After\nthe word ‘‘on’’ is shown, the probability of the EOU token\nis 0.372 since the probability that the additional phrase is\nattached after the observed sentence to contain the speciﬁc\ninformation additionally is 0.628, which is obtained from\nthe 4-gram LM. And, the probability of the EOU token is\ndecreased to almost zero after the word in the middle of the\nsentence ‘‘in’’ is observed since P(EOU|lights, on, in) ≈0.\nOn the other hand, the probability of the EOU token rapidly\nincreases after the last word in the sentence ‘‘kitchen’’ is\ndetected. The method to obtain the framewise probability of\nthe EOU token conditioned on the previous word sequence is\ndescribed as follows.\nThe ASR technique aims to determine the most likely word\nsequence ˆw, given the input acoustic feature sequence X,\nwhere ˆw is expressed as follows:\nˆw =argmax\nw\nP(w|X). (3)\nInstead, the Bayes’ rule represents it into the equivalent form\nas follows:\nˆw =argmax\nw\nP(X|w)P(w) (4)\nwhere the likelihood P(X|w) is determined by the AM usu-\nally based on the DNN and the prior probability P(w) is\nobtained by the LM. Here, the LM is utilized to derive the\nprobability of each word conditioned on the previous word\nhistory as P(wi|w<i). For large vocabulary continuous speech\nrecognition (LVCSR), it is approximated by the N-gram LM\naccording to the Markov chain rule, where the N-gram LM\ndetermines the probability of each word conditioned on the\nlast N −1 words only, instead of the entire word history.\nHowever, the major drawback of the N-gram LM originates\nfrom data sparsity when trained with insufﬁcient corpora.\nIt can be mitigated by the combination of discounting and\nbacking-off algorithms, called the Katz smoothing algorithm\n[47]. The 3-gram LM is suggested to obtain the probability of\nthe EOU token conditioned on the word history as follows:\nP(EOU|w<i)\n≈\n\n\n\nd C(wi−2,wi−1,EOU)\nC(wi−2,wi−1) if 0 <C ≤C′\nC(wi−2,wi−1,EOU)\nC(wi−2,wi−1) if C >C′\nα(wi−2,wi−1)P(EOU|wi−1) otherwise\n(5)\nwhere C′ is a count threshold value, C is short-hand\nfor C(wi−2,wi−1,EOU), d is a discount coefﬁcient, and\nα(wi−2,wi−1) is the normalisation constant. From (5),\nP(EOU|wi−1) also can be alternatively obtained via the\nbacking-off method if C(wi,EOU) =0 or the discounting\nmethod if 0 <C(wi,EOU) ≤C′. The 1-best ASR decoding\nhypothesis at t, called ˆwt , can be obtained as follows:\nˆwt =argmax\nw\nP(Xt |w)P(w) (6)\nwhere Xt = {x1,x2,..., xt }. The probability of the EOU\ntoken at t can be derived according to the last two words of the\n1-best ASR decoding hypothesis by employing the 3-gram\nLM as follows:\nP(EOU|Xt ) =\n∑\nw\nP(EOU|w,Xt )P(w|Xt ) (7)\n≊ P(EOU|ˆwt ,Xt ) (8)\n≊ P(EOU|ˆwt ) (9)\n≊ P(EOU|ˆwt,U−1,ˆwt,U ) (10)\nwhere ˆwt,u and U denote the u-th word of ˆwt and the number\nof words of ˆwt , respectively. Speciﬁcally, the probability of\nthe EOU token given Xt can be obtained by marginalizing\noverall all the possible hypotheses at t. It can be represented\nby (8) with the assumption that the probability of the 1-best\nhypothesis dominates the probability mass of all the possible\nhypotheses such that P( ˆwt |Xt ) =1. Furthermore, (8) can be\nrewritten as in (9) since it can be assumed that the probability\nof the EOU token is conditionally independent to Xt . Finally,\nthe probability of the EOU token given Xt can be determined\naccording to the last two words of the 1-best hypothesis at t\nvia the 3-gram LM approximation as in (10).\nIn this study, the LM-based EOU predictor is ﬁrst pre-\nsented to determine directly the probability of the EOU token\nP(EOU|Xt ) in an end-to-end manner. As depicted in the upper\npart of Fig. 3, the framewise probabilities of the EOU token in\nthe training stage are obtained from the 1-best ASR decoding\nhypothesis of each training-dataset ˆwt with the help of the\ndecoding module. The probability of the EOU token condi-\ntioned on the previous word history is obtained by the N-gram\nLM, used as the target for the training. Then, the proposed\nVOLUME 8, 2020 161113\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nFIGURE 3. Overall pipeline of our proposed LM-based end-to-end EOU\npredictor.\nLM-based EOU predictor using RNN is trained along with\nthe targeted probability of the EOU token. The key idea is to\ntrain the LSTM network to minimize the mean square error\n(MSE) function for the LM-based EOU predictor, which is\nexpressed as follows:\nhDE\n1,t =RNN(Xt ,2DE\n1 ) (11)\nhDE\n2,t =RNN(hDE\n1,t ,2DE\n2 ) (12)\nˆPDE(EOU|Xt ) =σ(hDE\n2,t VDE +bDE) (13)\nwhere 2DE\nl is the model parameter of l-th RNN layer and\nhDE\nl,t denotes the hidden state of the l-th hidden layer at the\nt-th frame for the DE, respectively. Also, VDE, bDE, and\nσ denote the weight parameter, bias parameter, and logistic\nsigmoid function, respectively. Once the proposed end-to-end\nEOU predictor is completely trained, the framewise posterior\nprobabilities of the EOU token are determined at the infer-\nence stage as in (11)–(13) without the actual ASR decoding\nprocess while eliminating the gray block as depicted in Fig. 1.\nFurthermore, they will be used as the LM knowledge for the\nﬁnal EPD decision.\nB. AFE-BASED EPD\nAccording to [31], the AFE-based EPD method can be used\nto classify each frame into four states, i.e., speech, initial\nsilence, ﬁnal silence, and intermediate silence to distinguish\nthe ﬁnal silence from the other silence states, where the high\nposterior probability of the ﬁnal silence is likely to be the true\nendpoint. The AFE-based EPD is formulated as follows:\nhAFE\n1,t =RNN(Xt ,2AFE\n1 ) (14)\nhAFE\n2,t =RNN(hAFE\n1,t ,2AFE\n2 ) (15)\nˆPAFE(EOU|Xt ) =softmax(hAFE\n2,t VAFE +bAFE) (16)\nwhere 2AFE\nl is the model parameter of l-th RNN layer and\nhAFE\nl,t denotes the hidden state of the l-th hidden layer at the\nt-th frame for the AFE, respectively. Also, VAFE and bAFE\ndenote the weight and bias parameters of the output layer,\nrespectively. All the parameters in the LSTM for the AFE-\nbased EPD are trained to minimize the cross-entropy (CE)\nerror function.\nC. PE-BASED AM\nAccording to the previous studies [19], [20], [48], and [49] on\nthe phone-aware training method using the latent feature of\nthe DNN-based AM (called PE), the main idea can be further\nimproved for other applications such as the speech enhance-\nment and SAD tasks. Hence, in this study, we incorporate the\nPE for the EPD task to reduce the endpoint error. The PE-\nbased ASR is derived as follows:\nhPE\n1,t =RNN(Xt ,2PE\n1 ) (17)\nhPE\n2,t =RNN(hPE\n1,t ,2PE\n2 ) (18)\nˆPPE(yt =ct |Xt ) =softmax(hPE\n2,t VPE +bPE) (19)\nwhere 2PE\nl is the model parameter of l-th RNN layer and\nhPE\nl,t denotes the hidden state of the l-th hidden layer at the t-\nth frame for the PE, respectively. Furthermore, VPE and bPE\nrepresent the weight and bias parameters of the output layer,\nrespectively. It is expected that the LSTM better models the\nPE-based AM to minimize the CE error function with the\nframewise senone label ct , which can be obtained by per-\nforming the forced alignment process based on the Gaussian\nmixture model-hidden Markov model (GMM-HMM)-based\nASR system [50].\nD. PROPOSED END-TO-END ENDPOINT DETECTION\nBASED ON ENSEMBLE RNNs\nWe propose the novel EPD framework that reduces the early\nand late endpoint times simultaneously by leveraging the\nAFE-based EPD algorithm with the acoustic and language\nmodeling knowledge. As in [19]–[21], which show that the\ncomplementary advantages of multiple features can be easily\ncombined using the DNN by injecting the features together,\nthe last hidden layers of the PE-based AM and the proposed\nLM-based EOU predictor are concatenated with that of the\nAFE-based EPD algorithm as the acoustic modeling context\nand language modeling context, respectively. After the AFE-\nbased EPD, PE-based AM, and LM-based EOU predictor\nare independently trained in accordance with each target,\nthe ensemble RNNs are concatenated at the level of the\nlast hidden layers and then fed into the DNN-based EPD\nclassiﬁer, which is used to classify each input frame into four\nstates indicating the speech, initial silence, ﬁnal silence, and\nintermediate silence, as follows:\nhEPD\n1,t =σ([hAFE\n2,t ,hPE\n2,t ,hDE\n2,t ]VEPD\n1 +bEPD\n1 ) (20)\nhEPD\n2,t =σ(hEPD\n1,t VEPD\n2 +bEPD\n2 ) (21)\nˆPEPD(EOU|Xt ) =softmax(hEPD\n2,t VEPD\n3 +bEPD\n3 ) (22)\nwhere hEPD\nl,t denotes the hidden state of the l-th layer at the t-\nth frame. In addition, VEPD\nl and bEPD\nl denote the weight and\nbias parameters at the l-th hidden layer, respectively. To build\n161114 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nthe model, the CE error function is directly applicable to the\nobjective criterion, thus, the posterior probability of the ﬁnal\nsilence representing the speech endpoint is established. After\nthe classiﬁer based on the DNN is trained, all the modules\nincluding the ensemble RNNs for extracting the AFE, PE, and\nDE and the DNN for the classiﬁer are dependently optimized\nagain by the joint retraining (JRT) process, which is similar\nto phase 3 of [19], in accordance with the EPD label to fur-\nther enhance the EPD performance, whereas they consist of\ndifferentiable parameters as shown in Fig. 1, which illustrates\nthe feed-forward and error back-propagation paths.\nIn the inference stage, the probability of the EOU is com-\nputed by feeding the input acoustic feature sequence into the\nproposed EPD algorithm. The EOU is ﬁnally detected when\nˆPEPD(EOU|Xt ) exceeds the probabilities corresponding to the\nspeech, initial silence, and intermediate silence.\nIV. EXPERIMENTS AND RESULTS\nThis section describes the performance evaluation of the\nproposed EPD approach. For the objective comparison, our\napproach was compared with the conventional GLDNN-\nbased EPD [31] and the EPD based on combining the AFE,\nWE, and DSFs [39]. Since the DSFs-based approach in [39]\nand the proposed EPD approach are commonly based on the\ncombination of the trained embeddings, such as [AFE, WE,\nDSFs] and [AFE, PE, DE], respectively, the performances of\nthe sub-EPD systems based on single embedding alone and\ntheir combinations were tested also to verify the superiority\nof the DE for the proposed EPD algorithm. In [31] and [39],\nthe performances of the EPD systems were evaluated using\nthe following metrics. First, the EPD performances were\nassessed using the late endpoint time describing how the ﬁnal\nEPD decision is triggered late compared with the EPD label.\nThe late endpoint time can be considered as the response\nlatency of the online speech recognition system since 1-best\nASR decoding hypothesis can be obtained when the EPD\nis triggered. Besides, the EPD performances were compared\nusing the WER since bad early endpoint errors undesirably\ncut off the speech region and increase the deletion error rate.\nIn order to evaluate the performance of the EPD systems\nin terms of early endpoint error itself also, we reported the\nWER as well as the early endpoint time, which describes how\nthe ﬁnal EPD decision is prematurely triggered compared\nwith the true EPD label. For the performance comparison,\nthe EOU of each speech sample on the evaluation-dataset was\nobtained by independently performing the EPD systems, and\nthen the EPD performances were assessed in the following.\nThe early endpoint time was obtained by averaging the gap\nbetween the actual EOU and the moment the EPD algorithm\nwas triggered within the speech samples for which the EPD\napproach was prematurely triggered. The late endpoint time\nwas obtained by averaging the gap between the actual EOU\nand the moment the EPD algorithm was triggered within the\nspeech samples for which the EPD decision was triggered\nlate. Then, the WER was evaluated by performing the ASR\ndecoding process from the ﬁrst frame to the EOU frame\ndetermined by each EPD algorithm, while the WER was\ncomputed by the summation of the substitution, deletion, and\ninsertion error rates [51].\nThe ﬁrst part of the experiments used a relatively small\nspeech dataset, namely CHiME-3, to evaluate and analyze\nthe conventional and proposed EPD algorithms with various\nacoustic conﬁgurations. The second part of the experiments\nscaled up the size of utterances to be augmented by using\nthe acoustic environment simulation method with the clean\nspeech database, namely SiTEC Dict01 [52]. These exper-\niments mainly show the effectiveness of the proposed EPD\nframework. Note that all the frameworks were implemented\nusing the TensorFlow library [53].\nA. CHiME-3 ASR TASK\n1) DATA PREPARATION\nWe emphasize that the simulation must be very realistic;\nhence, we selected the CHiME-3 dataset [45] developed\nfor the far-ﬁeld ASR task with a multi-microphone tablet\ndevice in everyday environments, i.e., a bus, cafe, pedestrian\narea, and street, each of which consists of real speech data\n(REAL) and simulated speech data (SIMU). The real speech\ndata consist of six-channel recordings and were sampled\nat 16 kHz. Twelve English speakers were asked to read\nthe sentences from the WSJ0 corpora [54] while using the\nmulti-microphone tablet. They were encouraged to adjust\ntheir reading positions so that the target distance continued\nto change over time. The simulated speech data were gen-\nerated by artiﬁcially mixing the clean utterances from the\nWSJ0 into background recordings. The speech data con-\nsists of three datasets, including the training-, development-,\nand evaluation-datasets, which have 18 h of speech data (3 h\nREAL and 15 h SIMU) uttered by 87 speakers, 2.9 h of speech\ndata uttered by 4 speakers, and 2.2 h of speech data uttered by\n4 speakers, respectively. The development- and evaluation-\ndatasets have a 1:1 ratio of REAL and SIMU. We used\nthe training- and development-datasets for the training of\neach EPD framework and the evaluation-dataset for the per-\nformance comparison. In particular, ‘‘Beamformit’’, which\nis a weighted delay-and-sum beamforming algorithm [55]\nwas performed to extract the speech signal of interest from\nbackground noise. Note that the beamforming algorithm was\ncarried out using only ﬁve microphones facing the speaker,\nand we excluded the second microphone since it was located\non the rear side of the tablet device and contained less speech.\nTo prepare the senone targets and P(EOU|Xt ) labels used to\ntrain the AM and the LM-based-EOU predictor, respectively,\nwe used the baseline ASR system of the CHiME-3 task\nprovided by the KALDI framework [56]. The baseline ASR\nsystem was prepared using the training- and development-\ndatasets described as follows. First, the training- and\ndevelopment-datasets were represented with 25 ms frames\nof 13-dim MFCC features computed every 10 ms with\nthe Hamming window. We obtained 1,952 types of senone\nlabels by training the GMM-HMM-based ASR system with\nVOLUME 8, 2020 161115\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\na 40-dim feature space maximum likelihood linear regres-\nsion (fMLLR) context by speaker adaptive training (SAT),\nwhereas the input feature was spliced with three left and three\nright feature frames (91-dimension). Subsequently, the DNN\nfor the AM, which has 7 hidden layers and 2,048 hidden units\nwith the sigmoid activation function on each hidden layer,\nwas trained as in the following steps. First, each hidden layer\nof the DNN was initialized via the layerwise unsupervised\nlearning process called pre-training by the contrastive diver-\ngence (CD) algorithm [57]. Then, the DNN was trained to\nminimize the CE error function, whereas the DNN input was\nalso spliced with ﬁve left and ﬁve right fMLLR contexts\n(440-dimension). Finally, the DNN was trained again with\nthe state-level minimum Bayes risk (sMBR) criteria [58].\nThe 3-gram LM was used for the baseline ASR system,\nwhich was developed within the 5k vocabulary and pruned\nby the pre-deﬁned threshold values. After the DNN-based\nAM was trained, the senone labels of each dataset were\nprepared by performing the ASR decoding process. Fur-\nthermore, the framewise P(EOU|Xt ) labels of each dataset\nwere established according to the word alignment, which\nwas obtained from the 1-best ASR decoding hypothesis.\nIn addition, we made framewise reference EPD decisions\non the enhanced speech data of each dataset by manually\nlabeling each frame as speech, initial silence, ﬁnal silence,\nand intermediate silence for every 10 ms.\n2) TRAINING PROCESS FOR EACH EPD MODEL\nThe proposed EPD framework was constructed as follows.\nFirst, the training- and development-datasets were repre-\nsented with 25 ms frames of 64-dim log-Mel ﬁlterbank\nenergies computed every 10 ms, which were used as the\ninput feature for the EPD task. The AFE-based EPD and\nPE-based AM consisted of two LSTM layers with 100-dim\ncells per layer and the fully-connected DNN-based clas-\nsiﬁer with the soft-max function, yielding the 4-dim and\n1,952-dim output, respectively, for classifying the input frame\ninto four types of states, which are speech, initial silence, ﬁnal\nsilence, and intermediate silence frames and senone labels,\nrespectively. The EOU predictor-based EPD also consisted\nof two LSTM layers with 100-dim cells per layer and the\nfully-connected DNN yielding the 1-dim output layer through\nthe sigmoid logistic function. The EOU predictor-based EPD\nwas trained with the MSE function where the probabilities\nof the EOU token P(EOU|Xt ) obtained using the N-gram\nLM were used to train the EOU predictor. After they were\ntrained, their last hidden layers were concatenated to train\nthe EPD classiﬁer consisting of two 100-dim fully-connected\nDNN layers and the 4-dim soft-max layer. The batch size\nwas set to 64. The learning rates for the training of the AFE-\nbased EPD, PE-based AM, LM-based EOU predictor, and\nclassiﬁer were set to 0.01, 0.01, 0.001, and 0.01, respectively,\nfor the ﬁrst 10 epochs, and then decreased by 10% after\neach epoch. When the proposed EPD architecture was jointly\nretrained for further optimization, the initial learning rate was\nset to 0.0001, and then decreased by 10% upon each epoch.\nFor the EPD performance comparisons, the conven-\ntional EPD approaches were established as follows. For the\nGLDNN-based EPD [31], the grid-LSTM used 12-dim grid-\nLSTM units, where the ﬁlter size was 8 with the stride 2\n(overlapped by 6). Furthermore, two LSTM layers with\n64-dim cells per layer and two 100-dim fully-connected DNN\nlayers with the 4-dim soft-max layer were cascaded. The\nGLDNN-based EPD was trained with the 64-dim log-Mel\nﬁlterbank energies in accordance with four types of labels:\nspeech, initial silence, ﬁnal silence, and intermediate silence.\nThe batch size and learning rate were set to 64 and 0.01,\nrespectively. As for [39], 64-dim log-Mel ﬁlterbank ener-\ngies were also used as the feature. The acoustic LSTM and\nword LSTM were constructed with two LSTM layers with\n100-dim cells per layer and the fully-connected DNN-based\nclassiﬁer. The acoustic LSTM was trained in accordance\nwith the four types of EPD labels, which are speech, initial\nsilence, ﬁnal silence, and intermediate silence unlike [39]\nsince the post-processing for safeguarding the lower and\nupper pause duration bounds was not used for a reasonable\ncomparison. The word LSTM was also trained with the binary\nlabels, which were given depending on whether the turn-\ntaking word or not and were obtained by performing the ASR\nbaseline for the CHiME-3 task. After the acoustic LSTM\nand word LSTM were trained, the classiﬁer consisting of the\ntwo 100-dim fully-connected DNN layers with 4-dim soft-\nmax function was trained from the sequential features, which\nwere composed of the last hidden layer of both LSTMs and\nthe DSFs (three types of expected pause durations), which\nwere obtained by performing the ASR decoding process in\nan online manner. The batch size and learning rate were set\nto 64 and 0.01, respectively, for training the acoustic LSTM,\nword LSTM, and classiﬁer.\nThe sub-EPD systems based on the single embedding\nalone or their combinations were built also in order to verify\nthe superiority of the DE for the proposed EPD. Speciﬁcally,\nthe embeddings including the AFE, PE, WE, and DE were\nprepared by feeding the training-dataset into the AFE-based\nEPD, AM, word LSTM, and proposed LM-based EPD system\nand capturing the hidden states at the level of the last hidden\nlayer, respectively. The classiﬁers of the sub-EPD system\nwere separately trained in accordance with the framewise\nendpoint target by feeding the single embedding alone or their\ncombinations into the EPD classiﬁers, while the CE error\nfunction was used. The batch size and learning rate were set\nto 64 and 0.01, respectively, for training each EPD classiﬁer.\nThe Adam optimization algorithm [59] was commonly\napplied for all the training processes. Furthermore, an early\nstopping scheme was performed using the development-\ndataset to avoid the over-ﬁtting, after 50 epochs were\ncompleted.\n3) EXPERIMENTAL RESULTS\nBefore demonstrating our experiments, we assessed the per-\nformance of the EPD systems based on the various DEs,\nwhich were obtained by the N-gram LM with different orders,\n161116 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nsince the performance of the LM-based EOU predictor for the\nEPD task is highly dependent on not only the DNN architec-\nture but also the N-gram LM used to build the targets. Table 1\nshows the average early and late endpoint times obtained\nwithin the development-dataset, where DE N denotes the EPD\nbased on the DE trained using the N-gram LM, and the bold\nnumbers indicate the best result among the DE-based EPD\nsystems. The performances of the AFE-, PE, and WE-based\nEPD algorithms are also reported for the relative performance\ncomparison. The DE trained with the 4-gram LM achieved\nlower endpoint errors compared with the others; thus, we used\nthe 4-gram LM for training the EOU predictor.\nTABLE 1. Performance comparison of the DE-based EPD algorithms,\nwhich were trained with theN-gram LM of different orders. All time\nvalues are in sec.\nThe proposed EPD algorithm and the conventional meth-\nods were extensively evaluated on the CHiME-3 ASR task to\nassess the EPD performance under the bus, cafe, pedestrian,\nand street scenarios for both the simulated acoustic condition\nand the everyday environment. Fig. 4 shows an example of\nthe prediction result of P(EOU|Xt ) and the ﬁnal EPD decision\naccording to each EPD algorithm under the REAL bus noise\nscenario, where this example includes the short pause regions\nat 2.4, 3.6, and 4.2 s and the long pause region from 4.8 to\n5.0 s. As shown in Fig. 4(b), P(EOU|Xt ) was observed to be\nhigh in the short and long pause regions and is likely to detect\nthe short pause as an endpoint since the GLDNN-based EPD\ncannot fully consider the language modeling knowledge such\nas the phone or word alignments. Especially, the probability\nof the EOU was sufﬁciently high to trigger the ﬁnal EPD\ndecision prematurely in the short and long pause regions.\nIn contrast, the ﬁnal EPD decision of the proposed EPD\nalgorithm was correctly triggered in the ﬁnal silence region.\nFurther, the late endpoint time could be reduced by the JRT\nprocess. The performance of the proposed EPD algorithm\nwas compared with that of the conventional EPD approaches\nin terms of objective measures described as follows.\nFirst, the performance of each EPD algorithm was eval-\nuated in terms of the early endpoint time. Table 2 shows\nthe performance comparison for the conventional and pro-\nposed EPD algorithms under the various acoustic condi-\ntions in terms of the early endpoint time where the bold\nnumbers indicate the best result in terms of the early end-\npoint time. In Table 2, the [embeddings] denotes the EPD\nsystem based on the given embeddings, where the CLDNN,\n[AFE, WE, DSFs], and [AFE, DE, PE] with or without\nJRT indicate [31], [39], and the proposed EPD framework,\nrespectively. As shown in Table 2, it was evident that\nthe [AFE] and [PE] classiﬁers yielded a higher endpoint\nerror compared with the [WE] and [DE] classiﬁers, which\nwere trained based on the 1-best ASR decoding hypothesis.\nFIGURE 4. Performance evaluation of endpoint detection algorithms in\nthe REAL bus noise condition. This example includes the short pause\nregions at 2.4, 3.6, and 4.2 s and the long pause region from 4.8 to 5.0 s.\nThese results indicate that the WE and DE are useful fea-\ntures for the EPD task to avoid the early endpoints since\nthey can distinguish the EOU from the intermediate silence\nwell compared with the AFE and PE, which were trained\nwithout considering the context of the input feature sequence.\nFurthermore, the [DE] classiﬁer achieved a better EPD per-\nformance than the [WE] classiﬁer, where the WE was trained\nto detect the turn-taking word and it cannot be considered\nreliable for the natural language, as mentioned earlier. And,\nthe performance of the [AFE] classiﬁer can be improved\nby incorporating the WE or DE as an additional input fea-\nture. Especially, the [AFE, WE] classiﬁer showed a higher\nendpoint error than the [AFE, DE] classiﬁer, which is more\ndesirable for the EPD task regarding the natural language.\nThe GLDNN-based EPD algorithm, which can be considered\nas the complex version of the [AFE] classiﬁer, showed a\nlower early endpoint error than the single embedding-based\nEPD system. However, the EPD systems based on their com-\nbination outperformed the GLDNN-based EPD approach in\nterms of the early endpoint time. The additional use of the\nDSFs for the EPD task could enhance the EPD performance\nof the [AFE, WE] classiﬁer. Furthermore, the proposed EPD\nalgorithm, namely [AFE, PE, DE] classiﬁer, showed a supe-\nrior EPD performance compared with that of the conventional\nEPD algorithms under the overall acoustic conditions, and\nthe early endpoint time of the proposed EPD algorithm was\nfurther improved by the JRT process as reported in Table 2.\nVOLUME 8, 2020 161117\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nTABLE 2. Performance comparison of the conventional and proposed EPD approaches for the CHiME-3 in terms of early endpoint time. All time values\nare in ms.\nMoreover, the performance of each EPD algorithm was com-\npared in terms of the late endpoint time. Table 3 shows the\nperformance comparison for the conventional and proposed\nEPD algorithms under the various acoustic conditions in\nterms of the late endpoint time. From Table 3, it is evident\nthat the [WE] classiﬁer exhibited the highest endpoint time\namong the single embedding-based EPD architectures. Fur-\nthermore, the late endpoint time of the [AFE] classiﬁer was\nreduced by using the WE or DE as an additional feature\nfor the EPD task, while the DE can be considered as a\nmore reliable feature for the EPD task compared with the\nWE. The proposed EPD framework yielded a superior EPD\nperformance than the conventional EPD approach in terms of\nthe late endpoint time. Moreover, the late endpoint time was\nfurther reduced by the JRT process.\nThe performance of each EPD algorithm was also assessed\nin terms of the WER by using the baseline ASR system.\nThe EOU frame of each speech utterance was obtained by\nperforming each EPD algorithm and then the ASR decoding\nwas accomplished from the ﬁrst frame to the EOU frame\ndetermined by each EPD algorithm. As reported in Table 4,\nthe proposed EPD algorithm also achieved a better per-\nformance than the conventional EPD approaches, and the\nWERs were further improved by the JRT scheme since this\nscheme can enhance the early and late endpoint times. The\nﬁnal decision of each EPD algorithm can also be obtained\nbased on a soft decision instead of a hard decision. The\nEPD decision makes the trade-off between a quick endpoint\nand avoiding cutting off the speech uttered by the user.\nMore speciﬁcally, an aggressive decision threshold provides\na faster response at the expense of increasing the WER,\nwhereas a lower WER increases the late endpoint time.\nTo show the trade-off between the WER and the late end-\npoint time for each EPD algorithm, the WER-median late\nendpoint time curve is shown in Fig. 5, which was obtained\nby varying the decision threshold; here, the lower curves\nare better. As shown in Fig. 5, the median late endpoint\ntimes of the GLDNN-based EPD approach, DSFs-based EPD\napproach, and the proposed EPD algorithm without and with\nthe JRT are approximately 270, 230, 190, and 170 ms, respec-\ntively, with the same WER of approximately 20%. As shown\nabove, the proposed EPD algorithm with the JRT process\nshowed a better EPD performance than the conventional EPD\napproaches.\nFIGURE 5. Performance evaluation of endpoint detection algorithms in\nterms of WER-median late endpoint time curve.\nB. LARGE-SCALE ASR TASK\n1) DATA PREPARATION\nTo assess the EPD performance of the conventional and\nproposed EPD approaches with large corpora, we used a\nlarge vocabulary continuous Korean speech dataset, namely\nDICT01, developed by the Speech Information Technol-\nogy and Industry Promotion Center (SiTEC) [52]. This\ndataset consists of 20,833 sentences, each containing 6 to\n25 words (average: 7.63 words). The speech database\nwas recorded with 200 males and 200 females and each\nspeaker uttered 104 or 105 sentences. The speech signal\nwas sampled at 16 kHz where the recording conditions\nare described in [52]. We randomly divided the speech\ndatabase into three datasets, which are the training-dataset\n(160 males and 160 females), development-dataset (20 males\nand 20 females), and evaluation-dataset (20 males and\n20 females). We made the reference decision on the clean\nspeech data of each dataset by manually labeling each frame\nas four types of state, which are speech, initial silence, ﬁnal\nsilence, and intermediate silence, for every 10 ms.\nWe constructed a noisy and reverberant speech database\nusing an image method [60] for a comparison among the\n161118 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nTABLE 3. Performance comparison of the conventional and proposed EPD approaches for the CHiME-3 in terms of late endpoint time. All time values are\nin ms.\nTABLE 4. Performance comparison of the conventional and proposed EPD approaches for the CHiME-3 in terms of WER (%).\nTABLE 5. Performance comparison of the conventional and proposed EPD approaches for the large-scale ASR task in terms of early endpoint time. All\ntime values are in ms.\nEPD approaches under the various acoustic conditions similar\nto real-life environments. We ﬁrst simulated the reverberant\nenvironments by convolving the clean speech of training-,\ndevelopment-, and evaluation-datasets with the room impulse\nresponses, which correspond to small rooms of the REVERB\nchallenge dataset for which the reverberation time T60 is\napproximately 0.25 s [61]. Then, the bus, cafe, pedestrian,\nand street noises obtained from CHiME-3 [45] were artiﬁ-\ncially added to each reverberant speech dataset in a time-\ndomain while maintaining the signal-to-noise ratio (SNR)\nat 5, 10, 15, and 20 dB. In addition, ofﬁce noise from\nYouTube was artiﬁcially added to the reverberant speech of\nthe evaluation-dataset to evaluate the performances of the\nconventional and proposed EPD approaches under the unseen\nacoustic condition. Consequently, approximately 1,342, 171,\nand 168 h of noisy and reverberant speech data of the\ntraining-, development-, and evaluation-datasets were pre-\npared, respectively.\n2) TRAINING PROCESS FOR EACH EPD MODEL\nWe constructed each EPD framework by using large corpora\nfor the performance comparison among the conventional\nVOLUME 8, 2020 161119\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nTABLE 6. Performance comparison of the conventional and proposed EPD approaches for the large-scale ASR task in terms of late endpoint time. All time\nvalues are in ms.\nTABLE 7. Performance comparison of the conventional and proposed EPD approaches for the large-scale ASR task in terms of WER (%).\nand proposed EPD approaches for the large-scale ASR task.\nThe experimental setup was similar to that of the previous\nCHiME-3 ASR task. First, the ASR baseline was trained to\nobtain the senones and framewise P(EOU|Xt ) labels, which\nwere respectively used to train the PE and LM-based EOU\npredictor as follows. The SAT algorithm was carried out\nwith the fMLLR features extracted from each utterance of\nthe training-dataset to extract the forced alignment. After\nthe DNN for the AM was initialized via the pre-training\nprocedure based on the CD algorithm [57], it was trained\nwith the CE error function and then trained again with\nthe sMBR criteria. In each step, as in the CHiME-3 task,\nthe development-set of large corpora was used for the early\nstopping scheme. The ASR decoding process was performed\nwith the training-dataset of large corpora to prepare the\nsenone labels for the training of the PE model. Furthermore,\nthe framewise P(EOU|Xt ) labels of the training-dataset were\nprepared using the 4-gram LM and the 1-best hypothesis\nobtained from the built-in ASR system.\nSecond, the conventional GLDNN-based EPD algorithm\nand DSFs-based EPD algorithm were constructed with\nthe conﬁgurations similar to the experimental setup of\nthe CHiME-3 task. The ensemble RNNs for the proposed\nLM-based EOU predictor, the PE-based AM, and the AFE-\nbased EPD modules were separately trained in accordance\nwith the P(EOU|Xt ) label and senones, which were obtained\nby performing the ASR system described above and the\nhand-made EPD label, respectively. Subsequently, their last\nhidden layers were concatenated to be fed into a fully-\nconnected DNN-based classiﬁer, which was then trained\naccording to the EPD label. Finally, the proposed EPD frame-\nwork was jointly retrained to optimize the EPD performance\n161120 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\nfurther. During all the training processes, the development-\ndataset was used to perform the early stopping scheme after\n50 epochs.\n3) EXPERIMENTAL RESULTS\nThe performances of the EPD frameworks for the large-scale\nASR task were also evaluated in terms of the early endpoint\ntime, late endpoint time, and WER by using the evaluation-\ndataset of the large corpora we prepared in this study.\nFirst, the proposed and conventional EPD algorithms were\nevaluated in terms of the early endpoint time under the rever-\nberant and noisy conditions, including the bus, cafe, pedes-\ntrian, street, and ofﬁce environments. Table 5 shows the early\nendpoint time of each EPD approach for the large-scale ASR\ntask, where the bold numbers indicate the best result in terms\nof the early endpoint time. It is shown in Table 5 that the [WE]\nand [DE] classiﬁers, which were trained according to the\n1-best ASR decoding hypothesis, yielded a relatively lower\nearly endpoint time compared with the [AFE] and [PE] clas-\nsiﬁers, which were trained without considering the context of\nthe input sequences such as the word or phone alignments.\nThe [WE] classiﬁer yielded a higher early endpoint time\ncompared with the GLDNN-based EPD method under the\noverall acoustic conditions. In contrast, the [DE] classiﬁer\nachieved a better EPD performance than the GLDNN-based\nEPD method under most of the low-SNR conditions in terms\nof the early endpoint time. The early endpoint time of the\n[AFE] classiﬁer was signiﬁcantly improved by incorporating\nthe WE or DE. From these results, it is concluded that the\ncontext-dependent embeddings such as the WE and DE can\nprevent the early endpoint within the short or long pause\nregions. While the performance of the [AFE, DE] classiﬁer\nwas enhanced by using the DSFs as the additional feature,\nthe proposed EPD framework yielded a superior EPD perfor-\nmance which was further improved by the JRT process. Note\nthat the proposed EPD algorithm also showed considerable\nperformance improvement under the ofﬁce noise environ-\nment as summarized in Table 5, where the ofﬁce is the unseen\nacoustic condition; hence, it was not used in the training-step.\nSecond, the proposed and conventional EPD algorithms\nwere evaluated in terms of the late endpoint time. Table 6\nsummarizes the late endpoint time of the EPD approaches for\nthe large-scale ASR task, where the bold numbers indicate\nthe best result in terms of the late endpoint time. While the\n[WE] classiﬁer showed the highest late endpoint time among\nthe EPD approaches based on the single embedding alone,\nthe [DE] classiﬁer achieved the late endpoint time that is\nrelatively closer to that of the [AFE] and [PE] classiﬁers.\nThe endpoint error of the [AFE] classiﬁer was considerably\nimproved by additionally employing the WE or DE. Note that\nthe late endpoint time of the [AFE, WE] classiﬁer was further\nimproved with the help of the DSFs, which can be obtained by\nthe online ASR decoding process with a great deal of compu-\ntation and a large amount of memory. Notably, the proposed\nEPD scheme yielded superior EPD performance, without the\nactual ASR decoding process, in terms of the late endpoint,\nwhich was further improved by the JRT process.\nFinally, the proposed and conventional EPD algorithms\nwere evaluated in terms of the WER. Table 7 shows the WER,\nwhich was obtained by performing the ASR system from\nthe ﬁrst frame to the EOU frame determined by each EPD\nalgorithm. As shown in Table 7, the proposed EPD approach\nyielded better performance in terms of the WER with the help\nof the superiority of the proposed EPD architecture, espe-\ncially in terms of the early endpoint time. Overall, the pro-\nposed EPD algorithm outperformed the conventional EPD\napproaches under both the seen and unseen noise conditions.\nV. CONCLUSION\nIn this paper, we proposed the speech EPD strategy for the\nrobust online low-latency speech recognition by combining\nthe AFE, DE, and PE to incorporate the acoustic and language\nmodeling knowledge into the AFE-based EPD.\nThe ﬁrst contribution of this study is to investigate the\nLM-based EOU predictor using the RNN to derive the frame-\nwise probabilities of EOU token given input speech without\nthe actual decoding process to consider the decoder states\nwhich are particularly useful for the EPD task but demands\na great deal of computation and a large amount of memory.\nSecond, we present the novel EPD architecture that can be\nconstructed by combining the last hidden states of the AE-\nbased EPD, the PE-based AM, and LM-based EOU predictor\nand training the DNN-based classiﬁer in accordance with the\nframewise endpoint label and be further enhanced by the JRT\ntechnique.\nThe superiority of the proposed EPD algorithm was\nassessed under the CHiME-3 and large-scale ASR tasks.\nAccording to the experimental results, the proposed EPD\nalgorithm showed a signiﬁcantly improved EPD performance\nin terms of both the endpoint accuracy and the WER.\nREFERENCES\n[1] N. G. Ward, A. G. Rivera, K. Ward, and D. G. Novick, ‘‘Root causes of\nlost time and user stress in a simple dialog system,’’ in Proc. Interspeech,\n2005, pp. 1565–1568.\n[2] A. Raux, D. Bohus, B. Langner, A. W. Black, and M. Eskenazi, ‘‘Doing\nresearch on a deployed spoken dialogue system: One year of let’s go!\nexperience,’’ in Proc. Interspeech, 2006, pp. 65–68.\n[3] W.-H. Shin, B.-S. Lee, Y .-K. Lee, and J.-S. Lee, ‘‘Speech/non-speech\nclassiﬁcation using multiple features for robust endpoint detection,’’ in\nProc. IEEE Int. Conf. Acoust., Speech, Signal Process., vol. 3, Jun. 2000,\npp. 1399–1402.\n[4] T. Kristjansson, S. Deligne, and P. Olsen, ‘‘V oicing features for robust\nspeech detection,’’ in Proc. Interspeech, 2005, pp. 369–372.\n[5] X. Li, H. Liu, Y . Zheng, and B. Xu, ‘‘Robust speech endpoint detection\nbased on improved adaptive band-partitioning spectral entropy,’’ in Proc.\nInt. Conf. Life Syst. Modeling Simulation (ICLSMS), Sep. 2007, pp. 36–45.\n[6] M. Fujimoto, K. Ishizuka, and T. Nakatani, ‘‘Study of integration of\nstatistical model-based voice activity detection and noise suppression,’’ in\nProc. Interspeech, 2008, pp. 2008–2011.\n[7] S. O. Sadjadi and J. H. L. Hansen, ‘‘Unsupervised speech activity detec-\ntion using voicing measures and perceptual spectral ﬂux,’’ IEEE Signal\nProcess. Lett., vol. 20, no. 3, pp. 197–200, Mar. 2013.\n[8] R. Hariharan, J. Hakkinen, and K. Laurila, ‘‘Robust end-of-utterance\ndetection for real-time speech recognition applications,’’ in Proc. IEEE Int.\nConf. Acoust., Speech, Signal Process., vol. 1, May 2001, pp. 249–252.\nVOLUME 8, 2020 161121\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\n[9] J. Sohn, N. Soo Kim, and W. Sung, ‘‘A statistical model-based voice\nactivity detection,’’ IEEE Signal Process. Lett., vol. 6, no. 1, pp. 1–3,\nJan. 1999.\n[10] H. Chung, S. J. Lee, and Y . Lee, ‘‘Endpoint detection using weighted ﬁnite\nstate transducer,’’ in Proc. Interspeech, 2013, pp. 700–703.\n[11] H. Chung, S. J. Lee, and Y . K. Lee, ‘‘Weighted ﬁnite state transducer-based\nendpoint detection using probabilistic decision logic,’’ ETRI J., vol. 36,\nno. 5, pp. 714–720, Oct. 2014.\n[12] X.-L. Zhang and J. Wu, ‘‘Deep belief networks based voice activity\ndetection,’’ IEEE Trans. Audio, Speech, Lang., Process., vol. 21, no. 4,\npp. 697–710, Apr. 2013.\n[13] X.-L. Zhang and J. Wu, ‘‘Denoising deep neural networks based voice\nactivity detection,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Pro-\ncess., May 2013, pp. 853–857.\n[14] I. Hwang and J. H. Chang, ‘‘V oice activity detection based on statistical\nmodel employing deep neural network,’’ in Proc. 10th Int. Conf. Intell. Inf.\nHiding Multimedia Signal Process., Aug. 2014, pp. 582–585.\n[15] I. Hwang, J. Sim, S.-H. Kim, K.-S. Song, and J.-H. Chang, ‘‘A statisti-\ncal model-based voice activity detection using multiple DNNs and noise\nawareness,’’ in Proc. Interspeech, 2015, pp. 2277–2281.\n[16] I. Hwang, H.-M. Park, and J.-H. Chang, ‘‘Ensemble of deep neural\nnetworks using acoustic environment classiﬁcation for statistical model-\nbased voice activity detection,’’ Comput. Speech Lang., vol. 38, pp. 1–12,\nJul. 2016.\n[17] X.-L. Zhang and D. Wang, ‘‘Boosting contextual information for deep\nneural network based voice activity detection,’’ IEEE/ACM Trans. Audio,\nSpeech, Lang. Process., vol. 24, no. 2, pp. 252–264, Feb. 2016.\n[18] M.-Y . Hwang and X. Huang, ‘‘Shared-distribution hidden Markov models\nfor speech recognition,’’ IEEE Trans. Speech Audio Process., vol. 1, no. 4,\npp. 414–420, Jan. 1993.\n[19] S. Thomas, G. Saon, M. V . Segbroeck, and S. S. Narayanan, ‘‘Improve-\nments to the IBM speech activity detection system for the DARPA\nRATS program, in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.\n(ICASSP), Apr. 2015, pp. 4500–4504.\n[20] L. Ferrer, M. Graciarena, and V . Mitra, ‘‘A phonetically aware system for\nspeech activity detection,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess. (ICASSP), Mar. 2016, pp. 5710–5714.\n[21] R. Masumura, T. Asami, H. Masataki, R. Ishii, and R. Higashinaka,\n‘‘Online end-of-turn detection from speech based on stacked time-\nasynchronous sequential networks,’’ in Proc. Interspeech, Aug. 2017,\npp. 1661–1665.\n[22] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[23] T. Hughes and K. Mierle, ‘‘Recurrent neural networks for voice activity\ndetection,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.,\nMay 2013, pp. 7378–7382.\n[24] F. Eyben, F. Weninger, S. Squartini, and B. Schuller, ‘‘Real-life voice\nactivity detection with LSTM recurrent neural networks and an application\nto hollywood movies,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., May 2013, pp. 483–487.\n[25] S. Tong, H. Gu, and K. Yu, ‘‘A comparative study of robustness of deep\nlearning approaches for V AD,’’ in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process. (ICASSP), Mar. 2016, pp. 5695–5699.\n[26] M. Shannon, G. Simko, S.-Y . Chang, and C. Parada, ‘‘Improved end-of-\nquery detection for streaming speech recognition,’’ in Proc. Interspeech,\nAug. 2017, pp. 1909–1913.\n[27] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, ‘‘Convolutional, long\nshort-term memory, fully connected deep neural networks,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr. 2015,\npp. 4580–4584.\n[28] R. Zazo, T. N. Sainath, G. Simko, and C. Parada, ‘‘Feature learning\nwith raw-waveform CLDNNs for voice activity detection,’’ in Proc. Inter-\nspeech, Sep. 2016, pp. 3668–3672.\n[29] T. N. Sainath and B. Li, ‘‘Modeling time-frequency patterns with LSTM\nvs. convolutional architectures for LVCSR tasks,’’ in Proc. Interspeech,\nSep. 2016, pp. 813–817.\n[30] N. Kalchbrenner, I. Danihelka, and A. Graves, ‘‘Grid long short-\nterm memory,’’ 2015, arXiv:1507.01526. [Online]. Available:\nhttp://arxiv.org/abs/1507.01526\n[31] S.-Y . Chang, B. Li, T. N. Sainath, G. Simko, and C. Parada,\n‘‘Endpoint detection using grid long short-term memory networks\nfor streaming speech recognition,’’ in Proc. Interspeech, Aug. 2017,\npp. 3812–3816.\n[32] S.-Y . Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi, A. van den Oord,\nand O. Vinyals, ‘‘Temporal modeling using dilated convolution and gating\nfor voice-activity-detection,’’ in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process. (ICASSP), Apr. 2018, pp. 5626–5630.\n[33] L. Ferrer, E. Shriberg, and A. Stolcke, ‘‘Is the speaker done\nyet? Faster and more accurate end-of-utterance detection using\nprosody,’’ in Proc. Int. Conf. Spoken Lang. Process. (ICSLP), 2002,\npp. 2061–2064.\n[34] L. Ferrer, E. Shriberg, and A. Stolcke, ‘‘A prosody-based approach to\nend-of-utterance detection that does not require speech recognition,’’ in\nProc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), vol. 1,\nApr. 2003, pp. 608–612.\n[35] A. Raux and M. Eskenazi, ‘‘Optimizing endpointing thresholds using\ndialogue features in a spoken dialogue system,’’ in Proc. 9th SIGdial\nWorkshop Discourse Dialogue (SIGdial), 2008, pp. 1–10.\n[36] B. Ramabhadran, O. Siohan, and A. Sethy, ‘‘The IBM 2007 speech\ntranscription system for European parliamentary speeches,’’ in Proc.\nIEEE Workshop Autom. Speech Recognit. Understand. (ASRU), 2007,\npp. 2609–2612.\n[37] B. Liu, B. Hoffmeister, and A. Rastrow, ‘‘Accurate endpointing\nwith expected pause duration,’’ in Proc. Interspeech , 2015,\npp. 2912–2916.\n[38] R. Maas, A. Rastrow, K. Goehner, G. Tiwari, S. Joseph, and B. Hoffmeister,\n‘‘Domain-speciﬁc utterance end-point detection for speech recognition,’’\nin Proc. Interspeech, Aug. 2017, pp. 1943–1947.\n[39] R. Maas, A. Rastrow, C. Ma, G. Lan, K. Goehner, G. Tiwari, S. Joseph, and\nB. Hoffmeister, ‘‘Combining acoustic embeddings and decoding features\nfor end-of-utterance detection in real-time far-ﬁeld speech recognition sys-\ntems,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP),\nApr. 2018, pp. 5544–5548.\n[40] S.-Y . Chang, R. Prabhavalkar, Y . He, T. N. Sainath, and G. Simko,\n‘‘Joint endpointing and decoding with end-to-end models,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2019,\npp. 5626–5630.\n[41] M. K. Mustafa, T. Allen, and K. Appiah, ‘‘A comparative review of\ndynamic neural networks and hidden Markov model methods for mobile\non-device speech recognition,’’ Neural Comput. Appl., vol. 31, no. S2,\npp. 891–899, Feb. 2019.\n[42] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,\nD. Rybach, A. Kannan, Y . Wu, R. Pang, Q. Liang, D. Bhatia, Y . Shangguan,\nB. Li, G. Pundak, K. Chai Sim, T. Bagby, S.-y. Chang, K. Rao, and\nA. Gruenstein, ‘‘Streaming End-to-end speech recognition for mobile\ndevices,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\n(ICASSP), May 2019, pp. 6381–6385.\n[43] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y .-Y . Lee, J. Yeo,\nD. Kim, S. Jung, J. Lee, M. Han, and C. Kim, ‘‘Attention based on-\ndevice streaming speech recognition with large speech corpus,’’ in Proc.\nIEEE Autom. Speech Recognit. Understand. Workshop (ASRU), Dec. 2019,\npp. 956–963.\n[44] L. Shi, ‘‘A general purpose semantic parser using framenet and wordnet,’’\nM.S. thesis, Univ. North Texas, Denton, TX, USA, 2004.\n[45] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, ‘‘The third ‘CHiME’\nspeech separation and recognition challenge: Dataset, task and baselines,’’\nin Proc. IEEE Workshop Autom. Speech Recognit. Understand. (ASRU),\nDec. 2015, pp. 504–511.\n[46] L. Lugosch, M. Ravanelli, P. Ignoto, V . S. Tomar, and Y . Bengio, ‘‘Speech\nmodel pre-training for end-to-end spoken language understanding,’’ in\nProc. Interspeech, Sep. 2019, pp. 814–818.\n[47] S. Katz, ‘‘Estimation of probabilities from sparse data for the language\nmodel component of a speech recognizer,’’ IEEE Trans. Acoust., Speech,\nSignal Process., vol. 35, no. 3, pp. 400–401, Mar. 1987.\n[48] H. Erdogan, J. R. Hershey, S. Watanabe, and J. Le Roux, ‘‘Phase-sensitive\nand recognition-boosted speech separation using deep recurrent neural\nnetworks,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\n(ICASSP), Apr. 2015, pp. 708–812.\n[49] B. Wu, M. Yu, L. Chen, M. Jin, D. Su, and D. Yu, ‘‘Improving\nspeech enhancement with phonetic embedding features,’’ in Proc. IEEE\nAutom. Speech Recognit. Understand. Workshop (ASRU), Dec. 2019,\npp. 645–651.\n[50] G. E. Dahl, D. Yu, L. Deng, and A. Acero, ‘‘Context-dependent pre-\ntrained deep neural networks for large-vocabulary speech recognition,’’\nIEEE Trans. Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 30–42,\nJan. 2012.\n161122 VOLUME 8, 2020\nI. Hwang, J.-H. Chang: End-to-End Speech EPD Utilizing Acoustic and Language Modeling Knowledge\n[51] D. Klakow and J. Peters, ‘‘Testing the correlation of word error rate and\nperplexity,’’Speech Commun., vol. 38, nos. 1–2, pp. 19–28, 2002.\n[52] Y . Lee, B. Kim, and Y . Um, ‘‘Speech information technology & industry\npromotion center in Korea: Activities and directions,’’ in Proc. Int. Conf.\nLang. Resour. Eval. (ICLRE), 2002, pp. 1851–1854.\n[53] M. Abadi et al. (2015). TensorFlow: Large-Scale Machine Learning on\nHeterogeneous Systems. [Online]. Available: http://tensorﬂow.org/\n[54] J. Garofalo, D. Graff, D. Paul, and D. Pallett, CSR-I, (WSJ0) Complete,\nvol. LDC93S6A. Philadelphia, PA, USA: Linguistic Data Consortium,\n1993.\n[55] X. Anguera, C. Wooters, and J. Hernando, ‘‘Acoustic beamforming for\nspeaker diarization of meetings,’’ IEEE Trans. Audio, Speech Lang. Pro-\ncess., vol. 15, no. 7, pp. 2011–2022, Sep. 2007.\n[56] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,\nM. Hannemann, P. Motlicek, Y . Qian, P. Schwarz, J. Silovsky,\nG. Stemmer, and K. Vesely, ‘‘The kaldi speech recognition toolkit,’’ in\nProc. IEEE Workshop Autom. Speech Recognit. Understand. (ASRU),\n2011, pp. 1–4.\n[57] G. E. Hinton, ‘‘Training products of experts by minimizing con-\ntrastive divergence,’’ Neural Comput., vol. 14, no. 8, pp. 1711–1800,\nAug. 2002.\n[58] B. Kingsbury, ‘‘Lattice-based optimization of sequence classiﬁcation cri-\nteria for neural-network acoustic modeling,’’ in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process., Apr. 2009, pp. 3761–3764.\n[59] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic opti-\nmization,’’ 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.\norg/abs/1412.6980\n[60] J. B. Allen and D. A. Berkley, ‘‘Image method for efﬁciently simulating\nsmall-room acoustics,’’ J. Acoustic Soc. Amer., vol. 65, no. 4, p. 950,\nApr. 1979.\n[61] K. Kinoshita, M. Delcroix, S. Gannot, E. A. P. Habets, R. Haeb-\nUmbach, W. Kellermann, V . Leutnant, R. Maas, T. Nakatani, B. Raj, A.\nSehr, and T. Yoshioka, ‘‘A summary of the REVERB challenge: State-\nof-the-art and remaining challenges in reverberant speech processing\nresearch,’’ EURASIP J. Adv. Signal Process., vol. 2016, no. 1, pp. 1–19,\nJan. 2016.\nINYOUNG HWANGreceived the B.S. degree in\nelectronics engineering from Suwon University,\nHwaseong, South Korea, in 2013, and the Ph.D.\ndegree in electronics and computer engineering\nfrom Hanyang University, Seoul, South Korea,\nin 2019. He is currently a Research Engineer of\nthe AI Service Division, AI Technology Unit,\nSK Telecom, Seoul. His research interests include\nautomatic speech recognition, voice activity detec-\ntion, speech endpoint detection, and machine\nlearning and deep learning applied to signal processing.\nJOON-HYUK CHANG (Senior Member, IEEE)\nreceived the B.S. degree in electronics engi-\nneering from Kyungpook National University,\nDaegu, South Korea, in 1998, and the M.S. and\nPh.D. degrees in electrical engineering from Seoul\nNational University, South Korea, in 2000 and\n2004, respectively. From 2000 to 2005, he was\nwith Netdus Corporation, Seoul, South Korea,\nas the CTO. From 2004 to 2005, he held a post-\ndoctoral position at the University of California\nat Santa Barbara, Santa Barbara, where he was involved in adaptive signal\nprocessing and audio coding. In 2005, he joined the Korea Institute of Sci-\nence and Technology, Seoul, as a Research Scientist, where he was involved\nin speech recognition. From 2005 to 2011, he was an Assistant Professor\nwith the School of Electronics Engineering, Inha University, Incheon, South\nKorea. He is currently a Full Professor with the School of Electronics Engi-\nneering, Hanyang University, Seoul. His research interests include speech\nrecognition, deep/machine learning, artiﬁcial intelligence (AI), speech pro-\ncessing, acoustic signal processing, and biomedical signal processing. He\nwas a recipient of the IEEE/IEEK IT Young Engineer in 2011. He is currently\nserving on the Editorial Board of the Digital Signal Processing(Elsevier).\nVOLUME 8, 2020 161123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8347622156143188
    },
    {
      "name": "Speech recognition",
      "score": 0.6787348985671997
    },
    {
      "name": "Decoding methods",
      "score": 0.6506684422492981
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6069672107696533
    },
    {
      "name": "Language model",
      "score": 0.5852046012878418
    },
    {
      "name": "Acoustic model",
      "score": 0.46499624848365784
    },
    {
      "name": "Embedding",
      "score": 0.41961753368377686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3662800192832947
    },
    {
      "name": "Artificial neural network",
      "score": 0.3252614140510559
    },
    {
      "name": "Speech processing",
      "score": 0.21556434035301208
    },
    {
      "name": "Algorithm",
      "score": 0.14798575639724731
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4575257",
      "name": "Hanyang University",
      "country": "KR"
    }
  ]
}