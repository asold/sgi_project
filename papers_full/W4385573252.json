{
    "title": "Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models",
    "url": "https://openalex.org/W4385573252",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3030359797",
            "name": "Qihuang Zhong",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2072270871",
            "name": "Liang, Ding",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1898662817",
            "name": "Li Shen",
            "affiliations": [
                "Jingdong (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2138582462",
            "name": "Peng Mi",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A2155941866",
            "name": "Juhua Liu",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A1911857409",
            "name": "Bo Du",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2104129307",
            "name": "Dacheng Tao",
            "affiliations": [
                "Jingdong (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2777662428",
        "https://openalex.org/W2143163787",
        "https://openalex.org/W2971033911",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2927103915",
        "https://openalex.org/W3130554079",
        "https://openalex.org/W4224884173",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2953271402",
        "https://openalex.org/W4313908941",
        "https://openalex.org/W759515131",
        "https://openalex.org/W2995463996",
        "https://openalex.org/W3205717164",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2888519496",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W3204181204",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3163251171",
        "https://openalex.org/W4287126759",
        "https://openalex.org/W4224248112",
        "https://openalex.org/W3207523779",
        "https://openalex.org/W3026404337",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2732724430",
        "https://openalex.org/W2153013403",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4285258432",
        "https://openalex.org/W3166792421",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2084840427",
        "https://openalex.org/W3176693010",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3176724752",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W4296142184",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4287324301",
        "https://openalex.org/W2963959597",
        "https://openalex.org/W4253067820",
        "https://openalex.org/W4287646898",
        "https://openalex.org/W1560153690",
        "https://openalex.org/W4305013198",
        "https://openalex.org/W2785047343",
        "https://openalex.org/W3092873005",
        "https://openalex.org/W2098297786",
        "https://openalex.org/W3173367141",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W4389891246",
        "https://openalex.org/W131533222",
        "https://openalex.org/W3199761064",
        "https://openalex.org/W2162245945",
        "https://openalex.org/W4226440416",
        "https://openalex.org/W2154652894"
    ],
    "abstract": "Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However, SAM adds a perturbation to each model parameter equally (but not all parameters contribute equally to the optimization of training), which we argue is sub-optimal and will lead to excessive computation. In this paper, we propose a novel optimization procedure, namely FSAM, which introduces a Fisher mask to improve the efficiency and performance of SAM. In short, instead of adding perturbation to all parameters, FSAM uses the Fisher information to identity the important parameters and formulates a Fisher mask to obtain the sparse perturbation, i.e., making the optimizer focus on these important parameters. Experiments on various tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently outperforms the vanilla SAM by 0.67 1.98 average score among four different pretrained models. We also empirically show that FSAM works well in other complex scenarios, e.g., fine-tuning on generation tasks or limited training data. Encouragingly, when training data is limited, FSAM improves the SAM by a large margin, i.e., up to 15.1.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4064–4085\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nImproving Sharpness-Aware Minimization with Fisher Mask\nfor Better Generalization on Language Models\nQihuang Zhong1∗, Liang Ding2, Li Shen2, Peng Mi3, Juhua Liu4†, Bo Du1†, Dacheng Tao2\n1 National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence, School of Computer Science\nand Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, China\n2 JD Explore Academy, China 3 School of Informatics, Xiamen University, China\n4 Research Center for Graphic Communication, Printing and Packaging, and Institute of Artificial Intelligence, Wuhan University, China\n{zhongqihuang,liujuhua,dubo}@whu.edu.cn, {dingliang1,shenli100}@jd.com, mipeng@stu.xmu.edu.cn, dacheng.tao@gmail.com\nAbstract\nFine-tuning large pretrained language models\non a limited training corpus usually suffers\nfrom poor generalization. Prior works show\nthat the recently-proposed sharpness-aware\nminimization (SAM) optimization method can\nimprove the model generalization. However,\nSAM adds a perturbation to each model param-\neter equally (but not all parameters contribute\nequally to the optimization of training), which\nwe argue is sub-optimal and will lead to exces-\nsive computation. In this paper, we propose a\nnovel optimization procedure, namely FSAM1,\nwhich introduces a Fisher mask to improve the\nefficiency and performance of SAM. In short,\ninstead of adding perturbation to all parame-\nters, FSAM uses the Fisher information to iden-\ntity the important parameters and formulates a\nFisher mask to obtain the sparse perturbation,\ni.e., making the optimizer focus on these impor-\ntant parameters. Experiments on various tasks\nin GLUE and SuperGLUE benchmarks show\nthat FSAM consistently outperforms the vanilla\nSAM by 0.67∼1.98 average score among four\ndifferent pretrained models. We also empiri-\ncally show that FSAM works well in other com-\nplex scenarios, e.g., fine-tuning on generation\ntasks or limited training data. Encouragingly,\nwhen training data is limited, FSAM improves\nthe SAM by a large margin, i.e., up to 15.1.\n1 Introduction\nThe “pretraining-finetuning” paradigm has become\nthe de facto standard for the community of natural\nlanguage processing (NLP) (Devlin et al., 2019;\nLiu et al., 2019; Clark et al., 2019b; Raffel et al.,\n2020; Brown et al., 2020; Lewis et al., 2020). Given\na pretrained language model (PLM), the dominant\nfine-tuning manner is tuning the entire pretrained\n∗Work was done when Qihuang was interning at JD\nExplore Academy.\n† Corresponding Authors: Juhua Liu (e-mail: liu-\njuhua@whu.edu.cn), Bo Du (e-mail: dubo@whu.edu.cn)\n1 https://github.com/WHU-ZQH/FSAM4PLM\nparameters for each downstream task (Radford\net al., 2018; Devlin et al., 2019). While fine-tuning\nthe entire PLM can improve performance on a wide\nrange of NLP tasks, it usually suffers from over-\nfitting and poorer generalization ability (Xu et al.,\n2021; Bahri et al., 2022), especially in the large-\nscale PLMs and limited training data scenarios.\nHence, some existing efforts attempt to provide\nmore regularization in the fine-tuning stage (Zhang\net al., 2018; Müller et al., 2019; Xu et al., 2021),\namong which the optimization of the training loss\nis an intuitive and effective method. Specifically,\nmotivated by the finding (Keskar et al., 2016;\nNeyshabur et al., 2017) that the smoother loss\nlandscape refers to the better model generalization,\nForet et al. (2020) propose the “sharpness-aware\nminimization” (SAM) to simultaneously minimize\nloss value and loss sharpness, where the sharpness\ncan be quantified as the maximized difference of\nloss when a perturbation is added to the current\nweights. In practice, SAM performs two forward-\nbackward computations for each optimization step,\nwhere the first forward-backward is to obtain the\nperturbation for each model parameter and the sec-\nond one is to update the parameters. Many prior\nworks (Wu et al., 2020; Zheng et al., 2021) show\nthe effectiveness of SAM in the vision domain, mo-\ntivated by this, Bahri et al. (2022) first apply the\nSAM to the language domain, more recently.\nAlthough Bahri et al. (2022) empirically show\nthe remarkable performance of SAM on several lan-\nguage understanding tasks, SAM calculates pertur-\nbations indiscriminately for all parameters, which\nis time-consuming and hinders the application of\nSAM. Furthermore, inspired by the finding (Keskar\net al., 2016) that only about 5% of parameters are\nsharp and rise steeply during optimization, we no-\ntice that not all parameters contribute equally to\nthe optimization of training . Hence, this raises\na question that whether we can calculate pertur-\nbations for only some individual parameters, and\n4064\nthus make the optimizer focus on these important\nparameters.\nTo this end, we propose a novel optimization\napproach, Fisher SAM (FSAM), which introduces\na Fisher mask to improve the efficiency and effec-\ntiveness of SAM. In short, FSAM first uses the\nFisher information (Fisher, 1922) as the metric to\nidentify the sharper parameters 2 and formulates\na binary Fisher mask correspondingly. Then, the\nFisher mask is multiplied with the perturbations\nto obtain the sparse perturbations, which are lastly\nused to perform regularization in the parameter\nupdate. In this way, only parts of sharper parame-\nters will be added into the perturbations, and the\noptimizer can thus focus more on these important\nparameters. Also, the sparse perturbations could\nensure the training acceleration via sparse back-\npropagation3. Moreover, one may concern that the\nsparse Fisher mask would affect the convergence\nrate of FSAM (Lin et al., 2019). Hence, we theoret-\nically provide the convergence analysis of FSAM,\nensuring that the convergence of FSAM is irrele-\nvant to the Fisher mask.\nWe conduct a large-scale and systematic study\nto evaluate the performance and effectiveness of\nFSAM. Firstly, we apply SAM and FSAM to fine-\ntune various PLMs on parts of GLUE and Super-\nGLUE benchmarks, where the results show that\nFSAM consistently outperforms the vanilla SAM\nby 0.67∼1.98 average score among these PLMs,\nand surpasses the Adam (Kingma and Ba, 2015)\noptimizer by 1.41 ∼1.91 points. Secondly, we\nconduct experiments on two popular generation\ntasks (i.e., XSUM and CoNLL2014) and prove that\nFSAM can deliver promising results against SAM.\nLastly, quantitative analysis and in-depth discus-\nsion demonstrate the universality and effectiveness\nof FSAM in various complex scenarios, and prove\nthat FSAM indeed brings better model generaliza-\ntion. Specifically, we show that our Fisher mask\nstrategy not only works well in the SAM, but also\ncan be applied to other SAM variants.\nTo summarize, our contributions are two-fold:\n(1) We propose a novel optimization approach\n(namely FSAM) with theoretical convergence guar-\n2 We refer to these parameters as the important ones, be-\ncause they will rise steeply during optimization and affect the\nmodel generalization significantly.\n3 Since the fine-grained sparse training is limited to the\nhardware, we do not achieve actual sparse speedup in this\nwork. Despite it, we still believe that FSAM has great potential\nto achieve true training acceleration in the future, with the\ndevelopment of hardware for fine-grained sparse operation.\nantee for PLMs. Specifically, FSAM improves the\nperformance and efficiency of recently-proposed\nSAM via a Fisher mask strategy, which can also be\napplied to more SAM variants. (2) Extensive exper-\niments show that FSAM consistently outperforms\nthe SAM by a large margin on both language un-\nderstanding and generation tasks. The systematic\nstudy demonstrates the effectiveness and universal-\nity of FSAM on improving model generalization.\n2 Related Work\nSAM and its variants.Hochreiter and Schmid-\nhuber (1994) first show the strong correlation be-\ntween the flat minima and the generalization of\na model, inspired by this, Foret et al. (2020) pro-\npose the SAM to find a flat minimum and thus\nimprove model generalization. While many ex-\nisting works prove the effectiveness of SAM on\nvarious computer vision tasks (Wu et al., 2020;\nChen et al., 2021; Zheng et al., 2021), the double\nforward-propagation process of SAM brings more\ncomputational cost. To this end, Du et al. (2021)\npropose an Efficient SAM (ESAM) for reducing the\ncomputational cost of SAM. Additionally, there are\nalso some efforts that focus on more efficient and\neffective SAM optimization (Zhuang et al., 2021;\nKwon et al., 2021; Mi et al., 2022).\nImproving Generalization. Recently, we have\nwitnessed numerous PLMs that achieved tremen-\ndous success in the community of NLP (Yang et al.,\n2019; Devlin et al., 2019; Brown et al., 2020; Lewis\net al., 2020; Raffel et al., 2020; Joshi et al., 2020;\nHe et al., 2020; Qi et al., 2021; Zhong et al., 2022).\nThe current dominant fine-tuning approach needs to\ntune all pretrained parameters for each downstream\ntask, which makes the PLM easily memorize the\ntraining data and thus leads to overfitting. To tackle\nthis issue, some works attempt to provide implicit\nand explicit regularization into the training of mod-\nels, such as dropout (Srivastava et al., 2014), label\nsmoothing (Müller et al., 2019), mixup (Zhang\net al., 2018) and other data-augmentation meth-\nods (Sennrich et al., 2016; Wang et al., 2018b;\nZhong et al., 2021; Wang et al., 2022; Ding et al.,\n2022). On the other hand, motivated by the suc-\ncessful applications of SAM in the vision domain,\nBahri et al. (2022) involve applying SAM to opti-\nmize the T5 (Raffel et al., 2020) model on multiple\nlanguage tasks and show that SAM can improve\nthe generalization of PLMs effectively.\nWe depart from the prior work (Bahri et al.,\n4065\n2022) and ours as follows: 1) different motivations:\ninstead of verifying the effect of vanilla SAM on\nseveral language understanding tasks, we aim to\nimprove the efficiency and effectiveness of SAM.\n2) different contributions: our main contribution\nis to propose a fisher mask strategy, which can be\napplied to both SAM and its variants. 3) more anal-\nysis: we provide more experimental results and\nanalysis towards the effectiveness of our method in\nmore complex scenarios.\n3 Methodology\nIn this section, we first review the Sharpness-Aware\nMinimization, and then propose our Sharpness-\nAware Minimization with Fisher mask, coined as\nFSAM. Finally, we theoretically analyze the con-\nvergence of FSAM with adaptive learning rate.\n3.1 Sharpness-Aware Minimization\nPreliminary. In this paper, we denote the weight\nof a neural network as w ∈ Rd. Suppose the\ntraining dataset S = {(xi,yi)}n\ni=1 i.i.d. drawn\nfrom the distribution D. The object function of\nthe data xi from Sis denote as fS(xi). Since the\nAdam (Kingma and Ba, 2015) and its variants are\nwidely used in NLP tasks, the learning rate is esti-\nmated via RMSProp/Adam style.\nSharpness-Aware Minimization. Foret et al.\n(2020) propose the Sharpness-Aware Minimiza-\ntion (SAM) to improve the generalization, which\nis achieved by the following min-max problem:\nmin\nw\nmax\n||ϵ||2≤ρ\nf(w+ ϵ), (1)\nwhere ρis a predefined value to control the neigh-\nborhood size, and the ϵis the perturbation vector\non model weight. The optimization is expected\nthat the model loss will not significantly rise with\na certain amount of weight change controlled by ρ,\nwhich is intuitively consistent with the generaliza-\ntion capacity of model.\nWith the Taylor expansion, the perturbation vec-\ntor ϵcould be achieved approximately:\nϵ∗ = arg max\n||ϵ||2≤ρ\nfS(w+ ϵ) (2)\n≈arg max\n||ϵ||2≤ρ\nfS(w) +ϵ·∇wf(w) (3)\n=ρ·∇wf(w)\n/\n||∇wf(w)||2, (4)\nand the object function could be simplified as\nmin\nw\nf(w+ ρ ∇wf(w)\n||∇wf(w)||2\n), (5)\nThe solution of the above function could be ob-\ntained by a two-step gradient descent. In the first\ngradient descent step, the perturbation vector ϵis\ncalculated by Equation 2. The second gradient\ndescent step is the actual weight update.\nHowever, despite the improvement of SAM on\nmany tasks, SAM requires a two-step gradient cal-\nculation which leads to the double overhead com-\npared to the conventional optimizer,e.g., Stochastic\nGradient Descent (SGD) and Adam.\n3.2 Sharpness-Aware Minimization with\nFisher Mask\nIn this subsection, we propose the Sharpness-\nAware Minimization with Fisher Mask (FSAM)\nin detail, which reduces the computation of SAM\nby sparse calculation.\nTo be specific, we compute only a fraction of the\nelements in the perturbation vector ϵ, which would\nbe multiplied by a sparse binary maskm∈{0,1}d.\nTo control the amount of perturbation, the sparse\nmask msatisfies 1Tm= (1−s) ·d, where the s\nis the predefined sparse ratio and empirically set to\n0.9. The objective function of FSAM is denoted as\nmin\nw\nfS(w+ ρ∇wf(w) ⊙m\n||∇wf(w)||2\n), (6)\nwhere ⊙is the Hadamard product, i.e., the element-\nwise multiplication. For the stability of optimiza-\ntion, we update the mask mwith a fixed interval\n(denoted as Fi) during training. The algorithm of\nFSAM is shown in Algorithm 1.\nTo find the optimal mask during training, we\napply the Fisher information to achieve sparse\nperturbation. The Fisher information is proposed\nby (Fisher, 1922) to measures the information car-\nried by an observable random variable about the\nunknown parameters of the distribution. The Fisher\ninformation is defined by\nF = Ex\n[\nEy∇log p(y|x)∇log p(y|x)T]\n, (7)\nwhere the p(y|x) is the output of model in machine\nlearning. However, due to the over-parameterized\nmodel in deep learning, the computation of Fisher\ninformation is unacceptable, i.e., F ∈R|w|×|w|.\nTo save the computational effort, we approximate\nFisher information as the diagonal matrix, i.e., F ∈\nR|w|. Consider the expectation in Equation 7, the\nfirst one is the data distribution x∼p(x), which is\n4066\nAlgorithm 1Fisher SAM (FSAM)\nInput: sparse ratio s, dense model w, binary mask m, update\ninterval Tm, base learning rate γ, ˆv−1 = δ2, training set\nS.\n1: Initialize wand mrandomly.\n2: for epoch t= 1,2 ...T do\n3: for each training iteration do\n4: Sample a batch from S: B\n5: Compute perturbation ϵby Eq. 2\n6: if t mod Tm = 0then\n7: Sample NFisher data from distribution S.\n8: Compute Empirical Fisher by Equation 9.\n9: m1 ←ArgTopK( ˆF,(1 −s) ·|w|)\n10: m0 ←ArgTopK(−ˆF,s ·|w|)\n11: Update mask mby merging: m= m0 ∪m1.\n12: end if\n13: ϵ←ϵ⊙m\n14: end for\n15: Compute SAM gradient gt = ∇fB(w+ ϵ)\n16: vt = β2vt−1 + (1−β2)[gt]2\n17: ˆvt = max(ˆvt−1,vt)\n18: w←w−γ∇gt ⊙ 1√ˆvt\n19: end for\n20: return Final weight of model w\nnot available in most tasks. We approximate it by\nsampling NFisher data from p(x):\nF = 1\nNFisher\nEy∇log p(y|xi)2. (8)\nThe second expectation is over p(y|x), which can\nbe achieved by the labelyi for data xi in supervised\nlearning. Finally, we calculate the Fisher informa-\ntion as \"Empirical Fisher\":\nˆF = 1\nNFisher\n∇log p(yi|xi)2. (9)\nSince the empirical Fisher is the same size as the\nweight, i.e., ˆF ∈R|w|, the value of the element\nin Fisher ˆF represents the importance of the cor-\nresponding element in weight w. Thus, we sort\nthe elements of ˆF in descending, and the weights\nwith top kFisher values will be perturbed, i.e., the\ncorresponding element in mask will be set to 1:\nm1 ←ArgTopK( ˆF,(1 −s) ·|w|), (10)\nwhere m1 is the set whose elements in the maskm\nare 1, i.e., m= {mi= 1|mi∈m}, and ArgTopK\n(x,k) returns the top klargest values among x. On\nthe other hand, the other weights with small Fisher\nvalues will not be perturbed, i.e., the corresponding\nelement in mask will be set to 0:\nm0 ←ArgTopK( ˆF,s ·|w|). (11)\n3.3 Theoretical Analysis\nIn this subsection, we theoretically analyze the con-\nvergence and generalization of FSAM. Due to the\nspace limitation, we only show the convergence\nanalysis here, and the generalization analysis and\nwhole proof are presented in Appendix A.1.\nAssumption 1. (L-smooth.) Consider f is differ-\nentiable with gradient Lipschitz property: It exists\nL> 0 s.t.\n||∇f(w) −∇f(v)||≤ L||w−v||,∀w,v ∈Rd.\nAssumption 2. (Bounded stochastic gradients.)\nThe variance of stochastic gradient is bounded:\nE[||∇fi(x) −∇f(x)||2] ≤σ2\nAssumption 3.(Bounded gradient.) The stochastic\ngradient is bounded: It exists G≥0 s.t.\n||∇fi(w)||∞≤G\nTheorem 1. Consider the function f under the\nassumption 1,2,3, and a fixed base learning rate γt\nsatisfies that γt ≤ δ\n8L, we have\n1\nT\nT−1∑\nt=0\nE||∇f(xt)||2 ≤2Gf(x0) −f∗\nγtT\n+ 20GL2ρ2\nδ + 2G3\nT d(1\nδ −1\nG)\n+ 4GγtL\nδ\nLρ2\nδ + 4GγtL\nδ\nσ2\nbδ\n+ 4γtLG3\nT d(G2 −δ2)\nThe Theorem 1 shows that when T is large, FSAM\ncould achieve the linear speedup convergence rate\nwith respect to mini-batch size bunder the setting\nof γt = O(\n√\nb\nT) and ρ= O(\n√\n1\nbT), i.e.,\n1\nT\nT−1∑\nt=0\nE||∇f(xt)||2 = O(\n√\n1\nbT)\n4 Experimental Setup\n4.1 Tasks and Datasets\nTo investigate the effectiveness and universality of\nour FSAM method, we conduct extensive experi-\nments on various NLP tasks. Specifically, different\nfrom Bahri et al. (2022) that only verify the method\non several language understanding tasks, we eval-\nuate our method on both language understanding\nand generation tasks.\n4067\nTable 1: Experimental results (dev scores) on various language understanding benchmarks. Comparison between\nvanilla SAM and our proposed FSAM applied to four widely used large-scale PLMs. The best results for each\nsetting are in bold. “A VG.” denotes the average scores on all tasks, which areunderlined. Results show that our\nFSAM brings consistent improvements across all understanding tasks among different PLMs.\nMethod CoLA MRPC STS-B RTE CB BoolQ WSC WiC A VG.\nMcc. Acc. F1. Pear. Spea. Acc. Acc. Acc. Acc. Acc.\nBERT-large\nAdam 62.8 87.3 91.1 89.5 89.3 70.7 87.5 74.3 68.3 72.7 79.35\nAdam+SAM 62.1 87.9 91.4 89.8 89.4 71.5 91.1 72.9 68.3 74.1 79.85\nAdam+FSAM 63.4 89.0 92.0 90.4 89.9 74.4 94.6 75.3 68.5 74.4 81.19\nELECTRA-large\nAdam 69.0 89.2 92.4 92.1 92.1 87.3 91.1 85.6 83.6 74.4 85.68\nAdam+SAM 63.9 91.9 94.2 92.4 92.4 89.2 92.9 82.2 84.6 72.4 85.61\nAdam+FSAM 69.6 92.4 94.5 92.3 92.5 88.8 96.4 85.9 89.4 74.1 87.59\nALBERT-xxlarge\nAdam 71.1 90.7 93.3 92.9 92.7 87.0 89.3 86.8 85.6 75.5 86.49\nAdam+SAM 69.9 90.7 93.2 92.6 92.4 88.1 91.1 87.7 82.7 76.6 86.50\nAdam+FSAM 72.3 91.9 94.2 93.0 92.8 88.8 91.1 87.9 86.5 76.6 87.51\nRoBERTa-large\nAdam 66.7 90.4 93.1 92.1 92.0 87.0 92.8 86.0 78.1 73.3 85.15\nAdam+SAM 68.5 90.7 93.3 91.5 91.3 87.7 96.4 84.2 81.3 74.0 85.89\nAdam+FSAM 69.5 90.7 93.2 91.9 91.6 87.7 98.2 86.8 81.5 74.5 86.56\nLanguage Understanding Tasks. Following\nmany previous works (Vu et al., 2022; Bahri et al.,\n2022; Zhong et al., 2022), we conduct experiments\non a combination of tasks from GLUE (Wang\net al., 2018a) and SuperGLUE (Wang et al.,\n2019) benchmarks, including linguistic acceptabil-\nity (CoLA), natural language inference (RTE, CB),\nparaphrase and similarity (MRPC and STS-B),\nquestion answering (BoolQ), word sense disam-\nbiguation (WiC) and coreference resolution (WSC).\nIn practice, we evaluate the performance with Accu-\nracy (“Acc.”) metric for most tasks, except the addi-\ntional F1 score for MRPC, the Pearson-Spearman\ncorrelations (“ Pear./Spea.”) for STS-B and the\nMatthew correlation (“Mcc.”) for CoLA.\nLanguage Generation Tasks. We also use two\npopular generation tasks following Liu et al.\n(2021); Zhang et al. (2022) as the benchmarks, i.e.,\nabstractive summarization (XSUM) and grammati-\ncal error correction (CoNLL2014). For the XSUM,\nwe report results in terms of standard ROUGE met-\nrics (Lin, 2004), i.e., Rouge-1, Rouge-2 and Rouge-\nL, respectively. For the CoNLL2014, MaxMatch\nscores (Dahlmeier and Ng, 2012) are used for eval-\nuation with Precision, Recall, and F0.5 values 4.\n4 Due to the space limitation, we present the details of all\nused tasks and datasets in Appendix A.2\n4.2 Implementations\nIn practice, we use the pretrained models and\ncode in HuggingFace5 (Wolf et al., 2019). Specif-\nically, for the understanding tasks, we employ 4\nwidely used PLMs in our study, i.e., BERT (Devlin\net al., 2019), ELECTRA (Clark et al., 2019b), AL-\nBERT (Lan et al., 2019) and RoBERTa (Liu et al.,\n2019). Furthermore, an representative sequence-\nto-sequence model, BART (Lewis et al., 2020), is\nused for the generation tasks.\nWe compare our proposed FSAM method with\nthe base optimizer (without using any SAM ap-\nproach) and vanilla SAM method. Specifically, the\nAdam (Kingma and Ba, 2015) is used as the base\noptimizer to tune our models. The β2 and weight\ndecay of Adam are set as 0.999 and 0.01. SAM\nand FSAM use the same settings as above. More\nspecially, we grid search for the neighborhood size\nof SAM and FSAM on {1e-2, 5e-3, 1e-3}. Ad-\nditionally, for each downstream task, we follow\nthe same hyper-parameter settings from the prior\nworks (Lewis et al., 2020; Xu et al., 2021). The\ndetailed hyper-parameters of fine-tuning on these\ndownstream tasks can be seen in Appendix A.3.\nWe report the averaged results over 5 random seeds\nfor NLU tasks, while for NLG tasks, we follow ex-\n5 https://github.com/huggingface/transformers\n4068\nTable 2: Experimental results on two popular generation tasks. We use the representative sequence-to-sequence\nPLM (BART) in this study. It shows that our FSAM works well on the language generation tasks as well. “ ‡”\nindicates that FSAM is significantly better than baselines at significance level p<0.05.\nMethod XSUM CoNLL2014 A VG.\nRouge_1 Rouge_2 Rouge_L Precision Recall F_0.5\nBART-large\nAdam 44.35 21.66 36.62 52.94 41.18 50.08 41.14\nAdam+SAM 44.81 21.95 36.97 53.52 41.76 50.70 41.62\nAdam+FSAM 45.03‡ 22.15‡ 37.14‡ 54.33‡ 42.15‡ 51.36‡ 42.03\nBART-base\nAdam 39.38 17.21 31.93 43.27 34.11 41.06 34.49\nAdam+SAM 40.38 18.00 33.00 50.39 33.51 45.78 36.84\nAdam+FSAM 40.60‡ 18.31‡ 33.28‡ 51.77‡ 34.04‡ 46.89‡ 37.48\nTable 3: Results of smaller PLMs with different opti-\nmizers on parts of understanding tasks. BERT-base and\nRoBERTa-base are used in this experiment.\nMethod CoLA MRPC STS-B RTE\nMcc. Acc. F1. Pear. Spea. Acc.\nBERT-base\nAdam 54.3 85.8 90.0 89.2 88.9 68.4\n-w SAM 53.0 87.3 90.9 89.3 89.1 66.4\n-w FSAM 53.8 87.7 91.3 89.5 89.2 70.0\nRoBERTa-base\nAdam 61.3 87.5 90.6 90.6 90.4 78.3\n-w SAM 60.6 89.2 92.1 90.5 90.4 78.7\n-w FSAM 61.4 89.5 92.5 90.7 90.4 80.1\nisting works (Collins et al., 2005; Ding et al., 2021)\nand use the Bootstrap test (Berg-Kirkpatrick et al.,\n2012) to calculate the statistical significance.\n5 Main Results\nFSAM outperforms vanilla SAM by a large mar-\ngin across different PLMs. Table 1 shows the\nresults of all understanding tasks. We can ob-\nserve that SAM achieves better average scores\nthan the base Adam in most scenarios, confirming\nthe effectiveness of SAM in improving generaliza-\ntion (Bahri et al., 2022). Moreover, with the help\nof our Fisher mask strategy, FSAM consistently im-\nproves the vanilla SAM by a large margin across all\nPLMs. Specifically, FSAM yields an improvement\nof up to 1.98 average score on ELECTRA, 1.01\naverage score on ALBERT and 1.34 average score\non BERT. The average improvement on RoBERTa\nis slight but also higher than 0.67.\nFSAM also works well on the generation tasks.\nPrior works (Kwon et al., 2021; Bahri et al., 2022),\nwhich involve the study of SAM or its variants, usu-\nally conduct experiments on the image or text clas-\nsification tasks, e.g., CIFAR-10 (Krizhevsky et al.,\n2009) and ImageNet (Krizhevsky et al., 2012). The\neffectiveness of optimizer on other types of tasks,\ne.g, generation tasks in NLP, has not been explored\nwell. Thus far, we evaluate our FSAM on the gen-\neration tasks and present the results in Table 2. It\ncan be seen that FSAM can deliver promising re-\nsults against the vanilla SAM as well. Note that\nboth FSAM and SAM outperform the base Adam\noptimizer, indicating the applicability of SAM and\nits variants on generation tasks.\nFSAM improves performance on various model\nsizes. To investigate whether our FSAM is help-\nful for various scales of PLMs, we evaluate the\nperformance on smaller PLMs, i.e., BERT-base,\nRoBERTa-base and BART-base. The results are\nshowed in Table 3 and Table 2, respectively. We\ncan see that FSAM consistently outperforms the\nvanilla SAM on multiple smaller PLMs, to be spe-\ncific, the relative improvements of BERT-base and\nBART-base are up to 0.92 and 0.64 average scores.\nThese results prove that FSAM works well on vari-\nous model sizes.\n6 Analysis and Discussion\nIn this section, we examine whether our approach\nworks in more complicated scenarios, and provide\na more intuitive comparison between different opti-\nmizers towards the generalization. More analysis\nand results can be found in Appendix.\n6.1 Parameter Analysis\nThere are two important hyper-parameters (i.e., s\nand Fi) in our FSAM, where the s refers to the\n4069\nFigure 1: Results of FSAM at various sparse rates.\nRoBERTa-base models is used.\nTable 4: Average performance (CoLA, MRPC, STS-B\nand RTE) of FSAM with differentFi, which denotes the\nfixed interval for updating Fisher mask.\nMethod 10 50 100 200 500\nFSAM-BERTbase 83.9 84.0 84.3 84.2 84.1\nFSAM-RoBERTabase 80.0 80.1 80.3 80.2 80.2\nsparse ratio and Fi is used to control the update\nfrequency of Fisher mask. Here, we evaluate the\nperformance of FSAM with different sand Fi on\nseveral downstream tasks to analyze their effects.\nFirstly, Figure 1 shows the results based on dif-\nferent s. We can observe FSAM outperforms the\nvanilla SAM and base Adam in most settings, indi-\ncating the robustness of FSAM. Specifically, when\nthe sparse ratio is 0.9, FSAM consistently achieves\nthe best performance on both tasks. Secondly, for\nFi, we show the performance of FSAM on different\nFi in Table 4. Too small Fi (e.g., 10) may lead to\nthe Fisher mask updating too fast, thus affecting\nthe stability of model optimization. Recall that we\nset s= 0.9 and Fi = 100as the default setting.\n6.2 Complementarity with Other Optimizers\nAs aforementioned, we show the effectiveness of\nour Fisher mask strategy on SAM optimization.\nTo further prove the universality of our proposed\nstrategy, we examine whether the strategy is com-\nplementary with i) more base optimizers and ii)\nother efficient SAM variants.\nTo verify i), we use the additional AMSGrad and\nAdagrad as the base optimizers and evaluate the\nperformance with different strategies, respectively.\nTable 5 lists the results of RoBERTa-large. It can\nbe seen that FSAM consistently achieves the best\nperformance upon these base optimizers, showing\nour strategy is not sensitive to the base optimizers.\nFor ii), we apply our strategy to another\ntwo cutting-edge SAM-variant optimizers, i.e.,\nESAM (Du et al., 2021) and GSAM (Zhuang et al.,\nTable 5: Results of other base optimizers,i.e., AMSGrad\nand Adagrad. RoBERTa-large is used.\nMethod CoLA MRPC STS-B RTE\nMcc. Acc. F1. Pear. Spea. Acc.\nRoBERTa-large\nAMSGrad 66.8 90.2 92.7 91.7 91.5 86.7\n-w SAM 67.3 90.2 93.0 91.6 91.4 87.0\n-w FSAM 68.7 90.4 93.2 91.2 91.1 87.7\nAdagrad 59.3 81.1 87.4 88.2 88.5 80.9\n-w SAM 57.5 88.0 91.4 84.6 85.7 83.4\n-w FSAM 57.5 90.0 92.8 86.8 87.1 86.3\nTable 6: Results of some SAM variants (i.e., ESAM (Du\net al., 2021) and GSAM (Zhuang et al., 2021)) with our\nFisher-masked strategy, denoted as “F_*”.\nMethod CoLA MRPC STS-B RTE\nMcc. Acc. F1. Pear. Spea. Acc.\nRoBERTa-large\nAdam 66.7 90.4 93.1 92.1 92.0 87.0\n-w ESAM 68.5 90.7 93.3 91.3 90.9 86.6\n-w F_ESAM 68.5 90.9 93.5 91.6 91.0 90.0\n-w GSAM 67.0 89.7 92.6 91.9 91.7 86.3\n-w F_GSAM 70.0 90.7 93.2 92.3 92.0 86.6\n2021). Table 6 shows the results, where F_ESAM\nand F_GSAM refer to the optimizations using our\nstrategy. When evaluating RoBERTa-large on these\ntasks, compared to the vanilla ESAM and GSAM,\nour method can bring a 0.70 average score improve-\nment. This indicates that our Fisher mask strategy\nis not only beneficial to the vanilla SAM, but also\ncan be applied to other efficient SAM variants.\n6.3 Results in Low-resource Scenarios\nPrior works (Chen et al., 2021; Bahri et al., 2022)\nshow that SAM helps more when there is less train-\ning data. Here, we verify how our Fisher mask\nstrategy affects the effectiveness of SAM in low-\nresource scenarios. In practice, we follow Bahri\net al. (2022) and sub-sample the training splits\nfor several GLUE datasets at rates ranging from\n10% to 90%. Notably, due to the space limitation,\nwe only report parts of results on BERT-large and\nRoBERTa-large in Figure 2.\nWe can observe consistent gains from both SAM\nand our FSAM across all sizes of sub-sampled\ntraining sets, which confirms the statement in prior\nwork (Bahri et al., 2022). Moreover, it can also\nbe seen that our FSAM improves the vanilla SAM\nby a large margin in low-resource scenarios, espe-\ncially when there is only 20% training data. More\n4070\nFigure 2: Results at various training data sampling rates. BERT-large and RoBERTa-large models are used. We can\nsee that our proposed module improves SAM by a large margin across all data size regimes.\nFigure 3: Analysis of task generalization. The model is\nfine-tuned on QNLI task and transferred to four different\ntasks. We can see that our FSAM consistently brings\nbetter generalization compared with vanilla SAM.\nspecifically, when fine-tuning the RoBERTa-large\non the STS-B dataset, the relative improvements\nof FSAM are up to 15.0 and 15.1 in terms of accu-\nracy and F1 score, respectively. These results show\nthat our method is more helpful in low-resource\nscenarios.\n6.4 Does FSAM Bring Better Generalization?\nWe prove the effectiveness of our FSAM by large-\nscale experiments as above. Here, to examine\nwhether FSAM indeed brings better generalization,\nwe i) measure the generalization properties (i.e.,\ntask generalization) of different optimizations, and\nii) visualize the generalization of models via the\ntraining loss landscapes.\nTask Generalization. The common wisdom is\nthat models with better generalization would per-\nform better on out-of-domain data (Xu et al., 2021).\nThus, to measure the generalization ability of the\nmodel quantitatively, we follow the experiments\nfrom Xu et al. (2021) and evaluate the performance\nof various fine-tuned models on out-of-domain data.\nIn practice, we first fine-tune RoBERTa-large on\nthe QNLI task (one of GLUE tasks) and then trans-\nfer it to other tasks, i.e., CoLA, MRPC, STS-B and\nRTE. The results of different optimization strate-\ngies are illustrated in Figure 3.\nWe can observe that FSAM consistently outper-\nforms the base Adam and vanilla SAM on different\ntransferred tasks. To be more specific, compared\nwith vanilla SAM, our FSAM brings a 2.37 relative\naverage improvement score on these tasks, indicat-\ning that our method helps more in improving the\ngeneralization of model.\nVisualization of Landscape. Here, we visual-\nize the loss landscapes of RoBERTa-base model\nfine-tuned on CoLA with different optimizers. In\npractice, we follow Li et al. (2018); Zan et al.\n(2022) and show the 3D loss surface results in\nFigure 4 by sampling 25 ×25 points in the range\nof [-1, 1] from random “filter normalized” direc-\ntions (Li et al., 2018). Additionally, following Hao\net al. (2019); He et al. (2021), we also plot the\n1D loss curve in Figure 5 by linear interpolation\nbetween the pretrained model weights before (de-\nnoted as θ0) and after (denoted as θ1) fine-tuning,\ni.e., “θ1 + α·(θ1 −θ0)”, where αis a scalar param-\neter that is ranged from -1 to 1. We can find that\n4071\nFigure 4: The loss surface of RoBERTa-base fine-tuned on CoLA with different optimizers. It can be seen that\nFSAM smooths the loss surface effectively,i.e., improving the model generalization.\nFigure 5: 1D visualization of loss landscapes of\nRoBERTa-base model fine-tuned on different tasks.\nthe landscape of FSAM is much flatter than both\nbase Adam and SAM, especially in the area of low\nloss. These results prove that FSAM can smooth\nthe loss landscape and improve the generalization\nof PLMs effectively.\n7 Conclusion\nIn this paper, we improve the recently-proposed\nSAM optimization method with a novel Fisher\nmask strategy, and propose a new approach FSAM.\nDifferent from the vanilla SAM that adds a con-\nstant perturbation to all parameters, FSAM uses\nthe Fisher information to calculate the Fisher mask\nand further obtains the sparse perturbation. Such a\nmethod can not only reduce the computation cost\nof optimization potentially, but also make the op-\ntimizer focus on the optimization of the important\nsharper parameters. Extensive experiments on five\nPLMs and various language understanding and gen-\neration tasks show that our FSAM consistently im-\nproves the performance of SAM by a large margin\nacross all PLMs and tasks. Additionally, in-depth\nanalysis and discussion demonstrate the robustness\nand universality of FSAM on improving the gener-\nalization of language models.\nLimitations\nIndeed, our work has some potential limitations,\nand we will discuss them in this section. Firstly,\nwe only evaluate the BART on two generation\ntasks with different optimizers, and prove the ef-\nfectiveness of our FSAM optimization method. It\nwould be more valuable to consider other sequence-\nto-sequence PLMs and more generation tasks,\ne.g., fine-tuning T5 (Raffel et al., 2020) on CNN-\nDM (Hermann et al., 2015).\nAdditionally, as aforementioned in Section 1,\nwe do not achieve the actual sparse training in\nthis work, due to the limitation of the hardware.\nSpecifically, to actually accelerate the unstructured\nsparsity (fine-grained sparsity), we need to imple-\nment the relevant sparse matrix calculation using\nthe CUDA API on the recent NVIDIA Ampere\nA100 GPUs (Choquette et al., 2021) equipped with\nSparse Tensor Cores (Pool, 2020) (Notably, al-\nthough there is python API ( i.e., ASP) provided\nby NVIDIA for accelerating the unstructured spar-\nsity, it is only applicable to accelerate the model\nparameter sparsity, but not to the gradient-level\nsparse acceleration in our FSAM scenario). Un-\nfortunately, it is relatively impracticable for us to\ndo that. However, we still believe that FSAM has\ngreat potential to achieve true training acceleration\nin the future, with the development of hardware for\nfine-grained sparse operation.\nAcknowledgements\nWe are grateful to the anonymous reviewers and\nthe area chair for their insightful comments and\nsuggestions. This work was supported in part by\nthe National Natural Science Foundation of China\nunder Grants 62141112, 62076186 and 62225113,\nand in part by the Science and Technology Ma-\njor Project of Hubei Province (Next-Generation AI\nTechnologies) under Grant 2019AEA170. The nu-\nmerical calculations in this paper have been done\non the supercomputing system in the Supercomput-\ning Center of Wuhan University.\n4072\nReferences\nDara Bahri, Hossein Mobahi, and Yi Tay. 2022.\nSharpness-aware minimization improves language\nmodel generalization. In ACL.\nTaylor Berg-Kirkpatrick, David Burkett, and Dan Klein.\n2012. An empirical investigation of statistical signifi-\ncance in nlp. In EMNLP.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nSemEval-2017.\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong.\n2021. When vision transformers outperform resnets\nwithout pre-training or strong data augmentations.\narXiv.\nShamil Chollampatt and Hwee Tou Ng. 2018. A multi-\nlayer convolutional encoder-decoder neural network\nfor grammatical error correction. In AAAI.\nJack Choquette, Wishwesh Gandhi, Olivier Giroux,\nNick Stam, and Ronny Krashinsky. 2021. Nvidia\na100 tensor core gpu: Performance and innovation.\nIEEE Micro.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019b. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn ICLR.\nMichael Collins, Philipp Koehn, and Ivona Ku ˇcerová.\n2005. Clause restructuring for statistical machine\ntranslation. In ACL.\nDaniel Dahlmeier and Hwee Tou Ng. 2012. Better eval-\nuation for grammatical error correction. In NAACL.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Investi-\ngating projection in naturally occurring discourse. In\nproceedings of Sinn und Bedeutung.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nLiang Ding, Longyue Wang, Xuebo Liu, Derek F Wong,\nDacheng Tao, and Zhaopeng Tu. 2021. Progres-\nsive multi-granularity training for non-autoregressive\ntranslation. In Findings of the ACL.\nLiang Ding, Longyue Wang, Shuming Shi, Dacheng\nTao, and Zhaopeng Tu. 2022. Redistributing low-\nfrequency words: Making the most of monolingual\ndata in non-autoregressive translation. In ACL.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third International Workshop on Paraphrasing\n(IWP2005).\nJiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou,\nLiangli Zhen, Rick Siow Mong Goh, and Vincent YF\nTan. 2021. Efficient sharpness-aware minimization\nfor improved training of neural networks. arXiv.\nRonald A Fisher. 1922. On the mathematical founda-\ntions of theoretical statistics. Philosophical transac-\ntions of the Royal Society of London. Series A, con-\ntaining papers of a mathematical or physical charac-\nter.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and\nBehnam Neyshabur. 2020. Sharpness-aware mini-\nmization for efficiently improving generalization. In\nICLR.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nWilliam B Dolan. 2007. The third pascal recognizing\ntextual entailment challenge. In ACL.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of bert.\nIn EMNLP.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. In ICLR.\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng\nDing, Liying Cheng, Jiawei Low, Lidong Bing, and\nLuo Si. 2021. On the effectiveness of adapter-based\ntuning for pretrained language model adaptation. In\nACL.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NeurIPS.\nSepp Hochreiter and Jürgen Schmidhuber. 1994. Sim-\nplifying neural nets by discovering flat minima. In\nNeurIPS.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. TACL.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-\ncedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.\n2016. On large-batch training for deep learning: Gen-\neralization gap and sharp minima. arXiv.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\n4073\nAlex Krizhevsky, Geoffrey Hinton, et al. 2009. Learn-\ning multiple layers of features from tiny images.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classification with deep convo-\nlutional neural networks.\nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and\nIn Kwon Choi. 2021. Asam: Adaptive sharpness-\naware minimization for scale-invariant learning of\ndeep neural networks. In ICML.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In ICLR.\nBeatrice Laurent and Pascal Massart. 2000. Adaptive es-\ntimation of a quadratic functional by model selection.\nAnnals of Statistics, pages 1302–1338.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In AAAI.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In ACL.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and\nTom Goldstein. 2018. Visualizing the loss landscape\nof neural nets. In NeurIPS.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out.\nTao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev,\nand Martin Jaggi. 2019. Dynamic model pruning\nwith feedback. In ICLR.\nXuebo Liu, Longyue Wang, Derek F Wong, Liang Ding,\nLidia S Chao, and Zhaopeng Tu. 2021. Understand-\ning and improving encoder layer fusion in sequence-\nto-sequence learning. In ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv.\nPeng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai\nSun, Rongrong Ji, and Dacheng Tao. 2022. Make\nsharpness-aware minimization stronger: A sparsified\nperturbation approach. arXiv preprint.\nRafael Müller, Simon Kornblith, and Geoffrey E Hinton.\n2019. When does label smoothing help? In NeurIPS.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In EMNLP.\nBehnam Neyshabur, Srinadh Bhojanapalli, David\nMcAllester, and Nati Srebro. 2017. Exploring gener-\nalization in deep learning. In NeurIPS.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The conll-2014 shared task on\ngrammatical error correction. In CoNLL-2014.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. In\nNAACL.\nJeff Pool. 2020. Accelerating sparsity in the nvidia\nampere architecture. GTC 2020.\nWeizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao,\nBartuer Zhou, Biao Cheng, Daxin Jiang, Jiusheng\nChen, Ruofei Zhang, et al. 2021. Prophetnet-x:\nLarge-scale pre-training models for english, chinese,\nmulti-lingual, dialog, and code generation. In ACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In ACL.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overfitting. JMLR.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou,\nand Daniel Cer. 2022. SPoT: Better frozen model\nadaptation through soft prompt transfer. In ACL.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018a. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In EMNLP.\nBing Wang, Liang Ding, Qihuang Zhong, Ximing Li,\nand Dacheng Tao. 2022. A contrastive cross-channel\ndata augmentation framework for aspect-based senti-\nment analysis. arXiv.\n4074\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\nbig. 2018b. Switchout: an efficient data augmen-\ntation algorithm for neural machine translation. In\nEMNLP.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv.\nDongxian Wu, Shu-Tao Xia, and Yisen Wang. 2020.\nAdversarial weight perturbation helps robust general-\nization. In NeurIPS.\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,\nBaobao Chang, Songfang Huang, and Fei Huang.\n2021. Raise a child in large language model: To-\nwards effective and generalizable fine-tuning. In\nEMNLP.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS.\nChangtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng\nLiu, and Dacheng Tao. 2022. On the complemen-\ntarity between pre-training and random-initialization\nfor resource-rich machine translation. In COLING.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In ICLR.\nZheng Zhang, Liang Ding, Dazhao Cheng, Xuebo Liu,\nMin Zhang, and Dacheng Tao. 2022. Bliss: Robust\nsequence-to-sequence learning via self-supervised\ninput representation. arXiv preprint.\nYaowei Zheng, Richong Zhang, and Yongyi Mao. 2021.\nRegularizing neural networks via adversarial model\nperturbation. In CVPR.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2022. E2s2: Encoding-enhanced\nsequence-to-sequence pretraining for language un-\nderstanding and generation. arXiv.\nQihuang Zhong, Fanzhou Zeng, Fei Liao, Juhua Liu,\nBo Du, and Jedi S Shang. 2021. Joint image and\nfeature adaptative attention-aware networks for cross-\nmodality semantic segmentation. Neural Computing\nand Applications.\nJuntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui,\nHartwig Adam, Nicha C Dvornek, James s Duncan,\nTing Liu, et al. 2021. Surrogate gap minimization\nimproves sharpness-aware training. In ICLR.\n4075\nA Appendix\nA.1 Missing Proof\nLemma 1.For any two vectorsx,y ∈Rd, and any\nscalar α> 1, we have the following inequality\n⟨x,y⟩≤ α2\n2 ||x||2 + 1\n2α2 ||y||2\nProof.\nRHS = α2\n2\nd∑\nj=1\n(x)2\nj + 1\n2α2\nd∑\nj=1\n(y)2\nj\n≥\nd∑\nj=1\n2\n√\nα2\n2 ·(x)2\nj · 1\n2α2 (y)2\nj = LHS\nLemma 2. We have the following inequality:\n⟨∇f(xt),γt\nb\n∑\nu∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )⊙\n( 1√\nˆvt−1\n− 1√ˆvt\n)⟩≤ γtG2|| 1√\nˆvt−1\n− 1√ˆvt\n||1\nProof.\n⟨∇f(xt),γt\nb\n∑\ni∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n⊙( 1√\nˆvt−1\n− 1√ˆvt\n)⟩\n≤γt\nb\nd∑\nj=1\n|(∇f(xt))(j)|\n×|\n∑\ni∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n⊙( 1√\nˆvt−1\n− 1√ˆvt\n)(j)|\n≤γtG\nb\nd∑\nj=1\n∑\ni∈B\n|∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n⊙( 1√\nˆvt−1\n− 1√ˆvt\n)(j)|\n≤γtG2\nb\nd∑\nj=1\n∑\ni∈B\n|( 1√\nˆvt−1\n− 1√ˆvt\n)(j)|\n≤γtG2||( 1√\nˆvt−1\n− 1√ˆvt\n)(j)||1\nLemma 3. With the term defined before, we have\nthe following inequality\n⟨∇f(xt),γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n−γt\nb\n∑\ni∈B\n∇fi(xt\n+ ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√\nˆvt−1\n⟩\n≤ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + 2µ2γtL2ρ2\nϵ ,\nwhere the µ> 1 is a undetermined scalar.\nProof.\nLHS = ⟨∇f(xt) ⊙\n√\n1√\nˆvt−1\n,\nγt\nb\n∑\ni∈B\n(∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n−∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )) ⊙\n√\n1√\nˆvt−1\n⟩\n4076\nBy using Lemma 1, we have\nLHS ≤µ2γt\n2b2 ||\n∑\ni∈B\n(∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n−∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )) ⊙\n√\n1√\nˆvt−1\n||2\n+ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n≤µ2γt\n2b\n∑\n||(∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n−∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )) ⊙\n√\n1√\nˆvt−1\n||2\n+ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n≤µ2γt\n2b\n∑\n||∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n−∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )||2\n·||\n√\n1√\nˆvt−1\n||2\n∞+ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n≤ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + µ2γtL2ρ2\n2bδ ·\n∑\n||∇f(xt) ⊙mt\n||∇f(xt)|| −\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ||2\n≤ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + µ2γtL2ρ2\n2bδ ·\n∑\n||( ∇f(xt)\n||∇f(xt)||−\n∑ ∇fi(xt)\n||∑ ∇fi(xt)||) ⊙mt||2\n≤ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + µ2γtL2ρ2\n2bδ ·\n∑\n|| ∇f(xt)\n||∇f(xt)||−\n∑ ∇fi(xt)\n||∑ ∇fi(xt)||||2 ·||mt||2\n∞\n≤ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + µ2γtL2ρ2\n2bδ ·\n∑\n|| ∇f(xt)\n||∇f(xt)||−\n∑ ∇fi(xt)\n||∑ ∇fi(xt)||||2\n≤ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + 2µ2γtL2ρ2\nδ\nThe µ> 0 is the term to be determined.\nLemma 4. We have the following inequality:\nE⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n⟩≤− γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+ E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + γtβ2L2ρ2\n2δ ,\nwhere the β >1 is a undetermined scalar.\nProof.\nLHS = −γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E⟨∇f(xt),γt\nb\n∑\ni∈B\n(∇f(xt)\n−∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )) ⊙ 1√\nˆvt−1\n⟩\n= −γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E⟨∇f(xt),γt\nb\n∑\ni∈B\n(∇fi(xt)\n−∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )) ⊙ 1√\nˆvt−1\n⟩\n4077\nBy using the Lemma 1, we have\nLHS ≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+Eγtβ2\n2 ||1\nb\n∑\ni∈B\n(∇fi(xt)\n−∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )) ⊙\n√\n1√\nˆvt−1\n||2\n≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+Eγtβ2\n2δ ||1\nb\n∑\ni∈B\n(∇fi(xt)\n−∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| ))||2\n≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+Eγtβ2\n2bδ\n∑\ni∈B\n||(∇fi(xt)\n−∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| ))||2\nBy the definition of L-smooth, we have\nLHS ≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+γtβ2L2ρ2\n2bδ E\n∑\ni∈B\n|| ∇f(xt)\n||∇f(xt)||⊙mt||2\n≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+γtβ2L2ρ2\n2δ\nLemma 5. We have the following inequality:\nL\n2 E||xt+1 −xt||2 ≤γ2\ntL\n2 [31 +α\nαδ\n·(E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + Lρ2\nδ )\n+ σ2\nbδ + (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\nProof.\nL\n2 E||xt+1 −xt||2\n= L\n2 E||γt\nb\n∑\n(∇fi(xt\n+ ρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )) ⊙ 1√ˆvt\n||2\n= Lγ2\nt\n2 E||1\nb\n∑\n(∇fi(xt + ρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n⊙ 1√\nˆvt−1\n) +1\nb\n∑\n(∇fi(xt + ρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ))\n⊙( 1√ˆvt\n− 1√\nˆvt−1\n)||2\nBy using the Lemma 1, we can obtain\nL\n2 E||xt+1 −xt||2\n≤Lγ2\nt\n2 [(1 +1\nα)E||1\nb\n∑\n(∇fi(xt+\nρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√\nˆvt−1\n)||2\n+ (1 +α)E||1\nb\n∑\n(∇fi(xt+\nρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )) ⊙( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n≤Lγ2\nt\n2 [(1 +1\nα)E||1\nb\n∑\n(∇fi(xt+\nρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√\nˆvt−1\n)||2\n+ (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n4078\nSimilar to the proof of Lemma 3, we have\nL\n2 E||xt+1 −xt||2\n≤Lγ2\nt\n2 [(1 +1\nα)E||1\nb\n∑\n(∇fi(xt\n+ ρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙\n√\n1√\nˆvt−1\n)||2\n·||\n√\n1√\nˆvt−1\n||2\n∞\n+ (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n≤Lγ2\nt\n2 [(1 +α\nαδ )E||1\nb\n∑\n(∇fi(xt\n+ ρ\nb\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙\n√\n1√\nˆvt−1\n)||2\n+ (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\nBy splitting the term E||1\nb\n∑(∇fi(xt\n+ ρ\nb\n∑∇fi(xt)⊙mt\n||∑∇fi(xt)|| ) ⊙\n√\n1√\nˆvt−1\n), we have\nLHS ≤Lγ2\nt\n2 [3(1 +α\nαδ )E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+ ||(1\nb\n∑\n∇fi(xt) −∇f(xt)) ⊙\n√\n1√\nˆvt−1\n||2\n+ ||(1\nb\n∑\n(∇fi(xt + ρ\n∑ ∇fi(xt)\n||∑ ∇fi(xt)||) −∇fi(xt))\n⊙\n√\n1√\nˆvt−1\n||2 + (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n≤Lγ2\nt\n2 [3(1 +α\nαδ )E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + σ2\nbδ\n+ ||(1\nb\n∑\n(∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) −∇fi(xt))\n⊙\n√\n1√\nˆvt−1\n||2 + (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n≤Lγ2\nt\n2 [3(1 +α\nαδ )E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + σ2\nbδ\n+ 1\nδ||(1\nb\n∑\n(∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n−∇fi(xt))||2 + (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n≤γ2\ntL\n2 [31 +α\nαδ (E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + Lρ2\nδ )\n+ σ2\nbδ + (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\nTheorem 2. By using the definition of L-smooth\nand the Lemma 2, 3, 4 and 5, we have\n1\nT\nT−1∑\nt=0\nE||∇f(xt)||2\n≤2Gf(x0) −f∗\nγtT\n+ 20GL2ρ2\nδ + 2G3\nT d(1\nδ −1\nG)\n+ 4GγtL\nδ\nLρ2\nδ + 4GγtL\nδ\nσ2\nbδ\n+ 4γtLG3\nT d(G2 −δ2)\nProof.\nf(xt+1) ≤f(xt) +⟨∇f(xt),xt+1 −xt⟩\n+L\n2 ||xt+1 −xt||2\n4079\nBy re-range it, we obtain\nf(xt+1) −f(xt)\n≤⟨∇f(xt),xt+1 −xt⟩+ L\n2 ||xt+1 −xt||2\n=⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt\n+ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√ˆvt\n⟩+ L\n2 ||xt+1 −xt||2\n=⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt\n+ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√\nˆvt−1\n⟩\n+⟨∇f(xt),γt\nb\n∑\ni∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n⊙( 1√\nˆvt−1\n− 1√ˆvt\n)⟩+ L\n2 ||xt+1 −xt||2\n=⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n⟩+ ⟨∇f(xt),γt\nb\n∑\ni∈B\n∇fi(xt+\nρ∇f(xt) ⊙mt\n||∇f(xt)|| ) ⊙ 1√\nˆvt−1\n−γt\nb\n·\n∑\ni∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√\nˆvt−1\n⟩\n+⟨∇f(xt),γt\nb\n∑\ni∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| )\n⊙( 1√\nˆvt−1\n− 1√ˆvt\n)⟩+ L\n2 ||xt+1 −xt||2\nFrom the Lemma 2, we have the following inequal-\nity. Specifically,\nf(xt+1) −f(xt)\n≤⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n⟩\n+ ⟨∇f(xt),γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n−γt\nb\n∑\ni∈B\n∇fi(xt + ρ\n∑ ∇fi(xt) ⊙mt\n||∑ ∇fi(xt)|| ) ⊙ 1√\nˆvt−1\n⟩\n+ γtG2|| 1√\nˆvt−1\n− 1√ˆvt\n||1 + L\n2 ||xt+1 −xt||2\nFrom the Lemma 3, we have the following inequal-\nity. Specifically,\nf(xt+1) −f(xt)\n≤⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n⟩+ L\n2 ||xt+1 −xt||2\n+ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + 2µ2γtL2ρ2\nδ\n+γtG2|| 1√\nˆvt−1\n− 1√ˆvt\n||1 + L\n2 ||xt+1 −xt||2\nBy taking the expectation, we have the following\ninequality. Specifically,\nEf(xt+1) −f(xt)\n≤E⟨∇f(xt),−γt\nb\n∑\ni∈B\n∇fi(xt + ρ∇f(xt) ⊙mt\n||∇f(xt)|| )\n⊙ 1√\nˆvt−1\n⟩+ L\n2 E||xt+1 −xt||2\n+ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + 2µ2γtL2ρ2\nδ\n+γtG2|| 1√\nˆvt−1\n− 1√ˆvt\n||1\nFrom the Lemma 4, we have the following inequal-\nity. Specifically,\nEf(xt+1) −f(xt)\n≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + γtβ2L2ρ2\n2δ\n+L\n2 E||xt+1 −xt||2 + 2µ2γtL2ρ2\nδ\n+ γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+γtG2|| 1√\nˆvt−1\n− 1√ˆvt\n||1\n4080\nFrom the Lemma 5, we can obtain\nEf(xt+1) −f(xt)\n≤−γt||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+E γt\n2β2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + γtβ2L2ρ2\n2δ\n+2µ2γtL2ρ2\nδ + γt\n2µ2 ||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+γtG2|| 1√\nˆvt−1\n− 1√ˆvt\n||1\n+γ2\ntL\n2 [31 +α\nαδ (E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + Lρ2\nδ )\n+σ2\nbδ + (1 +α)G2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2]\n= −γt(1 − 1\n2µ2 − 1\n2β2 −3µL(1 +α)\n2α2 )\n·E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n+2µ2γtL2ρ2\nδ + γtG2E|| 1√ˆvt\n− 1√\nˆvt−1\n||1\n+γtβ2L2ρ2\n2δ + 3γ2\ntL(1 +α)\n2αδ (Lρ2\nδ + σ2\nbδ)\n+γ2\ntL(1 +α)G2\n2 E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2\nSet the µ2 = β2 = 4,α = 3and set the γtL\nδ ≤1\n8 ,\nwe can simplify the inequality\nEf(xt+1) −f(xt)\n≤−γt\n2 E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2 + 8γtL2ρ2\nδ\n+ γtG2E||( 1√\nˆvt−1\n− 1√ˆvt\n)||1\n+ 2γtL2ρ2\nδ + 2γ2\ntL\nδ (Lρ2\nδ + σ2\nbδ)\n+ 2γ2\ntLG2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2\nNote that the 1√ˆvt\nis bounded, we re-arrange the\ninequality and achieve\nγt\n2GE||∇f(xt)||2 ≤γt\n2 E||∇f(xt) ⊙\n√\n1√\nˆvt−1\n||2\n≤−Ef(xt+1) +Ef(xt) +8γtL2ρ2\nδ\n+ γtG2E||( 1√\nˆvt−1\n− 1√ˆvt\n)||1\n+ 2γtL2ρ2\nδ + 2γ2\ntL\nδ (Lρ2\nδ + σ2\nbδ)\n+ 2γ2\ntLG2E||( 1√ˆvt\n− 1√\nˆvt−1\n)||2\nWe sum up it and we obtain\n1\nT\nT−1∑\nt=0\nE||∇f(xt)||2 ≤2GEf(x0) −Ef(xt+1)\nγtT\n+ 16GL2ρ2\nδ + 2G3\nT E\nT−1∑\nt=0\n|| 1√\nˆvt−1\n− 1√ˆvt\n||1\n+ 4GL2ρ2\nδ + 4GγtL\nδ (Lρ2\nδ + σ2\nbδ)\n+ 4γtLG3\nT E\nT−1∑\nt=0\n|| 1√ˆvt\n− 1√\nˆvt−1\n||2\n≤2Gf(x0) −f∗\nγtT + 16GL2ρ2\nδ\n+ 2G3\nT d(1\nδ −1\nG) +4GL2ρ2\nδ\n+ 4GγtL\nδ (Lρ2\nδ + σ2\nbδ) +4γtLG3\nT d(G2 −δ2)\n= 2Gf(x0) −f∗\nγtT + 20GL2ρ2\nδ\n+ 2G3\nT d(1\nδ −1\nG) +4GγtL\nδ\nLρ2\nδ\n+ 4GγtL\nδ\nσ2\nbδ + 4γtLG3\nT d(G2 −δ2)\nTheorem 3.With probability 1 −δover the choice\nof training set S∼D , we have the following in-\nequality.\nLD(w) ≤ max\n||ϵ||2≤ρ\nLS(w+ ϵ)\n+\n√klog(1 +||w||2\n2\nρ2 (1 +\n√\nlog n\nk )2) + 4 logn\nδ + O(1)\nn−1\nThe n = |S|, k is the number of weight w, and\nassuming that LD(w) ≤Eϵi∼N(0,ρ)[LD(w+ ϵ)]\nProof. The proof is mainly based on PAC-Bayesian\nGeneralization Bound theorem. We start by any\n4081\nprior Pover parameters with probability 1 −δfor\nany posterior distribution Q, we have\nEw∼W[LQ](w)] ≤Ew∼W[LS](w)]\n+\n√\nKL(Q||P) + logn\nδ\n2(n−1)\nAssume ϵ ∼N (0,σ), the ||ϵ||2\n2 follows the Chi-\nsquare distribution. From the Lemma 1 in (Laurent\nand Massart, 2000), we have the following inequal-\nity for any positive t:\nP(||ϵ||2\n2 −kσ2 ≥2σ2√\nkt+ 2tσ2) ≤exp(−t)\nCombine the above function and subtract the same\nconstant Con both side, following the assumption\nin (Foret et al., 2020), we finish the proof.\nA.2 Details of Tasks and Datasets\nAs mention in Section 4, we conduct extensive\nexperiments on parts of tasks from GLUE and Su-\nperGLUE. In addition, two widely-used generation\ntasks are also used in this work. Here, we introduce\nthe descriptions of the used tasks and datasets in de-\ntail. Firstly, we present the statistics of all datasets\nin Table 7. Then, each task is described as:\nCoLA. Corpus of Linguistic Acceptabil-\nity (Warstadt et al., 2019) is a binary single-\nsentence classification task to determine whether a\ngiven sentence is linguistically “acceptable”.\nMRPC. Microsoft Research Paraphrase Cor-\npus (Dolan and Brockett, 2005) is a task to predict\nwhether two sentences are semantically equivalent.\nSTS-B. Semantic Textual Similarity (Cer et al.,\n2017) is a task to predict how similar two sentences\nare on a 1-5 scale in terms of semantic meaning.\nRTE. Recognizing Textual Entailment (Gi-\nampiccolo et al., 2007), given a premise and a hy-\npothesis, is a task to predict whether the premise\nentails the hypothesis.\nQNLI. Question Natural Language Inference\nis a binary classification task constructed from\nSQuAD (Rajpurkar et al., 2016), which aims to\npredict whether a context sentence contains the\nanswer to a question sentence.\nCB. CommitmentBank (De Marneffe et al.,\n2019) is a task that can be framed as three-class\ntextual entailment on a corpus of 1,200 naturally\noccurring discourses.\nBoolQ. Boolean Question (Clark et al., 2019a)\nis a question answering task where each sample\nconsists of a short passage and a yes/no question\nabout the passage.\nWiC. Word-in-Context (Pilehvar and Camacho-\nCollados, 2019) is a word sense disambiguation\ntask that aims to predict whether the word is used\nwith the same sense in sentence pairs.\nWSC. Winograd Schema Challenge (Levesque\net al., 2012) is a co-reference resolution task which\naims to determine the correct refer-rent of the pro-\nnoun from among the provided choices.\nXSUM. The Extreme Summarization\ndataset (Narayan et al., 2018) is one of ab-\nstractive Summarization task that aims to convert\nthe given document into a short and adequate\nsummary in the same language.\nCoNLL2014. CoNLL2014 (Ng et al., 2014) is a\npopular grammatical error correction task that aims\nto rewrite the input sentence with grammatical er-\nrors into the corresponding correct sentence, where\nthe original and target sentences have the similar\nsentence lengths.\nA.3 Hyper-parameters of Fine-tuning\nIn this paper, we fine-tune four different large-scale\nPLMs with our FSAM on the tasks of GLUE and\nSuperGLUE, including BERT-large ( ∼340M) 6,\nELECTRA-large (∼340M) 7, ALBERT-xxlarge-\nv2 ( ∼223M) 8 and RoBERTa-large (∼355M) 9.\nAdditionally, the BART-large (∼406M) 10 is used\nfor the generation tasks. The training epochs/steps,\nbatch size, learning rate and warmup steps are listed\nin Table 8 and Tabel 10. Notably, the maximum\nsequence length of language understanding tasks\nis set as 128/256. For two generation tasks, we\nempirically set the minimum and maximum length\nof the XSUM dataset as 10 and 60, and closely\nfollow Chollampatt and Ng (2018) to preprocess\nthe data of CoNLL2014.\nA.4 Training Curves\nIn this sub-section, we visualize the training curves\nof Adam, SAM and FSAM in detail. Specifically,\nFigure 6 shows the evaluation metrics v.s. train-\ning epochs. The metric curves prove that FSAM\nboosts the performance effectively during the train-\ning, which shows the effectiveness of FSAM.\n6https://huggingface.co/bert-large-cased\n7https://huggingface.co/google/\nelectra-large-discriminator\n8https://huggingface.co/albert-xxlarge-v2\n9https://huggingface.co/roberta-large\n10https://huggingface.co/facebook/bart-large\n4082\nTable 7: Data statistics of all used tasks in this paper.\nTask Description #Train #Dev #Class\nGLUE\nCoLA linguistic acceptability classification 8.5K 1,042 2\nMRPC paraphrase classification 3.7K 409 2\nSTS-B semantic textual similarity 5.7K 1,501 -\nRTE natural language inference 2.5K 278 2\nQNLI natural language inference 104K 5,464 2\nSuperGLUE\nBoolQ question answering 9.4K 3,270 2\nCB natural language inference 250 57 2\nWiC word sense disambiguation 6K 638 2\nWSC coreference resolution 554 104 2\nGeneration XSUM abstractive summarization 204K 11,332 -\nCoNLL2014 grammatical error correction 1.3M 5,448 -\nTable 8: Hyper-parameters settings for BART model on\nthe generation tasks.\nSetting XSUM CoNLL2014\nLearning Rate 3e-5 2e-5\nBatch Size 56 800\nTraining Steps 15,000 15,000\nWarmup Steps 500 500\nGPUs 8 4\nA.5 More Results\nIn addition to the results in Table 1 with the momen-\ntum as 0.9, we also conduct the experiments with\nthe momentum as 0 (i.e., β1 = 0in Algorithm 1)\nto evaluate the influence of the adaptive learning\nrate. In practice, we evaluate the performance on\nseveral downstream tasks upon two base optimizers\n(Adam and AMSGrad) and two PLMs (RoBERTa-\nlarge and RoBERTa-base). Table 9 shows the re-\nsults. When there is no momentum term, FSAM\nalso achieves better performance against the vanilla\nSAM and base optimizers. These results prove the\nuniversality of FSAM in various scenarios.\n4083\nFigure 6: Accuracy on the dev set v.s. training epochs. We show some representative results on the four pretrained\nmodels. We can observe that our FSAM achieves the best performance among all tasks.\nTable 9: Results of RoBERTa-large and RoBERTa-base fine-tuned with different optimizers (β1 = 0). We can\nobserve that FSAM consistently outperforms the vanilla SAM and base optimizer among all settings.\nMethod\nRoBERTa-base, Adam RoBERTa-large, Adam\nCoLA MRPC STS-B RTE A VG.CoLA MRPC STS-B RTE A VG.Mcc. Acc. F1. Pear. Spea. Acc. Mcc. Acc. F1. Pear. Spea. Acc.\nAdam 61.3 87.5 90.6 90.4 90.2 78.3 83.05 67.0 89.0 91.9 91.9 91.6 88.4 86.63\nAdam+SAM 61.6 88.2 91.6 90.5 90.2 79.4 83.58 68.6 90.4 93.1 90.7 90.4 88.1 86.88\nAdam+FSAM 63.2 88.5 92.0 90.6 90.3 82.7 84.55 69.0 90.4 93.2 91.6 91.3 88.8 87.38\nMethod\nRoBERTa-base, AMSGrad RoBERTa-large, AMSGrad\nCoLA MRPC STS-B RTE A VG.CoLA MRPC STS-B RTE A VG.Mcc. Acc. F1. Pear. Spea. Acc. Mcc. Acc. F1. Pear. Spea. Acc.\nAMSGrad 60.1 88.5 91.6 90.3 90.2 79.1 83.30 63.8 89.7 92.4 90.0 90.4 87.4 85.62\nAMSGrad+SAM 60.7 89.2 92.2 90.2 89.9 80.2 83.73 68.5 90.0 92.8 91.6 91.2 87.0 86.85\nAMSGrad+FSAM62.0 88.7 91.9 90.5 90.4 79.8 83.88 68.5 90.2 92.9 91.6 91.0 88.1 87.05\n4084\nTable 10: Hyper-parameters settings for different pretrained models on the language understanding tasks. We set the\nbatch size to 16 for all settings. These settings are selected in best practice. Note that we apply these settings to\nfine-tune both large and small pretrained language models.\nModel Dataset Learning Rate Training Epochs/Steps Warmup Ratio/Steps\nBERT\nCoLA 2e-5 3 epochs 10%\nMRPC 2e-5 3 epochs 10%\nSTS-B 4e-5 3 epochs 10%\nRTE 4e-5 3 epochs 10%\nBoolQ 2e-5 10 epochs 10%\nCB 3e-5 20 epochs 10%\nWiC 2e-5 10 epochs 10%\nWSC 1e-5 20 epochs 10%\nELECTRA\nCoLA 1e-5 3 epochs 10%\nMRPC 3e-5 3 epochs 10%\nSTS-B 3e-5 10 epochs 10%\nRTE 2e-5 10 epochs 10%\nBoolQ 2e-5 10 epochs 10%\nCB 3e-5 20 epochs 10%\nWiC 2e-5 10 epochs 10%\nWSC 1e-5 20 epochs 10%\nALBERT\nCoLA 2e-5 3 epochs 10%\nMRPC 3e-5 3 epochs 10%\nSTS-B 3e-5 3 epochs 10%\nRTE 5e-5 3 epochs 10%\nBoolQ 1e-5 10 epochs 10%\nCB 4e-5 20 epochs 10%\nWiC 3e-5 10 epochs 10%\nWSC 1e-5 20 epochs 10%\nRoBERTa\nCoLA 1e-5 2668 steps 160 steps\nMRPC 1e-5 1148 steps 68 steps\nSTS-B 2e-5 1799 steps 107 steps\nRTE 2e-5 1018 steps 61 steps\nBoolQ 1e-5 10 epochs 10%\nCB 2e-5 20 epochs 10%\nWiC 2e-5 10 epochs 10%\nWSC 1e-5 20 epochs 10%\n4085"
}