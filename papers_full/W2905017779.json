{
    "title": "Kernel Transformer Networks for Compact Spherical Convolution",
    "url": "https://openalex.org/W2905017779",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5103041770",
            "name": "Yu-Chuan Su",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5012765543",
            "name": "Kristen Grauman",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2187966737",
        "https://openalex.org/W2565955132",
        "https://openalex.org/W2121169511",
        "https://openalex.org/W2798665861",
        "https://openalex.org/W2342662179",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W2963339238",
        "https://openalex.org/W2895640967",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2766306266",
        "https://openalex.org/W603908379",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W2964244417",
        "https://openalex.org/W2593745480",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W566730006",
        "https://openalex.org/W2885054293",
        "https://openalex.org/W2895696451",
        "https://openalex.org/W2605344185",
        "https://openalex.org/W2751706698",
        "https://openalex.org/W2611836059",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2963691181",
        "https://openalex.org/W2556362500",
        "https://openalex.org/W2103189262",
        "https://openalex.org/W2045556253",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W2766258444",
        "https://openalex.org/W2601564443",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963389277",
        "https://openalex.org/W2796422723",
        "https://openalex.org/W2890636441",
        "https://openalex.org/W2738767782",
        "https://openalex.org/W2963609011",
        "https://openalex.org/W2604274373",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W2979240068"
    ],
    "abstract": "Ideally, 360° imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. In this work, we present the Kernel Transformer Network (KTN). KTNs efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360° images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360° image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN's accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint.",
    "full_text": "Kernel Transformer Networks for Compact Spherical Convolution\nYu-Chuan Su\nThe University of Texas at Austin\nKristen Grauman\nFacebook AI Research\nThe University of Texas at Austin\nAbstract\nIdeally, 360◦ imagery could inherit the deep convolu-\ntional neural networks (CNNs) already trained with great\nsuccess on perspective projection images. However, exist-\ning methods to transfer CNNs from perspective to spheri-\ncal images introduce signiﬁcant computational costs and/or\ndegradations in accuracy. We present the Kernel Trans-\nformer Network (KTN) to efﬁciently transfer convolution\nkernels from perspective images to the equirectangular pro-\njection of 360◦ images. Given a source CNN for perspective\nimages as input, the KTN produces a function parameter-\nized by a polar angle and kernel as output. Given a novel\n360◦ image, that function in turn can compute convolutions\nfor arbitrary layers and kernels as would the source CNN\non the corresponding tangent plane projections. Distinct\nfrom all existing methods, KTNs allow model transfer: the\nsame model can be applied to different source CNNs with\nthe same base architecture. This enables application to mul-\ntiple recognition tasks without re-training the KTN. Validat-\ning our approach with multiple source CNNs and datasets,\nwe show that KTNs improve the state of the art for spherical\nconvolution. KTNs successfully preserve the source CNN’s\naccuracy, while offering transferability, scalability to typi-\ncal image resolutions, and, in many cases, a substantially\nlower memory footprint1.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n1. Introduction\nThe 360◦ camera is an increasingly popular technol-\nogy gadget, with sales expected to grow by 1500% before\n2022 [42]. As a result, the amount of360◦ data is increasing\nrapidly. For example, users uploaded more than a million\n360◦ videos to Facebook in less than 3 years [2]. Besides\nvideography, 360◦ cameras are also gaining attention for\nself-driving cars, automated drones, and VR/AR. Because\nalmost any application depends on semantic visual features,\nthis rising trend prompts an unprecedented need for visual\nrecognition algorithms on 360◦ images.\nToday’s wildly successful recognition CNNs are the re-\nsult of tremendous data curation and annotation effort [6,\n1Code and data available at http://vision.cs.utexas.edu/projects/ktn/\n360◦ image\nfΩ1\nfΩ2\n( )\n( )\n(C) Proposed KTN\n(A) Apply directly\n(B) Apply on tangent planes\nFigure 1: Our goal is to transfer CNNs trained on planar\nimages to 360◦ images. Common approaches either (A) ap-\nply CNNs directly on the equirectangular projection of a\n360◦ image or (B) project the content to tangent planes and\napply the models on the tangent planes. In contrast, Kernel\nTransformer Network (KTN) adapts the kernels in CNNs to\naccount for the distortion in 360◦ images.\n14, 16, 31, 36, 41], but they all assume perspective pro-\njection imagery. How can they be repurposed for 360◦\ndata? Existing methods often take an off-the-shelf model\ntrained on perspective images and either 1) apply it repeat-\nedly to multiple perspective projections of the 360◦ im-\nage [10,38,40,43] or 2) apply it once to a single equirectan-\ngular projection [19,30]. See Fig. 1(A,B). These two strate-\ngies, however, have severe limitations. The ﬁrst is expensive\nbecause it has to project the image and apply the recogni-\ntion model repeatedly. The second is inaccurate because the\nvisual content is distorted in equirectangular projection.\nTo overcome these challenges, recent work designs CNN\nmodels speciﬁcally for spherical data [11, 12, 15, 37, 46].\nBroadly speaking, they pursue one of three approaches.\nThe ﬁrst adapts the network architecture for equirectangu-\nlar projection and trains kernels of variable size to account\nfor its distortions [37]. While accurate, this approach suf-\nfers from signiﬁcant model bloat. The second approach\ninstead adapts the kernels on the sphere, resampling the\nkernels or projecting their tangent plane features [12, 46].\nWhile allowing kernel sharing and hence smaller mod-\n1\narXiv:1812.03115v2  [cs.CV]  9 Apr 2019\nels, this approach degrades accuracy—especially for deeper\nnetworks—due to an implicit interpolation assumption, as\nwe will explain below. The third approach deﬁnes convo-\nlution in the spectral domain [11, 15], which has signiﬁcant\nmemory overhead and thus far limited applicability to real-\nworld data. All of the above require retraining to handle a\nnew recognition task.\nIn light of these shortcomings, we propose the Ker-\nnel Transformer Network (KTN). The KTN adapts source\nCNNs trained on perspective images to 360◦ images. In-\nstead of learning a new CNN on 360◦ images for a speciﬁc\ntask, KTN learns a function that takes a kernel in the source\nCNN as input and transforms it to be applicable to a 360◦\nimage in its equirectangular projection. See Fig. 1 (C). The\nfunction accounts for the distortion in 360◦ images, return-\ning different transformations depending on both the polar\nangle θ and the source kernel. The model is trained to re-\nproduce the outputs of the source CNN on the perspective\nprojection for each tangent plane on an arbitrary 360◦ im-\nage. Hence, KTN learns to behave similarly to the source\nCNN while avoiding repeated projection of the image.\nKey highlights of the proposed KTN are its transferabil-\nity and compactness—both of which owe to our function-\nbased design. Once trained for a base architecture, the same\nKTN can transfer multiple source CNNs to 360◦ images.\nFor example, having trained a KTN for VGG [36] on Ima-\ngeNet classiﬁcation, we can transfer the same KTN to run\na VGG-based Pascal object detector on 360◦ panoramas.\nThis is possible because the KTN takes the source CNN as\ninput rather than embed the CNN kernels into its own pa-\nrameters (unlike [11,12,15,37,46]). Furthermore, since the\nKTN factorizes source kernels from transformations, it is\nimplementable with a lightweight network (e.g., increasing\nthe footprint of a VGG network by only 25%).\nResults show KTN models are orders of magnitude\nsmaller than the most accurate competitor, SphConv [37].\nCompared with Spherical U-Net [46] and SphereNet [12],\nKTN is much more data efﬁcient because it does not require\nany annotated 360◦ images for training, and it is more accu-\nrate because it avoids their feature interpolation assumption.\n2. Related Work\n360◦ vision Ongoing work explores new projection mod-\nels optimized for image display [5, 25, 44] or video stor-\nage [1, 4, 28, 29, 39]. We adopt the most common equirect-\nangular projection so our algorithm can be readily applied\nto existing data. Other work explores how to improve the\ndisplay of 360◦ video via video stabilization [22, 23, 27],\nnew display interfaces [32–34], and automatic view selec-\ntion [7, 10, 19, 30, 38, 40, 43]. The latter all rely on applying\nCNNs to 360◦ data, and could beneﬁt from our method.\nCNNs on spherical dataAs discussed above, early meth-\nods take either the expensive but accurate reprojection ap-\nproach [38, 40, 43, 45], or the inaccurate but fast direct\nequirectangular approach [19, 30]. Recent work improves\naccuracy by training vanilla CNNs on the cubemap projec-\ntion, which introduces less distortion [3, 7], but the model\nstill suffers from cubemap distortion and discontinuities and\nhas sub-optimal accuracy for tasks such as object detection.\nIn the last year, several methods develop new spherical\nCNN models. Some design CNN architectures that account\nfor the distortion in 360◦ images [12,37,46]. SphConv [37]\nlearns separate kernels for each row of the equirectangular\nprojection, training to reproduce the behavior of an off-the-\nshelf CNN and adjusting the kernel shape based on its lo-\ncation on the sphere. While more accurate than a vanilla\nCNN, SphConv increases the model size signiﬁcantly be-\ncause it unties kernel weights along the rows. In contrast,\nSphereNet [12] deﬁnes the kernels on the tangent plane and\nprojects features to the tangent planes before applying the\nkernels. Similarly, Spherical U-Net [46] deﬁnes the kernels\non the sphere and resamples the kernels on the grid points\nfor every location in the equirectangular projection. Both\nallow weight sharing, but they implicitly assume that fea-\ntures deﬁned on the sphere can be interpolated in the 2D\nplane deﬁned by equirectangular projection, which we show\nis problematic. Instead of learning independent kernels or\nusing a ﬁxed 2D transformation, our KTN learns a transfor-\nmation that considers both spatial and cross-channel corre-\nlation. Our model is more compact than SphConv by shar-\ning the kernels, and it is more accurate than SphereNet and\nSpherical U-Net by learning a more generic transformation.\nAnother strategy is to deﬁne convolution in the spectral\ndomain in order to learn rotation invariant CNNs. One ap-\nproach is to apply graph convolution and design the graph\nstructure [24] such that the outputs are rotation invariant.\nAnother approach transforms both the feature maps and\nkernels into the spectral domain and applies convolution\nthere [11, 15]. However, orientation is often semantically\nsigniﬁcant in real data (e.g., cars are rarely upside down)\nand so removing orientation can unnecessarily restrict dis-\ncrimination. In addition, these approaches require caching\nthe basis functions and the frequency domain feature maps\nin order to achieve efﬁcient computation. This leads to sig-\nniﬁcant memory overhead and limits the viable input res-\nolution. Both constraints limit the spectral methods’ accu-\nracy on real world 360◦ images. Finally, unlike any of the\nabove prior work [3,7,11,12,15,37,46], our KTN can trans-\nfer across different source CNNs with the same architecture\nto perform new tasks without re-training; all other methods\nrequire training a new model for each task.\nCNNs with geometric transformations For perspective\nimages, too, there is interest in encoding geometric trans-\nformations in CNN architectures. Spatial transformer net-\nworks [20] transform the feature map into a canonical\nview to achieve transformation invariance. Active convo-\nlution [21] and deformable convolution [13] model geomet-\nric transformations using the receptive ﬁeld of the kernel.\nWhile these methods account for geometric transformations\nin the input data, they are not suitable for 360◦ images be-\ncause the transformation is location dependent rather than\ncontent dependent in 360◦ images. Furthermore, all of them\nmodel only geometric transformation and ignore the cor-\nrelation between different channels in the feature map. In\ncontrast, our method captures the properties of360◦ images\nand the cross channel correlation in the features.\n3. Approach\nIn this section, we introduce the Kernel Transformer Net-\nwork for transferring convolutions to 360◦ images. We ﬁrst\nintroduce the KTN module, which can replace the ordinary\nconvolution operation in vanilla CNNs. We then describe\nthe architecture and objective function of KTN. Finally, we\ndiscuss the difference between KTN and existing methods\nfor learning CNNs on 360◦ data.\n3.1. KTN for Spherical Convolution\nOur KTN can be considered as an generalization of or-\ndinary convolutions in CNNs. In the convolution layers of\nvanilla CNNs, the same kernel is applied to the entire in-\nput feature map to generate the output feature map. The\nassumption underlying the convolution operation is that the\nfeature patterns, i.e., the kernels, are translation invariant\nand should remain the same over the entire feature map.\nThis assumption, however, does not hold in360◦ images. A\n360◦ image is deﬁned by the visual content projected on the\nsphere centered at the camera’s optical center. To represent\nthe image in digital format, the sphere has to be unwrapped\ninto a 2D pixel array, e.g., with equirectangular projection\nor cubemaps. Because all sphere-to-plane projections in-\ntroduce distortion, the feature patterns are not translation\ninvariant in the pixel space, and ordinary CNNs trained for\nperspective images do not perform well on 360◦ images.\nTo overcome this challenge, we propose the Kernel\nTransformer Network, which can generate kernels that ac-\ncount for the distortion. Assume an input feature map\nI ∈RH×W×C and a source kernel K ∈Rk×k×C deﬁned\nin undistorted images (i.e., perspective projection). Instead\nof applying the source kernel directly\nF[x,y] = Σi,jK[i,j] ∗I[x−i,y −j], (1)\nwe learn the KTN ( f) that generates different kernels for\ndifferent distortions:\nKΩ = f(K,Ω) (2)\nF[x,y] = Σi,jKΩ[i,j] ∗I[x−i,y −j] (3)\nwhere the distortion is parameterized byΩ. Because the dis-\ntortion in 360◦ images is location dependent, we can deﬁne\nΩ as a function on the sphere\nΩ = g(θ,φ), (4)\nwhere θand φare the polar and azimuthal angle in spheri-\ncal coordinates, respectively. Given the KTNs and the new\ndeﬁnition of convolution, our approach permits applying an\nordinary CNN to 360◦ images by replacing the convolution\noperation in Eq. 1 with Eq. 3.\nKTNs make it possible to take a CNN trained for some\ntarget task (recognition, detection, segmentation, etc.) on\nordinary perspective images and apply it directly to 360\npanoramas. Critically, KTNs do so without using any an-\nnotated 360◦ images. Furthermore, as we will see below,\nonce trained for a given architecture (e.g., VGG), the same\nKTN is applicable for a new task using that architecture\nwithout retraining the KTN. For example, we could train\nthe KTN according to a VGG network trained for ImageNet\nclassiﬁcation, then apply the same KTN to transfer a VGG\nnetwork trained for Pascal object detection; with the same\nKTN, both tasks can be translated to 360◦ images.\n3.2. KTN Architecture\nIn this work, we consider 360◦ images that are un-\nwrapped into 2D rectangular images using equirectangular\nprojection. Equirectangular projection is the most popular\nformat for 360◦ images and is part of the 360◦ video com-\npression standard [8]. The main beneﬁt of equirectangu-\nlar projection for KTNs is that the distortion depends only\non the polar angle. Because the polar angle has an one-to-\none correspondence with the image row ( y=θH/π) in the\nequirectangular projection pixel space, the distortion can be\nparameterized easily using Ω = g(θ,φ) = y. Furthermore,\nwe can generate one kernel and apply it to the entire row\ninstead of generating one kernel for each location, which\nleads to more efﬁcient computation.\nA KTN instance is based on a given CNN architecture.\nThere are two basic requirements for the KTN module.\nFirst, it has to be lightweight in terms of both model size\nand computational cost. A large KTN module would in-\ncur a signiﬁcant overhead in both memory and computation,\nwhich would limit the resolution of input 360◦ images dur-\ning both training and test time. Because 360◦ images by\nnature require a higher resolution representation in order to\ncapture the same level of detail compared with ordinary im-\nages, the accuracy of the model would degrade signiﬁcantly\nif we were forced to use lower resolution inputs.\nSecond, KTNs need to generate output kernels with vari-\nable size, because the appropriate kernel shape may vary in\na single 360◦ image. A common way to generalize convo-\nlution kernels on the 2D plane to 360◦ images is to deﬁne\nthe kernels on the tangent plane of the sphere. As a result,\nthe receptive ﬁeld of the kernel on the 360◦ image is the\nback projection of the receptive ﬁeld on the tangent plane,\nwhich varies at different polar angles [12,37,46]. While one\ncould address this naively by always generating the kernels\nin the largest possible size, doing so would incur signiﬁcant\noverhead in both computation and memory.\n360◦ Image\nθ\nEquirectangular Projection\nK\n⊕\nKθ\nθ\nθ\nChannel-wiseProjection Depth-wise\nConvolution\n1x1 Convolution\nFigure 2: KTN consists of row dependent channel-wise\nprojections that resize the kernel to the target size and depth\nseparable convolution blocks. It takes a source kernel K\nand θ as input and generates an output kernel KΩ. KΩ is\nthen applied to the360◦ image in its equirectangular projec-\ntion at row y=θH/π. The transformation accounts for the\ndistortion in equirectangular projection, while maintaining\ncross-channel interactions.\nWe address the ﬁrst requirement (size and cost) by em-\nploying depthwise separable convolutions [9,18] within the\nKTN. Instead of learning 3D (i.e., height×width×channels)\nkernels, KTN alternates betweenpointwise convolution that\ncaptures cross-channel correlation and depthwise convolu-\ntion that captures spatial correlation. Using the same 3x3\ndepthwise convolutions as in MobileNet [18], the computa-\ntion cost is about 8 to 9 times less than standard convolution.\nFurthermore, the model size overhead for KTN is roughly\n1/k2 of the source kernels, where most of the parameters\nare in the 1x1 convolution. The size overhead turns out to\nbe necessary, because cross channel correlation is captured\nonly by the 1x1 convolution in KTN, and removing it re-\nduces the ﬁnal spherical convolution accuracy signiﬁcantly.\nTo address the second requirement (variable-sized ker-\nnels), we learn a row dependent depthwise projection to re-\nsize the source kernel. The projection consists of hprojec-\ntion matrices Pi, for i ∈[1,h], where h is the number of\nrows in the 360◦ image. Let ri = hi ×wi be the target\nkernel receptive ﬁeld at row i. The projection matrix has\nthe size Pi ∈Rri×k2\n, which projects the source kernel into\nthe target size. Similar to the depthwise convolution, we\nperform channel-wise projection to reduce the model size.\nThe complete architecture for KTN is in Fig. 2. We use a\nResidual Network [17]-like architecture. For both the resid-\nual and shortcut branches, we ﬁrst apply the row depen-\ndent projection to resize the kernel to the target size. The\nresidual branch then applies depthwise separable convolu-\ntion twice. Our depthwise separable convolution block con-\nsists of ReLU-pointwise conv-ReLU-depthwise conv. This\ndesign removes the batch normalization used in MobileNet\nto reduce the model size and memory consumption. The\ntwo branches are added together to generate the output ker-\nnel, which is then applied to a 360◦ feature map as in Eq. 3.\nNote that while the KTN can be applied to different kernels,\nthe structure of a KTN depends on Pi, which is determined\nby the receptive ﬁeld of the source kernel. Therefore, we\nneed one KTN for each layer of a source CNN.\n3.3. KTN Objective and Training Process\nHaving introduced the KTN module and how to apply it\nfor CNNs on 360◦ images, we now describe the KTN ob-\njective function and training process. The goal of the KTN\nis to adapt the source kernel to the360◦ domain. Therefore,\nwe train the model to reproduce the outputs of the source\nkernels. Let Fl ∈RH×W×Cl\nand Fl+1 ∈RH×W×Cl+1\nbe the feature maps generated by thel-th and (l+1)-th layer\nof a source CNN respectively. Our goal is to minimize the\ndifference between the feature map generated by the source\nkernels Kl and that generated by the KTN module:\nL= ∥Fl+1 −fl(Kl,Ω) ∗Fl∥2 (5)\nfor any 360◦ image. Note that during training the feature\nmaps Fl are not generated by applying the source CNN di-\nrectly on the equirectangular projection of the360◦ images.\nInstead, for each point (x,y) in the 360◦ image, we project\nthe image content to the tangent plane of the sphere at\n(θ,φ) = (π×y\nH ,2π×x\nW ) (6)\nand apply the source CNN on the tangent plane. This en-\nsures that the target training values are accurately computed\non undistorted image content. Fl[x,y] is deﬁned as the l-th\nlayer outputs generated by the source CNN at the point of\ntangency. Our objective function is similar to that of Sph-\nConv [37], but, importantly, we optimize the model over the\nentire feature map instead of on a single polar angle in order\nto factor the kernel itself out of the KTN weights.\nThe objective function depends only on the source pre-\ntrained CNN and does not require any annotated data for\ntraining. In fact, it does not require image data speciﬁc to\nthe target task, because the loss is deﬁned over360◦ images.\nIn practice, we sample arbitrary 360◦ images for training\nregardless of the source CNN. For example, in experiments\nwe train a KTN on YouTube video frames and then apply it\nfor a Pascal object detection task. Our goal is to fully repro-\nduce the behavior of the source kernel. Therefore, even if\nthe training images do not contain the same objects, scenes,\netc. as are seen in the target task, the KTN should still mini-\nmize the loss in Eq. 5. Although KTN takes only the source\nkernels and θas input, the exact transformation f may de-\npend on all the feature maps Fl,Fl−1,...,F 1 to resolve\nthe error introduced by non-linearities. Our KTN learns the\nimportant components of those transformations from data.\nTable 1:Comparison of different approaches. EQUIRECTANGULAR and CUBEMAP refer to applying the given CNN directly\nto the equirectangular and cubemap projection, respectively. Supervised training means that the method requires annotated\n360 images. The model size is the size for a single layer, wherec,k,H refer to the number of channels, kernel size, and input\nresolution (bandwidth) respectively. Note that c∼H ≫kfor real images and source CNNs, and we keep only the leading\nterm for each method.\nTranslation Rotation Supervised Model Transferable\nInvariance Invariance Training Size Across Models\nEQUIRECTANGULAR No No No c2k2 No\nCUBEMAP No No No c2k2 No\nS2CNN [11] Yes Yes Yes c2H No\nSPHERICAL CNN [15] Yes Yes Yes c2H No\nSPHERICAL U-N ET [46] Yes No Yes c2k2 No\nSPHERE NET [12] Yes No Yes c2k2 No\nSPHCONV [37] Yes No No c2k2H No\nKTN Yes No No c2k2 + c2 Yes\nKTN’s transferability across source kernels is analogous to\nthe generalizability of visual features across natural images.\nIn general, the more visual diversity in the unlabeled train-\ning data, the more accurately we can expect the KTN to be\ntrained. While one could replace all convolution layers in a\nCNN with KTNs and train the entire model end-to-end us-\ning annotated 360◦ data, we believe that Eq. 5 is a stronger\ncondition while also enjoying the advantage of bypassing\nany annotated training data.\n3.4. Discussion\nCompared to existing methods for convolution for 360◦\nimages, the main beneﬁts of KTN are its compactness and\ntransferability. The information required to solve the target\ntask is encoded in the source kernel, which is fed into the\nKTN as an input rather than part of the model. As a result,\nthe same KTN can be applied to another CNN having the\nsame base architecture but trained for a different target task.\nIn other words, without additional training, the same KTN\nmodel can be used to solve multiple vision tasks on 360◦\nimages by replacing the source kernels, provided that the\nsource CNNs for each task have the same base architecture.\nMost related to our work is the spherical convolution ap-\nproach (SphConv) [37]. SphConv learns the kernels adapted\nto the distortion in equirectangular projection. Instead of\nlearning the transformation function f in Eq. 2, SphConv\nlearns KΩ directly, and hence must learn one KΩ for every\ndifferent row of the equirectangular image. While SphConv\nshould be more accurate than KTN theoretically (i.e., re-\nmoving any limitations on memory and training time and\ndata) our experimental results show that the two methods\nperform similarly in terms of accuracy. Furthermore, the\nnumber of parameters in SphConv is hundreds of times\nlarger than KTN, which makes SphConv much more dif-\nﬁcult to train and deploy. The difference in model size be-\ncomes even more signiﬁcant when there are multiple mod-\nels to be evaluated: the same KTN can apply to multi-\nple source CNNs and thus incurs only constant overhead,\nx1\nw2σ(w1x1)\nx2\nw2σ(w1x2)\nx\nc(x)\na b\nw1\nσ(x)\nw2\nFigure 3:Beyond the ﬁrst CNN layer, the feature interpo-\nlation assumption in SphereNet [12] yields only approxi-\nmated results. See text for details.\nwhereas SphConv must fully retrain and store a new model\nfor each source CNN. For example, if we want to apply\nﬁve different VGG-based CNNs to 360◦ images, SphConv\nwill take 29×5=145GB of space, while KTN takes only\n56×5+14=294MB (cf. Sec. 4.3). In addition, since Sph-\nConv trains KΩ for a single source kernel K, the model\ndoes not generalize to different source CNNs.\nSphereNet [12] formulates the transformation functionf\nusing the sphere-to-tangent-plane image projection. While\nthe projection transformation leads to an analytical solution\nfor f, it implicitly assumes that CNN feature maps can be\ninterpolated like pixels. This assumption is only true for\nthe ﬁrst layer in a network because of non-linear activation\nfunctions used in modern CNNs between convolution lay-\ners. Consider a two layer 1D convolution with a kernel of\nsize 1, as sketched in Fig. 3. If we interpolate the pixel ﬁrst\nand apply the kernels, the output of at location xis\nc(x) = w2 ×σ(w1(ax1 + bx2)). (7)\nHowever, if we apply the kernels and then interpolate the\nfeatures, the result is\nc(x) = aw2 ×σ(w1x1) + bw2 ×σ(w1x2). (8)\nThese two values are not equal because σis non-linear, and\nthe error will propagate as the network becomes deeper. The\ninterpolated feature can at most be an approximation for the\nexact feature. Our experimental results show that a projec-\ntion transformation for f leads to sub-optimal performance.\nFinally, other methods attempt to reduce distortion by\nunwrapping a single 360◦ image into multiple images using\nperspective projection locally [3,7], e.g., with cubemap pro-\njection. It is non-trivial to deﬁne convolution across multi-\nple image planes, where two cube faces meet. Prior work\naddresses this problem by “cube-padding” the feature maps\nusing output from adjacent image planes [3, 7], but exper-\nimental results indicate that the resultant features are not\naccurate enough and degrade the accuracy. The reason is\nthat the same object may have different appearance on dif-\nferent tangent planes, especially when the ﬁeld-of-view is\nlarge and introduces signiﬁcant perspective distortion. Al-\nternatively, one could sample the tangent planes densely and\napply convolution on each tangent plane independently, but\ndoing so incurs unrealistic computational overhead [38].\nTable 1 summarizes the tradeoffs between existing\nspherical convolution models. In short, KTN is distinct\nfrom all others in its ability to transfer to new tasks without\nany labeled data. Furthermore, KTN has the favorable prop-\nerties of a highly compact model and the ability to preserve\norientation-speciﬁc features (typically desirable for recog-\nnition and other high-level tasks).\n4. Experiments\nWe evaluate KTN on multiple datasets and multiple\nsource models. The goal is to 1) validate the accuracy of\nKTN as compared to other methods for learning CNNs on\n360◦ images, 2) demonstrate KTN’s ability to generalize to\nnovel source models, and 3) examine KTN’s memory and\ncomputation overhead compared to existing techniques.\nDatasets Our experiments make use of both unannotated\n360◦ videos and 360◦ images with annotation.\nSpherical MNIST is constructed from the MNIST dataset\nby back projecting the digits into equirectangular projection\nwith 160×80 resolution. The digit labels are used to train\nthe source CNN (recognition model), but they are not used\nto train the KTN. Classiﬁcation accuracy on the 360◦-iﬁed\ntest set is used as the evaluation metric.\nPano2Vid is a real world 360◦ video dataset [40]. We\nsample frames from non-overlapping videos for training\nand testing, and the frames are resized to 640×320 reso-\nlution. The models are trained to reproduce the convolution\noutputs of the source model, so no labels are required for\ntraining. The root-mean-square error (RMSE) of the ﬁnal\nconvolution outputs is used as the evaluation metric.\nPascal VOC 2007 is a perspective image dataset with\nobject annotations. We backproject the object bounding\nboxes to equirectangular projection with 640×320 resolu-\ntion. Following [37], we use the accuracy of the detector\nnetwork in Faster R-CNN on the validation set as the eval-\nuation metric. This dataset is used for evaluation only.\nSource Models For Spherical MNIST, we train the source\nCNN on the MNIST training set. The model consists\nof three convolution layers followed by one fully con-\nnected layer. Each convolution layer consists of 5x5Conv-\nMaxPool-ReLU, and the number of kernels is 32, 64, and\n128, respectively. For Pano2Vid and Pascal VOC, we take\noff-the-shelf Faster R-CNN [35] models with VGG archi-\ntecture [36] as the source model. The Faster R-CNN is\ntrained on Pascal VOC if not mentioned speciﬁcally. Source\nmodels are not ﬁne-tuned on 360◦ data in any form.\nBaselines We compare to the following existing methods:\n• EQUIRECTANGULAR —Apply ordinary CNNs on the\n360◦ image in its equirectangular projection.\n• CUBEMAP —Apply ordinary CNNs on the 360◦ image in\nits cubemap projection.\n• S2CNN [11]—We train S2CNN using the authors’ im-\nplementation. For Pano2Vid and Pascal VOC, we reduce\nthe input resolution to 64×64 due to memory limits (see\nSupp). We add a linear read-out layer at the end of the\nmodel to generate the ﬁnal feature map.\n• SPHERICAL CNN [15]—We train S PHERICAL CNN us-\ning the authors’ implementation. Again, the resolution of\ninput is scaled down to 80×80 due to memory limits for\nPano2Vidand Pascal VOC.\n• SPHERICAL U-N ET [46]—We use the spherical convo-\nlution layer in Spherical U-Net to replace ordinary con-\nvolution in CNN. Input resolution is reduced to 160×80\nfor Pano2Vidand Pascal VOC due to memory limits.\n• SPHERE NET [12]—We implement S PHERE NET using\nrow dependent channel-wise projection. 2 We derive the\nweights of the projection matrices using the feature pro-\njection operation and train the source kernels. For the\nPano2Vid dataset, we train each layer independently us-\ning the same objective as KTN due to memory limits.\n• SPHCONV [37]—We use the authors’ implementation.\n• PROJECTED —Similar to S PHERE NET, except that it uses\nthe source kernels without training.\nThe network architecture for E QUIRECTANGULAR and\nCUBEMAP is the same as the source model. For all meth-\nods, the number of layers and kernels are the same as the\nsource model.\nNote that the resolution reductions speciﬁed above were\nnecessary to even run those baseline models on the non-\nMNIST datasets, even with state-of-the-art GPUs. All ex-\nperiments were run on NVIDIA V100 GPU with 16GB\nmemory—the largest in any generally available GPU to-\nday. Therefore, the restriction is truly imposed by the latest\nhardware technology. Compatible with these limits, the res-\nolution in the authors’ own reported results is restricted to\n60 ×60 [11], 64 ×64 [15], or 150 ×300 [46]. On the\nSphericalMNIST dataset, all methods use the exact same\n2The authors’ code and data were not available at the time of publication.\nTable 2:Model accuracy.\nMNIST Pano2Vid Pascal VOC\n(Acc.↑) (RMSE ↓) (Acc. ↑)\nEQUIRECTANGULAR 95.24 3.44 41.63\nCUBEMAP 68.53 3.57 49.29\nS2CNN [11] 95.79 2.37 4.32\nSPHERICAL CNN [15] 97.48 2.36 6.06\nSPHERICAL U-N ET [46] 98.43 2.54 24.98\nSPHERE NET [12] 87.20 2.46 46.68\nSPHCONV [37] 98.72 1.50 63.54\nPROJECTED 10.70 4.24 6.15\nKTN 97.94 1.53 69.48\nimage resolution. The fact that KTN scales to higher res-\nolutions is precisely one of its technical advantages, which\nwe demonstrate on the other datasets.\nFor Spherical MNIST, the baselines are trained to predict\nthe digit projected to the sphere except S PHCONV . S PH-\nCONV and our KTN are trained to reproduce the conv3\noutputs of the source model. For Pano2Vid, all methods\nare trained to reproduce the conv5 3 outputs.\nPlease see Supp. ﬁle for additional details.\n4.1. Model Accuracy\nTable 2 summarizes the methods’ CNN accuracy on all\nthree 360◦ datasets. KTN performs on par with the best\nbaseline method (SPHCONV ) on Spherical MNIST. The re-\nsult veriﬁes that KTN can transfer the source kernels to the\nentire sphere by learning to reproduce the feature maps, and\nit can match the accuracy of existing models trained with\nannotated 360◦ images.\nKTN and S PHCONV perform signiﬁcantly better than\nthe other baselines on the high resolution datasets, i.e.,\nPano2Vid and Pascal VOC. S2CNN, S PHERICAL CNN,\nand S PHERICAL U-N ET suffer from their memory con-\nstraints, which as discussed above restricts them to lower\nresolution inputs. Their accuracy is signiﬁcantly worse\non realistic full resolution datasets. These models cannot\ntake higher resolution inputs even after using model paral-\nlelism over four GPUs with a total of 64GB of memory.\nAlthough EQUIRECTANGULAR and CUBEMAP are trained\nand applied on the full resolution inputs, they do not account\nfor the distortion in 360◦ images and yield lower accuracy.\nFinally, the performance of P ROJECTED and S PHERE NET\nsuggests that the transformation f cannot be modeled by a\ntangent plane-to-sphere projection. Although S PHERE NET\nshows that the performance can be signiﬁcantly improved\nby training the source kernels on 360◦ images, the accu-\nracy is still worse than KTN because feature interpolation\nintroduces error. The error accumulates across layers, as\ndiscussed in Sec. 3.4, which substantially degrades the ac-\ncuracy when applying a deep CNN. Note that the number of\nlearnable parameters in KTN is much smaller than that in\nFigure 4: KTN object detection examples on Pano2Vid.\nSee Supp. for detection examples on Pascal VOC.\nSPHERE NET, but it still achieves a much higher accuracy.\nInterestingly, although S PHCONV performs better in\nRMSE on Pano2Vid, KTN peforms better in terms of object\nclassiﬁcation accuracy on Pascal VOC. We attribute this to\nKTN’s inherent generalizability. S PHCONV has a larger\nnumber of parameters, and the kernels at different θ are\ntrained independently. In contrast, the parameters in KTN\nare shared across different θand thus trained with richer in-\nformation. Therefore, S PHCONV is more prone to overﬁt\nthe training loss, which is to minimize the RMSE for both\nmodels. Furthermore, our KTN has a signiﬁcant compact-\nness advantage over SPHCONV , as discussed above.\nSimilarly, although S PHERICAL U-N ET and\nSPHERE NET perform slightly worse than S2CNN\nand S PHERICAL CNN on Pano2Vid, they are signiﬁcantly\nbetter than those baselines on Pascal VOC . This result\nreinforces the practical limitations of imposing rotation\ninvariance. S2CNN and S PHERICAL CNN require full\nrotation invariance; the results show that orientation infor-\nmation is in fact important in tasks like object recognition.\nThus, the additional rotational invariance constraince limits\nthe expressiveness of the kernels and degrades the perfor-\nmance of S2CNN and S PHERICAL CNN. Furthermore,\nthe kernels in S2CNN and S PHERICAL CNN may span\nthe entire sphere, whereas spatial locality in kernels has\nproven important in CNNs for visual recognition.\nFig. 4 shows example outputs of KTN with a Faster R-\nCNN source model. The detector successfully detects ob-\njects despite the distortion. On the other hand, KTN can fail\nwhen a very close object cannot be captured in the ﬁeld-of-\nview of perspective images.\n4.2. Transferability\nNext, we evaluate the transferability of KTN across dif-\nferent source models on Pano2Vid. In particular, we eval-\nuate whether KTNs trained with a Faster R-CNN that is\ntrained on COCO can be applied to another Faster R-CNN\n(both using VGG architecture) that is trained on Pascal\nVOC and vice versa. We denote KTN trained on a different\nsource CNN than it is being tested on as KTN- TRANSFER\nand KTN otherwise.\n18◦ 36◦ 54◦ 72◦ 90◦0\n2\n4RMSE\nPascal VOC\nKTN KTN-Transfer Projected\n18◦ 36◦ 54◦ 72◦ 90◦0\n1\n2\n3\n4\n COCO\nFigure 5: Model transferability. The title indicates the\nsource CNN being tested. KTN performs almost identi-\ncally regardless of the source network it is trained on. The\nresults show we can learn a single KTN and apply it to other\nsource CNNs with the same architecture, even if that source\nmodel is trained for a different task.\nFig. 9 shows the results. The accuracy of KTN-\nTRANSFER is almost identical to KTN. The results demon-\nstrate that KTN indeed learns a task-independent transfor-\nmation and can be applied to different source models with\nthe same base architecture. None of the existing mod-\nels [11, 12, 15, 37, 46] are equipped to perform this kind\nof transfer, because they learn ﬁxed kernels for a speciﬁc\ntask in some form. Hence, the P ROJECTED baseline is the\nonly baseline shown in Fig. 9. Although P ROJECTED can\nbe applied to any source CNN without training, the per-\nformance is signiﬁcantly worse than KTN. Again, the re-\nsults indicate that a projection operation is not sufﬁcient to\nmodel the required transformation f. The proposed KTN\nis the ﬁrst approach to spherical convolution that translates\nacross models without requiring labeled 360◦ images or re-\ntraining. We also perform the same experiments between\nVGG trained for ImageNet classiﬁcation and Faster R-CNN\ntrained for Pascal object detection, and the results are simi-\nlar. See Supp.\n4.3. Size and Speed\nFinally, we compare the overhead introduced by KTN\nversus that required by the baseline methods. In particular,\nwe measure the model size and speed for the convolution\nlayers in the VGG architecture. For the model size, we com-\npute the total size of the parameters using 32-bit ﬂoating\npoint numbers for the weights. While there exist algorithms\nthat compress neural networks, they are equally applicable\nfor all methods. For the speed, we measure the average pro-\ncessing time (I/O excluded) of an image for computing the\nconv5 3 outputs. All methods are evaluated on a dedicated\nAWS p3.8xlarge instance. Because the model size for SPH-\nCONV is 29GB and cannot ﬁt in GPU memory (16GB), it is\nrun on CPUs. Other methods are run on GPUs.\nFig. 6 shows the results. We can see that the model size\nof KTN is very similar to EQUIRECTANGULAR , CUBEMAP\nand PROJECTED . In fact, it is only 25% (14MB) larger than\n102 103 104\n0\n20\n40\n60\n80\nSize (MB)\nAccuracy\n10−1 101 103\n0\n20\n40\n60\n80\nTime (s) / Image\nAccuracy\nEQUIRECTANGULAR\nCUBEMAP\nS2CNN[11]\nSPHERICALCNN [15]\nSPHERENET [12]\nSPHERICALU-NET [46]\nSPHCONV[37]\nPROJECTED\nKTN (Ours)\nFigure 6: Model size (top) and speed (bottom) vs. accu-\nracy for VGG. KTN is orders of magnitude smaller than\nSPHCONV , and it is similarly or more compact as all other\nmodels, while being signiﬁcantly more accurate.\nthe source CNN. At the same time, KTN achieves a much\nbetter accuracy compared with all the models that have a\ncomparable size. Compared with SPHCONV , KTN not only\nachieves a higher accuracy but is also orders of magnitude\nsmaller. Similarly, S2CNN and S PHERICAL CNN increase\nmodel size by 131% and 727% while performing worse in\nterms of accuracy. Note that we do not include parame-\nters that can be computed analytically, such as the bases\nfor S2CNN and the projection matrices for S PHERE NET,\nthough in practice they also add further memory overhead\nfor those baselines.\nOn the other hand, the computational cost of KTN is\nnaturally much higher than E QUIRECTANGULAR . The lat-\nter only needs to run the source CNN on an equirectangular\nimage, whereas the convolution kernels are generated at run\ntime for KTN. However, as all the results show, KTN is\nmuch more accurate. Furthermore, KTN is 26 times faster\nthan S PHCONV , since the smaller model size allows the\nmodel to be evaluated on GPU.\n5. Conclusion\nWe propose the Kernel Transformer Network for trans-\nfering CNNs from perspective images to360◦ images. KTN\nlearns a function that transforms a kernel to account for the\ndistortion in the equirectangular projection of 360◦ images.\nThe same KTN model can transfer to multiple source CNNs\nwith the same architecture, signiﬁcantly streamlining the\nprocess of visual recognition for 360◦ images. Our results\nshow KTN outperforms existing methods while providing\nsuperior scalability and transferability.\nAcknowledgement. We thank Carlos Esteves for the help\non S PHERICAL CNN experiments. This research is sup-\nported in part by NSF IIS-1514118, an AWS gift, a Google\nPhD Fellowship, and a Google Faculty Research Award.\nThe supplementary materials consist of:\nA Complete architecture of KTN\nB Experimental details\nC Model transferability experiment on Pascal VOC\nD Comparison of model accuracy versus depth\nE Discussion of multiple projections baselines\nF Additional qualitative detection examples\nA. KTN Architecture\nIn this section, we show the complete architecture of\nKTN. Fig. 7 shows how to apply KTN for spherical convo-\nlution. For each layer l∈{1,··· ,L}of the source CNN,\nwe learn a function fl that transforms the source kernel\nKl to Kl\nθi for every θ ∈[0,π]. The output kernel Kl\nθi is\nthen applied to the 360◦ equirectangular image at the corre-\nsponding row of θi. We ﬁnd that it is unnecessary to gen-\nerate one kernel for each row in the equirectangular pro-\njection, because spherical convolution kernels for adjacent\nrows are usually similar. In practice, we share the same\nkernel every ﬁve rows to reduce the computational cost and\nmodel size. Fig. 8 shows the full architecture of KTN. KTN\nuses a ResNet-like architecture. For both branches, it uses\na row dependent channel-wise projection to resize the ker-\nnel to the target size. The residual branch then applies two\ndepth separable convolution blocks before adding the out-\nput with that of the shortcut branch. Each depth separable\nconvolution block consists of ReLU-pointwise conv-ReLU-\ndepthwise conv.\nTo compute the target kernel size at a given polar angle,\nwe ﬁrst back project the receptive ﬁeld of the source kernel\nto equirectangular projection. The minimum bounding box\ncentered at the polar angle that can cover the receptive ﬁeld\non equirectangular projection is then selected as the target\nkernel shape. Note that we restrict the kernel height and\nwidth to be an odd number to ensure that the kernel is de-\nﬁned on the equirectangular pixel space. Because the size\nof the back projected receptive ﬁeld may grow rapidly and\nspan the entire image, we restrict the actual kernel width\nand height to be less then 65 pixels and dilate the kernel to\nincrease the effective receptive ﬁeld if necessary.\nFor Spherical Faster R-CNN, we deﬁne the bounding\nboxes on the tangent plane: we project the features to the\ntangent plane and apply the RPN and detector networks\nthere. The implementation is the same as S PHCONV [37].\nB. Experimental Details\nIn this section, we describe additional experimental de-\ntails that could not ﬁt in the main paper.\nB.1. Datasets\nThe following is an expanded version of the dataset de-\nscriptions in the main text.\nK\n1 f\n1\n( θ1 , · · · ,θ N )\nK1\nθ1\nK1\nθN\n· · · θ\nK\n2 f\n2\n( θ1 , · · · ,θ N )\nK2\nθ1\nK2\nθN\n· · · θ\n· · ·\n· · ·\nK\nL f\nL\n( θ1 , · · · ,θ N )\nKL\nθ1\nKL\nθN\n· · · θ\nSource CNN\nKTN\nInputs\nθ\nFigure 7:Application of spherical convolution using KTN.\nK\nθ\nInputs\nθ\nChannel-wiseProjection\n1x1 Convolution\nDepth-wise\nConvolution\n1x1 Convolution\nDepth-wise\nConvolution\nθ\n⊕\nKθ\nFigure 8:Full architecture of KTN.\nSpherical MNIST is constructed from the MNIST dataset\nby backprojecting the digits into equirectangular projec-\ntion. The resolution of the resultant 360◦ image is 160×80,\nand the digit covers a 65.5◦ ﬁeld-of-view (FOV). For the\ntraining set, we project each digit to a random polar angle\nθ∈[0,π]. For the test set, we project each digit to nine differ-\nent polar angles θ=8◦,16◦,..., 72◦, which results in a test\nset that is nine times larger. Note that we do not rotate the\ndigit itself because digits are oriented by deﬁnition (e.g. 6\nversus 9). All baselines are trained to predict the digit la-\nbel on the Spherical MNIST training set except SPHCONV ,\nwhich does not require such labels. Both KTN and S PH-\nCONV are trained to re-produce the top-most convolution\noutput (conv3). Classiﬁcation accuracy averaged across θ\nis used as the evaluation metric.\nPano2Vidis a real world 360◦ video dataset [40]. It con-\ntains 86 videos from four categories: “Hiking,” “Parade,”\n“Soccer,” and “Mountain Climbing.” Following [37], we\nsample 1,056 frames from the ﬁrst three categories for train-\ning and 168 frames from the last category for testing, and\nthe frames are resized to 640×320 resolution. The root-\nmean-square error (RMSE) over the ﬁnal convolution out-\nputs is used as the evaluation metric.\nPascal VOC is a perspective image dataset with object\nannotations. Similar to Spherical MNIST, we backproject\nthe object bounding boxes to equirectangular projection but\nwith 640×320 resolution. Each bounding box is projected\nto different polar angles θ∈{18◦,36◦,54◦,72◦,90◦}and\ncovers a 65.5◦ FOV . Because the perspective images do not\ncover the full 360◦ FOV , regions outside the FOV of the\noriginal image are zero-padded (black). This dataset is used\nfor evaluation only. Following the experiment setting of the\nFaster R-CNN [35] source model, we evaluate all methods\non the validation set of Pascal VOC 2007. We use the accu-\nracy of the detector network in Faster R-CNN as the eval-\nuation metric. The ground truth bounding box is used for\nROI-pooling during evaluation for all methods.\nB.2. Baselines\nIn this section, we expand on the implementation details\nof each baseline method. We keep the number of layers and\nkernels the same for all methods. For the Spherical MNIST\ndataset, the models consist of three convolution layers fol-\nlowed by a max-pooling over the spatial dimensions and a\nfully connected layer. The convolution layers have 32, 64,\nand 128 kernels respectively, and the resolution of the fea-\nture map is reduced by a factor of two using max-pooling\nafter each convolution layer. For the Pano2Vid and Pascal\nVOC datasets, the models have the same number of layers\nand kernels as the VGG16 architecture. Following S PH-\nCONV [37], we remove the max-pooling operation in the\nnetwork and use dilated convolution with factor of two in\nthe conv5 layers to increase the receptive ﬁeld. The differ-\nences between different methods are in the convolution and\npooling operations as described below.\n• EQUIRECTANGULAR —Apply ordinary CNNs on the\n360◦ image in its equirectangular projection.\n• CUBEMAP —Apply ordinary CNNs on the 360◦ image\nin its cubemap projection, with cube padding [7]. For\nthe PANO 2VID and PASCAL VOC datasets, the conv5 3\nfeature map is re-projected to equirectangular projection\nas the ﬁnal output.\n• S2CNN [11]—We use the S2Convolution and\nSO3Convolution in the authors’ implementation 3\nfor convolution. S2Convolution is applied in the ﬁrst\nconvolution layer, and SO3Convolution is used for the\nother layers. The default near identity grid is used\nfor both S2 and SO3 convolution. Furthermore, we\nreduce the feature map resolution by reducing the output\nbandwidth instead of using max-pooling following\nthe authors’ implementation. The input resolution is\n80×80 for Spherical MNIST and 64×64 for Pano2Vid\nand Pascal VOC. For Spherical MNIST , we use SO(3)\nintegration instead of max-pooling to reduce the ﬁnal\nfeature map. For Pano2Vid and Pascal VOC, because\nthe output of SO3Convolution is a 3D feature map, we\n3https://github.com/jonas-koehler/s2cnn\nadd a 1x1 convolution layer on top of the conv5 3 output\nto generate a 2D feature map. The feature map is then\nresized to 640×320 as the ﬁnal output. We reduce the\noutput bandwidth in conv2 2 and conv3 3 and distribute\nthe model to four NVIDIA V100 GPUs using model\nparallelism due to the GPU memory limit.\n• SPHERICAL CNN [15]—We use the sphconv module in\nthe authors’ implementation4 for convolution. Similar to\nS2CNN, we replace max-pooling with spectral pooling.\nFurthermore, we apply batch normalization in each con-\nvolution layer following the example code. The input\nresolution is 80×80 for all datasets. For the Pano2Vid\nand Pascal VOCdataset, we reduce the output bandwidth\nin conv4 1 and conv5 1 due to the memory limit. The\nconv5 3 feature map is resized to 640×320 as the ﬁnal\noutput.\n• SPHERICAL U-N ET [46]—We use the SphericalConv\nmodule in Spherical U-Net 5 for convolution. We apply\nbatch normalization and set the kernel size to 8×4 fol-\nlowing the authors’ example. For the Pano2Vid and Pas-\ncal VOC dataset, the input is resized to 160×80 due to\nmemory limit, and the conv5 3 feature map is resized to\n640×320 as the ﬁnal output. The model is distributed to\nfour NVIDIA V100 GPUs using model parallelism.\n• SPHERE NET [12]—We implement the S PHERE NET\nmodel using row dependent channel-wise projection. The\nauthors’ code and data were unavailable at the time of\nsubmission. Because feature projection is the weighted\nsum of the features, the projection weights can be com-\nbined with the kernel weights as a single kernel. We de-\nrive the weights of the channel-wise projection using the\nfeature projection operation and train the source kernels.\nFor the Pano2Vid dataset, we train each layer indepen-\ndently using the same objective function as KTN because\nthe entire model cannot ﬁt in GPU memory.\n• SPHCONV [37]—We use the authors’ implementation 6.\nBecause the model is too large to ﬁt into GPU memory\neven for evaluation, it is run on CPUs.7\n• PROJECTED —Assuming that the kernel transformation f\ncan be modeled using the tangent plane-to-sphere projec-\ntion, we derive the analytic solution for the kernels Kθ\nusing bilinear interpolation.\nNote that the aspect ratio for the inputs is 1:1 for S2CNN\nand SPHERICAL CNN. This is the requirement of the meth-\nods, so we reduce the resolution along the azimuthal angle.\nThe input aspect ratio for all other methods is 2:1 following\nthe common format of 360◦ images.\n4https://github.com/daniilidis-group/spherical-cnn\n5https://github.com/xuyanyu-shh/Saliency-detection-in-360-video\n6https://github.com/sammy-su/Spherical-Convolution\n7For the other baselines, testing is still possible with GPUs.\n18◦ 36◦ 54◦ 72◦ 90◦0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\nPascal VOC\nKTN\nKTN-Transfer\nProjected\nFigure 9:Model transferability on Pascal VOC.\n18◦ 36◦ 54◦ 72◦ 90◦0\n2\n4RMSE\nPascal VOC\nKTN KTN-Transfer Projected\n18◦ 36◦ 54◦ 72◦ 90◦0\n10\n20\nImageNet\nFigure 10:Transferability of ImageNet trained VGG.\nB.3. Training Details\nWe train all the methods using ADAM [26] for 40\nepochs. The learning rate is initialized to 1.0×10−3 and\nis decreased by a factor of 10 after 20 epochs. We also ap-\nply L2 regularization with weight 5×10−4. For Pano2Vid,\nthe batch size is set to one for S2CNN, two for Spherical\nCNN, and four for all other methods, which is again limited\nby the memory. For Spherical MNIST, the batch size is set\nto 64 for all methods except S2CNN, which uses a batch\nsize of 16. The weights are randomly initialized using a nor-\nmal distribution with standard deviation 0.01. The training\ntime for KTN on Pano2Vid is about a week using six AWS\np3.8xlarge instances with V100 GPUs.\nC. Transferability onPascal VOC\nAs noted in the main paper, in this section, we evaluate\nthe transferability of KTN on Pascal VOC. In particular, we\nmeasure whether the KTN model trained on a VGG source\nmodel can be applied to Faster R-CNN to perform object\ndetection. The result is in Fig. 9. Again, KTN performs\nalmost identical regardless of the source model on which\nit is trained. We also evaluate the transferability between\nVGG trained for ImageNet classiﬁcation and Faster R-CNN\ntrained for Pascal object detection. The result is in Fig. 10.\nThe results are consistent with that in Sec. 4.3 of the main\npaper and veriﬁes that KTN is transferable across source\nCNNs with the same architecture.\n5 10\n1\n2\nDepth\nRMSE\nSPHERENET[12]\nPROJECTED\nKTN (Ours)\nFigure 11:Model accuracy at different layers.\n18◦ 36◦ 54◦ 72◦ 90◦0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\nPascal VOC\nEquirectangular\nCubemap\nKTN\nFigure 12:Model accuracy of projection based baselines.\nD. Model Accuracy versus Depth\nAs discussed in the main paper, the interpolation as-\nsumption made by S PHERE NET [12] and the P ROJECTED\nbaselines is problematic, particularly at deeper layers as\nerrors accumulate. Hence, we compare the accuracy of\nSPHERE NET [12], P ROJECTED , and KTN with different\nnetwork depths. We change the network depth by feeding\nin the ground truth value of the intermediate layer and com-\npare the RMSE of conv5 3 outputs. The experiment is per-\nformed on Pano2Vidusing Faster R-CNN source model.\nThe results are in Fig. 11. Not surprisingly, the error in-\ncreases as the model depth increases for all methods. More\nimportantly, the gap between KTN and the other methods\nincreases as the network becomes deeper. The results sug-\ngest that the error of interpolated features increases as the\nnumber of non-linearities increases and is consistent with\nthe analysis in Sec. 3.4 in the main paper.\nE. Multiple Projections Baseline\nFig. 12 shows that the worst accuracy of KTN (at\nθ=18◦) outperforms the best accuracy of EQUIRECTANGU -\nLAR and C UBEMAP (at θ=90◦). While a possible method\nfor improving the performance of the projection based\nmethods (i.e. E QUIRECTANGULAR and C UBEMAP ) is to\naggregate the detection results from multiple projections\nto reduce the effect of distortion, the results suggest that\nEQUIRECTANGULAR and C UBEMAP is less accurate then\nKTN even if they are always evaluated on the less distorted\nregion. This implies that KTN will always be more accu-\nFigure 13:Failure cases on Pano2Vid.\nrate than E QUIRECTANGULAR and C UBEMAP no matter\nhow many different projections we sample. Furthermore,\nevaluating the model on multiple projections increases the\ncomputational cost and introduces the problem of how to\ncombine detection results, which is non-trivial especially in\ndense prediction problems such as depth prediction.\nF. Object Detection Examples\nIn this section, we show additional object detection ex-\namples. Fig. 14 and Fig. 15 show object detection examples\non the Pano2Vd and Pascal VOC dataset, respectively. No-\ntice how KTN can detect the distorted objects by translating\nthe source CNN appropriately to the spherical data.\nFig. 13 show failure examples on the Pano2Vid dataset.\nIn the ﬁrst example, the model fails to capture the entire\nhuman body and returns two positive detections instead of\none. This is caused by the fact that our method cannot han-\ndle close objects that cannot be captured by the FOV of per-\nspective images. In the second example, the model fails on\nthe top view of the person because a top view is very rare\nin ordinary images. The result indicates that the data distri-\nbution is different in 360◦ images and perspective images.\nThe performance of the model may be further improved if\nwe can train the source CNNs on 360◦ images.\nReferences\n[1] David Newman Adeel Abbas. A novel projection for omni-\ndirectional video. In Proc.SPIE 10396, 2017. 2\n[2] Brent Ayrey and Christopher Wong. Introducing face-\nbook 360 for gear vr. https://newsroom.fb.com/news/2017/03/\nintroducing-facebook-360-for-gear-vr/ , March 2017. 1\n[3] Wouter Boomsma and Jes Frellsen. Spherical convolutions\nand their application in molecular modelling. In NIPS, 2017.\n2, 6\n[4] Chip Brown. Bringing pixels front and center in\nVR video. https://www.blog.google/products/google-vr/\nbringing-pixels-front-and-center-vr-video/ , March 2017. 2\n[5] Che-Han Chang, Min-Chun Hu, Wen-Huang Cheng, and\nYung-Yu Chuang. Rectangling stereographic projection for\nwide-angle image visualization. In ICCV, 2013. 2\n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 1\n[7] Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-\nKai Wen, Tyng-Luh Liu, and Min Sun. Cube padding for\nweakly-supervised saliency prediction in 360◦ videos. In\nCVPR, 2018. 2, 6, 10\n[8] Byeongdoo Choi, Ye-Kui Wang, and Miska M. Hannuksela.\nWd on iso/iec 23000-20 omnidirectional media application\nformat. ISO/IEC JTC1/SC29/WG11, 2017. 3\n[9] Franc ¸ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 2017. 4\n[10] Shih-Han Chou, Yi-Chun Chen, Kuo-Hao Zeng, Hou-Ning\nHu, Jianlong Fu, and Min Sun. Self-view grounding given a\nnarrated 360◦ video. In AAAI, 2018. 1, 2\n[11] Taco Cohen, Mario Geiger, Jonas K ¨ohler, and Max Welling.\nSpherical cnns. In ICLR, 2018. 1, 2, 5, 6, 7, 8, 10\n[12] Benjamin Coors, Alexandru Paul Condurache, and Andreas\nGeiger. Spherenet: Learning spherical representations for\ndetection and classiﬁcation in omnidirectional images. In\nECCV, 2018. 1, 2, 3, 5, 6, 7, 8, 10, 11\n[13] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In ICCV, 2017. 2\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: a large-scale hierarchical image\ndatabase. In CVPR, 2009. 1\n[15] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-\ndia, and Kostas Daniilidis. Learning so(3) equivariant repre-\nsentations with spherical cnns. In ECCV, 2018. 1, 2, 5, 6, 7,\n8, 10\n[16] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.\nConvolutional two-stream network fusion for video action\nrecognition. In CVPR, 2016. 1\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 4\n[18] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 4\n[19] Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu\nCheng, Yung-Ju Chang, and Min Sun. Deep 360 pilot:\nLearning a deep agent for piloting through360◦ sports video.\nIn CVPR, 2017. 1, 2\n[20] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. In Advances in Neural Infor-\nmation Processing Systems, pages 2017–2025, 2015. 2\n[21] Yunho Jeon and Junmo Kim. Active convolution: Learning\nthe shape of convolution for image classiﬁcation. In CVPR,\n2017. 2\n[22] Mostafa Kamali, Atsuhiko Banno, Jean-Charles Bazin, In So\nKweon, and Katsushi Ikeuchi. Stabilizing omnidirectional\nvideos using 3d structure and spherical image warping. In\nIAPR MVA, 2011. 2\n[23] Shunichi Kasahara, Shohei Nagai, and Jun Rekimoto. First\nperson omnidirectional video: System design and implica-\ntions for immersive experience. In ACM TVX, 2015. 2\n[24] Renata Khasanova and Pascal Frossard. Graph-based clas-\nsiﬁcation of omnidirectional images. In ICCV Workshops,\n2017. 2\n[25] Yeong Won Kim, Chang-Ryeol Lee, Dae-Yong Cho,\nYong Hoon Kwon, Hyeok-Jae Choi, and Kuk-Jin Yoon. Au-\ntomatic content-aware projection for 360◦ videos. In ICCV,\n2017. 2\n[26] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 11\n[27] Johannes Kopf. 360 ◦ video stabilization. ACM Transactions\non Graphics (TOG), 35(6):195, 2016. 2\n[28] Evgeny Kuzyakov and David Pio. Under the hood: Building\n360 video. https://code.facebook.com/posts/1638767863078802/\nunder-the-hood-building-360-video/ , October 2015. 2\n[29] Evgeny Kuzyakov and David Pio. Next-generation\nvideo encoding techniques for 360 video and\nVR. https://code.facebook.com/posts/1126354007399553/\nnext-generation-video-encoding-techniques-for-360-video-and-vr/ ,\nJanuary 2016. 2\n[30] Wei-Sheng Lai, Yujia Huang, Neel Joshi, Chris Buehler,\nMing-Hsuan Yang, and Sing Bing Kang. Semantic-driven\ngeneration of hyperlapse from 360 ◦ video. IEEE Transac-\ntions on Visualization and Computer Graphics, PP(99):1–1,\n2017. 1, 2\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 1\n[32] Yen-Chen Lin, Yung-Ju Chang, Hou-Ning Hu, Hsien-Tzu\nCheng, Chi-Wen Huang, and Min Sun. Tell me where to\nlook: Investigating ways for assisting focus in 360 video. In\nCHI, 2017. 2\n[33] Yung-Ta Lin, Yi-Chi Liao, Shan-Yuan Teng, Yu-Ju Chung,\nLiwei Chan, and Bing-Yu Chen. Outside-in: Visualizing\nout-of-sight regions-of-interest in a 360◦ video using spatial\npicture-in-picture previews. In UIST, 2017. 2\n[34] Amy Pavel, Bj ¨orn Hartmann, and Maneesh Agrawala. Shot\norientation controls for interactive cinematography with 360\nvideo. In UIST, 2017. 2\n[35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NIPS, 2015. 6, 10\n[36] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015. 1, 2, 6\n[37] Yu-Chuan Su and Kristen Grauman. Learning spherical con-\nvolution for fast features from 360◦ imagery. In NIPS, 2017.\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n[38] Yu-Chuan Su and Kristen Grauman. Making 360◦ video\nwatchable in 2d: Learning videography for click free view-\ning. In CVPR, 2017. 1, 2, 6\n[39] Yu-Chuan Su and Kristen Grauman. Learning compressible\n360◦ video isomers. In CVPR, 2018. 2\n[40] Yu-Chuan Su, Dinesh Jayaraman, and Kristen Grauman.\nPano2vid: Automatic cinematography for watching 360◦\nvideos. In ACCV, 2016. 1, 2, 6, 9\n[41] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In ICCV, 2015. 1\n[42] Ville Ukonaho. Global 360 camera sales\nforecast by segment: 2016 to 2022. https:\n//www.strategyanalytics.com/access-services/devices/\nmobile-phones/emerging-devices/market-data/report-detail/\nglobal-360-camera-sales-forecast-by-segment-2016-to-2022 ,\nMarch 2017. 1\n[43] Youngjae Yu, Sangho Lee, Joonil Na, Jaeyun Kang, and\nGunhee Kim. A deep ranking model for spatio-temporal\nhighlight detection from a 360◦ video. In AAAI, 2018. 1,\n2\n[44] Lihi Zelnik-Manor, Gabriele Peters, and Pietro Perona.\nSquaring the circle in panoramas. In ICCV, 2005. 2\n[45] Yinda Zhang, Shuran Song, Ping Tan, and Jianxiong Xiao.\nPanocontext: A whole-room 3d context model for panoramic\nscene understanding. In ECCV, 2014. 2\n[46] Ziheng Zhang, Yanyu Xu, Jingyi Yu, and Shenghua Gao.\nSaliency detection in 360 ◦ videos. In ECCV, 2018. 1, 2,\n3, 5, 6, 7, 8, 10\nFigure 14:Object detection examples on Pano2Vid.\n Figure 15: Object detection examples on 360-iﬁed Pascal\nVOC images."
}