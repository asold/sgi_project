{
  "title": "XCiT: Cross-Covariance Image Transformers",
  "url": "https://openalex.org/W3171206729",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222055945",
      "name": "El-Nouby, Alaaeldin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222055943",
      "name": "Touvron, Hugo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223306857",
      "name": "Caron, Mathilde",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745251680",
      "name": "Bojanowski, Piotr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2946677435",
      "name": "Douze Matthijs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214255973",
      "name": "Joulin, Armand",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742358871",
      "name": "Laptev, Ivan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222398475",
      "name": "Neverova, Natalia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2233704439",
      "name": "Synnaeve, Gabriel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222055946",
      "name": "Verbeek, Jakob",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2296768883",
      "name": "Jégou, Hervé",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963154697",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W2911925209",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2100398441",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2971315489",
    "https://openalex.org/W2950181225",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W1976794880",
    "https://openalex.org/W3158846111",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2963588253",
    "https://openalex.org/W3157914380",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2162931300",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2414711238",
    "https://openalex.org/W3100623370",
    "https://openalex.org/W2174726731",
    "https://openalex.org/W2250384498",
    "https://openalex.org/W2141362318",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W1984309565",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2544587078",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W1556531089",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3012324658",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W3091156754"
  ],
  "abstract": "Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a \"transposed\" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.",
  "full_text": "XCiT: Cross-Covariance Image Transformers\nAlaaeldin El-Nouby1,2 Hugo Touvron1,3 Mathilde Caron1,2 Piotr Bojanowski1\nMatthijs Douze1 Armand Joulin1 Ivan Laptev2 Natalia Neverova1\nGabriel Synnaeve1 Jakob Verbeek1 Hervé Jégou1\n1Facebook AI 2Inria 3Sorbonne University\nAbstract\nFollowing tremendous success in natural language processing, transformers have re-\ncently shown much promise for computer vision. The self-attention operation underlying\ntransformers yields global interactions between all tokens, i.e. words or image patches, and\nenables ﬂexible modelling of image data beyond the local interactions of convolutions. This\nﬂexibility, however, comes with a quadratic complexity in time and memory, hindering\napplication to long sequences and high-resolution images. We propose a “transposed”\nversion of self-attention that operates across feature channels rather than tokens, where\nthe interactions are based on the cross-covariance matrix between keys and queries. The\nresulting cross-covariance attention (XCA) has linear complexity in the number of tokens,\nand allows efﬁcient processing of high-resolution images. Our cross-covariance image\ntransformer (XCiT) – built upon XCA – combines the accuracy of conventional transform-\ners with the scalability of convolutional architectures. We validate the effectiveness and\ngenerality of XCiT by reporting excellent results on multiple vision benchmarks, includ-\ning (self-supervised) image classiﬁcation on ImageNet-1k, object detection and instance\nsegmentation on COCO, and semantic segmentation on ADE20k.\n1 Introduction\nTransformers architectures [69] have provided quantitative and qualitative breakthroughs in\nspeech and natural language processing (NLP). Recently, Dosovitskiy et al.[22] established\ntransformers as a viable architecture for learning visual representations, reporting competitive\nresults for image classiﬁcation while relying on large-scale pre-training. Touvron et al. [65]\nhave shown on par or better accuracy/throughput compared to strong convolutional baselines\nsuch as EfﬁcientNets [58] when training transformers on ImageNet-1k using extensive data\naugmentation and improved training schemes. Promising results have been obtained for other\nvision tasks, including image retrieval [23], object detection and semantic segmentation [44,\n71, 81, 83], as well as video understanding [2, 7, 24].\nOne major drawback of transformers is the time and memory complexity of the core self-\nattention operation, that increases quadratically with the number of input tokens, or similarly\nnumber of patches in computer vision. For w×h images, this translates to a complexity\nof O(w2h2), which is prohibitive for most tasks involving high-resolution images, such as\nobject detection and segmentation. Various strategies have been proposed to alleviate this\ncomplexity, for instance using approximate forms of self-attention [ 44, 81], or pyramidal\narchitectures which progressively downsample the feature maps [71]. However, none of the\nexisting solutions are fully satisfactory, as they either trade complexity for accuracy, or their\ncomplexity remains excessive for processing very large images.\nWe replace the self-attention, as originally introduced by Vaswani et al.[69], with a “trans-\nposed” attention that we denote as “cross-covariance attention” (XCA). Cross-covariance\nattention substitutes the explicit full pairwise interaction between tokens by self-attention\namong features, where the attention map is derived from the cross-covariance matrix com-\nputed over the key and query projections of the token features. Importantly, XCA has a linear\ncomplexity in the number of patches. To construct our Cross-Covariance Image Transformers\n(XCiT), we combine XCA with local patch interaction modules that rely on efﬁcient depth-\nwise convolutions and point-wise feedforward networks commonly used in transformers, see\nFigure 1. XCA can be regarded as a form of a dynamic 1×1 convolution, which multiplies\nall tokens with the same data-dependent weight matrix. We ﬁnd that the performance of\nour XCA layer can be further improved by applying it on blocks of channels, rather than\ndirectly mixing all channels together. This “block-diagonal” shape of XCA further reduces\nthe computational complexity with a factor linear in the number of blocks.\nCode: https://github.com/facebookresearch/xcit\n1\narXiv:2106.09681v2  [cs.CV]  18 Jun 2021\n<latexit sha1_base64=\"8/rbFhaoAyt80VM2uVIEw0doO78=\">AAADlHicvVJba9swFHbsXbrs0mSDvexFLCkksKVOd+kYG6SEwUpKSbalDUSpkRUlFrEtVzouDcJ/aD9nb/s3k50MmnTPOyD56DvX7/j4ScgVuO7vku3cuXvv/s6D8sNHj5/sVqpPz5RIJWVDKkIhRz5RLOQxGwKHkI0SyUjkh+zcX3Rz+/kVk4qL+AcsEzaJyDzmM04JGMirln7u1XFEIKAk1EdZo/dq0ESfEQZ2Dfq7mEFErjMcshk0cBKQGESksUxDNn7jJjDRb82d6XfmGvQuMIgE7WN1KUFPvUWWYcnnATTrGJc3ymAeFy/f19+yC32KgUdModNs5Tkwn1VIQED3siLxPgaS1v+CgxVY38zr6VXjo272v6jcLLlFy8StiU29y8y0SqVQ6jUVV0RyElOGGqNuExEAFud/42O5fCJMVtQnQAN0HAOThOYW1DjpHze9Ss1tuYWg20p7rdSstfS9yi88FTSNTHoaEqXG7YIokcBpyLIyThVLCF2QORsbNSam14kulipDewaZopmQ5sSACvRmhCaRUsvIN545Z7Vty8F/2cYpzD5MNI+T1PCmq0KzNEQgUL6haMoloxAujUKo5KZXRAOST8LscdkMob1N+bZydtBqv2+1Bwe1jrsex471wnppNay2dWh1rK9W3xpa1K7ah3bHPnKeO5+crvNl5WqX1jHPrA1xTv8AZJQlDg==</latexit>\nLocal Patch Interaction (LPI)\n<latexit sha1_base64=\"KPZvdnRo7kwadB5MhbdNDPu+nJk=\">AAADmXicvVJbb9MwFE5bLiPcOpB42YtFU6kV0CXjKgTSpgq00WlqGdsq1V3kuG5rNYkz+2RaZeU38V9449/gpEVaO545kp3j71y/kxMkIVfgur9L5cqt23fubtyz7z94+OhxdfPJqRKppOyEilDIfkAUC3nMToBDyPqJZCQKQnYWzNq5/eySScVF/APmCRtGZBLzMacEDORvln7WHRwRmFIS6r2s0XnZa6LPCAO7An0sxhCRqwyHbAwNnExJDCLSWKYhG7x2ExjqN+bO9Ftz9TrnGESCtrG6kKBH/izLsOSTKTQdjO2VMpjHxSsI9PfsXB9h4BFT6ChbePbMZxEyJaA7WZF4GwNJnb9gbwE6q3l9vWi8387+F5XrJddombglsZF/kZlWqRRKvaLikkhOYspQo99uIgLA4vxvfLTt+qEwaVGXAJ2igxiYJDQ3ocZh96BpOy8cv1pzW24h6KbiLZWatZSuX/2FR4KmkalBQ6LUwCvYEgmchiyzcapYQuiMTNjAqDExDQ91sVkZqhtkhMZCmhMDKtDrEZpESs2jwHjmxNW6LQf/ZRukMP4w1DxOUkOeLgqN0xCBQPmaohGXjEI4NwqhkpteEZ2SfBpmmW0zBG+d8k3ldKflvWt5vZ3arrscx4a1ZT23GpZnvbd2rX2ra51YtPys/Kn8pfy1slXZq+xXvi1cy6VlzFNrRSrHfwAfuCXi</latexit>\n+\n<latexit sha1_base64=\"KPZvdnRo7kwadB5MhbdNDPu+nJk=\">AAADmXicvVJbb9MwFE5bLiPcOpB42YtFU6kV0CXjKgTSpgq00WlqGdsq1V3kuG5rNYkz+2RaZeU38V9449/gpEVaO545kp3j71y/kxMkIVfgur9L5cqt23fubtyz7z94+OhxdfPJqRKppOyEilDIfkAUC3nMToBDyPqJZCQKQnYWzNq5/eySScVF/APmCRtGZBLzMacEDORvln7WHRwRmFIS6r2s0XnZa6LPCAO7An0sxhCRqwyHbAwNnExJDCLSWKYhG7x2ExjqN+bO9Ftz9TrnGESCtrG6kKBH/izLsOSTKTQdjO2VMpjHxSsI9PfsXB9h4BFT6ChbePbMZxEyJaA7WZF4GwNJnb9gbwE6q3l9vWi8387+F5XrJddombglsZF/kZlWqRRKvaLikkhOYspQo99uIgLA4vxvfLTt+qEwaVGXAJ2igxiYJDQ3ocZh96BpOy8cv1pzW24h6KbiLZWatZSuX/2FR4KmkalBQ6LUwCvYEgmchiyzcapYQuiMTNjAqDExDQ91sVkZqhtkhMZCmhMDKtDrEZpESs2jwHjmxNW6LQf/ZRukMP4w1DxOUkOeLgqN0xCBQPmaohGXjEI4NwqhkpteEZ2SfBpmmW0zBG+d8k3ldKflvWt5vZ3arrscx4a1ZT23GpZnvbd2rX2ra51YtPys/Kn8pfy1slXZq+xXvi1cy6VlzFNrRSrHfwAfuCXi</latexit>\n+\n<latexit sha1_base64=\"h7pdr1AQwdjbtyG3A29ciWZxZzE=\">AAADtXicvVJbb9owFE5hly67lG6Pe7EGlUBbKbBLp0mTOlVCq1oh2NYWCVN04jhgkcSpfdIWWfmFe9vb/s0cYFKhe96R7Bx/5/7leEkoNDYavzcKxXv3HzzcfOQ+fvL02VZp+/mZlqli/JTJUKq+B5qHIuanKDDk/URxiLyQn3vTw9x+fsWVFjL+gbOEDyMYxyIQDNBCo+2NnzsVGgFOGITmS1Y9ftOrkc+EIr9B810GGMFNRkMeYJUmE4hRRoaqNOSDt40Eh+advTPz3l694wuKMiF7VF8qNP5ommVUifEEaxVK3ZUyVMTzl+eZb9mF6VAUEdekky08e/azCJkAmuNsnniPIqSVv2BvAVZW847MovH+Yfa/Rrldcm0sG7cczB9dZrZVpqTWu0xegRIQM06q/cMaAUQe53/jk+vunEiblnQB2YQcxcgVsNxEqifdo5qt/Lritjn3d9tSXYPySYfjtVRTUm23O7VRqdyoN+ZC7irNpVJ2ltIdlX5RX7I0svVZCFoPmnMmQKFgIc9cmmqeAJvCmA+sGoMdZmjmW5eRHYv4JJDKnhjJHL0dYSDSehZ51jMnRa/bcvBftkGKwcehEXGSWmLYolCQhgQlyVeY+EJxhuHMKsCUsL0SNoGcKbvoriWhuT7yXeWsVW9+qDd7rfJBY0nHpvPSeeVUnaaz7xw4X52uc+qwQqvQL0DBK+4Xh0W/GCxcCxvLmBfOihTlH4JSL/Q=</latexit>\nFeed-Forward Network (FFN)\n<latexit sha1_base64=\"FoAHg4HMSRcRayfjk7hi/qD5hAw=\">AAADwHicvVJba9swFHaTXTrv0nZ73ItYU0jY2ibdlUGhWyCstIRkW9tAlIZjWalFZMuRjttmwn9yD4P9m8lJBr3seQckH33n/vkEqRQG6/XfS6Xynbv37i8/8B8+evxkZXXt6bFRmWb8iCmpdC8Aw6VI+BEKlLyXag5xIPlJMG4W9pNzro1QyXecpnwQw1kiRoIBOmi4tvRro0JjwIiBtJ/y6sGrbo3sEor8Eu03NcIYLnMq+QirNI0gQRVbqjPJ+6/rKQ7sG3fn9q27ugenFFVKtqmZaLThcJznVIuzCGsVSv1rZahIZq8gsF/zU9umKGJuSDufe3bdZx4SAdqDfJZ4myJklb9gdw5Wrucd2nnjvWb+v0a5WvLGWC5uMVg4nOSuVaaVMZtMnYMWkDBOqr1mjQAiT4q/8dH3Nw6VS0s6gCwi+wlyDawwkephZ7/mKr90aVqch5stpS9Ah6TN8ULpMam2Wu2afwhTrttKx8PV9fpWfSbkttJYKOveQjrD1Z80VCyLXStMgjH9xowU0CiY5LlPM8NTYGM4432nJuDmGtjZAuZkwyEhGSntToJkhl6NsBAbM40D51nwY27aCvBftn6Gow8DK5I0cxyxeaFRJgkqUmwzCYXmDOXUKcC0cL0SFkFBmtt535HQuDnybeV4Z6vxbqvR3Vnfqy/oWPaeey+8qtfw3nt73hev4x15rLRbYiVZisufy1FZlSdz19LSIuaZd03KP/4AC440Kg==</latexit>\nLayerNorm\n<latexit sha1_base64=\"LeZ03nPOmmTYMNuojoWcyqxFVTk=\">AAAD63icvVNbb9MwFM5SLiPcNnjkxWKt1Aq6teMqJKRBpYppU9UC2yrVXeU4bmM1sTP7ZFtl5S/wwgMI8cof4o1/g5MWtAvPWIpzci7fOd9nx08irqHR+LXklq5cvXZ9+YZ389btO3dXVu/ta5kqyvaojKTq+0SziAu2Bxwi1k8UI7EfsQN/2srjB8dMaS7FR5glbBiTieBjTglY12jVXaqUcUwgpCQyb7LqzuNeDb1GGNgpmA9yDDE5zXDExlDFSUgEyNhglUZs8KSRwNA8tXtmntmtt3OIQSZoA+sjBSYYTbMMKz4JoVbG2DvXBnNRfPm+eZ8dmg4GHjONOtk8s2df85KQgNnJCuANDCQt/3H25s7yedyRmQ/eb2X/i8rZlhdo2boFsWB0lNlRqZJa16k8JooTQRmq9ls1RACYyE/jledVdqWFRV0CNETbApgiNA+h6m53u2Y7P7IwbcaCeluqE6IC1GFwItUUVdvtjk3YJTOmOlLFXuVtJOm0HnAykcJi9lv1v508LpIUEMgpE3q0stZYbxQLXTaaC2PNWazuaOUnDiRNY4tFI6L1oFnoRxRwGrHMw6lmCaFTMmEDawpiJRia4q5mqGI9ARpLZR8BqPCerTAk1noW+zYzl1JfjOXOf8UGKYxfDk1BjAk6bzROI0sS5RcfBVwxCtHMGoQqbmdFNCS5vvb38KwIzYuULxv7m+vN5+vN3ubaVmMhx7LzwHnoVJ2m88LZct45XWfPoW7ofnK/uF9Lcelz6Vvp+zzVXVrU3HfOrdKP3+CFRDA=</latexit>\ninput tokens\n<latexit sha1_base64=\"eruebNSaeV04bSIvV8Oyx0QMgAk=\">AAAD9nicvVNLb9NAEHYdHsUUaOHIZUUSKREkTcpTSEhFkSKqRFECtI2UTaP1epOsYu+6u+O2keUfwoUDCHHlt3Dj37C2A+qDMyt5Pf5m5vtmZr1u6HMNjcavNbtw7fqNm+u3nNsbd+7e29y6f6BlpCjbp9KXaugSzXwu2D5w8NkwVIwErs8O3UUr9R+eMKW5FB9hGbJxQGaCTzklYKDJlr1RLuGAwJwSP36bVDpPBlX0BmFgZxB/kFMIyFmCfTaFCg7nRIAMYqwin42eNkIYx8/MnsTPzTboHGGQIdrG+lhB7E0WSYIVn82hWsLYuSCDuci+XDd+nxzFPQw8YBr1kjxyYF55ypxA3Eky4m0MJCr9AQc5WLrIO4nzwoet5H+1cl7yUlsmb9WYNzlOTKlUSa1rVJ4QxYmgDFWGrSoiAEykp/HaccpdaWhRnwCdoz0BTBGaulCl29+rGuXHhqbNmFdrS3VKlId6DE6lWqBKu90zAV2yZKonVeCUXV/SRc3jZCaF4Ry2an+VnDIXYQQI5IIJ7ZS6eZ2lyWaxUW9kC101miujaK1Wf7L5E3uSRoGhpT7RetTMRkkUcOqzxMGRZiGhCzJjI2MKYlTGcfbbJqhsEA9NpTKPAJSh5zNiEmi9DFwTmU5VX/al4L98owimr8Zx1iITNBeaRr5pF6V3AHlcMQr+0hiEKm5qRXRO0lGbm+KYITQvt3zVONipN1/Um4Od4m5jNY5166H1yKpYTeultWu9s/rWvkVtbX+yv9hfC2eFz4Vvhe95qL22ynlgXViFH78Bn7pH/w==</latexit>\nL⇥\n<latexit sha1_base64=\"1CEKzsCFOnoRXAG5s9zr9j/7SaY=\">AAAEAnicvVNLb9NAEHYTHsW8WrggcVnRREoEaZPyFBJSUaSIqlGUAG0jZdNovd4kq6x33d1x28iyuPBXuHAAIa78Cm78G9Z2QH1wZiWvx9/MfN/MrNcLBTdQr/9aKhQvXb5ydfmae/3GzVu3V1bv7BkVacp2qRJK9z1imOCS7QIHwfqhZiTwBNv3Zs3Uv3/EtOFKvod5yIYBmUg+5pSAhUarhXvlEg4ITCkR8euksvOoV0WvEAZ2AvE7NYaAnCRYsDFUcDglElQQYx0JNnhcD2EYP7F7Ej+1W2/nAIMK0QY2hxpifzRLEqz5ZArVEsbuGRnMZfblefHb5CDuYOABM6iT5JE9+8pTpgTinSQj3sBAotIfsJeDpbO8ozgvvN9M/lcrpyXPtWXzFo35o8PElkq1MqZG1RHRnEjKUKXfrCICwGR6Gi9dt9xWlhZ1CdAp2pbANKGpC1Xa3e2qVX5oaVqM+bWW0sdE+6jD4FjpGaq0Wh0b0CZzpjtKB27ZE4rOaj4nEyUtZ79Z+6vklrkMI0CgZkwaS9vOCy25/SZ/j0RKMlpZq6/Xs4UuGo2FseYsVne08hP7ikaBlaCCGDNoZGMlGjgVLHFxZFhI6IxM2MCakljBYZz9wgkqW8RHY6XtIwFl6OmMmATGzAPPRqYTNud9Kfgv3yCC8YthnLXLJM2FxpGwraP0PiCfa0ZBzK1BqOa2VkSnJB27vTWuHULjfMsXjb3N9caz9UZvc22rvhjHsnPfeeBUnIbz3Nly3jhdZ9ehhQ+FT4Uvha/Fj8XPxW/F73loYWmRc9c5s4o/fgP7cUw7</latexit>\nXCiT layer\n<latexit sha1_base64=\"2zVyOUg3FwZMgvsI1cBI4DvtOPQ=\">AAACbHicbVFNbxMxEPVu+Sjho2nhAKqQLJKiVKrCbqGlF6QiLki9NIK0leIQeR1vYsUfW3sWNbL2xD/kxk/gwm/A2eRQWkbyzNObN/L4OSukcJAkv6J47c7de/fXHzQePnr8ZKO5uXXmTGkZ7zMjjb3IqONSaN4HAZJfFJZTlUl+ns0+Lfrn37l1wuivMC/4UNGJFrlgFAI1av5oE0Vhyqj0H6vOyV5vF3/ABPgV+C8mB0WvKiJ5Dh1STKkGozyxpeSDt0kBQ/8u5MofhNQ7+UbAFPgNcZcW/Hg0qypixWQKu21CGjvtXig1WAqv6dqjZivpJnXg2yBdgRZaxemo+ZOMDSsV18AkdW6Q1ttQC4JJXjVI6XhB2YxO+CBATRV3Q1+bVeGdwIxxbmw4GnDNXp/wVDk3V1lQLpxxN3sL8n+9QQn50dALXZTANVtelJcSg8EL5/FYWM5AzgOgzIqwK2ZTaimD8D+NYEJ688m3wdl+Nz3spr391vHRyo51tI1eoQ5K0Xt0jD6jU9RHDP2ONqLn0YvoT/ws3o5fLqVxtJp5iv6J+PVfmRq6EQ==</latexit>\nA(K, Q) = Softmax\n0\nBBBB@\n1\nCCCCA\n<latexit sha1_base64=\"zU2dNxLFyIwFvh8H/IjCuhUNVes=\">AAAC33ictVJLbxMxEPYur7I8GuDIxSKJlEoo7JZXL0hFvSD10gjSRorDyut4Eytee7FnUSNrL1w4gBBX/hY3/ghnnE0OoeXKSB5/+uYbzXjGWSmFhTj+FYRXrl67fmPnZnTr9p27u61790+trgzjQ6alNqOMWi6F4kMQIPmoNJwWmeRn2eJoFT/7yI0VWr2DZcknBZ0pkQtGwVNp63e3QwoKc0ale133jh8P9vArTICfg3urcyjoeU0kz6FHyjlVoAtHTCX5+GlcwsQ98752z70bHL8noEv8hNgPBtw0XdQ1MWI2h70OIVG3M/BXA9bCLV0n2u4hdevqo6P6//WTttpxP24MXwbJBrTRxk7S1k8y1awquAImqbXjpKlHDQgmeR2RyvKSsgWd8bGHihbcTlyznxp3PTPFuTb+KMANu53haGHtssi8cjUHezG2Iv8VG1eQH0ycUGUFXLF1obySGDReLRtPheEM5NIDyozwvWI2p4Yy8F8i8kNILj75Mjjd7ycv+slgv314sBnHDnqIHqEeStBLdIjeoBM0RCwgwafgS/A1pOHn8Fv4fS0Ng03OA/SXhT/+AEZZ55A=</latexit>\nAXC (K, Q) = Softmax\n0\nBBBB@\n1\nCCCCA\n<latexit sha1_base64=\"5rjicXqAdnEdJFdxF5d1qKWfYLY=\">AAACbHicbVHLbhMxFPVMeZTwaHgsQBWSRSYolaow0xboBqmIDVI3jSBtpThEHseTWPFjat9BjaxZ8Yfs+AQ2fAPOY1FaruR7j849V74+zkspHKTpryjeuHX7zt3Ne437Dx4+2mo+fnLqTGUZ7zMjjT3PqeNSaN4HAZKfl5ZTlUt+ls8+Lfpn37l1wuivMC/5UNGJFoVgFAI1av5oJ0RRmDIq/ce6c7zb28EfMAF+Cf6LKUDRy5pIXkCHlFOqwShPbCX5YD8tYegPQq7925B6x98ImBK/Ie7Cgh+PZnVNrJhMYSchpJH0Qg61nax0V2TJqNlKu+ky8E2QrUELreNk1PxJxoZVimtgkjo3yJbLUAuCSV43SOV4SdmMTvggQE0Vd0O/NKvG7cCMcWFsOBrwkr064alybq7yoFwY4673FuT/eoMKisOhF7qsgGu2uqioJAaDF87jsbCcgZwHQJkVYVfMptRSBuF/GsGE7PqTb4LTvW72rpv19lpHh2s7NtE2eoU6KEPv0RH6jE5QHzH0O9qKnkcvoj/xs3g7frmSxtF65in6J+LXfwGCCboR</latexit>\nQ\n<latexit sha1_base64=\"Iru02VIUgZo2sFV5BD2xOmnMFVk=\">AAACbHicbVHLbhMxFPVMeZTwaHgsQBWSRSYolaow0xboBqmIDVI3jSBtpThEHseTWPFjat9BjaxZ8Yfs+AQ2fAPOY1FaruR7j849V74+zkspHKTpryjeuHX7zt3Ne437Dx4+2mo+fnLqTGUZ7zMjjT3PqeNSaN4HAZKfl5ZTlUt+ls8+Lfpn37l1wuivMC/5UNGJFoVgFAI1av5oJ0RRmDIq/ce6c7zb28EfMAF+Cf6LKUDRy5pIXkCHlFOqwShPbCX5YD8tYegPQq7925B6x98ImBK/Ie7Cgh+PZnVNrJhMYSchpNFOeqEEkKx0V2TJqNlKu+ky8E2QrUELreNk1PxJxoZVimtgkjo3yJbLUAuCSV43SOV4SdmMTvggQE0Vd0O/NKvG7cCMcWFsOBrwkr064alybq7yoFwY4673FuT/eoMKisOhF7qsgGu2uqioJAaDF87jsbCcgZwHQJkVYVfMptRSBuF/GsGE7PqTb4LTvW72rpv19lpHh2s7NtE2eoU6KEPv0RH6jE5QHzH0O9qKnkcvoj/xs3g7frmSxtF65in6J+LXfwGBHroR</latexit>\nK >/\np\ndk\n<latexit sha1_base64=\"mzzZzEa2n4o+HNrlONK9hYkgqHE=\">AAAC73ictVJLbxMxEPYuFEp4NIUjF4tspFRC6W559YJU1AtSLo0gbaQ4rBzHm7XiXW/tWdTI2j/BhQMIceXvcOPf4N3k0AdXRvL48zczms9jzwopDIThH8+/dXvrzt3te637Dx4+2mnvPj41qtSMj5iSSo9n1HApcj4CAZKPC81pNpP8bLY8ruNnn7k2QuUfYVXwaUYXuUgEo+CoeNfb6gYko5AyKu27qjd4PtzDbzEBfgH2g0ogoxcVkTyBHilSmoPKLNGl5JMXYQFT+9L5yr5ybjj4REAVeJ+Ycw12Hi+rimixSGEvIKTVDYZucyAgKQU7qJrsfQK0DOrg1eMlSbFdixkfV/9PXtzuhP2wMXwTRBvQQRs7idu/yVyxMuM5MEmNmURNP6pBMMmrFikNLyhb0gWfOJjTjJupbd6rwl3HzHGitFs54Ia9XGFpZswqm7nMeg7meqwm/xWblJAcTq3IixJ4ztaNklJiULh+fDwXmjOQKwco08JpxSylmjJwX6TlhhBdv/JNcHrQj173o+FB5+hwM45t9BQ9Qz0UoTfoCL1HJ2iEmCe9L94377t/7n/1f/g/16m+t6l5gq6Y/+svFvnrxg==</latexit>\nˆK >/⌧\n<latexit sha1_base64=\"2F7+uxwf9NhMKztOTe242WTCH2g=\">AAAC8HictVJLbxMxEPYuUMryaApHLhZJpFRC6W559YJU1AtSLl1B2khxWHkdb9aK94E9ixpZ+yu4cAChXvtzuPFvcHZzoC1XRvL40zczms8zjkspNPj+b8e9dfvO1t3te979Bw8f7XR2H5/qolKMj1khCzWJqeZS5HwMAiSflIrTLJb8LF4er+NnX7jSosg/wqrks4wucpEIRsFS0a6z1e+RjELKqDTv6sHoebiH32IC/BzMhyKBjJ7XRPIEBqRMaQ5FZoiqJJ++8EuYmZfW1+aVdeHoE4GixPtEf1Zg5tGyrokSixT2eoR4/V5orwaQlIIZ1U36PgFa9byWC1uu513RFJlWzeS4/n/6ok7XH/qN4Zsg2IAu2thJ1PlF5gWrMp4Dk1TradD0owoEk7z2SKV5SdmSLvjUwpxmXM9Ms7Aa9y0zx0mh7MkBN+zfFYZmWq+y2Gau56Cvx9bkv2LTCpLDmRF5WQHPWdsoqSSGAq+3j+dCcQZyZQFlSlitmKVUUQb2j3h2CMH1J98EpwfD4PUwCA+6R4ebcWyjp+gZGqAAvUFH6D06QWPEnMz56nx3frjK/eb+dC/aVNfZ1DxBV8y9/APgy+yM</latexit>\nˆQ>\n<latexit sha1_base64=\"TgWpT0wMMIg4lLuzyVhnYotJxzk=\">AAADUnictVJLbxMxEPYmPEooNIUjF4skUiqhNFug7QWpqBekSFUDpI0Upyuv481a8T5qz6JGln8jEuLCD+HCAXA2QZQUjozk8edvZjwPTZhLoaHb/eJVqrdu37m7ca92f/PBw6369qMznRWK8QHLZKaGIdVcipQPQIDkw1xxmoSSn4ez44X9/ANXWmTpe5jnfJzQaSoiwSg4Ktj24laTJBRiRqV5bdu9Z/0d/AoT4Fdg3mURJPTKEskjaJM8pilkiSGqkHz0vJvD2Lxw2pqXTvV7FwSyHO8SfanATIKZtUSJaQw7TUJq17MQkZavMDRv7YU5ISASrvGJXTi2mn13lYDEFEzPlv/uEqBF8xfZX5Ll+/e/gVnWPTy2/7GTf6Zca8vFrRqbBJe2GdQb3U63FHwT+CvQQCs5DeqfyCRjRcJTYJJqPfLLOqkCwSS3NVJonlM2o1M+cjClLtXYlCthccsxExxlyp0UcMlejzA00XqehM5zUbJety3Iv9lGBUSHYyPSvACesmWiqJAYMrzYLzwRijOQcwcoU8LVillMFWXgtrDmhuCvt3wTnO11/P2O399rHB2uxrGBnqCnqI18dICO0Bt0igaIeR+9r95370flc+Vb1atWl64VbxXzGP0h1c2fJZESdQ==</latexit>\nA 2 RN ⇥N\n<latexit sha1_base64=\"LAiCoX4t7SbGeC9gI1mbitiJdNY=\">AAADUnictVJLbxMxEPYmPEookMKRi0VSKZVQmi2vXpCKekGKVDVA2khxuvI63qwV76P2LGpk+TciIS78EC4cAGc3SCSFIyN5/PmbtzVhLoWGXu+rV6vfuHnr9tadxt3te/cfNHcenumsUIwPWSYzNQqp5lKkfAgCJB/litMklPw8nB8v7ecfudIiSz/AIueThM5SEQlGwVHBjhfvtklCIWZUmje203862MOvMQF+BeZ9FkFCryyRPIIOyWOaQpYYogrJx896OUzMc6eteeHUoH9BIMvxPtGXCsw0mFtLlJjFsNcmpLFWhoi0fIWheWcvzAkBkXCNT2zlOXBXFRJTMH1bJt4nQIv2b3JQke31vIGpGh8d2/84yr8qbkzlwlZzTYNL2w6arV63Vwq+DvwVaKGVnAbNz2SasSLhKTBJtR77ZZtUgWCS2wYpNM8pm9MZHzuYUldqYsqVsHjXMVMcZcqdFHDJ/hlhaKL1Igmd57JlvWlbkn+zjQuIDidGpHkBPGVVoaiQGDK83C88FYozkAsHKFPC9YpZTBVl4Law4T7B3xz5Ojg76Povu/7goHV0uPqOLfQYPUEd5KNX6Ai9RadoiJj3yfvm/fB+1r7Uvte9er1yrXmrmEdoTerbvwD5khJ1</latexit>\nAXC 2 Rdk ⇥dq\n<latexit sha1_base64=\"yeUADHGIyi00gAsuWK+ziUGW3+s=\">AAAEBHicvVNLb9NAEHYTHsW8WrjBZUUTKRF9JOV5QWoVKaJqFCXQR6RsGq3Xm2SV9a67O24bWT5w4a9w4QBCXPkR3Pg3rO2A+uDMSl6Pv5n5Zr5ZrxcKbqBW+7VQKF67fuPm4i339p279+4vLT84MCrSlO1TJZTuecQwwSXbBw6C9ULNSOAJduhNG6n/8IRpw5Xcg1nIBgEZSz7ilICFhsuFR+USDghMKBHxdlLZXe1W0RuEgZ1B/F6NICBnCRZsBBUcTogEFcRYR4L1n9VCGMTP7Z7EL+zW3T3CoEK0gc2xhtgfTpMEaz6eQLWEsXuhDOYy+/K8+F1yFLcx8IAZ1E7yyK595SkTAvFukhFvYCBR6Q/YzcHSRd5hnDfeayT/S8r5kpdk2by5MH94nJTchlbGrDXUCdGcSMrQNgCT6TmgSq+xXXXdcktZVtQhQCdoRwLThOb+Vmenags/tYKbjPlrTaVPifZRm8Gp0lNUaTbbNqBFZky3lQ7csicUna75nIyVtJy2gFvmMowAgZoyaSxbK2/PcvYafA+JNHkV/YWHSyu19Vq20FWjPjdWnPnqDJd+Yl/RKLCiqCDG9OvZYIkGTgVLXBwZFhI6JWPWt6Yktsogzn7iBJUt4qOR0vaRgDL0fEZMAmNmgWcj0xmby74U/JevH8Ho9SDOpDNJ80KjSNgxoPRGIJ9rRkHMrEGo5rZXRCcknby9N64dQv2y5KvGweZ6/eV6vbu5srU6H8ei89h54lScuvPK2XLeOh1n36GFD4VPhS+Fr8WPxc/Fb8XveWhhYZ7z0Lmwij9+A261S44=</latexit>\nCross-Covariance Attention (XCA)\n<latexit sha1_base64=\"KPZvdnRo7kwadB5MhbdNDPu+nJk=\">AAADmXicvVJbb9MwFE5bLiPcOpB42YtFU6kV0CXjKgTSpgq00WlqGdsq1V3kuG5rNYkz+2RaZeU38V9449/gpEVaO545kp3j71y/kxMkIVfgur9L5cqt23fubtyz7z94+OhxdfPJqRKppOyEilDIfkAUC3nMToBDyPqJZCQKQnYWzNq5/eySScVF/APmCRtGZBLzMacEDORvln7WHRwRmFIS6r2s0XnZa6LPCAO7An0sxhCRqwyHbAwNnExJDCLSWKYhG7x2ExjqN+bO9Ftz9TrnGESCtrG6kKBH/izLsOSTKTQdjO2VMpjHxSsI9PfsXB9h4BFT6ChbePbMZxEyJaA7WZF4GwNJnb9gbwE6q3l9vWi8387+F5XrJddombglsZF/kZlWqRRKvaLikkhOYspQo99uIgLA4vxvfLTt+qEwaVGXAJ2igxiYJDQ3ocZh96BpOy8cv1pzW24h6KbiLZWatZSuX/2FR4KmkalBQ6LUwCvYEgmchiyzcapYQuiMTNjAqDExDQ91sVkZqhtkhMZCmhMDKtDrEZpESs2jwHjmxNW6LQf/ZRukMP4w1DxOUkOeLgqN0xCBQPmaohGXjEI4NwqhkpteEZ2SfBpmmW0zBG+d8k3ldKflvWt5vZ3arrscx4a1ZT23GpZnvbd2rX2ra51YtPys/Kn8pfy1slXZq+xXvi1cy6VlzFNrRSrHfwAfuCXi</latexit>\n+\n<latexit sha1_base64=\"PNIauBE2MLCA7j7XkFwTzTjbG/0=\">AAAESXicvVNLb9NAEHaTAsU82sKRy4omUiL6SMrzgtQqUkSVKEqgj0jZ1FqvN8kqa6+7O24bWf57XLhx4z9w4QBCnFjbQfQljqzk3fE3M9+8NG4ouIZa7ctCobh46/adpbv2vfsPHi6vrD461DJSlB1QKaTqu0QzwQN2ABwE64eKEd8V7MidNlL90SlTmstgH2YhG/pkHPARpwQM5KwWnHIJ+wQmlIh4N6m01ntV9BZhYOcQf5Aj8Ml5ggUbQQWHExKA9GOsIsEGz2shDOMX5k7il+bqtY4xyBBtYX2iIPacaZJgxccTqJYwti+FwTzI/lw3fp8cxx0M3GcadZLcsmee3GVCIG4lGfEWBhKV/oC9HCxd5nXiPPF+I/lfpVwMeaUs4zcvzHNOEpNqQ0mtNxrylChOAsrQLgAL0kGgSr+xW7XtclsaWtQlQCdoLwCmCM317e5e1UR+ZmiajHkbTanOiPJQh8GZVFNUaTY7xqBNZkx1pPLtsisknW54nIxlYDhNALvMgzACBHLKAm3Y2nl+hrPf4PtIpM7r6C9caiFTE7ppVmlTSsa2h/5hYqp2VtZqm7XsoOtCfS6sWfPTdVY+Y0/SyDeNoYJoPahn0yEKOBUssXGkWUjolIzZwIgBMaGGcbYJCSobxEMjqcwXAMrQix4x8bWe+a6xTBPWV3UpeJNuEMHozTDO2scCmgcaRcK0EqVrhTyuGAUxMwKhiptcEZ2QdHpm+WzThPrVkq8Lh9ub9Veb9d722s76vB1L1hPrqVWx6tZra8d6Z3WtA4sWPha+Fr4XfhQ/Fb8VfxZ/5aaFhbnPY+vSWSz+Bh0uZPw=</latexit>\nK 2 RN ⇥dk , Q 2 RN ⇥dq\n<latexit sha1_base64=\"FoAHg4HMSRcRayfjk7hi/qD5hAw=\">AAADwHicvVJba9swFHaTXTrv0nZ73ItYU0jY2ibdlUGhWyCstIRkW9tAlIZjWalFZMuRjttmwn9yD4P9m8lJBr3seQckH33n/vkEqRQG6/XfS6Xynbv37i8/8B8+evxkZXXt6bFRmWb8iCmpdC8Aw6VI+BEKlLyXag5xIPlJMG4W9pNzro1QyXecpnwQw1kiRoIBOmi4tvRro0JjwIiBtJ/y6sGrbo3sEor8Eu03NcIYLnMq+QirNI0gQRVbqjPJ+6/rKQ7sG3fn9q27ugenFFVKtqmZaLThcJznVIuzCGsVSv1rZahIZq8gsF/zU9umKGJuSDufe3bdZx4SAdqDfJZ4myJklb9gdw5Wrucd2nnjvWb+v0a5WvLGWC5uMVg4nOSuVaaVMZtMnYMWkDBOqr1mjQAiT4q/8dH3Nw6VS0s6gCwi+wlyDawwkephZ7/mKr90aVqch5stpS9Ah6TN8ULpMam2Wu2afwhTrttKx8PV9fpWfSbkttJYKOveQjrD1Z80VCyLXStMgjH9xowU0CiY5LlPM8NTYGM4432nJuDmGtjZAuZkwyEhGSntToJkhl6NsBAbM40D51nwY27aCvBftn6Gow8DK5I0cxyxeaFRJgkqUmwzCYXmDOXUKcC0cL0SFkFBmtt535HQuDnybeV4Z6vxbqvR3Vnfqy/oWPaeey+8qtfw3nt73hev4x15rLRbYiVZisufy1FZlSdz19LSIuaZd03KP/4AC440Kg==</latexit>\nLayerNorm\n<latexit sha1_base64=\"FoAHg4HMSRcRayfjk7hi/qD5hAw=\">AAADwHicvVJba9swFHaTXTrv0nZ73ItYU0jY2ibdlUGhWyCstIRkW9tAlIZjWalFZMuRjttmwn9yD4P9m8lJBr3seQckH33n/vkEqRQG6/XfS6Xynbv37i8/8B8+evxkZXXt6bFRmWb8iCmpdC8Aw6VI+BEKlLyXag5xIPlJMG4W9pNzro1QyXecpnwQw1kiRoIBOmi4tvRro0JjwIiBtJ/y6sGrbo3sEor8Eu03NcIYLnMq+QirNI0gQRVbqjPJ+6/rKQ7sG3fn9q27ugenFFVKtqmZaLThcJznVIuzCGsVSv1rZahIZq8gsF/zU9umKGJuSDufe3bdZx4SAdqDfJZ4myJklb9gdw5Wrucd2nnjvWb+v0a5WvLGWC5uMVg4nOSuVaaVMZtMnYMWkDBOqr1mjQAiT4q/8dH3Nw6VS0s6gCwi+wlyDawwkephZ7/mKr90aVqch5stpS9Ah6TN8ULpMam2Wu2afwhTrttKx8PV9fpWfSbkttJYKOveQjrD1Z80VCyLXStMgjH9xowU0CiY5LlPM8NTYGM4432nJuDmGtjZAuZkwyEhGSntToJkhl6NsBAbM40D51nwY27aCvBftn6Gow8DK5I0cxyxeaFRJgkqUmwzCYXmDOXUKcC0cL0SFkFBmtt535HQuDnybeV4Z6vxbqvR3Vnfqy/oWPaeey+8qtfw3nt73hev4x15rLRbYiVZisufy1FZlSdz19LSIuaZd03KP/4AC440Kg==</latexit>\nLayerNorm\n<latexit sha1_base64=\"jtdbbq1umaFiw3sS5idIFzHKLag=\">AAAEanicvVNba9swFHaTbOu8dWs72Bh9EWsCCW3SpLu+DFoCYSUhJOstEKdBlpVERJZc6bhtMIb9xr3tF+xlP2JynLHe2OMElo4/fef2HeQGnGmoVn8sZbK5Bw8fLT+2nzxdefZ8dW39RMtQEXpMJJeq52JNORP0GBhw2gsUxb7L6ak7rSf3pxdUaSbFEcwCOvDxWLARIxgMNFzLfCvkHR/DhGAe7cfF5na3hD4jB+gVRIdyBD6+ih1OR1B0ggkWIP3IUSGn/bfVAAbRO7PH0XuzdZtnDsgA7Tj6XEHkDadx7Cg2nkAp7zj2jTQOE/M/142+xmdR2wHmU43accrsmiN1mWCImvE88I4DOMz/AbspmL8Zdxilhffq8f9q5XrKW20Zv0Vj3vA8NqXWldS6XJcXWDEsCEX7AFQkg0DFXn2/ZB9SPirjv+AJ1pdYMEQBYV4p2XahJU1a1MFAJuhAAFWYpNRW56BkKtsyaRqUeuWGVJdYeahN4VKqKSo2Gm1DaOEZVW2pfLvgckmmZY/hsRQmpinALjARhIBATqnQJlorrd/E7NXZEeKJ8za6BuebyDSN7htmolrekLvoHxQjy3B1s1qpzhe6a9QWxqa1WJ3h6nfHkyT0jUiEY637tfn4sAJGOI1tJ9Q0wGSKx7RvTIFNqkE0fyoxKhjEQyOpzCcAzdHrHhH2tZ75rmEmBevbdwl4310/hNGnQTTXjwqSJhqF3GiJkneHPKYoAT4zBiaKmVoRmeBkfOZ12kaE2u2W7xonu5Xah0qtu7u5t72QY9nasN5YRatmfbT2rC9Wxzq2SOZndiX7Mvsq+yu3nnud20ipmaWFzwvrxsrlfwPJ7Gtm</latexit>\nSelf-attention (Vaswani et al.)\n<latexit sha1_base64=\"9edzzRL4uMWnIOw+vcyPEh06C+k=\">AAAEanicvVNba9swFHaTbOu8dWs72Bh9EasDCWvSpLu+DFoCYaUhJOtlgSgNsqwkIrLlSsdtgzHsN+5tv2Av+xGT44z1xh4nsHT86Tu37yA3FFxDrfZjKZcv3Lv/YPmh/ejxypOnq2vrJ1pGirJjKoVUPZdoJnjAjoGDYL1QMeK7gn11p430/us5U5rL4AhmIRv4ZBzwEacEDDRcy30rOtgnMKFExHtJ6WCrW0afEAZ2CfGhHIFPLhMs2AhKOJyQAKQfYxUJ1n9TC2EQvzV7Er8zW/fgFIMM0TbWZwpibzhNEqz4eAJlB2P7WhrMg/mf68ZfktO4jYH7TKN2kjG75shcJgTig2QeeBsDiZw/YDcDnetxh3FWeK+R/K9Wrqa80ZbxWzTmDc8Sx24oqXWlIc+J4iSgDO0BsCCdAyr1Gntlu3jIxKhC/qInRF+QgCMGiIhq2baLLWnSog4BOkH7ATBFaEZtdfZNAOe1UaTJmFdpSnVBlIfaDC6kmqJSs9k2hBaZMdWWyreLrpB0WvE4GcvAxDQV2EUehBEgkFMWaBOtldVvYvYa/AiJ1HkLXYGdA2SaRncNM1XNMeQu+gfFyDJc3axVa/OFbhv1hbFpLVZnuPode5JGvhGJCqJ1vz4fH1HAqWCJjSPNQkKnZMz6xgyISTWI508lQUWDeGgklfkCQHP0qkdMfK1nvmuYacH65l0K3nXXj2D0cRDP9WMBzRKNImG0ROm7Qx5XjIKYGYNQxU2tiE5IOj7zOm0jQv1my7eNk51q/X213t3Z3N1ayLFsbVivrJJVtz5Yu9Znq2MdWzT3M7+Sf55/kf9VWC+8LGxk1NzSwueZdW0VnN/Qhmtm</latexit>\nCross-Covariance Attention (XCA)\nFigure 1: Our XCiT layer consists of three main blocks, each preceded by LayerNorm and followed by a residual\nconnection: (i) the core cross-covariance attention (XCA) operation, (ii) the local patch interaction (LPI) module, and\n(iii) a feed-forward network (FFN). By transposing the query-key interaction, the computational complexity of XCA\nis linear in the number of data elements N, rather than quadratic as in conventional self-attention.\nGiven its linear complexity in the number of tokens, XCiT can efﬁciently process images\nwith more than thousand pixels in each dimension. Notably, our experiments show that XCiT\ndoes not compromise the accuracy and achieves similar results to DeiT [ 65] and CaiT [68]\nin comparable settings. Moreover, for dense prediction tasks such as object detection and\nimage segmentation, our models outperform popular ResNet [28] backbones as well as the\nrecent transformer-based models [44, 71, 81]. Finally, we also successfully apply XCiT to the\nself-supervised feature learning using DINO [12], and demonstrate improved performance\ncompared to a DeiT-based backbone [65].\nOverall, we summarize our contributions as follows:\n• We introduce cross-covariance attention (XCA), which provides a “transposed” alternative\nto conventional self-attention, attending over channels instead of tokens. Its complexity is\nlinear in the number of tokens, allowing for efﬁcient processing of high-resolution images,\nsee Figure 2.\n• XCA attends to a ﬁxed number of channels, irrespective of the number of tokens. As a\nresult, our models are signiﬁcantly more robust to changes in image resolution at test time,\nand are therefore more amenable to process variable-size images.\n• For image classiﬁcation, we demonstrate that our models are on par with state-of-the-\nart vision transformers for multiple model sizes using a simple columnar architecture,\ni.e., in which we keep the resolution constant across layers. In particular, our XCiT-L24\nmodel achieves 86.0% top-1 accuracy on ImageNet, outperforming its CaiT-M24 [68] and\nNFNet-F2 [10] counterparts with comparable numbers of parameters.\n• For dense prediction tasks with high-resolution images, our models outperform ResNet\nand multiple transformer-based backbones. On the COCO benchmark, we achieve a strong\nperformance of 48.5% and 43.7% mAP for object detection and instance segmentation\nrespectively. Moreover, we report 48.4% mIoU for semantic segmentation on the ADE20k\nbenchmark, outperforming the state-of-the-art Swin Transformer [44] backbones across all\ncomparable model sizes.\n• Finally, our XCiT model is highly effective in self-supervised learning setups, achieving\n80.9% top-1 accuracy on ImageNet-1k using DINO [12].\n2 Related work\nDeep vision transformers. Training deep vision transformers can be challenging due to\ninstabilities and optimization issues. Touvron et al. [68] successfully train models with up\nto 48 layers using LayerScale, which weighs contributions of residual blocks across layers\nand improves optimization. Additionally, the authors introduce class attention layers which\ndecouple the learning of patch features and the feature aggregation stage for classiﬁcation.\nSpatial structure in vision transformers. Yuan et al. [79] propose applying a soft split for\npatch projection with overlapping patches which is applied repeatedly across model layers,\nreducing the number of patches progressively. Han et al.[27] introduce a transformer module\n2\nfor intra-patch structure, exploiting pixel-level information and integrating with an inter-\npatch transformer to attain higher representation power. d’Ascoli et al. [19] consider the\ninitialization of self-attention blocks as a convolutional operator, and demonstrate that such\ninitialization improves the performance of vision transformers in low-data regimes. Graham\net al. [26] introduce LeViT, which adopts a multi-stage architecture with progressively reduced\nfeature resolution similar to popular convolutional architectures, allowing for models with\nhigh inference speed while retaining a strong performance. Moreover, the authors adopt a\nconvolution-based module for extracting patch descriptors. Yuan et al. [78] improve both the\nperformance and the convergence speed of vision transformers by replacing the linear patch\nprojection with convolutional layers and max-pooling, as well as modifying the feed-forward\nnetworks in each transformer layer to incorporate depth-wise convolutions.\nEfﬁcient attention. Numerous methods for efﬁcient self-attention have been proposed in\nthe literature to address the quadratic complexity of self-attention in the number of input\ntokens. These include restricting the span of the self-attention to local windows [ 48, 50],\nstrided patterns [14], axial patterns [30], or an adaptive computation across layers [57]. Other\nmethods provide an approximation of the self-attention matrix which can be achieved by a\nprojection across the token dimension [70], or through a factorization of the softmax-attention\nkernel [15, 37, 56, 77], which avoids explicit computation of the attention matrix. While\nconceptually different, our XCA performs similar computations without being sensitive to\nthe choice of the kernel. Similarly, Lee-Thorp et al. [41] achieve faster training by substituting\nself-attention with unparametrized Fourier Transform. Other efﬁcient attention methods rely\non local attention and adding a small number of global tokens, thus allowing interaction\namong all tokens only by hopping through the global tokens [1, 5, 34, 80].\nTransformers for high-resolution images. Several works adopt visual transformers to high-\nresolution image tasks beyond image classiﬁcation, such as object detection and image\nsegmentation. Wang et al. [71] design a model with a pyramidal architecture and address\ncomplexity by gradually reducing the spatial resolution of keys and values. Similarly, for\nvideo recognition Fan et al. [24] utilize pooling to reduce the resolution across the spatial and\ntemporal dimensions to allow for an efﬁcient computation of the attention matrix. Zhang et al.\n[81] adopt global tokens and local attention to reduce the model complexity, while Liu et al.\n[44] provide an efﬁcient method for local attention with shifted windows. In addition, Zheng\net al. [83] and Ranftl et al. [54] study problems like semantic segmentation and monocular\ndepth estimation with the quadratic self-attention operation.\nData-dependent layers. Our XCiT layer can be regarded as a “dynamic”1×1 convolution,\nwhich multiplies all token features with the same data-dependent weight matrix, derived\nfrom the key and query cross-covariance matrix. In the context of convolutional networks,\nDynamic Filter Networks [9] explore a related idea, using a ﬁlter generating subnetwork to\nproduce convolutional ﬁlters based on features in previous layers. Squeeze-and-Excitation\nnetworks [32] use data dependent 1×1 convolutions in convolutional architectures. Spatially\naverage-pooled features are fed to a 2-layer MLP which produces per channel scaling param-\neters. Closer in spirit to our work, Lambda layers propose a way to ensure global interaction\nin ResNet models [4]. Their “content-based lambda function” is computing a similar term as\nour cross-covariance attention, but differing in how the softmax and ℓ2 normalizations are\napplied. Moreover, Lambda layers also include speciﬁc position-based lambda functions, and\nLambdaNetworks are based on ResNets while XCiT follows the ViT architecture. Recently\ndata-independent analogues of self-attention have also been found to be an effective alternative\nto convolutional and self-attention layers for vision tasks [21, 46, 63, 67]. These methods treat\nentries in the attention map as learnable parameters, rather than deriving the attention map\ndynamically from queries and keys, but their complexity remains quadratic in the number of\ntokens. Zhao et al.[82] consider alternative attention forms in computer vision.\n3 Method\nIn this section, we ﬁrst recall the self-attention mechanism, and the connection between\nthe Gram and covariance matrices, which motivated our work. We then propose our cross-\ncovariance attention operation (XCA) – which operates along the feature dimension instead\nof token dimension in conventional transformers – and combine it with local patch interaction\nand feedforward layers to construct our Cross-Covariance Image Transformer (XCiT). See\nFigure 1 for an overview.\n3.1 Background\nToken self-attention. Self-attention, as introduced by Vaswani et al. [69], operates on an\ninput matrix X ∈RN×d, where Nis the number of tokens, each of dimensionalityd. The input\n3\n5122 10242 12322 16002\nImage resolution\n0\n5\n10\n15\n20\n25Peak GPU Memory (GB)\nDeiT-S\nCait-S12\nViL (Longformer)\nViL (Performer)\nSwin-T\nPVT Small\nResNet50\nXCiT-S12/16\nXCiT-S12/8\nFigure 2: Inference memory usage of vision transformer\nvariants. Our XCiT models scale linearly in the number\nof tokens, which makes it possible to scale to much\nlarger image sizes, even in comparison to approaches\nemploying approximate self-attention or a pyramidal\ndesign. All measurements are performed with a batch\nsize of 64 on a single V100-32GB GPU.\n160 224 288 384 448 512\nImage Resolution\n72\n74\n76\n78\n80\n82\n84ImageNet-val Top-1 Acc.\nDeiT-S\nResNet-50\nXCiT-S12/16\nXCiT-S12/8\nFigure 3: Performance when changing the resolution at\ntest-time for models with a similar number of parame-\nters. All networks were trained at resolution 224, w/o\ndistillation. XCiT is more tolerant to changes of resolu-\ntion than the Gram-based DeiT and beneﬁt more from\nthe “FixRes” effect [64] when inference is performed at\na larger resolution than at train-time.\nX is linearly projected to queries, keys and values, using the weight matrices Wq ∈Rd×dq ,\nWk ∈Rd×dk and Wv ∈Rd×dv , such that Q=XWq, K=XWk and V=XWv, where dq = dk.\nKeys and values are used to compute an attention map A(K,Q) = Softmax(QK⊤/√dk), and\nthe output of the self-attention operation is deﬁned as the weighted sum of N token features\nin V with the weights corresponding to the attention map: Attention(Q,K,V ) = A(K,Q)V.\nThe computational complexity of self-attention scales quadratically in N, due to pairwise\ninteractions between all N elements.\nRelationship between Gram and covariance matrices. To motivate our cross-covariance\nattention operation, we recall the relation between Gram and covariance matrices. The\nunnormalised d×d covariance matrix is obtained as C=X⊤X. The N×N Gram matrix\ncontains all pairwise innerproducts: G=XX⊤. The non-zero part of the eigenspectrum of\nthe Gram and covariance matrix are equivalent, and the eigenvectors of C and Gcan be\ncomputed in terms of each other. If V are the eigenvectors of G, then the eigenvectors of C\nare given by U=XV. To minimise the computational cost, the eigendecomposition of either\nthe Gram or covariance matrix can be obtained in terms of the decomposition of the other,\ndepending on which of the two matrices is the smallest.1\nWe draw upon this strong connection between the Gram and covariance matrices to con-\nsider if it is possible to avoid the quadratic cost to compute the N×N attention matrix, which\nis computed from the analogue of the N×N Gram matrix QK⊤=XWqW⊤\nk X⊤. Below we\nconsider how we can use the dk ×dq cross-covariance matrix, K⊤Q=W⊤\nk X⊤XWq, which can\nbe computed in linear time in the number of elements N, to deﬁne an attention mechanism.\n3.2 Cross-covariance attention\nWe propose a cross-covariance based self-attention function that operates along the feature\ndimension, rather than along the token dimension as in token self-attention. Using the\ndeﬁnitions of queries, keys and values from above, the cross-covariance attention function is\ndeﬁned as:\nXC-Attention(Q,K,V ) = VAXC(K,Q), AXC(K,Q) = Softmax\n(\nˆK⊤ˆQ/τ\n)\n, (1)\nwhere each output token embedding is a convex combination of the dv features of its cor-\nresponding token embedding in V. The attention weights Aare computed based on the\ncross-covariance matrix.\nℓ2-Normalization and temperature scaling. In addition to building our attention operation\non the cross-covariance matrix, we make a second modiﬁcation compared to token self-\nattention. We restrict the magnitude of the query and key matrices by ℓ2-normalising them,\nsuch that each column of length N of the normalised matrices ˆQand ˆKhas unit norm, and\nevery element in d×dcross-covariance matrix ˆK⊤ˆQis in the range [−1,1]. We observed that\ncontrolling the norm strongly enhances the stability of training, especially when trained with\na variable numbers of tokens. However, restricting the norm reduces the representational\n1For Cto represent the covariance, Xshould be centered, i.e. X1=0. For the relation between Cand G, however,\ncentering is not required.\n4\nTable 1: XCiT models. Design choices include model depth, patch embeddings dimensionality d, and the number of\nheads hused in XCA. By default our models are trained and tested at resolution 224 with patch sizes of 16×16. We\nalso train with distillation using a convolutional teacher (denoted Υ) as proposed by Touvron et al. [65]. Finally, we\nreport performance of our strongest models obtained with 8×8 patch size, ﬁne-tuned (↑) and tested at resolution\n384×384 (column @384/8), using distillation with a teacher that was also ﬁne-tuned @384.\nModel Depth d #heads #params GFLOPs ImageNet-1k-val top-1 acc. (%)\n@224/16 @384/8 @224/16 @224/16Υ @384/8Υ↑\nXCiT-N12 12 128 4 3M 0.5 6.4 69.9 72.2 77.8\nXCiT-T12 12 192 4 7M 1.2 14.3 77.1 78.6 82.4\nXCiT-T24 24 192 4 12M 2.3 27.3 79.4 80.4 83.7\nXCiT-S12 12 384 8 26M 4.8 55.6 82.0 83.3 85.1\nXCiT-S24 24 384 8 48M 9.1 106.0 82.6 83.9 85.6\nXCiT-M24 24 512 8 84M 16.2 188.0 82.7 84.3 85.8\nXCiT-L24 24 768 16 189M 36.1 417.9 82.9 84.9 86.0\npower of the operation by removing a degree of freedom. Therefore, we introduce a learnable\ntemperature parameter τ which scales the inner products before the Softmax, allowing for\nsharper or more uniform distribution of attention weights.\nBlock-diagonal cross-covariance attention. Instead of allowing all features to interact among\neach other, we divide them into a hgroups, or “heads”, in a similar fashion as multi-head\ntoken self-attention. We apply the cross-covariance attention separately per head where for\neach head, we learn separate weight matrices to project X to queries, keys and values, and\ncollect the corresponding weight matrices in the tensors Wq ∈Rh×d×dq , Wk ∈Rh×d×dk and\nWv ∈Rh×d×dv , where we set dk=dq=dv=d/h. Restricting the attention within heads has two\nadvantages: (i) the complexity of aggregating the values with the attention weights is reduced\nby a factor h; (ii) more importantly, we empirically observe that the block-diagonal version\nis easier to optimize, and typically leads to improved results. This observation is in line\nwith observations made for Group Normalization [73], which normalizes groups of channels\nseparately based on their statistics, and achieves favorable results for computer vision tasks\ncompared to Layer Normalization [3], which combines all channels in a single group. Figure 4\nshows that each head learns to focus on semantically coherent parts of the image, while being\nﬂexible to change what type of features it attends to based on the image content.\nComplexity analysis. The usual token self-attention with hheads has a time complexity\nof O(N2d) and memory complexity of O(hN2+Nd). Due to the quadratic complexity, it\nis problematic to scale token self-attention to images with a large number of tokens. Our\ncross-covariance attention overcomes this drawback as its computational cost of O(Nd2/h)\nscales linearly with the number of tokens, as does the memory complexity of O(d2/h+Nd).\nTherefore, our model scales much better to cases where the number of tokens N is large,\nand the feature dimension dis relatively small, as is typically the case, in particularly when\nsplitting the features into hheads.\n3.3 Cross-covariance image transformers\nTo construct our cross-covariance image transformers (XCiT), we adopt a columnar architec-\nture which maintains the same spatial resolution across layers, similarly to [22, 65, 68]. We\ncombine our cross-covariance attention (XCA) block with the following additional modules,\neach one being preceded by a LayerNorm [ 3]. See Figure 1 for an overview. Since in this\nsection we speciﬁcally design the model for computer vision tasks, tokens correspond to\nimage patches in this context.\nLocal patch interaction. In the XCA block communication between patches is only implicit\nthrough the shared statistics. To enable explicit communication across patches we add a simple\nLocal Patch Interaction (LPI) block after each XCA block. LPI consists of two depth-wise 3×3\nconvolutional layers with Batch Normalization and GELU non-linearity in between. Due to\nits depth-wise structure, the LPI block has a negligible overhead in terms of parameters, as\nwell as a very limited overhead in terms of throughput and memory usage during inference.\nFeed-forward network. As is common in transformer models, we add a point-wise feedfor-\nward network (FFN), which has a single hidden layer with 4dhidden units. While interaction\nbetween features is conﬁned within groups in the XCA block, and no feature interaction takes\nplace in the LPI block, the FFN allows for interaction across all features.\nGlobal aggregation with class attention. When training our models for image classiﬁcation,\nwe utilize the class attention layers as proposed by Touvron et al. [68]. These layers aggregate\nthe patch embeddings of the last XCiT layer through writing to a CLS token by one-way\nattention between the CLS tokens and the patch embeddings. The class attention is also\napplied per head, i.e. feature group.\n5\nTable 2: ImageNet classiﬁcation. Number of parameters,\nFLOPs, image resolution, and top-1 accuracy on ImageNet-\n1k and ImageNet-V2. Training strategies vary across mod-\nels, transformer-based models and the reported RegNet\nmostly follow recipes from DeiT [65].\nModel #params FLOPs Res. ImNet V2\nEfﬁcientNet-B5 RA [18] 30M 9.9B456 83.7 _\nRegNetY-4GF [53] 21M 4.0B 224 80.0 72.4\nDeiT-SΥ[65] 22M 4.6B 224 81.2 68.5\nSwin-T [44] 29M 4.5B 224 81.3 _\nCaiT-XS24Υ↑[68] 26M 19.3B 384 84.1 74.1\nXCiT-S12/16Υ 26M 4.8B224 83.3 72.5\nXCiT-S12/16Υ↑ 26M 14.3B384 84.7 74.1\nXCiT-S12/8Υ↑ 26M 55.6B384 85.1 74.8\nEfﬁcientNet-B7 RA [18] 66M 37.0B600 84.7 _\nNFNet-F0 [10] 72M 12.4B 256 83.6 72.6\nRegNetY-8GF [53] 39M 8.0B 224 81.7 72.4\nTNT-B [79] 66M 14.1B 224 82.8 _\nSwin-S [44] 50M 8.7B 224 83.0 _\nCaiT-S24Υ↑[68] 47M 32.2B 384 85.1 75.4\nXCiT-S24/16Υ 48M 9.1B224 83.9 73.3\nXCiT-S24/16Υ↑ 48M 26.9B384 85.1 74.6\nXCiT-S24/8Υ↑ 48M 105.9B384 85.6 75.7\nFix-EfﬁcientNet-B8 [66] 87M 89.5B800 85.7 75.9\nRegNetY-16GF [53] 84M 16.0B224 82.9 72.4\nSwin-B↑[44] 88M 47.0B 384 84.2 _\nDeiT-BΥ↑[65] 87M 55.5B 384 85.2 75.2\nCaiT-S48Υ↑[68] 89M 63.8B 384 85.3 76.2\nXCiT-M24/16Υ 84M 16.2B224 84.3 73.6\nXCiT-M24/16Υ↑ 84M 47.7B384 85.4 75.1\nXCiT-M24/8Υ↑ 84M 187.9B384 85.8 76.1\nNFNet-F2 [10] 194M 62.6B 352 85.1 74.3\nNFNet-F3 [10] 255M 114.8B 416 85.7 75.2\nCaiT-M24Υ↑[68] 186M 116.1B 384 85.8 76.1\nXCiT-L24/16Υ 189M 36.1B224 84.9 74.6\nXCiT-L24/16Υ↑ 189M 106.0B384 85.8 75.8\nXCiT-L24/8Υ↑ 189M 417.8B384 86.0 76.6\nFigure 4: Visualization of the attention map between\nthe CLS token and individual patches in the class-\nattention stage. For each column, each row represents\nthe attention map w.r.t. one head, corresponding to\nthe image in the ﬁrst raw. Each head seems salient\nto semantically coherent regions. Heads are sensitive\nto similar features within the same or across images\n(e.g. people or bird faces). They trigger on different\nconcepts when such features are missing (e.g., cockpit\nfor race cars).\nHandling images of varying resolution. In contrast to the attention map involved in token\nself-attention, in our case the covariance blocks are of ﬁxed size independent of the input\nimage resolution. The softmax always operates over the same number of elements, which\nmay explain why our models behave better when dealing with images of varying resolutions\n(see Figure 3). In XCiT we include additive sinusoidal positional encoding [ 69] with the\ninput tokens. We generate them in 64 dimensions from the 2d patch coordinates and then\nlinearly project to the transformer working dimension d. This choice is orthogonal to the use\nof learned positional encoding, as in ViT [22]. However, it is more ﬂexible since there is no\nneed to interpolate or ﬁne-tune the network when changing the image size.\nModel conﬁgurations. In Table 1 we list different variants of our model which we use in\nour experiments, with different choices for model width and depth. For the patch encoding\nlayer, unless mentioned otherwise, we adopt the alternative used by Graham et al. [26] with\nconvolutional patch projection layers. We also experimented with a linear patch projection\nas described in [22], see our ablation in Table 4. Our default patch size is 16×16, as in other\nvision transformer models including ViT [22], DeiT [65] and CaiT [68]. We also experiment\nwith smaller 8×8 patches, which has been observed to improve performance [12]. Note that\nthis is efﬁcient with XCiT as its complexity scales linearly which the number of patches, while\nViT, DeiT and CaiT scale quadratically.\n4 Experimental evaluation\nIn this section we demonstrate the effectiveness and versatility of XCiT on multiple computer\nvision benchmarks, and present ablations providing insight on the importance of its different\ncomponents. In the supplementary material we provide additional analysis, including the\nimpact on performance of image resolution in Section A.1 and of multiple approximate\nattention baselines in Section A.2.\n4.1 Image classiﬁcation\nWe use ImageNet-1k [20] to train and evaluate our models for image classiﬁcation. It consists\nof 1.28M training images and 50k validation images, labeled across 1,000 semantic categories.\nOur training setup follows the DeiT recipe [65]. We train our model for 400 epochs with the\nAdamW optimizer [45] using a cosine learning rate decay. In order to enhance the training of\nlarger models, we utilize LayerScale [68] and adjust the stochastic depth [33] for each of our\n6\nTable 3: Self-supervised learning. Top-1 acc. on ImageNet-\n1k. Wwe report with a crop-ratio 0.875 for consistency with\nDINO. For the last row it is set to 1.0 (improves from 80.7%\nto 80.9%). All models are trained for 300 epochs.\nSSL Method Model #params FLOPs Lineark-NN\nMoBY [76] Swin-T [44] 29M 4.5B 75.0 –DINO [12] ResNet-50 [28] 23M 4.1B 74.5 65.6DINO [12] ViT-S/16 [22] 22M 4.6B 76.1 72.8DINO [12] ViT-S/8 [22] 22M 22.4B 79.2 77.2DINO [12]XCiT-S12/16 26M 4.9B77.8 76.0DINO [12]XCiT-S12/8 26M 18.9B79.2 77.1\nDINO [12] ViT-B/16 [22] 87M 17.5B 78.2 76.1DINO [12] ViT-B/8 [22] 87M 78.2B 80.1 77.4DINO [12]XCiT-M24/16 84M 16.2B78.8 76.4DINO [12]XCiT-M24/8 84M 64.0B80.3 77.9DINO [12]XCiT-M24/8↑384 84M 188.0B80.9 -\nTable 4: Ablations of various architectural design\nchoices on the task of ImageNet-1k classiﬁcation us-\ning the XCiT-S12 model. Our baseline model uses\nthe convolutional projection adopted from LeVit.\nModel Ablation ImNet top-1 acc.\nXCiT-S12/16Baseline 82.0\nXCiT-S12/8 83.4\nXCiT-S12/16Linear patch proj. 81.1\nXCiT-S12/8 83.1\nXCiT-S12/16w/o LPI layer 80.8\nw/o XCA layer 75.9\nXCiT-S12/16w/oℓ2-normal. failed\nw/o learned temp.τ 81.8\nmodels accordingly (see the supplementary material for details). Following [68], images are\ncropped with crop ratio of 1.0 for evaluation. In addition to the ImageNet-1k validation set,\nwe report results for ImageNet-V2 [55] which has a distinct test set. Our implementation is\nbased on the Timm library [72].\nResults on ImageNet. We present a family of seven models in Table 1 with different operat-\ning points in terms of parameters and FLOPs. We observe that the performance of the XCiT\nmodels beneﬁts from increased capacity both in depth and width. Additionally, consistent\nwith [65, 68] we ﬁnd that using hard distillation with a convolutional teacher improves the\nperformance. Because of its linear complexity in the number of tokens, it is feasible to train\nXCiT at 384×384 resolution with small 8×8 patches, i.e. 2304 tokens, which provides a strong\nboost in performance across all conﬁgurations.\nWe compare to the state-of-the-art convolutional and transformer-based architectures\n[10, 44, 53, 58, 68] in Table 2. By varying the input image resolution and/or patch size, our\nmodels provide competitive or superior performance across model sizes and FLOP budgets.\nFirst, the models operating on 224×224 and 16×16 (e.g. XCiT-S12/16) enjoy high accuracy\nat relatively few FLOPs compared to their counterparts with comparable parameter count\nand FLOPs. Second, our models with 16×16 and 384×384 resolution images (e.g. XCiT-\nS12/16↑) yield an improved accuracy at the expense of higher FLOPs, and provide superior\nor on-par performance compared to state-of-the-art models with comparable computational\nrequirements. Finally, XCiT linear complexity allows us to scale to process 384×384 images\nwith 8 ×8 patch sizes (e.g. XCiT-S12/8↑), achieving the highest accuracy across the board,\nalbeit at a relatively high FLOPs count.\nClass attention visualization. In Figure 4 we show the class attention map obtained in the\nfeature aggregation stage. Each head focuses on different semantically coherent regions in the\nimage (e.g. faces or umbrellas). Furthermore, heads tend to focus on similar patterns across\nimages (e.g. bird head or human face), but adapts by focusing on other salient regions when\nsuch patterns are absent.\nRobustness to resolution changes. In Figure 3 we report the accuracy of XCiT-S12, DeiT-S\nand ResNet-50 trained on 224 ×224 images and evaluated at different image resolutions.\nWhile DeiT outperforms ResNet-50 when train and test resolutions are similar, it suffers\nfrom a larger drop in performance as the image resolution deviates farther from the training\nresolution. XCiT displays a substantially increased accuracy when train and test resolutions\nare similar, while also being robust to resolution changes, in particular for the model with\n8×8 patches.\nSelf-supervised learning. We train XCiT in a self-supervised manner using DINO [12] on\nImageNet-1k. In Table 3 we report performance using the linear and k-NN protocols as\nin [12]. Across model sizes XCiT obtains excellent accuracy with both protocols, substantially\nimproving DINO with ResNet-50 or ViT architectures, as well as over those reported for Swin-\nTransformer trained with MoBY [76]. Comparing the larger models to ViT, we also observed\nimproved performance for XCiT achieving a strong 80.3% accuracy. For fair comparison, all\nreported models have been trained for 300 epochs. Further improved performance of small\nmodels is reported by Caron et al. [12] when training for 800 epochs, which we expect to\ncarryover to XCiT based on the results presented here.\nAnalysis and ablations. In Table 4 we provide ablation experiments to analyse the impact\nof different design choices for our XCiT-S12 model. First, we observe the positive effect of\nusing the convolutional patch projection as compared to using linear patch projection, for both\n8×8 and 16×16 patches. Second, while removing the LPI layer reduces the accuracy by only\n1.2% (from 82.0 to 80.8), removing the XCA layer results in a large drop of 6.1%, underlining\nthe effectiveness of XCA. We noticed that the inclusion of two convolutional components –\nconvolutional patch projection and LPI – not only brings improvements in accuracy, but also\naccelerates training. Third, although we were able to ensure proper convergence without\n7\nTable 5: COCO object detection and instance segmentation\nperformance on the mini-val set. All backbones are pre-\ntrained on ImageNet-1k, use Mask R-CNN model [29] and\nare trained with the same 3x schedule.\nBackbone #paramsAPb APb50 APb75 APm APm50 APm75\nResNet18 [28] 31.2M36.9 57.1 40.0 33.6 53.9 35.7\nPVT-Tiny [71] 32.9M39.8 62.2 43.0 37.4 59.3 39.9\nViL-Tiny [81] 26.9M41.2 64.0 44.7 37.9 59.8 40.6\nXCiT-T12/16 26.1M42.764.346.4 38.561.241.1\nXCiT-T12/8 25.8M44.566.448.8 40.363.543.2\nResNet50 [28] 44.2M41.0 61.7 44.9 37.1 58.4 40.1\nPVT-Small [71]44.1M43.0 65.3 46.9 39.9 62.5 42.8\nViL-Small [81]45.0M43.4 64.9 47.0 39.6 62.1 42.4\nSwin-T [44] 47.8M46.0 68.1 50.3 41.6 65.1 44.9\nXCiT-S12/16 44.3M45.367.049.5 40.864.043.8\nXCiT-S12/8 43.1M47.068.951.7 42.366.045.4\nResNet101 [28]63.2M42.8 63.2 47.1 38.5 60.1 41.3\nResNeXt101-3262.8M44.0 64.4 48.0 39.2 61.4 41.9\nPVT-Medium [71]63.9M44.2 66.0 48.2 40.5 63.1 43.5\nViL-Medium [81]60.1M44.6 66.3 48.5 40.7 63.8 43.7\nSwin-S [44] 69.1M48.5 70.2 53.5 43.3 67.3 46.6\nXCiT-S24/16 65.8M46.568.050.9 41.865.245.0\nXCiT-S24/8 64.5M48.169.553.0 43.066.546.1\nResNeXt101-64 [75]101.9M44.4 64.9 48.8 39.7 61.9 42.6\nPVT-Large [71]81.0M44.5 66.0 48.3 40.7 63.4 43.7\nViL-Large [81]76.1M45.7 67.2 49.9 41.3 64.4 44.5\nXCiT-M24/16101.1M46.768.251.1 42.065.644.9\nXCiT-M24/8 98.9M48.570.353.4 43.767.546.9\nTable 6: ADE20k semantic segmentation perfor-\nmance using Semantic FPN [ 38] and UperNet [ 74]\n(in comparable settings). We do not include com-\nparisons with other state-of-the-art models that are\npre-trained on larger datasets [44, 54, 83].\nBackbone Semantic FPN UperNet\n#paramsmIoU#paramsmIoU\nResNet18 [28] 15.5M 32.9 - -\nPVT-Tiny [71] 17.0M 35.7M - -\nXCiT-T12/16 8.4M 38.1 33.7M 41.5\nXCiT-T12/8 8.4M 39.9 33.7 43.5\nResNet50 [28] 28.5M 36.7 66.5M 42.0\nPVT-Small [71] 28.2M 39.8 - -\nSwin-T [44] - - 59.9M 44.5\nXCiT-S12/16 30.4M 43.9 52.4M 45.9\nXCiT-S12/8 30.4M 44.2 52.3M 46.6\nResNet101 [28] 47.5M 38.8 85.5M 43.8\nResNeXt101-32 [75]47.1M 39.7 - -\nPVT-Medium [71]48.0M 41.6 - -\nSwin-S [44] - - 81.0M 47.6\nXCiT-S24/16 51.8M 44.6 73.8M 46.9\nXCiT-S24/8 51.8M 47.1 73.8M 48.1\nResNeXt101-64 [75]86.4M 40.2 - -\nPVT-Large [71] 65.1M 42.1 - -\nSwin-B [44] - - 121.0M 48.1\nXCiT-M24/16 90.8M 45.9 109.0M 47.6\nXCiT-M24/8 90.8M 46.9 108.9M 48.4\nℓ2-normalization of queries and keys by tweaking the hyper-parameters, we found that it\nprovides stability across model size (depth and width) and other hyper-parameters. Finally,\nwhile the learnable softmax temperature parameter is not critical, removing it drops accuracy\nby 0.2%. Additional ablations are provided in the supplementary material.\n4.2 Object detection and instance segmentation\nOur XCiT models can efﬁciently process high-resolution images (see Figure 2). Additionally,\nXCiT has a better adaptability to varying image resolutions compared to ViT models (see\nFigure 3). These two properties make XCiT a good ﬁt for dense prediction tasks including\ndetection and segmentation.\nWe evalutate XCiT for object detection and instance segmentation using the COCO bench-\nmark [42] which consists of 118k training and 5k validation images including bounding boxes\nand mask labels for 80 categories. We integrate XCiT as backbone in the Mask R-CNN [29]\ndetector with FPN [43]. Since the XCiT architecture is inherently columnar, we make it FPN-\ncompatible by extracting features from different layers (e.g., [4, 6, 8, 12] for XCiT-S12). All\nfeatures have a constant stride of 8 or 16 based on the patch size, and the feature resolutions\nare adjusted to have strides of [4, 8, 16, 32], similar to ResNet-FPN backbones, where the\ndownsampling is achieved by max pooling and the upsampling is obtained using a single\ntransposed convolution layer (see suppl. mat. for details). The model is trained for 36 epochs\n(3x schedule) using the AdamW optimizer with learning rate of 10−4, 0.05 weight decay and\n16 batch size. We adopt the multiscale training and augmentation strategy of DETR [11]. Our\nimplementation is based on the mmdetection library [13].\nResults on COCO. In Table 5 we report object detection and instance segmentation results of\nfour variants of XCiT using16×16 and 8×8 patches. We compare to ResNets [28] and concurrent\nefﬁcient vision transformers [44, 71, 81]. All models are trained using the 3x schedule after\nImageNet-1k pre-training. Note that other results with higher absolute numbers have been\nachieved when pre-training on larger datasets [ 44] or with longer schedules [ 4], and are\ntherefore not directly comparable to the reported results. First, across all model sizes XCiT\noutperforms the convolutional ResNet [28] and ResNeXt [75] by a large margin with either\npatch size. Second, we observe a similar increase in accuracy compared to PVT [ 71] and\nViL [81] backbones. Finally, XCiT provides a competitive performance with Swin [ 44] 2.\nFor relatively small models, XCiT-S12/8 outperforms its Swin-T counterpart with a decent\nmargin. On the other hand, Swin-S provides slightly stronger results compared to XCiT-S24/8.\nUtilizing smaller 8×8 patches leads to a consistent gain across all models.\n4.3 Semantic segmentation\nWe further show transferability of our models with semantic segmentation experiments on\nthe ADE20k dataset [84], which consists of 20k training and 5k validation images with labels\nover 150 semantic categories. We integrate our backbones in two segmentation methods:\n2We use report the results provided by the authors in their open-sourced code https://github.com/\nSwinTransformer/Swin-Transformer-Object-Detection\n8\nSemantic FPN [ 38] and UperNet [ 74]. We train for 80k and 160k iterations for Semantic\nFPN and UperNet respectively. Following [ 44], the models are trained using batch size\n16 and an AdamW optimizer with learning rate of 6 ×10−5 and 0.01 weight decay. We\napply the same method of extracting FPN features as explained in Section 4.2. We report the\nperformance using the standard single scale protocol (without multi-scale and ﬂipping). Our\nimplementation is based on the mmsegmentation library [17].\nResults on ADE20k. We present the semantic segmentation performance using XCiT back-\nbones in Table 6. First, for Semantic FPN [38], XCiT provides a superior performance com-\npared to ResNet, ResNeXt and PVT backbones using either option of patch size. Second,\ncompared to Swin Transformers using the same UperNet decoder [74], XCiT with 8×8 patches\nconsistently achieves a higher mIoU for different models. XCiT with 16×16 patches provides\na strong performance especially for smaller models where XCiT-S12/16 outperforms Swin-T.\n5 Conclusion\nContributions. We present an alternative to token self-attention which operates on the\nfeature dimension, eliminating the need for expensive computation of quadratic attention\nmaps. We build our XCiT models with the cross-covariance attention as its core component\nand demonstrate the effectiveness and generality of our models on various computer vision\ntasks. In particular, it exhibits a strong image classiﬁcation performance on par with state-of-\nthe-art transformer models while similarly robust to changing image resolutions as convnets.\nXCiT is effective as a backbone for dense prediction tasks, providing excellent performance\non object detection, instance and semantic segmentation. Finally, we showed that XCiT can\nbe a strong backbone for self-supervised learning, matching the state-of-the-art results with\nless compute. XCiT is a generic architecture that can readily be deployed in other research\ndomains where self-attention has shown success.\nReferences\n[1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,\nAnirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured\ninputs in transformers. In Conference on Empirical Methods in Natural Language Processing, 2020.\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇ ci´ c, and Cordelia Schmid.\nVivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[4] Irwan Bello. LambdaNetworks: Modeling long-range interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\n[5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n[6] Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. MultiGrain:\na uniﬁed image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019.\n[7] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095, 2021.\n[8] Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling in visual\nrecognition. In International Conference on Machine Learning, 2010.\n[9] B. De Brabandere, X. Jia, T. Tuytelaars, and L. Van Gool. Dynamic ﬁlter networks. In Advances in\nNeural Information Processing Systems, 2016.\n[10] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale\nimage recognition without normalization. arXiv preprint arXiv:2102.06171, 2021.\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, 2020.\n[12] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint\narXiv:2104.14294, 2021.\n[13] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen\nFeng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie\nZhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark.\narXiv preprint arXiv:1906.07155, 2019.\n9\n[14] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[15] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794, 2020.\n[16] Ondrej Chum, James Philbin, Josef Sivic, Michael Isard, and Andrew Zisserman. Total recall:\nAutomatic query expansion with a generative feature model for object retrieval. In International\nConference on Computer Vision, 2007.\n[17] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox\nand benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.\n[18] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\ndata augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, 2020.\n[19] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.\nConvit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint\narXiv:2103.10697, 2021.\n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Computer Vision and Pattern Recognition, 2009.\n[21] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. RepMLP: Re-parameterizing\nconvolutions into fully-connected layers for image recognition. arXiv preprint arXiv:2105.01883,\n2021.\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In International Conference on\nLearning Representations, 2021.\n[23] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Hervé Jégou. Training vision transformers\nfor image retrieval. arXiv preprint arXiv:2102.05644, 2021.\n[24] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021.\n[25] Albert Gordo, Jon Almazán, Jérôme Revaud, and Diane Larlus. End-to-end learning of deep visual\nrepresentations for image retrieval. International journal of Computer Vision, 124, 2017.\n[26] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and\nMatthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. arXiv preprint\narXiv:2104.01136, 2021.\n[27] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021.\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Computer Vision and Pattern Recognition, 2016.\n[29] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In International\nConference on Computer Vision, 2017.\n[30] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019.\n[31] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona,\nand Serge J. Belongie. The iNaturalist species classiﬁcation and detection dataset. arXiv preprint\narXiv:1707.06642, 2017.\n[32] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Computer Vision and Pattern\nRecognition, 2018.\n[33] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In European Conference on Computer Vision, 2016.\n[34] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.\nPerceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021.\n[35] Hervé Jégou, Matthijs Douze, and Cordelia Schmid. Hamming embedding and weak geometric\nconsistency for large scale image search. In European Conference on Computer Vision, 2008.\n[36] Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick Perez, and Cordelia Schmid.\nAggregating local image descriptors into compact codes. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 34(9), 2012.\n10\n[37] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are\nRNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine\nLearning, 2020.\n[38] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid\nnetworks. In Computer Vision and Pattern Recognition, 2019.\n[39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained\ncategorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13),\n2013.\n[40] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, CIFAR,\n2009.\n[41] James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with\nfourier transforms. arXiv preprint arXiv:2105.03824, 2021.\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference\non Computer Vision, 2014.\n[43] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.\nFeature pyramid networks for object detection. In Computer Vision and Pattern Recognition, 2017.\n[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[46] Luke Melas-Kyriazi. Do you even need attention? a stack of feed-forward layers does surprisingly\nwell on imagenet. arXiv preprint arXiv:2105.02723, 2021.\n[47] M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes.\nIn Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\n[48] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In International Conference on Machine Learning, 2018.\n[49] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object retrieval with large vocabularies\nand fast spatial matching. In Computer Vision and Pattern Recognition, 2007.\n[50] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise\nself-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.\n[51] Filip Radenovi´ c, Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondˇ rej Chum. Revisiting oxford\nand paris: Large-scale image retrieval benchmarking. In Computer Vision and Pattern Recognition,\n2018.\n[52] Filip Radenovi´ c, Giorgos Tolias, and Ondrej Chum. Fine-tuning CNN image retrieval with no\nhuman annotation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.\n[53] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing\nnetwork design spaces. In Computer Vision and Pattern Recognition, 2020.\n[54] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\narXiv preprint arXiv:2103.13413, 2021.\n[55] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers\ngeneralize to imagenet? In International Conference on Machine Learning, 2019.\n[56] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efﬁcient attention:\nAttention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, 2021.\n[57] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention\nspan in transformers. arXiv preprint arXiv:1905.07799, 2019.\n[58] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\nnetworks. In International Conference on Machine Learning. PMLR, 2019.\n[59] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,\nDamian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of\nthe ACM, 59(2):64–73, 2016.\n[60] Giorgos Tolias, Yannis Avrithis, and Hervé Jégou. Image search with selective match kernels:\naggregation across single and multiple images. International journal of Computer Vision, 116(3), 2016.\n11\n[61] Giorgos Tolias, Ronan Sicre, and Hervé Jégou. Particular object retrieval with integral max-pooling\nof cnn activations. In International Conference on Learning Representations, 2016.\n[62] Giorgos Tolias, Tomas Jenicek, and Ondˇ rej Chum. Learning and aggregating deep local descriptors\nfor instance-level recognition. In European Conference on Computer Vision, 2020.\n[63] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey\nDosovitskiy. MLP-Mixer: An all-MLP architecture for vision. arXiv preprint arXiv:2105.01601, 2021.\n[64] H Touvron, A Vedaldi, M Douze, and H Jégou. Fixing the train-test resolution discrepancy.Advances\nin Neural Information Processing Systems, 2019.\n[65] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers and distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[66] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020.\n[67] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard\nGrave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. ResMLP: Feedforward\nnetworks for image classiﬁcation with data-efﬁcient training. arXiv preprint arXiv:2105.03404, 2021.\n[68] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going\ndeeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\n[70] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[71] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\n[72] Ross Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019.\n[73] Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, 2018.\n[74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing for\nscene understanding. In European Conference on Computer Vision, 2018.\n[75] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual\ntransformations for deep neural networks. In Computer Vision and Pattern Recognition, 2017.\n[76] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-\nsupervised learning with swin transformers. arXiv preprint arXiv:2105.04553, 2021.\n[77] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and\nVikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. arXiv\npreprint arXiv:2102.03902, 2021.\n[78] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating\nconvolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021.\n[79] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token ViT: Training vision transformers from scratch on\nImageNet. arXiv preprint arXiv:2101.11986, 2021.\n[80] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\nsequences. arXiv preprint arXiv:2007.14062, 2020.\n[81] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding.arXiv\npreprint arXiv:2103.15358, 2021.\n[82] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition.\nIn Computer Vision and Pattern Recognition, 2020.\n[83] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a\nsequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[84] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ade20k dataset. In Computer Vision and Pattern Recognition, 2017.\n12\nXCiT: Cross-Covariance Image Transformers\nAppendix\nA Preliminary study on Vision Transformers (ViT)\nIn this appendix we report the results associated with our preliminary study on high-\nresolution transformers. Most of the experiments were carried out on the ViT architecture [22]\nwith DeiT training [ 65], and intended to analyze different aspects of transformers when\nconsidering images with varying resolution or high-resolution images speciﬁcally.\nA.1 Impact of resolution versus patch size\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 160  192  224  256  288  320  384\nImagenet-val top.1 acc\nImage size\nfix 16x16 tokens\nfix patch size: 16x16\nVariable patch size\nImage Size 80 112 160 256 320 384\nPatch Size 5 7 10 16 20 24\nTop-1 78.2 79.7 80.5 80.7 80.9 80.7\nVariable number of tokens size\nImage Size 160 224 256 288 320 384\n# of tokens 100 196 256 324 400 576\nTop-1 77.4 79.9 80.7 81.2 81.5 82.3\nFigure A.1: Impact of input resolution on accuracy for DeiT-S. We consider different image resolutions, and either\n(1) increase the patch size while keeping the number of tokens ﬁxed; or (2) keep the patch size ﬁxed and use more\ntokens. Larger input images are beneﬁcial if the number of tokens increases. The impact of a change of a resolution for\na constant number of patches (of varying size) is almost neutral. As one can observe, the main driver of performance\nis the number of patches. The patch size has a limited impact on the accuracy, except when considering very small\nones. We have observed and conﬁrmed similar trends with XCiT models.\nA.2 Approximate attention models in ViT with DeiT training\nIn Table A.1, we report the results that we obtain by replacing the Multi-headed Self-attention\noperation with efﬁcient variants [30, 56, 70, 71] in the DeiT-S backbone. First, we can notice\nthat for all efﬁcient self-attention choices there is a clear drop in performance compared to the\nDeit-S baseline. The spatial reduction attention (SRA) proposed in PVT [71] has a signiﬁcantly\nweaker performance compared to the full-attention with a quadratic complexity that is more\nefﬁcient than full-attention by only a constant factor R2. Linformer [ 70] provides a better\naccuracy compared to SRA, however, it is also clearly weaker than full-attention. Moreover,\nLinformer does not have the ﬂexibility of processing variable length sequences which limits its\napplication in many computer vision tasks. Efﬁcient attention [ 56] provides a better trade-off\nthan the aforementioned methods, with improved accuracy and linear complexity. However,\nit has a 3.6% drop in performance compared to full-attention. Finally, axial attention [ 30]\nprovides the strongest performance among the efﬁcient attention variants we studied with a\n1.5% drop in accuracy compared to the baseline. We observe a saving in memory usage, but a\ndrop in speed due to the separate row and column attention operations. Our observations are\nconsistent with [22].\nTable A.1: ImageNet Top-1 accuracy of efﬁcient self-attention variants (after 300 epochs of training).\nModel Complexity Top-1\nDeiT-S [65] O(N2) 79.9\nSRA (Average Pool) [71] O(N2/R2) 73.5\nSRA (Convolutional) [71] O(N2/R2) 74.0\nLinformer (k=√n) [70] O(kN) 75.7\nEfﬁcient Transformer [56] O(N) 76.3\nAxial [30] O(N\n√\nN) 78.4\nI\nA.3 Training and testing with varying resolution\nAs discussed in the main manuscript, for several tasks it is important that the network is able\nto handle images of varying resolutions. This is the case, for instance, for image segmentation,\nimage detection, or image retrieval where the object of interest may have very different sizes.\nWe present an analysis of train/test resolution trade-off in Table A.2.\nTable A.2: Trade-off between train and test resolutions for DeiT.MS refers to multi-scale training, where the models\nhave seen images from different resolutions at training time.\nTest / Train 160 224 256 288 320 MS\n160 77.2 75.9 73.3 68.2 59.6 76.3\n224 78.0 79.9 79.9 79.0 77.9 79.6\n256 77.3 80.4 80.7 80.2 79.9 80.6\n288 76.3 80.4 81.0 81.2 80.8 81.0\n320 75.0 80.1 80.9 81.3 81.5 81.3\nB Additional details of training and our architecture\nB.1 Sinusoidal Positional Encoding\nWe adopt a sinusoidal positional encoding as proposed by Vaswani et al.[69] and adapted\nto the 2D case by Carion et al. [11]. However we depart from this method in that we ﬁrst\nproduce this encoding in an intermediate 64-d space before projecting it to the working space\nof the transformers. More precisely, in our implementation each of thexand ycoordinates\nis encoded using 32 dimensions corresponding to cosine and sine functions with different\nfrequencies (16 frequency for each function). The encoding of both coordinates are eventually\nconcatenated to obtain a 64 dimension 2D positional encoding. Finally, the 64 dimension\npositional encoding is linearly projected to the working dimension of the model d.\nB.2 Obtaining Feature Pyramid for Dense Prediction\nFor state-of-the-art detection and segmentation models, FPN is an important component\nwhich provides features of multiple scales. We adapt XCiT to be compatible with FPN\ndetection and segmentation methods through a simple re-scaling of the features extracted\nfrom different layers. In particular, for models with 12 layers, we extract features from the 4th,\n6th, 8th and 12th layers respectively. As for models with 24 layers, we extract features from 8th,\n12th, 16th and 24th layers. Concerning the re-scaling of the features, the 4 feature levels are\ndownsized by a ratio of 4, 8, 16 and 32 compared to the input image size. Feature downsizing\nis performed with max pooling and upsampling is achieved using a single layer of transposed\nconvolutions with kernel size k= 2 and stride s= 2.\nB.3 Hyper-parameters: LayerScale initialization and Stochastic Depth drop-\nrate\nWe list the stochastic depth dr and LayerScale initialization ϵhyperparameters used by each\nof our models in Table B.1.\nTable B.1: Hyperparameters used for training our models, including the Stochastic depth drop rate dr and Layer-\nScale initialization ϵ.\nModel Patch size dr ϵ\nXCiT-N12 8 & 16 0.0 1.0\nXCiT-T12 8 & 16 0.0 1.0\nXCiT-T24 8 & 16 0.05 10−5\nXCiT-S12 8 & 16 0.05 1.0\nXCiT-S24 8 & 16 0.1 10−5\nXCiT-M24 8 & 16 0.15 10−5\nXCiT-L24 16 0.25 10−5\nXCiT-L24 8 0.3 10−5\nC Pseudo-code\nWe provide a PyTorch-style pseudo code of the Cross-covariance attention operation. The\npseudo code resembles the Timm library [ 72] implementation of token self-attention. We\nII\nshow that XCA only requires few modiﬁcations, namely the ℓ2 normalization, setting the\nlearnable temperature parameters and a transpose operation of the keys, queries and values.\nAlgorithm 1 Pseudocode of XCA in a PyTorch-like style.\n# self.qkv: nn.Linear(dim, dim * 3, bias=qkv_bias)\n# self.temp: nn.Parameter(torch.ones(num_headss, 1, 1))\ndef forward(self, x):\nB, N, C = x.shape\nqkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\nqkv = qkv.permute(2, 0, 3, 1, 4)\nq, k, v = qkv[0], qkv[1], qkv[2] # split into query, key and value\nq = q.transpose(-2, -1)\nk = k.transpose(-2, -1) # Transpose to shape (B, h, C, N)\nv = v.transpose(-2, -1)\nq = F.normalize(q, dim=-1, p=2) # L2 Normalization across the token dimension\nk = F.normalize(k, dim=-1, p=2)\nattn = (k @ q.transpose(-2, -1)) # Computing the block diagonal cross-covariance matrix\nattn = attn * self.temp # Adjusting the activations scale with temperature parameter\nattn = attn.softmax(dim=-1) # d x d attention map\nx = attn @ v # Apply attention to mix channels per token\nx = x.permute(0, 3, 1, 2).reshape(B, N, C)\nx = self.proj(x)\nreturn x\nD Additional results\nD.1 More XCiT models\nWe present additional results for our XCiT models in Table D.1. We include performance\nof 384×384 images using a 16 ×16 patch size as well as results for images with 224 ×224\nresolution using patch size of 8×8.\nTable D.1: ImageNet-1k top-1 accuracy of XCiT for additional combinations of image and patch sizes.\nModels Depth d #Blocks params P= (16×16) P= (8×8)\nGFLOPs @224 @224Υ @384↑ GFLOPs @224 @224Υ @384↑\nXCiT-N12 12 128 4 3M 0.5 69.9 72.2 75.4 2.1 73.8 76.3 77.8\nXCiT-T12 12 192 4 7M 1.2 77.1 78.6 80.9 4.8 79.7 81.2 82.4\nXCiT-T24 24 192 4 12M 2.3 79.4 80.4 82.6 9.2 81.9 82.6 83.7\nXCiT-S12 12 384 8 26M 4.8 82.0 83.3 84.7 18.9 83.4 84.2 85.1\nXCiT-S-24 24 384 8 48M 9.1 82.6 83.9 85.1 36.0 83.9 84.9 85.6\nXCiT-M24 24 512 8 84M 16.2 82.7 84.3 85.4 63.9 83.7 85.1 85.8\nXCiT-L24 24 768 16 189M 36.1 82.9 84.9 85.8 142.2 84.4 85.4 86.0\nD.2 Transfer Learning\nIn order to further demonstrate the ﬂexibility and generality of our models, we report transfer\nlearning experiments in Table D.2 for models that have been pre-trained using ImageNet-\n1k and ﬁnetuned for other datasets including CIFAR-10, CIFAR-100 [40], Flowers-102 [47],\nStanford Cars [39] and iNaturalist [31]. We observe that the XCiT models provide competitive\nperformance when compared to strong baselines like ViT-B, ViT-L, DeiT-B and EfﬁcientNet-B7.\nD.3 Image Retrieval\nContext of this study. Vision-based retrieval tasks such as landmark or particular object\nretrieval have been dominated in the last years by methods extracting features from high-\nresolution images. Traditionally, the image description was obtained as the aggregation of\nlocal descriptors, like in VLAD [36]. Most of the modern methods now rely on convolutional\nTable D.2: Evaluation on transfer learning.\nArchitecture CIFAR 10 CIFAR100 Flowers102 Cars iNat 18 iNat19\nEfﬁcientNet-B7 [58] 98.9 91.7 98.8 94.7 _ _\nViT-B/16 [22] 98.1 87.1 89.5 _ _ _\nViT-L/16 [22] 97.9 86.4 89.7 _ _ _\nDeit-B/16 [65]Υ 99.1 91.3 98.8 92.9 73.7 78.4\nXCiT-S24/16Υ 99.1 91.2 97.4 92.8 68.8 76.1\nXCiT-M24/16Υ 99.1 91.4 98.2 93.4 72.6 78.1\nXCiT-L24/16Υ 99.1 91.3 98.3 93.7 75.6 79.3\nIII\nneural networks [6, 25, 61]. In a recent paper, El-Nouby et al.[23] show promising results\nwith vision transformers, however they also underline the inherent scalability limitation\nassociated with the fact that ViT models do not scale well with image resolution. Therefore,\nit cannot compete with convolutional neural networks whose performance readily improve\nwith higher resolution images. Our XCiT models do not suffer from this limitation: our\nmodels scale linearly with the number of pixels, like convnets, and therefore makes it possible\nto use off-the-shelf methods initially developed for retrieval with high-resolution images.\nD.3.1 Datasets and evaluation measure\nIn each benchmark, a set of query images is searched in a database of images and the\nperformance is measured as the mean average precision.\nThe Holidays [ 35] dataset contains images of 500 different objects or scenes. We use\nthe version of the dataset where the orientation of images (portrait or landscape) has been\ncorrected. Oxford [49] is a dataset of building images, which corresponds to famous landmark\nin Oxford. A similar dataset has been produced for famous monuments in Paris and referred\nto as Paris6k [16].\nTable D.3: The basic statistics on the image retrieval datasets.\nnumber of images nb of instances\nDataset database queries\nHolidays 1491 500 500\nR-Oxford 4993 70 26\nWe use the revisited version of the Oxford benchmark [ 51], which breaks down the\nevaluation into easy, medium and hard categories. We report results on the \"medium\" and\n\"hard\" settings, as we observed that the ordering of techniques does not change under the\neasy measures.\nD.3.2 Image representation: global and local description with XCiT\nWe consider three existing methods to extract an image vector representations from the pre-\ntrained XCiT models. Note that to the best of our knowledge, for the ﬁrst time we extract\nlocal features from the output layer of a transformer layer, and treat them as patches fed to\ntraditional state-of-the-art methods based on matching local descriptors or CNN.\nCLS token. Similar to El-Nouby et al. [23] with ViT, we use the ﬁnal vector as the image\ndescriptor. In this context, the introduction of class-attention layers can be regarded as a way\nto learn the aggregation method.\nVLAD. We treat the patches before the class-attention layers as individual local descriptors,\nand aggregate them into a higher-dimensional vector by employing the Vector of locally\naggregated Descriptors [36].\nAMSK. We also apply the aggregated selective match kernel from Tolias et al.[60]. This\nmethod was originally introduced for local descriptors, but got adapted to convolutional\nnetworks. To the best of our knowledge this is the state of the art on several benchmarks [62].\nFor all these methods, we use the models presented in our main paper, starting from the\nversion ﬁne-tuned at resolution 384×384. By default the resolution is 768. This is comparable\nto the choice adopted in the literature for ResNet (e.g., 800 in the work by Berman et al. [6]).\nD.3.3 Experimental setting: Image retrieval with models pretrained on Imagenet1k only\nWe only consider models pre-trained on Imagenet-1k. Note that the literature reports signiﬁ-\ncant improvement when learning or ﬁne-tuning networks [52, 62] on specialized datasets (e.g.,\nof buildings for Oxford5k and Paris6k). We consider only XCiT-S12 models, since they have a\nnumber of parameters comparable to that of ResNet-50. We report the results in Table D.4.\nScaling resolution. As expected increasing the resolution with XCiT improves the perfor-\nmance steadily up to resolution 768. This shows that our models are very tolerant to resolution\nchanges considering that they have been ﬁne-tuned at resolution 384. The performance starts\nto saturates at resolution 1024, which led us to keep 784 as the pivot resolution.\nSelf-supervision. The networks XCiT pre-trained with self-supervision achieve a compara-\ntively better performance than their supervised counterpart on Holidays, however, we have\nthe opposite observation for ROxford.\nIV\nTable D.4: Instance retrieval experiments. The default resolution is 768. The default class token size is 128\ndimensions. The \"local descriptor\" representation extracted from the activations is in 128 dimensions. To our\nknowledge the state of the art with ResNet-50 on Holidays with Imagenet pre-training only is the Multigrain\nmethod [6], which achieves mAP=92.5%. Here we compare against this method under the same training setting, i.e.,\noff-the-shelf network pre-trained on Imagenet1k only and with the same training procedure and resolution. We refer\nthe reader to Tolias et al.[62] for the state of the art on ROxford, which involves some training on the target domain\nwith images depicted building and ﬁne-tuning at the target resolution.\nBase model parameters ROxford5k (mAP) Holidays (mAP)\nMedium Hard\nXCiT– class token\nXCiT-S12/16 30.1 8.7 86.0\nXCiT-S12/8 33.2 12.1 86.4\nXCiT-S12/16 resolution 224 12.7 2.4 71.5\nXCiT-S12/16 resolution 384 20.1 4.6 83.4\nXCiT-S12/16 resolution 512 26.6 5.8 84.6\nXCiT-S12/16 resolution 768 30.1 8.7 86.0\nXCiT-S12/16 resolution 1024 30.3 11.2 86.3\nXCiT-S12/16 self-supervised DINO 35.1 11.9 87.3\nXCiT-S12/8 self-supervised DINO 30.9 7.9 88.3\nXCiT– VLAD\nXCiT-S12/16 k=256 36.6 11.6 89.9\nXCiT-S12/16 k=1024 40.0 13.0 90.7\nXCiT– ASMK\nXCiT-S12/8 k=1024 36.5 9.4 90.4\nXCiT-S12/8 k=65536 42.0 12.9 92.3\nXCiT-S12/16 k=1024 35.2 11.5 90.4\nXCiT-S12/16 k=65536 40.0 15.0 92.0\nResNet-50 – ASMK\nResnet50 k=1024 41.6 14.6 86.0\nResnet50 k=65536 41.9 14.5 87.9\nMultigrain-resnet50 k=1024 32.9 9.4 87.9\nImpact of Image description. We adopt the class-token as the descriptor, and in our\nexperiments we veriﬁed that this aggregation method is better than average and GeM pooling\n[8, 52]. In Table D.4 one can see there is a large beneﬁt in employing a patch based method\nalong with our XCiT transformers: XCiT-VLAD performs signiﬁcantly better than the CLS\ntoken, likely thanks to the higher dimensionality. This is further magniﬁed with AMSK,\nwhere we obtain results approaching the absolute state of the art on Holidays, despite a sub-\noptimal training setting for image retrieval. This is interesting since our method has not been\nﬁne-tuned for retrieval tasks and we have not been adapted in any signiﬁcant way beyond\napplying off-the-shelf this aggregation technique. A direct comparison with ResNet-50 shows\nthat our XCiT method obtains competitive results in this comparable setting, slightly below\nthe ResNet-50 on ROxford but signiﬁcantly better on Holidays.\nD.4 Runtime and Memory Usage\nWe present the peak memory usage as well as the throughput of multiple models including\nfull-attention and efﬁcient vision transformers in Table D.5. Additionally, in Figure D.1 we plot\nthe processing speed represented as millisecond per image as a function of image resolution\nfor various models. We can observe that XCiT provides a strong trade-off, possessing the best\nscalability in terms of peak memory, even when compared to ResNet-50. Additionally, the\nprocessing time scales linearly with respect to resolution, with only ResNet-50 providing a\nbetter trade-off on that front.\nD.5 Queries and Keys magnitude visualizations\nOur XCA operation relies on the cross-covariance matrix of the queries ˆQand keys ˆKwhich\nare ℓ2 normalized across the patch dimension. Therefore, each element in the d×dmatrix\nrepresents a cosine similarity whose value is strongly inﬂuenced by the magnitude of each\npatch. In Figure D.2 we visualize the magnitude of patch embeddings in the queries and keys\nmatrices. We observe that patch embeddings with higher magnitude corresponds to more\nsalient regions in the image, providing a very cheap visualization and interpretation of which\nregions in the image contribute more in the cross-covariance attention.\nV\n5122 10242 12322 16002\nImage resolution\n0\n20\n40\n60\n80\n100\n120Millisecond / Image\nDeiT-S\nCait-S12\nViL (Longformer)\nViL (Performer)\nSwin-T\nPVT Small\nResNet50\nXCiT-S12/16\nXCiT-S12/8\nFigure D.1: We present the millisecond per image during inference of multiple models. Our XCiT-S12/16 model\nprovides a speed up for images with higher resolution compared to existing vision transformers, especially the ones\nwith quadratic complexity like DeiT and CaiT.\nTable D.5: Inference throughput and peak GPU memory usagefor our XCiT small model compared to other models\nof comparable size that include token self-attention. All models tested using batch size of 64 on a V100 GPU with\n32GB memory.\nModel #paramsImNet Image Resolution\n(×106) Top-1 2242 3842 5122 10242\n@224 im/secmem (MB)im/secmem (MB)im/secmem (MB)im/secmem (MB)\nResNet-50 25 79.0 1171 772 434 2078 245 3618 61 14178\nDeiT-S 22 79.9 974 433 263 1580 116 4020 N/A OOM\nCaiT-S12 26 80.8 671 577 108 2581 38 7117 N/A OOM\nPVT-Small 25 79.8 777 1266 256 3142 134 5354 N/A OOM\nSwin-T 29 81.3 704 1386 220 3890 120 6873 29 26915\nXCiT-S12/16 26 82.0 781 731 266 1372 151 2128 37 7312\n∥ ˆQ ∥ ∥ ˆK ∥\nFigure D.2: Visualization of the queries ˆQand keys ˆKnorm across the feature dimension. We empirically observe\nthat magnitude of patch embeddings in the queries and keys correlates with the saliency of their corresponding\nregion in the image.\nVI",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.797041118144989
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5498745441436768
    },
    {
      "name": "Transformer",
      "score": 0.5496873259544373
    },
    {
      "name": "Generality",
      "score": 0.5023596286773682
    },
    {
      "name": "Scalability",
      "score": 0.4996042251586914
    },
    {
      "name": "Covariance",
      "score": 0.4922317564487457
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4564632475376129
    },
    {
      "name": "Computational complexity theory",
      "score": 0.4191078245639801
    },
    {
      "name": "Segmentation",
      "score": 0.4184590280056
    },
    {
      "name": "Computer vision",
      "score": 0.3915608823299408
    },
    {
      "name": "Algorithm",
      "score": 0.2606729567050934
    },
    {
      "name": "Mathematics",
      "score": 0.10364970564842224
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210161954",
      "name": "Département d'Informatique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I29607241",
      "name": "École Normale Supérieure - PSL",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2279609970",
      "name": "Université de Lille",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1341640284",
      "name": "Centrum Wiskunde & Informatica",
      "country": "NL"
    }
  ]
}