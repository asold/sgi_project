{
    "title": "Effects and Mitigation of Out-of-vocabulary in Universal Language Models",
    "url": "https://openalex.org/W3194748659",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2151903096",
            "name": "Sangwhan Moon",
            "affiliations": [
                "Tokyo Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2030501650",
            "name": "Naoaki Okazaki",
            "affiliations": [
                "Tokyo Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035254119",
        "https://openalex.org/W3100129654",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W2963208801",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3044423116",
        "https://openalex.org/W2118434577",
        "https://openalex.org/W3104671987",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2962699518",
        "https://openalex.org/W2946328221",
        "https://openalex.org/W3116864188",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2983577274"
    ],
    "abstract": "One of the most important recent natural language processing (NLP) trends is transfer learning - using representations from language models implemented through a neural network to perform other tasks. While transfer learning is a promising and robust method, downstream task performance in transfer learning depends on the robustness of the backbone model's vocabulary, which in turn represents both the positive and negative characteristics of the corpus used to train it. With subword tokenization, out-of-vocabulary (OOV) is generally assumed to be a solved problem. Still, in languages with a large alphabet such as Chinese, Japanese, and Korean (CJK), this assumption does not hold. In our work, we demonstrate the adverse effects of OOV in the context of transfer learning in CJK languages, then propose a novel approach to maximize the utility of a pre-trained model suffering from OOV. Additionally, we further investigate the correlation of OOV to task performance and explore if and how mitigation can salvage a model with high OOV.",
    "full_text": "Journal of Information Processing Vol.29 490–503 (July 2021)\n[DOI: 10.2197/ipsjjip.29.490]\nRegular Paper\nEﬀects and Mitigation of Out-of-vocabulary in Universal\nLanguage Models\nSangwhan Moon1,†1,a) Naoaki Okazaki1,b)\nReceived: December 8, 2020, Accepted: April 2, 2021\nAbstract: One of the most important recent natural language processing (NLP) trends is transfer learning – using\nrepresentations from language models implemented through a neural network to perform other tasks. While transfer\nlearning is a promising and robust method, downstream task performance in transfer learning depends on the robust-\nness of the backbone model’s vocabulary, which in turn represents both the positive and negative characteristics of\nthe corpus used to train it. With subword tokenization, out-of-vocabulary (OOV) is generally assumed to be a solved\nproblem. Still, in languages with a large alphabet such as Chinese, Japanese, and Korean (CJK), this assumption does\nnot hold. In our work, we demonstrate the adverse e ﬀects of OOV in the context of transfer learning in CJK languages,\nthen propose a novel approach to maximize the utility of a pre-trained model su ﬀering from OOV. Additionally, we\nfurther investigate the correlation of OOV to task performance and explore if and how mitigation can salvage a model\nwith high OOV .\nKeywords: Natural language processing, Machine learning, Transfer learning, Language models\n1. Introduction\nUsing a large-scale neural language model as a pre-trained\nbackbone and transferring to a multitude of downstream tasks [6],\n[7], [21] has been one of the most signiﬁcant advancements in the\nﬁeld of natural language processing (NLP). This approach has\nbeen commonly used with convolutional neural networks trained\nagainst the ImageNet dataset and is commonly referred to as\ntransfer learning. Language models used in this form do not yet\nhave an o ﬃcial name but have been canonically called universal\nlanguage models [11], due to its universal applicability. Unlike\nthe domain of images or audio, pre-training language models for\nnatural language processing do not require any annotated data due\nto various self-supervising training methods which have been re-\ncently proposed. This allows models to be pre-trained at scale, as\nthere is a nearly inﬁnite supply of training data from text data on\nthe internet and through centuries worth of book corpora, given\nthat one can e ﬃciently digitize this into textual data and a ﬀord the\namount of compute power needed as the training data is scaled up.\nHowever, these methods still depend on a vocabulary bound to\nan embedding matrix. Due to the unavoidable growth of compu-\ntational budget required as the vocabulary size increases, many\nmethods have been proposed to reduce the vocabulary size to a\nmanageable size, notably through subword based methods. Sub-\nword based methods, such as Byte-Pair Encoding (BPE) [23],\nWordPiece [30], SentencePiece [13], which break the lexicons\ninto smaller subwords, have shown to be e ﬀective when applied to\nlanguages that utilize Latin-like alphabets in their writing system\n1 Tokyo Institute of Technology, Meguro, Tokyo 152–8550, Japan\n†1 Presently with Odd Concepts Inc., Meguro, Tokyo 153–0063, Japan\na) sangwhan@iki.ﬁ\nb) okazaki@c.titech.ac.jp\nto reduce the size of the vocabulary while increasing the robust-\nness against out-of-vocabulary (OOV) in downstream tasks. This\nis especially powerful when combined with transfer learning.\nAs these tokenizers still operate at Unicode character levels –\ncontrary to the names suggesting byte-level (which would com-\npletely mitigate OOV , as studied in Ref. [10]). Hence, the vocab-\nulary’s minimum size is twice the size of all unique characters\nin the corpus, as subword tokenizers store each character in pre-\nﬁx and su ﬃx form in the vocabulary. In commonly investigated\nlanguages, this still provides signiﬁcantly more ﬂexibility over\nlexicons. For these reasons, OOV issues have not been actively\nstudied as it simply does not surface. However, this problem is yet\nto be solved in Chinese, Japanese, and Korean (CJK) languages\ndue to the complex alphabets. We recognize that these unsolved\nproblems make the applicability of neural language models for\ntasks in the context of CJK languages less universal than that of\nother languages.\nThe high-level idea of our method is illustrated in Fig. 1, where\nˆı, a token missing from the vocabulary, is substituted with i.\nThis work expands on our our existing work [20], which adds\nan OOV vocabulary learning step before ﬁne-tuning for a down-\nstream task. This is done by re-assigning OOV subwords to exist-\ning subwords. Through experiments, we demonstrate the e ﬀects\nof OOV in a downstream task setup and compare the OOV miti-\ngation scheme’s eﬃcacy with and without additional pre-training.\nWe further investigate how OOV contributes to task contribution\nby artiﬁcially inducing OOV in a pre-trained model and verify our\nproposed method can recover the model to a useable state even in\nmoderately extreme OOV conditions.\nThis work is an extension of our work in the Proceedings of Empirical\nMethods in Natural Language Processing, 2020 [20].\nc⃝2021 Information Processing Society of Japan 490\nJournal of Information Processing Vol.29 490–503 (July 2021)\nFig. 1 Overview of the vocabulary patching process explained with OOV examples.\n2. Related Work\nThis work builds on the foundation of multiple natural lan-\nguage processing developments, which we will explain in the\nsections below. First, we disambiguate the concept of a language\nmodel, subword tokenization, then explain how it was combined\nto build a backbone model for di ﬀerent tasks and discuss the lim-\nitations.\n2.1 Neural Language Models\nA language model is formally deﬁned as a probability distri-\nbution over a sequence of tokens. Given a sequence of length\nm, a probability P(w\n1,...,w m) is assigned to the whole sequence.\nA neural language model is a sequence prediction model, com-\nmonly implemented as a classiﬁer that predicts a sequence based\non the past context. In the example above, given a current time\nstep t which satisﬁes t <m, it can predict P(wt|w1,...,w t−1)u p\nto the P(wm|w1,...,w m−1). This is done by training such a neu-\nral network with unlabeled text corpora. This is possible because\nnatural language can use coherently written sentences as a form\nof labeled data, unlike many other modalities. In the past, neu-\nral language models have been implemented using recurrent neu-\nral networks (RNN) such as long short-term memory networks\n(LSTM), but recent trends have shifted towards Transformer [26]\nbased networks due to the discovery of its applicability in other\ntasks, which we will discuss in Section 2.3.\n2.2 Subword Tokenization\nTokenization is the process of breaking a sequence into smaller\nunits for an algorithm to consume. In an NLP context, the most\ncommon practice is to tokenize at the whitespace level, which\ngenerally results in tokens becoming words in languages such as\nEnglish. However, when the algorithm that is expected to con-\nsume this is a neural network, the possible conjugations and per-\nmutations (such as numbers) become computationally intractable\nfor the network to process. For these reasons, traditional methods\nused forms of preprocessing, such as stemming or lemmatiza-\ntion or sample the vocabulary to a consumable size. This results\nin information loss, which can result in lost performance during\nevaluation.\nAdditionally, these methods are not robust to rare words, which\nin evaluation can result in out-of-vocabulary. Subword tok-\nenization was proposed as a mitigation for the out-of-vocabulary\nproblem. Byte-Pair Encoding (BPE) [23], WordPiece [30], and\nSentencePiece [14] are all subword tokenization methods, which\nbreak the lexicons into smaller subwords. These methods have\nbeen shown to make the vocabulary robust to rare words while\nminimizing information loss.\n2.3 Universal Language Models\nThe term Universal Language Model was ﬁrst coined in ULM-\nFiT [11], which uses a pre-trained LSTM language model’s [18]\nhidden state to perform a multitude of tasks and achieve signiﬁ-\ncant performance gains over per-task trained models. Around the\nsame time, it was also discovered that such language model pre-\ntraining applied to a Transformer [26] based model, which was\noriginally proposed for machine translation. The main di ﬀerence\nof this Transformer based language model, BERT [7], is that it\nused signiﬁcantly more compute power, was trained with a much\nlarger pre-train dataset, and used subword tokenization.\nWith the advent of BERT, numerous other Transformer based\narchitectures have been proposed and have shown to be extremely\neﬀective at being scaled up in terms of both increasing the amount\nof training data while also increasing the model capacity. Recent\nwork such as GPT-3 [2] demonstrated substantial performance\ngains by scaling up both the model and data and validated that\nlarger models are competitive in both zero-shot and few-shot set-\ntings.\nGenerally, the amount of pre-train data and model capacity is\ninversely proportional to the amount of downstream task data\nneeded [2]. A pre-trained model also acts as a better initializa-\ntion [1] as it also converges faster, reducing the amount of com-\nputation budget needed to achieve high performance on a given\ntask. Due to this, state-of-the-art research employs transfer learn-\ning in some form.\nWhile not all of the proposed backbone networks provide mul-\ntilingual models or evaluations, work such as Ref. [6] shows that\npre-training these models with multilingual corpora transferring\nwith language models is also e ﬀective in a multilingual setup.\nHowever, a multilingual model’s downside over a monolingual\nmodel is that multilingual models tend to su ﬀer from pre-train\ndata imbalance. This reﬂects the real world since di ﬀerent lan-\nguages diﬀer in the number of users the language has. Generally,\nthe amount of textual data one can acquire tends to be propor-\ntional to the number of users of each language and is also a ﬀected\nby socio-economical factors contributing to a lower deployment\nc⃝2021 Information Processing Society of Japan 491\nJournal of Information Processing Vol.29 490–503 (July 2021)\nrate of digital technology, resulting in fewer data.\n3. Preliminaries\n3.1 CJK Tokenization in Universal Language Models\nOut-of-vocabulary (OOV) in a subword tokenization context\ntypically happens when a character was never seen in a pre-train\ncontext. These missing words can be introduced either by setting\nan upper limit on the character coverage or the vocabulary size.\nHere, the latter is tied to the former – if the character itself has\nbeen pruned from the initial set of characters to be covered, it is\nnaturally infeasible to form a subword since the character is miss-\ning. This issue tends to be more signiﬁcant in languages with a\ndiverse alphabet, such as Chinese, Japanese, or Korean.\nChinese and Japanese use a large set of ideographs; some of\nwhich are rarely used, hence will not be statistically signiﬁcant\nenough to be selected for inclusion but can be crucial in spe-\nciﬁc tasks such as named entity recognition (NER) since rare\nideographs have moderate usage in names of people or locations.\nKorean, on the other hand, has a large alphabet for somewhat\nartiﬁcial reasons. While the modern Korean alphabet can be ex-\npressed with 41 characters, the encoding of Korean in a com-\nputational context is done in a way that it is expressed through a\ncombination of the underlying alphabet to form a composite char-\nacter, and has been standardized in Unicode as these composite\ncharacters instead of its native form.\nFor these reasons, when trained against a diverse set of lan-\nguages, the vocabulary size increases proportionally to the num-\nber of languages supported and needs to factor in the number\nof characters needed to express the language. For example, ex-\npressing English requires 26 characters, which results in 52 with\nboth upper and lower cases. To express this in character level\nsubwords, this results in 104 subwords, as both preﬁx and su ﬃx\nforms are required in a vocabulary. To express French using the\nsame vocabulary, the initial 26 can be re-used, and only 16 new\ncharacters speciﬁc to French are needed.\nHowever, in the context of CJK languages, the initial character\ncount is much larger, starting at approximately 2000\n*1 for com-\nmon characters. Full coverage for the CJK ideographs requires\n92,856 characters and 11,172 characters. While doubling the bud-\nget is not necessary, there are no cases in CJK languages *2,t h e\nmagnitude of budget required for the vocabulary is di ﬀerent from\nthat of a language using a Latin alphabet.\nExisting models have sampled portions of entire corpora or\nrelaxed constraints on character level coverage for these lan-\nguages to prevent the vocabulary from growing to an unmanage-\nable scale. As of today, this is an unavoidable trade-o ﬀwhen\ntraining multilingual models. This introduces a bottleneck for\ndownstream tasks since any character omitted causes information\nloss. The e ﬀect ampliﬁes when a large character level vocab-\nulary and scriptio continua languages *3 exist in the same con-\n*1 This approximation is based on the 2010 revised J ¯oy¯o kanji table for CJK\nideographs, and the KS X 1001 encoding standard for Korean.\n*2 The Kana system in Japanese has diacritics, for a subset of the characters.\n*3 Languages which are written without spaces. Chinese and Japanese qual-\nify as scriptio continua, while Korean is a special case where spacing\nrules are liberal and can be expressed without spaces in colloquial writ-\ning.\nTable 1 Examples of OOV in the task datasets. Here, we can observe that\nChinese has OOV in punctuation, Japanese in emoji, and Korean\nin spelling errors.\ntext, which is the case for all CJK languages. Examples of OOV\nin CJK languages can be seen in Table 1. Some methods have\nbeen proposed to mitigate this by decomposition of the charac-\nters [19], [24] to signiﬁcantly reduce the vocabulary budget while\nretaining all information, but have shown little adoption in the\nwild.\nIn a monolingual setup, one can use pre-trained models for the\ntarget task language. However, when considering a multilingual\nsetup, there is an additional layer of complexity by using an en-\nsemble of monolingual models, as language detection is required\nto determine which model and tokenization scheme to use for\neach input. The most straightforward approach here would be to\npre-train a monolingual model with a shared tokenization scheme\nfor all the required languages.\nHowever, the downside is the cost for pre-training; acquir-\ning a large corpus is a daunting task, and training a large mul-\ntilingual model for many researchers can be ﬁnancially infeasi-\nble. The high upfront cost and complexity when implementing\na multilingual system leaves transfer learning on an open, mul-\ntilingual model as an economically attractive alternative. Un-\nfortunately, due to corpus imbalance during pre-training, less-\ninvestigated languages, especially those with a diverse character\nset (such as CJK languages), OOV is likely to surface. Our moti-\nvation is to improve these languages’ performance without signif-\nicantly increasing the computation cost when using open-source\npre-trained models.\n3.2 BERT Tokenizer\nThe multilingual BERT model bert-base-multilingual-\ncased [7] we used performs two-phase tokenization, ﬁrst with\nwhitespace (token) followed by WordPiece [30] tokenization\n(subword token). An example output of the tokenizer is explained\nin Fig. 1. The preﬁx forms of the subwords are expressed in their\noriginal form, while su ﬃx forms are expressed by appending a\n## preﬁx.\nIf either form of the subword is missing in a token, the tok-\nenization fails, and the token surface is treated as OOV. Using\nthe example in Fig. 1, the su ﬃx form of ˆıis not in the vocabulary,\nhence the entire surface of the token plaˆıtbecomes OOV. This is\ndue to the greedy merging nature of the WordPiece algorithm and\nis not universal to all subword-based methods.\nDue to the dependency on initial whitespace tokenization,\nBERT’s tokenization is not expected to work well with scriptio\ncontinua languages, especially if it has a diverse alphabet. To\nworkaround this limitation, BERT’s tokenizer implements special\nc⃝2021 Information Processing Society of Japan 492\nJournal of Information Processing Vol.29 490–503 (July 2021)\nhanding *4 which artiﬁcially injects whitespace before and after\nCJK ideographs. This mechanism is not enabled for Korean.\n3.3 OOV Mitigation\nAs OOV was a much more prevalent problem in the context of\nword-based methods, it has been investigated further than OOV\nin subword-based methods.\nIn the context of word-based models, pre-processing the input\nwith stemming and lemmatization was a common practice, both\nto reduce OOV and the size of the vocabulary. Additionally, novel\nmethods such as dictionary-based post-processing [17] and dis-\ntributional representation based substitution [12] have been pro-\nposed.\nHowever, in the context of subword tokenization this has not\nbeen actively investigated, aside from Ref. [27], which proposes\nadding new words to the vocabulary, and Ref. [20], which is our\nwork.\nFor our experiments, we used four CJK datasets for evaluation.\nFor all tasks, we ﬁrst learn OOV words, perform ﬁne-tuning, then\nevaluate. The OOV rates noted for each dataset is the ratio of sen-\ntences containing at least one OOV token. We intentionally chose\nsentiment analysis datasets, as the pre-trained model used (bert-\nbase-multilingual-cased) was trained on Wikipedia and book cor-\npora, and a domain shift to user-written content had a higher like-\nlihood of su ﬀering from OOV due to words that are unlikely to\nappear in well-formed content. Theoretically, Korean is expected\nto su ﬀer the most, as the BERT tokenizer does not have special\ncase handling, hence is susceptible to the greedy merging of the\nunderlying WordPiece tokenizer.\n4. Proposed Method\nIn this section, we propose a method to mitigate OOV without\ntraining a new model. This is based on a hypothesis that OOV\nhas adversarial e ﬀects on task performance, which we also verify\nthrough experiments in Section 5.2. Our method is implemented\nas a modiﬁcation of the BERT tokenizer. In all mitigation exper-\niments, we compare with and without additional pre-training.\nThe BERT tokenizer is modiﬁed to support a secondary vocab-\nulary which points new words to existing words for our exper-\niments. This modiﬁed tokenizer is used instead of the original\ntokenizer in a BERT model. The approach consists of three steps.\nFirst, we perform a complete corpus analysis and search for all\nOOV surfaces by tokenizing the task corpus. An OOV surface in\nthe context of BERT is an entire space tokenized token. When-\never OOV occurs, we keep a record of the entire OOV surface,\nalong with the context.\nFor each OOV surface, we brute-force search to ﬁnd the maxi-\nmally speciﬁc OOV subword surface. An OOV subword surface\nis an actual subword missing in an OOV surface. In this step, we\ncompute a frequency table for both OOV and in-vocabulary sub-\nwords for a preference mechanism in the mitigation strategy. We\nobserved that most OOV subword surface cases were caused by\none character missing in the vocabulary during our experiments,\n*4 https://github.com/google-research/bert/blob/master/\ntokenization.py#L251\nwhich is a result of incomplete character coverage from the cor-\npora used for pre-training.\nFinally, we use this information to build a mitigation strategy\nfor the OOV subwords. Whenever applicable, we use the previ-\nously computed frequency of the OOV tokens to prioritize fre-\nquent OOV tokens over rare cases. Here, we evaluate di ﬀerent\nalgorithms for OOV mitigation, each of which we discuss in the\nindividual method sections below. After applying OOV mitiga-\ntion, we then optionally perform additional pre-training and eval-\nuate against the baseline.\nAdditional pre-training is the process of using the task corpus\nto train the model under a masked language modeling task, which\nis a form of additional pre-training, but against domain corpora.\nHere, the model is trained to ﬁll in a masked portion of a given\npassage, given the context. Formally, this is called a cloze task\nand is the same process used to train BERT initially.\nThis additional pre-training intends to adapt the model so that\nit learns the changes in the vocabulary introduced by our mitiga-\ntions, as the model has never seen the new subwords. This also\nhelps the model better learn adequate representations that are bet-\nter suited for the task domain. If the surrogate is assigned to a\nsubword from a di ﬀerent language, for example, when using un-\nseen subwords, this process is crucial. As this does not require\nan annotated corpus, it is also possible to make the model more\nrobust by providing extra corpora.\nSubstitution to mitigate OOV has been studied in Ref. [12].\nThis method depends on part-of-speech tagging or a secondary\ncorpus and model for similarity computation, challenging to ap-\nply in a subword model. Our approach’s signiﬁcance is that it\nworks for subword models and its practical applicability, as only\na downstream task corpus and a pre-trained model is required.\n4.1 Surrogated Tokens\nSurrogates, simply put, map a subword missing from the vo-\ncabulary to a subword that is already in the vocabulary of a pre-\ntrained model. There are intuitive ways to ﬁnd substitute words\nin a word-level setup, the most obvious being choosing a seman-\ntically similar word from a thesaurus. In a subword context, this\nis not as straightforward, as a subword generally has no meaning.\nIn our work, we discuss di ﬀerent surrogate selection processes.\nThe surrogate selection process assigns multiple subwords to the\nsame embedding, which is a trade-o ﬀthat limits the utility of the\nproposed method for generation tasks. As surrogates are only\nassigned once, to perform generation tasks when a subword is\npolysemic, one would need to use an auxiliary binary classiﬁer to\ndetermine which subword the prediction actually is. This is not\nrequired for tasks that do not require generation, such as classiﬁ-\ncation.\nThe embeddings between the newly added subword and the\nsurrogate are shared and updated together in the ﬁne-tuning pro-\ncess. The OOV subword frequency table we constructed in the\nsecond step of the process above is used to break ties and min-\nimize conﬂicts. For example, token A and B, both of which are\nOOV subwords, can end up with the same proposals {X,Y}in\npreference order. In this case, given A has a higher frequency, it\ngets precedence over B, so the surrogate map becomes A →X\nc⃝2021 Information Processing Society of Japan 493\nJournal of Information Processing Vol.29 490–503 (July 2021)\nFig. 2 Here, using the masked language modeling task from BERT, the OOV tokens are replaced with a\nmask to be predicted. The predictions are used as surrogates in an OOV-mitigated model.\nFig. 3 In character distance, The highlighted character is missing from the\nvocabulary. Observing the adjacent characters, in CJK ideographs\nthey share a radical, while in Korean they share two subcharacters.\nand B →Y. Our goal is to reﬁne the proposals to be in a state\nwhere one surrogate is assigned to only one OOV token.\n4.1.1 Character Distance\nThis method selects the surrogate with the shortest Unicode\ncodepoint distance from the OOV subword, limited to subword\ntokens within the vocabulary of the same length. In this process,\nwe perform an exhaustive search, formulated as the following.\nargmin\nw∈W′\n|ord(v) −ord(w)|1\nIn the formula above, vis the OOV subword, and W′is a subset\nof the vocabulary W which satisﬁes UTF-8 character level length\nequality |v|=|w|for w∈W. ord is the Unicode ordinal conversion\nfunction.\nThe intuition of this method builds on the characteristics of the\nCJK Unicode blocks, which allow us to cheaply approximate text\nor semantic similarity through the scalar values of the Unicode\ncodepoints as seen in Fig. 3. The properties which we intend to\nexploit are di ﬀerent depending on the target language. In CJK\nideographs, adjacent characters tend to share a radical, hence has\na bias towards semantic similarity.\nOn the other hand, in Korean, phonetically similar characters\nare adjacent. This approximates edit distance, as a Korean charac-\nter in Unicode is a combination of multiple sub-characters. This\nphonetic similarity di ﬀers from edit distance, as it tends to dis-\nallow edits on the ﬁrst two components of the character. In the\nevent of a distance tie, we used the candidate with a lower code-\npoint.\nFrequent subword tokens get preferential treatment and hence\nget surrogates with closer distance to an infrequent token. Once\na token has been assigned, it is not re-used as a surrogate.\n4.1.2 Unseen Subwords\nWe select tokens from the in-vocabulary token frequency ta-\nble, which were never seen in the current task as surrogates. As\ndownstream tasks for evaluation do not require the entire vocab-\nulary, we select random tokens with a frequency of 0 as surro-\ngates. In our experiments, this was implemented by overwriting\nthe existing unseen subword to the target subword. This allows\nguaranteed reconstruction of the original text, making it usable\nfor generation tasks, but at the cost of the embeddings being as-\nsigned to ones that the model has not seen in the context.\nThis method is analogous to increasing the model parameters\n(via vocabulary size), then pruning back to the original size, but\nas an in-place operation. Any word previously assigned was held\nout to prevent re-assignment. As the vocabulary will have a large\nnumber of tokens never seen in most downstream tasks, we do\nnot use any frequency preference here.\n4.1.3 Masked Language Model\nThe masked language model-based method uses BERT’s\nmasked language head to generate surrogate proposals, as illus-\ntrated in Fig. 2. Each subword OOV surface is replaced with the\nmask token and passed to the masked LM head with the whole\ncontext. The subword token with the highest probability is se-\nlected for each context, stored in a frequency table, to select the\nmost common prediction later. This results in deterministic sur-\nrogate mappings.\nWe use the same frequency preference as character distance,\nwhich allows frequent OOV subwords to have precedence when\nselecting surrogates. As with other methods, once a surrogate\nis assigned, it is held out. Therefore, less frequent words are as-\nsigned to the next most locally frequent surrogate. After the entire\nprocess, OOV subwords that were not assigned a surrogate are as-\nsigned to the candidate with the lowest frequency. This method\nhas the highest computation cost, as it requires inference on the\nmodel.\n4.2 Additional Tokens\nHere, we add new tokens to the vocabulary and increase the\nmodel size, motivated by prior work [29]. As this increases the\nnetwork parameters, these are used as a secondary baseline to be\ncompared with surrogates.\n4.2.1 Random Initialization\nAfter adding the missing subword to the vocabulary, then the\ncorresponding embedding is randomly initialized. This is analo-\ngous to how a model is commonly initialized, and also how new\ntokens are added to an existing vocabulary.\n4.2.2 Transfer Initialization\nTransfer initialization is done by following the ﬁrst step of the\nmasked language model task to generate a list of surrogates. We\nthen initialize by copying the embedding vector of the topmost\nprobable candidate of the OOV subword into the newly added\nOOV subword’s slot in the embedding matrix. These two tokens\nshare the same initial embeddings but are expected to diverge\nthrough ﬁne-tuning.\nc⃝2021 Information Processing Society of Japan 494\nJournal of Information Processing Vol.29 490–503 (July 2021)\nFig. 4 The experiment pipeline of our work. Evaluations have been labeled with the corresponding sec-\ntion where the results are disclosed. MLM here is the process of performing additional pre-training\nthrough a masked language modeling task.\n5. Datasets\n5.1 Naver Sentiment Movie Corpus\nThe Naver Sentiment Movie Corpus *5 (NSMC) [3] is a Korean\nsentiment analysis task, containing 200,000 user comments and\na corresponding binary label which indicates positive or negative\nsentiment. The OOV rate on the pre-trained BERT model was\n30.1% due to a large number of typos and the domain gap.\n5.2 Japanese Twitter Sentiment Analysis\nAs a second validation target language, we used a subset\n*6 of\na Japanese Twitter dataset [25] *7, which is a sentiment analysis\ntask with ﬁve possible labels. The subset contains 20 K Tweets\nand 2 K Tweets, respectively, for training and test. We observed\nthat a large portion of the OOV was from emojis during analy-\nsis, resulting in an OOV rate of 25.1% on the pre-trained BERT\nmodel.\n5.3 Chinese News Sentiment Analysis\nThe INEWS dataset is part of the ChineseGLUE *8 dataset.\nThe input is a short sentence from a news article, and the label\nis one of three labels denoting the tone of the sentence. This is\nalso a sentiment analysis task, with a split of 5 K train and 1 K\nvalidation, and an OOV rate of 20.1% on the pre-trained BERT\nmodel.\n5.4 KorQuAD 1.0\nKorQuAD 1.0\n*9 is a Korean version of the SQuAD [22] read-\ning comprehension task. The task involves answering a question\ngiven a passage of text, and consists of 10 K passages with 66 K\nquestions. The passages are from Wikipedia, which is commonly\nused as a part of large-scale training corpora. The result of this\nis a low OOV rate of 5.9% on the pre-trained BERT model. For\nthis task, additional pre-training was omitted to prevent the model\nfrom memorizing answers. We added this additional task to vali-\ndate our method against a low-OOV task.\n*5 https://github.com/e9t/nsmc\n*6 https://github.com/cynthia/japanese-twitter\n*7 http://www.db.info.gifu-u.ac.jp/data/Data 5d832973308d57446583ed9f\n*8 https://github.com/chineseGLUE\n*9 https://korquad.github.io/\n6. Experiments\nTo validate the eﬀectiveness of our method proposed in the pre-\nvious section, we perform multiple experiments against multiple\nCJK datasets in the upcoming sections. To thoroughly evaluate\nthe e ﬀects of our proposed scheme, we validate against both real\nand synthetic setups, using the di ﬀerent mitigation schemes ex-\nplained in Section 4. The high-level ﬂow of all experiments we\ndo here work is explained in Fig. 4. We compare the e ﬀects of dif-\nferent methods using a pre-trained multilingual BERT (bert-base-\nmultilingual-cased). Each method was tested with ﬁne-tuning,\nincluding a masked language modeling (additional pre-training)\ntask, or by ﬁne-tuning only against the task. Task-level ﬁne-\ntuning was included in every experiment to ensure fairness and\nis done by attaching a task head and training the downstream task\nmodel. This allows the model to learn how to accomplish the\ntask while adapting itself to produce better representations for the\ntask. For our experiments, we limited additional pre-training to\nthe task corpus to make the experiments reproducible with only\nthe task datasets.\nAll experiments that involved training the model were trained\nfor three epochs. The full list of hyperparameters used for the\nexperiments is listed in Table A ·1, in this paper’s appendix. Ev-\nery experiment in the upcoming section was run ﬁve times each,\nwith the random seed ﬁxed to an integer value of the run number\nin the range of [1 ..5]. The runs are then compared to the base-\nline scores to observe the statistical signiﬁcance of the di ﬀerent\nscores for each method. For the signiﬁcance test, we performed\na dependent t-test for paired samples, following the guidelines in\nRef. [9]. We used a p-value of p <0.05 to determine statisti-\ncal signiﬁcance and a ﬁxed seed (42) for any random algorithm\nto make the results deterministic, which guarantees reproducibil-\nity, as can be seen in Table 2. The evaluation was done with the\nreference implementation\n*10 from Ref. [9].\n6.1 Results on Task Datasets\nThe evaluation was done through the SST-2 GLUE task met-\nrics [28] for the sentiment analysis tasks, and EM /F1 evaluation\nfrom the SQuAD metrics for KorQuAD, as the two tasks are com-\n*10 https://github.com/rtmdrr/testSigniﬁcanceNLP\nc⃝2021 Information Processing Society of Japan 495\nJournal of Information Processing Vol.29 490–503 (July 2021)\nTable 2 Scores across ﬁve runs, accompanied with statistical signiﬁcance compared to baseline. Statisti-\ncally signiﬁcant points ( p <0.05) have been underlined in the p-values. Acc denotes accuracy\nand Std denotes standard deviation. Add models have more parameters. +MLM is with addi-\ntional pre-training on the task corpus. Results for KorQuAD have been scaled down by 100, and\nare without additional pre-training.\nNSMC (ko) Twitter (ja) INEWS (zh) KorQuAD (ko)\nModel Value Acc@+MLM Acc Acc@ +MLM Acc Acc@ +MLM Acc EM F1\nBERT (Baseline) Mean 0.8824 0.8785 0.7284 0.7192 0.8138 0.8074 0.7037 0.9005\nStd 0.0017 0.0006 0.0041 0.0058 0.0064 0.0047 0.0016 0.0013\nAdd (Transfer) Mean 0.8916 0.8844 0.7319 0.7223 0.8116 0.8082 0.7097 0.9030\nStd 0.0007 0.0006 0.0040 0.0060 0.0022 0.0082 0.0023 0.0011\np-value 0.0002 0.0000 0.1091 0.1623 0.2599 0.4437 0.0091 0.0041\nAdd (Random) Mean 0.8928 0.8848 0.7310 0.7211 0.8186 0.8106 0.7098 0.9029\nStd 0.0006 0.0004 0.0046 0.0041 0.0049 0.0065 0.0034 0.0018\np-value 0.0000 0.0000 0.2263 0.1639 0.1280 0.0601 0.0128 0.0248\nChar. Distance Mean 0.8926 0.8855 0.7304 0.7238 0.8122 0.8092 0.7094 0.9031\nStd 0.0009 0.0013 0.0037 0.0024 0.0097 0.0070 0.0026 0.0019\np-value 0.0001 0.0005 0.1499 0.0108 0.3152 0.1567 0.0115 0.0358\nUnseen Subwords Mean 0.8922 0.8846 0.7304 0.7225 0.8142 0.8102 0.7037 0.9013\nStd 0.0002 0.0013 0.0039 0.0038 0.0079 0.0065 0.0017 0.2112\np-value 0.0000 0.0000 0.1554 0.0649 0.4403 0.1441 0.5000 0.2934\nMasked LM Mean 0.8915 0.8842 0.7307 0.7225 0.8100 0.8090 0.7089 0.9027\nStd 0.0009 0.0006 0.0043 0.0058 0.0063 0.0047 0.1647 0.1283\np-value 0.0004 0.0002 0.1103 0.1219 0.1451 0.2801 0.0177 0.0614\npatible. Each model used the same dataset and training parame-\nters as the baseline, only with di ﬀerent OOV mitigation methods.\nThe results of these experiments are in Table 2.\nAdditionally, while Chinese and Japanese are both scriptio\ncontinua languages, BERT’s tokenizer treats CJK ideograph text\ndiﬀerently and breaks at every character by artiﬁcially injecting\nwhitespaces. This makes the a ﬀected surface from OOV signif-\nicantly smaller, resulting in less information loss. We expect to\nsee more considerable gains in Korean for these reasons, as the\nper-character break is not enabled.\n6.1.1 Naver Sentiment Movie Corpus\nDue to the larger OOV surface and frequency, we expect to ob-\nserve a modest increase in the best case compared to the baseline.\nAs seen in Table 2, we can indeed observe that regardless of the\nmitigation method, OOV mitigation, in general, improves accu-\nracy and the improvements are statistically signiﬁcant. The OOV\ntokens we observed here were from casual writing in user com-\nments, which shifts from the book corpus like domain used for\npre-train. This suggests that even without robust, representative\nembeddings, it is still better than losing information during tok-\nenization. We also hypothesize that performance improves by do-\nmain adaptation through additional pre-training because the ini-\ntial embeddings are not representative of the subword in context.\nAs this dataset had the most signiﬁcant gains in performance, we\ninvestigated the positive and negative examples in Fig. 5.A s w e\nhave observed in Table 3, there were more cases which improved\nwith our method. However, we also observed that negative cases\nemerge from additional pre-training, such as the samples in Fig. 5,\nsome of which we suspect can be attributed to surrogates being\nassigned to a di ﬀerent language’s Unicode page.\n6.1.2 Japanese Twitter Sentiment Analysis\nThis corpus showed a high OOV rate due to the frequent oc-\ncurrence of emoji in the text, and improper normalization of Uni-\ncode punctuation. We observe similar patterns with the results\nTable 3 Quantiﬁed improvements and regressions in performance across\nthe di ﬀerent tasks with samples a ﬀected by OOV. The results are\nfrom the best scoring Character Distance models compared to best\nbaseline models for each task.\nDataset Regressed Improved Delta\nNSMC 392 528 136\nKorQuAD 64 79 15\nTwitter 21 32 11\nINEWS 11 11 0\nfrom NSMC. Generally, we see only minor improvements, ex-\ncept for character distance – which was statistically signiﬁcant.\nWe observed that character distance assigned surrogates to Ko-\nrean characters *11.\n6.1.3 Chinese News Sentiment Analysis\nWhile we observed a high OOV rate in this dataset, the im-\nprovement was negligible. Analyzing the surrogates, we ob-\nserved that most of the OOV tokens were punctuation or un-\ncommon ideographs, which we expected to, and conﬁrmed to\nhave little e ﬀect in the downstream task performance. In Ta-\nble 3, not only does the improved cases cancel out, looking at\nthe OOV cases we considered the di ﬀerence to be training noise.\nWe hypothesize that small size of the dataset is likely to have\ncontributed to the negative results.\n6.1.4 KorQuAD 1.0\nWe did not expect signiﬁcant improvements due to the low\nOOV rate, and the results reﬂect this. While we still saw minor\nimprovements across the board, the di ﬀerence is incremental at\nbest, although some methods produced p-values which were con-\nsidered statistically signiﬁcant. The small delta can most likely\nbe attributed to the relatively low OOV rate and omission of ad-\nditional pre-training.\nGiven that our experiments’ results demonstrate that mitigat-\n*11 This would have been appropriate to demonstrate with examples, but due\nto the Twitter license agreement, reproducing the original text in this pa-\nper was not possible.\nc⃝2021 Information Processing Society of Japan 496\nJournal of Information Processing Vol.29 490–503 (July 2021)\nFig. 5 Positive, positive (with bad patches) and negative examples with the proposed method applied.\nNegative cases are surrogate assignments which had adversarial e ﬀects on performance, and posi-\ntive is the opposite. Positive with bad patches is a special case where the assignment looks incor-\nrect, but contributed positively to performance. The OOV surfaces have been marked in bold.\ning OOV improves task performance, in the next section, we ex-\nplore if our method can recover performance in high-OOV mod-\nels, which we have synthetically created through initial OOV and\nperformance correlation experiments.\n6.2 E ﬀects of OOV on Task Performance\nIn the previous section, we demonstrated the e ﬀects of our\nmethod on di ﬀerent tasks and languages. These experiments were\nconducted based on the hypothesis that OOV has an adversarial\nimpact on task performance. In this section, we artiﬁcially in-\nduce OOV on a pre-trained model through vocabulary pruning\nand correlate the OOV rate to task performance. With these syn-\nthetic OOV models, we use one of the tasks to investigate how\nOOV aﬀects task performance in a BERT model. Following this,\nwe apply our scheme to these synthetic models to verify if our\nproposed method is e ﬀective at recovering the performance of a\nbroken model.\nIn this section, we investigate the correlation between OOV\nand task performance by evaluating task performance using the\nbaseline BERT (bert-base-multilingual-cased) model, then com-\npare the results of that to models with varying OOV rates.\nWe use the three methods to eliminate the most frequent words,\nthe least frequent words, and random sampling. We compare dif-\nferent methods to ensure fairness, as the di ﬀerent methods exhibit\ndiﬀerent scenarios of how an OOV can be introduced in a down-\nstream task. NSMC was chosen because it was the largest dataset\nwe had for our experiments, and we assumed that the larger the\ntask corpus is, the more likely it will have a diverse vocabulary,\nhence being more susceptible to OOV.\nFor the frequency computation, we used two datasets. The\nﬁrst dataset we used is the kosentences\n*12 corpus. This corpus\nis a Korean corpus cleansed of Wiki markup from multiple pub-\nlicly available Wiki dumps. As we only use the Wikipedia part of\nkosentences, we will refer to the corpus as KoWiki in this paper.\nWe considered this to be a good approximation of what the back-\nbone model (bert-based-multilingual-cased) was initially trained\nwith. This is because almost every large-scale pre-train corpus\ncontains Wikipedia in some form. For this case, the frequency ta-\nble was initialized with every Korean subword in the model, and\nthe frequencies against the KoWiki corpus were updated on the\nfrequency table. Subwords in the model’s vocabulary, but not in\nthe KoWiki corpus, were kept at a 0. The second dataset used\nwas the actual task corpus, as using the task corpus is the most\neﬀective way to introduce OOV artiﬁcially.\nThis experiment intends to correlate the relation between OOV\nrate and task performance to conﬁrm our initial hypothesis. It is\nworth noting that as we do not train a model from scratch, this\nis an approximation and not an accurate representation of what\na pre-trained model’s vocabulary would have due to the prop-\nerties of subword tokenization depending on the character level\nn-gram distribution. This trade-o ﬀwas made for computational\neﬃciency reasons, as pre-training, a new model requires a signif-\nicant amount of computing power, and for our experiments, we\nwill need to train 42 models, which was computationally infeasi-\nble.\nIn our experiments, we prune subwords from the frequency ta-\nble in di ﬀerent ratios – for our experiments, we chose 0.1%, 1%,\n5%, 10%, 20%, and 50% as the target ratios. 20% and 50% are\n*12 https://github.com/cynthia/kosentences\nc⃝2021 Information Processing Society of Japan 497\nJournal of Information Processing Vol.29 490–503 (July 2021)\nFig. 6 Eﬀects of task performance caused by artiﬁcially induced OOV. All three pruning strategies have\nbeen compared in this plot.\nused to test extreme scenarios, to a point where it is likely that\nthe model predictions can be considered equivalent to random\nchoice. We use three di ﬀerent strategies for pruning the vocabu-\nlary, which we discuss in the subsections below. The ratio here is\nthe ratio of words of the frequency table’s vocabulary we prune\nfrom the vocabulary and should not be mistaken with the OOV\nratio discussed in the datasets section.\n6.2.1 Common Words\nRemoving the most frequent words is not a common scenario\nin any form, especially when it comes to a pre-trained model\nsetup. Ranking the vocabulary in order of frequency, we prune\nthe vocabulary from the top ranking (most frequent) word based\non the ratio to be pruned. For example, in a 1,000 word vocab-\nulary with a 5% prune rate, the end result will be a model that\nis missing 50 of the most frequent words. This was chosen to\ndemonstrate the extreme cases of unusable models, for instance,\nif a language that was expected to be supported was accidentally\nomitted from the training data.\n6.2.2 Rare Words\nThis method was chosen to simulate a scenario where the cor-\npus was sampled, or character coverage was reduced due to com-\nputational constraints. As least frequent subwords in a corpus\nwill be omitted from the vocabulary, we consider this a rough ap-\nproximation of what would happen when trade-o ﬀs are made due\nto the computational limitations. The process is the same as com-\nmon words, but in this case, pruning is in order of least frequent\nwords.\n6.2.3 Random Words\nIn random words, we randomly eliminate subwords from the\nvocabulary. The subwords list in the frequency table is used to\nselect target subwords to remove from the vocabulary. Based on\nthe target subword list, we randomly choose a word for removal\nand evaluate the performance.\nThis is also another approximation of the consequences of\ncomputational feasibility trade-o ﬀs, as with least frequent words.\nAs the distribution of subword frequency is expected to follow\nZipf’s law, even with random removal, we assume that the prob-\nability of an infrequent subword being chosen for deletion is in-\nversely proportional to the frequency of the given subword.\n6.3 Correlating Task Performance with OOV\nThe results of these experiments are summarized in Fig. 6, ac-\ncompanied by the full results in Table A ·2. An important point to\nnote here is that as this is a balanced, binary classiﬁcation task, it\nis unlikely that a model’s accuracy can go signiﬁcantly below 0.5.\nAs it converges towards 0.5, we can consider the model’s output\nto be equivalent to an equidistributed binary random number gen-\nerator, hence a random model.\nBased on the experiment results, the ﬁrst straightforward obser-\nvation we made is that removing rare words does not a ﬀect task\nperformance at all, regardless of how many are removed. Ana-\nlyzing the removed words, we observed that the removed words\nwere mostly words from a di ﬀerent language, which we suspect\nwill not have substantial contributions to task performance. On\nthe other hand, the other two methods used for pruning a ﬀect ac-\ncuracy, especially as the ratio increases.\nWe observed that pruning common words had immediate ef-\nfects, especially using the KoWiki corpus – as the e ﬀects are\napparent even at 0.1%. This is because the vocabulary of the\nfrequency table of KoWiki is larger than that of NSMC, 0.1%\npruned more subwords than the NSMC frequency table, which\nhad a smaller vocabulary. Removal of common words can have\ndevastating e ﬀects, as, without a matching su ﬃx form of a sub-\nword, the tokenizer’s greedy will fail. In both cases, we can see\nthat starting from around 5%; the model converges towards a ran-\ndom model’s performance. In these worst-case scenarios, we ob-\nserved that the model’s input had more OOV than actual sub-\nwords. In many cases, input to the model exclusively consisted\nc⃝2021 Information Processing Society of Japan 498\nJournal of Information Processing Vol.29 490–503 (July 2021)\nFig. 7 Performance recovery under di ﬀerent OOV rates. Note that the plot range for the y-axis is di ﬀerent\nbetween common, random, and rare.\nof OOV tokens.\nRandom pruning, on the other hand, tended to have a slower\neﬀect on task performance. This is expected, as the probability\nof pruning a common subword is lower than the probability of\npruning a less common word. We can still observe noticeable\nperformance decreases on both KoWiki and NSMC starting from\n5%. Unlike common, the model did not end up in a state compa-\nrable to random choice.\nEven when scaled up to 50%, pruning rare subwords did not\nhave signiﬁcant e ﬀects on the performance. This was somewhat\nunexpected, as we initially hypothesized it to a ﬀect the task per-\nformance with that many words removed. The reason turned out\nto be that even at 50%, most of the rare words only appeared once,\nand only a small portion (less than 5%) of these rare words were\nKorean – which explains the minimal e ﬀect on performance.\nWe use the models from this experiment with the same proto-\ncol we proposed to mitigate OOV for the recovery experiments.\nAmong the multiple methods proposed, we use character distance\n(CD), as it was shown to be e ﬀective while being computation-\nally eﬃcient, which allowed us to experiment with many di ﬀerent\nconﬁgurations.\n6.4 Recovery with Proposed Method\nThe results of this experiment are visualized in Fig. 7, and the\nfull results are in Table A ·3. The trends we observed in the\noriginal mitigation NSMC experiments repeat here. A model\nthat has been both through additional pre-training and OOV-\npatching consistently outperformed a model without additional\nc⃝2021 Information Processing Society of Japan 499\nJournal of Information Processing Vol.29 490–503 (July 2021)\nTable 4 OOV Rate tested on the task datasets used for our experiments with language-speciﬁc models.\nRates have been rounded the ﬁrst decimal digit. (0.0% is any value under 0.05%.)\nDataset Model OOV Tokens Total Tokens Token Rate OOV Sentences Total Sentences Sentence Rate\nNSMC bert-base-multi 81,603 5,185,891 1.5% 60,151 200,000 30.1%\nNSMC KR-BERT 360 4,773,732 0.0% 336 200,000 0.1%\nKorQuAD bert-base-multi 14,159 5,134,799 2.8% 8,569 144 K 5.9%\nKorQuAD KR-BERT 5,978 4,396,060 1.4% 2,393 144 K 1.7%\nTwitter bert-base-multi 10,310 985,345 1.0% 5,518 22,000 25.1%\nTwitter cl-tohoku-base-v2 26,566 951,286 2.8% 10,165 22,000 46.2%\nINEWS bert-base-multi 2,570 158,212 1.6% 1,278 6,355 20.1%\nINEWS bert-base-chinese 2,338 158,065 1.5% 1,119 6,355 17.6%\npre-training. In this particular setup, we hypothesize that addi-\ntional pre-training contributions in models with higher OOV rates\ncan be attributed to the fact that many subwords have now been\nmapped to semantically di ﬀerent words, so the model has to learn\nthe structure of the text nearly from scratch. However, our results\nsuggest that the proposed method can even be e ﬀective at im-\nproving performance in high-OOV conditions, such as a model\nthat was pre-trained on corpora extremely disparate from the tar-\nget task’s domain.\nWhile the model does not fully manage to recover to the best-\ncase performance fully, we also observed that extreme case mod-\nels such as those with comparable performance to a random\nmodel could also recover quite well. However, in these extreme\ncase models, we observed that due to the high amount of surro-\ngates needed, the model started borrowing subwords from other\nlanguages, from the CJK ideograph block as a surrogate for Ko-\nrean subwords.\nWe do not have any theoretical proof of why extreme cases like\nthis can also recover near-baseline performance. We hypothesize\nthat without additional pre-training, the model’s representations\nlack semantic or contextual information; hence it acts as a ran-\ndom embeddings model. This model can still be used to classify\ndata with a classiﬁcation head. With additional pre-training, the\nmodel re-learns the structure from the input, only with di ﬀerent\nembeddings, and due to the surrogate assignment being exclusive\nin our method, the model can adapt to inherent structure from the\ntask corpus.\n7. Applicability to Other Models\n7.1 Multilingual Models\nWhile our experiments are limited to BERT, the method can be\napplied to any model. Generally, our proposed method is most ef-\nfective when applied to greedy merging tokenizers such as Word-\nPiece, which is used by both BERT and ELECTRA [4]. This is\ndue to the fact that greedy merging results in whole chunks of text\nbeing lost during tokenization, as we have observed in the Fig. 5\nexamples.\nHowever, our method is applicable to most subword tokeniza-\ntion methods, such as Byte-pair Encoding (BPE), used by the\nmultilingual model XLM [15], and SentencePiece [14], used by\nanother multilingual model, XLM-R [5] also can beneﬁt from\nthis. The e ﬀects will be less signiﬁcant since both tokenizers are\nnot greedy. The expected e ﬀect is a diversiﬁcation of the UNK\ntoken by re-assigning it to di ﬀerent subwords instead of all OOV\ntokens being mapped to a single embedding. This is expected to\nmake it easier to train. In an actual byte-level\n*13 subword tok-\nenization, such as used in GPT-2 [21], our method is not expected\nto have any gains as there will always be a byte-level fallback.\n7.2 Monolingual Models\nFollowing the discussion on our method’s applicability to dif-\nferent multilingual models, we also investigated whether or not\nOOV is also a phenomenon in language-speciﬁc models. As our\nmethod depends on the occurrence of OOV in the ﬁrst place, if\nthere is a low OOV rate, the contributions of OOV mitigation are\nalso expected to be minor. To investigate this, we used three sep-\narate monolingual models for each language.\nFor Chinese, we used the o ﬃcial BERT Chinese model (bert-\nbase-chinese) released as part of the pre-trained models in\nRef. [8], with the BERT tokenizer, and for Japanese we used *14.\nFinally, for Korean, we used KR-BERT [16] with Normalization\nForm Compatibility Decomposition (NFKD) *15 pre-processing.\nThe subcharacter decomposition is similar to the work proposed\nin Ref. [19] and makes this method much more robust against\nOOV. Each of the monolingual models was used to tokenize\nthe respective language dataset compared with the multilingual\nmodel used in this work. The results are disclosed in Table 4.\nWe observed that Korean, which is the most e ﬀective language\nto our scheme, we can see that the amount of OOV tokens in this\nmodel is extremely low. Due to this, it is unlikely to have ad-\nversarial eﬀects on performance. While the OOV token ratio was\nstill above 1% for KorQuAD, most of this turned out to be caused\nby subwords in a foreign language (e.g., CJK Ideographs), which\nis unlikely to have severe e ﬀects as it is assumed that the reader\ndoes not necessarily have to comprehend this from the passage to\nproduce an answer\n*16 for the task.\nJapanese, on the other hand, showed an increase in OOV. This\nis likely because the pre-training corpus was Wikipedia, which is\nwell-formed text lacking colloquial writing, and Emojis, common\nin data sourced from social networks. In Chinese, there was very\nlittle di ﬀerence as with the multilingual model, so the e ﬀects of\napplying our method are likely to be the same as a multilingual\nmodel.\n*13 Byte-pair Encoding is commonly misrepresented, as while the original\nmethod does operate at byte-level, current applications all operate at Uni-\ncode character level.\n*14 https://github.com/cl-tohoku/bert-japanese\n*15 This model does not work if this normalization is omitted.\n*16 We conﬁrmed that none of the answers expected an answer in a di ﬀerent\nlanguage from the dataset.\nc⃝2021 Information Processing Society of Japan 500\nJournal of Information Processing Vol.29 490–503 (July 2021)\n8. Conclusions\nIn this work, we investigate the correlation between OOV and\ntask performance in the context of transfer learning. With dif-\nfering OOV rates, we conﬁrm our hypothesis that OOV directly\naﬀects the performance of a model in a transfer setup, to the point\nthat it makes the model comparable to a model that is randomly\nchoosing answers.\nAfter demonstrating examples and the e ﬀects of OOV triggered\ninformation loss with evaluation performance in a task under\na transfer learning setup, we propose multiple mitigating OOV\nmethods during downstream task ﬁne-tuning. We then demon-\nstrate and compare with no mitigation, mitigation through net-\nwork modiﬁcation, and surrogates, which require no network\nmodiﬁcation, and show how each approach a ﬀects downstream\ntasks. In particular, we show that vocabulary surrogates can\nprovide performance boosts with no additional computation cost\nat the model level, especially when paired with additional pre-\ntraining. Additionally, with the same experiments, we also con-\nﬁrm that tasks with lower OOV su ﬀer less than tasks with higher\nOOV.\nWe further explore the applicability of our work in extreme\ncases and use the high-OOV models we used to test our hy-\npothesis, combined with the proposed mitigation can recover the\nmodel’s capabilities and conclude that in a transfer learning setup,\ntokenization serves a signiﬁcant role in a pre-trained model’s ca-\npabilities.\n8.1 Future Work\nAdditionally, one of our work’s limitations is that most of the\nsurrogate methods cannot be used for generative tasks, as tokens\nare replaced with other tokens. We expect future work to explore\npotential solutions for this limitation. Finally, while we provided\na hypothesis, the reason why BERT can still perform using un-\nseen subwords for classiﬁcation is not an answered question and\nwarrants further investigation.\nAcknowledgments This paper is based on results obtained\nfrom a project, JPNP18002, commissioned by the New Energy\nand Industrial Technology Development Organization (NEDO).\nPart of the compute used for the experiments were provided by\nOdd Concepts Inc.\nThe authors also thank Won Ik Cho, Tatsuya Hiraoka, Sakae\nMizuki, Sho Takase, and Angela Smiley for their suggestions and\ninsightful discussions.\nReferences\n[1] Aji, A.F., Bogoychev, N., Heaﬁeld, K. and Sennrich, R.: In Neural\nMachine Translation, What Does Transfer Learning Transfer?, Proc.\n58th Annual Meeting of the Association for Computational Linguis-\ntics, pp.7701–7710, Association for Computational Linguistics (on-\nline), DOI: 10.18653 /v1/2020.acl-main.688 (2020).\n[2] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,\nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,\nHerbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,\nZiegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I. and Amodei, D.: Language models are\nfew-shot learners (2020).\n[3] Cho, W.I., Moon, S. and Song, Y .: Open Korean Corpora: A Practical\nReport, Proc. 2nd Workshop for NLP Open Source Software(NLP-\nOSS), pp.85–93, Association for Computational Linguistics (online),\nDOI: 10.18653/v1/2020.nlposs-1.12 (2020).\n[4] Clark, K., Luong, M.-T., Le, Q.V . and Manning, C.D.: ELECTRA:\nPre-training Text Encoders as Discriminators Rather Than Generators,\nICLR (2020) (online), available from ⟨https://openreview.net/\npdf?id=r1xMH1BtvB⟩.\n[5] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V ., Wenzek, G.,\nGuzm´an, F., Grave, E., Ott, M., Zettlemoyer, L. and Stoyanov, V .:\nUnsupervised Cross-lingual Representation Learning at Scale, Proc.\n58th Annual Meeting of the Association for Computational Linguis-\ntics, pp.8440–8451, Association for Computational Linguistics (on-\nline), DOI: 10.18653 /v1/2020.acl-main.747 (2020).\n[6] Conneau, A. and Lample, G.: Cross-lingual Language Model Pretrain-\ning, Advances in Neural Information Processing Systems 32, Wallach,\nH., Larochelle, H., Beygelzimer, A., d\n′Alch´e-Buc, F., Fox, E. and\nGarnett, R. (Eds.), pp.7057–7067, Curran Associates, Inc. (2019) (on-\nline), available from ⟨http://papers.nips.cc/paper/8928-cross-lingual-\nlanguage-model-pretraining.pdf⟩.\n[7] Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K.: BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Under-\nstanding, Proc. 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Tech-\nnologies, Volume 1(Long and Short Papers), pp.4171–4186, Associa-\ntion for Computational Linguistics (online), DOI: 10.18653 /v1/\nN19-1423 (2019).\n[8] Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K.: BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Under-\nstanding, Proc. 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Tech-\nnologies, Volume 1(Long and Short Papers), pp.4171–4186, Associa-\ntion for Computational Linguistics (online), DOI: 10.18653 /v1/\nN19-1423 (2019).\n[9] Dror, R., Baumer, G., Shlomov, S. and Reichart, R.: The Hitchhiker’s\nGuide to Testing Statistical Signiﬁcance in Natural Language Process-\ning, Proc. 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp.1383–1392, Association for\nComputational Linguistics (online), DOI: 10.18653 /v1/\nP18-1128 (2018).\n[10] Gillick, D., Brunk, C., Vinyals, O. and Subramanya, A.: Multilingual\nLanguage Processing From Bytes, Proc. 2016 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp.1296–1306, Association for Com-\nputational Linguistics (online), DOI: 10.18653 /v1/N16-1155 (2016).\n[11] Howard, J. and Ruder, S.: Universal Language Model Fine-tuning for\nText Classiﬁcation, Proc. 56th Annual Meeting of the Association for\nComputational Linguistics(Volume 1: Long Papers), pp.328–339, As-\nsociation for Computational Linguistics (online), DOI: 10.18653 /v1/\nP18-1031 (2018).\n[12] Kolachina, P., Riedl, M. and Biemann, C.: Replacing OOV Words\nFor Dependency Parsing With Distributional Semantics, Proc. 21st\nNordic Conference on Computational Linguistics, pp.11–19, Associ-\nation for Computational Linguistics (2017) (online), available from\n⟨https://www.aclweb.org/anthology/W17-0202⟩.\n[13] Kudo, T. and Richardson, J.: SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text pro-\ncessing, EMNLP 2018 – Proc. Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations(online), DOI:\n10.18653/v1/d18-2012 (2018).\n[14] Kudo, T. and Richardson, J.: SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for Neural Text Pro-\ncessing, Proc. 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pp.66–71, Association\nfor Computational Linguistics (online), DOI: 10.18653 /v1/D18-2012\n(2018).\n[15] Lample, G. and Conneau, A.: Cross-lingual Language Model Pretrain-\ning, Advances in Neural Information Processing Systems(NeurIPS)\n(2019).\n[16] Lee, S., Jang, H., Baik, Y ., Park, S. and Shin, H.: KR-BERT: A Small-\nScale Korean-Speciﬁc Language Model (2020).\n[17] Luong, T., Sutskever, I., Le, Q., Vinyals, O. and Zaremba, W.: Ad-\ndressing the Rare Word Problem in Neural Machine Translation, Proc.\n53rd Annual Meeting of the Association for Computational Linguis-\ntics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp.11–19, Association for Com-\nputational Linguistics (online), DOI: 10.3115 /v1/P15-1002 (2015).\n[18] Merity, S., Keskar, N.S. and Socher, R.: Regularizing and Optimizing\nLSTM Language Models, arXiv preprint arXiv:1708.02182 (2017).\n[19] Moon, S. and Okazaki, N.: Jamo Pair Encoding: Subchar-\nacter Representation-based Extreme Korean V ocabulary Compres-\nsion for E ﬃcient Subword Tokenization, Proc. 12th Language\nResources and Evaluation Conference , pp.3490–3497, European\nLanguage Resources Association (2020) (online), available from\nc⃝2021 Information Processing Society of Japan 501\nJournal of Information Processing Vol.29 490–503 (July 2021)\n⟨https://www.aclweb.org/anthology/2020.lrec-1.429⟩.\n[20] Moon, S. and Okazaki, N.: PatchBERT: Just-in-Time, Out-of-\nV ocabulary Patching, Proc. 2020 Conference on Empirical Methods\nin Natural Language Processing(EMNLP), pp.7846–7852, Associa-\ntion for Computational Linguistics (online), DOI: 10.18653 /v1/2020.\nemnlp-main.631 (2020).\n[21] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. and Sutskever,\nI.: Language Models are Unsupervised Multitask Learners, Technical\nReport (2018).\n[22] Rajpurkar, P., Zhang, J., Lopyrev, K. and Liang, P.: SQuad: 100,000 +\nquestions for machine comprehension of text, EMNLP 2016 – Proc.\nConference on Empirical Methods in Natural Language Processing\n(online), DOI: 10.18653 /v1/d16-1264 (2016).\n[23] Sennrich, R., Haddow, B. and Birch, A.: Neural machine translation\nof rare words with subword units, 54th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2016 – Long Papers(online),\nDOI: 10.18653/v1/p16-1162 (2016).\n[24] Stratos, K.: A Sub-Character Architecture for Korean Language Pro-\ncessing, Proc. 2017 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp.721–726, Association for Computational Lin-\nguistics (online), DOI: 10.18653 /v1/D17-1075 (2017).\n[25] Suzuki, Y .: Filtering Method for Twitter Streaming Data Using\nHuman-in-the-Loop Machine Learning, Journal of Information Pro-\ncessing, V ol.27, pp.404–410 (online), DOI: 10.2197 /ipsjjip.27.404\n(2019).\n[26] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, Ł. and Polosukhin, I.: Attention is All you Need, Ad-\nvances in Neural Information Processing Systems, Guyon, I., Luxburg,\nU.V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S. and\nGarnett, R. (Eds.), V ol.30, pp.5998–6008, Curran Associates, Inc.\n(2017).\n[27] Wan, Z., Wan, X. and Wang, W.: Improving Grammatical Error Cor-\nrection with Data Augmentation by Editing Latent Representation,\nProc. 28th International Conference on Computational Linguistics,\npp.2202–2212, International Committee on Computational Linguis-\ntics (2020) (online), available from ⟨https://www.aclweb.org/\nanthology/2020.coling-main.200⟩.\n[28] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O. and Bowman,\nS.: GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding, Proc. 2018 EMNLP Workshop\nBlackbox NLP: Analyzing and Interpreting Neural Networks for\nNLP, pp.353–355, Association for Computational Linguistics (on-\nline), DOI: 10.18653 /v1/W18-5446 (2018).\n[29] Wang, H., Yu, D., Sun, K., Chen, J. and Yu, D.: Improving Pre-\nTrained Multilingual Model with V ocabulary Expansion, Proc. 23rd\nConference on Computational Natural Language Learning(CoNLL),\npp.316–327, Association for Computational Linguistics (online),\nDOI: 10.18653/v1/K19-1030 (2019).\n[30] Wu, Y ., Schuster, M., Chen, Z., Le, Q.V ., Norouzi, M., Macherey,\nW., Krikun, M., Cao, Y ., Gao, Q., Macherey, K., Klingner, J., Shah,\nA., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y ., Kudo, T.,\nKazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C.,\nSmith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes,\nM. and Dean, J.: Google’s Neural Machine Translation System:\nBridging the Gap between Human and Machine Translation, CoRR,\nVo l . a b s/1609.08144 (2016) (online), available from ⟨http://arxiv.org/\nabs/1609.08144⟩.\nAppendix\nA.1 Hyperparameters\nWe ran our experiments as close as possible to the baseline\nparameters used by the publicly available benchmark scripts for\neach task type. This means most of the hyperparameters for all\nof the evaluation was done as close to the default values as pos-\nsible. For the OOV correlation and recovery tasks, we optimized\nthe sequence length and batch size parameter speciﬁcally to the\ntask to maximize VRAM usage for faster experimentation. The\nexact hyperparameters are disclosed in Table A·1 *17.\nThe masking probability for the MLM task was set to 0.15 for\nadditional pre-training. We did not use whole word masking for\n*17 The parameters used will use 23.5 GBs out of 24 GB of available VRAM\nwhen training using IEEE754 half-precision ﬂoating point tensors.\nour experiments.\nA.2 Environment and Computation Cost\nThe experiments in this paper were run on two di ﬀerent envi-\nronments. The additional pre-training and accompanied evalua-\ntion experiments were executed on a shared rt G.small instance\non the ABCI compute cluster *18.A nr t G.small node has six seg-\nregated CPU cores from a Xeon Gold 6148, a Tesla V100 GPU\nwith 16 GB VRAM, and 60 GBs of memory. The training data\nand experimental code was streamed from a shared GPFS mount.\nEach experiment requires a di ﬀerent amount of compute bud-\nget. The longest-running experiment ﬁnished in 10 hours of wall\nclock time, and the shortest ﬁnished in 2 hours of wall clock time.\nThe average runtime for each experiment was approximately 5.5\nhours.\nThe OOV correlation and recovery NSMC experiments were\nexecuted on a desktop computer with a Ryzen 9 3900XT 12-core\nprocessor, RTX3090 GPU with 24 GBs of VRAM, and 64 GBs\nof memory. The training data and experimental code were on\na local Phison E16 NVMe drive. Both tasks required the same\namount of compute budget, and the average runtime for each ex-\nperiment was approximately 40 minutes with the hyperparameter\noptimizations used above.\nA.3 Experiment Result Tables\nThe results obtained from all of the experiments, including\nthose omitted from the plots, are in Tables A·2 and A·3 respec-\ntively.\n*18 https://abci.ai/\nc⃝2021 Information Processing Society of Japan 502\nJournal of Information Processing Vol.29 490–503 (July 2021)\nTable A·1 Hyperparameters used to train each of the downstream task models.\nTask Optimizer Adam ϵ LR GradAccum Weight Decay Length Batch Size Epochs\nAdditional Pre-training Adam 1e-8 5e-5 1 0.0 512 6 3\nOOV Correlation (NSMC) Adam 1e-8 2e-5 1 0.0 160 160 3\nMitigation (GLUE) Adam 1e-8 2e-5 1 0.0 512 10 3\nQuestion Answering (KorQuAD) Adam 1e-8 3e-5 1 0.0 512 12 3\nOOV Recovery (NSMC) Adam 1e-8 2e-5 1 0.0 160 160 3\nTable A·2 Experiment results demonstrating the e ﬀects of OOV in an artiﬁcial setup. The percentages\nare the ratio of words removed based on the frequency table computed with di ﬀerent data.\nMethod is the di ﬀerent methods used for pruning.\nFrequency Table Method 0% 0.1% 1% 5% 10% 20% 50%\nKoWiki Common 0.87730 0.75882 0.64312 0.53632 0.52584 0.50994 0.49654\nKoWiki Rare 0.87730 0.86772 0.86730 0.86730 0.86842 0.86786 0.86928\nKoWiki Random 0.87730 0.86892 0.86456 0.85934 0.82758 0.79182 0.70068\nNSMC Common 0.87730 0.83650 0.70486 0.59100 0.49656 0.52088 0.50788\nNSMC Rare 0.87730 0.86772 0.86918 0.86766 0.86790 0.86762 0.86794\nNSMC Random 0.87730 0.86784 0.86494 0.85550 0.82104 0.76904 0.66556\nTable A·3 Recovery experiment results. Patched is with mitigation, Patched +MLM is with mitigation\nand additional pre-training.\nFrequency Table Sampler Mitigation 0% 0.1% 1% 5% 10% 20% 50%\nNone 0.87730 0.75882 0.64312 0.53632 0.52584 0.50994 0.49654\nKoWiki Common Patched (CD) 0.88390 0.87132 0.85702 0.85014 0.84756 0.85146 0.85120\nPatched (CD) +MLM 0.88850 0.88446 0.86514 0.86256 0.86918 0.86662 0.86562\nNone 0.87730 0.86772 0.86730 0.86730 0.86842 0.86786 0.86928\nKoWiki Rare Patched 0.88390 0.88094 0.88072 0.88072 0.88060 0.88124 0.88020\nPatched (CD) +MLM 0.88850 0.88588 0.88556 0.88572 0.88628 0.88530 0.88646\nNone 0.87730 0.86892 0.86456 0.85934 0.82758 0.79182 0.70068\nKoWiki Random Patched 0.88390 0.88092 0.88078 0.87752 0.87452 0.87012 0.85542\nPatched (CD) +MLM 0.88850 0.88582 0.88490 0.88522 0.88380 0.88334 0.87710\nNone 0.87730 0.83650 0.70486 0.59100 0.49656 0.52088 0.50788\nNSMC Common Patched 0.86830 0.87942 0.87530 0.86340 0.85716 0.84872 0.85000\nPatched (CD) +MLM 0.88850 0.88432 0.87788 0.86590 0.86146 0.86122 0.86040\nNone 0.87730 0.86772 0.86918 0.86766 0.86790 0.86762 0.86794\nNSMC Rare Patched 0.86830 0.88176 0.88208 0.88264 0.88242 0.88248 0.88266\nPatched (CD) +MLM 0.88850 0.88610 0.88622 0.88648 0.88446 0.88588 0.88490\nNone 0.87730 0.86784 0.86494 0.85550 0.82104 0.76904 0.66556\nNSMC Random Patched 0.86830 0.88288 0.88112 0.88264 0.87702 0.87506 0.86194\nPatched (CD) +MLM 0.88850 0.88488 0.88634 0.88196 0.87882 0.87388 0.86554\nSangwhan Moon received his master’s\ndegree in computer science from the\nGeorgia Institute of Technology in 2017.\nHe is a director of engineering at Odd\nConcepts Inc. and an elected member of\nthe World Wide Web Consortium (W3C)\nTechnical Architecture Group. He also\nis a standardization expert for the Korean\nTelecommunications Technology Association. His research in-\nterests include natural language processing, computer vision, ma-\nchine learning, and information retrieval.\nNaoaki Okazaki is a professor in School\nof Computing, Tokyo Institute of Technol-\nogy, Japan. Prior to this faculty position,\nhe worked as a post-doctoral researcher in\nUniversity of Tokyo (2007–2011), andas\nan associate professor in Tohoku Univer-\nsity (2011–2017). He is also a senior sci-\nentiﬁc research specialist of Ministry of\nEducation, Culture, Sports, Science and Technology (MEXT)\nand a visiting research scholar of the Artiﬁcial Intelligence Re-\nsearch Center (AIRC), National Institute of Advanced Industrial\nScience and Technology (AIST). His research areas include Nat-\nural Language Processing (NLP), Artiﬁcial Intelligence (AI), and\nMachine Learning.\nc⃝2021 Information Processing Society of Japan 503"
}