{
    "title": "A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP",
    "url": "https://openalex.org/W3196691776",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2101087330",
            "name": "Zhao Yu-cheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2389371305",
            "name": "Wang Guang-ting",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227687748",
            "name": "Tang, Chuanxin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1972028875",
            "name": "Luo Chong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2354302276",
            "name": "Zeng, Wenjun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202086093",
            "name": "Zha, Zheng-Jun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3102631365",
        "https://openalex.org/W3170227631",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W2911925209",
        "https://openalex.org/W3211347078",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W3165150763",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3130071011",
        "https://openalex.org/W3035452548",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W3158846111",
        "https://openalex.org/W2992308087",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W3153842237",
        "https://openalex.org/W3172064272",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W3174980028",
        "https://openalex.org/W3163465952",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W3172801447",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W639708223"
    ],
    "abstract": "Convolutional neural networks (CNN) are the dominant deep neural network (DNN) architecture for computer vision. Recently, Transformer and multi-layer perceptron (MLP)-based models, such as Vision Transformer and MLP-Mixer, started to lead new trends as they showed promising results in the ImageNet classification task. In this paper, we conduct empirical studies on these DNN structures and try to understand their respective pros and cons. To ensure a fair comparison, we first develop a unified framework called SPACH which adopts separate modules for spatial and channel processing. Our experiments under the SPACH framework reveal that all structures can achieve competitive performance at a moderate scale. However, they demonstrate distinctive behaviors when the network size scales up. Based on our findings, we propose two hybrid models using convolution and Transformer modules. The resulting Hybrid-MS-S+ model achieves 83.9% top-1 accuracy with 63M parameters and 12.3G FLOPS. It is already on par with the SOTA models with sophisticated designs. The code and models are publicly available at https://github.com/microsoft/SPACH.",
    "full_text": "A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP\nYucheng Zhao*†1 Guangting Wang*†1 Chuanxin Tang*2 Chong Luo2 Wenjun Zeng2\nZheng-Jun Zha 1\nUniversity of Science and Technology of China1 Microsoft Research Asia2\n{lnc, flylight}@mail.ustc.edu.cn {chutan, cluo, wezeng}@microsoft.com zhazj@ustc.edu.cn\nAbstract\nConvolutional neural networks (CNN) are the domi-\nnant deep neural network (DNN) architecture for com-\nputer vision. Recently, Transformer and multi-layer per-\nceptron (MLP)-based models, such as Vision Transformer\nand MLP-Mixer, started to lead new trends as they showed\npromising results in the ImageNet classiﬁcation task. In this\npaper, we conduct empirical studies on these DNN struc-\ntures and try to understand their respective pros and cons.\nTo ensure a fair comparison, we ﬁrst develop a uniﬁed\nframework called SPACH which adopts separate modules\nfor spatial and channel processing. Our experiments under\nthe SPACH framework reveal that all structures can achieve\ncompetitive performance at a moderate scale. However,\nthey demonstrate distinctive behaviors when the network\nsize scales up. Based on our ﬁndings, we propose two hy-\nbrid models using convolution and Transformer modules.\nThe resulting Hybrid-MS-S+ model achieves 83.9% top-1\naccuracy with 63M parameters and 12.3G FLOPS. It is\nalready on par with the SOTA models with sophisticated\ndesigns. The code and models are publicly available at\nhttps://github.com/microsoft/SPACH.\n1. Introduction\nConvolutional neural networks (CNNs) have been domi-\nnating the computer vision (CV) ﬁeld since the renaissance\nof deep neural networks (DNNs). They have demonstrated\neffectiveness in numerous vision tasks from image classiﬁ-\ncation [12], object detection [27], to pixel-based segmenta-\ntion [11]. Remarkably, despite the huge success of Trans-\nformer structure [37] in natural language processing (NLP)\n[8], the CV society still focuses on the CNN structure for\nquite some time.\nThe transformer structure ﬁnally made its grand debut\nin CV last year. Vision Transformer (ViT) [9] showed that\na pure Transformer applied directly to a sequence of image\npatches can perform very well on image classiﬁcation tasks,\n* Equal contribution. † Interns at MSRA.\nif the training dataset is sufﬁciently large. DeiT [35] further\ndemonstrated that Transformer can be successfully trained\non typical-scale dataset, such as ImageNet-1K [7], with ap-\npropriate data augmentation and model regularization.\nInterestingly, before the heat of Transformer dissipated,\nthe structure of multi-layer perceptrons (MLPs) was revived\nby Tolstikhin et al. in a work called MLP-Mixer [33]. MLP-\nMixer is based exclusively on MLPs applied across spa-\ntial locations and feature channels. When trained on large\ndatasets, MLP-Mixer attains competitive scores on image\nclassiﬁcation benchmarks. The success of MLP-Mixer sug-\ngests that neither convolution nor attention are necessary for\ngood performance. It sparked further research on MLP as\nthe authors wished [20, 26].\nHowever, as the reported accuracy on image classiﬁca-\ntion benchmarks continues to increase by new network de-\nsigns from various camps, no conclusion can be made as\nwhich structure among CNN, Transformer, and MLP per-\nforms the best or is most suitable for vision tasks. This is\npartly due to the pursuit of high scores that leads to multi-\nfarious tricks and exhaustive parameter tuning. As a result,\nnetwork structures cannot be fairly compared in a system-\natic way. The work presented in this paper ﬁlls this blank\nby conducting a series of controlled experiments over CNN,\nTransformer, and MLP in a uniﬁed framework.\nWe ﬁrst develop a uniﬁed framework called SPACH as\nshown in Fig. 1. It is mostly adopted from current Trans-\nformer and MLP frameworks, since convolution can also ﬁt\ninto this framework and is in general robust to optimization.\nThe SPACH framework contains a plug-and-play module\ncalled mixing block which could be implemented as con-\nvolution layers, Transformer layers, or MLP layers. Aside\nfrom the mixing block, other components in the frame-\nwork are kept the same when we explore different struc-\ntures. This is in stark contrast to previous work which com-\npares different network structures in different frameworks\nthat vary greatly in layer cascade, normalization, and other\nnon-trivial implementation details. As a matter of fact, we\nfound that these structure-free components play an impor-\ntant role in the ﬁnal performance of the model, and this is\narXiv:2108.13002v2  [cs.CV]  25 Nov 2021\ncommonly neglected in the literature.\nWith this uniﬁed framework, we design a series of con-\ntrolled experiments to compare the three network structures.\nThe results show that all three network structures could per-\nform well on the image classiﬁcation task when pre-trained\non ImageNet-1K. In addition, each individual structure has\nits distinctive properties leading different behaviors when\nthe network size scales up. We also ﬁnd several common\ndesign choices which contribute a lot to the performance of\nour SPACH framework. The detailed ﬁndings are listed in\nthe following.\n• Multi-stage design is standard in CNN models, but\nits effectiveness is largely overlooked in Transformer-\nbased or MLP-based models. We ﬁnd that the multi-\nstage framework consistently and notably outperforms\nthe single-stage framework no matter which of the\nthree network structures is chosen.\n• Local modeling is efﬁcient and crucial. With only\nlight-weight depth-wise convolutions, the convolution\nmodel can achieve similar performance as a Trans-\nformer model in our SPACH framework. By adding\na local modeling bypass in both MLP and Transformer\nstructures, a signiﬁcant performance boost is obtained\nwith negligible parameters and FLOPs increase.\n• MLP can achieve strong performance under small\nmodel sizes, but it suffers severely from over-ﬁtting\nwhen the model size scales up. We believe that over-\nﬁtting is the main obstacle that prevents MLP from\nachieving SOTA performance.\n• Convolution and Transformer are complementary in\nthe sense that convolution structure has the best gen-\neralization capability while Transformer structure has\nthe largest model capacity among the three structures.\nThis suggests that convolution is still the best choice in\ndesigning lightweight models but designing large mod-\nels should take Transformer into account.\nBased on these ﬁndings, we propose two hybrid mod-\nels of different scales which are built upon convolution and\nTransformer layers. Experimental results show that, when\na sweet point between generalization capability and model\ncapacity is reached, the performance of these straightfor-\nward hybrid models is already on par with SOTA models\nwith sophisticated architecture designs.\n2. Background\nCNN and its variants have dominated the vision do-\nmain. During the evolution of CNN models, useful expe-\nrience about the architecture design has been accumulated.\nRecently, two types of architectures, namely Transformer\n[9] and MLP [33], begin to emerge in the vision domain\nand have shown performance similar to the well-optimized\nCNNs. These results kindle a spark towards building better\nvision models beyond CNNs.\nConvolution-based vision modelsSince the entrance of\ndeep learning era pioneered by AlexNet [18], the computer\nvision community has devoted enormous efforts to design-\ning better vision backbones. In the past decade, most work\nfocused on improving the design of CNN, and a series of\nnetworks, including VGG [29], ResNet [12], SENet [15],\nXception [2], MoblieNet [14,28], and EfﬁcientNet [31,32],\nare designed. They achieve signiﬁcant accuracy improve-\nments in various vision tasks.\nA standard convolution layer learns ﬁlters in a 3D space,\nwith two spatial dimensions and one channel dimension.\nThus, the learning of spatial correlations and channel corre-\nlations are coupled inside a single convolution kernel. Dif-\nferently, A depth-wise convolution layer only learns spa-\ntial correlations by moving the learning process of channel\ncorrelations to an additional 1x1 convolution. The funda-\nmental hypothesis behind this design is that cross-channel\ncorrelations and spatial correlations are sufﬁciently decou-\npled that it is preferable not to map them jointly [2]. Re-\ncent work [31, 32] shows that depth-wise convolution can\nachieve both high accuracy and good efﬁciency, conﬁrm-\ning this hypothesis to some extent. In addition, the idea\nof decoupling spatial and channel correlations is adopted in\nthe vision Transformer. Therefore, this paper employs the\nspatial-channel decoupling idea in our framework design.\nTransformer-based vision models. With the success of\nTransformer in natural language processing (NLP) [8, 37],\nmany researchers start to explore the use of Transformer as\na stand-alone architecture for vision tasks. They are fac-\ning two main challenges. First, Transformer operates over a\ngroup of tokens, but no natural tokens, similar to the words\nin natural language, exist in an image. Second, images\nhave a strong local structure while the Transformer structure\ntreats all tokens equally and ignores locality. The pioneer-\ning work ViT [9] solved the ﬁrst challenge by simply divid-\ning an image into non-overlapping patches and treat each\npatch as a visual token. ViT also reveals that Transformer\nmodels trained on large-scale datasets could attain SOTA\nimage recognition performance. However, when the train-\ning data is insufﬁcient, ViT does not perform well due to the\nlack of inductive biases. DeiT [35] mitigates the problem\nby introducing a regularization and augmentation pipeline\non ImageNet-1K.\nSwin [21] and Twins [3] propose local ViT to address\nthe second challenge. They adopt locally-grouped self-\nattention by computing the standard self-attention within\nnon-overlapping windows. The local mechanism not only\nleads to performance improvement thanks to the reintroduc-\ntion of locality, but also bring sufﬁcient improvement on\nPatch Embedding\nMixing Block\nDownsample\nMixing Block\nMixing Block\nGlobal Avg. Pooling\nLinear Classifier\nClass\nPatch Embedding\nMixing Block\nMixing Block\nGlobal Avg. Pooling\nLinear Classifier\nClass\nDownsample\nMixing Block\nMixing Block\nMixing Block\nMixing Block\n…...\nN1N1 N2N2 N4N4\nNN\n…...\n…... …... …...\n(a) Single-stage SPACH framework, consisting mainly of N mixing blocks\n(c) Multi-stage SPACH framework, where the ith stage has Ni mixing blocks\nSpatial Mixing\nLayer Norm\n(b) A mixing block is composed of \nspatial mixing and channel mixing\nChannel Mixing\nMLP\nFigure 1. Illustration of the proposed experimental framework named SPACH.\nmemory and computational efﬁciency. Thus, the pyramid\nstructure becomes feasible again for vision Transformer.\nThere has been a blowout development in the design of\nTransformer-based vision models. Since this paper is not\nintended to review the progress of vision Transformer, we\nonly brieﬂy introduce some highly correlated Transformer\nmodels. CPVT [4] and CvT [39] introduce convolution\ninto Transformer blocks, bringing the desired translation-\ninvariance properties into ViT architecture. CaiT [36] in-\ntroduces a LayerScale approach to empower effective train-\ning of deeper ViT network. It is also discovered that some\nclass-attention layers built on top of ViT network offer more\neffective processing than the class embedding. LV-ViT [17]\nproposes a bag of training techniques to build a strong base-\nline for vision Transformer. LeViT [10] proposes a hybrid\nneural network for fast image classiﬁcation inference.\nMLP-based vision models. Although MLP is not a\nnew concept for the computer vision community, the re-\ncent progress on MLP-based visual models surprisingly\ndemonstrates, both conceptually and technically, that sim-\nple architecture can achieve competitive performance with\nCNN or Transformer [33]. The pioneering work MLP-\nMixer proposed a Mixer architecture using channel-mixing\nMLPs and channel-mixing MLPs to communicate between\ndifferent channels and spatial locations (tokens), respec-\ntively. It achieves promising results when trained on a large-\nscale dataset (i.e., JFT [30]). ResMLP [34] built a simi-\nlar MLP-based model with a deeper architecture. ResMLP\ndoes not need large-scale datasets and it achieves compa-\nrable accuracy/complexity trade-offs on ImageNet-1K with\nTransformer-based models. FF [23] showed that simply re-\nplacing the attention layer in ViT with an MLP layer applied\nover the patch dimension could achieve moderate perfor-\nmance on ImageNet classiﬁcation. gMLP [20] proposed a\ngating mechanism on MLP and suggested that self-attention\nis not a necessary ingredient for scaling up machine learn-\ning models.\n3. A Uniﬁed Experimental Framework\nIn order to fairly compare the three network struc-\ntures, we are in need of a uniﬁed framework that excludes\nother performance-affecting factors. Since recent MLP-\nbased networks have already shared a similar framework as\nTransformer-based networks, we build the uniﬁed experi-\nmental framework based on them and try to include CNN-\nbased network in this framework as well.\n3.1. Overview of the SPACH Framework\nWe build our experimental framework with reference\nto ViT [9] and MLP-Mixer [33]. Fig. 1(a) shows the\nsingle-stage version of the SPACH framework, which is\nused for our empirical study. The architecture is very sim-\nple and consists mainly of a cascade of mixing blocks, plus\nsome necessary auxiliary modules, such as patch embed-\nding, global average pooling, and a linear classiﬁer. Fig.\nModel SPACH-XXS SPACH-XS SPACH-S\nConv C = 384, R= 2.0, N= 12 C = 384, R= 2.0, N= 24 C = 512, R= 3.0, N= 24\nTransformer C = 192, R= 2.0, N= 12 C = 384, R= 2.0, N= 12 C = 512, R= 3.0, N= 12\nMLP C = 384, R= 2.0, N= 12 C = 384, R= 2.0, N= 24 C = 512, R= 3.0, N= 24\nConv-MS C = 64, R= 2.0\nNs = {2, 2, 6, 2}\nC = 96, R= 2.0\nNs = {3, 4, 12, 3}\nC = 128, R= 3.0\nNs = {3, 4, 12, 3}\nTransformer-MS C = 32, R= 2.0\nNs = {2, 2, 6, 2}\nC = 64, R= 2.0\nNs = {3, 4, 12, 3}\nC = 96, R= 3.0\nNs = {3, 4, 12, 3}\nMLP-MS C = 64, R= 2.0\nNs = {2, 2, 6, 2}\nC = 96, R= 2.0\nNs = {3, 4, 12, 3}\nC = 128, R= 3.0\nNs = {3, 4, 12, 3}\nTable 1. SPACH and SPACH-MS model variants. C: feature dimension, R: expansion ratio of MLP in Fc, N: number of mixing blocks\nof SPACH,Ns: number of mixing blocks in the ith stage of SPACH-MS.\n1(b) shows the details of the mixing block. Note that the\nspatial mixing and channel mixing are performed in consec-\nutive steps. The name SPACH for our framework is coined\nto emphasize the serial structure of SPAtial and CHannel\nprocessing.\nWe also enable a multi-stage variation, referred to as\nSPACH-MS, as shown in Fig. 1(c). Multi-stage is an impor-\ntant mechanism in CNN-based networks to improve the per-\nformance. Unlike the single-stage SPACH, which processes\nthe image in a low resolution by down-sampling the image\nby a large factor at the input, SPACH-MS is designed to\nkeep a high-resolution in the initial stage of the framework\nand progressively perform down-sampling. Speciﬁcally,\nour SPACH-MS contains four stages with down-sample ra-\ntios of 4, 8, 16, and 32, respectively. Each stage containsNs\nmixing blocks, where s is the stage index. Due to the ex-\ntremely high computational cost of Transformer and MLP\non high-resolution feature maps, we implement the mixing\nblocks in the ﬁrst stage with convolutions only. The fea-\nture dimension within a stage remains constant, and will be\nmultiplied with a factor of 2 after down-sampling.\nLet I ∈ R3×h×w denotes an input image, where 3 is\nthe RGB channels and H ×W is the spatial dimensions.\nOur SPACH framework ﬁrst passes the input image through\na Patch Embedding layer, which is the same as the one in\nViT, to convert I into patch embeddings Xp ∈RC×h\np ×w\np .\nHere p denotes patch size, which is 16 in the single-stage\nimplementation and 4 in the multi-stage implementation.\nAfter the cascades of mixing blocks, a classiﬁcation head\nimplemented by a linear layer is used for the supervised pre-\ntraining.\nWe list the hyper-parameters used in different model\nconﬁgurations in Table 1. Three model size for each\nvariations of SPACH are designed, namely SPACH-XXS,\nSPACH-XS and SPACH-S, by controlling the number of\nblocks, the number of channels, and the expansion ratio of\nConvolution\nMulti-head \nAttention\nLayer Norm.\nMLP\nLayer Norm.\nP.E.\nP.E.\n(a) Convolution (b) Transformer (c) MLP\nFigure 2. Three implementations of the spatial mixing module us-\ning convolution, Transformer, and MLP, respectively. P.E. denotes\npositional encoding, implemented by convolution in SPACH.\nchannel mixing MLP Fc. The model size, theoretical com-\nputational complexity (FLOPS), and empirical throughput\nare presented in Section 4. We measure the throughput us-\ning one P100 GPU.\n3.2. Mixing Block Design\nMixing blocks are key components in the SPACH frame-\nwork. As shown in Fig. 1(b), for an input feature X ∈\nRC×H×W , where C and H ×W denote channel and spatial\ndimensions, it is ﬁrst processed by a spatial mixing function\nFs and then by a channel mixing function Fc. Fs focuses\non aggregating context information from different spatial\nlocations while Fc focuses on channel information fusion.\nDenoting the output as Y , we can formulate a mixing block\nas:\nY = Fs(Fc(X)). (1)\nFollowing ViT [9], we use an MLP with appropriate nor-\nmalization and residual connection to implement Fc. The\nMLP here can be also viewed as a 1x1 convolution (also\nProperties Convolution Self-Attention MLP\nSparse\nConnectivity ✓\nDynamic\nWeight ✓\nGlobal\nReceptive Field ✓ ✓\nTable 2. Three desired properties in network design are seen in\ndifferent network structures.\nknown as point-wise convolution [2]) which is a special\ncase of regular convolution. Note that Fc only performs\nchannel fusion and does not explore any spatial context.\nThe spatial mixing function Fs is the key to implement\ndifferent architectures. As shown in Fig. 2, we imple-\nment three structures using convolution, self-attention, and\nMLP. The common components include normalization and\nresidual connection. Speciﬁcally, the convolution structure\nis implemented by a 3x3 depth-wise convolution, as chan-\nnel mixing will be handled separately in subsequent steps.\nFor the Transformer structure, there is a positional embed-\nding module in the original design. But recent research sug-\ngests that absolute positional embedding breaks translation\nvariance, which is not suitable for images. In view of this\nand inspired by recent vision transformer design [4,39], we\nintroduce a convolutional positional encoding (CPE) as a\nbypass in each spatial mixing module. The CPE module\nhas negligible parameters and FLOPs. For MLP-based net-\nwork, the pioneering work MLP-Mixer does not use any\npositional embedding, but we empirically ﬁnd that adding\nthe very lightweight CPE signiﬁcantly improves the model\nperformance, so we use the same treatment for MLP as for\nTransformer.\nThe three implementations of Fs have distinctive prop-\nerties as listed in Table 2. First, the convolution structure\nonly involves local connections so that it is computational\nefﬁcient. Second, the self-attention structure uses dynamic\nweight for each input instance so that model capacity is in-\ncreased. Moreover, it has a global receptive ﬁeld, which\nenables information to ﬂow freely across different positions\n[37]. Third, MLP structure has a global receptive ﬁeld just\nas the self-attention structure, but it does not use dynamic\nweight. In summary, these three properties seen in different\narchitectures are all desirable and may have positive inﬂu-\nence on the model performance or efﬁciency. We can ﬁnd\nconvolution and self-attention have complementary proper-\nties thus there is potential to build hybrid model to combine\nall desirable properties. Besides, MLP structure seems to\nbe inferior to self-attention in this analysis.\n0 10 20 30 4064\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n#params (M)\nImageNet Top-1 Acc (%)\nConvTransMLPConv-MSTrans-MSMLP-MS\n2004006008001,0001,2001,4001,60064\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\nThroughput (image/s)\nImageNet Top-1 Acc (%)\nConvTransMLPConv-MSTrans-MSMLP-MS\nFigure 3. The multi-stage models (named with -MS sufﬁx) always\nachieve a better performance than their single-stage counterparts.\n4. Empirical Studies on Mixing Blocks\nIn this section, we design a series of controlled exper-\niments to compare the three network structures. We ﬁrst\nintroduce the experimental settings in Section 4.1, and then\npresent our main ﬁndings in Section 4.2, 4.3, 4.4, and 4.5.\n4.1. Datasets and Training Pipelines\nWe conduct experiments on ImageNet-1K (IN-1K) [7]\nimage classiﬁcation which has 1k classes. The training set\nhas 1.28M images while the validation set has 50k images.\nThe Top-1 accuracy on a single crop is reported. Unless oth-\nerwise indicated, we use the input resolution of 224x224.\nMost of our training settings are inherited from DeiT [35].\nWe employ an AdamW [22] optimizer for 300 epochs with\na cosine decay learning rate scheduler and 20 epochs of\nlinear warm-up. The weight decay is 0.05, and the ini-\ntial learning rate is 0.005 ×batchsize\n512 . 8 GPUs with mini-\nbatch 128 per GPU are used in training, resulting a total\nbatch-size of 1024. We use exactly the same data augmen-\ntation and regularization conﬁgurations as DeiT, including\nRand-Augment [5], random erasing [42], Mixup [41], Cut-\nMix [40], stochastic depth [16], and repeated augmenta-\ntion [1, 13]. We use the same training pipeline for all com-\nparing models. And the implementation is built upon Py-\nTorch [24] and timm library [38].\n4.2. Multi-Stage is Superior to Single-Stage\nMulti-stage design is standard in CNN models, but it\nis largely overlooked in Transformer-based or MLP-based\nmodels. Our ﬁrst ﬁnding is that multi-stage design should\nalways be adopted in vision models no matter which of the\nthree network structures is chosen.\nTable 3 compares the image classiﬁcation performance\nNetwork Scale Model\nSingle-Stage Multi-Stage\n#param. FLOPs Throughput IN-1K #param. FLOPs Throughput IN-1K\n(image/s) Top-1 acc. (image/s) Top-1 acc.\nXXS\nConv 8M 1.4G 1513 72.1 5M 0.7G (-0.7G) 1576 73.3 (+1.2)\nTrans 4M 0.9G 1202 68.0 2M 0.5G (-0.4G) 1558 65.4 (-2.6)\nMLP 9M 1.8G 980 74.1 6M 0.9G (-0.9G) 1202 74.9 (+0.8)\nXS\nConv 15M 2.8G 770 77.3 17M 2.8G (-0.0G) 602 80.1 (+2.8)\nTrans 15M 3.1G 548 78.4 14M 3.1G (-0.0G) 441 80.1 (+1.7)\nMLP 17M 3.5G 503 78.5 19M 3.4G (-0.1G) 438 80.7 (+2.2)\nS\nConv 39M 7.4G 374 80.1 44M 7.2G (-0.2G) 328 81.6 (+1.5)\nTrans 33M 6.7G 328 81.7 40M 7.6G (+0.9G) 246 82.9 (+1.2)\nMLP 41M 8.7G 272 78.6 46M 8.2G (-0.5G) 254 82.1 (+3.5)\nTable 3. Model performance of SPACH and SPACH-MS at three network scales.\nModel #param. FLOPs throughput IN-1K\n(image/s) Top-1 acc.\nTrans-MS-S 40M 7.6G 246 82.9\nTrans-MS-S− 40M 7.6G 259 82.3\nMLP-MS-S 46M 8.2G 254 82.1\nMLP-MS-S− 46M 8.2G 274 80.1\nTable 4. Both Transformer structure and MLP structure beneﬁt\nfrom local modeling at a very small computational cost. The su-\nperscription - indicates without local modeling.\nbetween multi-stage framework and single-stage frame-\nwork. For all three network scales and all three network\nstructures, multi-stage framework consistently achieves bet-\nter complexity-accuracy trade-off. For the sake of easy\ncomparison, the changes of FLOPs and accuracy are high-\nlighted in Table 3. Most of the multi-stage models are de-\nsigned to have slightly fewer computational costs, but they\nmanage to achieve a higher accuracy than the correspond-\ning single-stage models. An accuracy loss of 2.6 points is\nobserved for the Transformer model at the XXS scale, but it\nis understandable as the multi-stage model happens to have\nonly half of the parameters and FLOPs of the corresponding\nsingle-stage model.\nIn addition, Fig. 3 shows how the image classiﬁcation\naccuracy changes with the size of model parameters and\nmodel throughput. Despite the different trends observed for\ndifferent network structures, the multi-stage models always\noutperform their single-stage counterparts.\nThis ﬁnding is consistent with the results reported in\nrecent work. Both Swin-Transformer [21] and TWins [3]\nadopt multi-stage framework and achieve a stronger perfor-\nmance than the single-stage framework DeiT [35]. Our em-\npirical study suggests that the use of multi-stage framework\ncan be an important reason.\n4.3. Local Modeling is Crucial\nAlthough it has been pointed out in many previous work\n[4,6,19,21,39] that local modeling is crucial for vision mod-\nels, we will show in this subsection how amazingly efﬁcient\nlocal modeling could be.\nIn our empirical study, the spatial mixing block of the\nconvolution structure is implemented by a3 ×3 depth-wise\nconvolution, which is a typical local modeling operation.\nIt is so light-weight that it only contributes to 0.3% of the\nmodel parameter and 0.5% of the FLOPs. However, as Ta-\nble 3 and Fig. 3 show, this structure can achieve competitive\nperformance when compared with the Transformer struc-\nture in the XXS and XS conﬁgurations.\nIt is due to the sheer efﬁciency of3×3 depth-wise convo-\nlution that we propose to use it as a bypass in both MLP and\nTransformer structures. The increase of model parameters\nand inference FLOPs is almost negligible, but the locality of\nthe models is greatly strengthened. In order to demonstrate\nhow local modeling helps the performance of Transformer\nand MLP structures, we carry out an ablation study which\nremoves this convolution bypass in the two structures.\nTable 4 shows the performance comparison between\nmodels with or without local modeling. The two models\nwe pick are the top performers in Table 3 when multi-stage\nframework is used and network scale is S. We can clearly\nﬁnd that the convolution bypass only slightly decreases the\nthroughput, but brings a notable accuracy increase to both\nmodels. Note that the convolution bypass is treated as\nconvolutional positional embedding in Trans-MS-S, so we\nbring back the standard patch embedding as in ViT [9] in\nTrans-MS-S−. For MLP-MS-S−, we follow the practice in\nMLP-Mixer and do not use any positional embedding. This\nexperiment conﬁrms the importance of local modeling and\nsuggests the use of3×3 depth-wise convolution as a bypass\nfor any designed network structures.\nModel #param. FLOPs throughput IN-1K\n(image/s) Top-1 acc.\nMLP-S 41M 8.7G 272 78.6\n+Shared 39M 8.7G 274 80.2\nMLP-MS-S 46M 8.2G 254 82.1\n+Shared 45M 8.2G 244 82.5\nTable 5. The performance of MLP models are greatly boosted\nwhen weight sharing is adopted to alleviate over-ﬁtting.\n4.4. A Detailed Analysis of MLP\nDue to the excessive number of parameters, MLP mod-\nels suffer severely from over-ﬁtting. We believe that over-\nﬁtting is the main obstacle for MLP to achieve SOTA per-\nformance. In this part, we discuss two mechanisms which\ncan potentially alleviate this problem.\nOne is the use of multi-stage framework. We have al-\nready shown in Table 3 that multi-stage framework brings\ngain. Such gain is even more prominent for larger MLP\nmodels. In particular, the MLP-MS-S models achieves 2.6\naccuracy gain over the single-stage model MLP-S. We be-\nlieve this owes to the strong generalization capability of the\nmulti-stage framework. Fig. 4 shows how the test accu-\nracy increases with the decrease of training loss. Over-\nﬁtting can be observed when the test accuracy starts to ﬂat-\nten. These results also lead to a very promising baseline for\nMLP-based models. Without bells and whistles, MLP-MS-\nS model achieves 82.1% ImageNet Top-1 accuracy, which\nis 5.7 points higher than the best results reported by MLP-\nMixer [33] when ImageNet-1K is used as training data.\nThe other mechanism is parameter reduction through\nweight sharing. We apply weight-sharing on the spatial\nmixing function Fs. For the single-stage model, all N mix-\ning blocks use the sameFs, while for the multi-stage model,\neach stage use the same same Fs for its Ns mixing blocks.\nWe present the results of S models in Table 5. We can\nﬁnd that the shared-weight variants, denoted by ”+Shared”,\nachieve higher accuracy with almost the same model size\nand computation cost. Although they are still inferior to\nTransformer models, the performance is on par with or even\nbetter than convolution models. Fig. 4 conﬁrms that using\nshared weights in the MLP-MS model further delays the ap-\npearance of over-ﬁtting signs. Therefore, we conclude that\nMLP-based models remain competitive if they could solve\nor alleviate the over-ﬁtting problem.\n4.5. Convolution and Transformer are Complemen-\ntary\nWe ﬁnd that convolution and Transformer are comple-\nmentary in the sense that convolution structure has the best\ngeneralization capability while Transformer structure has\nthe largest model capacity among the three structures we\n2.6 2.8 3 3.2 3.4 3.6\n74\n76\n78\n80\n82\nTrain loss\nImageNet Top-1 Acc (%)\nMLPMLP-MSMLP-MS-Shared\nFigure 4. Illustration of the over-ﬁtting problem in MLP-based\nmodels. Both multi-stage framework and weight sharing alleviate\nthe problem.\n2.5 3 3.5 4\n65\n70\n75\n80\nTrain loss\nImageNet Top-1 Acc (%)\nConv-MSTrans-MS\nFigure 5. Conv-MS has a better generalization capability than\nTrans-MS as it achieves a higher test accuracy at the same training\nloss before the model saturates.\ninvestigated.\nFig. 5 shows that, before the performance of Conv-MS\nsaturates, it achieves a higher test accuracy than Trans-MS\nat the same training loss. This shows that convolution mod-\nels generalize better than Transformer models. In particu-\nlar, when the training loss is relatively large, the convolution\nmodels show great superiority against Transformer models.\nThis suggests that convolution is still the best choice in de-\nsigning lightweight vision models.\nOn the other hand, both Fig. 3 and Fig. 5 show that\nTransformer models achieve higher accuracy than the other\ntwo structures when we increase the model size and allow\nfor higher computational cost. Recall that we have dis-\ncussed three properties of network architectures in Section\n3.2. It is now clear that the sparse connectivity helps to in-\ncrease generalization capability, while dynamic weight and\nglobal receptive ﬁeld help to increase model capacity.\n5. Hybrid Models\nAs discussed in Section 3.2 and 4.4, convolution and\nTransformer structures have complementary characteristics\nand have potential to be used in a single model. Based on\nthis observation, we construct hybrid models at the XS and\nS scales based on these two structures. The procedure we\nused to construct hybrid models is rather simple. We take\na multi-stage convolution-based model as the base model,\nand replace some selected layers with Transformer layers.\nConsidering the local modeling capability of convolutions\nand global modeling capability of Transformers, we tend to\ndo such replacement in later stages of the model. The de-\ntails of layer selection in the two hybrid models are listed as\nfollows.\n• Hybrid-MS-XS: It is based on Conv-MS-XS. The last\nten layers in Stage 3 and the last two layers in Stage\n4 are replaced by Transformer layers. Stage 1 and 2\nremain unchanged.\n• Hybrid-MS-S: It is based on Conv-MS-S. The last two\nlayers in Stage 2, the last ten layers in Stage 3, and the\nlast two layers in Stage 4 are replaced by Transformer\nlayers. Stage 1 remains unchanged.\nIn order to unleash the full potential of hybrid models,\nwe further adopt the deep patch embedding layer (PEL) im-\nplementation as suggested in LV-ViT [17]. Different from\ndefault PEL which uses one large (16x16) convolution ker-\nnel, the deep PEL uses four convolution kernels with ker-\nnel size {7, 3, 3, 2}, stride {2, 1, 1, 2}, and channel number\n{64, 64, 64, C}. By using small kernel sizes and more con-\nvolution kernels, deep PEL helps a vision model to explore\nthe locality inside single patch embedding vector. We mark\nmodels with deep PEL as ”Hybrid-MS-*+”.\nTable 6 shows comparison between our hybrid models\nand some of the state-of-the-art models based on CNN,\nTransformer, or MLP. All listed models are trained on\nImageNet-1K. Within the section of our models, we can ﬁnd\nthat hybrid models achieve better model size-performance\ntrade-off than pure convolution models or Transformer\nmodels. The Hybrid-MS-XS achieves 82.4% top-1 accu-\nracy with 28M parameters, which is higher than Conv-MS-\nS with 44M parameters and only a little lower than Trans-\nMS-S with 40M parameters. In addition, the Hybrid-MS-S\nachieve 83.7% top-1 accuracy with 63M parameters, which\nhas 0.8 point gain compared with Trans-MS-S.\nThe Hybrid-MS-S+ model we proposed achieves 83.9%\ntop-1 accuracy with 63M parameters. This number is higher\nthan the accuracy achieved by SOTA models Swin-B and\nCaiT-S36, which have model size of 88M and 68.2M, re-\nspectively. The FLOPs of our model is also fewer than\nthese two models. We believe Hybrid-MS-S can be serve\nas a strong yet simple baseline for future research on archi-\ntecture design of vision models.\n6. Conclusion\nThe objective of this work is to understand how the\nemerging Transformer and MLP structures compare with\nCNNs in the computer vision domain. We ﬁrst built a\nsimple and uniﬁed framework, called SPACH, that could\nModel #param. FLOPs IN-1K\nTop-1 acc.\nCNN\nRegNetY-4G [25] 21M 4.1G 80.0\nRegNetY-8G [25] 39M 8.0G 81.7\nRegNetY-16G [25] 84M 16.0G 82.9\nTransformer\nViT-B/16* [9] 86M - 77.9\nDeiT-S [35] 22M 4.6G 79.8\nDeiT-B [35] 86M 17.5G 81.8\nSwin-T [21] 29M 4.5G 81.3\nSwin-S [21] 50M 8.7G 83.0\nSwin-B [21] 88M 15.4G 83.5\nCaiT-XS24 [36] 26.6M 5.4G 81.8\nCaiT-S36 [36] 68.2M 13.9G 83.3\nCvT-13 [39] 20M 4.5G 81.6\nCvT-21 [39] 32M 7.1G 82.5\nMLP\nFF-Base [23] 62M - 74.9\nMixer-B/16 [33] 79M - 76.4\nResMLP-S24 [34] 30M 6.0G 79.4\nResMLP-B24 [34] 45M 23.0G 81.0\ngMLP-S [20] 20M 4.5G 79.4\ngMLP-B [20] 73M 15.8G 81.6\nOurs\nConv-MS-XS 17M 2.8G 80.1\nConv-MS-S 44M 7.2G 81.6\nTrans-MS-XS 14M 3.1G 80.1\nTrans-MS-S 40M 7.6G 82.9\nHybrid-MS-XS 28M 4.5G 82.4\nHybrid-MS-XS+ 28M 5.6G 82.8\nHybrid-MS-S 63M 11.2G 83.7\nHybrid-MS-S+ 63M 12.3G 83.9\nTable 6. Comparison of different models on ImageNet-1K clas-\nsiﬁcation. Compared models are grouped according to network\nstructures, and our models are listed in the last, Most models are\npre-trained with 224x224 images, except ViT-B/16*, which uses\n384x384 images.\nuse CNN, Transformer, or MLP as plug-and-play compo-\nnents. Under the SPACH framework, we discover with\na little surprise that all three network structures are simi-\nlarly competitive in terms of the accuracy-complexity trade-\noff, although they show distinctive properties when the net-\nwork scales up. In addition to the analysis of speciﬁc net-\nwork structures, we also investigate two important design\nchoices, namely multi-stage framework and local modeling,\nwhich are largely overlooked in previous work. Finally, in-\nspired by the analysis, we propose two hybrid models which\nachieve SOTA performance on ImageNet-1k classiﬁcation\nwithout bells and whistles.\nOur work also raises several questions worth exploring.\nFirst, realizing the fact that the performance of MLP-based\nmodels is largely affected by over-ﬁtting, is it possible to\ndesign a high-performing MLP model that is not subject to\nover-ﬁtting? Second, current analyses suggest that neither\nconvolution nor Transformer is the optimal structure across\nall model sizes. What is the best way to fuse these two\nstructures? Last but not least, do better visual models exist\nbeyond the known structures including CNN, Transformer,\nand MLP?\nReferences\n[1] Maxim Berman, Herv ´e J ´egou, Andrea Vedaldi, Iasonas\nKokkinos, and Matthijs Douze. Multigrain: a uni-\nﬁed image embedding for classes and instances. CoRR,\nabs/1902.05509, 2019. 5\n[2] Franc ¸ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, pages 1800–1807. IEEE\nComputer Society, 2017. 2, 5\n[3] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting spatial attention design in vision trans-\nformers. CoRR, abs/2104.13840, 2021. 2, 6\n[4] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-\nsitional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021. 3, 5, 6\n[5] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le.\nRandaugment: Practical automated data augmentation with\na reduced search space. In NeurIPS, 2020. 5\n[6] St ´ephane d’Ascoli, Hugo Touvron, Matthew L. Leavitt,\nAri S. Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional induc-\ntive biases. In ICML, volume 139 ofProceedings of Machine\nLearning Research, pages 2286–2296. PMLR, 2021. 6\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Fei-Fei Li. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248–255. IEEE Computer Society,\n2009. 1, 5\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT (1),\npages 4171–4186. Association for Computational Linguis-\ntics, 2019. 1, 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR. OpenReview.net, 2021. 1, 2, 3, 4, 6, 8\n[10] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,\nPierre Stock, Armand Joulin, Herv ´e J ´egou, and Matthijs\nDouze. Levit: a vision transformer in convnet’s clothing for\nfaster inference. CoRR, abs/2104.01136, 2021. 3\n[11] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\nGirshick. Mask R-CNN. In ICCV, pages 2980–2988. IEEE\nComputer Society, 2017. 1\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770–778. IEEE Computer Society, 2016. 1, 2\n[13] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoeﬂer, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In CVPR, pages\n8126–8135. IEEE, 2020. 5\n[14] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\ntional neural networks for mobile vision applications.CoRR,\nabs/1704.04861, 2017. 2\n[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In CVPR, pages 7132–7141. IEEE Computer Society,\n2018. 2\n[16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q. Weinberger. Deep networks with stochastic depth. In\nECCV (4), volume 9908 of Lecture Notes in Computer Sci-\nence, pages 646–661. Springer, 2016. 5\n[17] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi,\nXiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter:\nToken labeling for training better vision transformers. arXiv\npreprint arXiv:2104.10858, 2021. 3, 8\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In NIPS, pages 1106–1114, 2012. 2\n[19] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and\nLuc Van Gool. Localvit: Bringing locality to vision trans-\nformers. CoRR, abs/2104.05707, 2021. 6\n[20] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V . Le. Pay\nattention to mlps. CoRR, abs/2105.08050, 2021. 1, 3, 8\n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. CoRR, abs/2103.14030, 2021. 2, 6, 8\n[22] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. CoRR, abs/1711.05101, 2017. 5\n[23] Luke Melas-Kyriazi. Do you even need attention? A stack\nof feed-forward layers does surprisingly well on imagenet.\nCoRR, abs/2105.02723, 2021. 3, 8\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K ¨opf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS, pages 8024–8035, 2019. 5\n[25] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick,\nKaiming He, and Piotr Doll ´ar. Designing network design\nspaces. In CVPR, pages 10425–10433. IEEE, 2020. 8\n[26] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and\nJie Zhou. Global ﬁlter networks for image classiﬁcation.\nCoRR, abs/2107.00645, 2021. 1\n[27] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. IEEE Trans. Pattern Anal. Mach.\nIntell., 39(6):1137–1149, 2017. 1\n[28] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey\nZhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. InCVPR, pages 4510–4520.\nIEEE Computer Society, 2018. 2\n[29] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015. 2\n[30] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, pages 843–852. IEEE Computer\nSociety, 2017. 3\n[31] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML,\nvolume 97 of Proceedings of Machine Learning Research ,\npages 6105–6114. PMLR, 2019. 2\n[32] Mingxing Tan and Quoc V . Le. Efﬁcientnetv2: Smaller mod-\nels and faster training. In ICML, volume 139 of Proceedings\nof Machine Learning Research, pages 10096–10106. PMLR,\n2021. 2\n[33] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario\nLucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-\nchitecture for vision. CoRR, abs/2105.01601, 2021. 1, 2, 3,\n7, 8\n[34] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu\nCord, Alaaeldin El-Nouby, Edouard Grave, Armand Joulin,\nGabriel Synnaeve, Jakob Verbeek, and Herv ´e J ´egou.\nResmlp: Feedforward networks for image classiﬁcation with\ndata-efﬁcient training. CoRR, abs/2105.03404, 2021. 3, 8\n[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In ICML, volume 139 of Proceedings of Machine\nLearning Research, pages 10347–10357. PMLR, 2021. 1, 2,\n5, 6, 8\n[36] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. CoRR, abs/2103.17239, 2021. 3, 8\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, pages 5998–\n6008, 2017. 1, 2, 5\n[38] Ross Wightman. Pytorch image models. https :\n/ / github . com / rwightman / pytorch - image -\nmodels, 2019. 5\n[39] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing\nconvolutions to vision transformers. CoRR, abs/2103.15808,\n2021. 3, 5, 6, 8\n[40] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon\nOh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regulariza-\ntion strategy to train strong classiﬁers with localizable fea-\ntures. In ICCV, pages 6022–6031. IEEE, 2019. 5\n[41] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In ICLR (Poster). OpenReview.net, 2018. 5\n[42] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InAAAI, pages\n13001–13008. AAAI Press, 2020. 5"
}