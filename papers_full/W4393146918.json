{
  "title": "MindMap: Constructing Evidence Chains for Multi-Step Reasoning in Large Language Models",
  "url": "https://openalex.org/W4393146918",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2430690770",
      "name": "Yangyu Wu",
      "affiliations": [
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2047664486",
      "name": "Wei Song",
      "affiliations": [
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097327183",
      "name": "Miaomiao Cheng",
      "affiliations": [
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097019018",
      "name": "Fei Li",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2430690770",
      "name": "Yangyu Wu",
      "affiliations": [
        "Beijing Normal University",
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Beijing Normal University",
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2047664486",
      "name": "Wei Song",
      "affiliations": [
        "Beijing Normal University",
        "Capital Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097327183",
      "name": "Miaomiao Cheng",
      "affiliations": [
        "Capital Normal University",
        "Beijing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097019018",
      "name": "Fei Li",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6783166674",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W6773518419",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4385571886",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W6851024552",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W4308244210",
    "https://openalex.org/W6850668563",
    "https://openalex.org/W6851960618",
    "https://openalex.org/W6796301005",
    "https://openalex.org/W3115201256",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W6838311432",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W3084470717",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4292166681",
    "https://openalex.org/W4386080721",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W4294753225",
    "https://openalex.org/W4385767671"
  ],
  "abstract": "Large language models (LLMs) have demonstrated remarkable performance in various natural language processing tasks. However, they still face significant challenges in automated reasoning, particularly in scenarios involving multi-step reasoning. In this paper, we focus on the logical reasoning problem. The main task is to answer a question based on a set of available facts and rules. A lot of work has focused on guiding LLMs to think logically by generating reasoning paths, ignoring the structure among available facts. In this paper, we propose a simple approach MindMap by introducing evidence chains for supporting reasoning. An evidence chain refers to a set of facts that involve the same subject. In this way, we can organize related facts together to avoid missing important information. MindMap can be integrated with existing reasoning framework, such as Chain-of-Thought (CoT) and Selection-Inference (SI), by letting the model select relevant evidence chains instead of independent facts. The experimental results on the bAbI and ProofWriterOWA datasets demonstrate the effectiveness of MindMap.It can significantly improve CoT and SI, especially in multi-step reasoning tasks.",
  "full_text": "MindMap: Constructing Evidence Chains for Multi-Step Reasoning in Large\nLanguage Models\nYangyu Wu1, Xu Han1*, Wei Song1, Miaomiao Cheng1, Fei Li2\n1Beijing Key Laboratory of Electronic System Reliability Technology,\nCollege of Information Engineering, Capital Normal University, China\n2School of Cyber Science and Engineering, Wuhan University, China\n{wu, hanxu, wsong, miaomiao}@cnu.edu.cn\nlifei csnlp@whu.edu.cn\nAbstract\nLarge language models (LLMs) have demonstrated remark-\nable performance across a range of natural language process-\ning (NLP) tasks. However, they encounter significant chal-\nlenges in automated reasoning, especially in multi-step rea-\nsoning scenarios. In order to solve complex reasoning prob-\nlems, LLMs need to perform faithful multi-step reasoning\nbased on a given set of facts and rules. A lot of work has\nfocused on guiding LLMs to think logically by generating\nreasoning paths, but ignores the relationship among available\nfacts. In this paper, we introduce MindMap, a straightfor-\nward yet powerful approach for constructing evidence chains\nto support reasoning in LLMs. An evidence chain refers to\na set of facts that are associated with the same subject. In\nthis way, we can organize related facts together to avoid\nmissing relevant information. MindMap can seamlessly inte-\ngrate with existing reasoning frameworks, such as Chain-of-\nThought (CoT) and Selection-Inference (SI), by enabling the\nmodel to generate and select relevant evidence chains from\nindependent facts. The experimental results on the bAbI and\nProofWriterOW A datasets demonstrate the effectiveness of\nMindMap. Our approach can significantly enhance the per-\nformance of CoT and SI, particularly in multi-step reasoning\ntasks.\nIntroduction\nThe pursuit of general artificial intelligence has remained a\ncentral objective within the realm of artificial intelligence\nresearch (Silver et al. 2021; Goertzel and Pennachin 2007).\nRecent years have witnessed remarkable advancements in\nNatural Language Processing (NLP), largely attributing to\nthe emergence of large language models (LLMs) (Ouyang\net al. 2022). These models have exhibited exceptional ef-\nficacy across diverse tasks including machine translation,\nquestion answering, and reading comprehension (Yang et al.\n2023; Bang et al. 2023). The strategic expansion of lan-\nguage model scale yields tangible improvements across vari-\nous problem domains, with task performance exhibiting pos-\nitive correlations with model size (Creswell, Shanahan, and\nHiggins 2023). However, a study pointed that the benefits of\nscaling up are significantly reduced when dealing with com-\nplex problems (Rae et al. 2021). Particularly, the enhanced\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFacts:\nt=0 Mary took the football there.\nt=1 John travelled to the office.\nt=2 Mary travelled to the hallway.\nt=3 Daniel moved to the hallway.\nt=4 Daniel went to the office.\nt=5 John went to the hallway.\nt=6 John discarded the apple.\nt=7 Mary went back to the garden.\nt=8 Daniel went to the bedroom.\nQuestion:\nWhere was the football before the \ngarden?\nGround Truth:\nhallway\nChains:\nchain-0: Mary passed through \nthe hallway(t=2), garden(t=7) in \nsequence with football.\nchain-1: John passed through \nthe office(t=1), hallway(t=5) in \nsequence with apple.\nchain-2: Daniel passed through \nthe hallway(t=3), office(t=4), \nbedroom(t=8) in sequence.\nFact 0 Fact 1 Fact 2\nFact 3 Fact 4 Fact 5\nFact 6 Fact 7 Fact 8\nEvidence Chain Construction\nFact 0\nFact 2\nFact 7\nFact 1\nFact 5\nFact 6\nFact 3\nFact 4\nFact 8\nEvidence Chain Summarization\nChain 0 Chain 1 Chain 2\nEvidence Chain Utilization\nReasoning \nframework\n(CoT,SI,...)\nChain 0\nQuestion Select\n  Answer: hallway \nMary John Daniel\nFigure 1: Illustrating the working flow of the proposed\nMindMap approach based on an example from the bAbI\ndataset.\nadvantages of these models in tasks that involve sophisti-\ncated logical reasoning are less evident compared to other\ntasks (Wei et al. 2022).\nLogical reasoning is essential for advancing various sci-\nentific fields (Liu et al. 2020). It involves deducing new con-\nclusions from existing facts and rules. For instance, with\nfacts like “David picked up the apple” and “David went to\nthe bedroom,”, deducing the apple’s location being in the\nbedroom is a logical process. Such reasoning challenges\noften require multiple steps to be executed effectively to\ncomplete the reasoning process (Saparov and He 2023).\nAlthough LLMs shows good ability in learning from in-\nstructions and demonstrations in context to answer ques-\ntions (Brown et al. 2020; Dong et al. 2023; Min et al.\n2022), they struggle with complex logical reasoning, espe-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19270\ncially multi-step reasoning (Wei et al. 2022; Liu et al. 2023;\nKazemi et al. 2023).\nRecent approaches have focused on guiding LLMs to\nthink step-by-step to improve the performance in logical rea-\nsoning. For example, Chain of Thought (CoT) (Wei et al.\n2022) and Selection-Inference (SI) (Creswell, Shanahan,\nand Higgins 2023) frameworks try to construct reasoning\npaths or formulate reason procedures and obtain large im-\nprovements in many reasoning tasks.\nHowever, these methods general treat each individual fact\nas an isolated evidence, overlooking the inherent intercon-\nnections among these pieces of evidence. In this paper, we\nmake a focused contribution in organizing available facts for\nsupporting reasoning. When we deal with a specific prob-\nlem, only part of the available facts are relevant, while oth-\ners may be even noise. Therefore, it is important to group\nrelated facts together to prompt us to think more compre-\nhensively and deeply. Motivated by the procedure of man-\naging chains of custody in disclosure of crimes, we propose\nthe MindMap, which aims to construct evidence chains for\nsupporting logical reasoning.\nFigure 1 shows the workflow of the proposed MindMap\nfor responding to a question from the bAbI dataset. The\nframework consists of 3 modules: evidence chain construc-\ntion, evidence chain summarization and evidence chain uti-\nlization for reasoning. Specifically, an evidence chain is de-\nfined as a group of facts associated with the same subject,\nsuch as a series of events involving a person. Motivated by\nthe work of narrative event chains (Chambers and Jurafsky\n2008), we extract subjects from facts using NLP tools to\nform a subject set and construct an evidence chain for ev-\nery subject. To obtain more concise and coherent informa-\ntion, the evidence chain summarization module provides a\nsummary that covers the main content and entities in the ev-\nidence chain.\nIn this manner, MindMap utilizes a set of constructed ev-\nidence chains or their summaries, rather than a collection of\nindependent facts, to support further reasoning. MindMap\ncan be integrated into existing reasoning frameworks such as\nCoT and SI. Instead of selecting facts, MindMap enhances\nCoT and SI by selecting and updating evidence chains.\nWe conduct evaluation on the bAbI (Weston et al. 2016)\nand ProofWriterOW A (Tafjord, Dalvi, and Clark 2021)\ndatasets based on a LLM with 13B parameters. The experi-\nmental results show that MindMap can significantly improve\nthe performance of CoT and SI, especially in the multi-step\nreasoning setting. We observe that MindMap can help cover\nmany more supporting facts. Our method is straightforward,\nand its effectiveness underscores the necessity of organizing\navailable facts. It also indicates that the integration of tra-\nditional NLP tools with LLMs has potential in addressing\ncomplex reasoning problems.\nRelated Work\nLarge Language Models\nDue to the continuous advancement of deep learning tech-\nnology and the increase in computing power, remarkable\nprogress has been made in the development of LLMs (Rad-\nford et al. 2019; Chowdhery et al. 2023; Muennighoff et al.\n2023). Notably, GPT4, which was released recently, has\nachieved excellent results in various tasks (Katz et al. 2023;\nPeng et al. 2023; Nori et al. 2023). However, these large\nlanguage models possess numerous parameters and con-\nsume substantial resources, leading to the emergence of\nmany smaller open-source models in response to current de-\nmands. For instance, Llama(13B) (Touvron et al. 2023) has\ndemonstrated superior performance on most benchmarks\nwhen compared to GPT-3(175B) (Brown et al. 2020). Stan-\nford’s Alpaca (Touvron et al. 2023) and Vicuna (Chiang\net al. 2023) models, which are supervised fine-tuned ver-\nsions based on LLaMa, exhibit even stronger dialogue ca-\npabilities. Specifically, Vicuna utilizes GPT-4 for scoring\nand evaluation, boasting 13B parameters, and achieves up to\n90% of ChatGPT’s effectiveness (Chiang et al. 2023). Due to\ncomputation resource constraints, our experiments are con-\nducted based on Vicuna.\nReasoning with LLMs\nAutomated reasoning has been a challenging task in NLP.\nBefore the era of LLMs, the prevalent approaches to log-\nical reasoning were based on fine-tuning pre-trained mod-\nels (Clark, Tafjord, and Richardson 2020; Betz, V oigt, and\nRichardson 2021; Han et al. 2022). However, these methods\noften led to unrealistic inferences due to implicit label-data\ncorrelations (Zhang et al. 2023).\nRecently, LLMs have shown stronger reasoning abilities\ncompared to previous approaches (Dong et al. 2023; Min\net al. 2022). The strength of LLMs lies in their ability to\nautomatically learn from context through in-context learn-\ning (Dai et al. 2023; Min et al. 2022), enabling them to make\naccurate inferences by understanding specific contextual sit-\nuations with just a few examples. Consequently, LLMs be-\ncome more flexible in handling various tasks by only modi-\nfying the contextual hints (Dong et al. 2023).\nHowever, LLMs face significant challenges when it\ncomes to multi-step reasoning tasks (Zhou et al. 2023).\nConsequently, multi-step reasoning has emerged as a key\narea that LLMs need to address. A representative work in\nthis direction is the Chain-of-Thought (CoT) approach (Wei\net al. 2022), which aids the model in making correct infer-\nences by outputting a series of reasoning paths, thereby also\nenhancing the interpretability of the model’s outputs. De-\nspite its advantages, CoT has its limitations, including in-\nstances of incorrect reasoning and a tendency to fabricate\nfacts. Several subsequent improvements have been made to\nCoT, such as the Tree-of-Thought approach (Yao et al. 2023)\nand Graph-of-Thought approach (Besta et al. 2023). Re-\ncently, the Selection-Inference (SI) algorithm has proposed\nintroducing a separation between the inference and selection\nsteps. This allows the model to reason based on the selected\nrelevant facts, generate new conclusions, and iteratively up-\ndate the facts. SI is reported to alleviate the problem of fab-\nricating facts (Creswell, Shanahan, and Higgins 2023).\nHowever, these methods often fail to consider the relation-\nships among available facts, leading to the loss of important\nand relevant information during reasoning. In this paper, we\nintroduce evidence chains to connect related facts and use\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19271\n------------------Evidence Chain Construction--------------------- \nFact 0 Entity\nExtraction &\nDependency\nParsing\nSubjects:\n{Mary，John}\n----Evidence Chain Summarization------\nQuestion: Where is the apple? \nLLM\nChain-0\nchain-0: Mary \ntravelled to the \nhallway(t=0) and took \nthe apple.\nchain-1: John \ntravelled to the \nbedroom(t=1).Chain-1\n-Combining Different Strategy Reasoning --\nLLM\nQuestion-oriented Chain Selection \nchain-0: Mary travelled \nto the hallway(t=0) and \ntook the apple.\nMary:\nhallway \napple \nFact 0: t = 0 Mary travelled to the hallway.\nFact 1: t = 1 John travelled to the bedroom.\nFact 2: t = 2 Mary took the apple there.\nFact 1\nFact 2\nFact 0\nFact 2\nReasoning Framework (COT, SI, …)\nFact 1 Answer: Hallway.\nFigure 2: The main framework of the proposed MindMap approach.\nevidence chains for supporting reasoning. Our method can\nbe integrated with existing reasoning frameworks as a valu-\nable plug-in component.\nMethodology\nOverview\nWe aim to solve natural language reasoning problems\nbased on LLMs. Formally, given a set of facts F =\n{f1, f2, ..., fn}, a set of rules R = {r1, r2, ..., rm} and\na question q, we need to perform reasoning based on F\nand R to figure out an answer a to respond q, i.e., a =\nreasoning(F, R, q), where reasoning represents specific\nreasoning framework. The rule set R can be empty in the\ntask like reading comprehension via question answering.\nWe propose a framework called MindMap. The key idea\nis to introduce the concept of evidence chain to explore the\nstructure within the given facts to support reasoning.\nAn evidence chain is defined as a group of facts, i.e.,\ncs = {fs\n1 , fs\n2 , ..., fs\nn}, where the facts fs\n1 , fs\n2 , ..., fs\nn are all\nassociated with the same subject s. For example, s can be\na person, and cs can be a sequence of events that involve\nthe person s. Suppose there are k subjects S = {s1, ..., sk}\nin the set of facts F. So F can be re-organized as a set of\nevidence chains C = {c1, c2, ..., ck}. Accordingly, the rea-\nsoning task can be represented as a = reasoning(C, R, q).\nFigure 2 illustrates the main framework of MindMap. It\nhas 3 core components: evidence chain construction, evi-\ndence chain summarization, and evidence chain utilization\nfor inference.\nEvidence Chain Construction\nSubject extractionEach evidence chain is associated with\na subject. We first construct a set of subjects. Given a set of\nfacts F, we use the entity extraction and dependency parsing\nmodules in the Stanford CoreNLP toolkit (Manning et al.\n2014) to extract the entities which are subjects in the facts\nand group all of them into a set of subjects S.\nSubject-centric evidence chainAfter extracting the sub-\njects, we build an evidence chain for every subject simply\nby grouping all facts containing a specific subject. If there\nexists temporal information, the facts in an evidence chain\nwill be temporally sorted. Otherwise they would be sorted\nin the order as the original context.\nNotice that a fact may contain multiple entities, so it can\nbe involved in multiple evidence chains.\nEvidence Chain Summarization\nGiven a context, there may be multiple evidence chains, the\nlength of which could be short or long. To obtain a more\nconcise description of an evidence chain, we introduce the\nevidence chain summarization module.\nWe propose an entity-centric summarization manner\nbased on LLMs in a few-shot setting. We use instructions\nto guide a LLM to generate a summary covering main en-\ntities in each evidence chain. Below is an one-shot learning\nexample of the prompt that is used for summarization.\nTASK Instruction: Below are some\nstories about people moving objects\nbetween rooms. The facts are organized\nas evidence chains, each of which\ninvolves the same subject. Please Write\na summary for each chain and cover main\nentities in each chain.\nchain-0:\nat t=0 Daniel went to the kitchen.\nat t=1 Daniel picked up the apple\nthere.\nat t=3 Daniel journeyed to the garden.\nat t=9 Daniel travelled to the office.\nat t=12 Daniel left the apple.\nat t=13 Daniel went back to the\nbedroom.\nEntities about Daniel:\nkitchen,apple,garden,\noffice,apple,bedroom\nSummary:chain-0: Daniel passed through\nthe kitchen(t=0),garden(t=3),\noffice(t=9) in sequence with\napple.Then,he went to bedroom(t=13).\n···\nchain-to-be-summarized:\nat t=2 Sandra travelled to the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19272\nbathroom.\nat t=3 Sandra went to the bedroom.\nat t=8 Sandra journeyed to the office.\nat t=9 Sandra went back the bedroom.\nEntities about Sandra:\nbathroom,bedroom,office, bedroom\nSummary:[Let the model generate]\nIn this way, given an evidence chainc, we can get its sum-\nmary summarize(c).\nEvidence Chain Utilization for Inference\nUtilizing evidence chains to organize facts for supporting\nreasoning changes the reasoning formulation from a =\nreasoning(F, R, q) to a = reasoning(C, R, q) or a =\nreasoning({summarize(c), c∈ C}, R, q). During infer-\nence, the facts are organized as evidence chains rather than\na sequence of facts.\nThe change of the formulation does not affect the reason-\ning framework, indicating that we can integrate MindMap in\nany reasoning framework.\nMindMap enhanced CoT In the CoT setup, besides an-\nswering questions, explanations for the answers are also in-\ncluded to inspire the model to reason.\nMindMap enhances CoT based on evidence chains. The\nkey step is question-oriented evidence chain selectionthat\nwe let LLMs choose evidence chains that can help answer\nthe question. We use a few-demonstrations to guide the\nmodel to learn to integrate proper evidence chains for rea-\nsoning.\nWe show an example below with only one demonstra-\ntion.\nDemonstration 0\nThe evidence chains:\nchain-0: ...\nchain-1: ...\n...\nchain-t: ...\nQuestion-oriented chain selection: the\nquestion q0 can be inferred based on\nthe chain-0 and chain-3. Given the\nsummary that summarize(chain-0) and\nsummarize(chain-3), the answer should\nbe office.\nTest 1\nThe evidence chains:\nchain-0: ...\nchain-1: ...\n...\nQuestion-oriented chain selection: the\nquestion q1 can be inferred based on\n[Let the model generate]\nHere, q0 and q1 are specific questions which are included\nin the prompts to guide the model to select proper evidence\nchains for covering relevant and complete facts for reason-\ning, and filtering out irrelevant facts.\nMindMap enhanced SI We also try to integrate\nMindMap with the selection-inference (SI) reasoning frame-\nwork. SI divides the CoT reasoning framework into two\nparts: selection and inference. The selection module picks\nthe most relevant facts and rules for the given question. The\ninference module feeds the selected facts and rules to the\nLLMs for deriving new conclusions. These two modules it-\nerate, with each iteration adding a new conclusion to the\nevolving reasoning.\nWith MindMap, we select relevant evidence chains in-\nstead of choosing isolated facts in the selection stage. For\nthe newly generated conclusions, we extract subjects from\nthem. This procedure is similar to that of evichence chain\nconstruction. Then the newly generated conclusions are used\nfor updating the evidence chains for supporting later SI iter-\nations.\nExperimental Settings\nIn this section, we will introduce the datasets, the baseline\nmethods, and backbone model settings for evaluation.\nDatasets\nOur experiments are conducted using two challenging multi-\nstep logical reasoning datasets.\n• bAbI (Weston et al. 2016): Originating from the QA\nbAbI task, this dataset comprises a series of 20 tasks de-\nsigned to evaluate reading comprehension via question\nanswering. These tasks gauge understanding in various\ndimensions, including systems’ ability to deduce answers\nthrough reasoning. We conduct experiments using the\ntasks 1-3 of bAbI, where to logically answer a question,\n1-3 facts among a set of facts with temporal information\nare required.\n• ProofWriterOWA (Tafjord, Dalvi, and Clark 2021):\nThis synthetic dataset serves as a common benchmark\nfor assessing logical reasoning, particularly when facts\nand rules are presented in natural language. The dataset\nis divided into subsets based on the steps of infer-\nences, including 0, 1, 2, 3, and 5. Following previous\nwork (Creswell, Shanahan, and Higgins 2023), we di-\nvide this dataset into two parts: ProofWriter-PUD, the\nanswer set of which include True, False, and Unknown,\nand ProofWriter-PD, which excludes data labeled asUn-\nknown.\nRegarding the bAbI dataset, we use the full test set. For\nthe ProofWriterOW A dataset, due to the heavy inference\ncost, we only use the first 1,000 samples in the test set.\nBaseline Settings\nWe use the Vicuna-13B model (Chiang et al. 2023) in a few-\nshot setting as the Standard backbone model. We also con-\nsider two more advanced reasoning frameworks.\n• Chain-of-Thought (CoT) (Wei et al. 2022): We also\nconsider a CoT inspired baseline. It involves having\nLLMs generate a reasoning chain and leveraging its own\nreasoning capacity to provide interpretable model gen-\neration. A few examples in the prompt are used as CoT\ndemonstrations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19273\nproofWriter-PD proofWriter-PUD\nStrategy depth-0 depth-1 depth-2 depth-3 depth-5 depth-0 depth-1 depth-2 depth-3 depth-5\nstandard 0.224 0.192 0.146 0.115 0.086 0.533 0.518 0.497 0.487 0.487\nCoT 0.545 0.559 0.552 0.520 0.540 0.417 0.382 0.436 0.396 0.419\nCoT+MindMap 0.711 0.700 0.683 0.646 0.589 0.469 0.450 0.476 0.443 0.441\nrelative impr. 30.5% 25.2% 23.7% 24.2% 9.07% 12.5% 17.8% 9.17% 11.9% 5.25%\nSI 0.546 0.535 0.535 0.550 0.553 0.416 0.396 0.406 0.390 0.376\nSI+MindMap 0.723 0.694 0.666 0.604 0.537 0.466 0.458 0.465 0.445 0.43\nrelative impr. 32.4% 29.7% 24.5% 9.82% -2.89% 12.0% 15.7% 14.5% 14.1% 14.4%\nTable 1: Prediction accuracy on the proofWriter-PD and proofWriter-PUD datasets.\nbAbI\nStrategy task-1 task-2 task-3\nstandard 0.675 0.369 0.181\nCoT 0.607 0.467 0.281\nCoT+MindMap 0.881 0.473 0.340\nrelative impr. 45.1% 1.3% 21.0%\nSI 0.767 0.356 0.253\nSI+MindMap 0.803 0.392 0.318\nrelative impr. 4.7% 10.1% 25.7%\nTable 2: Prediction accuracy on the bAbI dataset.\n• Selection-Inference (SI)(Creswell, Shanahan, and Hig-\ngins 2023): The SI framework alternates between se-\nlection and inference to generate a sequence of inter-\npretable, casual reasoning steps leading to the final an-\nswer. During iterations, new conclusions can be gener-\nated and used for updating the fact set. SI runs 3 itera-\ntions for bAbI and 5 iterations for proofwriterOW A.\nThe standard, CoT and SI frameworks all adopt 5-shot set-\nting, and use the same examples for constructing demonstra-\ntion prompts. Details about prompt construction for these\nframeworks are based on the settings described in the ap-\npendix of the SI paper.\nResults and Analysis\nMain Results\nResults on ProofWriterOWA Table 1 shows the\nProofWriterOW A dataset results. In the ProofWriter-\nPD subset, both CoT and SI models did much better\nthan the usual baseline. MindMap also improved results\ncompared to many standard CoT and SI baselines. At\ndepth-5, the benefit of MindMap was not as clear, possibly\nbecause the problems were too complex for the model to\nfigure out the evidence chain accurately. But, we saw big\nimprovements from depth-0 to 4. This is mainly because\nMindMap can combine and sum up various facts well, help-\ning the model make more direct and precise decisions. The\nimprovements in CoT and SI with MindMap have shown\nsignificant performance boosts, highlighting MindMap’s\neffectiveness.\nOn the ProofWriter-PUD subset, MindMap also increases\nthe performance of CoT and SI, but the relative im-\nprovements become smaller compared with that on the\nProofWriter-PD subset. We can also see that the advanced\nreasoning frameworks cannot beat the standard baseline. We\nfind that the standard baseline prefers to predict the Un-\nknown label, while the ‘Unknown’ label accounts for about\n46% in the ProofWriter-PUD dataset. So the standard base-\nline may benefit from the imbalanced label distribution and\noutperforms CoT, SI with or without MindMap. Moreover,\npredicting Unknown should be more difficult, since in the\nProofWriterOW A dataset, Unknown means it is not possible\nto prove something is True or False. For True and False ex-\namples, there would be a reasoning trace leading to the final\nanswer, while there would be no trace for the Unknown ex-\namples. A model with a larger parameter scale may have a\nbetter ability in handling this complex problem.\nResults on bAbI dataset Table 2 illustrates the signifi-\ncant performance improvement on the bAbI dataset achieved\nthrough MindMap. Specifically, MindMap facilitated aver-\nage relative improvements of 22.5% in CoT and 13.5% in\nSI. The notable advancements in both CoT and SI, particu-\nlarly within the bAbI-3 subset, are particularly impressive,\nshowing MindMap’s strength in multi-step reasoning. This\nhighlights how much MindMap helps in improving reason-\ning and understanding skills.\nIn summary, across both datasets, MindMap has proven to\neffectively enhance the performance of existing reasoning\nframeworks, such as CoT and SI, particularly in scenarios\ninvolving multi-step reasoning.\nAblation Study\nWe conduct analyses on the bAbI dataset to quantify the im-\npact of the manipulating evidence chains within MindMap.\nTable 3 shows the ablation study results within two reason-\ning framework CoT and SI.\nSEC represents the proposed subject-centric evidence\nchain construction, which are then directly used for infer-\nence without evidence summarization. MindMap combines\nSEC and evidence chain summarization for inference. SEC\nshows big improvements in simple reasoning tasks, like\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19274\nCOT based on the original context:\nat t=0 Daniel moved to the office. \nat t=1 Daniel travelled to the bedroom. \nat t=2 John took the milk there. \nat t=3 John travelled to the garden. \nat t=4 Sandra moved to the office. \nat t=5 Daniel went back to the office.\nQuestion:Where is the milk? \nInference: at t=2 John took the milk there.We know that at \nt=5 Daniel went back to the office.Therefore, milk is in the \noffice. \nMindMap based on the evidence chain summaries:\nchain-0: Daniel passed through the \noffice(t=0),bedroom(t=1) in sequence.\nchain-1: John passed through the \nbedroom(t=2),garden(t=3) with milk in sequence.\nchain-2: Sandra passed through the office(t=4) in \nsequence.\nQuestion:Where is the milk? \nInference: According to chain-1: John passed through the \nbedroom(t=2),garden(t=3) with milk in \nsequence.Therefore,milk was in garden(t=3). \n(a) An example from the bAbI dataset.\nCOT based on the original context:\nThe bald eagle is cold. The bald eagle sees the bear. \nThe dog is rough. The lion likes the bald eagle. The \nlion sees the bear. The bear is big. The bear is rough. \nThe dog visits the bear. The lion is young. The lion \nsees the bald eagle. The lion visits the bear. The lion \nvisits the dog..\nRule: If something likes the bald eagle and sees the \nbear then it sees the dog. red things are rough. If \nsomeone sees the bear and visits the bear then it sees \nthe tiger.\nQuestion:The bald eagle sees the dog\nInference: The bald eagle sees the bear. We know that \nIf someone sees the bear and visits the bear then it \nsees the tiger. Therefore, the bald eagle sees the tiger, \nwhich means answer is Unknown.\n \nMindMap based on the evidence chain summaries:\nchain-0: The bald eagle is cold,sees the bear,likes the bald \neagle,sees the bald eagle.\nchain-1: The bear is big,rough,visited by the dog,lion.\nchain-2: The dog is rough,visits the bear.\nchain-3: The lion is young,likes the bald eagle,sees the bald \neagle,sees the bear,visits the bear,visits the dog\nRule: If something likes the bald eagle and sees the bear then \nit sees the dog. red things are rough. If someone sees the bear \nand visits the bear then it sees the tiger.\nQuestion:The bald eagle sees the dog\nInference: According to chain-0: The bald eagle is cold,sees \nthe bear,likes the bald eagle,sees the bald eagle.And \naccording the rules: If something likes the bald eagle and sees \nthe bear then it sees the dog.  Therefore, The bald eagle sees \nthe dog, which means answer is True.\n(b) An example from the ProofWriterOW A dataset.\nFigure 3: The traces of CoT and MindMap enhanced CoT for solving examples from the bAbI and ProofWriterOW A dataset\nfor the qualitative analysis.\nbAbI\nStrategy task-1 task-2 task-3\nstandard 0.675 0.369 0.181\nCoT 0.607 0.467 0.281\nCoT+SEC 0.789 0.462 0.26\nCoT+MindMap 0.881 0.473 0.340\nCoT+Auto-MindMap 0.372 0.233 0.183\nSI 0.767 0.356 0.253\nSI+SEC 0.860 0.451 0.269\nSI+MindMap 0.803 0.392 0.318\nSI+Auto-MindMap 0.364 0.218 0.166\nTable 3: The ablation study results. SEC: subject-centric ev-\nidence chain construction; Auto-MindMap: using the model\nto build and summarize evidence chains via instruction tun-\ning; MindMap: using the summaries of evidence chains for\ninference.\ntask-1, and it’s even more effective in the SI framework. This\nmeans that creating evidence chains helps the SI model pick\nbetter facts for reasoning. Also, MindMap significantly im-\nproves complex tasks, like task-3, involving multi-step rea-\nsoning. This indicates that evidence chain summarization\ncan enhance inference by compressing information and em-\nphasizing important parts in each evidence chain, which is\nimportant for multi-step reasoning, when multiple facts are\ninvolved and both useful and noisy information are mixed.\nFor simple reasoning tasks under the SI framework, evi-\ndence chain summarization seems unnecessary.\nWe also try to let the model automatically build evidence\nchain through instruction learning, called Auto-MindMap.\nHowever, Auto-MindMap often results in a decrease in per-\nformance, as this automatic process may add an extra burden\nto the model. The comparison between MindMap and Auto-\nMindMap also indicates that combining traditional NLP\ntools with LLMs is a simple and effective way to incorporate\nlinguistic motivated structures.\nQualitative Analysis\nWe further conduct a qualitative analysis by comparing\nthe behaviors of CoT and MindMap enhanced CoT. Fig-\nure 3 shows two comparison examples on the bAbI and\nProofWriterOW A datasets respectively.\nAs shown in Figure 3a, the example from the bABI\ndataset shows a reasoning problem, requiring to integrate\ntwo facts to infer the correct answer. CoT selects two un-\nrelated facts and one key fact involving John is missed dur-\ning inference. So it makes a wrong prediction. In contrast,\nMindMap correctly selects an evidence chain, which sum-\nmarizes the series of activities of John. This summary leads\nthe model to make a correct prediction.\nFigure 3b shows an example from the ProofWriterOW A\ndataset. We can also see that the MindMap can provide con-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19275\ncise summaries for related facts, which have a better match-\ning with the rules and help improve the inference perfor-\nmance.\nBased on the examples, we can see that one advantage of\nevidences is grouping related facts. This advantage can of-\nten avoid missing important and relevant information. We\nconduct an analysis of the coverage of supporting facts on\nthe task-2 and task-3 in bAbI dataset by matching the tem-\nporal id, e.g., t = 0, t = 1, between the predictions and\nthe gold supporting fact reference. MindMap can cover 69%\nand 47% supporting facts on task-2 and task-3 resepctively,\nwhile the same statistics for CoT are 48% and 23%. The\nanalysis confirms that the advantage of MindMap may be\nnot so important for simple reasoning but should be helpful\nfor multi-step reasoning.\nIn this work, we use subjects or entities to group facts.\nThis is consistent with our intuition, since people also often\ninfer some conclusions by tracking and analyzing someone’s\nbehavior or related events.\nError Analysis\nIn our study, we conducted an error analysis on a sam-\nple of outputs generated by MindMap. The identified errors\nbroadly fall into the following categories:\n• Summarization Errors: These errors originate from the\nevidence chain summarization module and manifest in\ntwo forms: the omission of crucial information and the\nintroduction of inaccurate or fabricated details (halluci-\nnations) during the summarization process. In our anal-\nysis of the bAbI dataset following LLMs’ summariza-\ntion, correct evidence chains were identified 68.9% of\nthe time on average, with a reduced accuracy of 63.8%\nspecifically for task-2. This lower rate of accurate evi-\ndence chain identification in task-2 accounts for the less\npronounced improvements observed with our method in\nthis particular task.\n• Evidence Chain Selection Errors: Challenges may\narise in selecting pertinent evidence chains for reason-\ning, even when LLMs produce informative and accurate\nsummaries. Situations that require consideration of mul-\ntiple evidence chains pose a particular challenge, often\nleading to errors in chain selection.\n• Hallucination errors during inference: When the cor-\nrect evidence chain is selected, LLMs may also hallu-\ncinate during paraphrasing the summary of an evidence\nchain, thereby introducing information bias and inac-\ncurate inferences. This includes modifying or fabricat-\ning evidence, ignoring key facts, and incorrectly aligning\nquestions and answers, all of which ultimately lead to in-\ncorrect conclusions. For example, the question in this ex-\nample is the squirrel visits the cow, and the answer given\nby LLM is chain-3: The squirrel is big. And according to\nthe rules: If something is round then it visits the cow. We\nknow that the squirrel is round . Therefore, the squirrel\nvisits the cow. LLM selects the correct chain. The model\nneeds to use two rules to get the final answer. However,\nin the subsequent reasoning process, LLMs fabricatedthe\nsquirrel is round in order to take shortcuts.\n• Model inference errors: The remaining errors can be\nattributed to the model’s inference capabilities. These er-\nrors still occur despite choosing the correct chain of evi-\ndence. Even after the model has reached the correct con-\nclusion, errors still occur when obtaining the final an-\nswer. For example,Therefore, Dave is nice. The question\nis “Dave is nice. ”, which means answer is false. This is\ndue to insufficient capabilities of the model.\nIn summary, our error analysis identified key factors that\ncontribute to reasoning errors. To reduce these errors and\nimprove overall performance, some solutions are proposed.\nThese include enhancements to evidence chain summariza-\ntion to ensure more accurate summaries to significantly re-\nduce error rates and using larger models to increase the\nLLM’s efficiency in complex reasoning tasks.\nConclusion\nIn this paper, we explore how structuring available facts\ncan enhance reasoning capabilities. The proposed MindMap\napproach organizes these facts into evidence chains, seam-\nlessly integrating with existing reasoning frameworks such\nas CoT and SI. Experiments are conducted on two complex\nmulti-step reasoning datasets. As shown in the results, both\nCoT and SI, when augmented with MindMap, can achieve\nsignificant improvements, particularly in multi-step reason-\ning tasks. Underscoring the importance of systematic orga-\nnization of available facts, our approach demonstrates su-\nperior performance to CoT and SI in recalling supporting\nfacts. The simplicity of constructing evidence chains sug-\ngests that integrating traditional natural language processing\ntools with LLMs could effectively tackle complex reasoning\nchallenges by leveraging linguistically motivated structures.\nHowever, our work still has some limitations. First, due\nto computational resource constraints, our evaluation was\nconducted using a LLM with 13B parameters. In future re-\nsearch, exploring a diverse range of models with varying\nsizes of parameters could prove beneficial. Second, though\nintuitive and heuristic, our approach to constructing subject-\ncentric evidence chains has been successfully used for evalu-\nations on synthetic datasets. Nevertheless, practical applica-\ntions could encounter challenges such as pronoun resolution,\nwhich we aim to address by enhancing the model’s capabil-\nities in autonomously constructing evidence chains. Third,\nalthough constructing evidence chains has proven effective\nfor organizing facts, exploring alternative methods remains a\nvaluable avenue. Employing semantic information or knowl-\nedge graphs could offer a more comprehensive approach to\norganizing facts, thereby improving the reasoning and judg-\nment capabilities of LLMs.\nAcknowledgments\nThis work is supported by the National Natural Science\nFoundation of China (No.62376166, No.62306188), the Na-\ntional Key Research and Development Programme of China\n(No.2022YFC3303504), and the Science Research & Devel-\nopment Program of Beijing Municipal Education Commis-\nsion (No.KM202010028004).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19276\nReferences\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A\nmultitask, multilingual, multimodal evaluation of chatgpt on\nreasoning, hallucination, and interactivity. ArXiv preprint,\nabs/2302.04023.\nBesta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.;\nGianinazzi, L.; Gajda, J.; Lehmann, T.; Podstawski, M.;\nNiewiadomski, H.; Nyczyk, P.; et al. 2023. Graph of\nthoughts: Solving elaborate problems with large language\nmodels. ArXiv preprint, abs/2308.09687.\nBetz, G.; V oigt, C.; and Richardson, K. 2021. Critical Think-\ning for Language Models. In Proceedings of the 14th Inter-\nnational Conference on Computational Semantics (IWCS) ,\n63–75. Groningen, The Netherlands (online): Association\nfor Computational Linguistics.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual.\nChambers, N.; and Jurafsky, D. 2008. Unsupervised learning\nof narrative event chains. In Proceedings of ACL-08: HLT,\n789–797.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.\n2023. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023).\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2023. Palm: Scaling language model-\ning with pathways. Journal of Machine Learning Research,\n24(240): 1–113.\nClark, P.; Tafjord, O.; and Richardson, K. 2020. Transform-\ners as Soft Reasoners over Language. In Bessiere, C., ed.,\nProceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI 2020, 3882–3890.\nijcai.org.\nCreswell, A.; Shanahan, M.; and Higgins, I. 2023. Selection-\nInference: Exploiting Large Language Models for Inter-\npretable Logical Reasoning. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023. OpenReview.net.\nDai, D.; Sun, Y .; Dong, L.; Hao, Y .; Ma, S.; Sui, Z.; and Wei,\nF. 2023. Why Can GPT Learn In-Context? Language Mod-\nels Secretly Perform Gradient Descent as Meta-Optimizers.\nIn Rogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds.,\nFindings of the Association for Computational Linguistics:\nACL 2023, Toronto, Canada, July 9-14, 2023, 4005–4019.\nAssociation for Computational Linguistics.\nDong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.;\nSun, X.; Xu, J.; and Sui, Z. 2023. A survey for in-context\nlearning. ArXiv preprint, abs/2301.00234.\nGoertzel, B.; and Pennachin, C. 2007. Artificial general in-\ntelligence, volume 2. Springer.\nHan, S.; Schoelkopf, H.; Zhao, Y .; Qi, Z.; Riddell, M.; Ben-\nson, L.; Sun, L.; Zubova, E.; Qiao, Y .; Burtell, M.; et al.\n2022. Folio: Natural language reasoning with first-order\nlogic. ArXiv preprint, abs/2209.00840.\nKatz, D. M.; Bommarito, M. J.; Gao, S.; and Arredondo,\nP. 2023. Gpt-4 passes the bar exam. Available at SSRN\n4389233.\nKazemi, M.; Kim, N.; Bhatia, D.; Xu, X.; and Ramachan-\ndran, D. 2023. LAMBADA: Backward Chaining for Auto-\nmated Reasoning in Natural Language. In Rogers, A.; Boyd-\nGraber, J. L.; and Okazaki, N., eds., Proceedings of the\n61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, 6547–6568. Association for Com-\nputational Linguistics.\nLiu, H.; Ning, R.; Teng, Z.; Liu, J.; Zhou, Q.; and Zhang, Y .\n2023. Evaluating the logical reasoning ability of chatgpt and\ngpt-4. ArXiv preprint, abs/2304.03439.\nLiu, J.; Cui, L.; Liu, H.; Huang, D.; Wang, Y .; and Zhang,\nY . 2020. LogiQA: A Challenge Dataset for Machine Read-\ning Comprehension with Logical Reasoning. In Bessiere,\nC., ed., Proceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence, IJCAI 2020, 3622–\n3628. ijcai.org.\nManning, C. D.; Surdeanu, M.; Bauer, J.; Finkel, J. R.;\nBethard, S.; and McClosky, D. 2014. The Stanford CoreNLP\nnatural language processing toolkit. In Proceedings of 52nd\nannual meeting of the association for computational linguis-\ntics: system demonstrations, 55–60.\nMin, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.;\nHajishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the\nRole of Demonstrations: What Makes In-Context Learning\nWork? In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, 11048–11064.\nAbu Dhabi, United Arab Emirates: Association for Compu-\ntational Linguistics.\nMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Bi-\nderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z. X.;\nSchoelkopf, H.; Tang, X.; Radev, D.; Aji, A. F.; Almubarak,\nK.; Albanie, S.; Alyafeai, Z.; Webson, A.; Raff, E.; and Raf-\nfel, C. 2023. Crosslingual Generalization through Multi-\ntask Finetuning. In Rogers, A.; Boyd-Graber, J. L.; and\nOkazaki, N., eds., Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14, 2023,\n15991–16111. Association for Computational Linguistics.\nNori, H.; King, N.; McKinney, S. M.; Carignan, D.; and\nHorvitz, E. 2023. Capabilities of gpt-4 on medical challenge\nproblems. ArXiv preprint, abs/2303.13375.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19277\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPeng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. In-\nstruction tuning with gpt-4. ArXiv preprint, abs/2304.03277.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann,\nJ.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.; Young,\nS.; et al. 2021. Scaling language models: Methods, anal-\nysis & insights from training gopher. ArXiv preprint,\nabs/2112.11446.\nSaparov, A.; and He, H. 2023. Language Models Are\nGreedy Reasoners: A Systematic Formal Analysis of Chain-\nof-Thought. In The Eleventh International Conference on\nLearning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net.\nSilver, D.; Singh, S.; Precup, D.; and Sutton, R. S. 2021.\nReward is enough. Artificial Intelligence, 299: 103535.\nTafjord, O.; Dalvi, B.; and Clark, P. 2021. ProofWriter: Gen-\nerating Implications, Proofs, and Abductive Statements over\nNatural Language. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, 3621–3634. On-\nline: Association for Computational Linguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. ArXiv preprint, abs/2302.13971.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWeston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2016.\nTowards AI-Complete Question Answering: A Set of Pre-\nrequisite Toy Tasks. In Bengio, Y .; and LeCun, Y ., eds.,\n4th International Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Confer-\nence Track Proceedings.\nYang, J.; Jin, H.; Tang, R.; Han, X.; Feng, Q.; Jiang, H.;\nYin, B.; and Hu, X. 2023. Harnessing the power of llms in\npractice: A survey on chatgpt and beyond. ArXiv preprint,\nabs/2304.13712.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023. Tree of thoughts: Delib-\nerate problem solving with large language models. ArXiv\npreprint, abs/2305.10601.\nZhang, H.; Li, L. H.; Meng, T.; Chang, K.; and den Broeck,\nG. V . 2023. On the Paradox of Learning to Reason from\nData. In Proceedings of the Thirty-Second International\nJoint Conference on Artificial Intelligence, IJCAI 2023,\n19th-25th August 2023, Macao, SAR, China, 3365–3373. ij-\ncai.org.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\nChi, E. H. 2023. Least-to-Most Prompting Enables Complex\nReasoning in Large Language Models. In The Eleventh In-\nternational Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19278",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.652993381023407
    },
    {
      "name": "Programming language",
      "score": 0.47545525431632996
    },
    {
      "name": "Natural language processing",
      "score": 0.37166500091552734
    }
  ]
}