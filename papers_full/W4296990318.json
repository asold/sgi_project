{
  "title": "A Novel Action Transformer Network for Hybrid Multimodal Sign Language Recognition",
  "url": "https://openalex.org/W4296990318",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5012911020",
      "name": "Sameena Javaid",
      "affiliations": [
        "Bahria University"
      ]
    },
    {
      "id": "https://openalex.org/A5109068918",
      "name": "Safdar Rizvi",
      "affiliations": [
        "Bahria University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W7072442012",
    "https://openalex.org/W3124683025",
    "https://openalex.org/W6778907217",
    "https://openalex.org/W2765113481",
    "https://openalex.org/W2944944843",
    "https://openalex.org/W3082666011",
    "https://openalex.org/W3046952127",
    "https://openalex.org/W3198078703",
    "https://openalex.org/W4212915101",
    "https://openalex.org/W3035101080",
    "https://openalex.org/W6787445331",
    "https://openalex.org/W6787280795",
    "https://openalex.org/W6607298093",
    "https://openalex.org/W6809040596",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6746023985",
    "https://openalex.org/W3145891875",
    "https://openalex.org/W4213043331",
    "https://openalex.org/W6887808216",
    "https://openalex.org/W6746278886",
    "https://openalex.org/W6793119350",
    "https://openalex.org/W6794053914",
    "https://openalex.org/W6755431558",
    "https://openalex.org/W6790441406",
    "https://openalex.org/W6756350527",
    "https://openalex.org/W2615105684",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4214551164",
    "https://openalex.org/W4214724298"
  ],
  "abstract": "Sign language fills the communication gap for people with hearing and speaking ailments. It includes both visual modalities, manual gestures consisting of movements of hands, and non-manual gestures incorporating body movements including head, facial expressions, eyes, shoulder shrugging, etc. Previously both gestures have been detected; identifying separately may have better accuracy, but much communicational information is lost. A proper sign language mechanism is needed to detect manual and non-manual gestures to convey the appropriate detailed message to others. Our novel proposed system contributes as Sign Language Action Transformer Network (SLATN), localizing hand, body, and facial gestures in video sequences. Here we are expending a Transformer-style structural design as a \"base network\" to extract features from a spatiotemporal domain. The model impulsively learns to track individual persons and their action context in multiple frames. Furthermore, a \"head network\" emphasizes hand movement and facial expression simultaneously, which is often crucial to understanding sign language, using its attention mechanism for creating tight bounding boxes around classified gestures. The model's work is later compared with the traditional identification methods of activity recognition. It not only works faster but achieves better accuracy as well. The model achieves overall 82.66% testing accuracy with a very considerable performance of computation with 94.13 Giga-Floating Point Operations per Second (G-FLOPS). Another contribution is a newly created dataset of Pakistan Sign Language for Manual and Non-Manual (PkSLMNM) gestures.",
  "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided\nthe original work is properly cited.\nechT PressScienceComputers, Materials & Continua\nDOI: 10.32604/cmc.2023.031924\nArticle\nA Novel Action Transformer Network for Hybrid Multimodal Sign Language\nRecognition\nSameena Javaid* and Safdar Rizvi\nDepartment of Computer Sciences, School of Engineering and Applied Sciences, Bahria University, Karachi Campus,\nKarachi, Pakistan\n*Corresponding Author: Sameena Javaid. Email: sameenajaved.bukc@bahria.edu.pk\nReceived: 30 April 2022; Accepted: 22 June 2022\nAbstract: Sign language fills the communication gap for people with hearing\nand speaking ailments. It includes both visual modalities, manual gestures\nconsisting of movements of hands, and non-manual gestures incorporating\nbody movements including head, facial expressions, eyes, shoulder shrugging,\netc. Previously both gestures have been detected; identifying separately may\nhave better accuracy, but much communicational information is lost. A proper\nsign language mechanism is needed to detect manual and non-manual gestures\nto convey the appropriate detailed message to others. Our novel proposed\nsystem contributes as Sign Language Action Transformer Network (SLATN),\nlocalizing hand, body, and facial gestures in video sequences. Here we are\nexpending a Transformer-style structural design as a “base network”to extract\nfeatures from a spatiotemporal domain. The model impulsively learns to track\nindividual persons and their action context in multiple frames. Furthermore, a\n“head network” emphasizes hand movement and facial expression simultane-\nously, which is often crucial to understanding sign language, using its attention\nmechanism for creating tight bounding boxes around classified gestures. The\nmodel’s work is later compared with the traditional identification methods of\nactivity recognition. It not only works faster but achieves better accuracy as\nwell. The model achieves overall 82.66% testing accuracy with a very consider-\nable performance of computation with 94.13 Giga-Floating Point Operations\nper Second (G-FLOPS). Another contribution is a newly created dataset of\nPakistan Sign Language for Manual and Non-Manual (PkSLMNM) gestures.\nKeywords: Sign language; gesture recognition; manual signs; non-manual\nsigns; action transformer network\n1 Introduction\nPeople who are deaf and mute rely heavily on sign language for communication. People move\ntheir hands to communicate as a nonverbal way of expressing thoughts, needs, and messages. The\nWorld Health Organization (WHO) declared it deafened 430 million individuals [1]. Deaf and mutes\ncommunicate with regular people with various signs from sign language. Sign language includes both\n524 CMC, 2023, vol.74, no.1\nmanual and non-manual gestures. In sign language, visual modalities are manual and non-manual\ngestures. Manual gestures utilize hands and signs only, whereas non-manual gestures incorporate\nbody movements, including head, shoulder shrugging, facial expression, etc. A common way of\ncommunication of including deaf, mute, and ordinary people is a facial expression [2]. It is a\nrational mental reaction towards any particular object that can feel like a strong feeling and is an\nessential aspect of effective communication, usually followed by physiological responses. Nonverbal\ncommunication takes the form of facial expressions and hand gestures as it expresses humans’ feeling\na lot quicker than verbal communication and is more effective. Neutral, happy, sad, angry, disgust,\nsurprise, and fear are a few universal emotional expressions. A person’s emotion can be analyzed in\nvarious stages as it defines a person’s current state, mood, and feelings. According to Mehrabian,\na Psychologist, emotion is conveyed 7 percent through language, 38 percent through voice, and 55\npercent through facial expression [3].\nA rational mental reaction towards any particular object, which can feel like a strong feeling and\nis an essential aspect of effective communication, is an emotion, usually followed by physiological\nresponses. One of the most critical impacts on our life is emotion, as they show our physical and\nbehavioral changes. Nonverbal communication, including audio, videos, photographs, and images, is\ndifficult to recognize. To communicate with one another in various ways, humans can do so through\nfacial expression, body language, speech, and body gestures [2]. The human brain can anticipate the\nother person’s emotion depending upon the situation based on the current mood standing in front of\nthem. Nonverbal communication takes the form of facial expressions as it expresses human feelings\na lot quicker than verbal communication and is more effective. A human’s face spreads emotional\ninformation in terms of facial expressions, including eyes, brows, mouth, and cheeks. Neutral, happy,\nsad, angry, disgust, surprise, and fear are a few universal emotional expressions.\nCurrently, automatic Sign Language (SL) analysis and recognition are considered by many\nresearchers, precisely intuitive SL interpretation. Two convincing ways to conceive such systems are:\nusing special devices like Leap Motion Controller (LMC), Kinect sensors, or other motion sensors\n[4,5]. Another way is vision-based automatic inspection and analysis [6], where researchers perform\nmachine learning and vision-based deep learning to understand Sign Language (SL) [7]. Techniques\nbased on some devices did not arouse well, requiring signers to wear expensive additional devices.\nTherefore, vision-based techniques are more adequate and require fewer devices and resources.\nMachine learning and deep learning convolutional neural networks can successfully recognize\nvarious alphabets and numbers of different sign languages and human facials such as good, bad,\ndisgusted, anger, happiness, sad, surprise, fear, etc. in both static and dynamic images in real-time\nbut unfortunately, very few contributions are made to combining sign language and facial expression\nof people considering deaf [8]. Working on only one problem simultaneously creates a loss of useful\ninformation. It is also observed that signers’ facial expressions and body posture frequently change\nwhile performing any sign to deliver the actual meaning and clear sense of the gesture concerning\nlanguage. Similarly, a single sign may correspond to several facial gestures, and a single facial\nexpression can combine with many signs. The point of the proposed strategy is to distinguish and\nperceive sign language, including body gestures of hands, arms, and facial expressions. These feelings\ninvolve profound learning with the most extreme exactness in a restricted chance to rival conventional\ntechniques. The main contributions of the proposed work are as follows:\n• Our first contribution is a novel Sign Language Action Transformer Network (SLATN)\narchitecture to interpret sign language with manual and non-manual features.\nCMC, 2023, vol.74, no.1 525\n• Our second contribution is creating a local database of Pakistan Sign Language (PSL) using\nseven basic expressions representing seven various adjectives in language, naming Pakistan Sign\nLanguage using Manual and Non-Manual (PSLMNM). To the best of our knowledge, this is\nthe first dataset of PSL having manual and non-manual gestures combined.\nSLATN can transform the performed action effectively to a different view at the abrasive level.\nBut the main limitation of the current architecture is that sometimes the higher appearance features\nare missing in the recorded video leading to motion blur. In the future, the availability of resources will\nmake it possible to improve the quality but apprehend delicate appearance and motion information\nusing some memory constrictions environment.\nThe following is the order in which the paper should be interpreted. Section 2 outlines state-of-\nthe-art literature on activity detection and recognition using deep learning and computer vision. The\nproposed technique for detecting the signs of deaf and mute people is presented in Section 3. Section\n4 spoke about how we generated our dataset, analyzed the results, and compared them with existing\nwork. Finally, Section 5 brings the study debate to a brief conclusion.\n2 Literature Review\nMachine learning and deep learning networks are more widely used as they evolve regularly. These\nsystems and networks make life much easier for us as now they’ve become an indispensable part of our\nmodern lives [9]. Deep learning sign language gesture and facial recognition systems capture different\nhuman emotions and messages and have overcome traditional methods in terms of accuracy and\nspeed. Many researchers have proposed their work in recognizing and detecting signs of varying sign\nlanguages and human emotions individually and successfully for normal and people with hearing and\nspeaking disabilities.\nIt is difficult for people with hearing disabilities to communicate. Also, this disability affects their\nunderstanding of their emotions as well. Yuan Tao et al. designed an application to read and recognize\nfacial expressions specifically for deaf and mute people [10], which helps to cross-language, emotional,\nand regional barriers for deaf and mute people. The designed app was tested on 630 images of gestures\nand scored an incredible accuracy of 94.22%. Another research study proposed a model by studying\nvarious emotions and electroencephalograph (EEG) signals of physically impaired people and autistic\nchildren through convolutional neural networks with Long Short Term Memory (LSTM) classifiers in\nKuwait [11]. The aim was to study the prominent rising need to recognize different facial expressions\nand emotions utilizing virtual markers in an optical flow model that worked efficiently and achieved\nmaximum accuracy of 99.81%.\nSimilarly, in another study, emotions are recognized using body gestures. The proposed model\nextracted features from the input video from an online, freely accessible dataset through the hashing\nmethod and used convolutional LSTM to utilize the extracted sequential information [12]. Working\non the same dataset, [13] suggested a model highlighting the same problem: identifying emotions using\nupper body movements and facial expressions. The convolutional neural network (CNN) extracts\nthe features while LSTM utilizes the sequential information and achieves 94.41% accuracy. It is also\nrecognized for independent sign language, comprising a 3D body, hands, and facial expressions. For\nthis purpose, SMPL-X extracts features of body shape, face, and writing using a single image [14]. The\n3D reconstructed output of the 3D model is utilized for SLR resulting in higher accuracy than raw\nRGB images.\n526 CMC, 2023, vol.74, no.1\nSign language acts as an indispensable source of communication for people with hearing and\nspeech disabilities, but detecting sign in a continuous video face many challenges in terms of accuracy\nand performance. Previously, most works were done on seeing any modality with static images. Few\nresearchers proposed a method to detect sign language in a continuous video, including gestures of\nboth hands, head pose, and eye gaze [15]. Features from the continuous video were extracted by\nusing Hidden Markov Model (HMM) and classified gestures through Independent Bayesian Classifier\nModel (IBCC) with 93% of good accuracy. Another researcher used an auto-encoder convolutional\nnetwork for a large-scale RGB video to design a novel 3D Gesture Segmentation Network (3D GS-\nNet) for developing a communicational environment for deaf people utilizing hands, body, and facial\nexpressions [16]. Compared with other state-of-the-art language recognition systems, the proposed\nmodel was tested using Moroccan sign language (MoSL) and performed better.\nIn summary, it is stated that several researchers studied manual or non-manual gesture interpre-\ntation in sign language but working with the correlation between the two is rare in the literature,\nalong with better accuracy and speed using video processing or dynamic sign language interpretation\nconsidering full-body gestures. A raw High Definition (HD) video processing, while considering\nmanual and non-manual gestures with maximum accuracy and limited time is still a requirement for\nDynamic SLR.\n3 Methodology\nThe section elaborates on the complete design and flow of the model. The proposed model is\nintended to classify humans and characterize their emotions and gestures at a specific time. It takes\nvideo clips as input and generates labeled bounding boxes around people doing activities appearing\nin the video. The model has two separate networks, i.e., base and head like Faster Region-based\nConvolutional Neural Network (F-RCNN). The base expends a 3Dimensional-Convolution structure\nto extract features, and region bounds are generated for the individuals present in the input video. The\nhead later utilizes these characteristics to predict bounding boxes with better accuracy, corresponding\nto each Region Proposal (RP). The head uses the features representation created by the base network\nusing Region of Interest (RoI) Pooling to predict the class label and regressor bounding boxes. These\nboxes are categorized as action classes with the background. The RPN proposal is regressed to a 4D\nvector to create strict bounding boxes around people. The base and the head networks are described\nbelow.\n3.1 Base Network\n3.1.1 I3D Model\nThe conventional deep learning models utilized a mono convolutional kernel, and before the\nfeature set is generated, this convolution kernel processes its input. Different kernels are used to\nanalyze the sprite separately in the Inception module. Although the resulting features are different,\na group of complementary features creates a subset of densely scattered features. Consequently, after\npassing various convolutional layers, unnecessary data is undermined. The Inflated 3-Dimension\n(I3D) module comprises three inception layers and two convolutional pooling layers.\nThe I3D module is inherited after GoogLeNet’s Inception network, but different sizes of convo-\nlutional kernels extract the features. In GoogLeNet [17], one convolution is performed on the yield of\neach prior layer which is succeeded by an activation function. Additional nonlinear features are joined\nby adding two three-dimensional convolutions. The inception module has four groups of input data,\nwhereas each group consists of one or more convolution and pooling operations. Finally, different\nCMC, 2023, vol.74, no.1 527\nconvolution kernels of various sizes are spliced together. The I3D model has a convolution operation\nfor every dataset’s adjacent feature, which continuously completes the action recognition on frames.\nA batch regularization module is connected to the system to accelerate the training process. A higher\nlearning rate can be used since the initialization does not affect the model. To enhance the depth of\nthe model, I3D has eight convolutional and four pooling layers. The kernel has a step size of 1× 1 ×\n1a n dap i x e ls i z eo f3× 3 × 3 in each convolutional layer. The numbers of filters are multiples of 64\nalong with each convolutional layer containing batch regularization along with an activation function\nwith a pooling layer, respectively. The pooling layer and the step size of the kernels in conv3, conv4,\nand conv5 are 1× 2 × 2. The remaining pooling layers have the same step size and kernel. There is\nonly spatial pooling in the first convolution layer, while the second, fourth and sixth convolution layers\nhave spatial-temporal pooling layers used. The yield size from convolutional layers is limited to\n1/4 and\n1/2 in space and time regions because of the pooling layers. As a result, I3D is more suitable for LSTM\nwith spatial-temporal features.\n3.1.2 I3D ShuffleNet\nIn conventional I3D, convolutional operations are performed by two kernels of equal 5× 5 × 5\nsize to extract features but at the same time cause excessive computational power. These kernels can be\nmodified by following specific dimensional rules, which will convolve images after learning different\nkernel combinations.Fig. 1shows kernels of 3× 3 × 3 size, which replace the 5× 5 × 5 convolutional\nkernel. In this replacement, almost 30% of trainable parameters are reduced.\nFigure 1:Inception module with 5× 5a n d3× 3 convolution kernel\n3.1.3 Channel Shuffling\nThe channel shuffle is made by using the concept of shuffleNet. As a deep learning network,\nFace ++ presented the shuffleNet, a CNN model that is highly efficient and robust. It is primarily\ndesigned to use with communication systems such as robots, drones, and phones and hence strives\nas the finest network in terms of accuracy and restricting computational power. The primary task of\nshuffleNet, which immensely reduces the number of computations while keeping the accuracy high, is\n528 CMC, 2023, vol.74, no.1\nthe channel shuffle and element-wise group convolution. The basic I3D, on the other hand, only uses\ngroup convolution, which appears as its major disadvantage because its output channel is generated\nby using the small chunks of the input channel. For shuffling, pixel-level group convolution is created,\nwhich decreases the complexity of computation generated due to the convolution operation. Group\nconvolution hampers the exchange of data between the channels, resulting in loss of feature extraction.\nChannel shuffling resolves this issue of exchanging information between the channels.Fig. 2represents\nthe channel shuffling in a descriptive way.\nFigure 2:Channel shuffling\nThe process of splitting channels is introduced. For the feature maps, the c channel splits into two\nsections. The first section consists of c-c’ channels, while the second has only c’ channels. One part\nhas an equal number of three convolutional channels, while the other is fixed to limit the amount of\nshuffle. Channel segmentation is the second part of splitting two groups, and convolutions of two 1× 1\nare not joined. To maintain the number of channels constant, the features of the two parts are merged\nto ensure that the information of these two parts engages. To generate the residual block, medians\nand point-to-point convolution are mixed. To enhance the flow of data through various groups on\nthe main feature of the bottleneck branch, the shuffleNet operation is succeeded by point-to-point\ngrouping convolution. The computation is also reduced by adding small depth split table branch pixel\nconvolution of sizes 3× 3 × 3a n d1× 1 × 1 after point to point grouping convolution. A layer of max-\npooling is introduced to replace the pixel by pixel summing operation, which can cause an increase in\ncomputation by expanding the dimension of the channel.\n3.1.4 Structure\nAfter incorporating time information, for I3D shuffleNet, the 2D convolution is enlarged to\n3D shuffleNet. In the proposed architecture SLATN, the instant normalization layer is introduced\ninstead of typical batch normalization. The input image features are merged using processing of the\nvarious inception convolution while the input of the channel shuffle is coming from the output of\nthe inception network, with 50 percent of the feature maps being inputted directly into the following\nnetwork. The shuffle operation works after the 6th layer inception module; the resulting output is\nmerged with the 9th Inception module. The exact terms for reusing features are used in DenseNet\nand CondenseNet. Later three channels split the second half and were analyzed independently using\nchannel segmentation. The whole batch needs to be loaded in memory for batch normalization, which\nrequires more memory than instant segmentation. The architecture is shown inFig. 3.\nCMC, 2023, vol.74, no.1 529\nFigure 3:I3D ShuffleNet\n3.2 Head Network\n3.2.1 Sign Language Action Transformer Framework\nAs the introduction states, the head of our framework is prompted by and reconfigured from the\nTransformer architecture [18]. It locates regions to join to use the RPN’s user box as a ‘query’ and\ncategorizes the data over the video to categorize their actions. The proposed architecture of the action\ntransformer head is to replace conventional reoccurring models for seq2seq activities like translators.\nThe essential principle of the prevailing is to calculate self-attention, which is performed by comparing\neach feature in a series. This is accomplished effectively by avoiding the use of the original features.\nThey are instead utilizing linear projections. Initially, features are mapped to query and memory,\nrepresented by (Q) and (K as key & V as value), having low dimensions. Where the result of the query\nis calculated as the sum of V (all weighted attention), attention weights are acquired from Q and K.\nThe proposed paper had a word as a query that had to get translated in practice; the input ad linear\nprojections as keys and values linear whereas the sequences of the output were generated. The lost\ninformation due to a non-convolutional setup, a system of embedding location is added for further\ndetails readers can read [18]a n d[19].\n530 CMC, 2023, vol.74, no.1\n3.2.2 Action Transformer\nThe proposed model uses a modified action transformer structure to understand videos better.\nIn this case, the values of the query (Q), key (K), and memory are fitted naturally according to the\nproblem: the recognized person is the query, and memory is the video of the person surrounding\nestimated into values and keys. The unit and the memory analyze the query to generate query vectors.\nThe unit then processes the query and memory, which results in the newly developed “query vector.”\nThe supposition that self-attention will assist the “query vector” with classification by adding context\nfrom the surroundings in the video. By convolving the yield from various heads at a specific layer,\ncascade features will be the following query; this unit could be heaped in different layers and head\nsequences, close to the initial structure [18]. The next layer uses this latest query to pay more attention\nto context features.Fig. 4depicts the above setup formfitting in our base network, ‘Tx’ is the sign\nlanguage action transformer unit, represented in blue. Further, this unit is explained thoroughly.\nFigure 4:The system architecture of the proposed model\nThe trunk output gives the initial feature maps for the key and value features, resulting in a shape\nof T′ H′ W′ D for each.\nWhile performing, query features are extracted from the center clip by ROI pooled features having\nsize 11 D for individual boxes and passed through a linear layer and query pre-processor(QPr).\nThe pooled features from ROI could be in a straight line averaged across the (QPr). However, the\nindividual’s spatial layout would be lost. Instead, we combine the derived 7×7 feature map cells into\na vector after reducing the dimensionality with a 1×1 convolution. At last, we use a linear layer\nto limit the dimensionality of this feature map to 128D. This process, known as the HighRes query\npreprocessing Swish function, is used as an activation to avoid the slow process of the training process\naround the zero gradients. Compared to the RPN proposal r, the Q-r feature is utilized. The K features\nare normalized by√ D; the dot product is used. This operation can be expressed briefly inEq. (1)[20].\na\n(r)\nxyt = QrKT\nxyt\n√D ; Ar =\n∑\nx,y,t\n[\nSwish\n(\na(r))]\nxyt Vxyt. (1)\nAn (r) gets a dropout and is added to the initial features of the query. The query can then pass\nthrough a remaining branch that includes a Layer Norm operation, a “Feed-Forward Network”(FFN)\nimplemented as a 2-layer MLP, and dropout. To obtain the revised query (Q\n′′), the concluding feature\nCMC, 2023, vol.74, no.1 531\nis passed through one more Layer Norm.Fig. 4(Tx unit) depicts the unit mentioned above structure\nand can be symbolized in the followingEqs. (2)and (3).\nQ(r)′\n= Layer Norm (Q(r) + Dropout\n(\nA(r))\n, (2)\nQ(r)′′\n= Layer Norm (Q(r)′\n+ Dropout(FFN\n(\nQ(r)′)\n. (3)\n4 Experiment\n4.1 Dataset\nTo identify manual and non-manual gestures simultaneously, a dataset of Pakistan Sign Language\n(PSL), named Pakistan Sign Language Manual and Non-Manual (PkSLMNM), consists of 180 people\nwith an average age group of 20 to 50. Of which 70 are females, and the rest 110 are males. All of the\nparticipants have submitted consent to taking part in this study. Furthermore, regarding ethical norms\nfor data collection and usage, the ethical review committee of the School of Engineering and Applied\nSciences, Bahria University Karachi Campus, has approved the data collection process and purpose\nof research under application ERC/ES/CS/005. PSLMNM dataset is solely collected for research\npurposes, and we ensure the procedure adapted for data collection is not harmful to the participants.\nSeveral researchers have worked on the computational automation of Pakistan Sign Language\n(PSL), but none were published online for open access. Initially, in the early 20 s Deaf Reach Program\nby Pakistan Sign Language (PSL) organization created a repository of Static and Dynamic signs\nof 5000 signs [21,22]. Recently from 2019 to 2022, few researchers [23,24], have taken steps toward\npublishing some datasets for computer vision, machine learning, and deep learning [25]. These\npublished datasets are collections of static sign language gestures comprised of still images. None of\nthem are casing dynamic sign language gestures with multi-modalities in the form of videos. Also,\nthe scope and variety of the dataset are limited, which results in tradeoffs between the efficiency of\nthe system while at the same time giving flexibility to the system. The primary objective of the current\ndataset PkSLMNM is to serve as a new benchmark in Pakistan Sign Language with the unique feature\nof dynamic gestures along with manual and non-manual modalities combined. The lexicon of this\ncurrent dataset is based on dynamic hand gestures. To the best of our knowledge, no dataset covers\nthe dynamic gestures in the publicly available dataset. The current dataset is also peerless for its vast\nlexicon, robustness, high recording quality, and unique dynamic hand gesture recognition syntactic\nproperty.\nThe video clip for the input is recorded using an HD camera in .MP4 format to save various\nfacial expressions supported by hand gestures such as bad, best, sad, glad, scared, stiff, and surprised.\nThe individual candidate has to keep a specific emotion for a few seconds, the average duration of all\nrecorded clips is 2 s long, and the average size of the input clip is 3MB.Figs. 5a–5g represents bad,\nbest, sad, glad, scared, stiff, and surprise adjectives of PSL portraying disgust, neutral, sad, happy,\nscared, angry, and surprise expressions, respectively.\nThe video clips are then preprocessed into frames at a rate of 25 frames/s to remove any noise which\ncan later affect the classification. The height and width of each frame are 1920× 1080. After removing\nnoise, the dataset is passed through the augmentation steps such as flips, contrast, and cropping. The\naugmented frames are well labeled with all seven different classes of emotions. The preprocessed data\nis divided into training, testing, and validation sets. Dataset is publically available for usage [26].\n532 CMC, 2023, vol.74, no.1\n(a) Gesture bad expressing disgust emotion\n(b) Gesture best expressing neutral emotion\n(c) Glad expressing happy emotion\n(d) Gesture sad expressing sad emotion\n(e) Scared expressing scared emotion\n(f) Gesture stiff expressing angry emotion\n(g) Gesture surprise expressing surprise emotion\nFigure 5:Few frames of PSLMNM dataset for each seven emotion-based gesture\n4.2 Implementation Details\nThe system used for training and testing was the 8th generation Intel Core i7processor with 32 GB\nRAM and Nvidia GeForce GTX 1650 GPU. The model was implemented on Ubuntu with Python3\nand TensorFlow. A leaky Rectified Linear Unit (leaky ReLU) was applied as an activation function in\ntesting and training the model. The model was trained by keeping the initial learning rate of 0.0005,\nCMC, 2023, vol.74, no.1 533\nmomentum 0.93, optimizer weight decay 0.001, and batch size of 64 with Adam as an optimization\nfunction and Xavier method for initializing weight of the convolutional kernels. The cross-entropy loss\nfunction was used, which is stated inEq. (4). Here ‘a’ is the actual value while ‘b’ is the predicted value\nby the model.\nThe loss was calculated as the difference in entropy between the expected and actual values.Fig. 6\ngraph between iteration along the x-axis and loss on the y-axis shows that the loss decreases gradually\nas the number of iterations increases.\nH (a, b) =−\n∑\na (x) logb (x) . (4)\nFigure 6:Error loss graph of training and validation\nThe proposed model achieved 86.12% training accuracy and 82.66% testing accuracy at 700000\niterations, as shown in Fig. 7. Our model can recognize the seven different actions formed by\nindividuals. InFig. 8, the top predicted activities for all categories using our model are depicted. It\nis noticeable that our proposed architecture can exploit the dominated expression to recognize the\nsign gesture.\nFigure 7:Training and validation accuracy graph\n534 CMC, 2023, vol.74, no.1\n(b) Best (neutral expression) (c) Glad (happy expression)\n(d) Sad (sad expression) (e) Scared (scared expression) (f) Surprise (surprise expression)\n(g) Stiff (angry expression))\n(a) Bad (disgust expression)\nFigure 8:Top predicted videos from the validation set using SLATN\nIt is already noticeable that our model can exploit the dominant facial expression to recognize any\nsign gesture. Due to this specific, our model predicts a few videos are misclassified. InFig. 9. (a) Video\nfrom a surprise gesture is classified in Glad as in maximum frames; the facial expression represents\na happy gesture which creates a failure mode. (b) Video from Bad gestures is misclassified in Best\nGestures due to dominated facial expression. The hand’s orientation is wrong and predictable in the\nkey-frames. (c) Gestures belonging to the Glad class art are misclassified due to the wide opening of\neyes, which dominates towards surprise as well as hand gesture is similar to surprise expression in most\nframes.\n(a) Surprise Gesture (b) Bad Gesture (c) Glad Gesture\nFigure 9:Misclassified videos from the validation set using SLATN\nCMC, 2023, vol.74, no.1 535\n4.3 Result and Experiments\nThe proposed model is compared with the conventional recognition and classification convolu-\ntional neural network models. For testing these models, 25 samples from the validation set are passed\nto each model for prediction.\nThe Tab. 1Shows the evaluation of models based on their performance, the computation required\nto process Giga FLOPS (GFLOPS), and parameters. The results show that the proposed SLATN\nmodel outperforms conventional models significantly with better performance and reducing the\nrequired computational power. It is important to notice that the proposed model had no frozen\nbackbone architecture, unlike a few models used in the comparison, which eventually makes such\nmodels slower. On the other hand, at the intermediate nodes of the network, SLATN enables point-\nto-point fusion while training, allowing it to learn and perform better, outperforming other models.\nTable 1: Different models comparison\nDifferent model mAP(%) GFLOPs Param (M)\nI3D + super-events [27] 19.41 4446.15 26.18\nViVit [28] 18.55 3992.00 47.00\nMViT (deep network) [29] 47.7 7080.00 53.00\nI3D + super-events + TGM [30] 22.56 4446.75 28.28\nViT-B-VTN [31] 79.80 4218.00 114.00\nI3D + STGCN [32] 19.09 4450.94 29.18\nSLATN (Proposed model) 66.10 94.13 6.80\n5 Conclusion\nThis paper proposes a novel, time-efficient action transformer network for sign language recog-\nnition named SLATN. SLATN is able to recognize and distinguish manual and non-manual ges-\ntures, where not only hand movements are considered, but the head and shoulder movements and\nexpressions are localized in video sequences. Current architecture is expending a Transformer-style\nstructural design as a “base network” to extract features from a spatiotemporal domain using our\nnewly developed dataset PkSLMNM. Further, a “head network” simultaneously emphasizes hand\nmovement and facial expression, using its attention mechanism to create tight bounding boxes around\nclassified gestures. The network identifies gestures such as bad, best, sad, glad, scared, stiff, and\nsurprised with good accuracy of 86.12% for training and 82.66% for testing and speed compared\nto the other present state-of-the-art algorithm providing mAP of 66.10% and Giga FLOPs as 94.13.\nProposed system can be used in offices, hospitals, educational institutes, law enforcement, surveillance,\netc. to provide a bridge between mute and normal persons. As the architecture has a limitation that\nsometimes the higher appearance features are missing in the recorded video leading to motion blur.\nIn the future, the availability of resources will make it possible to improve the quality but apprehend\ndelicate appearance and motion information using some memory constrictions environment. As well\nas, as a future direction we can expand the dataset, we can contribute to handle the blur or low feature\nframes in video frame selection and segmentation domains, we also has a future dimension to extract\nface as Region of Interest (ROI) and after extracting the features of facial gestures and simultaneously\n536 CMC, 2023, vol.74, no.1\nfrom the other body or hand movement information, applying features fusion to achieve best accuracy\nas well as efficiency.\nContributions: Conceptualization, S.J., and S.R.; Methodology, S.J., and S.R.; Software, S.J.; Vali-\ndation, S.J. and S.R.; Formal Analysis, S.J.; Investigation, S.J. and S.R.; Resources, S.J. and S.R.;\nData Curation, S.J.; Writing—original draft preparation, S.J.; Writing—review and editing, S.R.;\nVisualization, S.J.; Supervision, S.R.\nAcknowledgement: We acknowledge the contribution of organizations and people who participated\nin PkSLMNM ingenuity as participants, organizers, or evaluators.\nFunding Statement:The authors received no specific funding for this study.\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nReferences\n[1] W . H. Organization, “World report on hearing,” 2021. [Online]. Available: https://www.who.int/\npublications/i/item/world-report-on-hearing.\n[2] M. Jebali, A. Dakhli and M. Jemni, “Vision-based continuous sign language recognition using multimodal\nsensor fusion,”Evolving Systems, vol. 12, no. 4, pp. 1031–1044, 2021.\n[3] A. Mehrabian, “Communication without words,”Communication Theory, vol. 6, pp. 193–200, 2008.\n[4] P . Kumar, P . P . Roy and D. P . Dogra, “Independent Bayesian classifier combination based sign language\nrecognition using facial expression,”Information Sciences, vol. 428, pp. 30–48, 2018.\n[5] M. Deriche, S. O. Aliyu and M. Mohandes, “An intelligent arabic sign language recognition system using\na pair of LMCs with GMM based classification,”IEEE Sensors Journal, vol. 19, no. 18, pp. 8067–8078,\n2019.\n[6] R. Elakkiya, “Machine learning based sign language recognition: A review and its research frontier,”\nJournal of Ambient Intelligence and Humanized Computing, vol. 12, no. 7, pp. 7205–7224, 2021.\n[7] R. Rastgoo, K. Kiani and S. Escalera, “Sign language recognition: A deep survey,” Expert System\nApplications, vol. 164, pp. 113794, 2021.\n[8] E. P . da Silva, P . D. P . Costa, K. M. O. Kumada and J. M. De Martino, “Facial action unit detection\nmethodology with application in Brazilian sign language recognition,”Pattern Analysis and Applications,\nvol. 24, pp. 1–17, 2021.\n[9] M. T. Ubaid, T. Saba, H. U. Draz, A. Rehman and H. Kolivand, “Intelligent traffic signal automation\nbased on computer vision techniques using deep learning,”IT Professionals, vol. 24, no. 1, pp. 27–33, 2022.\n[10] Y . Tao, S. Huo and W . Zhou, “Research on communication app for deaf and mute people based on facial\nemotion recognition technology,” in2020 IEEE 2nd Int. Conf. on Civil Aviation Safety and Information\nTechnology (ICCASIT), Weihai, China, pp. 547–552, 2020.\n[11] A. Hassouneh, A. M. Mutawa and M. Murugappan, “Development of a real-time emotion recognition\nsystem using facial expressions and EEG based on machine learning and deep neural network methods,”\nInformatics in Medicine Unlocked, vol. 20, pp. 100372, 2020.\n[12] S. T. Ly, G. -S. Lee, S. -H. Kim and H. -J. Yang, “Emotion recognition via body gesture: Deep learning\nmodel coupled with keyframe selection,” inProc. of the 2018 Int. Conf. on Machine Learning and Machine\nIntelligence, Ha Noi, Viet Nam, pp. 27–31, 2018.\n[13] C. M. A. Ilyas, R. Nunes, K. Nasrollahi, M. Rehm and T. B. Moeslund, “Deep emotion recognition through\nupper body movements and facial expression,” in16th Int. Joint Conf. on Computer Vision, Imaging and\nComputer Graphics Theory and Applications VISIGRAPP (5: VISAPP), Setúbal, Portugal, pp. 669–679,\n2021.\nCMC, 2023, vol.74, no.1 537\n[14] A. Kratimenos, G. Pavlakos and P . Maragos, “Independent sign language recognition with 3D body, hands,\nand face reconstruction,” inICASSP 2021–2021 IEEE Int. Conf. on Acoustics, Speech and Signal Processing\n(ICASSP), Koya, Iraq, pp. 4270–4274, 2021.\n[15] M. Jebali, P . Dalle and M. Jemni, “Sign language recognition system based on prediction in human-\ncomputer interaction,” inInt. Conf. on Human-Computer Interaction, Heraklion, Crete, pp. 565–570, 2014.\n[16] A. Boukdir, M. Benaddy, A. Ellahyani, O. E. Meslouhi and M. Kardouchi, “3D gesture segmentation\nfor word-level arabic sign language using large-scale RGB video sequences and autoencoder convolutional\nnetworks,” Signal Image Video Processing, vol. 16, pp. 1–8, 2022.\n[17] C. Szegedy, W . Liu, Y . Jia, P . Sermanet, S. Reedet al.,“Going deeper with convolutions,” inProc. of the\nIEEE Conf. on Computer Vision and Pattern Recognition, Hong Kong, China, pp. 1–9, 2015.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Joneset al.,“Attention is all you need,”Advances in\nNeural Information Processing Systems, vol. 20, pp. 5998–6008, 2017.\n[19] N. Parmar, V . Ashish, U. Jakob, K. Lukasz, S. Noamet al.“Image Transformer.” in35th Int. Conf. on\nMachine Learning, Stockholm, Sweden, pp. 4055–4064. PMLR, 2018.\n[20] P . Ramachandran, B. Zoph and Q. V . Le, “Searching for activation functions,” ArXiv Preprint\nArXiv171005941, 2017.\n[21] “Deaf reach schools and training centers in Pakistan”, 2022. [Online]. Available:https://www.deafreach.\ncom/.\n[22] “PSL: Pakistan sign language”, 2022. [Online]. Available:https://psl.org.pk/.\n[23] A. Imran, A. Razzaq, I. A. Baig, A. Hussain, S. Shahidet al.,“Dataset of Pakistan sign language and\nautomatic recognition of hand configuration of urdu alphabet through machine learning,”Data in Brief,\nvol. 36, pp. 107021, 2021.\n[24] “Pakistan sign language dataset-Open Data Pakistan”, 2022. [Online]. Available:https://opendata.com.pk/\ndataset/pakistan-sign-language-dataset.\n[25] H. Zahid, M. Rashid, S. Hussain, F . Azim, S. A. Syedet al., “Recognition of urdu sign language: A\nsystematic review of the machine learning classification,”PeerJ Computer Science, vol. 8, pp. e883, 2022.\n[26] S. Javaid, “PkSLMNM: Pakistan sign language manual and non-manual gestures dataset,” 2022, [Online].\nAvailable:https://data.mendeley.com/datasets/m3m9924p3v/1(https://doi.org/10.17632/m3m9924p3v.1.).\n[27] A. J. Piergiovanni and M. S. Ryoo, “Learning latent super-events to detect multiple activities in videos,” in\nProc. of the IEEE Conf. on Computer Vision and Pattern Recognition, Salt Lake City, Utah, pp. 5304–5313,\n2018.\n[28] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucicet al.,“Vivit: A video vision transformer,” in2021\nIEEE/CVF Int. Conf. on Computer Vision (ICCV), Montreal, QC, Canada, pp. 6816–6826, 2021.\n[29] H. Fan, B. Xiong, K. Mangalam, Y . Li, Z. Yanet al.,“Multiscale vision transformers,” in2021 IEEE/CVF\nInt. Conf. on Computer Vision (ICCV), Montreal, QC, Canada, pp. 6804–6815, 2021.\n[30] A. J. Piergiovanni and M. Ryoo, “Temporal Gaussian mixture layer for videos,” inInt. Conf. on Machine\nLearning, long beach, California, pp. 5152–5161, 2019.\n[31] D. Neimark, O. Bar, M. Zohar and D. Asselmann, “Video transformer network,” inIEEE/CVF Int. Conf.\non Computer Vision Workshops (ICCVW), Montreal, BC, Canada, pp. 3156–3165, 2021.\n[32] P . Ghosh, Y . Yao, L. Davis and A. Divakaran, “Stacked spatio-temporal graph convolutional networks\nfor action segmentation,” inProc. of the IEEE/CVF Winter Conf. on Applications of Computer Vision,\nSnowmass Village, CO, USA, pp. 576–585, 2020.",
  "topic": "Gesture",
  "concepts": [
    {
      "name": "Gesture",
      "score": 0.891014575958252
    },
    {
      "name": "Sign language",
      "score": 0.7834926843643188
    },
    {
      "name": "Computer science",
      "score": 0.7646915912628174
    },
    {
      "name": "Facial expression",
      "score": 0.6272992491722107
    },
    {
      "name": "Transformer",
      "score": 0.559986412525177
    },
    {
      "name": "Speech recognition",
      "score": 0.5389829874038696
    },
    {
      "name": "Gesture recognition",
      "score": 0.4788122773170471
    },
    {
      "name": "Modalities",
      "score": 0.4551030695438385
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4222263693809509
    },
    {
      "name": "Engineering",
      "score": 0.08675196766853333
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "cited_by": 8
}