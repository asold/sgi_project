{
  "title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
  "url": "https://openalex.org/W4406233449",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2555732910",
      "name": "Zihan Song",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": null,
      "name": "Gyo-Yeob Hwang",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2002306827",
      "name": "Xin Zhang",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2035454408",
      "name": "Shan Huang",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2114628472",
      "name": "Byung-Kwon Park",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2555732910",
      "name": "Zihan Song",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": null,
      "name": "Gyo-Yeob Hwang",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2002306827",
      "name": "Xin Zhang",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2035454408",
      "name": "Shan Huang",
      "affiliations": [
        "Dong-A University"
      ]
    },
    {
      "id": "https://openalex.org/A2114628472",
      "name": "Byung-Kwon Park",
      "affiliations": [
        "Dong-A University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3203912530",
    "https://openalex.org/W2593758073",
    "https://openalex.org/W4376224599",
    "https://openalex.org/W2894170875",
    "https://openalex.org/W4281672228",
    "https://openalex.org/W2398936787",
    "https://openalex.org/W2037524964",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4256320997",
    "https://openalex.org/W2757092139",
    "https://openalex.org/W4392002118",
    "https://openalex.org/W4378469337",
    "https://openalex.org/W4385565111",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W4376850363",
    "https://openalex.org/W4366999773",
    "https://openalex.org/W4389523765",
    "https://openalex.org/W4385572482",
    "https://openalex.org/W4292356932",
    "https://openalex.org/W4389520387",
    "https://openalex.org/W4392489035",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4400529373",
    "https://openalex.org/W4385571189",
    "https://openalex.org/W4388747974",
    "https://openalex.org/W6860710830",
    "https://openalex.org/W4385963839",
    "https://openalex.org/W4378945542",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W6853859572",
    "https://openalex.org/W3104018737",
    "https://openalex.org/W6852874933",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W2926767350",
    "https://openalex.org/W3101708369"
  ],
  "abstract": null,
  "full_text": "A scientific-article key-insight \nextraction system based on multi-\nactor of fine-tuned open-source \nlarge language models\nZihan Song, Gyo-Yeob Hwang, Xin Zhang, Shan Huang & Byung-Kwon Park\nThe exponential growth of scientific articles has presented challenges in information organization \nand extraction. Automation is urgently needed to streamline literature reviews and enhance insight \nextraction. We explore the potential of Large Language Models (LLMs) in key-insights extraction from \nscientific articles, including OpenAI’s GPT-4.0, MistralAI’s Mixtral 8 × 7B, 01AI’s Yi, and InternLM’s \nInternLM2. We have developed an article-level key-insight extraction system based on LLMs, calling \nit ArticleLLM. After evaluating the LLMs against manual benchmarks, we have enhanced their \nperformance through fine-tuning. We propose a multi-actor LLM approach, merging the strengths \nof multiple fine-tuned LLMs to improve overall key-insight extraction performance. This work \ndemonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness \nof cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and \nknowledge discovery.\nKeywords Key-insight extraction, Large Language Model fine-tuning, Automatic information extraction\n*Correspondence: bpark@dau.ac.kr; Tel.: +82-10-3254-9260.\nIntroduction\nThe exponential growth of scientific articles has engendered a complexity in organizing, acquiring, and \namalgamating academic information. According to the investigation by Bornmann et al. 1, the overall growth \nrate of scientific articles stands at 4.1%, doubling every 17 years. The immediate increase of scientific articles \nhas accelerated information overload, hindered the discovery of new insights, and contributed to the potential \nspread of false information. Although most scientific articles are published in a structured text format to speed \nup understanding of knowledge, the basic content of these articles remains unstructured text. This means that \nthe literature review is still a time-consuming task that requires manual involvement2. Therefore, it is necessary \nto assist the literature review process by automatically extracting key information from scientific articles.\nThe automation of key information extraction can be categorized into two classes: metadata extraction and \nkey-insights extraction 3. Metadata extraction encompasses retrieving fundamental attributes from scientific \narticles, including title, author names, publication year, publishing entity, abstract, and other pertinent \nfoundational details. Researchers and digital repositories use metadata to determine the relevance of specific \narticles to their fields of interest or to facilitate search and filtering tasks. The key-insight extraction is a summary \nof the content of the article, such as the problem to solve, the methodology used, evaluation methods, results, \nlimitations, and the future work. Automatic retrieval of those insights will provide researchers with clear and \nconcise concepts of research articles and increase the efficiency of literature reviews.\nCompared with metadata extraction, key-insight extraction is more challenging. The scholarly domain has \nreported a high degree of accuracy in metadata extraction 4. However, key-insight extraction does not offer an \nexcellent solution because it covers only the parts of the article. Previous studies relied on machine learning \ntechnology to extract information at the phrase5 or sentence level6. Those approaches have limitations as is that \nthe model struggles to capture complex contexts and semantics at the phrase or sentence level, leading to poor \nperformance in capturing insight. Therefore, another key-insight extraction system is needed to extract key-\ninsights at the section or article level than at the phrase or sentence level. Moreover, the scarcity of annotated \ntraining data, the variations among different domains, and the ongoing evolution of research paradigms impede \nDong-A University, Busan 49315, Republic of Korea. email: bpark@dau.ac.kr\nOPEN\nScientific Reports |         (2025) 15:1608 1| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports\n\nthe effectiveness of those models. To our knowledge, there is no effective solution for automatic extraction of \nkey-insights from scientific articles.\nLarge Language Models (LLMs) such as GPT-4.0 provide possibilities to solve this challenge. LLMs represent \na pioneering advancement in the field of natural language processing, characterized by their colossal neural \nnetwork architectures, comprising billions to trillions of parameters. These LLMs have emerged as a forefront \ntechnology in contemporary artificial intelligence research and application, bearing transformative capabilities \nin text generation, comprehension, and processing. Bubeck et al.7 reported that GPT-4 has reached near-human \nperformance on a variety of natural language tasks. This opens new opportunities to deeply understand article \ncontents and extract key-insights from them. With their powerful contextual understanding and generation \ncapabilities, LLMs may be able to better capture the details of article contents, enabling more accurate extraction \nof key-insights. However, there are no studies evaluating the ability of LLMs in key-insights extraction.\nThis study aims to develop and evaluate an LLM-based system for extracting key insights from scientific \narticles. We explore the effectiveness of various state-of-the-art LLMs and enhance their performance through \nfine-tuning with high-quality datasets. Additionally, we advance their capabilities by constructing a multi-actor \nsystem to further improve performance. Specifically, we first employ OpenAI’s ChatGPT-4.0, MistralAI’s Mixtral \n8 × 7B, 01AI’s Yi, and InternLM’s InternLM2 respectively as a candidate LLM for extracting key-insights from \nscientific articles. As the next step, we evaluate the performance of each LLM on key-insight extraction tasks \nthrough the manual evaluation. We find that the performance of GPT-4.0 is close to human level. However, \nit is too expensive to use GPT-4.0 for key-insights extraction on a large scale. Therefore, we use the output of \nGPT-4 as a label for fine-tuning other open-source LLMs so as to improve their performance. However, the \nperformance of the fine-tuned LLMs still has not reached the peak. Therefore, instead of relying on a single LLM, \nwe present a multi-actor method to merge all the key-insights extracted by multiple fine-tuned open-source \nLLMs, demonstrating an advancement in the quality of key-insights. As a result, it is shown that we can extract \nkey-insights at article level only using the multiple fine-tuned open-source LLMs.\nRelated works\nKey-insight extraction\nKey-insight extraction refers to identifying valuable information for research contained in scientific articles. \nCurrent research extracts key-insights based on sentences 6 or phrases 5. These researchers extract specific \nsentences or phrases as key-insights. Sentence-level key-insight extraction can be viewed as a classification task, \nwhich classifies the sentences into specific classes. Phrase-level key-insight extraction is more concerned with \nextracting phrases, or fragments from the text. Key-insight extraction is mainly based on machine learning \ntechnology, such as Bayesian classifiers 8, Conditional Random Fields 9, Support Vector Machines10, and Deep \nNeural Networks11. Most of those studies are based on extracting key-insights from articles’ abstract. However, \nthose methods have two limitations:\n 1.  Abstract does not necessarily fully represent the key-insights of the article; for example, the limitations of the \nresearch and future research may not exist in the abstract.\n 2.  The extracted information may not be sufficient to capture the true meaning of the article. In many cases, the \nkey-insights of an article require a broader context synthesis based on the textual summary.\nThe extensive understanding of various topics exhibited by LLMs, exemplified by GPT-4, has garnered significant \nattention across the scientific community. Comprehensive evaluations have been conducted to assess GPT-4’s \nperformance across a multitude of natural language processing tasks 12–19. The results indicate that GPT-4’s \nperformance varies across different tasks. For instance, in such tasks as information retrieval 17, information \nextraction12,18, and text summarization7, GPT-4 demonstrates superior performance to traditional models. This \ncould be attributed to GPT-4’s training data encompassing diverse domain knowledge, enabling it to retrieve \nrelevant information effectively from a wide range of languages and document types. However, in the task \nof relation extraction, GPT-4’s performance falls short of benchmark models. Han et al. 14 reported that this \ndiscrepancy might be attributed to GPT-4’s limited understanding of subject-object relationships within relation \nextraction tasks.\nIn contrast to traditional methods that focus on paragraph-level and sentence-level key-insight extraction, an \nadvantage of LLMs is their capacity to perform full-text level key-insight extraction. This task can be considered \na synthesis of text summarization and information extraction. Currently, the use of LLMs for key-insight \nextraction tasks lacks a systematic evaluation. Existing large datasets intended to evaluate LLMs typically assess \ntheir performance in areas such as information extraction 20,21, text summarization22, and QA23. However, the \ntask of key-insight extraction demands that models have the capability to extract and synthesize information \nfrom multiple perspectives. Therefore, the performance of LLMs in information extraction, text summarization, \nand QA cannot be directly equated to their effectiveness in key-insight extraction tasks. Therefore, it is necessary \nto systematically evaluate the performance of LLMs on key-insight extraction tasks.\nFine-tuning of LLMs\nThe key behind achieving high performance of LLMs lies in the two main stages of the training process: (1) \ninitial pre-training on massive text corpora, endowing the LLM with an expansive grasp of linguistic knowledge \nand structure; (2) fine-tuning on specific tasks to adapt to distinct domains and applications. This dual-training \nparadigm equips LLMs with unparalleled adaptability, rendering them instrumental in the modern landscape \nof natural language processing. Since initial pre-training of LLMs requires a large amount of hardware support, \nscholars tend to fine-tune the pre-trained LLMs to adapt to tasks in different fields24.\nScientific Reports |         (2025) 15:1608 2| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\nSupervised fine-tuning refers to the process of taking a pre-trained model and adapting it to a new task \nor dataset by making small adjustments to its parameters 25. Compared with fine-tuning small-scale models, \nfine-tuning techniques for LLMs are more complex because of scalability and hardware performance issues. \nFine-tuning methods for LLMs are often called PEFT (Parameter-Efficient Fine-Tuning). PEFT methods aim to \ntweak only a small fraction of the LLM’s parameters, thereby mitigating computational and memory costs. This \napproach allows for the efficient adaptation of LLMs to specific tasks without the need for extensive resources, \nmaking high-quality LLM personalization more accessible.\nLoRA (Low-Rank Adaptation) 26 is a PEFT technology that optimizes LLMs like GPT-3 by introducing \ntrainable rank decomposition matrices into their architecture, significantly reducing the number of adjusted \nparameters while maintaining or improving LLM performance. As a result, LoRA distinguishes itself by \nproviding an optimal balance between LLM efficiency and performance enhancement. By harnessing the \npower of Low-Rank Adaptation, LoRA enables more nuanced and targeted adjustments to LLMs without the \nsubstantial increase in parameters typically associated with such refinements.\nMulti-actor approaches in LLM\nThe introduction of multi-actors in LLM systems for improving the performance of LLMs leverages the concept \nof ensemble learning by coordinating the efforts of multiple AI actors, each embodying an instance of LLM like \nGPT-4, to act in concert. Through such harmonious interaction, the actors combine their varied knowledge \nand contextual understanding, addressing intricate challenges with a level of efficiency and inventiveness that \nexceeds the scope of any single LLM. This transition from individual to collective AI endeavors symbolizes the \nnotion that the aggregate output of an ensemble of actors far exceeds what they could achieve independently.\nExtensive research has been conducted to enhance the performance of large language models (LLMs) for \nspecialized tasks using multi-actor approaches, yet there is no consensus on the optimal way for these actors to \ncollaborate. For instance, employing Dawid-Skene Model to iteratively optimize weights for each actor proves \nhighly effective for tasks where labels are definite 27. However, the multi-actor nature of LLMs significantly \nincreases computational demand28. To mitigate this, a novel routing architecture for multi-actor LLMs based \non a reward model has been introduced, which presumes each sub-actor’s proficiency in specific tasks 29. This \nmethod improves both efficiency and accuracy by allocating particular tasks to designated expert actors, similar \nin philosophy to the popular MoE (Mixture of Experts)30 model but with greater scalability because it avoids the \nhard-coding of expert models inherent in MoE architecture.\nIn the sphere of practical implementation, the research by Hong et al. 31 serves as a compelling illustration \nof the extraordinary capabilities of multi-actor systems. They have adeptly merged human-inspired Standard \nOperating Procedures (SOPs) with role specialization within an advanced meta-programming architecture, \ndemonstrating how structured cooperation can enhance the performance of LLMs to unprecedented levels. \nFurther, Liang et al.32 have advanced this area by creating a Multi-Agent Debate (MAD) framework, tailor-made \nto navigate the complexities of intricate reasoning challenges that confront LLMs. This framework provides a \nsystematic arena for agents to participate in deliberative debates, thereby boosting the collective intellectual \ncapacity of the ensemble of agents, showcasing the profound influence that well-orchestrated ensemble strategies \ncan have in transcending the limitations of existing AI paradigms.\nArticleLLM\nWe propose a scientific-article key-insight extraction system, called ArticleLLM, using multi-actor of multiple \nfine-tuned open-source LLMs. The key-insights we want to extract are the followings: aim of study, motivation \nof study, problem to solve , method used for solution , evaluation metrics, findings, contributions, limitations, and \nfuture work.\nOpen-source LLMs used for fine-tuning\nCompared to commercially available proprietary LLMs, fine-tuned and locally deployed open-source LLMs \nhold advantages in terms of data security, scalability, and cost-effectiveness 33. Fine-tuning is a pivotal process \nthat enables the LLM to perform better on specific tasks by adjusting the parameters of a pre-trained LLM to fit \nparticular datasets or application contexts34. Fine-tuning is more resource- and time-efficient than training from \nscratch, as it leverages the general knowledge acquired by the pre-trained LLM.\nIn this study, we use MistralAI’s Mixtral, 01AI’s Yi, and InternLM’s InternLM2 ranked by the transformers \nframework of Hugging Face35 as a candidate open-source LLM for fine-tuning as shown in Table 1. These LLMs \nall rank high in performance on AlpacaEval leaderboard 36 and are considered to have high potential for key-\ninsight extraction. Since these LLMs are compatible with the transformers framework of Hugging Face, they can \nbe fine-tuned based on the same set of methods.\nLLM name Type Size LLM version\nYi Open-Source 34-Billiant 01-ai/Yi-34B-Chat\nInternLM2 Open-Source 20-Billiant internlm/internlm2-chat-20b\nMixtral Open-Source 46.7-Billiant mistralai/Mixtral-8 × 7B-Instruct-v0.1\nTable 1. Open-source LLMs used for fine-tuning.\n \nScientific Reports |         (2025) 15:1608 3| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\nFine-tuning algorithm\nWe utilize the instruction fine-tuning37 method to refine LLMs. This method primarily involves targeted fine-\ntuning based on specific instructions atop the foundational language model, thereby enhancing the LLM’s \ncapability to comprehend and respond to user commands or inquiries. We fine-tuning the model using \nInstruction-Response pairs, where the Instructions serve as input data and the Responses are treated as labels. \nDuring the fine-tuning phase, the optimization algorithm modifies the LLM’s parameters by calculating the loss \nfunction between the predicted outputs and the actual labels.\nTo efficiently fine-tune LLMs with minimal performance loss, we employ a 4-bit precision loading approach \nusing the GPTQ algorithm 38. GPTQ aims to significantly reduce the LLMs’ memory and computational \nrequirements by compressing the weight bit representation from 32 bits to just 4 bits. This reduction not only \nminimizes the LLM’s size but also enhances its operational speed by treating weights more discretely.\nDuring the fine-tuning stage, we apply LoRA26 optimization to adjust the pre-trained LLM for specific tasks \nefficiently. LoRA operates by selectively modifying a subset of the LLM’s weights using low-rank matrices, \nmaintaining the original structure while enabling swift fine-tuning. This method allows for precise adjustment \nwithout retraining the full network. We adopt Adamw39 for optimization, which optimizes learning rates based \non gradient moments, ensuring a balance between task-specific performance enhancement and generalizability. \nWe use the default cross-entropy as the loss function because it can effectively handle the multi-class label \nprediction problem and ensure the accuracy of the probability distribution of the model output. We set the \ntrain epoch to 1 to avoid overfitting. The parameters governing the LoRA adaptation and the overall fine-tuning \nprocess are carefully selected as shown in Table 2 to balance between refining the LLM’s performance on specific \ntasks and maintaining its generalized capabilities.\nData set used and preparation for fine-tuning\nThe data set used in this study comes from the PDF files of arXiv 40 public articles having the keyword of \nhealthcare. The academic consensus suggests that the optimal dataset size for fine-tuning LLMs lies between \n1,000 and 10,000 dialogue entries 41,42, most of which consist of dialogue sentences of a few hundred words. \nHowever, given that the article data involved in key-insight extraction tasks are often dozens of times longer \nthan these dialogue entries, there is a significant increase in training time costs. Therefore, we choose to use \n1,000 articles as our training dataset. Among them, following the usual practice, we set the size of training data \nset to 700 articles.\nResearchers have confirmed that long articles can reduce the performance of LLMs 43. To fully exploit the \nperformance of a LLM, the length of the input text must be reduced. Moreover, a long article will require a \nlarge amount of GPU memory during the fine-tuning phase. Therefore, we selected short-length articles whose \nnumber of words are less than 7000 to form the dataset.\nConverting the PDF format of an article into the text format that can be processed by LLMs is a prerequisite \nfor executing subsequent algorithms. We use Tika-python44 to convert PDF format files into string data. Tika-\npython returns the string data contained in the entire PDF file. Since the string data may contain some useless \ncharacters and symbols, we also use Python’s pySBD45 library to filter out them.\nTo fine-tune the open-source LLMs in Table 1, the dataset is organized into the following structure: Instruction \nand Response. Instruction is an order given to the LLM, telling the LLM what information to extract and in what \nformat to return it. We showed in Table 3 the instruction used for extracting key-insights from the article text. \nResponse is the result expected to be returned by the LLM. In this supervised fine-tuning stage, the response \nrepresents key-insights of the article. We used the key-insights generated by GPT-4 as a label for the response.\nMulti-actor of fine-tuned LLMs\nThe multi-actor of the fine-tuned LLMs operates on the independently obtained response of each fine-tuned \nLLM: Mixtral FT, Yi FT, and InternLM2 FT. Each LLM is tasked with extracting key-insights according to the \nstructured instruction as showcased in Table  3. Following the initial extraction phase, each output from each \nLLM is subjected to a synthesis process. For this step, we harness the capability of one of the LLMs as the \ncenterpiece for integrating diverse perspectives and insights generated by the other LLMs.\nSpecifically, we used InternLM2 to summarize the information from the output of each LLM FT using \nthe instructions as shown in Table  4. To ensures that InternLM2 is optimally tailored to effectively integrate \nand summarize the outputs of the LLM FTs, we fine-tuned the InternLM2 using the dataset of 1,000 entities \nrandomly selected from the key-insight outputs of Mixtral FT, Yi FT, InternLM2 FT, and GPT-4. Among them, \nLora Fine-tuning process\nName Value Name Value\nLora rank 8 Optimizer Adamw\nLora alpha 16 Train epoch 1\nLora dropout 0.1 Learning rate 0.0002\nLora attention dimension 64 Loss function Cross entropy\nTask type CAUSAL_LM Maximum gradient norm 0.35\n- - Warmup ratio 0.03\nTable 2. Fine-tuning parameters.\n \nScientific Reports |         (2025) 15:1608 4| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\nthe key-insight sentences extracted by Mixtral FT, Yi FT, and InternLM2 FT are used as input, and the key-\ninsight sentences extracted by GPT-4 are used as labels.\nPerformance evaluation\nEvaluation metrics\nWe use the following three metrics to evaluate the performance of ArticleLLM on the key-insights extraction \ntask:\n• Manual evaluation Manual evaluation is considered the gold standard for key-insight extraction tasks3. How-\never, its high cost limits its application on large datasets. We use manual evaluation to measure the perfor -\nmance of the LLM in a small article dataset. Because the key-insight extraction task involves explicit goals \nand objectives, we use a relevance score to assess how accurately the LLM extracts key-insights. The relevance \nscore ranges from 0 to 1, where 0 indicates ‘completely irrelevant’ and 1, ‘completely relevant’ . Specifically, \nhuman researchers assess whether the key-insights extracted by the LLM comprehensively capture all critical \ninformation points identified in manually annotated references, evaluating the presence or absence of essen-\ntial details or elements. To reduce the subjective errors caused by humans, two researchers independently \nassessed the results and re-evaluated the contradictory parts.\n• GPT-4 score Through carefully designed instructions as shown in Table 5, GPT-4 scores the semantic similar-\nity of the extracted key-insights. The score ranges from 0 to 100, where 0 indicates ‘completely dissimilar’ and \n100, ‘completely similar’ . The key-insights generated by an open source LLM are evaluated by the key-insights \nEvaluate the semantic similarity between the input sentence and the standard sentence provided below. Rate the similarity on a scale from 0 to 100, where 0 means completely dissimilar \nand 100 means identical.\n{\nInput Sentence: [Input Sentence]\nStandard Sentence: [Standard Sentence]\n}\nThe expected JSON structure for your response is as follows:\n{\n“Semantic similarity score”: “”\n}\nTable 5. Instruction for semantic similarity.\n \nPlease combine the three parts of input sentences given below to refine and create a new, more comprehensive one part of sentences.\nThe expected JSON structure for your response is as follows:\n{\n“Summarized sentences”: “[Insert analysis here]”,\n}\nInput Part 1: [Sentences generated by the Mixtral FT]\nInput Part 2: [Sentences generated by the Yi FT]\nInput Part 3: [Sentences generated by the InternLM2 FT]\nTable 4. Instruction for summarizing the key-insights from each fine-tuned LLM.\n \nPlease proceed to conduct a scholarly analysis of the provided research manuscript. Your analysis should encapsulate the core components of the study as delineated in the enumeration \nbelow:\nAim: What is the aim of the study?\nMotivation: What is the motivation of the study?\nQuestions addressed: What question does this study address?\nMethods: What methods does the study use to solve the question?\nEvaluation metrics: What evaluation metrics are used in this study?\nFindings: What does the study find?\nContributions: What are the contributions of this study?\nLimitations: What are the limitations of this study?\nFuture work: What is the future work of this study?\nSubsequently, organize the distilled information into a structured JSON format, omitting any supplementary explanations. The expected JSON structure for your analytical summary is as \nfollows:\n{\n“ Aim”: “[Insert analysis here]”,\n“Motivation”: “[Insert analysis here]”,\n“Questions addressed”: “[Insert analysis here]”,\n“Method”: “[Insert analysis here]”,\n“Evaluation metrics”: “[Insert analysis here]”,\n“Findings”: “[Insert analysis here]”,\n“Contributions”: “[Insert analysis here]”,\n“Limitations”: “[Insert analysis here]”,\n“Future work”: “[Insert analysis here]”\n}\nResearch manuscript: [Research manuscript]\nTable 3. Instruction for extracting key-insights.\n \nScientific Reports |         (2025) 15:1608 5| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\ngenerated by GPT-4. Because, compared to Bleu score, GPT-4 is considered to better capture the deep seman-\ntic similarity between texts, GPT-4 score has been widely used in LLM evaluation46,47.\n• Vector similarity Vector similarity quantifies the semantic similarity between two sentences using the cosine \nsimilarity between their vector representations48. In this study, vector representations of each key-insight are \nproduced using the Sentence Transformer model, all-MiniLM-L6-v249. The similarity scores are normalized \nto a scale ranging from 0 to 100, where 0 indicates ’ completely dissimilar’ and 100 denotes ’ completely similar’ .\nEvaluation results\nFor evaluation test, we used a Linux server with 4 Nvidia TITAN RTX GPUs. The operating system is CentOS7. \nThe CPU is an Intel (R) Xeon (R) Silver 4114 CPU @ 2.20 GHz with 40 cores. We used a total of 300 articles for \ntest dataset. In terms of software, we employ Python’s Transformers library35 as the foundational framework for \nfine-tuning and inference processes. We utilize the PEFT library50 to perform parameter efficient fine-tuning.\nManual performance comparison of LLMs before fine-tuning\nThe manual evaluation results for the key-insight extraction performances of the LLMs before fine-tuning are \nshown in Table  6, using a total of 34 articles for test dataset. GPT-4 demonstrates superior efficacy across all \nkey-insights, achieving an average score of 0.97. This is followed by InternLM2, which exhibits commendable \nperformance with an average score of 0.80, showcasing its potential in extracting key-insights from scientific \nliterature. Conversely, Yi and Mixtral lag slightly behind, with average scores of 0.65 and 0.60, respectively. \nNotably, GPT-4 achieves perfect scores in understanding the aim, motivation, evaluation metrics, and \ncontribution of scientific articles, underscoring its advanced capability in discerning intricate academic content.\nPerformance comparison of fine-tuned LLMs\nThe performance comparison results are presented in Fig.  1, and the numerical values of the results are \nshown in Table  7. With respect to the GPT-4 score after fine-tuning, both Yi FT and Mixtral FT exhibited \nslight improvements, with their average scores increasing from 68.5 to 71.4 and from 49.4 to 51.6, respectively. \nFig. 1. ( a) LLM performance evaluated by GPT-4. (b) LLM performance evaluated by and vector similarity.\n \nGPT-4 Yi InternLM2 Mixtral\nAim 1.0 0.71 0.89 0.55\nMotivation 1.0 0.65 0.90 0.61\nMethods 0.97 0.68 0.88 0.59\nQuestion addressed 0.98 0.73 0.81 0.68\nEvaluation metrics 1.0 0.55 0.65 0.42\nResult 0.97 0.70 0.91 0.65\nLimitations 0.90 0.51 0.61 0.62\nContribution 1.0 0.72 0.85 0.75\nFuture work 0.92 0.68 0.77 0.56\nAverage 0.97 0.65 0.80 0.60\nTable 6. Manual performance evaluation for LLMS before fine-tuning.\n \nScientific Reports |         (2025) 15:1608 6| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\nInternLM2 and its fine-tuned counterpart, InternLM2 FT, stood out for their exceptional performance, with \nInternLM2 FT achieving the highest average score of 77.8. Unlike GPT-4 scores, the vector similarity scores \nshow less variation, suggesting this metric evaluates a different aspect. Specifically, after fine-tuning, Yi FT and \nMixtral FT showed marginal improvements in their scores, shifting from 64.2 to 65.8 and from 63.5 to 63.9 \nrespectively, showcasing slight but noticeable progress. InternLM2 and its fine-tuned version, InternLM2 FT, \ndelivered the highest scores among the individual models at 68.8, indicating a stronger semantic alignment.\nOn the other hand, the underperformance of all open-source LLMs in extracting “Evaluation metrics” and \n“Limitations” can be principally attributed to two specific challenges. First, the task of extracting “Evaluation \nmetrics” often involves identifying multiple distinct indicators such as accuracy, precision, and recall, which \nmay be intricately described within the text. Open-source LLMs may struggle with this multifaceted extraction, \nleading to the omission of certain indicators that are critical for a comprehensive evaluation. Second, the \nextraction of “Limitations” presents its unique set of difficulties, as not all authors explicitly mention limitations \nwithin their articles. This situation forces open-source LLMs to attempt the synthesis of plausible yet not entirely \naccurate limitations, which can deviate significantly from the article’s intended messages. These challenges \nhighlight the nuanced understanding and contextual interpretation required for accurately extracting such \nsophisticated elements from scientific texts, thereby suggesting the need for LLMs to be specifically fine-tuned \nor trained with a focus on recognizing and handling the diverse and complex nature of “Evaluation metrics” and \n“Limitations” in scientific literature.\nThe multi-actor approach consistently outperforms other models in both metrics, though the margin is \nnarrower in vector similarity, suggesting its overall superior linguistic and cognitive capabilities. Specifically, \nthe multi-actor approach effectively leverages the combined strengths of three fine-tuned LLMs (InternLM2 \nFT, Yi FT, and Mixtral FT). This collaborative strategy significantly enhances its ability to extract key-insights \nfrom scientific articles, as illustrated in Fig.  1 which summarizes performance across nine essential categories. \nThis underscores the synergistic effect of leveraging multiple fine-tuned LLMs, which collectively enhance the \nprecision and breadth of extracted key-insights, surpassing the capabilities of individual LLMs. Moreover, the \nlower score of “Limitations” compared to other categories suggests that while multi-actor approach significantly \nimproves performance, identifying areas requiring further refinement remains critical. Collectively, these results \nsolidify the position of multi-actor LLMs as a powerful tool in comprehensively understanding and analyzing \nthe complexities of scientific literature, outperforming singular fine-tuned LLMs in both depth and accuracy of \nextracted information.\nStatistical significance of results\nGiven that the observed performances are so close to each other, we need to see if the observed differences \nare statistically significant. Considering that our data have natural boundaries which may lead to a skewed \ndistribution, we implemented the Wilcoxon Signed-Rank Test 51 to compare the median differences between \nLLMs. This non-parametric method is more appropriate due to the potential non-normality of the data caused \nby the boundaries. Our null hypothesis states that there would be no significant difference in GPT-4 score or \nvector similarity, whereas the alternative hypothesis anticipates a noticeable difference. Supplementary Table 1 \npresents the results of the Wilcoxon Signed-Rank Test for all LLMs. Within the realm of GPT-4 scores, all fine-\nKey-insights Yi Yi FT Mixtral Mixtral FT InternLM2 InternLM2 FT Multi-Actor\nGPT-4 score\nAim 78.1 81.2 58.3 62.0 87.5 91.1 95.7\nMotivation 71.2 74.1 52.1 55.3 79.1 82.5 92.7\nMethods 72.1 77.0 54.3 57.3 81.0 84.7 93.2\nQuestion addressed 73.4 77.4 48.7 52.7 78.2 81.3 91.2\nEvaluation metrics 60.6 62.2 44.4 44.1 64.5 69.4 91.5\nFindings 74.1 79.5 52.7 53.0 79.8 84.5 96.4\nLimitations 46.1 47.2 32.6 32.7 44.5 49.7 66.3\nContribution 75.3 76.6 55.3 58.8 81.1 83.6 91.5\nFuture work 66.0 68.1 46.4 48.6 69.4 73.5 89.7\nAverage 68.5 71.4 49.4 51.6 73.9 77.8 89.8\nVector similarity\nAim 77.6 80.4 78.4 77.2 84.1 86.4 86.1\nMotivation 66.7 68.5 65.2 65.8 71.4 72.6 76.4\nMethods 66.0 69.3 65.5 67.1 72.2 74.4 78.8\nQuestion addressed 68.9 70.0 64.8 65.9 69.9 71.5 76.8\nEvaluation metrics 55.5 56.9 55.2 56.3 57.6 59.2 67.6\nFindings 66.7 68.8 65.3 67.4 69.7 72.1 76.7\nLimitations 47.2 48.4 47.9 46.9 46.5 48.3 55.0\nContribution 71.3 72.2 70.7 71.0 72.6 74.2 80.8\nFuture work 58.0 57.9 58.7 57.1 59.1 60.1 66.9\nAverage 64.2 65.8 63.5 63.9 67.0 68.8 73.9\nTable 7. The numerical results of LLM performance evaluated by GPT-4.\n \nScientific Reports |         (2025) 15:1608 7| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\ntuned models exhibit significant differences in certain key-insights (p < 0.05). InternLM2 FT shows significance \nin aim, methods, question addressed, evaluation metrics, findings, limitations, and future work; Yi FT in aim, \nquestion addressed, and findings; Mixtral FT in aim, motivation, question addressed, and contribution. In \nparticular, the multi-actor approach shows significance in all key-insights.\nHowever, the results of vector similarity differ from those of GPT-4, with no significant statistical differences \nobserved across all key-insights for Mixtral FT and InternLM2 FT. Yi FT shows significance only in aim, methods, \nand findings. This may be due to the fact that vector similarity is less sensitive to semantic variations. Figure 1b \nillustrates this point by demonstrating that the variance of vector similarity between different models is lower, \nresulting in closer curves. Nevertheless, the multi-actor approach shows significance in all key-insights except \naim. This reveals that the multi-actor approach significantly improves the key-insight extraction performance. \nOverall, the results of statistical tests highlight the significant improvements in key-insight extraction made by \nthe multi-actor approach, thereby validating its effectiveness.\nRepeated testing of GPT-4 score\nConsidering the potential variability in GPT-4 score outputs, where identical instructions may yield varying \nresults52, we conducted repeated experiments. As illustrated in Fig.  2, we performed 30 replicates of testing on \nthe GPT-4 scores across all key-insights. The mean of GPT-4 scores for 30 repeated tests is 88.4 ± 0.63. This result \nshows that while there is some variation in GPT-4 scores across repeated tests, this variation has minimal impact \non the overall outcome.\nDiscussion\nThe findings from this study underscore the effectiveness of fine-tuned LLMs for the extraction of key-insights \nfrom scientific articles. Notably, the fine-tuned version of the InternLM2, InternLM2 FT, emerged as a standout \nperformer, evidencing significant performance enhancements post-fine-tuning, thus directly evidencing \nthe benefits of fine-tuning. This enhancement in performance, particularly in terms of understanding and \narticulating critical scientific information, affirms the pivotal role of fine-tuning in optimizing LLMs for \nspecialized academic tasks.\nThe multi-actor of LLMs showcases their superiority in handling complex scientific texts by integrating the \nstrengths of the three individually fine-tuned LLMs—Mixtral FT, Yi FT, and InternLM2 FT—each bringing \ndifferent nuanced insights to the output. This multi-faceted approach ensures a more comprehensive analysis \nthan what any single LLM could achieve. The synthesis process, led by InternLM2 FT, exemplifies a sophisticated \nmethod of consolidating diverse perspectives into a coherent, analytically rich summary. This strategy significantly \nelevates the benchmark for automated text analysis, offering unparalleled precision and depth in extracting key-\ninsights from dense academic literature, and highlights its immense potential for advancing literature review \nand analysis. This development aligns with recent studies emphasizing the cooperation of multiple LLMs in \nimproving the performance of LLM applications across various fields 31,32. It is a kind of collective intelligence, \nwhere diverse sources of information lead to more robust and reliable conclusions, a concept widely supported \nin the interdisciplinary research community.\nDistinct from the existing multi-actor approaches 27–29, the approach for extracting key-insights from \nscientific articles merges the unique viewpoints from disparate models. Consequently, such customary solutions \nFig. 2. Repeat testing for GPT-4 score.\n \nScientific Reports |         (2025) 15:1608 8| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\nas weight updating27 and expert routing 29 prove ineffective for this task. Our approach addresses this by fine-\ntuning a specialized actor tasked with integrating and summarizing information from other actors. While this \nmethod may not achieve the execution efficiency of expert routing techniques, it significantly enhances the \naccuracy of key-insights extraction, ensuring maximal fidelity to the source article.\nThe results of this study also suggest an insight: the inherent abilities of the original LLMs are more important \nthan the enhancements brought about by fine-tuning. Despite the tangible improvements observed by post-fine-\ntuning, the baseline performance of an LLM such as InternLM2 sets a precedential standard for excellence in \nthe domain of key-insight extraction. This denotes that the foundational architecture and pre-training of these \nLLMs may be more pivotal in determining their effectiveness in complex academic tasks, overshadowing the \nincremental gains achieved through fine-tuning. Specifically, the fact that GPT-4 and InternLM2 demonstrated \nsuperior efficacy across various dimensions before fine-tuning, highlights the critical importance of the LLM’s \noriginal design and training corpus in grasping intricate academic content. Therefore, while fine-tuning serves \nas a valuable tool for LLM optimization, the selection of the base LLM is crucial for success in such specialized \napplications as key-insights extraction.\nAn advantage of ArticleLLM is its capability for local deployment, which facilitates the construction of a private \nlarge-scale literature management system. This feature is particularly crucial for applications that emphasize data \nconfidentiality and security. Another benefit of ArticleLLM is its cost-effectiveness. Extracting key-insights from \nan article of approximately 10,000 words using GPT-4 costs about $0.15. This expense can prove substantial \nfor building an extensive private literature management system. In contrast, a locally deployed ArticleLLM \neliminates those concerns. By the way, the approach of multi-actor of LLMs proposed in this study requires \nabout 10 min to extract key-insights from an article under our hardware setup. However, the server used in this \nresearch does not employ NVLink technology but rather relies on PCIe (Peripheral Component Interconnect \nExpress) for data transmission, indicating significant room for improvement in system performance efficiency53.\nCompared with previous research 3,5,8,10 on key-insight extraction, the advantage of ArticleLLM is that \nit can process the entire article to obtain more reasonable results, demonstrating the feasibility of LLMs for \nextracting key-insights from articles. ArticleLLM could revolutionize the way academic databases curate and \npresent information, allowing for a more nuanced and efficient retrieval process. Researchers could benefit from \ncontextually relevant summaries, thereby significantly enhancing their research productivity and knowledge \ndiscovery.\nThere are some limitations worth mentioning. Although the multi-actor system based on multiple fine-tuned \nLLMs has achieved excellent performance, the restriction to articles under 7,000 words due to GPU memory \nlimits and the focus on healthcare-related articles from arXiv may affect the generalizability of our findings. \nAdditionally, the execution efficiency of the system is still a concern due to the collaboration of multiple LLMs \ninvolved. The extraction of key-insights such as “limitations” that may not exist in the article or appear in an \nambiguous form requires further research. Compared to the arXiv papers used in this study, more rigorous \npeer-reviewed papers may avoid this problem. On the other hand, due to the hardware limitations, this study \nemployed 4-bit quantization to fine-tune the LLMs, inevitably leading to a performance loss.\nConclusion\nIn this paper, it is shown that we can extract key-insights from scientific articles only using open-source LLMs. \nOur findings affirm the critical role of fine-tuning in enhancing the proficiency of LLMs for extracting key-\ninsights, with InternLM2 FT showcasing remarkable improvements after fine-tuning. In addition, the multi-\nactor of LLMs, incorporating diverse perspectives from individually fine-tuned LLMs, significantly broadens \nthe scope and accuracy of resulting key-insights. However, the inherent architecture and pre-training of LLMs \nseem to be more influential than fine-tuning in determining their efficacy for extracting key-insights Looking \nforward, ArticleLLM can present a promising avenue for enhancing academic research productivity, though such \nchallenges as execution efficiency of ensemble systems warrant further exploration to optimize the utilization of \nLLMs in scholarly applications.\nData availability\nThe datasets generated during and/or analysed during the current study are available from the corresponding \nauthor on reasonable request.\nReceived: 1 April 2024; Accepted: 6 January 2025\nReferences\n 1. Bornmann, L., Haunschild, R. & Mutz, R. Growth rates of modern science: a latent piecewise growth curve approach to model \npublication numbers from established and new literature databases. Humanit. Soc. Sci. Commun. 8, 224 (2021).\n 2. Borah, R., Brown, A. W ., Capers, P . L. & Kaiser, K. A. Analysis of the time and workers needed to conduct systematic reviews of \nmedical interventions using data from the PROSPERO registry. BMJ Open. 7, e012545 (2017).\n 3. Nasar, Z., Jaffry, S. W . & Malik, M. K. Information extraction from scientific articles: a survey. Scientometrics 117, 1931–1990 \n(2018).\n 4. Boukhers, Z. & Bouabdallah, A. Vision and natural language for metadata extraction from scientific PDF documents. In Proceedings \nof the 22nd ACM/IEEE Joint Conference on Digital Libraries 1–5. https://doi.org/10.1145/3529372.3533295 (ACM, 2022).\n 5. Tateisi, Y ., Ohta, T., Miyao, Y ., Pyysalo, S. & Aizawa, A. Typed entity and relation annotation on computer science papers. In \nProceedings of the 10th International Conference on Language Resources and Evaluation. 3836–3843 (2016).\n 6. Kovačević, A., Konjović, Z., Milosavljević, B. & Nenadic, G. Mining methodologies from NLP publications: a case study in \nautomatic terminology recognition. Comput. Speech Lang. 26, 105–126 (2012).\nScientific Reports |         (2025) 15:1608 9| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\n 7. Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. Preprint at http://arxiv.org/abs/2303.12712 \n(2023).\n 8. Lakhanpal, S., Gupta, A. K. & Agrawal, R. Towards Extracting Domains from Research Publications. In Midwest Artificial \nIntelligence and Cognitive Science Conference. https://api.semanticscholar.org/CorpusID:5220521 (2015).\n 9. Hirohata, K., Okazaki, N., Ananiadou, S. & Ishizuka, M. Identifying Sections in Scientific Abstracts using Conditional Random \nFields. In Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I.  h t t p s : / / a c l a n t h o l o g y . o r \ng / I 0 8 - 1 0 5 0 /      (2008).\n 10. Ronzano, F . & Saggion, H. Dr. Inventor Framework: Extracting Structured Information from Scientific Publications. In International \nConference on Discovery Science (eds. Japkowicz, N. & Matwin, S.) vol. 9356, 209–220. https://doi.org/10.1007/978-3-319-24282-8  \n(Springer International Publishing, 2015).\n 11. He, H. et al. An Insight Extraction System on BioMedical Literature with Deep Neural Networks. In Proceedings of the 2017 \nConference on Empirical Methods in Natural Language Processing 2691–2701. https://doi.org/10.18653/v1/D17-1285 (Association \nfor Computational Linguistics, Stroudsburg, PA, USA, 2017).\n 12. Polak, M. P . & Morgan, D. Extracting Accurate Materials Data from Research Papers with Conversational Language Models and \nPrompt Engineering. Preprint at http://arxiv.org/abs/2303.05352 (2023).\n 13. Huang, J. & Tan, M. The role of ChatGPT in scientific communication: writing better scientific review articles. Am. J. Cancer Res. \n13, 1148–1154 (2023).\n 14. Han, R. et al. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and \nErrors. Preprint at http://arxiv.org/abs/2305.14450 (2023).\n 15. Yuan, C., Xie, Q. & Ananiadou, S. Zero-shot Temporal Relation Extraction with ChatGPT. Preprint at  h t t p : / / a r x i v . o r g / a b s / 2 3 0 4 . 0 \n5 4 5 4     (2023).\n 16. Wang, S., Scells, H., Koopman, B. & Zuccon, G. Can ChatGPT Write a Good Boolean Query for Systematic Review Literature \nSearch? Preprint at http://arxiv.org/abs/2302.03495 (2023).\n 17. Zhang, J., Chen, Y ., Niu, N., Wang, Y . & Liu, C. Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under \nZero-Shot Setting. Preprint at http://arxiv.org/abs/2304.12562 (2023).\n 18. Li, B. et al. Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, \nand Faithfulness. Preprint at http://arxiv.org/abs/2304.11633 (2023).\n 19. Sun, W . et al. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. Preprint at  h t t p : / / a r x i v . o r g \n/ a b s / 2 3 0 4 . 0 9 5 4 2     (2023).\n 20. Jahan, I., Laskar, M. T. R., Peng, C. & Huang, J. Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-\nTuned Generative Transformers. Preprint at http://arxiv.org/abs/2306.04504 (2023).\n 21. Kempf, S., Krug, M. & Puppe, F . K. I. E. T. A. Key-insight extraction from scientific tables. Appl. Intell. 53, 9513–9530 (2023).\n 22. Laban, P . et al. SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization. In Proceedings of \nthe 2023 Conference on Empirical Methods in Natural Language Processing 9662–9676.  h t t p s : / / a c l a n t h o l o g y . o r g / 2 0 2 3 . e m n l p - m a i n . \n6 0 0 /     (Association for Computational Linguistics, 2023).\n 23. Cai, H. et al. SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis. Preprint at http://arxiv.org/abs/2403.01976 \n(2024).\n 24. Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint at http://arxiv.org/abs/2307.09288 (2023).\n 25. Howard, J. & Ruder, S. Universal Language Model Fine-tuning for Text Classification. Preprint at http://arxiv.org/abs/1801.06146 \n(2018).\n 26. Hu, E. et al. Lora: Low-Rank Adaptation of Large Language Models. Preprint at https://arxiv.org/abs/2106.09685 (2021).\n 27. Fang, C. et al. LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value \nExtraction. Preprint at http://arxiv.org/abs/2403.00863 (2024).\n 28. Jiang, D., Ren, X. & Lin, B. Y . LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. \nPreprint at http://arxiv.org/abs/2306.02561 (2023).\n 29. Lu, K. et al. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. Preprint at  h t t p : / / a r x i v . o r g / a b s / \n2 3 1 1 . 0 8 6 9 2     (2023).\n 30. Jiang, A. Q. et al. Mixtral of Experts. Preprint at http://arxiv.org/abs/2401.04088 (2024).\n 31. Hong, S. et al. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. Preprint at  h t t p : / / a r x i v . o r g / a b s / 2 3 0 8 . 0 \n0 3 5 2     (2023).\n 32. Liang, T. et al. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Preprint at  h t t p : / / a r x i v . o \nr g / a b s / 2 3 0 5 . 1 9 1 1 8     (2023).\n 33. Chen, H. et al. ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up? Preprint at  h t t p : / / a r x i v \n. o r g / a b s / 2 3 1 1 . 1 6 9 8 9     (2023).\n 34. Huang, H., Qu, Y ., Liu, J., Y ang, M. & Zhao, T. An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge \nModels are Task-specific Classifiers. Preprint at http://arxiv.org/abs/2403.02839 (2024).\n 35. Wolf, T. et al. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. Preprint at  h t t p : / / a r x i v . o r g / a b s / 1 9 1 0 . 0 3 \n7 7 1     (2019).\n 36. Xuechen L. et al. AlpacaEval: An Automatic Evaluator of Instruction-following Models.  https://github.com/tatsu-lab/alpaca_eval \n(2023).\n 37. Chung, H. W . et al. Scaling Instruction-Finetuned Language Models. Preprint at http://arxiv.org/abs/2210.11416 (2022).\n 38. Frantar, E., Ashkboos, S., Hoefler, T. & Alistarh, D. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained \nTransformers. Preprint at https://arxiv.org/abs/2210.17323 (2022).\n 39. Ilya L. & Frank H. Decoupled Weight Decay Regularization. Preprint at https://arxiv.org/abs/1711.05101 (2019).\n 40. Paul Ginsparg. ArXiv. https://arxiv.org/ (1991).\n 41. Bakker, M. A. et al. Fine-tuning language models to find agreement among humans with diverse preferences. Preprint at  h t t p s : / / a \nr x i v . o r g / a b s / 2 2 1 1 . 1 5 0 0 6     (2022).\n 42. Hu, Z. et al. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. Preprint at  h t t p : / \n/ a r x i v . o r g / a b s / 2 3 0 4 . 0 1 9 3 3     (2023).\n 43. Liu, N. F . et al. Lost in the Middle: How Language Models Use Long Contexts. Preprint at http://arxiv.org/abs/2307.03172 (2023).\n 44. Chris A. M. et al. Tika-Python. https://github.com/chrismattmann/tika-python (2014).\n 45. Sadvilkar, N. & Neumann, M. PySBD: Pragmatic Sentence Boundary Disambiguation. Preprint at http://arxiv.org/abs/2010.09657 \n(2020).\n 46. Zheng, L. et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Preprint at http://arxiv.org/abs/2306.05685 (2023).\n 47. Gao, M., Hu, X., Ruan, J., Pu, X. & Wan, X. LLM-based NLG Evaluation: Current Status and Challenges. Preprint at  h t t p : / / a r x i v . o \nr g / a b s / 2 4 0 2 . 0 1 3 8 3     (2024).\n 48. Farouk, M. Measuring Sentences Similarity: A Survey. Preprint at http://arxiv.org/abs/1910.03940 (2019).\n 49. Reimers, N. & Gurevych, I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Preprint at  h t t p : / / a r x i v . o r g / a b \ns / 1 9 0 8 . 1 0 0 8 4     (2019).\n 50. Mangrulkar, S. et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. https://github.com/huggingface/peft \n(2022).\n 51. Wilcoxon, F . Individual comparisons by ranking methods. Biometrics Bull. 1, 80 (1945).\nScientific Reports |         (2025) 15:1608 10| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/\n 52. Kocoń, J. et al. Jack of all trades, master of none. Inf. Fusion. 99, 101861 (2023).\n 53. Li, A. et al. Evaluating Modern PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect. IEEE Trans. Parallel Distrib. Syst. 31, 94–110 \n(2020).\nAuthor contributions\nZ.S. wrote the main manuscript text. G.-Y .H. and B.-K.P . contributed to the manuscript by performing modifi-\ncations and proofreading. S.H. and X.Z. provided analysis and discussion on the results. All authors reviewed \nthe manuscript.\nFunding\nThis work was supported by the Dong-A University research fund.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 8 5 7 1 5 - 7     .  \nCorrespondence and requests for materials should be addressed to B.-K.P .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |         (2025) 15:1608 11| https://doi.org/10.1038/s41598-025-85715-7\nwww.nature.com/scientificreports/",
  "topic": "Key (lock)",
  "concepts": [
    {
      "name": "Key (lock)",
      "score": 0.8126156330108643
    },
    {
      "name": "Computer science",
      "score": 0.6312105655670166
    },
    {
      "name": "Data science",
      "score": 0.4905681014060974
    },
    {
      "name": "Automation",
      "score": 0.4199680685997009
    },
    {
      "name": "Work (physics)",
      "score": 0.41201210021972656
    },
    {
      "name": "Computer security",
      "score": 0.13297873735427856
    },
    {
      "name": "Engineering",
      "score": 0.1303102672100067
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}