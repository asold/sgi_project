{
    "title": "Taming Transformers for High-Resolution Image Synthesis",
    "url": "https://openalex.org/W3111551570",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2195991605",
            "name": "Patrick Esser",
            "affiliations": [
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A3023131083",
            "name": "Robin Rombach",
            "affiliations": [
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2269923714",
            "name": "Bjorn Ommer",
            "affiliations": [
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2195991605",
            "name": "Patrick Esser",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3023131083",
            "name": "Robin Rombach",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2269923714",
            "name": "Bjorn Ommer",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6724670942",
        "https://openalex.org/W2962785568",
        "https://openalex.org/W3034723751",
        "https://openalex.org/W6767480857",
        "https://openalex.org/W2981721547",
        "https://openalex.org/W6752910514",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W6693848384",
        "https://openalex.org/W6640963894",
        "https://openalex.org/W6780593937",
        "https://openalex.org/W6779093361",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6745560452",
        "https://openalex.org/W2963800363",
        "https://openalex.org/W6763239785",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W6762780510",
        "https://openalex.org/W6625168331",
        "https://openalex.org/W6784347278",
        "https://openalex.org/W6782908428",
        "https://openalex.org/W3025973238",
        "https://openalex.org/W6753059488",
        "https://openalex.org/W6772853553",
        "https://openalex.org/W6702130928",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W6767264202",
        "https://openalex.org/W3035687950",
        "https://openalex.org/W6767627451",
        "https://openalex.org/W6785102375",
        "https://openalex.org/W3108329879",
        "https://openalex.org/W6639732818",
        "https://openalex.org/W2955639361",
        "https://openalex.org/W6747491877",
        "https://openalex.org/W6786494455",
        "https://openalex.org/W6748148878",
        "https://openalex.org/W6758800702",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6692550842",
        "https://openalex.org/W3034776267",
        "https://openalex.org/W2922386270",
        "https://openalex.org/W6755312952",
        "https://openalex.org/W6690026940",
        "https://openalex.org/W2561196672",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W3034625979",
        "https://openalex.org/W2963522749",
        "https://openalex.org/W2962974533",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W6756533288",
        "https://openalex.org/W2471768434",
        "https://openalex.org/W2171108400",
        "https://openalex.org/W2507296351",
        "https://openalex.org/W2242818861",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3099188186",
        "https://openalex.org/W2963799213",
        "https://openalex.org/W3106570356",
        "https://openalex.org/W2962820504",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2971074500",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W3081167590",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W3118605064",
        "https://openalex.org/W2970718407",
        "https://openalex.org/W2963174698",
        "https://openalex.org/W2971480596",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2893749619",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W3083914148",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W2962897886",
        "https://openalex.org/W2949847915",
        "https://openalex.org/W3176823897",
        "https://openalex.org/W2902630600",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3104876213",
        "https://openalex.org/W2267126114",
        "https://openalex.org/W3162926177",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2269752429",
        "https://openalex.org/W2970315999",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2348664362",
        "https://openalex.org/W3092033429",
        "https://openalex.org/W2962760235",
        "https://openalex.org/W967544008",
        "https://openalex.org/W3103781353",
        "https://openalex.org/W2945924057",
        "https://openalex.org/W2963428348",
        "https://openalex.org/W2881214865",
        "https://openalex.org/W2963139417",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2994760783",
        "https://openalex.org/W3098510582"
    ],
    "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .",
    "full_text": "Taming Transformers for High-Resolution Image Synthesis\nPatrick Esser* Robin Rombach* Bj¨orn Ommer\nHeidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany\n*Both authors contributed equally to this work\nFigure 1. Our approach enables transformers to synthesize high-resolution images like this one, which contains 1280x460 pixels.\nAbstract\nDesigned to learn long-range interactions on sequential\ndata, transformers continue to show state-of-the-art results\non a wide variety of tasks. In contrast to CNNs, they contain\nno inductive bias that prioritizes local interactions. This\nmakes them expressive, but also computationally infeasi-\nble for long sequences, such as high-resolution images. We\ndemonstrate how combining the effectiveness of the induc-\ntive bias of CNNs with the expressivity of transformers en-\nables them to model and thereby synthesize high-resolution\nimages. We show how to (i) use CNNs to learn a context-\nrich vocabulary of image constituents, and in turn (ii) utilize\ntransformers to efﬁciently model their composition within\nhigh-resolution images. Our approach is readily applied\nto conditional synthesis tasks, where both non-spatial in-\nformation, such as object classes, and spatial information,\nsuch as segmentations, can control the generated image.\nIn particular, we present the ﬁrst results on semantically-\nguided synthesis of megapixel images with transformers and\nobtain the state of the art among autoregressive models on\nclass-conditional ImageNet. Code and pretrained models\ncan be found at https://git.io/JnyvK.\n1. Introduction\nTransformers are on the rise—they are now the de-facto\nstandard architecture for language tasks [74, 57, 58, 5]\nand are increasingly adapted in other areas such as audio\n[12] and vision [8, 16]. In contrast to the predominant vi-\nsion architecture, convolutional neural networks (CNNs),\nthe transformer architecture contains no built-in inductive\nprior on the locality of interactions and is therefore free\nto learn complex relationships among its inputs. However,\nthis generality also implies that it has to learn all relation-\nships, whereas CNNs have been designed to exploit prior\nknowledge about strong local correlations within images.\nThus, the increased expressivity of transformers comes with\nquadratically increasing computational costs, because all\npairwise interactions are taken into account. The result-\ning energy and time requirements of state-of-the-art trans-\nformer models thus pose fundamental problems for scaling\nthem to high-resolution images with millions of pixels.\nObservations that transformers tend to learn convolu-\ntional structures [16] thus beg the question: Do we have\nto re-learn everything we know about the local structure\nand regularity of images from scratch each time we train\na vision model, or can we efﬁciently encode inductive im-\nage biases while still retaining the ﬂexibility of transform-\ners? We hypothesize that low-level image structure is well\ndescribed by a local connectivity, i.e. a convolutional ar-\nchitecture, whereas this structural assumption ceases to be\neffective on higher semantic levels. Moreover, CNNs not\nonly exhibit a strong locality bias, but also a bias towards\nspatial invariance through the use of shared weights across\n1\narXiv:2012.09841v3  [cs.CV]  23 Jun 2021\nall positions. This makes them ineffective if a more holistic\nunderstanding of the input is required.\nOur key insight to obtain an effective and expressive\nmodel is that, taken together, convolutional and transformer\narchitectures can model the compositional nature of our vi-\nsual world [51] : We use a convolutional approach to efﬁ-\nciently learn a codebook of context-rich visual parts and,\nsubsequently, learn a model of their global compositions.\nThe long-range interactions within these compositions re-\nquire an expressive transformer architecture to model distri-\nbutions over their consituent visual parts. Furthermore, we\nutilize an adversarial approach to ensure that the dictionary\nof local parts captures perceptually important local struc-\nture to alleviate the need for modeling low-level statistics\nwith the transformer architecture. Allowing transformers to\nconcentrate on their unique strength—modeling long-range\nrelations—enables them to generate high-resolution images\nas in Fig. 1, a feat which previously has been out of reach.\nOur formulationgives control over the generated images by\nmeans of conditioning information regarding desired object\nclasses or spatial layouts. Finally, experiments demonstrate\nthat our approach retains the advantages of transformers by\noutperforming previous codebook-based state-of-the-art ap-\nproaches based on convolutional architectures.\n2. Related Work\nThe Transformer Family The deﬁning characteristic of\nthe transformer architecture [74] is that it models interac-\ntions between its inputs solely through attention [2, 36, 52]\nwhich enables them to faithfully handle interactions be-\ntween inputs regardless of their relative position to one an-\nother. Originally applied to language tasks, inputs to the\ntransformer were given by tokens, but other signals, such as\nthose obtained from audio [41] or images [8], can be used.\nEach layer of the transformer then consists of an attention\nmechanism, which allows for interaction between inputs at\ndifferent positions, followed by a position-wise fully con-\nnected network, which is applied to all positions indepen-\ndently. More speciﬁcally, the (self-)attention mechanism\ncan be described by mapping an intermediate representa-\ntion with three position-wise linear layers into three repre-\nsentations, query Q∈RN×dk , key K ∈RN×dk and value\nV ∈RN×dv , to compute the output as\nAttn(Q,K,V ) = softmax\n(QKt\n√dk\n)\nV ∈RN×dv . (1)\nWhen performing autoregressive maximum-likelihood\nlearning, non-causal entries of QKt, i.e. all entries be-\nlow its diagonal, are set to −∞and the ﬁnal output of the\ntransformer is given after a linear, point-wise transforma-\ntion to predict logits of the next sequence element. Since\nthe attention mechanism relies on the computation of inner\nproducts between all pairs of elements in the sequence, its\ncomputational complexity increases quadratically with the\nsequence length. While the ability to consider interactions\nbetween all elements is the reason transformers efﬁciently\nlearn long-range interactions, it is also the reason transform-\ners quickly become infeasible, especially on images, where\nthe sequence length itself scales quadratically with the res-\nolution. Different approaches have been proposed to reduce\nthe computational requirements to make transformers fea-\nsible for longer sequences. [55] and [76] restrict the recep-\ntive ﬁelds of the attention modules, which reduces the ex-\npressivity and, especially for high-resolution images, intro-\nduces assumptions on the independence of pixels. [12] and\n[26] retain the full receptive ﬁeld but can reduce costs for\na sequence of length nonly from n2 to n√n, which makes\nresolutions beyond 64 pixels still prohibitively expensive.\nConvolutional Approaches The two-dimensional struc-\nture of images suggests that local interactions are particu-\nlarly important. CNNs exploit this structure by restricting\ninteractions between input variables to a local neighborhood\ndeﬁned by the kernel size of the convolutional kernel. Ap-\nplying a kernel thus results in costs that scale linearly with\nthe overall sequence length (the number of pixels in the case\nof images) and quadratically in the kernel size, which, in\nmodern CNN architectures, is often ﬁxed to a small constant\nsuch as 3 ×3. This inductive bias towards local interactions\nthus leads to efﬁcient computations, but the wide range of\nspecialized layers which are introduced into CNNs to han-\ndle different synthesis tasks [53, 80, 68, 85, 84] suggest that\nthis bias is often too restrictive.\nConvolutional architectures have been used for autore-\ngressive modeling of images [70, 71, 10] but, for low-\nresolution images, previous works [55, 12, 26] demon-\nstrated that transformers consistently outperform their con-\nvolutional counterparts. Our approach allows us to ef-\nﬁciently model high-resolution images with transformers\nwhile retaining their advantages over state-of-the-art con-\nvolutional approaches.\nTwo-Stage Approaches Closest to ours are two-stage ap-\nproaches which ﬁrst learn an encoding of data and after-\nwards learn, in a second stage, a probabilistic model of this\nencoding. [13] demonstrated both theoretical and empirical\nevidence on the advantages of ﬁrst learning a data repre-\nsentation with a Variational Autoencoder (V AE) [38, 62],\nand then again learning its distribution with a V AE. [18, 78]\ndemonstrate similar gains when using an unconditional nor-\nmalizing ﬂow for the second stage, and [63, 64] when using\na conditional normalizing ﬂow. To improve training efﬁ-\nciency of Generative Adversarial Networks (GANs), [43]\nlearns a GAN [20] on representations of an autoencoder and\n[21] on low-resolution wavelet coefﬁcients which are then\n2\nFigure 2. Our approach uses a convolutional VQGAN to learn a codebook of context-rich visual parts, whose composition is subsequently\nmodeled with an autoregressive transformer architecture. A discrete codebook provides the interface between these architectures and a\npatch-based discriminator enables strong compression while retaining high perceptual quality. This method introduces the efﬁciency of\nconvolutional approaches to transformer based high resolution image synthesis.\ndecoded to images with a learned generator.\n[72] presents the Vector Quantised Variational Autoen-\ncoder (VQV AE), an approach to learn discrete represen-\ntations of images, and models their distribution autore-\ngressively with a convolutional architecture. [61] extends\nthis approach to use a hierarchy of learned representations.\nHowever, these methods still rely on convolutional density\nestimation, which makes it difﬁcult to capture long-range\ninteractions in high-resolution images. [8] models images\nautoregressively with transformers in order to evaluate the\nsuitability of generative pretraining to learn image repre-\nsentations for downstream tasks. Since input resolutions of\n32 ×32 pixels are still quite computationally expensive [8],\na VQV AE is used to encode images up to a resolution of\n192 ×192. In an effort to keep the learned discrete repre-\nsentation as spatially invariant as possible with respect to\nthe pixels, a shallow VQV AE with small receptive ﬁeld is\nemployed. In contrast, we demonstrate that a powerful ﬁrst\nstage, which captures as much context as possible in the\nlearned representation, is critical to enable efﬁcient high-\nresolution image synthesis with transformers.\n3. Approach\nOur goal is to exploit the highly promising learning ca-\npabilities of transformer models [74] and introduce them to\nhigh-resolution image synthesis up to the megapixel range.\nPrevious work [55, 8] which applied transformers to image\ngeneration demonstrated promising results for images up to\na size of 64 ×64 pixels but, due to the quadratically in-\ncreasing cost in sequence length, cannot simply be scaled\nto higher resolutions.\nHigh-resolution image synthesis requires a model that\nunderstands the global composition of images, enabling it to\ngenerate locally realistic as well as globally consistent pat-\nterns. Therefore, instead of representing an image with pix-\nels, we represent it as a composition of perceptually rich im-\nage constituents from a codebook. By learning an effective\ncode, as described in Sec. 3.1, we can signiﬁcantly reduce\nthe description length of compositions, which allows us to\nefﬁciently model their global interrelations within images\nwith a transformer architecture as described in Sec. 3.2.\nThis approach, summarized in Fig. 2, is able to generate\nrealistic and consistent high resolution images both in an\nunconditional and a conditional setting.\n3.1. Learning an Effective Codebook of Image Con-\nstituents for Use in Transformers\nTo utilize the highly expressive transformer architecture for\nimage synthesis, we need to express the constituents of an\nimage in the form of asequence. Instead of building on indi-\nvidual pixels, complexity necessitates an approach that uses\na discrete codebook of learned representations, such that\nany image x ∈RH×W×3 can be represented by a spatial\ncollection of codebook entries zq ∈Rh×w×nz , where nz is\nthe dimensionality of codes. An equivalent representation\nis a sequence of h·windices which specify the respective\nentries in the learned codebook. To effectively learn such\na discrete spatial codebook, we propose to directly incor-\nporate the inductive biases of CNNs and incorporate ideas\nfrom neural discrete representation learning [72]. First, we\nlearn a convolutional model consisting of an encoderEand\na decoder G, such that taken together, they learn to repre-\nsent images with codes from a learned, discrete codebook\nZ= {zk}K\nk=1 ⊂Rnz (see Fig. 2 for an overview). More\n3\nprecisely, we approximate a given image xby ˆx = G(zq).\nWe obtain zq using the encoding ˆz = E(x) ∈Rh×w×nz\nand a subsequent element-wise quantization q(·) of each\nspatial code ˆzij ∈Rnz onto its closest codebook entry zk:\nzq = q(ˆz) :=\n(\narg min\nzk∈Z\n∥ˆzij −zk∥\n)\n∈Rh×w×nz . (2)\nThe reconstruction ˆx≈xis then given by\nˆx= G(zq) =G(q(E(x))) . (3)\nBackpropagation through the non-differentiable quantiza-\ntion operation in Eq. (3) is achieved by a straight-through\ngradient estimator, which simply copies the gradients from\nthe decoder to the encoder [3], such that the model and\ncodebook can be trained end-to-end via the loss function\nLVQ(E,G, Z) =∥x−ˆx∥2 + ∥sg[E(x)] −zq∥2\n2\n+ ∥sg[zq] −E(x)∥2\n2. (4)\nHere, Lrec = ∥x−ˆx∥2 is a reconstruction loss, sg[·] denotes\nthe stop-gradient operation, and ∥sg[zq]−E(x)∥2\n2 is the so-\ncalled “commitment loss” [72].\nLearning a Perceptually Rich Codebook Using trans-\nformers to represent images as a distribution over latent im-\nage constituents requires us to push the limits of compres-\nsion and learn a rich codebook. To do so, we propose VQ-\nGAN, a variant of the original VQV AE, and use a discrimi-\nnator and perceptual loss [40, 30, 39, 17, 47] to keep good\nperceptual quality at increased compression rate. Note that\nthis is in contrast to previous works which applied pixel-\nbased [71, 61] and transformer-based autoregressive mod-\nels [8] on top of only a shallow quantization model. More\nspeciﬁcally, we replace the L2 loss used in [72] for Lrec by\na perceptual loss and introduce an adversarial training pro-\ncedure with a patch-based discriminator D[28] that aims to\ndifferentiate between real and reconstructed images:\nLGAN({E,G, Z},D) = [logD(x) + log(1−D(ˆx))] (5)\nThe complete objective for ﬁnding the optimal compression\nmodel Q∗= {E∗,G∗,Z∗}then reads\nQ∗= arg min\nE,G,Z\nmax\nD\nEx∼p(x)\n[\nLVQ(E,G, Z)\n+λLGAN({E,G, Z},D)\n]\n, (6)\nwhere we compute the adaptive weight λaccording to\nλ= ∇GL[Lrec]\n∇GL[LGAN] +δ (7)\nwhere Lrec is the perceptual reconstruction loss [81],∇GL[·]\ndenotes the gradient of its input w.r.t. the last layer L of\nthe decoder, and δ = 10−6 is used for numerical stability.\nTo aggregate context from everywhere, we apply a single\nattention layer on the lowest resolution. This training pro-\ncedure signiﬁcantly reduces the sequence length when un-\nrolling the latent code and thereby enables the application\nof powerful transformer models.\n3.2. Learning the Composition of Images with\nTransformers\nLatent Transformers With E and G available, we can\nnow represent images in terms of the codebook-indices of\ntheir encodings. More precisely, the quantized encoding of\nan image x is given by zq = q(E(x)) ∈Rh×w×nz and\nis equivalent to a sequence s ∈{0,..., |Z|−1}h×w of in-\ndices from the codebook, which is obtained by replacing\neach code by its index in the codebook Z:\nsij = ksuch that (zq)ij = zk. (8)\nBy mapping indices of a sequence s back to their corre-\nsponding codebook entries, zq =\n(\nzsij\n)\nis readily recov-\nered and decoded to an image ˆx= G(zq).\nThus, after choosing some ordering of the indices in\ns, image-generation can be formulated as autoregressive\nnext-index prediction: Given indices s<i, the transformer\nlearns to predict the distribution of possible next indices,\ni.e. p(si|s<i) to compute the likelihood of the full repre-\nsentation as p(s) =∏\ni p(si|s<i). This allows us to directly\nmaximize the log-likelihood of the data representations:\nLTransformer = Ex∼p(x) [−log p(s)] . (9)\nConditioned Synthesis In many image synthesis tasks a\nuser demands control over the generation process by provid-\ning additional information from which an example shall be\nsynthesized. This information, which we will call c, could\nbe a single label describing the overall image class or even\nanother image itself. The task is then to learn the likelihood\nof the sequence given this information c:\np(s|c) =\n∏\ni\np(si|s<i,c). (10)\nIf the conditioning information chas spatial extent, we ﬁrst\nlearn another VQGAN to obtain again an index-based rep-\nresentation r ∈{0,..., |Zc|−1}hc×wc with the newly ob-\ntained codebook Zc Due to the autoregressive structure of\nthe transformer, we can then simply prepend r to s and\nrestrict the computation of the negative log-likelihood to\nentries p(si|s<i,r). This “decoder-only” strategy has also\nbeen successfully used for text-summarization tasks [44].\nGenerating High-Resolution Images The attention\nmechanism of the transformer puts limits on the sequence\n4\nFigure 3. Sliding attention window.\nlength h·wof its inputs s. While we can adapt the number\nof downsampling blocks m of our VQGAN to reduce\nimages of size H ×W to h = H/2m ×w = W/2m, we\nobserve degradation of the reconstruction quality beyond\na critical value of m, which depends on the considered\ndataset. To generate images in the megapixel regime, we\ntherefore have to work patch-wise and crop images to\nrestrict the length of sto a maximally feasible size during\ntraining. To sample images, we then use the transformer\nin a sliding-window manner as illustrated in Fig. 3. Our\nVQGAN ensures that the available context is still sufﬁcient\nto faithfully model images, as long as either the statistics of\nthe dataset are approximately spatially invariant or spatial\nconditioning information is available. In practice, this is\nnot a restrictive requirement, because when it is violated,\ni.e. unconditional image synthesis on aligned data, we can\nsimply condition on image coordinates, similar to [42].\n4. Experiments\nThis section evaluates the ability of our approach to re-\ntain the advantages of transformers over their convolutional\ncounterparts (Sec. 4.1) while integrating the effectiveness\nof convolutional architectures to enable high-resolution im-\nage synthesis (Sec. 4.2). Furthermore, in Sec. 4.3, we in-\nvestigate how codebook quality affects our approach. We\nclose the analysis by providing a quantitative comparison\nto a wide range of existing approches for generative im-\nage synthesis in Sec. 4.4. Based on initial experiments, we\nusually set |Z|= 1024and train all subsequent transformer\nmodels to predict sequences of length 16 ·16, as this is the\nmaximum feasible length to train a GPT2-medium architec-\nture (307 M parameters) [58] on a GPU with 12GB VRAM.\nMore details on architectures and hyperparameters can be\nfound in the appendix (Tab. 7 and Tab. 8).\n4.1. Attention Is All You Need in the Latent Space\nTransformers show state-of-the-art results on a wide va-\nriety of tasks, including autoregressive image modeling.\nHowever, evaluations of previous works were limited to\ntransformers working directly on (low-resolution) pixels\n[55, 12, 26], or to deliberately shallow pixel encodings [8].\nThis raises the question if our approach retains the advan-\ntages of transformers over convolutional approaches.\nTo answer this question, we use a variety of conditional\nand unconditional tasks and compare the performance be-\ntween our transformer-based approach and a convolutional\napproach. For each task, we train a VQGAN with m = 4\ndownsampling blocks, and, if needed, another one for the\nNegative Log-Likelihood (NLL)\nData /\n# params\nTransformer\nP-SNAIL steps\nTransformer\nP-SNAIL time\nPixelSNAIL\nﬁxed time\nRIN / 85M 4.78 4.84 4.96\nLSUN-CT / 310M 4.63 4.69 4.89\nIN / 310M 4.78 4.83 4.96\nD-RIN / 180 M 4.70 4.78 4.88\nS-FLCKR / 310 M 4.49 4.57 4.64\nTable 1. Comparing Transformer and PixelSNAIL architectures\nacross different datasets and model sizes. For all settings, trans-\nformers outperform the state-of-the-art model from the PixelCNN\nfamily, PixelSNAIL in terms of NLL. This holds both when com-\nparing NLL at ﬁxed times (PixelSNAIL trains roughly 2 times\nfaster) and when trained for a ﬁxed number of steps. See Sec. 4.1\nfor the abbreviations.\nconditioning information, and then train both a transformer\nand a PixelSNAIL [10] model on the same representations,\nas the latter has been used in previous state-of-the-art two-\nstage approaches [61]. For a thorough comparison, we vary\nthe model capacities between 85M and 310M parameters\nand adjust the number of layers in each model to match one\nanother. We observe that PixelSNAIL trains roughly twice\nas fast as the transformer and thus, for a fair comparison,\nreport the negative log-likelihood both for the same amount\nof training time (P-SNAIL time) and for the same amount of\ntraining steps (P-SNAIL steps).\nResults Tab. 1 reports results for unconditional image\nmodeling on ImageNet (IN) [14], Restricted ImageNet\n(RIN) [65], consisting of a subset of animal classes from\nImageNet, LSUN Churches and Towers (LSUN-CT) [79],\nand for conditional image modeling of RIN conditioned on\ndepth maps obtained with the approach of [60] (D-RIN) and\nof landscape images collected from Flickr conditioned on\nsemantic layouts (S-FLCKR) obtained with the approach\nof [7]. Note that for the semantic layouts, we train the\nﬁrst-stage using a cross-entropy reconstruction loss due to\ntheir discrete nature. The results shows that the transformer\nconsistently outperforms PixelSNAIL across all tasks when\ntrained for the same amount of time and the gap increases\neven further when trained for the same number of steps.\nThese results demonstrate that gains of transformers carry\nover to our proposed two-stage setting.\n4.2. A Uniﬁed Model for Image Synthesis Tasks\nThe versatility and generality of the transformer architec-\nture makes it a promising candidate for image synthesis. In\nthe conditional case, additional information csuch as class\nlabels or segmentation maps are used and the goal is to learn\nthe distribution of images as described in Eq. (10). Using\nthe same setting as in Sec. 4.1 (i.e. image size 256 ×256,\nlatent size 16 ×16), we perform various conditional image\nsynthesis experiments:\n5\nconditioning samples\nFigure 4. Transformers within our setting unify a wide range of\nimage synthesis tasks. We show 256 ×256 synthesis results\nacross different conditioning inputs and datasets, all obtained with\nthe same approach to exploit inductive biases of effective CNN\nbased VQGAN architectures in combination with the expressiv-\nity of transformer architectures. Top row: Completions from un-\nconditional training on ImageNet. 2nd row: Depth-to-Image on\nRIN. 3rd row: Semantically guided synthesis on ADE20K. 4th\nrow: Pose-guided person generation on DeepFashion. Bottom\nrow: Class-conditional samples on RIN.\n(i): Semantic image synthesis , where we condition on\nsemantic segmentation masks of ADE20K [83], a web-\nscraped landscapes dataset (S-FLCKR) and COCO-Stuff\n[6]. Results are depicted in Figure 4, 5 and Fig. 6.\n(ii): Structure-to-image, where we use either depth or edge\ninformation to synthesize images from both RIN and IN\n(see Sec. 4.1). The resulting depth-to-image and edge-to-\nimage translations are visualized in Fig. 4 and Fig. 6.\n(iii): Pose-guided synthesis: Instead of using the semanti-\ncally rich information of either segmentation or depth maps,\nFig. 4 shows that the same approach as for the previous ex-\nperiments can be used to build a shape-conditional genera-\ntive model on the DeepFashion [45] dataset.\n(iv): Stochastic superresolution, where low-resolution im-\nages serve as the conditioning information and are thereby\nupsampled. We train our model for an upsampling factor of\n8 on ImageNet and show results in Fig. 6.\n(v): Class-conditional image synthesis: Here, the condi-\ntioning information cis a single index describing the class\nlabel of interest. Results for the RIN and IN dataset are\ndemonstrated in Fig. 4 and Fig. 8, respectively.\nAll of these examples make use of the same methodology.\nInstead of requiring task speciﬁc architectures or modules,\nthe ﬂexibility of the transformer allows us to learn appropri-\nate interactions for each task, while the VQGAN — which\ncan be reused across different tasks — leads to short se-\nquence lengths. In combination, the presented approach can\nbe understood as an efﬁcient, general purpose mechanism\nfor conditional image synthesis. Note that additional results\nfor each experiment can be found in the appendix, Sec. D.\nHigh-Resolution Synthesis The sliding window ap-\nproach introduced in Sec. 3.2 enables image synthesis be-\nyond a resolution of 256 ×256 pixels. We evaluate this\napproach on unconditional image generation on LSUN-CT\nand FacesHQ (see Sec. 4.3) and conditional synthesis on D-\nRIN, COCO-Stuff and S-FLCKR, where we show results\nin Fig. 1, 6 and the supplementary (Fig. 29-39). Note that\nthis approach can in principle be used to generate images\nof arbitrary ratio and size, given that the image statistics\nof the dataset of interest are approximately spatially invari-\nant or spatial information is available. Impressive results\ncan be achieved by applying this method to image genera-\ntion from semantic layouts on S-FLCKR, where a strong\nVQGAN can be learned with m = 5, so that its code-\nbook together with the conditioning information provides\nthe transformer with enough context for image generation\nin the megapixel regime.\n4.3. Building Context-Rich Vocabularies\nHow important are context-rich vocabularies? To inves-\ntigate this question, we ran experiments where the trans-\nformer architecture is kept ﬁxed while the amount of con-\ntext encoded into the representation of the ﬁrst stage is var-\nied through the number of downsampling blocks of ourVQ-\nGAN. We specify the amount of context encoded in terms\nof reduction factor in the side-length between image in-\nputs and the resulting representations, i.e. a ﬁrst stage en-\ncoding images of size H ×W into discrete codes of size\nH/f ×W/f is denoted by a factor f. For f = 1, we re-\nproduce the approach of [8] and replace our VQGAN by a\nk-means clustering of RGB values with k= 512.\nDuring training, we always crop images to obtain inputs of\nsize 16 ×16 for the transformer, i.e. when modeling im-\nages with a factor f in the ﬁrst stage, we use crops of size\n16f ×16f. To sample from the models, we always apply\nthem in a sliding window manner as described in Sec. 3.\nResults Fig. 7 shows results for unconditional synthesis of\nfaces on FacesHQ, the combination of CelebA-HQ [31] and\n6\nFigure 5. Samples generated from semantic layouts on S-FLCKR.\nSizes from top-to-bottom: 1280 ×832, 1024 ×416 and 1280 ×\n240 pixels. Best viewed zoomed in. A larger visualization can be\nfound in the appendix, see Fig 29.\nFFHQ [33]. It clearly demonstrates the beneﬁts of power-\nful VQGANs by increasing the effective receptive ﬁeld of\nthe transformer. For small receptive ﬁelds, or equivalently\nsmall f, the model cannot capture coherent structures. For\nan intermediate value of f = 8, the overall structure of\nimages can be approximated, but inconsistencies of facial\nfeatures such as a half-bearded face and of viewpoints in\ndifferent parts of the image arise. Only our full setting of\nf = 16can synthesize high-ﬁdelity samples. For analogous\nresults in the conditional setting on S-FLCKR, we refer to\nthe appendix (Fig. 13 and Sec. C).\nTo assess the effectiveness of our approach quantitatively,\nwe compare results between training a transformer directly\non pixels, and training it on top of a VQGAN’s latent code\nwith f = 2, given a ﬁxed computational budget. Again, we\nfollow [8] and learn a dictionary of 512 RGB values on CI-\nFAR10 to operate directly on pixel space and train the same\ntransformer architecture on top of ourVQGAN with a latent\ncode of size 16 ×16 = 256. We observe improvements of\n18.63% for FIDs and 14.08×faster sampling of images.\nFigure 6. Applying the sliding attention window approach (Fig. 3)\nto various conditional image synthesis tasks. Top: Depth-to-image\non RIN, 2nd row: Stochastic superresolution on IN, 3rd and 4th\nrow: Semantic synthesis on S-FLCKR, bottom: Edge-guided syn-\nthesis on IN. The resulting images vary between 368 ×496 and\n1024 ×576, hence they are best viewed zoomed in.\nDataset ours SPADE [53] Pix2PixHD (+aug) [75] CRN [9]\nCOCO-Stuff 22.4 22.6/23.9(*) 111.5 (54.2) 70.4\nADE20K 35.5 33.9/35.7(*) 81.8 (41.5) 73.3\nTable 2. FID score comparison for semantic image synthesis\n(256 ×256 pixels). (*): Recalculated with our evaluation protocol\nbased on [50] on the validation splits of each dataset.\n4.4. Benchmarking Image Synthesis Results\nIn this section we investigate how our approach quantita-\ntively compares to existing models for generative image\nsynthesis. In particular, we assess the performance of our\nmodel in terms of FID and compare to a variety of estab-\nlished models (GANs, V AEs, Flows, AR, Hybrid). The\nresults on semantic synthesis are shown in Tab. 2, where\nwe compare to [53, 75, 35, 9], and the results on uncon-\nditional face synthesis are shown in Tab. 3. While some\ntask-specialized GAN models report better FID scores, our\napproach provides a uniﬁed model that works well across\na wide range of tasks while retaining the ability to encode\nand reconstruct images. It thereby bridges the gap between\npurely adversarial and likelihood-based approaches.\n7\nf1 f2 f8 f16 downsampling factor\n1.0 3.86 65.81 280.68 speed-up\nFigure 7. Evaluating the importance of effective codebook for HQ-Faces (CelebA-HQ and FFHQ) for a ﬁxed sequence length|s|= 16·16 =\n256. Globally consistent structures can only be modeled with a context-rich vocabulary (right). All samples are generated with temperature\nt = 1.0 and top-k sampling with k = 100. Last row reports the speedup over the f1 baseline which operates directly on pixels and takes\n7258 seconds to produce a sample on a NVIDIA GeForce GTX Titan X.\nCelebA-HQ256×256 FFHQ256×256\nMethod FID ↓ Method FID ↓\nGLOW [37] 69.0 VDV AE ( t = 0.7) [11] 38.8\nNV AE [69] 40.3 VDV AE ( t = 1.0) 33.5\nPIONEER (B.) [23] 39.2 (25.3) VDV AE ( t = 0.8) 29.8\nNCPV AE [1] 24.8 VDV AE ( t = 0.9) 28.5\nV AEBM [77] 20.4 VQGAN+P.SNAIL 21.9\nStyle ALAE [56] 19.2 BigGAN 12.4\nDC-V AE [54] 15.8 ours(k=300) 9.6\nours(k=400) 10.2 U-Net GAN (+aug) [66] 10.9 (7.6)\nPGGAN [31] 8.0 StyleGAN2 (+aug) [34] 3.8 (3.6)\nTable 3. FID score comparison for face image synthesis. CelebA-\nHQ results reproduced from [1, 54, 77, 24], FFHQ from [66, 32].\nAutoregressive models are typically sampled with a decod-\ning strategy [27] such as beam-search, top-k or nucleus\nsampling. For most of our results, including those in Tab. 2,\nwe use top-k sampling with k = 100 unless stated other-\nwise. For the results on face synthesis in Tab. 3, we com-\nputed scores for k ∈{100,200,300,400,500}and report\nthe best results, obtained with k= 400for CelebA-HQ and\nk = 300 for FFHQ. Fig. 10 in the supplementary shows\nFID and Inception scores as a function of k.\nClass-Conditional Synthesis on ImageNet To address a\ndirect comparison with the previous state-of-the-art for au-\ntoregressive modeling of class-conditional image synthesis\non ImageNet, VQV AE-2 [61], we train a class-conditional\nImageNet transformer on 256 ×256 images, using a VQ-\nGAN with dim Z = 16384 and f = 16, and addition-\nally compare to BigGAN [4], IDDPM [49], DCTransformer\n[48] and ADM [15] in Tab. 4. Note that our model uses\n≃10×less parameters than VQV AE-2, which has an esti-\nmated parameter count of 13.5B (estimate based on [67]).\nSamples of this model for different ImageNet classes are\nshown in Fig. 8. We observe that the adversarial training\nof the corresponding VQGAN enables sampling of high-\nquality images with realistic textures, of comparable or\nhigher quality than existing approaches such as BigGAN\nand VQV AE-2, see also Fig. 14-17 in the supplementary.\nModel acceptance rate FID IS\nmixed k, p = 1.0 1.0 17.04 70.6 ±1.8\nk = 973, p = 1.0 1.0 29.20 47.3 ±1.3\nk = 250, p = 1.0 1.0 15.98 78.6 ±1.1\nk = 973, p = 0.88 1.0 15.78 74.3 ±1.8\nk = 600, p = 1.0 0.05 5.20 280.3 ±5.5\nmixed k, p = 1.0 0.5 10.26 125.5 ±2.4\nmixed k, p = 1.0 0.25 7.35 188.6 ±3.3\nmixed k, p = 1.0 0.05 5.88 304.8 ±3.6\nmixed k, p = 1.0 0.005 6.59 402.7 ±2.9\nDCTransformer [48] 1.0 36.5 n/a\nVQV AE-2 [61] 1.0 ∼31 ∼45\nVQV AE-2 n/a ∼10 ∼330\nBigGAN [4] 1.0 7.53 168.6 ±2.5\nBigGAN-deep 1.0 6.84 203.6 ±2.6\nIDDPM [49] 1.0 12.3 n/a\nADM-G, no guid. [15] 1.0 10.94 100.98\nADM-G, 1.0 guid. 1.0 4.59 186.7\nADM-G, 10.0 guid. 1.0 9.11 283.92\nval. data 1.0 1.62 234.0 ±3.9\nTable 4. FID score comparison for class-conditional synthesis\non 256 ×256 ImageNet, evaluated between 50k samples and the\ntraining split. Classiﬁer-based rejection sampling as in VQV AE-2\nuses a ResNet-101 [22] classiﬁer. BigGAN(-deep) evaluated via\nhttps://tfhub.dev/deepmind truncated at 1.0. “Mixed”\nk refers to samples generated with different top-k values, herek ∈\n{100, 200, 250, 300, 350, 400, 500, 600, 800, 973}.\nQuantitative results are summarized in Tab. 4. We report\nFID and Inception Scores for the best k/p in top-k/top-p\nsampling. Following [61], we can further increase quality\nvia classiﬁer-rejection, which keeps only the best m-out-\nof-nsamples in terms of the classiﬁer’s score, i.e. with an\nacceptance rate ofm/n. We use a ResNet-101 classiﬁer [22].\nWe observe that our model outperforms other autoregres-\nsive approaches (VQV AE-2, DCTransformer) in terms of\nFID and IS, surpasses BigGAN and IDDPM even for low\nrejection rates and yields scores close to the state of the art\nfor higher rejection rates, see also Fig. 9.\nHow good is theVQGAN? Reconstruction FIDs obtained\nvia the codebook provide an estimate on the achievable FID\nof the generative model trained on it. To quantify the per-\n8\nFigure 8. Samples from our class-conditional ImageNet model trained on 256 ×256 images.\nFigure 9. FID and Inception Score as a function of top-k, nucleus and rejection ﬁltering.\nModel Codebook Size dim Z FID/val FID/train\nVQV AE-2 64 ×64 & 32 ×32 512 n/a ∼10\nDALL-E [59] 32 ×32 8192 32.01 33.88\nVQGAN 16 ×16 1024 7.94 10.54\nVQGAN 16 ×16 16384 4.98 7.41\nVQGAN∗ 32 ×32 8192 1.49 3.24\nVQGAN 64 ×64 & 32 ×32 512 1.45 2.78\nTable 5. FID on ImageNet between reconstructed validation split\nand original validation (FID/val) and training (FID/train) splits.\n∗trained with Gumbel-Softmax reparameterization as in [59, 29].\nformance gains of our VQGAN over discrete V AEs trained\nwithout perceptual and adversarial losses ( e.g. VQV AE-2,\nDALL-E [59]), we evaluate this metric on ImageNet and\nreport results in Tab. 5. Our VQGAN outperforms non-\nadversarial models while providing signiﬁcantly more com-\npression (seq. length of 256 vs. 5120 = 322 + 642 for\nVQV AE-2,256 vs 1024 for DALL-E). As expected, larger\nversions of VQGAN (either in terms of larger codebook\nsizes or increased code lengths) further improve perfor-\nmance. Using the same hierarchical codebook setting as in\nVQV AE-2 with our model provides the best reconstruction\nFID, albeit at the cost of a very long and thus impractical\nsequence. The qualitative comparison corresponding to the\nresults in Tab. 5 can be found in Fig. 12.\n5. Conclusion\nThis paper adressed the fundamental challenges that previ-\nously conﬁned transformers to low-resolution images. We\nproposed an approach which represents images as a compo-\nsition of perceptually rich image constituents and thereby\novercomes the infeasible quadratic complexity when mod-\neling images directly in pixel space. Modeling constituents\nwith a CNN architecture and their compositions with a\ntransformer architecture taps into the full potential of their\ncomplementary strengths and thereby allowed us to rep-\nresent the ﬁrst results on high-resolution image synthesis\nwith a transformer-based architecture. In experiments, our\napproach demonstrates the efﬁciency of convolutional in-\nductive biases and the expressivity of transformers by syn-\nthesizing images in the megapixel range and outperforming\nstate-of-the-art convolutional approaches. Equipped with a\ngeneral mechanism for conditional synthesis, it offers many\nopportunities for novel neural rendering approaches.\nThis work has been supported by the German Research Foundation\n(DFG) projects 371923335, 421703927 and a hardware donation from\nNVIDIA corporation.\n9\nTaming Transformers for High-Resolution\nImage Synthesis\n–\nSupplementary Material\nThe supplementary material for our workTaming Transformers for High-Resolution Image Synthesisis structured as follows:\nFirst, Sec. A summarizes changes to a previous version of this paper. In Sec. B, we present hyperparameters and architectures\nwhich were used to train our models. Next, extending the discussion of Sec. 4.3, Sec. C presents additional evidence for the\nimportance of perceptually rich codebooks and its interpretation as a trade-off between reconstruction ﬁdelity and sampling\ncapability. Additional results on high-resolution image synthesis for a wide range of tasks are then presented in Sec. D, and\nSec. E shows nearest neighbors of samples. Finally, Sec. F contains results regarding the ordering of image representations.\nA. Changelog\nWe summarize changes between this version 1 of the paper and its previous version 2.\nIn the previous version, Eq. (4) had a weighting term βon the commitment loss, and Tab. 8 reported a value of β = 0.25\nfor all models. However, due to a bug in the implementation,βwas never used and all models have been trained withβ = 1.0.\nThus, we removed βin Eq. (4).\nWe updated class-conditional synthesis results on ImageNet in Sec. 4.4. The previous results, included here in Tab. 6\nfor completeness, were based on a slightly different implementation where the transformer did not predict the distribution\nof the ﬁrst token but used a histogram for it. The new model has been trained for 2.4 million steps with a batch size of\n16 accumulated over 8 batches, which took 45.8 days on a single A100 GPU. The previous model had been trained for\n1.0 million steps. Furthermore, the FID values were based on 50k (18k) samples against 50k (18k) training examples (to\ncompare with MSP). For better comparison with other works, the current version reports FIDs based on 50k samples against\nall training examples of ImageNet using torch-fidelity [50]. We updated all qualitative ﬁgures showing samples from\nthis model and added visualizations of the effect of tuning top-k/por rejection rate in Fig. 14-26.\nTo provide a better overview, we also include results from works that became available after the previous version of our\nwork. Speciﬁcally, we include results on reconstruction quality of the VQV AE from [59] in Tab. 5 and Fig. 12 (which replaces\nthe previous qualitative comparison), and results on class-conditional ImageNet sampling from [49, 48, 15] in Tab. 4. Note\nthat with the exception of BigGAN and BigGAN-deep [4], no models or sampling results are available for the methods we\ncompare to in Tab. 4. Thus, we can only report the numbers from the respective papers but cannot re-evaluate them with the\nsame code. We follow the common evaluation protocol for class-conditional ImageNet synthesis from [4] and evaluate 50k\nsamples from the model against the whole training split of ImageNet. However, it is not clear how different implementations\nresize the training images. In our code, we use the largest center-crop and resize it bilinearly with anti-aliasing to 256 ×256\nusing Pillow [73]. FID and Inception Scores are then computed with torch-fidelity [50].\nWe updated face-synthesis results in Tab. 3 based on a slightly different implementation as in the case of class-conditional\nImageNet results and improve the previous results slightly. In addition, we evaluate the ability of our NLL-based training to\ndetect overﬁtting. We train larger models (FFHQ (big) and CelebA-HQ (big) in Tab. 8) on the face datasets, and show nearest\nneighbors of samples obtained from checkpoints with the best NLL on the validation split and the training split in Sec. E. We\nalso added Fig. 10, which visualizes the effect of tuning kin top-k sampling on FID and IS.\nB. Implementation Details\nThe hyperparameters for all experiments presented in the main paper and supplementary material can be found in Tab. 8.\nExcept for the c-IN (big), COCO-Stuff and ADE20K models, these hyperparameters are set such that each transformer model\ncan be trained with a batch-size of at least 2 on a GPU with 12GB VRAM, but we generally train on 2-4 GPUs with an\naccumulated VRAM of 48 GB. If hardware permits, 16-bit precision training is enabled.\n1https://arxiv.org/abs/2012.09841v3\n2https://arxiv.org/abs/2012.09841v2\n10\nDataset ours-previous (+R) BigGAN (-deep) MSP\nIN 256, 50K 19.8 (11.2) 7.1 (7.3) n.a.\nIN 256, 18K 23.5 9.6 (9.7) 50.4\nDataset ours-previous ours-new\nCelebA-HQ 256 10.7 10.2\nFFHQ 256 11.4 9.6\nTable 6. Results from a previous version of this paper, see also Sec. A. Left: Previous results on class-conditional ImageNet synthesis\nwith a slightly different implementation and evaluated against 50k and 18k training examples instead of the whole training split. See Tab. 4\nfor new, improved results evaluated against the whole training split. Right: Previous results on face-synthesis with a slightly different\nimplementation compared to the new implementation. See also Tab. 3 for comparison with other methods.\nFigure 10. FID and Inception Score as a function of top-k for CelebA-HQ (left) and FFHQ (right).\nEncoder Decoder\nx∈RH×W×C zq ∈Rh×w×nz\nConv2D →RH×W×C′\nConv2D →Rh×w×C′′\nm×{Residual Block, Downsample Block}→ Rh×w×C′′\nResidual Block →Rh×w×C′′\nResidual Block →Rh×w×C′′\nNon-Local Block →Rh×w×C′′\nNon-Local Block →Rh×w×C′′\nResidual Block →Rh×w×C′′\nResidual Block →Rh×w×C′′\nm×{Residual Block, Upsample Block}→ RH×W×C′\nGroupNorm, Swish, Conv2D →Rh×w×nz GroupNorm, Swish, Conv2D →RH×W×C\nTable 7. High-level architecture of the encoder and decoder of our VQGAN. The design of the networks follows the architecture presented\nin [25] with no skip-connections. For the discriminator, we use a patch-based model as in [28]. Note that h = H\n2m , w = W\n2m and f = 2m.\nVQGAN Architecture The architecture of our convolutional encoder and decoder models used in theVQGAN experiments\nis described in Tab. 7. Note that we adopt the compression rate by tuning the number of downsampling stepsm. Further note\nthat λin Eq. 5 is set to zero in an initial warm-up phase. Empirically, we found that longer warm-ups generally lead to better\nreconstructions. As a rule of thumb, we recommend setting λ= 0for at least one epoch.\nTransformer Architecture Our transformer model is identical to the GPT2 architecture [58] and we vary its capacity\nmainly through varying the amount of layers (see Tab. 8). Furthermore, we generally produce samples with a temperature\nt= 1.0 and a top-kcutoff at k= 100(with higher top-kvalues for larger codebooks).\nC. On Context-Rich Vocabularies\nSec. 4.3 investigated the effect of the downsampling factor f used for encoding images. As demonstrated in Fig. 7, large\nfactors are crucial for our approach, since they enable the transformer to model long-range interactions efﬁciently. However,\nsince larger f correspond to larger compression rates, the reconstruction quality of the VQGAN starts to decrease after a\ncertain point, which is analyzed in Fig. 11. The left part shows the reconstruction error (measured by LPIPS [81]) versus the\nnegative log-likelihood obtained by the transformer for values off ranging from 1 to 64. The latter provides a measure of the\nability to model the distribution of the image representation, which increases with f. The reconstruction error on the other\nhand decreases with f and the qualitative results on the right part show that beyond a critical value of f, in this case f = 16,\nreconstruction errors become severe. At this point, even when the image representations are modeled faithfully, as suggested\nby a low negative log-likelihood, sampled images are of low-ﬁdelity, because the reconstruction capabilities provide an upper\nbound on the quality that can be achieved.\nHence, Fig. 11 shows that we must learn perceptually rich encodings,i.e. encodings with a largefand perceptually faithful\nreconstructions. This is the goal of our VQGAN and Fig. 12 compares its reconstruction capabilities against the VQV AE [72]\n11\nExperiment nlayer # params [M] nz |Z| dropout length (s) ne m\nRIN 12 85 64 768 0.0 512 1024 4\nc-RIN 18 128 64 768 0.0 257 768 4\nD-RINv1 14 180 256 1024 0.0 512 768 4\nD-RINv2 24 307 256 1024 0.0 512 1024 4\nIN 24 307 256 1024 0.0 256 1024 4\nc-IN 24 307 256 1024 0.0 257 1024 4\nc-IN (big) 48 1400 256 16384 0.0 257 1536 4\nIN-Edges 24 307 256 1024 0.0 512 1024 3\nIN-SR 12 153 256 1024 0.0 512 1024 3\nS-FLCKR, f = 4 24 307 256 1024 0.0 512 1024 2\nS-FLCKR, f = 16 24 307 256 1024 0.0 512 1024 4\nS-FLCKR, f = 32 24 307 256 1024 0.0 512 1024 5\n(FacesHQ, f= 1)∗ 24 307 – 512 0.0 512 1024 –\nFacesHQ, f = 2 24 307 256 1024 0.0 512 1024 1\nFacesHQ, f = 4 24 307 256 1024 0.0 512 1024 2\nFacesHQ, f = 8 24 307 256 1024 0.0 512 1024 3\nFacesHQ∗∗, f = 16 24 307 256 1024 0.0 512 1024 4\nFFHQ∗∗, f = 16 28 355 256 1024 0.0 256 1024 4\nCelebA-HQ∗∗, f = 16 28 355 256 1024 0.0 256 1024 4\nFFHQ (big) 24 801 256 1024 0.0 256 1664 4\nCelebA-HQ (big) 24 801 256 1024 0.0 256 1664 4\nCOCO-Stuff 32 651 256 8192 0.0 512 1280 4\nADE20K 28 405 256 4096 0.1 512 1024 4\nDeepFashion 18 129 256 1024 0.0 340 768 4\nLSUN-CT 24 307 256 1024 0.0 256 1024 4\nCIFAR-10 24 307 256 1024 0.0 256 1024 1\nTable 8. Hyperparameters. For every experiment, we set the number of attention heads in the transformer to nh = 16. nlayer denotes the\nnumber of transformer blocks, # params the number of transformer parameters,nz the dimensionality of codebook entries, |Z|the number\nof codebook entries, dropout the dropout rate for training the transformer, length (s) the total length of the sequence, ne the embedding\ndimensionality and m the number of downsampling steps in the VQGAN. D-RINv1 is the experiment which compares to Pixel-SNAIL in\nSec. 4.1. Note that the experiment (FacesHQ, f = 1)∗ does not use a learned VQGAN but a ﬁxed k-means clustering algorithm as in [8]\nwith K = 512centroids. A preﬁx “c” refers to a class-conditional model. The models marked with a ‘∗∗‘ are trained on the sameVQGAN.\nused in DALL-E [59]. We observe that for f = 8 and 8192 codebook entries, both the VQV AE and VQGAN capture the\nglobal structure faithfully. However, the textures produced by the VQV AE are blurry, whereas those of theVQGAN are crisp\nand realistic looking (e.g. the stone texture and the fur and tail of the squirrel). When we increase the compression rate of the\nVQGAN further to f = 16, we see that some reconstructed parts are not perfectly aligned with the input anymore ( e.g. the\npaw of the squirrel), but, especially with slightly larger codebooks, the reconstructions still look realistic. This demonstrates\nhow the VQGAN provides high-ﬁdelity reconstructions at large factors, and thereby enables efﬁcient high-resolution image\nsynthesis with transformers.\nTo illustrate how the choice of f depends on the dataset, Fig. 13 presents results on S-FLCKR. In the left part, it shows,\nanalogous to Fig. 7, how the quality of samples increases with increasing f. However, in the right part, it shows that\nreconstructions remain faithful perceptually faithful even for f32, which is in contrast to the corresponding results on faces\nin Fig. 11. These results might be explained by a higher perceptual sensitivity to facial features as compared to textures, and\nallow us to generate high-resolution landscapes even more efﬁciently with f = 32.\nD. Additional Results\nQualitative Comparisons The qualitative comparison corresponding to Tab. 4 and Tab. 6 can be found in Fig. 14, 15, 16\nand 17. Since no models are available for VQV AE-2 and MSP, we extracted results directly from the supplementary 3 and\n3https://drive.google.com/file/d/1H2nr_Cu7OK18tRemsWn_6o5DGMNYentM/view?usp=sharing\n12\nfrom the provided samples 4, respectively. For BigGAN, we produced the samples via the provided model 5. Similarly, the\nqualitative comparison with the best competitor model (SPADE) for semantic synthesis on standard benchmarks (see Tab. 2)\ncan be found in Fig. 40 (ADE20K) and Fig. 41 (COCO-Stuff)6.\nComparison to Image-GPT To further evaluate the effectiveness of our approach, we compare to the state-of-the-art\ngenerative transformer model on images, ImageGPT [8]. By using immense amounts of compute the authors demonstrated\nthat transformer models can be applied to the pixel-representation of images and thereby achieved impressive results both in\nrepresentation learning and image synthesis. However, as their approach is conﬁned to pixel-space, it does not scale beyond\na resolution of 192 ×192. As our approach leverages a strong compression method to obtain context-rich representations\nof images and then learns a transformer model, we can synthesize images of much higher resolution. We compare both\napproaches in Fig. 27 and Fig. 28, where completions of images are depicted. Both plots show that our approach is able\nto synthesize consistent completions of dramatically increased ﬁdelity. The results of [8] are obtained from https://\nopenai.com/blog/image-gpt/.\nAdditional High-Resolution Results Fig. 29, 30, 31 and Fig. 32 contain additional HR results on the S-FLCKR dataset\nfor both f = 16(m= 4) and f = 32(m= 5) (semantically guided). In particular, we provide an enlarged version of Fig. 5\nfrom the main text, which had to be scaled down due to space constraints. Additionally, we use our sliding window approach\n(see Sec. 3) to produce high-resolution samples for the depth-to-image setting on RIN in Fig. 33 and Fig. 34, edge-to-image\non IN in Fig. 35, stochastic superresolution on IN in Fig. 36, more examples on semantically guided landscape synthesis\non S-FLCKR in Fig. 37 with f = 16 and in Fig. 38 with f = 32, and unconditional image generation on LSUN-CT (see\nSec. 4.1) in Fig. 39. Moreover, for images of size 256 ×256, we provide results for generation from semantic layout on\n(i) ADE20K in Fig. 40 and (ii) COCO-Stuff in Fig. 41, depth-to-image on IN in Fig. 42, pose-guided person generation in\nFig. 43 and class-conditional synthesis on RIN in Fig. 44.\nE. Nearest Neighbors of Samples\nOne advantage of likelihood-based generative models over, e.g., GANs is the ability to evaluate NLL on training data and\nvalidation data to detect overﬁtting. To test this, we trained large models for face synthesis, which can easily overﬁt them,\nand retained two checkpoints on each dataset: One for the best validation NLL (at the 10th and 13th epoch for FFHQ and\nCelebA-HQ, respectively), and another for the best training NLL (at epoch 1000). We then produced samples from both\ncheckpoints and retrieved nearest neighbors from the training data based on the LPIPS similarity metric [81]. The results\nare shown in Fig. 45, where it can be observed that the checkpoints with best training NLL (best train NLL) reproduce the\ntraining examples, whereas samples from the checkpoints with best validation NLL (best val. NLL) depict new faces which\nare not found in the training data.\nBased on these results, we can conclude that early-stopping based on validation NLL can prevent overﬁtting. Furthermore,\nthe bottleneck for our approach on face synthesis is given by the dataset size since it has the capacity to almost perfectly ﬁt\nthe training data. Unfortunately, FID scores cannot detect such an overﬁtting. Indeed, the best train NLL checkpoints achieve\nFID scores of 3.86 on CelebA-HQ and 2.68 on FFHQ, compared to 10.2 and 9.6 for the best val. NLL checkpoints. While\nvalidation NLL provides a way to detect overﬁtting for likelihood-based models, it is not clear if early-stopping based on it\nis optimal if one is mainly interested in the quality of samples. To address this and the evaluation of GANs, new metrics will\nbe required which can differentiate between models that produce new, high-quality samples and those that simply reproduce\nthe training data.\nOur class-conditional ImageNet model does not display overﬁtting according to validation NLL, and the nearest neighbors\nshown in Fig. 46 also provide evidence that the model produces new, high-quality samples.\nF. On the Ordering of Image Representations\nFor the “classical” domain of transformer models, NLP, the order of tokens is deﬁned by the language at hand. For images\nand their discrete representations, in contrast, it is not clear which linear ordering to use. In particular, our sliding-window\napproach depends on a row-major ordering and we thus investigate the performance of the following ﬁve different permu-\ntations of the input sequence of codebook indices: (i) row major, or raster scan order, where the image representation is\n4https://bit.ly/2FJkvhJ\n5https://tfhub.dev/deepmind/biggan-deep-256/1\n6samples were reproduced with the authors’ ofﬁcial implementation available athttps://github.com/nvlabs/spade/\n13\nunrolled from top left to bottom right. (ii) spiral out, which incorporates the prior assumption that most images show a\ncentered object. (iii) z-curve, also known as z-order or morton curve, which introduces the prior of preserved locality when\nmapping a 2D image representation onto a 1D sequence. (iv) subsample, where preﬁxes correspond to subsampled repre-\nsentations, see also [46]. (v) alternate, which is related to row major, but alternates the direction of unrolling every row. (vi)\nspiral in, a reversed version ofspiral out which provides the most context for predicting the center of the image. A graphical\nvisualization of these permutation variants is shown in Fig. 47. Given aVQGAN trained on ImageNet, we train a transformer\nfor each permutation in a controlled setting, i.e. we ﬁx initialization and computational budget.\nResults Fig.47 depicts the evolution of negative log-likelihood for each variant as a function of training iterations, with\nﬁnal values given by (i) 4.767, (ii) 4.889, (iii) 4.810, (iv) 5.015, (v) 4.812, (vi) 4.901. Interestingly, row major performs best\nin terms of this metric, whereas the more hierarchical subsample prior does not induce any helpful bias. We also include\nqualitative samples in Fig. 48 and observe that the two worst performing models in terms of NLL ( subsample and spiral in)\ntend to produce more textural samples, while the other variants synthesize samples with much more recognizable structures.\nOverall, we can conclude that the autoregressive codebook modeling isnot permutation-invariant, but the commonrow major\nordering [71, 8] outperforms other orderings.\n14\n0.0 0.2 0.4 0.6 0.8\nreconstruction error\n102\n103\n104\n105\nnegative log-likelihood\narea of\nhigh-ﬁdelity\narea of\nlow-ﬁdelity\nf1\nf2\nf4\nf8\nf16\nf32\nf64\nf2 f4 f8 f16 f32 f64\nrecon-\nstruction\nsample\nsamples\nrec. error 0.11 ±0.02 0 .20 ±0.03 0 .23 ±0.04 0 .38 ±0.07 0 .63 ±0.08 0 .66 ±0.11\nnll 5.66 ·104 1.29 ·104 4.10 ·103 2.32 ·103 2.28 ·102 6.75 ·101\nFigure 11. Trade-off between negative log-likelihood (nll) and reconstruction error. While context-rich encodings obtained with large\nfactors f allow the transformer to effectively model long-range interactions, the reconstructions capabilities and hence quality of samples\nsuffer after a critical value (here, f = 16). For more details, see Sec. C.\nFigure 12. Comparing reconstruction capabilities between VQV AEs andVQGANs. Numbers in parentheses denote compression factor and\ncodebook size. With the same compression factor and codebook size, VQGANs produce more realistic reconstructions compared to blurry\nreconstructions of VQV AEs. This enables increased compression rates forVQGAN while retaining realistic reconstructions. See Sec. C.\nf4 f16 f32\n factor reconstructions\nf4\nf16\nf32\nFigure 13. Samples on landscape dataset (left) obtained with different factorsf, analogous to Fig. 7. In contrast to faces, a factor off = 32\nstill allows for faithful reconstructions (right). See also Sec. C.\n15\nours VQV AE-2 [61] BigGAN [4] MSP [19]\nFigure 14. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 28: spotted\nsalamander (top) and 97: drake (bottom). We report class labels as in VQV AE-2 [61].\n16\nours VQV AE-2 [61] BigGAN [4] MSP [19]\nFigure 15. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 108: sea\nanemone (top) and 141: redshank (bottom). We report class labels as in VQV AE-2 [61].\n17\nours VQV AE-2 [61] BigGAN [4] MSP [19]\nFigure 16. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 11: goldﬁnch\n(top) and 22: bald eagle (bottom).\n18\nours VQV AE-2 [61] BigGAN [4] MSP [19]\nFigure 17. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 0: tench (top)\nand 9: ostrich (bottom).\n19\n933: cheeseburger\nacc. rate 1.0 acc. rate 0.5 acc. rate 0.1\n992: agaric\nacc. rate 1.0 acc. rate 0.5 acc. rate 0.1\n200: tibetian terrier\nacc. rate 1.0 acc. rate 0.5 acc. rate 0.1\nFigure 18. Visualizing the effect of increased rejection rate ( i.e. lower acceptance rate) by using a ResNet-101 classiﬁer trained on\nImageNet and samples from our class-conditional ImageNet model. Higher rejection rates tend to produce images showing more central,\nrecognizable objects compared to the unguided samples. Here, k = 973, p = 1.0 are ﬁxed for all samples. Note that k = 973 is the\neffective size of the VQGAN’s codebook,i.e. it describes how many entries of the codebook with dim Z= 16384are actually used.\n20\n933: cheeseburger\nk = 973 k = 300 k = 100\n992: agaric\nk = 973 k = 300 k = 100\n200: tibetian terrier\nk = 973 k = 300 k = 100\nFigure 19. Visualizing the effect of varying k in top-k sampling ( i.e. truncating the probability distribution per image token) by using\na ResNet-101 classiﬁer trained on ImageNet and samples from our class-conditional ImageNet model. Lower values of k produce more\nuniform, low-entropic images compared to samples obtained with full k. Here, an acceptance rate of 1.0 and p = 1.0 are ﬁxed for all\nsamples. Note that k = 973 is the effective size of the VQGAN’s codebook, i.e. it describes how many entries of the codebook with\ndim Z= 16384are actually used.\n21\n933: cheeseburger\np = 1.0 p = 0.96 p = 0.84\n992: agaric\np = 1.0 p = 0.96 p = 0.84\n200: tibetian terrier\np = 1.0 p = 0.96 p = 0.84\nFigure 20. Visualizing the effect of varying p in top-p sampling (or nucleus sampling [27]) by using a ResNet-101 classiﬁer trained on\nImageNet and samples from our class-conditional ImageNet model. Lowering p has similar effects as decreasing k, see Fig. 19. Here, an\nacceptance rate of 1.0 and k = 973are ﬁxed for all samples.\n22\nFigure 21. Random samples on 256 ×256 class-conditional ImageNet with k ∈[100, 200, 250, 300, 350, 400, 500, 600, 800, 973] ,\np = 1.0, acceptance rate 1.0. FID: 17.04, IS: 70.6 ±1.8. Please see https://git.io/JLlvY for an uncompressed version.\n23\nFigure 22. Random samples on 256 ×256 class-conditional ImageNet with k = 600, p = 1.0, acceptance rate 0.05. FID: 5.20, IS:\n280.3 ±5.5. Please see https://git.io/JLlvY for an uncompressed version.\n24\nFigure 23. Random samples on 256 ×256 class-conditional ImageNet with k = 250, p = 1.0, acceptance rate 1.0. FID: 15.98, IS:\n78.6 ±1.1. Please see https://git.io/JLlvY for an uncompressed version.\n25\nFigure 24. Random samples on 256 ×256 class-conditional ImageNet with k = 973, p = 0.88, acceptance rate 1.0. FID: 15.78, IS:\n74.3 ±1.8. Please see https://git.io/JLlvY for an uncompressed version.\n26\nFigure 25. Random samples on 256 ×256 class-conditional ImageNet with k ∈[100, 200, 250, 300, 350, 400, 500, 600, 800, 973] ,\np = 1.0, acceptance rate 0.005. FID: 6.59, IS: 402.7 ±2.9. Please see https://git.io/JLlvY for an uncompressed version.\n27\nFigure 26. Random samples on 256 ×256 class-conditional ImageNet with k ∈[100, 200, 250, 300, 350, 400, 500, 600, 800, 973] ,\np = 1.0, acceptance rate 0.05. FID: 5.88, IS: 304.8 ±3.6. Please see https://git.io/JLlvY for an uncompressed version.\n28\nconditioning ours (top) vs iGPT [8] (bottom)\nFigure 27. Comparing our approach with the pixel-based approach of [8]. Here, we use ourf = 16S-FLCKR model to obtain high-ﬁdelity\nimage completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and\nthree of [8] (bottom).\n29\nconditioning ours (top) vs iGPT [8] (bottom)\nFigure 28. Comparing our approach with the pixel-based approach of [8]. Here, we use ourf = 16S-FLCKR model to obtain high-ﬁdelity\nimage completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and\nthree of [8] (bottom).\n30\nFigure 29. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 ×832, 1024 ×416 and 1280 ×240\npixels.\n31\nFigure 30. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1536 ×512, 1840 ×1024, and 1536 ×620\npixels.\n32\nFigure 31. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 2048 ×512, 1460 ×440, 2032 ×448 and\n2016 ×672 pixels.\n33\nFigure 32. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 ×832, 1024 ×416 and 1280 ×240\npixels.\n34\nconditioning samples\nFigure 33. Depth-guided neural rendering on RIN with f = 16using the sliding attention window.\n35\nconditioning samples\nFigure 34. Depth-guided neural rendering on RIN with f = 16using the sliding attention window.\n36\nconditioning samples\nFigure 35. Intentionally limiting the receptive ﬁeld can lead to interesting creative applications like this one: Edge-to-Image synthesis on\nIN with f = 8, using the sliding attention window.\n37\nconditioning samples\nFigure 36. Additional results for stochastic superresolution with an f = 16model on IN, using the sliding attention window.\n38\nconditioning samples\nFigure 37. Samples generated from semantic layouts on S-FLCKR with f = 16, using the sliding attention window.\n39\nconditioning samples\nFigure 38. Samples generated from semantic layouts on S-FLCKR with f = 32, using the sliding attention window.\n40\nFigure 39. Unconditional samples from a model trained on LSUN Churches & Towers, using the sliding attention window.\n41\nconditioning ground truth ours SPADE [53]\nFigure 40. Qualitative comparison to [53] on 256 ×256 images from the ADE20K dataset.\n42\nconditioning ground truth ours SPADE [53]\nFigure 41. Qualitative comparison to [53] on 256 ×256 images from the COCO-Stuff dataset.\n43\nconditioning samples conditioning samples\nFigure 42. Conditional samples for the depth-to-image model on IN.\nconditioning samples conditioning samples\nFigure 43. Conditional samples for the pose-guided synthesis model via keypoints on DeepFashion.\nclass exemplar samples class exemplar samples\nFigure 44. Samples produced by the class-conditional model trained on RIN.\n44\nsample nearest neighbors\nCelebA-HQ (best val. NLL)\nCelebA-HQ (best train NLL)\nFFHQ (best val. NLL)\nFFHQ (best train NLL)\nFigure 45. Nearest neighbors for our face-models trained on FFHQ and CelebA-HQ ( 256 ×256 pix), based on the LPIPS [82] distance.\nThe left column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding\nclass (increasing distance) in the training dataset. We evaluate two different model checkpoints for each dataset: Best val. NLL denotes\nthe minimal NLL over the course of training, evaluated on unseen testdata. For this checkpoint, both models generate crisp, high-quality\nsamples not present in the training data. However, when drastically overﬁtting the model, it reproduces samples from the training data\n(best train NLL). Although not an ideal measure of image quality, NLL thus provides a proxy on model selection, whereas FID does not.\nSee also Sec. E.\n45\nk= 250, p= 1.0, a= 1.0\nk= 973, p= 0.88, a= 1.0\nmixedk, p= 1.0, a= 0.05\nmixedk, p= 1.0, a= 0.005\nFigure 46. Nearest neighbors for our class-conditional ImageNet model ( 256 ×256 pix), based on the LPIPS [82] distance. The left\ncolumn shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding class\n(increasing distance) in the training dataset. Our model produces new, unseen high-quality images, not present in the training data.\n46\nrow major spiral in spiral out z-curve subsample alternate\n0 1 2 3\n4 5 6 7\n8 9 10 11\n12 13 14 15\n0 11 10 9\n1 12 15 8\n2 13 14 7\n3 4 5 6\n15 4 5 6\n14 3 0 7\n13 2 1 8\n12 11 10 9\n0 1 4 5\n2 3 6 7\n8 9 12 13\n10 11 14 15\n0 4 1 5\n8 12 9 13\n2 6 3 7\n10 14 11 15\n0 1 2 3\n7 6 5 4\n8 9 10 11\n15 14 13 12\n0 100000 200000 300000 400000 500000 600000 700000 800000\ntraining step\n4.8\n5.0\n5.2\n5.4\n5.6\nnegative log-likelihood\nsubsample\nspiralin\nspiralout\nalternate\nzcurve\nrowmajor\nBatch-wise Training Loss\n100000 200000 300000 400000 500000 600000 700000 800000\ntraining step\n4.8\n4.9\n5.0\n5.1\n5.2\nnegative log-likelihood\nsubsample\nspiralin\nspiralout\nalternate\nzcurve\nrowmajor\nValidation Loss\nFigure 47. Top: All sequence permutations we investigate, illustrated on a4 ×4 grid. Bottom: The transformer architecture is permutation\ninvariant but next-token prediction is not: The average loss on the validation split of ImageNet, corresponding to the negative log-likelihood,\ndiffers signiﬁcantly between different prediction orderings. Among our choices, the commonly used row-major order performs best.\n47\nRow Major Subsample\nZ-Curve Spiral Out\nAlternating Spiral In\nFigure 48. Random samples from transformer models trained with different orderings for autoregressive prediction as described in Sec. F.\n48\nReferences\n[1] Jyoti Aneja, Alexander G. Schwing, Jan Kautz, and Arash Vahdat. NCP-V AE: variational autoencoders with noise contrastive priors.\nCoRR, abs/2010.02917, 2020. 8\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2016.\n2\n[3] Yoshua Bengio, Nicholas L ´eonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for\nconditional computation. CoRR, abs/1308.3432, 2013. 4\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In 7th\nInternational Conference on Learning Representations, ICLR, 2019. 8, 10, 16, 17, 18, 19\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nModels are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020. 1\n[6] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and stuff classes in context. In Computer vision and pattern\nrecognition (CVPR), 2018 IEEE conference on. IEEE, 2018. 6\n[7] Liang-Chieh Chen, G. Papandreou, I. Kokkinos, Kevin Murphy, and A. Yuille. DeepLab: Semantic Image Segmentation with\nDeep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2018. 5\n[8] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative\npretraining from pixels. 2020. 1, 2, 3, 4, 5, 6, 7, 12, 13, 14, 29, 30\n[9] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded reﬁnement networks. In IEEE International Con-\nference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017 , pages 1520–1529. IEEE Computer Society, 2017.\n7\n[10] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In\nICML, volume 80 of Proceedings of Machine Learning Research, pages 863–871. PMLR, 2018. 2, 5\n[11] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020.\n8\n[12] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. 1, 2, 5\n[13] Bin Dai and David P. Wipf. Diagnosing and enhancing V AE models. In 7th International Conference on Learning Representations,\nICLR, 2019. 2\n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR, 2009. 5\n[15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. 8, 10\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at\nscale. 2020. 1\n[17] Alexey Dosovitskiy and Thomas Brox. Generating Images with Perceptual Similarity Metrics based on Deep Networks. In Advances\nin Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems, NeurIPS, 2016. 4\n[18] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. A Disentangling Invertible Interpretation Network for Explaining Latent Repre-\nsentations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020. 2\n[19] Jeffrey De Fauw, Sander Dieleman, and Karen Simonyan. Hierarchical autoregressive image models with auxiliary decoders. CoRR,\nabs/1903.04933, 2019. 16, 17, 18, 19\n[20] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua\nBengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural\nInformation Processing Systems, NeurIPS, 2014. 2\n[21] Seungwook Han, Akash Srivastava, Cole L. Hurwitz, Prasanna Sattigeri, and David D. Cox. not-so-biggan: Generating high-ﬁdelity\nimages on a small compute budget. CoRR, abs/2009.04433, 2020. 2\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385,\n2015. 8\n[23] Ari Heljakka, Arno Solin, and Juho Kannala. Pioneer networks: Progressively growing generative autoencoder. In C. V . Jawahar,\nHongdong Li, Greg Mori, and Konrad Schindler, editors,Computer Vision - ACCV 2018 - 14th Asian Conference on Computer Vision,\nPerth, Australia, December 2-6, 2018, Revised Selected Papers, Part I, 2018. 8\n[24] Ari Heljakka, Arno Solin, and Juho Kannala. Towards photographic image manipulation with balanced growing of generative\nautoencoders. In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March\n1-5, 2020, pages 3109–3118. IEEE, 2020. 8\n49\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 11\n[26] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. CoRR,\nabs/1912.12180, 2019. 2, 5\n[27] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR. OpenRe-\nview.net, 2020. 8, 22\n[28] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-Image Translation with Conditional Adversarial Networks.\nIn 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2017. 4, 11\n[29] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144 ,\n2016. 9\n[30] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV (2),\nvolume 9906 of Lecture Notes in Computer Science, pages 694–711. Springer, 2016. 4\n[31] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation.\nCoRR, abs/1710.10196, 2017. 6, 8\n[32] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks\nwith limited data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors,Ad-\nvances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual, 2020. 8\n[33] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE\nConference on Computer Vision and Pattern Recognition, (CVPR) 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4401–4410.\nComputer Vision Foundation / IEEE, 2019. 7\n[34] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image\nquality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,\nJune 13-19, 2020, pages 8107–8116. IEEE, 2020. 8\n[35] Prateek Katiyar and Anna Khoreva. Improving augmentation and evaluation schemes for semantic image synthesis, 2021. 7\n[36] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks, 2017. 2\n[37] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In Advances in Neural\nInformation Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS, 2018. 8\n[38] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representa-\ntions, ICLR, 2014. 2\n[39] Alex Lamb, Vincent Dumoulin, and Aaron C. Courville. Discriminative regularization for generative models.CoRR, abs/1602.03220,\n2016. 4\n[40] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a\nlearned similarity metric, 2015. 4\n[41] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with transformer network. In AAAI, pages\n6706–6713. AAAI Press, 2019. 2\n[42] Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen. COCO-GAN: generation\nby parts via conditional coordinating. In ICCV, pages 4511–4520. IEEE, 2019. 5\n[43] Jinlin Liu, Yuan Yao, and Jianqiang Ren. An acceleration framework for high resolution image synthesis. CoRR, abs/1909.03611,\n2019. 2\n[44] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia\nby summarizing long sequences. In ICLR (Poster). OpenReview.net, 2018. 4\n[45] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval\nwith rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 6\n[46] Jacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel networks and multidimensional upscaling.\nIn 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net,\n2019. 14\n[47] Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-ﬁdelity generative image compression, 2020. 4\n[48] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations, 2021. 8, 10\n[49] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. 8, 10\n[50] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-ﬁdelity performance\nmetrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738. 7, 10\n[51] B. Ommer and J. M. Buhmann. Learning the compositional nature of visual objects. In 2007 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 1–8, 2007. 2\n[52] Ankur P. Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language\ninference, 2016. 2\n50\n[53] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic Image Synthesis with Spatially-Adaptive Normalization.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2019. 2, 7, 42, 43\n[54] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder, 2020. 8\n[55] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer.\nIn ICML, volume 80 of Proceedings of Machine Learning Research, pages 4052–4061. PMLR, 2018. 2, 3, 5\n[56] Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 14092–14101. IEEE, 2020. 8\n[57] A. Radford. Improving language understanding by generative pre-training. 2018. 1\n[58] A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask\nlearners. 2019. 1, 5, 11\n[59] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot\ntext-to-image generation, 2021. 9, 10, 12\n[60] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation:\nMixing datasets for zero-shot cross-dataset transfer.IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.\n5\n[61] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with vq-vae-2, 2019. 3, 4, 5, 8, 16, 17,\n18, 19\n[62] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep\ngenerative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML ,\n2014. 2\n[63] Robin Rombach, Patrick Esser, and Bj ¨orn Ommer. Making sense of cnns: Interpreting deep representations and their invariances\nwith inns. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII , volume 12362 of Lecture Notes in Computer\nScience, pages 647–664. Springer, 2020. 2\n[64] Robin Rombach, Patrick Esser, and Bjorn Ommer. Network-to-network translation with conditional invertible neural networks. In\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems ,\nvolume 33, pages 2784–2797. Curran Associates, Inc., 2020. 2\n[65] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Computer vision with a\nsingle (robust) classiﬁer. In ArXiv preprint arXiv:1906.09453, 2019. 5\n[66] Edgar Sch ¨onfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204–\n8213. IEEE, 2020. 8\n[67] Kim Seonghyeon. Implementation of generating diverse high-ﬁdelity images with vq-vae-2 in pytorch, 2020. 8\n[68] Aliaksandr Siarohin, St ´ephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image anima-\ntion. In Conference on Neural Information Processing Systems (NeurIPS), December 2019. 2\n[69] Arash Vahdat and Jan Kautz. NV AE: A deep hierarchical variational autoencoder. In Hugo Larochelle, Marc’Aurelio Ranzato,\nRaia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 8\n[70] A ¨aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In ICML, volume 48 of JMLR\nWorkshop and Conference Proceedings, pages 1747–1756. JMLR.org, 2016. 2\n[71] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image\ngeneration with pixelcnn decoders, 2016. 2, 4, 14\n[72] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. 3, 4, 11\n[73] Hugo van Kemenade, wiredfool, Andrew Murray, Alex Clark, Alexander Karpinsky, Ondrej Baranovi ˇc, Christoph Gohlke, Jon\nDufresne, Brian Crowell, David Schmidt, Konstantin Kopachev, Alastair Houghton, Sandro Mani, Steve Landey, vashek, Josh Ware,\nJason Douglas, David Caro, Uriel Martinez, Steve Kossouho, Riley Lahd, Stanislau T., Antony Lee, Eric W. Brown, Oliver Tonnhofer,\nMickael Bonﬁll, Peter Rowlands, Fahad Al-Saidi, German Novikov, and Michał G´orny. python-pillow/pillow: 8.2.0, Apr. 2021. 10\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information\nProcessing Systems, NeurIPS, 2017. 1, 2, 3\n[75] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and\nsemantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2018. 7\n[76] Dirk Weissenborn, Oscar T ¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In ICLR. OpenReview.net, 2020. 2\n[77] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. Vaebm: A symbiosis between variational autoencoders and energy-based\nmodels, 2021. 8\n51\n[78] Zhisheng Xiao, Qing Yan, Yi-an Chen, and Yali Amit. Generative latent ﬂow: A framework for non-adversarial image generation.\nCoRR, abs/1905.10485, 2019. 2\n[79] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep\nlearning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 5\n[80] Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. Cross-Domain Correspondence Learning for Exemplar-Based Image\nTranslation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020. 2\n[81] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as\na Perceptual Metric. In CVPR, 2018. 4, 11, 13\n[82] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a\nperceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2018. 45, 46\n[83] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through\nthe ade20k dataset. arXiv preprint arXiv:1608.05442, 2016. 6\n[84] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros. View synthesis by appearance ﬂow, 2017. 2\n[85] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-adaptive normalization,\n2019. 2\n52"
}