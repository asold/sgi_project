{
  "title": "Larger-Context Language Modelling",
  "url": "https://openalex.org/W2207587218",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104257223",
      "name": "Wang Tian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2139666201",
    "https://openalex.org/W2118776487",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W1571227886",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W2182417295",
    "https://openalex.org/W889023230",
    "https://openalex.org/W6908809",
    "https://openalex.org/W1573488949",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2950726992",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2953061907"
  ],
  "abstract": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling. We call this larger-context language model. We introduce a late fusion approach to a recurrent language model based on long short-term memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank), we demon- strate that the proposed model improves perplexity significantly. In the experi- ments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM. By analyzing the trained larger- context language model, we discover that content words, including nouns, adjec- tives and verbs, benefit most from an increasing number of context sentences. This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.",
  "full_text": "Under review as a conference paper at ICLR 2016\nLARGER -CONTEXT LANGUAGE MODELLING\nWITH RECURRENT NEURAL NETWORK\nTian Wang\nCenter for Data Science\nNew York University\nt.wang@nyu.edu\nKyunghyun Cho\nCourant Institute of Mathematical Sciences\nand Center for Data Science\nNew York University\nkyunghyun.cho@nyu.edu\nABSTRACT\nIn this work, we propose a novel method to incorporate corpus-level discourse\ninformation into language modelling. We call this larger-context language model.\nWe introduce a late fusion approach to a recurrent language model based on long\nshort-term memory units (LSTM), which helps the LSTM unit keep intra-sentence\ndependencies and inter-sentence dependencies separate from each other. Through\nthe evaluation on three corpora (IMDB, BBC, and Penn TreeBank), we demon-\nstrate that the proposed model improves perplexity signiﬁcantly. In the experi-\nments, we evaluate the proposed approach while varying the number of context\nsentences and observe that the proposed late fusion is superior to the usual way\nof incorporating additional inputs to the LSTM. By analyzing the trained larger-\ncontext language model, we discover that content words, including nouns, adjec-\ntives and verbs, beneﬁt most from an increasing number of context sentences. This\nanalysis suggests that larger-context language model improves the unconditional\nlanguage model by capturing the theme of a document better and more easily.\n1 I NTRODUCTION\nThe goal of language modelling is to estimate the probability distribution of various linguistic units,\ne.g., words, sentences (Rosenfeld, 2000). Among the earliest techniques were count-based n-gram\nlanguage models which intend to assign the probability distribution of a given word observed after a\nﬁxed number of previous words. Later Bengio et al. (2003) proposed feed-forward neural language\nmodel, which achieved substantial improvements in perplexity over count-based language models.\nBengio et al. showed that this neural language model could simultaneously learn the conditional\nprobability of the latest word in a sequence as well as a vector representation for each word in a\npredeﬁned vocabulary.\nRecently recurrent neural networks have become one of the most widely used models in language\nmodelling (Mikolov et al., 2010). Long short-term memory unit (LSTM, Hochreiter & Schmidhu-\nber, 1997) is one of the most common recurrent activation function. Architecturally speaking, the\nmemory state and output state are explicitly separated by activation gates such that the vanishing\ngradient and exploding gradient problems described in Bengio et al. (1994) is avoided. Motivated\nby such gated model, a number of variants of RNNs (e.g. Cho et al. (GRU, 2014b), Chung et al.\n(GF-RNN, 2015)) have been designed to easily capture long-term dependencies.\nWhen modelling a corpus, these language models assume the mutual independence among sen-\ntences, and the task is often reduced to assigning a probability to a single sentence. In this work, we\npropose a method to incorporate corpus-level discourse dependency into neural language model. We\ncall this larger-context language model. It models the inﬂuence of context by deﬁning a conditional\nprobability in the form of P(wn|w1:n−1,S), where w1,...,w n are words from the same sentence,\nand Srepresents the context which consists a number of previous sentences of arbitrary length.\nWe evaluated our model on three different corpora (IMDB, BBC, and Penn TreeBank). Our ex-\nperiments demonstrate that the proposed larger-context language model improve perplexity for sen-\ntences, signiﬁcantly reducing per-word perplexity compared to the language models without context\n1\narXiv:1511.03729v2  [cs.CL]  25 Dec 2015\nUnder review as a conference paper at ICLR 2016\ninformation. Further, through Part-Of-Speech tag analysis, we discovered that content words, in-\ncluding nouns, adjectives and verbs, beneﬁt the most from increasing number of context sentences.\nSuch discovery led us to the conclusion that larger-context language model improves the uncondi-\ntional language model by capturing the theme of a document.\nTo achieve such improvement, we proposed a late fusion approach, which is a modiﬁcation to the\nLSTM such that it better incorporates the discourse context from preceding sentences. In the exper-\niments, we evaluated the proposed approach against early fusion approach with various numbers of\ncontext sentences, and demonstrated the late fusion is superior to the early fusion approach.\nOur model explores another aspect of context-dependent recurrent language model. It is novel in\nthat it also provides an insightful way to feed information into LSTM unit, which could beneﬁt all\nencoder-decoder based applications.\n2 B ACKGROUND : S TATISTICAL LANGUAGE MODELLING\nGiven a document D = (S1,S2,...,S L) which consists of Lsentences, statistical language mod-\nelling aims at computing its probability P(D). It is often assumed that each sentence in the whole\ndocument is mutually independent from each other:\nP(D) ≈\nL∏\nl=1\nP(Sl). (1)\nWe call this probability (before approximation) acorpus-level probability. Under this assumption of\nmutual independence among sentences, the task of language modelling is often reduced to assigning\na probability to a single sentence P(Sl).\nA sentence Sl = (w1,w2,...,w Tl ) is a variable-length sequence of words or tokens. By assuming\nthat a word at any location in a sentence is largely predictable by preceding words, we can rewrite\nthe sentence probability into\nP(S) =\nTl∏\nt=1\np(wt|w<t), (2)\nwhere w<t is a shorthand notation for all the preceding words. We call this a sentence-level proba-\nbility.\nThis rewritten probability expression can be either directly modelled by a recurrent neural network\n(Mikolov et al., 2010) or further approximated as a product ofn-gram conditional probabilities such\nthat\nP(S) ≈\nTl∏\nt=1\np(wt|wt−1\nt−n), (3)\nwhere wt−1\nt−n = (wt−n,...,w t−1). The latter is called n-gram language modelling . See, e.g.,\n(Kneser & Ney, 1995) for detailed reviews on the most widely used techniques forn-gram language\nmodelling.\nThe most widely used approach to this statistical language modelling is n-gram language model in\nEq. (3). This approach builds a large table of n-gram statistics based on a training corpus. Each\nrow of the table contains as its key the n-gram phrase and its number of occurrences in the training\ncorpus. Based on these statistics, one can estimate the n-gram conditional probability (one of the\nterms inside the product in Eq. (3)) by\np(wt|wt−1\nt−n) = c(wt−n,...,w t−1,wt)∑\nw′∈V c(wt−n,...,w t−1,w′),\nwhere c(·) is the count in the training corpus, and V is the vocabulary of all unique words/tokens.\nAs this estimate suffers severely from data sparsity (i.e., most n-grams do not occur at all in the\ntraining corpus), many smoothing/back-off techniques have been proposed over decades. One of the\nmost widely used smoothing technique is a modiﬁed Kneser-Ney smoothing (Kneser & Ney, 1995)\n2\nUnder review as a conference paper at ICLR 2016\nMore recently, Bengio et al. (2003) proposed to use a feedforward neural network to model those\nn-gram conditional probabilities to avoid the issue of data sparsity. This model is often referred to\nas neural language model.\nThis n-gram language modelling is however limited due to then-th order Markov assumption made\nin Eq. (3). Hence, Mikolov et al. (2010) proposed recently to use a recurrent neural network to\ndirectly model Eq. (2) without making any Markov assumption. We will refer to this approach of\nusing a recurrent neural network for language modeling as recurrent language modelling.\nA recurrent language model is composed of two function–transition and output functions. The\ntransition function reads one word wt and updates its hidden state such that\nht = φ(wt,ht−1) , (4)\nwhere h0 is an all-zero vector. φis a recurrent activation function, and two most commonly used\nones are long short-term memory units (LSTM, Hochreiter & Schmidhuber, 1997) and gated recur-\nrent units (GRU, Cho et al., 2014b). For more details on these recurrent activation units, we refer\nthe reader to (Jozefowicz et al., 2015; Greff et al., 2015).\nAt each timestep, the output function computes the probability over all possible next words in the\nvocabulary V. This is done by\np(wt+1 = w′|wt\n1) ∝exp (gw′ (ht)) . (5)\ngis commonly implemented as an afﬁne transformation:\ng(ht) =Woht + bo,\nwhere Wo ∈R|V |×d and bo ∈R|V |.\nThe whole model is trained by maximizing the log-likelihood of a training corpus often using\nstochastic gradient descent with backpropagation through time (see, e.g., Rumelhart et al., 1988).\nThese different approaches to language modelling have been extensively tested against each other\nin terms of speech recognition and machine translation in recent years (Sundermeyer et al., 2015;\nBaltescu & Blunsom, 2014; Schwenk, 2007). Often the conclusion is that all three techniques tend\nto have different properties and qualities dependent on many different factors, such as the size of\ntraining corpus, available memory and target application. In many cases, it has been found that it is\nbeneﬁcial to combine all these techniques together in order to achieve the best language model.\nOne most important thing to note in this conventional approach to statistical language modelling is\nthat every sentence in a document is assumed independent from each other (see Eq. (1).) This raises\na question on how strong an assumption this is, how much impact this assumption has on the ﬁnal\nlanguage model quality and how much gain language modelling can get by making this assumption\nless strong.\n2.1 L ANGUAGE MODELLING WITH LONG SHORT-TERM MEMORY\nHere let us brieﬂy describe a long short-term memory unit which is widely used as a recurrent\nactivation function φ(see Eq. (4)) for language modelling (see, e.g., Graves, 2013).\nA layer of long short-term memory (LSTM) unit consists of three gates and a single memory cell.\nThree gates–input, output and forget– are computed by\nit =σ(Wixt + Uiht−1 + bi) (6)\not =σ(Woxt + Uoht−1 + bo) (7)\nft =σ(Wf xt + Uf ht−1 + bf ) , (8)\nwhere σis a sigmoid function. xt is the input at the t-th timestep.\nThe memory cell is computed by\nct = ft ⊙ct−1 + it ⊙tanh (Wcx + Ucht−1 + bc) , (9)\nwhere ⊙is an element-wise multiplication. This adaptive leaky integration of the memory cell al-\nlows the LSTM to easily capture long-term dependencies in the input sequence, and this has recently\nbeen widely adopted many works involving language models (see, e.g., Sundermeyer et al., 2015).\nThe output, or the activation of this LSTM layer, is then computed as\nht = ot ⊙tanh(ct).\n3\nUnder review as a conference paper at ICLR 2016\n3 L ARGER -CONTEXT LANGUAGE MODELLING\nIn this paper, we aim not at improving the sentence-level probability estimation P(S) (see Eq. (2))\nbut at improving the corpus-level probability P(D) from Eq. (1) directly. One thing we noticed\nat the beginning of this work is that it is not necessary for us to make the assumption of mutual\nindependence of sentences in a corpus. Rather, similarly to how we model a sentence probability,\nwe can loosen this assumption by\nP(D) ≈\nL∏\nl=1\nP(Sl|Sl−1\nl−n), (10)\nwhere Sl−1\nl−n = (Sl−n,Sl−n+1,...,S l−1). ndecides on how many preceding sentences each condi-\ntional sentence probability conditions on, similarly to what happens with a usual n-gram language\nmodelling.\nFrom the statistical modelling’s perspective, estimating the corpus-level language probability in\nEq. (10) is equivalent to build a statistical model that approximates\nP(Sl|Sl−1\nl−n) =\nTl∏\nt=1\np(wt|w<t,Sl−1\nl−n), (11)\nsimilarly to Eq. (2). One major difference from the existing approaches to statistical language mod-\nelling is that now each conditional probability of a next word is conditioned not only on the preceding\nwords in the same sentence, but also on the n−1 preceding sentences.\nA conventional, count-based n-gram language model is not well-suited due to the issue of data\nsparsity. In other words, the number of rows in the table storingn-gram statistics will explode as the\nnumber of possible sentence combinations grows exponentially with respect to both the vocabulary\nsize, each sentence’s length and the number of context sentences.\nEither neural or recurrent language modelling however does not suffer from this issue of data spar-\nsity. This makes these models ideal for modelling thelarger-context sentence probabilityin Eq. (11).\nMore speciﬁcally, we are interested in adapting the recurrent language model for this.\nIn doing so, we answer two questions in the following subsections. First, there is a question of how\nwe should represent the context sentencesSl−1\nl−n. We consider two possibilities in this work. Second,\nthere is a large freedom in how we build a recurrent activation function to be conditioned on the\ncontext sentences. We also consider two alternatives in this case.\n3.1 C ONTEXT REPRESENTATION\nA sequence of preceding sentences can be represented in many different ways. Here, let us describe\ntwo alternatives we test in the experiments.\nThe ﬁrst representation is to simply bag all the words in the preceding sentences into a single vector\ns ∈ [0,1]|V |. Any element of s corresponding to the word that exists in one of the preceding\nsentences will be assigned the frequency of that word, and otherwise 0. This vector is multiplied\nfrom left by a matrix P which is tuned together with all the other parameters:\np = Ps.\nWe call this representation p a bag-of-words (BoW) context.\nSecond, we try to represent the preceding context sentences as a sequence of bag-of-words. Each\nbag-of-word sj is the bag-of-word representation of the j-th context sentence, and they are put into\na sequence (sl−n,..., sl−1). Unlike the ﬁrst BoW context, this allows us to incorporate the order\nof the preceding context sentences.\nThis sequence of BoW vectors are read by a recurrent neural network which is separately from the\none used for modelling a sentence (see Eq. (4).) We use LSTM units as recurrent activations, and\nfor each context sentence in the sequence, we get\nzt = φ(xt,zt−1) ,\nfor t= l−n,...,l −1. We set the last hidden state zl−1 of this recurrent neural network, to which\nwe refer as a context recurrent neural network, as the context vector p.\n4\nUnder review as a conference paper at ICLR 2016\nAttention-based Context Representation The sequence of BoW vectors can be used in a bit\ndifferent way from the above. Instead of a unidirectional recurrent neural network, we ﬁrst use a\nbidirectional recurrent neural network to read the sequence. The forward recurrent neural network\nreads the sequence as usual in a forward direction, and the reverse recurrent neural network in the\nopposite direction. The hidden states from these two networks are then concatenated for each context\nsentence in order to form a sequence of annotation vectors (zl−n,..., zl−1).\nUnlike the other approaches, in this case, the context vectorp differs for each wordwt in the current\nsentence, and we denote it bypt. The context vectorpt for the t-th word is computed as the weighted\nsum of the annotation vectors:\npt =\nl−1∑\nl′=l−n\nαt,l′ zl′ ,\nwhere the attention weight αt,l′ is computed by\nαt,l′ = exp score (zl′ ,ht)\n∑l−1\nk=l−n exp score (zk,ht)\n.\nht is the hidden state of the recurrent language model of the current sentence from Eq. (5). The\nscoring function score(zl′ ,ht) returns a relevance score of the l′-th context sentence with respect to\nht.\n3.2 C ONDITIONAL LSTM\nEarly Fusion Once the context vector p is computed from the n preceding sentences, we need\nto feed this into the sentence-level recurrent language model. One most straightforward way is to\nsimply consider it as an input at every time step such that\nx = E⊤wt + Wpp,\nwhere E is the word embedding matrix that transforms the one-hot vector of the t-th word into a\ncontinuous word vector. This x is used by the LSTM layer as the input, as described in Sec. 2.1. We\ncall this approach an early fusion of the context into language modelling.\nLate Fusion In addition to this approach, we propose here a modiﬁcation to the LSTM such that it\nbetter incorporates the context from the preceding sentences (summarized by pt.) The basic idea is\nto keep dependencies within the sentence being modelled ( intra-sentence dependencies) and those\nbetween the preceding sentences and the current sent (inter-sentence dependencies) separately from\neach other.\nWe let the memory cellct of the LSTM in Eq. (9) to model intra-sentence dependencies. This simply\nmeans that there is no change to the existing formulation of the LSTM, described in Eqs. (6)–(9).\nThe inter-sentence dependencies are reﬂected on the interaction between the memory cell ct, which\nmodels intra-sentence dependencies, and the context vector p, which summarizes the npreceding\nsentences. We model this by ﬁrst computing the amount of inﬂuence of the preceding context\nsentences as\nrt = σ(Wr (Wpp) +Wrc + br) .\nThis vector rt controls the strength of each of the elements in the context vector p. This amount of\ninﬂuence from the npreceding sentences is decided based on the currently captured intra-sentence\ndependency structures and the preceding sentences.\nThis controlled context vector rt ⊙(Wpp) is then used to compute the output of the LSTM layer\nsuch that\nht = ot ⊙tanh (ct + rt ⊙(Wpp)) .\nThis is illustrated in Fig. 1 (b).\nWe call this approach a late fusion, as the effect of the preceding context is fused together with the\nintra-sentence dependency structure in the later stage of the recurrent activation.\n5\nUnder review as a conference paper at ICLR 2016\n(a) (b)\nFigure 1: Graphical illustration of the proposed (a) early fusion and (b) late fusion.\nLate fusion is a simple, but effective way to mitigate the issue of vanishing gradient in corpus-\nlevel language modelling. By letting the context representation ﬂow without having to pass through\nsaturating nonlinear activation functions, it provides a linear path through which the gradient for the\ncontext ﬂows easily.\n4 R ELATED WORK\n4.1 C ONTEXT -DEPENDENT RECURRENT LANGUAGE MODEL\nThis possibility of extending a neural or recurrent language modeling to incorporate larger context\nwas explored earlier. Especially, (Mikolov & Zweig, 2012) proposed an approach, called context-\ndependent recurrent neural network language model, very similar to the proposed approach here.\nThe basic idea of their approach is to use a topic distribution, represented as a vector of probabili-\nties, of previousnwords when computing the hidden state of the recurrent neural network each time.\nIn doing so, the words used to compute the topic distribution often went over the sentence boundary,\nmeaning that this distribution vector was summarizing a part of a preceding sentence. Nevertheless,\ntheir major goal was to use this topic distribution vector as a way to “convey contextual informa-\ntion about the sentence being modeled.” More recently, Mikolov et al. (2014) proposed a similar\napproach however without relying on external topic modelling.\nThere are three major differences in the proposed approach from the work by Mikolov & Zweig\n(2012). First, the goal in this work is to explicitly model preceding sentences to better approximate\nthe corpus-level probability (see Eq. (10)) rather than to get a better context of the current sentence.\nSecond, Mikolov & Zweig (2012) use an external method, such as latent Dirichlet allocation (Blei\net al., 2003) or latent semantics analysis (Dumais, 2004) to extract a feature vector, where we learn\nthe whole model, including the context vector extraction, end-to-end. Third, we propose a late\nfusion approach which is well suited for the LSTM units which have recently been widely adopted\nmany works involving language models (see, e.g., Sundermeyer et al., 2015). This late fusion is\nlater shown to be superior to the early fusion approach.\nSimilarly, Sukhbaatar et al. (2015) proposed more recently to use a memory network for language\nmodelling with a very large context of a hundred to two hundreds preceding words. The major\ndifference to the proposed approach is in the lack of separation between the context sentences and\nthe current sentence being processed. There are two implications from this approach. First, each\nsentence, depending on its and the preceding sentences’ lengths, is conditioned on a different number\nof context sentences. Second, words in the beginning of the sentence being modelled tend to have\na larger context (in terms of the number of preceding sentences they are being conditioned on) than\nthose at the end of the sentence. These issues do not exist in the proposed approach here.\nUnlike these earlier works, the proposed approach here uses sentence boundaries explicitly. This\nmakes it easier to integrate with downstream applications, such as machine translation and speech\nrecognition, at the decoding level which almost always works sentence-wise.\n6\nUnder review as a conference paper at ICLR 2016\nIt is however important to notice that these two previous works by Mikolov & Zweig (2012) and\nSukhbaatar et al. (2015) are not in competition with the proposed larger-context recurrent language\nmodel. Rather, all these three are orthogonal to each other and can be combined.\n4.2 D IALOGUE MODELLING WITH RECURRENT NEURAL NETWORKS\nA more similar model to the proposed larger-context recurrent language model is a hierarchical\nrecurrent encoder decoder (HRED) proposed recently by Serban et al. (2015). The HRED consists\nof three recurrent neural networks to model a dialogue between two people from the perspective of\none of them, to which we refer as a speaker. If we consider the last utterance of the speaker being\nmodelled by the decoder of the HRED, this model can be considered as a larger-context recurrent\nlanguage model with early fusion.\nAside the fact that the ultimate goals differ (in their case, dialogue modelling and in our case, doc-\nument modelling), there are two technical differences. First, they only test with the early fusion\napproach. We show later in the experiments that the proposed late fusion gives a better language\nmodelling quality than the early fusion. Second, we use a sequence of bag-of-words to represent the\npreceding sentences, while the HRED a sequence of sequences of words. This allows the HRED\nto potentially better model the order of the words in each preceding sentence, but it increases com-\nputational complexity (one more recurrent neural network) and decreases statistical efﬁcient (more\nparameters with the same amount of data.)\nAgain, the larger-context language model proposed here is not competing against the HRED. Rather,\nit is a variant, with differences in technical details, that is being evaluated speciﬁcally for document\nlanguage modelling. We believe many of the components in these two models are complementary\nto each other and may improve each other. For instance, the HRED may utilize the proposed late\nfusion, and the larger-context recurrent language model here may represent the context sentences as\na sequence of sequences of words, instead of a BoW context or a sequence of BoW vectors.\n4.3 S KIP -THOUGHT VECTORS\nPerhaps the most similar work is the skip-thought vector by Kiros et al. (2015). In their work, a\nrecurrent neural network is trained to read a current sentence, as a sequence of words, and extract\na so-called skip-thought vector of the sentence. There are two other recurrent neural networks\nwhich respectively model preceding and following sentences. If we only consider the prediction of\nthe following sentence, then this model becomes a larger-context recurrent language model which\nconsiders a single preceding sentence as a context.\nAs with the other previous works we have discussed so far, the major difference is in the ultimate\ngoal of the model. Kiros et al. (2015) fully focused on using their model to extract a good, generic\nsentence vector, while in this paper we are focused on obtaining a good language model. There are\nless major technical differences. First, the skip-thought vector model conditions only on the imme-\ndiate preceding sentence, while we extend this to multiple preceding sentences. The experiments\nlater will show the importance of having a larger context. Second, similarly to the two other previ-\nous works by Mikolov & Zweig (2012) and Serban et al. (2015), the skip-thought vector model only\nimplements early fusion.\n4.4 N EURAL MACHINE TRANSLATION : C ONDITIONAL LANGUAGE MODELLING\nNeural machine translation is another related approach (Forcada & ˜Neco, 1997; Kalchbrenner &\nBlunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014). In neural machine\ntranslation, often two recurrent neural networks are used. The ﬁrst recurrent neural network, called\nan encoder, reads a source sentence, represented as a sequence of words in a source language, to\nform a context vector, or a set of context vectors. The other recurrent neural network, called a\ndecoder, then, models the target translation conditioned on this source context.\nThis is similar to the proposed larger-context recurrent language model, if we consider the source\nsentence as a preceding sentence in a corpus. The major difference is in the ultimate application,\nmachine translation vs. language modelling, and technically, the differences between neural ma-\n7\nUnder review as a conference paper at ICLR 2016\nchine translation and the proposed larger-context language model are similar to those between the\nHRED and the larger-context language model.\nSimilarly to the other previous works we discussed earlier, it is possible to incorporate the proposed\nlarger-context language model into the existing neural machine translation framework, and also to\nincorporate advanced mechanisms from the neural machine translation framework. Attention mech-\nanism was introduced by Bahdanau et al. (2014) with intention to build a variable-length context\nrepresentation in source sentence. In larger-context language model, this mechanism is applied on\ncontext sentences (see Sec. 3.1,) and we present the results in the later section showing that the\nattention mechanism indeed improves the quality of language modelling.\n4.5 C ONTEXT -DEPENDENT QUESTION -ANSWERING MODELS\nContext-dependent question-answering is a task in which a model is asked to answer a question\nbased on the facts from a natural language paragraph. The question and answer are often formulated\nas ﬁlling in a missing word in a query sentence (Hermann et al., 2015; Hill et al., 2015). This task is\nclosely related to the larger-context language model we proposed in this paper in the sense that its\ngoal is to build a model to learn\np(qk|q<k,q>k,D), (12)\nwhere qk is the missing k-th word in a query Q, and q<k and q>k are the context words from the\nquery. Dis the paragraph containing facts about this query. Often, it is explicitly constructed so that\nthe query qdoes not appear in the paragraph D.\nIt is easy to see the similarity between Eq. (12) and one of the conditional probabilities in the r.h.s. of\nEq. (11). By replacing the context sentencesSl−1\nl−n in Eq. (11) withDin Eq. (12) and conditioningwt\non both the preceding and following words, we get a context-dependent question-answering model.\nIn other words, the proposed larger-context language model can be used for context-dependent\nquestion-answering, however, with computational overhead. The overhead comes from the fact that\nfor every possible answer the conditional probability completed query sentence must be evaluated.\n5 E XPERIMENTAL SETTINGS\n5.1 M ODELS\nThere are six possible combinations of the proposed methods. First, there are two ways of represent-\ning the context sentences; (1) bag-of-words (BoW) and (2) a sequence of bag-of-words (SeqBoW),\nfrom Sec. 3.1. There are two separate ways to incorporate the SeqBoW; (1) with attention mecha-\nnism (ATT) and (2) without it. Then, there are two ways of feeding the context vector into the main\nrecurrent language model (RLM); (1) early fusion (EF) and (2) late fusion (LF), from Sec. 3.2. We\nwill denote these six possible models by\n1. RLM-BoW-EF- n\n2. RLM-SeqBoW-EF- n\n3. RLM-SeqBoW-ATT-EF-n\n4. RLM-BoW-LF- n\n5. RLM-SeqBoW-LF- n\n6. RLM-SeqBoW-ATT-LF-n\nn denotes the number of preceding sentences to have as a set of context sentences. We test four\ndifferent values of n; 1, 2, 4 and 8.\nAs a baseline, we also train a recurrent language model without any context information. We refer\nto this model by RLM. Furthermore, we also report the result with the conventional, count-based\nn-gram language model with the modiﬁed Kneser-Ney smoothing with KenLM (Heaﬁeld et al.,\n2013).\n8\nUnder review as a conference paper at ICLR 2016\nIMDB BBC Penn TreeBank\n# Sentences # Words # Sentences # Words # Sentences # Words\nTraining 930,139 21M 37,207 890K 42,068 888K\nValidation 152,987 3M 1,998 49K 3,370 70K\nTest 151,987 3M 2,199 53K 3,761 79K\nTable 1: Statistics of IMDB, BBC and Penn TreeBank\nEach recurrent language model uses 1000 LSTM units and is trained with Adadelta (Zeiler, 2012)\nto maximize the log-likelihood deﬁned as\nL(θ) = 1\nK\nK∑\nk=1\nlog p(Sk|Sk−1\nk−n).\nWe early-stop training based on the validation log-likelihood and report the perplexity on the test set\nusing the best model according to the validation log-likelihood.\nWe use only those sentences of length up to 50 words when training a recurrent language model for\nthe computational reason. For KenLM, we used all available sentences in a training corpus.\n5.2 D ATASETS\nWe evaluate the proposed larger-context language model on three different corpora. For detailed\nstatistics, see Table 1.\nIMDB Movie Reviews A set of movie reviews is an ideal dataset to evaluate many different set-\ntings of the proposed larger-context language models, because each review is highly likely of a single\ntheme (the movie under review.) A set of words or the style of writing will be well determined based\non the preceding sentences.\nWe use the IMDB Move Review Corpus (IMDB) prepared by Maas et al. (2011). 1 This corpus\nhas 75k training reviews and 25k test reviews. We use the 30k most frequent words for recurrent\nlanguage models.\nBBC Similarly to movie reviews, each new article tends to convey a single theme. We use the\nBBC corpus prepared by Greene & Cunningham (2006). 2 Unlike the IMDB corpus, this corpus\ncontains news articles which are almost always written in a formal style. By evaluating the proposed\napproaches on both the IMDB and BBC corpora, we can tell whether the beneﬁts from larger context\nexist in both informal and formal languages. We use the 10k most frequent words for recurrent\nlanguage models.\nBoth with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization.3\nPenn Treebank We evaluate a normal recurrent language model, count-based n-gram language\nmodel as well as the proposed RLM-BoW-EF- nand RLM-BoW-LF-nwith varying n = 1,2,4,8\non the Penn Treebank Corpus. We preprocess the corpus according to (Mikolov et al., 2011) and use\na vocabulary of 10k words.\n6 R ESULTS AND ANALYSIS\n6.1 C ORPUS -LEVEL PERPLEXITY\nWe evaluated the models, including all the proposed approaches (RLM-{BoW,SeqBoW}-{ATT,∅}-\n{EF,LF}-n), on the IMDB corpus. In Fig. 2 (a), we see three major trends. First, RLM-BoW,\n1 http://ai.stanford.edu/˜amaas/data/sentiment/\n2 http://mlg.ucd.ie/datasets/bbc.html\n3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/\ntokenizer/tokenizer.perl\n9\nUnder review as a conference paper at ICLR 2016\n(a) IMDB (b) Penn Treebank (c) BBC\nFigure 2: Corpus-level perplexity on (a) IMDB, (b) Penn Treebank and (c) BBC. The count-based\n5-gram language models with Kneser-Ney smoothing respectively resulted in the perplexities of\n110.20, 148 and 127.32, and are not shown here. Note that we did not show SeqBoW in the cases of\nn= 1, as this is equivalent to BoW.\neither with the early fusion or late fusion, outperforms both the count-based n-gram and recurrent\nlanguage model (LSTM) regardless of the number of context sentences. Second, the improvement\ngrows as the number nof context sentences increases, and this is most visible with the novel late\nfusion. Lastly, we see that the RLM-SeqBoW does not work well regardless of the fusion type\n(RLM-SeqBow-EF not shown), while after using attention-based model RLM-SeqBow-ATT, the\nperformance is greatly improved.\nBecause of the second observation from the IMDB corpus, that the late fusion clearly outperforms\nthe early fusion, we evaluated only RLM-{BoW,SeqBoW}-{ATT}-LF-n’s on the other two corpora.\nOn the other two corpora, PTB and BBC, we observed a similar trend of RLM-SeqBoW-ATT-LF-\nn and RLM-BoW-LF-n outperforming the two conventional language models, and that this trend\nstrengthened as the number nof the context sentences grew. We also observed again that the RLM-\nSeqBoW-ATT-LF outperforms RLM-SeqBoW-LF and RLM-BoW in almost all the cases.\nFrom these experiments, the beneﬁt of allowing larger context to a recurrent language model is clear,\nhowever, with the right choice of the context representation (see Sec. 3.1) and the right mechanism\nfor feeding the context information to the recurrent language model (see Sec. 3.2.) In these exper-\niments, the sequence of bag-of-words representation with attention mechanism, together with the\nlate fusion was found to be the best choice in all three corpora.\nOne possible explanation on the failure of the SeqBoW representation with a context recurrent neural\nnetwork is that it is simply difﬁcult for the context recurrent neural network to compress multiple\nsentences into a single vector. This difﬁculty in training a recurrent neural network to compress\na long sequence into a single vector has been observed earlier, for instance, in neural machine\ntranslation (Cho et al., 2014a). Attention mechanism, which was found to avoid this problem in\nmachine translation (Bahdanau et al., 2014), is found to solve this problem in our task as well.\n6.2 A NALYSIS : P ERPLEXITY PER PART-OF-SPEECH TAG\nNext, we attempted at discovering why the larger-context recurrent language model outperforms\nthe unconditional recurrent language model. In order to do so, we computed the perplexity per\npart-of-speech (POS) tag.\nWe used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003)\nto tag each word of each sentence in the corpora. 4 We then computed the perplexity of each word\nand averaged them for each tag type separately. Among the 36 POS tags used by the Stanford POS\nTagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ,\nVB, PRP, CC), of which we combined NN and NNS into a new tage Noun and VB and VBZ into a\nnew tag Verb.\nWe show the results using the RLM-BoW-LF and RLM-SeqBoW-ATT-LF on all three corpora–\nIMDB, BBC and Penn Treebank– in Fig. 3. We observe that the predictability, measured by the\nperplexity (negatively correlated), grows most for nouns (Noun) and adjectives (JJ) as the number\n4 http://nlp.stanford.edu/software/tagger.shtml\n10\nUnder review as a conference paper at ICLR 2016\n(a) IMDB (b) BBC (b) Penn Treebank\n(i) RLM-BoW-LF\n(a) IMDB (b) BBC (b) Penn Treebank\n(ii) RLM-SeqBoW-ATT-LF\nFigure 3: Perplexity per POS tag on the (a) IMDB, (b) BBC and (c) Penn Treebank corpora.\nof context sentences increases. They are followed by verbs (Verb). In other words, nouns, adjectives\nand verbs are the ones which become more predictable by a language model given more context. We\nhowever noticed the relative degradation of quality in coordinating conjunctions (CC), determiners\n(DT) and personal pronouns (PRP).\nIt is worthwhile to note that nouns, adjectives and verbs are open-class, content, words, and conjunc-\ntions, determiners and pronouns are closed-class, function, words (see, e.g., Miller, 1999). The func-\ntions words often play grammatical roles, while the content words convey the content of a sentence\nor discourse, as the name indicates. From this, we may carefully conclude that the larger-context\nlanguage model improves upon the conventional, unconditional language model by capturing the\ntheme of a document, which is reﬂected by the improved perplexity on “content-heavy” open-class\nwords (Chung & Pennebaker, 2007). In our experiments, this came however at the expense of slight\ndegradation in the perplexity of function words, as the model’s capacity stayed same (though, it is\nnot necessary.)\nThis observation is in line with a recent ﬁnding by Hill et al. (2015). They also observed signiﬁ-\ncant gain in predicting open-class, or content, words when a question-answering model, including\nhumans, was allowed larger context.\n7 C ONCLUSION\nIn this paper, we proposed a method to improve language model on corpus-level by incorporating\nlarger context. Using this model results in the improvement in perplexity on the IMDB, BBC and\nPenn Treebank corpora, validating the advantage of providing larger context to a recurrent language\nmodel.\nFrom our experiments, we found that the sequence of bag-of-words with attention is better than bag-\nof-words for representing the context sentences (see Sec. 3.1), and the late fusion is better than the\nearly fusion for feeding the context vector into the main recurrent language model (see Sec. 3.2). Our\npart-of-speech analysis revealed that content words, including nouns, adjectives and verbs, beneﬁt\nmost from an increasing number of context sentences (see Sec. 6.2). This analysis suggests that\nlarger-context language model improves perplexity because it captures the theme of a document\nbetter and more easily.\n11\nUnder review as a conference paper at ICLR 2016\nTo explore the potential of such a model, there are several aspects in which more research needs\nto be done. First, the three datasets we used in this paper are relatively small in the context of\nlanguage modelling, therefore the proposed larger-context language model should be evaluated on\nlarger corpora. Second, more analysis, beyond the one based on part-of-speech tags, should be\nconducted in order to better understand the advantage of such larger-context models. Lastly, it is\nimportant to evaluate the impact of the proposed larger-context models in downstream tasks such as\nmachine translation and speech recognition.\nACKNOWLEDGMENTS\nThis work is done as a part of the course DS-GA 1010-001 Independent Study in Data Science at\nthe Center for Data Science, New York University.\nREFERENCES\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nBaltescu, Paul and Blunsom, Phil. Pragmatic neural language modelling in machine translation.\narXiv preprint arXiv:1412.7119, 2014.\nBengio, Yoshua, Simard, Patrice, and Frasconi, Paolo. Learning long-term dependencies with gra-\ndient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.\nBengio, Yoshua, Ducharme, R´ejean, Vincent, Pascal, and Janvin, Christian. A neural probabilistic\nlanguage model. The Journal of Machine Learning Research, 3:1137–1155, 2003.\nBlei, David M, Ng, Andrew Y , and Jordan, Michael I. Latent dirichlet allocation. the Journal of\nmachine Learning research, 3:993–1022, 2003.\nCho, Kyunghyun, van Merrienboer, Bart, Bahdanau, Dzmitry, and Bengio, Yoshua. On the proper-\nties of neural machine translation: Encoder-decoder approaches. In Eighth Workshop on Syntax,\nSemantics and Structure in Statistical Translation (SSST-8), 2014a.\nCho, Kyunghyun, Van Merri ¨enboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry, Bougares, Fethi,\nSchwenk, Holger, and Bengio, Yoshua. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014b.\nChung, Cindy and Pennebaker, James W. The psychological functions of function words. Social\ncommunication, pp. 343–359, 2007.\nChung, Junyoung, Gulcehre, Caglar, Cho, Kyunghyun, and Bengio, Yoshua. Gated feedback recur-\nrent neural networks. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML-15), pp. 2067–2075, 2015.\nDumais, Susan T. Latent semantic analysis. Annual review of information science and technology,\n38(1):188–230, 2004.\nForcada, Mikel L and ˜Neco, Ram ´on P. Recursive hetero-associative memories for translation. In\nBiological and Artiﬁcial Computation: From Neuroscience to Technology, pp. 453–462. Springer,\n1997.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\nGreene, Derek and Cunningham, P´adraig. Practical solutions to the problem of diagonal dominance\nin kernel document clustering. In Proc. 23rd International Conference on Machine learning\n(ICML’06), pp. 377–384. ACM Press, 2006.\nGreff, Klaus, Srivastava, Rupesh Kumar, Koutn ´ık, Jan, Steunebrink, Bas R, and Schmidhuber,\nJ¨urgen. Lstm: A search space odyssey. arXiv preprint arXiv:1503.04069, 2015.\n12\nUnder review as a conference paper at ICLR 2016\nHeaﬁeld, Kenneth, Pouzyrevsky, Ivan, Clark, Jonathan H., and Koehn, Philipp. Scalable modi-\nﬁed Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics , pp. 690–696, Soﬁa, Bulgaria, August 2013. URL\nhttp://kheafield.com/professional/edinburgh/estimate_paper.pdf.\nHermann, Karl Moritz, Ko ˇcisk`y, Tom´aˇs, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Suley-\nman, Mustafa, and Blunsom, Phil. Teaching machines to read and comprehend. arXiv preprint\narXiv:1506.03340, 2015.\nHill, Felix, Bordes, Antoine, Chopra, Sumit, and Weston, Jason. The goldilocks principle: Reading\nchildren’s books with explicit memory representations.arXiv preprint arXiv:1511.02301, 2015.\nHochreiter, Sepp and Schmidhuber, J ¨urgen. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nJozefowicz, Rafal, Zaremba, Wojciech, and Sutskever, Ilya. An empirical exploration of recurrent\nnetwork architectures. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML-15), pp. 2342–2350, 2015.\nKalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. In EMNLP, pp.\n1700–1709, 2013.\nKiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Richard S, Torralba, Antonio, Urtasun,\nRaquel, and Fidler, Sanja. Skip-thought vectors. arXiv preprint arXiv:1506.06726, 2015.\nKneser, Reinhard and Ney, Hermann. Improved backing-off for m-gram language modeling. In\nAcoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on,\nvolume 1, pp. 181–184. IEEE, 1995.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y , and Potts, Christo-\npher. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp.\n142–150. Association for Computational Linguistics, 2011.\nMikolov, Tomas and Zweig, Geoffrey. Context dependent recurrent neural network language model.\nIn SLT, pp. 234–239, 2012.\nMikolov, Tomas, Karaﬁ´at, Martin, Burget, Lukas, Cernock `y, Jan, and Khudanpur, Sanjeev. Recur-\nrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference\nof the International Speech Communication Association, Makuhari, Chiba, Japan, September\n26-30, 2010, pp. 1045–1048, 2010.\nMikolov, Tom´aˇs, Kombrink, Stefan, Burget, Luk ´aˇs, ˇCernock`y, Jan Honza, and Khudanpur, San-\njeev. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2011 IEEE International Conference on, pp. 5528–5531. IEEE, 2011.\nMikolov, Tomas, Joulin, Armand, Chopra, Sumit, Mathieu, Michael, and Ranzato, Marc’Aurelio.\nLearning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.\nMiller, George A. On knowing a word. Annual Review of psychology, 50(1):1–19, 1999.\nRosenfeld, Ronald. Two decades of statistical language modeling: where do we go from here. In\nProceedings of the IEEE, 2000.\nRumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by\nback-propagating errors. Cognitive modeling, 5:3, 1988.\nSchwenk, Holger. Continuous space language models. Computer Speech & Language, 21(3):492–\n518, 2007.\nSerban, Iulian V , Sordoni, Alessandro, Bengio, Yoshua, Courville, Aaron, and Pineau, Joelle. Hier-\narchical neural network generative models for movie dialogues.arXiv preprint arXiv:1507.04808,\n2015.\n13\nUnder review as a conference paper at ICLR 2016\nSukhbaatar, Sainbayar, Szlam, Arthur, Weston, Jason, and Fergus, Rob. End-to-end memory net-\nworks. arXiv preprint arXiv:1503.08895, 2015.\nSundermeyer, Martin, Ney, Hermann, and Schluter, Ralf. From feedforward to recurrent lstm neural\nnetworks for language modeling. Audio, Speech, and Language Processing, IEEE/ACM Transac-\ntions on, 23(3):517–529, 2015.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV . Sequence to sequence learning with neural net-\nworks. In Advances in neural information processing systems, pp. 3104–3112, 2014.\nToutanova, Kristina, Klein, Dan, Manning, Christopher D, and Singer, Yoram. Feature-rich part-of-\nspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the\nNorth American Chapter of the Association for Computational Linguistics on Human Language\nTechnology-Volume 1, pp. 173–180. Association for Computational Linguistics, 2003.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701,\n2012.\n14",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.947059154510498
    },
    {
      "name": "Computer science",
      "score": 0.8494378924369812
    },
    {
      "name": "Language model",
      "score": 0.7554490566253662
    },
    {
      "name": "Context (archaeology)",
      "score": 0.7131358981132507
    },
    {
      "name": "Natural language processing",
      "score": 0.694683313369751
    },
    {
      "name": "Sentence",
      "score": 0.6541768312454224
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6513962149620056
    },
    {
      "name": "Context model",
      "score": 0.49839282035827637
    },
    {
      "name": "Word (group theory)",
      "score": 0.4894677400588989
    },
    {
      "name": "Linguistics",
      "score": 0.28209516406059265
    },
    {
      "name": "Object (grammar)",
      "score": 0.06398919224739075
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}