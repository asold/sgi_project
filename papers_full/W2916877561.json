{
  "title": "The Annotated Transformer",
  "url": "https://openalex.org/W2916877561",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2294834069",
      "name": "Alexander Rush",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2594990650",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964265128"
  ],
  "abstract": "A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.",
  "full_text": "Proceedings of Workshop for NLP Open Source Software, pages 52–60\nMelbourne, Australia, July 20, 2018.c⃝2018 Association for Computational Linguistics\n52\nThe Annotated Transformer\nAlexander M. Rush\nsrush@seas.harvard.edu\nHarvard University\nAbstract\nA major aim of open-source NLP is\nto quickly and accurately reproduce\nthe results of new work, in a manner\nthat the community can easily use and\nmodify. While most papers publish\nenough detail for replication, it still\nmay be difﬁcult to achieve good re-\nsults in practice. This paper is an ex-\nperiment. In it, I consider a worked\nexercise with the goal of implement-\ning the results of the recent paper.\nThe replication exercise aims at sim-\nple code structure that follows closely\nwith the original work, while achiev-\ning an efﬁcient usable system. An im-\nplicit premise of this exercise is to en-\ncourage researchers to consider this\nmethod for new results.\n1 Introduction\nReplication of published results remains a\nchallenging issue in open-source NLP . When\na new paper is published with major im-\nprovements, it is common for many mem-\nbers of the community to independently re-\nproduce the numbers experimentally, which\nis often a struggle. Practically this makes it\ndifﬁcult to improve scores, but also impor-\ntantly it is a pedagogical issue if students can-\nnot reproduce results from scientiﬁc publica-\ntions.\nThe recent turn towards deep learning has\nexerbated this issue. New models require\nextensive hyperparameter tuning and long\ntraining times. Small mistakes can cause ma-\njor issues. Fortunately though, new toolsets\nhave made it possible to write simpler more\nmathematically declarative code.\nIn this experimental paper, I propose an ex-\nercise in open-source NLP . The goal is to tran-\nscribe a recent paper into a simple and under-\nstandable form. The document itself is pre-\nsented as an annotated paper. That is the\nmain document (in different font) is an ex-\ncerpt of the recent paper “Attention is All You\nNeed” (Vaswani et al., 2017). I add annota-\ntion in the form of italicized comments and\ninclude code in PyTorch directly in the paper\nitself.\nNote this document itself is presented as a\nblog post 1 and is completely executable as a\nnotebook. In the spirit of reproducibility this\nwork itself is distilled from the same source\nwith images inline.\n1Presented at http://nlp.seas.harvard.\nedu/2018/04/03/attention.html with source\ncode at https://github.com/harvardnlp/\nannotated-transformer\n53\n2 Background\nThe goal of reducing sequential computa-\ntion also forms the foundation of the Extended\nNeural GPU, ByteNet and ConvS2S, all of\nwhich use convolutional neural networks as\nbasic building block, computing hidden rep-\nresentations in parallel for all input and out-\nput positions. In these models, the number\nof operations required to relate signals from\ntwo arbitrary input or output positions grows\nin the distance between positions, linearly\nfor ConvS2S and logarithmically for ByteNet.\nThis makes it more difﬁcult to learn depen-\ndencies between distant positions. In the\nTransformer this is reduced to a constant\nnumber of operations, albeit at the cost of\nreduced effective resolution due to averag-\ning attention-weighted positions, an effect we\ncounteract with Multi-Head Attention.\nSelf-attention, sometimes called intra-\nattention is an attention mechanism relating\ndifferent positions of a single sequence in or-\nder to compute a representation of the se-\nquence. Self-attention has been used suc-\ncessfully in a variety of tasks including read-\ning comprehension, abstractive summariza-\ntion, textual entailment and learning task-\nindependent sentence representations. End-\nto-end memory networks are based on a re-\ncurrent attention mechanism instead of se-\nquencealigned recurrence and have been\nshown to perform well on simple-language\nquestion answering and language modeling\ntasks.\nTo the best of our knowledge, however, the\nTransformer is the ﬁrst transduction model re-\nlying entirely on self-attention to compute rep-\nresentations of its input and output without\nusing sequence aligned RNNs or convolution.\n3 Model Architecture\nMost competitive neural sequence trans-\nduction models have an encoder-decoder\nstructure (Bahdanau et al., 2014). Here, the\nencoder maps an input sequence of symbol\nrepresentations (x1, ...,xn) to a sequence of\ncontinuous representations z = ( z1, ...,zn).\nGiven z, the decoder then generates an out-\nput sequence (y1, ...,ym) of symbols one el-\nement at a time. At each step the model\nis auto-regressive (Graves, 2013), consum-\ning the previously generated symbols as ad-\nditional input when generating the next.\nclass EncoderDecoder(nn.Module):\n\"\"\"\nA standard Encoder-Decoder architecture.\nBase for this and many other models.\n\"\"\"\ndef __init__(self, encoder, decoder, src_embed,\ntgt_embed, generator):\nsuper(EncoderDecoder, self).__init__()\nself.encoder = encoder\nself.decoder = decoder\nself.src_embed = src_embed\nself.tgt_embed = tgt_embed\nself.generator = generator\ndef forward(self, src, tgt, src_mask, tgt_mask):\n\"Take in and process masked src and target sequences.\"\nreturn self.decode(self.encode(src, src_mask),\nsrc_mask,\ntgt, tgt_mask)\ndef encode(self, src, src_mask):\nreturn self.encoder(self.src_embed(src), src_mask)\ndef decode(self, memory, src_mask, tgt, tgt_mask):\nreturn self.decoder(self.tgt_embed(tgt), memory,\nsrc_mask, tgt_mask)\nclass Generator(nn.Module):\n\"Define standard linear + softmax generation step.\"\ndef __init__(self, d_model, vocab):\nsuper(Generator, self).__init__()\nself.proj = nn.Linear(d_model, vocab)\ndef forward(self, x):\nreturn F.log_softmax(self.proj(x), dim=-1)\nThe Transformer follows this overall archi-\ntecture using stacked self-attention and point-\nwise, fully connected layers for both the en-\ncoder and decoder, shown in the left and right\nhalves of Figure 1, respectively.\n\n54\n3.1 Encoder and Decoder Stacks\n3.1.1 Encoder\nThe encoder is composed of a stack ofN = 6\nidentical layers.\ndef clones(module, N):\n\"Produce N identical layers.\"\nreturn nn.ModuleList([copy.deepcopy(module)\nfor _ in range(N)])\nclass Encoder(nn.Module):\n\"Core encoder is a stack of N layers\"\ndef __init__(self, layer, N):\nsuper(Encoder, self).__init__()\nself.layers = clones(layer, N)\nself.norm = LayerNorm(layer.size)\ndef forward(self, x, mask):\n\"Pass the input/mask through each layer in turn.\"\nfor layer in self.layers:\nx = layer(x, mask)\nreturn self.norm(x)\nWe employ a residual connection (He et al.,\n2016) around each of the two sub-layers, fol-\nlowed by layer normalization (Ba et al., 2016).\nclass LayerNorm(nn.Module):\n\"Construct a layernorm module (See citation for details).\"\ndef __init__(self, features, eps=1e-6):\nsuper(LayerNorm, self).__init__()\nself.a_2 = nn.Parameter(torch.ones(features))\nself.b_2 = nn.Parameter(torch.zeros(features))\nself.eps = eps\ndef forward(self, x):\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nreturn (self.a_2 * (x - mean) /\n(std + self.eps) + self.b_2)\nThat is, the output of each sub-layer\nis LayerNorm(x + Sublayer(x)), where\nSublayer(x) is the function implemented\nby the sub-layer itself. We apply dropout\n(Srivastava et al., 2014) to the output of each\nsub-layer, before it is added to the sub-layer\ninput and normalized.\nTo facilitate these residual connections, all\nsub-layers in the model, as well as the em-\nbedding layers, produce outputs of dimension\ndmodel = 512.\nclass SublayerConnection(nn.Module):\n\"\"\"\nA layer norm followed by a residual connection.\nNote norm is not applied to residual x.\n\"\"\"\ndef __init__(self, size, dropout):\nsuper(SublayerConnection, self).__init__()\nself.norm = LayerNorm(size)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, x, sublayer):\n\"Apply residual connection to sublayer fn.\"\nreturn x + self.dropout(sublayer(self.norm(x)))\nEach layer has two sub-layers. The ﬁrst is a\nmulti-head self-attention mechanism, and the\nsecond is a simple, position-wise fully con-\nnected feed-forward network.\nclass EncoderLayer(nn.Module):\n\"Encoder calls self-attn and feed forward.\"\ndef __init__(self, size, self_attn,\nfeed_forward, dropout):\nsuper(EncoderLayer, self).__init__()\nself.self_attn = self_attn\nself.feed_forward = feed_forward\nsublayer = SublayerConnection(size, dropout)\nself.sublayer = clones(sublayer, 2)\nself.size = size\ndef forward(self, x, mask):\n\"Follow Figure 1 (left) for connections.\"\nx = self.sublayer[0](x, lambda x:\nself.self_attn(x, x, x, mask))\nreturn self.sublayer[1](x, self.feed_forward)\n3.1.2 Decoder\nThe decoder is also composed of a stack of\nN = 6 identical layers.\nclass Decoder(nn.Module):\n\"Generic N layer decoder with masking.\"\ndef __init__(self, layer, N):\nsuper(Decoder, self).__init__()\nself.layers = clones(layer, N)\nself.norm = LayerNorm(layer.size)\ndef forward(self, x, memory, src_mask, tgt_mask):\nfor layer in self.layers:\nx = layer(x, memory, src_mask, tgt_mask)\nreturn self.norm(x)\nIn addition to the two sub-layers in each\nencoder layer, the decoder inserts a third\nsub-layer, which performs multi-head atten-\ntion over the output of the encoder stack.\nSimilar to the encoder, we employ residual\nconnections around each of the sub-layers,\nfollowed by layer normalization.\nclass DecoderLayer(nn.Module):\n\"Decoder calls self-attn, src-attn, and feed forward.\"\ndef __init__(self, size, self_attn,\nsrc_attn, feed_forward, dropout):\nsuper(DecoderLayer, self).__init__()\nself.self_attn = self_attn\nself.src_attn = src_attn\nself.feed_forward = feed_forward\nsublayer = SublayerConnection(size, dropout)\nself.sublayer = clones(sublayer, 3)\nself.size = size\ndef forward(self, x, memory, s_mask, t_mask):\n\"Follow Figure 1 (right) for connections.\"\nm = memory\nx = self.sublayer[0](x, lambda x:\nself.self_attn(x, x, x, t_mask))\nx = self.sublayer[1](x, lambda x:\nself.src_attn(x, m, m, s_mask))\nreturn self.sublayer[2](x, self.feed_forward)\nWe also modify the self-attention sub-layer\nin the decoder stack to prevent positions\nfrom attending to subsequent positions. This\nmasking, combined with fact that the output\nembeddings are offset by one position, en-\nsures that the predictions for position i can\ndepend only on the known outputs at posi-\ntions less than i.\ndef subsequent_mask(size):\n\"Mask out subsequent positions.\"\nattn_shape = (1, size, size)\nsubsequent_mask = np.triu(np.ones(attn_shape), k=1)\nreturn torch.from_numpy(\nsubsequent_mask.astype('uint8')) == 0\n55\n3.1.3 Attention\nAn attention function can be described as\nmapping a query and a set of key-value pairs\nto an output, where the query, keys, values,\nand output are all vectors. The output is com-\nputed as a weighted sum of the values, where\nthe weight assigned to each value is com-\nputed by a compatibility function of the query\nwith the corresponding key.\nWe call our particular attention \"Scaled\nDot-Product Attention\". The input consists of\nqueries and keys of dimension dk, and values\nof dimension dv. We compute the dot prod-\nucts of the query with all keys, divide each by√dk, and apply a softmax function to obtain\nthe weights on the values.\nIn practice, we compute the attention func-\ntion on a set of queries simultaneously,\npacked together into a matrix Q. The keys\nand values are also packed together into ma-\ntrices K and V. We compute the matrix of\noutputs as:\nAttention(Q, K, V) =softmax( QKT\n√dk\n)V\ndef attention(query, key, value, mask=None, dropout=None):\n\"Compute 'Scaled Dot Product Attention'\"\nd_k = query.size(-1)\nkey_t = key.transpose(-2, -1)\nscores = torch.matmul(query, key_t) / math.sqrt(d_k)\nif mask is not None:\nscores = scores.masked_fill(mask == 0, -1e9)\np_attn = F.softmax(scores, dim=-1)\nif dropout is not None:\np_attn = dropout(p_attn)\nreturn torch.matmul(p_attn, value), p_attn\nThe two most commonly used attention\nfunctions are additive attention (Bahdanau\net al., 2014), and dot-product (multiplicative)\nattention. Dot-product attention is identical to\nour algorithm, except for the scaling factor of\n1√dk\n. Additive attention computes the com-\npatibility function using a feed-forward net-\nwork with a single hidden layer. While the\ntwo are similar in theoretical complexity, dot-\nproduct attention is much faster and more\nspace-efﬁcient in practice, since it can be im-\nplemented using highly optimized matrix mul-\ntiplication code.\nWhile for small values of dk the two mech-\nanisms perform similarly, additive attention\noutperforms dot product attention without\nscaling for larger values of dk (Britz et al.,\n2017). We suspect that for large values of\ndk, the dot products grow large in magni-\ntude, pushing the softmax function into re-\ngions where it has extremely small gradients\n(To illustrate why the dot products get large,\nassume that the components of q and k are\nindependent random variables with mean 0\nand variance 1. Then their dot product, q ·k =\n∑dk\ni=1 qiki, has mean 0 and variance dk.). To\ncounteract this effect, we scale the dot prod-\nucts by 1√dk\n.\nMulti-head attention allows the model to\njointly attend to information from different\nrepresentation subspaces at different posi-\ntions. With a single attention head, averaging\ninhibits this.\nMultiHead(Q, K, V) =Concat(head1, ..., headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , VWV\ni )\nWhere the projections are parameter ma-\ntrices WQ\ni ∈ Rdmodel×dk , WK\ni ∈ Rdmodel×dk ,\nWV\ni ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel . In\nthis work we employ h = 8 parallel attention\nlayers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced\n56\ndimension of each head, the total computa-\ntional cost is similar to that of single-head at-\ntention with full dimensionality.\nclass MultiHeadedAttention(nn.Module):\ndef __init__(self, h, d_model, dropout=0.1):\n\"Take in model size and number of heads.\"\nsuper(MultiHeadedAttention, self).__init__()\nassert d_model % h == 0\n# We assume d_v always equals d_k\nself.d_k = d_model // h\nself.h = h\nself.linears = clones(nn.Linear(d_model, d_model), 4)\nself.attn = None\nself.dropout = nn.Dropout(p=dropout)\ndef forward(self, query, key, value, mask=None):\n\"Implements Figure 2\"\nif mask is not None:\n# Same mask applied to all h heads.\nmask = mask.unsqueeze(1)\nnb = query.size(0)\n# 1) Do all the linear projections in batch from d_model => h x d_k\nquery, key, value = [\nl(x).view(nb, -1, self.h, self.d_k).transpose(1, 2)\nfor l, x in zip(self.linears, (query, key, value))]\n# 2) Apply attention on all the projected vectors in batch.\nx, self.attn = attention(query, key, value, mask=mask,\ndropout=self.dropout)\n# 3) \"Concat\" using a view and apply a final linear.\nx = x.transpose(1, 2).contiguous().view(\nnb, -1, self.h * self.d_k)\nreturn self.linears[-1](x)\n3.2 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of\nthe layers in our encoder and decoder con-\ntains a fully connected feed-forward network,\nwhich is applied to each position separately\nand identically. This consists of two linear\ntransformations with a ReLU activation in be-\ntween.\nFFN(x) =max(0, xW1 + b1)W2 + b2\nWhile the linear transformations are the same\nacross different positions, they use different\nparameters from layer to layer. Another way\nof describing this is as two convolutions with\nkernel size 1. The dimensionality of input and\noutput is dmodel = 512, and the inner-layer has\ndimensionality df f = 2048.\nclass PositionwiseFeedForward(nn.Module):\n\"Implements FFN equation.\"\ndef __init__(self, d_model, d_ff, dropout=0.1):\nsuper(PositionwiseFeedForward, self).__init__()\nself.w_1 = nn.Linear(d_model, d_ff)\nself.w_2 = nn.Linear(d_ff, d_model)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, x):\nreturn self.w_2(self.dropout(F.relu(self.w_1(x))))\n3.3 Embeddings and Softmax\nSimilarly to other sequence transduction\nmodels, we use learned embeddings to con-\nvert the input tokens and output tokens to\nvectors of dimension dmodel. We also use\nthe usual learned linear transformation and\nsoftmax function to convert the decoder out-\nput to predicted next-token probabilities. In\nour model, we share the same weight ma-\ntrix between the two embedding layers and\nthe pre-softmax linear transformation, similar\nto (Press and Wolf, 2016). In the embedding\nlayers, we multiply those weights by √dmodel.\nclass Embeddings(nn.Module):\ndef __init__(self, d_model, vocab):\nsuper(Embeddings, self).__init__()\nself.lut = nn.Embedding(vocab, d_model)\nself.d_model = d_model\ndef forward(self, x):\nreturn self.lut(x) * math.sqrt(self.d_model)\n3.4 Positional Encoding\nSince our model contains no recurrence and\nno convolution, in order for the model to make\nuse of the order of the sequence, we must in-\nject some information about the relative or ab-\nsolute position of the tokens in the sequence.\nTo this end, we add \"positional encodings\" to\nthe input embeddings at the bottoms of the\nencoder and decoder stacks. The positional\nencodings have the same dimension dmodel\nas the embeddings, so that the two can be\nsummed. There are many choices of posi-\ntional encodings, learned and ﬁxed (Gehring\net al., 2017).\nIn this work, we use sine and cosine func-\ntions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel )\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimen-\nsion. That is, each dimension of the posi-\ntional encoding corresponds to a sinusoid.\nThe wavelengths form a geometric progres-\nsion from 2π to 10000 ·2π. We chose this\nfunction because we hypothesized it would\nallow the model to easily learn to attend by\nrelative positions, since for any ﬁxed offset k,\nPEpos+k can be represented as a linear func-\ntion of PEpos .\nIn addition, we apply dropout to the sums of\nthe embeddings and the positional encodings\nin both the encoder and decoder stacks. For\nthe base model, we use a rate of Pdrop = 0.1.\n57\nclass PositionalEncoding(nn.Module):\n\"Implement the PE function.\"\ndef __init__(self, d_model, dropout, max_len=5000):\nsuper(PositionalEncoding, self).__init__()\nself.dropout = nn.Dropout(p=dropout)\n# Compute the positional encodings once in log space.\npe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(0, d_model, 2) *\n-(math.log(10000.0) / d_model))\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\npe = pe.unsqueeze(0)\nself.register_buffer('pe', pe)\ndef forward(self, x):\nx = x + Variable(self.pe[:, :x.size(1)],\nrequires_grad=False)\nreturn self.dropout(x)\nplt.figure(figsize=(15, 5))\npe = PositionalEncoding(20, 0)\ny = pe.forward(Variable(torch.zeros(1, 100, 20)))\nplt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\nplt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\nNone\nWe also experimented with using learned\npositional embeddings (Gehring et al., 2017)\ninstead, and found that the two versions pro-\nduced nearly identical results. We chose\nthe sinusoidal version because it may al-\nlow the model to extrapolate to sequence\nlengths longer than the ones encountered\nduring training.\ndef make_model(src_vocab, tgt_vocab, N=6,\nd_model=512, d_ff=2048, h=8, dropout=0.1):\n\"Helper: Construct a model from hyperparameters.\"\nc = copy.deepcopy\nattn = MultiHeadedAttention(h, d_model)\nff = PositionwiseFeedForward(d_model, d_ff, dropout)\nposition = PositionalEncoding(d_model, dropout)\nd = d_model\nmodel = EncoderDecoder(\nEncoder(EncoderLayer(d, c(attn), c(ff), dropout), N),\nDecoder(DecoderLayer(d, c(attn), c(attn),\nc(ff), dropout), N),\nnn.Sequential(Embeddings(d, src_vocab), c(position)),\nnn.Sequential(Embeddings(d, tgt_vocab), c(position)),\nGenerator(d_model, tgt_vocab))\n# This was important from their code.\n# Initialize parameters with Glorot / fan_avg.\nfor p in model.parameters():\nif p.dim() > 1:\nnn.init.xavier_uniform(p)\nreturn model\n4 Training\nThis section describes the training regime for\nour models.\n4.1 Batches and Masking\nclass Batch:\n\"Batch of data with mask for training.\"\ndef __init__(self, src, trg=None, pad=0):\nself.src = src\nself.src_mask = (src != pad).unsqueeze(-2)\nif trg is not None:\nself.trg = trg[:, :-1]\nself.trg_y = trg[:, 1:]\nself.trg_mask = self.make_std_mask(self.trg, pad)\nself.ntokens = (self.trg_y != pad).data.sum()\n@staticmethod\ndef make_std_mask(tgt, pad):\n\"Create a mask to hide padding and future words.\"\ntgt_mask = (tgt != pad).unsqueeze(-2)\ntgt_mask = tgt_mask & Variable(\nsubsequent_mask(tgt.size(-1))\n.type_as(tgt_mask.data))\nreturn tgt_mask\n4.2 Training Loop\ndef run_epoch(data_iter, model, loss_compute):\n\"Standard Training and Logging Function\"\nstart = time.time()\ntotal_tokens = 0\ntotal_loss = 0\ntokens = 0\nfor i, batch in enumerate(data_iter):\nout = model.forward(batch.src, batch.trg,\nbatch.src_mask, batch.trg_mask)\nloss = loss_compute(out, batch.trg_y, batch.ntokens)\ntotal_loss += loss\ntotal_tokens += batch.ntokens\ntokens += batch.ntokens\nif i % 50 == 1:\nelapsed = time.time() - start\nprint(\"Epoch Step: %d Loss: %f Tokens / Sec: %f\" %\n(i, loss / batch.ntokens, tokens / elapsed))\nstart = time.time()\ntokens = 0\nreturn total_loss / total_tokens\n4.3 Training Data and Batching\nWe trained on the standard WMT 2014\nEnglish-German dataset consisting of about\n4.5 million sentence pairs. Sentences were\nencoded using byte-pair encoding, which has\na shared source-target vocabulary of about\n37000 tokens. For English-French, we used\nthe signiﬁcantly larger WMT 2014 English-\nFrench dataset consisting of 36M sentences\nand split tokens into a 32000 word-piece vo-\ncabulary.\nSentence pairs were batched together by\napproximate sequence length. Each training\nbatch contained a set of sentence pairs con-\ntaining approximately 25000 source tokens\nand 25000 target tokens.\nglobal max_src_in_batch, max_tgt_in_batch\ndef batch_size_fn(new, count, sofar):\n\"Calculate total number of tokens + padding.\"\nglobal max_src_in_batch, max_tgt_in_batch\nif count == 1:\nmax_src_in_batch = 0\nmax_tgt_in_batch = 0\nmax_src_in_batch = max(max_src_in_batch,\nlen(new.src))\nmax_tgt_in_batch = max(max_tgt_in_batch,\nlen(new.trg) + 2)\nsrc_elements = count * max_src_in_batch\ntgt_elements = count * max_tgt_in_batch\nreturn max(src_elements, tgt_elements)\n4.4 Hardware and Schedule\nWe trained our models on one machine with 8\nNVIDIA P100 GPUs. For our base models us-\ning the hyperparameters described through-\n58\nout the paper, each training step took about\n0.4 seconds. We trained the base models for\na total of 100,000 steps or 12 hours. For our\nbig models, step time was 1.0 seconds. The\nbig models were trained for 300,000 steps\n(3.5 days).\n4.5 Optimizer\nWe used the Adam optimizer (Kingma and\nBa, 2014) with β1 = 0.9, β2 = 0.98 and\nϵ = 10−9. We varied the learning rate over\nthe course of training, according to the for-\nmula:\nlrate = d−0.5\nmodel·\nmin(step _num−0.5, step _num ·warmup _steps −1.5)\nThis corresponds to increasing the learning\nrate linearly for the ﬁrst warmup _steps train-\ning steps, and decreasing it thereafter propor-\ntionally to the inverse square root of the step\nnumber. We used warmup _steps = 4000.\nclass NoamOpt:\n\"Optim wrapper that implements rate.\"\ndef __init__(self, model_size, factor,\nwarmup, optimizer):\nself.optimizer = optimizer\nself._step = 0\nself.warmup = warmup\nself.factor = factor\nself.model_size = model_size\nself._rate = 0\ndef step(self):\n\"Update parameters and rate\"\nself._step += 1\nrate = self.rate()\nfor p in self.optimizer.param_groups:\np['lr'] = rate\nself._rate = rate\nself.optimizer.step()\ndef rate(self, step=None):\n\"Implement `lrate` above\"\nif step is None:\nstep = self._step\nreturn self.factor * (\nself.model_size ** (-0.5) *\nmin(step ** (-0.5), step * self.warmup ** (-1.5)))\ndef get_std_opt(model):\nreturn NoamOpt(model.src_embed[0].d_model, 2, 4000,\ntorch.optim.Adam(model.parameters(),\nlr=0,\nbetas=(0.9, 0.98),\neps=1e-9))\n# Three settings of the lrate hyperparameters.\nopts = [NoamOpt(512, 1, 4000, None),\nNoamOpt(512, 1, 8000, None),\nNoamOpt(256, 1, 4000, None)]\nplt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts]\nfor i in range(1, 20000)])\nplt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\nNone\n4.6 Regularization\n4.6.1 Label Smoothing\nDuring training, we employed label smooth-\ning of value ϵls = 0.1 (Szegedy et al., 2015).\nThis hurts perplexity, as the model learns to\nbe more unsure, but improves accuracy and\nBLEU score.\nclass LabelSmoothing(nn.Module):\n\"Implement label smoothing.\"\ndef __init__(self, size, padding_idx, smoothing=0.0):\nsuper(LabelSmoothing, self).__init__()\nself.criterion = nn.KLDivLoss(size_average=False)\nself.padding_idx = padding_idx\nself.confidence = 1.0 - smoothing\nself.smoothing = smoothing\nself.size = size\nself.true_dist = None\ndef forward(self, x, target):\nassert x.size(1) == self.size\ntrue_dist = x.data.clone()\ntrue_dist.fill_(self.smoothing / (self.size - 2))\ntrue_dist.scatter_(1, target.data.unsqueeze(1),\nself.confidence)\ntrue_dist[:, self.padding_idx] = 0\nmask = torch.nonzero(target.data == self.padding_idx)\nif mask.dim() > 0:\ntrue_dist.index_fill_(0, mask.squeeze(), 0.0)\nself.true_dist = true_dist\nreturn self.criterion(x,\nVariable(true_dist,\nrequires_grad=False))\n#Example of label smoothing.\ncrit = LabelSmoothing(5, 0, 0.4)\npredict = torch.FloatTensor(\n[[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0]])\nv = crit(Variable(predict.log()),\nVariable(torch.LongTensor([2, 1, 0])))\n# Show the target distributions expected by the system.\nplt.imshow(crit.true_dist)\nNone\n\n59\ncrit = LabelSmoothing(5, 0, 0.1)\ndef loss(x):\nd = x + 3 * 1\npredict = torch.FloatTensor([[0, x / d, 1 / d,\n1 / d, 1 / d]])\nreturn crit(Variable(predict.log()),\nVariable(torch.LongTensor([1]))).data[0]\nplt.plot(np.arange(1, 100),\n[loss(x) for x in range(1, 100)])\nNone\n4.7 Loss Computation\nclass SimpleLossCompute:\n\"A simple loss compute and train function.\"\ndef __init__(self, generator,\ncriterion, opt=None):\nself.generator = generator\nself.criterion = criterion\nself.opt = opt\ndef __call__(self, x, y, norm):\nx = self.generator(x)\nloss = self.criterion(\nx.contiguous().view(-1, x.size(-1)),\ny.contiguous().view(-1)) / norm\nloss.backward()\nif self.opt is not None:\nself.opt.step()\nself.opt.optimizer.zero_grad()\nreturn loss.data[0] * norm\n5 Decoding and Visualization\n5.1 Greedy Decoding\ndef greedy_decode(model, src, src_mask,\nmax_len, start_sym):\nmemory = model.encode(src, src_mask)\nys = torch.ones(1, 1).fill_(start_sym).type_as(src.data)\nfor i in range(max_len - 1):\nout = model.decode(memory, src_mask,\nVariable(ys),\nVariable(\nsubsequent_mask(ys.size(1))\n.type_as(src.data)))\nprob = model.generator(out[:, -1])\n_, next_word = torch.max(prob, dim=1)\nnext_word = next_word.data[0]\nys = torch.cat([ys,\ntorch.ones(1, 1)\n.type_as(src.data)\n.fill_(next_word)],\ndim=1)\nreturn ys\nmodel.eval()\nsent = \"\"\"@@@The @@@log @@@file @@@can @@@be @@@sent @@@secret ly\n@@@with @@@email @@@or @@@FTP @@@to @@@a @@@specified\n@@@receiver\"\"\".split()\nsrc = torch.LongTensor([[SRC.stoi[w] for w in sent]])\nsrc = Variable(src)\nsrc_mask = (src != SRC.stoi[\"<blank>\"]).unsqueeze(-2)\nout = greedy_decode(model, src, src_mask,\nmax_len=60,\nstart_symbol=TGT.stoi[\"<s>\"])\nprint(\"Translation:\", end=\"\\t\")\ntrans = \"<s> \"\nfor i in range(1, out.size(1)):\nsym = TGT.itos[out[0, i]]\nif sym == \"</s>\":\nbreak\ntrans += sym + \" \"\nprint(trans)\n5.2 Attention Visualization\ntgt_sent = trans.split()\ndef draw(data, x, y, ax):\nseaborn.heatmap(data,\nxticklabels=x, square=True,\nyticklabels=y, vmin=0.0, vmax=1.0,\ncbar=False, ax=ax)\nfor layer_num in range(1, 6, 2):\nfig, axs = plt.subplots(1, 4, figsize=(20, 10))\nprint(\"Encoder Layer\", layer_num + 1)\nlayer = model.encoder.layers[layer_num]\nfor h in range(4):\ndraw(layer.self_attn.attn[0, h].data,\nsent, sent if h == 0 else [], ax=axs[h])\nplt.show()\nfor layer_num in range(1, 6, 2):\nfig, axs = plt.subplots(1, 4, figsize=(20, 10))\nprint(\"Decoder Self Layer\", layer_num + 1)\nlayer = model.decoder.layers[layer_num]\nfor h in range(4):\ndraw(layer.self_attn.attn[0, h]\n.data[:len(tgt_sent), :len(tgt_sent)],\ntgt_sent, tgt_sent if h == 0 else [], ax=axs[h])\nplt.show()\nprint(\"Decoder Src Layer\", layer_num + 1)\nfig, axs = plt.subplots(1, 4, figsize=(20, 10))\nfor h in range(4):\ndraw(layer.src_attn.attn[0, h].data[\n:len(tgt_sent), :len(sent)],\nsent, tgt_sent if h == 0 else [], ax=axs[h])\nplt.show()\n6 Conclusion\nThis paper presents a replication exercise of\nthe transformer network. Consult the full on-\nline version for features such as multi-gpu\ntraining, real experiments on full translation\nproblems, and pointers to other extensions\nsuch as beam search, sub-word models, and\nmodel averaging. The goal is to explore a lit-\nerate programming experiment of interleav-\ning model replication with formal writing.\nWhile not always possible, this modality can\nbe useful for transmitting ideas and encour-\naging faster open-source uptake. Addition-\nally this method can be an easy way to learn\nabout a model alongside its implementation.\n60\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. arXiv\npreprint arXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nDenny Britz, Anna Goldie, Minh-Thang Luong,\nand Quoc V . Le. 2017. Massive exploration\nof neural machine translation architectures.\nCoRR, abs/1703.03906.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N. Dauphin. 2017. Convo-\nlutional sequence to sequence learning. CoRR,\nabs/1705.03122.\nAlex Graves. 2013. Generating sequences with re-\ncurrent neural networks. CoRR, abs/1308.0850.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2016. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 770–778.\nDiederik P . Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nOﬁr Press and Lior Wolf. 2016. Using the out-\nput embedding to improve language models.\nCoRR, abs/1608.05859.\nNitish Srivastava, Geoffrey Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: A simple\nway to prevent neural networks from overﬁt-\nting. The Journal of Machine Learning Research,\n15(1):1929–1958.\nChristian Szegedy, Vincent Vanhoucke, Sergey\nIoffe, Jonathon Shlens, and Zbigniew Wojna.\n2015. Rethinking the inception architecture for\ncomputer vision. CoRR, abs/1512.00567.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. CoRR, abs/1706.03762.",
  "topic": "USable",
  "concepts": [
    {
      "name": "USable",
      "score": 0.803349494934082
    },
    {
      "name": "Computer science",
      "score": 0.7949689626693726
    },
    {
      "name": "Transformer",
      "score": 0.6625199317932129
    },
    {
      "name": "Publication",
      "score": 0.640295684337616
    },
    {
      "name": "Software engineering",
      "score": 0.36031341552734375
    },
    {
      "name": "Distributed computing",
      "score": 0.3238838315010071
    },
    {
      "name": "World Wide Web",
      "score": 0.22921833395957947
    },
    {
      "name": "Electrical engineering",
      "score": 0.15394166111946106
    },
    {
      "name": "Engineering",
      "score": 0.11930590867996216
    },
    {
      "name": "Voltage",
      "score": 0.07817766070365906
    },
    {
      "name": "Advertising",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ]
}