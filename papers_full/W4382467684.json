{
    "title": "Learning Progressive Modality-Shared Transformers for Effective Visible-Infrared Person Re-identification",
    "url": "https://openalex.org/W4382467684",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2124137433",
            "name": "Hu Lu",
            "affiliations": [
                "Jiangsu University"
            ]
        },
        {
            "id": "https://openalex.org/A4382481519",
            "name": "Xuezhang Zou",
            "affiliations": [
                "Jiangsu University"
            ]
        },
        {
            "id": "https://openalex.org/A2100749226",
            "name": "Ping-Ping Zhang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2124137433",
            "name": "Hu Lu",
            "affiliations": [
                "Jiangsu University"
            ]
        },
        {
            "id": "https://openalex.org/A4382481519",
            "name": "Xuezhang Zou",
            "affiliations": [
                "Jiangsu University"
            ]
        },
        {
            "id": "https://openalex.org/A2100749226",
            "name": "Ping-Ping Zhang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214891830",
        "https://openalex.org/W3142901842",
        "https://openalex.org/W2992337907",
        "https://openalex.org/W2808260522",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W3139438386",
        "https://openalex.org/W3206025282",
        "https://openalex.org/W3202788649",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W3208382980",
        "https://openalex.org/W2997877744",
        "https://openalex.org/W3106715802",
        "https://openalex.org/W3008199030",
        "https://openalex.org/W3197896218",
        "https://openalex.org/W2596603442",
        "https://openalex.org/W3194065175",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2980028015",
        "https://openalex.org/W3004990178",
        "https://openalex.org/W2777534232",
        "https://openalex.org/W3114048060",
        "https://openalex.org/W2788846526",
        "https://openalex.org/W3042589888",
        "https://openalex.org/W2998792609",
        "https://openalex.org/W3035673257",
        "https://openalex.org/W3178838461",
        "https://openalex.org/W3168126494",
        "https://openalex.org/W4312936309",
        "https://openalex.org/W3098711604",
        "https://openalex.org/W6743440100",
        "https://openalex.org/W2981476249",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4283792038",
        "https://openalex.org/W3207576380",
        "https://openalex.org/W3107848599",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W4288944547",
        "https://openalex.org/W3200038036",
        "https://openalex.org/W4287330514",
        "https://openalex.org/W3207070197",
        "https://openalex.org/W2998633236",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W3176633985",
        "https://openalex.org/W3034519219",
        "https://openalex.org/W3143016713",
        "https://openalex.org/W3205959870",
        "https://openalex.org/W3034494316",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3205065953",
        "https://openalex.org/W2985033611"
    ],
    "abstract": "Visible-Infrared Person Re-Identification (VI-ReID) is a challenging retrieval task under complex modality changes. Existing methods usually focus on extracting discriminative visual features while ignoring the reliability and commonality of visual features between different modalities. In this paper, we propose a novel deep learning framework named Progressive Modality-shared Transformer (PMT) for effective VI-ReID. To reduce the negative effect of modality gaps, we first take the gray-scale images as an auxiliary modality and propose a progressive learning strategy. Then, we propose a Modality-Shared Enhancement Loss (MSEL) to guide the model to explore more reliable identity information from modality-shared features. Finally, to cope with the problem of large intra-class differences and small inter-class differences, we propose a Discriminative Center Loss (DCL) combined with the MSEL to further improve the discrimination of reliable features. Extensive experiments on SYSU-MM01 and RegDB datasets show that our proposed framework performs better than most state-of-the-art methods. For model reproduction, we release the source code at https://github.com/hulu88/PMT.",
    "full_text": "Learning Progressive Modality-Shared Transformers for Effective\nVisible-Infrared Person Re-identification\nHu Lu1, Xuezhang Zou1, Pingping Zhang2*\n1School of Computer Science and Communication Engineering, Jiangsu University\n2School of Artificial Intelligence, Dalian University of Technology\nluhu@ujs.edu.cn;2222108016@stmail.ujs.edu.cn;zhpp@dlut.edu.cn\nAbstract\nVisible-Infrared Person Re-Identification (VI-ReID) is a chal-\nlenging retrieval task under complex modality changes. Ex-\nisting methods usually focus on extracting discriminative vi-\nsual features while ignoring the reliability and commonal-\nity of visual features between different modalities. In this\npaper, we propose a novel deep learning framework named\nProgressive Modality-shared Transformer (PMT) for effec-\ntive VI-ReID. To reduce the negative effect of modality gaps,\nwe first take the gray-scale images as an auxiliary modal-\nity and propose a progressive learning strategy. Then, we\npropose a Modality-Shared Enhancement Loss (MSEL) to\nguide the model to explore more reliable identity informa-\ntion from modality-shared features. Finally, to cope with\nthe problem of large intra-class differences and small inter-\nclass differences, we propose a Discriminative Center Loss\n(DCL) combined with the MSEL to further improve the dis-\ncrimination of reliable features. Extensive experiments on\nSYSU-MM01 and RegDB datasets show that our proposed\nframework performs better than most state-of-the-art meth-\nods. For model reproduction, we release the source code at\nhttps://github.com/hulu88/PMT.\nIntroduction\nPerson Re-Identification (ReID) aims to retrieve the same\nperson under different cameras and times. It can be utilized\nin many real-world applications, such as video surveillance,\nsmart security, etc. Recently, with the advances of deep\nlearning, person ReID has witnessed great success in perfor-\nmance and deployment. However, most of the existing ReID\nmethods target on the visible environment. Thus, they can\nbe regarded as visible-visible ReID. In fact, most of visible-\nvisible ReID methods can not work well at nighttime. To\naddress this problem, images captured by infrared cameras\nare considered in practical scenarios, which greatly help\nthe ReID under different modalities and result in Visible-\nInfrared Person Re-Identification (VI-ReID).\nCompared with single-modality ReID, VI-ReID has three\nmain challenges: 1) The large modality gap will make it dif-\nficult to align the identify-related features of the two modali-\nties. 2) Infrared images are more sensitive to light conditions\n*The corresponding author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Several typical cases in visible-infrared person re-\nidentification. (a) Discriminative information is not always\nvisible due to posture or viewpoint changes. (b) Partial infor-\nmation may disappear due to the modality shift and different\nlighting conditions. (c) Modality-based clothing change due\nto the large time span.\nthan visible images, resulting in less discriminative features\nfor cross-modality matching. 3) Modality-based clothing\nchanges can occur due to the large time span, which further\nincreases the difficulty of robust feature extraction.\nTo reduce the heterogeneous differences between two\nmodalities, existing approaches (Ye et al. 2021; Gao et al.\n2021; Chen et al. 2021b) mainly use a dual-stream network\nstructure. The non-shared weight components are first used\nto extract modality-specific features separately before learn-\ning modality-shared features. Although these methods can\neffectively benefit from modality-specific features and deal\nwith inter-modality differences, they can hardly extract ef-\nfective modality-shared features. Meanwhile, there are also\nsome Generative Adversarial Network (GAN)-based meth-\nods (Li et al. 2020; Dai et al. 2018; Wang et al. 2019) that\ngenerate cross-modality images by learning modality trans-\nformed patterns. However, these methods usually introduce\nadditional image noises and huge computational costs. Thus,\nthey are difficult to deploy in practical scenarios.\nIn addition, some outstanding methods aim to extract\nmore discriminative information. For example, (Zhu et al.\n2020; Zhang et al. 2021b) horizontally divide the portrait\ninto multiple regions to align independent local features\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1835\nand focus on extracting fine-grained discriminative features.\nHowever, due to the great challenges, overreliance on these\nfeatures may lead to wrong matches, as shown in Fig. 1.\nTherefore, recent VI-ReID works mainly focus on reducing\nthe feature differences between modalities.\nIn this work, we propose a novel deep learning framework\nnamed Progressive Modality-shared Transformer (PMT) to\nextract reliable modality-invariant features for effective VI-\nReID. To this end, we first propose a progressive learn-\ning strategy with Transformers (Dosovitskiy et al. 2020)\nto reduce the gap between visible and infrared modali-\nties. More specifically, we improve the hard triplet loss\nand introduce gray-scale images as an auxiliary modality to\nlearn modality-independent patterns. Besides, we propose a\nModality-Shared Enhancement Loss (MSEL) to reduce the\nnegative effects of modality differences and enhance the fea-\ntures with modality-shared information. Finally, we propose\na Discriminative Center Loss (DCL) to deal with the large\nintra-class variance, further enhancing the discrimination of\nreliable modality-shared features. Extensive experiments on\nSYSU-MM01 and RegDB datasets show that our framework\nperforms better than most state-of-the-art methods.\nOur main contributions are summarized as follows:\n• We propose a novel deep learning framework (i.e.,PMT)\nfor effective VI-ReID, focusing on extracting more ro-\nbust modality-shared features.\n• We propose a new Modality-Shared Enhancement Loss\n(MSEL) to enhance the modality-shared features, thus ef-\nfectively addressing the problem of feature unreliability.\n• We propose a new Discriminative Center Loss (DCL)\nto deal with large intra-class differences and further en-\nhance the discrimination of modality-invariant features.\n• Extensive experimental results on the SYSU-MM01 and\nRegDB datasets show that our proposed method achieves\na new state-of-the-art performance.\nRelated Work\nVisible-infrared Person ReID\nVisible-infrared person ReID aims to retrieve the same per-\nson under different image modalities. In fact, Wu et al. (Wu\net al. 2017) first explicitly defined the VI-ReID task and\nbuilt a large-scale and challenging dataset. Existing VI-\nReID methods usually adopt dual-stream networks and mine\nmodality-shared features. For example, Ye et al. (Ye et al.\n2018) propose an effective dual-stream network to explore\nmodality-specific and modality-shared features simultane-\nously. Lu et al. (Lu et al. 2020) propose a mechanism of\nfeature information complementarity to exploit the potential\nof modality-specific features. Gao et al. (Gao et al. 2021)\npropose a multi-feature space joint optimization network\nto enhance modality-shared features. Zhang et al. (Zhang\net al. 2021b) introduce a dual-stream network to achieve the\nglobal-local multiple granularity learning. Based on dual-\nstream networks, Zhu et al. (Zhu et al. 2020) propose a\nHeterogeneous-Center (HC) loss to reduce the modality\ngaps. Liu et al. (Liu, Tan, and Zhou 2020) further de-\nsign a heterogeneous-center triplet loss and explore the pa-\nrameter sharing methods to improve the feature represen-\ntation ability. Ye et al. (Ye, Shen, and Shao 2020) intro-\nduce gray-scale images as auxiliary modalities and real-\nize the homogeneous augmented tri-modal learning. Fu et\nal. (Fu et al. 2021) propose the cross-modality neural archi-\ntecture search and improve the structural effectiveness for\nVI-ReID. Hao et al. (Hao et al. 2021) introduce a modality\nconfusion mechanism and a center aggregation method to re-\nduce the differences between modalities. Meanwhile, many\nimage generation-based methods are developed to mitigate\nthe large modality gap. For example, Dai et al. (Dai et al.\n2018) introduce the GAN framework for cross-modality im-\nage generation, and propose the so-called cmGAN for fea-\nture learning. Furthermore, Wang et al. (Wang et al. 2019)\npropose the AlignGAN and convert visible images to in-\nfrared images with joint pixel and feature alignment. Choiet\nal. (Choi et al. 2020) attempt to disentangle cross-modality\nrepresentations with hierarchical structures. Li et al. (Li\net al. 2020) introduce an auxiliary X-modality to gener-\nate robust features and bridge the different modalities. Al-\nthough the above methods are somewhat effective, they usu-\nally introduce additional image noises and huge computa-\ntional costs. Thus, they are difficult to deploy in practical\nscenarios.\nTransformer in Person ReID\nTransformers (Vaswani et al. 2017) are initially proposed\nin Natural Language Processing (NLP). Recently, they have\nbeen utilized for some computer vision tasks, including per-\nson Re-ID. For visible-visible person ReID, He et al. (He\net al. 2021) improve the Vision Transformer (ViT) (Dosovit-\nskiy et al. 2020) with a side information embedding and a\njigsaw patch module to learn discriminative features. Zhu et\nal. (Zhu et al. 2021) add the learnable vectors of part tokens\nto learn part features and integrate the part alignment into\nthe self-attention. Lai et al. (Lai, Chai, and Wei 2021) uti-\nlize Transformers to generate adaptive part divisions. Zhang\net al. (Zhang et al. 2021a) propose a hierarchical aggrega-\ntion Transformer for integrating multi-level features. Chen\net al. (Chen et al. 2021a) propose an omni-relational high-\norder Transformer for person Re-ID. Ma et al. (Ma, Zhao,\nand Li 2021) propose a pose-guided inter-and intra-part re-\nlational Transformer for occluded person Re-ID. As for VI-\nReID, Liang et al. (Liang et al. 2021) improve the single-\nmodality Transformer and try to remove modality-specific\ninformation. Chen et al. (Chen et al. 2022) utilize the human\nkey-point information and introduce a structure-aware posi-\ntional Transformer to learn semantic-aware modality-shared\nfeatures. Although all the above Transformer-based methods\nhave achieved superior performances, they generally lack\nof desirable modality-invariant properties. In this work, we\nexplore the progressive modality-shared Transformers and\nlearn reliable features for the VI-ReID task.\nThe Proposed Method\nIn this section, we introduce the details of the proposed Pro-\ngressive Modality-shared Transformer (PMT) for VI-ReID.\nAs shown in Fig. 2, we first take the gray-scale images as an\n1836\nFigure 2: The framework of our proposed Progressive Modality-shared Transformer (PMT). To deal with the large modality\ngap, we propose a progressive learning strategy: 1) At the first stage, we feed gray-scale images and infrared images into a\nweight-shared Transformer supervised by LID and LPHT for modality-independent feature extraction. 2) At the second stage,\nwe utilize visual images and infrared images to improve the modality-shared features with LMSEL and LDCL.\nauxiliary modality and adopt a weight-shared ViT (Dosovit-\nskiy et al. 2020) as our feature extractor to capture modality-\ninvariant features. Then, we propose a progressive learn-\ning strategy to deal with the large modality gap. Besides,\na Modality-Shared Enhancement Loss (MSEL) is employed\nfor enhancing modality-shared features. Finally, a Discrim-\ninative Center Loss (DCL) is introduced to further improve\nthe discrimination of reliable modality-shared features.\nProgressive Learning Strategy\nAlthough previous weight-shared structures can capture\nmore modality-shared features, they are also susceptible to\nmodality-specific noises. Besides, the pre-trained weights on\nImageNet (Deng et al. 2009) generally have a stronger re-\nliance on low-level features, such as color or texture. Thus,\ndirectly using these pre-trained models may miss some\nmodality-specific information. Considering above facts, we\ndesign a progressive learning strategy. The key idea is to re-\nmove color information of visible images through gray-scale\nimages. It also helps to learn the modality-independent dis-\ncriminative patterns. In this way, the negative effects from\nlarge modality gaps are effectively mitigated.\nFormally, we denote the visible image and the infrared im-\nage as xvis and xir, respectively. Then, the gray-scale image\ncorresponding to xvis can be denoted as xgray . By feeding\n{xvis, xgray , xir} into a weight-shared Transformer F(·),\nwe can obtain their corresponding embedding vectors:\nfv = F\n\u0000\nxvis\u0001\n, fg = F (xgray ) , fir = F\n\u0000\nxir\u0001\n. (1)\nTo reduce the impact of modality-specific information, we\nfurther propose a Progressive Hard Triplet Loss (PHT). Sim-\nilar to most VI-ReID methods, in each mini-batch, we ran-\ndomly select P identities and then select each identity’s K\nvisible and K infrared images. Then, the proposed progres-\nFigure 3: Illustration of the Progressive Learning Strategy.\nsive hard triplet loss can be denoted as:\nLPHT =\n\u001a\nLIntra , X =\n\b\nxgray , xir\t\nLGlobal, X =\n\b\nxvis, xir\t (2)\nLIntra =\nPKX\ni=1\n\u0014\nmax\n∀yi=yj\nD\n\u0000\nfg\ni , fg\nj\n\u0001\n− min\n∀yi̸=yk\nD (fg\ni , fg\nk ) +m\n\u0015\n+\n+\nPKX\ni=1\n\u0014\nmax\n∀yi=yj\nD\n\u0010\nfir\ni , fir\nj\n\u0011\n− min\n∀yi̸=yk\nD\n\u0010\nfir\ni , fir\nk\n\u0011\n+ m\n\u0015\n+\n.\n(3)\nLGlobal =\n2PKX\ni=1\n\u0014\nmax\n∀yi=yj\nD (fi, fj) − min\n∀yi̸=yk\nD (fi, fk) +m\n\u0015\n+\n.\n(4)\nwhere D(·, ·) represents a distance metric. yi is the identity\nlabel of the i-th image. [z]+ = max(z, 0). m is a margin.\nAs shown in Fig. 3, we divide the entire training pro-\ncess into two stages. At the first stage, the framework\n1837\nFigure 4: Motivations of the proposed Modality-Shared En-\nhancement Loss. It suppresses the unreliable features that\nonly appear in one modality and enhances the modality-\nshared features.\ntakes {xgray , xir} as input, independently sampling positive\nand negative samples within each modality. With Lintra,\nthe framework mainly focuses on learning the modality-\nindependent discriminative patterns, thus effectively allevi-\nating the negative effects caused by the large gap between\nvisible and infrared modalities. At the second stage, we re-\nplace the inputs with {xvis, xir} to take full advantages of\nmodality-specific information for more fine-grained learn-\ning. With Lglobal, the framework will no longer distinguish\ndifferent modalities and select positive and negative samples\nonly based on feature distances. This can keep the raw im-\nage information, and allow the model to be benefited from\nmodality-specific information.\nModality-Shared Enhancement Loss\nIn real scenes, there are large modality differences between\nvisible and infrared images. Thus, it is essential to ex-\ntract modality-invariant features. As shown in Fig. 4 (a),\nthe red backpack appears only in the visible modality, so\nover-reliance on such features will lead to failure in cross-\nmodality retrieval. Therefore, we introduce the MSEL to\nappropriately suppress the unreliable features that only ap-\npear in one modality and enhance the utilization of reliable\nmodality-invariant features.\nTo the above goal, we explore potential information from\nall samples in a mini batch. Formally, we denote the anchor\nfeatures of the infrared and visible modality asfir\na and fvis\na ,\nrespectively. Without loss of generality, we take fir\na as an\nexample. Firstly, we calculate its average distance to other\npositive samples under the intra modality and cross modal-\nity, denoted as:\nDintra = 1\nK − 1\nKX\ni=1\ni̸=a\nD\n\u0000\nfir\na , fir\ni\n\u0001\n, (5)\nDcross = 1\nK\nKX\ni=1\nD\n\u0000\nfir\na , fvis\ni\n\u0001\n. (6)\nThen, the LMSEL is defined as:\nLMSEL = 1\n2PK\nPX\np=1\n\"2KX\na=1\n\u0000\nDintra\na − Dcross\na\n\u00012\n#\n. (7)\nFigure 5: Geometric illustrations of the (a) Modality-Shared\nEnhancement Loss and (b) Discriminative Center Loss.\nIn Eq. 7, LMSEL penalizes the difference betweenDintra\nand Dcross. When discriminative features that appear only\nwithin one modality, then the difference betweenDintra and\nDcross will increase, and such anomalies will be captured\nby LMSEL . During the bi-direction optimization process\nof Dintra and Dcross, the unreliable features that appear in\nonly one modality will be suppressed, while the more reli-\nable features that appear in both modalities will be enhanced\nas shown in Fig. 4 (b). Fig. 5 (a) shows the geometric il-\nlustrations of the MSEL. It encourages feature embeddings\nsubject to a spherical distribution.\nDiscriminative Center Loss\nSimilar to visible-visible person ReID, the same person may\nsuffer large intra-class differences due to typical variations\nin pose, point of view, illumination, etc. They greatly in-\ncrease the difficulty of feature alignment between different\nmodalities. To address this problem, we propose a Discrim-\ninative Center Loss (DCL) to exploit the example relation-\nships between center instances and enhance the discrimina-\ntive power of reliable modality-shared features.\nFirstly, to obtain the robust representation of each identity,\nwe compute the feature center under the two modalities by:\ncyi = 1\n2K\n KX\nj=1\nfvis\nj +\nKX\nk=1\nfir\nk\n!\n. (8)\nHere, cyi denotes the feature center of theyth\ni identity. Then\nwe calculate the average distance of cyi to all other negative\nsamples as a dynamic margin, which can be denoted as:\ndneg\nyi = 1\n2K(P − 1)\nX\n∀yj̸=yi\n∥fj − cyi∥2 . (9)\nFinally, the LDCL is defined as:\nLDCL =\nPP\ni=1\nmean\nyj=yi\n∥fj − cyi∥2\nPP\ni=1\nmean\n∥fk−cyi∥2<dneg\nyi\nyk̸=yi\n∥fk − cyi∥2\n. (10)\nBy minimizing Eq. 10, the intra-class compactness and\ninter-class separability will be improved. Fig. 5 (b) shows\nthe geometric illustrations of the DCL. The utilization of\nLDCL has two main advantages: 1) It can utilize modality-\nspecific features and capture more potential relationships\nthan the center-center solution. 2) The dynamic sampling\nthrough dneg\nyi can effectively focus on relatively difficult ex-\namples. The effectiveness will be verified by experiments.\n1838\nOverall Objective Function\nFor model training, we adopt a hybrid loss function for our\nprogressive learning framework. At the first stage, we utilize\nthe identity loss LID (Zheng, Zheng, and Yang 2017) and\nLIntra to learn modality-independent features:\nL1 = LIntra + LID . (11)\nAt the second stage, we further extract the reliable modality-\nshared features withLMSEL and enhance the discrimination\nwith LDCL. The loss function can be defined as:\nL2 = LGlobal + LID + λ1LMSEL + λ2LDCL. (12)\nHere, the parametersλ1 and λ2 are used to balance the terms\nof LMSEL and LDCL, respectively.\nExperiments\nExperimental Setting\nDatasets. In this work, we follow previous methods and\nconduct experiments on two public VI-ReID datasets.\nSYSU-MM01 (Wu et al. 2017) has a total of 286,628 vis-\nible images and 15,792 infrared images with 491 different\nperson identities. The training set contains 22,258 visible\nimages and 11,909 infrared images of 395 persons, and the\ntest set contains images of another 96 different identities.\n3803 infrared images are used as the query set, and from the\nother visible images, 301 images are randomly selected as\nthe gallery set. Besides, there are two search modes. Theall-\nsearch mode uses all images for testing, while the indoor-\nsearch mode only uses the indoor images.\nRegDB (Nguyen et al. 2017) contains a total of 412 differ-\nent person identities. For each person, 10 visible images and\n10 infrared images are captured. We follow the evaluation\nprotocol in (Ye et al. 2018) and randomly select all images\nof 206 identities for training and the remaining 206 identities\nfor testing. To obtain the stable results, we randomly divide\nthis dataset ten times for independent training and testing.\nEvaluation metrics. We use Cumulative Matching Char-\nacteristics (CMC), Mean Average Precision (mAP), and\nMean Inverse Negative Penalty (mINP) (Ye et al. 2021) as\nour main evaluation metrics.\nImplementation details. Our proposed method is im-\nplemented with the Huawei-Mindspore toolbox and one\nNVIDIA RTX3090 GPU. We adopt the ViT-B/16 (Doso-\nvitskiy et al. 2020) pre-trained on ImageNet (Deng et al.\n2009) as our backbone and set the overlap stride to 12 to\nbalance speed and performance. All person images are re-\nsized to 256×128 with horizontal flipping and random eras-\ning for data augmentation. For infrared images, color jitter\nand gaussian blur are additionally applied. The batch size is\nset to 64, containing a total of 8 different identities. For each\nidentity, 4 visible images and 4 infrared images are sam-\npled. We adopt AdamW optimizer with a cosine annealing\nlearning rate scheduler for training. The basic learning rate\nis set to 3e−4 and weight decay is set to 1e−4. We train 24\nepochs for the SYSU-MM01 and 36 epochs for the RegDB.\nFor both datasets, the epoch t of the first stage is set to 6, the\ntrade-off parameters λ1 and λ2 are set to 0.5, and the mar-\ngin parameter m is set to 0.1. The 768-dimensional features\nafter the BN layer are used for testing.\nAblation Studies\nIn this subsection, we conduct experiments to verify the ef-\nfects of different modules on the SYSU-MM01 dataset un-\nder the all-search mode.\nEffectiveness of the progressive learning strategy. We\nevaluate the effectiveness in terms of both image modality\nand loss function. For the image modality, the comparison\nresults are shown in Tab. 1. “Baseline (RGB)” and “Baseline\n(Gray)” indicates the baseline model trained with the RGB-\nIR modality and Grayscale-IR modality, respectively. “Base-\nline (Gray-RGB)” means that the first t epochs of training\nuses the Grayscale-IR modality and the rest uses the RGB-\nIR modality. From the results, one can see that directly us-\ning the RGB-IR modality and Grayscale-IR modality shows\ninferior performances. With a progressive learning strategy,\nthe model can show better results, indicating the effective-\nness of reducing the modality differences. Our proposed\nstrategy significantly improves the “Baseline (RGB)” model\nby 5.16% Rank-1, 5.04% mAP, and 5.74% mINP.\nBesides, based on the “Baseline (Gray-RGB)” model, we\nreplace the hard triplet loss with WRT (Ye et al. 2021), HCT\n(Liu, Tan, and Zhou 2020) and our proposed PHT. The com-\nparison results are shown in Tab. 2. These results further\nprove the effectiveness of our progressive learning strategy.\nEffects with MSEL and DCL. Tab. 3 shows the com-\nparison results of different settings with MSEL and DCL.\n“Base(PL)” only adopts the progressive learning strategy.\n“MSEL (Cosine)” and “MSEL (Euclid)” mean that the\nmodel uses the cosine distance and Euclidean distance, re-\nspectively. The experimental results show that the model\nwith MSEL consistently improves the performance. The\n“MSEL (Euclid)” model brings the best results with 3.44%\nRank-1, 2.21% mAP, and 1.91% mINP improvement com-\npared to “Base (PL)”.\nAs for the DCL, “DCL (Hard)” means only selecting the\nclosest negative sample for each identity center. “DCL (All)”\nmeans selecting all negative samples for each identity cen-\nter, and “DCL (Dyn)” means dynamically selecting nega-\ntive samples based on Eq. 9. As shown in Tab. 3, the LDCL\ncan bring a consistent improvement. Our dynamic selec-\ntion shows much better relative results with 2.38% Rank-1,\n3.50% mAP, and 4.89% mINP when compared with “Base\nTransition Schemes Rank-1 Rank-10 mAP mINP\nBaseline (RGB) 52.51 88.21 51.30 38.51\nBaseline (Gray) 56.51 91.76 54.22 39.75\nBaseline (RGB-Gray) 57.24 91.87 54.95 40.25\nBaseline (Gray-RGB) 59.07 92.53 56.86 42.81\nTable 1: Effects of different image modalities.\nMethods Rank-1 Rank-10 mAP mINP\nHardTri (m = 0.1) 59.07 92.53 56.86 42.81\nWRT 54.90 92.33 54.74 42.30\nHCT (m = 0.3) 59.51 92.38 56.68 42.05\nPHT (m = 0.1) 61.67 93.02 59.26 45.49\nTable 2: Effects of progressive learning losses.\n1839\nMethods Rank-1 Rank-10 mAP mINP\nBase (PL) 61.67 93.02 59.26 45.49\n+MSEL (Cosine) 64.19 93.45 60.67 46.15\n+MSEL (Euclid) 65.11 93.81 61.47 47.40\n+DCL (Hard) 63.22 94.09 62.13 49.97\n+DCL (All) 62.86 94.15 61.32 48.73\n+DCL (Dyn) 64.05 94.71 62.76 50.38\n+MSEL (Euclid)+DCL (Dyn) 67.53 95.36 64.98 51.86\nTable 3: Comparison results with MSEL and DCL.\nMethods Rank-1 mAP mINP\nAGW 58.19 56.50 43.52\nAGW + MSEL 62.16 59.66 46.38\nAGW + MSEL + PL 65.97 62.15 47.30\nAGW + MSEL + PL + DCL 67.09 64.25 50.89\nOurs 67.53 64.98 51.86\nTable 4: Comparison results with CNN-based backbones.\n(PL)”. Combined with MSEL, the model can bring a fur-\nther 2.42% Rank-1, 3.51% mAP and 4.46% mINP improve-\nment. These results fully demonstrate the effectiveness of\nour MSEL and DCL.\nEffects of CNN backbones. To further study the effective-\nness and generalization of our proposed methods, we also\ncarry out experiments with CNN-based frameworks. As a\ntypical example, we take the outstanding AGW method with\nrandom erasing (Ye et al. 2021). The experimental results are\nlisted in Tab. 4. The results show that by adding MSEL, the\nmodel delivers a performance gain of 3.97% Rank-1, 3.16%\nmAP, and 2.86% mINP. The results indicate that LMSEL\ncan also be compatible with variants of different triples.\nWith the full modules (“MSEL+PL+DCL”), the model can\nachieve a performance gain of 8.90% Rank-1, 7.75% mAP,\nand 8.34% mINP. These facts clearly demonstrate the gen-\neralization of our proposed methods on CNN-based frame-\nworks. However, our Transformer-based framework shows\nbetter results than CNN-based ones, as shown in the last row.\nTrade-off parameters. We conduct additional experi-\nments to evaluate the effect of trade-off parameters λ1 and\nλ2. As shown in Fig. 6,LMSEL is not sensitive to the param-\neter settings, while LDCL is stable within a certain range.\nVisualization analysis. To analyse the visual effect of our\nproposed model, we present some typical visual examples.\nAs shown in Fig. 7, we use the Grad-CAM (Selvaraju et al.\n2017) to generate attention maps of query images with our\nmodels. Besides, the top 10 retrieval results are also pro-\nvided in Fig. 7 (d). One can observe that with our MSEL,\nthe model focuses on more discriminative regions and ex-\ntracts decent modality-shared features. Thus, the model can\neffectively deal with complex scenarios in Fig. 1. In ad-\ndition, we randomly sample 10,000 positive and negative\nmatching pairs from the test set and visualize their cosine\nsimilarity distribution, as shown in Fig. 8. One can observe\nthat the similarity of positive cross-modality matching pairs\nincreases by introducing MSEL, which indicates that MSEL\nenhances the utilization of reliable modality-invariant fea-\nFigure 6: Performance effects of trade-off parametersλ1 and\nλ2. In the top sub-figure, λ2 = 0.5, λ1 ∈ [0.1, 0.9] and in\nthe bottom sub-figure, λ1 = 0.5, λ2 ∈ [0.1, 0.9].\nFigure 7: Attention maps and retrieval results. (a) Query im-\nages. (b) Attention maps w/o MSEL. (c) Attention maps w/\nMSEL. (d) Top 10 retrieval results.\nFigure 8: The cosine similarity distribution of positive and\nnegative matching pairs from the test set.\ntures. By further introducing DCL, the range of cosine simi-\nlarity between negative and positive pairs is significantly ex-\npanded. The visualization shows the ability of LDCL to ex-\nplore potential information and effectively improve the dis-\ncrimination of feature embeddings.\n1840\nMethods All search Indoor search\nr = 1 r = 10 r = 20 mAP mINP r = 1 r = 10 r = 20 mAP mINP\nZero-Pad (Wu et al. 2017) 14.80 54.12 71.33 15.95 - 20.58 68.38 85.79 26.92 -\nHi-CMD (Choi et al. 2020) 34.94 77.58 - 35.94 - - - - - -\nCMSP (Wu et al. 2020) 43.56 86.25 - 44.98 - 48.62 89.50 - 57.50 -\nexpAT Loss (Ye et al. 2020a) 38.57 76.64 86.39 38.61 - - - - - -\nAGW (Ye et al. 2021) 47.50 84.39 92.14 47.65 35.30 54.17 91.14 95.98 62.97 59.23\nHAT (Ye, Shen, and Shao 2020) 55.29 92.14 97.36 53.89 - 62.10 95.75 99.20 69.37 -\nLBA (Park et al. 2021) 55.41 91.12 - 54.14 - 58.46 94.13 - 66.33 -\nNFS (Chen et al. 2021b) 56.91 91.34 96.52 55.45 - 62.79 96.53 99.07 69.79 -\nMSO (Gao et al. 2021) 58.70 92.06 97.20 56.42 42.04 63.09 96.61 - 70.31 -\nCM-NAS (Fu et al. 2021) 61.99 92.87 97.25 60.02 - 67.01 97.02 99.32 72.95 -\nMID (Huang et al. 2022) 60.27 92.90 - 59.40 - 64.86 96.12 - 70.12 -\nSPOT (Chen et al. 2022) 65.34 92.73 97.04 62.25 48.86 69.42 96.22 99.12 74.63 70.48\nFMCNet (Zhang et al. 2022) 66.34 - - 62.51 - 68.15 - - 74.09 -\nPMT (Ours) 67.53 95.36 98.64 64.98 51.86 71.66 96.73 99.25 76.52 72.74\nTable 5: Comparisons with state-of-the-art methods under all-search and indoor-search modes on the SYSU-MM01 dataset.\nMethods V to T T to V\nr = 1 mAP r = 1 mAP\nDDAG (Ye et al. 2020b) 69.34 63.46 68.06 61.80\nexpAT Loss (Ye et al. 2020a) 66.48 67.31 67.45 66.51\nAGW (Ye et al. 2021) 70.05 66.37 70.49 65.90\nHAT (Ye, Shen, and Shao 2020) 71.83 67.56 70.02 66.30\nMSO (Gao et al. 2021) 73.6 66.9 74.6 67.5\nLBA (Park et al. 2021) 74.17 67.64 72.43 65.46\nNFS (Chen et al. 2021b) 80.54 72.10 77.95 69.79\nMCLNet (Hao et al. 2021) 80.31 73.07 75.93 69.49\nSPOT (Chen et al. 2022) 80.35 72.46 79.37 72.26\nPMT (Ours) 84.83 76.55 84.16 75.13\nTable 6: Comparison results under Visible-Thermal and\nThermal-Visible modes on the RegDB dataset.\nComparison with State-of-the-Arts\nIn this subsection, we compare our proposed PMT with\nother state-of-the-art methods on SYSU-MM01 and RegDB.\nSYSU-MM01: Tab. 5 shows the comparison results on the\nSYSU-MM01 dataset. One can observe that our proposed\nmethod outperforms other weight-shared methods [expAT\n(Ye et al. 2020a), HAT (Ye, Shen, and Shao 2020)] by at least\n12.24% in Rank-1 and 11.09% in mAP under the all-search\nmode. Moreover, compared with dual-stream-based meth-\nods [LBA (Park et al. 2021), NFS (Chen et al. 2021b), SPOT\n(Chen et al. 2022)], our proposed method also has substan-\ntial performance advantages. Under the indoor-search mode,\nour proposed method shows much better results in terms of\nRank-1, mAP and mINP. In terms of Rank-10 and Rank-20,\nCM-NAS (Fu et al. 2021) shows best results. The main rea-\nson may be that the searched network by CM-NAS is more\nhelpful for global discrimination. However, our proposed\nmethod delivers very comparable results. All the above re-\nsults fully demonstrate that our proposed PMT can effec-\ntively reduce the large modality gap and utilize more reliable\nmodality-shared features.\nRegDB: In Tab. 6, we report the comparison results on the\nRegDB dataset. The results show that our proposed method\nachieves excellent performances in both Visible to Thermal\n(V toT) and Thermal to Visible (TtoV) modes. More specif-\nically, our proposed method achieves an expressive perfor-\nmance of 84.83% Rank-1 and 76.55% mAP under theV toT\nmode, showing a 4% performance gain than other best meth-\nods. For the more challenging TtoV mode, our method also\nshows great performance advantages. These results also in-\ndicate that our proposed method is more robust against dif-\nferent datasets and query patterns.\nConclusion\nIn this paper, we propose a novel deep learning-based frame-\nwork named PMT, which effectively improves the perfor-\nmance of VI-ReID by fully exploring reliable modality-\ninvariant features. With gray-scale images as an auxiliary\nmodality, our framework mitigates the large gap between\nRGB-IR modalities through a progressive learning strategy.\nMeanwhile, our proposed MSEL and DCL can effectively\nextract more reliable and discriminative features, bringing\nstronger performance and robustness. Moreover, the pro-\nposed methods have a good generalization. By applying\nour methods to CNN-based backbones, they can also bring\nsignificant performance improvements. Experimental results\non two public VI-ReID benchmarks verify the effectiveness\nof our proposed framework. In the future, we will explore\nmore effective Transformer structures to further improve the\nfeature representation ability.\nAcknowledgments\nThis work was supported in part by the National Natu-\nral Science Foundation of China (NSFC) (No. 91538201,\n62101092), the CAAI-Huawei MindSpore Open Fund un-\nder Grant CAAIXSJLJJ-2021-067A, and the Fundamen-\ntal Research Funds for the Central Universities (No.\nDUT20RC(3)083).\n1841\nReferences\nChen, C.; Ye, M.; Qi, M.; Wu, J.; Jiang, J.; and Lin, C.-W.\n2022. Structure-Aware Positional Transformer for Visible-\nInfrared Person Re-Identification. IEEE Transactions on Im-\nage Processing, 31: 2352–2364.\nChen, X.; Xu, J.; Xu, J.; and Gao, S. 2021a. OH-Former:\nOmni-Relational High-Order Transformer for Person Re-\nIdentification. arXiv:2109.11159.\nChen, Y .; Wan, L.; Li, Z.; Jing, Q.; and Sun, Z. 2021b. Neu-\nral feature search for rgb-infrared person re-identification.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 587–597.\nChoi, S.; Lee, S.; Kim, Y .; Kim, T.; and Kim, C. 2020.\nHi-CMD: Hierarchical cross-modality disentanglement for\nvisible-infrared person re-identification. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 10257–10266.\nDai, P.; Ji, R.; Wang, H.; Wu, Q.; and Huang, Y . 2018. Cross-\nmodality person re-identification with generative adversarial\ntraining. In International Joint Conference on Artificial In-\ntelligence, volume 1, 6.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, 248–255. IEEE.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In In-\nternational Conference on Learning Representations.\nFu, C.; Hu, Y .; Wu, X.; Shi, H.; Mei, T.; and He, R.\n2021. CM-NAS: Cross-modality neural architecture search\nfor visible-infrared person re-identification. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 11823–11832.\nGao, Y .; Liang, T.; Jin, Y .; Gu, X.; Liu, W.; Li, Y .; and Lang,\nC. 2021. MSO: Multi-feature space joint optimization net-\nwork for rgb-infrared person re-identification. In Proceed-\nings of the 29th ACM International Conference on Multime-\ndia, 5257–5265.\nHao, X.; Zhao, S.; Ye, M.; and Shen, J. 2021. Cross-\nmodality person re-identification via modality confusion and\ncenter aggregation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 16403–16412.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W.\n2021. Transreid: Transformer-based object re-identification.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 15013–15022.\nHuang, Z.; Liu, J.; Li, L.; Zheng, K.; and Zha, Z.-J. 2022.\nModality-Adaptive Mixup and Invariant Decomposition for\nRGB-Infrared Person Re-Identification. arXiv preprint\narXiv:2203.01735.\nLai, S.; Chai, Z.; and Wei, X. 2021. Transformer Meets Part\nModel: Adaptive Part Division for Person Re-Identification.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 4150–4157.\nLi, D.; Wei, X.; Hong, X.; and Gong, Y . 2020. Infrared-\nvisible cross-modal person re-identification with an x\nmodality. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, volume 34, 4610–4617.\nLiang, T.; Jin, Y .; Gao, Y .; Liu, W.; Feng, S.; Wang, T.;\nand Li, Y . 2021. CMTR: Cross-modality Transformer for\nVisible-infrared Person Re-identification. arXiv preprint\narXiv:2110.08994.\nLiu, H.; Tan, X.; and Zhou, X. 2020. Parameter sharing\nexploration and hetero-center triplet loss for visible-thermal\nperson re-identification. IEEE Transactions on Multimedia,\n23: 4414–4425.\nLu, Y .; Wu, Y .; Liu, B.; Zhang, T.; Li, B.; Chu, Q.; and Yu, N.\n2020. Cross-modality person re-identification with shared-\nspecific feature transfer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n13379–13389.\nMa, Z.; Zhao, Y .; and Li, J. 2021. Pose-guided inter-\nand intra-part relational transformer for occluded person re-\nidentification. In Proceedings of the 29th ACM International\nConference on Multimedia, 1487–1496.\nNguyen, D. T.; Hong, H. G.; Kim, K. W.; and Park, K. R.\n2017. Person recognition system based on a combination of\nbody images from visible light and thermal cameras. Sen-\nsors, 17(3): 605.\nPark, H.; Lee, S.; Lee, J.; and Ham, B. 2021. Learn-\ning by aligning: Visible-infrared person re-identification us-\ning cross-modal correspondences. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n12046–12055.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localization. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 618–626.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need.Advances in Neural Information Pro-\ncessing Systems, 30.\nWang, G.; Zhang, T.; Cheng, J.; Liu, S.; Yang, Y .; and\nHou, Z. 2019. RGB-infrared cross-modality person re-\nidentification via joint pixel and feature alignment. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 3623–3632.\nWu, A.; Zheng, W.-S.; Gong, S.; and Lai, J. 2020. Rgb-ir\nperson re-identification by cross-modality similarity preser-\nvation. International Journal of Computer Vision, 128(6):\n1765–1785.\nWu, A.; Zheng, W.-S.; Yu, H.-X.; Gong, S.; and Lai, J. 2017.\nRGB-infrared cross-modality person re-identification. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 5380–5389.\nYe, H.; Liu, H.; Meng, F.; and Li, X. 2020a. Bi-directional\nexponential angular triplet loss for RGB-infrared person re-\nidentification. IEEE Transactions on Image Processing, 30:\n1583–1595.\n1842\nYe, M.; Lan, X.; Li, J.; and Yuen, P. 2018. Hierarchi-\ncal discriminative learning for visible thermal person re-\nidentification. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 32.\nYe, M.; Shen, J.; J Crandall, D.; Shao, L.; and Luo, J.\n2020b. Dynamic dual-attentive aggregation learning for\nvisible-infrared person re-identification. In European Con-\nference on Computer Vision, 229–247. Springer.\nYe, M.; Shen, J.; Lin, G.; Xiang, T.; Shao, L.; and Hoi, S. C.\n2021. Deep learning for person re-identification: A survey\nand outlook. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 44(6): 2872–2893.\nYe, M.; Shen, J.; and Shao, L. 2020. Visible-infrared per-\nson re-identification via homogeneous augmented tri-modal\nlearning. IEEE Transactions on Information Forensics and\nSecurity, 16: 728–739.\nZhang, G.; Zhang, P.; Qi, J.; and Lu, H. 2021a. Hat: Hierar-\nchical aggregation transformers for person re-identification.\nIn Proceedings of the 29th ACM International Conference\non Multimedia, 516–525.\nZhang, L.; Du, G.; Liu, F.; Tu, H.; and Shu, X.\n2021b. Global-local multiple granularity learning for cross-\nmodality visible-infrared person reidentification. IEEE\nTransactions on Neural Networks and Learning Systems.\nZhang, Q.; Lai, C.; Liu, J.; Huang, N.; and Han, J. 2022. FM-\nCNet: Feature-Level Modality Compensation for Visible-\nInfrared Person Re-Identification. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7349–7358.\nZheng, Z.; Zheng, L.; and Yang, Y . 2017. A discriminatively\nlearned cnn embedding for person reidentification. ACM\nTransactions on Multimedia Computing, Communications,\nand Applications (TOMM), 14(1): 1–20.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom erasing data augmentation. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34,\n13001–13008.\nZhu, K.; Guo, H.; Zhang, S.; Wang, Y .; Huang, G.; Qiao,\nH.; Liu, J.; Wang, J.; and Tang, M. 2021. Aaformer:\nAuto-aligned transformer for person re-identification. arXiv\npreprint arXiv:2104.00921.\nZhu, Y .; Yang, Z.; Wang, L.; Zhao, S.; Hu, X.; and Tao,\nD. 2020. Hetero-center loss for cross-modality person re-\nidentification. Neurocomputing, 386: 97–109.\n1843"
}