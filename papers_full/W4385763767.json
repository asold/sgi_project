{
  "title": "Transformers in Time Series: A Survey",
  "url": "https://openalex.org/W4385763767",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5048346353",
      "name": "Qingsong Wen",
      "affiliations": [
        "Alibaba Group (United States)",
        "Bellevue Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A5101890076",
      "name": "Tian Zhou",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5101497311",
      "name": "Chaoli Zhang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100646095",
      "name": "Weiqi Chen",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5035357260",
      "name": "Ziqing Ma",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5087158377",
      "name": "Junchi Yan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5054846625",
      "name": "Liang Sun",
      "affiliations": [
        "Alibaba Group (United States)",
        "Bellevue Hospital Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035376600",
    "https://openalex.org/W4225539031",
    "https://openalex.org/W2892035503",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W3216549012",
    "https://openalex.org/W4281681455",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3204263062",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3184127157",
    "https://openalex.org/W4306884390",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W3155567600",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4280550128",
    "https://openalex.org/W3122820950",
    "https://openalex.org/W3142110751",
    "https://openalex.org/W4210263262",
    "https://openalex.org/W3212393417",
    "https://openalex.org/W4226071363",
    "https://openalex.org/W3097237405",
    "https://openalex.org/W4382318973",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W3000386982",
    "https://openalex.org/W3187289530",
    "https://openalex.org/W2995744795",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W4244521888",
    "https://openalex.org/W1536447791",
    "https://openalex.org/W2964758013",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3097294131",
    "https://openalex.org/W3034749137",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4310416691",
    "https://openalex.org/W3022643593",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W3035591463",
    "https://openalex.org/W2965425016",
    "https://openalex.org/W4225494949",
    "https://openalex.org/W3122154272",
    "https://openalex.org/W3080157892",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3198381997",
    "https://openalex.org/W3109365969",
    "https://openalex.org/W3035016175",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W4280531713",
    "https://openalex.org/W3172443934",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W3204801262",
    "https://openalex.org/W3213421678",
    "https://openalex.org/W4207023128",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2885311373",
    "https://openalex.org/W4285483867",
    "https://openalex.org/W3212890323",
    "https://openalex.org/W3191026187",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W2981830988"
  ],
  "abstract": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance.",
  "full_text": "Transformers in Time Series: A Survey\nQingsong Wen1, Tian Zhou2, Chaoli Zhang2, Weiqi Chen2, Ziqing Ma2, Junchi Yan3, Liang Sun1\n1DAMO Academy, Alibaba Group, Bellevue, USA\n2DAMO Academy, Alibaba Group, Hangzhou, China\n3Department of CSE, MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University\n{qingsong.wen, tian.zt, chaoli.zcl, jarvus.cwq, maziqing.mzq, liang.sun}@alibaba-inc.com,\nyanjunchi@sjtu.edu.cn\nAbstract\nTransformers have achieved superior performances\nin many tasks in natural language processing and\ncomputer vision, which also triggered great inter-\nest in the time series community. Among multiple\nadvantages of Transformers, the ability to capture\nlong-range dependencies and interactions is espe-\ncially attractive for time series modeling, leading\nto exciting progress in various time series appli-\ncations. In this paper, we systematically review\nTransformer schemes for time series modeling by\nhighlighting their strengths as well as limitations.\nIn particular, we examine the development of time\nseries Transformers in two perspectives. From the\nperspective of network structure, we summarize the\nadaptations and modifications that have been made\nto Transformers in order to accommodate the chal-\nlenges in time series analysis. From the perspective\nof applications, we categorize time series Trans-\nformers based on common tasks including forecast-\ning, anomaly detection, and classification. Empiri-\ncally, we perform robust analysis, model size anal-\nysis, and seasonal-trend decomposition analysis to\nstudy how Transformers perform in time series. Fi-\nnally, we discuss and suggest future directions to\nprovide useful research guidance.\n1 Introduction\nThe innovation of Transformer in deep learning [Vaswani et\nal., 2017] has brought great interests recently due to its ex-\ncellent performances in various domains [Kenton and others,\n2019; Dosovitskiy et al., 2021 ]. Over the past few years,\nnumerous Transformers have been proposed to significantly\nadvance the state-of-the-art performances of various tasks.\nThere are quite a few literature reviews from different aspects,\nsuch as in NLP [Han et al., 2021], CV [Han et al., 2022], and\nefficient Transformers [Tay et al., 2022].\nTransformers have shown great modeling ability for long-\nrange dependencies and interactions in sequential data and\nthus are appealing to time series modeling. Many variants\nof Transformer have been proposed to address special chal-\nlenges in time series modeling and have been successfully\napplied to various time series tasks, such as forecasting [Li\net al., 2019; Zhou et al., 2022 ], anomaly detection [Xu et\nal., 2022; Tuli et al., 2022 ], and classification [Zerveas et\nal., 2021; Yang et al., 2021]. Specifically, seasonality or pe-\nriodicity is an important feature of time series [Wen et al.,\n2021a]. How to effectively model long-range and short-range\ntemporal dependency and capture seasonality simultaneously\nremains a challenge [Wu et al., 2021; Wen et al., 2022]. We\nnote that there exist several surveys related to deep learning\nfor time series, including forecasting [Lim and Zohren, 2021;\nBenidis et al., 2022; Torres et al., 2021 ], classification [Is-\nmail Fawaz et al., 2019 ], anomaly detection [Choi et al.,\n2021; Bl ´azquez-Garc´ıa et al., 2021 ], and data augmenta-\ntion [Wen et al., 2021b ], but there is no comprehensive sur-\nvey for Transformers in time series. As Transformer for time\nseries is an emerging subject in deep learning, a systematic\nand comprehensive survey on time series Transformers would\ngreatly benefit the time series community.\nIn this paper, we aim to fill the gap by summarizing the\nmain developments of time series Transformers. We first\ngive a brief introduction about vanilla Transformer, and then\npropose a new taxonomy from perspectives of both network\nmodifications and application domains for time series Trans-\nformers. For network modifications, we discuss the improve-\nments made on both low-level (i.e., module) and high-level\n(i.e., architecture) of Transformers, to optimize the perfor-\nmance of time series modeling. For applications, we analyze\nand summarize Transformers for popular time series tasks,\nincluding forecasting, anomaly detection, and classification.\nFor each time series Transformer, we analyze its insights,\nstrengths, and limitations. To provide practical guidelines on\nhow to effectively use Transformers for time series modeling,\nwe conduct extensive empirical studies that examine multiple\naspects of time series modeling, including robustness anal-\nysis, model size analysis, and seasonal-trend decomposition\nanalysis. We conclude this work by discussing possible fu-\nture directions for time series Transformers, including induc-\ntive biases for time series Transformers, Transformers and\nGNN for time series, pre-trained Transformers for time series,\nTransformers with architecture level variants, and Transform-\ners with NAS for time series. To the best of our knowledge,\nthis is the first work to comprehensively and systematically\nreview the key developments of Transformers for modeling\ntime series data. We hope this survey will ignite further re-\nsearch interests in time series Transformers. A corresponding\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6778\nresource that has been continuously updated can be found in\nthe GitHub repository1.\n2 Preliminaries of the Transformer\n2.1 Vanilla Transformer\nThe vanilla Transformer [Vaswani et al., 2017] follows most\ncompetitive neural sequence models with an encoder-decoder\nstructure. Both encoder and decoder are composed of multi-\nple identical blocks. Each encoder block consists of a multi-\nhead self-attention module and a position-wise feed-forward\nnetwork while each decoder block inserts cross-attention\nmodels between the multi-head self-attention module and the\nposition-wise feed-forward network.\n2.2 Input Encoding and Positional Encoding\nUnlike LSTM or RNN, the vanilla Transformer has no recur-\nrence. Instead, it utilizes the positional encoding added in the\ninput embeddings, to model the sequence information. We\nsummarize some positional encodings below.\nAbsolute Positional Encoding\nIn vanilla Transformer, for each position index t, encoding\nvector is given by\nPE (t)i =\n\u001asin(ωit) i%2 = 0\ncos(ωit) i%2 = 1 (1)\nwhere ωi is the hand-crafted frequency for each dimension.\nAnother way is to learn a set of positional embeddings for\neach position which is more flexible [Kenton and others,\n2019; Gehring et al., 2017].\nRelative Positional Encoding\nFollowing the intuition that pairwise positional relationships\nbetween input elements is more beneficial than positions of\nelements, relative positional encoding methods have been\nproposed. For example, one of such methods is to add a\nlearnable relative positional embedding to keys of attention\nmechanism [Shaw et al., 2018].\nBesides the absolute and relative positional encodings,\nthere are methods using hybrid positional encodings that\ncombine them together [Ke et al., 2021]. Generally, the po-\nsitional encoding is added to the token embedding and fed to\nTransformer.\n2.3 Multi-head Attention\nWith Query-Key-Value (QKV) model, the scaled dot-product\nattention used by Transformer is given by\nAttention(Q, K, V) =softmax(QKT\n√Dk\n)V (2)\nwhere queries Q ∈ RN×Dk , keys K ∈ RM×Dk , values\nV ∈ RM×Dv , N, Mdenote the lengths of queries and keys\n(or values), and Dk, Dv denote the dimensions of keys (or\nqueries) and values. Transformer uses multi-head attention\nwith H different sets of learned projections instead of a sin-\ngle attention function as\nMultiHeadAttn(Q, K\n, V) = Concat (head1, ··· , headH)WO,\nwhere headi = Attention(QWQ\ni , KWK\ni , VWV\ni ).\n1 https://github.com/qingsongedu/time-series-transformers-review\nTime Series\nTransformers\nNetwork \nModifications\nApplication\nDomains\nPositional \nEncoding\nAttention\nModule\nArchitecture\nLevel Forecasting Anomaly \nDetection Classification\nTime Series \nForecasting\nSpatio-Temporal \nForecasting\nEvent \nForecasting\nVanilla \nEncoding\nLearnable \nEncoding\nTimestamp \nEncoding\nFigure 1: Taxonomy of Transformers for time series modeling from\nthe perspectives of network modifications and application domains.\n2.4 Feed-forward and Residual Network\nThe feed-forward network is a fully connected module as\nFFN (H′) =ReLU(H′W1 + b1)W2 + b2, (3)\nwhere H′ is outputs of previous layer, W1 ∈ RDm×Df ,\nW2 ∈ RDf ×Dm , b1 ∈ RDf , b2 ∈ RDm are trainable pa-\nrameters. In a deeper module, a residual connection module\nfollowed by a layer normalization module is inserted around\neach module. That is,\nH′ = LayerNorm(SelfAttn (X) +X), (4)\nH = LayerNorm(FFN (H′) +H′), (5)\nwhere SelfAttn (.) denotes self-attention module and\nLayerNorm (.) denotes the layer normalization operation.\n3 Taxonomy of Transformers in Time Series\nTo summarize the existing time series Transformers, we pro-\npose a taxonomy from perspectives of network modifications\nand application domains as illustrated in Fig. 1. Based on\nthe taxonomy, we review the existing time series Transform-\ners systematically. From the perspective of network modi-\nfications, we summarize the changes made on both module\nlevel and architecture level of Transformer in order to ac-\ncommodate special challenges in time series modeling. From\nthe perspective of applications, we classify time series Trans-\nformers based on their application tasks, including forecast-\ning, anomaly detection, and classification. In the following\ntwo sections, we would delve into the existing time series\nTransformers from these two perspectives.\n4 Network Modifications for Time Series\n4.1 Positional Encoding\nAs the ordering of time series matters, it is of great impor-\ntance to encode the positions of input time series into Trans-\nformers. A common design is to first encode positional infor-\nmation as vectors and then inject them into the model as an\nadditional input together with the input time series. How to\nobtain these vectors when modeling time series with Trans-\nformers can be divided into three main categories.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6779\nVanilla Positional Encoding. A few works[Li et al., 2019]\nsimply introduce vanilla positional encoding (Section 2.2)\nused in [Vaswani et al., 2017 ], which is then added to the\ninput time series embeddings and fed to Transformer. Al-\nthough this approach can extract some positional information\nfrom time series, they were unable to fully exploit the impor-\ntant features of time series data.\nLearnable Positional Encoding. As the vanilla positional\nencoding is hand-crafted and less expressive and adaptive,\nseveral studies found that learning appropriate positional em-\nbeddings from time series data can be much more effective.\nCompared to fixed vanilla positional encoding, learned em-\nbeddings are more flexible and can adapt to specific tasks.\n[Zerveas et al., 2021 ] introduces an embedding layer in\nTransformer that learns embedding vectors for each position\nindex jointly with other model parameters. [Lim et al., 2021]\nuses an LSTM network to encode positional embeddings,\nwhich can better exploit sequential ordering information in\ntime series.\nTimestamp Encoding. When modeling time series in real-\nworld scenarios, the timestamp information is commonly\naccessible, including calendar timestamps (e.g., second,\nminute, hour, week, month, and year) and special timestamps\n(e.g., holidays and events). These timestamps are quite in-\nformative in real applications but hardly leveraged in vanilla\nTransformers. To mitigate the issue, Informer [Zhou et al.,\n2021] proposed to encode timestamps as additional positional\nencoding by using learnable embedding layers. A similar\ntimestamp encoding scheme was used in Autoformer [Wu et\nal., 2021] and FEDformer [Zhou et al., 2022].\n4.2 Attention Module\nCentral to Transformer is the self-attention module. It can\nbe viewed as a fully connected layer with weights that are\ndynamically generated based on the pairwise similarity of in-\nput patterns. As a result, it shares the same maximum path\nlength as fully connected layers, but with a much less num-\nber of parameters, making it suitable for modeling long-term\ndependencies.\nAs we show in the previous section the self-attention mod-\nule in the vanilla Transformer has a time and memory com-\nplexity of O(N2) (N is the input time series length), which\nbecomes the computational bottleneck when dealing with\nlong sequences. Many efficient Transformers were proposed\nto reduce the quadratic complexity that can be classified into\ntwo main categories: (1) explicitly introducing a sparsity bias\ninto the attention mechanism like LogTrans [Li et al., 2019]\nand Pyraformer [Liu et al., 2022a]; (2) exploring the low-rank\nproperty of the self-attention matrix to speed up the computa-\ntion, e.g. Informer [Zhou et al., 2021] and FEDformer [Zhou\net al., 2022]. Table ?? shows both the time and memory com-\nplexity of popular Transformers applied to time series mod-\neling, and more details about these models will be discussed\nin Section 5.\n4.3 Architecture-based Attention Innovation\nTo accommodate individual modules in Transformers for\nmodeling time series, a number of works [Zhou et al., 2021;\nMethods Training T\nesting\nTime\nMemory Steps\nTransformer [V\naswani et al., 2017] O\n\u0000\nN2\u0001\nO\n\u0000\nN2\u0001\nN\nLogTrans [Li et\nal., 2019] O(N log N) O (N log N) 1\nInformer [Zhou et al.\n, 2021] O(N log N) O(N log N) 1\nAutoformer [Wu et\nal., 2021] O(N log N) O(N log N) 1\nPyraformer [Liu et al.\n, 2022a] O(N) O(N) 1\nQuatformer [Chen et al.\n, 2022] O(2cN) O(2cN) 1\nFEDformer [Zhou et al.\n, 2022] O(N) O(N) 1\nCrossformer [Zhang and\nYan, 2023] O( D\nL2\nseg\nN2) O(N) 1\nTable 1: Complexity comparisons of popular time series Transform-\ners with different attention modules.\nLiu et al., 2022a] seek to renovate Transformers on the archi-\ntecture level. Recent works introduce hierarchical architec-\nture into Transformer to take into account the multi-resolution\naspect of time series. Informer [Zhou et al., 2021 ] inserts\nmax-pooling layers with stride 2 between attention blocks,\nwhich down-sample series into its half slice. Pyraformer[Liu\net al., 2022a ] designs a C-ary tree-based attention mecha-\nnism, in which nodes at the finest scale correspond to the orig-\ninal time series, while nodes in the coarser scales represent\nseries at lower resolutions. Pyraformer developed both intra-\nscale and inter-scale attentions in order to better capture tem-\nporal dependencies across different resolutions. Besides the\nability to integrate information at different multi-resolutions,\na hierarchical architecture also enjoys the benefits of efficient\ncomputation, particularly for long-time series.\n5 Applications of Time Series Transformers\nIn this section, we review the applications of Transformer to\nimportant time series tasks, including forecasting, anomaly\ndetection, and classification.\n5.1 Transformers in Forecasting\nHere we examine three common types of forecasting tasks\nhere, i.e. time series forecasting, spatial-temporal forecast-\ning, and event forecasting.\nTime Series Forecasting\nA lot of work has been done to design new Transformer vari-\nants for forecasting tasks in the latest years. Module-level\nand architecture-level variants are two large categories and\nthe former consists of the majority of the up-to-date works.\nModule-level variants In the module-level variants for\ntime series forecasting, their main architectures are similar\nto the vanilla Transformer with minor changes. Researchers\nintroduce various time series inductive biases to design new\nmodules. The following summarized work consists of three\ndifferent types: designing new attention modules, exploring\nthe innovative way to normalize time series data, and utilizing\nthe bias for token inputs, as shown in Figure 2.\nThe first type of variant for module-level Transformers is to\ndesign new attention modules, which is the category with the\nlargest proportion. Here we first describe six typical works:\nLogTrans [Li et al., 2019 ], Informer [Zhou et al., 2021 ],\nAST [Wu et al., 2020a], Pyraformer [Liu et al., 2022a], Quat-\nformer [Chen et al., 2022 ], and FEDformer [Zhou et al.,\n2022], all of which exploit sparsity inductive bias or low-rank\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6780\nFigure 2: Categorization of module-level Transformer variants for\ntime series forecasting.\napproximation to remove noise and achieve a low-order cal-\nculation complexity. LogTrans[Li et al., 2019] proposes con-\nvolutional self-attention by employing causal convolutions\nto generate queries and keys in the self-attention layer. It\nintroduces sparse bias, a Logsparse mask, in self-attention\nmodel that reduces computational complexity from O(N2)\nto O(N log N). Instead of using explicit sparse bias, In-\nformer [Zhou et al., 2021] selects dominant queries based on\nqueries and key similarities, thus achieving similar improve-\nments as LogTrans in computational complexity. It also de-\nsigns a generative style decoder to produce long-term fore-\ncasting directly and thus avoids accumulative error in us-\ning one forward-step prediction for long-term forecasting.\nAST [Wu et al., 2020a] uses a generative adversarial encoder-\ndecoder framework to train a sparse Transformer model for\ntime series forecasting. It shows that adversarial training\ncan improve time series forecasting by directly shaping the\noutput distribution of the network to avoid error accumula-\ntion through one-step ahead inference. Pyraformer [Liu et\nal., 2022a ] designs a hierarchical pyramidal attention mod-\nule with a binary tree following the path, to capture temporal\ndependencies of different ranges with linear time and mem-\nory complexity. FEDformer [Zhou et al., 2022 ] applies at-\ntention operation in the frequency domain with Fourier trans-\nform and wavelet transform. It achieves a linear complex-\nity by randomly selecting a fixed-size subset of frequency.\nNote that due to the success of Autoformer and FEDformer,\nit has attracted more attention in the community to explore\nself-attention mechanisms in the frequency domain for time\nseries modeling. Quatformer [Chen et al., 2022 ] proposes\nlearning-to-rotate attention (LRA) based on quaternions that\nintroduce learnable period and phase information to depict in-\ntricate periodical patterns. Moreover, it decouples LRA using\na global memory to achieve linear complexity.\nThe following three works focus on building an explicit\ninterpretation ability of models, which follows the trend of\nExplainable Artificial Intelligence (XAI). TFT [Lim et al.,\n2021] designs a multi-horizon forecasting model with static\ncovariate encoders, gating feature selection, and temporal\nself-attention decoder. It encodes and selects useful infor-\nmation from various covariates to perform forecasting. It\nalso preserves interpretability by incorporating global, tem-\nporal dependency, and events. ProTran [Tang and Matteson,\n2021] and SSDNet [Lin et al., 2021 ] combine Transformer\nwith state space models to provide probabilistic forecasts.\nProTran designs a generative modeling and inference proce-\ndure based on variational inference. SSDNet first uses Trans-\nformer to learn the temporal pattern and estimate the param-\neters of SSM, and then applies SSM to perform the seasonal-\ntrend decomposition and maintain the interpretable ability.\nThe second type of variant for module-level Transformers\nis how to normalize time series data. For example, Non-\nstationary Transformer [Liu et al., 2022b] mainly focuses on\nmodifying the normalization mechanism as shown in Fig-\nure 2. It explores the over-stationarization problem in time\nseries forecasting tasks with a relatively simple plugin series\nstationary and De-stationary module to modify and boost the\nperformance of various attention blocks.\nThe third type of variant for module-level Transformer is\nutilizing the bias for token input. Autoformer [Wu et al.,\n2021] adopts a segmentation-based representation mecha-\nnism. It devises a simple seasonal-trend decomposition ar-\nchitecture with an auto-correlation mechanism working as\nan attention module. The auto-correlation block measures\nthe time-delay similarity between inputs signal and aggre-\ngates the top-k similar sub-series to produce the output with\nreduced complexity. PatchTST [Nie et al., 2023 ] utilizes\nchannel-independent where each channel contains a single\nunivariate time series that shares the same embedding within\nall the series, and subseries-level patch design which seg-\nmentation of time series into subseries-level patches that are\nserved as input tokens to Transformer. Such ViT [Dosovit-\nskiy et al., 2021] alike design improves its numerical perfor-\nmance in long-time time-series forecasting tasks a lot. Cross-\nformer [Zhang and Yan, 2023] proposes a Transformer-based\nmodel utilizing cross-dimension dependency for multivariate\ntime series forecasting. The input is embedded into a 2D\nvector array through the novel dimension-segment-wise em-\nbedding to preserve time and dimension information. Then,\na two-stage attention layer is used to efficiently capture the\ncross-time and cross-dimension dependency.\nArchitecture-level variants Some works start to design\na new transformer architecture beyond the scope of the\nvanilla transformer. Triformer [Cirstea et al., 2022] design a\ntriangular,variable-specific patch attention. It has a triangular\ntree-type structure as the later input size shrinks exponentially\nand a set of variable-specific parameters making a multi-\nlayer Triformer maintain a lightweight and linear complex-\nity. Scaleformer [Shabani et al., 2023] proposes a multi-scale\nframework that can be applied to the baseline transformer-\nbased time series forecasting models (FEDformer[Zhou et al.,\n2022], Autoformer[Wu et al., 2021], etc.). It can improve the\nbaseline model’s performance by iteratively refining the fore-\ncasted time series at multiple scales with shared weights.\nRemarks Note that DLinear [Zeng et al., 2023 ] questions\nthe necessity of using Transformers for long-term time series\nforecasting, and shows that a simpler MLP-based model can\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6781\nachieve better results compared to some Transformer base-\nlines through empirical studies. However, we notice that a re-\ncent Transformer model PatchTST[Nie et al., 2023] achieves\na better numerical result compared to DLinear for long-term\ntime series forecasting. Moreover, there is a thorough the-\noretical study [Yun et al., 2020 ] showing that the Trans-\nformer models are universal approximators of sequence-to-\nsequence functions. It is a overclaim to question the poten-\ntial of any type of method for time series forecasting based\nsolely on experimental results from some variant instanti-\nations of such method, especially for Transformer models\nwhich already demonstrate the performances in most machine\nlearning-based tasks. Therefore, we conclude that summariz-\ning the recent Transformer-based models for time series fore-\ncasting is necessary and would benefit the whole community.\nSpatio-Temporal Forecasting\nIn spatio-temporal forecasting, both temporal and spatio-\ntemporal dependencies are taken into account in time series\nTransformers for accurate forecasting.\nTraffic Transformer [Cai et al., 2020] designs an encoder-\ndecoder structure using a self-attention module to capture\ntemporal-temporal dependencies and a graph neural network\nmodule to capture spatial dependencies. Spatial-temporal\nTransformer [Xu et al., 2020] for traffic flow forecasting takes\na step further. Besides introducing a temporal Transformer\nblock to capture temporal dependencies, it also designs a\nspatial Transformer block, together with a graph convolu-\ntion network, to better capture spatial-spatial dependencies.\nSpatio-temporal graph Transformer [Yu et al., 2020] designs\nan attention-based graph convolution mechanism that is able\nto learn a complicated temporal-spatial attention pattern to\nimprove pedestrian trajectory prediction. Earthformer [Gao\net al., 2022 ] proposes a cuboid attention for efficient space-\ntime modeling, which decomposes the data into cuboids and\napplies cuboid-level self-attention in parallel. It shows that\nEarthformer achieves superior performance in weather and\nclimate forecasting. Recently, AirFormer [Liang et al., 2023]\ndevises a dartboard spatial self-attention module and a causal\ntemporal self-attention module to efficiently capture spatial\ncorrelations and temporal dependencies, respectively. Fur-\nthermore, it enhances Transformers with latent variables to\ncapture data uncertainty and improve air quality forecasting.\nEvent Forecasting\nEvent sequence data with irregular and asynchronous times-\ntamps are naturally observed in many real-life applications,\nwhich is in contrast to regular time series data with equal\nsampling intervals. Event forecasting or prediction aims to\npredict the times and marks of future events given the his-\ntory of past events, and it is often modeled by temporal point\nprocesses (TPP) [Yan et al., 2019; Shchur et al., 2021].\nRecently, several neural TPP models incorporate Trans-\nformers in order to improve the performance of event pre-\ndiction. Self-attentive Hawkes process (SAHP) [Zhang et al.,\n2020] and Transformer Hawkes process (THP) [Zuo et al.,\n2020] adopt Transformer encoder architecture to summarize\nthe influence of historical events and compute the intensity\nfunction for event prediction. They modify the positional en-\ncoding by translating time intervals into sinusoidal functions\nsuch that the intervals between events can be utilized. Later, a\nmore flexible named attentive neural datalog through time (A-\nNDTT) [Mei et al., 2022] is proposed to extend SAHP/THP\nschemes by embedding all possible events and times with at-\ntention as well. Experiments show that it can better capture\nsophisticated event dependencies than existing methods.\n5.2 Transformers in Anomaly Detection\nTransformer based architecture also benefits the time se-\nries anomaly detection task with the ability to model tem-\nporal dependency, which brings high detection quality [Xu\net al., 2022 ]. Besides, in multiple studies, including\nTranAD [Tuli et al., 2022 ], MT-RV AE[Wang et al., 2022 ],\nand TransAnomaly [Zhang et al., 2021 ], researchers pro-\nposed to combine Transformer with neural generative models,\nsuch as V AEs[Kingma and Welling, 2014] and GANs [Good-\nfellow et al., 2014], for better performance in anomaly detec-\ntion. We will elaborate on these models in the following part.\nTranAD [Tuli et al., 2022 ] proposes an adversarial train-\ning procedure to amplify reconstruction errors as a sim-\nple Transformer-based network tends to miss small devia-\ntion of anomaly. GAN style adversarial training procedure\nis designed by two Transformer encoders and two Trans-\nformer decoders to gain stability. Ablation study shows that,\nif Transformer-based encoder-decoder is replaced, F1 score\ndrops nearly 11%, indicating the effect of Transformer archi-\ntecture on time series anomaly detection.\nMT-RV AE[Wang et al., 2022] and TransAnomaly [Zhang\net al., 2021 ] combine V AE with Transformer, but they\nshare different purposes. TransAnomaly combines V AE with\nTransformer to allow more parallelization and reduce training\ncosts by nearly 80%. In MT-RV AE, a multiscale Transformer\nis designed to extract and integrate time-series information at\ndifferent scales. It overcomes the shortcomings of traditional\nTransformers where only local information is extracted for\nsequential analysis.\nGTA [Chen et al., 2021c ] combines Transformer with\ngraph-based learning architecture for multivariate time series\nanomaly detection. Note that, MT-RV AE is also for multi-\nvariate time series but with few dimensions or insufficient\nclose relationships among sequences where the graph neu-\nral network model does not work well. To deal with such\nchallenge, MT-RV AE modifies the positional encoding mod-\nule and introduces feature-learning module. Instead, GTA\ncontains a graph convolution structure to model the influence\npropagation process. Similar to MT-RV AE, GTA also consid-\ners “global” information, yet by replacing vanilla multi-head\nattention with a multi-branch attention mechanism, that is,\na combination of global-learned attention, vanilla multi-head\nattention, and neighborhood convolution.\nAnomalyTrans [Xu et al., 2022 ] combines Transformer\nand Gaussian prior-Association to make anomalies more dis-\ntinguishable. Sharing similar motivation as TranAD, Anom-\nalyTrans achieves the goal in a different way. The insight is\nthat it is harder for anomalies to build strong associations with\nthe whole series while easier with adjacent time points com-\npared with normality. In AnomalyTrans, prior-association\nand series-association are modeled simultaneously. Besides\nreconstruction loss, the anomaly model is optimized by the\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6782\nminimax strategy to constrain the prior- and series- associa-\ntions for more distinguishable association discrepancy.\n5.3 Transformers in Classification\nTransformer is proved to be effective in various time series\nclassification tasks due to its prominent capability in captur-\ning long-term dependency. GTN [Liu et al., 2021 ] uses a\ntwo-tower Transformer with each tower respectively work-\ning on time-step-wise attention and channel-wise attention.\nTo merge the feature of the two towers, a learnable weighted\nconcatenation (also known as ‘gating’) is used. The proposed\nextension of Transformer achieves state-of-the-art results on\n13 multivariate time series classifications. [Rußwurm and\nK¨orner, 2020 ] studied the self-attention based Transformer\nfor raw optical satellite time series classification and obtained\nthe best results compared with recurrent and convolutional\nneural networks. Recently, TARNet[Chowdhury et al., 2022]\ndesigns Transformers to learn task-aware data reconstruction\nthat augments classification performance, which utilizes at-\ntention score for important timestamps masking and recon-\nstruction and brings superior performance.\nPre-trained Transformers are also investigated in classifica-\ntion tasks. [Yuan and Lin, 2020] studies the Transformer for\nraw optical satellite image time series classification. The au-\nthors use self-supervised pre-trained schema because of lim-\nited labeled data. [Zerveas et al., 2021] introduced an unsu-\npervised pre-trained framework and the model is pre-trained\nwith proportionally masked data. The pre-trained models are\nthen fine-tuned in downstream tasks such as classification.\n[Yang et al., 2021 ] proposes to use large-scale pre-trained\nspeech processing model for downstream time series classi-\nfication problems and generates 19 competitive results on 30\npopular time series classification datasets.\n6 Experimental Evaluation and Discussion\nWe conduct preliminary empirical studies on a typical chal-\nlenging benchmark dataset ETTm2 [Zhou et al., 2021 ] to\nanalyze how Transformers work on time series data. Since\nclassic statistical ARIMA/ETS [Hyndman and Khandakar,\n2008] models and basic RNN/CNN models perform inferior\nto Transformers in this dataset as shown in[Zhou et al., 2021;\nWu et al., 2021], we focus on popular time series Transform-\ners with different configurations in the experiments.\nRobustness Analysis\nA lot of works we describe above carefully design attention\nmodules to lower the quadratic calculation and memory com-\nplexity, though they practically use a short fixed-size input to\nachieve the best result in their reported experiments. It makes\nus question the actual usage of such an efficient design. We\nperform a robust experiment with prolonging input sequence\nlength to verify their prediction power and robustness when\ndealing with long-term input sequences in Table ??.\nAs in Table ??, when we compare the prediction results\nwith prolonging input length, various Transformer-based\nmodel deteriorates quickly. This phenomenon makes a lot\nof carefully designed Transformers impractical in long-term\nforecasting tasks since they cannot effectively utilize long in-\nput information. More works and designs need to be inves-\nModel Transformer\nAutoformer Informer Reformer LogFormer\nIn\nput Len\n96 0.557 0.239 0.428 0.615\n0.667\n192 0.710 0.265 0.385 0.686\n0.697\n336 1.078 0.375\n1.078 1.359 0.937\n720 1.691 0.315\n1.057 1.443 2.153\n1440 0.936 0.552\n1.898 0.815 0.867\nTable 2: The MSE comparisons in robustness experiment of fore-\ncasting 96 steps for ETTm2 dataset with prolonging input length.\nModel Transformer\nAutoformer Informer Reformer LogFormer\nLayer\nNum\n3 0.557 0.234 0.428 0.597\n0.667\n6 0.439 0.282 0.489 0.353\n0.387\n12 0.556 0.238\n0.779 0.481 0.562\n24 0.580 0.266\n0.815 1.109 0.690\n48 0.461 NaN\n1.623 OOM 2.992\nTable 3: The MSE comparisons in model size experiment of fore-\ncasting 96 steps for ETTm2 dataset with different number of layers.\ntigated to fully utilize long sequence input for better perfor-\nmance.\nModel Size Analysis\nBefore being introduced into the field of time series predic-\ntion, Transformer has shown dominant performance in NLP\nand CV communities [Vaswani et al., 2017; Kenton and oth-\ners, 2019; Han et al., 2021; Han et al., 2022 ]. One of the\nkey advantages Transformer holds in these fields is being able\nto increase prediction power through increasing model size.\nUsually, the model capacity is controlled by Transformer’s\nlayer number, which is commonly set between 12 to 128. Yet\nas shown in the experiments of Table ??, when we compare\nthe prediction result with different Transformer models with\nvarious numbers of layers, the Transformer with 3 to 6 layers\noften achieves better results. It raises a question about how to\ndesign a proper Transformer architecture with deeper layers\nto increase the model’s capacity and achieve better forecast-\ning performance.\nSeasonal-Trend Decomposition Analysis\nIn recent studies, researchers [Wu et al., 2021; Zhou et al.,\n2022; Lin et al., 2021; Liu et al. , 2022a ] begin to realize\nthat the seasonal-trend decomposition[Cleveland et al., 1990;\nWen et al., 2020 ] is a crucial part of Transformer’s perfor-\nmance in time series forecasting. As an experiment shown in\nTable ??, we adopt a simple moving average seasonal-trend\ndecomposition architecture proposed in [Wu et al., 2021 ] to\ntest various attention modules. It can be seen that the simple\nseasonal-trend decomposition model can significantly boost\nmodel’s performance by 50 % to 80%. It is a unique block\nand such performance boosting through decomposition seems\na consistent phenomenon in time series forecasting for Trans-\nformer’s application, which is worth further investigating for\nmore advanced and carefully designed time series decompo-\nsition schemes.\n7 Future Research Opportunities\n7.1 Inductive Biases for Time Series Transformers\nVanilla Transformer does not make any assumptions about\ndata patterns and characteristics. Although it is a general\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6783\nModel FEDformer Autoformer Informer LogTrans Reformer Transformer Promotion\nMSE Ori Decomp\nOri Decomp Ori Decomp Ori Decomp Ori Decomp Ori Decomp Relative\nOut\nLen\n96 0.457 0.203\n0.581 0.255 0.365 0.354 0.768 0.231 0.658 0.218 0.604 0.204 53%\n192 0.841 0.269\n1.403 0.281 0.533 0.432 0.989 0.378 1.078 0.336 1.060 0.266 62%\n336 1.451 0.325\n2.632 0.339 1.363 0.481 1.334 0.362 1.549 0.366 1.413 0.375 75%\n720 3.282 0.421\n3.058 0.422 3.379 0.822 3.048 0.539 2.631 0.502 2.672 0.537 82%\nTable 4: The MSE comparisons in ablation experiments of seasonal-trend decomposition analysis. ’Ori’ means the original version without\nthe decomposition. ’Decomp’ means with decomposition. The experiment is performed on ETTm2 dataset with prolonging output length.\nand universal network for modeling long-range dependen-\ncies, it also comes with a price, i.e., lots of data are needed\nto train Transformer to improve the generalization and avoid\ndata overfitting. One of the key features of time series data\nis its seasonal/periodic and trend patterns [Wen et al., 2019;\nCleveland et al., 1990 ]. Some recent studies have shown\nthat incorporating series periodicity [Wu et al., 2021] or fre-\nquency processing [Zhou et al., 2022] into time series Trans-\nformer can enhance performance significantly. Moreover, it\nis interesting that some studies adopt a seemly opposite in-\nductive bias, but both achieve good numerical improvement:\n[Nie et al., 2023] removes the cross-channel dependency by\nutilizing a channel-independent attention module, while an\ninteresting work [Zhang and Yan, 2023] improves its experi-\nmental performance by utilizing cross-dimension dependency\nwith a two-stage attention mechanism. Clearly, we have noise\nand signals in such a cross-channel learning paradigm, but a\nclever way to utilize such inductive bias to suppress the noise\nand extract the signal is still desired. Thus, one future direc-\ntion is to consider more effective ways to induce inductive\nbiases into Transformers based on the understanding of time\nseries data and characteristics of specific tasks.\n7.2 Transformers and GNN for Time Series\nMultivariate and spatio-temporal time series are becoming\nincreasingly common in applications, calling for additional\ntechniques to handle high dimensionality, especially the abil-\nity to capture the underlying relationships among dimen-\nsions. Introducing graph neural networks (GNNs) is a natural\nway to model spatial dependency or relationships among di-\nmensions. Recently, several studies have demonstrated that\nthe combination of GNN and Transformers/attentions could\nbring not only significant performance improvements like in\ntraffic forecasting[Cai et al., 2020; Xuet al., 2020] and multi-\nmodal forecasting [Li et al., 2021], but also better understand-\ning of the spatio-temporal dynamics and latent causality. It is\nan important future direction to combine Transformers and\nGNNs for effectively spatial-temporal modeling.\n7.3 Pre-trained Transformers for Time Series\nLarge-scale pre-trained Transformer models have signif-\nicantly boosted the performance for various tasks in\nNLP [Kenton and others, 2019; Brown et al., 2020 ] and\nCV [Chen et al., 2021a ]. However, there are limited works\non pre-trained Transformers for time series, and existing stud-\nies mainly focus on time series classification [Zerveas et al.,\n2021; Yang et al., 2021 ]. Therefore, how to develop appro-\npriate pre-trained Transformer models for different tasks in\ntime series remains to be examined in the future.\n7.4 Transformers with Architecture Level Variants\nMost developed Transformer models for time series main-\ntain the vanilla Transformer’s architecture with modifications\nmainly in the attention module. We might borrow the idea\nfrom Transformer variants in NLP and CV which also have\narchitecture-level model designs to fit different purposes,\nsuch as lightweight [Wu et al., 2020b; Mehta et al., 2021 ],\ncross-block connectivity [Bapna et al., 2018], adaptive com-\nputation time [Dehghani et al., 2019; Xin et al., 2020 ], and\nrecurrence [Dai et al., 2019]. Therefore, one future direction\nis to consider more architecture-level designs for Transform-\ners specifically optimized for time series data and tasks.\n7.5 Transformers with NAS for Time Series\nHyper-parameters, such as embedding dimension and the\nnumber of heads/layers, can largely affect the performance\nof Transformers. Manual configuring these hyper-parameters\nis time-consuming and often results in suboptimal perfor-\nmance. AutoML technique like Neural architecture search\n(NAS) [Elsken et al., 2019; Wang et al., 2020 ] has been a\npopular technique for discovering effective deep neural archi-\ntectures, and automating Transformer design using NAS in\nNLP and CV can be found in recent studies [So et al., 2019;\nChen et al., 2021b]. For industry-scale time series data which\ncan be of both high dimension and long length, automatically\ndiscovering both memory- and computational-efficient Trans-\nformer architectures is of practical importance, making it an\nimportant future direction for time series Transformers.\n8 Conclusion\nIn this paper, we provide a survey on time series Transform-\ners. We organize the reviewed methods in a new taxonomy,\nsummarize representative methods in each category, discuss\ntheir strengths and limitations by experimental evaluation,\nand highlight future research directions.\nReferences\n[Bapna et al., 2018] Ankur Bapna, Mia Xu Chen, Orhan Firat,\nYuan Cao, and Yonghui Wu. Training deeper neural machine\ntranslation models with transparent attention. In EMNLP, 2018.\n[Benidis et al., 2022] Konstantinos Benidis, Syama Sundar Ranga-\npuram, Valentin Flunkert, Yuyang Wang, Danielle Maddix, , et al.\nDeep learning for time series forecasting: Tutorial and literature\nsurvey. ACM Computing Surveys, 55(6):1–36, 2022.\n[Bl´azquez-Garc´ıa et al., 2021] Ane Bl ´azquez-Garc´ıa, Angel\nConde, Usue Mori, et al. A review on outlier/anomaly detection\nin time series data. ACM Computing Surveys, 54(3):1–33, 2021.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6784\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, et al. Lan-\nguage models are few-shot learners. NeurIPS, 2020.\n[Cai et al., 2020] Ling Cai, Krzysztof Janowicz, Gengchen Mai,\nBo Yan, and Rui Zhu. Traffic transformer: Capturing the conti-\nnuity and periodicity of time series for traffic forecasting. Trans-\nactions in GIS, 24(3):736–755, 2020.\n[Chen et al., 2021a] Hanting Chen, Yunhe Wang, Tianyu Guo,\nChang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\net al. Pre-trained image processing transformer. In CVPR, 2021.\n[Chen et al., 2021b] Minghao Chen, Houwen Peng, Jianlong Fu,\nand Haibin Ling. AutoFormer: Searching transformers for vi-\nsual recognition. In CVPR, 2021.\n[Chen et al., 2021c] Zekai Chen, Dingshuo Chen, Xiao Zhang, Zix-\nuan Yuan, and Xiuzhen Cheng. Learning graph structures with\ntransformer for multivariate time series anomaly detection in IoT.\nIEEE Internet of Things Journal, 2021.\n[Chen et al., 2022] Weiqi Chen, Wenwei Wang, Bingqing Peng,\nQingsong Wen, Tian Zhou, and Liang Sun. Learning to rotate:\nQuaternion transformer for complicated periodical time series\nforecasting. In KDD, 2022.\n[Choi et al., 2021] Kukjin Choi, Jihun Yi, Changhwa Park, and\nSungroh Yoon. Deep learning for anomaly detection in time-\nseries data: Review, analysis, and guidelines.IEEE Access, 2021.\n[Chowdhury et al., 2022] Ranak Roy Chowdhury, Xiyuan Zhang,\nJingbo Shang, Rajesh K Gupta, and Dezhi Hong. TARNet: Task-\naware reconstruction for time-series transformer. In KDD, 2022.\n[Cirstea et al., 2022] Razvan-Gabriel Cirstea, Chenjuan Guo, Bin\nYang, Tung Kieu, Xuanyi Dong, and Shirui Pan. Triformer: Tri-\nangular, variable-specific attentions for long sequence multivari-\nate time series forecasting. In IJCAI, 2022.\n[Cleveland et al., 1990] Robert Cleveland, William Cleveland, Jean\nMcRae, et al. STL: A seasonal-trend decomposition procedure\nbased on loess. Journal of Official Statistics, 6(1):3–73, 1990.\n[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\nCarbonell, Quoc V . Le, et al. Transformer-XL: Attentive lan-\nguage models beyond a fixed-length context. In ACL, 2019.\n[Dehghani et al., 2019] Mostafa Dehghani, Stephan Gouws, Oriol\nVinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal trans-\nformers. In ICLR, 2019.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. In ICLR, 2021.\n[Elsken et al., 2019] Elsken, Thomas, Jan Hendrik Metzen, and\nFrank Hutter. Neural architecture search: A survey. Journal of\nMachine Learning Research, 2019.\n[Gao et al., 2022] Zhihan Gao, Xingjian Shi, Hao Wang, Yi Zhu,\nBernie Wang, Mu Li, et al. Earthformer: Exploring space-time\ntransformers for earth system forecasting. In NeurIPS, 2022.\n[Gehring et al., 2017] Jonas Gehring, Michael Auli, David Grang-\nier, Denis Yarats, and Yann N Dauphin. Convolutional sequence\nto sequence learning. In ICML, 2017.\n[Goodfellow et al., 2014] Ian Goodfellow, Jean Pouget-Abadie,\nMehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, et al.\nGenerative adversarial nets. NeurIPS, 2014.\n[Han et al., 2021] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian\nGu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, et al. Pre-\ntrained models: Past, present and future. AI Open, 2021.\n[Han et al., 2022] Kai Han, Yunhe Wang, Hanting Chen, Xinghao\nChen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, et al. A\nsurvey on vision transformer. IEEE TPAMI, 45(1):87–110, 2022.\n[Hyndman and Khandakar, 2008] Rob J Hyndman and Yeasmin\nKhandakar. Automatic time series forecasting: the forecast pack-\nage for r. Journal of statistical software, 27:1–22, 2008.\n[Ismail Fawaz et al., 2019] Hassan Ismail Fawaz, Germain\nForestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-\nAlain Muller. Deep learning for time series classification: a\nreview. Data mining and knowledge discovery, 2019.\n[Ke et al., 2021] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking\npositional encoding in language pre-training. In ICLR, 2021.\n[Kenton and others, 2019] Jacob Devlin Ming-Wei Chang Kenton\net al. BERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In NAACL-HLT, 2019.\n[Kingma and Welling, 2014] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes. In ICLR, 2014.\n[Li et al., 2019] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou,\nWenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the\nlocality and breaking the memory bottleneck of transformer on\ntime series forecasting. In NeurIPS, 2019.\n[Li et al., 2021] Longyuan Li, Jian Yao, Li Wenliang, Tong He,\nTianjun Xiao, Junchi Yan, David Wipf, and Zheng Zhang. Grin:\nGenerative relation and intention network for multi-agent trajec-\ntory prediction. In NeurIPS, 2021.\n[Liang et al., 2023] Yuxuan Liang, Yutong Xia, Songyu Ke, Yiwei\nWang, Qingsong Wen, Junbo Zhang, Yu Zheng, and Roger Zim-\nmermann. AirFormer: Predicting nationwide air quality in china\nwith transformers. In AAAI, 2023.\n[Lim and Zohren, 2021] Bryan Lim and Stefan Zohren. Time-\nseries forecasting with deep learning: a survey. Philosophical\nTransactions of the Royal Society, 2021.\n[Lim et al., 2021] Bryan Lim, Sercan ¨O Arık, Nicolas Loeff, and\nTomas Pfister. Temporal fusion transformers for interpretable\nmulti-horizon time series forecasting. International Journal of\nForecasting, 37(4):1748–1764, 2021.\n[Lin et al., 2021] Yang Lin, Irena Koprinska, and Mashud Rana.\nSSDNet: State space decomposition neural network for time se-\nries forecasting. In ICDM, 2021.\n[Liu et al., 2021] Minghao Liu, Shengqi Ren, Siyuan Ma, Jiahui\nJiao, Yizhou Chen, Zhiguang Wang, and Wei Song. Gated trans-\nformer networks for multivariate time series classification. arXiv\npreprint arXiv:2103.14438, 2021.\n[Liu et al., 2022a] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li,\nWeiyao Lin, Alex X. Liu, and Schahram Dustdar. Pyraformer:\nLow-complexity pyramidal attention for long-range time series\nmodeling and forecasting. In ICLR, 2022.\n[Liu et al., 2022b] Yong Liu, Haixu Wu, Jianmin Wang, and Ming-\nsheng Long. Non-stationary transformers: Exploring the station-\narity in time series forecasting. In NeurIPS, 2022.\n[Mehta et al., 2021] Sachin Mehta, Marjan Ghazvininejad, Srini\nIyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Delight: Deep\nand light-weight transformer. In ICLR, 2021.\n[Mei et al., 2022] Hongyuan Mei, Chenghao Yang, and Jason Eis-\nner. Transformer embeddings of irregularly spaced events and\ntheir participants. In ICLR, 2022.\n[Nie et al., 2023] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong,\nand Jayant Kalagnanam. A time series is worth 64 words: Long-\nterm forecasting with transformers. In ICLR, 2023.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6785\n[Rußwurm and K¨orner, 2020] Marc Rußwurm and Marco K ¨orner.\nSelf-attention for raw optical satellite time series classification.\nISPRS J. Photogramm. Remote Sens., 169:421–435, 11 2020.\n[Shabani et al., 2023] Amin Shabani, Amir Abdi, Lili Meng, and\nTristan Sylvain. Scaleformer: iterative multi-scale refining trans-\nformers for time series forecasting. In ICLR, 2023.\n[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish\nVaswani. Self-attention with relative position representations. In\nNAACL, 2018.\n[Shchur et al., 2021] Oleksandr Shchur, Ali Caner T ¨urkmen, Tim\nJanuschowski, and Stephan G ¨unnemann. Neural temporal point\nprocesses: A review. In IJCAI, 2021.\n[So et al., 2019] David So, Quoc Le, and Chen Liang. The evolved\ntransformer. In ICML, 2019.\n[Tang and Matteson, 2021] Binh Tang and David Matteson. Proba-\nbilistic transformer for time series analysis. In NeurIPS, 2021.\n[Tay et al., 2022] Yi Tay, Mostafa Dehghani, Dara Bahri, and Don-\nald Metzler. Efficient transformers: A survey. ACM Computing\nSurveys, 55(6):1–28, 2022.\n[Torres et al., 2021] Jos´e F. Torres, Dalil Hadjout, Abderrazak Se-\nbaa, Francisco Mart ´ınez- ´Alvarez, and Alicia Troncoso. Deep\nlearning for time series forecasting: a survey. Big Data, 2021.\n[Tuli et al., 2022] Shreshth Tuli, Giuliano Casale, and Nicholas R\nJennings. TranAD: Deep transformer networks for anomaly de-\ntection in multivariate time series data. In VLDB, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, et al. Attention is all you need. In NeurIPS, 2017.\n[Wang et al., 2020] Xiaoxing Wang, Chao Xue, Junchi Yan, Xi-\naokang Yang, Yonggang Hu, et al. MergeNAS: Merge operations\ninto one for differentiable architecture search. In IJCAI, 2020.\n[Wang et al., 2022] Xixuan Wang, Dechang Pi, Xiangyan Zhang,\net al. Variational transformer-based anomaly detection approach\nfor multivariate time series. Measurement, page 110791, 2022.\n[Wen et al., 2019] Qingsong Wen, Jingkun Gao, Xiaomin Song,\nLiang Sun, Huan Xu, et al. RobustSTL: A robust seasonal-trend\ndecomposition algorithm for long time series. In AAAI, 2019.\n[Wen et al., 2020] Qingsong Wen, Zhe Zhang, Yan Li, and Liang\nSun. Fast RobustSTL: Efficient and robust seasonal-trend decom-\nposition for time series with complex patterns. In KDD, 2020.\n[Wen et al., 2021a] Qingsong Wen, Kai He, Liang Sun, Yingying\nZhang, Min Ke, et al. RobustPeriod: Time-frequency mining for\nrobust multiple periodicities detection. In SIGMOD, 2021.\n[Wen et al., 2021b] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin\nSong, Jingkun Gao, Xue Wang, and Huan Xu. Time series data\naugmentation for deep learning: A survey. In IJCAI, 2021.\n[Wen et al., 2022] Qingsong Wen, Linxiao Yang, Tian Zhou, and\nLiang Sun. Robust time series analysis and applications: An in-\ndustrial perspective. In KDD, 2022.\n[Wu et al., 2020a] Sifan Wu, Xi Xiao, Qianggang Ding, Peilin\nZhao, Ying Wei, and Junzhou Huang. Adversarial sparse trans-\nformer for time series forecasting. In NeurIPS, 2020.\n[Wu et al., 2020b] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin,\nand Song Han. Lite transformer with long-short range attention.\nIn ICLR, 2020.\n[Wu et al., 2021] Haixu Wu, Jiehui Xu, Jianmin Wang, and Ming-\nsheng Long. Autoformer: Decomposition transformers with\nauto-correlation for long-term series forecasting. In NeurIPS,\n2021.\n[Xin et al., 2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu,\nand Jimmy J. Lin. DeeBERT: Dynamic early exiting for acceler-\nating bert inference. In ACL, 2020.\n[Xu et al., 2020] Mingxing Xu, Wenrui Dai, Chunmiao Liu, Xing\nGao, Weiyao Lin, Guo-Jun Qi, and Hongkai Xiong. Spatial-\ntemporal transformer networks for traffic flow forecasting. arXiv\npreprint arXiv:2001.02908, 2020.\n[Xu et al., 2022] Jiehui Xu, Haixu Wu, Jianmin Wang, and Ming-\nsheng Long. Anomaly Transformer: Time series anomaly detec-\ntion with association discrepancy. In ICLR, 2022.\n[Yan et al., 2019] Junchi Yan, Hongteng Xu, and Liangda Li. Mod-\neling and applications for temporal point processes. In KDD,\n2019.\n[Yang et al., 2021] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-\nYu Chen. V oice2series: Reprogramming acoustic models for\ntime series classification. In ICML, 2021.\n[Yu et al., 2020] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and\nShuai Yi. Spatio-temporal graph transformer networks for pedes-\ntrian trajectory prediction. In ECCV, 2020.\n[Yuan and Lin, 2020] Yuan Yuan and Lei Lin. Self-supervised pre-\ntraining of transformers for satellite image time series classifica-\ntion. IEEE J-STARS, 14:474–487, 2020.\n[Yun et al., 2020] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat, Sashank J. Reddi, et al. Are transformers universal ap-\nproximators of sequence-to-sequence functions? In ICLR, 2020.\n[Zeng et al., 2023] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang\nXu. Are transformers effective for time series forecasting? In\nAAAI, 2023.\n[Zerveas et al., 2021] George Zerveas, Srideepika Jayaraman,\nDhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A\ntransformer-based framework for multivariate time series repre-\nsentation learning. In KDD, 2021.\n[Zhang and Yan, 2023] Yunhao Zhang and Junchi Yan. Cross-\nformer: Transformer utilizing cross-dimension dependency for\nmultivariate time series forecasting. In ICLR, 2023.\n[Zhang et al., 2020] Qiang Zhang, Aldo Lipani, Omer Kirnap, and\nEmine Yilmaz. Self-attentive Hawkes process. In ICML, 2020.\n[Zhang et al., 2021] Hongwei Zhang, Yuanqing Xia, et al. Unsu-\npervised anomaly detection in multivariate time series through\ntransformer-based variational autoencoder. In CCDC, 2021.\n[Zhou et al., 2021] Haoyi Zhou, Shanghang Zhang, Jieqi Peng,\nShuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. In-\nformer: Beyond efficient transformer for long sequence time-\nseries forecasting. In AAAI, 2021.\n[Zhou et al., 2022] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue\nWang, Liang Sun, and Rong Jin. FEDformer: Frequency en-\nhanced decomposed transformer for long-term series forecasting.\nIn ICML, 2022.\n[Zuo et al., 2020] Simiao Zuo, Haoming Jiang, Zichong Li, Tuo\nZhao, and Hongyuan Zha. Transformer Hawkes process. In\nICML, 2020.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6786",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7005069851875305
    },
    {
      "name": "Transformer",
      "score": 0.6954469084739685
    },
    {
      "name": "Time series",
      "score": 0.5364312529563904
    },
    {
      "name": "Categorization",
      "score": 0.426440566778183
    },
    {
      "name": "Artificial intelligence",
      "score": 0.305425763130188
    },
    {
      "name": "Machine learning",
      "score": 0.2898412346839905
    },
    {
      "name": "Engineering",
      "score": 0.2018413543701172
    },
    {
      "name": "Electrical engineering",
      "score": 0.09999969601631165
    },
    {
      "name": "Voltage",
      "score": 0.08229216933250427
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210108985",
      "name": "Bellevue Hospital Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ],
  "cited_by": 780
}