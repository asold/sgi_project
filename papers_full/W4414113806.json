{
  "title": "Critical Assessment of Large Language Models’ (ChatGPT) Performance in Data Extraction for Systematic Reviews: Exploratory Study",
  "url": "https://openalex.org/W4414113806",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2222952014",
      "name": "Hesam Mahmoudi",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2670378536",
      "name": "Doris Chang",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2103161172",
      "name": "Hannah Lee",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A315543400",
      "name": "Navid Ghaffarzadegan",
      "affiliations": [
        "Capital Meeting Planning"
      ]
    },
    {
      "id": "https://openalex.org/A2558724433",
      "name": "Mohammad S. Jalali",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2222952014",
      "name": "Hesam Mahmoudi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2670378536",
      "name": "Doris Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103161172",
      "name": "Hannah Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A315543400",
      "name": "Navid Ghaffarzadegan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2558724433",
      "name": "Mohammad S. Jalali",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4200219459",
    "https://openalex.org/W3179828707",
    "https://openalex.org/W4389166370",
    "https://openalex.org/W4385297391",
    "https://openalex.org/W4282937925",
    "https://openalex.org/W4213127247",
    "https://openalex.org/W4382310450",
    "https://openalex.org/W1629765770",
    "https://openalex.org/W4395050309",
    "https://openalex.org/W4392791588",
    "https://openalex.org/W4392343921",
    "https://openalex.org/W4399812563",
    "https://openalex.org/W4388703015",
    "https://openalex.org/W4367368990",
    "https://openalex.org/W4399207674",
    "https://openalex.org/W4400981465",
    "https://openalex.org/W4412364864",
    "https://openalex.org/W3155313092",
    "https://openalex.org/W3016867869",
    "https://openalex.org/W3126874005",
    "https://openalex.org/W3026039810",
    "https://openalex.org/W3091913860",
    "https://openalex.org/W3008049045",
    "https://openalex.org/W3087879971",
    "https://openalex.org/W3016129757",
    "https://openalex.org/W3093800076",
    "https://openalex.org/W3129361321",
    "https://openalex.org/W4386322180",
    "https://openalex.org/W4387450484",
    "https://openalex.org/W4410157139",
    "https://openalex.org/W4406140367",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W4400411501",
    "https://openalex.org/W3163124033"
  ],
  "abstract": "Abstract Background Systematic literature reviews (SLRs) are foundational for synthesizing evidence across diverse fields and are especially important in guiding research and practice in health and biomedical sciences. However, they are labor intensive due to manual data extraction from multiple studies. As large language models (LLMs) gain attention for their potential to automate research tasks and extract basic information, understanding their ability to accurately extract explicit data from academic papers is critical for advancing SLRs. Objective Our study aimed to explore the capability of LLMs to extract both explicitly outlined study characteristics and deeper, more contextual information requiring nuanced evaluations, using ChatGPT (GPT-4). Methods We screened the full text of a sample of COVID-19 modeling studies and analyzed three basic measures of study settings (ie, analysis location, modeling approach, and analyzed interventions) and three complex measures of behavioral components in models (ie, mobility, risk perception, and compliance). To extract data on these measures, two researchers independently extracted 60 data elements using manual coding and compared them with the responses from ChatGPT to 420 queries spanning 7 iterations. Results ChatGPT’s accuracy improved as prompts were refined, showing improvements of 33% and 23% between the initial and final iterations for extracting study settings and behavioral components, respectively. In the initial prompts, 26 (43.3%) of 60 ChatGPT responses were correct. However, in the final iteration, ChatGPT extracted 43 (71.7%) of the 60 data elements, showing better performance in extracting explicitly stated study settings (28/30, 93.3%) than in extracting subjective behavioral components (15/30, 50%). Nonetheless, the varying accuracy across measures highlighted its limitations. Conclusions Our findings underscore LLMs’ utility in extracting basic as well as explicit data in SLRs by using effective prompts. However, the results reveal significant limitations in handling nuanced, subjective criteria, emphasizing the necessity for human oversight.",
  "full_text": null,
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    }
  ]
}