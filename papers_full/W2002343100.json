{
    "title": "A General Technique to Train Language Models on Language Models",
    "url": "https://openalex.org/W2002343100",
    "year": 2005,
    "authors": [
        {
            "id": "https://openalex.org/A4208041620",
            "name": "Mark-Jan Nederhof",
            "affiliations": [
                "University of Groningen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2111041233",
        "https://openalex.org/W2036540855",
        "https://openalex.org/W2170336504",
        "https://openalex.org/W2081516556",
        "https://openalex.org/W2089385274",
        "https://openalex.org/W2125529971",
        "https://openalex.org/W92394816",
        "https://openalex.org/W2582290567",
        "https://openalex.org/W89965911",
        "https://openalex.org/W4205167092",
        "https://openalex.org/W124331494",
        "https://openalex.org/W177913229",
        "https://openalex.org/W2103005629",
        "https://openalex.org/W2084162167",
        "https://openalex.org/W1687940340",
        "https://openalex.org/W2123099569",
        "https://openalex.org/W1503669439",
        "https://openalex.org/W1968330359",
        "https://openalex.org/W1574901103"
    ],
    "abstract": "We show that under certain conditions, a language model can be trained on the basis of a second language model. The main instance of the technique trains a finite automaton on the basis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between grammar and trained automaton is provably minimal. This is a substantial generalization of an existing algorithm to train an n-gram model on the basis of a probabilistic context-free grammar.",
    "full_text": "A General Technique to Train Language\nModels on Language Models\nMark-Jan Nederhof∗\nUniversity of Groningen\nWe show that under certain conditions, a language model can be trained on the basis of a\nsecond language model. The main instance of the technique trains a ﬁnite automaton on the\nbasis of a probabilistic context-free grammar, such that the Kullback-Leibler distance between\ngrammar and trained automaton is provably minimal. This is a substantial generalization of\nan existing algorithm to train an n-gram model on the basis of a probabilistic context-free\ngrammar.\n1. Introduction\nIn this article, the term language model is used to refer to any description that assigns\nprobabilities to strings over a certain alphabet. Language models have important\napplications in natural language processing, and in particular, in speech recognition\nsystems (Manning and Sch ¨utze 1999).\nLanguage models often consist of a symbolic description of a language, such as\na ﬁnite automaton (FA) or a context-free grammar (CFG), extended by a probability\nassignment to, for example, the transitions of the FA or the rules of the CFG, by which\nwe obtain a probabilistic ﬁnite automaton (PFA) or probabilistic context-free grammar\n(PCFG), respectively. For certain applications, one may ﬁrst determine the symbolic part\nof the automaton or grammar and in a second phase try to ﬁnd reliable probability\nestimates for the transitions or rules. The current article is involved with the second\nproblem, that of extending FAs or CFGs to become PFAs or PCFGs. We refer to this\nprocess as training.\nTraining is often done on the basis of a corpus of actual language use in a certain\ndomain. If each sentence in this corpus is annotated by a list of transitions of an\nFA recognizing the sentence or a parse tree for a CFG generating the sentence, then\ntraining may consist simply in relative frequency estimation. This means that we estimate\nprobabilities of transitions or rules by counting their frequencies in the corpus, relative\nto the frequencies of the start states of transitions or to the frequencies of the left-hand\nside nonterminals of rules, respectively. By this estimation, the likelihood of the corpus\nis maximized.\nThe technique we introduce in this article is different in that training is done on\nthe basis not of a ﬁnite corpus, but of an input language model. Our goal is to ﬁnd\nestimations for the probabilities of transitions or rules of the input FA or CFG such that\n∗ Faculty of Arts, Humanities Computing, P .O. Box 716, NL-9700 AS Groningen, The Netherlands.\nE-mail: markjan@let.rug.nl.\nSubmission received: 20th January 2004; Revised submission received: 5th August 2004; Accepted for\npublication: 19th September 2004\n© 2005 Association for Computational Linguistics\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nComputational Linguistics Volume 31, Number 2\nthe resulting PFA or PCFG approximates the input language model as well as possible,\nor more speciﬁcally, such that the Kullback-Leibler (KL) distance (or relative entropy)\nbetween the input model and the trained model is minimized. The input FA or CFG to\nbe trained may be structurally unrelated to the input language model.\nThis technique has several applications. One is an extension with probabilities\nof existing work on approximation of CFGs by means of FAs (Nederhof 2000). The\nmotivation for this work was that application of FAs is generally less costly than\napplication of CFGs, which is an important beneﬁt when the input is very large, as\nis often the case in, for example, speech recognition systems. The practical relevance of\nthis work was limited, however, by the fact that in practice one is more interested in\nthe probabilities of sentences than in a purely Boolean distinction between grammatical\nand ungrammatical sentences.\nSeveral approaches were discussed by Mohri and Nederhof (2001) to extend this\nwork to approximation of PCFGs by means of PFAs. A ﬁrst approach is to directly map\nrules with attached probabilities to transitions with attached probabilities. Although\nthis is computationally the easiest approach, the resulting PFA may be a very inaccurate\napproximation of the probability distribution described by the input PCFG. In particu-\nlar, there may be assignments of probabilities to the transitions of the same FA that lead\nto more accurate approximating language models.\nA second approach is to train the approximating FA by means of a corpus. If\nthe input PCFG was itself obtained by training on a corpus, then we already possess\ntraining material. However, this may not always be the case, and no training material\nmay be available. Furthermore, as a determinized approximating FA may be much\nlarger than the input PCFG, the sparse-data problem may be more severe for the\nautomaton than it was for the grammar.\n1 Hence, even if sufﬁcient material was available\nto train the CFG, it may not be sufﬁcient to accurately train the FA.\nA third approach is to construct a training corpus from the PCFG by means of\na (pseudo)random generator of sentences, such that sentences that are more likely\naccording to the PCFG are generated with greater likelihood. This has been proposed\nby Jurafsky et al. (1994), for the special case of bigrams, extending a nonprobabilistic\ntechnique by Zue et al. (1991). It is not clear, however, whether this idea is feasible\nfor training of ﬁnite-state models that are larger than bigrams. The reason is that\nvery large corpora would have to be generated in order to obtain accurate probability\nestimates for the PFA. Note that the number of parameters of a bigram model is\nbounded by the square of the size of the lexicon; such a bound does not exist for\ngeneral PFAs.\nThe current article discusses a fourth approach. In the limit, it is equivalent to the\nthird approach above, as if an inﬁnite corpus were constructed on which the PFA is\ntrained, but we have found a way to avoid considering sentences individually. The key\nidea that allows us to handle an inﬁnite set of strings generated by the PCFG is that we\nconstruct a new grammar that represents the intersection of the languages described by\nthe input PCFG and the FA. Within this new grammar, we can compute the expected\nfrequencies of transitions of the FA, using a fairly standard analysis of PCFGs. These\nexpected frequencies then allow us to determine the assignment of probabilities to\ntransitions of the FA that minimizes the KL distance between the PCFG and the resulting\nPFA.\n1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized\napproximating FAs that can be much larger than the input CFGs.\n174\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nNederhof Training Models on Models\nThe only requirement is that the FA to be trained be unambiguous, by which we\nmean that each input string can be recognized by at most one computation of the FA.\nThe special case of n-grams has already been formulated by Stolcke and Segal (1994),\nrealizing an idea previously envisioned by Rimon and Herz (1991). Ann-gram model is\nhere seen as a (P)FA that contains exactly one state for each possible history of then − 1\npreviously read symbols. It is clear that such an FA is unambiguous (even deterministic)\nand that our technique therefore properly subsumes the technique by Stolcke and Segal\n(1994), although the way that the two techniques are formulated is rather different. Also\nnote that the FA underlying ann-gram model acceptsany input string over the alphabet,\nwhich does not hold for general (unambiguous) FAs.\nAnother application of our work involves determinization and minimization of\nPFAs. As shown by Mohri (1997), PFAs cannot always be determinized, and no practical\nalgorithms are known to minimize arbitrary nondeterministic (P)FAs. This can be a\nproblem when deterministic or small PFAs are required. We can, however, always\ncompute a minimal deterministic FA equivalent to an input FA. The new results in this\narticle offer a way to extend this determinized FA to a PFA such that it approximates\nthe probability distribution described by the input PFA as well as possible, in terms of\nthe KL distance.\nAlthough the proposed technique has some limitations, in particular, that the model\nto be trained is unambiguous, it is by no means restricted to language models based on\nﬁnite automata or context-free grammars, as several other probabilistic grammatical\nformalisms can be treated in a similar manner.\nThe structure of this article is as follows. We provide some preliminary deﬁnitions\nin Section 2. Section 3 discusses how the expected frequency of a rule in a PCFG can be\ncomputed. This is an auxiliary step in the algorithms to be discussed below. Section 4\ndeﬁnes a way to combine a PFA and a PCFG into a new PCFG that extends a well-known\nrepresentation of the intersection of a regular and a context-free language. Thereby\nwe merge the input model and the model to be trained into a single structure. This\nstructure is the foundation for a number of algorithms, presented in section 5, which\nallow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1),\ntraining of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an\nunambiguous FA on the basis of a PFA (section 5.3).\n2. Preliminaries\nMany of the deﬁnitions on probabilistic context-free grammars are based on Santos\n(1972) and Booth and Thompson (1973), and the deﬁnitions on probabilistic ﬁnite\nautomata are based on Paz (1971) and Starke (1972).\nA context-free grammar G is a 4-tuple (\nΣ, N, S, R), where Σ and N are two ﬁnite\ndisjoint sets of terminals and nonterminals, respectively,S ∈ N is the start symbol,a n d\nR is a ﬁnite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪ N)∗.A\nprobabilistic context-free grammar G is a 5-tuple (Σ, N, S, R, pG ), where Σ, N, S and R\nare as above, and pG is a function from rules in R to probabilities.\nIn what follows, symbol a ranges over the set Σ, symbols w, v range over the\nset Σ∗, symbols A, B range over the set N, symbol X ranges over the set Σ ∪ N,\nsymbols α, β, γ range over the set ( Σ ∪ N)∗, symbol ρ ranges over the set R,a n d\nsymbols d, e range over the set R∗. With slight abuse of notation, we treat a rule\nρ = (A → α) ∈ R as an atomic symbol when it occurs within a string dρe ∈ R∗.\nThe symbol ϵ denotes the empty string. String concatenation is represented by\noperator · or by empty space.\n175\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nComputational Linguistics Volume 31, Number 2\nFor a ﬁxed (P)CFG G, we deﬁne the relation ⇒ on triples consisting of two strings\nα, β ∈ (Σ ∪ N)∗ and a rule ρ ∈ R by α\nρ\n⇒ β, if and only if α is of the form wAδ and β\nis of the form wγδ, for some w ∈ Σ∗ and δ ∈ (Σ ∪ N)∗,a n d ρ = (A → γ). A leftmost\nderivation (in G) is a string d = ρ1 ··· ρm, m ≥ 0, such that α0\nρ1⇒ α1\nρ2⇒ ···\nρm⇒ αm,f o r\nsome α0, ... , αm ∈ (Σ ∪ N)∗; d = ϵ is always a leftmost derivation. In the remainder\nof this article, we let the term derivation refer to leftmost derivation, unless spec-\niﬁed otherwise. If α0\nρ1⇒ ···\nρm⇒ αm for some α0, ... , αm ∈ (Σ ∪ N)∗, then we say that\nd = ρ1 ··· ρm derives αm from α0, and we write α0\nd⇒ αm; ϵ derives any α0 ∈ (Σ ∪ N)∗\nfrom itself. A derivation d such that S d⇒ w, for some w ∈ Σ∗, is called a complete\nderivation. We say that G is unambiguous if for each w ∈ Σ∗, S d⇒ w for at most\none d ∈ R∗.\nLet G be a ﬁxed PCFG ( Σ, N, S, R, pG ). For α, β ∈ (Σ ∪ N)∗ and d = ρ1 ··· ρm ∈ R∗,\nm ≥ 0, we deﬁne pG (α d⇒ β) = ∏m\ni=1 pG (ρi)i f α d⇒ β,a n dpG (α d⇒ β) = 0 otherwise. The\nprobability pG (w) of a string w ∈ Σ∗ is deﬁned to be ∑\nd pG (S d⇒ w).\nPCFG G is said to be proper if ∑\nρ,α pG (A\nρ\n⇒ α) = 1 for all A ∈ N,t h a ti s ,i ft h e\nprobabilities of all rulesρ = (A → α) with left-hand sideA s u mt oo n e .P C F GG is said to\nbe consistent if ∑\nw pG (w) = 1. Consistency implies that the PCFG deﬁnes a probability\ndistribution on the set of terminal strings. There is a practical sufﬁcient condition for\nconsistency that is decidable (Booth and Thompson 1973).\nAP C F Gi ss a i dt ob e reduced if for each nonterminal A, there are d\n1, d2 ∈ R∗,\nw1, w2 ∈ Σ∗,a n d β ∈ (Σ ∪ N)∗ such that pG (S\nd1⇒ w1Aβ) · pG (w1Aβ\nd2⇒ w1w2) > 0. In\nwords, if a PCFG is reduced, then for each nonterminalA, there is at least one derivation\nd1d2 with nonzero probability that derives a string w1w2 from S and that includes\nsome rule with left-hand side A.AP C F G G that is not reduced can be turned into\none that is reduced and that describes the same probability distribution, provided that∑\nw pG (w) > 0. This reduction consists in removing from the grammar any nonterminal\nA for which the above conditions do not hold, together with any rule that contains\nsuch a nonterminal; see Aho and Ullman (1972) for reduction of CFGs, which is very\nsimilar.\nA ﬁnite automaton M is a 5-tuple ( Σ, Q, q0, qf , T), where Σ and Q are two\nﬁnite sets of terminals and states, respectively, q0, qf ∈ Q are the initial and ﬁnal\nstates, respectively, and T is a ﬁnite set of transitions, each of the form r a↦→ s, where\nr ∈ Q −{ qf }, s ∈ Q,a n da ∈ Σ.2 A probabilistic ﬁnite automaton M is a 6-tuple (Σ, Q,\nq0, qf , T, pM), where Σ, Q, q0, qf ,a n dT are as above, andpM is a function from transitions\nin T to probabilities.\nIn what follows, symbols q, r, s range over the set Q, symbol τ ranges over the set T,\nand symbol c ranges over the set T∗.\nFor a ﬁxed (P)FA M, we deﬁne a conﬁguration to be an element of Q × Σ∗, and we\ndeﬁne the relation ⊢ on triples consisting of two conﬁgurations and a transition τ ∈ T\nby ( r, w)\nτ\n⊢ (s, w′)i fa n do n l yi fw is of the form aw′, for some a ∈ Σ,a n d τ = (r a↦→ s).\nA computation (in M) is a string c = τ1 ··· τm, m ≥ 0, such that ( r0, w0)\nτ1\n⊢ (r1, w1)τ2\n⊢ ···\nτm\n⊢ (rm, wm), for some ( r0, w0), ... ,( rm, wm) ∈ Q × Σ∗; c = ϵ is always a compu-\ntation. If (r0, w0)\nτ1\n⊢ ···\nτm\n⊢ (rm, wm) for some (r0, w0), ... ,( rm, wm) ∈ Q × Σ∗ and c = τ1 ···\nτm ∈ T∗, then we write (r0, w0)\nc\n⊢ (rm, wm). We say that c recognizes w if (q0, w)\nc\n⊢ (qf , ϵ).\n2 That we only allow one ﬁnal state is not a serious restriction with regard to the set of strings we can\nprocess; only when the empty string is to be recognized could this lead to difﬁculties. Lifting the\nrestriction would encumber the presentation with treatment of additional cases without affecting,\nhowever, the validity of the main results.\n176\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nNederhof Training Models on Models\nLet M be a ﬁxed FA ( Σ, Q, q0, qf , T). The language L(M) accepted by M is\ndeﬁned to be {w ∈ Σ∗ |∃ c[(q, w)\nc\n⊢ (qf , ϵ)]}.W es a y M is unambiguous if for each\nw ∈ Σ∗,( q0, w)\nc\n⊢ (qf , ϵ) for at most one c ∈ T∗.W es a y M is deterministic if for each\n(r, w) ∈ Q × Σ∗, there is at most one combination of τ ∈ T and ( s, w′) ∈ Q × Σ∗ such\nthat ( r, w)\nτ\n⊢ (s, w′). Turning a given FA into one that is deterministic and accepts the\nsame language is called determinization. All FAs can be determinized. Turning a given\n(deterministic) FA into the smallest (deterministic) FA that accepts the same language\nis called minimization. There are effective algorithms for minimization of deterministic\nFAs.\nLet M be a ﬁxed PFA ( Σ, Q, q0, qf , T, pM). For ( r, w), (s, v) ∈ Q × Σ∗ and\nc = τ1 ··· τm ∈ T∗, we deﬁne pM((r, w)\nc\n⊢ (s, v)) = ∏m\ni=1 pM(τi)i f( r, w)\nc\n⊢ (s, v), and\npM((r, w)\nc\n⊢ (s, v)) = 0 otherwise. The probability pM(w)o fas t r i n gw ∈ Σ∗ is deﬁned\nto be ∑\nc pM((q0, w)\nc\n⊢ (qf , ϵ)).\nPFA M is said to be proper if ∑\nτ,a,s: τ=(r a↦→s)∈T pM(τ) = 1 for all r ∈ Q −{ qf }.\n3. Expected Frequencies of Rules\nLet G be a PCFG ( Σ, N, S, R, pG ). We assume without loss of generality that S does not\noccur in the right-hand side of any rule from R. For each rule ρ, we deﬁne\nE(ρ) =\n∑\nd,d′,w\npG (S\ndρd′\n⇒ w)( 1 )\nIf G is proper and consistent, (1) is the expected frequency ofρ in a complete derivation.\nEach complete derivation dρd′can be written as dρd′′d′′′,w i t hd′= d′′d′′′, where\nS d⇒ w′Aβ, A\nρ\n⇒ α, α d′′\n⇒ w′′, β d′′′\n⇒ w′′′ (2)\nfor some A, α, β, w′, w′′,a n dw′′′. Therefore\nE(ρ) = outer(A) · pG (ρ) · inner(α)( 3 )\nwhere we deﬁne\nouter(A) =\n∑\nd,w′,β,d′′′,w′′′\npG (S d⇒ w′Aβ) · pG (β d′′′\n⇒ w′′′)( 4 )\ninner(α) =\n∑\nd′′,w′′\npG (α d′′\n⇒ w′′)( 5 )\nfor each A ∈ N and α ∈ (Σ ∪ N)∗. From the deﬁnition of inner, we can easily derive the\nfollowing equations:\ninner(a) = 1( 6 )\ninner(A) =\n∑\nρ,α:\nρ=(A→α)\npG (ρ) · inner(α)( 7 )\ninner(Xβ) = inner(X) · inner(β)( 8 )\n177\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nComputational Linguistics Volume 31, Number 2\nThis can be taken as a recursive deﬁnition of inner, assuming β ̸= ϵ in (8). Similarly, we\ncan derive a recursive deﬁnition of outer:\nouter(S) = 1( 9 )\nouter(A) =\n∑\nρ,B,α,β:\nρ=(B→αAβ)\nouter(B) · pG (ρ) · inner(α) · inner(β) (10)\nfor A ̸= S.\nIn general, there may be cyclic dependencies in the equations for inner and outer;\nthat is, for certain nonterminals A, inner(A)a n d outer(A) may be deﬁned in terms\nof themselves. There may even be no closed-form expression for inner(A). However,\none may approximate the solutions to arbitrary precision by means of ﬁxed-point\niteration.\n4. Intersection of Context-Free and Regular Languages\nWe recall a construction from Bar-Hillel, Perles, and Shamir (1964) that computes the\nintersection of a context-free language and a regular language. The input consists of a\nCFG G\n= (Σ, N, S, R)a n da nF AM = (Σ, Q, q0, qf , T); note that we assume, without loss\nof generality, that G and M share the same set of terminals Σ.\nThe output of the construction is CFG G∩ = (Σ, N∩, S∩, R∩), where N∩ = Q ×\n(Σ ∪ N) × Q, S∩ = (q0, S, qf ), and R∩ consists of the set of rules that is obtained as\nfollows:\nr For each rule ρ = (A → X1 ··· Xm) ∈ R, m ≥ 0, and each sequence of states\nr0, ... , rm ∈ Q,l e tt h er u l eρ∩ = ((r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm))\nbe in R∩;f o rm = 0, R∩ contains a rule ρ∩ = ((r0, A, r0) → ϵ) for each\nstate r0.\nr For each transition τ = (r a↦→ s) ∈ T,l e tt h er u l eρ∩ = ((r, a, s) → a)b e\nin R∩.\nNote that for each rule ( r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm)f r o mR∩, there is a\nunique rule A → X1 ··· Xm from R from which it has been constructed by the above.\nSimilarly, each rule (r, a, s) → a uniquely identiﬁes a transition r a↦→ s. This means that if\nwe take a derivation d∩ in G∩, we can extract a sequence h1(d∩) of rules from G and a\nsequence h2(d∩) of transitions fromM, where h1 and h2 are string homomorphisms that\nwe deﬁne pointwise as\nh1(ρ∩) = ρ if ρ∩ = ((r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm))\nand ρ = (A → X1 ··· Xm)\n(11)\nϵ if ρ∩ = ((r, a, s) → a) (12)\nh2(ρ∩) = τ if ρ∩ = ((r, a, s) → a)a n dτ = (r a↦→ s) (13)\nϵ if ρ∩ = ((r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm)) (14)\n178\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nNederhof Training Models on Models\nWe deﬁne h(d∩) = (h1(d∩), h2(d∩)). It can be easily shown that if h(d∩) = (d, c)a n d\nS∩\nd∩⇒ w, then for the same w, we have S d⇒ w and (q0, w)\nc\n⊢ (qf , ϵ). Conversely, if for some\nw, d,a n dc we have S d⇒ w and (q0, w)\nc\n⊢ (qf , ϵ), then there is precisely one derivation d∩\nsuch that h(d∩) = (d, c)a n dS∩\nd∩⇒ w.\nIt was observed by Lang (1994) that G∩ can be seen as a parse forest ,t h a ti s ,a\ncompact representation of all parse trees according to G that derive strings recognized\nby M. The construction can be generalized to, for example, tree-adjoining grammars\n(Vijay-Shanker and Weir 1993) and range concatenation grammars (Boullier 2000;\nBertsch and Nederhof 2001). The construction for the latter also has implications for\nlinear context-free rewriting systems (Seki et al. 1991).\nThe construction has been extended by Nederhof and Satta (2003) to apply to a\nPCFG G = (Σ, N, S, R, pG )a n daP F A M = (Σ, Q, q0, qf , T, pM). The output is a\nPCFG G∩ = (Σ, N∩, S∩, R∩, p∩), where N∩, S∩,a n d R∩ are as before, and p∩ is\ndeﬁned by\np∩((r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm)) = pG (A → X1 ··· Xm) (15)\np∩((r, a, s) → a) = pM(r a↦→ s) (16)\nIf d∩, d,a n dc are such that h(d∩) = (d, c), then clearly p∩(d∩) = pG (d) · pM(c).\n5. Training Models on Models\nWe restrict ourselves to a few cases of the general technique of training a model on the\nbasis of another model.\n5.1 Training a PFA on a PCFG\nLet us assume we have a proper and consistent PCFG G = (Σ, N, S, R, pG )a n da nF A\nM = (Σ, Q, q0, qf , T) that is unambiguous. This FA may have resulted from (nonprob-\nabilistic) approximation of CFG ( Σ, N, S, R), but it may also be totally unrelated to G.\nNote that an FA is guaranteed to be unambiguous if it is deterministic; any FA can be\ndeterminized. Our goal is now to assign probabilities to the transitions from FA M to\nobtain a proper PFA that approximates the probability distribution described by G as\nwell as possible.\nLet us deﬁne 1 as the function that maps each transition from T to one. This means\nthat for each r, w, c and s, 1((r, w)\nc\n⊢ (s, ϵ)) = 1i f( r, w)\nc\n⊢ (s, ϵ), and 1((r, w)\nc\n⊢ (s, ϵ)) = 0\notherwise.\nOf the set of strings generated by G, a subset is recognized by computations of M;\nnote again that there can be at most one such computation for each string. The expected\nfrequency of a transition τ in such computations is given by\nE(τ) =\n∑\nw,c,c′\npG (w) · 1((q0, w)\ncτc′\n⊢ (qf , ϵ)) (17)\nNow we construct the PCFG G∩ as explained in section 4 from the PCFG G and the\nPFA (Σ, Q, q0, qf , T, 1). Let τ = (r a↦→ s) ∈ T and ρ = ((r, a, s) → a). On the basis of the\n179\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nComputational Linguistics Volume 31, Number 2\nproperties of function h, we can now rewrite E(τ)a s\nE(τ) =\n∑\nd,w,c,c′\npG (S d⇒ w) · 1((q0, w)\ncτc′\n⊢ (qf , ϵ))\n=\n∑\ne,d,w,c,c′:\nh(e)=(d,cτc′)\npG (S d⇒ w) · 1((q0, w)\ncτc′\n⊢ (qf , ϵ))\n=\n∑\ne,e′,w\np∩(S∩\neρe′\n⇒ w)\n= E(ρ) (18)\nHereby we have expressed the expected frequency of a transition τ = (r a↦→ s)i n\nterms of the expected frequency of rule ρ = ((r, a, s) → a) in derivations in PCFG G∩.\nIt was explained in section 3 how such a value can be computed. Note that since\nby deﬁnition 1(τ) = 1, also p∩(ρ) = 1. Furthermore, for the right-hand side a of ρ,\ninner(a) = 1. Therefore,\nE(τ) = outer((r, a, s)) · p∩(ρ) · inner(a)\n= outer((r, a, s)) (19)\nTo obtain the required PFA ( Σ, Q, q0, qf , T, pM), we now deﬁne the probability\nfunction pM for each τ = (r a↦→ s) ∈ T as\npM(τ) = outer((r, a, s))∑\na′,s′:(r a′\n↦→s′)∈T\nouter((r, a′, s′)) (20)\nThat such a relative frequency estimator pM minimizes the KL distance between pG and\npM on the domain L(M) is proven in the appendix.\nAn example with ﬁnite languages is given in Figure 1. We have, for example,\npM(q0\na↦→ q1) = outer((q0, a, q1))\nouter((q0, a, q1)) + outer((q0, c, q1))\n=\n1\n3\n1\n3 + 2\n3\n= 1\n3 (21)\n5.2 Training a PCFG on a PFA\nSimilarly to section 5.1, we now assume we have a proper PFA M = (Σ, Q, q0,\nqf , T, pM)a n daC F G G = (Σ, N, S, R) that is unambiguous. Our goal is to ﬁnd a\nfunction pG that lets proper and consistent PCFG ( Σ, N, S, R, pG ) approximate M as\nwell as possible. Although CFGs used for natural language processing are usually\nambiguous, there may be cases in other ﬁelds in which we may assume grammars are\nunambiguous.\n180\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nNederhof Training Models on Models\nFigure 1\nExample of input PCFG G, with rule probabilities between square brackets, input FA M, the\nreduced PCFG G∩, and the resulting trained PFA.\nLet us deﬁne 1 as the function that maps each rule from R to one. Of the set of\nstrings recognized by M, a subset can be derived in G. The expected frequency of a rule\nρ in those derivations is given by\nE(ρ) =\n∑\nd,d′,w\npM(w) · 1(S\ndρd′\n⇒ w) (22)\nNow we construct the PCFG G∩ from the PCFG G = (Σ, N, S, R, 1)a n dt h e\nPFA M as explained in section 4. Analogously to section 5.1, we obtain for each\nρ = (A → X1 ··· Xm)\nE(ρ) =\n∑\nr0,r1,...,rm\nE((r0, A, rm) → (r0, X1, r1) ··· (rm−1, Xm, rm))\n=\n∑\nr0,r1,...,rm\nouter((r0, A, rm)) · inner((r0, X1, r1) ··· (rm−1, Xm, rm)) (23)\nTo obtain the required PCFG (Σ, N, S, R, pG ), we now deﬁne the probability function\npG for each ρ = (A → α)a s\npG (ρ) = E(ρ)∑\nρ′=(A→α′)∈R E(ρ′) (24)\nThe proof that this relative frequency estimator pG minimizes the KL distance between\npM and pG on the domain L(G) is almost identical to the proof in the appendix for a\nsimilar claim from section 5.1.\n5.3 Training a PFA on a PFA\nWe now assume we have a proper PFA M1 = (Σ, Q1, q0,1, qf,1, T1, p1)a n da nF A\nM2 = (Σ, Q2, q0,2, qf,2, T2) that is unambiguous. Our goal is to ﬁnd a function p2 so that\n181\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nComputational Linguistics Volume 31, Number 2\nproper PFA (Σ, Q2, q0,2, qf,2, T2, p2) approximates M1 as well as possible, minimizing\nthe KL distance between p1 and p2 on the domain L(M2).\nOne way to solve this problem is to mapM2 to an equivalent right-linear CFGG and\nthen to apply the algorithm from section 5.2. The obtained probability function pG can\nbe translated back to an appropriate function p2. For this special case, the construction\nfrom section 4 can be simpliﬁed to the “cross-product” construction of ﬁnite automata\n(see, e.g., Aho and Ullman 1972). The simpliﬁed forms of the functions inner and outer\nfrom section 3 are commonly called forward and backward, respectively, and they are\ndeﬁned by systems of linear equations. As a result, we can compute exact solutions, as\nopposed to approximate solutions by iteration.\nAppendix\nWe now prove that the choice ofp\nM in section 5.1 is such that it minimizes the Kullback-\nLeibler distance between pG and pM, restricted to the domain L(M). Without this\nrestriction, the KL distance is given by\nD(pG∥pM) =\n∑\nw\npG (w) · log pG (w)\npM(w) (25)\nThis can be used for many applications mentioned in section 1. For example, an FA M\napproximating a CFG G is guaranteed to be such that L(M) ⊇ L(G) in the case of most\npractical approximation algorithms. However, if there are stringsw such that w /∈ L(M)\nand pG (w) > 0, then (25) is inﬁnite, regardless of the choice of pM. We therefore restrict\npG to the domain L(M) and normalize it to obtain\npG|M(w) = pG (w)\nZ ,i f w ∈ L(M) (26)\n0, otherwise (27)\nwhere Z = ∑\nw:w∈L(M) pG (w). Note that pG|M = pG if L(M) ⊇ L(G). Our goal is now to\nshow that our choice of pM minimizes\nD(pG|M∥pM) =\n∑\nw:w∈L(M)\npG|M(w) · log pG|M(w)\npM(w)\n= log 1\nZ + 1\nZ\n∑\nw:w∈L(M)\npG (w) · log pG (w)\npM(w) (28)\nAs Z is independent of pM, it is sufﬁcient to show that our choice of pM minimizes\n∑\nw:w∈L(M)\npG (w) · log pG (w)\npM(w) (29)\nNow consider the expression\n∏\nτ\npM(τ)E(τ) (30)\n182\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nNederhof Training Models on Models\nBy the usual proof technique with Lagrange multipliers, it is easy to show that our\nchoice of pM in section 5.1, given by\npM(τ) = E(τ)∑\nτ′,a′,s′:τ′=(r a′\n↦→s′)∈T\nE(τ′) (31)\nfor each τ = (r a↦→ s) ∈ T, is such that it maximizes (30), under the constraint of\nproperness.\nFor τ ∈ T and w ∈ Σ∗, we deﬁne #τ(w)t ob ez e r o ,i fw /∈ L(M), and otherwise to be\nthe number of occurrences ofτ in the (unique) computation that recognizesw. Formally,\n#τ(w) = ∑\nc,c′1((q0, w)\ncτc′\n⊢ (qf , ϵ)). We rewrite (30) as\n∏\nτ\npM(τ)E(τ) =\n∏\nτ\npM(τ)\n∑\nw pG (w)·#τ(w)\n=\n∏\nw\n∏\nτ\npM(τ)pG (w)·#τ(w)\n=\n∏\nw\n(∏\nτ\npM(τ)#τ(w)\n)pG (w)\n=\n∏\nw:pM(w)>0\npM(w)pG (w)\n=\n∏\nw:pM(w)>0\n2pG (w)·log pM(w)\n=\n∏\nw:pM(w)>0\n2pG (w)·log pM(w)−pG (w)·log pG (w)+pG (w)·log pG (w)\n=\n∏\nw:pM(w)>0\n2−pG (w)·log pG (w)\npM(w) +pG (w)·log pG (w)\n= 2−∑\nw:pM(w)>0 pG (w)·log pG (w)\npM(w) · 2\n∑\nw:pM(w)>0 pG (w)·log pG (w) (32)\nWe have already seen that the choice of pM that maximizes (30) is given by (31), and\n(31) implies pM(w) > 0 for all w such that w ∈ L(M)a n dpG (w) > 0. Since pM(w) > 0i s\nimpossible for w /∈ L(M), the value of\n2\n∑\nw:pM(w)>0 pG (w)·log pG (w) (33)\nis determined solely by pG and by the condition that pM(w) > 0 for all w such that\nw ∈ L(M)a n d pG (w) > 0. This implies that (30) is maximized by choosing pM such\nthat\n2−∑\nw:pM(w)>0 pG (w)·log pG (w)\npM(w) (34)\n183\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nComputational Linguistics Volume 31, Number 2\nis maximized, or alternatively that\n∑\nw:pM(w)>0\npG (w) · log pG (w)\npM(w) (35)\nis minimized, under the constraint that pM(w) > 0 for all w such that w ∈ L(M)a n d\npG (w) > 0. For this choice of pM, (29) equals (35).\nConversely, if a choice of pM minimizes (29), we may assume that pM(w) > 0f o r\nall w such that w ∈ L(M)a n dpG (w) > 0, since otherwise (29) is inﬁnite. Again, for this\nchoice of pM, (29) equals (35). It follows that the choice ofpM that minimizes (29) concurs\nwith the choice of pM that maximizes (30), which concludes our proof.\nAcknowledgments\nComments by Khalil Sima’an, Giorgio Satta,\nYuval Krymolowski, and anonymous\nreviewers are gratefully acknowledged. The\nauthor is supported by the PIONIER Project\nAlgorithms for Linguistic Processing, funded\nby NWO (Dutch Organization for Scientiﬁc\nResearch).\nReferences\nAho, Alfred V . and Jeffrey D. Ullman. 1972.\nParsing, volume 1 of The Theory of Parsing,\nTranslation and Compiling. Prentice Hall,\nEnglewood Cliffs, NJ.\nBar-Hillel, Yehoshua, M. Perles, and\nE. Shamir. 1964. On formal properties of\nsimple phrase structure grammars. In\nYehoshua Bar-Hillel, editor, Language and\nInformation: Selected Essays on Their Theory\nand Application. Addison-Wesley, Reading,\nMA, pages 116–150.\nBertsch, Eberhard and Mark-Jan Nederhof.\n2001. On the complexity of some\nextensions of RCG parsing. In Proceedings\nof the Seventh International Workshop on\nParsing Technologies, pages 66–77, Beijing,\nOctober.\nBooth, Taylor L. and Richard A. Thompson.\n1973. Applying probabilistic measures to\nabstract languages. IEEE Transactions on\nComputers, C-22(5):442–450.\nBoullier, Pierre. 2000. Range concatenation\ngrammars. In Proceedings of the Sixth\nInternational Workshop on Parsing\nTechnologies, pages 53–64, Trento, Italy,\nFebruary.\nJurafsky, Daniel, Chuck Wooters, Gary\nTajchman, Jonathan Segal, Andreas\nStolcke, Eric Fosler, and Nelson Morgan.\n1994. The Berkeley Restaurant Project. In\nProceedings of the International Conference on\nSpoken Language Processing (ICSLP-94),\npages 2139–2142, Yokohama, Japan.\nLang, Bernard. 1994. Recognition can be\nharder than parsing. Computational\nIntelligence, 10(4):486–494.\nManning, Christopher D. and Hinrich\nSch ¨utze. 1999. Foundations of Statistical\nNatural Language Processing. MIT Press,\nCambridge, MA.\nMohri, Mehryar. 1997. Finite-state\ntransducers in language and speech\nprocessing. Computational Linguistics,\n23(2):269–311.\nMohri, Mehryar and Mark-Jan Nederhof.\n2001. Regular approximation of\ncontext-free grammars through\ntransformation. In J.-C. Junqua and G. van\nNoord, editors, Robustness in Language and\nSpeech Technology. Kluwer Academic,\npages 153–163.\nNederhof, Mark-Jan. 2000. Practical\nexperiments with regular approximation\nof context-free languages. Computational\nLinguistics, 26(1):17–44.\nNederhof, Mark-Jan and Giorgio Satta. 2003.\nProbabilistic parsing as intersection. In\nProceedings of the Eighth International\nWorkshop on Parsing Technologies,p a g e s\n137–148, Laboratoire Lorrain de recherche\nen informatique et ses applications\n(LORIA), Nancy, France, April.\nPaz, Azaria. 1971. Introduction to Probabilistic\nAutomata. Academic Press, New York.\nRimon, Mori and J. Herz. 1991. The\nrecognition capacity of local syntactic\nconstraints. In Proceedings of the Fifth\nConference of the European Chapter of the\nACL, pages 155–160, Berlin, April.\nSantos, Eugene S. 1972. Probabilistic\ngrammars and automata. Information and\nControl, 21:27–47.\nSeki, Hiroyuki, Takashi Matsumura,\nMamoru Fujii, and Tadao Kasami.\n1991. On multiple context-free grammars.\nTheoretical Computer Science,\n88:191–229.\n184\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025\nNederhof Training Models on Models\nStarke, Peter H. 1972. Abstract Automata.\nNorth-Holland, Amsterdam.\nStolcke, Andreas and Jonathan Segal. 1994.\nPrecise N-gram probabilities from\nstochastic context-free grammars. In\nProceedings of the 32nd Annual Meeting\nof the ACL, pages 74–79, Las Cruces,\nNM, June.\nVijay-Shanker, K. and David J. Weir.\n1993. The use of shared forests in\ntree adjoining grammar parsing. In\nProceedings of the Sixth Conference of the\nEuropean Chapter of the ACL, pages 384–393,\nUtrecht, The Netherlands, April.\nZue, Victor, James Glass, David Goodine,\nHong Leung, Michael Phillips, Joseph\nPolifroni, and Stephanie Seneff. 1991.\nIntegration of speech recognition and\nnatural language processing in the MIT\nVoyager system. In Proceedings of the\nICASSP-91, Toronto, volume 1, pages\n713–716.\n185\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/0891201054223986 by guest on 05 November 2025"
}