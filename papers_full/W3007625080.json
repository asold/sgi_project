{
    "title": "A Simplified Fully Quantized Transformer for End-to-end Speech Recognition",
    "url": "https://openalex.org/W3007625080",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5004876675",
            "name": "Alex Bie",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5033303445",
            "name": "Bharat Venkitesh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5024708666",
            "name": "João Monteiro",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5003389639",
            "name": "Md. Akmal Haidar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5028862918",
            "name": "Mehdi Rezagholizadeh",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2947946877",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W2941814890",
        "https://openalex.org/W2787752464",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3098873988",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2193413348",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W2981910001",
        "https://openalex.org/W2901837390",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2114766824",
        "https://openalex.org/W2963122961",
        "https://openalex.org/W2102113734",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W1828163288",
        "https://openalex.org/W6908809",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2963736842",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2964299589",
        "https://openalex.org/W2938974662",
        "https://openalex.org/W1494198834"
    ],
    "abstract": "While significant improvements have been made in recent years in terms of end-to-end automatic speech recognition (ASR) performance, such improvements were obtained through the use of very large neural networks, unfit for embedded use on edge devices. That being said, in this paper, we work on simplifying and compressing Transformer-based encoder-decoder architectures for the end-to-end ASR task. We empirically introduce a more compact Speech-Transformer by investigating the impact of discarding particular modules on the performance of the model. Moreover, we evaluate reducing the numerical precision of our network's weights and activations while maintaining the performance of the full-precision model. Our experiments show that we can reduce the number of parameters of the full-precision model and then further compress the model 4x by fully quantizing to 8-bit fixed point precision.",
    "full_text": "A SIMPLIFIED FULLY QUANTIZED TRANSFORMER FOR END-TO-END SPEECH\nRECOGNITION\nAlex Bie∗ , Bharat Venkitesh, Joao Monteiro, Md Akmal Haidar, Mehdi Rezagholizadeh\nHuawei Noah’s Ark Lab, Montreal Research Centre, Canada\nalexbie98@gmail.com, {bharat.venkitesh, joao.monteiro, md.akmal.haidar, mehdi.rezagholizadeh}@huawei.com\nABSTRACT\nWhile signiﬁcant improvements have been made in recent years\nin terms of end-to-end automatic speech recognition (ASR) perfor-\nmance, such improvements were obtained through the use of very\nlarge neural networks, unﬁt for embedded use on edge devices. That\nbeing said, in this paper, we work on simplifying and compressing\nTransformer-based encoder-decoder architectures for the end-to-\nend ASR task. We empirically introduce a more compact Speech-\nTransformer by investigating the impact of discarding particular\nmodules on the performance of the model. Moreover, we evaluate\nreducing the numerical precision of our network’s weights and activa-\ntions while maintaining the performance of the full-precision model.\nOur experiments show that we can reduce the number of parameters\nof the full-precision model and then further compress the model 4x\nby fully quantizing to 8-bit ﬁxed point precision.\nIndex Terms— automatic speech recognition, sequence-to-\nsequence, quantization, compression, Transformer\n1. INTRODUCTION\nEnd-to-end automatic speech recognition (ASR) systems combine\nthe functionality of acoustic, pronunciation, and language modelling\ncomponents into a single neural network. Early approaches to end-to-\nend ASR employ CTC [1, 2]; however these models require rescoring\nwith an external language model (LM) to obtain good performance\n[3]. RNN encoder-decoder [4, 5] equipped with attention [6], origi-\nnally proposed for machine translation, is an effective approach for\nend-to-end ASR [3, 7]. These systems see less of a performance drop\nin the no-LM setting [3].\nMore recently, the Transformer [8] encoder-decoder architecture\nhas been applied to ASR [ 9, 10, 11]. Transformer training is paral-\nlelizable across time, leading to faster training times than recurrent\nmodels [8]. This makes them especially amenable to the large audio\ncorpora encountered in speech recognition. Furthermore, Transform-\ners are powerful autoregressive models [12, 13], and have achieved\nreasonable ASR results without incurring the storage and computa-\ntional overhead associated with using LM’s during inference [10].\nAlthough current end-to-end technology has seen signiﬁcant im-\nprovements in accuracy, computational requirements in terms of both\ntime and space for performing inference with such models remains\nprohibitive for edge devices. Thus, there has been increased inter-\nest in reducing model sizes to enable on-device computation. The\nmodel compression literature explores many techniques to tackle the\nproblem, including: quantization [14], pruning [15, 16], and knowl-\nedge distillation [17, 18]. An all-neural, end-to-end solution based on\nRNN-T [ 19] is presented in [20]. The authors make several runtime\n∗Work performed during an internship at Huawei Noah’s Ark Lab.\nFig. 1: Components of end-to-end Transformer ASR as described\nin [ 10]. 2D convolutional blocks are used for feature extraction and\ndown-sampling. 1D causal convolutions are applied on the decoder\nside. Our proposed simpliﬁed Transformer only uses the green and\nred blocks (no decoder causal convolutions or sinusoidal positional\nencodings); as such, the decoder receives no explicit positional infor-\nmation.\noptimizations to inference and perform post-training quantization,\nallowing the model to be successfully deployed to edge devices.\nIn this contribution, we turn our focus to reﬁning the Transformer\narchitecture so as to enable its use on edge devices. The absence of\nrecurrent connections in Transformers provides a signiﬁcant advan-\ntage in terms of speeding up computation, and therefore, quantizing a\nTransformer-based ASR system would be an important step towards\non-device ASR. We report ﬁndings on direct improvements to the\nmodel through removing components which do not signiﬁcantly af-\nfect performance, and ﬁnally reduce the numerical precision of model\nweights and activations. Speciﬁcally, we reduced the dimensionality\nof the inner representations throughout the model, removed convo-\nlutional layers employed prior to the decoder’s layers (as in Fig. 1),\nand ﬁnally performed 8-bit quantization to the model’s weights and\nactivations (following [ 21] which introduces a fully quantized trans-\nformer for machine translation and language modeling). As veriﬁed\nin terms of recognition performance, our results on the Librispeech\ndataset [22] support the claim that one can recover the original per-\nformance even after greatly reducing model’s computational require-\nments.\nThe remainder of this work is organized as follows: section 2\narXiv:1911.03604v4  [cs.CL]  24 Mar 2020\ngives an overview of Transformer-based ASR, and section 3 describes\nthe details of the quantization scheme. Section 4 describes our exper-\niments with the Librispeech dataset. Section 5 is a discussion of our\nresults. Connection to prior work is presented in section 6. Finally,\nwe draw conclusions and describe future directions in section 7.\n2. TRANSFORMER NETWORKS FOR ASR\nCasting ASR as a sequence-to-sequence task, the Transformer en-\ncoder takes as input a sequence of frame-level acoustic features\n(x1, ..., xT ), and maps it to a sequence of high-level representa-\ntions (h1, ..., hN ). The decoder generates a transcription (y1, ..., yL)\none token at a time. Each choice of output token yl is conditioned\non the hidden states (h1, ..., hN ) and previously generated tokens\n(y1, ..., yl−1) through attention mechanisms. The typical choice for\nacoustic features are frame-level log-Mel ﬁlterbank coefﬁcents. The\ntarget transcripts are represented by word-level tokens or sub-word\nunits such as characters or produced through byte pair encoding [23].\n2.1. Transformer architecture\nThe encoder and decoder of the Transformer are stacks of N Trans-\nformer layers. The layers of the encoder iteratively reﬁne the rep-\nresentation of the input sequence with a combination of multi-head\nself-attention and frame-level afﬁne transformations. Speciﬁcally, the\ninputs to each layer are projected into keys K, queries Q, and values\nV . Scaled dot product attention is then used to compute a weighted\nsum of values for each query vector:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (1)\nwhere dk is the dimension of the keys. We obtain multi-head attention\nby performing this computation h times independently with different\nsets of projections, and concatenating:\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO (2)\nheadi = Attention(QWQ\ni , KWK\ni , V WV\ni ) (3)\nThe W∗\ni are learned linear transformations W∗\ni : dmodel →d∗, and\nWO : h ·dv →dmodel. We use d∗ = dmodel/h. The self-attention\noperation allows frames to gather context from all timesteps and\nbuild an informative sequence of high-level features. The outputs of\nmulti-head attention go through a 2-layer position-wise feed-forward\nnetwork with hidden size dff .\nFFN(x) = W2 ReLu(W1x + b1) + b2 (4)\nOn the decoder side, each layer performs two rounds of multi-\nhead attention: the ﬁrst one being self-attention over the representa-\ntions of previously emitted tokens ( Q = K = V ), and the second\nbeing attention over the output of the ﬁnal layer of the encoder ( Q\nare previous layer outputs, K = V are (h1, ..., hN )). The output of\nthe ﬁnal decoder layer for token yl−1 is used to predict the following\ntoken yl. Other components of the architecture such as sinusoidal\npositional encodings, residual connections and layer normalization\nare described in [8].\n2.2. Convolutional layers\nFollowing previous work [ 9, 10, 24], we apply frequency-time 2-\ndimensional convolution blocks followed by max pooling to our au-\ndio features, prior to feeding them into the encoder, as seen in Fig.\n1. We can achieve signiﬁcant savings in computation given that the\nresulting length of the input is considerably reduced and the com-\nputation required for self-attention layers scales quadratically with\nrespect to the sequence length.\nMoreover, it has been shown that temporal convolutions are ef-\nfective in modeling time dependencies [ 25], and serves to encode\nordering into learned high level representations of the input signal.\nBased on these observations, [10] proposes to replace sinusoidal po-\nsitional encodings in the Transformer with convolutions, employing\n2D convolutions over spectrogram inputs and 1D causal convolutions\nover word embeddings in the decoder (pictured in Fig. 1).\n3. MODEL COMPRESSION\nA simple approach to reducing computational requirements is to re-\nduce the precision requirements for weights and activations in the\nmodel. It is shown in [ 26] that stochastic uniform quantization is\nan unbiased estimator of its input and quantizing the weights of\na network is equivalent to adding Gaussian noise over parameters,\nwhich can induce a regularization effect and help avoid overﬁtting.\nQuantization has several advantages: 1) Computation is performed\nin ﬁxed-point precision, which can be done more efﬁciently on hard-\nware. 2) With 8-bit quantization, the model can be compressed up\nto 4 times of its original size. 3) In several architectures, memory\naccess dominates power consumption, and moving 8-bit data is four\ntimes more efﬁcient when compared to 32-bit ﬂoating point data. All\nthree factors contribute to faster inference, with 2-3x times speed\nup [14] and further improvements are possible with optimized low\nprecision vector arithmetic. In this section, we summarize the quan-\ntization approach given in [21], which is deployed to fully quantize\nour simpliﬁed Transformer-based ASR.\n3.1. Quantization scheme\nWe use a uniform quantization function Q : [ a, b] ⊆ ℜ →\n[−2K−1, 2K−1 −1] ⊆ Z which maps real values (weights and\nactivations) in the range of [a, b] to K-bit signed integers:\nQ(x) = round(x −a\n∆ ) (5)\nwith ∆ = b−a\n2K−1 . In the case that x is not in the range of [a, b], we\nﬁrst apply the clamp operator:\nclamp(x; a, b) = min(max(x, a), b) (6)\nThe de-quantization function D(.) is given by:\nD(xQ) = xQ ×∆ + a (7)\nwhere xQ = Q(x) refers to the quantized integer value correspond-\ning to the real value x. During training, forward propagation sim-\nulates the effects of quantized inference by incorporating the de-\nquantized values of both weights and activations in the forward pass\nﬂoating-point arithmetic operations. We then apply the quantization\noperation and the de-quantization operation according to eq. 5 and\neq. 7 respectively to each layer. The clamping ranges are computed\ndifferently for weights and activations. For a weight matrix X, we\nset a and b to be Xmin and Xmax respectively. For activations, the\nclamping range depends on the x, the input to the layer. We calculate\n[a, b] by keeping track of xmin and xmax for each mini-batch during\ntraining, and aggregating them using an exponential moving average\nwith smoothing parameter set to 0.9 [21]. Quantization of activations\nstarts after a ﬁxed number of steps (3000). This ensures that the net-\nwork has reached a more stable stage and the estimated ranges do not\nexclude a signiﬁcant fraction of values. We quantize to K = 8-bit\nprecision in our experiments.\nTable 1: WER (%) results of different hyperparameter conﬁgurations\nfor Conv-Context. The ﬁrst 2 rows are taken directly from [10].\nModel dmodel\nLayers Params dev test\nEnc Dec clean other clean other\nConv-\nContext\n1024 16 6 315M 4.8 12.7 4.7 12.9\n6 6 138M 5.6 14.5 5.7 15.3\n512 6 6 52M 5.3 14.9 5.7 14.8\n3.2. Quantization choices\nThe inputs and weights of all matrix multiplications are quantized\nwhile the addition operations are ignored since they do not lead to\ncomputational gains at inference time. In the multi-head attention\nmodule, we quantize attention weights, softmax layer and scaled dot\nproduct and extend the same to all the layer norms in the model. For\nthe position-wise feed forward network and convolution layers, we\nquantize the weights and activations.\n4. EXPERIMENTS\nWe use the open-source, sequence modelling toolkitfairseq [27]. We\nconduct our experiments on LibriSpeech 960h [ 22], and follow the\nsame setup as [ 10]: the input features are 80-dimensional log-Mel\nﬁlterbanks extracted from 25ms windows every 10ms, and the output\ntokens come from a 5K subword vocabulary created with sentence-\npiece [28] “unigram”. For fair comparison, we also optimize with\nAdaDelta [29] with learning rate=1.0 and gradient clipping at 10.0,\nand run for 80 epochs, averaging checkpoints saved over the last 30\nepochs. The dropout rate was set to 0.15.\n4.1. Comparison of Transformer variants\nWe perform preliminary experiments comparing full-precision Trans-\nformer variants and choose one to quantize. We start from Conv-\nContext [10] that proposes to replace sinusoidal positional encodings\nin the encoder and decoder with 2D convolutions (over audio features)\nand 1D convolutions (over previous token embeddings) respectively.\nMotivated by recent results in Transformer-based speech recogni-\ntion [11] and language modelling [ 30], we allocate our parameter\nbudget towards depth over width, and retrain their model under the\nconﬁguration of Transformer Base [ 8], namely: 6 encoder/decoder\nlayers, dmodel = 512, 8 heads, and dff = 2048. We obtain a satis-\nfactory trade-off between model size and performance (Table 1), and\nadopt this conﬁguration for the remainder of this work.\nNext, we propose removing the 1D convolutional layers on the\ndecoder side, based on previous work [ 30] demonstrating that the\nautoregressive Transformer training setup provides enough of a po-\nsitional signal for Transformer decoders to reconstruct order in the\ndeeper layers. We observe that removing these layers do not affect\nour performance, and reduce our parameter count from 52M to 51M.\nFinally, we add positional encodings on top of this conﬁguration and\nsee, counter-intuitively, that our performance degrades. These results\nare pictured in Table 2.\n4.2. Quantization\nFor quantization, we restrict our attention to our proposed simpliﬁed\nTransformer (no decoder-side convolutions or positional encodings),\nsince it performs well and is the least complex of the Transformer\nvariants. We compare the results of quantization-aware training to\nthe full-precision model, as well as to the results of post-training\nTable 2: Comparison of 3 full-precision model variants.\nModel 1D\nConv\nPos.\nenc. Params dev test\nclean other clean other\nConv-Context \u0013 \u0017 52M 5.3 14.9 5.7 14.8\nProposed \u0017 \u0017 51M 5.6 14.2 5.5 14.8\n+ Pos. enc. \u0017 \u0013 6.0 14.6 6.0 14.5\nquantization. In post-training quantization, we start from the aver-\naged full-precision model, keep the weights ﬁxed, and compute the\nclamping range [a, b] for our activations over 1k training steps. To\nreport the checkpoint-averaged result of quantization-aware training,\nwe average the weights of quantization-aware training checkpoints,\ninitialize our activation ranges [a, b] with checkpoint averages, and\nadjust them over 1k training steps. In both cases, no additional up-\ndates are made to the weights.\nOur results are summarised in Table 3. Our quantized models\nperform comparably to the full-precision model, and represent rea-\nsonable trade-offs in accuracy for model size and inference time. The\nlast row of the table represents a result of 10x compression over the\n138M parameter baseline with no loss in performance. Quantization-\naware training scheme did not result in signiﬁcant gains over post-\nquantization.\nTable 3: Quantization results of our proposed model(no positional\nencodings or decoder-side convolutions).\nModel Fully\nquantized\ndev test\nclean other clean other\nFull-precision \u0017 5.6 14.2 5.5 14.8\nPost-training quant \u0013 5.6 14.6 5.6 15.1\nQuant-aware training \u0013 5.4 14.5 5.5 15.2\n5. DISCUSSION\n5.1. Representing positional information\nThe 3 Transformer variants explored in this work differ in how they\npresent token-positional information to the decoder. We study their\nbehaviour to get a better understanding as to why our proposed sim-\npliﬁed model performs well.\nWe remark that sinusoidal position encodings hurt performance\nbecause of longer sequences at test time. It has been observed that\ndecoder-side positional encodings do worse than 1D convolutions\n[10] (and also nothing at all, from our results). This performance\ndrop is from under-generation; on dev-clean, our proposed model’s\nWER increases 5.6 →6.0 after adding positional encodings, with\ndeletion rate increasing 0.7 →1.3. Our plot in Fig. 2 shows that this\ncan be attributed to the inability of sinusoidal positional encodings\nto generalize to lengths longer than encountered in the training set.\nExamining the same plot, we notice utterances with large dele-\ntion counts in the outputs of models without sinusoidal positional\nencoding. An example is shown in Fig. 3. Our models without si-\nnusoidal positional encoding exhibit skipping.We hypothesize the\nissue lies in the time-axis translation-invariance of decoder inputs:\nrepeated n-grams confuse the decoder into losing its place in the in-\nput audio. Cross-attention visualizations between inputs to the ﬁnal\ndecoder layer and encoder outputs (left column of Fig. 4) support this\nhypothesis. We remark that being able to handle repetition is crucial\nFig. 2: A plot of reference length vs. deletion for the dev-clean system\noutput of our 3 models. The histograms in orange represent the length\ndistribution of training transcriptions.\nReference This second part is divided into two, for in the ﬁrstI speak of\nher as regards the nobleness of hersoul relating some of her\nvirtues proceeding from her soul. In the secondI speak of her\nas regards the nobleness of herbody narrating some of her\nbeauties here love saith concerning her.\nConvContext Thesecond parthasdivided into two for in the ﬁrstI speak of\nher as regards the nobleness of hersoulrelatingsomeofher\nvirtuesproceedingfromhersoul.InthesecondIspeakofher\nasregardsthenoblenessofherbody narrating some of her\nbeauties here love saith concerning her.\nFig. 3: An example of ”skipping” taken from dev-clean. Punctuation\nis added for readability. In bold are repeated n-grams. The output\nof our proposed model is mostly identical to 1D Conv. The model\nemploying positional encodings makes no errors.\nfor transcribing spontaneous speech. Imposing constraints on atten-\ntion matrices or expanding relative positional information context are\nsome possible approaches for addressing this problem.\nFinally, we afﬁrm the hypothesis proposed in [30] that the Trans-\nformer with no positional encodings reconstructs ordering in deeper\nlayers. The second column of Fig. 4 show visualizations of cross-\nattention as we go up the decoder stack.\n5.2. Training the Transformer\nWe observe no signiﬁcant gain with quantization-aware training. Fur-\nthermore, it increases training time by more than 4x due to its expan-\nsion of our computational graph. We note that in post-quantization,\nthe 1k steps used to ﬁne-tune activation clamping ranges is very\nimportant. Without this step, system output is degenerate. In our\nexperiments, we found that training with large batch sizes (80k au-\ndio frames) was necessary for convergence. Similar optimization be-\nhaviour was observed across all experiments: a plateau at ∼25%\nframe-level accuracy followed by a jump to 80% within 2 or 3 epochs.\nThis jump was not observed when training with smaller batch sizes.\n6. RELATION TO PRIOR WORK\nTransformers for speech recognition.Several studies have focused\non adapting Transformer networks for end-to-end speech recognition.\nIn particular, [9, 10] present models augmenting Transformers with\nconvolutions. [11] focuses on reﬁning the training process, and show\nthat Transformer-based end-to-end ASR is highly competitive with\nstate-of-the-art methods over 15 datasets. These studies focus only\non performance, and do not consider trade-offs required for edge de-\nployment.\nCompression with knowledge distillation.[17] proposes a knowl-\nedge distillation strategy applied to Transformer ASR to recover the\nperformance of a larger model with fewer parameters. Distilled mod-\nels still work in 32-bit ﬂoating point, and do not take advantage of\nfaster, more energy-efﬁcient hardware available when working with\n8-bit ﬁxed-point. Additionally, we believe this work is orthogonal to\nFig. 4: Decoder-encoder attention matrices for the utterance in Fig. 3.\nOn the left column, we see the models without positional encoding\nsometimes exhibit bi-modality in attention distributions over the in-\nput audio. The transcription for the repeated section attends to both\npositions in the input audio. When decoding, the shorter path that\nskips the segment between the repetition has higher likelihood.\nours, and the two methods can be combined for further improvement.\nTransformer quantization. Quantization strategies for the trans-\nformer have been proposed in the context of machine translation [31,\n21] and BERT [32]. [ 21] quantizes both weights and activations of\nthe transformer for machine translation and language modeling tasks.\nNecessity of positional encodings.For language modelling, [ 30]\nachieve better perplexity scores without positional encodings, and\nargue that the autoregressive setup used to train the Transformer de-\ncoder provides a sufﬁcient positional signal.\n7. CONCLUSION\nIn this paper, we proposed a compact Transformer-based end-to-end\nASR system, fully quantized to enable edge deployment. The pro-\nposed compact version has a smaller hidden size and no decoder side\nconvolutions or positional encodings. We then fully quantize it to\n8-bit ﬁxed point. Compared to the 138M baseline we started from,\nwe achieve more than 10x compression with no loss in performance.\nThe ﬁnal model also takes advantage of efﬁcient hardware to enable\nfast inference. Our training strategy and model conﬁgurations are\nnot highly tuned. Future work includes exploring additional training\nstrategies and incorporating text data, as to bring highly performant,\nsingle-pass, end-to-end ASR to edge devices.\n8. ACKNOWLEDGEMENTS\nWe would like to thank our colleagues Ella Charlaix, Eyy ¨ub Sari,\nand Gabriele Prato for their valuable insights and suggestions for\nquantization experiments.\n9. REFERENCES\n[1] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen\nSchmidhuber, “Connectionist temporal classiﬁcation: labelling\nunsegmented sequence data with recurrent neural networks,” in\nProceedings of the 23rd ICML. ACM, 2006, pp. 369–376.\n[2] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech\nrecognition with recurrent neural networks,” in ICML, 2014,\npp. 1764–1772.\n[3] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon\nBrakel, and Yoshua Bengio, “End-to-end attention-based large\nvocabulary speech recognition,” in 2016 IEEE ICASSP. IEEE,\n2016, pp. 4945–4949.\n[4] Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio, “Learning phrase representations using RNN\nencoder–decoder for statistical machine translation,” in Pro-\nceedings of the 2014 EMNLP Conference), Doha, Qatar, Oct.\n2014, pp. 1724–1734, ACL.\n[5] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to\nsequence learning with neural networks,” inAdvances in neural\ninformation processing systems, 2014, pp. 3104–3112.\n[6] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine transla-\ntion by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[7] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and\nspell: A neural network for large vocabulary conversational\nspeech recognition,” in 2016 IEEE ICASSP. IEEE, 2016, pp.\n4960–4964.\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems, 2017, pp.\n5998–6008.\n[9] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recognition,”\nin 2018 IEEE ICASSP. IEEE, 2018, pp. 5884–5888.\n[10] A. Mohamed, D. Okhonko, and L. Zettlemoyer, “Trans-\nformers with convolutional context for asr,” arXiv preprint\narXiv:1904.11660, 2019.\n[11] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, et al., “A\ncomparative study on transformer vs rnn in speech applications,”\narXiv preprint arXiv:1909.06317, 2019.\n[12] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised multitask\nlearners,” .\n[13] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,\nA. Ku, and D. Tran, “Image transformer,” in International\nConference on Machine Learning, 2018, pp. 4052–4061.\n[14] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,\nH. Adam, and D. Kalenichenko, “Quantization and training of\nneural networks for efﬁcient integer-arithmetic-only inference,”\nin Proceedings of the IEEE Conference on CVPR, 2018, pp.\n2704–2713.\n[15] Yann LeCun, John S Denker, and Sara A Solla, “Optimal brain\ndamage,” in Advances in neural information processing sys-\ntems, 1990, pp. 598–605.\n[16] Song Han, Huizi Mao, and William J Dally, “Deep compression:\nCompressing deep neural networks with pruning, trained quanti-\nzation and huffman coding,” arXiv preprint arXiv:1510.00149,\n2015.\n[17] H.-G. Kim, H. Na, H. Lee, J. Lee, T. G. Kang, M.-J. Lee, and\nY . S. Choi, “Knowledge distillation using output errors for\nself-attention end-to-end models,” in ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2019, pp. 6181–6185.\n[18] Yoon Kim and Alexander M Rush, “Sequence-level knowledge\ndistillation,” arXiv preprint arXiv:1606.07947, 2016.\n[19] Alex Graves, “Sequence transduction with recurrent neural\nnetworks,” arXiv preprint arXiv:1211.3711, 2012.\n[20] Y . He, T. N. Sainath, R. Prabhavalkar, I. Mcgraw, R. Alvarez,\nD. Zhao, D. Rybach, Y . Kannan, A. Wu, and R et al. Pang,\n“Streaming end-to-end speech recognition for mobile devices.,”\n2018.\n[21] Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh,\n“Fully quantized transformer for improved translation,” arXiv\npreprint arXiv:1910.10485, 2019.\n[22] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.\n[23] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neural\nmachine translation of rare words with subword units,” in Pro-\nceedings of ACL, 2016, pp. 1715–1725.\n[24] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Bat-\ntenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen,\net al., “Deep speech 2: End-to-end speech recognition in en-\nglish and mandarin,” in International conference on machine\nlearning, 2016, pp. 173–182.\n[25] Shaojie Bai, J Zico Kolter, and Vladlen Koltun, “An empirical\nevaluation of generic convolutional and recurrent networks for\nsequence modeling,” arXiv preprint arXiv:1803.01271, 2018.\n[26] A. Polino, R. Pascanu, and D. Alistarh, “Model compression via\ndistillation and quantization,”arXiv preprint arXiv:1802.05668,\n2018.\n[27] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam\nGross, Nathan Ng, David Grangier, and Michael Auli, “fairseq:\nA fast, extensible toolkit for sequence modeling,” in Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics (Demonstra-\ntions), 2019, pp. 48–53.\n[28] T. Kudo and J. Richardson, “Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neu-\nral text processing,” arXiv preprint arXiv:1808.06226, 2018.\n[29] M. D. Zeiler, “Adadelta: an adaptive learning rate method,”\narXiv preprint arXiv:1212.5701, 2012.\n[30] K. Irie, A. Zeyer, R. Schl ¨uter, and H. Ney, “Language model-\ning with deep transformers,” arXiv preprint arXiv:1905.04226,\n2019.\n[31] Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek\nMenon, Sun Choi, Kushal Datta, and Vikram Saletore, “Efﬁ-\ncient 8-bit quantization of transformer neural machine language\ntranslation model,” arXiv preprint arXiv:1906.00532, 2019.\n[32] Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat,\n“Q8bert: Quantized 8bit bert,”arXiv preprint arXiv:1910.06188,\n2019."
}