{
  "title": "SCTNet: Single-Branch CNN with Transformer Semantic Information for Real-Time Segmentation",
  "url": "https://openalex.org/W4393159321",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2976283186",
      "name": "Zhengze Xu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108437239",
      "name": "Dongyue Wu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2162987107",
      "name": "Changqian Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2953550412",
      "name": "Xiangxiang Chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153253016",
      "name": "Nong Sang",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2125091711",
      "name": "Changxin Gao",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2976283186",
      "name": "Zhengze Xu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108437239",
      "name": "Dongyue Wu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2153253016",
      "name": "Nong Sang",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2125091711",
      "name": "Changxin Gao",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6730587030",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3157274912",
    "https://openalex.org/W3169769133",
    "https://openalex.org/W6754852571",
    "https://openalex.org/W3159337199",
    "https://openalex.org/W4296544717",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2902930830",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W6774475981",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W4312688875",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3188026614",
    "https://openalex.org/W4306311936",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W4281665072",
    "https://openalex.org/W3014795891",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W4224116763",
    "https://openalex.org/W2611259176",
    "https://openalex.org/W6730342312",
    "https://openalex.org/W2895340641",
    "https://openalex.org/W6741437728",
    "https://openalex.org/W4312785900",
    "https://openalex.org/W2561196672",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2775208825",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W4318719552",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W4386076267",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4287198652",
    "https://openalex.org/W4302275239",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W4312599212",
    "https://openalex.org/W4293406525",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3110440461",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3196904463",
    "https://openalex.org/W2964217532",
    "https://openalex.org/W4214524539"
  ],
  "abstract": "Recent real-time semantic segmentation methods usually adopt an additional semantic branch to pursue rich long-range context. However, the additional branch incurs undesirable computational overhead and slows inference speed. To eliminate this dilemma, we propose SCTNet, a single branch CNN with transformer semantic information for real-time segmentation. SCTNet enjoys the rich semantic representations of an inference-free semantic branch while retaining the high efficiency of lightweight single branch CNN. SCTNet utilizes a transformer as the training-only semantic branch considering its superb ability to extract long-range context. With the help of the proposed transformer-like CNN block CFBlock and the semantic information alignment module, SCTNet could capture the rich semantic information from the transformer branch in training. During the inference, only the single branch CNN needs to be deployed. We conduct extensive experiments on Cityscapes, ADE20K, and COCO-Stuff-10K, and the results show that our method achieves the new state-of-the-art performance. The code and model is available at https://github.com/xzz777/SCTNet.",
  "full_text": "SCTNet: Single-Branch CNN with Transformer Semantic Information for\nReal-Time Segmentation\nZhengze Xu1*, Dongyue Wu1, Changqian Yu2, Xiangxiang Chu2, Nong Sang1, Changxin Gao1‚Ä†\n1National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence\nand Automation, Huazhong University of Science and Technology\n2Meituan\n{m202273191, dongyue wu nsang, cgao}@hust.edu.cn, y-changqian@outlook.com, cxxgtxy@gmail.com\nAbstract\nRecent real-time semantic segmentation methods usually\nadopt an additional semantic branch to pursue rich long-range\ncontext. However, the additional branch incurs undesirable\ncomputational overhead and slows inference speed. To elimi-\nnate this dilemma, we propose SCTNet, a single branch CNN\nwith transformer semantic information for real-time segmen-\ntation. SCTNet enjoys the rich semantic representations of an\ninference-free semantic branch while retaining the high ef-\nficiency of lightweight single branch CNN. SCTNet utilizes\na transformer as the training-only semantic branch consid-\nering its superb ability to extract long-range context. With\nthe help of the proposed transformer-like CNN block CF-\nBlock and the semantic information alignment module, SCT-\nNet could capture the rich semantic information from the\ntransformer branch in training. During the inference, only the\nsingle branch CNN needs to be deployed. We conduct exten-\nsive experiments on Cityscapes, ADE20K, and COCO-Stuff-\n10K, and the results show that our method achieves the new\nstate-of-the-art performance. The code and model is available\nat https://github.com/xzz777/SCTNet.\nIntroduction\nAs a fundamental task in computer vision, semantic segmen-\ntation aims to assign a semantic class label to each pixel in\nthe input image. It plays a vital role in autonomous driving,\nmedical image processing, mobile applications, and many\nother fields. In order to achieve better segmentation perfor-\nmance, recent semantic segmentation methods pursue abun-\ndant long-range context. Different methods have been pro-\nposed to capture and encode rich contextual information,\nincluding large receptive fields (Chen et al. 2014, 2017,\n2018), multi-scale feature fusion (Ronneberger, Fischer, and\nBrox 2015; Zhao et al. 2017), self-attention mechanism (Fu\net al. 2019; Huang et al. 2019; Yuan et al. 2018; Zhao et al.\n2018b; Dosovitskiy et al. 2020), etc. Among them, the self-\nattention mechanism, as an essential component of trans-\nformers, has been proven to have a remarkable ability to\nmodel long-range context. Although these works improve\nsignificantly, they usually lead to high computational costs.\n*Work strenthened during an internship at Meituan.\n‚Ä†Corresponding author\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nSTDC2-75\nPIDNet-M\nAFFormer-B-100\nAFFormer-B-75\nSFNet-R18\nRTFormer-S\nRTFormer-B\nTopFormer-B-100\nSeaFormer-B-100\nSCTNet-S-75\nSCTNet-B-100\nSCTNet-B-75\nSCTNet-B-50\nBiSeNetV2-L\nSegNext-T-100\nSegNext-T-75\nDDRNet23\nDDRNet23Slim\nPIDNet-S\n75\n76\n77\n78\n79\n80\n81\n20 30 40 50 60 70 80 90 100 110 120 130 140 150\nAccuracy (mIoU%)\nInference Speed (FPS)\nFigure 1: The speed-accuracy performance on Cityscapes\nvalidation set. Our methods are presented in red stars, while\nothers are presented in blue dots. Our SCTNet establishes a\nnew state-of-the-art speed-accuracy trade-off.\nNote that self-attention-based works even have square com-\nputation complexity with respect to image resolution, which\nsignificantly increases latency in processing high-resolution\nimages. These limitations hinder their application in real-\ntime semantic segmentation.\nMany recent real-time works adopt a bilateral architecture\nto extract high-quality semantic information at a fast speed.\nBiSeNet (Yu et al. 2018) proposes a bilateral network to sep-\narate the detailed spatial features and ample contextual infor-\nmation at early stages and process them in parallel, which is\nshown in Figure 2(a). Following BiseNet (Yu et al. 2018),\nBiSeNetV2 (Yu et al. 2021) and STDC (Fan et al. 2021)\nmake further efforts to strengthen the capability to extract\nrich long-range context or reduce the computational costs of\nthe spatial branch. To balance inference speed and accuracy,\nDDRNet (Pan et al. 2022), RTFormer (Wang et al. 2022),\nand SeaFormer (Wan et al. 2023) adopt a feature-sharing ar-\nchitecture that divides spatial and contextual features at the\ndeep stages, as shown in Figure 2(b). However, these meth-\nods introduce dense fusion modules between two branches\nto boost the semantic information of extracted features. In\nconclusion, all these bilateral methods suffer from limited\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6378\nInput1/41/81/161/32FMOutput\n1/81/8FMFM\n(a) (b) (c)\nInput1/41/81/161/32\n1/2\nFM\n1/41/8\nOutput\nInput\nOutput\n1/161/32SIAMSIAM\n1/41/8 1/41/81/161/32\nFigure 2: Real-time semantic segmentation paradigms. (a)\nDecoupled bilateral network divides a semantic branch and\na spatial branch at the early stage. (b) Feature sharing bilat-\neral network separates the two branches at the latter stage\nand adopts dense fusion modules. (c) Our SCTNet applies\na single hierarchy branch with a semantic extraction trans-\nformer, free from the extra branch and costly fusion module\nin inference. FM: Fusion Module, SIAM: Semantic Infor-\nmation Alignment Module. Dashed arrows and boxes denote\ntraining-only.\ninference speed and high computational costs due to the ad-\nditional branch and multiple fusion modules.\nTo eliminate the aforementioned dilemma, we propose\na single-branch CNN with transformer semantic informa-\ntion for real-time segmentation (SCTNet). It can extract se-\nmantic information efficiently without heavy computation\ncaused by the bilateral network. Specifically, SCTNet learns\nlong-range context from a training-only transformer seman-\ntic branch to the CNN branch. To mitigate the semantic gap\nbetween the transformer and CNN, we elaborately design\na transformer-like CNN block called CFBlock and utilize a\nshared decoder head before the alignment. With the aligned\nsemantic information in training, the single-branch CNN\ncan encode the semantic information and spatial details\njointly. Therefore, SCTNet could align the semantic repre-\nsentation from the large effective receptive field of trans-\nformer architecture while maintaining the high efficiency\nof a lightweight single branch CNN architecture in infer-\nence. The overall architecture is illustrated in Figure 2(c).\nExtensive experimental results on three challenging datasets\ndemonstrate that the proposed SCTNet has a better trade-off\nbetween accuracy and speed than previous works. Figure 1\nintuitively shows the comparison between SCTNet and other\nreal-time segmentation methods on the Cityscapes val set.\nThe main contributions of the proposed SCTNet can be\nsummarized as the following three aspects:\n‚Ä¢ We propose a novel single-branch real-time segmentation\nnetwork called SCTNet. By learning to extract rich se-\nmantic information utilizing semantic information align-\nment from the transformer to CNN, SCTNet enjoys high\naccuracy of the transformerwhile maintaining fast in-\nference speed of the lightweight single branch CNN.\n‚Ä¢ To alleviate the semantic gap between CNN features\nand transformer features, we design theCFBlock (Conv-\nFormer Block), which could capture long-range context\nas a transformer block using only convolution opera-\ntions. Moreover, we propose SIAM(Semantic Informa-\ntion Alignment Module) to align the features in a more\neffective way.\n‚Ä¢ Extensive experimental results show that the proposed\nSCTNet outperforms existing state-of-the-art meth-\nods for real-time semantic segmentation on Cityscapes,\nADE20K, and COCO-Stuff-10K. SCTNet provides a\nnew view of boosting the speed and improving the per-\nformance for real-time semantic segmentation.\nRelated Work\nSemantic Segmentation.FCN (Long, Shelhamer, and Dar-\nrell 2015) leads to the tendency to utilize CNN for se-\nmantic segmentation. Following FCN, a series of improved\nCNN-based semantic segmentation methods are proposed.\nDeepLab (Chen et al. 2017) enlarges the receptive field with\ndilated convolution. PSPNet (Zhao et al. 2017), U-Net (Ron-\nneberger, Fischer, and Brox 2015), and RefineNet (Lin et al.\n2017) fuse different level feature representations to capture\nmulti-scale context. Some methods (Fu et al. 2019; Huang\net al. 2019; Yuan et al. 2018; Zhao et al. 2018b)propose\nvarious attention modules to improve segmentation perfor-\nmance. In recent years, transformer has been adopted for\nsemantic segmentation and shows promising performance.\nSETR (Zheng et al. 2021) directly applies the vision trans-\nformer to image segmentation for the first time. PVT (Wang\net al. 2021) introduces the typical hierarchical architecture\nin CNN into the transformer-based semantic segmentation\nmodel. SegFormer (Xie et al. 2021) proposes an efficient\nmulti-scale transformer-based segmentation model.\nReal-time Semantic Segmentation.Early real-time seman-\ntic segmentation methods (Paszke et al. 2016; Wu, Shen,\nand Hengel 2017) usually accelerate inference by compress-\ning channels or fast down-sampling. ICNet (Zhao et al.\n2018a) first introduces a multi-resolution image cascade net-\nwork to accelerate the speed. BiSeNetV1 (Yu et al. 2018)\nand BiSeNetV2 (Yu et al. 2021) adopt two-branch architec-\nture and feature fusion modules to achieve a better trade-\noff between speed and accuracy. STDC (Fan et al. 2021)\nrethinks the two-branch network of BiSeNet, removes the\nspatial branch, and adds a detailed guidance module. DDR-\nNets (Pan et al. 2022) achieves a better trade-off by shar-\ning branches in the early stages. Very recently, some effi-\ncient transformer methods for real-time segmentation have\nbeen proposed, but they still have unresolved problems. Top-\nFormer (Zhang et al. 2022) only uses transformer on 1/64\nscale of the feature maps, leading to low accuracy. RT-\nFormer (Wang et al. 2022) and SeaFormer (Wan et al. 2023)\nneed frequent interaction between the two branches. This ad-\nditional computation slows down the inference speed. In ad-\ndition, there are also some single-branch and multi-branch\nmethods in real-time segmentation.\nAttention mechanism. Attention mechanism has been\nwidely used in computer vision in recent years. Many meth-\nods contribute to attention mechanism with linear com-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6379\nBackbone Feature \nAlignment\nSIAM\nShared Decoder\nHead Alignment\nm\nDecoder\nStem\nConv Block\nConv Block\nConv Block\nTransformer \nBlock\nStem\nTransformer \nBlock\nTransformer \nBlock\nTransformer \nBlock CF Block\nTraining Only\nùêªùêª\n4 √ó ùëäùëä\n4 √ó C\nConv Block\nCF Block\nCF Block\nDecoder\nConv Block\nùêªùêª\n8 √ó ùëäùëä\n8 √ó 2C ùêªùêª\n16 √ó ùëäùëä\n16 √ó 4C ùêªùêª\n32 √ó ùëäùëä\n32 √ó 8C\nùêªùêªùëêùëê √ó ùëäùëäùëêùëê √ó ùê∂ùê∂ùëêùëê\nùêªùêª √ó ùëäùëä √ó ùê∂ùê∂ùë°ùë°\nùêªùêª √ó ùëäùëä √ó ùê∂ùê∂ùë°ùë°\nResize&Projection\n‚Ä¶\nReshape\nBFA\nSemantic Alignment Loss\nProjection\nDecoder\nDecoder\nDecoder\nSemantic Alignment Loss\nSDHA\nFigure 3: The architecture of SCTNet. CFBlock (Conv-Former Block, detailed in Figure 4) takes advantage of the training-only\nTransformer branch (greyed-out in the dashed box) via SIAM (Semantic Information Alignment Module) which is composed\nof BFA (Backbone Feature Alignment) and SDHA (Shared Decoder Head Alignment).\nplexity. Among them, Some classical linear attention like\nSwin (Liu et al. 2021) and MSG (Fang et al. 2022) con-\ntains frequent shift or reshape operations which bring lots of\nlatency. MSCA (Guo et al. 2022b) shows a promising per-\nformance, but the large kernel is not employ-friendly, and\nthe multi-scale design of attention further incurs inference\nspeed. External attention (Guo et al. 2022a) has a very sim-\nple form. It uses external parameters as the key and value and\nimplements the attention mechanism with two linear layers.\nGFA(GPU-Friendly Attention) (Wang et al. 2022) improves\nexternal attention by replacing head split in EA with group\ndouble norm, which is more friendly for GPU devices.\nMethodology\nMotivation\nRemoving the semantic branch of bilateral networks can\nsignificantly speed up the inference. However, this results\nin shallow single-branch networks that lack long-range se-\nmantic information, leading to low accuracy. While using\ndeep encoders and powerful decoders or complex enhance-\nment modules can recover accuracy, it slows down the infer-\nence process. To address this issue, we propose a training-\nonly alignment method that enriches semantic information\nwithout sacrificing inference speed. Specifically, we pro-\nposed SCTNet, a single-branch convolution network with a\ntraining-only semantic extraction transformer, which owns\nhigh accuracy of transformer and fast inference speed of\nCNN. The overview of SCTNet is presented in Figure 3.\nConv-Former Block\nAs different types of networks, the feature representations\nextracted by CNN and transformer significantly differ. Di-\nrectly aligning the features between the CNN and the trans-\nformer makes the learning process difficult, resulting in lim-\nited performance improvement. In order to make the CNN\nbranch easily learns how to extract high-quality semantic in-\nformation from the transformer branch, we design the Conv-\nFormer Block. Conv-Former Block simulates the structure\nof the transformer block as much as possible to learn the se-\nmantic information of the transformer branch better. Mean-\nwhile, the Conv-Former Block implements the attention\nfunction using only efficient convolution operations.\nThe structure of the Conv-Former Block is similar to the\nstructure of a typical transformer encoder (Vaswani et al.\n2017), as presented in the left of Figure 4. The process can\nbe described as follows:\nf = Norm(x + ConvAttention(x)),\ny = Norm(f + FFN (f)), (1)\nwhere Norm(¬∑) refers to batch normalization (Ioffe and\nSzegedy 2015), and x, f, y denote input, hidden feature and\noutput, respectively.\nConvolutional Attention. Attention mechanisms used for\nreal-time segmentation should have the property of low la-\ntency and powerful semantic extraction ability. As discussed\nin the related work, We believe GFA is a potential candidate.\nOur convolutional attention is derived from GFA.\nThere are two main differences between GFA and the pro-\nposed convolutional attention. Firstly, we replace the matrix\nmultiplication in GFA with pixel-wise convolution opera-\ntions. Point convolution is equivalent to pixel-to-pixel multi-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6380\nConvolution \nAttention\nAdd & Norm\nFFN\nAdd & Norm\nInput\nOutput\nGDN\nH\nW\nC\n‚Ä¶\nC\nk\nN\n‚Ä¶\nC\nk\nN\nH\nW\nC\n‚Ä¶\nC\nN\nk\nC\nkN\n‚Ä¶\nGDN\nùë≤ùíÜùíöùüè\nùë≤ùíÜùíöùüê ùëΩùíÇùíçùíñùíÜùüê\nùëΩùíÇùíçùíñùíÜùüè\nùë∏ùíñùíÜùíìùíö\nFigure 4: Design of Conv-Former Block (left) and the de-\ntails of convolutional attention (right). GDN means Grouped\nDouble Normalization. ‚äó means convolution operations, ‚äï\nstands for addition, and k means the kernel size.\nplication but without feature flattening and reshaping opera-\ntions. These operations are detrimental to maintaining the in-\nherent spatial structure and bring in extra inference latency.\nMoreover, convolution provides a more flexible way to ex-\ntend external parameters. Then, due to the semantic gap be-\ntween the transformer and CNN, it is not enough to capture\nrich context that simply calculates the similarity between\nseveral learnable vectors and each pixel and then enhances\nthe pixels according to the similarity map and the learn-\nable vectors. To better align the semantic information of the\ntransformer, we enlarge the learnable vectors to learnable\nkernels. On the one hand, this converts the similarity calcu-\nlation between pixel and learnable vectors to that between\npixel patches with learnable kernels. On the other hand, the\nconvolution operation with learnable kernels retains more\nlocal spatial information to some extent. The operations of\nconvolution attention can be summarized as follows:\nX = Œ∏ (X ‚äó K) ‚äó KT , (2)\nwhere X ‚àà RC√óH√óW ,K ‚àà RC√óN√ók√ók, KT ‚àà\nRN√óC√ók√ók represents input image and learnable query and\nkey, respectively. C, H, W denote the channel, height, and\nwidth of the feature map, respectively. N denotes the num-\nber of learnable parameters, and k denotes the kernel size of\nthe learnable parameters. Œ∏ symbolizes the grouped double\nnormalization, which applies softmax on the dimension of\nH √ó W and grouped L2 Norm on the dimension of N. ‚äó\nmeans convolution operations.\nTaking efficiency into consideration, we implement the\nconvolution attention with stripe convolution rather than\nstandard convolutions. More specifically, we utilize a 1 √ó k\nand a k √ó 1 convolution to approximate a k √ó k convolu-\ntion layer. Figure 4 illustrate the implementation details of\nconvolution attention.\nFeed Forward Network.Typical FFN plays a vital role in\nproviding position encoding and embedding channels. The\ntypical FFN (Feed Forward Network) in recent transformer\nmodels consists of a expand point convolution, a depth-wise\n3√ó3 convolution, and a squeeze point convolution. Different\nfrom typical FFN, our FFN is made up of two standard 3 √ó\n3 convolution layers. Compared with the typical FFN, our\nFFN is more efficient and provides a larger receptive field.\nSemantic Information Alignment Module\nA simple yet effective alignment module is proposed to con-\nduct the feature learning in the training process, as shown in\nFigure 3. It can be divided into backbone feature alignment\nand shared decoder head alignment.\nBackbone Feature Alignment.Thanks to the transformer-\nlike architecture of the Conv-Former Block, the alignment\nloss can easily align the Conv-Former Block‚Äôs features with\nthe features of transformers. In short, the backbone feature\nalignment first down-sample or up-sample the feature from\nthe transformer and CNN branches for alignment. Then it\nprojects the feature of the CNN to the dimension of the trans-\nformer. The projection can: 1) unify the number of channels\nand 2) avoid direct alignment of features, which damages\nthe supervision of ground truth for the CNN in the training\nprocess. Finally, a semantic alignment loss is applied to the\nprojected features to align the semantic representations.\nShared Decoder Head Alignment.Transformer decoders\noften use the features of multiple stages for complex de-\ncoding, while SCTNet decoder only picks the features of\nstage2&stage4. Considering the significant difference in de-\ncoding space between them, direct alignment of the decod-\ning features and output logits can only get limited improve-\nment. Therefore, we propose shared decoder head align-\nment. Specifically, the concatenation stage2&stage4 features\nof the single-branch CNN are input into a point convolution\nto expand the dimension. Then the high-dimension features\nare passed through the transformer decoder. The transformer\ndecoder‚Äôs new output features and logits are used to calcu-\nlate alignment loss with its origin outputs.\nOverall Architecture\nTo reduce computational costs while obtain rich semantic\ninformation, we simplify the popular two-branches archi-\ntecture to one swift CNN branch for inference and a trans-\nformer branch for semantic alignment only for training.\nBackbone. To improve the inference speed, SCTNet adopts\na typical hierarchical CNN backbone. SCTNet starts from\na stem block consisting of two sequential 3√ó3 convolution\nlayers. The former two stages consist of stacked residual\nblocks (He et al. 2016), and the latter two stages include\nthe proposed transformer-like blocks called Conv-Former\nBlocks (CFBlocks). The CFBlock employs several elabo-\nrately designed convolution operations to perform the simi-\nlar long-range context capturing function of the transformer\nblock. We apply a convdown layer consisting of a strid-\nden convolution with batch normal and ReLu activation for\ndown-sampling at the beginning of stage 2 ‚àº 4, which is\nomitted in Figure 3 for clarity.\nDecoder Head. The decoder head consists of a\nDAPPM (Pan et al. 2022) and a segmentation head.\nTo further enrich the context information, we add a DAPPM\nafter the output of stage 4. Then we concatenate the output\nwith the feature map of Stage 2. Finally, this output feature\nis passed into a segmentation head. Precisely, the segmen-\ntation head consists of a 3√ó3 Conv-BN-ReLU operator\nfollowed by a 1√ó1 convolution classifier.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6381\nMethod Reference #Params‚Üì Resolution FPS(TRT)‚Üë FPS(Torch)‚Üë mIoU(%)‚Üë\nSFNet-ResNet18 ECCV 2020 12.3M 2048 √ó 1024 50.5 24.0 79.0\nAFFormer-B-Seg100 AAAI 2023 3.0M 2048 √ó 1024 58.3 28.4 78.7\nAFFormer-B-Seg75 AAAI 2023 3.0M 1536 √ó 768 96.4 38.6 76.5\nAFFormer-B-Seg50 AAAI 2023 3.0M 1024 √ó 512 148.4 49.5 73.5\nSegNext-T-Seg100 NeurIPS 2022 4.3M 2048 √ó 1024 46.5 28.1 79.8\nSegNext-T-Seg75 NeurIPS 2022 4.3M 1536 √ó 768 78.3 45.6 78.0\nPIDNet-S CVPR 2023 7.6M 2048 √ó 1024 127.1 93.2 78.8\nPIDNet-M CVPR 2023 34.4M 2048 √ó 1024 90.7 39.8 80.1\nCNN-based Bilateral Networks\nBiSeNet-ResNet18 ECCV 2018 49.0M 1536 √ó 768 182.9 112.3 74.8\nBiSeNetV2-L IJCV 2021 - 1024 √ó 512 102.3 67.6 75.8\nSTDC1-Seg75 CVPR 2021 14.2M 1536 √ó 768 209.5 101.9 74.5\nSTDC2-Seg75 CVPR 2021 22.2M 1536 √ó 768 149.2 84.3 77.0\nSTDC1-Seg50 CVPR 2021 14.2M 1024 √ó 512 397.6 146.2 72.2\nSTDC2-Seg50 CVPR 2021 22.2M 1024 √ó 512 279.7 94.6 74.2\nDDRNet-23-S TIP 2022 5.7M 2048 √ó 1024 138.9 106.7 77.8\nDDRNet-23 TIP 2022 20.1M 2048 √ó 1024 101.9 56.7 79.5\nTransformer-based Bilateral Networks\nTopFormer-B-Seg100 CVPR 2022 5.1M 2048 √ó 1024 128.4 81.4 76.3\nTopFormer-B-Seg50 CVPR 2022 5.1M 1024 √ó 512 410.9 95.7 70.7\nSeaFormer-B-Seg100 ICLR 2023 8.6M 2048 √ó 1024 103.6 37.5 77.7\nSeaFormer-B-Seg50 ICLR 2023 8.6M 1024 √ó 512 231.6 45.2 72.2\nRTFormer-S NeurIPS 2022 4.8M 2048 √ó 1024 - 89.6 76.3\nRTFormer-B NeurIPS 2022 16.8M 2048 √ó 1024 - 50.2 79.3\nSCTNet-S-Seg50 Ours 4.6M 1024 √ó 512 451.2 160.3 72.8\nSCTNet-S-Seg75 Ours 4.6M 1536 √ó 768 233.3 149.2 76.1\nSCTNet-B-Seg50 Ours 17.4M 1024 √ó 512 374.6 144.9 76.5\nSCTNet-B-Seg75 Ours 17.4M 1536 √ó 768 186.6 105.2 79.8\nSCTNet-B-Seg100 Ours 17.4M 2048 √ó 1024 105.0 62.8 80.5\nTable 1: Comparisons with other state-of-the-art real-time methods on Cityscapes val set. Seg100, Seg75, Seg50 denote the\ninput size of 1024 √ó 2048, 768 √ó 1536, 512 √ó 1024, respectively. #Params refers to the number of parameters.\nTraining Phase. It is well known that transformer excels\nat capturing global semantic context. On the other hand,\nCNN has been widely proven to be better at modeling hierar-\nchical locality information than transformers. Motivated by\nthe advantages of transformer and CNN, we explore equip-\nping a real-time segmentation network with both merits. We\npropose a single-branch CNN that learns to align its fea-\ntures with those of a powerful transformer, which is illus-\ntrated in the blue dotted box in Figure 3. This feature align-\nment enables the single-branch CNN to extract both rich\nglobal context and detailed spatial information. Specifically,\nthere are two streams in the training phase. SCTNet adopts\na train-only transformer as the semantic branch to extract\npowerful global semantic context. The semantic information\nalignment module supervises the convolution branch to align\nhigh-quality global context from the transformer.\nInference Phase. To avoid the sizeable computation costs\nof two branches, only the CNN branch is deployed in the in-\nference. With the transformer-aligned semantic information,\nthe single-branch CNN can generate accurate segmentation\nresults without the extra semantic extraction or costly dense\nfusion. Specifically, the input image is fed into a single-\nbranch hierarchy convolution backbone. Then the decoder\nhead picks up the features in the backbone and conducts sim-\nple concatenation followed by pixel-wise classification.\nAlignment Loss\nFor better alignment of semantic information, a alignment\nloss focusing on semantic information rather than spatial in-\nformation is needed. In the implementation, we use CWD\nLoss (channel-wise distillation loss) (Shu et al. 2021) as the\nalignment loss, which shows better results than other loss\nfunctions. CWD Loss can be summarized as follows:\nœï(xc) = exp(xc,i\nT )\nPW¬∑H\ni=1 exp(xc,i\nT )\n, (3)\nLcwd = T 2\nC\nCX\nc=1\nH¬∑WX\ni=1\nœï(xc,i\nT ) ¬∑ log\nhœï(xc,i\nT )\nœï(xc,i\nS )\ni\n, (4)\nwhere c = 1 , 2, ..., Cindexes the channel, and i =\n1, 2, ..., H¬∑ W denotes the spatial location, xT and xS are\nthe feature maps of the transformer branch and CNN branch,\nrespectively.œï converts the feature activation into a channel-\nwise probability distribution, removing the influences of\nscales between the transformer and the compact CNN. To\nminimize Lcwd, œï(xc,i\nS ) should be large when œï(xc,i\nT ) is\nlarge. But when œï(xc,i\nT ) is small, the value of œï(xc,i\nS ) does\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6382\nnot matter. This force the CNN to learn the distribution of the\nforeground salience, which contains the semantic informa-\ntion. T denotes a hyper-parameter called temperature. And\nthe larger T is, the softer the probability distribution is.\nExperiments\nDatasets and Implementation Details\nWe conduct experiments of the SCTNet on three datasets,\ni.e., Cityscapes (Cordts et al. 2016), ADE20K (Zhou et al.\n2017), and COCO-Stuff-10K (Caesar, Uijlings, and Ferrari\n2018) to demonstrate the effectiveness of our method. For a\nfair comparison, we build our base model SCTNet-B with\na comparable size to RTFormer-B/DDRNet-23/STDC2.\nFurthermore, we also introduce a smaller variant called\nSCTNet-S. We first pre-train our CNN backbones on Ima-\ngeNet (Deng et al. 2009), then fine-tune it on semantic seg-\nmentation datasets. The semantic transformer branch in the\ntraining phrase can be any hierarchical transformer network.\nIn our implementation, we choose SegFormer as the trans-\nformer branch for all experiments. We measure the inference\nspeed of all methods on a single NVIDIA RTX 3090. All\nreported FPS results are obtained under the same input reso-\nlution for fair performance comparison unless specified. For\nCityscapes, we measure the speed implemented with both\ntorch and tensor-RT.\nComparison with State-of-the-art Methods\nResults on Cityscapes. The corresponding results on\nCityscapes(Cordts et al. 2016) are shown in Table 1. Our\nSCTNet outperforms other real-time methods by a large\nmargin and attains the best speed-accuracy trade-off with\nboth tensorRT and Torch implementations. For example,\nour SCTNet-B-Seg100 achieves 80.5% mIoU at 62.8 FPS,\nwhich is a new state-of-the-art performance for real-time\nsegmentation. Our SCTNet-B-Seg75 reaches 79.8% mIoU,\nwhich is better than the state-of-the-art transformer-based\nbilateral network RTFormer-B and cnn-based bilateral net-\nwork DDRNet-23 in accuracy but has a two times faster\nspeed. Our SCTNet-B is faster at all input resolutions with\nbetter mIoU results than all other methods. Besides, our\nSCTNet-S also achieves a better trade-off compared with\nSTDC2 (Fan et al. 2021), RTFormer-S (Wang et al. 2022),\nSeaFormer-B (Wan et al. 2023) and TopFormer-B (Zhang\net al. 2022).\nResults on ADE20K. On ADE20K(Zhou et al. 2017),\nour SCTNet achieves the best accuracy with the fastest\nspeed. For instance, our SCTNet-B achieves 43.0% mIoU\nat superior 145.1 FPS, which is about 1.6 times faster\nthan RTFormer-B (Wang et al. 2022) with 0.9% higher\nmIoU performance. Our SCTNet-S reaches 37.7% mIoU\nwhile keeping the highest FPS among all other methods on\nADE20K (Zhou et al. 2017). Considering the large variety of\nimages and various semantic categories in ADE20K (Zhou\net al. 2017), this outstanding results further also demonstrate\nthe generalization capability of our SCTNet.\nResults on COCO-Stuff-10K. The corresponding results\non COCO-Stuff-10K are shown in Table 3. SCTNet shows\nSOTA performance and maintains the highest inference\nMethod #Params‚Üì FPS‚Üë mIoU(%)‚Üë\nFCN(MV2) 9.8M 64.4‚àó 19.7\nPSPNet(MV2) 13.7M 57.7‚àó 29.6\nDeepLabV3+(MV2) 15.4M 43.1‚àó 34.0\nSegFormerB0 3.8M 84.4 37.4\nTopFormer-B 5.1M 96.2 39.2\nSeaFormer-B 8.6M 44.5 41.0\nSegNext-T 4.3M 60.3 41.1\nAFFormer-B 3.0M 49.6 41.8\nRTFormer-S 4.8M 95.2 36.7\nRTFormer-B 16.8M 93.4 42.1\nSCTNet-S 4.7M 158.4 37.7\nSCTNet-B 17.4M 145.1 43.0\nTable 2: Comparisons with other state-of-the-art real-time\nmethods on ADE20K. The FPS is measured at resolution\n512 √ó 512. * means speed from other papers, MV2 stands\nfor MobileNetV2.\nMethod #Params‚Üì FPS‚Üë mIoU(%)‚Üë\nPSPNet50 - 6.6‚àó 32.6\nICNet - 35.7‚àó 29.1\nBiSeNetV2-L - 65.1 28.7\nTopFormer-B 5.1M 94.7 33.4\nSeaFormer-B 8.6M 41.9 34.1\nAFFormer-B 3.0M 46.5 35.1\nDDRNet23 20.1M 108.8 32.1\nRTFormer-B 16.8M 90.9 35.3\nSCTNet-B 17.4M 141.5 35.9\nTable 3: Comparisons with other state-of-the-art real-time\nmethods on COCO-Stuff-10K test set. The FPS is measured\nat resolution 640 √ó 640.\nspeed on COCO-Stuff-10K in real-time semantic segmen-\ntation methods. With the input size 640 √ó 640, SCTNet-B\nachieves 35.9% mIoU at 141.5 FPS, which is 0.6% higher\nthan RTFormer-B, and about 1.6 times faster.\nAblation Study\nComparison on Different Types of Blocks.To verify the\neffectiveness of our proposed CFBlock, we replace the CF-\nBlocks with other kinds of convolution blocks and trans-\nformer blocks in real-time segmentation. For quick evalu-\nations, all these results in Table 4 are not pre-trained on Im-\nageNet. We select four kinds of blocks for comparison. As\nshown in Table 4, our CFBlock outperforms the typical Res-\nBlock FPS‚Üë mIoU(%)‚Üë param\nResBlock 66.7 77.9 15.3M\nSegFormerBlock 57.3 77.7 22.2M\nGFABlock 66.2 78.5 16.3M\nMSCANBlock 60.5 79.3 19.8M\nCFBlock (Ours) 62.8 79.4 17.4M\nTable 4: Comparison of different blocks.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6383\n(a) Image (b) GT (c) DDRNet-23 (d) RTFormer-B (e) SCTNet-B\nFigure 5: Visualization results on Cityscapes validation set. Compared with DDRNet-23(Pan et al. 2022) and RTFormer-\nB (Wang et al. 2022), SCTNet-B generates masks with finer details as highlighted in the light blue box and more accurate\nlarge-area predictions, as highlighted in the yellow box.\nBlock and the lightweight SegFormer Block by a significant\nmIoU margin. Moreover, compared with the state-of-the-\nart GFABlock (Wang et al. 2022) and MSCANBlock from\nSegNext (Guo et al. 2022b), our CFBlock get better speed\nand accuracy trade-off. Our CFBlock has 0.9% higher mIoU\nthan GFABlock and maintains the similar performance with\nfewer parameters and faster speed than MSCANBlock. This\nalso demonstrates that our SCTNet can better mitigate the\ngap of semantic information between CNN and transformer\nwhile getting rid of the high computation cost.\nEffectiveness of the Semantic Information Alignment\nModule. Although our SIAM(semantic information align-\nment module) is closely related to the elaborately designed\nSCTNet, it can also improve the performance of other CNN\nand transformer segmentation methods. As presented in Ta-\nble 5, employing our SIAM attains consistent improvements\non SegFormer, SegNext, SeaFormer, and DDRNet, which\nproves the effectiveness and generalization capability of our\nproposed SIAM. At the same time, as representatives of the\nbilateral-branch transformer and the bilateral-branch CNN\nnetwork, the improvements of SeaFormer and DDRNet are\nrelatively slim. This may be attributed to the fact that their\nbilateral-branch network structure already benefits from the\nadditional semantic branch. And this also confirms that the\ncooperation of our SIMA and training-only transformer does\nplay the role of the semantic branch in the bilateral-branch\nnetwork, leading to improvements in the accuracy of the\nsingle-branch network.\nComponents Ablation. We explore the effect of the pro-\nposed components in Table 6. Take Seg100 as an example,\nsimply replacing the Resblock with our CFBlock brings a\n2.1% improvement of mIoU with little speed loss. The BFA\nleads to a 1.2% higher mIoU, and the SDHA further attains\na 0.8% improvement of mIoU without sacrificing speed.\nVisualization Results\nFigure 5 shows visualization results on Cityscapes (Cordts\net al. 2016) validation set. Compared with DDRNet and\nBlock Seg100(%) Seg75(%) Seg50(%)\nSegNe\nxt-T 79.8 78.0 -\nSegNe\nxt-T+SIAM 80.1(+0.3) 78.2(+0.2) -\nSegF\normer-B0 74.7 74.4 70.7\nSegF\normer-B0+SIAM 77.3(+2.6) 76.8(+2.4) 72.5(+1.8)\nSeaFormer\n-B 77.7 - 72.2\nSeaFormer\n-B+SIAM 78.1(+0.4) - 72.5(+0.3)\nDDRNet-23 79.5 - -\nDDRNet-23+SIAM 79.6(+0.1) - -\nSCTNet-B-SIAM 78.5 77.5 75.2\nSCTNet-B (Ours) 80.5(+2.0) 79.8(+2.3) 76.5(+1.3)\nTable 5: Comparison of the effect of the SIAM.\nComponents Seg100(%) Seg75(%) Seg50(%) FPS(Seg100)\nBaseline 76.4 76.0 73.0 66.7\n+CFBlock 78.5(+2.1) 77.5(+1.5) 75.2(+2.2) 62.8\n+BFA‚àó 79.7(+1.2) 79.1(+1.6) 75.7(+0.5) 62.8\n+SDHA 80.5(+0.8) 79.8(+0.7) 76.5(+0.8) 62.8\nTable 6: Ablation studies on the components of SCTNet\nRTFormer, our SCTNet provides not only better results for\nthose classes with large areas like roads, sidewalks, and big\ntrucks but also more accurate boundaries for small or thin\nobjects such as poles, traffic lights, traffic signs, and cars.\nThis indicates that SCTNet extracts high-quality long-range\ncontext while preserving fine details.\nConclusion\nIn this paper, we propose SCTNet, a novel single-branch\narchitecture that can extract high-quality long-range con-\ntext without extra inference computation cost. Extensive ex-\nperiments demonstrate that SCTNet achieves new state-of-\nthe-art results. Moreover, by demonstrating the efficiency of\nSCTNet, we provide a novel insight for the semantic branch\nin the bilateral-branch network and a new way to boost the\nreal-time segmentation community by not only adopting the\nstructure of the transformer but also unitizing its knowledge.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6384\nAcknowledgments\nThis work is supported by Hubei Provincial Natural Science\nFoundation of China No.2022CFA055 and the National Nat-\nural Science Foundation of China No.62176097.\nReferences\nBo, D.; Pichao, W.; and Wang, F. 2023. AFFormer:\nHead-Free Lightweight Semantic Segmentation with Linear\nTransformer. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence.\nCaesar, H.; Uijlings, J.; and Ferrari, V . 2018. Coco-stuff:\nThing and stuff classes in context. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 1209‚Äì1218.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2014. Semantic image segmentation with deep\nconvolutional nets and fully connected crfs. arXiv preprint\narXiv:1412.7062.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2017. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully\nconnected crfs. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 40(4): 834‚Äì848.\nChen, L.-C.; Zhu, Y .; Papandreou, G.; Schroff, F.; and\nAdam, H. 2018. Encoder-decoder with atrous separable\nconvolution for semantic image segmentation. In European\nConference on Computer Vision, 801‚Äì818.\nCordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,\nM.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.\n2016. The cityscapes dataset for semantic urban scene un-\nderstanding. In IEEE Conference on Computer Vision and\nPattern Recognition, 3213‚Äì3223.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, 248‚Äì255. Ieee.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFan, M.; Lai, S.; Huang, J.; Wei, X.; Chai, Z.; Luo, J.; and\nWei, X. 2021. Rethinking bisenet for real-time semantic seg-\nmentation. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 9716‚Äì9725.\nFang, J.; Xie, L.; Wang, X.; Zhang, X.; Liu, W.; and Tian,\nQ. 2022. Msg-transformer: Exchanging local spatial infor-\nmation by manipulating messenger tokens. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n12063‚Äì12072.\nFu, J.; Liu, J.; Tian, H.; Li, Y .; Bao, Y .; Fang, Z.; and Lu,\nH. 2019. Dual attention network for scene segmentation.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3146‚Äì3154.\nGuo, M.-H.; Liu, Z.-N.; Mu, T.-J.; and Hu, S.-M. 2022a. Be-\nyond self-attention: External attention using two linear lay-\ners for visual tasks. IEEE Transactions on Pattern Analysis\nand Machine Intelligence.\nGuo, M.-H.; Lu, C.-Z.; Hou, Q.; Liu, Z.; Cheng, M.-M.; and\nHu, S.-M. 2022b. Segnext: Rethinking convolutional atten-\ntion design for semantic segmentation. Advances in Neural\nInformation Processing Systems, 35: 1140‚Äì1156.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. InIEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 770‚Äì778.\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y .; and\nLiu, W. 2019. Ccnet: Criss-cross attention for semantic seg-\nmentation. In IEEE International Conference on Computer\nVision, 603‚Äì612.\nIoffe, S.; and Szegedy, C. 2015. Batch normalization: Accel-\nerating deep network training by reducing internal covariate\nshift. In International Conference on Machine Learning,\n448‚Äì456. pmlr.\nLi, X.; You, A.; Zhu, Z.; Zhao, H.; Yang, M.; Yang, K.; Tan,\nS.; and Tong, Y . 2020. Semantic flow for fast and accurate\nscene parsing. In European Conference on Computer Vision,\n775‚Äì793. Springer.\nLin, G.; Milan, A.; Shen, C.; and Reid, I. 2017. Refinenet:\nMulti-path refinement networks for high-resolution seman-\ntic segmentation. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 1925‚Äì1934.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 10012‚Äì\n10022.\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convo-\nlutional networks for semantic segmentation. In IEEE Con-\nference on Computer Vision and Pattern Recognition, 3431‚Äì\n3440.\nPan, H.; Hong, Y .; Sun, W.; and Jia, Y . 2022. Deep dual-\nresolution networks for real-time and accurate semantic seg-\nmentation of traffic scenes.IEEE Transactions on Intelligent\nTransportation Systems, 24(3): 3448‚Äì3460.\nPaszke, A.; Chaurasia, A.; Kim, S.; and Culurciello, E. 2016.\nEnet: A deep neural network architecture for real-time se-\nmantic segmentation. arXiv preprint arXiv:1606.02147.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-\nvolutional networks for biomedical image segmentation. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, 234‚Äì241. Springer.\nShu, C.; Liu, Y .; Gao, J.; Yan, Z.; and Shen, C. 2021.\nChannel-wise knowledge distillation for dense prediction.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5311‚Äì5320.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. Neural Information Processing Sys-\ntems, 30.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6385\nWan, Q.; Huang, Z.; Lu, J.; Yu, G.; and Zhang, L.\n2023. SeaFormer: Squeeze-enhanced Axial Transformer\nfor Mobile Semantic Segmentation. arXiv preprint\narXiv:2301.13156.\nWang, J.; Gou, C.; Wu, Q.; Feng, H.; Han, J.; Ding, E.; and\nWang, J. 2022. RTFormer: Efficient Design for Real-Time\nSemantic Segmentation with Transformer. In Advances in\nNeural Information Processing Systems.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 568‚Äì578.\nWu, Z.; Shen, C.; and Hengel, A. v. d. 2017. Real-time\nsemantic image segmentation via spatial sparsity. arXiv\npreprint arXiv:1712.00213.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34: 12077‚Äì12090.\nXu, J.; Xiong, Z.; and Bhattacharyya, S. P. 2023. PIDNet:\nA Real-Time Semantic Segmentation Network Inspired by\nPID Controllers. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 19529‚Äì\n19539.\nYu, C.; Gao, C.; Wang, J.; Yu, G.; Shen, C.; and Sang, N.\n2021. Bisenet v2: Bilateral network with guided aggregation\nfor real-time semantic segmentation. International Journal\nof Computer Vision, 129: 3051‚Äì3068.\nYu, C.; Wang, J.; Peng, C.; Gao, C.; Yu, G.; and Sang, N.\n2018. BiSeNet: Bilateral segmentation network for real-\ntime semantic segmentation. In European Conference on\nComputer Vision, 325‚Äì341.\nYuan, Y .; Huang, L.; Guo, J.; Zhang, C.; Chen, X.; and\nWang, J. 2018. Ocnet: Object context network for scene\nparsing. arXiv preprint arXiv:1809.00916.\nZhang, W.; Huang, Z.; Luo, G.; Chen, T.; Wang, X.; Liu,\nW.; Yu, G.; and Shen, C. 2022. TopFormer: Token pyra-\nmid transformer for mobile semantic segmentation. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 12083‚Äì12093.\nZhao, H.; Qi, X.; Shen, X.; Shi, J.; and Jia, J. 2018a. Icnet for\nreal-time semantic segmentation on high-resolution images.\nIn European Conference on Computer Vision, 405‚Äì420.\nZhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyramid\nscene parsing network. In IEEE Conference on Computer\nVision and Pattern Recognition, 2881‚Äì2890.\nZhao, H.; Zhang, Y .; Liu, S.; Shi, J.; Loy, C. C.; Lin, D.; and\nJia, J. 2018b. Psanet: Point-wise spatial attention network\nfor scene parsing. In European Conference on Computer\nVision, 267‚Äì283.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 6881‚Äì6890.\nZhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; and\nTorralba, A. 2017. Scene parsing through ade20k dataset. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 633‚Äì641.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6386",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.6254153251647949
    },
    {
      "name": "Computer science",
      "score": 0.6062573790550232
    },
    {
      "name": "Transformer",
      "score": 0.49028971791267395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4622020721435547
    },
    {
      "name": "Natural language processing",
      "score": 0.4112016260623932
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3410833775997162
    },
    {
      "name": "Electrical engineering",
      "score": 0.11918920278549194
    },
    {
      "name": "Engineering",
      "score": 0.10214060544967651
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}